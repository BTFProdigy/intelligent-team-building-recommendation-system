First Joint Conference on Lexical and Computational Semantics (*SEM), pages 282?287,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UCM-I: A Rule-based Syntactic Approach for Resolving the Scope of
Negation
Jorge Carrillo de Albornoz, Laura Plaza, Alberto D??az and Miguel Ballesteros
Universidad Complutense de Madrid
C/ Prof. Jose? Garc??a Santesmases, s/n
28040 Madrid (Spain)
{jcalbornoz,lplazam,albertodiaz,miballes}@fdi.ucm.es
Abstract
This paper presents one of the two contribu-
tions from the Universidad Complutense de
Madrid to the *SEM Shared Task 2012 on Re-
solving the Scope and Focus of Negation. We
describe a rule-based system for detecting the
presence of negations and delimitating their
scope. It was initially intended for process-
ing negation in opinionated texts, and has been
adapted to fit the task requirements. It first
detects negation cues using a list of explicit
negation markers (such as not or nothing), and
infers other implicit negations (such as affixal
negations, e.g, undeniable or improper) by us-
ing semantic information from WordNet con-
cepts and relations. It next uses the informa-
tion from the syntax tree of the sentence in
which the negation arises to get a first approxi-
mation to the negation scope, which is later re-
fined using a set of post-processing rules that
bound or expand such scope.
1 Introduction
Detecting negation is important for many NLP tasks,
as it may reverse the meaning of the text affected
by it. In information extraction, for instance, it is
obviously important to distinguish negated informa-
tion from affirmative one (Kim and Park, 2006). It
may also improve automatic indexing (Mutalik et
al., 2001). In sentiment analysis, detecting and deal-
ing with negation is critical, as it may change the
polarity of a text (Wiegand et al, 2010). How-
ever, research on negation has mainly focused on the
biomedical domain, and addressed the problem of
detecting if a medical term is negated or not (Chap-
man et al, 2001), or the scope of different negation
signals (Morante et al, 2008).
During the last years, the importance of process-
ing negation is gaining recognition by the NLP re-
search community, as evidenced by the success of
several initiatives such as the Negation and Spec-
ulation in Natural Language Processing workshop
(NeSp-NLP 2010)1 or the CoNLL-2010 Shared
Task2, which aimed at identifying hedges and their
scope in natural language texts. In spite of this, most
of the approaches proposed so far deal with negation
in a superficial manner.
This paper describes our contribution to the
*SEM Shared Task 2012 on Resolving the Scope
and Focus of Negation. As its name suggests, the
task aims at detecting the scope and focus of nega-
tion, as a means of encouraging research in negation
processing. In particular, we participate in Task 1:
scope detection. For each negation in the text, the
negation cue must be detected, and its scope marked.
Moreover, the event or property that is negated must
be recognized. A comprehensive description of the
task may be found in (Morante and Blanco, 2012).
For the sake of clarity, it is important to define
what the organization of the task understands by
negation cue, scope of negation and negated event.
The words that express negation are called negation
cues. Not and no are common examples of such
cues. Scope is defined as the part of the mean-
ing that is negated, and encloses all negated con-
cepts. The negated event is the property that is
1http://www.clips.ua.ac.be/NeSpNLP2010/
2www.inf.u-szeged.hu/rgai/conll2010st/
282
negated by the cue. For instance, in the sentence:
[Holmes] did not [say anything], the scope is en-
closed in square brackets, the negation cue is under-
lined and the negated event is shown in bold. More
details about the annotation of negation cues, scopes
and negated events may be found in (Morante and
Daelemans, 2012).
The system presented to the shared task is an
adaptation of the one published in (Carrillo de Al-
bornoz et al, 2010), whose aim was to detect and
process negation in opinionated text in order to im-
prove polarity and intensity classification. When
classifying sentiments and opinions it is important
to deal with the presence of negations and their ef-
fect on the emotional meaning of the text affected by
them. Consider the sentence (1) and (2). Sentence
(1) expresses a positive opinion, whereas that in sen-
tence (2) the negation word not reverses the polarity
of such opinion.
(1) I liked this hotel.
(2) I didn?t like this hotel.
Our system has the main advantage of being sim-
ple and highly generic. Even though it was origi-
nally conceived for treating negations in opinionated
texts, a few simple modifications have been suffi-
cient to successfully address negation in a very dif-
ferent type of texts, such as Conan Doyle stories. It
is rule-based and does not need to be trained. It also
uses semantic information in order to automatically
detect the negation cues.
2 Methodology
As already told, the UCM-I system is a modified ver-
sion of the one presented in (Carrillo de Albornoz
et al, 2010). Next sections detail the modifications
performed to undertake the present task.
2.1 Detecting negation cues
Our previous work was focused on explicit nega-
tions (i.e., those introduced by negation tokens such
as not, never). In contrast, in the present work
we also consider what we call implicit negations,
which includes affixal negation (i.,e., words with
prefixes such as dis-, un- or suffixes such as -less;
e.g., impatient or careless), inffixal negation (i.e.,
pointlessness, where the negation cue less is in the
middle of the noun phrase). Note that we did not
Table 1: Examples of negation cues.
Explicit negation cues
no not non nor
nobody never nowhere ...
Words with implicit negation cues
unpleasant unnatural dislike impatient
fearless hopeless illegal ...
have into account these negation cues when ana-
lyzing opinionated texts because these words them-
selves usually appear in affective lexicons with their
corresponding polarity values (i.e., impatient, for in-
stance, appears in SentiWordNet with a negative po-
larity value).
In order to detect negation cues, we use a list of
predefined negation signals, along with an automatic
method for detecting new ones. The list has been
extracted from different previous works (Councill et
al., 2010; Morante, 2010). This list also includes the
most frequent contracted forms (e.g., don?t, didn?t,
etc.). The automated method, in turn, is intended
for discovering in text new affixal negation cues. To
this end, we first find in the text all words with pre-
fixes dis-, a-, un-, in-, im-, non-, il-, ir- and the suf-
fix -less that present the appropriate part of speech.
Since not all words with such affixes are negation
cues, we use semantic information from WordNet
concepts and relations to decide. In this way, we re-
trieve from WordNet the synset that correspond to
each word, using WordNet::SenseRelate (Patward-
han et al, 2005) to correctly disambiguate the mean-
ing of the word according to its context, along with
all its antonym synsets. We next check if, after re-
moving the affix, the word exists in WordNet and
belongs to any of the antonym synsets. If so, we
consider the original word to be a negation cue (i.e.,
the word without the affix has the opposite meaning
than the lexical item with the affix).
Table 1 presents some examples of explicit nega-
tion cues and words with implicit negation cues. For
space reasons, not all cues are shown. We also con-
sider common spelling errors such as the omission
of apostrophes (e.g., isnt or nt). They are not likely
to be found in literary texts, but are quite frequent in
user-generated content.
This general processing is, however, improved
with two rules:
283
Table 2: Examples of false negation cues.
no doubt without a doubt not merely not just
not even not only no wonder ...
1. False negation cues: Some negation words
may be also used in other expressions with-
out constituting a negation, as in sentence (3).
Therefore, when the negation token belongs
to such expressions, this is not processed as a
negation. Examples of false negation cues are
shown in Table 2.
(3) ... the evidence may implicate not only your
friend Mr. Stapleton but his wife as well.
2. Tag questions: Some sentences in the cor-
pora present negative tag questions in old En-
glish grammatical form, as it may shown in
sentences (4) and (5). We have implemented a
specific rule to deal with this type of construc-
tions, so that they are not treated as negations.
(4) You could easily recognize it , could you not?.
(5) But your family have been with us for several
generations , have they not?
2.2 Delimiting the scope of negation
The scope of a negation is determined by using the
syntax tree of the sentence in which the negation
arises, as generated by the Stanford Parser.3 To this
end, we find in the syntax tree the first common an-
cestor that encloses the negation token and the word
immediately after it, and assume all descendant leaf
nodes to the right of the negation token to be af-
fected by it. This process may be seen in Figure
1, where the syntax tree for the sentence: [Watson
did] not [solve the case] is shown. In this sentence,
the method identifies the negation token not and as-
sumes its scope to be all descendant leaf nodes of the
common ancestor of the words not and solve (i.e.,
solve the case).
This modeling has the main advantage of being
highly generic, as it serves to delimit the scope of
negation regardless of what the negated event is (i.e.,
the verb, the subject, the object of the verb, an ad-
jective or an adverb). As shown in (Carrillo de Al-
3http://nlp.stanford.edu/software/lex-parser.shtml
Figure 1: Syntax tree of the sentence: Watson did not
solve the case.
bornoz et al, 2010), it behaves well when determin-
ing the scope of negation for the purpose of classi-
fying product reviews in polarity classes. However,
we have found that this scope is not enough for the
present task, and thus we have implemented a set of
post-processing rules to expand and limit the scope
according to the task guidelines:
1. Expansion to subject. This rule expands the
negation scope in order to include the subject of
the sentence within it. In this way, in sentence
(6) the appropriate rule is fired to include ?This
theory? within the negation scope.
(6) [This theory would] not [work].
It must be noted that, for polarity classifica-
tion purposes, we do not consider the subject
of the sentence to be part of this scope. Con-
sider, for instance, the sentence: The beauti-
ful views of the Eiffel Tower are not guaranteed
in all rooms. According to traditional polarity
classification approaches, if the subject is con-
sidered as part of the negation scope, the polar-
ity of the positive polar expression ?beautiful?
should be changed, and considered as negative.
2. Subordinate boundaries. Our original nega-
tion scope detection method works well with
coordinate sentences, in which negation cues
scope only over their clause, as if a ?boundary?
exists between the different clauses. This oc-
curs, for instance, in the sentence:
284
Table 3: List of negation scope delimiters.
Tokens POS
so, because, if, while
INuntil, since, unless
before, than, despite IN
what, whose WP
why, where WRB
however RB
?,?, - , :, ;, (, ), !, ?, . -
(7) [It may be that you are] not [yourself lumi-
nous], but you are a conductor of light.
It also works properly in subordinate sentences,
when the negation occurs in the subordinate
clause, as in: You can imagine my surprise
when I found that [there was] no [one there].
However, it may fail in some types of subor-
dinate sentences, where the scope should be
limited to the main clause, but our model pre-
dict both clauses to be affected by the negation.
This is the case for the sentences where the de-
pendent clause is introduced by the subordinate
conjunctions in Table 3. An example of such
type of sentence is (8), where the conjunction
token because introduces a subordinate clause
which is out of the negation scope. To solve this
problem, the negation scope detection method
includes a set of rules to delimit the scope in
those cases, using as delimiters the conjunc-
tions in Table 3. Note that, since some of these
delimiters are ambiguous, their part of speech
tags are used to disambiguate them.
(8) [Her father] refused [to have anything to do
with her] because she had married without his
consent.
3. Prepositional phrases: Our original method
also fails to correctly determine the negation
scope when the negated event is followed by
a prepositional phrase, as it may be seen in
Figure 2, where the syntax tree for the sen-
tence: [There was] no [attempt at robbery] is
shown. Note that, according to our original
model, the phrase ?at robbery? does not belong
to the negation scope. This is an error that was
not detected before, but has been fixed for the
present task.
Figure 2: Syntax tree for the sentence: There was no at-
tempt at robbery.
2.3 Finding negated events
We only consider a single type of negated events,
so that, when a cue word contains a negative affix,
the word after removing the affix is annotated as the
negated event. In this way, ?doubtedly? is correctly
annotated as the negated event in sentence (9). How-
ever, the remaining types of negated events are rele-
gated to future work.
(9) [The oval seal is] undoubtedly [a plain
sleeve-link].
3 Evaluation Setup
The data collection consists of a development set, a
training set, and two test sets of 787, 3644, 496 and
593 sentences, respectively from different stories by
Conan Doyle (see (Morante and Blanco, 2012) for
details). Performance is measured in terms of recall,
precision and F-measure for the following subtasks:
? Predicting negation cues.
? Predicting both the scope and cue.
? Predicting the scope, the cue does not need to
be correct.
? Predicting the scope tokens, where not a full
scope match is required.
? Predicting negated events.
? Full evaluation, which requires all elements to
be correct.
285
Table 4: Results for the development set.
Metric Pr. Re. F-1
Cues 92.55 86.13 89.22
Scope (cue match) 86.05 44.05 58.27
Scope (no cue match) 86.05 44.05 58.27
Scope tokens (no cue match) 88.05 59.05 70.69
Negated (no cue match) 65.00 10.74 18.43
Full negation 74.47 20.23 31.82
4 Evaluation Results
The results of our system when evaluated on the de-
velopment set and the two test sets (both jointly and
separately), are shown in Tables 4, 5, and 6.
It may be seen from these tables that our sys-
tem behaves quite well in the prediction of negation
cues subtask, achieving around 90% F-measure in
all data sets, and the second position in the com-
petition. Performance in the scope prediction task,
however, is around 60% F-1, and the same results
are obtained if the correct prediction of cues is re-
quired (Scope (cue match)). This seems to indicate
that, for all correct scope predictions, our system
have also predicted the negation cues correctly. Ob-
viously these results improve for the Scope tokens
measure, achieving more than 77% F-1 for the Card-
board data set. We also got the second position in
the competition for these three subtasks. Concerning
detection of negated events, our system gets poor re-
sults, 22.85% and 19.81% F-1, respectively, in each
test data set. These results affect the performance
of the full negation prediction task, where we get
32.18% and 32.96% F-1, respectively. Surprisingly,
the result in the test sets are slightly better than those
in the development set, and this is due to a better be-
havior of the WordNet-based cue detection method
in the formers than in the later.
5 Discussion
We next discuss and analyze the results above.
Firstly, and regarding detection of negation cues, our
initial list covers all explicit negations in the devel-
opment set, while the detection of affixal negation
cues using our WordNet-based method presents a
precision of 100% but a recall of 53%. In particu-
lar, our method fails when discovering negation cues
such as unburned, uncommonly or irreproachable,
where the word after removing the affix is a derived
form of a verb or adjective.
Secondly, and concerning delimitation of the
scope, our method behaves considerably well. We
have found that it correctly annotates the negation
scope when the negation affects the predicate that
expresses the event, but sometimes fails to include
the subject of the sentence in such scope, as in:
[I know absolutely] nothing [about the fate of this
man], where our method only recognizes as the
negation scope the terms about the fate of this man.
The results have also shown that the method fre-
quently fails when the subject of the sentence or the
object of an event are negated. This occurs, for
instance, in sentences: I think, Watson, [a brandy
and soda would do him] no [harm] and No [woman
would ever send a reply-paid telegram], where we
only point to ?harm? and ?woman? as the scopes.
We have found a further category of errors in the
scope detection tasks, which concern some types
of complex sentences with subordinate conjunctions
where our method limits the negation scope to the
main clause, as in sentence: [Where they came from,
or who they are,] nobody [has an idea] , where our
method limits the scope to ?has an idea?. However,
if the negation cue occurs in the subordinate clause,
the method behaves correctly.
Thirdly, with respect to negated event detection,
as already told our method gets quite poor results.
This was expected, since our system was not orig-
inally designed to face this task and thus it only
covers one type of negated events. Specifically,
it correctly identifies the negated events for sen-
tences with affixal negation cues, as in: It is most
improper, most outrageous, where the negated event
is ?proper?. However, it usually fails to identify
these events when the negation affects the subject
of the sentence or the object of an event.
6 Conclusions and Future Work
This paper presents one of the two contributions
from the Universidad Complutense de Madrid to the
*SEM Shared Task 2012. The results have shown
that our method successes in identifying negation
cues and performs reasonably well when determin-
ing the negation scope, which seems to indicate that
a simple unsupervised method based on syntactic in-
formation and a reduced set of post-processing rules
286
Table 5: Results for the test sets (jointly).
Metric Gold System Tp Fp Fn Precision Recall F-1
Cues 264 278 241 29 23 89.26 91.29 90.26
Scopes (cue match) 249 254 116 24 133 82.86 46.59 59.64
Scopes (no cue match) 249 254 116 24 133 82.86 46.59 59.64
Scope tokens (no cue match) 1805 1449 1237 212 568 85.37 68.53 76.03
Negated (no cue match) 173 33 22 11 151 66.67 12.72 21.36
Full negation 264 278 57 29 207 66.28 21.59 32.57
Table 6: Results for the Cardboard and Circle test sets.
Metric
Cardboard set Circle set
Pr. Re. F-1 Pr. Re. F-1
Cues 90.23 90.23 90.23 88.32 92.37 90.30
Scope (cue match) 83.33 46.88 60.00 82.35 46.28 59.26
Scope (no cue match) 83.33 46.88 60.00 82.35 46.28 59.26
Scope tokens (no cue match) 84.91 72.08 77.97 85.96 64.50 73.70
Negated (no cue match) 66.67 13.79 22.85 66.67 11.63 19.81
Full negation 68.29 21.05 32.18 64.44 22.14 32.96
is a viable approach for dealing with negation. How-
ever, detection of negated events is the main weak-
ness of our approach, and this should be tackled in
future work. We also plan to improve our method
for detecting affixal negations to increment its recall,
by using further WordNet relations such as ?derived
from adjective?, and ?pertains to noun?, as well as
to extend this method to detect infixal negations.
Acknowledgments
This research is funded by the Spanish Ministry of
Science and Innovation (TIN2009-14659-C03-01)
and the Ministry of Education (FPU program).
References
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gerva?s. 2010. A hybrid approach to emotional sen-
tence polarity and intensity classification. In Proceed-
ings of the 14th Conference on Computational Natural
Language Learning (CoNLL 2010), pages 153?161.
W. W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper,
and B.G. Buchanan. 2001. A simple algorithm for
identifying negated findings and diseases in discharge
summaries. J Biomed Inform, 34:301?310.
Isaac Councill, Ryan McDonald, and Leonid Velikovich.
2010. What?s great and what?s not: learning to classify
the scope of negation for improved sentiment analysis.
In Proceedings of the Workshop on Negation and Spec-
ulation in Natural Language Processing, pages 51?59.
Jung-Jae Kim and Jong C. Park. 2006. Extracting con-
trastive information from negation patterns in biomed-
ical literature. ACM Trans. on Asian Language Infor-
mation Processing, 5(1):44?60.
Roser Morante and Eduardo Blanco. 2012. Sem 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the 1st Joint Conference on
Lexical and Computational Semantics (*SEM 2012).
Roser Morante and Walter Daelemans. 2012.
Conandoyle-neg: Annotation of negation in conan
doyle stories. In Proceedings of the 8th International
Conference on Language Resources and Evaluation.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the scope of negation in
biomedical texts. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 715?724.
Roser Morante. 2010. Descriptive Analysis of Negation
Cues in Biomedical Texts. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation.
A.G. Mutalik, A. Deshpande, and P.M. Nadkarni. 2001.
Use of general-purpose negation detection to augment
concept indexing of medical documents. A quantita-
tive study using the UMLS. J Am Med Inform Assoc,
8(6):598?609.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2005. SenseRelate::TargetWord: a generalized
framework for word sense disambiguation. In Pro-
ceedings of the ACL 2005 on Interactive poster and
demonstration sessions, pages 73?76.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, pages 60?68.
287
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 288?293,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UCM-2: a Rule-Based Approach to Infer the Scope of Negation via
Dependency Parsing
Miguel Ballesteros, Alberto D??az, Virginia Francisco,
Pablo Gerva?s, Jorge Carrillo de Albornoz and Laura Plaza
Natural Interaction Based on Language Group
Complutense University of Madrid
Spain
{miballes, albertodiaz, virginia}@fdi.ucm.es,
pgervas@sip.ucm.es, {jcalbornoz, lplazam}@fdi.ucm.es
Abstract
UCM-2 infers the words that are affected by
negations by browsing dependency syntactic
structures. It first makes use of an algo-
rithm that detects negation cues, like no, not
or nothing, and the words affected by them
by traversing Minipar dependency structures.
Second, the scope of these negation cues is
computed by using a post-processing rule-
based approach that takes into account the in-
formation provided by the first algorithm and
simple linguistic clause boundaries. An initial
version of the system was developed to handle
the annotations of the Bioscope corpus. For
the present version, we have changed, omitted
or extended the rules and the lexicon of cues
(allowing prefix and suffix negation cues, such
as impossible or meaningless), to make it suit-
able for the present task.
1 Introduction
One of the challenges of the *SEM Shared Task
(Morante and Blanco, 2012) is to infer and classify
the scope and event associated to negations, given
a training and a development corpus based on Co-
nan Doyle stories (Morante and Daelemans, 2012).
Negation, simple in concept, is a complex but essen-
tial phenomenon in any language. It turns an affir-
mative statement into a negative one, changing the
meaning completely. We believe therefore that be-
ing able to handle and classify negations we would
be able to improve several text mining applications.
Previous to this Shared Task, we can find several
systems that handle the scope of negation in the state
of the art. This is a complex problem, because it re-
quires, first, to find and capture the negation cues,
and second, based on either syntactic or semantic
representations, to identify the words that are di-
rectly (or indirectly) affected by these negation cues.
One of the main works that started this trend in natu-
ral language processing was published by Morante?s
team (2008; 2009), in which they presented a ma-
chine learning approach for the biomedical domain
evaluating it on the Bioscope corpus.
In 2010, a Workshop on Negation and Spec-
ulation in Natural Language Processing (Morante
and Sporleder, 2010) was held in Uppsala, Swe-
den. Most of the approaches presented worked in
the biomedical domain, which is the most studied in
negation detection.
The system presented in this paper is a modifica-
tion of the one published in Ballesteros et al (2012).
This system was developed in order to replicate (as
far as possible) the annotations given in the Bio-
scope corpus (Vincze et al, 2008). Therefore, for
the one presented in the task we needed to modify
most of the rules to make it able to handle the more
complex negation structures in the Conan Doyle cor-
pus and the new challenges that it represents. The
present paper has the intention of exemplifying the
problems of such a system when the task is changed.
Our system presented to the Shared Task is based
on the following properties: it makes use of an algo-
rithm that traverses dependency structures, it classi-
fies the scope of the negations by using a rule-based
approach that studies linguistic clause boundaries
and the outcomes of the algorithm for traversing
dependency structures, it applies naive and simple
288
solutions to the problem of classifying the negated
event and it does not use the syntactic annotation
provided in the Conan Doyle corpus (just in an ex-
ception for the negated event annotation).
In Section 2 we describe the algorithms that we
propose for inferring the scope of negation and the
modifications that we needed to make to the previ-
ous version. In Section 3 we discuss the evaluation
performed with the blind test set and development
set and the error analysis over the development set.
Finally, in Section 4 we give our conclusions and
suggestions for future work.
2 Methodology
Our system consists of two algorithms: the first one
is capable of inferring words affected by the negative
operators (cues) by traversing dependency trees and
the second one is capable of annotating sentences
within the scope of negations. This second algo-
rithm is the one in which we change the behaviour in
a deeper way. The first one just serves as a consult-
ing point in some of the rules of the second one. By
using the training set and development set provided
to the authors we modified, omitted or changed the
old rules when necessary.
The first algorithm which traverses a dependency
tree searching for negation cues to determine the
words affected by negations, was firstly applied (at
an earlier stage) to a very different domain (Balles-
teros et al, 2010) obtaining interesting results. At
that time, the Minipar parser (Lin, 1998) was se-
lected to solve the problem in a simple way with-
out needing to carry out several machine learning
optimizations which are well known to be daunting
tasks. We also selected Minipar because at that mo-
ment we only needed unlabelled parsing.
Therefore, our system consists of three different
modules: a static negation cue lexicon, an algorithm
that from a parse given by Minipar and the nega-
tion cue lexicon produces a set of words affected
by the negations, and a rule-based system that pro-
duces the annotation of the scope of the studied sen-
tence. These components are described in the fol-
lowing sections.
In order to annotate the sentence as it is done in
the Conan Doyle corpus, we also developed a post-
processing system that makes use of the outcomes
of the initial system and produces the expected out-
put. Besides this, we also generate a very naive rule-
based approach to handle the problem of annotating
the negated event.
It is worth to mention that we did not make
use of the syntactic annotation provided in the Co-
nan Doyle corpus, our input is the plain text sen-
tence. Therefore, the system could work without the
columns that are included in the annotation, just with
the word forms. We only make use of the annota-
tion when we annotate the negated event, checking
the part-of-speech tag to ascertain whether the cor-
responding word is a verb or not. The system could
work without these columns but only the results of
the negated event would be affected.
2.1 Negation Cue Lexicon
The lexicon containing the negation cues is static. It
can be extended indefinitely but it has the restriction
that it does not learn and it does not grow automat-
ically when applying it to a different domain. The
lexicon used in the previous system (Ballesteros et
al., 2012) was also static but it was very small com-
pared to the one employed by the present system,
just containing less than 20 different negation cues.
Therefore, in addition to the previous lexicon, we
analysed the training set and development sets and
extracted 153 different negation cues (plus the ones
already present in the previous system). We stored
these cues in a file that feeds the system when it
starts. Table 1 shows a small excerpt of the lexicon.
not no neither..nor
unnecessary unoccupied unpleasant
unpractical unsafe unseen
unshaven windless without
Table 1: Excerpt of the lexicon
2.2 Affected Wordforms Detection Algorithm
The algorithm that uses the outcomes of Minipar is
the same employed in (Ballesteros et al, 2012) with-
out modifications. It basically traverses the depen-
dency structures and returns for each negation cue a
set of words affected by the cue.
The algorithm takes into account the way of han-
dling main verbs by Minipar, in which these verbs
289
appear as heads and the auxiliary verbs are depen-
dants of them. Therefore, the system first detects the
nodes that contain a word which is a negation cue,
and afterwards it does the following:
? If the negation cue is a verb, such as lack, it is
marked as a negation cue.
? If the negation cue is not a verb, the algorithm
marks the main verb (if it exists) that governs
the structure as a negation cue.
For the rest of nodes, if a node depends directly
on any of the ones previously marked as negation
cue, the system marks it as affected. The negation is
also propagated until finding leaves, so wordforms
that are not directly related to the cues are detected
too.
Finally, by using all of the above, the algorithm
generates a list of words affected by each negation
cue.
2.3 Scope Classification Algorithm
This second algorithm is the one that has suffered
the deepest modifications from the first version. The
previous version handled the annotation as it is done
in the Bioscope corpus. The algorithm works as fol-
lows:
? The system opens a scope when it finds a new
negation cue detected by the affected word-
forms detection algorithm. In Bioscope, only
the sentences in passive voice include the sub-
ject inside the scope. However, the Conan
Doyle corpus does not contain this exception
always including the subject in the scope when
it exists. Therefore, we modified the decision
that fires this rule, and we apply the way of an-
notating sentences in passive voice for all the
negation cues, either passive or active voice
sentences.
Therefore, for most of the negation cues the
system goes backward and opens the scope
when it finds the subject involved or a marker
that indicates another statement, like a comma.
There are some exceptions to this, such as
scopes in which the cue is without or nei-
ther...nor. For them the system just opens the
scope at the cue.
? The system closes a scope when there are no
more wordforms to be added, i.e.:
? It finds words that indicate another state-
ment, such as but or because.
? No more words in the output of the first
algorithm.
? End of the sentence.
? We also added a new rule that can handle the
negation cues that are prefix or suffix of another
word, such as meaning-less: if the system finds
a cue word like this, it then annotates the suffix
or prefix as the cue (such as less) and the rest of
the word as part of the scope. Note that the Af-
fected Wordforms Detection algorithm detects
the whole word as a cue word.
2.4 Negated Event Handling
In order to come up with a solution that could pro-
vide at least some results in the negated event han-
dling, we decided to do the following:
? When the cue word contains a negative prefix
or a negative suffix, we annotate the word as
the negated event.
? When the cue word is either not or n?t and the
next word is a verb, according to the part-of-
speech annotation of the Conan Doyle corpus,
we annotate the verb as the negated event.
2.5 Post-Processing Step
The post-processing step basically processes the an-
notated sentence with Bioscope style, (we show
an example for clarification: <scope>There is
<cue>no</cue> problem</scope>). It tokenizes
the sentences, in which each token is a word or a
wordform, after that, it does the following:
? If the token contains the string <scope>, the
system just starts a new scope column reserv-
ing three new columns and it puts the word in
the first free ?scope? column. Because it means
that there is a new scope for the present sen-
tence.
? If the token is between a <cue> annotation, the
system puts it in the corresponding free ?cue?
column of the scope already opened.
290
? If the token is annotated as ?negated event?, the
system just puts the word in the last column of
the scope already opened.
Note that these three rules are not exclusive and
can be fired for the same token, but in this case they
are fired in the same order as they are presented.
3 Results and Discussion
In this section we first show the evaluation results
and second the error analysis after studying the re-
sults on the development set.
3.1 Results
In this section we show the results obtained in two
different tables: Table 2 shows the results of the sys-
tem with the test set, Table 3 shows the results of the
system with the development set.
As we can observe, the results for the develop-
ment set are higher than the ones obtained for the
test set. The reason is simple, we used the develop-
ment set (apart from the training set) to modify the
rules and to make the system able to annotate the
sentences of the test set.
Note that our system only detects some of the
negation cues (around 72% F1 and 76% F1, respec-
tively, for the test and development sets). We there-
fore believe that one of the main drawbacks of the
present system is the static lexicon of cues. In the
previous version, due to the simplicity of the task,
this was not an issue. However, it is worth noting
that once the negation is detected the results are not
that bad, we show a high precision in most of the
tasks. But the recall suffers due to the coverage of
the lexicon.
It is also worth noting that for the measure Scope
tokens, which takes into account the tokens included
in the scope but not a full scope match, our system
provides interesting outcomes (around 63% F1 and
73% F1, respectively), showing that it is able to an-
notate the tokens in a similar way. We believe that
this fact evidences that the present system comes
from a different kind of annotation and a different
domain, and the extension or modification of such a
system is a complex task.
We can also observe that the negated events re-
sults are very low (around 17.46% F1 and 22.53%
F1, respectively), but this was expected because by
using our two rules we are only covering two cases
and moreover, these two cases are not always behav-
ing in the same way in the corpora.
3.2 Error Analysis
In this section we analyse the different errors of our
system with respect to the development set. This set
contains 787 sentences, of which 144 are negation
sentences containing 168 scopes, 173 cues and 122
negation events.
With respect to the negation cue detection we
have obtained 58 false negatives (fn) and 16 false
positives (fp). These results are not directly derived
from the static lexicon of cues. The main problem is
related with the management of sentences with more
than one scope. The majority of the errors have been
produced because in some cases all the cues are as-
signed to all the scopes detected in the same sen-
tence, generating fp, and in other cases the cues of
the second and subsequent scopes are ignored, gen-
erating fn. The first case occurs in sentences like
(1), no and without are labelled as cues in the two
scopes. The second case occurs in sentences like
(2), where neither the second scope nor the second
cue are labelled. In sentence (3) un is labelled as
cue two times (unbrushed, unshaven) but within the
same scope, generating a fp in the first scope and a
fn in the second one.
? (1) But no [one can glance at your toilet and at-
tire without [seeing that your disturbance dates
from the moment of your waking .. ?]]
? (2) [You do ]n?t [mean] - . [you do] n?t [mean
that I am suspected] ? ?
? (3) Our client smoothed down [his] un[brushed
hair] and felt [his] un[shaven chin].
We also found false negatives that occur in multi
word negation cues as by no means, no more and
rather than.
A different kind of false positives is related to
modality cues, dialogue elements and special cases
(Morante and Blanco, 2012). For example, no in (4),
not in (5) and save in (6).
? (4) ? You traced him through the telegram , no
[doubt]., ? said Holmes .
291
Test set gold system tp fp fn precision (%) recall (%) F1 (%)
Cues: 264 235 170 39 94 81.34 64.39 71.88
Scopes(cue match): 249 233 96 47 153 67.13 38.55 48.98
Scopes(no cue match): 249 233 96 48 152 66.90 38.96 49.24
Scope tokens(no cue match): 1805 2096 1222 874 583 58.30 67.70 62.65
Negated(no cue match): 173 81 36 42 134 46.15 21.18 29.03
Full negation: 264 235 29 39 235 42.65 10.98 17.46
Table 2: Test set results.
Development gold system tp fp fn precision (%) recall (%) F1 (%)
Cues: 173 161 115 16 58 87.79 66.47 75.66
Scopes(cue match): 168 160 70 17 98 80.46 41.67 54.90
Scopes(no cue match): 168 160 70 17 98 80.46 41.67 54.90
Scope tokens(no cue match): 1348 1423 1012 411 336 71.12 75.07 73.04
Negated(no cue match): 122 71 35 31 82 53.03 29.91 38.25
Full negation: 173 161 24 16 149 60.00 13.87 22.53
Table 3: Development set results.
? (5) ? All you desire is a plain statement , [is it]
not ? ?.
? (6) Telegraphic inquiries ... that [Marx knew]
nothing [of his customer save that he was a
good payer] .
We can also find problems with affixal negations,
that is, bad separation of the affix and root of the
word. For example, in (7) dissatisfied was erro-
neously divided in di- and ssatisfied. Again, it is
derived from the use of a static lexicon.
? (7) He said little about the case, but from
that little we gathered that [he also was not
dis[satisfied] at the course of events].
Finally, we could also find cases that may be due
to annotation errors. For example, incredible is not
annotated as negation cue in (8). The annotation of
this cue we think is inconsistent, it appears 5 times
in the training corpus, 2 times is labelled as cue, but
3 times is not. According to the context in this sen-
tence, incredible means not credible.
? (8) ?Have just had most incredible and
grotesque experience.
With respect to the full scope detection, most of
the problems are due again to the management of
sentences with more than one scope. We have ob-
tained 98 fn and 17 fp. Most of the problems are
related with affixal negations, as in (9), in which all
the words are included in the scope, which accord-
ing to the gold standard is not correct.
? (9) [Our client looked down with a rueful face
at his own] un[conventional appearance].
With respect to the scope tokens detection, the
results are higher, around 73% F1 in scope tokens
compared to 55% in full match scopes. The reason
is because our system included tokens for the ma-
jority of scopes, increasing the recall until 75% but
lowering the precision due to the inclusion of more
fp.
4 Conclusions and Future Work
In this paper we presented our participation in the
SEM-Shared Task, with a modification of a rule-
based system that was designed to be used in a dif-
ferent domain. As the main conclusion we could say
that modifying such a system to perform in a differ-
ent type of texts is complicated. However, taking
into account this fact, and the results obtained, we
are tempted to say that our system presents compet-
itive results.
292
We believe that the present system has a lot of
room for improvement: (i) improve the manage-
ment of sentences with more than one scope modify-
ing the scope classification algorithm and the post-
processing step, (ii) replacing the dependency parser
with a state-of-the-art parser in order to get higher
performance, or (iii) proposing a different way of
getting a reliable lexicon of cues, by using a seman-
tic approach that informs if the word has a negative
meaning in the context of the sentence. Again, this
could be achieved by using one of the parsers pre-
sented in the ConLL 2008 Shared Task (Surdeanu et
al., 2008).
Acknowledgments
This research is funded by the Spanish Ministry
of Education and Science (TIN2009-14659-C03-01
Project).
References
Miguel Ballesteros, Rau?l Mart??n, and Bele?n D??az-Agudo.
2010. Jadaweb: A cbr system for cooking recipes. In
Proceedings of the Computing Cooking Contest of the
International Conference of Case-Based Reasoning.
Miguel Ballesteros, Virginia Francisco, Alberto D??az,
Jesu?s Herrera, and Pablo Gerva?s. 2012. Inferring the
scope of negation in biomedical documents. In Pro-
ceedings of the 13th International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLING 2012), New Delhi. Springer.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proceedings of the Workshop on the
Evaluation of Parsing Systems, Granada.
Roser Morante and Eduardo Blanco. 2012. Sem 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics (*SEM 2012),
Montreal, Canada.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In Proceedings of the Thirteenth Conference on
Computational Natural Language Learning, CoNLL
?09, pages 21?29, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Roser Morante and Walter Daelemans. 2012.
Conandoyle-neg: Annotation of negation in conan
doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC). Istanbul, Turkey.
Roser Morante and Caroline Sporleder, editors. 2010.
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, Uppsala, Swe-
den.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the scope of negation in
biomedical texts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 715?724, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings of
the Twelfth Conference on Natural Language Learn-
ing, pages 159?177, Manchester, United Kingdom.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9+.
293
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 153?161,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Hybrid Approach to Emotional Sentence Polarity and 
Intensity Classification  
Jorge Carrillo de Albornoz, Laura Plaza, Pablo Gerv?s 
Universidad Complutense de Madrid 
Madrid, Spain 
{jcalbornoz,lplazam}@fdi.ucm.es, pgervas@sip.ucm.es 
  
 
 
 
 
 
Abstract 
In this paper, the authors present a new ap-
proach to sentence level sentiment analysis. 
The aim is to determine whether a sentence 
expresses a positive, negative or neutral sen-
timent, as well as its intensity. The method 
performs WSD over the words in the sentence 
in order to work with concepts rather than 
terms, and makes use of the knowledge in an 
affective lexicon to label these concepts with 
emotional categories.  It also deals with the ef-
fect of negations and quantifiers on polarity 
and intensity analysis. An extensive evaluation 
in two different domains is performed in order 
to determine how the method behaves in 2-
classes (positive and negative), 3-classes (posi-
tive, negative and neutral) and 5-classes 
(strongly negative, weakly negative, neutral, 
weakly positive and strongly positive) classifi-
cation tasks. The results obtained compare fa-
vorably with those achieved by other systems 
addressing similar evaluations. 
1 Introduction 
Sentiment analysis has gained much attention 
from the research community in recent years. It 
is concerned with the problem of discovering 
emotional meanings in text, and most common 
tasks usually include emotion labeling (assigning 
a text its main emotion), polarity recognition 
(classifying a statement into positive or negative) 
and subjectivity identification (determining 
whether a text is subjective or objective). The 
growing research interest is mainly due to the 
practical applications of sentiment analysis. 
Companies and organizations are interested in 
finding out costumer sentiments and opinions, 
while individuals are interested in others? opi-
nions when purchasing a product or deciding 
whether or not watching a movie. 
Many approaches have dealt with sentiment 
analysis as the problem of classifying product or 
service reviews (Pang et al, 2002; Turney, 
2002), while others have attempted to classify 
news items (Devitt and Ahmad, 2007). The task 
is usually addressed as a 2-classes classification 
problem (positive vs. negative). Recent works 
have included the neutral class, trying to detect 
not only the polarity but also the absence of emo-
tional meaning (Wilson et al, 2005; Esuli and 
Sebastiani, 2006). However, few approaches try 
to face a more fine-grained prediction of the in-
tensity (e.g. classifying the polarity into strongly 
negative, weakly negative, neutral, weakly posi-
tive and strongly positive). 
Another important problem of most of these 
approximations is that they usually work with 
terms, and so disregard the contextual meaning 
of those terms in the sentence (Martineau and 
Finin, 2009; Moilanen and Pulman, 2007). The 
use of word disambiguation is not usual in this 
task, due to the fact that most approaches use 
lexical resources created to work with terms. 
However, it is essential to correctly capture the 
meaning of these terms within the text. 
In this paper, we present a hybrid approach 
based on machine learning techniques and lexical 
rules to classify sentences according to their po-
larity and intensity. Thus, given an input text, the 
method is able to determine the polarity of each 
sentence (i.e. if it is negative or positive), as well 
as its intensity. The system tackles the effect of 
negations and quantifiers in sentiment analysis, 
and addresses the problem of word ambiguity, 
taken into account the contextual meaning of the 
terms in the text by using a word sense disam-
biguation algorithm. 
The paper is organized as follows. Section 2 
exposes some background and related work on 
sentiment analysis. Section 3 presents the lexical 
resources and corpora used by the system. Sec-
153
tion 4 describes the method proposed for polarity 
and intensity classification. Section 5 presents 
the evaluation framework and discusses the ex-
perimental results. Finally, section 6 provides 
concluding remarks and future lines of work. 
2 Related work 
The sentiment analysis discipline in computa-
tional linguistic is mainly focused on identify-
ing/classifying different emotional contents with-
in a phrase, sentence or document. This field 
usually encloses tasks such as emotion identifica-
tion, subjectivity classification and polarity rec-
ognition. Sentiment analysis has obtained great 
popularity in the last years mostly due to its suc-
cessful application to different business domains, 
such as the evaluation of products and services, 
where the goal is to discern whether the opinion 
expressed by a user about a product or service is 
favorable or unfavorable. 
Focusing on polarity recognition, the aim of 
this task is the classification of texts into positive 
or negative according to their emotional mean-
ing. Most of the approaches rely on machine 
learning techniques or rule based methods. Sta-
tistical approaches based on term frequencies and 
bags of words are frequently used in machine 
learning approximations. Pang et al (2002) 
present a comparison between three different 
machine learning algorithms trained with bags of 
features computed over term frequencies, and 
conclude that SVM classifiers can be efficiently 
used in polarity identification. Martineau and 
Finin (2009) use a similar approach where the 
words are scored using a Delta TF-IDF function 
before classifying the documents. On the other 
hand, Meena and Prabhakar (2007) study the ef-
fect of conjuncts in polarity recognition using 
rule based methods over the syntax tree of the 
sentence. Whitelaw et al (2005) introduce the 
concept of ?appraisal groups? which are com-
bined with bags of word features to automatical-
ly classify movie reviews. To this aim, they use a 
semi-automated method to generate a lexicon of 
appraising adjectives and modifiers. 
During the past few years, the problem of po-
larity recognition has been usually faced as a step 
beyond the identification of the subjectivity or 
objectivity of texts (Wiebe et al, 1999). Differ-
ent approximations have been proposed to deal 
with this problem. Pang and Lee (2004) propose 
a graph-based method which finds minimum cuts 
in a document graph to classify the sentences 
into subjective or objective. After that, they use a 
bag of words approximation to classify the sub-
jective sentences into positive or negative. Kim 
and Hovy (2004) also introduce a previous step 
to identify the subjectivity of sentences regarding 
a certain topic, and later classify these sentences 
into positives or negatives. 
Most recent approaches do not only deal with 
the 2-classes classification problem, but also in-
troduce a new class representing neutrality. Thus, 
the aim of these works is to classify the text into 
positive, negative or neutral. Wilson et al (2005) 
present a double subjectivity classifier based on 
features such as syntactic classes and sentence 
position, and more semantic features such as ad-
jective graduation. The first classifier determines 
the subjectivity or neutrality of the phrases in the 
text, while the second determines its polarity (in-
cluding neutrality). Esuli and Sebastiani (2006) 
also address this problem testing three different 
variants of a semi-supervised method, and classi-
fy the input into positive, negative or neutral. 
The method proposed yields good results in the 
2-classes polarity classification, while the results 
decrease when dealing with 3-classes. A more 
ambitious classification task is proposed by 
Brooke (2009), where the goal is to measure the 
intensity of polarity. To this aim, the author clas-
sifies the input into 3-classes (strongly-negative, 
ambivalent, and strongly-positive), 4 classes 
(strongly-negative, weakly-negative, weakly-
positive and strongly-positive) and 5-classes 
(strongly-negative, weakly-negative, ambivalent, 
weakly-positive and strongly-positive). The re-
sults decrease considerably with the number of 
classes, from 62% of accuracy for 3-classes to 
38% of accuracy for 5-classes. 
3 Corpora and resources 
The evaluation of the system has been carried out 
using two corpora from two very distinct do-
mains: the Sentence Polarity Movie Review Da-
taset1 and the one used in the SemEval 2007 Af-
fective Text task 2
                                                 
1 http://www.cs.cornell.edu/People/pabo/movie-
review-data/  
. The first one consists of 
10.662 sentences selected from different movie 
review websites. These sentences are labeled as 
positive or negative depending on whether they 
express a positive or negative opinion within the 
movie review. The second one consists of a 
training set and a test set of 250 and 1000 news 
headlines respectively, extracted from different 
news sites. Each sentence is labeled with a value 
2 http://www.cse.unt.edu/~rada/affectivetext/ 
154
between -100 and 100, where -100 means highly 
negative emotional intensity, 100 means highly 
positive and 0 means neutral. To the purpose of 
this work, the test set from the SemEval corpus 
and 1000 sentences randomly extracted from the 
Sentence Polarity Movie Review corpus (500 
positive and 500 negative) were used as evalua-
tion datasets.  
In order to identify the emotional categories 
associated to the concepts in the sentences, an 
affective lexical database based on semantic 
senses, instead of terms, is needed. To this aim, 
the authors have tested different resources and 
finally selected the WordNet Affect affective 
database (Strapparava and Valitutti, 2004). This 
affective lexicon has the particularity of assign-
ing emotional categories to synsets of the Word-
Net lexical database (Miller et al, 1990), allow-
ing the system to correctly disambiguate the 
terms using one of the many WordNet-based 
word sense disambiguation algorithms. The emo-
tional categories in WordNet Affect are orga-
nized hierarchically, and its first level distin-
guishes between positive-emotion, negative-
emotion, neutral-emotion and ambiguous-
emotion. The second level encloses the emotion-
al categories themselves, and consists of a set of 
32 categories. For this work, a subset of 16 emo-
tional categories from this level has been se-
lected, since the hierarchy proposed in WordNet 
Affect is considerably broader than those com-
monly used in sentiment analysis. On the other 
hand, the first level of emotional categories may 
be useful to predict the polarity, but it is clearly 
not enough to predict the intensity of this polari-
ty. To be precise, the subset of emotional catego-
ries used in this work is: {joy, love, liking, calm-
ness, positive-expectation, hope, fear, sadness, 
dislike, shame, compassion, despair, anxiety, 
surprise, ambiguous-agitation and ambiguous-
expectation}. The authors consider this subset to 
be a good representation of the human feeling.  
Since the WordNet Affect hierarchy does not 
provide an antonym relationship, the authors has 
created that relation for the previous set of emo-
tional categories.  Only relationships between 
emotional categories with a strongly opposite 
meaning are created, such as liking-disliking and 
joy-sadness. The purpose of this antonym rela-
tionship is twofold: first, it contributes to handle 
negation forms; and second, it can be used to 
automatically expand the affective lexicon. Both 
issues are discussed in detail later in the docu-
ment. 
On the other hand, since a good amount of 
words with a highly emotional meaning, such as 
dead, cancer and violent, are not labeled in 
WordNet Affect, these words have been manual-
ly labeled by the authors and have been later ex-
tended with their synonyms, antonyms and de-
rived adjectives using the corresponding seman-
tic and lexical relations in WordNet. This process 
has been done in two steps in order to measure 
the effect of the number of synsets labeled on the 
classification accuracy, as described in section 5.  
The WordNet Affect 1.1 lexicon consists of a 
set of 911 synsets. However, the authors have 
detected that a good number of these synsets 
have been labeled more than once, and with dif-
ferent emotional categories (e.g. the synset 
?a#00117872 {angered, enraged, furious, infu-
riated, maddened}? is labeled with three different 
categories: anger, fury and infuriation). Thus, 
after removing these synsets and those labeled 
with an emotional category not included in the 
16-categories subset used in this work, the affec-
tive lexicon presents 798 synsets. After the first 
step of semi-automatic labeling, the affective 
lexicon increased the number of synsets in 372, 
of which 100 synsets were manually labeled, and 
272 were automatically derived throughout the 
WordNet relations. The second and last step of 
semi-automatic labeling added 603 synsets to the 
lexicon, of which 200 synsets were manually 
labeled, and 403 were automatically derived.  
The final lexicon presents 1773 synsets and 4521 
words labeled with an emotional category. Table 
1 shows the distribution of the affective lexicon 
in grammatical categories. 
 
Grammatical  
Category 
WNAffect 
 
WNAffect + 
1st  step 
WNAffect + 
 2nd step 
Nouns 280 440 699 
Verbs 122 200 309 
Adjectives 273 394 600 
Adverbs 123 136 165 
 
Table 1: Distribution in grammatical categories of the syn-
sets in the affective lexicon. 
4 The method 
In this section, the method for automatically 
labeling sentences with an emotional intensity 
and polarity is presented. The problem is faced 
as a text classification task, which is accomplish-
es throughout four steps. Each step is explained 
in detail in the following subsections.  
155
4.1 Pre-processing: POS tagging and con-
cept identification 
In order to determine the appropriate emotional 
category for each word in its context, a pre-
processing step is accomplished to translate each 
term in the sentence to its adequate sense in 
WordNet. To this aim, the system analyzes the 
text, splits it into sentences and tags the tokens 
with their part of speech. The Gate architecture3 
and the Stanford Parser4
Once the sentences have been split and tagged, 
the method maps each word of each sentence 
into its sense in WordNet according to its con-
text. To this end, the lesk WSD algorithm im-
plemented in the WordNet Sense-Relate perl 
package is used (Patwardhan et al, 2005). The 
disambiguation is carried out only over the 
words belonging to the grammatical categories 
noun, verb, adjective and adverb, as only these 
categories can present an emotional meaning. As 
a result, we get the stem and sense in WordNet 
of each word, and this information is used to re-
trieve its synset.  
 were selected to carry 
out this process. In particular the Annie English 
Tokeniser, Hash Gazetter, RegEx Sentence Split-
ter and the Stanford Parser modules in Gate are 
used to analyze the input. In this step also the 
syntax tree and dependencies are retrieved from 
the Stanford Parser. These features will be used 
in the post-processing step in order to identify 
the negations and the quantifiers, as well as their 
scope. 
A good example of the importance of perform-
ing word disambiguation can be shown in the 
sentence ?Test to predict breast cancer relapse is 
approved? from the SemEval news corpus. The 
noun cancer has five possible entries in WordNet 
and only one refers to a ?malignant growth or 
tumor?, while the others are related with ?astrol-
ogy? and the ?cancer zodiacal constellation?. 
Obviously, without a WSD algorithm, the wrong 
synset will be considered, and a wrong emotion 
will be assigned to the concept. 
Besides, to enrich the emotion identification 
step, the hypernyms of each concept are also re-
trieved from WordNet. 
4.2 Emotion identification 
The aim of the emotion identification step is to 
map the WordNet concepts previously identified 
to those present in the affective lexicon, as well 
                                                 
3 http://gate.ac.uk/ 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
as to retrieve from this lexicon the corresponding 
emotional category of each concept.  
We hypothesize that the hypernyms of a con-
cept entail the same emotions than the concept 
itself, but the intensity of such emotions decreas-
es as we move up the hierarchy (i.e. the more 
general the hypernym becomes, the less its emo-
tional intensity is).  Following this hypothesis, 
when no entry is found in the affective lexicon 
for a given concept, the emotional category asso-
ciated to its nearest hypernym, if any, is used to 
label the concept. However, only a certain level 
of hypernymy is accepted, since an excessive 
generalization introduces some noise in the emo-
tion identification. This parameter has been em-
pirically set to 3 (Carrillo de Albornoz et al, 
2010). Previous experiments have shown that, 
upper this level, the working hypothesis becomes 
unreliable. 
The sentence ?Siesta cuts risk of heart disease 
death study finds? clearly illustrates the process 
described above. In this sentence, the concepts 
risk, death and disease are labeled with an emo-
tional category: in particular, the categories as-
signed to them are fear, fear and dislike respec-
tively. However, while the two firsts are re-
trieved from the affective lexicon by their own 
synsets, the last one is labeled through its hyper-
nym: since no matching is found for disease in 
the lexicon, the analysis over its hypernyms de-
tects the category dislike assigned to the synset 
of its first hypernym, which contains words such 
as illness and sickness, and the same emotion 
(dislike) is assigned to disease. 
It must be noted that, to perform this analysis, 
a previous mapping between 2.1 and 1.6 Word-
Net versions was needed, since the method and 
the affective lexicon work on different versions 
of the database.  
4.3 Post-processing: Negation and quantifi-
ers detection 
Once the concepts of the sentence have been la-
beled with their emotional categories, the next 
step aims to detect and solve the effect of the 
negations and the quantifiers on the emotional 
categories identified in the previous step.  
The effect of negation has been broadly stu-
died in NLP (Morante and Daelemans, 2009) and 
sentiment analysis (Jia et al, 2009). Two main 
considerations must be taken into account when 
dealing with negation. First, the negation scope 
may affect only a word (no reason), a proposi-
tion (Beckham does not want to play again for 
Real) or even a subject (No one would like to do 
156
this). Different approximations have been pro-
posed to delimit the scope of negation. Some 
assume the scope to be those words between the 
negation token and the first punctuation mark 
(Pang et al, 2002), others consider a fixed num-
ber of words after the negation token (Hu and 
Liu, 2004). Second, the impact of negation is 
usually neutralized by reversing the polarity of 
the sentence (Polanyi and Zaenen, 2006) or using 
contextual valence shifters which increase or 
dismiss the final value of negativity or positivity 
of the sentence (Kennedy and Inkpen, 2006). 
In this work, the negation scope is detected us-
ing the syntax tree and dependencies generated 
by the Stanford Parser. The dependency neg al-
lows us to easily determine the presence of sev-
eral simple types of negation, such as those pre-
ceded by don?t, didn?t, not, never, etc. Other 
words not identified with this dependency, but 
also with a negation meaning, such as no, none? 
nor or nobody, are identified using a negation 
token list. To determine the negation scope, we 
find in the syntax tree the first common ancestor 
that encloses the negation token and the word 
immediately after it, and assume all descendant 
leaf nodes to be affected by the negation.  
For each concept in the sentence that falls into 
the scope of a negation, the system retrieves its 
antonym emotional category, if any, and assigns 
this category to the concept. If no antonym emo-
tion is obtained, the concept is labeled with no 
emotion, according to the premise that the nega-
tion may change or neutralize the emotional po-
larity. An example of this process can be shown 
in the sentence ?Children and adults enamored 
of all things pokemon won't be disappointed?. In 
this sentence, the Stanford Parser discovers a 
negation and the system, through the syntax tree, 
determines that the scope of the negation enclos-
es the words ?won?t be disappointed?. As the 
synset of ?disappointed? has been labeled with 
the emotional category despair, its antonym is 
retrieved, and the emotional category of the an-
tonym, hope, is used to label the concept.  
On the other hand, the quantifiers are words 
considered in sentiment analysis as amplifiers or 
downtoners (Quirk et al, 1985). That is to say, 
the word very in the sentence ?That is a very 
good idea? amplifies the intensity of the emo-
tional meaning and the positivity of the sentence, 
while the word less in the sentence ?It is less 
handsome than I was expecting? dismisses its 
intensity and polarity. The most common ap-
proach to identify quantifiers is the use of lists of 
words which play specific grammatical roles in 
the sentence. These lists normally contain a fixed 
value for all positive words and another value for 
all negative words (Polanyi and Zaenen, 2006). 
By contrast, Brooke (2009) proposes a novel ap-
proach where each quantifier is assigned its own 
polarity and weight.  
The quantifiers are usually represented as sen-
tence modifiers, assuming their scope to be the 
whole sentence and modifying its overall polari-
ty. However, when dealing with sentences like 
?The house is really nice and the neighborhood 
is not bad?, these approaches assume that the 
quantifier really amplifies the intensity of both 
conjunctives, when it only should amplify the 
intensity of the first one. By contrast, our ap-
proach determines the scope of the quantifiers by 
the syntax tree and the dependencies over them. 
Thus, when a quantifier is detected in a sentence, 
the dependencies are checked and only those that 
play certain roles, such as adverbial or adjectival 
modifiers, are considered. All concepts affected 
by a quantifier are marked with the weight cor-
responding to that quantifier, which will serve to 
amplify/dismiss the emotions of these concepts 
in the classification step.  The quantifier list used 
here is the one proposed in Brooke (2009). 
The sentence ?Stale first act, scrooge story, 
blatant product placement, some very good com-
edic songs? illustrates the analysis of the quan-
tifiers. The system detects two tokens which are 
in the quantifier list and play the appropriate 
grammatical roles. The first quantifier some af-
fects to the words ?very good comedic songs?, 
while the second quantifier very only affects to 
?good?. So these concepts are marked with the 
specific weight of each quantifier. Note that the 
concept ?good? is marked twice. 
4.4 Sentence classification 
Up to this point, the sentence has been labeled 
with a set of emotional categories, negations and 
their scope have been detected and the quantifi-
ers and the concepts affected by them have been 
identified. In this step, this information is used to 
translate the sentence into a Vector of Emotional 
Occurrences (VEO), which will be the input to 
the machine learning classification algorithm. 
Thus, each sentence is represented as a vector of 
16 values, each of one representing an emotional 
category. The VEO vector is generated as fol-
lows: 
? If the concept has been labeled with an 
emotional category, the position of the 
vector for this category is increased in 1. 
157
? If no emotional category has been found 
for the concept, then the category of its 
first hypernym labeled is used. As the 
hypernym generalizes the meaning of the 
concept, the value assigned to the position 
of the emotional category in the VEO is 
weighted as follows: 
[ ] [ ]
1.
1
+
+=
DepthHyper
iVEOiVEO  
? If a negation scope encloses the concept, 
then the antonym emotion is used, as de-
scribed in the previous step. The emotion-
al category position of this antonym in the 
VEO is increased in 0.9. Different tests 
have been carried out to set this parameter, 
and the 0.9 value got the best results. The 
reason for using a lower value for the 
emotional categories derived from nega-
tions is that the authors consider that a ne-
gation changes the emotional meaning of a 
concept but usually in a lower percentage. 
For example, the sentence ?The neighbor-
hood is not bad? does not necessarily 
mean that it is a good neighborhood, but it 
is a quite acceptable one.  
? If a concept is affected by a quantifier, 
then the weight of that quantifier is added 
to the position in the VEO of the emotion-
al category assigned to the concept. 
Thus, a sentence like ?This movie?. isn?t 
worth the energy it takes to describe how really 
bad it is? will be represented by the VEO [1.0, 0, 
0.0, 0, 0, 0.0, 0, 0, 2.95, 0, 0, 0, 0, 0, 0, 0].  In 
this sentence, the concept movie is labeled with 
the emotional category joy, the concept worth is 
labeled with positive-expectation, the concept 
energy is labeled with liking, and the concept 
bad is labeled with dislike. Since the concepts 
worth and energy fall into the negation scope, 
they both change their emotional category to dis-
like. Besides, since the concept bad is amplified 
by the quantifier really, the weight of this con-
cept in the VEO is increased in 0.15. 
5 Evaluation framework and results 
In this work, two different corpora have been 
used for evaluation (see Section 3): a movie re-
view corpus containing 1000 sentences labeled 
with either a positive or negative polarity; and a 
news headlines corpus composed of 1000 sen-
tences labeled with an emotional intensity value 
between -100 and 100. 
To determine the best machine learning algo-
rithm for the task, 20 classifiers currently imple-
mented in Weka5
5.1 Evaluating polarity classification 
 were compared. We only show 
the results of the best performance classifiers: a 
logistic regression model (Logistic), a C4.5 deci-
sion tree (J48Graph) and a support vector ma-
chine (LibSVM). The best outcomes for the three 
algorithms were reported when using their de-
fault parameters, except for J48Graph, where the 
confidence factor was set to 0.2. The evaluation 
is accomplishes using 10-fold cross validation. 
Therefore, 100 instances of each dataset are held 
back for testing in each fold, and the additional 
900 instances are used for training. 
We first analyze the effect of expanding the cov-
erage of the emotional lexicon by semi-
automatically adding to WordNet Affect more 
synsets labeled with emotional categories, as ex-
plained in Section 3. To this end, we compare the 
results of the method using three different affec-
tive lexical databases: WordNet Affect and 
WordNet Affect extended with 372 and 603 syn-
sets, respectively. For the sake of comparing the 
results in both corpora, the news dataset has been 
mapped to a -100/100 classification (-100 = [-
100, 0), 100 = [0,100]). 
Table 2 shows the results as average precision 
and accuracy of these experiments. Note that, as 
the weight of mislabeling for both classes is the 
same and the classes are balanced, accuracy is 
equal to recall in all cases. Labeling 975 new 
synsets significantly improves the performance 
of our system in both datasets and for all ML 
techniques. In particular, the best improvement is 
achieved by the Logistic classifier: from 52.7% 
to 72.4% of accuracy in the news dataset, and 
from 50.5% to 61.5% of accuracy in the movies 
dataset.  
 
Emotional  
Lexicon 
Method 
News Corpus Movie Reviews 
Pr. Ac. Pr. Ac. 
WNAffect 
Logistic 52.8 52.7 51.3 50.5 
J48Graph 27.7 52.6 50 50 
LibSVM 27.7 52.6 53.2 50.6 
WNAffect + 
372 synsets 
Logistic 69.9 65.2 53.9 53.8 
J48Graph 70.1 64.8 55.3 55.1 
LibSVM 68.9 63.9 52 51.8 
WNAffect + 
603 synsets 
Logistic 73.8 72.4 61.6 61.5 
J48Graph 73.6 70.9 60.9 60.9 
LibSVM 71.6 70.3 62.5 59.4 
 
Table 2: Precision and accuracy percentages achieved by 
our system using different affective databases. 
 
                                                 
5 http://www.cs.waikato.ac.nz/ml/weka/ 
158
We have observed that, especially in the news 
dataset, an important number of sentences that 
are labeled with a low positive or negative emo-
tional intensity could be perfectly considered as 
neutral. The intensity of these sentences highly 
depends on the previous knowledge and particu-
lar interpretation of the reader. For instance, the 
sentence ?Looking beyond the iPhone? does not 
express any emotion itself, unless you are fan or 
detractor of Apple. However, this sentence is 
labeled in the corpus with a 15 intensity value. It 
is likely that these kinds of sentences introduce 
noise into the dataset. In order to estimate the 
influence of such sentences in the experimental 
results, we conducted a test removing from the 
news dataset those instances with an intensity 
value in the range [-25, 25]. As expected, the 
accuracy of the method increases substantially, 
i.e. from 72.4% to 76.3% for logistic regression. 
The second group of experiments is directed to 
evaluate if dealing with negations and quantifiers 
improves the performance of the method. To this 
end, the approach described in Section 4.3 was 
applied to both datasets. Table 3 shows that 
processing negations consistently improves the 
accuracy of all algorithms in both datasets; while 
the effect of the quantifiers is not straightforward. 
Even if we expected that using quantifiers would 
lead to better results, the performance in both 
datasets decreases in 2 out of the 3 ML algo-
rithms. However, combining both features im-
proves the results in both datasets. The reason 
seems to be that, when no previous negation de-
tection is performed, if the emotional category 
assigned to certain concepts are wrong (because 
these concepts are affected by negations), the 
quantifiers will be weighting the wrong emotions.  
 
Features Method 
News Corpus Movie Reviews 
Pr. Ac. Pr. Ac. 
Negations  
Logistic 74.2 72.5 61.7 61.6 
J48Graph 74.1 71.2 62.8 62.6 
LibSVM 72.7 71.1 62.4 60.1 
Quantifiers 
Logistic 73.7 72.2 61.9 61.9 
J48Graph 73.6 70.9 59.5 59.5 
LibSVM 72.1 70.6 61.1 59 
Negations + 
Quantifiers 
Logistic 74.4 72.7 62.4 62.4 
J48Graph 74.1 71.2 62.5 62.1 
LibSVM 72.8 71.2 62.6 60.5 
 
Table 3: Precision and accuracy of the system improved 
with negation and quantifier detection. 
 
The comparison with related work is difficult 
due to the different datasets and methods used in 
the evaluations. For instance, Pang et al (2002) 
use the Movie Review Polarity Dataset, achiev-
ing an accuracy of 82.9% training a SVM over a 
bag of words. However, their aim was to deter-
mine the polarity of documents (i.e. the whole 
movie reviews) instead of sentences. When 
working at the sentence level, the information 
from the context is missed, and the results are 
expected to be considerably lower. As a matter 
of fact, it happens that many sentences in the 
Sentence Polarity Movie Review Dataset are la-
beled as positive or negative, but do not express 
any polarity when taken out of the context of the 
overall movie review. This conclusion is also 
drawn by Meena and Prabhakar (2007), who 
achieve an accuracy of 39% over a movie review 
corpus (not specified) working at the sentence 
level, using a rule based method to analyze the 
effect of conjuncts. This accuracy is well below 
that of our method (62.6%).  
Molianen and Pulman (2007) present a senti-
ment composition model where the polarity of a 
sentence is calculated as a complex function of 
the polarity of its parts. They evaluate their sys-
tem over the SemEval 2007 news corpus, and 
achieve an accuracy of 65.6%, under our same 
experimental conditions, which is also signifi-
cantly lower than the accuracy obtained by our 
method.  
5.2 Evaluating intensity classification 
Apart from identifying of polarity, we also want 
to examine the ability of our system to determine 
the emotional intensity in the sentences. To this 
aim, we define two intensity distributions: the 3-
classes and the 5-classes distribution. For the 
first distribution, we map the news dataset to 3-
classes: negative [-100, -50), neutral [-50, 50) 
and positive [50, 100]. For the second distribu-
tion, we map the dataset to 5-classes: strongly 
negative [-100, -60), negative [-60, -20), neutral 
[-20, 20), positive [20, 60) and strongly positive 
[60, 100]. We can see in Table 4 that, as the 
number of intensity classes increases, the results 
are progressively worse, since the task is pro-
gressively more difficult. 
 
Intensity 
classes 
Method 
News Corpus 
Pr. Ac. 
2-classes 
Logistic 74.4 72.7 
J48Graph 74.1 71.2 
LibSVM 72.8 71.2 
3-classes 
Logistic 60.2 63.8 
J48Graph 66 64.8 
LibSVM 54.8 64.6 
5-classes 
Logistic 48.3 55.4 
J48Graph 47.3 54.8 
LibSVM 43.1 53.1 
 
Table 4: Precision and accuracy in three different intensity 
classification tasks. 
159
The 3-classes distribution coincides exactly 
with that used in one of the SemEval 2007 Af-
fective task, so that we can easily compare our 
results with those of the systems that participated 
in the task. The CLaC and CLaC-NB systems 
(Andreevskaia and Bergler, 2007) achieved, re-
spectively, the best precision and recall. CLaC 
reported a precision of 61.42 % and a recall of 
9.20%; while CLaC-NB reported a precision of 
31.18% and a recall of 66.38%. Our method 
clearly outperforms both systems in precision, 
while provides a recall (which is equal to the ac-
curacy) near to that of the best system. Besides, 
our results for both metrics are well-balanced, 
which does not occur in the other systems. 
Regarding the 5-classes distribution evalua-
tion, to the authors? knowledge no other work 
has been evaluated under these conditions. How-
ever, our system reports promising results: using 
5 classes it achieves better results than other par-
ticipant in the SemEval task using just 3 classes 
(Chaumartin, 2007; Katz et al, 2007). 
5.3 Evaluating the effect of  word ambiguity 
on sentiment analysis 
A further test has been conducted to examine the 
effect of word ambiguity on the classification 
results. To this aim, we repeated the experiments 
above without using WSD. First, we simply as-
signed to each word its first sense in WordNet. 
Second, we selected these senses randomly.  The 
results are presented in Table 5. We only show 
those of the best algorithm for each intensity dis-
tribution.  
 
Intensity classes Method 
News Corpus 
Pr. Ac. 
2-classes (Logistic) 
WSD 74.4 72.6 
1st Sense 71.6 69.3 
Random Sense 69.1 64.1 
3-classes (J48Graph) 
WSD 66 64.8 
1st Sense 59 62.9 
Random Sense 50.8 61 
5-classes  (Logistic) 
WSD 48.3 55.4 
1st Sense 43.7 53.8 
Random Sense 46.8 51.6 
 
Table 5: Precision and accuracy for three different word 
disambiguation strategies. 
 
It can be observed that, even though the use of 
word disambiguation improves the classification 
precision and accuracy, the improvement with 
respect to the first sense heuristic is less than ex-
pected. This may be due to the fact that the 
senses of the words in WordNet are ranked ac-
cording to their frequency, and so the first sense 
of a word is also the most frequent one. Besides, 
the Most Frequent Sense (MFS) heuristic in 
WSD is usually regarded as a difficult competi-
tor. On the contrary, the improvement with re-
spect to the random sense heuristic is quite re-
markable. 
 
6 Conclusions and future work 
In this paper, a novel approach to sentence level 
sentiment analysis has been described. The sys-
tem has resulted in a good method for sentence 
polarity classification, as well as for intensity 
identification. The results obtained outperform 
those achieved by other systems which aim to 
solve the same task.  
Nonetheless, some considerations must be 
noted. Even with the extended affective lexicon, 
around 1 in 4 sentences of each corpus has not 
been assigned any emotional category, some-
times because their concepts are not labeled in 
the lexicon, but mostly because their concepts do 
not have any emotional meaning per se. A test on 
the news corpus removing those sentences not 
labeled with any emotional meaning has been 
performed for the 2-classes classification prob-
lem, allowing the method to obtain an accuracy 
of 81.7%. However, to correctly classify these 
sentences, it would be necessary to have addi-
tional information about their contexts (i.e. the 
body of the news item, its section in the newspa-
per, etc.).   
Finally, the authors plan to extend the method 
to deal with modal and conditional operators, 
which will allow us to distinguish among situa-
tions that have happened, situations that are hap-
pening, situations that could, might or possibly 
happen or will happen, situations that are wished 
to happen, etc. 
 
Acknowledgments 
This research is funded by the Spanish Ministry 
of Science and Innovation (TIN2009-14659-
C03-01), the Comunidad Autonoma de Madrid 
and the European Social Fund through the IV 
PRICIT program, and the Spanish Ministry of 
Education through the FPU program. 
References  
Julian Brooke. 2009. A Semantic Approach to Auto-
mated Text Sentiment Analysis. Simon Fraser 
University. Ph. D. Thesis. 
Jorge Carrillo de Albornoz, Laura Plaza and Pablo 
Gerv?s. 2010. Improving Emotional Intensity Clas-
160
sification using Word Sense Disambiguation. Re-
search in Computing Science 46 :131-142. 
Fran?ois-R?gis Chaumartin. 2007. UPAR7: A Know-
ledge-based System for Headline Sentiment Tag-
ging. In Proceedings of the 4th Workshop on Se-
mantic Evaluations (SemEval 2007), pages 422-
425. 
Ann Devitt and Khurshid Ahmad. 2007. Sentiment 
Polarity Identification in Financial News: A Cohe-
sion-based Approach. In Proceedings of the 45th 
Annual Meeting of the ACL, pages 984-991. 
Andrea Esuli and Fabrizio Sebastiani. 2006. Deter-
mining Term Subjectivity and Term Orientation for 
Opinion Mining. In Proceedings of the 11th Confe-
rence of the EACL, pages 193-200. 
Minging Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of the 
10th ACM SIGKDD Conference on Knowledge 
Discovery and Data Mining, pages 168-177. 
Lifeng Jia, Clement Yu and Weiji Meng. 2009. The 
Effect of Negation on Sentiment Analysis and Re-
trieval Effectiveness. In Proceeding of the 18th  
ACM Conference on Information and Knowledge 
Management, pages 1827-1830. 
Phil Katz, Matthew Singleton and Richard Wicen-
towski. 2007. SWAT-MP: the SemEval-2007 Sys-
tems for Task 5 and Task 14. In Proceedings of the 
4th Workshop on Semantic Evaluations (SemEval 
2007), pages 308-313. 
Alistair Kennedy and Diana Inkpen. 2006. Sentiment 
Classification of Movie Reviews Using Contextual 
Valence Shifters. Computational Intelligence 
22(2): 110-125. 
Soo-Min Kim and Eduard Hovy. 2004. Determining 
the Sentiment of Opinions. In Proceedings of COL-
ING 2004, pages 1367-1373. 
Justin Martineau and Tim Finin. 2009. Delta TFIDF: 
An Improved Feature Space for Sentiment Analy-
sis. In Proceedings of the 3rd AAAI International 
Conference on Weblogs and Social Media. 
Arun Meena and T.V. Prabhakar. 2007. Sentence 
Level Sentiment Analysis in the Presence of Con-
juncts Using Linguistic Analysis. In Proceedings 
of ECIR 2007, pages 573-580. 
George A. Miller, Richard Beckwith, Christiane Fell-
baum Derek Gross and Katherine Miller. 1990. In-
troduction to WordNet: An On-Line Lexical Data-
base. International Journal of Lexicography 
3(4):235-244. 
Karo Moilanen and Stephen Pulman. 2007. Sentiment 
Composition. In Proceedings of RANLP 2007, 
pages 378-382. 
Roser Morante and Walter Daelemans. 2009. A Meta-
learning Approach to Processing the Scope of Ne-
gation. In Proceedings of the CONLL 2009, pages 
21-29. 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment Classification using 
Machine Learning Techniques. In Proceedings of 
CoRR 2002. 
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis using Subjectivity Sum-
marization based on Minimum Cuts. In Proceed-
ings of the 42nd  Annual Meeting of the ACL, pages 
271-278. 
Siddharth Patwardhan, Satanjeev Banerjee and Ted 
Pedersen. 2005. SenseRelate::TargetWord - A Ge-
neralized Framework for Word Sense Disambigua-
tion. In Proceedings of the ACL 2005 on Interac-
tive Poster and Demonstration Sessions, pages 73-
76. 
Livia Polanyi and Annie Zaenen. 2006. Contextual 
Valence Shifters. Computing Attitude and Affect in 
Text: Theory and Applications. In The Information 
Retrieval Series 20, pages 1-10. 
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech 
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman. 
Carlo Strapparava and Alessandro Valitutti. 2004. 
Wordnet-Affect: an Affective Extension of Word-
Net. In Proceedings of the LREC 2004, pages 
1083-1086.  
Peter D. Turney. 2002. Thumbs up or Thumbs 
down?: Semantic Orientation applied to Unsuper-
vised Classification of Reviews. In Proceedings of 
the 40th Annual Meeting of the ACL, pages 417-
424. 
Casey Whitelaw, Navendu Garg and Shlomo Arga-
mon. 2005. Using Appraisal Groups for Sentiment 
Analysis. In Proceedings of the 14th ACM Confe-
rence on Information and Knowledge Manage-
ment, pages 625-631. 
Janyce M. Wiebe, Rebecca F. Bruce and Thomas P. 
O?Hara. 1999. Development and Use of a Gold-
standard Data Set for Subjectivity Classification. In 
Proceedings of the 37th Annual Meeting of the 
ACL, pages 246-253. 
Theresa Wilson, Janyce Wiebe and Paul Hoffman. 
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of the 
HLT-EMNLP 2005, pages 347-354. 
161
