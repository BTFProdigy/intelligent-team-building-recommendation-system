UDel: Refining a Method of Named Entity Generation
Charles F. Greenbacker, Nicole L. Sparks, Kathleen F. McCoy, and Che-Yu Kuo
Department of Computer and Information Sciences
University of Delaware
Newark, Delaware, USA
[charlieg|sparks|mccoy|kuo]@cis.udel.edu
Abstract
This report describes the methods and re-
sults of a system developed for the GREC
Named Entity Challenge 2010. We de-
tail the refinements made to our 2009 sub-
mission and present the output of the self-
evaluation on the development data set.
1 Introduction
The GREC Named Entity Challenge 2010 (NEG)
is an NLG shared task whereby submitted systems
must select a referring expression from a list of
options for each mention of each person in a text.
The corpus is a collection of 2,000 introductory
sections from Wikipedia articles about individual
people in which all mentions of person entities
have been annotated. An in-depth description of
the task, along with the evaluation results from the
previous year, is provided by Belz et al (2009).
Our 2009 submission (Greenbacker and Mc-
Coy, 2009a) was an extension of the system we
developed for the GREC Main Subject Refer-
ence Generation Challenge (MSR) (Greenbacker
and McCoy, 2009b). Although our system per-
formed reasonably-well in predicting REG08-
Type in the NEG task, our string accuracy scores
were disappointingly-low, especially when com-
pared to the other competing systems and our own
performance in the MSR task. As suggested by the
evaluators (Belz et al, 2009), this was due in large
part to our reliance on the list of REs being in a
particular order, which had changed for the NEG
task.
2 Method
The first improvement we made to our existing
methods related to the manner by which we se-
lected the specific RE to employ. In 2009, we
trained a series of decision trees to predict REG08-
Type based on our psycholinguistically-inspired
feature set (described in (Greenbacker and Mc-
Coy, 2009c)), and then simply chose the first op-
tion in the list of REs matching the predicted type.
For 2010, we incorporated the case of each RE
into our target attribute so that the decision tree
classifier would predict both the type and case for
the given reference. Then, we applied a series
of rules governing the length of initial and sub-
sequent REs involving a person?s name (following
Nenkova and McKeown (2003)), as well as ?back-
offs? if the predicted type or case were not avail-
able.
Another improvement we made involved our
method of determining whether the use of a pro-
noun would introduce ambiguity in a given con-
text. Previously, we searched for references to
other people entities since the most recent mention
of the entity at hand, and if any were found, we
assumed these would cause the use of a pronoun
to be ambiguous. However, this failed to account
for the fact that personal pronouns in English are
gender-specific (ie. the mention of a male individ-
ual would not make the use of ?she? ambiguous).
So, we refined this by determining the gender of
each named entity (by seeing which personal pro-
nouns were associated with it in the list of REs),
and only noting ambiguity when the current entity
and candidate interfering antecedent were of the
same gender.
Other small changes from 2009 include an ex-
panded abbreviation set in the sentence segmenter,
separate decision trees for the main subject and
other entities, and fixing how we handled embed-
ded REF elements with unspecified mention IDs.
3 Results
Scores for REG08-Type precision & recall, string
accuracy, and string-edit distance are presented in
Figure 1. These were computed on the entire de-
velopment set, as well as the three subsets, us-
ing the geval.pl self-evaluation tool provided in the
NEG participants? pack.
While we were able to achieve an improvement
of nearly 50% over our 2009 scores in string ac-
curacy, we saw less than a 1% gain in overall
REG08-Type performance.
Metric Score
Type Precision/Recall 0.757995735607676
String Accuracy 0.650496141124587
Mean Edit Distance 0.875413450937156
Normalized Distance 0.319266300067796
(a) Scores on the entire development set.
Metric Score
Type Precision/Recall 0.735294117647059
String Accuracy 0.623287671232877
Mean Edit Distance 0.839041095890411
Normalized Distance 0.345490867579909
(b) Scores on the ?Chefs? subset.
Metric Score
Type Precision/Recall 0.790769230769231
String Accuracy 0.683544303797468
Mean Edit Distance 0.882911392405063
Normalized Distance 0.279837251356239
(c) Scores on the ?Composers? subset.
Metric Score
Type Precision/Recall 0.745928338762215
String Accuracy 0.642140468227425
Mean Edit Distance 0.903010033444816
Normalized Distance 0.335326519731057
(d) Scores on the ?Inventors? subset.
Figure 1: Scores on the development set obtained
via the geval.pl self-evaluation tool. REG08-Type
precision and recall were equal in all four sets.
4 Conclusions
The fact that our string accuracy scores improved
over our 2009 submission far more than REG08-
Type prediction is hardly surprising. Our efforts
during this iteration of the NEG task were primar-
ily focused on enhancing our methods of choosing
the best RE once the reference type was selected.
We remain several points below the best-
performing team from 2009 (ICSI-Berkeley), pos-
sibly due to the inclusion of additional items in
their feature set, or the use of Conditional Ran-
dom Fields as their learning technique (Favre and
Bohnet, 2009).
5 Future Work
Moving forward, we hope to expand our feature
set by including the morphology of words immedi-
ately surrounding the reference, as well as a more
extensive reference history, as suggested by (Favre
and Bohnet, 2009). We suspect that these features
may play a significant role in determining the type
of referenced used, the prediction of which acts as
a ?bottleneck? in generating exact REs.
We would also like to compare the efficacy of
several different machine learning techiques as ap-
plied to our feature set and the NEG task.
References
Anja Belz, Eric Kow, and Jette Viethen. 2009. The
GREC named entity generation challenge 2009:
Overview and evaluation results. In Proceedings
of the 2009 Workshop on Language Generation and
Summarisation (UCNLG+Sum 2009), pages 88?98,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Benoit Favre and Bernd Bohnet. 2009. ICSI-
CRF: The generation of references to the main
subject and named entities using conditional ran-
dom fields. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 99?100, Suntec, Singapore,
August. Association for Computational Linguistics.
Charles Greenbacker and Kathleen McCoy. 2009a.
UDel: Extending reference generation to multiple
entities. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 105?106, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Charles Greenbacker and Kathleen McCoy. 2009b.
UDel: Generating referring expressions guided by
psycholinguistc findings. In Proceedings of the
2009 Workshop on Language Generation and Sum-
marisation (UCNLG+Sum 2009), pages 101?102,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Charles F. Greenbacker and Kathleen F. McCoy.
2009c. Feature selection for reference generation as
informed by psycholinguistic research. In Proceed-
ings of the CogSci 2009 Workshop on Production of
Referring Expressions (PRE-Cogsci 2009), Amster-
dam, July.
Ani Nenkova and Kathleen McKeown. 2003. Improv-
ing the coherence of multi-document summaries:
a corpus study for modeling the syntactic realiza-
tion of entities. Technical Report CUCS-001-03,
Columbia University, Computer Science Depart-
ment.
UDel: Named Entity Recognition and Reference Regeneration
from Surface Text
Nicole L. Sparks, Charles F. Greenbacker, Kathleen F. McCoy, and Che-Yu Kuo
Department of Computer and Information Sciences
University of Delaware
Newark, Delaware, USA
[sparks|charlieg|mccoy|kuo]@cis.udel.edu
Abstract
This report describes the methods and re-
sults of a system developed for the GREC
Named Entity Recognition and GREC
Named Entity Regeneration Challenges
2010. We explain our process of automat-
ically annotating surface text, as well as
how we use this output to select improved
referring expressions for named entities.
1 Introduction
Generation of References in Context (GREC) is a
set of shared task challenges in NLG involving a
corpus of introductory sentences from Wikipedia
articles. The Named Entity Recognition (GREC-
NER) task requires participants to recognize all
mentions of people in a document and indicate
which mentions corefer. In the Named Entity Re-
generation (GREC-Full) task, submitted systems
attempt to improve the clarity and fluency of a
text by generating improved referring expressions
(REs) for all references to people. Participants are
encouraged to use the output from GREC-NER as
input for the GREC-Full task. To provide ample
opportunities for improvement, a certain portion
of REs in the corpus have been replaced by more-
specified named references. Ideally, the GREC-
Full output will be more fluent and have greater
referential clarity than the GREC-NER input.
2 Method
The first step in our process to complete the
GREC-NER task is to prepare the corpus for in-
put into the parser by stripping all XML tags and
segmenting the text into sentences. This is accom-
plished with a simple script based on common ab-
breviations and sentence-final punctuation.
Next, the files are run through the Stanford
Parser (The Stanford Natural Language Process-
ing Group, 2010), providing a typed dependency
representation of the input text from which we ex-
tract the syntactic functions (SYNFUNC) of, and
relationships between, words in the sentence.
The unmarked segmented text is also used
as input for the Stanford Named Entity Recog-
nizer (The Stanford Natural Language Processing
Group, 2009). We eliminate named entity tags for
locations and organizations, leaving only person
entities behind. We find the pronouns and com-
mon nouns (e.g. ?grandmother?) referring to per-
son entities that the NER tool does not tag. We
also identify the REG08-Type and case for each
RE. Entities found by the NER tool are marked
as names, and the additional REs we identified
are marked as either pronouns or common nouns.
Case values are determined by analyzing the as-
signed type and any type dependency representa-
tion (provided by the parser) involving the entity.
At this stage we also note the gender of each pro-
noun and common noun, the plurality of each ref-
erence, and begin to deal with embedded entities.
The next step identifies which tagged mentions
corefer. We implemented a coreference resolu-
tion tool using a shallow rule-based approach in-
spired by Lappin and Leass (1994) and Bontcheva
et al (2002). Each mention is compared to all
previously-seen entities on the basis of case, gen-
der, SYNFUNC, plurality, and type. Each en-
tity is then evaluated in order of appearance and
compared to all previous entities starting with the
most recent and working back to the first in the
text. We apply rules to each of these pairs based
on the REG08-Type attribute of the current en-
tity. Names and common nouns are analyzed us-
ing string and word token matching. We collected
extensive, cross-cultural lists of male and female
first names to help identify the gender of named
entities, which we use together with SYNFUNC
values for pronoun resolution. Separate rules gov-
ern gender-neutral pronouns such as ?who.? By
the end of this stage, we have all of the resources
MUC-6 CEAF B-CUBED
Corpus F prec. recall F prec. recall F prec. recall
Entire Set 71.984 69.657 74.471 68.893 68.893 68.893 72.882 74.309 71.509
Chefs 71.094 65.942 77.119 65.722 65.722 65.722 71.245 69.352 73.244
Composers 68.866 66.800 71.064 68.672 68.672 68.672 71.929 73.490 70.433
Inventors 76.170 77.155 75.210 72.650 72.650 72.650 75.443 80.721 70.812
Table 1: Self-evaluation scores for GREC-NER.
necessary to complete the GREC-NER task.
As a post-processing step, we remove all extra
(non-GREC) tags used in previous steps, re-order
the remaining attributes in the proper sequence,
add the list of REs (ALT-REFEX), and write the
final output following the GREC format. At this
point, the GREC-NER task is concluded and its
output is used as input for the GREC-Full task.
To improve the fluency and clarity of the text
by regenerating the referring expressions, we rely
on the system we developed for the GREC Named
Entity Challenge 2010 (NEG), a refined version
of our 2009 submission (Greenbacker and Mc-
Coy, 2009a). This system trains decision trees
on a psycholinguistically-inspired feature set (de-
scribed by Greenbacker and McCoy (2009b)) ex-
tracted from a training corpus. It predicts the most
appropriate reference type and case for the given
context, and selects the best match from the list of
available REs. For the GREC-Full task, however,
instead of using the files annotated by the GREC
organizers as input, we use the files we annotated
automatically in the GREC-NER task. By keep-
ing the GREC-NER output in the GREC format,
our NEG system was able to successfully run un-
modified and generate our output for GREC-Full.
3 Results
Scores calculated by the GREC self-evaluation
tools are provided in Table 1 for GREC-NER and
in Table 2 for GREC-Full.
Corpus NIST BLEU-4
Entire Set 8.1500 0.7953
Chefs 7.5937 0.7895
Composers 7.5381 0.8026
Inventors 7.5722 0.7936
Table 2: Self-evaluation scores for GREC-Full.
4 Conclusions
Until we compare our results with others teams or
an oracle, it is difficult to gauge our performance.
However, at this first iteration of these tasks, we?re
pleased just to have end-to-end RE regeneration
working to completion with meaningful output.
5 Future Work
Future improvements to our coreference resolu-
tion approach involve analyzing adjacent text, uti-
lizing more of the parser output, and applying ma-
chine learning to our GREC-NER methods.
References
Kalina Bontcheva, Marin Dimitrov, Diana Maynard,
Valentin Tablan, and Hamish Cunningham. 2002.
Shallow Methods for Named Entity Coreference
Resolution. In Cha??nes de re?fe?rences et re?solveurs
d?anaphores, workshop TALN 2002, Nancy, France.
Charles Greenbacker and Kathleen McCoy. 2009a.
UDel: Extending reference generation to multiple
entities. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 105?106, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Charles F. Greenbacker and Kathleen F. McCoy.
2009b. Feature selection for reference generation as
informed by psycholinguistic research. In Proceed-
ings of the CogSci 2009 Workshop on Production of
Referring Expressions (PRE-Cogsci 2009), Amster-
dam, July.
Shalom Lappin and Herbert J. Leass. 1994. An Algo-
rithm for Pronominal Anaphora Resolution. Com-
putational Linguistics, 20(4):535?561.
The Stanford Natural Language Processing Group.
2009. Stanford Named Entity Recognizer.
http://nlp.stanford.edu/software/
CRF-NER.shtml.
The Stanford Natural Language Processing Group.
2010. The Stanford Parser: A statistical parser.
http://nlp.stanford.edu/software/
lex-parser.shtml.
