	

	Unsupervised Learning of Arabic Stemming using a Parallel Corpus
Monica Rogati
?
Computer Science Department,
Carnegie Mellon University
mrogati@cs.cmu.edu
Scott McCarley
IBM TJ Watson
Research Center
jsmc@watson.ibm.com
Yiming Yang
Language Technologies Institute,
Carnegie Mellon University
yiming@cs.cmu.edu
Abstract
This paper presents an unsupervised learn-
ing approach to building a non-English
(Arabic) stemmer. The stemming model
is based on statistical machine translation
and it uses an English stemmer and a small
(10K sentences) parallel corpus as its sole
training resources. No parallel text is
needed after the training phase. Mono-
lingual, unannotated text can be used to
further improve the stemmer by allow-
ing it to adapt to a desired domain or
genre. Examples and results will be given
for Arabic , but the approach is applica-
ble to any language that needs affix re-
moval. Our resource-frugal approach re-
sults in 87.5% agreement with a state of
the art, proprietary Arabic stemmer built
using rules, affix lists, and human anno-
tated text, in addition to an unsupervised
component. Task-based evaluation using
Arabic information retrieval indicates an
improvement of 22-38% in average pre-
cision over unstemmed text, and 96% of
the performance of the proprietary stem-
mer above.
1 Introduction
Stemming is the process of normalizing word vari-
ations by removing prefixes and suffixes. From an
?
Work done while a summer intern at IBM TJ Watson Re-
search Center
information retrieval point of view, prefixes and suf-
fixes add little or no additional meaning; in most
cases, both the efficiency and effectiveness of text
processing applications such as information retrieval
and machine translation are improved.
Building a rule-based stemmer for a new, arbitrary
language is time consuming and requires experts
with linguistic knowledge in that particular lan-
guage. Supervised learning also requires large quan-
tities of labeled data in the target language, and qual-
ity declines when using completely unsupervised
methods. We would like to reach a compromise
by using a few inexpensive and readily available re-
sources in conjunction with unsupervised learning.
Our goal is to develop a stemmer generator that
is relatively language independent (to the extent that
the language accepts stemming) and is trainable us-
ing little, inexpensive data. This paper presents
an unsupervised learning approach to non-English
stemming. The stemming model is based on statisti-
cal machine translation and it uses an English stem-
mer and a small (10K sentences) parallel corpus as
its sole training resources.
A parallel corpus is a collection of sentence pairs
with the same meaning but in different languages
(i.e. United Nations proceedings, bilingual newspa-
pers, the Bible). Table 1 shows an example that uses
the Buckwalter transliteration (Buckwalter, 1999).
Usually, entire documents are translated by humans,
and the sentence pairs are subsequently aligned by
automatic means. A small parallel corpus can be
available when native speakers and translators are
not, which makes building a stemmer out of such
corpus a preferable direction.
Arabic English
m$rwE Altqryr Draft report
wAkdt mmvlp zAm-
byA End ErDhA
lltqryr An bldhA
y$hd tgyyrAt xTyrp
wbEydp Almdy fy
AlmydAnyn Al-
syAsy wAlAqtSAdy
In introducing the report,
the representative of Zam-
bia emphasised that her
country was undergoing
serious and far-reaching
changes in the political
and economic field.
Table 1: A Tiny Arabic-English Parallel Corpus
We describe our approach towards reaching this
goal in section 2. Although we are using resources
other than monolingual data, the unsupervised na-
ture of our approach is preserved by the fact that
no direct information about non-English stemming
is present in the training data.
Monolingual, unannotated text in the target lan-
guage is readily available and can be used to further
improve the stemmer by allowing it to adapt to a de-
sired domain or genre. This optional step is closer to
the traditional unsupervised learning paradigm and
is described in section 2.4, and its impact on stem-
mer quality is described in 3.1.4.
Our approach (denoted by UNSUP in the rest of
the paper) is evaluated in section 3.1 by compar-
ing it to a proprietary Arabic stemmer (denoted by
GOLD). The latter is a state of the art Arabic stem-
mer, and was built using rules, suffix and prefix lists,
and human annotated text. GOLD is an earlier ver-
sion of the stemmer described in (Lee et al, ).
The task-based evaluation section 3.2 compares
the two stemmers by using them as a preprocessing
step in the TREC Arabic retrieval task. This section
also presents the improvement obtained over using
unstemmed text.
1.1 Arabic details
In this paper, Arabic was the target language but the
approach is applicable to any language that needs
affix removal. In Arabic, unlike English, both pre-
fixes and suffixes need to be removed for effective
stemming. Although Arabic provides the additional
challenge of infixes, we did not tackle them because
they often substantially change the meaning. Irregu-
lar morphology is also beyond the scope of this pa-
per. As a side note for readers with linguistic back-
ground (Arabic in particular), we do not claim that
the resulting stems are units representing the entire
paradigm of a lexical item. The main purpose of
stemming as seen in this paper is to conflate the to-
ken space used in statistical methods in order to im-
prove their effectiveness. The quality of the result-
ing tokens as perceived by humans is not as impor-
tant, since the stemmed output is intended for com-
puter consumption.
1.2 Related Work
The problem of unsupervised stemming or morphol-
ogy has been studied using several different ap-
proaches. For Arabic, good results have been ob-
tained for plural detection (Clark, 2001). (Gold-
smith, 2001) used a minimum description length
paradigm to build Linguistica, a system for which
the reported accuracy for European languages is cca.
83%. Note that the results in this section are not di-
rectly comparable to ours, since we are focusing on
Arabic.
A notable contribution was published by Snover
(Snover, 2002), who defines an objective function to
be optimized and performs a search for the stemmed
configuration that optimizes the function over all
stemming possibilities of a given text.
Rule-based stemming for Arabic is a problem
studied by many researchers; an excellent overview
is provided by (Larkey et al, ).
Morphology is not limited to prefix and suffix re-
moval; it can also be seen as mapping from a word to
an arbitrary meaning carrying token. Using an LSI
approach, (Schone and Jurafsky, ) obtained 88% ac-
curacy for English. This approach also deals with
irregular morphology, which we have not addressed.
A parallel corpus has been successfully used be-
fore by (Yarowsky et al, 2000) to project part of
speech tags, named entity tags, and morphology in-
formation from one language to the other. For a par-
allel corpus of comparable size with the one used
in our results, the reported accuracy was 93% for
French (when the English portion was also avail-
able); however, this result only covers 90% of the
tokens. Accuracy was later improved using suffix
trees.
(Diab and Resnik, 2002) used a parallel corpus
for word sense disambiguation, exploiting the fact
that different meanings of the same word tend to be
translated into distinct words.
2 Approach
Figure 1: Approach Overview
Our approach is based on the availability of the
following three resources:
? a small parallel corpus
? an English stemmer
? an optional unannotated Arabic corpus
Our goal is to train an Arabic stemmer using these
resources. The resulting stemmer will simply stem
Arabic without needing its English equivalent.
We divide the training into two logical steps:
? Step 1: Use the small parallel corpus
? Step 2: (optional) Use the monolingual corpus
The two steps are described in detail in the fol-
lowing subsections.
2.1 Step 1: Using the Small Parallel Corpus
Figure 2: Step 1 Iteration
In Step 1, we are trying to exploit the English
stemmer by stemming the English half of the paral-
lel corpus and building a translation model that will
establish a correspondence between meaning carry-
ing substrings (the stem) in Arabic and the English
stems.
For our purposes, a translation model is a matrix
of translation probabilities p(Arabic stem| English
stem) that can be constructed based on the small
parallel corpus (see subsection 2.2 for more details).
The Arabic portion is stemmed with an initial guess
(discussed in subsection 2.1.1)
Conceptually, once the translation model is built,
we can stem the Arabic portion of the parallel corpus
by scoring all possible stems that an Arabic word
can have, and choosing the best one. Once the Ara-
bic portion of the parallel corpus is stemmed, we can
build a more accurate translation model and repeat
the process (see figure 2). However, in practice, in-
stead of using a harsh cutoff and only keeping the
best stem, we impose a probability distribution over
the candidate stems. The distribution starts out uni-
form and then converges towards concentrating most
of the probability mass in one stem candidate.
2.1.1 The Starting Point
The starting point is an inherent problem for un-
supervised learning. We would like our stemmer to
give good results starting from a very general initial
guess (i.e. random). In our case, the starting point
is the initial choice of the stem for each individual
word. We distinguish several solutions:
? No stemming.
This is not a desirable starting point, since affix
probabilities used by our model would be zero.
? Random stemming
As mentioned above, this is equivalent to im-
posing a uniform prior distribution over the
candidate stems. This is the most general start-
ing point.
? A simple language specific rule - if available
If a simple rule is available, it would provide a
better than random starting point, at the cost of
reduced generality. For Arabic, this simple rule
was to use Al as a prefix and p as a suffix. This
rule (or at least the first half) is obvious even
to non-native speakers looking at transliterated
text. It also constitutes a surprisingly high base-
line.
2.2 The Translation Model ?
We adapted Model 1 (Brown et al, 1993) to our
purposes. Model 1 uses the concept of alignment
between two sentences e and f in a parallel corpus;
the alignment is defined as an object indicating for
each word ei which word fj generated it. To ob-
tain the probability of an foreign sentence f given the
English sentence e, Model 1 sums the products of
the translation probabilities over all possible align-
ments:
Pr(f |e) ?
?
{a}
m
?
j=1
t(fj|eaj )
The alignment variable ai controls which English
word the foreign word fi is aligned with. t(f |e) is
simply the translation probability which is refined
iteratively using EM. For our purposes, the transla-
tion probabilities (in a translation matrix) are the fi-
nal product of using the parallel corpus to train the
translation model.
To take into account the weight contributed by
each stem, the model?s iterative phase was adapted
to use the sum of the weights of a word in a sentence
instead of the count.
2.3 Candidate Stem Scoring
As previously mentioned, each word has a list of
substrings that are possible stems. We reduced the
problem to that of placing two separators inside each
Arabic word; the ?candidate stems? are simply the
substrings inside the separators. While this may
seem inefficient, in practice words tend to be short,
and one or two letter stems can be disallowed.
An initial, naive approach when scoring the stem
would be to simply look up its translation probabil-
ity, given the English stem that is most likely to be
its translation in the parallel sentence (i.e. the En-
glish stem aligned with the Arabic stem candidate).
Figure 3 presents scoring examples before normal-
ization.
?Note that the algorithm to build the translation model is
not a ?resource? per se, since it is a language-independent algo-
rithm.
English Phrase: the advisory committee
Arabic Phrase: Alljnp AlAst$Aryp
Task: stem AlAst$Aryp
Choices Score
AlAst$Aryp 0.2
AlAst$Aryp 0.7
AlAst$Aryp 0.8
AlAst$Aryp 0.1
.
.
.
.
.
.
Figure 3: Scoring the Stem
However, this approach has several drawbacks
that prevent us from using it on a corpus other than
the training corpus. Both of the drawbacks below
are brought about by the small size of the parallel
corpus:
? Out-of-vocabulary words: many Arabic stems
will not be seen in the small corpus
? Unreliable translation probabilities for low-
frequency stems.
We can avoid these issues if we adopt an alternate
view of stemming a word, by looking at the prefix
and the suffix instead. Given the word, the choice
of prefix and suffix uniquely determines the stem.
Since the number of unique affixes is much smaller
by definition, they will not have the two problems
above, even when using a small corpus. These prob-
abilities will be considerably more reliable and are
a very important part of the information extracted
from the parallel corpus. Therefore, the score of a
candidate stem should be based on the score of the
corresponding prefix and the suffix, in addition to
the score of the stem string itself:
score(?pas??) = f(p) ? f(a) ? f(s)
where a = Arabic stem, p = prefix, s=suffix
When scoring the prefix and the suffix, we could
simply use their probabilities from the previous
stemming iteration. However, there is additional in-
formation available that can be successfully used to
condition and refine these probabilities (such as the
length of the word, the part of speech tag if given
etc.).
English Phrase: the advisory committee
Arabic Phrase: Alljnp AlAst$Aryp
Task: stem AlAst$Aryp
Choices Score
AlAst$Aryp 0.8
AlAst$Aryp 0.7
AlAst$Ary 0.6
AlAst$Aryp 0.1
.
.
.
.
.
.
Figure 4: Alternate View: Scoring the Prefix and
Suffix
2.3.1 Scoring Models
We explored several stem scoring models, using
different levels of available information. Examples
include:
? Use the stem translation probability alone
score = t(a|e)
where a = Arabic stem, e = corresponding word
in the English sentence
? Also use prefix (p) and suffix (s) conditional
probabilities; several examples are given in ta-
ble 2.
Probability con-
ditioned on
Scoring Formula
the candidate stem t(a|e) ? p(p,s|a)+p(s|a)?p(p|a)2
the length of the
unstemmed Arabic
word (len)
t(a|e) ?
p(p,s|len)+p(s|len)?p(p|len)
2
the possible pre-
fixes and/or suf-
fixes
t(a|e) ? p(s|Spossible) ?
p(p|Ppossible)
the first and last
letter
t(a|e)?p(s|last)?p(p|first)
Table 2: Example Scoring Models
The first two examples use the joint probability
of the prefix and suffix, with a smoothing back-off
(the product of the individual probabilities). Scor-
ing models of this form proved to be poor perform-
ers from the beginning, and they were abandoned in
favor of the last model, which is a fast, good approx-
imation to the third model in Table 2. The last two
models successfully solve the problem of the empty
prefix and suffix accumulating excessive probability,
which would yield to a stemmer that never removed
any affixes. The results presented in the rest of the
paper use the last scoring model.
2.4 Step 2: Using the Unlabeled Monolingual
Data
This optional second step can adapt the trained stem-
mer to the problem at hand. Here, we are moving
away from providing the English equivalent, and we
are relying on learned prefix, suffix and (to a lesser
degree) stem probabilities. In a new domain or cor-
pus, the second step allows the stemmer to learn new
stems and update its statistical profile of the previ-
ously seen stems.
This step can be performed using monolingual
Arabic data, with no annotation needed. Even
though it is optional, this step is recommended since
its sole resource can be the data we would need to
stem anyway (see Figure 5).
Arabic Arabic
StemmedStemmerUnstemmed
Figure 5: Step 2 Detail
Step 1 produced a functional stemming model.
We can use the corpus statistics gathered in Step 1
to stem the new, monolingual corpus. However, the
scoring model needs to be modified, since t(a|e) is
no longer available. By removing the conditioning,
the first/last letter scoring model we used becomes
score = p(a) ? p(s|last) ? p(p|first)
The model can be updated if the stem candidate
score/probability distribution is sufficiently skewed,
and the monolingual text can be stemmed iteratively
using the new model. The model is thus adapted to
the particular needs of the new corpus; in practice,
convergence is quick (less than 10 iterations).
3 Results
3.1 Unsupervised Training and Testing
For unsupervised training in Step 1, we used a small
parallel corpus: 10,000 Arabic-English sentences
from the United Nations(UN) corpus, where the En-
glish part has been stemmed and the Arabic translit-
erated.
For unsupervised training in Step 2, we used a
larger, Arabic only corpus: 80,000 different sen-
tences in the same dataset.
The test set consisted of 10,000 different sen-
tences in the UN dataset; this is the testing set used
below unless specified.
We also used a larger corpus ( a year of Agence
France Press (AFP) data, 237K sentences) for Step 2
training and testing, in order to gauge the robustness
and adaptation capability of the stemmer. Since the
UN corpus contains legal proceedings, and the AFP
corpus contains news stories, the two can be seen as
coming from different domains.
3.1.1 Measuring Stemmer Performance
In this subsection the accuracy is defined as agree-
ment with GOLD. GOLD is a state of the art, pro-
prietary Arabic stemmer built using rules, suffix and
prefix lists, and human annotated text, in addition
to an unsupervised component. GOLD is an ear-
lier version of the stemmer described in (Lee et al,
). Freely available (but less accurate) Arabic light
stemmers are also used in practice.
When measuring accuracy, all tokens are consid-
ered, including those that cannot be stemmed by
simple affix removal (irregulars, infixes). Note that
our baseline (removing Al and p, leaving everything
unchanged) is higher that simply leaving all tokens
unchanged.
For a more relevant task-based evaluation, please
refer to Subsection 3.2.
3.1.2 The Effect of the Corpus Size: How little
parallel data can we use?
We begin by examining the effect that the size of
the parallel corpus has on the results after the first
step. Here, we trained our stemmer on three dif-
ferent corpus sizes: 50K, 10K, and 2K sentences.
The high baseline is obtained by treating Al and p
as affixes. The 2K corpus had acceptable results (if
this is all the data available). Using 10K was sig-
nificantly better; however the improvement obtained
when five times as much data (50K) was used was
insignificant. Note that different languages might
have different corpus size needs. All other results
Figure 6: Results after Step 1 : Corpus Size Effect
in this paper use 10K sentences.
3.1.3 The Knowledge-Free Starting Point after
Step 1
Figure 7: Results after Step 1 : Effect of Knowing
the Al+p rule
Although severely handicapped at the beginning,
the knowledge-free starting point manages to narrow
the performance gap after a few iterations. Knowing
the Al+p rule still helps at this stage. However, the
performance gap is narrowed further in Step 2 (see
figure 8), where the knowledge free starting point
benefitted from the monolingual training.
3.1.4 Results after Step 2: Different Corpora
Used for Adaptation
Figure 8 shows the results obtained when aug-
menting the stemmer trained in Step 1. Two dif-
ferent monolingual corpora are used: one from the
same domain as the test set (80K UN), and one from
a different domain/corpus, but three times larger
(237K AFP). The larger dataset seems to be more
useful in improving the stemmer, even though the
domain was different.
Figure 8: Results after Step 2 (Monolingual Corpus)
The baseline and the accuracy after Step 1 are pre-
sented for reference.
3.1.5 Cross-Domain Robustness
Figure 9: Results after Step 2 : Using a Different
Test Set
We used an additional test set that consisted of
10K sentences taken from AFP, instead of UN as in
previous experiments shown in figure 8 . Its pur-
pose was to test the cross-domain robustness of the
stemmer and to further examine the importance of
applying the second step to the data needing to be
stemmed.
Figure 9 shows that, even though in Step 1 the
stemmer was trained on UN proceedings, the re-
sults on the cross-domain (AFP) test set are compa-
rable to those from the same domain (UN, figure 8).
However, for this particular test set the baseline was
much higher; thus the relative improvement with re-
spect to the baseline is not as high as when the unsu-
pervised training and testing set came from the same
collection.
3.2 Task-Based Evaluation : Arabic
Information Retrieval
Task Description:
Given a set of Arabic documents and an Arabic
query, find a list of documents relevant to the query,
and rank them by probability of relevance.
We used the TREC 2002 documents (several
years of AFP data), queries and relevance judg-
ments. The 50 queries have a shorter, ?title? compo-
nent as wel as a longer ?description?. We stemmed
both the queries and the documents using UNSUP
and GOLD respectively. For comparison purposes,
we also left the documents and queries unstemmed.
The UNSUP stemmer was trained with 10K UN
sentences in Step 1, and with one year?s worth of
monolingual AFP data (1995) in Step 2.
Evaluation metric: The evaluation metric used
below is mean average precision (the standard IR
metric), which is the mean of average precision
scores for each query. The average precision of a
single query is the mean of the precision scores after
each relevant document retrieved. Note that aver-
age precision implicitly includes recall information.
Precision is defined as the ratio of relevant docu-
ments to total documents retrieved up to that point
in the ranking.
Results
Figure 10: Arabic Information Retrieval Results
We looked at the effect of different testing con-
ditions on the mean average precision for the 50
queries. In Figure 10, the first set of bars uses the
query titles only, the second set adds the descrip-
tion, and the last set restricts the results to one year
(1995), using both the title and description. We
tested this last condition because the unsupervised
stemmer was refined in Step 2 using 1995 docu-
ments. The last group of bars shows a higher relative
improvement over the unstemmed baseline; how-
ever, this last condition is based on a smaller sample
of relevance judgements (restricted to one year) and
is therefore not as representative of the IR task as the
first two testing conditions.
4 Conclusions and Future Work
This paper presents an unsupervised learning ap-
proach to building a non-English (Arabic) stemmer
using a small sentence-aligned parallel corpus in
which the English part has been stemmed. No paral-
lel text is needed to use the stemmer. Monolingual,
unannotated text can be used to further improve the
stemmer by allowing it to adapt to a desired domain
or genre. The approach is applicable to any language
that needs affix removal; for Arabic, our approach
results in 87.5% agreement with a proprietary Ara-
bic stemmer built using rules, affix lists, and hu-
man annotated text, in addition to an unsupervised
component. Task-based evaluation using Arabic in-
formation retrieval indicates an improvement of 22-
38% in average precision over unstemmed text, and
93-96% of the performance of the state of the art,
language specific stemmer above.
We can speculate that, because of the statistical
nature of the unsupervised stemmer, it tends to fo-
cus on the same kind of meaning units that are sig-
nificant for IR, whether or not they are linguistically
correct. This could explain why the gap betheen
GOLD and UNSUP is narrowed with task-based
evaluation and is a desirable effect when the stem-
mer is to be used for IR tasks.
We are planning to experiment with different lan-
guages, translation model alternatives, and to extend
task-based evaluation to different tasks such as ma-
chine translation and cross-lingual topic detection
and tracking.
5 Acknowledgements
We would like to thank the reviewers for their help-
ful observations and for identifying Arabic mis-
spellings. This work was partially supported by
the Defense Advanced Research Projects Agency
and monitored by SPAWAR under contract No.
N66001-99-2-8916. This research is also spon-
sored in part by the National Science Foundation
(NSF) under grants EIA-9873009 and IIS-9982226,
and in part by the DoD under award 114008-
N66001992891808. However, any opinions, views,
conclusions and findings in this paper are those of
the authors and do not necessarily reflect the posi-
tion of policy of the Government and no official en-
dorsement should be inferred.
References
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of machine translation:
Parameter estimation. In Computational Linguistics,
pages 263?311.
Tim Buckwalter. 1999. Buckwalter transliteration.
http://www.cis.upenn.edu/?cis639/arabic/info/translit-
chart.html.
Alexander Clark. 2001. Learning morphology with pair
hidden markov models. In ACL (Companion Volume),
pages 55?60.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
255?262, July.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. In Computational
Linguistics.
Leah Larkey, Lisa Ballesteros, and Margaret Connell.
Improving stemming for arabic information retrieval:
Light stemming and co-occurrence analysis. In SIGIR
2002, pages 275?282.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. Language model
based arabic word segmentation. In To appear in ACL
2003.
Patrick Schone and Daniel Jurafsky. Knowledge-free in-
duction of morphology using latent semantic analy-
sis. In 4th Conference on Computational Natural Lan-
guage Learning, Lisbon, 2000.
Matthew Snover. 2002. An unsupervised knowledge
free algorithm for the learning of morphology in natu-
ral languages. Master?s thesis, Washington University,
May.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2000. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora.
Customizing Parallel Corpora at the Document Level  
Monica ROGATI and Yiming YANG 
Computer Science Department, Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA 15213 
mrogati@cs.cmu.edu, yiming@cs.cmu.edu 
 
 
Abstract 
Recent research in cross-lingual 
information retrieval (CLIR) established the 
need for properly matching the parallel corpus 
used for query translation to the target corpus. 
We propose a document-level approach to 
solving this problem: building a custom-made 
parallel corpus by automatically assembling it 
from documents taken from other parallel 
corpora. Although the general idea can be 
applied to any application that uses parallel 
corpora, we present results for CLIR in the 
medical domain. In order to extract the best-
matched documents from several parallel 
corpora, we propose ranking individual 
documents by using a length-normalized 
Okapi-based similarity score between them and 
the target corpus. This ranking allows us to 
discard 50-90% of the training data, while 
avoiding the performance drop caused by a 
good but mismatched resource, and even 
improving CLIR effectiveness by 4-7% when 
compared to using all available training data. 
1 Introduction 
Our recent research in cross-lingual information 
retrieval (CLIR) established the need for properly 
matching the parallel corpus used for query 
translation to the target corpus (Rogati and Yang, 
2004). In particular, we showed that using a 
general purpose machine translation (MT) system 
such as SYSTRAN, or a general purpose parallel 
corpus - both of which perform very well for news 
stories (Peters, 2003) - dramatically fails in the 
medical domain. To explore solutions to this 
problem, we used cosine similarity between 
training and target corpora as respective weights 
when building a translation model. This approach 
treats a parallel corpus as a homogeneous entity, an 
entity that is self-consistent in its domain and 
document quality. In this paper, we propose that 
instead of weighting entire resources, we can select 
individual documents from these corpora in order 
to build a parallel corpus that is tailor-made to fit a 
specific target collection. To avoid confusion, it is 
helpful to remember that in IR settings the true test 
data are the queries, not the target documents. The 
documents are available off-line and can be (and 
usually are) used for training and system 
development. In other words, by matching the 
training corpora and the target documents we are 
not using test data for training.  
(Rogati and Yang, 2004) also discusses 
indirectly related work, such as query translation 
disambiguation and building domain-specific 
language models for speech recognition. We are 
not aware of any additional related work. 
In addition to proposing individual documents 
as the unit for building custom-made parallel 
corpora, in this paper we start exploring the criteria 
used for individual document selection by 
examining the effect of ranking documents using 
the length-normalized Okapi-based similarity score 
between them and the target corpus. 
2 Evaluation Data 
2.1 Medical Domain Corpus: Springer 
The Springer corpus consists of 9640 documents 
(titles plus abstracts of medical journal articles) 
each in English and in German, with 25 queries in 
both languages, and relevance judgments made by 
native German speakers who are medical experts 
and are fluent in English. We split this parallel 
corpus into two subsets, and used the first subset 
(4,688 documents) for training, and the remaining 
subset (4,952 documents) as the test set in all our 
experiments. This configuration allows us to 
experiment with CLIR in both directions (EN-DE 
and DE-EN). We applied an alignment algorithm 
to the training documents, and obtained a sentence-
aligned parallel corpus with about 30K sentences 
in each language.  
2.2 Training Corpora 
In addition to Springer, we have used four other 
English-German parallel corpora for training: 
? NEWS is a collection of 59K sentence 
aligned news stories, downloaded from the 
web (1996-2000), and available at 
http://www.isi.edu/~koehn/publications/de-
news/ 
? WAC is a small parallel corpus obtained by 
mining the web (Nie et al, 2000), in no 
particular domain 
? EUROPARL is a parallel corpus provided 
by (Koehn). Its documents are sentence 
aligned European Parliament proceedings. 
This is a large collection that has been 
successfully used for CLEF, when the target 
corpora were collections of news stories 
(Rogati and Yang, 2003). 
? MEDTITLE is an English-German parallel 
corpus consisting of 549K paired titles of 
medical journal articles. These titles were 
gathered from the PubMed online database 
(http://www.ncbi.nlm.nih.gov/PubMed/).  
Table 1 presents a summary of the five training 
corpora characteristics. 
 
Name Size (sent) Domain 
NEWS 59K news 
WAC 60K mixed 
EUROPAR
L 665K politics 
SPRINGE
R 30K medical 
MEDTITL
E 550K medical 
 
Table 1. Characteristics of Parallel Training 
Corpora 
 
3 Selecting Documents from Parallel Corpora   
While selecting and weighing entire training 
corpora is a problem already explored by (Rogati 
and Yang, 2004), in this paper we focus on a lower 
granularity level: individual documents in the 
parallel corpora. We seek to construct a custom 
parallel corpus, by choosing individual documents 
which best match the testing collection. We 
compute the similarity between the test collection 
(in German or English) and each individual 
document in the parallel corpora for that respective 
language.  We have a choice of similarity metrics, 
but since this computation is simply retrieval with 
a long query, we start with the Okapi model  
(Robertson, 1993), as implemented by the Lemur 
system (Olgivie and Callan, 2001). Although the 
Okapi model takes into account average document 
length, we compare it with its length-normalized 
version, measuring per-word similarity. The two 
measures are identified in the results section by 
?Okapi? and ?Normalized?. 
Once the similarity is computed for each 
document in the parallel corpora, only the top N 
most similar documents are kept for training. They 
are an approximation of the domain(s) of the test 
collection. Selecting N has not been an issue for 
this corpus   (values between 10-75% were safe). 
However, more generally, this parameter can be 
tuned to a different test corpus as any other 
parameter. Alternatively, the document score can 
also be incorporated into the translation model, 
eliminating the need for thresholding. 
4 CLIR Method 
We used a corpus-based approach, similar to that 
in (Rogati and Yang, 2003). Let L1 be the source 
language and L2 be the target language. The cross-
lingual retrieval consists of the following steps: 
1. Expanding a query in L1 using blind 
feedback 
2. Translating the query by taking the dot 
product between the query vector (with 
weights from step 1) and a translation 
matrix obtained by calculating translation 
probabilities or term-term similarity using 
the parallel corpus. 
3. Expanding the query in L2 using blind 
feedback 
4. Retrieving documents in L2 
Here, blind feedback is the process of retrieving 
documents and adding the terms of the top-ranking 
documents to the query for expansion. We used 
simplified Rocchio positive feedback as 
implemented by Lemur (Olgivie and Callan, 2001). 
For the results in this paper, we have used 
Pointwise Mutual Information (PMI) instead of 
IBM Model 1 (Brown et al, 1993), since (Rogati 
and Yang, 2004) found it to be as effective on 
Springer, but faster to compute.  
 
5 Results and Discussion 
5.1 Empirical Settings 
For the retrieval part of our system, we adapted 
Lemur (Ogilvie and Callan, 2001)  to allow the use 
of weighted queries. Several parameters were 
tuned, none of them on the test set.  In our corpus-
based approach, the main parameters are those 
used in query expansion based on pseudo-
relevance, i.e., the maximum number of documents 
and the maximum number of words to be used, and 
the relative weight of the expanded portion with 
respect to the initial query. Since the Springer 
training set is fairly small, setting aside a subset of 
the data for parameter tuning was not desirable. 
We instead chose parameter values that were stable 
on the CLEF collection (Peters, 2003): 5 and 20 as 
the maximum numbers of documents and words, 
respectively. The relative weight of the expanded 
portion with respect to the initial query was set to 
0.5. The results were evaluated using mean 
average precision (AvgP), a standard performance 
measure for IR evaluations. 
In the following sections, DE-EN refers to 
retrieval where the query is in German and the 
documents in English, while EN-DE refers to 
retrieval in the opposite direction. 
5.2 Using the Parallel Corpora Separately 
Can we simply choose a parallel corpus that 
performed very well on news stories, hoping it is 
robust across domains? Natural approaches also 
include choosing the largest corpus available, or 
using all corpora together. Figure 1 shows the 
effect of these strategies. 
 
 
Figure 1. CLIR results on the Springer test set by 
using PMI with different training corpora. 
 
 
We notice that choosing the largest collection 
(EUROPARL), using all resources available 
without weights (ALL), and even choosing a large 
collection in the medical domain (MEDTITLE) are 
all sub-optimal strategies.   
Given these results, we believe that resource 
selection and weighting is necessary. Thoroughly 
exploring weighting strategies is beyond the scope 
of this paper and it would involve collection size, 
genre, and translation quality in addition to a 
measure of domain match. Here, we start by 
selecting individual documents that match the 
domain of the test collection. We examine the 
effect this choice has on domain-specific CLIR.  
5.3 Using Okapi weights to build a custom 
parallel corpus 
Figures 2 and 3 compare the two document 
selection strategies discussed in Section 3 to using 
all available documents, and to the ideal (but not 
truly optimal) situation where there exists a ?best? 
resource to choose and this collection is known. By 
?best?, we mean one that can produce optimal 
results on the test corpus, with respect to the given 
metric In reality, the true ?best? resource is 
unknown: as seen above, many intuitive choices 
for the best collection are not optimal. 
 
40
45
50
55
60
1 10 100
Percent Used (log)
A
v
er
ag
e 
Pr
ec
is
io
n
Okapi Normalized
All Corpora Best Corpus
 
 
Figure 2. CLIR  DE-EN performance vs. Percent 
of Parallel Documents  Used. ?Best Corpus? is 
given by an oracle and is usually unknown. 
 
 
50
55
60
65
70
1 10 100
Percent Used (log)
A
v
er
ag
e 
Pr
ec
is
io
n
Okapi Normalized
All Corpora Best Corpus
 
 
Figure 3. CLIR  EN-DE performance vs. Percent 
of Parallel Documents Used. ?Best Corpus? is 
given by an oracle and is usually unknown 
 
0
10
20
30
40
50
60
70
EN-DE DE-ENAvgP.
SPRINGER MEDTITLE WAC
NEWS EUROPARL ALL
 Notice that the normalized version performs better 
and is more stable. Per-word similarity is, in this 
case, important when the documents are used to 
train translation scores: shorter parallel documents 
are better when building the translation matrix. Our 
strategy accounts for a 4-7% improvement over 
using all resources with no weights, for both 
retrieval directions. It is also very close to the 
?oracle? condition, which chooses the best 
collection in advance. More importantly, by using 
this strategy we are avoiding the sharp 
performance drop when using a mismatched, 
although very good, resource (such as 
EUROPARL). 
 
6 Future Work 
We are currently exploring weighting strategies 
involving collection size, genre, and estimating 
translation quality in addition to a measure of 
domain match.  Another question we are 
examining is the granularity level used when 
selecting resources, such as selection at the 
document or cluster level.  
Similarity and overlap between resources 
themselves is also worth considering while 
exploring tradeoffs between redundancy and noise.  
We are also interested in how these approaches 
would apply to other domains.  
 
7 Conclusions 
We have examined the issue of selecting 
appropriate training resources for cross-lingual 
information retrieval. We have proposed and 
evaluated a simple method for creating a 
customized parallel corpus from other available 
parallel corpora by matching the domain of the test 
documents with that of individual parallel 
documents. We noticed that choosing the largest 
collection, using all resources available without 
weights, and even choosing a large collection in 
the medical domain are all sub-optimal strategies. 
The techniques we have presented here are not 
restricted to CLIR and can be applied to other 
areas where parallel corpora are necessary, such as 
statistical machine translation. The trained 
translation matrix can also be reused and can be 
converted to any of the formats required by such 
applications. 
 
8 Acknowledgements 
We would like to thank Ralf Brown for collecting 
the MEDTITLE and SPRINGER data.  
This research is sponsored in part by the National 
Science Foundation (NSF) under grant IIS-
9982226, and in part by the DOD under award 
114008-N66001992891808. Any opinions and 
conclusions in this paper are the authors? and do 
not necessarily reflect those of the sponsors. 
References 
Brown, P.F, Pietra, D., Pietra, D, Mercer, R.L. 1993.The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. In Computational Linguistics, 
19:263-312 
Koehn, P. Europarl: A Multilingual Corpus for 
Evaluation of Machine Translation. Draft, 
Unpublished. 
Nie, J. Y., Simard, M. and Foster, G.. 2000. Using 
parallel web pages for multi-lingual IR. In C. 
Peters(Ed.),  Proceedings of the CLEF 2000 forum  
Ogilvie, P. and Callan, J. 2001.  Experiments using the 
Lemur toolkit. In Proceedings of the Tenth Text 
Retrieval Conference (TREC-10).  
Peters, C. 2003. Results of the CLEF 2003 Cross-Language 
System Evaluation Campaign. Working Notes for the 
CLEF 2003 Workshop, 21-22 August, Trondheim, 
Norway 
Robertson, S.E. and all. 1993. Okapi at TREC. In The 
First TREC Retrieval Conference, Gaithersburg, MD.  
pp. 21-30 
Rogati, M and Yang, Y. 2003. Multilingual Information 
Retrieval using Open, Transparent Resources in 
CLEF 2003 . In C. Peters (Ed.), Results of the 
CLEF2003 cross-language evaluation forum 
Rogati, M and Yang, Y. 2004. Resource Selection for 
Domain Specific Cross-Lingual IR. In Proceedings of 
ACM SIGIR Conference on Research and 
Development in Information Retrieval (SIGIR'04). 
