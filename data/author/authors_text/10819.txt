The Complexity of Ranking Hypotheses in
Optimality Theory
Jason Riggle?
University of Chicago
Given a constraint set with k constraints in the framework of Optimality Theory (OT), what is
its capacity as a classification scheme for linguistic data? One useful measure of this capacity
is the size of the largest data set of which each subset is consistent with a different grammar
hypothesis. This measure is known as the Vapnik-Chervonenkis dimension (VCD) and is a
standard complexity measure for concept classes in computational learnability theory. In this
work, I use the three-valued logic of Elementary Ranking Conditions to show that the VCD of
Optimality Theory with k constraints is k?1. Analysis of OT in terms of the VCD establishes
that the complexity of OT is a well-behaved function of k and that the ?hardness? of learning in
OT is linear in k for a variety of frameworks that employ probabilistic definitions of learnability.
1. Introduction
Given a set CON of k constraints in the framework of Optimality Theory (OT; Prince
and Smolensky 1993), what is the capacity of CON as a classification scheme for samples
of language data? In OT, constraints are functions that map candidates to natural num-
bers, where each candidate is a member of the (possibly infinite) set of possible deriva-
tions of an input form i supplied by the candidate generating function GEN(i). The
number that a constraint Ci assigns to a candidate indicates how many times that
candidate violates Ci. A grammar is a ranking of the constraints that imposes a total
ordering on CON, RCON (or simply R when CON is clear from the context), and the
language that is generated by grammar R is the set of candidates that are optimal
according toR as in Definition 1.
Definition 1
a. Candidate a is more harmonic than candidate b according to R, written a b,
if they share the same input and a is assigned fewer violations by the highest-
ranked constraint that assigns different numbers of violations to a and b.
b. Candidate a is optimal according to rankingR iff no other candidate generated
by GEN is more harmonic than a.
Because each of the k! rankings of CON is a different grammar that generates a
potentially unique language, one natural measure of the classificatory capacity of CON
? Department of Linguistics, University of Chicago, 1010 E. 59th St., Chicago, IL 60637.
jriggle@uchicago.edu. Many thanks to Alan Prince, Jeff Heinz, Greg Kobele, Colin Wilson, and two
anonymous Computational Linguistics reviewers for helpful comments and suggestions. Any errors are, of
course, my own.
Submission received: 22 December 2006; revised submission received: 9 October 2007; accepted for
publication: 17 December 2007.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 1
is the upper bound of k! languages in what Prince and Smolensky (1993, page 27) dub
the factorial typology of the constraint set. Another complexity metric that is useful in
analyses of learnability (especially for non-finite concept classes) is the cardinality of
the largest data set of which each subset corresponds to a different ranking hypothesis.
The idea of measuring the complexity of a concept class (in the case at hand, a set of
grammars) in this way comes from the work of Vapnik and Chervonenkis (1971) and is
known as the Vapnik-Chervonenkis dimension (VCD). In OT, the VCD of a constraint
set CON (i.e., the concept class consisting of languages generated by rankings of CON)
is the size of the largest sample (set of candidates) that is shatterable as in Definition 2.
Definition 2
A sample S is shatterable by a constraint set CON iff, for every partitioning of S into two
disjoint sets T and F (including the null/whole partition), there is at least one ranking
RCON that makes every s ? T optimal but no s ? F optimal.
Vapnik and Chervonenkis?s definition of shatterability has interesting implications
for samples consisting of OT candidates. For instance, each candidate in a shatterable
sample S must be an input ? output mapping for a unique input form because two
candidates a and b with the same input would either tie with identical sets of violations
or show harmonic inequality. In the case of a tie, no ranking could realize a partitioning
that separates a and b and, in the case of harmonic inequality, no ranking could realize
a partitioning in which a and b are simultaneously optimal. More generally, the VCD
places an upper bound on the number of distinct grammar hypotheses that can be real-
ized over any sample of linguistic data consisting of OT candidates, and thus provides
a ready measure of the complexity of the hypothesis space in Optimality Theory.
The VCD of a concept class is obviously not independent of its size. As Blumer
et al (1989) point out, for any finite concept class C, the VCD is bounded at log2 |C|
because it takes at least 2d hypotheses to associate a unique hypothesis with every sub-
set of a sample of size d. Thus, because the number of grammars (hypotheses) over
k constraints is finite?one grammar for each of the k! rankings?the VCD of OT is
bounded at log2 k!. Or, put more simply, because log2 x! ? x log2 x, this establishes
k log2 k as an upper bound on the VCD of OT. In this article, I will show how the
structure of the hypothesis space in Optimality Theory provides a tighter bound on
the VCD of OT than the bound established by the finitude of the hypothesis space. I
will improve upon the inherent bound of k log2 k by showing that the VCD of OT with
k constraints is actually bounded at k ? 1 and thus grows linearly with the size of |CON|.
The complexity measured by the VC dimension has a number of ramifications for
learning in Optimality Theory. For instance, the VCD of a concept class places an abso-
lute lower bound on the number of mistakes that any error-driven learning algorithm
can be guaranteed of achieving (Littlestone 1988). This fact tells us that it may yet
be possible to improve upon the quadratic mistake bound of (k2 ? k)/2 for Recursive
Constraint Demotion (Tesar and Smolensky 1993, 2000; Tesar 1995, 1997, 1998), the
reigning mistake bound for any OT learning algorithm. The VCD of a concept class
also provides a very general bound on the number of data samples that are required for
learning in probabilistic models of learning that will be discussed in Section 5.
2. Elementary Ranking Conditions
The main result for the VC dimension of OT will be given in Section 4. First, some
supporting results will be established showing that there is an upper bound of k ? 1 on
48
Riggle Ranking Hypotheses in OT
shatterable sets of statements about constraint rankings that are expressed with Prince?s
(2002) Elementary Ranking Conditions.
If our sample space X consists of candidates, then any x ? X can be described
in terms of the set of constraint rankings under which x is optimal. Prince (2002)
provides a scheme for encoding this kind of ranking information called an Elementary
Ranking Condition (ERC). In this section, I will review some formal properties of ERCs
that are relevant for establishing the VC dimension of OT. Prince demonstrates many
formal properties of ERCs beyond those covered here and shows that ERCS are equiv-
alent to the implication-negation fragment of the three-valued relevance logic RM3 (cf.
Anderson and Belnap 1975). This section will review properties of ERCs that are most
relevant for the results at hand. For formal proofs and a complete exposition of the logic
of ERCs, see Prince (2002).
For a constraint set CON containing k constraints, ERCs are k-length vectors that
use the symbols L, e, and W to encode logical statements about rankings. Each con-
straint is assigned an arbitrary numeric index, and in each ERC ?, the ith coordinate ?i
refers to the constraint with ith index Ci. The meaning of an ERC is that at least one
constraint whose corresponding coordinate contains a W outranks all of the constraints
whose coordinates contain L?s. Thus, ?W, e, L, L? means that C1 outranks both C3 and
C4, while ?L, L, W, W? means that either C3 or C4 outranks both C1 and C2. ERCs can
be constructed by comparing candidates as in Definition 3. Note that Ci(a) denotes the
number of times candidate a violates the constraint with index i.
Definition 3
Given a constraint set CON with k constraints indexed {1 ... k} and two candidates
that share the same input, the function ercCON(a, b) returns an ERC ? = ??1, ...,?k? that
describes the rankings under which a b.1
erc(a, b) = ??1, ...,?k?where
?
?
?
?
?
?i = W if Ci(a) < Ci(b)
?i = L if Ci(a) > Ci(b)
?i = e if Ci(a) = Ci(b)
The symbol W in ?i of erc(a, b) = ? is a mnemonic for the fact that Ci favors a
(the winner), whereas an L in coordinate i is a mnemonic for the fact that Ci favors b
(the loser). An e in ?i indicates that the candidates are equivalent according to Ci.
Example 1
input C1 C2 C3
cand. a * ** * erc(b, a) = ?L, W, W ? = b a if C2 or C3 outranks C1
cand. b ** * erc(a, b) = ?W, L, L ? = a b if C1 outranks C2 and C3
(*?s indicate number of violations)
Note the symmetry between erc(a, b) = ?W, L, L ?, which says that candidate a is more
harmonic than b under any ranking where C1 outranks both C2 and C3, and erc(b, a),
1 The function ercCON(a, b), or simply erc(a, b) when CON is clear from context, is undefined for candidate
input ? output mappings with different inputs because they cannot be meaningfully compared.
49
Computational Linguistics Volume 35, Number 1
which says that b is more harmonic than a under any ranking where either C2 or C3
outranksC1. This symmetry reflects the fact that erc(a, b) and erc(b, a) encode antithetical
ranking conditions. The opposition between these ERCs follows straightforwardly from
the fact that only one of the two candidates can be optimal under any given ranking.
The illustrative tableaux presented with OT analyses can be turned into sets of
ERCs by making pairwise comparisons between the violations for one designated (or
observed) winner and the violations for each other candidate.
Example 2
input C1 C2 C3
cand. a * ** winner
cand. b ** * erc(a, b) = ?W, L, e ? = a b if C1 outranks C2
cand. c * ** erc(a, c) = ?e, L, W? = a c if C3 outranks C2
cand. d * * * erc(a, d) = ?e, L, W? = a d if C3 outranks C2
cand. e ** ** erc(a, e) = ?W, e, e ? = a e under every ranking
cand. f *** * erc(a, f ) = ?L, W, W? = a f if C2 or C3 outranks C1
The comparison of candidate a with candidate e in Example 2, erc(a, e) = ?W, e, e?,
yields an odd ranking condition that does not actually express a particular ranking (no
constraint has an L), but instead indicates that C1 favors candidate a and no constraint
favors candidate e. In this case, candidate e is said to be harmonically bounded by
candidate a because there can be no ranking under which e is more harmonic than
a. Conversely, if candidate e were designated the winner, then erc(e, a) = ?L, e, e?. This
ERC also does not encode a specific ranking, but rather indicates that the mere existence
of candidate a as an alternative means that no ranking can make candidate e optimal.
Like most OT tableaux, Example 2 is an illustration of how a handful of candidates
fare with respect to one another according to a particular set of constraints. To know
which rankings (if any) make candidate a globally optimal, it would be necessary to
define the candidate generating function GEN in order to obtain a representation of the
entire set of ERCs {erc(a, x) | x ? GEN(input)} = ERCS(a). This is not as daunting as it
might appear because, even though |GEN(input)|may be infinite, the fact that the num-
ber of k-length ERCs is finite guarantees that each of the candidates in GEN(input) will
map to one of a finite number of ERC sets. Furthermore, as Riggle (2004) demonstrates,
the standard OT assumption of the universality of faithfulness constraints that pe-
nalize changes to the input guarantees that all but finitely many of the members of
GEN(input) will be harmonically bounded. Riggle also presents an algorithm for com-
puting this finite set of contenders (i.e., candidates that are not harmonically bounded)
that can be used in cases where GEN is restricted so that it is a rational function.2
Regardless of how optimization is computed, what is relevant for the assessment of
the VCD of OT is the definition of optimality. Following Definition 1, a rankingRCON can
be seen as a function from candidates to True (if they are optimal) or False (if they are
2 GEN is rational if it is representable as a finite state transducer. Riggle?s (2004) CONTENDERS algorithm is
an extension of Ellison?s (1994) application of Dijkstra?s (1959) ?shortest paths? algorithm to optimization
in OT that operates over finite-state representations of GEN and EVAL. Ellison showed that if harmony is
used as the ?distance? to be optimized, then optimal outputs can be efficiently found. The CONTENDERS
algorithm follows a similar strategy but, instead of finding the shortest (i.e., most harmonic) path for one
ranking, the algorithm finds all non-harmonically-bounded paths and thereby optimizes for all rankings.
50
Riggle Ranking Hypotheses in OT
not). The entire ERC set for a candidate ERCS(a) describes exactly the rankings under
which candidate a is a globally optimal candidate.
The reduction of candidates to ERC sets makes it possible to use the logic of ERCs
to reason about candidates. Most of the time, the ERCs of interest are those that contain
at least one L and one W?what Prince calls nontrivial ERCs. ERCs that contain W?s
but no L?s are generated when a candidate is compared with another candidate that it
harmonically bounds, such as erc(a, e) = ?W, e, e ? in Example 2. This ERC reveals that
candidate e cannot be optimal but yields no information about what rankings make
candidate a optimal. Similarly, no ranking information can be gleaned from the all-e
ERC that results from comparing ?tied? candidates that have the same violations.
Finally, ERCs like erc(e, a) = ?L, e, e ?, with L?s but no W?s reveal nothing other than the
fact that candidate e cannot be optimal under any ranking.
The most relevant logical relation for ERCs is that of entailment. The entailment
relation among nontrivial ERCs is given in Definition 4 (Prince 2002, page 6, Proposi-
tion 1.1).
Definition 4
For nontrivial ERCs ? and ?, ? ? ? iff each ?i ? ? entails ?i ? ? where L ? e ? W.
Because nontrivial ERCs encode disjunctions of conjunctions (i.e., [C1 or ... Cn] outranks
[C1? and ... Cn? ]), entailments of the form ? ? ? line up with the logical operations of
disjunction introduction (whenever ? has W where ? has an L or an e) and conjunction
elimination (whenever ? has an e where ? has an L).
Example 3
?W, L, L, e? ? ?W, e, L, e? i.e., If C1 outranks C2 and C3 then C1 outranks C3.
?W, e, L, e? ? ?W, e, L, W? i.e., If C1 outranks C3 then C1 or C4 outranks C3.
?W, L, L, e? ? ?W, e, L, W? i.e., If C1 outranks C2 and C3 then C1 or C4 outranks C3.
In addition to revealing entailments among individual ranking conditions, the logic
of ERCs makes it possible to derive new ranking conditions that are entailed by the
combination of other ERCs. Prince (2002, page 8) provides a logical operation called
fusion that derives entailments from sets of ERCs.
Definition 5
The fusion of ERC set ? is a single ERC ? that is entailed by ? where:
?i = L if any ERC in ? has an L in its i
th coordinate,
?i = e if every ERC in ? has an e in its i
th coordinate,
?i = W otherwise.
Every ERC entailed by ? is entailed by the fusion of a subset of ? (Prince 2002,
page 14). Thus, the operation of fusion can reveal nonobvious entailments among ERCs.
Consider ? = {?W,W, e, L?, ?L, W, W, e?, ?W, e, L, W?}. The ERCs in ? denote, respectively,
?C1 orC2 outranks C4,? ?C2 orC3 outranks C1,? and ?C1 orC4 outranksC3.? The fusion
of ? is ?L, W, L, L?, which encodes the inference from ? that C2 outranks C1, C3, and C4.
The operation of fusion can also reveal inconsistencies in ERC sets. Consider the
set ? = {?W, L, W?, ?L, W, W?, ?W,W, L?}. Fusing ? yields ?L, L, L?. As with harmonically
bounded candidates, this ERC shows that no constraint ranking is consistent with
the statements in ? (in fact, they are circular). Prince refers to the class of ERCs with
51
Computational Linguistics Volume 35, Number 1
L?s but no W?s as L+. He shows that these ERCs arise from fusion if and only if the
fused set contains incompatible ranking conditions.
Definition 6
An ERC set is consistent iff it has no subset that fuses to an ERC in L+ (Prince 2002,
page 11).
For any consistent ERC set there is a constraint ranking (often several) of which
all of its ERCs are true statements (Prince 2002, page 21). The ERCs in an inconsistent
set, on the other hand, can never all be true of a single ranking. Inconsistency can arise
from a single pair of candidates (e.g., ERCS(e) in Example 2 contains erc(e, a) = ?L, e, e?).
Inconsistency can also arise across multiple candidate comparisons (e.g., ERCS(d) in
Example 2 contains erc(d, a) = ?e, W, L? and erc(d, c) = ?e, L, W?). This latter type of in-
consistency, where several of the ERCs associated with a candidate fuse to L+, arises
from what Samek-Lodovici and Prince (1999) call collective harmonic bounding.
Finally, it is possible for inconsistencies to arise when ERCs for several candidates
with distinct inputs are combined. For example, if ERCS(x) = {?W, L, W?}, ERCS(y) =
{?L, W, W?}, and ERCS(z) = {?W,W, L?} then, even though x, y, and z may be candidates
for distinct inputs (i.e., come from different tableaux), the union of their ERCs fuses
to ?L, L, L? ? L+ and thereby reveals that there is no ranking under which all three
candidates are simultaneously optimal.
Inverting the W?s and L?s of an ERC produces its antithetical counterpart that is true
whenever the original ERC is false and vice versa. This opposition can be exploited in
describing the range of consistent ERC sets.
Definition 7
The negation of ? is ?where: ?i = W if ?i = L, ?i = L if ?i = W, and ?i = e if ?i = e.
Provided that ? is not all e?s, every ranking is described by either ? or ? but not both
(Prince 2002, page 42). In this way, ERC negation is just the standard notion of negation
in three-valued logics. The opposition between ? and ? makes a binary partition on
the space of rankings. This is intuitively obvious for simple statements like ?W, L, e? and
?L, W, e?. The opposition is a bit less intuitive for more complex conditions like ?W, L, L?
and ?L, W, W?, but the fact that erc(a, b) is the antithesis of erc(b, a) makes it abundantly
clear (i.e., if a and b are not tied, then every ranking must prefer one or the other). The
antithetical relationship between an ERC and its negation is reflected in the operation
of fusion by the fact that fusing antithetical ERCs will always yield an ERC in L+.
3. The VCD of Elementary Ranking Conditions
Before turning to the question of the VC dimension of the sample space in OT, it will be
helpful to define shatterability purely in terms of ERCs and thereby to establish a bound
on the VCD of sets of ERCs. We will say that an ERC ? is true of a given ranking R if
the condition imposed by ? is consistent with the linear ordering of the constraints
defined byR.
Definition 8
An ERC set ? over constraints CON is shatterable iff for every subset ? ? ?, there is a
rankingRCON of which all ERCs in ??? are true while all the ERCs in? are false.
52
Riggle Ranking Hypotheses in OT
From this definition of shatterability for sets of ERCs, it is immediately clear that only
nontrivial ERCs can occur in shatterable sets.
Lemma 1
Every ERC in a shatterable set must contain at least one L and one W.
Proof : The ERCs of L+ cannot occur in a shatterable set because there is no ranking
of which they are true. Conversely, ERCs with no L?s cannot occur in shatterable sets
because there is no ranking of which they are false. 
With Definition 8 in hand, and having excluded the trivial ERCs from the picture,
it will be possible to reduce shatterability for ERC sets to consistency under negation.
First, a definition of negation for sets of ERCs.
Definition 9
A partial negation of ERC set ? is obtained by negating every ERC in a subset? ? ?.
For example: ? = {?W, L, L?, ?e, W, L?} has four partial negations: one per subset.
{
? = ?W, L, L?
? = ?e, W, L?
} {
? = ?L, W, W?
? = ?e, W, L?
} {
? = ?W, L, L?
? = ?e, L, W?
} {
? = ?L, W, W?
? = ?e, L, W?
}
Theorem 1
An ERC set ? is shatterable iff every partial negation of ? is consistent.
Proof : Suppose every partial negation of ? is consistent. Thus, for any partial negation
in which ? is the negated subset of ? and ? is the rest of ?, it is the case that there
is a ranking R of which all the ERCs of ?+ ? are true. Because a nontrivial ERC and
its negation are never both true of the same ranking and trivial ERCs cannot occur in
shatterable sets, the ERCs in ?? ? are false of ranking R while the ERCs of ? are true.
Because ? was arbitrary, it is the case that for every subset of ?, there is a ranking of
which the ERCs in that subset are false while the rest are true, and thus consistency
under partial negation is sufficient for shatterability. If, on the other hand, there is a
partial negation that is not consistent, then there is a subset of ? such that if the ERCs
in that subset are negated, the resulting?+ ? is not consistent. However, because there
is no ranking of which the members of an inconsistent ERC set are all true, ? is not
shatterable because there is no ranking of which the ERCs in ? are truewhile the ERCs in
?? ? are false. Thus, consistency under partial negation is both necessary and sufficient
for shatterability. 
Corollary 1
Every subset of a shatterable ERC set is itself shatterable.
Proof : Because each partial negation of a shatterable ERC set must, by definition, be
consistent and because every subset of a consistent set must also be consistent, it is the
case that every subset of a shatterable set is consistent under every partial negation and
is thus shatterable. 
Defining shatterability in terms of partial negation lines up with the commonsense
observation that no set containing ? and ?where ? ? ? is shatterable because there can
be no ranking of which the former is true while the latter is false. This is neatly captured
by the fact that if ? ? ?, no superset of {?,?} can be shattered because fusing {?,?}
is guaranteed to yield an ERC in L+. The requirement of consistency under partial
negation also shows why relatively weak conditions like ?W,W, L? and ?W, L, W? cannot
co-occur in shatterable sets even though neither entails the other. In this case, fusing the
53
Computational Linguistics Volume 35, Number 1
negation of both ERCs yields ?L, L, L? ? L+. This follows transparently from the fact that
either the statement ?C1 or C2 outranks C3? or the statement ?C1 or C3 outranks C2? is
true of any ranking of three constraints.
The definition of shatterability for ERC sets in terms of consistency under partial
negation makes it easy to demonstrate that for |CON| = k, there are shatterable ERC
sets of size k ? 1. Diagonal ERC sets provide a particularly simple example of a class of
shatterable ERC sets of this size.
Definition 10
ERC set ? is diagonal if its members can be given
as a list L? in which each nth ERC in the list has a
W in its nth coordinate, an L in its n + 1th coordinate,
and e?s everywhere else.
E.g., ? =
?
?
?
?
?
?
?
?W, L, e, e, e ?
? e, W, L, e, e ?
? e, e, W, L, e ?
? e, e, e, W, L ?
?
?
?
?
?
?
?
Lemma 2
Diagonal ERC sets are shatterable.
Proof : Assume that ? is a diagonal ERC set and ?? is an arbitrary subset of an arbitrary
partial negation of?. If n is the number of ERCs in?? then, by the definition of diagonal
ERC sets, there must be at least n + 1 coordinates (columns) in ?? that are filled with
L or W for some ERC in ?? (i.e., are not all-e columns). Because each of the n ERCs has
only one L, at most n columns contain L?s, thus the fusion of ?? contains at least one W.
Because ?? was an arbitrary subset, no subset fuses to L+. Because the partial negation
was arbitrary, every partial negation is consistent and thus ? is shatterable. 
From the shatterability of diagonal ERC sets (with k ? 1 members if |CON| = k), we
obtain a lower bound of k ? 1 on the VCD of ERC sets. Having established that there
are shatterable sets of k-length ERCs with k ? 1 members, what remains to be shown is
that no set larger than k ? 1 is shatterable.
Definition 11
Coordinate Ci is W-unique in ERC set ? if ? has a partial negation ?
? such that in the
fusion of ??, ? = ??1, ...,?k?, the only coordinate that contains a W is ?i.
Definition 12
The minor ??,j of an ERC set ? is a new set ?
? in which ERC ? has been removed and
the jth coordinate has been removed from the remaining ERCs.
For example, if ? =
?
?
?
?
?
?
?
? : ? L, L, W, e, W ?
? : ? e, e, e, L, W ?
? : ? e, e, W, L, e ?
? : ? L, W, e, e, e ?
?
?
?
?
?
?
?
then ??, 3 =
?
?
?
? : ? L, L, e, W ?
? : ? e, e, L, W ?
? : ? L, W, e, e ?
?
?
?
As illustrated in Definition 12, the term ?minor? used here is analogous to the
standard notion of the minor of a matrix. It is straightforward to show that every
shatterable ERC set contains shatterable minors that can be obtained by removing one
constraint?s coordinate (column) and one ERC (row).
Lemma 3
Reduction Lemma. ? If ? is a shatterable ERC set, then it has a shatterable minor ??,j.
54
Riggle Ranking Hypotheses in OT
Proof : By Corollary 1, for any ? ? ?, ?? {?} is shatterable. In ?? {?} there must
be at least one coordinate Cj that is not W-unique. If this were not the case and every
coordinate in ?? {?} was W-unique, then one of the L?s in ? would occlude the only
W in a partial negation of ?, making it inconsistent contra the assumption that ? is
shatterable (? must have at least one L by Lemma 1). Because Cj is not W-unique, for
every partial negation of every subset of ?? {?}, there is a coordinate other than Cj
that fuses to W. This being the case, shatterability is preserved if Cj is eliminated. Thus,
the minor ??,j is shatterable as required. 
Theorem 2
For k > 1, the largest shatterable set of k-length ERCs has k ? 1 members.
Proof : If x is the size of the largest shatterable set of k-length ERCs and y is the size of
the largest shatterable set of (k + 1)-length ERCs, then y is not greater than x + 1. This
must be so because if y ? x + 2 then x could not be the size of the largest shatterable
set of k-length ERCs because a set of (k + 1)-length ERCs would have a shatterable
minor larger than x. Because ?W, L? and ?L, W? are the only nontrivial ERCs for k = 2
and because they are antithetical and thus cannot co-occur in a shatterable set, the
largest shatterable ERC set at k = 2 consists of a single ERC. This base case establishes
an upper bound of k ? 1 on the size of shatterable ERC sets and the diagonal ERC sets
provide a lower bound of k ? 1. Together these bounds place the cardinality of the
largest shatterable set at exactly k ? 1. 
Along with the diagonal ERC sets, there are many shatterable ERC sets with k ? 1
members, but no shatterable sets with more than k ? 1 members. What remains now is
to connect this result for ERC sets back to the realm of candidates.
4. The VCD of Optimality Theory
The question posed at the outset of this article was: for a constraint set CON with k
constraints thatmap candidates to natural numbers, what is the cardinality of the largest
set of candidates S such that, for each subset T ? S, there is at least one ranking RCON
under which every t in T is optimal, but no s in S ? T is optimal? Clearly, the answer
to this question depends greatly on details of the constraints in CON. However, if we
reduce candidates to the ERC sets associated with them, it is possible to place an upper
bound on the size of S without knowing anything about CON other than its size k.
Recall that a candidate c is mapped to True by ranking RCON just in case every ERC
in ERCS(c) is consistent withR. Conversely, c is mapped to False byR if any of the ERCs
in ERCS(c) is not consistent with R. This notion can be extended to sets of candidates
as follows. If S is a set of candidates, then ERCS(S) is the union of ERCS(s) for all s ? S.
A sample S is accepted by rankingR just in case every? in ERCS(S) is consistent withR.
Conversely, S is rejected byR if any ? in ERCS(S) is not consistent withR. Furthermore,
if ERCS(S) is consistent, then there must be at least one ranking that accepts S. In this
case, we will refer to S as a consistent sample. The concepts of partial negation and
W-uniqueness also have analogs for candidate sets.
Definition 13
For a consistent sample S, a partial exclusion is a partial negation of ERCS(S) that
rejects some F ? S by rendering ercs( f ) inconsistent for each f ? F while preserving the
consistency of ercs(s) for every s ? (S ? F).
55
Computational Linguistics Volume 35, Number 1
Definition 14
Ci isw-unique in S if there is a partitioning of S into T and F under which Ci is the only
coordinate that fuses to W in ERCS(T) for every partial exclusion that rejects F.
The property of W-uniqueness in samples crucially contrasts with what one might call
being semi-unique?the case where for at least one, but not all, of the partial exclusions
that reject F, Ci is the only column that fuses to W in ERCS(T).
Definition 15
Given a constraint set CON and a sample S, the minor Sx,j is obtained by removing
candidate x from S and removing constraint Cj from CON.
By extending partial negation, W-uniqueness, and the concept of minors to the realm of
samples, it is straightforward to show that shatterable samples have shatterable minors.
Lemma 4
Shatterable samples have shatterable minors.
Proof : Assume that S is a shatterable sample. Because removing candidate x from S has
no effect on whether the remainder of S can be shattered, S ? {x} is also shatterable.
Sample S ? {x} must have at least one coordinate that is not W-unique. If this were
not the case, then, because ERCS(x) must contain at least one coordinate with an L
(else there would be no way to reject {x}), the presence of x in S would place an L in a
W-unique coordinate in S ? {x}. However, this would make it impossible to associate a
ranking with at least one partitioning of S into accepted and rejected subsets contra the
assumption that S is shatterable. If Cj is a coordinate in S ? {x} that is not W-unique,
then, for every partial exclusion that rejects F, under each partitioning of S ? {x} into
T and F, there is at least one other coordinate Ci that fuses to W. Thus, Cj could be
removed from CON while preserving the shatterability of S ? {x}. Therefore, Sx,j is a
shatterable minor as required. 
The crucial piece of the proof in Section 3 that the VCD of ERC sets is k ? 1 was
the illustration of a one-to-one relationship between k and the bound on shatterable sets
by showing that removing an ERC from a shatterable set makes it possible to remove
a coordinate from the remaining ERCs while preserving shatterability. If shatterable
ERC sets could be larger than k ? 1, then it would have been necessary to remove sev-
eral ERCs before it was possible to safely remove a coordinate from the remaining
ERCs. Because ERC sets and candidate samples both have shatterable minors, a similar
strategy will show that shatterable samples must also grow at a one-to-one rate with k.
Theorem 3
If |CON| = k, then the size of shatterable sample sets is bounded at k ? 1.
Proof : If k = 2, a sample consisting of a single candidate can be shattered if ERCS(s) is
{?W, L?} or {?L, W?}, but no larger sample can be shattered. If there were such a sample,
it would contain at least two candidates a and b and there would be a ranking under
which both candidates were optimal, a ranking under which neither candidate was
optimal, a ranking that made a but not b optimal, and another ranking that made b
but not a optimal. This state of affairs requires at least four distinct rankings, which is
impossible with only two constraints. Thus, it is established that, at k = 2, the largest
shatterable sample set has at most one candidate.
56
Riggle Ranking Hypotheses in OT
If S is the largest shatterable sample for k constraints, then, at k + 1, the size of
the largest shatterable sample is |S|+ 1. If this were not the case, there would be a
shatterable sample X such that |X| ? |S|+ 2 for k + 1 constraints. However, because
shatterable samples have shatterable minors (Lemma 4), this would mean that there
was a shatterable sample of size |S|+ 1 for k constraints, contrary to the assumption
that S was largest. Given the base case that |S| = 1 when k = 2, the cardinality of
shatterable samples is thus bounded at k ? 1 as required. 
The bound of k ? 1 defines the limiting case that is obtained when there can be can-
didates in the sample space for any ERC set. In actual practice, the specific details of
the constraints in CON and the range of ways that they interact will determine which
elements of the powerset of the set of k-length ERCs are associated with candidates in
the sample space. This means that the VC dimension of a specific constraint set CON
can be much lower than |CON| ? 1. Nonetheless, the result that the VCD of OT can be
at most |CON| ? 1 is propitious for the learnability of Optimality Theoretic grammars.
5. Conclusions
Bounding the VC dimension of OT according to the number of constraints in CON es-
tablishes a general property of the sets of ranking hypotheses that can be associated
with sets of candidates. This bound is independent of any assumptions about how the
ERC sets for candidates are computed, independent of any assumptions about how
optimizations are computed, and independent of any assumptions about the formal
properties of constraints other than that they map candidates to N.
The linear growth of the VCD with |CON| = k provides a very general positive
learnability result for OT. Blumer et al (1989), building on the learning model of Valiant
(1984), define a concept class C as uniformly learnable if there is a learning algorithm
A such that, for any error threshold 
 and confidence level ?, if A is given m training
samples randomly drawn according to a probability distribution ? over the sample
space, then A has at least probability ? of generating a hypothesis whose likelihood
of misclassifying any point in the sample space drawn randomly according to ? is less
than 
. Blumer et al link the VC dimension to learnability by showing that concept
classes are uniformly learnable if and only if they have a finite VCD. Moreover, they
show that upper bounds on m can be established for learning that depend only on the
VC dimension of the concept class to be learned. The bound on m according to d =VCD
from Blumer et al is given in Equation (1).
m ?
?
4


(
d ln 12
 + ln
2
?
)?
(1)
This is a worst-case bound that holds for the most adversarial probability distributions
over the sample space and the worst consistent learning algorithms (i.e., algorithms
that are consistent in that they correctly classify all data in the training set, but worst-
case in that they err maximally on all unobserved data). Specific OT learning algorithms
that have tighter bounds and non-worst-case probability distributions over samples will
certainly present a different picture.
For a concrete example of OT learning, consider a version of Prince and Smolensky?s
(1993) basic CV syllable theory in which candidates are mappings from {C, V}* to {C,
V, .}*, and for each input i ? {C, V}*, the candidate set produced by GEN(i) represents
57
Computational Linguistics Volume 35, Number 1
all ways of modifying i through deletion and insertion of ., C, and V.3 If CON contains (i)
a constraint against deletion, (ii) a constraint against V insertion, (iii) a constraint against
C insertion, (iv) a constraint against syllables with codas, and (v) a constraint against
syllables without onsets, then the range of possible rankings of these five constraints
allows for 120 different grammars which in turn define twelve different languages
(i.e., twelve subsets of the sample space X = {C, V}*? {C, V, .}*).
If learners are trained with positive evidence in the form of optimal input ? output
mappings, then the probability distribution over the sample space can be characterized
in terms of the probability distribution over the input strings in {C, V}*. Each optimal
candidate a = (i ? o) provides information about the teacher?s ranking in the form of
ERCS(a) = {erc(a, b)|b = (i ? x) ? GEN(i)}. Riggle (2004) shows that, because the func-
tions in this system are all rational (i.e., finite state representable), the set ERCS(a) can
be derived via an algorithm called CONTENDERS. In this system, ERCS(a) can contain
from zero to twelve ERCs. The zero-ERC cases arise for input strings that share the
same optimal output under all rankings (i.e., /CV/?[.CV.]). The sets top out at twelve
because there are never more than twelve contenders (i.e., non-harmonically-bounded
candidates) for any given input string. The twelve ERC bound is a consequence of the
fact the 120 rankings only realize twelve distinct languages.4
As noted in Section 1, candidates with the same input cannot co-occur in shatterable
sets. Because of this, the bound on shatterable samples established in Section 4 carries
transparently over to the more general case where learners are trained with optimal
(i, o) mappings and then tested with novel inputs. Because the set of contenders is
determined solely by GEN and CON (which the learner is presumed to have access to)
if the learner can compute CONTENDERS(i), then testing on novel inputs reduces to
having the learner select one optimal candidate from the set of contenders, which
in turn reduces to binary questions of harmonic inequality between pairs a and b in
CONTENDERS(i), which in turn reduces to the question of which of erc(a, b) or erc(b, a) is
consistent with the ERCs gleaned from previous observations.
This is merely one sketch of how the learning problem in OT can be formulated
so that the VC dimension can predict its success. There are undoubtedly other possible
formulations. Furthermore, as noted, real-world cases will often contain details that are
more relevant than the VC dimension in predicting learnability. For instance, in syl-
lable structure grammar just described, there are inputs for which the CONTENDERS
algorithm generates one candidate per language in the factorial typology. In such a
case, the ERC set for a single optimal candidate can serve as a ?global trigger? that is
sufficient to uniquely identify the teacher?s language. Further analysis with specific
constraints and OT learning algorithms like Recursive Constraint Demotion (Tesar 1995,
1997, 1998; Tesar and Smolensky 1993, 2000), the Gradual Learning Algorithm (Boersma
1997, 1998; Boersma and Hayes 2001), and the ERC-Union learner (Riggle 2004) will
surely yield further insights and a less abstract picture of learning in Optimality Theory.
The VCD is an extremely robust metric that characterizes hardness inmany learning
frameworks (Haussler, Kearns, and Schapire 1992) and is applicable without any as-
sumptions other than that the learner is consistent. Any learner that bases its hypotheses
on the union of the ERCs associated with the data on which it is trained is guaranteed to
be consistent, and thus an extremely simple ERC-union learner can learn OT grammars
3 C and V represent consonants and vowels respectively and ?.? represents a syllable boundary marker.
4 Riggle (2004) extends Prince and Smolensky?s nine-way factorial typology to twelve with a slightly looser
GEN. In this case, because log2 12 = 4, the ERC-based VCD bound is the same as that obtained by the
finitude of the typology. Usually, however, we do not have the luxury of knowing the size of C.
58
Riggle Ranking Hypotheses in OT
from random training texts whose size m is linear in k. This linear bound on the re-
lationship between k and sample complexity is a nice tightening of the k log2 k bound
that follows from the finitude of k! and contrasts starkly with pessimistic assessments
of learnability suggested by the factorial relationship between k and the number of
possible grammars.
References
Anderson, Alan R. and Nuel D. Belnap, Jr.
1975. Entailment - The Logic of Relevance and
Necessity. Princeton University Press.
Blumer, Anselm, Andrzej Ehrenfeucht,
David Haussler, and Manfred K.
Warmuth. 1989. Learnability and the
Vapnik-Chervonenkis dimension. Journal
of the ACM, 36(4):929?965.
Boersma, Paul. 1997. How we learn variation,
optionality, and probability. Proceedings of
the Institute of Phonetic Sciences, 21:43?58.
Boersma, Paul. 1998. Functional Phonology:
Formalizing the Interactions between
Articulatory and Perceptual Drives. Ph.D.
thesis, The Hague.
Boersma, Paul and Bruce Hayes. 2001.
Empirical tests of the gradual learning
algorithm. Linguistic Inquiry, 32:45?86.
Dijkstra, Edsger. W. 1959. A note on
two problems in connexion with graphs.
Numerische Mathematik, 1:269?271.
Ellison, T. Mark. 1994. Phonological
derivation in optimality theory. In
Proceedings of the Fifteenth Conference on
Computational Linguistics, pages 1007?1013,
Kyoto, Japan. doi:dx.doi.org/10.3115/
991250.991312.
Haussler, David, Michael Kearns, and Robert
Schapire. 1992. Bounds on the sample
complexity of Bayesian learning using
information theory and the VC dimension.
Technical Report UCSC-CRL-91-44.
Littlestone, Nick. 1988. Learning quickly
when irrelevant attributes abound:
A new linear-threshold algorithm. Machine
Learning, 2(4):285?318.
Prince, Alan. 2002. Entailed ranking arguments.
ROA 500. Available at http://roa.rutgers.edu.
Prince, Alan and Paul Smolensky. 1993.
Optimality Theory: Constraint Interaction in
Generative Grammar. Blackwell, Malden,
MA.
Riggle, Jason. 2004. Generation, Recognition,
and Learning in Finite State Optimality
Theory. Ph.D. thesis, University of
California, Los Angeles.
Samek-Lodovici, Vieri and Alan Prince. 1999.
Optima. ROA 785. Available at
http://roa.rutgers.edu.
Tesar, Bruce. 1995. Computational Optimality
Theory. Ph.D. thesis, University of
Colorado.
Tesar, Bruce. 1997. Multi-recursive constraint
demotion. ROA 197. Available at
http://roa.rutgers.edu.
Tesar, Bruce. 1998. Error-driven learning in
Optimality Theory via the efficient
computation of optimal forms. In Is the
Best Good Enough? Optimality and
Competition in Syntax, ed. Pilar Barbosa,
Danny Fox, Paul Hagstran, Martha J.
McGinnis, and David Pesetsky. MIT Press,
Cambridge, MA.
Tesar, Bruce and Paul Smolensky. 1993.
The learnability of optimality theory:
An algorithm and some basic complexity
results. Unpublished manuscript.
Department of Computer Science
& Institute of Cognitive Science,
University of Colorado at Boulder.
Tesar, Bruce and Paul Smolensky. 2000.
Learnability in Optimality Theory. MIT Press,
Cambridge, MA.
Valiant, Leslie G. 1984. A theory of the
learnable. Communications of the ACM,
27(11):1134?1142.
Vapnik, V. N. and A. Chervonenkis. 1971.
On the uniform convergence of relative
frequencies of events to their probabilities.
Theory of Probability and its Applicaions,
16:264?280.
59

Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, page 28,
Columbus, Ohio, USA June 2008. c?2008 Association for Computational Linguistics
Invited Talk:
Counting Rankings
Jason Riggle
University of Chicago
jriggle@uchicago.edu
Abstract
In this talk, I present a recursive algorithm to calculate the number of rankings that are consistent with a
set of data (optimal candidates) in the framework of Optimality Theory (OT; Prince and Smolensky 1993).1
Computing this quantity, which I call r-volume, makes possible a simple and effective Bayesian heuristic in
learning ? all else equal, choose candidates that are preferred by the highest number of rankings consistent
with previous observations. This heuristic yields an r-volume learning algorithm (RVL) that is guaranteed
to make fewer than k lg k errors while learning rankings of k constraints. This log-linear error bound is
an improvement over the quadratic bound of Recursive Constraint Demotion (RCD; Tesar and Smolensky
1996) and it is within a logarithmic factor of the best possible mistake bound for any OT learning algorithm.
Computing r-volume: The violations in an OT tableau can be given as a [n ? k] array of integers in
which the first row t
1
corresponds to the winner. Following Prince (2002), the ranking information can be
extracted by comparing t
1
with each ?losing? row t
2
, ..., tn to create an Elementary Ranking Condition as
follows: erc(t
1
, ti) = ??1, ..., ?k? where ?j = L if t1,j < ti,j , ?j = W if t1,j > ti,j , and ?j = e otherwise.
The meaning of ? is that at least one constraint associated with W dominates all those associated with L.
input C
1
C
2
C
3
candidate t
1
* ** winner
candidate t
2
** * erc(t
1
, t
2
) = ?W, L, e ? i.e. t
1
beats t
2
if C
1
outranks C
2
candidate t
3
** erc(t
1
, t
3
) = ?L, L, W? i.e. t
1
beats t
3
if C
3
outranks C
1
and C
2
candidate t
4
*** * erc(t
1
, t
4
) = ?L, W, W? i.e. t
1
beats t
4
if C
2
or C
3
outranks C
1
For a set E of length-k ERCs, E?wi denotes
a set E? derived from E by removing ERCs
with W in column i and removing column i.
r-vol
(
Ek
)
=
?
1?i?k
?
?
?
0 if xi = L for any x ? E
(k ? 1)! if xi = W for all x ? E
r (E ? wi) otherwise
Mistake bounds: To make predictions, RVL selects in each tableau the candidate that yields the highest
r-volume when the ERCs that allow it to win are combined with E (the ERCs for past winners). To establish
a mistake bound, assume that the RVL chooses candidate e when, in fact, candidate o was optimal according
to the target ranking RT . Assuming e 6= o, the rankings that make o optimal must be half or fewer of the
rankings consistent with E or else RVL would have chosen o. Because all rankings that make candidates
other than o optimal will be eliminated once the ERCs for o are added to E, each error reduces the number
of rankings consistent with all observed data by at least half and thus there can be no more than lg k! errors.
Applications: The r-volume seems to encode ?restrictiveness? in a way similar to Tesar and Prince?s
(1999) r-measure. As a factor in learning, it predicts typological frequency (cf. Bane and Riggle 2008) and
priors other than the ?flat? distribution over rankings can easily be included to test models of ranking bias.
More generally, this research suggests the concept of g-volume for any parameterized model of grammar.
1Full bibliography available on the Rutgers Optimality Archive (roa.rutgers.edu) with the paper Counting Rankings.
28
Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 29?38,
Columbus, Ohio, USA June 2008. c?2008 Association for Computational Linguistics
Three Correlates of the Typological Frequency of Quantity-Insensitive
Stress Systems
Max Bane and Jason Riggle
Department of Linguistics
University of Chicago
Chicago, IL 60637, USA
bane@uchicago.edu, jriggle@uchicago.edu
Abstract
We examine the typology of quantity-
insensitive (QI) stress systems and ask to what
extent an existing optimality theoretic model
of QI stress can predict the observed typolog-
ical frequencies of stress patterns. We find
three significant correlates of pattern attesta-
tion and frequency: the trigram entropy of a
pattern, the degree to which it is ?confusable?
with other patterns predicted by the model,
and the number of constraint rankings that
specify the pattern.
1 Introduction
A remarkable characteristic of human language is
that the typological distribution of many linguis-
tic properties is extremely uneven. For example,
Maddieson?s (1984) survey of phonemic inventories
finds that a total of 921 distinct sounds are used by
a sample of 451 languages, yet the average language
employs only about 30 of those. Furthermore, some
sounds are so commonly attested as to be almost uni-
versal (e.g., /m/, /k/), while others are vanishingly
rare (/K/, /?/). Heinz (2007) combines two pre-
vious typologies of accentual stress (Bailey, 1995;
Gordon, 2002), and finds that among a sample of
306 languages with quantity-insensitive (QI) stress
systems, 26 distinct stress patterns are found,1 while
over 60% of the languages surveyed use one of just 3
of these patterns. If we begin to look at morphosyn-
tactic or semantic properties, the combinatorics of
1These figures include only those quantity-insensitive stress
patterns according to which there is exactly one possible assign-
ment of stress per word length in syllables.
possible systems veritably explodes, leaving each at-
tested language with an even smaller slice of the log-
ical possibilities.
Most typological studies have attempted to give
accounts of linguistic phenomena that simultane-
ously:
? predict as many attested languages or patterns
as possible, and
? predict as few unattested languages or patterns
as possible.
We will refer to this goal as the ?inclusion-
exclusion? criterion of a linguistic model. Com-
paratively few attempts have been made to explain
or predict the relative frequencies with which lan-
guages or patterns are observed to occur in cross-
linguistic samples (though see Liljencrants and
Lindblom 1972, de Boer 2000, Moreton to appear,
and others for work proceeding in this direction).
This paper examines the typology of QI stress
systems, as reported by Heinz (2007), and asks to
what extent an existing optimality theoretic (Prince
and Smolensky, 1993) model of QI stress, developed
by Gordon (2002) to meet the inclusion-exclusion
criterion, can predict the observed typological fre-
quencies of stress patterns. Gordon?s model pre-
dicts a total of 152 possible stress patterns, which,
as far as we are aware, represent the current best at-
tempt at satisfying the inclusion-exclusion criterion
for QI stress, failing to generate only two attested
stress patterns (unknown to Gordon at the time), and
generating 128 unattested patterns. We show that
Gordon?s model can offer at least three novel, sta-
tistically significant predictors of which of the 152
generated patterns are actually attested, and of the
29
cross-linguistic frequencies of the attested patterns.
Namely:
i. Of the 152 stress patterns predicted by Gor-
don?s model, the attested and frequent ones ex-
hibit significantly lower trigram entropy than
the unattested and infrequent,
ii. the length of forms, in syllables, that must be
observed to uniquely identify a stress pattern is
significantly lower for the attested patterns than
for the unattested, and
iii. the number of constraint rankings in Gordon?s
model that are consistent with a stress pattern
is a significant predictor both of which patterns
are attested and of the relative frequencies of
the attested patterns.
In what follows, Section 2 presents an overview of
the basic theoretical background and empirical facts
of quantity-insensitive stress that guide this study,
including a review of Heinz?s (2007) typology and a
description of Gordon?s (2002) OT model. Section 3
then introduces the three proposed correlates of at-
testedness and frequency that can be applied to Gor-
don?s framework, together with statistical analyses
of their significance as predictors. Finally, Section 4
offers a discussion of the interpretation of these find-
ings, as well as some concluding remarks.
2 Quantity-Insensitive Stress Patterns
2.1 Assumptions and Definitions
We will follow Gordon (2002) and Heinz (2007) in
taking a stress system to be any accentual system
that satisfies ?culminativity? in the sense of Prince
(1983); that is, any accentual system in which there
is always one most prominent accentual unit per ac-
centual domain. In this case, we assume that the
accentual unit is the syllable, and that the domain
is the prosodic word. Thus, any given syllable of a
word may bear primary, secondary, or no stress (we
ignore the possibility of tertiary or other stress), but
there must always be exactly one primary stressed
syllable per word.
We further restrict our attention in this study to
quantity-insensitive (QI) stress systems, which are
those stress systems according to which the assign-
ment of stresses to a word?s syllables depends only
n Albanian Malakmalak
2 ??? ???
3 ???? ????
4 ????? ????`?
5 ?????? ?????`?
6 ??????? ????`??`?
Table 1: The stress assignments of n-syllable words for
2 ? n ? 6 in the QI stress patterns of Albanian and
Malakmalak.
on the number of syllables present (a quantity as-
sumed to be fixed when stress is assigned), and not
on the segmental contents of the syllables. We will
refer to ?stress systems? and ?stress patterns? inter-
changeably.
As two concrete examples of QI stress systems,
consider those of Albanian (Chafe, 1977; also
shared by many other languages) and Malakmalak
(an Australian language; Birk, 1976). These pat-
terns are illustrated in Table 1 for words of length
two through six syllables.2 The former is a simple
fixed system in which primary stress is always lo-
cated on the penultimate syllable, while no other syl-
lable bears stress. The latter is rather more complex,
requiring stress on even numbered syllables from the
right, the leftmost being primary. Crucially, neither
system is sensitive to notions like syllabic weight,
nor to any other properties of the syllables? contents.
Formally, one can consider a QI stress pattern up
to length n (in syllables), Pn, to be a set of strings
over the alphabet ? = {?, ?`, ??}:
(1) Pn = {w2, . . . , wn},
where each wi encodes the locations of stress in a
word of i syllables, satisfying:
(2) |wi| = i, wi ? ??, and
wi contains ?? exactly once.
Thus for a given maximum number of syllables n,
there are
n?
i=2
i2(i?1) = n! ? 2
n(n?1)
2
2Here and throughout this paper, ? refers to an unstressed
syllable, ?` indicates a syllable bearing secondary stress, and ??
indicates primary stress.
30
L076 L118 L004 L132 L110 L044 L008 L022 L143 L065 L054 L095 L040 L077 L071 L033 L113 L037 LXX1 L047 L042 L089 LXX2 LXX3 L082 L0840.
00
0.05
0.10
0.15
0.20
0.25
Frequencies of Attested Stress Patterns
Stress Pattern
Freq
uenc
y
Figure 1: Frequency of attestation of each of the 26 distinct stress patterns. Error bars indicate standard Poisson
sampling error.
logically possible QI stress patterns. We will fol-
low Gordon (2002) by imposing a maximum word
length of 8 syllables for purposes of distinguishing
one stress pattern from another in the typology, and
of determining the set of distinct patterns predicted
by the model. We are therefore dealing with a uni-
verse of 8!228 = 10,823,317,585,920 theoretically
possible stress systems.
2.2 The Typology
The typological data on which this study is based
are due to Heinz (2007), who has made them freely
available.3 This database is a combination of
? that from Bailey (1995), itself gathered from
Halle and Vergnaud (1987) and Hayes (1995),
and
? the collection put together by Gordon (2002)
from previous surveys by Hyman (1977) and
Hayes (1980), as well as from additional source
grammars.
The combined database is intended to be fairly ex-
haustive, sampling a total of 422 genetically and ge-
ographically diverse languages with stress systems.
Of those 422 languages, 318 are identified as pos-
sessing quantity-insensitive stress, and we further
confine our attention to the 306 of those with sys-
tems that uniquely determine the stress of each word
as a function of syllable-count (i.e., with no option-
ality). We should note that it is possible for one lan-
3The typology is available as a MySQL database at
http://www.ling.udel.edu/heinz/diss/
l
ll
lll
l
ll
l
ll l
l
l
l
l
l
l
l
l
l
l
lll
0 5 10 15 20 250
.00
0.05
0.10
0.15
0.20
0.25
Zipf Fit of Frequency?Rank vs Frequency of Attested Stress Patterns
Frequency Rank
Frequ
ency
Fitted zipf distribution95% Confidence interval of fit
Figure 2: Regressed Zipf distribution of stress pattern fre-
quencies; Zipf?s exponent is found to be 1.05 ? 0.15 at
95% confidence.
guage to contribute more than one distinct stress pat-
tern to our dataset, as in the case of Lenakel (Lynch,
1974), for instance, which employs one regular pat-
tern for nouns and another for verbs and adjectives.
Between these 306 languages, we find a total
of 26 distinct QI stress systems, which is quite a
bit fewer than expected by chance, given the sam-
ple size and the 10.8 trillion a priori possible sys-
tems. Figure 1 shows the frequency with which
each pattern is attested, arranged in decreasing order
of frequency. The distribution of patterns is essen-
tially Zipfian; a nonlinear regression of the frequen-
cies against Zipf?s law (using the Gauss-Newton
method) achieves strong statistical significance (p <
0.001) and can account for 80.9% of the variance in
31
Constraint(s) Penalizes. . .
ALIGNEDGE each edge of the word with no stress.
ALIGN({?`, ??}, L/R) each (primary or secondary) stressed syllable for each other (stressed or un-
stressed) syllable between it and the left/right edge.
ALIGN(??, L/R) each primary stressed syllable for each secondary stressed syllable between it and
the left/right edge.
NONFINALITY the last syllable if it is stressed.
*LAPSE each adjacent pair of unstressed syllables.
*CLASH each adjacent pair of stressed syllables.
*EXTLAPSE each occurrence of three consecutive unstressed syllables.
*LAPSELEFT/RIGHT the left/right-most syllable if more than one unstressed syllable separates it from
the left/right edge.
*EXTLAPSERIGHT the right-most syllable if more than two unstressed syllables separate it from the
right edge.
Table 2: Gordon?s (2002) constraint set.
frequency (Figure 2).
The top three most common patterns, together ac-
counting for over 60% of the sampled languages, are
all simple fixed primary stress systems: fixed final
stress (24.2% of systems), fixed initial stress (22.5%
of systems), and fixed penultimate stress (19.6% of
systems). It is possible that fixed primary systems
may be somewhat overrepresented, as the descrip-
tive sources can be expected to occasionally fail to
report the presence of secondary stress; even so, the
preponderance of such systems would seem to be
substantial. The great majority of distinctly attested
systems are quite rare, the median frequency being
0.65% of sampled languages. Some examples of
cross-linguistically unlikely patterns include that of
Georgian, with antepenultimate primary stress and
initial secondary stress, and that of Ic?ua? Tupi, which
shows penultimate primary stress in words of four or
fewer syllables, but antepenultimate stress in longer
words.
There is some reason to believe that this sample is
fairly representative of the whole population of QI
stress patterns used by the world?s languages. While
it is true that the majority of sampled patterns are
rare, it is by no means the case that the majority
of sampled languages exhibit rare stress patterns.
In fact, of the N = 306 sampled languages, just
n1 = 13 of them present stress patterns that are
attested only once. Thus, according to the com-
monly used Good-Turing estimate (a distribution-
free method of estimating type frequencies in a pop-
ulation from a sample of tokens; Good, 1953), we
should expect to reserve approximately n1N = 4.3%
of total probability-mass (or frequency-mass) for un-
seen stress patterns. In other words, we would be
surprised to find that the actual population of lan-
guages contains much more than N
1?n1N
= 27.15 dis-
tinct patterns, i.e., about one more than found in this
sample.
2.3 Gordon?s (2002) Model
Gordon (2002) has developed an optimality theo-
retic model of QI stress with the goal of satisfying
the inclusion-exclusion criterion on an earlier subset
of Heinz?s (2007) typology. The model is footless,
consisting of twelve constraints stated in terms of a
metrical grid, without reference to feet or other met-
rical groupings (or, equivalently, simply in terms of
linear {?, ?`, ??}-sequences). The twelve constraints
are summarized in Table 2.
In addition to these, Gordon?s model imple-
ments a sort of ?meta-constraint? on rankings: he
assumes that one of the primary alignment con-
straints ALIGN(??, L/R) is always lowest ranked,
so that in any given tableau either ALIGN(??, L) or
ALIGN(??, R) is ?active,? but never both. Formally,
we take this to mean that the model specifies two
EVALS: an EVAL-L with ALIGN(??, R) excluded
from CON, and an EVAL-R with ALIGN(??, L) ex-
cluded. The set of stress systems predicted by the
whole model is then simply the union of the systems
predicted by EVAL-L and by EVAL-R. This ranking
32
restriction is meant to capture the probably univer-
sal generalization that primary stress always appears
either to the left or right of the secondary stresses
in a word, without vacillating from side to side for
different word lengths. Gordon also assumes that
candidate forms violating culminativity (i.e., forms
without exactly one primary stressed syllable), are
always excluded, either by some filter on the output
of GEN or by an always highly ranked CULMINATE
constraint against them.4
Gordon?s model is capable of representing 2 ?
11! = 79,833,600 QI stress grammars (11! rank-
ings of the constraints associated with EVAL-L plus
the 11! rankings for EVAL-R). We replicated Gor-
don?s (2002) calculation of the factorial typology of
distinct QI stress patterns that this grammar space
predicts by implementing the constraints as finite-
state transducers,5 composing the appropriate com-
binations of these to produce finite-state implemen-
tations of EVAL-L and EVAL-R, respectively (see
Riggle, 2004), and iteratively constructing consis-
tent subsets of the members of the cross-products of
candidate forms for each word length (two through
eight syllables). See Riggle et al(2007) and Prince
(2002) for the mathematical and algorithmic details.
The factorial typology of stress systems that is
yielded agrees with that reported by Gordon (2002).
The model predicts a total of 152 distinct possible
systems. All but two of the 26 systems attested
in Heinz?s (2007) database are among these. The
two patterns that Gordon?s model fails to generate
are those of Bhojpuri (as described by Tiwari, 1960;
Shukla, 1981), and Ic?ua? Tupi (Abrahamson, 1968).
Both of these patterns were unknown to Gordon at
the time he proposed his model, and each is attested
only once in the typology.
In addition to failing to generate two of the at-
tested stress systems, Gordon?s model also predicts
4We follow Gordon in remaining agnostic on this point, as
the same set of possible stress patterns results from either im-
plementation.
5The reader may notice that the ALIGN(??, L/R) and
ALIGN({?`, ??}, L/R) constraints (defined in Table 2) involve
a kind of counting that cannot generally be accomplished by
finite-state transducers. This is perhaps a theoretically unde-
sirable property of Gordon?s model (see Heinz et al(2005) for
such a critique), but in any case, this general problem does not
affect us here, as we ignore the possibility of words any longer
than eight syllables (following Gordon; see Section 2.1).
ll
l
ll ll
l
l
l
l
l
Attested Unattested0
.5
0.6
0.7
0.8
0.9
Trigram Entropy
Figure 3: Trigram entropy (average bits per symbol) of
attested versus unattested stress patterns; attested patterns
have significantly lower entropy.
128 patterns that are unattested. Gordon (2002) ar-
gues that a certain amount of overgeneration is to
be expected of any model, since the majority of
distinct attested systems are extremely rare; thus
failure to observe a pattern in a limited sample
is not strong evidence that the pattern is impossi-
ble. The Good-Turing estimate of unseen patterns
(Section 2.2 above), however, suggests that signifi-
cantly less overgeneration may still be desired. Gor-
don also argues that the overgenerated patterns are
not pathologically different from the sorts of pat-
terns that we do see (though Section 3 below de-
scribes several statistically detectable differences).
In any case, Gordon?s model of QI stress is among
the most explicitly formulated approaches currently
available, and offers a comparatively ?tight? fit to
the typological data.
3 Predicting Typological Frequency
3.1 k-gram Entropy
A frequently offered and examined hypothesis is
that, all else being equal, human communicative
systems adhere to some principle of least effort
(whether in terms of articulation or processing), pre-
ferring simple structures to complicated ones when
additional complexity would afford no concomitant
advantage in communicative efficiency or expres-
siveness. This line of reasoning suggests that typo-
logically frequent properties should tend to exhibit
33
(a) (b)
l l
ll
l
l l
l
l l
l
ll ll
lll
l
l
l
0.00 0.05 0.10 0.15 0.20 0.250.
5
0.6
0.7
0.8
0.9
Frequency vs Trigram Entropy
Typological Frequency of Pattern
Trig
ram
 Ent
ropy
 (bits
/sym
bol)
l
high low0.
5
0.6
0.7
0.8
0.9
Trigram Entropy
Figure 4: (a) typological frequency of attested stress patterns versus their trigram entropy, and (b) the trigram entropy
of high-frequency (above median) patterns versus low-frequency (below median) patterns.
greater simplicity (according to some metric) than
those that are rarer. One also expects, according to
this hypothesis, that among the set of patterns pre-
dicted by a linguistic model such as Gordon?s, the
simpler ones should have a greater chance of attes-
tation in typological samples. We find evidence con-
sistent with both of these expectations in the case of
QI stress systems, according to at least one informa-
tion theoretic definition of complexity.
In order to calculate measures of complexity for
each attested and predicted stress pattern, we con-
struct bigram and trigram models of the transi-
tion probabilities between syllable types (?, ?`, ??) in
forms of two through eight syllables for each pat-
tern. That is, if each stress is taken to be a set of
forms as in (1) (with n = 8 in this case), satisfying
(2), then across all forms (i.e., word-lengths) one can
count the number of occurrences of each k-length
sequence (k-gram) of ?, ?`, ?? and word boundaries
to arrive at conditional probabilities for each sylla-
ble type (or a word boundary) given the previous
k?1 syllables. With these probabilities one can then
compute the Shannon entropy of the stress pattern as
an index of its complexity; this is interpreted as the
number of bits needed to describe the pattern (i.e.,
list its forms) under an efficient encoding, given the
k-gram probability model. Stress patterns in which
it is difficult to accurately predict the value of a syl-
lable on the basis of the previous k?1 syllables will
possess greater entropy, and thus be deemed more
complex, than those in which such predictions can
be made with greater accuracy.
We find that in the case of a bigram probability
model (k = 2), the attested stress systems predicted
by Gordon?s model do not differ in entropy signifi-
cantly6 from those that are unattested; we also find
no significant correlation between bigram entropy
and the typological frequency of attested systems.
Under a trigram probability model (k = 3), how-
ever, entropy is a significant predictor of both
whether a system is attested, and if it is attested,
of its frequency in the sample. Figure 3 gives box-
plots comparing the distribution of trigram entropy
for those systems predicted by Gordon?s model (plus
the two unpredicted systems) that are attested ver-
sus those that are unattested. The attested QI stress
systems are significantly less entropic than the unat-
tested, according to a two-sided Mann-Whitney U -
test: U = 1196, p = 0.021 (if the two unpredicted
patterns are excluded, then U = 923.5, p < 0.01).
Among attested systems, trigram entropy appears
to bear a nonlinear relationship to typological fre-
6Throughout this study, we adopt a 95% confidence standard
of significance, i.e., p < 0.05.
34
quency (see Figure 4). A significant linear correla-
tion does not exist, and the 13 attested patterns with
greater than median frequency have only mildly sig-
nificantly lower entropy than the 13 with less than
median frequency (according to another two-sided
U -test: U = 51.5, p = 0.0856); if, however,
the single high-frequency pattern with outlying en-
tropy is excluded (the lone point indicated in Fig-
ure 4b), then the difference is more robustly signifi-
cant: U = 39.5, p = 0.0323. Interestingly, the en-
tropies of the above-median patterns are tightly con-
strained to a narrow band of values (variance 0.012
square bits/symbol), whereas the below-median pat-
terns show much greater variation in their complex-
ity (variance 0.028 square bits/symbol).
3.2 Confusability Vectors
The second metric we examine is motivated by con-
siderations of learnability. Some QI stress patterns
are very similar to each other in the sense that one
must observe fairly long forms (i.e., forms with
many syllables) in order to distinguish them from
each other. For instance, in the case of Albanian
and Malakmalak (Table 1 above), the two systems
give identical stress assignments for words of two or
three syllables; to tell them apart, one must com-
pare words with four or more syllables. The de-
gree of similarity, or ?confusability? in this sense,
between stress systems varies considerably for dif-
ferent pairs of languages. Assuming a tendency for
short words to be encountered more frequently by
language learners than long words, we might ex-
pect stress patterns that are easily identified at short
word-lengths to be more faithfully acquired than
those requiring longer observations for unambigu-
ous identification. In particular, if we take the 152
patterns predicted by Gordon?s model to constitute
the set of possible QI stress systems, then we hy-
pothesize that those patterns that stand out as unique
at shorter lengths should be more typologically ?sta-
ble?: more likely to be attested, more frequently at-
tested, or both.
To test this, we determine a confusability vector
for each predicted pattern. This is simply a tuple of
7 integers in which the value of the ith component
indicates how many of the other 151 predicted pat-
terns the given pattern agrees with on forms of two
through i+1 syllables. For example, the confusabil-
l
Attested Unattested3
4
5
6
7
8
Syllable?Count for Unique Identification
Figure 5: Attested stress patterns have significantly lower
pivots than unattested ones.
ity vector of Albanian?s (fixed penultimate primary;
see Table 1) stress pattern is:
?101, 39, 10, 0, 0, 0, 0?
This means that for words of two syllables, this
stress system agrees with 101 of the other predicted
systems, for words of two through three syllables it
agrees with 39, and for two through four syllables it
agrees with 10. Once words of five or more syllables
are included in the comparison, it is unique among
the stress patterns predicted, confusable with none.
A confusability vector allows us to calculate two
quantities for a given stress pattern: its confusabil-
ity sum, which is just the sum of all the components
of the vector, and a confusability pivot, which is the
number i such that the (i ? 1)th component7 of the
vector is the first component with value 0. Thus the
confusability sum of the fixed penultimate primary
stress system is 101+39+10 = 150, and its confus-
ability pivot is 5, indicating that it achieves unique-
ness among Gordon?s predicted systems at five syl-
lables.
We find that those of the predicted systems that
are typologically attested have very significantly
lower confusability pivots than the unattested sys-
tems (see Figure 5; Mann-Whitney U -test: U =
1005.5, p < 0.001). One might wonder whether
this is simply due to the fact that primary-only stress
7We count vector components beginning at 1.
35
systems are most likely to be attested, and that such
systems are independently expected to have lower
confusability pivots than those with secondary stress
(indeed, a two-sided Mann-Whitney test indicates
that the pivots of primary-only systems are signifi-
cantly lower: U = 214, p < 0.01). However, it
appears that confusability pivots are in fact indepen-
dently robust predictors of attestedness. When only
the predicted patterns with secondary stress are con-
sidered, the pivots of the attested ones remain signif-
icantly lower than those of the unattested, albeit by
a smaller margin (U = 846, p = 0.027). Confus-
ability sums, on the other hand, are not significant
predictors of attestedness in either case.
Neither pivots nor sums alone correlate well with
the typological frequency of attested systems, but to-
gether they can predict approximately 27% of the
variance in frequencies; a multilinear regression of
the form
f(x) = ?+ ?s(x) + ?p(x),
where f(x), s(x), and p(x) are the frequency, con-
fusability sum, and pivot of pattern x, respectively,
yields significant (p < 0.05) values for all coeffi-
cients (R2 = 0.271).
3.3 Ranking Volume
The two typological predictors discussed above (en-
tropy and confusability) are only weakly ?post-
theoretical? in the sense that, while they depend on
a set of predicted stress patterns according to some
linguistic theory or model (such as Gordon?s), they
can be computed without reference to the particular
form of the model. In contrast, the third and last cor-
relate that we consider is entirely specified and mo-
tivated by the optimality theoretic form of Gordon?s
model.
We define the ranking volume, or r-volume, of
a language generated by an optimality theoretic
model as the number of total constraint orderings
(i.e., grammars) that specify the language. Rig-
gle (2008) describes a method of applying the logic
of Prince?s (2002) elementary ranking conditions to
compute this quantity. Using this method, we find
that the number of rankings of Gordon?s constraints
that are consistent with a stress pattern predicted by
his model is a significant correlate of attestedness,
l
l
Attested Unattested
10
11
12
13
14
15
16
log(r?volume)
Figure 6: Of the predicted stress patterns, those that are
attested are consistent with significantly more constraint-
rankings. The natural logarithms of r-volume are shown
here for greater ease of comparison.
and if the pattern is attested, of its typological fre-
quency. In the case of Gordon?s model, with its
ranking meta-constraint and bifurcated EVAL (as de-
scribed in Section 2.3), the total r-volume of each
pattern is actually the sum of two quantities: the pat-
tern?s r-volume under the 11 constraints correspond-
ing to EVAL-L (which excludes ALIGN(??, R)), and
its r-volume under the 11 constraints of EVAL-R
(which conversely excludes ALIGN(??, R)). Most of
the predicted patterns are only generated by one of
the EVALS, but some can be specified by either con-
straint set, and thus will tend to be consistent with
more rankings. It just so happens that Gordon?s
choice of constraints ensures that these doubly gen-
erated patterns are of precisely the same sort that
are typologically most frequent: fixed primary stress
systems. This appears to account for much of the
predictive power of r-volume in this model.
The distribution of r-volume among the 152 pre-
dicted stress patterns is almost perfectly Zipfian.
A nonlinear Gauss-Newton regression of r-volumes
against Zipf?s law finds a highly significant fit (with
Zipf?s exponent = 0.976 ? 0.02, p < 0.001) that
accounts for 96.8% of the variance. The attested
patterns tend to have significantly greater r-volumes
than those unattested; two-sided Mann-Whitney?s
U = 2113.5, p < 0.01 (see Figure 6). On aver-
36
11 12 13 14 15 160.0
0
0.05
0.10
0.15
0.20
log(r?volume) vs Frequency
log(r?volume)
Patte
rn Fr
eque
ncy
l
l l
l
l l
l
ll l
l
l
l
l
l
l
l
l
ll l
Linear RegressionExponential Regression
Figure 7: Linear and exponential regressions of typologi-
cal frequency as a function of the natural logarithm of the
pattern?s r-volume.
age, the attested stress patterns are consistent with
1,586,437 rankings each, versus 299,118.1 rankings
for the unattested ones.
Furthermore, the frequency of attested patterns
has a strong linear correlation with r-volume: R2 =
0.7236, p < 0.001. However, a linear rela-
tion is probably not appropriate, as a normal Q-Q
plot of the residuals of the regression indicates an
upper-quartile deviation from linearity, and Cook?s
distance metric indicates that several data-points
exert disproportionate influence on the explained
variance. Instead, typological frequency seems to
be better modeled as a function of the logarithm
of the r-volume; Figure 7 illustrates both a lin-
ear (R2 = 0.39, p < 0.05) and exponential
(R2 = 0.704, p < 0.001) fit of frequencies to log-
transformed r-volumes.
4 Interpretation and Future Work
The correlates of attestation and frequency reported
here suggest novel ways that linguistic models might
be used to make testable predictions about typol-
ogy. Two of these correlates?k-gram entropy and
confusability?are particularly general, their calcu-
lation requiring only the set of possible languages
or patterns that a model can specify. It remains an
interesting question whether these same quantities
retain predictive power for other sorts of data and
models than are considered here, and whether such
correlations might fruitfully be incorporated into an
evaluation metric for linguistic models.
The r-volume result motivates a particular line of
further research on the nature of constraints in OT:
how exactly the contents of a constraint set deter-
mine the distribution of r-volumes in the factorial
typology. In addition, there are several other po-
tentially relevant concepts in the literature, includ-
ing Anttila?s (1997, 2002, 2007) ranking-counting
model of variation, Anttila and Andrus? (2006) ?T-
orders? and Prince and Tesar?s (1999) ?restrictive-
ness measure,? whose relations to r-volume merit
examination. Our results for r-volume in this case
also suggest that a fully generalized notion of para-
metric grammar volume may be worth investigating
across different kinds of models and various typo-
logical phenomena.
Insofar as the three correlates? strength as typo-
logical predictors depends on the set of stress pat-
terns generated by Gordon?s model, their signif-
icance is consistent with the hypothesis that the
model is useful and has some predictive power. Such
statistical significance is rather surprising, since
Gordon?s model was developed primarily as an at-
tempt to satisfy the inclusion-exclusion criterion,
without any explicit eye toward the kinds of pre-
dictions that these correlates seem to suggest it can
make. This is especially true of r-volume, as it is the
correlate most tightly coupled to the OT particulars
of Gordon?s model. These findings motivate further
research on the general relationship, if any, between
the inclusion-exclusion predictions of a model (opti-
mality theoretic or otherwise) and its frequency pre-
dictions according to the measures presented here.
On the other hand, the entropy and confusability re-
sults suggest the intriguing possibility of discarding
such a model altogether, and instead picking the at-
tested stress systems (and their frequencies) directly
from the large pool of logically possible ones, ac-
cording to these measures and others like them.
Acknowledgements
We owe many thanks to Jeff Heinz for the typologi-
cal data used in this study, and to Alan Yu, Morgan
Sonderegger, and the anonymous reviewers of SIG-
MORPHON 2008 for insightful commentary.
37
References
A. Abrahamson. 1968. Constrastive distribution of
phoneme classes in Ic?ua? Tupi. Anthropological Lin-
guistics, 10(6):11?21.
Arto Anttila and Curtis Andrus. 2006. T-Orders.
Manuscript, Stanford University.
Arto Anttila. 1997. Deriving variation from gram-
mar. In Frans Hinskens, Roeland van Hout, and Leo
Wetzels, editors, Variation, Change and Phonological
Theory, pages 35?68. John Benjamins Press, Amster-
dam/Philadelphia.
Arto Anttila. 2002. Variation and phonological the-
ory. In Jack Chambers, Peter Trudgill, and Na-
talie Schilling-Estes, editors, Handbook of Language
Variation and Change, pages 206?243. Blackwell,
Malden, Mass.
Arto Anttila. 2007. Variation and optionality. In Paul
de Lacy, editor, The Cambridge Handbook of Phonol-
ogy. Cambridge University Press, Cambridge.
Todd Bailey. 1995. Nonmetrical Constraints on Stress.
Ph.D. thesis, University of Minnesota.
D.B.W. Birk. 1976. The Malakmalak Language, Daly
River (Western Arnhem Land). Australian National
University, Canberra.
Bart de Boer. 2000. Self-organization in vowel systems.
Journal of Phonetics, 28:441?465.
I.J. Good. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3/4):237?264, December.
Matthew Gordon. 2002. A factorial typology of
quantity-insensitive stress. Natural Language and
Linguistic Theory, 20(3):491?552.
Morris Halle and Jean-Roger Vergnaud. 1987. An Essay
on Stress. MIT Press, Cambridge, MA.
Bruce Hayes. 1980. A Metrical Theory of Stress Rules.
Ph.D. thesis, MIT, Cambridge, MA.
Bruce Hayes. 1995. Metrical Stress Theory: Princi-
ples and Case Studies. University of Chicago Press,
Chicago.
Jeffrey Heinz, Greg Kobele, and Jason Riggle. 2005. Ex-
ploring the typology of quantity-insensitive stress sys-
tems without gradient constraints. Handout, 2005 An-
nual Meeting of the Linguistic Society of America.
Jeffrey Nicholas Heinz. 2007. Inductive Learning of
Phonotactic Patterns. Ph.D. thesis, UCLA.
Larry Hyman. 1977. On the nature of linguistic stress.
In Larry Hyman, editor, Studies in Stress and Accent,
pages 37?82. University of Southern California, De-
partment of Linguistics, Los Angeles.
Johan Liljencrants and Bjorn Lindblom. 1972. Numer-
ical simulation of vowel quality systems: The role of
perceptual contrast. Language, 48(4):839?862.
John Lynch. 1974. Lenakel Phonology. Ph.D. thesis,
University of Hawaii.
Ian Maddieson. 1984. Patterns of Sounds. Cambridge
University Press, Cambridge.
Elliott Moreton. in press. Learning bias as a factor in
phonological typology. In Charles Chang and Anna
Havnie, editors, Proceedings of the 26th Meeting of
the West Coast Conference on Formal Linguistics.
Alan Prince and Paul Smolensky. 1993. Optimality
theory: Constraint interaction in generative grammar.
Ms., Rutgers University and University of Colorado,
Boulder.
Alan Prince and Bruce Tesar. 1999. Learning phonotac-
tic distributions. Ms., ROA 535.
Alan Prince. 1983. Relating to the grid. Linguistic In-
quiry, 14:19?100.
Alan Prince. 2002. Entailed ranking arguments. Rutgers
Optimality Archive, ROA-500.
Jason Riggle, Max Bane, James Kirby, and Jeremy
O?Brien. 2007. Efficiently computing OT typologies.
In 2007 Annual Meeting of the Linguistic Society of
America.
Jason Riggle. 2004. Generation, Recognition, and
Learning in Finite State Optimality Theory. Ph.D. the-
sis, UCLA.
Jason Riggle. 2008. Counting rankings. Manuscript,
University of Chicago. Draft available at
http://hum.uchicago.edu/?jriggle/.
Shaligram Shukla. 1981. Bhojpuri Grammar. George-
town University Press.
Udai Tiwari. 1960. The Origin and Development of Bho-
jpuri. Number 10 in Asiatic Society Monograph. Asi-
atic Society, Calcutta.
38
