Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1092?1103,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning General Connotation of Words using Graph-based Algorithms
Song Feng Ritwik Bose Yejin Choi
Department of Computer Science
Stony Brook University
NY 11794, USA
songfeng, rbose, ychoi@cs.stonybrook.edu
Abstract
In this paper, we introduce a connotation lex-
icon, a new type of lexicon that lists words
with connotative polarity, i.e., words with pos-
itive connotation (e.g., award, promotion) and
words with negative connotation (e.g., cancer,
war). Connotation lexicons differ from much
studied sentiment lexicons: the latter concerns
words that express sentiment, while the former
concerns words that evoke or associate with
a specific polarity of sentiment. Understand-
ing the connotation of words would seem to
require common sense and world knowledge.
However, we demonstrate that much of the
connotative polarity of words can be inferred
from natural language text in a nearly unsu-
pervised manner. The key linguistic insight
behind our approach is selectional preference
of connotative predicates. We present graph-
based algorithms using PageRank and HITS
that collectively learn connotation lexicon to-
gether with connotative predicates. Our em-
pirical study demonstrates that the resulting
connotation lexicon is of great value for sen-
timent analysis complementing existing senti-
ment lexicons.
1 Introduction
In this paper, we introduce a connotation lexicon,
a new type of lexicon that lists words with conno-
tative polarity, i.e., words with positive connotation
(e.g., award, promotion) and words with negative
connotation (e.g., cancer, war). Connotation lexi-
cons differ from sentiment lexicons that are studied
in much of previous research (e.g., Esuli and Sebas-
tiani (2006), Wilson et al (2005a)): the latter con-
cerns words that express sentiment either explicitly
or implicitly, while the former concerns words that
evoke or even simply associate with a specific polar-
ity of sentiment. To our knowledge, there has been
no previous research that investigates polarized con-
notation lexicons.
Understanding the connotation of words would
seem to require common sense and world knowl-
edge at first glance, which in turn might seem to re-
quire human encoding of knowledge base. However,
we demonstrate that much of the connotative polar-
ity of words can be inferred from natural language
text in a nearly unsupervised manner.
The key linguistic insight behind our approach is
selectional preference of connotative predicates. We
define a connotative predicate as a predicate that
has selectional preference on the connotative polar-
ity of some of its semantic arguments. For instance,
in the case of the connotative predicate ?prevent?,
there is strong selectional preference on negative
connotation with respect to the thematic role (se-
mantic role) ?THEME?. That is, statistically speak-
ing, people tend to associate negative connotation
with the THEME of ?prevent?, e.g., ?prevent can-
cer? or ?prevent war?, rather than positive conno-
tation, e.g., ?prevent promotion?. In other words,
even though it is perfectly valid to use words with
positive connotation in the THEME role of ?pre-
vent?, statistically more dominant connotative po-
larity is negative. Similarly, the THEME of ?con-
gratulate? or ?praise? has strong selectional prefer-
ence on positive connotation.
The theoretical concept supporting the selective
1092
accomplish, achieve, advance, advocate, admire,
applaud, appreciate, compliment, congratulate,
develop, desire, enhance, enjoy, improve, praise,
promote, respect, save, support, win
Table 1: Positively Connotative Predicates w.r.t. THEME
alleviate, accuse, avert, avoid, cause, complain,
condemn, criticize, detect, eliminate, eradicate,
mitigate, overcome, prevent, prohibit, protest, re-
frain, suffer, tolerate, withstand
Table 2: Negatively Connotative Predicates w.r.t. THEME
preference of connotative predicates is that of se-
mantic prosody in corpus linguistics. Semantic
prosody describes how some of the seemingly neu-
tral words (e.g., ?cause?) can be perceived with pos-
itive or negative polarity because they tend to col-
locate with words with corresponding polarity (e.g.,
Sinclair (1991), Louw et al (1993), Stubbs (1995),
Stefanowitsch and Gries (2003)). In this work, we
demonstrate that statistical approaches that exploit
this very concept of semantic prosody can success-
fully infer connotative polarity of words.
Having described the key linguistic insight, we
now illustrate our graph-based algorithms. Figure 1
depicts the mutually reinforcing relation between
connotative predicates (nodes on the left-hand side)
and words with connotative polarity (node on the
right-hand side). The thickness of edges represents
the strength of the association between predicates
and arguments. For brevity, we only consider conno-
tation of words that appear in the THEME thematic
role.
We expect that words that appear often in the
THEME role of various positively (or negatively)
connotative predicates are likely to be words with
positive (or negative) connotation. Likewise, pred-
icates whose THEME contains words with mostly
positive (or negative) connotation are likely to be
positively (or negatively) connotative predicates. In
short, we can induce the connotative polarity of
words using connotative predicates, and inversely,
we can learn new connotative predicates based on
words with connotative polarity.
We hypothesize that this mutually reinforcing re-
Prevent 
Avoid 
Alleviate Cancer 
Incident 
Promotion 
Overcome Tragedy 
Figure 1: Bipartite graph of connotative predicates and
arguments. Edge weights are proportionate to the associ-
ation strength.
lation between connotative predicates and their ar-
guments can be captured via graph centrality in
graph-based algorithms. Given a small set of seed
words for connotative predicates, our algorithms
collectively learn connotation lexicon together with
connotative predicates in a nearly unsupervised
manner. A number of different graph representa-
tions are explored using both PageRank (Page et al,
1999) and HITS (Kleinberg, 1999) algorithms. Em-
pirical study demonstrates that our graph based al-
gorithms are highly effective in learning both con-
notation lexicon and connotative predicates.
Finally, we quantify the practical value of our
connotation lexicon in concrete sentiment analysis
applications, and demonstrate that the connotation
lexicon is of great value for sentiment classification
tasks complementing conventional sentiment lexi-
cons.
2 Connotation Lexicon & Connotative
Predicate
In this section, we define connotation lexicon and
connotative predicates more formally, and contrast
them against words in conventional sentiment lexi-
cons.
2.1 Connotation Lexicon
This lexicon lists words with positive and negative
connotation, as defined below.
? Words with positive connotation: In this
work, we define words with positive connota-
tion as those that describe physical objects or
abstract concepts that people generally value,
cherish or care about. For instance, we regard
words such as ?freedom?, ?life?, or ?health? as
1093
words with positive connotation. Some of these
words may express subjectivity either explic-
itly or implicitly, e.g., ?joy? or ?satisfaction?.
However, a substantial number of words with
positive connotation are purely objective, such
as ?life?, ?health?, ?tenure?, or ?scientific?.
? Words with negative connotation: We define
words with negative connotation as those that
describe physical objects or abstract concepts
that people generally disvalue or avoid. Sim-
ilarly as before, some of these words may ex-
press subjectivity (e.g., ?disappointment?, ?hu-
miliation?), while many other are purely objec-
tive (e.g., ?bedbug?, ?arthritis, ?funeral?).
Note that this explicit and intentional inclusion of
objective terms makes connotation lexicons differ
from sentiment lexicons: most conventional senti-
ment lexicons have focused on subjective words by
definition (e.g., Wilson et al (2005b)), as many re-
searchers use the term sentiment and subjectivity in-
terchangeably (e.g., Wiebe et al (2005)).
2.2 Connotative Predicate
In this work, connotative predicates are those that
exhibit selectional preference on the connotative po-
larity of some of their arguments. We emphasize that
the polarity of connotative predicates does not coin-
cide with the polarity of sentiment in conventional
sentiment lexicons, as will be elaborated below.
? Positively connotative predicate: In this
work, we define positively connotative predi-
cates as those that expect positive connotation
in some arguments. For example, ?congratu-
late? or ?save? are positively connotative pred-
icates that expect words with positive conno-
tation in the THEME argument: people typi-
cally congratulate something positive, and save
something people care about. More examples
are shown in Table 1.
? Negatively connotative predicate: In this
work, we define negatively connotative predi-
cates as those that expect negative connotation
in some arguments. For instance, predicates
such as ?prevent? or ?suffer? tend to project
negative connotation in the THEME argument.
More examples are shown in Table 2.
Note that positively connotative predicates are not
necessarily positive sentiment words. For instance
?save? is not a positive sentiment word in the
lexicon published by Wilson et al (2005b). In-
versely, (strongly) positive sentiment words are not
necessarily (strongly) positively connotative predi-
cates, e.g., ?illuminate?, ?agree?. Likewise, neg-
atively connotative predicates are not necessarily
negative sentiment words. For instance, predicates
such as ?prevent?, ?detect?, or ?cause? are not
negative sentiment words, but they tend to corre-
late with negative connotation in the THEME argu-
ment. Inversely, (strongly) negative sentiment words
are not necessarily (strongly) negatively connotative
predicates, e.g., ?abandon? (?abandoned [something
valuable]?).
3 Graph Representation
In this section, we explore the graphical representa-
tion of our task. Figure 1 depicts the key intuition as
a bipartite graph, where the nodes on the left-hand
side correspond to connotative predicates, and the
nodes on the right-hand side correspond to words in
the THEME argument. There is an edge between a
predicate p and an argument a, if the argument a
appears in the THEME role of the predicate p. For
brevity, we explore only verbs as the predicates, and
words in the THEME role of the predicates as argu-
ments. Our work can be readily extended to exploit
other predicate-argument relations however.
Note that there are many sources of noise in the
construction of graph. For instance, some of the
predicates might be negated, changing the semantic
dynamics between the predicate and the argument.
In addition, there might be many unusual combina-
tions of predicates and arguments, either due to data
processing errors or due to idiosyncratic use of lan-
guage. Some of such combinations can be valid ones
(e.g., ?prevent promotion?), challenging the learning
algorithm with confusing evidence.
We hypothesize that by focusing on the important
part of the graph via centrality analysis , it is possible
to infer connotative polarity of words despite various
noise introduced in the graph structure. This implies
that it is important to construct the graph structure so
as to capture important linguistic relations between
predicates and arguments. With this goal in mind,
1094
we next explore the directionality of the edges and
different strategies to assign weights to them.
3.1 Undirected (Symmetric) Graph
First we explore undirected edges. In this case,
we assign weight for each undirected edge between
a predicate p and an argument a. Intuitively, the
weight should correspond to the strength of relat-
edness or association between the predicate p and
the argument a. We use Pointwise Mutual Infor-
mation (PMI), as it has been used by many pre-
vious research to quantify the association between
two words (e.g., Turney (2001), Church and Hanks
(1990)). The PMI score between p and a is defined
as follows:
w(p? a) := PMI(p, a) = log P (p, a)P (p)P (a)
The log of the ratio is positive when the pair of
words tends to co-occur and negative when the pres-
ence of one word correlates with the absence of the
other word.
3.2 Directed (Asymmetric) Graph
Next we explore directed edges. That is, for each
connected pair of a predicate p and an argument a,
there are two edges in opposite directions: e(p? a)
and e(a ? p). In this case, we explore the use
of asymmetric weights using conditional probabil-
ity. In particular, we define weights as follows:
w(p? a) := P (a|p) = P (p, a)P (p)
w(a? p) := P (p|a) = P (p, a)P (a)
Having defined the graph structure, next we explore
algorithms that analyze graph centrality via random
walks. In particular, we investigate the use of HITS
algorithm (Section 4), and PageRank (Section 5).
4 Lexicon Induction using HITS
The graph representation described thus far (Sec-
tion 3) captures general semantic relations between
predicates and arguments, rather than those specific
to connotative predicates and arguments. Therefore
in this section, we explore techniques to augment
the graph representation so as to bias the centrality
of the graph toward connotative predicates and argu-
ments.
In order to establish a learning bias, we start with
a small set of seed words for just connotative predi-
cates. We use 20 words for each polarity, as listed in
Table 1 and Table 2. These seed words act as prior
knowledge in our learning. We explore two different
techniques to incorporate prior knowledge into ran-
dom walk, as will be elaborated in Section 4.2 & 4.3,
followed by brief description of HITS in Section 4.1.
4.1 Hyperlink-Induced Topic Search (HITS)
HITS (Hyperlink-Induced Topic Search) algorithm
(Kleinberg, 1999), also known as Hubs and author-
ities, is a link analysis algorithm that is particularly
suitable to model mutual reinforcement between two
different types of nodes: hubs and authorities. The
definitions of hubs and authorities are given recur-
sively. A (good) hub is a node that points to many
(good) authorities, and a (good) authority is a node
pointed by many (good) hubs.
Notice that the mutually reinforcing relation-
ship is precisely what we intend to model between
connotative predicates and arguments. Let G =
(P,A,E) be the bipartite graph, where P is the set
of nodes corresponding to connotative predicates, A
is the set of nodes corresponding to arguments, and
E is the set of edges among nodes. (Pi, Aj) ? E
if and only if the predicate Pi and the argument Ai
occur together as a predicate ? argument pair in the
corpus. The co-occurrence matrix derived from our
corpus is denoted as L, where
Lij =
{
w(i, j) if(Pi, Aj) ? E
0 otherwise
The value of w(i, j) is set to w(i ? j) as defined
in Section 3.1 for undirected graphs, and w(i ? j)
defined in Section 3.2 for directed graphs.
Let a(Ai) and h(Ai) be the authority and hub
score respectively, for a given node Ai ? A. Then
we compute the authority and hub score recursively
as follows:
a(Ai) =
?
Pi,Aj?E
w(i, j)h(Aj) +
?
Pj ,Ai?E
h(Pj)w(j, i)
h(Ai) =
?
Pi,Aj?E
w(i, j)a(Aj) +
?
Pj ,Ai?E
a(Pj)w(j, i)
1095
The scores a(Pi) and h(Pi) for Pi ? P are defined
similarly as above.
In what follows, we describe two different tech-
niques to incorporate prior knowledge. Note that it
is possible to apply each of the following techniques
to both directed and undirected graph representa-
tions introduced in Section 3. Also note that for each
technique, we construct two separate graphsG+ and
G? corresponding to positive and negative polarity
respectively. That is, G+ learns positively connota-
tive predicates and arguments, while G? learns neg-
atively connotative predicates and arguments.
4.2 Prior Knowledge via Truncated Graph
First we introduce a method based on graph trunca-
tion. In this method, when constructing the bipartite
graph, we limit the set of predicates P to only those
words in the seed set, instead of including all words
that can be predicates. In a way, the truncated graph
representation can be viewed as the query induced
graph on which the original HITS algorithm was in-
vented (Kleinberg, 1999).
The truncated graph is very effective in reducing
the level of noise that can be introduced by predi-
cates of the opposite polarity. It may seem like we
cannot discover new connotative predicates in the
truncated graph however, as the graph structure is
limited only to the seed predicates. We address this
issue by alternating truncation to different side of the
graph, i.e., left (predicates) or right (arguments), as
illustrated in Figure 1, through multiple rounds of
HITS.
For instance, we start with the graph G =
(P o, A,E(P o)) that is truncated only on the left-
hand side, with the seed predicates P o. Here,E(P o)
denotes the reduced set of edges discarding those
edges that connect to predicates not in P o. Then, we
apply HITS algorithm until convergence to discover
new words with connotation, and this completes the
first round of HITS.
Next we begin the second round. Let Ao be the
new words with connotation that are found in the
first round. We now set Ao as seed words for the
second phase of HITS, where we construct a new
graph G = (P,Ao, E(Ao)) that is truncated only
on the right-hand side, with full candidate words for
predicates included on the left-hand side. This al-
ternation can be repeated multiple times to discover
many new connotative predicates and arguments.
4.3 Prior Knowledge via Focussed Graph
In the truncated graph described above, one poten-
tial concern is that the discovery of new words with
connotation is limited to those that happen to corre-
late well with the seed predicates. To mitigate this
problem, we explore an alternative technique based
on the full graph, which we will name as focussed
graph.
In this method, instead of truncating the graph, we
simply emphasize the important portion of the graph
via edge weights. That is, we assign high weights to
those edges that connect a seed predicate with an ar-
gument, while assigning low weights for those edges
that connect to a predicate outside the seed set. This
way, we allow predicates not in the seed set to par-
ticipate in hubs and authority scores, but in a much
suppressed way. This method can be interpreted as
a smoothed version of the truncated graph described
in Section 4.2.
More formally, if the node Ai is connected to
the seed predicate Pj , the value of co-occurrence
matrix Lij is defined by prior knowledge(e.g.
PMI(Ai, Pj) or P (Ai|Pj) ), otherwise a small con-
stant  is assigned to the edge.
Lij =
{
w(i, j) ifPj ? Eo
 otherwise
Similarly to the truncated graph, we proceed with
multiple rounds of HITS, focusing different part of
the bipartite graph alternately.
5 Lexicon Induction using PageRank
In this section, we explore the use of another popu-
lar approach for link analysis: PageRank (Page et
al., 1999). We first describe PageRank algorithm
briefly in Section 5.1, then introduce two different
techniques to incorporate prior knowledge in Sec-
tion 5.2 and 5.3.
5.1 PageRank
Let G = (V,E) be the graph, where vi ? V =
P ? A are nodes (words) for the disjunctive set of
predicates (P ) and arguments (A), and e(i,j) ? E
are edges. Let In(i) be the set of nodes with an
edge leading to ni and similarly, Out(i) be the set
1096
of nodes that ni has an edge leading to. At a given
iteration of the algorithm, we update the score of ni
as follows:
S(i) = ?
?
j?In(i)
S(j)? w(i, j)|Out(i)| + (1? ?) (1)
where the value ? is constant damping factor. The
value of ? is typically set to 0.85. The value of
w(i, j) is set to w(i?j) as defined in Section 3.1 for
undirected graphs, and w(i ? j) as defined in Sec-
tion 3.2 for directed graphs. As before, we will con-
sider two different techniques to incorporate prior
knowledge into the graph analysis as follows.
5.2 Prior Knowledge via Truncated Graph
Unlike HITS, which was originally invented for a
query-induced graph, PageRank is typically applied
to the full graph. However, we can still apply the
truncation technique introduced in Section 4.2 to
PageRank as well. To do so, when constructing the
bipartite graph, we limit the set of predicates P to
only those words in the seed set, instead of including
all words that can be predicates. Graph truncation
eliminates the noise that can be introduced by pred-
icates of the opposite polarity. However, in order to
learn new predicates, we need to perform multiple
rounds of PageRank, truncating different side of the
bipartite graph alternately. Refer to Section 4.2 for
futher details.
5.3 Prior Knowledge via Teleportation
We next explore what is known as teleportation
technique for topic sensitive PageRank (Haveliwala,
2002). For this, we use the following equation that
is slightly augmented from Equation 1.
S(i) = ?
?
j?In(i)
S(j)? w(i, j)|Out(i)| + (1? ?) i (2)
Here, the new term i is a smoothing factor that pre-
vents cliques in the graph from garnering reputation
through feedback (Bianchini et al (2005)). In or-
der to emphasize important portion of the graph, i.e.,
subgraphs connected to the seed set, we assign non-
zero  scores to only those important nodes, i.e., the
seed set. Intuitively, this will cause the random walk
to restart from the seed set with (1??) = 0.15 prob-
ability for each step.
6 The Use of Google Web 1T Data
In order to implement the network of connotative
predicates and arguments, we need a substantially
large amount of documents. The quality of the co-
occurrence statistics is expected to be proportionate
to the size of corpus, but collecting and process-
ing such a large amount of data is not trivial. We
therefore resort to the Google Web 1T data (Brants
and Franz., 2006), which consists of Google n-gram
counts (frequency of occurrence of each n-gram) for
1 ? n ? 5. The use of Web 1T data will lessen the
challenge with respect to data acquisition, while still
allowing us to enjoy the co-occurrence statistics of
web-scale data. Because Web 1T data is just n-gram
statistics, rather than a collection of normal docu-
ments, it does not provide co-occurrence statistics of
any random word pairs. However, it provides a nice
approximation to the particular co-occurrence statis-
tics we are interested in, which are, predicate ? ar-
gument pairs. This is because the THEME argument
of a verb predicate is typically on the right hand side
of the predicate, and the argument is within the close
range of the predicate.
We now describe how to derive co-occurrence
statistics of each predicate ? argument pair using the
Web 1T data. For a given predicate p and an argu-
ment a, we add up the count (frequency) of all n-
grams (2 ? n ? 5) that match the following pattern:
[p] [?]n?2 [a]
where p must be the first word (head), a must be the
last word (tail), and [?]n?2 matches any n? 2 num-
ber of words between p and a. Note that this rule
enforces the argument a to be on the right hand side
of the predicate p. To reduce the level of noise, we
do not allow the wildcard [?] to match any punctu-
ation mark, as such n-grams are likely to cross sen-
tence boundaries representing invalid predicate ? ar-
gument relations. We consider a word as a predicate
if it is tagged as a verb by a Part-of-Speech tagger
(Toutanova and Manning, 2000). For argument [a],
we only consider content-words.
The use of web n-gram statistics necessarily in-
vites certain kinds of noise. For instance, some of
the [p] [?]n?2 [a] patterns might not correspond to
a valid predicate ? argument relation. However, we
expect that our graph-based algorithms ? HITS and
1097
Lexicon FREQ HITS-sT HITS-aT HITS-sF HITS-aF Page-aT Page-aF
Top 100 73.6 67.8 77.7 67.8 48.4 76.3 77.0
Top 1000 67.8 60.6 68.8 60.6 38.0 68.4 68.5
Top MAX 65.8 57.6 66.5 57.6 39.1 65.5 65.7
Table 3: Comparison Result with General Inquirer Lexicon(%)
Lexicon FREQ HITS-sT HITS-aT HITS-sF HITS-aF Page-aT Page-aF
Top 100 83.0 79.3 86.3 79.3 55.8 86.3 87.2
Top 1000 80.3 67.3 81.3 67.3 46.5 80.7 80.3
Top MAX 71.5 62.7 72.2 62.7 45.4 71.1 72.3
Table 4: Comparison Result with OpinionFinder (%)
PageRank ? will be able to discern valid relations
from noise, by focusing on the important part of the
graph. In other words, we expect that good predi-
cates will be supported by good arguments, and vice
versa, thereby resulting in a reliable set of predicates
and arguments that are mutually supported by each
other.
7 Experiments
As a baseline, we use a simple method dubbed
FREQ, which uses co-occurrence frequency with
respect to the seed predicates. Using the pattern
[p] [?]n?2 [a] (see Section 6), we collect two sets
of n-gram records: one set using the positive con-
notative predicates, and the other using the negative
connotative predicates. With respect to each set, we
calculate the following for each word a,
? Given [a], the number of unique [p] as f1
? Given [a], the number of unique phrases [?]n?2
as f2
? The number of occurrences of [a] as f3
We then obtain the score ?a+ for positive connota-
tion and ?a? for negative connotation using the fol-
lowing equations that take a linear combination of
f1, f2, and f3 that we computed above with respect
to each polarity.
?a+ = ?? ?f1+ + ? ? ?f2+ + ? ? ?f3+ (3)
?a? = ?? ?f1? + ? ? ?f2? + ? ? ?f3? (4)
Note that the coefficients ?, ? and ? are determined
experimentally. We assign positive polarity to the
word a, if ?a+ >> ?a? and vice versa.
7.1 Comparison against Sentiment Lexicon
The polarity defined in the connotation lexicon dif-
fers from that of conventional sentiment lexicons in
which we aim to recognize more subtle sentiment
that correlates with words. Nevertheless, we provide
agreement statistics between our connotation lexi-
con and conventional sentiment lexicons for com-
parison purposes. We collect statistics with respect
to the following two resources: General Inquirer
(Stone and Hunt, 1963) and Opinion Finder (Wilson
et al, 2005b).
For polarity ? ? {+,?}, let countsentlex(?) denote
the total number of words labeled as ? in a given
sentiment lexicon, and let countagreement(?) denote
the total number of words labeled as ? by both the
given sentiment lexicon and our connotation lexi-
con. In addition, let countoverlap(?) denote the total
number of words that are labeled as ? by our conno-
tation lexicon that are also included in the reference
lexicon with or without the same polarity. Then we
compute prec? as follows:
prec? % =
countagreement(?)
countoverlap(?)
? 100
We compare prec? % for three different segments
of our lexicon: the top 100, top 1000, and the entire
lexicon. We compare the lexicons provided by the
seven variations of our algorithm. Results are shown
in Table 3 & 4.
The acronym of each different method is defined
as follows: HITS-sT & HITS-aT correspond to
the Symmetric (undirected) and Asymmetric (di-
rected) version of the Truncated method respec-
tively. HITS-sF & HITS-aF correspond to the
1098
Positive: include, offer, obtain, allow, build, in-
crease, ensure, contain, pursue, fulfill, maintain,
recommend, represent, require, respect
Negative: abate, die, condemn, deduce, investi-
gate, commit, correct, apologize, debilitate, dis-
pel, endure, exacerbate, indicate, induce, mini-
mize
Table 5: Examples of newly discovered connotative pred-
icates
Positive: boogie, housewarming, persuasiveness,
kickoff, playhouse, diploma, intuitively, monu-
ment, inaugurate, troubleshooter, accompanist
Negative: seasickness, overleap, gangrenous,
suppressing, fetishist, unspeakably, doubter,
bloodmobile, bureaucratized
Table 6: Examples of newly discovered words with con-
notations: these words are treated as neutral in some con-
ventional sentiment lexicons.
symmetric and asymmetric version of the Focused
method. Finally, Page-aT & Page-aF correspond to
the Truncation and teleportation (Focused) respec-
tively.
Asymmetric HITS on a directed truncated graph
(HITS-aT) and topic-sensitive PageRank (Page-aF)
achieve the best performance in most cases, espe-
cially for top ranked words which have a higher
average frequency. The difference between these
two top performers is not large, but statistically
significant using wilcoxon test with p < 0.03.
Standard PageRank (Page-aT) achieves the third
best performance overall. All these top performing
ones (HITS-aT, Page-aF, Page-aT) outperform the
baseline approach (FREQ) statistically significantly
with p < 0.001. For brevity, we omit the PageRank
results based on the undirected graphs, as the perfor-
mance of those was not as good as that of directed
ones.
7.2 Extrinsic Evaluation via Sentiment
Analysis
Next we perform extrinsic evaluation to quantify the
practical value of our connotation lexicon in con-
crete sentiment analysis applications. In particular,
we make use of our connotation lexicon for binary
sentiment classification tasks in two different ways:
? Unsupervised classification by voting. We de-
fine r as the ratio of positive polarity words to
negative polarity words in the lexicon. In our
experiment, penalty is 0 for positive and ?0.5
for negative.
score(x+) = 1 + penalty+(r,#positive)
score(x?) = ?1 + penalty?(r,#negative)
? Supervised classification using SVM. We use
bag-of-words features for baseline. In order
to quantify the effect of different lexicons, we
add additional features based on the following
scores as defined below:
scoreraw(x) =
?
wx
s(w)
scorepurity(x) =
scoreraw(x)?
wx abs(s(w))
The two corpora we use are SemEval2007 (Strap-
parava and Mihalcea, 2007) and Sentiment Twitter.1
The Twitter dataset consists of tweets containing ei-
ther a smiley emoticon (representing positive senti-
ment) or a frowny emoticon (representing negative
sentiment), we randomly select 50000 smiley tweets
and 50000 frowny tweets.2 We perform a 5-fold
cross validation.
In Table 8, we find very promising results, partic-
ularly for Twitter dataset, which is known to be very
noisy. Notice that the use of Top 6k words from
our connotation lexicon along with OpinionFinder
lexicon boost the performance up to 78.0%, which
is significantly better than than 71.4% using only
the conventional OpinionFinder lexicon. This result
shows that our connotation lexicon nicely comple-
ments existing sentiment lexicon, improving practi-
cal sentiment analysis tasks.
1http://www.stanford.edu/? alecmgo/cs224n/twitterdata.
2009.05.25.c.zip
2We filter out stop-words and words appearing less than 3
times. For Twitter, we also remove usernames of the format
@username occurring within tweet bodies.
1099
Algorithm 1st Round 2nd RoundAcc. F-val Acc. F-val
Voting 68.7 65.4 71.0 68.5
Bag of Words 69.9 65.1 69.9 65.1
(??) + OpFinder 74.7 75.0 74.7 75.0
BoW + Top 2k 73.3 74.5 73.7 75.4
(??) + OpFinder 72.8 73.5 75.0 77.6
BoW + Top 6k 76.6 77.1 74.5 75.3
(??) + OpFinder 74.1 73.5 75.2 76.0
BoW + Top 10k 74,1 73.5 74.2 73.8
(??) + OpFinder 73.5 74.3 74.7 75.1
Table 7: SemEval Classification Result(%) ? (??) denotes
that all features in the previous row are copied over.
Algorithm 1st Round 2nd RoundAcc. F-val Acc. F-val
Voting 60.4 59.1 62.6 61.3
Bag of Words 69.9 72.1 69.9 72.1
(??) + OpFinder 70.3 71.4 70.3 71.4
BoW + Top 2k 71.3 65.4 72.7 73.3
(??) + OpFinder 69.4 63.1 73.1 74.6
BoW + Top 6k 77.2 69.0 76.4 77.6
(??) + OpFinder 76.4 72.0 76.8 78.0
BoW + Top 10k 73.3 73.5 73.7 74.1
(??) + OpFinder 74.1 69.5 73.5 74.2
Table 8: Twitter Classification Result(%) ? (??) denotes
that all features in the previous row are copied over.
7.3 Intrinsic Evaluation via Human Judgment
In order to measure the quality of the connotation
lexicon, we also perform human judgment study on
a subset of the lexicon. Human judges are asked to
quantify the degree of connotative polarity of each
given word using an integer value between 1 and 5,
where 1 and 5 correspond to the most negative and
positive connotation respectively. When computing
the annotator agreement score or evaluating our con-
notation lexicon against human judgment, we con-
solidate 1 and 2 into a single negative class and 4
and 5 into a single positive class. The Kappa score
between two human annotators is 0.78.
As a control set, we also include 100 words taken
from the General Inquirer lexicon: 50 words with
positive sentiment, and 50 words with negative sen-
timent. These words are included so as to mea-
sure the quality of human judgment against a well-
established sentiment lexicon. The words were pre-
sented in a random order so that the human judges
will not know which words are from the General In-
quirer lexicon and which are from our connotative
lexicon. For the words in the control set, the anno-
tators achieved 94% (97% lenient) accuracy on the
positive set and 97% on the negative set.
Note that some words appear in both positive and
negative connotation graphs, while others appear in
only one of them. For instance, if a given word x
appears as an argument for only positive connotative
predicates, but never for negative ones, then xwould
appear only in the positive connotation graph. This
means that for such word, we can assume the conno-
tative polarity even without applying the algorithms
for graph centrality. Therefore, we first evaluate the
accuracy of the polarity of such words that appear
only in one of the connotation graphs. We discard
words with low frequency (300 in terms of Google
n-gram frequency), and randomly select 50 words
from each polarity. The accuracy of such words is
88% by strict evaluation and 94.5% by lenient eval-
uation, where lenient evaluation counts words in our
polarized connotation lexicon to be correct if the hu-
man judges assign non-conflicting polarities, i.e., ei-
ther neutral or identical polarity.
For words that appear in both positive and nega-
tive connotation graphs, we determine the final po-
larity of such words as one with higher scores given
by HITS or PageRank. We randomly select words
that rank at 5% of top 100, top 1000, top 2000, and
top 5000 by each algorithm for human judgment.
We only evaluate the top performing algorithms ?
HITS-aT and Page-aF ? and FREQ baseline. The
stratified performance for each of these methods is
given in Table 9.
8 Related Work
Graph based approaches have been used in many
previous research for lexicon induction. A tech-
nique named label propagation (Zhu and Ghahra-
mani, 2002) has been used by Rao and Ravichan-
dran (2009) and Velikovich et al (2010), while ran-
dom walk based approaches, PageRank in particular,
have been used by Esuli and Sebastiani (2007). In
our work, we explore the use of both HITS (Klein-
berg, 1999) and PageRank (Page et al, 1999) and
1100
Average Positive Negative
Top # Str. Len. Str. Len. Str. Len.
FREQ
@100 73.5 87.3 72.2 91.1 74.7 83.5
@1000 51.8 78.6 44.4 75.6 81.8 90.9
@2000 66.9 74.7 73.1 84.2 57.3 60.0
@5000 61.5 81.3 61.4 84.1 62.0 70.0
HITS-aT
@100 61.3 79.8 74.4 93.3 47.0 65.1
@1000 39.6 75.5 48.1 77.8 30.8 73.1
@2000 57.7 72.1 78.0 86.0 41.0 60.7
@5000 55.6 73.5 69.7 85.7 44.3 63.8
Page-aF
@100 63.0 78.6 74.7 91.2 50.0 64.6
@1000 53.7 72.2 54.5 72.7 53.1 71.9
@2000 56.5 79.6 67.2 91.8 42.6 63.8
@5000 57.1 76.2 75.7 91.0 43.3 65.3
Table 9: Human Annotation Accuracies(%) ? Str. de-
notes strict evaluation & Len. denotes lenient evaluation.
present systematic comparison of various options for
graph representation and encoding of prior knowl-
edge. We are not aware of any previous research
that made use of HITS algorithm for connotation or
sentiment lexicon induction.
Much of previous research investigated the use of
dictionary network (e.g., WordNet) for lexicon in-
duction (e.g., Kamps et al (2004), Takamura et al
(2005), Adreevskaia and Bergler (2006), Esuli and
Sebastiani (2006), Su and Markert (2009), Moham-
mad et al (2009)), while relatively less research in-
vestigated the use of web documents (e.g., Kaji and
Kitsuregawa (2007), Velikovich et al (2010))).
Wilson et al (2005b) first introduced the sen-
timent lexicon, spawning a great deal of research
thereafter. At the beginning, sentiment lexicons
were designed to include only those words that ex-
press sentiment, that is, subjective words. However
in recent years, sentiment lexicons started expand-
ing to include some of those words that simply asso-
ciate with sentiment, even if those words are purely
objective (e.g., Velikovich et al (2010), Baccianella
et al (2010)). This trend applies even to the most re-
cent version of the lexicon of Wilson et al (2005b).
We conjecture that this trend of broader coverage
suggests that such lexicons are practically more use-
ful than sentiment lexicons that include only those
words that are strictly subjective. In this work, we
make this transition more explicit and intentional,
by introducing a novel connotation lexicon.
Mohammad and Turney (2010) focussed on emo-
tion evoked by common words and phrases. The
spirit of their work shares some similarity with ours
in that it aims to find the emotion evoked by words,
as opposed to expressed. Two main differences are:
(1) our work aims to discover even more subtle asso-
ciation of words with sentiment, and (2) we present
a nearly unsupervised approach, while Mohammad
and Turney (2010) explored the use of Mechanical
Turk to build the lexicon based on human judgment.
In the work of Osgood et al (1957), it has been
discussed that connotative meaning of words can
be measured in multiple scales of semantic differ-
ential, for example, the degree of ?goodness? and
?badness?. Our work presents statistical approaches
that measure one such semantic differential auto-
matically. Our graph construction to capture word-
to-word relation is analogous to that of Collins-
Thompson and Callan (2007), where the graph rep-
resentation was used to model more general defini-
tions of words.
9 Conclusion
We introduced the connotation lexicon, a novel lex-
icon that list words with connotative polarity, which
will be made publically available. We also pre-
sented graph-based algorithms for learning conno-
tation lexicon together with connotative predicates
in a nearly unsupervised manner. Our approaches
are grounded on the linguistic insight with respect to
the selectional preference of connotative predicates.
Empirical study demonstrates the practical value of
the connotation lexicon for sentiment analysis en-
couraging further research in this direction.
Acknowledgments
We wholeheartedly thank the reviewers for very
helpful and insightful comments.
References
Alina Adreevskaia and Sabine Bergler. 2006. Mining
wordnet for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In 11th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 209?216.
1101
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Monica Bianchini, Marco Gori, and Franco Scarselli.
2005. Inside pagerank. ACM Trans. Internet Technol.,
5:92?128, February.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Comput. Linguist., 16:22?29, March.
K. Collins-Thompson and J. Callan. 2007. Automatic
and human scoring of word definition responses. In
Proceedings of NAACL HLT, pages 476?483.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. In In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC06, pages
417?422.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pagerank-
ing wordnet synsets: An application to opinion min-
ing. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 424?
431. Association for Computational Linguistics.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In
Proceedings of the Eleventh International World Wide
Web Conference, Honolulu, Hawaii.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive collec-
tion of HTML documents. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1075?1083.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to mea-
sure semantic orientation of adjectives. In Proceed-
ings of the 4th International Conference on Language
Resources and Evaluation (LREC), pages 1115?1118.
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. JOURNAL OF THE ACM,
46(5):604?632.
B. Louw, M. Baker, G. Francis, and E. Tognini-Bonelli.
1993. Irony in the text or insincerity in the writer?
the diagnostic potential of semantic prosodies. TEXT
AND TECHNOLOGY IN HONOUR OF JOHN SIN-
CLAIR, pages 157?176.
Saif Mohammad and Peter Turney. 2010. Emotions
evoked by common words and phrases: Using me-
chanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, pages 26?34, Los Angeles, CA, June.
Association for Computational Linguistics.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.
Generating high-coverage semantic orientation lexi-
cons from overtly marked words and a thesaurus.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
599?608, Singapore, August. Association for Compu-
tational Linguistics.
C. E. Osgood, G. Suci, and P. Tannenbaum. 1957. The
measurement of meaning. University of Illinois Press,
Urbana, IL.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-66,
Stanford InfoLab, November.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In EACL ?09:
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 675?682, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
John Sinclair. 1991. Corpus, concordance, colloca-
tion. Describing English language. Oxford University
Press.
A. Stefanowitsch and S.T. Gries. 2003. Collostructions:
Investigating the interaction of words and construc-
tions. International Journal of Corpus Linguistics,
8(2):209?243.
Philip J. Stone and Earl B. Hunt. 1963. A computer ap-
proach to content analysis: studies using the general
inquirer system. In Proceedings of the May 21-23,
1963, spring joint computer conference, AFIPS ?63
(Spring), pages 241?256, New York, NY, USA. ACM.
Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: affective text. In SemEval ?07: Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations, pages 70?74, Morristown, NJ, USA.
Association for Computational Linguistics.
M. Stubbs. 1995. Collocations and semantic profiles:
on the cause of the trouble with quantitative studies.
Functions of language, 2(1):23?55.
Fangzhong Su and Katja Markert. 2009. Subjectivity
recognition on word senses via semi-supervised min-
cuts. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 1?9. Association for Computational
Linguistics.
1102
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of ACL-05, 43rd Annual
Meeting of the Association for Computational Linguis-
tics, Ann Arbor, US. Association for Computational
Linguistics.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In In EMNLP/VLC
2000, pages 63?70.
Peter Turney. 2001. Mining the web for synonyms: Pmi-
ir versus lsa on toefl.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164?
210.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.
2005a. Opinionfinder: a system for subjectivity anal-
ysis. In Proceedings of HLT/EMNLP on Interactive
Demonstrations, pages 34?35, Morristown, NJ, USA.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 347?354, Morristown, NJ, USA. Association for
Computational Linguistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. In Technical Report CMU-CALD-02-107.
CarnegieMellon University.
1103
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1522?1533, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Characterizing Stylistic Elements in Syntactic Structure
Song Feng Ritwik Banerjee Yejin Choi
Department of Computer Science
Stony Brook University
NY 11794, USA
songfeng, rbanerjee, ychoi@cs.stonybrook.edu
Abstract
Much of the writing styles recognized in
rhetorical and composition theories involve
deep syntactic elements. However, most
previous research for computational sty-
lometric analysis has relied on shallow
lexico-syntactic patterns. Some very re-
cent work has shown that PCFG models
can detect distributional difference in syn-
tactic styles, but without offering much in-
sights into exactly what constitute salient
stylistic elements in sentence structure
characterizing each authorship. In this
paper, we present a comprehensive ex-
ploration of syntactic elements in writing
styles, with particular emphasis on inter-
pretable characterization of stylistic ele-
ments. We present analytic insights with
respect to the authorship attribution task
in two different domains.
1 Introduction
Much of the writing styles recognized in rhetor-
ical and composition theories involve deep syn-
tactic elements in style (e.g., Bain (1887), Kem-
per (1987) Strunk and White (2008)). However,
previous research for automatic authorship at-
tribution and computational stylometric analy-
sis have relied mostly on shallow lexico-syntactic
patterns (e.g., Mendenhall (1887), Mosteller
and Wallace (1984), Stamatatos et al(2001),
Baayen et al(2002), Koppel and Schler (2003),
Zhao and Zobel (2007), Luyckx and Daelemans
(2008)).
Some very recent works have shown that
PCFG models can detect distributional differ-
ence in sentence structure in gender attribution
(Sarawgi et al 2011), authorship attribution
(Raghavan et al 2010), and native language
identification (Wong and Dras, 2011). However,
still very little has been understood exactly what
constitutes salient stylistic elements in sentence
structures that characterize each author. Al-
though the work of Wong and Dras (2011) has
extracted production rules with highest informa-
tion gain, their analysis stops short of providing
insight any deeper than what simple n-gram-
level analysis could also provide.1 One might
even wonder whether PCFG models are hing-
ing mostly on leaf production rules, and whether
there are indeed deep syntactic differences at all.
This paper attempts to answer these questions.
As an example of syntactic stylistic elements
that have been much discussed in rhetorical the-
ories, but have not been analyzed computation-
ally, let us consider two contrasting sentence
styles: loose (cumulative) and periodic:2 a loose
sentence places the main clause at the begin-
ning, and then appends subordinate phrases and
clauses to develop the main message. In con-
trast, a periodic sentence starts with subordi-
nate phrases and clauses, suspending the most
1For instance, missing determiners in English text
written by Chinese speakers, or simple n-gram anomaly
such as frequent use of ?according to? by Chinese speak-
ers (Wong and Dras, 2011).
2Periodic sentences were favored in classical times,
while loose sentences became more popular in the modern
age.
1522
Hobbs Joshi Lin McDon
S?ROOT ? S , CC S PP?PRN ? IN NP NP?S ? NN CD NP?NP ? DT NN POS
NP?PP ? DT NP?PP ? NP PRN SBAR NP?NP ? DT NN NNS WHNP?SBAR ? IN
VP?VP ? TO VP S?ROOT ? PP NP VP . S?ROOT ? SBAR , NP VP . NP?PP ? NP SBAR
PP?PP ? IN S PRN?NP ? -LRB- PP -RRB- NP?PP ? NP : NP SBAR?PP ? WHADVP S
NP?PP ? NP , PP NP?NP ? NNP S?ROOT ? PP , NP VP . SBAR?S ? WHNP S
VP?S ? VBZ ADJP S S?SBAR ? PP NP VP NP?NP ? PDT DT NNS PP?NP ? IN SBAR
VP?SINV ? VBZ S?ROOT ? LST NP VP . NP?VP ? DT NN SBAR SBAR?NP ? WHNP S
VP?S ? VBD S CONJP?NP ? RB RB IN SBAR?S ? WHADVP S SBAR?PP ? SBAR CC SBAR
VP?S ? VBG PP NP?PP ? NP PRN PP PRN?NP ? -LRB- NP -RRB- PP?VP ? IN
ADVP?VP ? RB PP NP?NP ? NP , NP NP?PP ? NN NN S?SBAR ? VP
Table 1: Top 10 most discriminative production rules for each author in the scientific domain.
loose Christopher Columbus finally
reached the shores of San Salvador
after months of uncertainty at
sea, the threat of mutiny, and a
shortage of food and water.
periodic After months of uncertainty at sea,
the threat of mutiny, and a short-
age of food and water, Christopher
Columbus finally reached the shores
of San Salvador.
Table 2: Loose/Periodic sentence with identical set
of words and POS tags
important part to the end. The example in Ta-
ble 2 highlights the difference:
Notice that these two sentences comprise of an
identical set of words and part-of-speech. Hence,
shallow lexico-syntactic analysis will not be able
to catch the pronounced stylistic difference that
is clear to a human reader.
One might wonder whether we could gain in-
teresting insights simply by looking at the most
discriminative production rules in PCFG trees.
To address this question, Table 1 shows the
top ten most discriminative production rules
for authorship attribution for scientific articles,3
ranked by LIBLINEAR (Fan et al 2008).4 Note
that terminal production rules are excluded so
as to focus directly on syntax.
It does provide some insights, but not to a sat-
isfactory degree. For instance, Hobbs seems to
favor inverted declarative sentences (SINV) and
adverbs with prepositions (RB PP). While the
latter can be easily obtained by simple part-of-
3See Section 2 for the description of the dataset.
4We use Berkeley PCFG parser (Petrov and Klein,
2007) for all experiments.
speech analysis, the former requires using parse
trees. We can also observe that none of the
top 10 most discriminative production rules for
Hobbs includes SBAR tag, which represents sub-
ordinate clauses. But examining discriminative
rules alone is limited in providing more compre-
hensive characterization of idiolects.
Can we unveil something more in deep syntac-
tic structure that can characterize the collective
syntactic difference between any two authors?
For instance, what can we say about distribu-
tional difference between loose and periodic sen-
tences discussed earlier for each author? As can
be seen in Table 1, simply enumerating most dis-
criminative rules does not readily answer ques-
tions such as above.
In general, production rules in CFGs do not
directly map to a wide variety of stylistic el-
ements in rhetorical and composition theories.
This is only as expected however, partly because
CFGs are not designed for stylometric analysis
in the first place, and also because some syntac-
tic elements can go beyond the scope of context
free grammars.
As an attempt to reduce this gap between
modern statistical parsers and cognitively recog-
nizable stylistic elements, we explore two com-
plementary approaches:
1. Translating some of the well known stylistic
elements of rhetorical theories into PCFG
analysis (Section 3).
2. Investigating different strategies of analyz-
ing PCFG trees to extract author charac-
teristics that are interesting as well as in-
terpretable (Sections 4 & 5).
1523
Algorithm 1 Sentence Type-1 Identification
Input: Parse tree t(Nr) of sentence s
Output: Type of s.
if S ? Ltop then
if SBAR /? ?(Nr) then
return COMPOUND
else
return COMPLEX-COMPOUND
else
if VP ? Ltop then
if SBAR /? ?(Nr) then
return SIMPLE
else
return COMPLEX
return OTHER
We present analytic insights with respect to
the authorship attribution task in two distinct
domains.
2 Data
For the empirical analysis of authorship attri-
bution, we use two different datasets described
below. Sections 3, 4 & 5 provide the details of
our stylometric analysis.
Scientific Paper We use the ACL Anthol-
ogy Reference Corpus (Bird et al 2008). Since
it is nearly impossible to determine the gold-
standard authorship of a paper written by multi-
ple authors, we select 10 authors who have pub-
lished at least 8 single-authored papers. We in-
clude 8 documents per author, and remove cita-
tions, tables, formulas from the text using sim-
ple heuristics.5
Novels We collect 5 novels from 5 English au-
thors: Charles Dickens, Edward Bulwer-Lytton,
Jane Austen, Thomas Hardy and Walter Scott.
We select the first 3000 sentences from each
novel and group every 50 consecutive sentences
into 60 documents per novel per author.
5Some might question whether the size of the dataset
used here is relatively small in comparison to typical
dataset comprised of thousands of documents in conven-
tional text categorization. We point out that authorship
attribution is fundamentally different from text catego-
rization in that it is often practically impossible to collect
more than several documents for each author. Therefore,
it is desirable that the attribution algorithms to detect
the authors based on very small samples.
Algorithm 2 Sentence Type-II Identification
Input: Parse tree t(Nr) of sentence s
Output: Type of s.
k ? 1
while k ? ? do
if Ltopk 6= VP then
if S ? ?(Ltopk ) or SBAR ? ?(L
top
k ) then
return PERIODIC
else
if S ? ?(Ltopk ) or SBAR ? ?(L
top
k ) then
return LOOSE
return OTHER
3 Sentence Types
In this section, we examine well-known sentence
types that are recognized in the literature, but
have not been analyzed computationally.
Type-I Identification ? Simple/Complex/
Compound/Complex-Compound: PCFG
trees do not provide this information directly,
hence we must construct an algorithm to derive
it. The key to identifying these sentences is the
existence of dependent and independent clauses.
For the former, we rely on the SBAR tag, while
for the latter, we first define the sequence of
nodes right below the root (e.g., [NP VP .] shown
in the horizontal box in Figure 1). We call this
the top structural level. We then check whether
S (in addition to the root S) appears in this
sequence.
Formally, let Ltop = {Ni} be the set of nodes
in the top structural level, and ? = |Ltop|. Let
t(Nr) be the tree rooted at Nr, and ?(Nr) de-
note the set of nodes in t(Nr). Algorithm 1
shows the procedure to determine the type-I
class of a sentence based on its PCFG tree.6
Type-II Identification ? Loose/Periodic:
A sentence can also be classified as loose or
periodic, and we present Algorithm 2 for this
identification. We perform a mini-evaluation on
20 previously unseen sentences for each type7.
Our algorithm was able to perform type-I iden-
tification on all sentences correctly. In type-II
6Note that Algorithm 1 & 2 rely on the use of Berkeley
parser (Petrov and Klein, 2007).
7These were gathered from several online quizzes
for English learners. E.g., http://grammar.about.com,
http://a4esl.org
1524
Type Hobbs Joshi Lin McDon
simple 40.0 41.7 50.2 27.9
cplex 40.8 40.7 37.6 48.4
cpnd 7.9 5.6 3.9 5.5
cpxnd 8.5 9.2 7.7 15.5
other 2.8 2.8 0.6 2.7
loose 27.6 26.4 26.9 30.8
perio 11.1 11.7 15.2 16.4
other 61.3 61.9 57.9 52.8
Table 3: Sentence Types (%) in scientific data.
identification, it labeled all loose sentences cor-
rectly, and achieved 90% accuracy on periodic
sentences.
Discussion Tables 3 & 4 show the sentence
type distribution in scientific data and novels,
respectively.8 We see that different authors are
characterized by different distribution of sen-
tence types. For instance, in Table 3, Lin is
a prolific user of simple sentences while McDon
prefers employing complex sentences. McDon
also uses complex-compound sentences quite of-
ten (15.5%), more than twice as frequently as
Lin. Notice that all authors use loose sen-
tences much more often than periodic sentences,
a known trend in modern English.
In Table 4, we see the opposite trend among
19th-century novels: with the exception of Jane
Austen, all authors utilize periodic sentences
comparatively more often. We also notice
that complex and complex-compound sentences
abound, as expected from classic literary proses.
Can we determine authorship solely based on the
distribution of sentence types?
We experiment with a SVM classifier using just
6 features (one feature for each sentence type in
Table 3), and we achieve accuracy 36.0% with
the scientific data. Given that a random base-
line would achieve only about 10% accuracy, this
demonstrates that the distribution of sentence
types does characterize an idiolect to some de-
gree.
8Due to space limitation, we present analyses based
on 4 authors from the scientific data.
Type Dickens B-Lyt Austen Hardy Scott
simple 26.0 21.2 23.9 25.6 17.5
cplex 24.4 21.8 24.8 25.6 31.8
cpnd 15.3 15.2 12.6 16.3 11.7
cpxnd 20.8 23.3 31.1 18.9 28.7
other 13.5 18.5 7.6 13.6 10.3
loose 11.5 10.8 17.9 14.5 15.3
perio 19.5 13.6 14.0 16.2 18.0
other 69.0 75.6 68.1 69.3 66.7
Table 4: Sentence Types (%) in Novels
4 Syntactic Elements Based on
Production Rules
In this section, we examine three different as-
pects of syntactic elements based on production
rules.
4.1 Syntactic Variations
We conjecture that the variety of syntactic
structure, which most previous research in com-
putational stylometry has not paid much atten-
tion to, provides an interesting insight into au-
thorship. One way to quantify the degree of syn-
tactic variations is to count the unique produc-
tion rules. In Tables 5, we show the extent of
syntactic variations employed by authors using
the standard deviation ? and the coverage of an
author:
C(a) :=
|R(a)|
| ?a R(a)|
? 100
whereR(a) denotes the set of unique production
rules used by author a, and ?a iterates over all
authors. In order to compare among authors,
we also show these parameters normalized with
respect to the highest value. Our default setting
is to exclude all lexicalized rules in the produc-
tions to focus directly on the syntactic varia-
tions. In our experiments (Section 6), however,
we do augment the rules with (a) ancestor nodes
to capture deeper syntactic structure and (b)
lexical (leaf) nodes.
As hypothesized, these statistics provide us
new insights into the authorship. For instance,
we find that McDon employs a wider variety of
syntactic structure than others, while Lin?s writ-
ing exhibits relatively the least variation. More-
over, comparing Joshi and Hobbs, it is inter-
esting to see the standard deviation differ a lot
1525
Hobbs Joshi Lin McDon Dickens B-Lyt Austen Hardy Scott
C 36.0 37.6 32.8 42.6 30.9 28.8 36.2 30.0 24.1
Cnorm 0.84 0.88 0.77 1.0 0.85 0.79 1.0 0.83 0.67
? 51.5 39.2 63.3 44.4 88.3 81.6 98.0 125.3 114.7
?norm 0.81 0.62 1.0 0.7 0.7 0.65 0.78 1.0 0.92
Table 5: Syntactic variations of different authors in the scientific domain.
Hobbs Joshi Lin McDon
# 136 # 142 # 124 # 161
S ? S CC S . S ? ADVP PP NP VP . S ? SBAR NP VP . S ? S NP VP .
S ? CC NP VP . S ? PP NP ADVP VP . FRAG ? NP : S . S ? S : S .
S ? S VP . S ? NP VP S ? NP VP . S ? SBAR VP .
S ? NP NP VP . S ? S S CC S . S ? PP VP . S ? SBAR S CC S .
S ? PP NP VP . S ? ADVP NP VP . S ? NP ADVP VP . S ? NP PP VP .
Table 6: Most discriminative sentence outlines in the scientific data. #N shows the number of unique
sentence outlines of each author.
(51.5 and 39.2), in spite of their C scores being
similar: 36.0% and 37.6%, respectively. This
indicates that Hobbs tends to use a certain sub-
set production rules much more frequently than
Joshi. Lin exhibits the highest standard devia-
tion in spite of having least syntactic variation,
indicating that he uses a much smaller subset of
productions regularly, while ocassionally deviat-
ing to other rules.
Similarly, among novels, Jane Austen?s writ-
ing has the highest amount of variation, while
Walter Scott?s writing style is the least varied.
Even though authors from both datasets display
similar C scores (Table 5), the difference in ? is
noteworthy. The significantly higher linguistic
variation is to be expected in creative writing
of such stature. It is interesting to note that
the authors with highest coverage ? Austen and
Dickens ? have much lower deviation in their
syntactic structure when compared to Hardy
and Scott. This indicates that while Austen and
Dickens consistently employ a wider variety of
sentence structures in their writing, Hardy and
Scott follow a relatively more uniform style with
sporadic forays into diverse syntactic constructs.
4.2 Sentence Outlines
Although the approach of Section 4.1 give us a
better and more general insight into the char-
acteristics of each author, its ability to provide
insight on deep syntactic structure is still lim-
ited, as it covers production rules at all levels of
the tree. We thus shift our focus to the top level
of the trees, e.g., the second level (marked in a
horizontal box) in Tree (1) of Figure 1, which
gives us a better sense of sentence outlines.
Tables 6 and 7 present the most discrimina-
tive sentence outlines of each author in the scien-
tific data and novels, respectively. We find that
McDon is a prolific user of subordinate clauses,
indicating his bias towards using complex sen-
tences. The rule ?S ? SBAR S CC S? shows
his inclination towards complex-compound sen-
tences as well. These inferences are further sup-
ported by the observations in Table 3. Another
observation of possible interest is the tendency
of Joshi and Lin to begin sentences with prepo-
sitional phrases.
In comparing Table 6 and Table 7, notice
the significantly higher presence of complex and
compound-complex structures in the latter9.
The most discriminating sentence outlines for
Jane Austen, for instance, are all indicative of
complex-compound sentences. This is further
supported by Table 4.
5 Syntactic Elements Based on Tree
Topology
In this section, we investigate quantitative tech-
niques to capture stylistic elements in the tree
9The presence of ?FRAG? is not surprising. Inten-
tional use of verbless sentence fragments, known as sce-
sis onomaton, was often employed by authors such as
Dickens and Bulwer-Lytton (Quinn, 1995).
1526
Dickens Bulwer-Lytton Austen Hardy Scott
# 1820 # 1696 # 2137 # 1772 # 1423
SQ ? NNP . SBARQ ? WHNP S . S ? S : CC S . S ? S NP VP . S ? NP PRN VP .
FRAG ? NP . FRAG ? INTJ NP . S ? S CC S : CC S . S ? ADVP NP VP . S ? PP NP VP .
SINV ? NP VP NP . S ? S : S CC S . S ? S : CC S : CC S . S ? FRAG : S . S ? S S : S .
INTJ ? UH . FRAG ? CC NP . S ? S : S : CC S . S ? INTJ NP VP . S ? NP PP VP .
SBARQ ? WHNP SQ . FRAG ? NP ADJP . S ? SBAR S : CC S . S ? NP VP . S ? ADVP PRN NP VP .
Table 7: Most discriminative sentence outlines in the novel data. #N shows the number of unique sentence
outlines of each author.
Metrics Scientific Data Novels
Hobbs Joshi Lin McDon Dickens B-Lyt Austen Hardy Scott
sen-len avg 23.7 26.0 21.0 32.2 24.1 26.7 31.4 21.5 34.1
hT avg 5.8 5.3 5.9 4.8 4.7 5.0 5.4 4.9 5.9
hF avg 2.4 2.1 2.5 1.9 1.9 1.9 2.1 1.9 2.1
wL avg 5.0 4.8 5.5 4.2 4.1 4.4 4.7 3.8 4.9
?H avg 1.2 1.1 1.1 1.0 1.1 1.1 1.3 1.2 1.4
?S avg 1.9 1.8 1.8 1.7 1.0 1.1 1.2 1.0 1.4
Table 8: Tree topology metrics for scientific data and novels.
topology. Figure 1 shows three different parse
trees to accompany our discussion.10 Notice
that sentence (1) is a loose sentence, and sen-
tence (2) is periodic. In general, loose sentences
grow deep and unbalanced, while periodic sen-
tences are relatively more balanced and wider.
For a tree t rooted at NR with a height n, let
T be the set of leaf nodes, and let F be the set
of furcation nodes, and let ?(Ni, Nj) denote the
length of the shortest path from Ni to Nj . In-
spired by the work of Shao (1990), we analyze
tree topology with the following four measure-
ments:
? Leaf height (hT = {hTi , Ni ? T }), where
hTi = ?(Ni, NR) Ni ? T . For instance, the
leaf height of ?free? of Tree (2) in Fig. 1
is 6.
? Furcation height (hF = {hFi , Ni ? F}),
where hFi is the maximum leaf height within
the subtree rooted at Ni. In Figure 1, for
example, the furcation height of the VP in
Tree (2) (marked in triangle) is 3.
? Level width (wL = {wl, 1 ? l ? n}),
where wl = |{Ni : ?(Ni, NR) = l}|. E.g., w4
of Tree (1) in Figure 1 is 6.
10Example sentences are taken from Lin (1997), Joshi
(1992), and Lin (1995).
? Horizontal ?H = {?Hi , Ni ? F} , and
Vertical Imbalance ?S = {?Si , Ni ? F}.
Let C be the set of child nodes of Nk. If
|C| ? 2, then
?Hk =
?
?
?
? 1
n
|C|?
i=1
(hFi ?H)
2
where H = 1|C|
?|C|
i=1 h
F
i . Similarly,
?Sk =
?
?
?
? 1
n
|C|?
i=1
(s(Ni)? S)2
where S = 1|C|
?|C|
i=1 s(Ni) and s(Ni) is the
number of leaf nodes of tree rooted at Ni.
As shown in Figure 1, the imbalance of the
internal node VP in Tree (2) (marked in
triangle) is 0.5 horizontally, and 0.5 verti-
cally.
To give an intuition on the relation between
these measurements and different tree struc-
tures, Table 9 provides the measurements of the
three trees shown in Figure 1.
Note that all three sentences are of similar
length but show different tree structures. Tree
(1) and Tree (2) differ in that Tree (1) is
highly unbalanced and grows deep, while Tree
1527
Figure 1: Parsed trees
Metrics Tree (1) Tree (2) Tree (3)
# of tokens 15 13 13
maxi {hTi } 11 6 6
maxi {wLi } 6 9 9
maxi {?Hi } 4.97 1.6 1.7
maxi {?Si } 4 1.5 4.7
Table 9: Tree Topology Statistics for Figure 1.
(2) is much better balanced and grows shorter
but wider. Comparing Tree (2) and Tree (3),
they have the same max Leaf height, Level
width, and Horizontal Imbalance, but the
latter has bigger Vertical Imbalance, which
quantifies the imbalance in terms of the text
span covered by subtrees.
We provide these topological metrics for au-
thors from both datasets in Table 8.
6 Experiments & Evaluation
In our experiments, we utilize a set of features
motivated by PCFG trees. These consist of sim-
ple production rules and other syntactic features
based on tree-traversals. Table 10 describes
these features with examples from Tree (2), us-
ing the portion marked by the triangle.
These sets of production rules and syntax fea-
tures are used to build SVM classifiers using LI-
BLINEAR (Fan et al 2008), wherein all fea-
ture values are encoded as term-frequencies nor-
malized by document size. We run 5-fold cross-
validation with training and testing split first as
80%/20%, and then as 20%/80%.
We would like to point out that the latter con-
figuration is of high practical importance in au-
thorship attribution, since we may not always
have sufficient training data in realistic situa-
tions, e.g., forensics (Luyckx and Daelemans,
2008).
Lexical tokens provide strong clues by creat-
ing features that are specific to each author: re-
search topics in the scientific data, and proper
nouns such as character names in novels. To
lessen such topical bias, we lemmatize and rank
words according to their frequency (in the entire
dataset), and then consider the top 2,000 words
only. Leaf-node productions with words outside
this set are disregarded.
Our experimental results (Tables 11 & 12)
show that not only do deep syntactic features
perform well on their own, but they also signif-
icantly improve over lexical features. We also
show that adding the style11 features further
improves performance.
1528
Features
pr Rules excluding terminal productions.
E.g., VP ? VBG NP
synv Traversal from a non-leaf node to its grand-
parent (embedded rising).
E.g., VP?S ? PP
synh Left-to-right traversal in the set of all non-
leaf children of a node.
E.g., VBG ? NP (for node VP)
synv+h synv ? synh
syn0 No tree traversal. Feature comprises inte-
rior nodes only.
syn? Union of all edges to child nodes, except
when child is a leaf node.
E.g., {VP ? VBG, VP ? NP}
synl syn? ? { edge to parent node}
style11 The set of 11 extra stylistic features. 6 val-
ues from the distribution of sentence types
(Section 3), and 5 topological metrics (Sec-
tion 5) characterizing the height, width and
imbalance of a tree.
Variations
p?r Each production rule is augmented with the
grandparent node.
? Terminal (leaf) nodes are included.
Table 10: Features and their lexico-syntactic varia-
tions. Illustration: p?r? denotes the set of production
rules pr (including terminal productions) that are
augmented with their grandparent nodes.
To quantify the amount of authorship infor-
mation carried in the set style11, we experi-
ment with a SVM classifier using only 11 fea-
tures (one for each metric), and achieve accu-
racy of 42.0% and 52.0% with scientific data
and novels, respectively. Given that a random-
guess baseline would achieve only 10% and 20%
(resp.), and that the classification is based on
just 11 features, this experiment demonstrates
how effectively the tree topology statistics cap-
ture idiolects. In general, lexicalized features
yield higher performance even after removing
topical words. This is expected since tokens
such as function words play an important role
in determining authorship (e.g., Mosteller and
Wallace (1984), Garcia and Martin (2007), Arg-
amon et al(2007)).
A more important observation, however, is
that even after removing the leaf production
rules, accuracy as high as 93% (scientific) and
92.2% (novels) are obtained using syntactic fea-
Features Scientific Novels
+style11 +style11
style11 20.6 ? 43.1 ?
Unigram 56.9 ? 69.3 ?
synh 53.7 53.7 68.3 67.9
syn0 22.9 31.1 57.8 62.5
syn? 43.4 44.0 63.6 65.7
synl 51.1 51.7 71.3 72.8
synv+h 54.0 55.7 72.0 73.2
syn?h 63.1 64.0 72,1 73.2
syn?0 56.6 56.0 73.1 74.1
syn?? 56.3 57.2 74.0 74.9
syn?l 64.6 65.4 74.9 75.3
syn?v+h 64.0 67.7 74.0 74.7
pr 50.3 53.4 67.0 66.7
p?r 59.1 60.6 69.7 68.7
pr? 63.7 65.1 71.5 73.2
p?r? 66.3 69.4 73.6 74.9
Table 11: Authorship attribution with 20% train-
ing data. Improvement with addition of style11
shown in bold.
tures, which demonstrates that there are syn-
tactic patterns unique to each author. Also no-
tice that using only production rules, we achieve
higher accuracy in novels (90.1%), but the ad-
dition of style11 features yields better results
with scientific data (93.0%).
Using different amounts of training data pro-
vides insight about the influence of lexical clues.
In the scientific dataset, increasing the amount
of training data decreases the average perfor-
mance difference between lexicalized and unlex-
icalized features: 13.5% to 11.6%. In novels,
however, we see the opposite trend: 6.1% in-
creases to 8.1%.
We further observe that with scientific data,
increasing the amount of training data improves
the average performance across all unlexicalized
feature-sets from 50.0% to 82.9%, an improve-
ment of 32.8%. For novels, the corresponding
improvement is small in comparison: 17.0%.
This difference is expected. While authors
such as Dickens or Hardy have their unique writ-
ing styles that a classifier can learn based on few
documents, capturing idiolects in the more rigid
domain of scientific writing is far from obvious
with little training data.
1529
Features Scientific Novels
+style11 +style11
style11 42.0 ? 52.0 ?
Unigram 88.0 ? 92.7 ?
synh 85.0 85.0 87.6 88.9
syn0 40.0 53.0 66.4 72.3
syn? 78.0 82.0 80.3 82.3
synl 85.0 92.0 89.3 92.2
synv+h 89.0 93.0 90.1 91.2
syn?h 93.0 93.0 93.7 93.9
syn?0 92.0 94.0 92.1 93.2
syn?? 93.0 94.0 93.4 94.5
syn?l 93.0 95.0 94.9 95.2
syn?v+h 94.0 96.0 94.7 94.8
pr 85.0 86.0 86.7 86.7
p?r 87.0 89.0 88.2 89.3
pr? 93.0 94.0 92.1 93.2
p?r? 94.0 95.0 94.5 95.1
Table 12: Authorship attribution with 80% train-
ing data.
Turning to lexicalized features, we note that
with more training data, lexical cues perform
better in scientific domain than in novels. With
80% data used for training, the average per-
formance of lexicalized feature-sets with science
data is 94.4%, and slightly lower at 94.3% for
novels. With less training data, however, these
figures are 63.5% and 74.3% respectively.
Finally, we point out that adding the style
features derived from sentence types and tree
topologies almost always improves the perfor-
mance. In scientific data, syn?v+h with style11
features shows the best performance (96%),
while syn?l yields the best results for novels
(95.2%). For unlexicalized features, adding
style11 to synv+h and synl yields respective
improvements of 4.0% and 2.9% in the two
datasets.
7 Related Work
There are several hurdles in authorship attribu-
tion. First and foremost, writing style is ex-
tremely domain-dependent. Much of previous
research has focused on several domains of writ-
ing, such as informal modern writing in blogs
and online messages (Zheng et al 2006), rela-
tively formal contemporary texts such as news
articles (Raghavan et al 2010), or classical lit-
erature like novels and proses (e.g., (Burrows,
2002), (Hoover, 2004)).
The nature of these features have also var-
ied considerably. Character level n-grams have
been used by several researchers; most notably
by Peng et al(2003), by Houvardas and Sta-
matatos (2006) for feature selection, and by Sta-
matatos (2006) in ensemble learning. Keselj et
al. (2003) employed frequency measures on n-
grams for authorship attribution.
Others, such as Zhao and Zobel (2005), Arg-
amon and Levitan (2004), Garcia and Martin
(2007), have used word-level approaches instead,
incorporating the differential use of function
words by authors.
More sophisticated linguistic cues have been
explored as well: parts-of-speech n-grams
(Diederich et al 2003), word-level statistics to-
gether with POS-sequences (Luyckx and Daele-
mans, 2008), syntactic labels from partial pars-
ing (Hirst and Feiguina, 2007), etc. The use
of syntactic features from parse trees in au-
thorship attribution was initiated by Baayen et
al. (1996), and more recently, Raghavan et al
(2010) have directly employed PCFG language
models in this area.
Syntactic features from PCFG parse trees
have also been used for gender attribution
(Sarawgi et al 2011), genre identification (Sta-
matatos et al 2000), native language identifi-
cation (Wong and Dras, 2011) and readability
assessment (Pitler and Nenkova, 2008). The
primary focus of most previous research, how-
ever, was to attain better classification accuracy,
rather than providing linguistic interpretations
of individual authorship and their stylistic ele-
ments.
Our work is the first to attempt authorship
attribution of scientific papers, a contemporary
domain where language is very formal, and the
stylistic variations have limited scope. In ad-
dition to exploring this new domain, we also
present a comparative study expounding the
role of syntactic features for authorship attri-
bution in classical literature. Furthermore, our
work is also the first to utilize tree topological
1530
features (Chan et al 2010) in the context of
stylometric analysis.
8 Conclusion
In this paper, we have presented a comprehen-
sive exploration of syntactic elements in writing
styles, with particular emphasis on interpretable
characterization of stylistic elements, thus dis-
tinguishing our work from other recent work on
syntactic stylometric analysis. Our analytical
study provides novel statistically supported in-
sights into stylistic elements that have not been
computationally analyzed in previous literature.
In the future, we plan to investigate the use of
syntactic feature generators for text categoriza-
tion (e.g., Collins and Duffy (2002), Moschitti
(2008), Pighin and Moschitti (2009)) for stylom-
etry analysis.
Acknowledgments Yejin Choi is partially
supported by the Stony Brook University Office
of the Vice President for Research. We thank
reviewers for many insightful and helpful com-
ments.
References
Shlomo Argamon and Shlomo Levitan. 2004. Mea-
suring the usefulness of function words for author-
ship attribution. Literary and Linguistic Comput-
ing, pages 1?3.
Shlomo Argamon, Casey Whitelaw, Paul Chase,
Sobhan Raj Hota, Navendu Garg, and Shlomo
Levitan. 2007. Stylistic text classification using
functional lexical features: Research articles. J.
Am. Soc. Inf. Sci. Technol., 58(6):802?822.
H. Baayen, H. Van Halteren, and F. Tweedie. 1996.
Outside the cave of shadows: Using syntactic an-
notation to enhance authorship attribution. Lit-
erary and Linguistic Computing, 11(3):121.
H. Baayen, H. van Halteren, A. Neijt, and
F. Tweedie. 2002. An experiment in authorship
attribution. In 6th JADT. Citeseer.
A. Bain. 1887. English Composition and Rhetoric:
Intellectual elements of style. D. Appleton and
company.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph,
M.Y. Kan, D. Lee, B. Powley, D.R. Radev, and
Y.F. Tan. 2008. The acl anthology reference
corpus: A reference dataset for bibliographic re-
search in computational linguistics. In Proc.
of the 6th International Conference on Language
Resources and Evaluation Conference (LREC08),
pages 1755?1759.
J. Burrows. 2002. Delta: A measure of stylistic dif-
ference and a guide to likely authorship. Literary
and Linguistic Computing, 17(3):267?287.
Samuel W. K. Chan, Lawrence Y. L. Cheung, and
Mickey W. C. Chong. 2010. Tree topological fea-
tures for unlexicalized parsing. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics: Posters, COLING ?10, pages
117?125, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: kernels over
discrete structures, and the voted perceptron. In
Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, ACL ?02,
pages 263?270, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
J. Diederich, J. Kindermann, E. Leopold, and
G. Paass. 2003. Authorship attribution with
support vector machines. Applied Intelligence,
19(1):109?123.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?
1874.
Antonion Miranda Garcia and Javier Calle Mar-
tin. 2007. Function words in authorship attribu-
tion studies. Literary and Linguistic Computing,
22(1):49?66.
Graeme Hirst and Olga Feiguina. 2007. Bigrams of
syntactic labels for authorship discrimination of
short texts. Literary and Linguistic Computing,
22(4):405?417.
D. L. Hoover. 2004. Testing burrow?s delta. Literary
and Linguistic Computing, 19(4):453?475.
J. Houvardas and E. Stamatatos. 2006. N-gram fea-
ture selection for author identification. In Proc.
of the 12th International Conference on Artificial
Intelligence: Methodology, Systems and Applica-
tions, volume 4183 of LNCS, pages 77?86, Varna,
Bulgaria. Springer.
Aravind K. Joshi. 1992. Statistical language mod-
eling. In Proceedings of a Workshop Held at Har-
riman, New York, February 23-26, 1992. Associa-
tion for Computational Linguistics.
S. Kemper. 1987. Life-span changes in syntactic
complexity. Journal of gerontology, 42(3):323.
Vlado Keselj, Fuchun Peng, Nick Cercone, and
Calvin Thomas. 2003. N-gram-based author pro-
files for authorship attribution. In Proc. of the
1531
Pacific Association for Computational Linguistics,
pages 255?264.
M. Koppel and J. Schler. 2003. Exploiting stylistic
idiosyncrasies for authorship attribution. In Pro-
ceedings of IJCAI, volume 3, pages 69?72. Cite-
seer.
D. Lin. 1995. University of manitoba: descrip-
tion of the pie system used for muc-6. In Pro-
ceedings of the 6th conference on Message under-
standing, pages 113?126. Association for Compu-
tational Linguistics.
D. Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Pro-
ceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics and Eighth
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 64?71.
Association for Computational Linguistics.
Kim Luyckx and Walter Daelemans. 2008. Author-
ship attribution and verification with many au-
thors and limited data. In COLING ?08, pages
513?520.
T.C. Mendenhall. 1887. The characteristic curves of
composition. Science, ns-9(214S):237?246.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization.
In Proceedings of the 17th ACM conference on In-
formation and knowledge management, CIKM ?08,
pages 253?262, New York, NY, USA. ACM.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case
of the Federalist Papers. Springer-Verlag.
Fuchun Peng, Dale Schuurmans, Shaojun Wang, and
Vlado Keselj. 2003. Language independent au-
thorship attribution using character level language
models. In Proceedings of the tenth conference on
European chapter of the Association for Compu-
tational Linguistics - Volume 1, EACL ?03, pages
267?274, Stroudsburg, PA, USA. Association for
Computational Linguistics.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411.
Daniele Pighin and Alessandro Moschitti. 2009. Re-
verse engineering of tree kernel feature spaces. In
Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing: Vol-
ume 1 - Volume 1, EMNLP ?09, pages 111?120,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: a unified framework for predicting
text quality. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 186?195, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Arthus Quinn. 1995. Figures of Speech: 60 Ways To
Turn A Phrase. Routledge.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using
probabilistic context-free grammars. In Proceed-
ings of the ACL 2010 Conference Short Papers,
pages 38?42, Uppsala, Sweden. Association for
Computational Linguistics.
Ruchita Sarawgi, Kailash Gajulapalli, and Yejin
Choi. 2011. Gender attribution: tracing stylo-
metric evidence beyond topic and genre. In Pro-
ceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning, CoNLL ?11,
pages 78?86, Stroudsburg, PA, USA. Association
for Computational Linguistics.
K.T. Shao. 1990. Tree balance. Systematic Biology,
39(3):266.
Efstathios Stamatatos, George Kokkinakis, and
Nikos Fakotakis. 2000. Automatic text catego-
rization in terms of genre and author. Comput.
Linguist., 26(4):471?495.
E. Stamatatos, N. Fakotakis, and G. Kokkinakis.
2001. Computer-based authorship attribution
without lexical measures. Computers and the Hu-
manities, 35(2):193?214.
E. Stamatatos. 2006. Ensemble-based author iden-
tification using character n-grams. ReCALL, page
4146.
W. Strunk and E.B. White. 2008. The elements of
style. Penguin Group USA.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identifica-
tion. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 1600?1610, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ying Zhao and Justin Zobel. 2005. Effective
and scalable authorship attribution using func-
tion words. In Proceedings of the Second Asia
conference on Asia Information Retrieval Technol-
ogy, AIRS?05, pages 174?189, Berlin, Heidelberg.
Springer-Verlag.
Y. Zhao and J. Zobel. 2007. Searching with style:
Authorship attribution in classic literature. In
Proceedings of the thirtieth Australasian confer-
ence on Computer science-Volume 62, pages 59?
68. Australian Computer Society, Inc.
Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan
Huang. 2006. A framework for authorship identi-
fication of online messages: Writing-style features
1532
and classification techniques. J. Am. Soc. Inf. Sci.
Technol., 57(3):378?393.
1533
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1753?1764,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Success with Style: Using Writing Style to Predict the Success of Novels
Vikas Ganjigunte Ashok Song Feng Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
vganjiguntea, songfeng, ychoi@cs.stonybrook.edu
Abstract
Predicting the success of literary works is a
curious question among publishers and aspir-
ing writers alike. We examine the quantitative
connection, if any, between writing style and
successful literature. Based on novels over
several different genres, we probe the predic-
tive power of statistical stylometry in discrim-
inating successful literary works, and identify
characteristic stylistic elements that are more
prominent in successful writings. Our study
reports for the first time that statistical stylom-
etry can be surprisingly effective in discrim-
inating highly successful literature from less
successful counterpart, achieving accuracy up
to 84%. Closer analyses lead to several new
insights into characteristics of the writing style
in successful literature, including findings that
are contrary to the conventional wisdom with
respect to good writing style and readability.
1 Introduction
Predicting the success of novels is a curious ques-
tion among publishers, professional book reviewers,
aspiring and even expert writers alike. There are po-
tentially many influencing factors, some of which
concern the intrinsic content and quality of the book,
such as interestingness, novelty, style of writing, and
engaging storyline, but external factors such as so-
cial context and even luck can play a role. As a re-
sult, recognizing successful literary work is a hard
task even for experts working in the publication in-
dustries. Indeed, even some of the best sellers and
award winners can go through several rejections be-
fore they are picked up by a publisher.1
Perhaps due to its obvious complexity of the prob-
lem, there has been little previous work that attempts
to build statistical models that predict the success of
literary works based on their intrinsic content and
quality. Some previous studies do touch on the no-
tion of stylistic aspects in successful literature, e.g.,
extensive studies in Literature discuss literary styles
of significant authors (e.g., Ellega?rd (1962), Mc-
Gann (1998)), while others consider content char-
acteristics such as plots, characteristics of charac-
ters, action, emotion, genre, cast, of the best-selling
novels and blockbuster movies (e.g., Harvey (1953),
Hall (2012), Yun (2011)).
All these studies however, are qualitative in na-
ture, as they rely on the knowledge and insights of
human experts on literature. To our knowledge, no
prior work has undertaken a systematic quantitative
investigation on the overarching characterization of
the writing style in successful literature. In consid-
eration of widely different styles of authorship (e.g.,
Escalante et al (2011), Peng et al (2003), Argamon
et al (2003)), it is not even readily clear whether
there might be common stylistic elements that help
discriminating highly successful ones from less suc-
cessful counterpart.
In this work, we present the first study that in-
vestigates this unstudied and unexpected connection
between stylistic elements and the literary success.
The key findings of our research reveal that there
exists distinct linguistic patterns shared among suc-
1E.g., Paul Harding?s ?Tinkers? that won 2010 Pulitzer Prize
for Fiction and J. K. Rowling?s ?Harry Potter and the Philoso-
pher?s Stone? that sold over 450 million copies.
1753
cessful literature, at least within the same genre,
making it possible to build a model with surprisingly
high accuracy (up to 84%) in predicting the success
of a novel. This result is surprising for two reasons.
First, we tackle the hard task of predicting the suc-
cess of novels written by previously unseen authors,
avoiding incidental learning of authorship signature,
since previous research demonstrated that one can
achieve very high accuracy in authorship attribution
(as high as 96% in some experimental setup) (e.g.,
Raghavan et al (2010), Feng et al (2012)). Sec-
ond, we aim to discriminate highly successful nov-
els from less successful, but nonetheless published
books written by professional writers, which are un-
doubtedly of higher quality than average writings.
It is important to note that the task we tackle here
is much harder than discriminating highly success-
ful works from those that have not even passed the
scrutinizing eyes of publishers.
In order to quantify the success of literary works,
and to obtain corresponding gold standard labels,
one needs to first define ?success?. For practi-
cal convenience, we largely rely on the download
counts available at Project Gutenberg as a surrogate
to quantify the success of novels. For a small num-
ber of novels however, we also consider award re-
cipients (e.g., Pulitzer, Nobel), and Amazon?s sales
records to define a novel?s success. We also ex-
tend our empirical study to movie scripts, where we
quantify the success of movies based on the aver-
age review scores at imdb.com. We leave analysis
based on other measures of literary success as future
research.
In this study, we do not attempt to separate out
success based on literary quality (award winners)
from success based on popularity (commercial hit,
often in spite of bad literary quality), mainly because
it is not practically easy to determine whether the
high download counts are due to only one reason or
the other. We expect that in many cases, the two
different aspects of success are likely to coincide,
however. In the case of the corpus obtained from
Project Gutenberg, where most of our experiments
are conducted, we expect that the download counts
are more indicative of success based on the literary
quality (which then may have resulted in popularity)
rather than popularity without quality.
We examine several genres in fiction and movie
GENRE #BOOKS ?? ?+
Adventure 409 17 100
Detective / Mystery 374 25 90
Fiction 1148 7 125
Historical Fiction 374 25 115
Love Stories 342 16 85
Poetry 580 9 70
Science Fiction 902 30 100
Short Stories 1117 9 224
Table 1: # of books available per genre at Gutenberg with
download thresholds used to define more successful (?
?+) and less successful (? ??) classes.
scripts, e.g., adventure stories, mystery, fiction, his-
torical fiction, sci-fi, short stories, as well as poetry,
and present systematic analyses based on lexical and
syntactic features which have been known to be ef-
fective in a variety of NLP tasks ranging from au-
thorship attribution (e.g., Raghavan et al (2010)),
genre detection (e.g., Rayson et al (2001), Douglas
and Broussard (2000)), gender identification (e.g.,
Sarawgi et al (2011)) and native language detection
(e.g., Wong and Dras (2011)).
Our empirical results demonstrate that (1) statis-
tical stylometry can be surprisingly effective in dis-
criminating successful literature, achieving accuracy
up to 84%, (2) some elements of successful styles
are genre-dependent while others are more univer-
sal. In addition, this research results in (3) find-
ings that are somewhat contrary to the conventional
wisdom with respect to the connection between suc-
cessful writing styles and readability, (4) interesting
correlations between sentiment / connotation and the
literary success, and finally, (5) comparative insights
between fiction and nonfiction with respect to the
successful writing style.
2 Dataset Construction
For our experiments, we procure novels from project
Gutenberg2. Project Gutenberg houses over 40, 000
books available for free download in electronic for-
mat and provides a catalog containing brief descrip-
tions (title, author, genre, language, download count,
etc.) of these books. We experiment with genres in
Table 1, which have sufficient number of books al-
lowing us to construct reasonably sized datasets.
We use the download counts in Gutenberg-catalog
2http://www.gutenberg.org/
1754
Figure 1: Differences in POS tag distribution between more successful and less successful books across different
genres. Negative (positive) value indicates higher percentage in less (more) successful class.
as a surrogate to measure the degree of success of
novels. For each genre, we determine a lower bound
(?+) and an upper bound (??) of download counts as
shown in Table 1 to categorize the available books
as more successful and less successful respectively.
These thresholds are set to obtain at least 50 books
for each class, and for each genre. To balance the
data, for each genre, we construct a dataset of 100
novels (50 per class).
We make sure that no single author has more than
2 books in the resulting dataset, and in the major-
ity of the cases, only one book has been taken from
each author.3 Furthermore, we make sure that the
books from the same author do not show up in both
training and test data. These constraints make sure
that we learn general linguistic patterns of success-
ful novels, rather than a particular writing style of a
few successful authors.
3 Methodology
In what follows, we describe five different aspects of
linguistic styles we measure quantitatively. The first
three correspond to the features that have been fre-
quently utilized in previous studies in related tasks,
3The complete list of novels used for each genre in our
dataset is available at http://www.cs.stonybrook.
edu/?ychoi/successwithstyle/
e.g., genre detection (e.g., Kessler et al (1997))
and authorship attribution (e.g., Stamatatos (2009)),
while the last two are newly explored in this work.
I. Lexical Choices: unigrams and bigrams.
II. Distribution of Word Categories: Many pre-
vious studies have shown that the distribution of
part-of-speech (POS) tags alone can reveal surpris-
ing insights on genre and authorship (e.g., Koppel
and Schler (2003)), hence we examine their distri-
butions with respect to the success of literary works.
III. Distribution of Grammar Rules: Recent
studies reported that features based on CFG rules are
helpful in authorship attribution (e.g., Raghavan et
al. (2010), Feng et al (2012)). We experiment with
four different encodings of production rules:
? ?: lexicalized production rules (all production
rules, including those with terminals)
? ?G: lexicalized production rules prepended
with the grandparent node.
? ?: unlexicalized production rules (all produc-
tion rules except those with terminals).
? ?G: unlexicalized production rules prepended
with the grandparent node.
1755
FEATURE
GENRE
Avg
Avg w/o
HistoryAdven Myster Fiction Histor Love Poetr Sci-fi Short
POS 74.0 63.9 72.0 47.0 65.9 63.0 63.0 67.0 64.5 66.9
Unigram 84.0 73.0 75.0 60.0 82.0 71.0 61.0 57.0 70.3 71.8
Bigram 81.0 73.0 75.0 51.0 72.0 70.0 59.0 57.0 67.2 69.5
? 73.0 71.0 75.0 54.0 78.0 74.0 71.0 77.0 71.6 74.1
?G 75.0 74.0 75.0 58.0 81.0 72.0 76.0 77.0 73.5 75.7
? 72.0 70.0 65.0 53.0 70.0 66.0 64.0 71 66.3 68.2
?G 72.0 69.0 74.0 55.0 75.0 69.0 67.0 73.0 69.2 71.2
?+Unigram 79.0 73.0 73.0 59.0 80.0 73.0 71.0 73.0 72.6 74.5
?G+Unigram 80.0 74.0 74.0 56.0 82.0 72.0 73.0 72.0 72.8 75.2
?+Unigram 82.0 72.0 73.0 56.0 81.0 69.0 62.0 59.0 69.2 71.1
?G+Unigram 80.0 73.0 74.0 58.0 82.0 70.0 65.0 58.0 70 71.7
PHR 74.0 65.0 65.0 56.0 64.0 62.0 69.0 71.0 65.7 67.1
PHR+CLS 75.0 69.0 64.0 61.0 59.0 62.0 69.0 67.0 65.7 66.4
PHR+Unigram 80.0 74.0 71.0 56.0 79.0 73.0 67.0 66.0 70.7 72.8
PHR+CLS+Unigram 80.0 75.0 71.0 56.0 79.0 73.0 66.0 66.0 70.7 72.8
Table 2: Classification results in accuracy (%).
IV. Distribution of Constituents: PCFG gram-
mar rules are overly specific to draw a big picture
on the distribution of large, recursive syntactic units.
We hypothesize that the distribution of constituents
can serve this purpose, and that it will reveal inter-
esting and more interpretable insights into writing
styles in highly successful literature. Despite its rel-
ative simplicity, we are not aware of previous work
that looks at the distribution of constituents directly.
In particular, we are interested in examining the dis-
tribution of phrasal and/or clausal tags as follows:
(i) Phrasal tag percent (PHR) - percentage distribu-
tion of phrasal tags.4 (ii) Clausal tag percent (CLS)
- percentage distribution of clausal tags.
V. Distribution of Sentiment and Connotation:
Finally, we examine whether the distribution of sen-
timent and connotation words, and their polarity, has
any correlation with respect to the success of literary
works. We are not aware of any previous work that
looks into this connection.
4 Prediction Performance
We use LibLinear SVM (Fan et al, 2008) with
L2 tuned over training data, and all performance is
based on 5-fold cross validation. We take 1000 sen-
tences from the beginning of each book. POS fea-
tures are encoded as unit normalized frequency and
all other features are encoded as tf-idf.5
4The percentage of any phrasal tag is the count of occurrence
of that tag over the sum of counts of all phrasal tags.
5POS tags are obtained using the Stanford POS tagger
(Toutanova and Manning, 2000), and parse trees are based on
the Stanford parser (Klein and Manning, 2003).
Prediction Results Table 2 shows the classifica-
tion results. The best performance reaches as high
as 84% in accuracy. In fact, in all genres except
for history, the best performance is at least 74%,
if not higher. Another notable observation is that
even in the poetry genre, which is not prose, the ac-
curacy gets as high as 74%. This level of perfor-
mance is not entirely anticipated, given that (1) the
test data consists of books written only by previously
unseen authors, and (2) each author has widely dif-
ferent writing style, and (3) we do not have training
data at scale, and (4) we aim to tackle the hard task
of discriminating highly successful ones from less
successful, but nonetheless successful ones, as all of
them were, after all, good enough to be published.6
Prediction with Varying Thresholds of Down-
load Counts Before we proceed to comprehensive
analysis of writing style that are prominent in more
successful literature (?5), in Table 3, we present how
the prediction accuracy varies as we adjust the defi-
nition of more-successful and less-successful litera-
ture, by gradually increasing (decreasing) the thresh-
old ?? (?+). As we reduce the gap between ?? and
?+, the performance decreases, which shows that in-
deed there are notable statistical differences in lin-
guistic patterns between novels with high and low
download counts, and the stylistic difference mono-
tonically increases (thereby higher prediction accu-
racy) as we increase the gap between two classes.
6In our pilot study, we also experimented with the binary
classification task of discriminating highly successful ones from
those that are not even published (unpublished online novels),
and it was a much easier task as expected.
1756
?? ?+ ACCURACY
17 100 84.0
25 90 78.4
35 80 77.6
45 70 76.4
55 60 73.5
Table 3: Accuracy (%) with varying thresholds of down-
load counts for ADVENTURE with unigram features.
This is particularly interesting as the size of training
data set is actually monotonically decreasing (mak-
ing it harder to achieve high accuracy) while we in-
crease the separation between ?? and ?+.
5 Analysis of Successful Writing Styles
5.1 Insights Based on Lexical Choices
It is apparent from Table 2 that unigram features
yield curiously high performance in many genres.
We therefore examine discriminative unigrams for
ADVENTURE, shown in Table 4. Interestingly, less
successful books rely on verbs that are explicitly de-
scriptive of actions and emotions (e.g., ?wanted?,
?took?, ?promised?, ?cried?, ?cheered?, etc.), while
more successful books favor verbs that describe
thought-processing (e.g., ?recognized?, ?remem-
bered?), and verbs that serve the purpose of quotes
and reports (e.g,. ?say?). Also, more successful
books use discourse connectives and prepositions
more frequently, while less successful books rely
more on topical words that could be almost cliche?,
e.g., ?love?, typical locations, and involve more ex-
treme (e.g., ?breathless?) and negative words (e.g.,
?risk?).
5.2 Distribution of Sentiment & Connotation
We also determine the distribution of sentiment and
connotation words separately for each class (Table
5) to check if there exists a connection with respect
to successful writing styles.7 We first compare dis-
tribution of sentiment and connotation for the entire
words. As can be seen in Table 5 ? Top, there are
not notable differences. However, when we compare
distribution only with respect to discriminative uni-
grams only (i.e., features with non-zero weights), as
7We use MPQA subjectivity lexicon (Wilson et al, 2005)
and connotation lexicon (Feng et al, 2013) for determining sen-
timent and connotation of words respectively.
Less Successful
CATEGORY UNIGRAMS
Negative
never, risk, worse, slaves, hard,
murdered, bruised, heavy, prison,
Body Parts face, arm, body, skins
Location
room, beach, bay, hills,
avenue, boat, door
Emotional / want, went, took, promise,
Action Verbs cry, shout, jump, glare, urge
Extreme Words
never, very, breathless, sacred
slightest, absolutely, perfectly
Love Related desires, affairs
More Successful
CATEGORY UNIGRAMS
Negation not
Report / Quote said, words, says
Self Reference I, me , my
Connectives
and, which, though, that,
as, after, but, where, what,
whom, since, whenever
Prepositions up, into, out, after, in, within
Thinking Verbs recognized, remembered
Table 4: Discriminative unigrams for ADVENTURE.
shown in Table 5 ? Bottom, we find substantial dif-
ferences in all genres. In particular, discriminative
unigrams that characterize less successful novels in-
volve significantly more sentiment-laden words.
5.3 Distribution of Word Categories
Summarized analysis of POS distribution across all
genres is reported in Table 6. It can be seen that
prepositions, nouns, pronouns, determiners and ad-
jectives are predictive of highly successful books
whereas less successful books are characterized by
higher percentage of verbs, adverbs, and foreign
words. Per genre distributions of POS tags are vi-
sualized in Figure 1. Interestingly, some POS tags
show almost universal patterns (e.g., prepositions
(IN), NNP, WP, VB), while others are more genre-
specific.
In Relation to Journalism Style The work of
Douglas and Broussard (2000) reveals that informa-
tive writing (journalism) involves increased use of
nouns, prepositions, determiners and coordinating
conjunctions whereas imaginative writing (novels)
involves more use of verbs and adverbs, as has been
also confirmed by Rayson et al (2001). Compar-
ing their findings with Table 6, we find that highly
1757
Adven Myster Fiction Histor Love Poetr Sci-fi Short
- + - + - + - + - + - + - + - +
+ve S 4.7 4.9 4.8 4.6 5.6 4.9 5.0 5.1 5.5 5.1 6.3 5.7 4.1 3.7 4.7 4.8
-ve S 4.0 4.0 4.0 4.0 4.3 4.2 4.2 4.2 4.1 4.2 4.3 4.3 2.9 2.9 3.8 4.0
Tot S 8.7 8.9 8.9 8.7 9.9 9.0 9.2 9.3 9.6 9.3 10.6 9.9 7.0 6.7 8.5 8.9
+ve C 22.3 22.5 22.3 22.5 23.7 23.0 23.0 23.2 23.34 23.3 23.8 22.9 21.2 20.6 22.6 22.7
-ve C 19.4 19.6 19.8 19.8 20.3 19.5 19.2 19.4 20.2 20.4 17.7 17.4 16.6 16.7 18.3 18.9
Total C 41.7 42.1 42.1 42.3 44.0 42.5 42.3 42.6 43.5 43.7 41.5 40.3 37.9 37.3 41.0 41.6
+ve S 3.5 1.8 4.1 2.0 3.7 1.4 3.0 1.0 3.4 1.3 3.9 2.0 7.3 5.9 5.1 2.7
-ve S 5.5 3.4 6.3 3.6 5.5 2.9 4.7 1.9 5.1 2.6 5.8 3.3 9.0 8.0 7.3 4.8
Total S 9.1 5.2 10.4 5.6 9.2 4.3 7.7 3.0 8.5 3.9 9.7 5.2 16.3 13.9 12.4 7.5
+ve C 12.9 8.9 14.3 9.8 12.9 8.5 11.5 6.2 12.0 7.7 14.0 9.6 19.6 19.2 16.5 11.9
-ve C 14.1 9.8 15.2 10.9 13.7 9.9 12.4 7.0 12.9 8.5 14.3 10.3 20.0 19.7 17.0 13.3
Total C 27.0 18.7 29.5 20.7 26.6 18.4 23.9 13.2 24.87 16.1 28.3 19.8 39.7 38.9 33.5 25.2
Table 5: Top: Distribution of sentiment (connotation) among entire unigrams. Bottom: distribution of sentiment
(connotation) among discriminative unigrams. ?S? and ?C? stand for sentiment and connotation respectively.
More Successful
CATEGORY SUB-CATEGORY DIFF
Prepositions General 0.00592
Determiners General 0.00226
Nouns
Plural 0.00189
Proper (Singular) 0.00016
Coord. conj. General 0.00118
Numbers General 0.00102
Pronouns
Posesseive 0.00081
General WH 0.00042
Possessive WH 5.4E-05
Adjectives
General 0.00102
Superlative 0.00011
Less Successful
CATEGORY SUB-CATEGORY DIFF
Adverbs
General -0.00272
General WH -0.00028
Verbs
Base -0.00239
Non-3rd sing. present -0.00084
Past tense -0.00041
Past participle -0.00039
3rd person sing. present -0.00036
Modal -0.00091
Foreign General -0.00067
Symbols General -0.00018
Interjections General -0.00016
Table 6: Top discriminative POS tags.
successful books tend to bear closer resemblance to
informative articles.
5.4 Distribution of Constituents
It can be seen in Table 2 that deep syntactic fea-
tures expressed in terms of different encodings of
production rules consistently yield good perfor-
mance across almost all genres. Production rules
are overly specific to gain more generalized, in-
terpretable, high-level insights however (Feng et
al., 2012). Therefore, similarly as word categories
(POS), we consider the categories of nonterminal
nodes of the parse trees, in particular, phrasal and
clausal tags, as they represent the gist of constituent
structure that goes beyond shallow syntactic infor-
mation represented by POS.
Table 8 shows how the distribution of phrasal and
clausal tags differ for successful books when com-
puted over all genres. Positive (negative) DIFF val-
ues indicate that the corresponding tags are favored
in more successful (less successful) books when
counted across all genres. We also report the num-
ber of genres (#Genres) in which the individual dif-
ference is positive / negative.
In terms of phrasal tags, we find that more suc-
cessful books are composed of higher percentage of
PP, NP and wh-noun phrases (WHNP), whereas less
successful books are composed of higher percentage
of VP, adverb phrases (ADVP), interjections (INTJ)
and fragments (FRAG). Notice that this observation
is inline with our earlier findings with respect to the
distribution of POS.
In regard to clausal tags, more successful books
involve more clausal tags that are necessary for com-
plex sentence structure and inverted sentence struc-
ture (SBAR, SBARQ and SQ) whereas less success-
ful books rely more on simple sentence structure (S).
Figure 2 shows the visualization of the distribution
of these phrasal and clausal tags.
It is also worth to mention that phrasal and clausal
1758
Figure 2: Difference between phrasal and clausal tag percentage distributions of more successful and less successful
books across different genres. Specifically, we plot D??D+, where D+ is the phrasal tag distribution (in %) of more
successful books and D? is the phrasal tag distribution (in %) of less successful books.
READABILITY INDICES More Succ. Less Succ.
FOG index 9.88 9.80
Flesch index 87.48 87.64
Table 7: Readability: Lower FOG and higher Flesch in-
dicate higher readability (numbers in Boldface).
tags alone can yield classification performance that
are generally better than that of POS tags, in spite of
the very small feature set (26 tags in total). In fact,
constituent tags deliver the best performance in case
of historical fiction genre (Table 2).
Connection to Readability Pitler and Nenkova
(2008) provide comprehensive insights into assess-
ment of readability. In their work, among the most
discriminating features characterizing text with bet-
ter readability is increased use of verb phrases (VP).
Interestingly, contrary to the conventional wisdom ?
that readability is of desirable quality of good writ-
ings ? our findings in Table 2 suggest that the in-
creased use of VP correlates strongly with the writ-
ing style of the opposite spectrum of highly success-
ful novels.
As a secondary way of probing the connection be-
tween readability and the writing style of successful
literature, we also compute two different readabil-
ity measures that have been used widely in prior
literature (e.g., Sierra et al (1992), Blumenstock
(2008), Ali et al (2010)): (i) Flesch reading ease
score (Flesch, 1948), (ii) Gunning FOG index (Gun-
ning, 1968). The overall weighted average readabil-
ity scores are reported in Table 7. Again, we find that
less successful novels have higher readability com-
pared to more successful ones.
The work of Sawyer et al (2008) provides yet
another interesting contrasting point, where the au-
thors found that award winning academic papers in
marketing journals correlate strongly with increased
readability, characterized by higher percentage of
simple sentences. We conjecture that this opposite
trend is likely to be due to difference between fic-
tion and nonfiction, leaving further investigation as
future research.
In sum, our analysis reveals an intriguing and
unexpected observation on the connection between
readability and the literary success ? that they cor-
relate into the opposite directions. Surely our find-
ings only demonstrate correlation, not to be con-
1759
Phrasal + ? DIFF #+Gen/#
?
Gen
ADJP 0.030 0.031 -6E-4 5/3
ADJP 0.030 0.031 -6E-4 5/3
ADVP 0.052 0.054 -0.002 2/6
CONJP 3E-4 3E-4 2E-5 5/3
FRAG 0.008 0.008 -1E-4 2/6
LST 2E-4 1E-4 5E-5 6/2
NAC 9E-6 6E-6 3E-6 5/3
NP 0.459 0.453 0.005 6/2
NX 1E-4 1E-4 -4E-7 3/5
PP 0.122 0.117 0.005 7/1
PRN 0.005 0.004 2E-4 4/4
PRT 0.010 0.010 -5E-4 3/5
QP 0.001 0.001 7E-5 6/2
RRC 8E-5 8E-5 6E-6 6/2
UCP 8E-4 7E-4 1E-4 8/0
VP 0.292 0.300 -0.008 1/7
WHADJP 2E-4 2E-4 -5E-5 1/7
WHAVP 0 0 0 -
WHNP 0.013 0.012 0.001 8/0
WHPP 0.001 9E-4 1E-4 6/2
X 0.001 0.001 -4E-5 4/4
Clausal + ? DIFF +Gen/#
?
Gen
SBAR 0.166 0.164 0.002 4/4
SQ 0.020 0.018 0.002 7/1
SBARQ 0.014 0.013 0.001 7/1
SINV 0.018 0.018 -6E-5 5/3
S 0.781 0.785 -0.004 3/5
Table 8: Overall Phrasal / Clausal Tag Distribution and
analysis. All values are rounded to [3-5] decimal places.
fused as causation, between readability and literary
success. We conjecture that the conceptual complex-
ity of highly successful literary work might require
syntactic complexity that goes against readability.
6 Literature beyond Project Gutenberg
One might wonder how the prediction algorithms
trained on the dataset based on Project Gutenberg
might perform on books not included at Guten-
berg. This section attempts to address such a ques-
tion. Due to the limited availability of electronically
available books that are free of charge however, we
could not procure more than a handful of books.8
6.1 Highly Successful Books
First, we apply the classifiers trained on the Project
Gutenberg dataset (all genres merged) on a few ex-
tremely successful novels (Pulitzer prize, National
Award recipients, etc). Table 9 shows the results of
8We report our prediction results on all books beyond
Project Gutenberg of which we managed to get electronic
copies, i.e., the results in Table 9 are not cherry-picked.
MORE SUCCESSFUL
BOOK (Q) PDKL UPDKL Su S??
?Don Quixote? 0.139 0.152 + +
? Miguel De Cervantes
?Other Voices, Other Rooms? 0.014 0.010 + +
? Truman Capote
?The Fixer? 0.013 0.015 + +
? Bernard Malamud
?Robinson Crusoe? 0.042 0.051 + +
? Daniel Defoe
?The old man and the sea? 0.065 0.060 + +
? Ernest Hemingway
?A Tale of Two Cities? 0.027 0.030 + +
? Charles Dickens
?Independence Day? 0.031 0.026 + +
? Richard Ford
?Rabbit At Rest? 0.047 0.048 + +
? John Updike
?American Pastoral? 0.039 0.043 + +
? Philip Roth
?Dr Jackel and Mr. Hyde? 0.036 0.037 + +
? Robert Stevenson
LESS SUCCESSFUL
?The lost symbol? 0.046 0.042 - -
? Dan Brown
?The magic barrel? 0.0288 0.0284 + -
? Bernard Malamud
?Two Soldiers? 0.130 0.117 - +
? William Faulkner
?My life as a man? 0.046 0.052 - +
? Philip Roth
Table 9: Prediction on books beyond Gutenberg. Shaded
entries indicate incorrect predictions.
two classification options: (1) KL-divergence based,
and (2) unigram-feature based.
Although KL-divergence based prediction was
not part of the classifiers that we explored in the pre-
vious sections, we include it here mainly to provide
better insights as to which well-known books share
closer structural similarity to either more or less suc-
cessful writing style. As a probability model, we use
the distributions of phrasal tags, as those can give us
insights on deep syntactic structure while suppress-
ing potential noises due to topical variances. Table 9
shows symmetrised KL-divergence between each of
the previously unseen novels and the collection of
books from Gutenberg corresponding to more suc-
cessful (less successful) labels. For prediction, the
label with smaller KL is chosen.
Based only on the distribution of 26 phrasal tags,
the KL divergence classifier is able to make correct
1760
predictions on 7 out of 10 books, a surprisingly high
performance based on mere 26 features. Of course,
considering only the distribution of phrasal tags is
significantly less informed than considering numer-
ous other features that have shown substantially bet-
ter performance, e.g., unigrams and CFG rewrite
rules. Therefore, we also present the SVM classi-
fier trained on unigram features. It turns out uni-
gram features are powerful enough to make correct
predictions for all ten books in Table 9.
Hemingway and Minimalism It is good to think
about where and why KL-divergence-based ap-
proach fails. In fact, when we included Heming-
way?s The Old Man and the Sea into the test set, we
were expecting some level of confusions when rely-
ing only on high-level syntactic structure, as Hem-
ingway?s signature style is minimalism, with 70%
of his sentences corresponding to simple sentences.
Not surprisingly, more adequately informed clas-
sifiers, e.g., SVM with unigram features, are still
able to recognize Hemingway?s writings as those of
highly successful ones.
6.2 Less Successful Books
In order to obtain less successful books, we consider
the Amazon seller?s rank included in the product de-
tails of a book. The less successful books considered
in Table 9 had an Amazon seller?s rank beyond 200k
(higher rank indicating less commercial success) ex-
cept Dan Brown?s The lost symbol, which we in-
cluded mainly because of negative critiques it had
attracted from media despite its commercial success.
As shown in Table 9, all three classifiers make (ar-
guably) correct predictions on Dan Brown?s book.9
This result also supports our earlier assumption on
the nature of novels available at Project Gutenberg
? that they would be more representative of liter-
ary success than general popularity (with or without
literary quality).
7 Predicting Success of Movie Scripts
We have seen successful results in the novel domain,
but can stylometry-based prediction work on very
different domains, such as screenplays? Unlike nov-
els, movie scripts are mostly in dialogues, which
9Most notable pattern based on phrasal tag analysis is a sig-
nificantly increased use of fragments (FRAG), which associates
strongly with less successful books in our dataset.
FEATURE Adven Fanta Roman Thril
POS 62.0 58.0 61.7 56.0
Unigram 62.0 81.3 70.0 80.0
Bigrams 73.3 84.7 80.8 76.0
? 66.0 81.3 70.0 76.0
?G 62.0 69.3 86.7 60.0
? 62.0 81.3 78.3 76.0
?G 69.3 77.3 77.5 68.0
?+Uni 62.0 85.3 70.0 76.0
?G+Uni 54.7 81.3 70.0 76.0
?+Uni 58.0 89.3 70.0 76.0
?G+Uni 58.0 84.7 70.0 76.0
PHR 46.0 42.7 65.8 80.0
PHR+CLR 76.7 31.3 65.8 80.0
PHR+Uni 62.0 81.3 70.0 80.0
PHR+CLR+Uni 62.0 81.3 70.0 80.0
Table 10: Classification results on movie dialogue data
(rating ? 8 vs rating ? 5.5).
are likely to be more informal. Also, what to keep
in mind is that much of the success of movies de-
pends on factors beyond the quality of writing of the
scripts, such as the quality of acting, the popularity
of actors, budgets, artistic taste of directors and pro-
ducers, editing and so forth.
We use the Movie Script Dataset introduced in
Danescu-Niculescu-Mizil and Lee (2011). It in-
cludes the dialogue scripts of 617 movies. The aver-
age rating of all movies is 6.87. We consider movies
with IMDb rating ? 8 as ?more successful?, the
ones with IMDb rating ? 5.5 as ?less successful?.
We combine all the dialogues of each movie and
filter out the movies with less than 200 sentences.
There are 11 genres (?ADVENTURE?, ?FANTASY?,
?ROMANCE?, ?THRILLER?, ?ACTION?, ?COMEDY?,
?CRIME?, ?DRAMA?, ?HORROR?, ?MYSTERY?, ?SCI-
FI?) with 15 movies or more per class, we take 15
movies per class and perform classification tasks
with the same experiment setting as Table 2.
Table 10, we show some of the example genres
with relatively successful outcome, reaching as high
as 89.3% accuracy in FANTASY genre. We would
like to note however that in many other genres, the
prediction did not work as well as it did for the novel
domain. We suspect that there are at least two rea-
sons for this: it must be partly due to very limited
data size ? only 15 instances per class with the rat-
ing threshold we selected for defining the success of
1761
movies. The second reason is due to many other ex-
ternal factors that can also influence the success of
movies, as discussed earlier.
8 Related Work
Predicting success of novels and movies: To the
best of our knowledge, our work is the first that pro-
vides quantitative insights into the unstudied con-
nection between the writing style and the success of
literary works. There have been some previous work
that aims to gain insights into the secret recipe of
successful books, but most were qualitative, based
only on a dozen of books, focusing mainly on the
high-level content of the books, such as the per-
sonalities of protagonists, antagonists, the nature of
plots (e.g., Harvey (1953), Yun (2011)). In con-
trast, our work examines a considerably larger col-
lection of books (800 in total) over eight different
sub-genres, providing insights into lexical, syntac-
tic, and discourse patterns that characterize the writ-
ing styles commonly shared among the successful
literature. Another relevant work has been on a dif-
ferent domain of movies (Yun, 2011), however, the
prediction is based only on external, non-textual in-
formation such as the reputation of actors and direc-
tors, and the power of distribution systems etc, with-
out analyzing the actual content of the movie scripts.
Text quality and readability: Louis (2012) ex-
plored various features that measure the quality of
text, which has some high-level connections to our
work. Combining the insights from Louis (2012)
with our results, we find that the characteristics of
text quality explored in Louis (2012), readability of
text in particular, do not correspond to the prominent
writing style of highly successful literature. There
have been a number of other work that focused on
predicting and measuring readability (e.g., Kate et
al. (2010), Pitler and Nenkova (2008), Schwarm and
Ostendorf (2005), Heilman and Eskenazi (2006) and
Collins-Thompson et al (2004)) employing various
linguistic features.
There is an important difference however, in re-
gard to the nature of the selected text for analysis:
most studies in readability focus on differentiating
good writings from noticeably bad writings, often
involving machine generated text or those written
by ESL students. In contrast, our work essentially
deals with differentiating good writings from even
better writings. After all, all the books that we an-
alyzed are written by expert writers who passed the
scrutinizing eyes of publishers, hence it is reason-
able to expect that the writing quality of even less
successful books is respectful.
Predicting success among academic papers: In
the domain of academic papers, which belongs to
the broad genre of non-fiction, the work of Sawyer
et al (2008) investigated the stylistic characteris-
tics of award winning papers in marketing journals,
and found that the readability plays an important
role. Combined with our study which focuses on fic-
tion and creative writing, it suggests that the recipe
for successful publications can be very different de-
pending on whether it belongs to fiction or nonfic-
tion. The work of Bergsma et al (2012) is also
somewhat relevant to ours in that their work in-
cluded differentiating the writing styles of workshop
papers from major conference papers, where the lat-
ter would be generally considered to be more suc-
cessful.
9 Conclusion
We presented the first quantitative study that learns
to predict the success of literary works based on their
writing styles. Our empirical results demonstrated
that statistical stylometry can be surprisingly effec-
tive in discriminating successful literature, achiev-
ing accuracy up to 84% in the novel domain and
89% in the movie domain. Furthermore, our study
resulted in several insights including: lexical and
syntactic elements of successful styles, the connec-
tion between successful writing style and readabil-
ity, the connection between sentiment / connotation
and the literary success, and last but not least, com-
parative insights between successful writing styles
of fiction and nonfiction.
Acknowledgments This research was supported
in part by the Stony Brook University Office of
the Vice President for Research, and in part by gift
from Google. We thank anonymous reviewers, Steve
Greenspan, and Mike Collins for helpful comments
and suggestions, Alex Berg for the title, and Arun
Nampally for helping with the preliminary work.
1762
References
Omar Ali, Ilias N Flaounas, Tijl De Bie, Nick Mosdell,
Justin Lewis, and Nello Cristianini. 2010. Automat-
ing news content analysis: An application to gender
bias and readability. Journal of Machine Learning
Research-Proceedings Track, 11:36?43.
Shlomo Argamon, Moshe Koppel, Jonathan Fine, and
Anat Rachel Shimoni. 2003. Gender, genre, and writ-
ing style in formal written texts. TEXT-THE HAGUE
THEN AMSTERDAM THEN BERLIN-, 23(3):321?
346.
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric analysis of scientific articles. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 327?337.
Association for Computational Linguistics.
Joshua E Blumenstock. 2008. Automatically assessing
the quality of wikipedia articles.
Kevyn Collins-Thompson, James P. Callan, and James P.
Callan. 2004. A language modeling approach to pre-
dicting reading difficulty. In HLT-NAACL, pages 193?
200.
Cristian Danescu-Niculescu-Mizil and Lillian Lee. 2011.
Chameleons in imagined conversations: A new ap-
proach to understanding coordination of linguistic
style in dialogs. In Proceedings of the Workshop on
Cognitive Modeling and Computational Linguistics,
ACL 2011.
Dan Douglas and Kathleen M Broussard. 2000. Long-
man grammar of spoken and written english. TESOL
Quarterly, 34(4):787?788.
Alvar Ellega?rd. 1962. A Statistical method for determin-
ing authorship: the Junius Letters, 1769-1772, vol-
ume 13. Go?teborg: Acta Universitatis Gothoburgen-
sis.
Hugo J Escalante, Thamar Solorio, and M Montes-y
Go?mez. 2011. Local histograms of character n-
grams for authorship attribution. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
volume 1, pages 288?298.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871?1874.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012.
Characterizing stylistic elements in syntactic struc-
ture. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1522?1533. Association for Computational Lin-
guistics.
Song Feng, Jun Sak Kang, Polina Kuznetsova, and Yejin
Choi. 2013. Connotation lexicon: A dash of senti-
ment beneath the surface meaning. In Proceedings of
the 51th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers), Sofia,
Bulgaria, Angust. Association for Computational Lin-
guistics.
Rudolph Flesch. 1948. A new readability yardstick.
Journal of applied psychology, 32(3):221.
Robert Gunning. 1968. The technique of clear writing.
McGraw-Hill New York.
James W Hall. 2012. Hit Lit: Cracking the Code of
the Twentieth Century?s Biggest Bestsellers. Random
House Digital, Inc.
John Harvey. 1953. The content characteristics of best-
selling novels. Public Opinion Quarterly, 17(1):91?
114.
Michael Heilman and Maxine Eskenazi. 2006. Language
learning: Challenges for intelligent tutoring systems.
In Proceedings of the workshop of intelligent tutor-
ing systems for ill-defined tutoring systems. Eight in-
ternational conference on intelligent tutoring systems,
pages 20?28.
Rohit J Kate, Xiaoqiang Luo, Siddharth Patwardhan,
Martin Franz, Radu Florian, Raymond J Mooney,
Salim Roukos, and Chris Welty. 2010. Learning to
predict readability using diverse linguistic features. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 546?554. Associa-
tion for Computational Linguistics.
Brett Kessler, Geoffrey Numberg, and Hinrich Schu?tze.
1997. Automatic detection of text genre. In Proceed-
ings of the 35th Annual Meeting of the Association for
Computational Linguistics and Eighth Conference of
the European Chapter of the Association for Computa-
tional Linguistics, pages 32?38. Association for Com-
putational Linguistics.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Associ-
ation for Computational Linguistics.
Moshe Koppel and Jonathan Schler. 2003. Exploit-
ing stylistic idiosyncrasies for authorship attribution.
In Proceedings of IJCAI?03 Workshop on Computa-
tional Approaches to Style Analysis and Synthesis, vol-
ume 69, page 72. Citeseer.
Annie Louis. 2012. Automatic metrics for genre-specific
text quality. Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies: Student Research Workshop, page 54.
Jerome McGann. 1998. The Poetics of Sensibility: A
Revolution in Literary Style. Oxford University Press.
1763
Fuchun Peng, Dale Schuurmans, Shaojun Wang, and
Vlado Keselj. 2003. Language independent author-
ship attribution using character level language mod-
els. In Proceedings of the tenth conference on Eu-
ropean chapter of the Association for Computational
Linguistics-Volume 1, pages 267?274. Association for
Computational Linguistics.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: a unified framework for predicting text qual-
ity. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 186?195, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using proba-
bilistic context-free grammars. In Proceedings of the
ACL 2010 Conference Short Papers, pages 38?42. As-
sociation for Computational Linguistics.
Paul Rayson, Andrew Wilson, and Geoffrey Leech.
2001. Grammatical word class variation within the
british national corpus sampler. Language and Com-
puters, 36(1):295?306.
Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi.
2011. Gender attribution: tracing stylometric evi-
dence beyond topic and genre. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning, pages 78?86. Association for Com-
putational Linguistics.
Alan G Sawyer, Juliano Laran, and Jun Xu. 2008.
The readability of marketing journals: Are award-
winning articles better written? Journal of Marketing,
72(1):108?117.
Sarah E. Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?05, pages 523?530, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Arlene E Sierra, Mark A Bisesi, Terry L Rosenbaum, and
E James Potchen. 1992. Readability of the radiologic
report. Investigative radiology, 27(3):236?239.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for information Science and Technology,
60(3):538?556.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In In EMNLP/VLC
2000, pages 63?70.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347?354.
Association for Computational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1600?1610.
Association for Computational Linguistics.
Chang-Joo Yun. 2011. Performance evaluation of intel-
ligent prediction models on the popularity of motion
pictures. In Interaction Sciences (ICIS), 2011 4th In-
ternational Conference on, pages 118?123. IEEE.
1764
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1469?1473,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Keystroke Patterns as Prosody in Digital Writings:
A Case Study with Deceptive Reviews and Essays
Ritwik Banerjee Song Feng Jun S. Kang
Computer Science
Stony Brook University
{rbanerjee, songfeng, junkang}
@cs.stonybrook.edu
Yejin Choi
Computer Science & Engineering
University of Washington
yejin@cs.washington.edu
Abstract
In this paper, we explore the use of keyboard
strokes as a means to access the real-time writ-
ing process of online authors, analogously to
prosody in speech analysis, in the context of
deception detection. We show that differences
in keystroke patterns like editing maneuvers
and duration of pauses can help distinguish be-
tween truthful and deceptive writing. Empiri-
cal results show that incorporating keystroke-
based features lead to improved performance
in deception detection in two different do-
mains: online reviews and essays.
1 Introduction
Due to the practical importance of detecting deceit, in-
terest in it is ancient, appearing in papyrus dated back
to 900 B.C. (Trovillo, 1939). In more recent years, sev-
eral studies have shown that the deceiver often exhibits
behavior that belies the content of communication, thus
providing cues of deception to an observer. These in-
clude linguistic (e.g., Newman et al. (2003), Hancock
et al. (2004)) as well as paralinguistic (e.g., Ekman et
al. (1991), DePaulo et al. (2003)) cues. Recognizing
deception, however, remains a hard task for humans,
who perform only marginally better than chance (Bond
and DePaulo, 2006; Ott et al., 2011).
Recent studies suggest that computers can be sur-
prisingly effective in this task, albeit in limited domains
such as product reviews. Prior research has employed
lexico-syntactic patterns (Ott et al., 2011; Feng et al.,
2012) as well as online user behavior (Fei et al., 2013;
Mukherjee et al., 2013). In this paper, we study the
effect of keystroke patterns for deception detection in
digital communications, which might be helpful in un-
derstanding the psychology of deception and help to-
ward trustful online communities. This allows us to in-
vestigate differences in the writing and revisional pro-
cesses of truthful and fake writers. Our work thus
shares intuition with HCI research linking keystroke
analysis to cognitive processes (Vizer et al., 2009; Epp
et al., 2011) and psychology research connecting cog-
nitive differences to deception (Ekman, 2003; Vrij et
al., 2006).
Recent research has shown that lying generally im-
poses a cognitive burden (e.g., McCornack (1997), Vrij
et al. (2006)) which increases in real-time scenar-
ios (Ekman, 2003). Cognitive burden has been known
to produce differences in keytroke features (Vizer et
al., 2009; Epp et al., 2011). Previous research has not,
however, directly investigated any quantitative connec-
tion between keystroke patterns and deceptive writing.
In this paper, we posit that cognitive burdens in
deception may lead to measurable characteristics in
keystroke patterns. Our contributions are as follows:
(1) introducing keystroke logs as an extended linguis-
tic signal capturing the real-time writing process (anal-
ogous to prosody in speech analysis) by measuring the
writing rate, pauses and revision rate. (2) showing
their empirical value in deception detection, (3) provid-
ing novel domain-specific insights into deceptive writ-
ing, and (4) releasing a new corpus of deception writ-
ings in new domains.
1
2 Related Work
Prior research has focused mainly on using keystroke
traits as a behvioral biometric. Forsen et al. (1977)
first demonstrated that users can be distinguished by the
way they type their names. Subsequent work showed
that typing patterns are unique to individuals (Leggett
and Williams, 1988), and can be used for authentica-
tion (Cho et al., 2000; Bergadano et al., 2002) and in-
trusion detection (Killourhy and Maxion, 2009).
Keystroke pauses have been linked to linguistic pat-
terns in discourse (e.g. Matsuhashi (1981), van Hell et
al. (2008)) and regarded as indications of cognitive bur-
den (e.g., Johansson (2009), Zulkifli (2013)). In this pa-
per, we present the first empirical study that quantita-
tively measures the deception cues in real-time writing
process as manifested in keystroke logs.
3 Data Collection
As discussed by Gokhman et al. (2012), the crowd-
sourcing approach to soliciting deceptive content sim-
ulates the real world of online deceptive content cre-
ators. We collected the data via Amazon Mechanical
Turk.
2
Turkers were led to a separate website where
keylogging was enabled, and asked to write truthful
and deceptive texts (? 100 words) on one of three top-
1
Available at http://www3.cs.stonybrook.
edu/
?
junkang/keystroke/
2
https://www.mturk.com/mturk
1469
ArrowKey Del MouseUp
0
5
10
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
Fre
que
ncy
 of e
ditin
g ke
ystr
oke
s
Deceptive Truthful
Figure 1: Number of keystrokes corresponding to the three
types of edit patterns (E
3
): (a) use of arrow keys, (b) deletion
(Delete and Backspace) and (c) text selection with mouse.
ics: restaurant review, gay marriage and gun control.
Each Turker was required to agree to their typing be-
ing logged. Since copy/paste operations defeat our pur-
pose of studying keystrokes in the typing process, they
were disabled. This restriction also acts as a hindrance
to plagiarism. All texts were reviewed manually, and
those not meeting the requirements (due to the being
too short, plagiarized content, etc.) were disregarded.
Writing task design: The task was designed such
that each Turker wrote a pair of texts, one truthful and
one deceptive, on the same topic. For restaurant re-
views, they were asked to write a truthful review of
a restaurant they liked, and a deceptive review of a
restaurant they have never been to or did not like. For
the other two topics ? ?gun control? and ?gay marriage?
? we asked their opinion: support, neutral, or against.
Then, they were asked to write a truthful and a decep-
tive essay articulating, respectively, their actual opin-
ion and its opposite.
3
The tasks further were divided
into two ?flows?: writing the truthful text before the de-
ceptive one, and vice versa. Each Turker was assigned
only one flow, and was not allowed to participate in the
other. After completing this, each Turker was asked to
copy their own typing, i.e., re-type the two texts.
Finally, in order to get an idea of the cognitive bur-
den associated with truthful and deceptive writing, we
asked the Turkers which task was easier for them. Of
the 196 participants, 152 answered ?truthful?, 40 an-
swered ?deceptive? and only 4 opted for ?not sure?.
What are logged: We deployed a keylogger to cap-
ture the mouse and keyboard events in the ?text area?.
The events KeyUp, KeyDown and MouseUp, along with
the keycode and timestamp were logged.
4
For the three
topics restaurant review, gay marriage and gun control
we obtained 1000, 800 and 800 texts, respectively.
In the remainder of this paper, k
dn
and k
up
denote
the KeyDown and KeyUp events for a key k. For any
3
To prevent a change in opinion depending on task avail-
ability, Turkers were redirected to other tasks if their opinion
was neutral, or if we had enough essays of their opinion.
4
Printable (e.g., alphanumeric characters) as well as non-
printable keystrokes like (e.g., ?Backspace?), are logged.
Document Sentence Word Key Press
1.5
2.0
2.5
1.5
2.0
2.5
First?only
First+Second
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
Tim
e ta
ken
 (rel.
 to c
opy t
ask
)
Deceptive Truthful
Figure 2: Average normalized timespan ??(e) for documents,
sentences, words and key presses. The top row considers only
the first text, while the bottom row considers both flows.
event e, its timespan, i.e., the time interval between the
beginning and end of e, is denoted by ?(e).
4 Feature Design
Keystroke logging enables the study of two types of in-
formation that go beyond conventional linguistic anal-
ysis. First, it captures editing processes (e.g., deletions,
insertions made by changing cursor position, etc.).
Second, it reveals the temporal aspect of text generation
(e.g., duration, latency). Our exploration of these fea-
tures and their application in deception detection is mo-
tivated by the similarities between text and speech gen-
eration. Editing patterns, for instance, can be viewed as
attempts to veil incoherence in deceptive writing and
temporal patterns like latency or pause can be treated
as analogous to disfluency.
Different people, of course, have varying typing
skills, and some may type faster than others. In or-
der to control for this variation, we normalize all event
timespans ?(e) with respect to the corresponding event
timespan in the copy task:
?
?(e) = ?(e)/?(e
copy
).
4.1 Editing Patterns
In this work, we treat keys that are used only for edit-
ing as different from others. Text editing is done by
employing a small subset of available keys: deletion
keys (?Backspace? and ?Delete?), arrow keys (?, ?,
? and ?) and by using the mouse for text selection
(i.e., the ?MouseUp? event). The three types of editing
keystrokes are collectively denoted by
E
3
= ?|DEL| , |MSELECT| , |ARROW|?
where
(i) |DEL| = number of deletion keystrokes
(ii) |MSELECT| = number of ?MouseUp? events, and
(iii) |ARROW| = number of arrow keystrokes
The editing differences between truthful and deceptive
writing across all three topics are shown in Fig. 1.
4.2 Temporal Aspects
Each event is logged with a keycode and a timestamp.
In order to study the temporal aspects of digital writ-
ing, we calculate the timespan of different linguistic
1470
Topic Features Flow
First + Second First-only
Restaurants
BoW 73.9 78.8
BoW + T
6
74.3 79.1
BoW + T
6
+ E
3
74.6 80.3
?
Gun Control
(Support)
BoW 86.5 80.0
BoW + T
6
86.8 82.5
?
BoW + T
6
+ E
3
88.0
?
83.5
?
Gun Control
(Oppose)
BoW 88.5 88.0
BoW + T
6
89.8 87.5
BoW + T
6
+ E
3
90.8
?
89.1
Gay Marriage
(Support)
BoW 92.5 92.0
BoW + T
6
93.8 92.5
BoW + T
6
+ E
3
94.3
?
92.0
Gay Marriage
(Oppose)
BoW 84.5 86.5
BoW + T
6
85.0 87.0
BoW + T
6
+ E
3
85.3 86.8
Table 1: SVM classifier performance for truthful vs. de-
ceptive writing. Statistically significant improvements over
the baseline are marked * (p < 0.05) and ? (p < 0.1).
E
3
= ?|DEL| , |MSELECT| , |ARROW|? denotes the editing
keystrokes, and T
6
is the set of normalized timespans of
documents, words (plus preceding keystroke), all keystrokes,
spaces, non-whitespace keystrokes and inter-word intervals:
T
6
= {??(D), ??(k), ??(SP), ??(?SP), ??(?W), ??(k
prv
+ W)}
units such as words, sentences and even entire docu-
ments. Further, we separately inspect the timespans
of different parts of speech, function words and con-
tent words. In addition to event timespans, intervals
between successive events (e.g., inter-word and inter-
sentence pauses) and pauses preceding or succeeding
and event (e.g., time interval before and after a function
word) are measured as well.
5 Experimental Results
This section describes our experimental setup and
presents insights based on the obtained results. All
classification experiments use 5-fold cross validation
with 80/20 division for training and testing. In addition
to experimenting on the entire dataset, we also sepa-
rately analyze the texts written first (of the two texts in
each ?flow?). This additional step is taken in order to
eliminate the possibility of a text being primed by its
preceding text.
Deception cues in keystroke patterns: To empiri-
cally check whether keystroke features can help distin-
guish between truthful and deceptive writing, we de-
sign binary SVM classifiers.
5
Unigrams with tf-idf
encoding is used as the baseline. The average baseline
accuracy across all topics is 82.58% when considering
both texts of a flow, and 83.62% when considering only
the first text of each flow. The better performance in the
latter possibly indicates that the second text of a flow
exhibits some amount of lexical priming with the first.
The high accuracy of the baseline is not surprising.
Previous work by Ott et al. (2011) reported similar per-
5
We use the LIBLINEAR (Fan et al., 2008) package.
??(W) ??(kprv + W)
D > T T > D D > T T > D
our best when one
if get quality other
when well even get
were your on service
it?s fresh by been
quality not me their
dishes my has not
the one also with
i?ve had go friendly
on hat we great
they of had an
we other is our
friendly very at are
has love which really
at service from but
wait great dishes favorite
an really or very
go you re about
is but would will
which been just here
Table 2: Top 20 words in restaurant reviews with greatest
timespan difference between deceptive and truthful writing.
formance of unigram models. The focus of our work
is to explore the completely new feature space of ty-
pographic patterns in deception detection. We draw
motivation from parallels between the text generation
and speech generation processes. Prosodic concepts
such as speed, disfluency and coherence can be real-
ized in typographic behavior by analyzing timestamp
of keystrokes, pauses and editing patterns, respectively.
Based on the differences in the temporal aspects of
keystrokes, we extract six timespan features to improve
this baseline. This set, denoted by T
6
, comprises of
(i)
?
?(D) = timespan of entire document
(ii)
?
?(k
prv
+W) = average timespan of word plus pre-
ceding keystroke
(iii)
?
?(k) = average keystroke timespan
(iv)
?
?(SP) = average timespan of spaces
(v)
?
?(?SP) = average timespan of non whitesp-
ace keystrokes
(vi)
?
?(?W) = average interval between words.
The improvements attained by adding T
6
to the base-
line are shown in Table 1. Adding the edit patterns (E
3
)
(cf. ? 4.1) further improves the performance (with the
exception of two cases) by 0.7?3.5%.
Writing speed, pauses and revisions: To study the
temporal aspect of language units across all topics,
we first consider all texts, and then restrict to only
the first of each ?flow?. The timespan measurements
are presented in Fig. 2, showing the average duration
of typing documents, sentences, words and individual
keystrokes. The timespans are measured as the inter-
val between the first and the last keystrokes. The sen-
tence timespan, for instance, does not include the gap
between a sentence end and the first keystroke marking
the beginning of the next.
The sentence timespans for ?gay marriage? and ?gun
1471
DT+TD
120
130
140
150
160
170
All Words
Function Words
Content Words
Nouns Verbs Adjectives
Adverbs
Tim
esp
an (m
s) Deceptive
Truthful
(a)
DT+TD
350
400
450
500
550
Function Words
Content Words
Nouns Verbs Adjectives
Adverbs
Tim
esp
an (m
s) Deceptive
Truthful
(b)
Figure 3: Event timespans in restaurant reviews: (a) language units, and (b) language units including their preceding k
dn
.
control? are lower in truthful writing, even though the
document timespans are higher. This difference implies
that the writer is spending a longer period of time to
think before commencing the next sentence, but once
started, the actual typing proceeds rapidly.
Apart from restaurant reviews, truthful writers have
typed slower. This may be due to exercising better care
while expressing their honest opinion.
For restaurant reviews, the document, sentence and
word timespans are significantly higher in deceptive
writing. This, however, is not the case for documents
and words in the other two topics. We conjecture that
this is because deception is harder to write for prod-
uct reviews, due to their dependence on factual details.
Gun control and gay marriage, on the other hand, are
topics well discussed in media, and it is possible that
the writers are aware of the arguments that go against
their personal belief. The frequency of revisional oc-
currences (i.e., keys used for editing) shown in Fig. 1,
too, supports the thought that writing fake reviews may
be harder than adopting a fake stance on well-known
issues. Deceptive reviews exhibit a higher number of
revisions than truthful ones, but essays show the oppo-
site trend. Our findings align with previous studies (Ott
et al., 2011) which showed that deception cues are do-
main dependent.
Writing speed variations over word categories:
Next, we investigate whether there is any quantitative
difference in the writing rate over different words with
respect to the deceptive and truthful intent of the author.
In an attempt to understand this, we analyze words
which show the highest timespan difference between
deceptive and truthful writings.
Table 2 presents words in the restaurant review
topic for which deceptive writers took a lot longer
than truthful writers, and vice versa. Some word cat-
egories exhibit common trends across all three top-
ics. Highly subjective words, for instance (e.g., ?love?,
?best?, ?great?) are words over which truthful writers
spent more time.
Deceptive and truthful texts differ in the typing rate
of first- and second-person pronouns. Deceptive re-
views reveal more time spent in using 2
nd
-person pro-
nouns, as shown by ?you? and ?your?. This finding
throws some light on how people perceive text cues.
Toma and Hancock (2012) showed that readers per-
form poorly at deception detection because they rely on
unrelated text cues such as 2
nd
-person pronouns. Our
analysis indicates that people associate the use of 2
nd
-
person pronouns more with deception not only while
reading, but while writing as well.
Deceptive reviews also exhibit longer time spans for
1
st
-person pronouns (e.g., ?we?, ?me?), which have
been known to be useful in deception detection (New-
man et al., 2003; Ott et al., 2011). Newman et al.
(2003) attributed the less frequent usage of 1
st
-person
pronouns to psychological distancing. The longer time
taken by deceptive writers in our data is a possible sign
of increased cognitive burden when the writer is unable
to maintain the psychological distance. Deceptive re-
viewers also paused a lot more around relative clauses,
e.g., ?if?, ?when?, and ?which?.
In essays, however, the difference in timespans of
1
st
-person and 2
nd
-person pronouns as well as the
timespan difference in relative clauses were insignifi-
cant (< 50ms).
A broader picture of the temporal difference in using
different types of words is presented in Fig. 3, which
shows deceptive reviewers spending less time on ad-
verbs as compared to truthful writers, but more time on
nouns, verbs, adjectives, function words and content
words. They also exhibited significantly longer pauses
before nouns, verbs and function words.
6 Conclusion
In this paper, we investigated the use of typographic
style in deception detection and presented distinct tem-
poral and revisional aspects of keystroke patterns that
improve the characterization of deceptive writing. Our
study provides novel empirically supported insights
into the writing and editing processes of truthful and
deceptive writers. It also presents the first application
of keylogger data used to distinguish between true and
fake texts, and opens up a new range of questions to
better understand what affects these different keystroke
patterns and what they exhibit. It also suggests new
possibilities for making use of keystroke information
as an extended linguistic signal to accompany writings.
Acknowledgements
This research is supported in part by gift from Google.
1472
References
Francesco Bergadano, Daniele Gunetti, and Claudia Pi-
cardi. 2002. User Authentication through Keystroke
Dynamics. ACM Transactions on Information and
System Security (TISSEC), 5(4):367?397.
Charles F Bond and Bella M DePaulo. 2006. Accu-
racy of Deception Judgments. Personality and So-
cial Psychology Review, 10(3):214?234.
Sungzoon Cho, Chigeun Han, Dae Hee Han, and
Hyung-Il Kim. 2000. Web-based Keystroke Dy-
namics Identity Verification Using Neural Network.
Journal of Organizationl Computing and Electronic
Commerce, 10(4):295?307.
Bella M DePaulo, James J Lindsay, Brian E Mal-
one, Laura Muhlenbruck, Kelly Charlton, and Harris
Cooper. 2003. Cues to Deception. Psychological
Bulletin, 129(1):74.
Paul Ekman, Maureen O?Sullivan, Wallace V Friesen,
and Klaus R Scherer. 1991. Invited Article: Face,
Voice and Body in Detecting Deceit. Journal of
Nonverbal Behavior, 15(2):125?135.
Paul Ekman. 2003. Darwin, Deception, and Facial Ex-
pression. Annals of the New York Academy of Sci-
ences, 1000(1):205?221.
Clayton Epp, Michael Lippold, and Regan L Mandryk.
2011. Identifying Emotional States Using Keystroke
Dynamics. In Proc. of the SIGCHI Conference on
Human Factors in Computing Systems, pages 715?
724. ACM.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. The Jour-
nal of Machine Learning Research, 9:1871?1874.
Geli Fei, Arjun Mukherjee, Bing Liu, Meichun Hsu,
Malu Castellanos, and Riddhiman Ghosh. 2013.
Exploiting Burstiness in Reviews for Review Spam-
mer Detection. In ICWSM, pages 175?184.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012.
Syntactic Stylometry for Deception Detection. In
Proc. 50th Annual Meeting of the ACL, pages 171?
175. ACL.
George E Forsen, Mark R Nelson, and Raymond J
Staron Jr. 1977. Personal Attributes Authentication
Techniques. Technical report, DTIC Document.
Stephanie Gokhman, Jeff Hancock, Poornima Prabhu,
Myle Ott, and Claire Cardie. 2012. In Search of a
Gold Standard in Studies of Deception. In Compu-
tational Approaches to Deception Detection, pages
23?30. ACL.
Jeffrey T Hancock, L Curry, Saurabh Goorha, and
Michael T Woodworth. 2004. Lies in Conversa-
tion: An Examination of Deception Using Auto-
mated Linguistic Analysis. In Annual Conference
of the Cognitive Science Society, volume 26, pages
534?540.
Victoria Johansson. 2009. Developmental Aspects of
Text Production in Writing and Speech. Ph.D. thesis,
Lund University.
Kevin S Killourhy and Roy A Maxion. 2009. Compar-
ing Anomaly-Detection Algorithms for Keystroke
Dynamics. In Dependable Systems & Networks,
2009. DSN?09., pages 125?134. IEEE.
John Leggett and Glen Williams. 1988. Verifying
Identity Via Keystroke Characteristics. Interna-
tional Journal of Man-Machine Studies, 28(1):67?
76.
Ann Matsuhashi. 1981. Pausing and Planning: The
Tempo of Written Discourse Production. Research
in the Teaching of English, pages 113?134.
Steven A McCornack. 1997. The Generation of De-
ceptive Messages: Laying the Groundwork for a Vi-
able Theory of Interpersonal Deception. In John O
Greene, editor, Message Production: Advances in
Communication Theory. Erlbaum, Mahwah, NJ.
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and
Natalie Glance. 2013. What Yelp Fake Review Fil-
ter Might be Doing. In ICSWM, pages 409?418.
Matthew L Newman, James W Pennebaker, Diane S
Berry, and Jane M Richards. 2003. Lying Words:
Predicting Deception from Linguistic Styles. Per-
sonality and Social Psychology Bulletin, 29(5):665?
675.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T
Hancock. 2011. Finding Deceptive Opinion Spam
by Any Stretch of the Imagination. In Proc. 49th
Annual Meeting of the ACL: HLT, pages 309?319.
ACL.
Catalina L Toma and Jeffrey T Hancock. 2012. What
Lies Beneath: The Linguistic Traces of Deception in
Online Dating Profiles. Journal of Communication,
62(1):78?97.
Paul V Trovillo. 1939. A History of Lie Detection.
Journal of Criminal Law and Criminology (1931-
1951), 29:848?881.
Janet G van Hell, Ludo Verhoeven, and Liesbeth M van
Beijsterveldt. 2008. Pause Time Patterns in Writ-
ing Narrative and Expository Texts by Children and
Adults. Discourse Processes, 45(4-5):406?427.
Lisa M Vizer, Lina Zhou, and Andrew Sears. 2009.
Automated Stress Detection Using Keystroke and
Linguistic Features: An Exploratory Study. In-
ternational Journal of Human-Computer Studies,
67(10):870?886.
Aldert Vrij, Ronald Fisher, Samantha Mann, and
Sharon Leal. 2006. Detecting Deception by Ma-
nipulating Cognitive Load. Trends in Cognitive Sci-
ences, 10(4):141?142.
Putri Zulkifli. 2013. Applying Pause Analysis to Ex-
plore Cognitive Processes in the Copying of Sen-
tences by Second Language Users. Ph.D. thesis,
University of Sussex.
1473
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 171?175,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Syntactic Stylometry for Deception Detection
Song Feng Ritwik Banerjee Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
songfeng, rbanerjee, ychoi@cs.stonybrook.edu
Abstract
Most previous studies in computerized de-
ception detection have relied only on shal-
low lexico-syntactic patterns. This pa-
per investigates syntactic stylometry for
deception detection, adding a somewhat
unconventional angle to prior literature.
Over four different datasets spanning from
the product review to the essay domain,
we demonstrate that features driven from
Context Free Grammar (CFG) parse trees
consistently improve the detection perfor-
mance over several baselines that are based
only on shallow lexico-syntactic features.
Our results improve the best published re-
sult on the hotel review data (Ott et al,
2011) reaching 91.2% accuracy with 14%
error reduction.
1 Introduction
Previous studies in computerized deception de-
tection have relied only on shallow lexico-
syntactic cues. Most are based on dictionary-
based word counting using LIWC (Pennebaker
et al, 2007) (e.g., Hancock et al (2007), Vrij et
al. (2007)), while some recent ones explored the
use of machine learning techniques using sim-
ple lexico-syntactic patterns, such as n-grams
and part-of-speech (POS) tags (Mihalcea and
Strapparava (2009), Ott et al (2011)). These
previous studies unveil interesting correlations
between certain lexical items or categories with
deception that may not be readily apparent to
human judges. For instance, the work of Ott
et al (2011) in the hotel review domain results
in very insightful observations that deceptive re-
viewers tend to use verbs and personal pronouns
(e.g., ?I?, ?my?) more often, while truthful re-
viewers tend to use more of nouns, adjectives,
prepositions. In parallel to these shallow lexical
patterns, might there be deep syntactic struc-
tures that are lurking in deceptive writing?
This paper investigates syntactic stylometry
for deception detection, adding a somewhat un-
conventional angle to prior literature. Over four
different datasets spanning from the product re-
view domain to the essay domain, we find that
features driven from Context Free Grammar
(CFG) parse trees consistently improve the de-
tection performance over several baselines that
are based only on shallow lexico-syntactic fea-
tures. Our results improve the best published re-
sult on the hotel review data of Ott et al (2011)
reaching 91.2% accuracy with 14% error reduc-
tion. We also achieve substantial improvement
over the essay data of Mihalcea and Strapparava
(2009), obtaining upto 85.0% accuracy.
2 Four Datasets
To explore different types of deceptive writing,
we consider the following four datasets spanning
from the product review to the essay domain:
I. TripAdvisor?Gold: Introduced in Ott et
al. (2011), this dataset contains 400 truthful re-
views obtained from www.tripadviser.com and
400 deceptive reviews gathered using Amazon
Mechanical Turk, evenly distributed across 20
Chicago hotels.
171
TripAdvisor?Gold TripAdvisor?Heuristic
Deceptive Truthful Deceptive Truthful
NP?PP ? DT NNP NNP NNP S?ROOT ? VP . NP?S ? PRP VP?S ? VBZ NP
SBAR?NP ? S NP?NP ? $ CD SBAR?S ? WHADVP S NP?NP ? NNS
NP?VP ? NP SBAR PRN?NP ? LRB NP RRB VP?S ? VBD PP WHNP?SBAR ? WDT
NP?NP ? PRP$ NN NP?NP ? NNS S?SBAR ? NP VP NP?NP ? NP PP PP
NP?S ? DT NNP NNP NNP NP?S ? NN S?ROOT ? PP NP VP . NP?S ? EX
VP?S ? VBG PP NP?PP ? DT NNP VP?S ? VBD S NX?NX ? JJ NN
NP?PP ? PRP$ NN NP?PP ? CD NNS NP?S ? NP CC NP NP?NP ? NP PP
VP?S ? MD ADVP VP NP?NP ? NP PRN NP?S ? PRP$ NN VP?S ? VBZ RB NP
VP?S ? TO VP PRN?NP ? LRB PP RRB NP?PP ? DT NNP PP?NP ? IN NP
ADJP?NP ? RBS JJ NP?NP ? CD NNS NP?PP ? PRP$ NN PP?ADJP ? TO NP
Table 1: Most discriminative rewrite rules (r?): hotel review datasets
Figure 1: Parsed trees
II. TripAdvisor?Heuristic: This dataset
contains 400 truthful and 400 deceptive reviews
harvested from www.tripadviser.com, based
on fake review detection heuristics introduced
in Feng et al (2012).1
III. Yelp: This dataset is our own creation
using www.yelp.com. We collect 400 filtered re-
views and 400 displayed reviews for 35 Italian
restaurants with average ratings in the range of
[3.5, 4.0]. Class labels are based on the meta
data, which tells us whether each review is fil-
tered by Yelp?s automated review filtering sys-
tem or not. We expect that filtered reviews
roughly correspond to deceptive reviews, and
displayed reviews to truthful ones, but not with-
out considerable noise. We only collect 5-star
reviews to avoid unwanted noise from varying
1Specifically, using the notation of Feng et al (2012),
we use data created by Strategy-dist? heuristic, with
HS ,S as deceptive and H ?S , T as truthful.
degree of sentiment.
IV. Essays: Introduced in Mihalcea and
Strapparava (2009), this corpus contains truth-
ful and deceptive essays collected using Amazon
Mechanic Turk for the following three topics:
?Abortion? (100 essays per class), ?Best Friend?
(98 essays per class), and ?Death Penalty? (98
essays per class).
3 Feature Encoding
Words Previous work has shown that bag-of-
words are effective in detecting domain-specific
deception (Ott et al, 2011; Mihalcea and Strap-
parava, 2009). We consider unigram, bigram,
and the union of the two as features.
Shallow Syntax As has been used in many
previous studies in stylometry (e.g., Argamon-
Engelson et al (1998), Zhao and Zobel (2007)),
we utilize part-of-speech (POS) tags to encode
shallow syntactic information. Note that Ott
et al (2011) found that even though POS tags
are effective in detecting fake product reviews,
they are not as effective as words. Therefore, we
strengthen POS features with unigram features.
Deep syntax We experiment with four differ-
ent encodings of production rules based on the
Probabilistic Context Free Grammar (PCFG)
parse trees as follows:
? r: unlexicalized production rules (i.e., all
production rules except for those with ter-
minal nodes), e.g., NP2 ? NP3 SBAR.
? r?: lexicalized production rules (i.e., all
production rules), e.g., PRP ? ?you?.
? r?: unlexicalized production rules combined
with the grandparent node, e.g., NP2 ?VP
172
TripAdvisor Yelp Essay
Gold Heur Abort BstFr Death
unigram 88.4 74.4 59.9 70.0 77.0 67.4
words bigram 85.8 71.5 60.7 71.5 79.5 55.5
uni + bigram 89.6 73.8 60.1 72.0 81.5 65.5
pos(n=1) + unigram 87.4 74.0 62.0 70.0 80.0 66.5
shallow syntax pos(n=2) + unigram 88.6 74.6 59.0 67.0 82.0 66.5
+words pos(n=3) + unigram 88.6 74.6 59.3 67.0 82.0 66.5
r 78.5 65.3 56.9 62 67.5 55.5
deep syntax r? 74.8 65.3 56.5 58.5 65.5 56.0
r? 89.4 74.0 64.0 70.1 77.5 66.0
r?? 90.4 75 63.5 71.0 78 67.5
r + unigram 89.0 74.3 62.3 76.5 82.0 69.0
deep syntax r? + unigram 88.5 74.3 62.5 77.0 81.5 70.5
+words r? + unigram 90.3 75.4 64.3 74.0 85.0 71.5
r?? + unigram 91.2 76.6 62.1 76.0 84.5 71.0
Table 2: Deception Detection Accuracy (%).
1 ? NP3 SBAR.
? r??: lexicalized production rules (i.e., all
production rules) combined with the grand-
parent node, e.g., PRP?NP 4 ? ?you?.
4 Experimental Results
For all classification tasks, we use SVM classi-
fier, 80% of data for training and 20% for test-
ing, with 5-fold cross validation.2 All features
are encoded as tf-idf values. We use Berkeley
PCFG parser (Petrov and Klein, 2007) to parse
sentences. Table 2 presents the classification
performance using various features across four
different datasets introduced earlier.3
4.1 TripAdvisor?Gold
We first discuss the results for the TripAdvisor?
Gold dataset shown in Table 2. As reported in
Ott et al (2011), bag-of-words features achieve
surprisingly high performance, reaching upto
89.6% accuracy. Deep syntactic features, en-
coded as r?? slightly improves this performance,
achieving 90.4% accuracy. When these syntactic
features are combined with unigram features, we
attain the best performance of 91.2% accuracy,
2We use LIBLINEAR (Fan et al, 2008) with L2-
regulization, parameter optimized over the 80% training
data (3 folds for training, 1 fold for testing).
3Numbers in italic are classification results reported
in Ott et al (2011) and Mihalcea and Strapparava (2009).
yielding 14% error reduction over the word-only
features.
Given the power of word-based features, one
might wonder, whether the PCFG driven fea-
tures are being useful only due to their lexi-
cal production rules. To address such doubts,
we include experiments with unlexicalized rules,
r and r?. These features achieve 78.5% and
74.8% accuracy respectively, which are signifi-
cantly higher than that of a random baseline
(?50.0%), confirming statistical differences in
deep syntactic structures. See Section 4.4 for
concrete exemplary rules.
Another question one might have is whether
the performance gain of PCFG features are
mostly from local sequences of POS tags, indi-
rectly encoded in the production rules. Compar-
ing the performance of [shallow syntax+words]
and [deep syntax+words] in Table 2, we find sta-
tistical evidence that deep syntax based features
offer information that are not available in simple
POS sequences.
4.2 TripAdvisor?Heuristic & Yelp
The performance is generally lower than that of
the previous dataset, due to the noisy nature
of these datasets. Nevertheless, we find similar
trends as those seen in the TripAdvisor?Gold
dataset, with respect to the relative performance
differences across different approaches. The sig-
173
TripAdvisor?Gold TripAdvisor?Heur
Decep Truth Decep Truth
VP PRN VP PRN
SBAR QP WHADVP NX
WHADVP S SBAR WHNP
ADVP PRT WHADJP ADJP
CONJP UCP INTJ WHPP
Table 3: Most discriminative phrasal tags in PCFG
parse trees: TripAdvisor data.
nificance of these results comes from the fact
that these two datasets consists of real (fake)
reviews in the wild, rather than manufactured
ones that might invite unwanted signals that
can unexpectedly help with classification accu-
racy. In sum, these results indicate the exis-
tence of the statistical signals hidden in deep
syntax even in real product reviews with noisy
gold standards.
4.3 Essay
Finally in Table 2, the last dataset Essay con-
firms the similar trends again, that the deep syn-
tactic features consistently improve the perfor-
mance over several baselines based only on shal-
low lexico-syntactic features. The final results,
reaching accuracy as high as 85%, substantially
outperform what has been previously reported
in Mihalcea and Strapparava (2009). How ro-
bust are the syntactic cues in the cross topic set-
ting? Table 4 compares the results of Mihalcea
and Strapparava (2009) and ours, demonstrat-
ing that syntactic features achieve substantially
and surprisingly more robust results.
4.4 Discriminative Production Rules
To give more concrete insights, we provide
10 most discriminative unlexicalized production
rules (augmented with the grand parent node)
for each class in Table 1. We order the rules
based on the feature weights assigned by LIB-
LINEAR classifier. Notice that the two produc-
tion rules in bolds ? [SBAR?NP? S] and [NP
?VP? NP SBAR] ? are parts of the parse tree
shown in Figure 1, whose sentence is taken from
an actual fake review. Table 3 shows the most
discriminative phrasal tags in the PCFG parse
training: A & B A & D B & D
testing: DeathPen BestFrn Abortion
M&S 2009 58.7 58.7 62.0
r? 66.8 70.9 69.0
Table 4: Cross topic deception detection accuracy:
Essay data
trees for each class. Interestingly, we find more
frequent use of VP, SBAR (clause introduced
by subordinating conjunction), and WHADVP
in deceptive reviews than truthful reviews.
5 Related Work
Much of the previous work for detecting de-
ceptive product reviews focused on related, but
slightly different problems, e.g., detecting dupli-
cate reviews or review spams (e.g., Jindal and
Liu (2008), Lim et al (2010), Mukherjee et al
(2011), Jindal et al (2010)) due to notable dif-
ficulty in obtaining gold standard labels.4 The
Yelp data we explored in this work shares a sim-
ilar spirit in that gold standard labels are har-
vested from existing meta data, which are not
guaranteed to align well with true hidden la-
bels as to deceptive v.s. truthful reviews. Two
previous work obtained more precise gold stan-
dard labels by hiring Amazon turkers to write
deceptive articles (e.g., Mihalcea and Strappa-
rava (2009), Ott et al (2011)), both of which
have been examined in this study with respect
to their syntactic characteristics. Although we
are not aware of any prior work that dealt
with syntactic cues in deceptive writing directly,
prior work on hedge detection (e.g., Greene and
Resnik (2009), Li et al (2010)) relates to our
findings.
6 Conclusion
We investigated syntactic stylometry for decep-
tion detection, adding a somewhat unconven-
tional angle to previous studies. Experimental
results consistently find statistical evidence of
deep syntactic patterns that are helpful in dis-
criminating deceptive writing.
4It is not possible for a human judge to tell with full
confidence whether a given review is a fake or not.
174
References
S. Argamon-Engelson, M. Koppel, and G. Avneri.
1998. Style-based text categorization: What
newspaper am i reading. In Proc. of the AAAI
Workshop on Text Categorization, pages 1?4.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?
1874.
S. Feng, L. Xing, Gogar A., and Y. Choi. 2012.
Distributional footprints of deceptive product re-
views. In Proceedings of the 2012 International
AAAI Conference on WebBlogs and Social Media,
June.
S. Greene and P. Resnik. 2009. More than
words: Syntactic packaging and implicit senti-
ment. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 503?511. Asso-
ciation for Computational Linguistics.
J.T. Hancock, L.E. Curry, S. Goorha, and M. Wood-
worth. 2007. On lying and being lied to: A lin-
guistic analysis of deception in computer-mediated
communication. Discourse Processes, 45(1):1?23.
Nitin Jindal and Bing Liu. 2008. Opinion spam
and analysis. In Proceedings of the international
conference on Web search and web data mining,
WSDM ?08, pages 219?230, New York, NY, USA.
ACM.
Nitin Jindal, Bing Liu, and Ee-Peng Lim. 2010.
Finding unusual review patterns using unexpected
rules. In Proceedings of the 19th ACM Confer-
ence on Information and Knowledge Management,
pages 1549?1552.
X. Li, J. Shen, X. Gao, and X. Wang. 2010. Ex-
ploiting rich features for detecting hedges and
their scope. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning?Shared Task, pages 78?83. Association
for Computational Linguistics.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing
Liu, and Hady Wirawan Lauw. 2010. Detecting
product review spammers using rating behaviors.
In Proceedings of the 19th ACM international con-
ference on Information and knowledge manage-
ment, CIKM ?10, pages 939?948, New York, NY,
USA. ACM.
R. Mihalcea and C. Strapparava. 2009. The lie de-
tector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages
309?312. Association for Computational Linguis-
tics.
Arjun Mukherjee, Bing Liu, Junhui Wang, Natalie S.
Glance, and Nitin Jindal. 2011. Detecting group
review spam. In Proceedings of the 20th Interna-
tional Conference on World Wide Web (Compan-
ion Volume), pages 93?94.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, pages 309?319, Portland, Ore-
gon, USA, June. Association for Computational
Linguistics.
J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gon-
zales, and R.J. Booth. 2007. The development
and psychometric properties of liwc2007. Austin,
TX, LIWC. Net.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411.
A. Vrij, S. Mann, S. Kristen, and R.P. Fisher. 2007.
Cues to deception and ability to detect lies as a
function of police interview styles. Law and hu-
man behavior, 31(5):499?518.
Ying Zhao and Justin Zobel. 2007. Searching with
style: authorship attribution in classic literature.
In Proceedings of the thirtieth Australasian confer-
ence on Computer science - Volume 62, ACSC ?07,
pages 59?68, Darlinghurst, Australia, Australia.
Australian Computer Society, Inc.
175
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1774?1784,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Connotation Lexicon:
A Dash of Sentiment Beneath the Surface Meaning
Song Feng Jun Seok Kang Polina Kuznetsova Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
songfeng, junkang, pkuznetsova, ychoi@cs.stonybrook.edu
Abstract
Understanding the connotation of words
plays an important role in interpreting sub-
tle shades of sentiment beyond denotative
or surface meaning of text, as seemingly
objective statements often allude nuanced
sentiment of the writer, and even purpose-
fully conjure emotion from the readers?
minds. The focus of this paper is draw-
ing nuanced, connotative sentiments from
even those words that are objective on the
surface, such as ?intelligence?, ?human?,
and ?cheesecake?. We propose induction
algorithms encoding a diverse set of lin-
guistic insights (semantic prosody, distri-
butional similarity, semantic parallelism of
coordination) and prior knowledge drawn
from lexical resources, resulting in the first
broad-coverage connotation lexicon.
1 Introduction
There has been a substantial body of research
in sentiment analysis over the last decade (Pang
and Lee, 2008), where a considerable amount of
work has focused on recognizing sentiment that is
generally explicit and pronounced rather than im-
plied and subdued. However in many real-world
texts, even seemingly objective statements can be
opinion-laden in that they often allude nuanced
sentiment of the writer (Greene and Resnik, 2009),
or purposefully conjure emotion from the readers?
minds (Mohammad and Turney, 2010). Although
some researchers have explored formal and statis-
tical treatments of those implicit and implied sen-
timents (e.g. Wiebe et al (2005), Esuli and Sebas-
tiani (2006), Greene and Resnik (2009), Davidov
et al (2010)), automatic analysis of them largely
remains as a big challenge.
In this paper, we concentrate on understanding
the connotative sentiments of words, as they play
an important role in interpreting subtle shades of
sentiment beyond denotative or surface meaning
of text. For instance, consider the following:
Geothermal replaces oil-heating; it helps re-
ducing greenhouse emissions.1
Although this sentence could be considered as a
factual statement from the general standpoint, the
subtle effect of this sentence may not be entirely
objective: this sentence is likely to have an influ-
ence on readers? minds in regard to their opinion
toward ?geothermal?. In order to sense the subtle
overtone of sentiments, one needs to know that the
word ?emissions? has generally negative connota-
tion, which geothermal reduces. In fact, depend-
ing on the pragmatic contexts, it could be precisely
the intention of the author to transfer his opinion
into the readers? minds.
The main contribution of this paper is a broad-
coverage connotation lexicon that determines the
connotative polarity of even those words with ever
so subtle connotation beneath their surface mean-
ing, such as ?Literature?, ?Mediterranean?, and
?wine?. Although there has been a number of
previous work that constructed sentiment lexicons
(e.g., Esuli and Sebastiani (2006), Wilson et al
(2005a), Kaji and Kitsuregawa (2007), Qiu et
al. (2009)), which seem to be increasingly and
inevitably expanding over words with (strongly)
connotative sentiments rather than explicit senti-
ments alone (e.g., ?gun?), little prior work has di-
rectly tackled this problem of learning connota-
tion,2 and much of the subtle connotation of many
seemingly objective words is yet to be determined.
1Our learned lexicon correctly assigns negative polarity to
emission.
2A notable exception would be the work of Feng et al
1774
POSITIVE NEGATIVE
FEMA, Mandela, Intel, Google, Python, Sony, Pulitzer,
Harvard, Duke, Einstein, Shakespeare, Elizabeth, Clooney,
Hoover, Goldman, Swarovski, Hawaii, Yellowstone
Katrina, Monsanto, Halliburton, Enron, Teflon, Hi-
roshima, Holocaust, Afghanistan, Mugabe, Hutu, Sad-
dam, Osama, Qaeda, Kosovo, Helicobacter, HIV
Table 1: Example Named Entities (Proper Nouns) with Polar Connotation.
A central premise to our approach is that it is
collocational statistics of words that affect and
shape the polarity of connotation. Indeed, the ety-
mology of ?connotation? is from the Latin ?com-
? (?together or with?) and ?notare? (?to mark?).
It is important to clarify, however, that we do not
simply assume that words that collocate share the
same polarity of connotation. Although such an
assumption played a key role in previous work for
the analogous task of learning sentiment lexicon
(Velikovich et al, 2010), we expect that the same
assumption would be less reliable in drawing sub-
tle connotative sentiments of words. As one ex-
ample, the predicate ?cure?, which has a positive
connotation typically takes arguments with nega-
tive connotation, e.g., ?disease?, when used as the
?relieve? sense.3
Therefore, in order to attain a broad cover-
age lexicon while maintaining good precision, we
guide the induction algorithm with multiple, care-
fully selected linguistic insights: [1] distributional
similarity, [2] semantic parallelism of coordina-
tion, [3] selectional preference, and [4] seman-
tic prosody (e.g., Sinclair (1991), Louw (1993),
Stubbs (1995), Stefanowitsch and Gries (2003))),
and also exploit existing lexical resources as an ad-
ditional inductive bias.
We cast the connotation lexicon induction task
as a collective inference problem, and consider ap-
proaches based on three distinct types of algorith-
mic framework that have been shown successful
for conventional sentiment lexicon induction:
Random walk based on HITS/PageRank (e.g.,
Kleinberg (1999), Page et al (1999), Feng
et al (2011) Heerschop et al (2011),
Montejo-Ra?ez et al (2012))
Label/Graph propagation (e.g., Zhu and Ghahra-
(2011) but with practical limitations. See ?3 for detailed dis-
cussion.
3Note that when ?cure? is used as the ?preserve? sense, it
expects objects with non-negative connotation. Hence word-
sense-disambiguation (WSD) presents a challenge, though
not unexpectedly. In this work, we assume the general conno-
tation of each word over statistically prevailing senses, leav-
ing a more cautious handling of WSD as future work.
mani (2002), Velikovich et al (2010))
Constraint optimization (e.g., Roth and Yih
(2004), Choi and Cardie (2009), Lu et al
(2011)).
We provide comparative empirical results over
several variants of these approaches with compre-
hensive evaluations including lexicon-based, hu-
man judgments, and extrinsic evaluations.
It is worthwhile to note that not all words have
connotative meanings that are distinct from deno-
tational meanings, and in some cases, it can be dif-
ficult to determine whether the overall sentiment is
drawn from denotational or connotative meanings
exclusively, or both. Therefore, we encompass any
sentiment from either type of meanings into the
lexicon, where non-neutral polarity prevails over
neutral one if some meanings lead to neutral while
others to non-neutral.4
Our work results in the first broad-coverage
connotation lexicon,5 significantly improving both
the coverage and the precision of Feng et al
(2011). As an interesting by-product, our algo-
rithm can be also used as a proxy to measure the
general connotation of real-world named entities
based on their collocational statistics. Table 1
highlights some example proper nouns included in
the final lexicon.
The rest of the paper is structured as follows.
In ?2 we describe three types of induction algo-
rithms followed by evaluation in ?3. Then we re-
visit the induction algorithms based on constraint
optimization in ?4 to enhance quality and scala-
bility. ?5 presents comprehensive evaluation with
human judges and extrinsic evaluations. Related
work and conclusion are in ?6 and ?7.
4In general, polysemous words do not seem to have con-
flicting non-neutral polarities over different senses, though
there are many exceptions, e.g., ?heat?, or ?fine?. We treat
each word in each part-of-speech as a separate word to reduce
such cases, otherwise aim to learn the most prevalent polar-
ity in the corpus with respect to each part-of-speech of each
word.
5Available at http://www.cs.stonybrook.edu/
?ychoi/connotation.
1775
? 
Pred-Arg 
Arg-Arg 
pred-arg distr sim 
enjoy 
thank 
writing 
profit 
help 
investment 
aid 
reading 
Figure 1: Graph for Graph Propagation (?2.2).
? 
? 
synonyms antonyms 
prevent 
suffer  
enjoy 
thank 
tax  
loss 
writing 
profit 
preventing 
gain 
investment 
bonus  
pred-arg distr sim 
flu  
cold  
Figure 2: Graph for ILP/LP (?2.3, ?4.2).
2 Connotation Induction Algorithms
We develop induction algorithms based on three
distinct types of algorithmic framework that have
been shown successful for the analogous task of
sentiment lexicon induction: HITS & PageRank
(?2.1), Label/Graph Propagation (?2.2), and Con-
straint Optimization via Integer Linear Program-
ming (?2.3). As will be shown, each of these ap-
proaches will incorporate additional, more diverse
linguistic insights.
2.1 HITS & PageRank
The work of Feng et al (2011) explored the use
of HITS (Kleinberg, 1999) and PageRank (Page
et al, 1999) to induce the general connotation
of words hinging on the linguistic phenomena of
selectional preference and semantic prosody, i.e.,
connotative predicates influencing the connotation
of their arguments. For example, the object of
a negative connotative predicate ?cure? is likely
to have negative connotation, e.g., ?disease? or
?cancer?. The bipartite graph structure for this
approach corresponds to the left-most box (labeled
as ?pred-arg?) in Figure 1.
2.2 Label Propagation
With the goal of obtaining a broad-coverage lexi-
con in mind, we find that relying only on the struc-
ture of semantic prosody is limiting, due to rel-
atively small sets of connotative predicates avail-
able.6 Therefore, we extend the graph structure
as an overlay of two sub-graphs (Figure 1) as de-
scribed below:
6For connotative predicates, we use the seed predicate set
of Feng et al (2011), which comprises of 20 positive and 20
negative predicates.
Sub-graph #1: Predicate?Argument Graph
This sub-graph is the bipartite graph that encodes
the selectional preference of connotative predi-
cates over their arguments. In this graph, conno-
tative predicates p reside on one side of the graph
and their co-occurring arguments a reside on the
other side of the graph based on Google Web 1T
corpus.7 The weight on the edges between the
predicates p and arguments a are defined using
Point-wise Mutual Information (PMI) as follows:
w(p? a) := PMI(p, a) = log2
P (p, a)
P (p)P (a)
PMI scores have been widely used in previous
studies to measure association between words
(e.g., Turney (2001), Church and Hanks (1990)).
Sub-graph #2: Argument?Argument Graph
The second sub-graph is based on the distribu-
tional similarities among the arguments. One pos-
sible way of constructing such a graph is simply
connecting all nodes and assign edge weights pro-
portionate to the word association scores, such as
PMI, or distributional similarity. However, such a
completely connected graph can be susceptible to
propagating noise, and does not scale well over a
very large set of vocabulary.
We therefore reduce the graph connectivity by
exploiting semantic parallelism of coordination
(Bock (1986), Hatzivassiloglou and McKeown
7We restrict predicte-argument pairs to verb-object pairs
in this study. Note that Google Web 1T dataset consists of
n-grams upto n = 5. Since n-gram sequences are too short
to apply a parser, we extract verb-object pairs approximately
by matching part-of-speech tags. Empirically, when overlaid
with the second sub-graph, we found that it is better to keep
the connectivity of this sub-graph as uni-directional. That is,
we only allow edges to go from a predicate to an argument.
1776
POSITIVE NEGATIVE NEUTRAL
n. avatar, adrenaline, keynote, debut,
stakeholder, sunshine, cooperation
unbeliever, delay, shortfall, gun-
shot, misdemeanor, mutiny, rigor
header, mark, clothing, outline,
grid, gasoline, course, preview
v. handcraft, volunteer, party, ac-
credit, personalize, nurse, google
sentence, cough, trap, scratch, de-
bunk, rip, misspell, overcharge
state, edit, send, put, arrive, type,
drill, name, stay, echo, register
a. floral, vegetarian, prepared, age-
less, funded, contemporary
debilitating, impaired, swollen,
intentional, jarring, unearned
same, cerebral, west, uncut, auto-
matic, hydrated, unheated, routine
Table 2: Example Words with Learned Connotation: Nouns(n), Verbs(v), Adjectives(a).
(1997), Pickering and Branigan (1998)). In par-
ticular, we consider an undirected edge between a
pair of arguments a1 and a2 only if they occurred
together in the ?a1 and a2? or ?a2 and a1? coor-
dination, and assign edge weights as:
w(a1 ? a2) = CosineSim(??a1,??a2) =
??a1 ? ??a2
||??a1|| ||??a2||
where ??a1 and ??a2 are co-occurrence vectors for a1
and a2 respectively. The co-occurrence vector for
each word is computed using PMI scores with re-
spect to the top n co-occurring words.8 n (=50)
is selected empirically. The edge weights in two
sub-graphs are normalized so that they are in the
comparable range.9
Limitations of Graph-based Algorithms
Although graph-based algorithms (?2.1, ?2.2) pro-
vide an intuitive framework to incorporate various
lexical relations, limitations include:
1. They allow only non-negative edge weights.
Therefore, we can encode only positive (sup-
portive) relations among words (e.g., distri-
butionally similar words will endorse each
other with the same polarity), while miss-
ing on exploiting negative relations (e.g.,
antonyms may drive each other into the op-
posite polarity).
2. They induce positive and negative polarities
in isolation via separate graphs. However, we
expect that a more effective algorithm should
induce both polarities simultaneously.
3. The framework does not readily allow incor-
porating a diverse set of soft and hard con-
straints.
8We discard edges with cosine similarity ? 0, as those
indicate either independence or the opposite of similarity.
9Note that cosine similarity does not make sense for the
first sub-graph as there is no reason why a predicate and an ar-
gument should be distributionally similar. We experimented
with many different variations on the graph structure and
edge weights, including ones that include any word pairs that
occurred frequently enough together. For brevity, we present
the version that achieved the best results here.
2.3 Constraint Optimization
Addressing limitations of graph-based algorithms
(?2.2), we propose an induction algorithm based
on Integer Linear Programming (ILP). Figure 2
provides the pictorial overview. In comparison to
Figure 1, two new components are: (1) dictionary-
driven relations targeting enhanced precision, and
(2) dictionary-driven words (i.e., unseen words
with respect to those relations explored in Figure
1) targeting enhanced coverage. We formulate in-
sights in Figure 2 using ILP as follows:
Definition of sets of words:
1. P+: the set of positive seed predicates.
P?: the set of negative seed predicates.
2. S: the set of seed sentiment words.
3. Rsyn: word pairs in synonyms relation.
Rant: word pairs in antonyms relation.
Rcoord: word pairs in coordination relation.
Rpred: word pairs in pred-arg relation.
Rpred+(?) : Rpred based on P+ (P?).
Definition of variables: For each word i, we
define binary variables xi, yi, zi ? {0, 1}, where
xi = 1 (yi = 1, zi = 1) if and only if i has a pos-
itive (negative, neutral) connotation respectively.
For every pair of word i and j, we define binary
variables dpqij where p, q ? {+,?, 0} and dpqij = 1
if and only if the polarity of i and j are p and q
respectively.
Objective function: We aim to maximize:
F = ?prosody + ?coord + ?neu
where ?prosody is the scores based on semantic
prosody, ?coord captures the distributional similar-
ity over coordination, and ?neu controls the sen-
sitivity of connotation detection between positive
(negative) and neutral. In particular,
?prosody =
Rpred?
i,j
wpredi,j (d++i,j + d??i,j ? d+?i,j ? d?+i,j )
?coord =
Rcoord?
i,j
wcoordi,j (d++i,j + d??i,j + d00i,j)
1777
?neu = ?
Rpred?
i,j
wpredi,j ? zj
Soft constraints (edge weights): The weights in
the objective function are set as follows:
wpred(p, a) = freq(p, a)?
(p,x)?Rpred
freq(p, x)
wcoord(a1, a2) = CosSim(??a1,??a2) =
??a1 ? ??a2
||??a1|| ||??a2||
Note that the same wcoord(a1, a2) has been used
in graph propagation described in Section 2.2. ?
controls the sensitivity of connotation detection
such that higher value of ? will promote neutral
connotation over polar ones.
Hard constrains for variable consistency:
1. Each word i has one of {+,?, ?} as polarity:
?i, xi + yi + zi = 1
2. Variable consistency between dpqij and
xi, yi, zi:
xi + xj ? 1 ? 2d++i,j ? xi + xj
yi + yj ? 1 ? 2d??i,j ? yi + yj
zi + zj ? 1 ? 2d00i,j ? zi + zj
xi + yj ? 1 ? 2d+?i,j ? xi + yj
yi + xj ? 1 ? 2d?+i,j ? yi + xj
Hard constrains for WordNet relations:
1. Cant: Antonym pairs will not have the same
positive or negative polarity:
?(i, j) ? Rant, xi + xj ? 1, yi + yj ? 1
For this constraint, we only consider
antonym pairs that share the same root, e.g.,
?sufficient? and ?insufficient?, as those pairs
are more likely to have the opposite polarities
than pairs without sharing the same root, e.g.,
?east? and ?west?.
2. Csyn: Synonym pairs will not have the oppo-
site polarity:
?(i, j) ? Rsyn, xi + yj ? 1, xj + yi ? 1
3 Experimental Result I
We provide comprehensive comparisons over vari-
ants of three types of algorithms proposed in ?2.
We use the Google Web 1T data (Brants and Franz
(2006)), and POS-tagged ngrams using Stanford
POS Tagger (Toutanova and Manning (2000)). We
filter out the ngrams with punctuations and other
special characters to reduce the noise.
3.1 Comparison against Conventional
Sentiment Lexicon
Note that we consider the connotation lexicon to
be inclusive of a sentiment lexicon for two prac-
tical reasons: first, it is highly unlikely that any
word with non-neutral sentiment (i.e., positive or
negative) would carry connotation of the oppo-
site, i.e., conflicting10 polarity. Second, for some
words with distinct sentiment or strong connota-
tion, it can be difficult or even unnatural to draw a
precise distinction between connotation and senti-
ment, e.g., ?efficient?. Therefore, sentiment lexi-
cons can serve as a surrogate to measure a subset
of connotation words induced by the algorithms,
as shown in Table 3 with respect to General In-
quirer (Stone and Hunt (1963)) and MPQA (Wil-
son et al (2005b)).11
Discussion Table 3 shows the agreement statis-
tics with respect to two conventional sentiment
lexicons. We find that the use of label propaga-
tion alone [PRED-ARG (CP)] improves the per-
formance substantially over the comparable graph
construction with different graph analysis algo-
rithms, in particular, HITS and PageRank ap-
proaches of Feng et al (2011). The two com-
pletely connected variants of the graph propa-
gation on the Pred-Arg graph, [? PRED-ARG
(PMI)] and [? PRED-ARG (CP)], do not neces-
sarily improve the performance over the simpler
and computationally lighter alternative, [PRED-
ARG (CP)]. The [OVERLAY], which is based
on both Pred-Arg and Arg-Arg subgraphs (?2.2),
achieves the best performance among graph-based
algorithms, significantly improving the precision
over all other baselines. This result suggests:
1 The sub-graph #2, based on the semantic par-
allelism of coordination, is simple and yet
very powerful as an inductive bias.
2 The performance of graph propagation varies
significantly depending on the graph topol-
ogy and the corresponding edge weights.
Note that a direct comparison against ILP for top
N words is tricky, as ILP does not rank results.
Only for comparison purposes however, we assign
10We consider ?positive? and ?negative? polarities conflict,
but ?neutral? polarity does not conflict with any.
11In the case of General Inquirer, we use words in POSITIV
and NEGATIV sets as words with positive and negative labels
respectively.
1778
GENINQ EVAL MPQA EVAL
100 1,000 5,000 10,000 ALL 100 1,000 5,000 10,000 ALL
ILP 97.6 94.5 84.5 80.8 80.4 98.0 89.7 84.6 81.2 78.4
OVERLAY 97.0 95.1 78.8 (78.3) 78.3 98.0 93.4 82.1 77.7 77.7? PRED-ARG (PMI) 91.0 91.4 76.1 (76.1) 76.1 88.0 89.1 78.8 75.1 75.1?PRED-ARG (CP) 88.0 85.4 76.2 (76.2) 76.2 87.0 82.6 78.0 76.3 76.3
PRED-ARG (CP) 91.0 91.0 81.0 (81.0) 81.0 88.0 91.5 80.0 78.3 78.3
HITS-ASYMT 77.0 68.8 - - 66.5 86.3 81.3 - - 72.2
PAGERANK-ASYMF 77.0 68.5 - - 65.7 87.2 80.3 - - 72.3
Table 3: Evaluation of Induction Algorithms (?2) with respect to Sentiment Lexicons (precision%).
ranks based on the frequency of words for ILP. Be-
cause of this issue, the performance of top ?1k
words of ILP should be considered only as a con-
servative measure. Importantly, when evaluated
over more than top 5k words, ILP is overall the
top performer considering both precision (shown
in Table 3) and coverage (omitted for brevity).12
4 Precision, Coverage, and Efficiency
In this section, we address three important aspects
of an ideal induction algorithm: precision, cover-
age, and efficiency. For brevity, the remainder of
the paper will focus on the algorithms based on
constraint optimization, as it turned out to be the
most effective one from the empirical results in ?3.
Precision In order to see the effectiveness of the
induction algorithms more sharply, we had used a
limited set of seed words in ?3. However to build a
lexicon with substantially enhanced precision, we
will use as a large seed set as possible, e.g., entire
sentiment lexicons13.
Broad coverage Although statistics in Google
1T corpus represent a very large amount of text,
words that appear in pred-arg and coordination re-
lations are still limited. To substantially increase
the coverage, we will leverage dictionary words
(that are not in the corpus) as described in ?2.3
and Figure 2.
Efficiency One practical problem with ILP is ef-
ficiency and scalability. In particular, we found
that it becomes nearly impractical to run the ILP
formulation including all words in WordNet plus
all words in the argument position in Google Web
1T. We therefore explore an alternative approach
based on Linear Programming in what follows.
12In fact, the performance of PRED-ARG variants for top
10K w.r.t. GENINQ is not meaningful as no additional word
was matched beyond top 5k words.
13Note that doing so will prevent us from evaluating
against the same sentiment lexicon used as a seed set.
4.1 Induction using Linear Programming
One straightforward option for Linear Program-
ming formulation may seem like using the same
Integer Linear Programming formulation intro-
duced in ?2.3, only changing the variable defini-
tions to be real values ? [0, 1] rather than integers.
However, because the hard constraints in ?2.3 are
defined based on the assumption that all the vari-
ables are binary integers, those constraints are not
as meaningful when considered for real numbers.
Therefore we revise those hard constraints to en-
code various semantic relations (WordNet and se-
mantic coordination) more directly.
Definition of variables: For each word i, we de-
fine variables xi, yi, zi ? [0, 1]. i has a positive
(negative) connotation if and only if the xi (yi) is
assigned the greatest value among the three vari-
ables; otherwise, i is neutral.
Objective function: We aim to maximize:
F = ?prosody + ?coord + ?syn + ?ant + ?neu
?prosody =
Rpred+?
i,j
wpred
+
i,j ? xj +
Rpred??
i,j
wpred
?
i,j ? yj
?coord =
Rcoord?
i,j
wcoordi,j ? (dc++i,j + dc??i,j )
?syn = W syn
Rsyn?
i,j
(ds++i,j + ds??i,j )
?ant = W ant
Rant?
i,j
(da++i,j + da??i,j )
?neu = ?
Rpred?
i,j
wpredi,j ? zj
Hard constraints We add penalties to the
objective function if the polarity of a pair of words
is not consistent with its corresponding semantic
relations. For example, for synonyms i and j, we
introduce a penalty W syn (a positive constant) for
ds++i,j , ds??i,j ? [?1, 0], where we set the upper
bound of ds++i,j (ds??i,j ) as the signed distance of
1779
FORMULA POSITIVE NEGATIVE ALLR P F R P F R P F
ILP ?prosody + Csyn + Cant 51.4 85.7 64.3 44.7 87.9 59.3 48.0 86.8 61.8
?prosody + Csyn + Cant + CS 61.2 93.3 73.9 52.4 92.2 66.8 56.8 92.8 70.5
?prosody + ?coord + Csyn + Cant 67.3 75.0 70.9 53.7 84.4 65.6 60.5 79.7 68.8
?prosody + ?coord + Csyn + Cant + CS 62.2 96.0 75.5 51.5 89.5 65.4 56.9 92.8 70.5
LP ?prosody + ?syn + ?ant 24.4 76.0 36.9 23.6 78.8 36.3 24.0 77.4 36.6
?prosody + ?syn + ?ant + ?S 71.6 87.8 78.9 68.8 84.6 75.9 70.2 86.2 77.4
?prosody + ?coord + ?syn + ?ant 67.9 92.6 78.3 64.6 89.1 74.9 66.3 90.8 76.6
?prosody + ?coord + ?syn + ?ant + ?S 78.6 90.5 84.1 73.3 87.1 79.6 75.9 88.8 81.8
Table 4: ILP/LP Comparison on MQPA? (%).
xi and xj (yi and yj) as shown below:
For (i, j) ? Rsyn,
ds++i,j ? xi ? xj , ds++i,j ? xj ? xi
ds??i,j ? yi ? yj , ds??i,j ? yj ? yi
Notice that ds++i,j , ds??i,j satisfying above inequal-
ities will be always of negative values, hence in
order to maximize the objective function, the LP
solver will try to minimize the absolute values of
ds++i,j , ds??i,j , effectively pushing i and j toward
the same polarity. Constraints for semantic coor-
dination Rcoord can be defined similarly. Lastly,
following constraints encode antonym relations:
For (i, j) ? Rant ,
da++i,j ? xi ? (1? xj), da++i,j ? (1? xj)? xi
da??i,j ? yi ? (1? yj), da??i,j ? (1? yj)? yi
Interpretation Unlike ILP, some of the vari-
ables result in fractional values. We consider a
word has positive or negative polarity only if the
assignment indicates 1 for the corresponding po-
larity and 0 for the rest. In other words, we treat
all words with fractional assignments over differ-
ent polarities as neutral. Because the optimal so-
lutions of LP correspond to extreme points in the
convex polytope formed by the constraints, we ob-
tain a large portion of words with non-fractional
assignments toward non-neutral polarities. Alter-
natively, one can round up fractional values.
4.2 Empirical Comparisons: ILP v.s. LP
To solve the ILP/LP, we run ILOG CPLEX Opti-
mizer (CPLEX, 2009)) on a 3.5GHz 6 core CPU
machine with 96GB RAM. Efficiency-wise, LP
runs within 10 minutes while ILP takes several
hours. Table 4 shows the results evaluated against
MPQA for different variations of ILP and LP.
We find that LP variants much better recall and
F-score, while maintaining comparable precision.
Therefore, we choose the connotation lexicon by
LP (C-LP) in the following evaluations in ?5.
5 Experimental Results II
In this section, we present comprehensive intrin-
sic ?5.1 and extrinsic ?5.2 evaluations comparing
three representative lexicons from ?2 & ?4: C-
LP, OVERLAY, PRED-ARG (CP), and two popular
sentiment lexicons: SentiWordNet (Baccianella et
al., 2010) and GI+MPQA.14 Note that C-LP is the
largest among all connotation lexicons, including
?70,000 polar words.15
5.1 Intrinsic Evaluation: Human Judgements
We evaluate 4000 words16 using Amazon Me-
chanical Turk (AMT). Because we expect that
judging a connotation can be dependent on one?s
cultural background, personality and value sys-
tems, we gather judgements from 5 people for
each word, from which we hope to draw a more
general judgement of connotative polarity. About
300 unique Turkers participated the evaluation
tasks. We gather gold standard only for those
words for which more than half of the judges
agreed on the same polarity. Otherwise we treat
them as ambiguous cases.17 Figure 3 shows a part
of the AMT task, where Turkers are presented with
questions that help judges to determine the subtle
connotative polarity of each word, then asked to
rate the degree of connotation on a scale from -
5 (most negative) and 5 (most positive). To draw
14GI+MPQA is the union of General Inquirer and MPQA.
The GI, we use words in the ?Positiv? & ?Negativ? set. For
SentiWordNet, to retrieve the polarity of a given word, we
sum over the polarity scores over all senses, where positive
(negative) values correspond to positive (negative) polarity.
15?13k adj, ?6k verbs, ?28k nouns, ?22k proper nouns.
16We choose words that are not already in GI+MPQA and
obtain most frequent 10,000 words based on the unigram fre-
quency in Google-Ngram, then randomly select 4000 words.
17We allow Turkers to mark words that can be used with
both positive and negative connotation, which results in about
7% of words that are excluded from the gold standard set.
1780
Figure 3: A Part of AMT Task Design.
YES NO
QUESTION % Avg % Avg
?Enjoyable or pleasant? 43.3 2.9 16.3 -2.4
?Of a good quality? 56.7 2.5 6.1 -2.7
?Respectable / honourable? 21.0 3.3 14.0 -1.1
?Would like to do or have? 52.5 2.8 11.5 -2.4
Table 5: Distribution of Answers from AMT.
the gold standard, we consider two different voting
schemes:
? ?V ote: The judgement of each Turker is
mapped to neutral for ?1 ? score ? 1, pos-
itive for score ? 2, negative for score ? 2,
then we take the majority vote.
? ?Score: Let ?(i) be the sum (weighted vote)
of the scores given by 5 judges for word i.
Then we determine the polarity label l(i) of i
as:
l(i) =
?
?
?
positive if ?(i) > 1
negative if ?(i) < ?1
neutral if ?1 ? ?(i) ? 1
The resulting distribution of judgements is shown
in Table 5 & 6. Interestingly, we observe
that among the relatively frequently used English
words, there are overwhelmingly more positively
connotative words than negative ones.
In Table 7, we show the percentage of words
with the same label over the mutual words by the
two lexicon. The highest agreement is 77% by
C-LP and the gold standard by AMTV ote. How
good is this? It depends on what is the natural de-
gree of agreement over subtle connotation among
people. Therefore, we also report the degree of
agreement among human judges in Table 7, where
we compute the agreement of one Turker with re-
spect to the gold standard drawn from the rest of
the Turkers, and take the average across over all
five Turkers18. Interestingly, the performance of
18In order to draw the gold standard from the 4 remaining
Turkers, we consider adjusted versions of ?V ote and ?Score
schemes described above.
POS NEG NEU UNDETERMINED
?V ote 50.4 14.6 24.1 10.9
?Score 67.9 20.6 11.5 n/a
Table 6: Distribution of Connotative Polarity from
AMT.
C-LP SENTIWN HUMAN JUDGES
?V ote 77.0 71.5 66.0
?Score 73.0 69.0 69.0
Table 7: Agreement (Accuracy) against AMT-
driven Gold Standard.
Turkers is not as good as that of C-LP lexicon. We
conjecture that this could be due to generally vary-
ing perception of different people on the connota-
tive polarity,19 while the corpus-driven induction
algorithms focus on the general connotative po-
larity corresponding to the most prevalent senses
of words in the corpus.
5.2 Extrinsic Evaluation
We conduct lexicon-based binary sentiment clas-
sification on the following two corpora.
SemEval From the SemEval task, we obtain a
set of news headlines with annotated scores (rang-
ing from -100 to 87). The positive/negative scores
indicate the degree of positive/negative polarity
orientation. We construct several sets of the posi-
tive and negative texts by setting thresholds on the
scores as shown in Table 8. ?? n? indicates that
the positive set consists of the texts with scores
? n and the negative set consists of the texts with
scores ? ?n.
Emoticon tweets The sentiment Twitter data20
consists of tweets containing either a smiley
emoticon (positive sentiment) or a frowny emoti-
con (negative sentiment). We filter out the tweets
with question marks or more than 30 words, and
keep the ones with at least two words in the union
of all polar words in the five lexicons in Table 8,
and then randomly select 10000 per class.
We denote the short text (e.g., content of tweets
or headline texts from SemEval) by t. w repre-
sents the word in t. W+/W? is the set of posi-
19Pearson correlation coefficient among turkers is 0.28,
which corresponds to a positive small to medium correlation.
Note that when the annotation of turkers is aggregated, we
observe agreement as high as 77% with respect to the learned
connotation lexicon.
20http://www.stanford.edu/?alecmgo/
cs224n/twitterdata.2009.05.25.c.zip
1781
DATA
LEXICON TWEET SEMEVAL
?20 ?40 ?60 ?80
C-LP 70.1 70.8 74.6 80.8 93.5
OVERLAY 68.5 70.0 72.9 76.8 89.6
PRED-ARG (CP) 60.5 64.2 69.3 70.3 79.2
SENTIWN 67.4 61.0 64.5 70.5 79.0
GI+MPQA 65.0 64.5 69.0 74.0 80.5
Table 8: Accuracy on Sentiment Classification
(%).
tive/negative words of the lexicon. We define the
weight of w as s(w). If w is adjective, s(w) = 2;
otherwise s(w) = 1. Then the polarity of each text
is determined as follows:
pol(t) =
?
????
????
positive if
W+?
w?t
s(w) ?
W??
w?t
s(w)
negative if
W+?
w?t
s(w) <
W??
w?t
s(w)
As shown in Table 8, C-LP generally performs
better than the other lexicons on both corpora.
Considering that only very simple classification
strategy is applied, the result by the connotation
lexicon is quite promising.
Finally, Table 1 highlights interesting exam-
ples of proper nouns with connotative polarity,
e.g., ?Mandela?, ?Google?, ?Hawaii? with pos-
itive connotation, and ?Monsanto?, ?Hallibur-
ton?, ?Enron? with negative connotation, sug-
gesting that our algorithms could potentially serve
as a proxy to track the general connotation of real
world entities. Table 2 shows example common
nouns with connotative polarity.
5.3 Practical Remarks on WSD and MWEs
In this work we aim to find the polarity of most
prevalent senses of each word, in part because it
is not easy to perform unsupervised word sense
disambiguation (WSD) on a large corpus in a reli-
able way, especially when the corpus consists pri-
marily of short n-grams. Although the resulting
lexicon loses on some of the polysemous words
with potentially opposite polarities, per-word con-
notation (rather than per-sense connotation) does
have a practical value: it provides a convenient
option for users who wish to avoid the burden of
WSD before utilizing the lexicon. Future work in-
cludes handling of WSD and multi-word expres-
sions (MWEs), e.g., ?Great Leader? (for Kim
Jong-Il), ?Inglourious Basterds? (a movie title).21
21These examples credit to an anonymous reviewer.
6 Related Work
A very interesting work of Mohammad and Tur-
ney (2010) uses Mechanical Turk in order to build
the lexicon of emotions evoked by words. In con-
trast, we present an automatic approach that in-
fers the general connotation of words. Velikovich
et al (2010) use graph propagation algorithms for
constructing a web-scale polarity lexicon for sen-
timent analysis. Although we employ the same
graph propagation algorithm, our graph construc-
tion is fundamentally different in that we integrate
stronger inductive biases into the graph topology
and the corresponding edge weights. As shown
in our experimental results, we find that judicious
construction of graph structure, exploiting multi-
ple complementing linguistic phenomena can en-
hance both the performance and the efficiency of
the algorithm substantially. Other interesting ap-
proaches include one based on min-cut (Dong et
al., 2012) or LDA (Xie and Li, 2012). Our pro-
posed approaches are more suitable for encoding
a much diverse set of linguistic phenomena how-
ever. But our work use a few seed predicates with
selectional preference instead of relying on word
similarity. Some recent work explored the use
of constraint optimization framework for inducing
domain-dependent sentiment lexicon (Choi and
Cardie (2009), Lu et al (2011)). Our work dif-
fers in that we provide comprehensive insights into
different formulations of ILP and LP, aiming to
learn the much different task of learning the gen-
eral connotation of words.
7 Conclusion
We presented a broad-coverage connotation lexi-
con that determines the subtle nuanced sentiment
of even those words that are objective on the sur-
face, including the general connotation of real-
world named entities. Via a comprehensive eval-
uation, we provided empirical insights into three
different types of induction algorithms, and pro-
posed one with good precision, coverage, and effi-
ciency.
Acknowledgments
This research was supported in part by the Stony
Brook University Office of the Vice President for
Research. We thank reviewers for many insightful
comments and suggestions, and for providing us
with several very inspiring examples to work with.
1782
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
J. Kathryn Bock. 1986. Syntactic persistence
in language production. Cognitive psychology,
18(3):355?387.
Thorsten Brants and Alex Franz. 2006. {Web 1T 5-
gram Version 1}.
Yejin Choi and Claire Cardie. 2009. Adapting a po-
larity lexicon using integer linear programming for
domain-specific sentiment classification. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2 -
Volume 2, EMNLP ?09, pages 590?598, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16:22?29, March.
ILOG CPLEX. 2009. High-performance software for
mathematical programming and optimization. U RL
http://www.ilog.com/products/cplex.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the
Fourteenth Conference on Computational Natural
Language Learning, CoNLL ?10, pages 107?116,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Xishuang Dong, Qibo Zou, and Yi Guan. 2012. Set-
similarity joins based semi-supervised sentiment
analysis. In Neural Information Processing, pages
176?183. Springer.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC06), pages 417?422.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 1092?1103. Association for Computa-
tional Linguistics.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 503?511, Boulder, Colorado, June.
Association for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the eighth conference on
European chapter of the Association for Computa-
tional Linguistics, pages 174?181. Association for
Computational Linguistics.
Bas Heerschop, Alexander Hogenboom, and Flavius
Frasincar. 2011. Sentiment lexicon creation from
lexical resources. In Business Information Systems,
pages 185?196. Springer.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive col-
lection of html documents. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. JOURNAL OF THE ACM,
46(5):604?632.
Bill Louw. 1993. Irony in the text or insincerity in
the writer. Text and technology: In honour of John
Sinclair, pages 157?176.
Yue Lu, Malu Castellanos, Umeshwar Dayal, and
ChengXiang Zhai. 2011. Automatic construction
of a context-aware sentiment lexicon: an optimiza-
tion approach. In Proceedings of the 20th interna-
tional conference on World wide web, pages 347?
356. ACM.
Saif Mohammad and Peter Turney. 2010. Emotions
evoked by common words and phrases: Using me-
chanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 26?34, Los Angeles,
CA, June. Association for Computational Linguis-
tics.
Arturo Montejo-Ra?ez, Eugenio Mart??nez-Ca?mara,
M. Teresa Mart??n-Valdivia, and L. Alfonso Uren?a
Lo?pez. 2012. Random walk weighting over sen-
tiwordnet for sentiment polarity detection on twit-
ter. In Proceedings of the 3rd Workshop in Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 3?10, Jeju, Korea, July. Association
for Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report
1999-66, Stanford InfoLab, November.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135.
Martin J Pickering and Holly P Branigan. 1998. The
representation of verbs: Evidence from syntactic
priming in language production. Journal of Mem-
ory and Language, 39(4):633?651.
1783
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1199?1204, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. Defense Technical Information Center.
John Sinclair. 1991. Corpus, concordance, colloca-
tion. Describing English language. Oxford Univer-
sity Press.
Anatol Stefanowitsch and Stefan Th Gries. 2003. Col-
lostructions: Investigating the interaction of words
and constructions. International journal of corpus
linguistics, 8(2):209?243.
Philip J. Stone and Earl B. Hunt. 1963. A computer
approach to content analysis: studies using the gen-
eral inquirer system. In Proceedings of the May 21-
23, 1963, spring joint computer conference, AFIPS
?63 (Spring), pages 241?256, New York, NY, USA.
ACM.
Michael Stubbs. 1995. Collocations and semantic pro-
files: on the cause of the trouble with quantitative
studies. Functions of language, 2(1):23?55.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in
a maximum entropy part-of-speech tagger. In In
EMNLP/VLC 2000, pages 63?70.
Peter Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The via-
bility of web-derived polarity lexicons. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Eval-
uation (formerly Computers and the Humanities),
39(2/3):164?210.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: a system for subjec-
tivity analysis. In Proceedings of HLT/EMNLP on
Interactive Demonstrations, pages 34?35, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ?05: Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 347?354, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Rui Xie and Chunping Li. 2012. Lexicon construc-
tion: A topic model approach. In Systems and Infor-
matics (ICSAI), 2012 International Conference on,
pages 2299?2303. IEEE.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. In Technical Report CMU-CALD-02-107.
CarnegieMellon University.
1784
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1544?1554,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
ConnotationWordNet:
Learning Connotation over the Word+Sense Network
Jun Seok Kang Song Feng Leman Akoglu Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
junkang, songfeng, leman, ychoi@cs.stonybrook.edu
Abstract
We introduce ConnotationWordNet, a con-
notation lexicon over the network of words
in conjunction with senses. We formulate
the lexicon induction problem as collec-
tive inference over pairwise-Markov Ran-
dom Fields, and present a loopy belief
propagation algorithm for inference. The
key aspect of our method is that it is
the first unified approach that assigns the
polarity of both word- and sense-level
connotations, exploiting the innate bipar-
tite graph structure encoded in WordNet.
We present comprehensive evaluation to
demonstrate the quality and utility of the
resulting lexicon in comparison to existing
connotation and sentiment lexicons.
1 Introduction
We introduce ConnotationWordNet, a connotation
lexicon over the network of words in conjunction
with senses, as defined in WordNet. A connotation
lexicon, as introduced first by Feng et al (2011),
aims to encompass subtle shades of sentiment a
word may conjure, even for seemingly objective
words such as ?sculpture?, ?Ph.D.?, ?rosettes?.
Understanding the rich and complex layers of con-
notation remains to be a challenging task. As a
starting point, we study a more feasible task of
learning the polarity of connotation.
For non-polysemous words, which constitute a
significant portion of English vocabulary, learning
the general connotation at the word-level (rather
than at the sense-level) would be a natural oper-
ational choice. However, for polysemous words,
which correspond to most frequently used words,
it would be an overly crude assumption that the
same connotative polarity should be assigned for
all senses of a given word. For example, consider
?abound?, for which lexicographers of WordNet
prescribe two different senses:
? (v) abound: (be abundant of plentiful; exist
in large quantities)
? (v) abound, burst, bristle: (be in a state of
movement or action) ?The room abounded
with screaming children?; ?The garden bris-
tled with toddlers?
For the first sense, which is the most commonly
used sense for ?abound?, the general overtone of
the connotation would seem positive. That is, al-
though one can use this sense in both positive and
negative contexts, this sense of ?abound? seems
to collocate more often with items that are good to
be abundant (e.g., ?resources?), than unfortunate
items being abundant (e.g., ?complaints?).
However, as for the second sense, for which
?burst? and ?bristle? can be used interchangeably
with respect to this particular sense,
1
the general
overtone is slightly more negative with a touch of
unpleasantness, or at least not as positive as that of
the first sense. Especially if we look up the Word-
Net entry for ?bristle?, there are noticeably more
negatively connotative words involved in its gloss
and examples.
This word sense issue has been a universal chal-
lenge for a range of Natural Language Processing
applications, including sentiment analysis. Recent
studies have shown that it is fruitful to tease out
subjectivity and objectivity corresponding to dif-
ferent senses of the same word, in order to improve
computational approaches to sentiment analysis
(e.g. Pestian et al (2012), Mihalcea et al (2012)
Balahur et al (2014)). Encouraged by these recent
successes, in this study, we investigate if we can
attain similar gains if we model the connotative
polarity of senses separately.
There is one potential practical issue we would
like to point out in building a sense-level lexical
resource, however. End-users of such a lexicon
may not wish to deal with Word Sense Disam-
1
Hence a sense in WordNet is defined by synset (= syn-
onym set), which is the set of words sharing the same sense.
1544
biguation (WSD), which is known to be often too
noisy to be incorporated into the pipeline with re-
spect to other NLP tasks. As a result, researchers
often would need to aggregate labels across differ-
ent senses to derive the word-level label. Although
such aggregation is not entirely unreasonable, it
does not seem to be the most optimal and princi-
pled way of integrating available resources.
Therefore, in this work, we present the first uni-
fied approach that learns both sense- and word-
level connotations simultaneously. This way, end-
users will have access to more accurate sense-level
connotation labels if needed, while also having ac-
cess to more general word-level connotation la-
bels. We formulate the lexicon induction problem
as collective inference over pairwise-Markov Ran-
dom Fields (pairwise-MRF) and derive a loopy be-
lief propagation algorithm for inference.
The key aspect of our approach is that we ex-
ploit the innate bipartite graph structure between
words and senses encoded in WordNet. Although
our approach seems conceptually natural, previous
approaches, to our best knowledge, have not di-
rectly exploited these relations between words and
senses for the purpose of deriving lexical knowl-
edge over words and senses collectively. In ad-
dition, previous studies (for both sentiment and
connotation lexicons) aimed to produce only ei-
ther of the two aspects of the polarity: word-level
or sense-level, while we address both.
Another contribution of our work is the intro-
duction of loopy belief propagation (loopy-BP)
as a lexicon induction algorithm. Loopy-BP in
our study achieves statistically significantly better
performance over the constraint optimization ap-
proaches previously explored. In addition, it runs
much faster and it is considerably easier to imple-
ment. Last but not least, by using probabilistic rep-
resentation of pairwise-MRF in conjunction with
Loopy-BP as inference, the resulting solution has
the natural interpretation as the intensity of con-
notation. This contrasts to approaches that seek
discrete solutions such as Integer Linear Program-
ming(Papadimitriou and Steiglitz, 1998).
ConnotationWordNet, the final outcome of our
study, is a new lexical resource that has conno-
tation labels over both words and senses follow-
ing the structure of WordNet. The lexicon is pub-
licly available at: http://www.cs.sunysb.
edu/
?
junkang/connotation_wordnet.)
In what follows, we will first describe the net-
memberships)antonyms)Pred1Arg)Arg1Arg)
?)
?)
prevent)
suffer)
enjoy)
achieve)
pain)
losses)
life)
profit)
success)
win)
investment)
injure)accident)
wound)ache)
word)sense)
gain)
hurt)
put)on)
lose)
injure,))wound)
gain,))put)on)
win,)gain,)acquire)
win,)profits,)))winnings)
ConnotaAve)Predicates) Arguments) Senses)
Figure 1: G
WORD+SENSE
with words and senses.
work of words and senses (Section 2), then intro-
duce the representation of the network structure as
pairwise Markov Random Fields, and a loopy be-
lief propagation algorithm as collective inference
(Section 3). We then present comprehensive eval-
uation (Section 4 & 5 & 6), followed by related
work (Section 7) and conclusion (Section 8).
2 Network of Words and Senses
The connotation graph, called G
WORD+SENSE
, is a
heterogeneous graph with multiple types of nodes
and edges. As shown in Figure 1, it contains two
types of nodes; (i) lemmas (i.e., words, 115K)
and (ii) synsets (63K), and four types of edges;
(t
1
) predicate-argument (179K), (t
2
) argument-
argument (144K), (t
3
) argument-synset (126K),
and (t
4
) synset-synset (3.4K) edges.
The predicate-argument edges, first introduced
by Feng et al (2011), depict the selectional prefer-
ence of connotative predicates (i.e., the polarity of
a predicate indicates the polarity of its arguments)
and encode their co-occurrence relations based
on the Google Web 1T corpus. The argument-
argument edges are based on the distributional
similarities among the arguments. The argument-
synset edges capture the synonymy between argu-
ment nodes through the corresponding synsets. Fi-
nally, the synset-synset edges depict the antonym
relations between synset pairs.
In general, our graph construction is similar to
that of Feng et al (2013), but there are a few im-
portant differences. Most notably, we model both
words and synsets explicitly, and exploit the mem-
bership relations between words and senses. We
expect that edges between words and senses will
encourage senses that belong to the same word to
1545
receive the same connotation label. Conversely,
we expect that these edges will also encourage
words that belong to the same sense (i.e., synset
definition) to receive the same connotation label.
Another benefit of our approach is that for var-
ious WordNet relations (e.g., antonym relations),
which are defined over synsets (not over words),
we can add edges directly between corresponding
synsets, rather than projecting (i.e., approximat-
ing) those relations over words. Note that the lat-
ter, which has been employed by several previous
studies (e.g., Kamps et al (2004), Takamura et al
(2005), Andreevskaia and Bergler (2006), Su and
Markert (2009), Lu et al (2011), Kaji and Kit-
suregawa (2007), Feng et al (2013)), could be a
source of noise, as one needs to assume that the
semantic relation between a pair of synsets trans-
fers over the pair of words corresponding to that
pair of synsets. For polysemous words, this as-
sumption may be overly strong.
3 Pairwise Markov Random Fields and
Loopy Belief Propagation
We formulate the task of learning sense- and word-
level connotation lexicon as a graph-based clas-
sification task (Sen et al, 2008). More formally,
we denote the connotation graph G
WORD+SENSE
by
G = (V,E), in which a total of n word and synset
nodes V = {v
1
, . . . , v
n
} are connected with
typed edges e(v
i
, v
j
, t) ? E, where edge types
t ? {pred-arg, arg-arg, syn-arg, syn-syn} de-
pict the four edge types as described in Section
2. A neighborhood function N , where N
v
=
{u| e(u, v) ? E} ? V , describes the underlying
network structure.
In our collective classification formulation, each
node in V is represented as a random variable that
takes a value from an appropriate class label do-
main; in our case, L = {+,?} for positive and
negative connotation. In this classification task,
we denote by Y the nodes the labels of which need
to be assigned, and let y
i
refer to Y
i
?s label.
3.1 Pairwise Markov Random Fields
We next define our objective function. We pro-
pose to use an objective formulation that utilizes
pairwise Markov Random Fields (MRFs) (Kinder-
mann and Snell, 1980), which we adapt to our
problem setting. MRFs are a class of probabilistic
graphical models that are suited for solving infer-
ence problems in networked data. An MRF con-
sists of an undirected graph where each node can
be in any of a finite number of states (i.e., class
labels). The state of a node is assumed to be de-
pendent on each of its neighbors and independent
of other nodes in the graph.
2
In pairwise MRFs,
the joint probability of the graph can be written as
a product of pairwise factors, parameterized over
the edges. These factors are referred to as clique
potentials in general MRFs, which are essentially
functions that collectively determine the graph?s
joint probability.
Specifically, let G = (V,E) denote a network
of random variables, where V consists of the un-
observed variables Y that need to be assigned val-
ues from label set L. Let ? denote a set of clique
potentials that consists of two types of factors:
? For each Y
i
? Y , ?
i
? ? is a prior map-
ping ?
i
: L ? R
?0
, where R
?0
denotes non-
negative real numbers.
? For each e(Y
i
, Y
j
, t) ? E, ?
t
ij
? ? is a com-
patibility mapping ?
t
ij
: L ? L ? R
?0
.
Objective formulation Given an assignment y
to all the unobserved variables Y and x to ob-
served ones X (variables with known labels, if
any), our objective function is associated with the
following joint probability distribution
P (y|x) =
1
Z(x)
?
Y
i
?Y
?
i
(y
i
)
?
e(Y
i
,Y
j
,t)?E
?
t
ij
(y
i
, y
j
)
(1)
where Z(x) is the normalization function. Our
goal is then to infer the maximum likelihood as-
signment of states (i.e., labels) to unobserved vari-
ables (i.e., nodes) that will maximize Equation (1).
Problem Definition Having introduced our
graph-based classification task and objective for-
mulation, we define our problem more formally.
Given
- a connotation graph G = (V,E) of words
and synsets connected with typed edges,
- prior knowledge (i.e., probabilities) of (some
or all) nodes belonging to each class,
- compatibility of two nodes with a given pair
of labels being connected to each other;
Classify the nodes Y
i
? Y , into one of two classes;
L = {+,?}, such that the class assignments y
i
maximize our objective in Equation (1).
We can further rank the network objects by the
probability of their connotation polarity.
2
This assumption yields a pairwise Markov Random Field
(MRF); a special case of general MRFs (Yedidia et al, 2003).
1546
3.2 Loopy Belief Propagation
Finding the best assignments to unobserved vari-
ables in our objective function is the inference
problem. The brute force approach through enu-
meration of all possible assignments is exponen-
tial and thus intractable. In general, exact in-
ference is known to be NP-hard and there is
no known algorithm which can be theoretically
shown to solve the inference problem for gen-
eral MRFs. Therefore in this work, we em-
ploy a computationally tractable (in fact linearly
scalable with network size) approximate infer-
ence algorithm called Loopy Belief Propagation
(LBP) (Yedidia et al, 2003), which we extend to
handle typed graphs like our connotation graph.
Our inference algorithm is based on iterative
message passing and the core of it can be concisely
expressed as the following two equations:
m
i?j
(y
j
) = ?
?
y
i
?L
(
?
t
ij
(y
i
, y
j
) ?
i
(y
i
)
?
Y
k
?N
i
?Y\Y
j
m
k?i
(y
i
)
)
, ?y
j
? L (2)
b
i
(y
i
) = ? ?
i
(y
i
)
?
Y
j
?N
i
?Y
m
j?i
(y
i
),?y
i
? L
(3)
A message m
i?j
is sent from node i to node j
and captures the belief of i about j, which is the
probability distribution over the labels of j; i.e.
what i ?thinks? j?s label is, given the current la-
bel of i and the type of the edge that connects i
and j. Beliefs refer to marginal probability dis-
tributions of nodes over labels; for example b
i
(y
i
)
denotes the belief of node i having label y
i
. ? and
? are the normalization constants, which respec-
tively ensure that each message and each set of
marginal probabilities sum to 1. At every iteration,
each node computes its belief based on messages
received from its neighbors, and uses the compat-
ibility mapping to transform its belief into mes-
sages for its neighbors. The key idea is that after
enough iterations of message passes between the
nodes, the ?conversations? are likely to come to a
consensus, which determines the marginal proba-
bilities of all the unknown variables.
The pseudo-code of our method is given in Al-
gorithm 1. It first initializes all messages to 1
and priors to unbiased (i.e., equal) probabilities
for all nodes except the seed nodes for which the
sentiment is known (lines 3-9). It then proceeds
by making each Y
i
? Y communicate messages
Algorithm 1: CONNOTATION INFERENCE
1 Input: Connotation graph G=(V,E), prior
potentials ?
s
for seed words s ? S, and
compatibility potentials ?
t
ij
2 Output: Connotation label probabilities for
each node i ? V \P
3 foreach e(Y
i
, Y
j
, t) ? E do // initialize msg.s
4 foreach y
j
? L do
5 m
i?j
(y
j
)? 1
6 foreach i ? V do // initialize priors
7 foreach y
j
? L do
8 if i ? S then ?
i
(y
j
)? ?
i
(y
j
) else
?
i
(y
j
)? 1/|L|
9 repeat // iterative message passing
10 foreach e(Y
i
, Y
j
, t) ? E, Y
j
? Y
V \S
do
11 foreach y
j
? L do
12 Use Equation (2)
13 until all messages stop changing
14 foreach Y
i
? Y
V \S
do // compute final beliefs
15 foreach y
i
? L do
16 Use Equation (3)
with their neighbors in an iterative fashion until
the messages stabilize (lines 10-14), i.e. conver-
gence is reached.
3
At convergence, we calculate
the marginal probabilities, that is of assigning Y
i
with label y
i
, by computing the final beliefs b
i
(y
i
)
(lines 15-17). We use these maximum likelihood
probabilities for label assignment; for each node i,
we assign the label L
i
? max
y
i
b
i
(y
i
).
To completely define our algorithm, we need to
instantiate the potentials ?, in particular the priors
and the compatibilities, which we discuss next.
Priors The prior beliefs ?
i
of nodes can be suit-
ably initialized if there is any prior knowledge for
their connotation sentiment (e.g., enjoy is posi-
tive, suffer is negative). As such, our method
is flexible to integrate available side information.
In case there is no prior knowledge available, each
node is initialized equally likely to have any of the
possible labels, i.e.,
1
|L|
as in Algorithm 1 (line 9).
Compatibilities The compatibility potentials
can be thought of as matrices, with entries
3
Although convergence is not theoretically guaranteed, in
practice LBP converges to beliefs within a small threshold of
change (e.g., 10
?6
) fairly quickly with accurate results (Pan-
dit et al, 2007; McGlohon et al, 2009; Akoglu et al, 2013).
1547
?t
ij
(y
i
, y
j
) that give the likelihood of a node hav-
ing label y
i
, given that it has a neighbor with label
y
j
to which it is connected through a type t edge.
A key difference of our method from earlier mod-
els is that we use clique potentials that differ for
edge types, since the connotation graph is hetero-
geneous. This is exactly because the compatibil-
ity of class labels of two adjacent nodes depends
on the type of the edge connecting them: e.g.,
+
syn-arg
?????? + is highly compatible, whereas +
syn-syn
?????? + is unlikely; as syn-arg edges capture
synonymy; i.e., words-sense memberships, while
syn-syn edges depict antonym relations.
A sample instantiation of the compatibilities
is shown in Table 1. Notice that the potentials
for pred-arg, arg-arg, and syn-arg capture ho-
mophily, i.e., nodes with the same label are likely
to connect to each other through these types of
edges.
4
On the other hand, syn-syn edges con-
nect nodes that are antonyms of each other, and
thus the compatibilities capture the reverse rela-
tionship among their labels.
Table 1: Instantiation of compatibility potentials.
Entry ?
t
ij
(y
i
, y
j
) is the compatibility of a node
with label y
i
having a neighbor labeled y
j
, given
the edge between i and j is type t, for small .
t: t
1
A
P + ?
+ 1- 
?  1-
t: t
2
A
A + ?
+ 1-2 2
? 2 1-2
(t
1
) pred-arg (t
2
) arg-arg
t: t
3
A
S + ?
+ 1- 
?  1-
t: t
4
S
S + ?
+  1-
? 1- 
(t
3
) syn-arg (t
4
) syn-syn
(synonym relations) (antonym relations)
Complexity analysis Most demanding compo-
nent of Algorithm 1 is the iterative message pass-
ing over the edges (lines 10-14), with time com-
plexity O(ml
2
r), where m = |E| is the num-
ber of edges in the connotation graph, l = |L|,
the classes, and r, the iterations until convergence.
Often, l is quite small (in our case, l = 2) and
r  m. Thus running time grows linearly with the
number of edges and is scalable to large datasets.
4
arg-arg edges are based on co-occurrence (see Section
2), which does not carry as strong indication of the same con-
notation as e.g., synonymy. Thus, we enforce less homophily
for nodes connected through edges of arg-arg type.
4 Evaluation I: Agreement with
Sentiment Lexicons
ConnotationWordNet is expected to be the super-
set of a sentiment lexicon, as it is highly likely for
any word with positive/negative sentiment to carry
connotation of the same polarity. Thus, we use
two conventional sentiment lexicons, General In-
quirer (GENINQ) (Stone et al, 1966) and MPQA
(Wilson et al, 2005b), as surrogates to measure
the performance of our inference algorithm.
4.1 Variants of Graph Construction
The construction of the connotation graph, de-
noted by G
WORD+SENSE
, which includes words and
synsets, has been described in Section 2. In ad-
dition to this graph, we tried several other graph
constructions, the first three of which have previ-
ously been used in (Feng et al, 2013). We briefly
describe these graphs below, and compare perfor-
mance on all the graphs in the proceeding.
G
WORD
W/ PRED-ARG: This is a (bipartite)
subgraph of G
WORD+SENSE
, which only includes
the connotative predicates and their arguments. As
such, it contains only type t
1
edges. The edges
between the predicates and the arguments can be
weighted by their Point-wise Mutual Information
(PMI)
5
based on the Google Web 1T corpus.
G
WORD
W/ OVERLAY: The second graph is also
a proper subgraph of G
WORD+SENSE
, which in-
cludes the predicates and all the argument words.
Predicate words are connected to their arguments
as before. In addition, argument pairs (a
1
, a
2
) are
connected if they occurred together in the ?a
1
and
a
2
? or ?a
2
and a
1
? coordination (Hatzivassiloglou
and McKeown, 1997; Pickering and Branigan,
1998). This graph contains both type t
1
and t
2
edges. The edges can also be weighted based on
the distributional similarities of the word pairs.
G
WORD
: The third graph is a super-graph of
G
WORD
W/ OVERLAY, with additional edges,
where argument pairs in synonym and antonym
relation are connected to each other. Note that un-
like the connotation graph G
WORD+SENSE
, it does
not contain any synset nodes. Rather, the words
that are synonyms or antonyms of each other are
directly linked in the graph. As such, this graph
contains all edge types t
1
through t
4
.
5
PMI scores are widely used in previous studies to mea-
sure association between words (e.g., (Church and Hanks,
1990), (Turney, 2001), (Newman et al, 2009)).
1548
GWORD+SENSE
W/ SYNSIM: This is a super-
graph of our original G
WORD+SENSE
graph; that
is, it has all the predicate, arguments, and synset
nodes, as well as the four types of edges between
them. In addition, we add edges of a fifth type t
5
between the synset nodes to capture their similar-
ity. To define similarity, we use the glossary def-
initions of the synsets and derive three different
scores. Each score utilizes the count(s
1
, s
2
) of
overlapping nouns, verbs, and adjectives/adverbs
among the glosses of the two synsets s
1
and s
2
.
G
WORD+SENSE
W/ SYNSIM1: We discard edges
with count less than 3. The weighted version has
the counts normalized between 0 and 1.
G
WORD+SENSE
W/ SYNSIM2: We normalize
the counts by the length of the gloss (the
avg of two lengths), that is, p = count /
avg(len gloss(s
1
), len gloss(s
2
))
and discard edges with p < 0.5. The weighted
version contains p values as edge weights.
G
WORD+SENSE
W/ SYNSIM3: To further sparsify
the graph we discard edges with p < 0.6. To
weigh the edges, we use the cosine similarity be-
tween the gloss vectors of the synsets based on the
TF-IDF values of the words the glosses contain.
Note that the connotation inference algorithm,
as given in Algorithm 1, remains exactly the same
for all the graphs described above. The only dif-
ference is the set of parameters used; while G
WORD
W/ PRED-ARG and G
WORD
W/ OVERLAY contain
one and two edge types, respectively and only use
compatibilities (t
1
) and (t
2
), G
WORD
uses all four
as given in Table 1. The G
WORD+SENSE
W/ SYN-
SIM graphs use an additional compatibility matrix
for the synset similarity edges of type t
5
, which is
the same as the one used for t
1
, i.e., similar synsets
are likely to have the same connotation label. This
flexibility is one of the key advantages of our al-
gorithm as new types of nodes and edges can be
added to the graph seamlessly.
4.2 Sentiment-Lexicon based Performance
In this section, we first compare the performance
of our connotation graph G
WORD+SENSE
to graphs
that do not include synset nodes but only words.
Then we analyze the performance when the addi-
tional synset similarity edges are added. First, we
briefly describe our performance measures.
The sentiment lexicons we use as gold standard
are small, compared to the size (i.e., number of
words) our graphs contain. Thus, we first find
the overlap between each graph and a senti-
GENINQ MPQA
P R F F
Variations of G
WORD
W/ PRED-ARG 88.0 67.6 76.5 57.3
W/ PRED-ARG-W 84.9 68.9 76.1 57.8
W/ OVERLAY 87.8 70.4 78.1 58.4
W/ OVERLAY-W 82.2 67.7 74.2 54.2
G
WORD
88.5 83.1 85.7 69.7
G
WORD
-W 75.5 71.5 73.4 53.2
Variations of G
WORD+SENSE
G
WORD+SENSE
88.8 84.1 86.4 70.0
G
WORD+SENSE
-W 76.8 73.0 74.9 54.6
W/ SYNSIM1 87.2 83.3 85.2 67.9
W/ SYNSIM2 83.9 80.8 82.3 65.1
W/ SYNSIM3 86.5 83.2 84.8 67.8
W/ SYNSIM1-W 88.0 84.3 86.1 69.2
W/ SYNSIM2-W 86.4 83.7 85.0 68.5
W/ SYNSIM3-W 86.7 83.4 85.0 68.2
Table 2: Connotation inference performance on
various graphs. ?-W? indicates weighted versions
(see ?4.1). P: precision, R: recall, F: F1-score (%).
ment lexicon. Note that the overlap size may be
smaller than the lexicon size, as some sen-
timent words may be missing from our graphs.
Then, we calculate the number of correct la-
bel assignments. As such, precision is defined as
(correct / overlap), and recall as (correct
/ lexicon size). Finally, F1-score is their har-
monic mean and reflects the overall accuracy.
As shown in Table 2 (top), we first observe that
including the synonym and antonym relations in
the graph, as with G
WORD
and G
WORD+SENSE
, im-
prove the performance significantly, almost by an
order of magnitude, over graphs G
WORD
W/ PRED-
ARG and G
WORD
W/ OVERLAY that do not contain
those relation types. Furthermore, we notice that
the performances on the G
WORD+SENSE
graph are
better than those on the word-only graphs. This
shows that including the synset nodes explicitly in
the graph structure is beneficial. What is more,
it gives us a means to obtain connotation labels
for the synsets themselves, which we use in the
evaluations in the next sections. Finally, we note
that using the unweighted versions of the graphs
provide relatively more robust performance, po-
tentially due to noise in the relative edge weights.
Next we analyze the performance when the new
edges between synsets are introduced, as given in
Table 2 (bottom). We observe that connecting the
synset nodes by their gloss-similarity (at least in
the ways we tried) does not yield better perfor-
mance than on our original G
WORD+SENSE
graph.
Different from earlier, the weighted versions of
the similarity based graphs provide better perfor-
1549
mance than their unweighted counterparts. This
suggests that glossary similarity would be a more
robust means to correlate nodes; we leave it as fu-
ture work to explore this direction for predicate-
argument and argument-argument relations.
4.3 Parameter Sensitivity
Our belief propagation based connotation senti-
ment inference algorithm has one user-specified
parameter  (see Table 1). To study the sensitivity
of its performance to the choice of , we reran our
experiments for  = {0.02, 0.04, . . . , 0.24}
6
and
report the accuracy results on our G
WORD+SENSE
in
Figure 2 for the two lexicons. The results indicate
that the performances remain quite stable across a
wide range of the parameter choice.
precisionrecallF-scoreP
erfor
man
ce
0
20
40
60
80
100
?0.02 0.06 0.10 0.14 0.18 0.22
precisionrecallF-scoreP
erfor
manc
e
0
20
40
60
80
100
?0.02 0.06 0.10 0.14 0.18 0.22
(a) GENINQ EVAL (b) MPQA EVAL
Figure 2: Performance is stable across various .
5 Evaluation II: Human Evaluation on
ConnotationWordNet
In this section, we present the result of human
evaluation we executed using Amazon Mechani-
cal Turk (AMT). We collect two separate sets of
labels: a set of labels at the word-level, and an-
other set at the sense-level. We first describe the
labeling process of sense-level connotation: We
selected 350 polysemous words and one of their
senses, and each Turker was asked to rate the con-
notative polarity of a given word (or of a given
sense), from -5 to 5, 0 being the neutral.
7
For each
word, we asked 5 Turkers to rate and we took the
average of the 5 ratings as the connotative inten-
sity score of the word. We labeled a word as nega-
tive if its intensity score is less than 0 and positive
otherwise. For word-level labels we apply similar
procedure as above.
6
Note that for  > 0.25, compatibilities of ?
t
2
in Table 1
are reversed, hence the maximum of 0.24.
7
Because senses in WordNet can be tricky to understand,
care should be taken in designing the task so that the Turkers
will focus only on the corresponding sense of a word. There-
fore, we provided the part of speech tag, the WordNet gloss
of the selected sense, and a few examples as given in Word-
Net. As an incentive, each Turker was rewarded $0.07 per hit
which consists of 10 words to label.
Lexicon Word-level Sense-level
SentiWordNet 27.22 14.29
OpinionFinder 31.95 -
Feng2013 62.72 -
G
WORD+SENSE
(95%) 84.91 83.43
G
WORD+SENSE
(99%) 84.91 83.71
E-G
WORD+SENSE
(95%) 86.98 86.29
E-G
WORD+SENSE
(99%) 86.69 85.71
Table 3: Word-/Sense-level evaluation results
5.1 Word-Level Evaluation
We first evaluate the word-level assignment of
connotation, as shown in Table 3. The agreement
between the new lexicon and human judges varies
between 84% and 86.98%. Sentiment lexicons
such as SentiWordNet (Baccianella et al (2010))
and OpinionFinder (Wilson et al (2005a)) show
low agreement rate with human, which is some-
what as expected: human judges in this study are
labeling for subtle connotation, not for more ex-
plicit sentiment. OpinionFinder?s low agreement
rate was mainly due to the low hit rate of the words
(successful look-up rate, 33.43%). Feng2013 is
the lexicon presented in (Feng et al, 2013) and it
showed a relatively higher 72.13% hit rate.
Note that belief propagation was run until 95%
and 99% of the nodes were converged in their
beliefs. In addition, the seed words with known
connotation labels originally consist of 20 positive
and 20 negative predicates. We also extended the
seed set with the sentiment lexicon words and de-
note these runs with E- for ?Extended?.
5.2 Sense-Level Evaluation
We also examined the agreement rates on the
sense-level. Since OpinionFinder and Feng2013
do not provide the polarity scores at the sense-
level, we excluded them from this evaluation. Be-
cause sense-level polarity assignment is a harder
(more subtle) task, the performance of all lexicons
decreased to some degree in comparison to that of
word-level evaluations.
5.3 Pair-wise Intensity Ranking
A notable goodness of our induction algorithm is
that the outcome of the algorithm can be inter-
preted as an intensity of the corresponding conno-
tation. But are these values meaningful? We an-
swer this question in this section. We formulate a
pair-wise ranking task as a binary decision task as
follows: given a pair of words, we ask which one
is more positive (or more negative) than the other.
Since we collect human labels based on scales, we
1550
Lexicon Correct Undecided
SentiWordNet 33.77 23.34
G
WORD+SENSE
(95%) 74.83 0.58
G
WORD+SENSE
(99%) 73.01 0.58
E-G
WORD+SENSE
(95%) 73.84 1.16
E-G
WORD+SENSE
(99%) 74.01 1.16
Table 4: Results of pair-wise intensity evaluation,
for intensity difference threshold = 2.0
already have this information at hand. Because
different human judges have different notion of
scales however, subtle differences are more likely
to be noisy. Therefore, we experiment with vary-
ing degrees of differences in their scales, as shown
in Figure 3. Threshold values (ranging from 0.5 to
3.0) indicate the minimum differences in scales for
any pair of words, for the pair to be included in the
test set. As expected, we observe that the perfor-
mance improves as we increase the threshold (as
pairs get better separated). Within range [0.5, 1.5]
(249 pairs examined), the accuracies are as high as
68.27%, which shows that even the subtle differ-
ences of the connotative intensities are relatively
well reflected in the new lexicons.
SentiWordNetGWord+Sense(95%)GWord+Sense(99%)e-GWord+Sense(95%)e-GWord+Sense(99%)A
ccu
rac
y (%
)
40
60
80
Threshold
0.5 1.0 2.0 3.0
Figure 3: Trend of accuracy for pair-wise intensity
evaluation over threshold
The results for pair-wise intensity evaluation
(threshold=2.0, 1,208 pairs) are given in Table 4.
Despite that intensity is generally a harder prop-
erty to measure (than the coarser binary catego-
rization of polarities), our connotation lexicons
perform surprisingly well, reaching up to 74.83%
accuracy. Further study on the incorrect cases re-
veals that SentiWordNet has many pair of words
with the same polarity score (23.34%). Such cases
seems to be due to the limited score patterns of
SentiWordNet. The ratio of such cases are ac-
counted as Undecided in Table 4.
6 Evaluation III: Sentiment Analysis
using ConnotationWordNet
Finally, to show the utility of the resulting lexi-
con in the context of a concrete sentiment analysis
task, we perform lexicon-based sentiment analy-
sis. We experiment with SemEval dataset (Strap-
parava and Mihalcea, 2007) that includes the hu-
man labeled dataset for predicting whether a news
headline is a good news or a bad news, which we
expect to have a correlation with the use of con-
notative words that we focus on in this paper. The
good/bad news are annotated with scores (ranging
from -100 to 87). We construct several data sets by
applying different thresholds on scores. For exam-
ple, with the threshold set to 60, we discard the in-
stances whose scores lie between -60 and 60. For
comparison, we also test the connotation lexicon
from (Feng et al, 2013) and the combined senti-
ment lexicon GENINQ+MPQA.
Note that there is a difference in how humans
judge the orientation and the degree of connota-
tion for a given word out of context, and how the
use of such words in context can be perceived as
good/bad news. In particular, we conjecture that
humans may have a bias toward the use of posi-
tive words, which in turn requires calibration from
the readers? minds (Pennebaker and Stone, 2003).
That is, we might need to tone down the level of
positiveness in order to correctly measure the ac-
tual intended positiveness of the message.
With this in mind, we tune the appropriate cali-
bration from a small training data, by using 1 fold
from N fold cross validation, and using the re-
maining N ? 1 folds as testing. We simply learn
the mixture coefficient ? to scale the contribution
of positive and negative connotation values. We
tune this parameter ?
8
for other lexicons we com-
pare against as well. Note that due to this param-
eter learning, we are able to report better perfor-
mance for the connotation lexicon of (Feng et al,
2013) than what the authors have reported in their
paper (labeled with *) in Table 5.
Table 5 shows the results for N=15, where the
new lexicon consistently outperforms other com-
petitive lexicons. In addition, Figure 4 shows that
the performance does not change much based on
the size of training data used for parameter tuning
(N={5, 10, 15, 20}).
7 Related Work
Several previous approaches explored the use of
graph propagation for sentiment lexicon induction
(Velikovich et al, 2010) and connotation lexicon
8
What is reported is based on ? ? {20, 40, 60, 80}. More
detailed parameter search does not change the results much.
1551
Lexicon
SemEval Threshold
20 40 60 80
Instance Size 955 649 341 86
Feng2013 71.5 77.1 81.6 90.5
GENINQ+MPQA 72.8 77.2 80.4 86.7
G
WORD+SENSE
(95%) 74.5 79.4 86.5 91.9
G
WORD+SENSE
(99%) 74.6 79.4 86.8 91.9
E-G
WORD+SENSE
(95%) 72.5 76.8 82.3 87.2
E-G
WORD+SENSE
(99%) 72.6 76.9 82.5 87.2
Feng2013* 70.8 74.6 80.8 93.5
GENINQ+MPQA* 64.5 69.0 74.0 80.5
Table 5: SemEval evaluation results, for N=15
Feng2013MPQA+GenInqGWord+Sense(95%)GWord+Sense(99%)e-GWord+Sense(95%)e-GWord+Sense(99%)
Acc
ura
cy 
(%)
50
60
70
80
N
5 10 15 20
Figure 4: Trend of SemEval performance over N ,
the number of CV folds
induction (Feng et al, 2013). Our work intro-
duces the use of loopy belief propagation over
pairwise-MRF as an alternative solution to these
tasks. At a high-level, both approaches share the
general idea of propagating confidence or belief
over the graph connectivity. The key difference,
however, is that in our MRF representation, we
can explicitly model various types of word-word,
sense-sense and word-sense relations as edge po-
tentials. In particular, we can naturally encode re-
lations that encourage the same assignment (e.g.,
synonym) as well as the opposite assignment (e.g.,
antonym) of the polarity labels. Note that integra-
tion of the latter is not straightforward in the graph
propagation framework.
There have been a number of previous studies
that aim to construct a word-level sentiment lex-
icon (Wiebe et al, 2005; Qiu et al, 2009) and
a sense-level sentiment lexicon (Esuli and Sebas-
tiani, 2006). But none of these approaches con-
sidered to induce the polarity labels at both the
word-level and sense-level. Although we focus on
learning connotative polarity of words and senses
in this paper, the same approach would be applica-
ble to constructing a sentiment lexicon as well.
There have been recent studies that address
word sense disambiguation issues for sentiment
analysis. SentiWordNet (Esuli and Sebastiani,
2006) was the very first lexicon developed for
sense-level labels of sentiment polarity. In recent
years, Akkaya et al (2009) report a successful em-
pirical result where WSD helps improving senti-
ment analysis, while Wiebe and Mihalcea (2006)
study the distinction between objectivity and sub-
jectivity in each different sense of a word, and
their empirical effects in the context of sentiment
analysis. Our work shares the high-level spirit of
accessing the sense-level polarity, while also de-
riving the word-level polarity.
In recent years, there has been a growing re-
search interest in investigating more fine-grained
aspects of lexical sentiment beyond positive and
negative sentiment. For example, Mohammad and
Turney (2010) study the affects words can evoke
in people?s minds, while Bollen et al (2011) study
various moods, e.g., ?tension?, ?depression?, be-
yond simple dichotomy of positive and negative
sentiment. Our work, and some recent work by
Feng et al (2011) and Feng et al (2013) share this
spirit by targeting more subtle, nuanced sentiment
even from those words that would be considered
as objective in early studies of sentiment analysis.
8 Conclusion
We have introduced a novel formulation of lexicon
induction operating over both words and senses,
by exploiting the innate structure between the
words and senses as encoded in WordNet. In addi-
tion, we introduce the use of loopy belief propaga-
tion over pairwise-Markov Random Fields as an
effective lexicon induction algorithm. A notable
strength of our approach is its expressiveness: var-
ious types of prior knowledge and lexical relations
can be encoded as node potentials and edge po-
tentials. In addition, it leads to a lexicon of bet-
ter quality while also offering faster run-time and
easiness of implementation. The resulting lexi-
con, called ConnotationWordNet, is the first lex-
icon that has polarity labels over both words and
senses. ConnotationWordNet is publicly available
for research and practical use.
Acknowledgments
This research was supported by the Army Re-
search Office under Contract No. W911NF-14-1-
0029, Stony Brook University Office of Vice Pres-
ident for Research, and gifts from Northrop Grum-
man Aerospace Systems and Google. We thank
reviewers for many insightful comments and sug-
gestions.
1552
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1-Volume 1, pages 190?199. Association for Com-
putational Linguistics.
Leman Akoglu, Rishi Chandy, and Christos Faloutsos.
2013. Opinion fraud detection in online reviews by
network effects.
Alina Andreevskaia and Sabine Bergler. 2006. Min-
ing wordnet for a fuzzy sentiment: Sentiment tag
extraction from wordnet glosses. In EACL, pages
209?216.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC, volume 10, pages 2200?2204.
Alexandra Balahur, Rada Mihalcea, and Andr?es Mon-
toyo. 2014. Computational approaches to subjec-
tivity and sentiment analysis: Present and envisaged
methods and applications. Computer Speech & Lan-
guage, 28(1):1?6.
Johan Bollen, Huina Mao, and Alberto Pepe. 2011.
Modeling public mood and emotion: Twitter senti-
ment and socio-economic phenomena. In ICWSM.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 1(16):22?29.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC06, pages 417?422.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 1092?1103. Association for Computa-
tional Linguistics.
Song Feng, Jun Seok Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash
of sentiment beneath the surface meaning. In The
Association for Computer Linguistics, pages 1774?
1784.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the Joint ACL/EACL Con-
ference, pages 174?181.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive col-
lection of html documents. In EMNLP-CoNLL,
pages 1075?1083.
Jaap Kamps, MJ Marx, Robert J Mokken, and Maarten
De Rijke. 2004. Using wordnet to measure seman-
tic orientations of adjectives.
Ross Kindermann and J. L. Snell. 1980. Markov Ran-
dom Fields and Their Applications.
Yue Lu, Malu Castellanos, Umeshwar Dayal, and
ChengXiang Zhai. 2011. Automatic construction
of a context-aware sentiment lexicon: an optimiza-
tion approach. In Proceedings of the 20th interna-
tional conference on World wide web, pages 347?
356. ACM.
Mary McGlohon, Stephen Bay, Markus G. Anderle,
David M. Steier, and Christos Faloutsos. 2009.
Snare: a link analytic system for graph labeling
and risk detection. In John F. Elder IV, Franoise
Fogelman-Souli, Peter A. Flach, and Mohammed
Zaki, editors, KDD, pages 1265?1274. ACM.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2012. Multilingual subjectivity and sentiment anal-
ysis. In Tutorial Abstracts of ACL 2012, pages 4?4.
Association for Computational Linguistics.
Saif Mohammad and Peter Turney. 2010. Emotions
evoked by common words and phrases: Using me-
chanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 26?34, Los Angeles,
CA, June. Association for Computational Linguis-
tics.
David Newman, Sarvnaz Karimi, and Lawrence Cave-
don. 2009. External evaluation of topic models.
In Australasian Document Computing Symposium,
pages 11?18, Sydney, December.
Shashank Pandit, Duen Horng Chau, Samuel Wang,
and Christos Faloutsos. 2007. Netprobe: a fast and
scalable system for fraud detection in online auction
networks. In WWW, pages 201?210.
Christos H Papadimitriou and Kenneth Steiglitz. 1998.
Combinatorial optimization: algorithms and com-
plexity. Courier Dover Publications.
James W Pennebaker and Lori D Stone. 2003. Words
of wisdom: language use over the life span. Journal
of personality and social psychology, 85(2):291.
John P Pestian, Pawel Matykiewicz, Michelle Linn-
Gust, Brett South, Ozlem Uzuner, Jan Wiebe, K Bre-
tonnel Cohen, John Hurdle, Christopher Brew, et al
2012. Sentiment analysis of suicide notes: A shared
task. Biomedical Informatics Insights, 5(Suppl.
1):3.
Martin J. Pickering and Holly P. Branigan. 1998. The
representation of verbs: Evidence from syntactic
priming in language production. Journal of Mem-
ory and Language, 39:633?651.
1553
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In IJCAI, volume 9, pages
1199?1204.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise
Getoor, Brian Gallagher, and Tina Eliassi-Rad.
2008. Collective classification in network data. AI
Magazine, 29(3):93?106.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press, Cambridge, MA.
Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 70?74. Association for Computational
Linguistics.
Fangzhong Su and Katja Markert. 2009. Subjectiv-
ity recognition on word senses via semi-supervised
mincuts. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 1?9. Association for Com-
putational Linguistics.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 133?140. Association for Computational
Linguistics.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-01), pages 491?502, Freiburg,
Germany.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The via-
bility of web-derived polarity lexicons. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 1065?1072. Asso-
ciation for Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Eval-
uation (formerly Computers and the Humanities),
39(2/3):164?210.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: A system for subjec-
tivity analysis. In Proceedings of HLT/EMNLP on
Interactive Demonstrations, pages 34?35. Associa-
tion for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technologies Conference/Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP 2005), Vancouver, CA.
Jonathan S. Yedidia, William T. Freeman, and Yair
Weiss. 2003. Understanding belief propagation and
its generalizations. In Exploring AI in the new mil-
lennium, pages 239?269.
1554
