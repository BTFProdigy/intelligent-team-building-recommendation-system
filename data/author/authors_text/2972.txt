Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 165?168,
Prague, June 2007. c?2007 Association for Computational Linguistics
HIT-IR-WSD: A WSD System for English Lexical Sample Task 
Yuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang and Ting Liu 
Information Retrieval Lab 
Harbin Institute of technology 
Harbin, China, 150001 
{yhguo,wxche}@ir.hit.edu.cn 
 
 
Abstract 
HIT-IR-WSD is a word sense disambigua-
tion (WSD) system developed for English 
lexical sample task (Task 11) of Semeval 
2007 by Information Retrieval Lab, Harbin 
Institute of Technology. The system is 
based on a supervised method using an 
SVM classifier. Multi-resources including 
words in the surrounding context, the part-
of-speech of neighboring words, colloca-
tions and syntactic relations are used. The 
final micro-avg raw score achieves 81.9% 
on the test set, the best one among partici-
pating runs. 
1 Introduction 
Lexical sample task is a kind of WSD evaluation 
task providing training and test data in which a 
small pre-selected set of target words is chosen and 
the target words are marked up. In the training data 
the target words? senses are given, but in the test 
data are not and need to be predicted by task par-
ticipants. 
HIT-IR-WSD regards the lexical sample task 
as a classification problem, and devotes to extract 
effective features from the instances. We didn?t use 
any additional training data besides the official 
ones the task organizers provided. Section 2 gives 
the architecture of this system. As the task pro-
vides correct word sense for each instance, a su-
pervised learning approach is used. In this system, 
we choose Support Vector Machine (SVM) as 
classifier. SVM is introduced in section 3. Know-
ledge sources are presented in section 4. The last 
section discusses the experimental results and 
present the main conclusion of the work performed. 
2 The Architecture of the System 
HIT-IR-WSD system consists of 2 parts: feature 
extraction and classification. Figure 1 portrays the 
architecture of the system. 
 
Figure?1:?The?architecture?of?HIT?IR?WSD?
165
Features are extracted from original instances 
and are made into digitized features to feed the 
SVM classifier. The classifier gets the features of 
training data to make a model of the target word. 
Then it uses the model to predict the sense of target 
word in the test data. 
3 Learning Algorithm 
SVM is an effective learning algorithm to WSD 
(Lee and Ng, 2002). The SVM tries to find a 
hyperplane with the largest margin separating the 
training samples into two classes. The instances in 
the same side of the hyperplane have the same 
class label. A test instance?s feature decides the 
position where the sample is in the feature space 
and which side of the hyperplane it is. In this way, 
it leads to get a prediction. SVM could be extended 
to tackle multi-classes problems by using one-
against-one or one-against-rest strategy. 
In the WSD problem, input of SVM is the fea-
ture vector of the instance. Features that appear in 
all the training samples are arranged as a vector 
space. Every instance is mapped to a feature vector. 
If the feature of a certain dimension exists in a 
sample, assign this dimension 1 to this sample, else 
assign it 0. For example, assume the feature vector 
space is <x1, x2, x3, x4, x5, x6, x7>; the instance is 
?x2 x6 x5 x7?. The feature vector of this sample 
should be <0, 1, 0, 0, 1, 1, 1>.  
The implementation of SVM here is libsvm 1 
(Chang and Lin, 2001) for multi-classes. 
4 Knowledge Sources 
We used 4 kinds of features of the target word and 
its context as shown in Table 1. 
Part of the original text of an example is ?? 
This is the <head>age</head> of new media , the 
era of ??. 
Name Extraction Tools Example 
Surrounding 
words 
WordNet 
(morph)2 
?, this, be, age, new, 
medium, ,, era, ? 
Part-of-
speech SVMTool
3 
DT_0, VBZ_0, DT_0, 
NN_t, IN_1, JJ_1, 
NNS_1 
                                                 
1?http://www.csie.ntu.edu.tw/~cjlin/libsvm/?
2?http://wordnet.princeton.edu/man/morph.3WN.html?
3?http://www.lsi.upc.es/~nlp/SVMTool/?
Collocation  
this_0, be_0, the_0, 
age_t, of_1, new_1, 
medium_1, ,_1, the_1 
Syntactic 
relation MaltParser
4 
SYN_HEAD_is 
SYN_HEADPOS_VBZ 
SYN_RELATION_PRD 
SYN_HEADRIGHT 
Table?1:?Features?the?system?extracted?
The next 4 subsections elaborate these features. 
4.1 Words in the Surrounding Context 
We take the neighboring words in the context of 
the target word as a kind of features ignoring their 
exact position information, which is called bag-of-
words approach. 
Mostly, a certain sense of a word is tend to ap-
pear in a certain kind of context, so the context 
words could contain some helpful information to 
disambiguate the sense of the target word. 
Because there would be too many context words 
to be added into the feature vector space, data 
sparseness problem is inevitable. We need to re-
duce the sparseness as possible as we can. A sim-
ple way is to use the words? morphological root 
forms. In addition, we filter the tokens which con-
tain no alphabet character (including punctuation 
symbols) and stop words. The stop words are 
tested separately, and only the effective ones 
would be added into the stop words list. All re-
maining words in the instance are gathered, con-
verted to lower case and replaced by their morpho-
logical root forms. The implementation for getting 
the morphological root forms is WordNet (morph). 
4.2 Part-of-Speechs of Neighboring Words 
As mentioned above, the data sparseness is a se-
rious problem in WSD. Besides changing tokens to 
their morphological root forms, part-of-speech is a 
good choice too. The size of POS tag set is much 
smaller than the size of surrounding words set. 
And the neighboring words? part-of-speeches also 
contain useful information for WSD. In this part, 
we use a POS tagger (Gim?nez and M?rquez, 2004) 
to assign POS tags to those tokens.  
We get the left and right 3 words? POS tags to-
gether with their position information in the target 
words? sentence.  
For example, the word age is to be disambi-
guated in the sentence of ?? This is the 
                                                 
4?http://w3.msi.vxu.se/~nivre/research/MaltParser.html?
166
<head>age</head> of new media , the era of ??. 
The features then will be added to the feature vec-
tor are ?DT_0, VBZ_0, DT_0, NN_t, IN_1, JJ_1, 
NNS_1?, in which _0/_1 stands for the word with 
current POS tag is in the left/right side of the target 
word. The POS tag set in use here is Penn Tree-
bank Tagset5. 
4.3 Collocations 
Different from bag-of-words, collocation feature 
contains the position information of the target 
words? neighboring words. To make this feature in 
the same form with the bag-of-words, we appended 
a symbol to each of the neighboring words? mor-
phological root forms to mark whether this word is 
in the left or in the right of the target word. Like 
POS feature, collocation was extracted in the sen-
tence where the target word belongs to. The win-
dow size of this feature is 5 to the left and 5 to the 
right of the target word, which is attained by em-
pirical value. In this part, punctuation symbol and 
stop words are not removed. 
Take the same instance last subsection has men-
tioned as example. The features we extracted are 
?this_0, be_0, the_0, age_t, of_1, new_1, me-
dium_1?. Like POS, _0/_1 stands for the word is 
in the left/right side of the target word. Then the 
features were added to the feature vector space. 
4.4 Syntactic Relations 
Many effective context words are not in a short 
distance to the target word, but we shouldn?t en-
large the window size too much in case of includ-
ing too many noises. A solution to this problem is 
to use the syntactic relations of the target word and 
its parent head word. 
We use Nivre et al, (2006)?s dependency parser. 
In this part, we get 4 features from every instance: 
head word of the target word, the head word?s POS, 
the head word?s dependency relation with the tar-
get word and the relative position of the head word 
to the target word. 
Still take the same instance which has been 
mentioned in the las subsection as example. The 
features we extracted are ?SYN_HEAD_is, 
SYN_HEADPOS_VBZ, SYN_RELATION_PRD, 
SYN_HEADRIGHT?, in which SYN_HEAD_is 
stands for is is the head word of age; 
SYN_HEADPOS_VBZ stands for the POS of the 
                                                 
5?http://www.lsi.upc.es/~nlp/SVMTool/PennTreebank.html?
head word is is VBZ; SYN_RELATION_PRD 
stands for the relationship between the head word 
is and target word age is PRD; and 
SYN_HEADRIGHT stands for the target word age 
is in the right side of the head word is. 
5 Data Set and Results 
This English lexical sample task: Semeval 2007 
task 116 provides two tracks of the data set for par-
ticipants. The first one is from LDC and the second 
from web. 
We took part in this evaluation in the second 
track. The corpus is from web. In this track the task 
organizers provide a training data and test data set 
for 20 nouns and 20 adjectives. 
In order to develop our system, we divided the 
training data into 2 parts: training and development 
sets. The size of the training set is about 2 times of 
the development set. The development set contains 
1,781 instances. 
4 kinds of features were merged into 15 combi-
nations. Here we use a vector (V) to express which 
features are used. The four dimensions stand for 
syntactic relations, POS, surrounding words and 
collocations, respectively. For example, 1010 
means that the syntactic relations feature and the 
surrounding words feature are used. 
V Precision V Precision
0001 78.6% 1001 78.2% 
0010 80.3% 1010 81.9% 
0011 82.0% 1011 82.8% 
0100 70.4% 1100 73.3% 
0101 79.0% 1101 79.1% 
0110 82.1% 1110 82.5% 
0111 82.9% 1111 82.9% 
1000 72.6%   
Table?2:?Results?of?Combinations?of?Features?
From Table 2, we can conclude that the sur-
rounding words feature is the most useful kind of 
features. It obtains much better performance than 
other kinds of features individually. In other words, 
without it, the performance drops a lot. Among 
these features, syntactic relations feature is the 
most unstable one (the improvement with it is un-
stable), partly because the performance of the de-
pendency parser is not good enough. As the ones 
with the vector 0111 and 1111 get the best perfor-
                                                 
6http://nlp.cs.swarthmore.edu/semeval/tasks/task11/descript
ion.shtml?
167
mance, we chose all of these kinds of features for 
our final system. 
A trade-off parameter C in SVM is tuned, and 
the result is shown in Figure 2. We have also tried 
4 types of kernels of the SVM classifier (parame-
ters are set by default). The experimental results 
show that the linear kernel is the most effective as 
Table 3 shows. 
 
Figure?2:?Accuracy?with?different?C?parameters?
Kernel 
Function 
Type 
Linear Poly-nomial RBF
Sig-
moid
Accuracy 82.9% 68.3% 68.3% 68.3%
Table?3:?Accuracy?with?different?kernel?function?
types?
Another experiment (as shown in Figure 3) also 
validate that the linear kernel is the most suitable 
one. We tried using polynomial function. Unlike 
the parameters set by default above (g=1/k, d=3), 
here we set its Gama parameter as 1 (g=1) but oth-
er parameters excepting degree parameter are still 
set by default. The performance gets better when 
the degree parameter is tuned towards 1. That 
means the closer the kernel function to linear func-
tion the better the system performs. 
 
Figure?3:?Accuracy?with?different?degree? in?po?
lynomial?function?
In order to get the relation between the system 
performance and the size of training data, we made 
several groups of training-test data set from the 
training data the organizers provided. Each of them 
has the same test data but different size of training 
data which are 2, 3, 4 and 5 times of the test data 
respectively. Figure 4 shows the performance 
curve with the training data size. Indicated in Fig-
ure 4, the accuracy increases as the size of training 
data enlarge, from which we can infer that we 
could raise the performance by using more training 
data potentially. 
 
Figure?4:?Accuracy?s?trend?with?the?training?da?
ta?size?
Feature extraction is the most time-consuming 
part of the system, especially POS tagging and 
parsing which take 2 hours approximately on the 
training and test data. The classification part (using 
libsvm) takes no more than 5 minutes on the train-
ing and test data. We did our experiment on a PC 
with 2.0GHz CPU and 960 MB system memory. 
Our official result of HIT-IR-WSD is: micro-
avg raw score 81.9% on the test set, the top one 
among the participating runs. 
Acknowledgement 
We gratefully acknowledge the support for this 
study provided by the National Natural Science 
Foundation of China (NSFC) via grant 60435020, 
60575042, 60575042 and 60675034. 
References 
Lee, Y. K., and Ng, H. T. 2002. An empirical evaluation 
of knowledge sources and learning algorithms for 
word sense disambiguation. In Proceedings of 
EMNLP02, 41?48. 
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a 
library for support vector machines. 
Jes?s Gim?nez and Llu?s M?rquez. 2004. SVMTool: A 
general POS tagger generator based on Support Vec-
tor Machines. Proceedings of the 4th International 
Conference on Language Resources and Evaluation 
(LREC'04). Lisbon, Portugal. 
Nivre, J., Hall, J., Nilsson, J., Eryigit, G. and Marinov, S. 
2006. Labeled Pseudo-Projective Dependency Pars-
ing with Support Vector Machines. In Proceedings of 
the Tenth Conference on Computational Natural 
Language Learning (CoNLL). 
168
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 49?54,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Dependency-based Syntactic and Semantic Parsing
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, Ting Liu
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
{car, lzh, yqli, yhguo, qinb, tliu}@ir.hit.edu.cn
Abstract
Our CoNLL 2009 Shared Task system in-
cludes three cascaded components: syntactic
parsing, predicate classification, and semantic
role labeling. A pseudo-projective high-order
graph-based model is used in our syntactic de-
pendency parser. A support vector machine
(SVM) model is used to classify predicate
senses. Semantic role labeling is achieved us-
ing maximum entropy (MaxEnt) model based
semantic role classification and integer linear
programming (ILP) based post inference. Fi-
nally, we win the first place in the joint task,
including both the closed and open challenges.
1 System Architecture
Our CoNLL 2009 Shared Task (Hajic? et al, 2009):
multilingual syntactic and semantic dependencies
system includes three cascaded components: syn-
tactic parsing, predicate classification, and semantic
role labeling.
2 Syntactic Dependency Parsing
We extend our CoNLL 2008 graph-based
model (Che et al, 2008) in four ways:
1. We use bigram features to choose multiple pos-
sible syntactic labels for one arc, and decide the op-
timal label during decoding.
2. We extend the model with sibling features (Mc-
Donald, 2006).
3. We extend the model with grandchildren fea-
tures. Rather than only using the left-most and right-
most grandchildren as Carreras (2007) and Johans-
son and Nugues (2008) did, we use all left and right
grandchildren in our model.
4. We adopt the pseudo-projective approach in-
troduced in (Nivre and Nilsson, 2005) to handle the
non-projective languages including Czech, German
and English.
2.1 Syntactic Label Determining
The model of (Che et al, 2008) decided one la-
bel for each arc before decoding according to uni-
gram features, which caused lower labeled attach-
ment score (LAS). On the other hand, keeping all
possible labels for each arc made the decoding in-
efficient. Therefore, in the system of this year, we
adopt approximate techniques to compromise, as
shown in the following formulas.
f lbluni(h, c, l) = f lbl1 (h, 1, d, l) ? f lbl1 (c, 0, d, l)
L1(h, c) = arg maxK1l?L(w ? f lbluni(h, c, l))
f lblbi (h, c, l) = f lbl2 (h, c, l)
L2(h, c) = arg maxK2l?L1(h,c)(w ? {f lbluni ? f lblbi })
For each arc, we firstly use unigram features to
choose the K1-best labels. The second parameter of
f lbl1 (?) indicates whether the node is the head of the
arc, and the third parameter indicates the direction.
L denotes the whole label set. Then we re-rank the
labels by combining the bigram features, and choose
K2-best labels. During decoding, we only use the
K2 labels chosen for each arc (K2 ? K1 < |L|).
2.2 High-order Model and Algorithm
Following the Eisner (2000) algorithm, we use spans
as the basic unit. A span is defined as a substring
of the input sentence whose sub-tree is already pro-
duced. Only the start or end words of a span can link
with other spans. In this way, the algorithm parses
the left and the right dependence of a word indepen-
dently, and combines them in the later stage.
We follow McDonald (2006)?s implementation of
first-order Eisner parsing algorithm by modifying its
scoring method to incorporate high-order features.
Our extended algorithm is shown in Algorithm 1.
There are four different span-combining opera-
tions. Here we explain two of them that correspond
to right-arc (s < t), as shown in Figure 1 and 2. We
49
Algorithm 1 High-order Eisner Parsing Algorithm
1: C[s][s][c] = 0, 0 ? s ? N , c ? cp, icp # cp: complete; icp: incomplete
2: for j = 1 to N do
3: for s = 0 to N do
4: t = s+ jL
5: if t > N then
6: break
7: end if
# Create incomplete spans
8: C[s][t][icp] = maxs?r<t;l?L2(s,t)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(s, r, t, l))
9: C[t][s][icp] = maxs?r<t;l?L2(t,s)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(t, r, s, l))
# Create complete spans
10: C[s][t][cp] = maxs<r?t;l=C[s][r][icp].label(C[s][r][icp] + C[r][t][cp] + Scp(s, r, t, l))
11: C[t][s][cp] = maxs?r<t;l=C[t][r][icp].label(C[r][s][cp] + C[t][r][icp] + Scp(t, r, s, l))
12: end for
13: end for
follow the way of (McDonald, 2006) and (Carreras,
2007) to represent spans. The other two operations
corresponding to left-arc are similar.
 
Figure 1: Combining two spans into an incomplete span
Figure 1 illustrates line 8 of the algorithm in Al-
gorithm 1, which combines two complete spans into
an incomplete span. A complete span means that
only the head word can link with other words fur-
ther, noted as ??? or ???. An incomplete span
indicates that both the start and end words of the
span will link with other spans in the future, noted as
?99K? or ?L99?. In this operation, we combine two
smaller spans, sps?r and spr+1?t, into sps99Kt with
adding arcs?t. As shown in the following formu-
las, the score of sps99Kt is composed of three parts:
the score of sps?r, the score of spr+1?t, and the
score of adding arcs?t. The score of arcs?t is
determined by four different feature sets: unigram
features, bigram features, sibling features and left
grandchildren features (or inside grandchildren fea-
tures, meaning that the grandchildren lie between s
and t). Note that the sibling features are only related
to the nearest sibling node of t, which is denoted as
sck here. And the inside grandchildren features are
related to all the children of t. This is different from
the models used by Carreras (2007) and Johansson
and Nugues (2008). They only used the left-most
child of t, which is tck? here.
ficp(s, r, t, l) = funi(s, t, l) ? fbi(s, t, l)
? fsib(s, sck, t) ? {?k?i=1 fgrand(s, t, tci, l)}
Sicp(s, r, t, l) = w ? ficp(s, r, t, l)
S(sps99Kt) = S(sps?r) + S(spr+1?t)
+ Sicp(s, r, t, l)
In Figure 2 we combine sps99Kr and spr?t into
sps?t, which explains line 10 in Algorithm 1. The
score of sps?t also includes three parts, as shown
in the following formulas. Although there is no new
arc added in this operation, the third part is neces-
sary because it reflects the right (or called outside)
grandchildren information of arcs?r.
r trc1 rcks r s tr rc1 rck
l l
 
Figure 2: Combining two spans into a complete span
fcp(s, r, t, l) = ?ki=1 fgrand(s, r, rci, l)
Scp(s, r, t, l) = w ? fcp(s, r, t, l)
S(sps?t) = S(sps99Kr)
+ S(spr?t) + Scp(s, r, t, l)
50
2.3 Features
As shown above, features used in our model can be
decomposed into four parts: unigram features, bi-
gram features, sibling features, and grandchildren
features. Each part can be seen as two different sets:
arc-related and label-related features, except sibling
features, because we do not consider labels when us-
ing sibling features. Arc-related features can be un-
derstood as back-off of label-related features. Actu-
ally, label-related features are gained by simply at-
taching the label to the arc-features.
The unigram and bigram features used in our
model are similar to those of (Che et al, 2008), ex-
cept that we use bigram label-related features. The
sibling features we use are similar to those of (Mc-
Donald, 2006), and the grandchildren features are
similar to those of (Carreras, 2007).
3 Predicate Classification
The predicate classification is regarded as a super-
vised word sense disambiguation (WSD) task here.
The task is divided into four steps:
1. Target words selection: predicates with multi-
ple senses appearing in the training data are selected
as target words.
2. Feature extraction: features in the context
around these target words are extracted as shown in
Table 4. The detailed explanation about these fea-
tures can be found from (Che et al, 2008).
3. Classification: for each target word, a Support
Vector Machine (SVM) classifier is used to classify
its sense. As reported by Lee and Ng (2002) and
Guo et al (2007), SVM shows good performance on
the WSD task. Here libsvm (Chang and Lin, 2001)
is used. The linear kernel function is used and the
trade off parameter C is 1.
4. Post processing: for each predicate in the test
data which does not appear in the training data, its
first sense in the frame files is used.
4 Semantic Role Labeling
The semantic role labeling (SRL) can be divided
into two separate stages: semantic role classification
(SRC) and post inference (PI).
During the SRC stage, a Maximum en-
tropy (Berger et al, 1996) classifier is used to
predict the probabilities of a word in the sentence
Language No-duplicated-roles
Catalan arg0-agt, arg0-cau, arg1-pat, arg2-atr, arg2-loc
Chinese A0, A1, A2, A3, A4, A5,
Czech ACT, ADDR, CRIT, LOC, PAT, DIR3, COND
English A0, A1, A2, A3, A4, A5,
German A0, A1, A2, A3, A4, A5,
Japanese DE, GA, TMP, WO
Spanish arg0-agt, arg0-cau, arg1-pat, arg1-tem, arg2-atr,
arg2-loc, arg2-null, arg4-des, argL-null, argM-
cau, argM-ext, argM-fin
Table 1: No-duplicated-roles for different languages
to be each semantic role. We add a virtual role
?NULL? (presenting none of roles is assigned)
to the roles set, so we do not need semantic role
identification stage anymore. For a predicate
of each language, two classifiers (one for noun
predicates, and the other for verb predicates) predict
probabilities of each word in a sentence to be each
semantic role (including virtual role ?NULL?). The
features used in this stage are listed in Table 4.
The probability of each word to be a semantic role
for a predicate is given by the SRC stage. The re-
sults generated by selecting the roles with the largest
probabilities, however, do not satisfy some con-
strains. As we did in the last year?s system (Che et
al., 2008), we use the ILP (Integer Linear Program-
ming) (Punyakanok et al, 2004) to get the global op-
timization, which is satisfied with three constrains:
C1: Each word should be labeled with one and
only one label (including the virtual label ?NULL?).
C2: Roles with a small probability should never
be labeled (except for the virtual role ?NULL?). The
threshold we use in our system is 0.3.
C3: Statistics show that some roles (except for
the virtual role ?NULL?) usually appear once for
a predicate. We impose a no-duplicate-roles con-
straint with a no-duplicate-roles list, which is con-
structed according to the times of semantic roles?
duplication for each single predicate. Table 1 shows
the no-duplicate-roles for different languages.
Our maximum entropy classifier is implemented
with Maximum Entropy Modeling Toolkit1. The
classifier parameters are tuned with the development
data for different languages respectively. lp solve
5.52 is chosen as our ILP problem solver.
1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
2http://sourceforge.net/projects/lpsolve
51
5 Experiments
5.1 Experimental Setup
We participate in the CoNLL 2009 shared task
with all 7 languages: Catalan (Taule? et al, 2008),
Chinese (Palmer and Xue, 2009), Czech (Hajic? et
al., 2006), English (Surdeanu et al, 2008), Ger-
man (Burchardt et al, 2006), Japanese (Kawahara
et al, 2002), and Spanish (Taule? et al, 2008). Be-
sides the closed challenge, we also submitted the
open challenge results. Our open challenge strategy
is very simple. We add the SRL development data
of each language into their training data. The pur-
pose is to examine the effect of the additional data,
especially for out-of-domain (ood) data.
Three machines (with 2.5GHz Xeon CPU and
16G memory) were used to train our models. Dur-
ing the peak time, Amazon?s EC2 (Elastic Com-
pute Cloud)3 was used, too. Our system requires
15G memory at most and the longest training time
is about 36 hours.
During training the predicate classification (PC)
and the semantic role labeling (SRL) models, golden
syntactic dependency parsing results are used. Pre-
vious experiments show that the PC and SRL test re-
sults based on golden parse trees are slightly worse
than that based on cross trained parse trees. It is,
however, a pity that we have no enough time and ma-
chines to do cross training for so many languages.
5.2 Results and Discussion
In order to examine the performance of the ILP
based post inference (PI) for different languages, we
adopt a simple PI strategy as baseline, which se-
lects the most likely label (including the virtual la-
bel ?NULL?) except for those duplicate non-virtual
labels with lower probabilities (lower than 0.5). Ta-
ble 2 shows their performance on development data.
We can see that the ILP based post inference can
improve the precision but decrease the recall. Ex-
cept for Czech, almost all languages are improved.
Among them, English benefits most.
The final system results are shown in Table 3.
Comparing with our CoNLL 2008 (Che et al, 2008)
syntactic parsing results on English4, we can see that
our new high-order model improves about 1%.
3http://aws.amazon.com/ec2/
4devel: 85.94%, test: 87.51% and ood: 80.73%
Precision Recall F1
Catalan simple 78.68 77.14 77.90
Catalan ILP 79.42 76.49 77.93
Chinese simple 80.74 74.36 77.42
Chinese ILP 81.97 73.92 77.74
Czech simple 88.54 84.68 86.57
Czech ILP 89.23 84.05 86.56
English simple 83.03 83.55 83.29
English ILP 85.63 83.03 84.31
German simple 78.88 75.87 77.34
German ILP 82.04 74.10 77.87
Japanese simple 88.04 70.68 78.41
Japanese ILP 89.23 70.16 78.56
Spanish simple 76.73 75.92 76.33
Spanish ILP 77.71 75.34 76.51
Table 2: Comparison between different PI strategies
For the open challenge, because we did not mod-
ify the syntactic training data, its results are the same
as the closed ones. We can, therefore, examine the
effect of the additional training data on SRL. We can
see that along with the development data are added
into the training data, the performance on the in-
domain test data is increased. However, it is inter-
esting that the additional data is harmful to the ood
test.
6 Conclusion and Future Work
Our CoNLL 2009 Shared Task system is com-
posed of three cascaded components. The pseudo-
projective high-order syntactic dependency model
outperforms our CoNLL 2008 model (in English).
The additional in-domain (devel) SRL data can help
the in-domain test. However, it is harmful to the ood
test. Our final system achieves promising results. In
the future, we will study how to solve the domain
adaptive problem and how to do joint learning be-
tween syntactic and semantic parsing.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60675034, and the ?863? National High-
Tech Research and Development of China via grant
2008AA01Z144.
52
Syntactic Accuracy (LAS) Semantic Labeled F1 Macro F1 Score
devel test ood devel test ood devel test ood
Catalan closed 86.65 86.56 ?? 77.93 77.10 ?? 82.30 81.84 ??open ?? ?? 77.36 ?? 81.97
Chinese closed 75.73 75.49 ?? 77.74 77.15 ?? 76.79 76.38 ??open ?? ?? 77.23 ?? 76.42
Czech closed 80.07 80.01 76.03 86.56 86.51 85.26 83.33 83.27 80.66open ?? ?? 86.57 85.21 ?? 83.31 80.63
English closed 87.09 88.48 81.57 84.30 85.51 73.82 85.70 87.00 77.71open ?? ?? 85.61 73.66 ?? 87.05 77.63
German closed 85.69 86.19 76.11 77.87 78.61 70.07 81.83 82.44 73.19open ?? ?? 78.61 70.09 ?? 82.44 73.20
Japanese closed 92.55 92.57 ?? 78.56 78.26 ?? 85.86 85.65 ??open ?? ?? 78.35 ?? 85.70
Spanish closed 87.22 87.33 ?? 76.51 76.47 ?? 81.87 81.90 ??open ?? ?? 76.66 ?? 82.00
Average closed ?? 85.23 77.90 ?? 79.94 76.38 ?? 82.64 77.19open 80.06 76.32 82.70 77.15
Table 3: Final system results
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In LREC-2006.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In EMNLP/CoNLL-
2007.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li,
Bing Qin, Ting Liu, and Sheng Li. 2008. A cascaded
syntactic and semantic dependency parsing system. In
CoNLL-2008.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Advances in Probabilistic
and Other Parsing Technologies.
Yuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang,
and Ting Liu. 2007. HIT-IR-WSD: A wsd system for
english lexical sample task. In SemEval-2007.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In CoNLL-2009.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In EMNLP-2008.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In LREC-2002.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning al-
gorithms for word sense disambiguation. In EMNLP-
2002.
Ryan McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In ACL-2005.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1).
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer linear
programming inference. In Coling-2004.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL-2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In LREC-2008.
53
Catalan Chinese Czech English German Japanese Spanish
ChildrenPOS ? ? ??
ChildrenPOSNoDup ? ? ? ?
ConstituentPOSPattern ? ? ? ? ? ? ? ? ? ? ? ? ? ?
ConstituentPOSPattern+DepRelation ? ? ? ? ? ?
ConstituentPOSPattern+DepwordLemma ? ? ? ? ? ?
ConstituentPOSPattern+HeadwordLemma ? ? ? ? ? ? ? ? ? ?
DepRelation N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? N M ? ?
DepRelation+DepwordLemma ? ? ? ?
DepRelation+Headword N M N M N N M N M N
DepRelation+HeadwordLemma ? ? ? ? ? ? ? ?
DepRelation+HeadwordLemma+DepwordLemma ? ? ? ? ? ? ? ? ? ? ? ?
DepRelation+HeadwordPOS N M N M N M N M N M N
Depword ? ? ? ?
DepwordLemma ? ? ? ? ? ? ? ? ? ? ? ?
DepwordLemma+HeadwordLemma ? ? ? ? ? ?
DepwordLemma+RelationPath ? ? ? ? ? ? ? ? ? ?
DepwordPOS N M N M N M ? ? N M N M ? ? N M
DepwordPOS+HeadwordPOS ? ? ? ?
DownPathLength ? ? ? ?
FirstLemma ? ? ? ? ? ? ? ? ? ? ? ?
FirstPOS ? ? ? ?
FirstPOS+DepwordPOS ? ? ? ? ? ?
FirstWord ? ? ? ?
Headword N M N M N M N M N M ? ? N
HeadwordLemma N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? ? ? N
HeadwordLemma+RelationPath ? ? ? ? ? ? ? ? ? ? ? ?
HeadwordPOS N M N M N M ? ? N M ? ? N M ? ? N M
LastLemma ? ? ? ? ? ? ? ? ? ?
LastPOS ? ? ? ?
LastWord ? ?
Path ? ? ? ? ? ? ? ? ? ? ? ?
Path+RelationPath ? ? ? ? ? ? ? ? ? ?
PathLength ? ? ? ? ? ? ? ? ? ? ? ?
PFEAT N M N M N M
PFEATSplit N M ? ? N M ? ? N M ? ? N M ? ?
PFEATSplitRemoveNULL N M N M N M
PositionWithPredicate ? ? ? ? ? ? ? ? ? ?
Predicate N M ? ? N M N M ? ? N M N M N M ? ?
Predicate+PredicateFamilyship ? ? ? ? ? ? ? ? ? ?
PredicateBagOfPOSNumbered M N M N M N M
PredicateBagOfPOSNumberedWindow5 N M N M N M N M N M
PredicateBagOfPOSOrdered N M N M N M N M N
PredicateBagOfPOSOrderedWindow5 N M N M N M N M N M N M
PredicateBagOfPOSWindow5 N N M N M N M N M N
PredicateBagOfWords M N M N N M N M
PredicateBagOfWordsAndIsDesOfPRED N M N M M N M N M
PredicateBagOfWordsOrdered M N M N M M N M N M
PredicateChildrenPOS N M ? ? N M N M N M N M N M ? ?
PredicateChildrenPOSNoDup N M N M N M N M N M N M
PredicateChildrenREL N M ? ? N M N M N M N M ? ? N M
PredicateChildrenRELNoDup N M ? ? N M N M N M N M ? ? N M
PredicateFamilyship ? ?
PredicateLemma N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? ? ? N M ? ?
PredicateLemma+PredicateFamilyship ? ? ? ? ? ?
PredicateSense ? ? ? ? ? ? ? ? ? ? ? ? ? ?
PredicateSense+DepRelation ? ? ? ?
PredicateSense+DepwordLemma ? ? ? ?
PredicateSense+DepwordPOS ? ? ? ?
PredicateSiblingsPOS N M N M N N M N M N M
PredicateSiblingsPOSNoDup N M ? ? N M N M N M N M N M ? ?
PredicateSiblingsREL N M ? ? N M N M N M N M N M
PredicateSiblingsRELNoDup N M N M ? ? M N M N M ? ? N M ? ?
PredicateVoiceEn N M
PredicateWindow5Bigram N M N M N M N M
PredicateWindow5BigramPOS N M N M N M N M N M N M
RelationPath ? ? ? ? ? ? ? ? ? ? ? ? ? ?
SiblingsPOS ? ? ? ?
SiblingsREL ?
SiblingsRELNoDup ? ? ? ?
UpPath ? ? ? ? ? ? ?
UpPathLength ? ?
UpRelationPath ? ? ? ? ? ?
UpRelationPath+HeadwordLemma ? ? ? ? ? ? ? ?
Table 4: Features that are used in predicate classification (PC) and semantic role labeling (SRL). N: noun predicate
PC, M: verb predicate PC, ?: noun predicate SRL, ?: verb predicate SRL.
54
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 863?868,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Microblog Entity Linking by Leveraging Extra Posts
Yuhang Guo, Bing Qin?, Ting Liu , Sheng Li
Research Center for Social Computing and Information Retrieval
School of Computer Science and Technology
Harbin Institute of Technology, China
{yhguo, bqin?, tliu, sli}@ir.hit.edu.cn
Abstract
Linking name mentions in microblog posts to
a knowledge base, namely microblog entity
linking, is useful for text mining tasks on mi-
croblog. Entity linking in long text has been
well studied in previous works. However few
work has focused on short text such as mi-
croblog post. Microblog posts are short and
noisy. Previous method can extract few fea-
tures from the post context. In this paper we
propose to use extra posts for the microblog
entity linking task. Experimental results show
that our proposed method significantly im-
proves the linking accuracy over traditional
methods by 8.3% and 7.5% respectively.
1 Introduction
Microblogging services (e.g. Twitter) are attracting
millions of users to share and exchange their ideas
and opinions. Millions of new microblog posts are
generated on such open broadcasting platforms ev-
ery day 1. Microblog provides a fruitful and instant
channel of global information publication and acqui-
sition.
A necessary step for the information acquisition
on microblog is to identify which entities a post is
about. Such identification can be challenging be-
cause the entity mention may be ambiguous. Let?s
begin with a real post from Twitter.
(1) No excuse for floods tax, says Abbott
URL
?Corresponding author
1See http://blog.twitter.com/2011/06/ 200-million-tweets-
per-day.html.
This post is about an Australia political lead-
er, Tony Abbot, and his opinion on flood tax
policy. To understand that this post mentions
Tony Abbot is not trivial because the name Ab-
bot can refer to many people and organization-
s. In the Wikipedia page of Abbott, there list-
s more than 20 Abbotts, such as baseball player
Jim Abbott, actor Bud Abbott and company
Abbott Laboratories, etc..
Given a knowledge base (KB) (e.g. Wikipedia),
entity linking is the task to identify the referent KB
entity of a target name mention in plain text. Most
current entity linking techniques are designed for
long text such as news/blog articles (Mihalcea and
Csomai, 2007; Cucerzan, 2007; Milne and Witten,
2008; Han and Sun, 2011; Zhang et al, 2011; Shen
et al, 2012; Kulkarni et al, 2009; Ratinov et al,
2011). Entity linking for microblog posts has not
been well studied.
Comparing with news/blog articles, microblog
posts are:
short each post contains no more than 140 charac-
ters;
fresh the new entity-related content may have not
been included in the knowledge base;
informal acronyms and spoken language writing
style are common.
Due to these properties, few feature can be ex-
tracted from a post. Without enough features, pre-
vious entity linking methods may fail. In order to
overcome the feature sparseness, we turn to another
property of microblog:
863
redundancy For each day, over 340M short mes-
sages are posted in twitter. Similar information
may be posted in different expressions.
For example, we find the following post,
(2) Julia Gillard and Tony Abbott on
the flood levy just after 8.30am on
@612brisbane!
The content of post (2) is highly related to post
(1). In contrast to the confusing post (1), the text
in post (2) explicitly indicates that the Abbott here
refers to the Australian political leader. This inspires
us to bridge the confusing post and the knowledge
base with other posts.
In this paper, we approach the microblog entity
linking by leveraging extra posts. A straightforward
method is to expand the post context with similar
posts, which we call Context-Expansion-based Mi-
croblog Entity Linking (CEMEL). In this method,
we first construct a query with the given post and
then search for it in a collection of posts. From the
search result, we select the most similar posts for the
context expansion. The disambiguation will benefit
from the extra posts if, hopefully, they are related
to the given post in content and include explicit fea-
tures for the disambiguation.
Furthermore, we propose a Graph-based Mi-
croblog Entity Linking (GMEL) method. In contrast
to CEMEL, the extra posts in GMEL are not directly
added into the context. Instead, they are represented
as nodes in a graph, and weighted by their similarity
with the target post. We use an iterative algorithm
in this graph to propagate the entity weights through
the edges between the post nodes.
We conduct experiments on real microblog da-
ta which we harvested from Twitter. Current enti-
ty linking corpus, such as the TAC-KBP data (M-
cNamee and Dang, 2009), mainly focuses on long
text. And few microblog entity linking corpus is
publicly available. In this work, we manually anno-
tated a microblog entity linking corpus. This corpus
inherit the target names from TAC-KBP2009. So it
is comparable with the TAC-KBP2009 corpus.
Experimental results show that the performance
of previous methods drops on microblog posts com-
paring with on long text. Both of CEMEL and
GMEL can significantly improve the performance
over baselines, which means that entity linking sys-
tem on microblog can be improved by leveraging ex-
tra posts. The results also show that GMEL outper-
forms CEMEL significantly.
We summarize our contributions as follows.
? We propose a context-expansion-based and a
graph-based method for microblog entity link-
ing by leveraging extra posts.
? We annotate a microblog entity linking corpus
which is comparable to an existing long text
corpus.
? We show the inefficiency of previous method
on the microblog corpus and our method can
significantly improve the results.
2 Task defination
The microblog entity linking task is that, for a name
mention in a microblog post, the system is to find the
referent entity of the name in a knowledge base, or
return a NIL mark if the entity is absence from the
knowledge base. This definition is close to the en-
tity linking task in the TAC-KBP evaluation (Ji and
Grishman, 2011) except for the context of the target
name is microblog post whereas in TAC-KBP the
context is news article or web log.
Several related tasks have been studied on mi-
croblog posts. In Meij et al (2012)?s work, they
link a post, rather than a name mention in the post,
to relevant Wikipedia concepts. Guo et al (2013a)
and Liu et al (2013) define entity linking as to first
detect all the mentions in a post and then link the
mentions to the knowledge base. In contrast, our
definition (as well as the TAC-KBP definition) fo-
cuses on a concerned name mention across different
posts/documents.
3 Method
A typical entity linking system can be broken down
into two steps:
candidate generation This step narrows down the
candidate entity range from any entity in the
world to a limited set.
candidate ranking This step ranks the candidates
and output the top ranked entity as the result.
864
Figure 1: An example of the GMEL graph. p1 . . . p4 are
post nodes and c1 . . . c3 are candidate entity nodes. Each
post node is connected to the corresponding candidate n-
odes from the knowledge base. The edges between the
nodes are weighted by the similarity between them.
In this paper, we use the candidate generation
method described in Guo et al(2013). For the candi-
date ranking, we use a Vector Space Model (VSM)
and a Learning to Rank (LTR) as baselines. VSM
is an unsupervised method and LTR is a supervised
method. Both of them have achieved the state-of-
the-art performances in the TAC-KBP evaluations.
The major challenge in microblog entity linking
is the lack of context in the post. An ideal solu-
tion is to expand the context with the posts which
contain the same entity. However, automatically
judging whether a name mention in two documents
refers to the same entity, namely cross document co-
reference, is not trivial. Here our solution is to rank
the posts by their possibility of co-reference to the
target one and select the most possible co-referent
posts for the expansion.
CEMEL is based on the assumption that, given a
name and two posts where the name is mentioned,
the higher similarity between the posts the high-
er possibility of their co-reference and that the co-
referent posts may contains useful features for the
disambiguation. However, two literally similar posts
may not be co-referent. If such non co-referent post
is expanded to the context, noises may be included.
Take the following post as an example.
(3) AG Abbott says that bullets have
crossed the border from Mexico to
Texas at least four times. URL
This post is similar to post (1) because they both
contains ?says? and ?URL?. But the Abbott in post
(3) refers to the Texas Attorney General Greg Ab-
bott. In this mean, the expanded context in post (3)
could mislead the disambiguation for post (1). Such
noise can be controlled by setting a strict number of
posts to expand the context or weighting the contri-
bution of this post to the target one.
Our CEMEL method consists of the following
steps: First we construct a query with the terms from
the target post. Second we search for the query in a
microblog post collection using a common informa-
tion retrieval model such as the vector space model.
Note that here we limit the searched posts must con-
tain the target name mention. Then we expand the
target post with top N similar posts and use a typical
entity linking method (such as VSM and LTR) with
the expanded context.
Figure 1 illustrates the graph of GMEL. Each n-
ode of this graph represents an candidate entity (e.g.
c1 . . . c3) or a post of the given target name (e.g.
p1 . . . p4) In this graph, each node represents an en-
tity or a post of the given target name. Between each
pair of post nodes, each pair of entity nodes and each
post node and its candidate entity nodes, there is an
edge. The edge is weighted by the similarity be-
tween the two linked nodes. Entity nodes are labeled
by themselves and candidate nodes are initialized as
unlabeled nodes. For the edges between post node
pairs and entity node pairs, we use cosine similari-
ty. For the edges between a post node and its can-
didate entity nodes, we use the score given by tra-
ditional entity linking methods. We use an iterative
algorithm on this graph to propagate the labels from
the entity nodes to the post nodes. We adapt Label
Propagation (LP) (Zhu and Ghahramani, 2002) and
Modified Adsorption (MAD) (Talukdar and Pereira,
2010) for the iteration over the graph.
4 Experiment
4.1 Data Annotation
Till now, few microblog entity linking data is pub-
licly available. In this work, we manually annotate
a data set on microblog posts2. We collect 15.6 mil-
lion microblog posts in Twitter dated from January
23 to February 8, 2011. In order to compare with ex-
isting entity linking on long text, we select a subset
of target names from TAC-KBP2009 and inherit the
knowledge base in the TAC-KBP evaluation. The
2We published this data so that researchers can reproduce
our results.
865
Figure 2: Percentage of the co-reference posts in the top
N similar posts
Figure 3: Impact of expansion post number in CEMEL
TAC-KBP2009 data set includes 513 target names.
We search for all the target names in the post col-
lection and get 26,643 matches. We randomly sam-
ple 120 posts for each of the top 30 most frequently
matched target names and filter out non-English and
overly short (i.e. less than 3 words) posts. Then
we get 2,258 posts for 25 target names and manual-
ly link the target name mentions in the posts to the
TAC-KBP knowledge base.
In order to evaluate the assumption in CEMEL:
similar posts tend to co-reference, we randomly s-
elect 10 posts for 5 target names respectively and
search for the posts in the post collection. From
the search result of each of the 50 posts, we select
the top 20 posts and manually annotate if they co-
reference with the query post.
4.2 Settings
We generate candidates with the method described
in (Guo et al, 2013b) and use Vector Space Mod-
el (VSM) (Varma et al, 2009) and Learning to Rank
(LTR) (Zheng et al, 2010) as the ranking model. We
Figure 4: Accuracy of GMEL with different rate of extra
post nodes
use Lucene and ListNet with default settings for the
VSM and LTR implementation respectively. We use
bigram feature for VSM and the feature set of (Chen
et al, 2011) for LTR. LTR is evaluated with 10-fold
cross validation. Given a target name, the GMEL
graph includes all the evaluation posts as well as a
set of extra post nodes searched from the post collec-
tion with the query of the target name. We filter out
determiners, interjections, punctuations, emoticon-
s, discourse markers and URLs in the posts with a
twitter part-of-speech tagger (Owoputi et al, 2013).
The similarity between a post and its candidate en-
tities is set with the score given by VSM or LTR
and the similarity between other nodes is set with the
corresponding cosine similarity. We employ junto3
with default settings for the iterative algorithm im-
plementation .
4.3 Results
Figure 2 shows the relationship between similari-
ty and co-reference. From this figure we can see
that the percentage decreases with the growth of N.
When the N is up to 10, about 60% of the similar
posts co-reference with the query post and the de-
crease speed slows down. The Pearson correlation
coefficient between the percentage and the number
of top N is -0.843, which shows a significant corre-
lation between the two variables (with p-value 0.01
under t-test).
Figure 3 shows the impact of the extra post num-
ber for the context expansion in CEMEL. We can see
that the accuracies of VSM and LTR are improved
3See https://github.com/parthatalukdar/junto
866
Figure 5: Label entropy of GMEL with different rate of
extra post nodes
Figure 6: Accuracy of the systems
by CEMEL. The improvements peak with 5-10 ex-
tra posts. Then more extra posts will pull down the
accuracy.
Figure 4 shows the accuracy of GMEL. The x-axis
is the rate of the extra post number over the evalu-
ation post number. We can see that the accuracy of
MAD increases with the number of extra post nodes
at first and then turns to be stable. The accuracy of
LP increases at first and drops when more extra posts
are added into the graph.
Figure 5 shows the information entropy of the la-
bels in LP and MAD. The curves show that the pre-
diction of LP tends to converge into a small number
of labels. This is because LP prefers smoothing la-
belings over the graph (Talukdar and Pereira, 2010).
We also evaluate our baselines on TAC-KBP2009
data set (LTR is trained on TAC-KBP2010 data set).
The accuracy of VSM and LTR are 0.8338 and
0.8372 respectively, which are comparable with the
state-of-the-art result (Hachey et al, 2013).
Figure 6 shows the performances of the systems
on the microblog data. We set the optimal expansion
post number of CEMEL and use MAD algorithm for
GMEL with all searched extra post nodes. From this
figure we can see that the results of VSM and LTR
baselines are comparable and both of them are sig-
nificantly lower than that on TAC-KBP2009 data.
CEMEL improves the VSM and LTR baselines by
4.3% and 2.7% respectively. GMEL improves VSM
and LTR by 8.3% and 7.5% respectively. The results
of GMEL are also significantly better than CEMEL.
All of the improvements are significant under Z-test
with p < 0.05.
5 Conclusion
In this paper we approach microblog entity linking
by leveraging extra posts. We propose a context-
expansion-based and a graph-based method. Exper-
imental results on our data set show that the per-
formance of traditional method drops on the mi-
croblog data. The graph-based method outperform-
s the context-expansion-based method and both of
them significantly improve the accuracy of tradition-
al methods. In the graph-based method the modified
adsorption algorithm performs better than the label
propagation algorithm.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
61273321, 61073126, 61133012 and the National
863 Leading Technology Research Project via grant
2012AA011102. We would like to thank to Wanx-
iang Che, Ruiji Fu, Yanyan Zhao, Wei Song and
several anonymous reviewers for their constructive
comments and suggestions.
References
Zheng Chen, Suzanne Tamang, Adam Lee, and Heng Ji.
2011. A toolkit for knowledge base population. In
SIGIR, pages 1267?1268.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Method-
s in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
708?716, Prague, Czech Republic, June. Association
for Computational Linguistics.
867
Stephen Guo, Ming-Wei Chang, and Emre Kiciman.
2013a. To link or not to link? a study on end-to-
end tweet entity linking. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 1020?1030, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Yuhang Guo, Bing Qin, Yuqin Li, Ting Liu, and Sheng
Li. 2013b. Improving candidate generation for entity
linking. In Elisabeth Mtais, Farid Meziane, Mohamad
Saraee, Vijayan Sugumaran, and Sunil Vadera, edi-
tors, Natural Language Processing and Information
Systems, volume 7934 of Lecture Notes in Computer
Science, pages 225?236. Springer Berlin Heidelberg.
Ben Hachey, Will Radford, Joel Nothman, Matthew Hon-
nibal, and James R. Curran. 2013. Evaluating en-
tity linking with wikipedia. Artificial Intelligence,
194(0):130 ? 150. ?ce:title?Artificial Intelligence,
Wikipedia and Semi-Structured Resources?/ce:title?.
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Techologies, pages 945?954, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1148?1158, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation
of wikipedia entities in web text. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?09,
pages 457?466, New York, NY, USA. ACM.
Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou, Furu
Wei, and Yi Lu. 2013. Entity linking for tweets. In
Proceedings of the 51th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association for
Computational Linguistics.
P. McNamee and H.T. Dang. 2009. Overview of
the tac 2009 knowledge base population track. In
Proceedings of the Second Text Analysis Conference
(TAC2009).
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.
2012. Adding semantics to microblog posts. In Pro-
ceedings of the fifth ACM international conference on
Web search and data mining, WSDM ?12, pages 563?
572, New York, NY, USA. ACM.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In CIKM
?07: Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, pages 233?242, New York, NY, USA. ACM.
David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In CIKM ?08: Proceeding of the 17th
ACM conference on Information and knowledge man-
agement, pages 509?518, New York, NY, USA. ACM.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In NAACL2013,
pages 380?390, Atlanta, Georgia, June. Association
for Computational Linguistics.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computation-
al Linguistics: Human Language Technologies, pages
1375?1384, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2012. Linden: linking named entities with knowl-
edge base via semantic knowledge. In Proceedings of
the 21st international conference on World Wide We-
b, WWW ?12, pages 449?458, New York, NY, USA.
ACM.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1473?1481, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Vasudeva Varma, Vijay Bharat, Sudheer Kovelamudi,
Praveen Bysani, Santosh GSK, Kiran Kumar N, Kran-
thi Reddy, Karuna Kumar, and Nitin Maganti. 2009.
Iiit hyderabad at tac 2009. In Proceedings of the Sec-
ond Text Analysis Conference (TAC 2009), Gaithers-
burg, Maryland, USA, November.
Wei Zhang, Yan Chuan Sim, Jian Su, and Chew Lim Tan.
2011. Entity linking with effective acronym expan-
sion, instance selection, and topic modeling. In Toby
Walsh, editor, IJCAI 2011, pages 1909?1914.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan
Zhu. 2010. Learning to link entities with knowledge
base. In NAACL2010, pages 483?491, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. Technical report, Technical Report CMU-
CALD-02-107, Carnegie Mellon University.
868
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 407?410,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
HIT-CIR: An Unsupervised WSD System Based on Domain Most
Frequent Sense Estimation
Yuhang Guo, Wanxiang Che, Wei He, Ting Liu, Sheng Li
Harbin Institute of Technolgy
Harbin, Heilongjiang, PRC
yhguo@ir.hit.edu.cn
Abstract
This paper presents an unsupervised sys-
tem for all-word domain specific word
sense disambiguation task. This system
tags target word with the most frequent
sense which is estimated using a thesaurus
and the word distribution information in
the domain. The thesaurus is automati-
cally constructed from bilingual parallel
corpus using paraphrase technique. The
recall of this system is 43.5% on SemEval-
2 task 17 English data set.
1 Introduction
Tagging polysemous word with its most frequent
sense (MFS) is a popular back-off heuristic in
word sense disambiguation (WSD) systems when
the training data is inadequate. In past evalua-
tions, MFS from WordNet performed even bet-
ter than most of the unsupervised systems (Snyder
and Palmer, 2004; Navigli et al, 2007).
MFS is usually obtained from a large scale
sense tagged corpus, such as SemCor (Miller et al,
1994). However, some polysemous words have
different MFS in different domains. For example,
in the Koeling et al (2005) corpus, target word
coach means ?manager? mostly in the SPORTS
domain but means ?bus? mostly in the FINANCE
domain. So when the MFS is applied to specific
domains, it needs to be re-estimated.
McCarthy et al (2007) proposed an unsuper-
vised predominant word sense acquisition method
which obtains domain specific MFS without sense
tagged corpus. In their method, a thesaurus, in
which words are connected with their distribu-
tional similarity, is constructed from the domain
raw text. Word senses are ranked by their preva-
lence score which is calculated using the thesaurus
and the sense inventory.
In this paper, we propose another way to con-
struct the thesaurus. We use statistical machine
Figure 1: The architecture of HIT-CIR
translation (SMT) techniques to extract paraphrase
pairs from bilingual parallel text. In this way, we
avoid calculating similarities between every pair
of words and could find semantic similar words or
compounds which have dissimilar distributions.
Our system is comprised of two parts: the word
sense ranking part and the word sense tagging part.
Senses are ranked according to their prevalence
score in the target domain, and the predominant
sense is used to tag the occurrences of the target
word in the test data. The architecture of this sys-
tem is shown in Figure 1.
The word sense ranking part includes following
steps.
1. Tag the POS of the background text, count
the word frequency in each POS, and get the
polysemous word list of the POS.
2. Using SMT techniques to extract phrase table
407
Figure 2: Word sense ranking for the noun backbone
from the bilingual corpus. Extract the para-
phrases (called as neighbor words) with the
phrase table for each word in the polysemous
word list.
3. Calculate the prevalence score of each sense
of the target words, rank the senses with the
score and obtain the predominant sense.
We applied our system on the English data set
of SemEval-2 specific domain WSD task. This
task is an all word WSD task in the environ-
mental domain. We employed the domain back-
ground raw text provided by the task organizer as
well as the English WordNet 3.0 (Fellbaum, 1998)
and the English-Spanish parallel corpus from Eu-
roparl (Koehn, 2005).
This paper is organized as follows. Section 2
introduces how to rank word senses. Section 3
presents how to obtain the most related words of
the target words. We describe the system settings
in Section 4 and offer some discussions in Sec-
tion 5.
2 Word Sense Ranking
In our method, word senses are ranked according
to their prevalence score in the specific domain.
According to the assumption of McCarthy et al
(2007), the prevalence score is affected by the fol-
lowing two factors: (1) The relatedness score be-
tween a given sense of the target word and the
target word?s neighbor word. (2) The similarity
between the target word and its neighbor word.
In addition, we add another factor, (3) the impor-
tance of the neighbor word in the specific domain.
In this paper, ?neighbor words? means the words
which are most semantically similar to the target
word.
Figure 2 illustrates the word sense ranking pro-
cess of noun backbone. The contribution of a
neighbor word to a given word sense is measured
by the similarity between them and weighted by
the importance of the neighbor word in the tar-
get domain and the relatedness between the neigh-
bor word and the target word. Sum up the con-
tributions of each neighbor words, and we get the
prevalence score of the word sense.
Formally, the prevalence score of sense s
i
of a
target word w is assigned as follows:
ps(w, s
i
) =
?
n
j
?N
w
rs(w, n
j
) ? ns(s
i
, n
j
) ? dw(n
j
)
(1)
where
ns(s
i
, n
j
) =
sss(s
i
, n
j
)
?
s
i
?
?senses(w)
sss(s
i
?
, n
j
)
, (2)
sss(s
i
, n
j
) = max
s
x
?senses(n
j
)
sss
?
(s
i
, s
x
). (3)
rs(w, n
j
) is the relatedness score between w and
a neighbor word n
j
. N
w
= {n
1
, n
2
, . . . , n
k
}
is the top k relatedness score neighbor word set.
ns(s
i
, n
j
) is the normalized form of the sense sim-
ilarity score between sense s
i
and the neighbor
word n
j
(i.e. sss(s
i
, n
j
)). We define this score
with the maximum WordNet similarity score be-
tween s
i
and the senses of n
j
(i.e. sss
?
(s
i
, n
j
)).
In our system, lesk algorithm is used to measure
the sense similarity score between word senses.
408
Figure 3: Finding the neighbor words of noun backbone
The similarity of this algorithm is the count of
the number of overlap words in the gloss or the
definition of the senses (Banerjee and Pedersen,
2002). The domain importance weight dw(n
j
) is
assigned with the count of n
j
in the domain back-
ground corpus. For the neighbor word that does
not occur in the domain background text, we use
the add-one strategy. We will describe how to ob-
tain n
j
and rs in Section 3.
3 Thesaurus Construction
The neighbor words of the target word as well as
the relatedness score are obtained by extracting
paraphrases from bilingual parallel texts. When
a word is translated from source language to tar-
get language and then translated back to the source
language, the final translation may have the same
meaning to the original word but with different ex-
pressions (e.g. different word or compound). The
translation in the same language could be viewed
as a paraphrase term or, at least, related term of the
original word.
For example, in Figure 3, English noun back-
bone can be translated to columna, columna verte-
bral, pilar and convicciones etc. in Spanish, and
these words also have other relevant translations
in English, such as vertebral column, column, pil-
lar and convictions etc., which are semantically re-
lated to the target word backbone.
We use a statistical machine translation sys-
tem to calculate the translation probability from
English to another language (called as pivot lan-
guage) as well as the translation probability from
that language to English. By multiplying these
two probabilities, we get a paraphrase probabil-
ity. This method was defined in (Bannard and
Callison-Burch, 2005).
In our system, we choose the top k paraphrases
as the neighbor words of the target word, which
have the highest paraphrase probability. Note that
there are two directions of the paraphrase, from
target word to its neighbor word and from the
neighbor word to the target word. We choose
the paraphrase score of the former direction as
the relatedness score (rs). Because the higher
of the score in this direction, the target word is
more likely paraphrased to that neighbor word,
and hence the prevalence of the relevant target
word sense will be higher than other senses. For-
mally, the relatedness score is given by
rs(w, n
j
) =
?
f
p(f |w)p(n
j
|f), (4)
where f is the pivot language word.
We use the English-Spanish parallel text from
Europarl (Koehn, 2005). We choose Spanish as
the pivot language because in the both directions
the BLEU score of the translation between English
and Spanish is relatively higher than other English
and other languages (Koehn, 2005).
4 Data set and System Settings
The organizers of the SemEval-2 specific domain
WSD task provide no training data but raw back-
ground data in the environmental domain. The En-
glish background data is obtained from the offi-
cial web site of World Wide Fund (WWF), Euro-
pean Centre for Nature Conservation (ECNC), Eu-
ropean Commission and the United Nations Eco-
nomic Commission for Europe (UNECE). The
size of the raw text is around 15.5MB after sim-
ple text cleaning. The test data is from WWF and
ECNC, and contains 1398 occurrence of 436 tar-
get words.
For the implementation, we used bpos (Shen et
al., 2007) for the POS tagging. The maximum
409
number of the neighbor word of each target word k
was set to 50. We employed Giza++
1
and Moses
2
to get the phrase table from the bilingual paral-
lel corpus. TheWordNet::Similarity package
3
was
applied for the implement of the lesk word sense
similarity algorithm.
For the target word that is not in the polysemous
word list, we use the MFS from WordNet as the
back-off method.
5 Discussion and Future Work
The recall of our system is 43.5%, which is lower
than that of the MFS baseline, 50.5% (Agirre et
al., 2010). The baseline uses the most frequent
sense from the SemCor corpus (i.e. the MFS of
WordNet). This means that for some target words,
the MFS from SemCor is better than the domain
MFS we estimated in the environmental domain.
In the future, we will analysis errors in detail to
find the effects of the domain on the MFS.
For the domain specific task, it is better to use
parallel text in the domain of the test data in our
method. However, we didn?t find any available
parallel text in the environmental domain yet. In
the future, we will try some parallel corpus acqui-
sition techniques to obtain relevant corpus for en-
vironmental domain for our method.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, the ?863? National High-
Tech Research and Development of China via
grant 2008AA01Z144, and Natural Scientific Re-
search Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069).
References
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Shu kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluations (SemEval-2010), Association for Com-
putational Linguistics.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In CICLing ?02: Proceedings
1
http://www.fjoch.com/GIZA++.html
2
http://www.statmt.org/moses/
3
http://wn-similarity.sourceforge.net/
of the Third International Conference on Compu-
tational Linguistics and Intelligent Text Processing,
pages 136?145, London, UK. Springer-Verlag.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL
?05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 597?
604, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In The Tenth Ma-
chine Translation Summit, Phuket, Thailand.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing, pages 419?426, Vancouver, British
Columbia, Canada, October. Association for Com-
putational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553?590, December.
G. A. Miller, C. Leacock, R. Tengi, and R. Bunker.
1994. A semantic concordance. In Proc. ARPA
Human Language Technology Workshop ?93, pages
303?308, Princeton, NJ, March. distributed as Hu-
man Language Technology by San Mateo, CA: Mor-
gan Kaufmann Publishers.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 30?35, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760?767, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Rada Mihalcea and Phil
Edmonds, editors, Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, pages 41?43, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
410
