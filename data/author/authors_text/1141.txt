Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 698?706,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Jointly Combining Implicit Constraints Improves Temporal Ordering
Nathanael Chambers and Dan Jurafsky
Department of Computer Science
Stanford University
Stanford, CA 94305
{natec,jurafsky}@stanford.edu
Abstract
Previous work on ordering events in text has
typically focused on local pairwise decisions,
ignoring globally inconsistent labels. How-
ever, temporal ordering is the type of domain
in which global constraints should be rela-
tively easy to represent and reason over. This
paper presents a framework that informs lo-
cal decisions with two types of implicit global
constraints: transitivity (A before B and B be-
fore C implies A before C) and time expression
normalization (e.g. last month is before yes-
terday). We show how these constraints can
be used to create a more densely-connected
network of events, and how global consis-
tency can be enforced by incorporating these
constraints into an integer linear programming
framework. We present results on two event
ordering tasks, showing a 3.6% absolute in-
crease in the accuracy of before/after classifi-
cation over a pairwise model.
1 Introduction
Being able to temporally order events is a neces-
sary component for complete document understand-
ing. Interest in machine learning approaches for this
task has recently been encouraged through the cre-
ation of the Timebank Corpus (Pustejovsky et al,
2003). However, most work on event-event order-
ing has focused on improving classifiers for pair-
wise decisions, ignoring obvious contradictions in
the global space of events when misclassifications
occur. A global framework to repair these event or-
dering mistakes has not yet been explored.
This paper addresses three main factors involved
in a global framework: the global optimization al-
gorithm, the constraints that are relevant to the task,
and the level of connectedness across pairwise de-
cisions. We employ Integer Linear Programming to
address the first factor, drawing from related work
in paragraph ordering (Bramsen et al, 2006). After
finding minimal gain with the initial model, we ex-
plore reasons for and solutions to the remaining two
factors through temporal reasoning and transitivity
rule expansion.
We analyze the connectivity of the Timebank Cor-
pus and show how textual events can be indirectly
connected through a time normalization algorithm
that automatically creates new relations between
time expressions. We show how this increased con-
nectivity is essential for a global model to improve
performance.
We present three progressive evaluations of our
global model on the Timebank Corpus, showing a
3.6% gain in accuracy over its original set of re-
lations, and an 81% increase in training data size
from previous work. In addition, we present the first
results on Timebank that include an unknown rela-
tion, establishing a benchmark for performance on
the full task of document ordering.
2 Previous Work
Recent work on classifying temporal relations
within the Timebank Corpus built 6-way relation
classifiers over 6 of the corpus? 13 relations (Mani et
al., 2006; Mani et al, 2007; Chambers et al, 2007).
A wide range of features are used, ranging from sur-
face indicators to semantic classes. Classifiers make
698
local pairwise decisions and do not consider global
implications between the relations.
The TempEval-07 (Verhagen et al, 2007) contest
recently used two relations, before and after, in a
semi-complete textual classification task with a new
third relation to distinguish relations that can be la-
beled with high confidence from those that are un-
certain, called vague. The task was a simplified clas-
sification task from Timebank in that only one verb,
the main verb, of each sentence was used. Thus, the
task can be viewed as ordering the main events in
pairwise sentences rather than the entire document.
This paper uses the core relations of TempEval
(before,after,vague) and applies them to a full docu-
ment ordering task that includes every labeled event
in Timebank. In addition, we extend the previous
work by including a temporal reasoning component
and embedding it within a global constraint model.
3 The Timebank Corpus
The Timebank Corpus (Pustejovsky et al, 2003) is
a corpus of 186 newswire articles that are tagged
for events, time expressions, and relations between
the events and times. The individual events are fur-
ther tagged for temporal information such as tense,
modality and grammatical aspect. Time expressions
use the TimeML (Ingria and Pustejovsky, 2002)
markup language. There are 6 main relations and
their inverses in Timebank: before, ibefore, includes,
begins, ends and simultaneous.
This paper describes work that classifies the re-
lations between events, making use of relations be-
tween events and times, and between the times
themselves to help inform the decisions.
4 The Global Model
Our initial model has two components: (1) a pair-
wise classifier between events, and (2) a global con-
straint satisfaction layer that maximizes the confi-
dence scores from the classifier. The first is based
on previous work (Mani et al, 2006; Chambers et
al., 2007) and the second is a novel contribution to
event-event classification.
4.1 Pairwise Classification
Classifying the relation between two events is the
basis of our model. A soft classification with confi-
dence scores is important for the global maximiza-
tion step that is described in the next section. As
in Chambers et al (2007), we build support vec-
tor machine (SVM) classifiers and use the probabili-
ties from pairwise SVM decisions as our confidence
scores. These scores are then used to choose an op-
timal global ordering.
Following our previous work, we use the set of
features summarized in figure 1. They vary from
POS tags and lexical features surrounding the event,
to syntactic dominance, to whether or not the events
share the same tense, grammatical aspect, or aspec-
tual class. These features are the highest performing
set on the basic 6-way classification of Timebank.
Feature Description
Word* The text of the event
Lemma* The lemmatized head word
Synset* The WordNet synset of head word
POS* 4 POS tags, 3 before, and 1 event
POS bigram* The POS bigram of the event and its
preceding tag
Prep* Preposition lexeme, if in a preposi-
tional phrase
Tense* The event?s tense
Aspect* The event?s grammatical aspect
Modal* The modality of the event
Polarity* Positive or negative
Class* The aspecual class of the event
Tense Pair The two concatenated tenses
Aspect Pair The two concatenated aspects
Class Pair The two concatenated classes
POS Pair The two concatenated POS tags
Tense Match true if the events have the same tense
Aspect Match true if the events have the same as-
pect
Class Match true if the events have the same class
Dominates true if the first event syntactically
dominates the second
Text Order true if the first event occurs first in
the document
Entity Match true if they share an entity as an ar-
gument
Same Sent true if both events are in the same
sentence
Figure 1: The features to learn temporal relations be-
tween two events. Asterisks (*) indicate features that are
duplicated, one for each of the two events.
We use Timebank?s hand tagged attributes in the
feature values for the purposes of this comparative
699
before after unknown
A r1 B .5 .3 .2
B r2 C .4 .3 .3
A r3 C .4 .5 .1
total 1.3 1.1 .6
A r1 B .5 .3 .2
B r2 C .4 .3 .3
A r3 C .2 .7 .1
total 1.1 1.3 .6
Figure 2: Two sets of confidence scores. The first set
chooses before for all three labels, and the second chooses
after. Other lower-scoring valid relation sets also exist,
such as before, unknown, and before.
study of global constraints, described next.
4.2 Global Constraints
Pairwise classifiers can make contradictory classifi-
cations due to their inability to consider other deci-
sions. For instance, the following three decisions are
in conflict:
A before B
B before C
A after C
Transitivity is not taken into account. In fact, there
are several ways to resolve the conflict in this exam-
ple. Given confidence scores (or probabilities) for
each possible relation between the three pairs, we
can compute an optimal label assignment. Differ-
ent scores can lead to different conflict resolutions.
Figure 2 shows two resolutions given different sets
of scores. The first chooses before for all three rela-
tions, while the second chooses after.
Bramsen et al (2006) presented a variety of ap-
proaches to using transitivity constraints to help in-
form pairwise decisions. They found that Integer
Linear Programming (ILP) performed the best on a
paragraph ordering task, consistent with its property
of being able to find the optimal solution for a set
of constraints. Other approaches are variations on
a greedy strategy of adding pairs of events one at a
time, ordered by their confidence. These can lead to
suboptimal configurations, although they are guar-
anteed to find a solution. Mani et al (2007) sub-
sequently proposed one of these greedy strategies as
well, but published results are not available. We also
implemented a greedy best-first strategy, but found
ILP outperformed it.
Our Integer Linear Programming framework uses
the following objective function:
max
?
i
?
j
pijxij (1)
with added constraints:
?i?j xij ? {0, 1} (2)
?i xi1 + xi2 + ... + xim = 1 (3)
where xij represents the ith pair of events classified
as the jth relation of m relations. Thus, each pair
of events generates m variables. Given n pairs of
events, there are n ? m variables. pij is the proba-
bility of classifying pair i with relation j. Equation
2 (the first constraint) simply says that each variable
must be 0 or 1. Equation 3 contains m variables for
a single pair of events i representing its m possible
relations. It states that one relation must be set to 1
and the rest to 0. In other words, a pair of events
cannot have two relations at the same time. Finally,
a transitivity constraint is added for all connected
pairs i, j, k, for each transitivity condition that infers
relation c given a and b:
xia + xjb ? xkc <= 1 (4)
We generated the set of constraints for each doc-
ument and used lpsolve1 to solve the ILP constraint
problem.
The transitivity constraints are only effective if
the available pairwise decisions constitute a con-
nected graph. If pairs of events are disconnected,
then transitivity makes little to no contribution be-
cause these constraints are only applicable to con-
nected chains of events.
4.3 Transitive Closure
In order to connect the event graph, we draw on
work from (Mani et al, 2006) and apply transitive
closure to our documents. Transitive closure was
first proposed not to address the problem of con-
nected event graphs, but rather to expand the size
of training data for relations such as before. Time-
bank is a relatively small corpus with few examples
1http://sourceforge.net/projects/lpsolve
700
Total Event-Event Relations After Closure
before after
Timebank 592 656
+ closure 3919 3405
Figure 3: The number of event-event relations after tran-
sitive closure.
of each relation. One way of expand the training
set is through transitive rules. A few rules are given
here:
A simultaneous B ?A before C ? B before C
A includes B ?A ibefore C ? B before C
A before B ?A ends C ? B after C
While the original motivation was to expand the
training size of tagged relations, this approach also
creates new connections in the graph, replacing pre-
viously unlabeled event pairs with their true rela-
tions. We adopted this approach and closed the orig-
inal set of 12 relations to help connect the global
constraint model.
4.4 Initial Experiment
The first evaluation of our global temporal model
is on the Timebank Corpus over the labeled rela-
tions before and after. We merged ibefore and iafter
into these two relations as well, ignoring all oth-
ers. We use this task as a reduced evaluation to
study the specific contribution of global constraints.
We also chose this strict ordering task because it is
well defined from a human understanding perspec-
tive. Snow et al (2008) shows that average inter-
net users can make before/after decisions with very
high confidence, although the distinction with an un-
known relation is not as clear. An evaluation includ-
ing unknown (or vague as in TempEval) is presented
later.
We expanded the corpus (prior to selecting the be-
fore/after relations) using transitive closure over all
12 relations as described above. Figure 3 shows the
increase in data size. The number of before and after
relations increase by a factor of six.
We trained and tested the system with 10-fold
cross validation and micro-averaged accuracies. The
folds were randomly generated to separate the 186
files into 10 folds (18 or 19 files per fold). The same
10-way split is used for all the evaluations. We used
Comparative Results
Training Set Accuracy
Timebank Pairwise 66.8%
Global Model 66.8%
Figure 4: Using the base Timebank annotated tags for
testing, accuracy on before/after tags in the two models.
libsvm2 to implement our SVM classifiers.
Figure 4 shows the results from our ILP model
with transitivity constraints. The first row is the
baseline pairwise classification trained and tested on
the original Timebank relations. The second row
gives performance with ILP. The model shows no
improvement. The global ILP constraints did affect
local decisions, changing 175 of them (out of 7324),
but the changes cancelled out and had no affect on
overall accuracy.
4.5 Loosely Connected Graph
Why didn?t a global model help? The problem lies
in the graph structure of Timebank?s annotated rela-
tions. The Timebank annotators were not required
to annotate relations between any particular pair of
events. Instead, they were instructed to annotate
what seemed appropriate due to the almost insur-
mountable task of annotating all pairs of events. A
modest-sized document of 30 events, for example,
would contain
(30
2
)
= 435 possible pairs. Anno-
tators thus marked relations where they deemed fit,
most likely between obvious and critical relations to
the understanding of the article. The vast majority of
possible relations are untagged, thus leaving a large
set of unlabeled (and disconnected) unknown rela-
tions.
Figure 5 graphically shows all relations that are
annotated between events and time expressions in
one of the shorter Timebank documents. Nodes rep-
resent events and times (event nodes start with the
letter ?e?, times with ?t?), and edges represent tempo-
ral relations. Solid lines indicate hand annotations,
and dotted lines indicate new rules from transitive
closure (only one, from event e4 to time t14). As
can be seen, the graph is largely disconnected and
a global model contributes little information since
transitivity constraints cannot apply.
2http://www.csie.ntu.edu.tw/? cjlin/libsvm
701
Timebank Annotation of wsj 0551
Figure 5: Annotated relations in document wsj 0551.
The large amount of unlabeled relations in the
corpus presents several problems. First, building a
classifier for these unknown relations is easily over-
whelmed by the huge training set. Second, many of
the untagged pairs have non-unknown ordering rela-
tions between them, but were missed by the annota-
tors. This point is critical because one cannot filter
this noise when training an unknown classifier. The
noise problem will appear later and will be discussed
in our final experiment. Finally, the space of an-
notated events is very loosely connected and global
constraints cannot assist local decisions if the graph
is not connected. The results of this first experiment
illustrate this latter problem.
Bethard et al (2007) strengthen the claim that
many of Timebank?s untagged relations should not
be left unlabeled. They performed an independent
annotation of 129 of Timebank?s 186 documents,
tagging all events in verb-clause relationships. They
found over 600 valid before/after relations that are
untagged in Timebank, on average three per docu-
ment. One must assume that if these nearby verb-
clause event pairs were missed by the annotators,
the much larger number of pairs that cross sentence
boundaries were also missed.
The next model thus attempts to fill in some of the
gaps and further connect the event graph by using
two types of knowledge. The first is by integrating
Bethard?s data, and the second is to perform tempo-
ral reasoning over the document?s time expressions
(e.g. yesterday or january 1999).
5 A Global Model With Time
Our initial model contained two components: (1) a
pairwise classifier between events, and (2) a global
constraint satisfaction layer. However, due to the
sparseness in the event graph, we now introduce
a third component addressing connectivity: (3) a
temporal reasoning component to inter-connect the
global graph and assist in training data expansion.
One important aspect of transitive closure in-
cludes the event-time and time-time relations during
closure, not just the event-event links. Starting with
5,947 different types of relations, transitive rules in-
crease the dataset to approximately 12,000. How-
ever, this increase wasn?t enough to be effective in
global reasoning. To illustrate the sparsity that still
remains, if each document was a fully connected
graph of events, Timebank would contain close to
160,000 relations3, more than a 13-fold increase.
More data is needed to enrich the Timebank event
graph. Two types of information can help: (1) more
event-event relations, and (2) a separate type of in-
formation to indirectly connect the events: event-
X-event. We incorporate the new annotations from
Bethard et al (2007) to address (1) and introduce
a new temporal reasoning procedure to address (2).
The following section describes this novel approach
to adding time expression information to further
connect the graph.
5.1 Time-Time Information
As described above, we use event-time relations to
produce the transitive closure, as well as annotated
time-time relations. It is unclear if Mani et al (2006)
used these latter relations in their work.
However, we also add new time-time links that
are deduced from the logical time intervals that they
describe. Time expressions can be resolved to time
intervals with some accuracy through simple rules.
New time-time relations can then be added to our
space of events through time stamp comparisons.
Take this newswire example:
The Financial Times 100-share index shed 47.3 points to
close at 2082.1, down 4.5% from the previous Friday,
and 6.8% from Oct. 13, when Wall Street?s plunge helped
spark the current weakness in London.
3Sum over the # of events nd in each document d,
(
nd
2
)
702
The first two expressions (?previous Friday?
and ?Oct. 13?) are in a clear before relation-
ship that Timebank annotators captured. The
?current? expression, is correctly tagged with the
PRESENT REF attribute to refer to the document?s
timestamp. Both ?previous Friday? and ?Oct. 13?
should thus be tagged as being before this expres-
sion. However, the annotators did not tag either
of these two before relations, and so our timestamp
resolution procedure fills in these gaps. This is a
common example of two expressions that were not
tagged by the annotators, yet are in a clear temporal
relationship.
We use Timebank?s gold standard TimeML an-
notations to extract the dates and times from the
time expressions. In addition, those marked as
PRESENT REF are resolved to the document times-
tamp. Time intervals that are strictly before or after
each other are thus labeled and added to our space
of events. We create new before relations based on
the following procedure:
if event1.year < event2.year
return true
if event1.year == event2.year
if event1.month < event2.month
return true
if event1.month == event2.month
if event1.day < event2.day
return true
end
end
return false
All other time-time orderings not including the
before relation are ignored (i.e. includes is not cre-
ated, although could be with minor changes).
This new time-time knowledge is used in two sep-
arate stages of our model. The first is just prior to
transitive closure, enabling a larger expansion of our
tagged relations set and reduce the noise in the un-
known set. The second is in the constraint satisfac-
tion stage where we add our automatically computed
time-time relations (with the gold event-time rela-
tions) to the global graph to help correct local event-
event mistakes.
Total Event-Event Relations After Closure
before after
Timebank 3919 3405
+ time-time 5604 5118
+ time/bethard 7111 6170
Figure 6: The number of event-event before and after re-
lations after transitive closure on each dataset.
Comparative Results with Closure
Training Set Accuracy
Timebank Pairwise 66.8%
Global Model 66.8%
Global + time/bethard 70.4%
Figure 7: Using the base Timebank annotated tags for
testing, the increase in accuracy on before/after tags.
5.2 Temporal Reasoning Experiment
Our second evaluation continues the use of the two-
way classification task with before and after to ex-
plore the contribution of closure, time normaliza-
tion, and global constraints.
We augmented the corpus with the labeled rela-
tions from Bethard et al (2007) and added the au-
tomatically created time-time relations as described
in section 5.1. We then expanded the corpus using
transitive closure. Figure 6 shows the progressive
data size increase as we incrementally add each to
the closure algorithm.
The time-time generation component automati-
cally added 2459 new before and after time-time re-
lations into the 186 Timebank documents. This is
in comparison to only 157 relations that the human
annotators tagged, less than 1 per document on av-
erage. The second row of figure 6 shows the dras-
tic effect that these time-time relations have on the
number of available event-event relations for train-
ing and testing. Adding both Bethard?s data and
the time-time data increases our training set by 81%
over closure without it.
We again performed 10-fold cross validation with
micro-averaged accuracies, but each fold tested only
on the transitively closed Timebank data (the first
row of figure 6). The training set used all available
data (the third row of figure 6) including the Bethard
data as well as our new time-time links.
703
Figure 7 shows the results from the new model.
The first row is the baseline pairwise classification
trained and tested on the original relations only. Our
model improves by 3.6% absolute. This improve-
ment is statistically significant (p < 0.000001, Mc-
Nemar?s test, 2-tailed).
5.3 Discussion
To further illustrate why our model now improves
local decisions, we continue our previous graph ex-
ample. The actual text for the graph in figure 5 is
shown here:
docstamp: 10/30/89 (t14)
Trustcorp Inc. will become(e1) Society Bank & Trust
when its merger(e3) is completed(e4) with Society Corp.
of Cleveland, the bank said(e5). Society Corp., which is
also a bank, agreed(e6) in June(t15) to buy(e8) Trustcorp
for 12.4 million shares of stock with a market value of
about $450 million. The transaction(e9) is expected(e10)
to close(e2) around year end(t17).
The automatic time normalizer computes and adds
three new time-time relations, two connecting t15
and t17 with the document timestamp, and one con-
necting t15 and t17 together. These are not other-
wise tagged in the corpus.
Time-Time + Closure
Figure 8: Before and after time-time links with closure.
Figure 8 shows the augmented document. The
double-line arrows indicate the three new time-time
relations and the dotted edges are the new relations
added by our transitive closure procedure. Most crit-
ical to this paper, three of the new edges are event-
event relations that help to expand our training data.
If this document was used in testing (rather than
training), these new edges would help inform our
transitive rules during classification.
Even with this added information, disconnected
segments of the graph are still apparent. However,
the 3.6% performance gain encourages us to move
to the final full task.
6 Final Experiment with Unknowns
Our final evaluation expands the set of relations to
include unlabeled relations and tests on the entire
dataset available to us. The following is now a clas-
sification task between the three relations: before,
after, and unknown.
We duplicated the previous evaluation by adding
the labeled relations from Bethard et al (2007) and
our automatically created time-time relations. We
then expanded this dataset using transitive closure.
Unlike the previous evaluation, we also use this en-
tire dataset for testing, not just for training. Thus, all
event-event relations in Bethard as well as Timebank
are used to expand the dataset with transitive closure
and are used in training and testing. We wanted to
fully evaluate document performance on every pos-
sible event-event relation that logically follows from
the data.
As before, we converted IBefore and IAfter into
before and after respectively, while all other rela-
tions are reduced to unknown. This relation set co-
incides with TempEval-07?s core three relations (al-
though they use vague instead of unknown).
Rather than include all unlabeled pairs in our un-
known set, we only include the unlabeled pairs that
span at most one sentence boundary. In other words,
events in adjacent sentences are included in the un-
known set if they were not tagged by the Timebank
annotators. The intuition is that annotators are more
likely to label nearby events, and so events in adja-
cent sentences are more likely to be actual unknown
relations if they are unlabeled. It is more likely that
distant events in the text were overlooked by con-
venience, not because they truly constituted an un-
known relationship.
The set of possible sentence-adjacent unknown re-
lations is very large (approximately 50000 unknown
compared to 7000 before), and so we randomly se-
lect a percentage of these relations for each evalu-
704
Classification Accuracy
% unk base global global+time
0 72.0% 72.2% 74.0%
1 69.4% 69.5% 71.3%
3 65.5% 65.6% 67.1%
5 63.7% 63.8% 65.3%
7 61.2% 61.6% 62.8%
9 59.3% 59.5% 60.6%
11 58.1% 58.4% 59.4%
13 57.1% 57.1% 58.1%
Figure 9: Overall accuracy when training with different
percentages of unknown relations included. 13% of un-
knowns is about equal to the number of befores.
ation. We used the same SVM approach with the
features described in section 4.1.
6.1 Results
Results are presented in figure 9. The rows in the
table are different training/testing runs on varying
sizes of unknown training data. There are three
columns with accuracy results of increasing com-
plexity. The first, base, are results from pairwise
classification decisions over Timebank and Bethard
with no global model. The second, global, are re-
sults from the Integer Linear Programming global
constraints, using the pairwise confidence scores
from the base evaluation. Finally, the global+time
column shows the ILP results when all event-time,
time-time, and automatically induced time-time re-
lations are included in the global graph.
The ILP approach does not alone improve perfor-
mance on the event-event tagging task, but adding
the time expression relations greatly increases the
global constraint results. This is consistent with the
results from out first two experiments. The evalua-
tion with 1% of the unknown tags shows an almost
2% improvement in accuracy. The gain becomes
smaller as the unknown set increases in size (1.0%
gain with 13% unknown). Unknown relations will
tend to be chosen as more weight is given to un-
knowns. When there is a constraint conflict in the
global model, unknown tends to be chosen because
it has no transitive implications. All improvements
from base to global+time are statistically significant
(p < 0.000001, McNemar?s test, 2-tailed).
Base Pairwise Classification
precision recall f1-score
before 61.4 55.4 58.2
after 57.6 53.1 55.3
unk 53.0 62.8 57.5
Global+Time Classification
precision recall f1-score
before 63.7 (+2.3) 57.1 (+2.2) 60.2 (+2.0)
after 60.3 (+2.7) 54.3 (+2.9) 57.1 (+1.8)
unk 52.0 (-1.0) 62.9 (+0.1) 56.9 (-0.6)
Figure 10: Precision and Recall for the base pairwise de-
cisions and the global constraints with integrated time in-
formation.
The first row of figure 9 corresponds to the re-
sults in our second experiment in figure 7, but shows
higher accuracy. The reason is due to our different
test sets. This final experiment includes Bethard?s
event-event relations in testing. The improved per-
formance suggests that the clausal event-event rela-
tions are easier to classify, agreeing with the higher
accuracies originally found by Bethard et al (2007).
Figure 10 shows the precision, recall, and f-score
for the evaluation with 13% unknowns. This set was
chosen for comparison because it has a similar num-
ber of unknown labels as before labels. We see an
increase in precision in both the before and after de-
cisions by up to 2.7%, an increase in recall up to
2.9%, and an fscore by as much as 2.0%. The un-
known relation shows mixed results, possibly due to
its noisy behavior as discussed throughout this pa-
per.
6.2 Discussion
Our results on the two-way (before/after) task show
that adding additional implicit temporal constraints
and then performing global reasoning results in
significant improvements in temporal ordering of
events (3.6% absolute over simple pairwise deci-
sions).
Both before and after also showed increases in
precision and recall in the three-way evaluation.
However, unknown did not parallel this improve-
ment, nor are the increases as dramatic as in the two-
way evaluation. We believe this is consistent with
the noise that exists in the Timebank corpus for un-
labeled relations. Evidence from Bethard?s indepen-
705
dent annotations directly point to missing relations,
but the dramatic increase in the size of our closure
data (81%) from adding a small amount of time-time
relations suggests that the problem is widespread.
This noise in the unknown relation may be damp-
ening the gains that the two way task illustrates.
This work is also related to the task of event-time
classification. While not directly addressed in this
paper, the global methods described within clearly
apply to pairwise models of event-time ordering as
well.
Further progress in improving global constraints
will require new methods to more accurately iden-
tify unknown events, as well as new approaches to
create implicit constraints over the ordering. We ex-
pect such an improved ordering classifier to be used
to improve the performance of tasks such as summa-
rization and question answering about the temporal
nature of events.
Acknowledgments
This work is funded in part by DARPA through IBM
and by the DTO Phase III Program for AQUAINT.
We also thank our anonymous reviewers for many
helpful suggestions.
References
Steven Bethard, James H. Martin, and Sara Klingenstein.
2007. Timelines from text: Identification of syntac-
tic temporal relations. In International Conference on
Semantic Computing.
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Inducing temporal graphs.
In Proceedings of EMNLP-06.
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying temporal relations between events.
In Proceedings of ACL-07, Prague, Czech Republic.
R Ingria and James Pustejovsky. 2002. TimeML specifi-
cation 1.0. In http://www.time2002.org.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In Proceedings of ACL-06, July.
Inderjeet Mani, Ben Wellner, Marc Verhagen, and James
Pustejovsky. 2007. Three approaches to learning
tlinks in timeml. Technical Report CS-07-268, Bran-
deis University.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew
See, David Day, Lisa Ferro, Robert Gaizauskas, Mar-
cia Lazo, Andrea Setzer, and Beth Sundheim. 2003.
The timebank corpus. Corpus Linguistics, pages 647?
656.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Ng. 2008. Cheap and fast - but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP-08, Waikiki, Hawaii,
USA.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Workshop on Semantic Evalu-
ations.
706
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 173?176,
Prague, June 2007. c?2007 Association for Computational Linguistics
Classifying Temporal Relations Between Events
Nathanael Chambers and Shan Wang and Dan Jurafsky
Department of Computer Science
Stanford University
Stanford, CA 94305
{natec,shanwang,jurafsky}@stanford.edu
Abstract
This paper describes a fully automatic two-
stage machine learning architecture that
learns temporal relations between pairs of
events. The first stage learns the temporal
attributes of single event descriptions, such
as tense, grammatical aspect, and aspectual
class. These imperfect guesses, combined
with other linguistic features, are then used
in a second stage to classify the temporal re-
lationship between two events. We present
both an analysis of our new features and re-
sults on the TimeBank Corpus that is 3%
higher than previous work that used perfect
human tagged features.
1 Introduction
Temporal information encoded in textual descrip-
tions of events has been of interest since the early
days of natural language processing. Lately, it has
seen renewed interest as Question Answering, Infor-
mation Extraction and Summarization domains find
it critical in order to proceed beyond surface under-
standing. With the recent creation of the Timebank
Corpus (Pustejovsky et al, 2003), the utility of ma-
chine learning techniques can now be tested.
Recent work with the Timebank Corpus has re-
vealed that the six-class classification of temporal
relations is very difficult, even for human annotators.
The highest score reported on Timebank achieved
62.5% accuracy when using gold-standard features
as marked by humans (Mani et al, 2006). This pa-
per describes an approach using features extracted
automatically from raw text that not only dupli-
cates this performance, but surpasses its accuracy
by 3%. We do so through advanced linguistic fea-
tures and a surprising finding that using automatic
rather than hand-labeled tense and aspect knowledge
causes only a slight performance degradation.
We briefly describe current work on temporal or-
dering in section 2. Section 4 describes the first stage
of basic temporal extraction, followed by a full de-
scription of the second stage in 5. The evaluation
and results on Timebank then follow in section 6.
2 Previous Work
Mani et. al (2006) built a MaxEnt classifier that as-
signs each pair of events one of 6 relations from an
augmented Timebank corpus. Their classifier relies
on perfect features that were hand-tagged in the cor-
pus, including tense, aspect, modality, polarity and
event class. Pairwise agreement on tense and aspect
are also included. In a second study, they applied
rules of temporal transitivity to greatly expand the
corpus, providing different results on this enlarged
dataset. We could not duplicate their reported per-
formance on this enlarged data, and instead focus on
performing well on the Timebank data itself.
Lapata and Lascarides (2006) trained an event
classifier for inter-sentential events. They built a cor-
pus by saving sentences that contained two events,
one of which is triggered by a key time word (e.g.
after and before). Their learner was based on syntax
and clausal ordering features. Boguraev and Ando
(2005) evaluated machine learning on related tasks,
but not relevant to event-event classification.
Our work is most similar to Mani?s in that we are
173
learning relations given event pairs, but our work ex-
tends their results both with new features and by us-
ing fully automatic linguistic features from raw text
that are not hand selected from a corpus.
3 Data
We used the Timebank Corpus (v1.1) for evaluation,
186 newswire documents with 3345 event pairs.
Solely for comparison with Mani, we add the 73
document Opinion Corpus (Mani et al, 2006) to cre-
ate a larger dataset called the OTC. We present both
Timebank and OTC results so future work can com-
pare against either. All results below are from 10-
fold cross validation.
4 Stage One: Learning Event Attributes
The task in Stage One is to learn the five tempo-
ral attributes associated with events as tagged in the
Timebank Corpus. (1) Tense and (2) grammatical
aspect are necessary in any approach to temporal
ordering as they define both temporal location and
structure of the event. (3) Modality and (4) polar-
ity indicate hypothetical or non-occuring situations,
and finally, (5) event class is the type of event (e.g.
process, state, etc.). The event class has 7 values in
Timebank, but we believe this paper?s approach is
compatible with other class divisions as well. The
range of values for each event attribute is as follows,
also found in (Pustejovsky et al, 2003):
tense none, present, past, future
aspect none, prog, perfect, prog perfect
class report, aspectual, state, I state
I action, perception, occurrence
modality none, to, should, would, could
can, might
polarity positive, negative
4.1 Machine Learning Classification
We used a machine learning approach to learn each
of the five event attributes. We implemented both
Naive Bayes and Maximum Entropy classifiers, but
found Naive Bayes to perform as well or better than
Maximum Entropy. The results in this paper are
from Naive Bayes with Laplace smoothing.
The features we used on this stage include part of
speech tags (two before the event), lemmas of the
event words, WordNet synsets, and the appearance
tense POS-2-event, POS-1-event
POS-of-event, have word, be word
aspect POS-of-event, modal word, be word
class synset
modality none
polarity none
Figure 1: Features selected for learning each tempo-
ral attribute. POS-2 is two tokens before the event.
Timebank Corpus
tense aspect class
Baseline 52.21 84.34 54.21
Accuracy 88.28 94.24 75.2
Baseline (OTC) 48.52 86.68 59.39
Accuracy (OTC) 87.46 88.15 76.1
Figure 2: Stage One results on classification.
of auxiliaries and modals before the event. This lat-
ter set included all derivations of be and have auxil-
iaries, modal words (e.g. may, might, etc.), and the
presence/absence of not. We performed feature se-
lection on this list of features, learning a different set
of features for each of the five attributes. The list of
selected features for each is shown in figure 1.
Modality and polarity did not select any features
because their majority class baselines were so high
(98%) that learning these attributes does not provide
much utility. A deeper analysis of event interaction
would require a modal analysis, but it seems that a
newswire domain does not provide great variation
in modalities. Consequently, modality and polarity
are not used in Stage Two. Tense, aspect and class
are shown in figure 2 with majority class baselines.
Tense classification achieves 36% absolute improve-
ment, aspect 10% and class 21%. Performance on
the OTC set is similar, although aspect is not as
good. These guesses are then passed to Stage Two.
5 Stage Two: Event-Event Features
The task in this stage is to choose the temporal re-
lation between two events, given the pair of events.
We assume that the events have been extracted and
that there exists some relation between them; the
task is to choose the relation. The Timebank Corpus
uses relations that are based on Allen?s set of thir-
174
teen (Allen, 1984). Six of the relations are inverses
of the other six, and so we condense the set to be-
fore, ibefore, includes, begins, ends and simultane-
ous. We map the thirteenth identity into simultane-
ous. One oddity is that Timebank includes both dur-
ing and included by relations, but during does not
appear in Timebank documentation. While we don?t
know how previous work handles this, we condense
during into included by (invert to includes).
5.1 Features
Event Specific: The five temporal attributes from
Stage One are used for each event in the pair, as well
as the event strings, lemmas and WordNet synsets.
Mani added two other features from these, indica-
tors if the events agree on tense and aspect. We add
a third, event class agreement. Further, to capture
the dependency between events in a discourse, we
create new bigram features of tense, aspect and class
(e.g. ?present past? if the first event is in the present,
and the second past).
Part of Speech: For each event, we include the Penn
Treebank POS tag of the event, the tags for the two
tokens preceding, and one token following. We use
the Stanford Parser1 to extract them. We also extend
previous work and create bigram POS features of the
event and the token before it, as well as the bigram
POS of the first event and the second event.
Event-Event Syntactic Properties: A phrase P is
said to dominate another phrase Q if Q is a daugh-
ter node of P in the syntactic parse tree. We lever-
age the syntactic output of the parser to create the
dominance feature for intra-sentential events. It is
either on or off, depending on the two events? syn-
tactic dominance. Lapata used a similar feature for
subordinate phrases and an indicator before for tex-
tual event ordering. We adopt these features and also
add a same-sentence indicator if the events appear in
the same sentence.
Prepositional Phrase: Since preposition heads are
often indicators of temporal class, we created a new
feature indicating when an event is part of a prepo-
sitional phrase. The feature?s values range over 34
English prepositions. Combined with event dom-
inance (above), these two features capture direct
1http://nlp.stanford.edu/software/lex-parser.shtml
intra-sentential relationships. To our knowledge, we
are the first to use this feature in temporal ordering.
Temporal Discourse: Seeing tense as a type of
anaphora, it is a natural conclusion that the rela-
tionship between two events becomes stronger as
the textual distance draws closer. Because of this,
we adopted the view that intra-sentential events are
generated from a different distribution than inter-
sentential events. We therefore train two models
during learning, one for events in the same sen-
tence, and the other for events crossing sentence
boundaries. It essentially splits the data on the
same sentence feature. As we will see, this turned
out to be a very useful feature. It is called the split
approach in the next section.
Example (require, compromise):
?Their solution required a compromise...?
Features
(lemma1: require) (lemma2: compromise) (dominates: yes)
(tense-bigram: past-none) (aspect-bigram: none-none) (tense-
match: no) (aspect-match: yes) (before: yes) (same-sent: yes)
6 Evaluation and Results
All results are from a 10-fold cross validation us-
ing SVM (Chang and Lin, 2001). We also eval-
uated Naive Bayes and Maximum Entropy. Naive
Bayes (NB) returned similar results to SVM and we
present feature selection results from NB to compare
the added value of our new features.
The input to Stage Two is a list of pairs of events;
the task is to classify each according to one of six
temporal relations. Four sets of results are shown
in figure 3. Mani, Mani+Lapata and All+New cor-
respond to performance on features as listed in the
figure. The three table columns indicate how a gold-
standard Stage One (Gold) compares against imper-
fect guesses (Auto) and the guesses with split distri-
butions (Auto-Split).
A clear improvement is seen in each row, indi-
cating that our new features provide significant im-
provement over previous work. A decrease in per-
formance is seen between columns gold and auto,
as expected, because imperfect data is introduced,
however, the drop is manageable. The auto-split dis-
tributions make significant gains for the Mani and
Lapata features, but less when all new features are
175
Timebank Corpus Gold Auto Auto-Split
Baseline 37.22 37.22 46.58
Mani 50.97 50.19 53.42
Mani+Lapata 52.29 51.57 55.10
All+New 60.45 59.13 59.43
Mani stage one attributes, tense/aspect-match, event strings
Lapata dominance, before, lemma, synset
New prep-phrases, same-sent, class-match, POS uni/bigrams,
tense/aspect/class-bigrams
Figure 3: Incremental accuracy by adding features.
Same Sentence Diff Sentence
POS-1 Ev1 2.5% Tense Pair 1.6%
POS Bigram Ev1 3.5% Aspect Ev1 0.5%
Preposition Ev1 2.0% POS Bigram 0.2%
Tense Ev2 0.7% POS-1 Ev2 0.3%
Preposition Ev2 0.6% Word EV2 0.2%
Figure 4: Top 5 features as added in feature selection
w/ Naive Bayes, with their percentage improvement.
involved. The highest fully-automatic accuracy on
Timebank is 59.43%, a 4.3% gain from our new fea-
tures. We also report 67.57% gold and 65.48% auto-
split on the OTC dataset to compare against Mani?s
reported hand-tagged features of 62.5%, a gain of
3% with our automatic features.
7 Discussion
Previous work on OTC achieved classification accu-
racy of 62.5%, but this result was based on ?perfect
data? from human annotators. A low number from
good data is at first disappointing, however, we show
that performance can be improved through more lin-
guistic features and by isolating the distinct tasks of
ordering inter-sentential and intra-sentential events.
Our new features show a clear improvement over
previous work. The features that capture dependen-
cies between the events, rather than isolated features
provide the greatest utility. Also, the impact of im-
perfect temporal data is surprisingly minimal. Us-
ing Stage One?s results instead of gold values hurts
performance by less than 1.4%. This suggests that
much of the value of the hand-coded information
can be achieved via automatic approaches. Stage
One?s event class shows room for improvement, yet
the negative impact on Event-Event relationships is
manageable. It is conceivable that more advanced
features would better classify the event class, but im-
provement on the event-event task would be slight.
Finally, it is important to note the difference in
classifying events in the same sentence vs. cross-
boundary. Splitting the 3345 pairs of corpus events
into two separate training sets makes our data more
sparse, but we still see a performance improvement
when using Mani/Lapata features. Figure 4 gives a
hint to the difference in distributions as the best fea-
tures of each task are very different. Intra-sentence
events rely on syntax cues (e.g. preposition phrases
and POS), while inter-sentence events use tense and
aspect. However, the differences are minimized as
more advanced features are added. The final row in
figure 3 shows minimal split improvement.
8 Conclusion
We have described a two-stage machine learning
approach to event-event temporal relation classifi-
cation. We have shown that imperfect event at-
tributes can be used effectively, that a range of event-
event dependency features provide added utility to a
classifier, and that events within the same sentence
have distinct characteristics from those across sen-
tence boundaries. This fully automatic raw text ap-
proach achieves a 3% improvement over previous
work based on perfect human tagged features.
Acknowledgement: This work was supported in
part by the DARPA GALE Program and the DTO
AQUAINT Program.
References
James Allen. 1984. Towards a general theory of action and
time. Artificial Intelligence, 23:123?154.
Branimir Boguraev and Rie Kubota Ando. 2005. Timeml-
compliant text analysis for temporal reasoning. In IJCA-05.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a li-
brary for support vector machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Mirella Lapata and Alex Lascarides. 2006. Learning sentence-
internal temporal relations. In Journal of AI Research, vol-
ume 27, pages 85?117.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min Lee,
and James Pustejovsky. 2006. Machine learning of temporal
relations. In ACL-06, July.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See,
David Day, Lisa Ferro, Robert Gaizauskas, Marcia Lazo,
Andrea Setzer, and Beth Sundheim. 2003. The timebank
corpus. Corpus Linguistics, pages 647?656.
176
Proceedings of ACL-08: HLT, pages 789?797,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unsupervised Learning of Narrative Event Chains
Nathanael Chambers and Dan Jurafsky
Department of Computer Science
Stanford University
Stanford, CA 94305
{natec,jurafsky}@stanford.edu
Abstract
Hand-coded scripts were used in the 1970-80s
as knowledge backbones that enabled infer-
ence and other NLP tasks requiring deep se-
mantic knowledge. We propose unsupervised
induction of similar schemata called narrative
event chains from raw newswire text.
A narrative event chain is a partially ordered
set of events related by a common protago-
nist. We describe a three step process to learn-
ing narrative event chains. The first uses unsu-
pervised distributional methods to learn narra-
tive relations between events sharing corefer-
ring arguments. The second applies a tempo-
ral classifier to partially order the connected
events. Finally, the third prunes and clusters
self-contained chains from the space of events.
We introduce two evaluations: the narrative
cloze to evaluate event relatedness, and an or-
der coherence task to evaluate narrative order.
We show a 36% improvement over baseline
for narrative prediction and 25% for temporal
coherence.
1 Introduction
This paper induces a new representation of struc-
tured knowledge called narrative event chains (or
narrative chains). Narrative chains are partially or-
dered sets of events centered around a common pro-
tagonist. They are related to structured sequences of
participants and events that have been called scripts
(Schank and Abelson, 1977) or Fillmorean frames.
These participants and events can be filled in and
instantiated in a particular text situation to draw in-
ferences. Chains focus on a single actor to facili-
tate learning, and thus this paper addresses the three
tasks of chain induction: narrative event induction,
temporal ordering of events and structured selection
(pruning the event space into discrete sets).
Learning these prototypical schematic sequences
of events is important for rich understanding of text.
Scripts were central to natural language understand-
ing research in the 1970s and 1980s for proposed
tasks such as summarization, coreference resolu-
tion and question answering. For example, Schank
and Abelson (1977) proposed that understanding
text about restaurants required knowledge about the
Restaurant Script, including the participants (Cus-
tomer, Waiter, Cook, Tables, etc.), the events consti-
tuting the script (entering, sitting down, asking for
menus, etc.), and the various preconditions, order-
ing, and results of each of the constituent actions.
Consider these two distinct narrative chains.
accused X W joined
X claimed W served
X argued W oversaw
dismissed X W resigned
It would be useful for question answering or tex-
tual entailment to know that ?X denied ? is also a
likely event in the left chain, while ? replaces W?
temporally follows the right. Narrative chains (such
as Firing of Employee or Executive Resigns) offer
the structure and power to directly infer these new
subevents by providing critical background knowl-
edge. In part due to its complexity, automatic in-
duction has not been addressed since the early non-
statistical work of Mooney and DeJong (1985).
The first step to narrative induction uses an entity-
based model for learning narrative relations by fol-
789
lowing a protagonist. As a narrative progresses
through a series of events, each event is character-
ized by the grammatical role played by the protag-
onist, and by the protagonist?s shared connection to
surrounding events. Our algorithm is an unsuper-
vised distributional learning approach that uses core-
ferring arguments as evidence of a narrative relation.
We show, using a new evaluation task called narra-
tive cloze, that our protagonist-based method leads
to better induction than a verb-only approach.
The next step is to order events in the same nar-
rative chain. We apply work in the area of temporal
classification to create partial orders of our learned
events. We show, using a coherence-based evalua-
tion of temporal ordering, that our partial orders lead
to better coherence judgements of real narrative in-
stances extracted from documents.
Finally, the space of narrative events and temporal
orders is clustered and pruned to create discrete sets
of narrative chains.
2 Previous Work
While previous work hasn?t focused specifically on
learning narratives1, our work draws from two lines
of research in summarization and anaphora resolu-
tion. In summarization, topic signatures are a set
of terms indicative of a topic (Lin and Hovy, 2000).
They are extracted from hand-sorted (by topic) sets
of documents using log-likelihood ratios. These
terms can capture some narrative relations, but the
model requires topic-sorted training data.
Bean and Riloff (2004) proposed the use of
caseframe networks as a kind of contextual role
knoweldge for anaphora resolution. A case-
frame is a verb/event and a semantic role (e.g.
<patient> kidnapped). Caseframe networks are re-
lations between caseframes that may represent syn-
onymy (<patient> kidnapped and <patient> ab-
ducted) or related events (<patient> kidnapped and
<patient> released). Bean and Riloff learn these
networks from two topic-specific texts and apply
them to the problem of anaphora resolution. Our
work can be seen as an attempt to generalize the in-
tuition of caseframes (finding an entire set of events
1We analyzed FrameNet (Baker et al, 1998) for insight, but
found that very few of the frames are event sequences of the
type characterizing narratives and scripts.
rather than just pairs of related frames) and apply it
to a different task (finding a coherent structured nar-
rative in non-topic-specific text).
More recently, Brody (2007) proposed an ap-
proach similar to caseframes that discovers high-
level relatedness between verbs by grouping verbs
that share the same lexical items in subject/object
positions. He calls these shared arguments anchors.
Brody learns pairwise relations between clusters of
related verbs, similar to the results with caseframes.
A human evaluation of these pairs shows an im-
provement over baseline. This and previous case-
frame work lend credence to learning relations from
verbs with common arguments.
We also draw from lexical chains (Morris and
Hirst, 1991), indicators of text coherence from word
overlap/similarity. We use a related notion of protag-
onist overlap to motivate narrative chain learning.
Work on semantic similarity learning such as
Chklovski and Pantel (2004) also automatically
learns relations between verbs. We use similar dis-
tributional scoring metrics, but differ with our use
of a protagonist as the indicator of relatedness. We
also use typed dependencies and the entire space of
events for similarity judgements, rather than only
pairwise lexical decisions.
Finally, Fujiki et al (2003) investigated script ac-
quisition by extracting the 41 most frequent pairs of
events from the first paragraph of newswire articles,
using the assumption that the paragraph?s textual or-
der follows temporal order. Our model, by contrast,
learns entire event chains, uses more sophisticated
probabilistic measures, and uses temporal ordering
models instead of relying on document order.
3 The Narrative Chain Model
3.1 Definition
Our model is inspired by Centering (Grosz et al,
1995) and other entity-based models of coherence
(Barzilay and Lapata, 2005) in which an entity is in
focus through a sequence of sentences. We propose
to use this same intuition to induce narrative chains.
We assume that although a narrative has several
participants, there is a central actor who character-
izes a narrative chain: the protagonist. Narrative
chains are thus structured by the protagonist?s gram-
matical roles in the events. In addition, narrative
790
events are ordered by some theory of time. This pa-
per describes a partial ordering with the before (no
overlap) relation.
Our task, therefore, is to learn events that consti-
tute narrative chains. Formally, a narrative chain
is a partially ordered set of narrative events that
share a common actor. A narrative event is a tu-
ple of an event (most simply a verb) and its par-
ticipants, represented as typed dependencies. Since
we are focusing on a single actor in this study, a
narrative event is thus a tuple of the event and the
typed dependency of the protagonist: (event, depen-
dency). A narrative chain is a set of narrative events
{e1, e2, ..., en}, where n is the size of the chain, and
a relation B(ei, ej) that is true if narrative event ei
occurs strictly before ej in time.
3.2 The Protagonist
The notion of a protagonist motivates our approach
to narrative learning. We make the following as-
sumption of narrative coherence: verbs sharing
coreferring arguments are semantically connected
by virtue of narrative discourse structure. A single
document may contain more than one narrative (or
topic), but the narrative assumption states that a se-
ries of argument-sharing verbs is more likely to par-
ticipate in a narrative chain than those not sharing.
In addition, the narrative approach captures gram-
matical constraints on narrative coherence. Simple
distributional learning might discover that the verb
push is related to the verb fall, but narrative learning
can capture additional facts about the participants,
specifically, that the object or patient of the push is
the subject or agent of the fall.
Each focused protagonist chain offers one per-
spective on a narrative, similar to the multiple per-
spectives on a commercial transaction event offered
by buy and sell.
3.3 Partial Ordering
A narrative chain, by definition, includes a partial
ordering of events. Early work on scripts included
ordering constraints with more complex precondi-
tions and side effects on the sequence of events. This
paper presents work toward a partial ordering and
leaves logical constraints as future work. We focus
on the before relation, but the model does not pre-
clude advanced theories of temporal order.
4 Learning Narrative Relations
Our first model learns basic information about a
narrative chain: the protagonist and the constituent
subevents, although not their ordering. For this we
need a metric for the relation between an event and
a narrative chain.
Pairwise relations between events are first ex-
tracted unsupervised. A distributional score based
on how often two events share grammatical argu-
ments (using pointwise mutual information) is used
to create this pairwise relation. Finally, a global nar-
rative score is built such that all events in the chain
provide feedback on the event in question (whether
for inclusion or for decisions of inference).
Given a list of observed verb/dependency counts,
we approximate the pointwise mutual information
(PMI) by:
pmi(e(w, d), e(v, g)) = log
P (e(w, d), e(v, g))
P (e(w, d))P (e(v, g))
(1)
where e(w, d) is the verb/dependency pair w and d
(e.g. e(push,subject)). The numerator is defined by:
P (e(w, d), e(v, g)) =
C(e(w, d), e(v, g))
?
x,y
?
d,f C(e(x, d), e(y, f))
(2)
where C(e(x, d), e(y, f)) is the number of times the
two events e(x, d) and e(y, f) had a coreferring en-
tity filling the values of the dependencies d and f .
We also adopt the ?discount score? to penalize low
occuring words (Pantel and Ravichandran, 2004).
Given the debate over appropriate metrics for dis-
tributional learning, we also experimented with the
t-test. Our experiments found that PMI outperforms
the t-test on this task by itself and when interpolated
together using various mixture weights.
Once pairwise relation scores are calculated, a
global narrative score can then be built such that all
events provide feedback on the event in question.
For instance, given all narrative events in a docu-
ment, we can find the next most likely event to occur
by maximizing:
max
j:0<j<m
n?
i=0
pmi(ei, fj) (3)
where n is the number of events in our chain and
ei is the ith event. m is the number of events f in
our training corpus. A ranked list of guesses can be
built from this summation and we hypothesize that
791
Known events:
(pleaded subj), (admits subj), (convicted obj)
Likely Events:
sentenced obj 0.89 indicted obj 0.74
paroled obj 0.76 fined obj 0.73
fired obj 0.75 denied subj 0.73
Figure 1: Three narrative events and the six most likely
events to include in the same chain.
the more events in our chain, the more informed our
ranked output. An example of a chain with 3 events
and the top 6 ranked guesses is given in figure 1.
4.1 Evaluation Metric: Narrative Cloze
The cloze task (Taylor, 1953) is used to evaluate a
system (or human) for language proficiency by re-
moving a random word from a sentence and having
the system attempt to fill in the blank (e.g. I forgot
to the waitress for the good service). Depend-
ing on the type of word removed, the test can evalu-
ate syntactic knowledge as well as semantic. Deyes
(1984) proposed an extended task, discourse cloze,
to evaluate discourse knowledge (removing phrases
that are recoverable from knowledge of discourse re-
lations like contrast and consequence).
We present a new cloze task that requires narra-
tive knowledge to solve, the narrative cloze. The
narrative cloze is a sequence of narrative events in a
document from which one event has been removed.
The task is to predict the missing verb and typed de-
pendency. Take this example text about American
football with McCann as the protagonist:
1. McCann threw two interceptions early.
2. Toledo pulled McCann aside and told him he?d start.
3. McCann quickly completed his first two passes.
These clauses are represented in the narrative model
as five events: (threw subject), (pulled object),
(told object), (start subject), (completed subject).
These verb/dependency events make up a narrative
cloze model. We could remove (threw subject) and
use the remaining four events to rank this missing
event. Removing a single such pair to be filled in au-
tomatically allows us to evaluate a system?s knowl-
edge of narrative relations and coherence. We do not
claim this cloze task to be solvable even by humans,
New York Times Editorial
occupied subj brought subj rejecting subj
projects subj met subj appeared subj
offered subj voted pp for offer subj
thinks subj
Figure 2: One of the 69 test documents, containing 10
narrative events. The protagonist is President Bush.
but rather assert it as a comparative measure to eval-
uate narrative knowledge.
4.2 Narrative Cloze Experiment
We use years 1994-2004 (1,007,227 documents) of
the Gigaword Corpus (Graff, 2002) for training2.
We parse the text into typed dependency graphs
with the Stanford Parser (de Marneffe et al, 2006)3,
recording all verbs with subject, object, or preposi-
tional typed dependencies. We use the OpenNLP4
coreference engine to resolve the entity mentions.
For each document, the verb pairs that share core-
ferring entities are recorded with their dependency
types. Particles are included with the verb.
We used 10 news stories from the 1994 section
of the corpus for development. The stories were
hand chosen to represent a range of topics such as
business, sports, politics, and obituaries. We used
69 news stories from the 2001 (year selected ran-
domly) section of the corpus for testing (also re-
moved from training). The test set documents were
randomly chosen and not preselected for a range of
topics. From each document, the entity involved
in the most events was selected as the protagonist.
For this evaluation, we only look at verbs. All
verb clauses involving the protagonist are manu-
ally extracted and translated into the narrative events
(verb,dependency). Exceptions that are not included
are verbs in headlines, quotations (typically not part
of a narrative), ?be? properties (e.g. john is happy),
modifying verbs (e.g. hurried to leave, only leave is
used), and multiple instances of one event.
The original test set included 100 documents, but
2The document count does not include duplicate news sto-
ries. We found up to 18% of the corpus are duplications, mostly
AP reprints. We automatically found these by matching the first
two paragraphs of each document, removing exact matches.
3http://nlp.stanford.edu/software/lex-parser.shtml
4http://opennlp.sourceforge.net
792
those without a narrative chain at least five events in
length were removed, leaving 69 documents. Most
of the removed documents were not stories, but gen-
res such as interviews and cooking recipes. An ex-
ample of an extracted chain is shown in figure 2.
We evalute with Narrative Cloze using leave-one-
out cross validation, removing one event and using
the rest to generate a ranked list of guesses. The test
dataset produces 740 cloze tests (69 narratives with
740 events). After generating our ranked guesses,
the position of the correct event is averaged over all
740 tests for the final score. We penalize unseen
events by setting their ranked position to the length
of the guess list (ranging from 2k to 15k).
Figure 1 is an example of a ranked guess list for a
short chain of three events. If the original document
contained (fired obj), this cloze test would score 3.
4.2.1 Baseline
We want to measure the utility of the protago-
nist and the narrative coherence assumption, so our
baseline learns relatedness strictly based upon verb
co-occurence. The PMI is then defined as between
all occurrences of two verbs in the same document.
This baseline evaluation is verb only, as dependen-
cies require a protagonist to fill them.
After initial evaluations, the baseline was per-
forming very poorly due to the huge amount of data
involved in counting all possible verb pairs (using a
protagonist vastly reduces the number). We exper-
imented with various count cutoffs to remove rare
occurring pairs of verbs. The final results use a base-
line where all pairs occurring less than 10 times in
the training data are removed.
Since the verb-only baseline does not use typed
dependencies, our narrative model cannot directly
compare to this abstracted approach. We thus mod-
ified the narrative model to ignore typed dependen-
cies, but still count events with shared arguments.
Thus, we calculate the PMI across verbs that share
arguments. This approach is called Protagonist.
The full narrative model that includes the grammat-
ical dependencies is called Typed Deps.
4.2.2 Results
Experiments with varying sizes of training data
are presented in figure 3. Each ranked list of
candidate verbs for the missing event in Base-
1995 1996 1997 1998 1999 2000 2001 2002 2003 20040
500
1000
1500
2000
2500
3000
Training Data from 1994?X
Ran
ked
 Po
sitio
n
Narrative Cloze Test
 
  Baseline Protagonist Typed Deps
Figure 3: Results with varying sizes of training data. Year
2003 is not explicitly shown because it has an unusually
small number of documents compared to other years.
line/Protagonist contained approximately 9 thou-
sand candidates. Of the 740 cloze tests, 714 of the
removed events were present in their respective list
of guesses. This is encouraging as only 3.5% of the
events are unseen (or do not meet cutoff thresholds).
When all training data is used (1994-2004), the
average ranked position is 1826 for Baseline and
1160 for Protagonist (1 being most confident). The
Baseline performs better at first (years 1994-5), but
as more data is seen, the Baseline worsens while
the Protagonist improves. This verb-only narrative
model shows a 36.5% improvement over the base-
line trained on all years. Results from the full Typed
Deps model, not comparable to the baseline, paral-
lel the Protagonist results, improving as more data is
seen (average ranked position of 1908 with all the
training data). We also ran the experiment with-
out OpenNLP coreference, and instead used exact
and substring matching for coreference resolution.
This showed a 5.7% decrease in the verb-only re-
sults. These results show that a protagonist greatly
assists in narrative judgements.
5 Ordering Narrative Events
The model proposed in the previous section is de-
signed to learn the major subevents in a narrative
chain, but not how these events are ordered. In this
section we extend the model to learn a partial tem-
poral ordering of the events.
793
There are a number of algorithms for determining
the temporal relationship between two events (Mani
et al, 2006; Lapata and Lascarides, 2006; Cham-
bers et al, 2007), many of them trained on the Time-
Bank Corpus (Pustejovsky et al, 2003) which codes
events and their temporal relationships. The cur-
rently highest performing of these on raw data is the
model of temporal labeling described in our previ-
ous work (Chambers et al, 2007). Other approaches
have depended on hand tagged features.
Chambers et al (2007) shows 59.4% accuracy on
the classification task for six possible relations be-
tween pairs of events: before, immediately-before,
included-by, simultaneous, begins and ends. We fo-
cus on the before relation because the others are
less relevant to our immediate task. We combine
immediately-before with before, and merge the other
four relations into an other category. At the binary
task of determining if one event is before or other,
we achieve 72.1% accuracy on Timebank.
The above approach is a two-stage machine learn-
ing architecture. In the first stage, the model uses
supervised machine learning to label temporal at-
tributes of events, including tense, grammatical as-
pect, and aspectual class. This first stage classi-
fier relies on features such as neighboring part of
speech tags, neighboring auxiliaries and modals, and
WordNet synsets. We use SVMs (Chambers et al
(2007) uses Naive Bayes) and see minor perfor-
mance boosts on Timebank. These imperfect clas-
sifications, combined with other linguistic features,
are then used in a second stage to classify the tem-
poral relationship between two events. Other fea-
tures include event-event syntactic properties such
as the syntactic dominance relations between the
two events, as well as new bigram features of tense,
aspect and class (e.g. ?present past? if the first event
is in the present, and the second past), and whether
the events occur in the same or different sentences.
5.1 Training a Temporal Classifier
We use the entire Timebank Corpus as super-
vised training data, condensing the before and
immediately-before relations into one before rela-
tion. The remaining relations are merged into other.
The vast majority of potential event pairs in Time-
bank are unlabeled. These are often none relations
(events that have no explicit relation) or as is of-
ten the case, overlap relations where the two events
have no Timebank-defined ordering but overlap in
time. Even worse, many events do have an order-
ing, but they were not tagged by the human annota-
tors. This could be due to the overwhelming task of
temporal annotation, or simply because some event
orderings are deemed more important than others in
understanding the document. We consider all un-
tagged relations as other, and experiment with in-
cluding none, half, and all of them in training.
Taking a cue from Mani et al (2006), we also
increased Timebank?s size by applying transitivity
rules to the hand labeled data. The following is an
example of the applied transitive rule:
if run BEFORE fall and fall BEFORE injured
then run BEFORE injured
This increases the number of relations from 37519
to 45619. Perhaps more importantly for our task,
of all the added relations, the before relation is
added the most. We experimented with original vs.
expanded Timebank and found the expanded per-
formed slightly worse. The decline may be due to
poor transitivity additions, as several Timebank doc-
uments contain inconsistent labelings. All reported
results are from training without transitivity.
5.2 Temporal Classifier in Narrative Chains
We classify the Gigaword Corpus in two stages,
once for the temporal features on each event (tense,
grammatical aspect, aspectual class), and once be-
tween all pairs of events that share arguments. This
allows us to classify the before/other relations be-
tween all potential narrative events.
The first stage is trained on Timebank, and the
second is trained using the approach described
above, varying the size of the none training rela-
tions. Each pair of events in a gigaword document
that share a coreferring argument is treated as a sepa-
rate ordering classification task. We count the result-
ing number of labeled before relations between each
verb/dependency pair. Processing the entire corpus
produces a database of event pair counts where con-
fidence of two generic events A and B can be mea-
sured by comparing how many before labels have
been seen versus their inverted order B and A5.
5Note that we train with the before relation, and so transpos-
ing two events is similar to classifying the after relation.
794
5.3 Temporal Evaluation
We want to evaluate temporal order at the narrative
level, across all events within a chain. We envision
narrative chains being used for tasks of coherence,
among other things, and so it is desired to evaluate
temporal decisions within a coherence framework.
Along these lines, our test set uses actual narrative
chains from documents, hand labeled for a partial
ordering. We evaluate coherence of these true chains
against a random ordering. The task is thus deciding
which of the two chains is most coherent, the orig-
inal or the random (baseline 50%)? We generated
up to 300 random orderings for each test document,
averaging the accuracy across all.
Our evaluation data is the same 69 documents
used in the test set for learning narrative relations.
The chain from each document is hand identified
and labeled for a partial ordering using only the be-
fore relation. Ordering was done by the authors and
all attempts were made to include every before re-
lation that exists in the document, or that could be
deduced through transitivity rules. Figure 4 shows
an example and its full reversal, although the evalu-
ation uses random orderings. Each edge is a distinct
before relation and is used in the judgement score.
The coherence score for a partially ordered nar-
rative chain is the sum of all the relations that our
classified corpus agrees with, weighted by how cer-
tain we are. If the gigaword classifications disagree,
a weighted negative score is given. Confidence is
based on a logarithm scale of the difference between
the counts of before and after classifications. For-
mally, the score is calculated as the following:
?
E:x,y
?
???
???
log(D(x, y)) if x?y and B(x, y) > B(y, x)
?log(D(x, y)) if x?y and B(y, x) > B(x, y)
?log(D(x, y)) if !x?y & !y?x & D(x, y) > 0
0 otherwise
where E is the set of all event pairs, B(i, j) is how
many times we classified events i and j as before in
Gigaword, and D(i, j) = |B(i, j) ? B(j, i)|. The
relation i?j indicates that i is temporally before j.
5.4 Results
Out approach gives higher scores to orders that co-
incide with the pairwise orderings classified in our
gigaword training data. The results are shown in fig-
ure 5. Of the 69 chains, 6 did not have any ordered
events and were removed from the evaluation. We
Figure 4: A narrative chain and its reverse order.
All ? 6 ? 10
correct 8086 75% 7603 78% 6307 89%
incorrect 1738 1493 619
tie 931 627 160
Figure 5: Results for choosing the correct ordered chain.
(? 10) means there were at least 10 pairs of ordered
events in the chain.
generated (up to) 300 random orderings for each of
the remaining 63. We report 75.2% accuracy, but 22
of the 63 had 5 or fewer pairs of ordered events. Fig-
ure 5 therefore shows results from chains with more
than 5 pairs, and also 10 or more. As we would
hope, the accuracy improves the larger the ordered
narrative chain. We achieve 89.0% accuracy on the
24 documents whose chains most progress through
time, rather than chains that are difficult to order
with just the before relation.
Training without none relations resulted in high
recall for before decisions. Perhaps due to data spar-
sity, this produces our best results as reported above.
6 Discrete Narrative Event Chains
Up till this point, we have learned narrative relations
across all possible events, including their temporal
order. However, the discrete lists of events for which
Schank scripts are most famous have not yet been
constructed.
We intentionally did not set out to reproduce ex-
plicit self-contained scripts in the sense that the
?restaurant script? is complete and cannot include
other events. The name narrative was chosen to im-
ply a likely order of events that is common in spoken
and written retelling of world events. Discrete sets
have the drawback of shutting out unseen and un-
795
Figure 6: An automatically learned Prosecution Chain.
Arrows indicate the before relation.
likely events from consideration. It is advantageous
to consider a space of possible narrative events and
the ordering within, not a closed list.
However, it is worthwhile to construct discrete
narrative chains, if only to see whether the combina-
tion of event learning and ordering produce script-
like structures. This is easily achievable by using
the PMI scores from section 4 in an agglomerative
clustering algorithm, and then applying the ordering
relations from section 5 to produce a directed graph.
Figures 6 and 7 show two learned chains after
clustering and ordering. Each arrow indicates a be-
fore relation. Duplicate arrows implied by rules of
transitivity are removed. Figure 6 is remarkably ac-
curate, and figure 7 addresses one of the chains from
our introduction, the employment narrative. The
core employment events are accurate, but cluster-
ing included life events (born, died, graduated) from
obituaries of which some temporal information is in-
correct. The Timebank corpus does not include obit-
uaries, thus we suffer from sparsity in training data.
7 Discussion
We have shown that it is possible to learn narrative
event chains unsupervised from raw text. Not only
do our narrative relations show improvements over
a baseline, but narrative chains offer hope for many
other areas of NLP. Inference, coherence in summa-
rization and generation, slot filling for question an-
swering, and frame induction are all potential areas.
We learned a new measure of similarity, the nar-
Figure 7: An Employment Chain. Dotted lines indicate
incorrect before relations.
rative relation, using the protagonist as a hook to ex-
tract a list of related events from each document.
The 37% improvement over a verb-only baseline
shows that we may not need presorted topics of doc-
uments to learn inferences. In addition, we applied
state of the art temporal classification to show that
sets of events can be partially ordered. Judgements
of coherence can then be made over chains within
documents. Further work in temporal classification
may increase accuracy even further.
Finally, we showed how the event space of narra-
tive relations can be clustered to create discrete sets.
While it is unclear if these are better than an uncon-
strained distribution of events, they do offer insight
into the quality of narratives.
An important area not discussed in this paper is
the possibility of using narrative chains for semantic
role learning. A narrative chain can be viewed as
defining the semantic roles of an event, constraining
it against roles of the other events in the chain. An
argument?s class can then be defined as the set of
narrative arguments in which it appears.
We believe our model provides an important first
step toward learning the rich causal, temporal and
inferential structure of scripts and frames.
Acknowledgment: This work is funded in part
by DARPA through IBM and by the DTO Phase III
Program for AQUAINT through Broad Agency An-
nouncement (BAA) N61339-06-R-0034. Thanks to the
reviewers for helpful comments and the suggestion for a
non-full-coreference baseline.
796
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Christian
Boitet and Pete Whitelock, editors, ACL-98, pages 86?
90, San Francisco, California. Morgan Kaufmann Pub-
lishers.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 141?148.
David Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference reso-
lution. Proc. of HLT/NAACL, pages 297?304.
Samuel Brody. 2007. Clustering Clauses for High-
Level Relation Detection: An Information-theoretic
Approach. Proceedings of the 43rd Annual Meeting
of the Association of Computational Linguistics, pages
448?455.
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying temporal relations between events.
In Proceedings of ACL-07, Prague, Czech Republic.
Timothy Chklovski and Patrick Pantel. 2004. Verbocean:
Mining the web for fine-grained semantic verb rela-
tions. In Proceedings of EMNLP-04.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC-06, pages 449?454.
Tony Deyes. 1984. Towards an authentic ?discourse
cloze?. Applied Linguistics, 5(2).
Toshiaki Fujiki, Hidetsugu Nanba, and Manabu Oku-
mura. 2003. Automatic acquisition of script knowl-
edge from a text collection. In EACL, pages 91?94.
David Graff. 2002. English Gigaword. Linguistic Data
Consortium.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modelling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2).
Mirella Lapata and Alex Lascarides. 2006. Learning
sentence-internal temporal relations. In Journal of AI
Research, volume 27, pages 85?117.
C.Y. Lin and E. Hovy. 2000. The automated acquisi-
tion of topic signatures for text summarization. Pro-
ceedings of the 17th conference on Computational
linguistics-Volume 1, pages 495?501.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In Proceedings of ACL-06, July.
Raymond Mooney and Gerald DeJong. 1985. Learning
schemata for natural language processing. In Ninth In-
ternational Joint Conference on Artificial Intelligence
(IJCAI), pages 681?687.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indicator of the
structure of text. Computational Linguistics, 17:21?
43.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. Proceedings of
HLT/NAACL, 4:321?328.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew
See, David Day, Lisa Ferro, Robert Gaizauskas, Mar-
cia Lazo, Andrea Setzer, and Beth Sundheim. 2003.
The timebank corpus. Corpus Linguistics, pages 647?
656.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding. Lawrence Erlbaum.
Wilson L. Taylor. 1953. Cloze procedure: a new tool for
measuring readability. Journalism Quarterly, 30:415?
433.
797
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 602?610,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Unsupervised Learning of Narrative Schemas and their Participants
Nathanael Chambers and Dan Jurafsky
Stanford University, Stanford, CA 94305
{natec,jurafsky}@stanford.edu
Abstract
We describe an unsupervised system for learn-
ing narrative schemas, coherent sequences or sets
of events (arrested(POLICE,SUSPECT), convicted(
JUDGE, SUSPECT)) whose arguments are filled
with participant semantic roles defined over words
(JUDGE = {judge, jury, court}, POLICE = {police,
agent, authorities}). Unlike most previous work in
event structure or semantic role learning, our sys-
tem does not use supervised techniques, hand-built
knowledge, or predefined classes of events or roles.
Our unsupervised learning algorithm uses corefer-
ring arguments in chains of verbs to learn both rich
narrative event structure and argument roles. By
jointly addressing both tasks, we improve on pre-
vious results in narrative/frame learning and induce
rich frame-specific semantic roles.
1 Introduction
This paper describes a new approach to event se-
mantics that jointly learns event relations and their
participants from unlabeled corpora.
The early years of natural language processing
(NLP) took a ?top-down? approach to language
understanding, using representations like scripts
(Schank and Abelson, 1977) (structured represen-
tations of events, their causal relationships, and
their participants) and frames to drive interpreta-
tion of syntax and word use. Knowledge structures
such as these provided the interpreter rich infor-
mation about many aspects of meaning.
The problem with these rich knowledge struc-
tures is that the need for hand construction, speci-
ficity, and domain dependence prevents robust and
flexible language understanding. Instead, mod-
ern work on understanding has focused on shal-
lower representations like semantic roles, which
express at least one aspect of the semantics of
events and have proved amenable to supervised
learning from corpora like PropBank (Palmer et
al., 2005) and Framenet (Baker et al, 1998). Un-
fortunately, creating these supervised corpora is an
expensive and difficult multi-year effort, requiring
complex decisions about the exact set of roles to
be learned. Even unsupervised attempts to learn
semantic roles have required a pre-defined set of
roles (Grenager and Manning, 2006) and often a
hand-labeled seed corpus (Swier and Stevenson,
2004; He and Gildea, 2006).
In this paper, we describe our attempts to learn
script-like information about the world, including
both event structures and the roles of their partic-
ipants, but without pre-defined frames, roles, or
tagged corpora.
Consider the following Narrative Schema, to be
defined more formally later. The events on the left
follow a set of participants through a series of con-
nected events that constitute a narrative:
A search B
A arrest B
D convict B
B plead C
D acquit B
D sentence B
A = Police
B = Suspect
C = Plea
D = Jury
Events Roles
Being able to robustly learn sets of related
events (left) and frame-specific role information
about the argument types that fill them (right)
could assist a variety of NLP applications, from
question answering to machine translation.
Our previous work (Chambers and Jurafsky,
2008) relied on the intuition that in a coherent text,
any two events that are about the same participants
are likely to be part of the same story or narra-
tive. The model learned simple aspects of nar-
rative structure (?narrative chains?) by extracting
events that share a single participant, the protag-
onist. In this paper we extend this work to rep-
resent sets of situation-specific events not unlike
scripts, caseframes (Bean and Riloff, 2004), and
FrameNet frames (Baker et al, 1998). This paper
shows that verbs in distinct narrative chains can be
merged into an improved single narrative schema,
while the shared arguments across verbs can pro-
vide rich information for inducing semantic roles.
602
2 Background
This paper addresses two areas of work in event
semantics, narrative event chains and semantic
role labeling. We begin by highlighting areas in
both that can mutually inform each other through
a narrative schema model.
2.1 Narrative Event Chains
Narrative Event Chains are partially ordered sets
of events that all involve the same shared par-
ticipant, the protagonist (Chambers and Jurafsky,
2008). A chain contains a set of verbs represent-
ing events, and for each verb, the grammatical role
filled by the shared protagonist.
An event is a verb together with its constellation
of arguments. An event slot is a tuple of an event
and a particular argument slot (grammatical rela-
tion), represented as a pair ?v, d? where v is a verb
and d ? {subject, object, prep}. A chain is a tu-
ple (L,O) where L is a set of event slots and O is
a partial (temporal) ordering. We will write event
slots in shorthand as (X pleads) or (pleads X) for
?pleads, subject? and ?pleads, object?. Below is
an example chain modeling criminal prosecution.
L = (X pleads), (X admits), (convicted X), (sentenced X)
O = {(pleads, convicted), (convicted, sentenced), ...}
A graphical view is often more intuitive:
admits
pleads
sentenced
convicted
(X admits)
(X pleads)
(convicted X)
(sentenced X)
In this example, the protagonist of the chain
is the person being prosecuted and the other un-
specified event slots remain unfilled and uncon-
strained. Chains in the Chambers and Jurafsky
(2008) model are ordered; in this paper rather than
address the ordering task we focus on event and ar-
gument induction, leaving ordering as future work.
The Chambers and Jurafsky (2008) model
learns chains completely unsupervised, (albeit af-
ter parsing and resolving coreference in the text)
by counting pairs of verbs that share corefer-
ring arguments within documents and computing
the pointwise mutual information (PMI) between
these verb-argument pairs. The algorithm creates
chains by clustering event slots using their PMI
scores, and we showed this use of co-referring ar-
guments improves event relatedness.
Our previous work, however, has two major
limitations. First, the model did not express
any information about the protagonist, such as its
type or role. Role information (such as knowing
whether a filler is a location, a person, a particular
class of people, or even an inanimate object) could
crucially inform learning and inference. Second,
the model only represents one participant (the pro-
tagonist). Representing the other entities involved
in all event slots in the narrative could potentially
provide valuable information. We discuss both of
these extensions next.
2.1.1 The Case for Arguments
The Chambers and Jurafsky (2008) narrative
chains do not specify what type of argument fills
the role of protagonist. Chain learning and clus-
tering is based only on the frequency with which
two verbs share arguments, ignoring any features
of the arguments themselves.
Take this example of an actual chain from an
article in our training data. Given this chain of five
events, we want to choose other events most likely
to occur in this scenario.
hunt
use
accuse
suspect
search
fly
charge
?
One of the top scoring event slots is (fly X). Nar-
rative chains incorrectly favor (fly X) because it is
observed during training with all five event slots,
although not frequently with any one of them. An
event slot like (charge X) is much more plausible,
but is unfortunately scored lower by the model.
Representing the types of the arguments can
help solve this problem. Few types of arguments
are shared between the chain and (fly X). How-
ever, (charge X) shares many arguments with (ac-
cuse X), (search X) and (suspect X) (e.g., criminal
and suspect). Even more telling is that these argu-
ments are jointly shared (the same or coreferent)
across all three events. Chains represent coherent
scenarios, not just a set of independent pairs, so we
want to model argument overlap across all pairs.
2.1.2 The Case for Joint Chains
The second problem with narrative chains is that
they make judgments only between protagonist ar-
guments, one slot per event. All entities and slots
603
in the space of events should be jointly considered
when making event relatedness decisions.
As an illustration, consider the verb arrest.
Which verb is more related, convict or capture?
A narrative chain might only look at the objects
of these verbs and choose the one with the high-
est score, usually choosing convict. But in this
case the subjects offer additional information; the
subject of arrest (police) is different from that of
convict (judge). A more informed decision prefers
capture because both the objects (suspect) and
subjects (police) are identical. This joint reason-
ing is absent from the narrative chain model.
2.2 Semantic Role Labeling
The task of semantic role learning and labeling
is to identify classes of entities that fill predicate
slots; semantic roles seem like they?d be a good
model for the kind of argument types we?d like
to learn for narratives. Most work on semantic
role labeling, however, is supervised, using Prop-
bank (Palmer et al, 2005), FrameNet (Baker et
al., 1998) or VerbNet (Kipper et al, 2000) as
gold standard roles and training data. More re-
cent learning work has applied bootstrapping ap-
proaches (Swier and Stevenson, 2004; He and
Gildea, 2006), but these still rely on a hand la-
beled seed corpus as well as a pre-defined set of
roles. Grenegar and Manning (2006) use the EM
algorithm to learn PropBank roles from unlabeled
data, and unlike bootstrapping, they don?t need a
labeled corpus from which to start. However, they
do require a predefined set of roles (arg0, arg1,
etc.) to define the domain of their probabilistic
model.
Green and Dorr (2005) use WordNet?s graph
structure to cluster its verbs into FrameNet frames,
using glosses to name potential slots. We differ in
that we attempt to learn frame-like narrative struc-
ture from untagged newspaper text. Most sim-
ilar to us, Alishahi and Stevenson (2007) learn
verb specific semantic profiles of arguments us-
ing WordNet classes to define the roles. We learn
situation-specific classes of roles shared by multi-
ple verbs.
Thus, two open goals in role learning include
(1) unsupervised learning and (2) learning the
roles themselves rather than relying on pre-defined
role classes. As just described, Chambers and Ju-
rafsky (2008) offers an unsupervised approach to
event learning (goal 1), but lacks semantic role
knowledge (goal 2). The following sections de-
scribe a model that addresses both goals.
3 Narrative Schemas
The next sections introduce typed narrative chains
and chain merging, extensions that allow us to
jointly learn argument roles with event structure.
3.1 Typed Narrative Chains
The first step in describing a narrative schema is to
extend the definition of a narrative chain to include
argument types. We now constrain the protagonist
to be of a certain type or role. A Typed Narrative
Chain is a partially ordered set of event slots that
share an argument, but now the shared argument
is a role defined by being a member of a set of
types R. These types can be lexical units (such as
observed head words), noun clusters, or other se-
mantic representations. We use head words in the
examples below, but we also evaluate with argu-
ment clustering by mapping head words to mem-
ber clusters created with the CBC clustering algo-
rithm (Pantel and Lin, 2002).
We define a typed narrative chain as a tuple
(L,P,O) with L and O the set of event slots
and partial ordering as before. Let P be a set of
argument types (head words) representing a single
role. An example is given here:
L = {(hunt X), (X use), (suspect X), (accuse X), (search X)}
P = {person, government, company, criminal, ...}
O = {(use, hunt), (suspect, search), (suspect, accuse) ... }
3.2 Learning Argument Types
As mentioned above, narrative chains are learned
by parsing the text, resolving coreference, and ex-
tracting chains of events that share participants. In
our new model, argument types are learned simul-
taneously with narrative chains by finding salient
words that represent coreferential arguments. We
record counts of arguments that are observed with
each pair of event slots, build the referential set
for each word from its coreference chain, and then
represent each observed argument by the most fre-
quent head word in its referential set (ignoring pro-
nouns and mapping entity mentions with person
pronouns to a constant PERSON identifier).
As an example, the following contains four
worker mentions:
But for a growing proportion of U.S. workers, the troubles re-
ally set in when they apply for unemployment benefits. Many
workers find their benefits challenged.
604
L = {X arrest, X charge, X raid, X seize,
X confiscate, X detain, X deport }
P = {police, agent, authority, government}
Figure 1: A typed narrative chain. The four top
arguments are given. The orderingO is not shown.
The four bolded terms are coreferential and
(hopefully) identified by coreference. Our algo-
rithm chooses the head word of each phrase and
ignores the pronouns. It then chooses the most
frequent head word as the most salient mention.
In this example, the most salient term is workers.
If any pair of event slots share arguments from this
set, we count workers. In this example, the pair (X
find) and (X apply) shares an argument (they and
workers). The pair ((X find),(X apply)) is counted
once for narrative chain induction, and ((X find),
(X apply), workers) once for argument induction.
Figure 1 shows the top occurring words across
all event slot pairs in a criminal scenario chain.
This chain will be part of a larger narrative
schema, described in section 3.4.
3.3 Event Slot Similarity with Arguments
We now formalize event slot similarity with argu-
ments. Narrative chains as defined in (Chambers
and Jurafsky, 2008) score a new event slot ?f, g?
against a chain of size n by summing over the
scores between all pairs:
chainsim(C, ?f, g?) =
nX
i=1
sim(?ei, di? , ?f, g?) (1)
where C is a narrative chain, f is a verb with
grammatical argument g, and sim(e, e?) is the
pointwise mutual information pmi(e, e?). Grow-
ing a chain by one adds the highest scoring event.
We extend this function to include argument
types by defining similarity in the context of a spe-
cific argument a:
sim(?e, d? ,
?
e?, d?
?
, a) =
pmi(?e, d? ,
?
e?, d?
?
) + ? log freq(?e, d? ,
?
e?, d?
?
, a)
(2)
where ? is a constant weighting factor and
freq(b, b?, a) is the corpus count of a filling the
arguments of events b and b?. We then score the
entire chain for a particular argument:
score(C, a) =
n?1X
i=1
nX
j=i+1
sim(?ei, di? , ?ej , dj? , a) (3)
Using this chain score, we finally extend
chainsim to score a new event slot based on the
argument that maximizes the entire chain?s score:
chainsim?(C, ?f, g?) =
max
a
(score(C, a) +
nX
i=1
sim(?ei, di? , ?f, g? , a))
(4)
The argument is now directly influencing event
slot similarity scores. We will use this definition
in the next section to build Narrative Schemas.
3.4 Narrative Schema: Multiple Chains
Whereas a narrative chain is a set of event slots,
a Narrative Schema is a set of typed narrative
chains. A schema thus models all actors in a set
of events. If (push X) is in one chain, (Y push) is
in another. This allows us to model a document?s
entire narrative, not just one main actor.
3.4.1 The Model
A narrative schema is defined as a 2-tuple N =
(E,C) with E a set of events (here defined as
verbs) and C a set of typed chains over the
event slots. We represent an event as a verb v
and its grammatical argument positions Dv ?
{subject, object, prep}. Thus, each event slot
?v, d? for all d ? Dv belongs to a chain c ? C
in the schema. Further, each c must be unique for
each slot of a single verb. Using the criminal pros-
ecution domain as an example, a narrative schema
in this domain is built as in figure 2.
The three dotted boxes are graphical represen-
tations of the typed chains that are combined in
this schema. The first represents the event slots in
which the criminal is involved, the second the po-
lice, and the third is a court or judge. Although our
representation uses a set of chains, it is equivalent
to represent a schema as a constraint satisfaction
problem between ?e, d? event slots. The next sec-
tion describes how to learn these schemas.
3.4.2 Learning Narrative Schemas
Previous work on narrative chains focused on re-
latedness scores between pairs of verb arguments
(event slots). The clustering step which built
chains depended on these pairwise scores. Narra-
tive schemas use a generalization of the entire verb
with all of its arguments. A joint decision can be
made such that a verb is added to a schema if both
its subject and object are assigned to chains in the
schema with high confidence.
For instance, it may be the case that (Y
pull over) scores well with the ?police? chain in
605
police,
agent
criminal,
suspect
guilty,
innocent
judge,
jury
arrest
charge
convict
sentence
arrest
charge
convict
plead
sentence
police,agent
judge,jury
arrest
charge
convict
plead
sentence
criminal,suspect
Figure 2: Merging typed chains into a single unordered Narrative Schema.
figure 3. However, the object of (pull over A)
is not present in any of the other chains. Police
pull over cars, but this schema does not have a
chain involving cars. In contrast, (Y search) scores
well with the ?police? chain and (search X) scores
well in the ?defendant? chain too. Thus, we want
to favor search instead of pull over because the
schema is already modeling both arguments.
This intuition leads us to our event relatedness
function for the entire narrative schema N , not
just one chain. Instead of asking which event slot
?v, d? is a best fit, we ask if v is best by considering
all slots at once:
narsim(N, v) =
?
d?Dv
max(?, max
c?CN
chainsim?(c, ?v, d?)) (5)
whereCN is the set of chains in our narrativeN . If
?v, d? does not have strong enough similarity with
any chain, it creates a new one with base score ?.
The ? parameter balances this decision of adding
to an existing chain in N or creating a new one.
3.4.3 Building Schemas
We use equation 5 to build schemas from the set
of events as opposed to the set of event slots that
previous work on narrative chains used. In Cham-
bers and Jurafsky (2008), narrative chains add the
best ?e, d? based on the following:
max
j:0<j<m
chainsim(c, ?vj , gj?) (6)
where m is the number of seen event slots in the
corpus and ?vj , gj? is the jth such possible event
slot. Schemas are now learned by adding events
that maximize equation 5:
max
j:0<j<|v|
narsim(N, vj) (7)
where |v| is the number of observed verbs and vj
is the jth such verb. Verbs are incrementally added
to a narrative schema by strength of similarity.
arrest
charge
seize
confiscate
defendant, nichols, 
smith, simpson
police, agent, 
authorities, government
license
immigrant, reporter, 
cavalo, migrant, alien
detain
deport 
raid
Figure 3: Graphical view of an unordered schema
automatically built starting from the verb ?arrest?.
A ? value that encouraged splitting was used.
4 Sample Narrative Schemas
Figures 3 and 4 show two criminal schemas
learned completely automatically from the NYT
portion of the Gigaword Corpus (Graff, 2002).
We parse the text into dependency graphs and re-
solve coreferences. The figures result from learn-
ing over the event slot counts. In addition, figure 5
shows six of the top 20 scoring narrative schemas
learned by our system. We artificially required the
clustering procedure to stop (and sometimes con-
tinue) at six events per schema. Six was chosen
as the size to enable us to compare to FrameNet
in the next section; the mean number of verbs in
FrameNet frames is between five and six. A low
? was chosen to limit chain splitting. We built a
new schema starting from each verb that occurs in
more than 3000 and less than 50,000 documents
in the NYT section. This amounted to approxi-
mately 1800 verbs from which we show the top
20. Not surprisingly, most of the top schemas con-
cern business, politics, crime, or food.
5 Frames and Roles
Most previous work on unsupervised semantic
role labeling assumes that the set of possible
606
A produce B
A sell B
A manufacture B
A *market B
A distribute B
A -develop B
A ? {company, inc, corp, microsoft,
iraq, co, unit, maker, ...}
B ? {drug, product, system, test,
software, funds, movie, ...}
B trade C
B fell C
A *quote B
B fall C
B -slip C
B rise C
A ? {}
B ? {dollar, share, index, mark, currency,
stock, yield, price, pound, ...}
C ? {friday, most, year, percent, thursday
monday, share, week, dollar, ...}
A boil B
A slice B
A -peel B
A saute B
A cook B
A chop B
A ? {wash, heat, thinly, onion, note}
B ? {potato, onion, mushroom, clove,
orange, gnocchi }
A detain B
A confiscate B
A seize B
A raid B
A search B
A arrest B
A ? {police, agent, officer, authorities,
troops, official, investigator, ... }
B ? {suspect, government, journalist,
monday, member, citizen, client, ... }
A *uphold B
A *challenge B
A rule B
A enforce B
A *overturn B
A *strike down B
A ? {court, judge, justice, panel, osteen,
circuit, nicolau, sporkin, majority, ...}
B ? {law, ban, rule, constitutionality,
conviction, ruling, lawmaker, tax, ...}
A own B
A *borrow B
A sell B
A buy back B
A buy B
A *repurchase B
A ? {company, investor, trader, corp,
enron, inc, government, bank, itt, ...}
B ? {share, stock, stocks, bond, company,
security, team, funds, house, ... }
Figure 5: Six of the top 20 scored Narrative Schemas. Events and arguments in italics were marked
misaligned by FrameNet definitions. * indicates verbs not in FrameNet. - indicates verb senses not in
FameNet.
found
convict
acquit
defendant, nichols, 
smith, simpson
jury, juror, court, 
judge, tribunal, senate
sentence
deliberate
deadlocked
Figure 4: Graphical view of an unordered schema
automatically built from the verb ?convict?. Each
node shape is a chain in the schema.
classes is very small (i.e, PropBank roles ARG0
and ARG1) and is known in advance. By con-
trast, our approach induces sets of entities that ap-
pear in the argument positions of verbs in a nar-
rative schema. Our model thus does not assume
the set of roles is known in advance, and it learns
the roles at the same time as clustering verbs into
frame-like schemas. The resulting sets of entities
(such as {police, agent, authorities, government}
or {court, judge, justice}) can be viewed as a kind
of schema-specific semantic role.
How can this unsupervised method of learning
roles be evaluated? In Section 6 we evaluate the
schemas together with their arguments in a cloze
task. In this section we perform a more qualitative
evalation by comparing our schema to FrameNet.
FrameNet (Baker et al, 1998) is a database of
frames, structures that characterize particular sit-
uations. A frame consists of a set of events (the
verbs and nouns that describe them) and a set
of frame-specific semantic roles called frame el-
ements that can be arguments of the lexical units
in the frame. FrameNet frames share commonali-
ties with narrative schemas; both represent aspects
of situations in the world, and both link semanti-
cally related words into frame-like sets in which
each predicate draws its argument roles from a
frame-specific set. They differ in that schemas fo-
cus on events in a narrative, while frames focus on
events that share core participants. Nonetheless,
the fact that FrameNet defines frame-specific ar-
gument roles suggests that comparing our schemas
and roles to FrameNet would be elucidating.
We took the 20 learned narrative schemas de-
scribed in the previous section and used FrameNet
to perform qualitative evaluations on three aspects
of schema: verb groupings, linking structure (the
mapping of each argument role to syntactic sub-
ject or object), and the roles themselves (the set of
entities that constitutes the schema roles).
Verb groupings To compare a schema?s event
selection to a frame?s lexical units, we first map
the top 20 schemas to the FrameNet frames that
have the largest overlap with each schema?s six
verbs. We were able to map 13 of our 20 narra-
tives to FrameNet (for the remaining 7, no frame
contained more than one of the six verbs). The
remaining 13 schemas contained 6 verbs each for
a total of 78 verbs. 26 of these verbs, however,
did not occur in FrameNet, either at all, or with
the correct sense. Of the remaining 52 verb map-
pings, 35 (67%) occurred in the closest FrameNet
frame or in a frame one link away. 17 verbs (33%)
607
occurred in a different frame than the one chosen.
We examined the 33% of verbs that occurred in
a different frame. Most occurred in related frames,
but did not have FrameNet links between them.
For instance, one schema includes the causal verb
trade with unaccusative verbs of change like rise
and fall. FrameNet separates these classes of verbs
into distinct frames, distinguishing motion frames
from caused-motion frames.
Even though trade and rise are in different
FrameNet frames, they do in fact have the narra-
tive relation that our system discovered. Of the 17
misaligned events, we judged all but one to be cor-
rect in a narrative sense. Thus although not exactly
aligned with FrameNet?s notion of event clusters,
our induction algorithm seems to do very well.
Linking structure Next, we compare a
schema?s linking structure, the grammatical
relation chosen for each verb event. We thus
decide, e.g., if the object of the verb arrest (arrest
B) plays the same role as the object of detain
(detain B), or if the subject of detain (B detain)
would have been more appropriate.
We evaluated the clustering decisions of the 13
schemas (78 verbs) that mapped to frames. For
each chain in a schema, we identified the frame
element that could correctly fill the most verb ar-
guments in the chain. The remaining arguments
were considered incorrect. Because we assumed
all verbs to be transitive, there were 156 arguments
(subjects and objects) in the 13 schema. Of these
156 arguments, 151 were correctly clustered to-
gether, achieving 96.8% accuracy.
The schema in figure 5 with events detain, seize,
arrest, etc. shows some of these errors. The object
of all of these verbs is an animate theme, but con-
fiscate B and raid B are incorrect; people cannot
be confiscated/raided. They should have been split
into their own chain within the schema.
Argument Roles Finally, we evaluate the
learned sets of entities that fill the argument slots.
As with the above linking evaluation, we first iden-
tify the best frame element for each argument. For
example, the events in the top left schema of fig-
ure 5 map to the Manufacturing frame. Argument
B was identified as the Product frame element. We
then evaluate the top 10 arguments in the argument
set, judging whether each is a reasonable filler of
the role. In our example, drug and product are cor-
rect Product arguments. An incorrect argument is
test, as it was judged that a test is not a product.
We evaluated all 20 schemas. The 13 mapped
schemas used their assigned frames, and we cre-
ated frame element definitions for the remaining 7
that were consistent with the syntactic positions.
There were 400 possible arguments (20 schemas,
2 chains each), and 289 were judged correct for a
precision of 72%. This number includes Person
and Organization names as correct fillers. A more
conservative metric removing these classes results
in 259 (65%) correct.
Most of the errors appear to be from parsing
mistakes. Several resulted from confusing objects
with adjuncts. Others misattached modifiers, such
as including most as an argument. The cooking
schema appears to have attached verbal arguments
learned from instruction lists (wash, heat, boil).
Two schemas require situations as arguments, but
the dependency graphs chose as arguments the
subjects of the embedded clauses, resulting in 20
incorrect arguments in these schema.
6 Evaluation: Cloze
The previous section compared our learned knowl-
edge to current work in event and role semantics.
We now provide a more formal evaluation against
untyped narrative chains. The two main contribu-
tions of schema are (1) adding typed arguments
and (2) considering joint chains in one model. We
evaluate each using the narrative cloze test as in
(Chambers and Jurafsky, 2008).
6.1 Narrative Cloze
The cloze task (Taylor, 1953) evaluates human un-
derstanding of lexical units by removing a random
word from a sentence and asking the subject to
guess what is missing. The narrative cloze is a
variation on this idea that removes an event slot
from a known narrative chain.Performance is mea-
sured by the position of the missing event slot in a
system?s ranked guess list.
This task is particularly attractive for narrative
schemas (and chains) because it aligns with one
of the original ideas behind Schankian scripts,
namely that scripts help humans ?fill in the blanks?
when language is underspecified.
6.2 Training and Test Data
We count verb pairs and shared arguments over
the NYT portion of the Gigaword Corpus (years
1994-2004), approximately one million articles.
608
1995 1996 1997 1998 1999 2000 2001 2002 2003 20041000
1050
1100
1150
1200
1250
1300
1350
Training Data from 1994?X
Ran
ked
 Po
sitio
n
Narrative Cloze Test
 
  Chain Typed Chain Schema Typed Schema
Figure 6: Results with varying sizes of training
data.
We parse the text into typed dependency graphs
with the Stanford Parser (de Marneffe et al, 2006),
recording all verbs with subject, object, or prepo-
sitional typed dependencies. Unlike in (Chambers
and Jurafsky, 2008), we lemmatize verbs and ar-
gument head words. We use the OpenNLP1 coref-
erence engine to resolve entity mentions.
The test set is the same as in (Chambers and Ju-
rafsky, 2008). 100 random news articles were se-
lected from the 2001 NYT section of the Gigaword
Corpus. Articles that did not contain a protagonist
with five or more events were ignored, leaving a
test set of 69 articles. We used a smaller develop-
ment set of size 17 to tune parameters.
6.3 Typed Chains
The first evaluation compares untyped against
typed narrative event chains. The typed model
uses equation 4 for chain clustering. The dotted
line ?Chain? and solid ?Typed Chain? in figure 6
shows the average ranked position over the test set.
The untyped chains plateau and begin to worsen
as the amount of training data increases, but the
typed model is able to improve for some time af-
ter. We see a 6.9% gain at 2004 when both lines
trend upwards.
6.4 Narrative Schema
The second evaluation compares the performance
of the narrative schema model against single nar-
rative chains. We ignore argument types and use
untyped chains in both (using equation 1 instead
1http://opennlp.sourceforge.net/
of 4). The dotted line ?Chain? and solid ?Schema?
show performance results in figure 6. Narrative
Schemas have better ranked scores in all data sizes
and follow the previous experiment in improving
results as more data is added even though untyped
chains trend upward. We see a 3.3% gain at 2004.
6.5 Typed Narrative Schema
The final evaluation combines schemas with ar-
gument types to measure overall gain. We eval-
uated with both head words and CBC clusters
as argument representations. Not only do typed
chains and schemas outperform untyped chains,
combining the two gives a further performance
boost. Clustered arguments improve the re-
sults further, helping with sparse argument counts
(?Typed Schema? in figure 6 uses CBC argu-
ments). Overall, using all the data (by year 2004)
shows a 10.1% improvement over untyped narra-
tive chains.
7 Discussion
Our significant improvement in the cloze evalua-
tion shows that even though narrative cloze does
not evaluate argument types, jointly modeling the
arguments with events improves event cluster-
ing. Likewise, the FrameNet comparison suggests
that modeling related events helps argument learn-
ing. The tasks mutually inform each other. Our
argument learning algorithm not only performs
unsupervised induction of situation-specific role
classes, but the resulting roles and linking struc-
tures may also offer the possibility of (unsuper-
vised) FrameNet-style semantic role labeling.
Finding the best argument representation is an
important future direction. The performance of
our noun clusters in figure 6 showed that while the
other approaches leveled off, clusters continually
improved with more data. The exact balance be-
tween lexical units, clusters, or more general (tra-
ditional) semantic roles remains to be solved, and
may be application specific.
We hope in the future to show that a range of
NLU applications can benefit from the rich infer-
ential structures that narrative schemas provide.
Acknowledgments
This work is funded in part by NSF (IIS-0811974).
We thank the reviewers and the Stanford NLP
Group for helpful suggestions.
609
References
Afra Alishahi and Suzanne Stevenson. 2007. A com-
putational usage-based model for learning general
properties of semantic roles. In The 2nd European
Cognitive Science Conference, Delphi, Greece.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Christian
Boitet and Pete Whitelock, editors, ACL-98, pages
86?90, San Francisco, California. Morgan Kauf-
mann Publishers.
David Bean and Ellen Riloff. 2004. Unsupervised
learning of contextual role knowledge for corefer-
ence resolution. Proc. of HLT/NAACL, pages 297?
304.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08, Hawaii, USA.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
David Graff. 2002. English Gigaword. Linguistic
Data Consortium.
Rebecca Green and Bonnie J. Dorr. 2005. Frame se-
mantic enhancement of lexical-semantic resources.
In ACL-SIGLEX Workshop on Deep Lexical Acqui-
sition, pages 57?66.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In EMNLP.
Shan He and Daniel Gildea. 2006. Self-training and
co-training for semantic role labeling: Primary re-
port. Technical Report 891, University of Rochester.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In Proceedings of AAAI-2000, Austin, TX.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: A corpus annotated
with semantic roles. Computational Linguistics,
31(1):71?106.
Patrick Pantel and Dekang Lin. 2002. Document clus-
tering with committees. In ACM Conference on Re-
search and Development in Information Retrieval,
pages 199?206, Tampere, Finland.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding. Lawrence Erl-
baum.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In EMNLP.
Wilson L. Taylor. 1953. Cloze procedure: a new tool
for measuring readability. Journalism Quarterly,
30:415?433.
610
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 165?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Alignments and Leveraging Natural Logic
Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon
Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage
Eric Yeh, Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{natec,dcer,grenager,dlwh,loeki,wcmac,mcdm,dramage,yeh1,manning}@stanford.edu
Abstract
We describe an approach to textual infer-
ence that improves alignments at both the
typed dependency level and at a deeper se-
mantic level. We present a machine learning
approach to alignment scoring, a stochas-
tic search procedure, and a new tool that
finds deeper semantic alignments, allowing
rapid development of semantic features over
the aligned graphs. Further, we describe a
complementary semantic component based
on natural logic, which shows an added gain
of 3.13% accuracy on the RTE3 test set.
1 Introduction
Among the many approaches to textual inference,
alignment of dependency graphs has shown utility
in determining entailment without the use of deep
understanding. However, discovering alignments
requires a scoring function that accurately scores
alignment and a search procedure capable of approx-
imating the optimal mapping within a large search
space. We address the former requirement through
a machine learning approach for acquiring lexical
feature weights, and we address the latter with an
approximate stochastic approach to search.
Unfortunately, the most accurate aligner can-
not capture deeper semantic relations between two
pieces of text. For this, we have developed a tool,
Semgrex, that allows the rapid development of de-
pendency rules to find specific entailments, such as
familial or locative relations, a common occurence
in textual entailment data. Instead of writing code by
hand to capture patterns in the dependency graphs,
we develop a separate rule-base that operates over
aligned dependency graphs. Further, we describe a
separate natural logic component that complements
our textual inference system, making local entail-
ment decisions based on monotonic assumptions.
The next section gives a brief overview of the sys-
tem architecture, followed by our proposal for im-
proving alignment scoring and search. New coref-
erence features and the Semgrex tool are then de-
scribed, followed by a description of natural logic.
2 System Overview
Our system is a three stage architecture that con-
ducts linguistic analysis, builds an alignment be-
tween dependency graphs of the text and hypothesis,
and performs inference to determine entailment.
Linguistic analysis identifies semantic entities, re-
lationships, and structure within the given text and
hypothesis. Typed dependency graphs are passed
to the aligner, as well as lexical features such as
named entities, synonymity, part of speech, etc. The
alignment stage then performs dependency graph
alignment between the hypothesis and text graphs,
searching the space of possible alignments for the
highest scoring alignment. Improvements to the
scorer, search algorithm, and automatically learned
weights are described in the next section.
The final inference stage determines if the hy-
pothesis is entailed by the text. We construct a set
of features from the previous stages ranging from
antonyms and polarity to graph structure and seman-
tic relations. Each feature is weighted according to a
set of hand-crafted or machine-learned weights over
165
the development dataset. We do not describe the fea-
tures here; the reader is referred to de Marneffe et al
(2006a) for more details. A novel component that
leverages natural logic is also used to make the final
entailment decisions, described in section 6.
3 Alignment Model
We examine three tasks undertaken to improve the
alignment phase: (1) the construction of manu-
ally aligned data which enables automatic learning
of alignment models, and effectively decouples the
alignment and inference development efforts, (2) the
development of new search procedures for finding
high-quality alignments, and (3) the use of machine
learning techniques to automatically learn the pa-
rameters of alignment scoring models.
3.1 Manual Alignment Annotation
While work such as Raina et al (2005) has tried
to learn feature alignment weights by credit assign-
ment backward from whether an item is answered
correctly, this can be very difficult, and here we fol-
low Hickl et al (2006) in using supervised gold-
standard alignments, which help us to evaluate and
improve alignment and inference independently.
We built a web-based tool that allows annotators
to mark semantic relationships between text and hy-
pothesis words. A table with the hypothesis words
on one axis and the text on the other allows re-
lationships to be marked in the corresponding ta-
ble cell with one of four options. These relation-
ships include text to hypothesis entailment, hypothe-
sis to text entailment, synonymy, and antonymy. Ex-
amples of entailment (from the RTE 2005 dataset)
include pairs such as drinking/consumption, coro-
navirus/virus, and Royal Navy/British. By distin-
guishing between these different types of align-
ments, we can capture some limited semantics in the
alignment process, but full exploitation of this infor-
mation is left to future work.
We annotated the complete RTE2 dev and
RTE3 dev datasets, for a total of 1600 aligned
text/hypothesis pairs (the data is available at
http://nlp.stanford.edu/projects/rte/).
3.2 Improving Alignment Search
In order to find ?good? alignments, we define both a
formal model for scoring the quality of a proposed
alignment and a search procedure over the alignment
space. Our goal is to build a model that maximizes
the total alignment score of the full datasetD, which
we take to be the sum of the alignment scores for all
individual text/hypothesis pairs (t, h).
Each of the text and hypothesis is a semantic de-
pendency graph; n(h) is the set of nodes (words)
and e(h) is the set of edges (grammatical relations)
in a hypothesis h. An alignment a : n(h) 7? n(t) ?
{null} maps each hypothesis word to a text word
or to a null symbol, much like an IBM-style ma-
chine translation model. We assume that the align-
ment score s(t, h, a) is the sum of two terms, the first
scoring aligned word pairs and the second the match
between an edge between two words in the hypoth-
esis graph and the corresponding path between the
words in the text graph. Each of these is a sum, over
the scoring function for individual word pairs sw and
the scoring function for edge path pairs se:
s(t, h, a) =
?
hi?n(h)
sw(hi, a(hi))
+
?
(hi,hj)?e(h)
se((hi, hj), (a(hi), a(hj)))
The space of alignments for a hypothesis with m
words and a text with n words contains (n + 1)m
possible alignments, making exhaustive search in-
tractable. However, since the bulk of the alignment
score depends on local factors, we have explored
several search strategies and found that stochastic
local search produces better quality solutions.
Stochastic search is inspired by Gibbs sampling
and operates on a complete state formulation of the
search problem. We initialize the algorithm with the
complete alignment that maximizes the greedy word
pair scores. Then, in each step of the search, we
seek to randomly replace an alignment for a single
hypothesis word hi. For each possible text word tj
(including null), we compute the alignment score if
we were to align hi with tj . Treating these scores as
log probabilities, we create a normalized distribution
from which we sample one alignment. This Gibbs
sampler is guaranteed to give us samples from the
posterior distribution over alignments defined im-
plicitly by the scoring function. As we wish to find a
maximum of the function, we use simulated anneal-
ing by including a temperature parameter to smooth
166
the sampling distribution as a function of time. This
allows us to initially explore the space widely, but
later to converge to a local maximum which is hope-
fully the global maximum.
3.3 Learning Alignment Models
Last year, we manually defined the alignment scor-
ing function (de Marneffe et al, 2006a). However,
the existence of the gold standard alignments de-
scribed in section 3.1 enables the automatic learning
of a scoring function. For both the word and edge
scorers, we choose a linear model where the score is
the dot product of a feature and a weight vector:
sw(hi, tj) = ?w ? f(hi, tj), and
se((hi, hj), (tk, t`)) = ?e ? f((hi, hj), (tk, t`)).
Recent results in machine learning show the ef-
fectiveness of online learning algorithms for struc-
ture prediction tasks. Online algorithms update their
model at each iteration step over the training set. For
each datum, they use the current weight vector to
make a prediction which is compared to the correct
label. The weight vector is updated as a function
of the difference. We compared two different up-
date rules: the perceptron update and the MIRA up-
date. In the perceptron update, for an incorrect pre-
diction, the weight vector is modified by adding a
multiple of the difference between the feature vector
of the correct label and the feature vector of the pre-
dicted label. We use the adaptation of this algorithm
to structure prediction, first proposed by (Collins,
2002). TheMIRA update is a proposed improvement
that attempts to make the minimal modification to
the weight vector such that the score of the incorrect
prediction for the example is lower than the score of
the correct label (Crammer and Singer, 2001).
We compare the performance of the perceptron
and MIRA algorithms on 10-fold cross-validation
on the RTE2 dev dataset. Both algorithms improve
with each pass over the dataset. Most improve-
ment is within the first five passes. Table 1 shows
runs for both algorithms over 10 passes through the
dataset. MIRA consistently outperforms perceptron
learning. Moreover, scoring alignments based on the
learned weights marginally outperforms our hand-
constructed scoring function by 1.7% absolute.
A puzzling problem is that our overall per-
formance decreased 0.87% with the addition of
Perfectly aligned
Individual words Text/hypothesis pairs
Perceptron 4675 271
MIRA 4775 283
Table 1: Perceptron and MIRA results on 10-fold cross-
validation on RTE2 dev for 10 passes.
RTE3 dev alignment data. We believe this is due
to a larger proportion of ?irrelevant? and ?relation?
pairs. Irrelevant pairs are those where the text and
hypothesis are completely unrelated. Relation pairs
are those where the correct entailment judgment re-
lies on the extraction of relations such as X works
for Y, X is located in Y, or X is the wife of Y. Both
of these categories do not rely on alignments for en-
tailment decisions, and hence introduce noise.
4 Coreference
In RTE3, 135 pairs in RTE3 dev and 117 in
RTE3 test have lengths classified as ?long,? with
642 personal pronouns identified in RTE3 dev and
504 in RTE3 test. These numbers suggest that re-
solving pronomial anaphora plays an important role
in making good entailment decisions. For exam-
ple, identifying the first ?he? as referring to ?Yunus?
in this pair from RTE3 dev can help alignment and
other system features.
P: Yunus, who shared the 1.4 million prize Friday with the
Grameen Bank that he founded 30 years ago, pioneered the con-
cept of ?microcredit.?
H: Yunus founded the Grameen Bank 30 years ago.
Indeed, 52 of the first 200 pairs from RTE3 dev
were deemed by a human evaluator to rely on ref-
erence information. We used the OpenNLP1 pack-
age?s maximum-entropy coreference utility to per-
form resolution on parse trees and named-entity data
from our system. Found relations are stored and
used by the alignment stage for word similarity.
We evaluated our system with and without coref-
erence over RTE3 dev and RTE3 test. Results are
shown in Table 3. The presence of reference infor-
mation helped, approaching significance on the de-
velopment set (p < 0.1, McNemar?s test, 2-tailed),
but not on the test set. Examination of alignments
and features between the two runs shows that the
alignments do not differ significantly, but associated
1http://opennlp.sourceforge.net/
167
weights do, thus affecting entailment threshold tun-
ing. We believe coreference needs to be integrated
into all the featurizers and lexical resources, rather
than only with word matching, in order to make fur-
ther gains.
5 Semgrex Language
A core part of an entailment system is the ability to
find semantically equivalent patterns in text. Pre-
viously, we wrote tedious graph traversal code by
hand for each desired pattern. As a remedy, we
wrote Semgrex, a pattern language for dependency
graphs. We use Semgrex atop the typed dependen-
cies from the Stanford Parser (de Marneffe et al,
2006b), as aligned in the alignment phase, to iden-
tify both semantic patterns in a single text and over
two aligned pieces of text. The syntax of the lan-
guage was modeled after tgrep/Tregex, query lan-
guages used to find syntactic patterns in trees (Levy
and Andrew, 2006). This speeds up the process of
graph search and reduces errors that occur in com-
plicated traversal code.
5.1 Semgrex Features
Rather than providing regular expression match-
ing of atomic tree labels, as in most tree pattern
languages, Semgrex represents nodes as a (non-
recursive) attribute-value matrix. It then uses regular
expressions for subsets of attribute values. For ex-
ample, {word:run;tag:/?NN/} refers to any
node that has a value run for the attribute word and
a tag that starts with NN, while {} refers to any node
in the graph.
However, the most important part of Semgrex is
that it allows you to specify relations between nodes.
For example, {} <nsubj {} finds all the depen-
dents of nsubj relations. Logical connectives can
be used to form more complex patterns and node
naming can help retrieve matched nodes from the
patterns. Four base relations, shown in figure 1, al-
low you to specify the type of relation between two
nodes, in addition to an alignment relation (@) be-
tween two graphs.
5.2 Entailment Patterns
A particularly useful application of Semgrex is to
create relation entailment patterns. In particular, the
IE subtask of RTE has many characteristics that are
Semgrex Relations
Symbol #Description
{A} >reln {B} A is the governor of a reln relation
with B
{A} <reln {B} A is the dependent of a reln relation
with B
{A} >>reln {B} A dominates a node that is the
governor of a reln relation with B
{A} <<reln {B} A is the dependent of a node that is
dominated by B
{A} @ {B} A aligns to B
Figure 1: Semgrex relations between nodes.
not well suited to the core alignment features of our
system. We began integrating Semgrex into our sys-
tem by creating semantic alignment rules for these
IE tasks.
T: Bill Clinton?s wife Hillary was in Wichita today, continuing
her campaign.
H: Bill Clinton is married to Hillary. (TRUE)
Pattern:
({}=1
<nsubjpass ({word:married} >pp to {}=2))
@ ({} >poss ({lemma:/wife/} >appos {}=3))
This is a simplified version of a pattern that looks
for marriage relations. If it matches, additional pro-
grammatic checks ensure that the nodes labeled 2
and 3 are either aligned or coreferent. If they are,
then we add a MATCH feature, otherwise we add a
MISMATCH. Patterns included other familial rela-
tions and employer-employee relations. These pat-
terns serve both as a necessary component of an IE
entailment system and as a test drive of Semgrex.
5.3 Range of Application
Our rules for marriage relations correctly matched
six examples in the RTE3 development set and one
in the test set. Due to our system?s weaker per-
formance on the IE subtask of the data, we ana-
lyzed 200 examples in the development set for Sem-
grex applicability. We identified several relational
classes, including the following:
? Work: works for, holds the position of
? Location: lives in, is located in
? Relative: wife/husband of, are relatives
? Membership: is an employee of, is part of
? Business: is a partner of, owns
? Base: is based in, headquarters in
These relations make up at least 7% of the data, sug-
gesting utility from capturing other relations.
168
6 Natural Logic
We developed a computational model of natural
logic, the NatLog system, as another inference en-
gine for our RTE system. NatLog complements our
core broad-coverage system by trading lower recall
for higher precision, similar to (Bos and Markert,
2006). Natural logic avoids difficulties with translat-
ing natural language into first-order logic (FOL) by
forgoing logical notation and model theory in favor
of natural language. Proofs are expressed as incre-
mental edits to natural language expressions. Edits
represent conceptual contractions and expansions,
with truth preservation specified natural logic. For
further details, we refer the reader to (Sa?nchez Va-
lencia, 1995).
We define an entailment relation v between
nouns (hammer v tool), adjectives (deafening v
loud), verbs (sprint v run), modifiers, connectives
and quantifiers. In ordinary (upward-monotone)
contexts, the entailment relation between compound
expressions mirrors the entailment relations be-
tween their parts. Thus tango in Paris v dance
in France, since tango v dance and in Paris v in
France. However, many linguistic constructions cre-
ate downward-monotone contexts, including nega-
tion (didn?t sing v didn?t yodel), restrictive quanti-
fiers (few beetles v few insects) and many others.
NatLog uses a three-stage architecture, compris-
ing linguistic pre-processing, alignment, and entail-
ment classification. In pre-processing, we define a
list of expressions that affect monotonicity, and de-
fine Tregex patterns that recognize each occurrence
and its scope. This monotonicity marking can cor-
rectly account for multiple monotonicity inversions,
as in no soldier without a uniform, and marks each
token span with its final effective monotonicity.
In the second stage, word alignments from our
RTE system are represented as a sequence of atomic
edits over token spans, as entailment relations
are described across incremental edits in NatLog.
Aligned pairs generate substitution edits, unaligned
premise words yield deletion edits, and unaligned
hypothesis words yield insertion edits. Where pos-
sible, contiguous sequences of word-level edits are
collected into span edits.
In the final stage, we use a decision-tree classi-
fier to predict the elementary entailment relation (ta-
relation symbol in terms of v RTE
equivalent p = h p v h, h v p yes
forward p < h p v h, h 6v p yes
reverse p = h h v p, p 6v h no
independent p # h p 6v h, h 6v p no
exclusive p | h p v ?h, h v ?p no
Table 2: NatLog?s five elementary entailment relations. The last
column indicates correspondences to RTE answers.
ble 2) for each atomic edit. Edit features include
the type, effective monotonicity at affected tokens,
and their lexical features, including syntactic cate-
gory, lemma similarity, and WordNet-derived mea-
sures of synonymy, hyponymy, and antonymy. The
classifier was trained on a set of 69 problems de-
signed to exercise the feature space, learning heuris-
tics such as deletion in an upward-monotone context
yields<, substitution of a hypernym in a downward-
monotone context yields =, and substitution of an
antonym yields |.
To produce a top-level entailment judgment, the
atomic entailment predictions associated with each
edit are composed in a fairly obvious way. If r is any
entailment relation, then (= ? r) ? r, but (# ? r) ?
#. < and= are transitive, but (< ? =) ? #, and so
on.
We do not expect NatLog to be a general-purpose
solution for RTE problems. Many problems depend
on types of inference that it does not address, such
as paraphrase or relation extraction. Most pairs have
large edit distances, and more atomic edits means
a greater chance of errors propagating to the final
output: given the entailment composition rules, the
system can answer yes only if all atomic-level pre-
dictions are either< or =. Instead, we hope to make
reliable predictions on a subset of the RTE problems.
Table 3 shows NatLog performance on RTE3. It
makes positive predictions on few problems (18%
on development set, 24% on test), but achieves good
precision relative to our RTE system (76% and 68%,
respectively). For comparison, the FOL-based sys-
tem reported in (Bos and Markert, 2006) attained a
precision of 76% on RTE2, but made a positive pre-
diction in only 4% of cases. This high precision sug-
gests that superior performance can be achieved by
hybridizing NatLog with our core RTE system.
The reader is referred to (MacCartney and Man-
169
ID Premise(s) Hypothesis Answer
518 The French railway company SNCF is cooperating in
the project.
The French railway company is called SNCF. yes
601 NUCOR has pioneered a giant mini-mill in which steel
is poured into continuous casting machines.
Nucor has pioneered the first mini-mill. no
Table 4: Illustrative examples from the RTE3 test suite
RTE3 Development Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.25 68.66 66.99 67.25
Core -coref 49.88 66.42 64.32 64.88
NatLog 18.00 76.39 26.70 58.00
Hybrid, bal. 50.00 69.75 67.72 68.25
Hybrid, opt. 55.13 69.16 74.03 69.63
RTE3 Test Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.00 61.75 60.24 60.50
Core -coref 50.00 60.25 58.78 59.00
NatLog 23.88 68.06 31.71 57.38
Hybrid, bal. 50.00 64.50 62.93 63.25
Hybrid, opt. 54.13 63.74 67.32 63.62
Table 3: Performance on the RTE3 development and test sets.
% yes indicates the proportion of yes predictions made by the
system. Precision and recall are shown for the yes label.
ning, 2007) for more details on NatLog.
7 System Results
Our core systemmakes yes/no predictions by thresh-
olding a real-valued inference score. To construct
a hybrid system, we adjust the inference score by
+x if NatLog predicts yes, ?x otherwise. x is cho-
sen by optimizing development set accuracy when
adjusting the threshold to generate balanced predic-
tions (equal numbers of yes and no). As another
experiment, we fix x at this value and adjust the
threshold to optimize development set accuracy, re-
sulting in an excess of yes predictions. Results for
these two cases are shown in Table 3. Parameter
values tuned on development data yielded the best
performance. The optimized hybrid system attained
an absolute accuracy gain of 3.12% over our RTE
system, corresponding to an extra 25 problems an-
swered correctly. This result is statistically signifi-
cant (p < 0.01, McNemar?s test, 2-tailed).
The gain cannot be fully attributed to NatLog?s
success in handling the kind of inferences about
monotonicity which are the staple of natural logic.
Indeed, such inferences are quite rare in the RTE
data. Rather, NatLog seems to have gained primarily
by being more precise. In some cases, this precision
works against it: NatLog answers no to problem 518
(table 4) because it cannot account for the insertion
of called. On the other hand, it correctly rejects the
hypothesis in problem 601 because it cannot account
for the insertion of first, whereas the less-precise
core system was happy to allow it.
Acknowledgements
This material is based upon work supported in
part by the Disruptive Technology Office (DTO)?s
AQUAINT Phase III Program.
References
Johan Bos and Katja Markert. 2006. When logical inference
helps determining textual entailment (and when it doesn?t).
In Proceedings of the Second PASCAL RTE Challenge.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of EMNLP-2002.
Koby Crammer and Yoram Singer. 2001. Ultraconservative
online algorithms for multiclass problems. In Proceedings
of COLT-2001.
Marie-Catherine de Marneffe, Bill MacCartney, Trond Grena-
ger, Daniel Cer, Anna Rafferty, and Christopher D. Manning.
2006a. Learning to distinguish valid textual entailments. In
Second Pascal RTE Challenge Workshop.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006b. Generating typed dependency
parses from phrase structure parses. In 5th Int. Conference
on Language Resources and Evaluation (LREC 2006).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006. Recognizing textual entail-
ment with LCC?s GROUNDHOG system. In Proceedings of
the Second PASCAL RTE Challenge.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
Proceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation.
Bill MacCartney and Christopher D. Manning. 2007. Natu-
ral logic for textual inference. In ACL Workshop on Textual
Entailment and Paraphrasing.
Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005.
Robust textual inference via learning and abductive reason-
ing. In AAAI 2005, pages 1099?1105.
Victor Sa?nchez Valencia. 1995. Parsing-driven inference: Nat-
ural logic. Linguistic Analysis, 25:258?285.
170
NAACL HLT Demonstration Program, pages 1?2,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Demonstration of PLOW: A Dialogue System for One-Shot Task 
Learning
James Allen, Nathanael Chambers, George Ferguson
* 
, Lucian Galescu, Hyuckchul Jung, 
Mary Swift
* 
and William Taysom
Florida Institute for Human and Machine Cognition, Pensacola, FL 32502
*Computer Science Department, University of Rochester, Rochester, NY 14627
Introduction
We describe a system that can learn new 
procedure models effectively from one 
demonstration by the user. Previous work to learn 
tasks through observing a demonstration (e.g., 
Lent & Laird, 2001) has required observing many 
examples of the same task. One-shot learning of 
tasks presents a significant challenge because the 
observed sequence is inherently incomplete ? the 
user only performs the steps required for the 
current situation.  Furthermore, their decision-
making processes, which reflect the control 
structures in the procedure, are not revealed. 
We will demonstrate a system called PLOW 
(Procedural Learning on the Web) that learns task 
knowledge through observation accompanied by a 
natural language ?play-by-play?. Natural 
language (NL) alleviates many task learning 
problems by identifying (i) a useful level of 
abstraction of observed actions; (ii) parameter 
dependencies; (iii) hierarchical structure; (iv) 
semantic relationships between the task and the 
items involved in the actions; and (v) control 
constructs not otherwise observable. Various 
specialized reasoning modules in the system 
communicate and collaborate with each other to 
interpret the user?s intentions, build a task model 
based on the interpretation, and check consistency 
between the learned task and prior knowledge.
The play-by-play approach in NL enables our 
task learning system to build a task with high-
level constructs that are not inferable from 
observed actions alone. In addition to the 
knowledge about task structure, NL also provides 
critical information to transform the observed 
actions into more robust and reliable executable 
forms. Our system learns how to find objects used 
in the task, unifying the linguistic information of 
the objects with the semantic representations of 
the user?s NL descriptions about them.  The 
objects can then be reliably found in dynamic and 
complex environments. See Jung et al(2006) and 
Chambers et al(2006) for more details on the 
PLOW system.
The PLOW System
PLOW learns tasks executable on the web 
involving actions such as navigation, information 
extraction and form filling, and can learn iterative 
steps that operate over lists of objects on pages. 
Figure 1 shows the system during learning a task 
to find publications for a specified author. Upper 
left is the Mozilla browser, in which the user can 
demonstrate action and the system can execute 
actions in a mixed-initiative fashion. The user 
may speak or type to the system (SR output is 
lower right), and PLOW combines knowledge 
from the language and the demonstrated actions to 
produce a parameterized procedure (described in 
generated natural language in the upper right 
corner). Figure 2 shows a complete training 
dialogue in which PLOW learns how to find 
article titles. To save space, simple 
acknowledgments by the system are not shown.
Figure 1: PLOW learning a task
1
Evaluation
The PLOW system was evaluated by independent 
evaluators who considered four task learning 
systems developed in the CALO project. There 
were 16 human subjects who received training on 
each of the systems and who worked through a 
number of successful scripted training sessions 
with each. They were then given ten new 
problems, ranging from slight variations to 
problems they had seen to problems that were 
substantially new. They were free to choose which 
problems to work on and which system to use and 
the resulting tasks learned were tested with 
different settings of the parameters and scored out 
of a total of 4 points based on a complex 
predefined evaluation criteria (not known to the 
developers). The PLOW system did well in the 
test, not only receiving the highest average score 
on tasks learned by a system (figure 3) but also 
was strongly preferred by the users and selected 
more than half the time (figure 4). 
The Demonstration
If we are allowed a presentation we will 
demonstrate PLOW live on a task selected by the 
audience. In addition, we would like to have the 
system available for an extended period of time 
during the conference so that attendees can spend 
time using the system to teach it simple tasks. The 
system runs on a laptop and all that is needed for 
a demo is internet access.
Acknowledgements & References
This work was supported by DARPA grant NBCH-
D-03-0010 under a subcontract from SRI International, 
ONR grant N000140510314, and NSF grant5-28096.
Chambers, N. et al (2006). Using Semantics to Identify 
Web Objects. Proceedings AAAI.
Jung, H., J. Allen, et al (2006). One-Shot Proce-dure 
Learning from Instruction and Observation. FLAIRS, 
Melbourne, FL. 
Lent, M. and Laird, J. (2001)  Learning Procedural 
Knowledge through Observation,  Proc.  of the Intl 
Conf. on Knowledge Capture.
   Figure 3: Average score (out of 4)   Figure 4: User preference for systems (55 trials)
U: Let me show you how to find someone's publications 
U: Go to this webpage 
Navigates to http://scholar.google.com
U: Click on the advanced search link 
Clicks Advanced Scholar Search 
U: Put the author's name here 
Types the name in the Author text field 
U: Click the search button 
Clicks on the Search Scholar button 
U: OK here is the list of results 
Selects the table on page
PLOW: Can you check that I understand the data. 
PLOW: puts up a new window with the results. 
U: OK this is good.
U: this is the article title 
Highlights a title 
PLOW: attempts to find all titles and displays them
U: This is wrong.
PLOW OK
U: This is another article title 
Highlights another correct title 
PLOW: recomputes all the titles and displays them
Switches back to Scholar window
U: OK click next to get more results
Clicks the Next link 
PLOW: When should I stop searching?
U: Get thirty results 
PLOW: OK. I will do that
PLOW: iterates through pages until it gets 30 results
PLOW: I?m done.
U: We're done with the demonstration 
U: Let's practice
PLOW: what is the name of the author?
Enters a new name
PLOW executes the procedure successfully!!
Figure 2: Learning to find titles of an author?s publications (U = user)
2
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 85?88, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Two diverse systems built using
generic components for spoken dialogue
(Recent Progress on TRIPS)
James Allen, George Ferguson, Mary Swift, Amanda Stent, Scott Stoness, 
Lucian Galescu, Nathan Chambers, Ellen Campana, and Gregory Aist
University of Rochester
Computer Science Department
UR Comp Sci RC 270226
Rochester NY 14627 USA
{james, ferguson, swift, stoness,
campana, gaist}
@cs.rochester.edu
Institute for
Human and Machine Cognition
40 South Alcaniz St.
Pensacola FL 32502
{lgalescu,nchambers}@ihmc.us
State University of New York at
Stony Brook
1418 Computer Science
Stony Brook University
Stony Brook NY 11794 USA
stent@cs.sunysb.edu
Abstract
This  paper  describes  recent  progress  on  the
TRIPS architecture for developing spoken-lan-
guage dialogue systems.  The interactive poster
session will include demonstrations of two sys-
tems built using TRIPS: a computer purchas-
ing assistant, and an object placement (and ma-
nipulation) task.
1 Introduction
Building a robust spoken dialogue system for a new
task currently requires considerable effort,  includ-
ing  extensive  data  collection,  grammar  develop-
ment, and building a dialogue manager that drives
the  system using its  "back-end" application (e.g.
database query, planning and scheduling). We de-
scribe progress in an effort to build a generic dia-
logue system that  can be rapidly customized to a
wide range of different types of applications, pri-
marily  by  defining a  domain-specific  task  model
and the interfaces to the back-end systems. This is
achieved by  using generic  components  (i.e.,  ones
that apply in any practical domain) for all stages of
understanding  and developing techniques for rapid-
ly customizing the generic components to new do-
mains  (e.g.  Aist,  Allen,  and  Galescu  2004).  To
achieve this goal we have made several innovations,
including (1) developing domain independent mod-
els of  semantic and  contextual  interpretation,  (2)
developing generic  dialogue  management  compo-
nents based on an abstract  model of collaborative
problem solving, and (3) extensively using an ontol-
ogy-mapping system that connects the domain inde-
pendent representations to the representations/query
languages used by the back-end applications,  and
which is used to automatically optimize the perfor-
mance of the system in the specific domain.
2 Theoretical  Underpinnings:  The Prob-
lem-Solving Model of Dialogue
While many have observed that communication
is a specialized form of joint action that happens to
involve language and that dialogue can be viewed
as collaborative problem solving, very few imple-
mented systems have been explicitly based on these
ideas. Theories of speech act interpretation as inten-
tion recognition have been developed  (including ex-
tensive  prior  work  in  TRIPS'  predecessor,  the
TRAINS project), but have been generally consid-
ered impractical for actual systems.  Planning mod-
els  have been more successful  on the  generation
side, and some systems have used the notion of exe-
cuting explicit task models to track and drive the in-
teractions  (e.g.,  Sidner  and  Rich's  COLLAGEN
framework). But collaborative problem solving, and
dialogue in general, is much more general than exe-
cuting tasks. In our applications, in addition to exe-
cuting tasks, we see dialogue that is used to define
the task (i.e., collaborative planning), evaluate the
task (e.g., estimating how long it will take,  com-
paring options,  or  likely effects),    debug a  task
(e.g., identifying and discussing problems and how
to remedy them), learn new tasks (e.g., by demon-
stration and instruction).
85
In the remainder of the paper, we'll first discuss
the methods we've developed for building dialogue
systems using generic components.  We'll then de-
scribe two systems implemented using the TRIPS
architecture that we will demonstrate at the interac-
tive poster session.
3 Generic Methods:  Ontology Mappings
and Collaborative Problem Solving
The goal of our work is to develop generic spoken
dialogue technology that can be rapidly customized
to new applications, tasks and domains. To do this,
we have developed generic domain independent rep-
resentations not only of sentence meaning but also
of the collaborative actions that are performed by
the speech acts as one engages in dialogue. Further-
more, we need to be able to easily connect these
generic representations to a wide range of different
domain specific task models and applications, rang-
ing from data base query systems to state-of-the-art
planning and scheduling systems.  This  paper  de-
scribes  the  approach  we  have  developed  in  the
TRIPS system. TRIPS is now being used in a wide
range of diverse applications, from interactive plan-
ning (e.g., developing evacuation plans), advice giv-
ing  (e.g.,  a  medication  advisor  (Ferguson  et  al.
2002)),  controlling teams of robots,   collaborative
assistance (e.g., an assistant that can help you pur-
chase a computer, as described in this paper), sup-
porting human learning, and most recently having
the computer  learn (or  be  taught)  tasks,  such as
learning to perform tasks on the web.  Even though
the tasks and domains differ dramatically, these ap-
plications use the same set of core understanding
components. 
The key to supporting such a range of tasks and ap-
plications is the use of a general ontology-mapping
system. This allows the developer to express a set
of mapping rules that translate the generic knowl-
edge representation into the specific representations
used by the back-end applications (called the KR
representation).   In  order  to  support  generic dis-
course processing, we represent these mappings as
a chain of simpler transformations. These represen-
tations are thus transformed in several stages. The
first,  using the ontology mapping rules,  maps the
LF representation into an intermediary representa-
tion (AKRL - the abstract KR language) that has a
generic syntax  but  whose content is  expressed in
terms of the KR ontology. The second stage is a
syntactic transformation that occurs at the time that
calls to the back-end applications actually occur so
that  interactions  occur  in  the  representations  the
back-end expects.   In  addition to  using ontology
mapping to  deal  with the representational  issues,
TRIPS is unique in that it uses a generic model of
collaborative problem solving to drive the dialogue
itself  (e.g.  Allen,  Blaylock,  and  Ferguson 2002).
This model forms the basis of a generic component
(the collaboration manager) that supports both in-
tention recognition to identify the intended speech
acts and their content, planning the system's actions
to respond to the user (or that take initiative), and
providing utterance realization goals to the genera-
tion system. To develop this, we have been develop-
ing  a  generic  ontology  of  collaborative  problem
solving acts, which provide the framework for man-
aging  the  dialogue.  The  collaboration  manager
queries a domain-specific task component in order
to  make  decisions  about  interpretations  and  re-
sponses.
4 TRIPS  Spoken  Dialogue  Interface  to
the CALO Purchasing Assistant 
The CALO project is a large multisite effort which
aims  at  building  a  computerized  assistant  that
learns how to help you with day-to-day tasks. The
overarching goal of the CALO project is to 
... create cognitive software systems, that is,
systems that can reason, learn from experi-
ence, be told what to do, explain what they
are doing, reflect on their experience, and re-
spond robustly to surprise (Mark and Per-
rault 2004). 
Within this broad mandate, one of our current areas
of focus is user-system dialogue regarding the task
of purchasing - including eliciting user needs, de-
scribing possibilities, and reviewing & finalizing a
purchase  decision.  (Not  necessarily  as  discrete
stages; these elements may be interleaved as appro-
priate for the specific item(s) and setting.)  Within
the purchasing domain,  we began with computer
purchasing and have branched out to other equip-
ment such as projectors.
How to help with purchasing? The family of tasks
involving purchasing items online, regardless of the
type of item, have a  number of elements in com-
mon. The process of purchasing has some common
86
dialogue elements - reporting on the range of fea-
tures  available,  allowing the user  to specify con-
straints, and so forth.  Also, regarding the goal that
must be reached at the end of the task, the eventual
item must:
Meet requirements.  The item needs to meet some
sort of user expectations. This could be as arbitrary
as a specific part number, or as compositional - and
amenable to machine understanding -  as  a  set  of
physical  dimensions (length,  width,  height,  mass,
etc.) 
Be approved. Either the system will have the au-
thority to approve it (cf. Amazon's one-click order-
ing system), or more commonly the user will review
and confirm the purchase. In an office environment
the approval process may extend to include review
by a supervisor, such as might happen with an item
costing over (say) $1000. 
Be available. (At  one time a  certain  electronics
store in California had the habit of leaving out floor
models of laptops beyond the point where any were
actually available for sale.  (Perhaps to entice the
unwitting customer into an ?upsale?, that is, buying
a  similar  but  more  expensive  computer.))  On  a
more serious note, computer specifications change
rapidly, and so access to online information about
available  computers  (provided  by  other  research
within CALO) would be important in order to en-
sure that the user can actually order the machine he
or she has indicated a preference for.  
At  the interactive poster  session,  we will demon-
strate some of the current spoken dialogue capabili-
ty related to the CALO task of purchasing equip-
ment.  We will demonstrate a number of the aspects
of the system such as initiating a conversation, dis-
cussing specific requirements,  presenting possible
equipment to purchase,  system-initiated reminders
to ask for supervisor approval for large purchases,
and finalizing a decision to purchase. 
Figure 1. Fruit carts display.
87
5 TRIPS  Spoken  Dialogue  Interface  to
choosing,  placing,  painting,  rotating,
and filling (virtual) fruit carts
TRIPS is versatile in its applications, as we've said
previously.  We hope to also demonstrate an inter-
face to  a  system for  using spoken commands to
modifying, manipulating, and placing objects on a
computer-displayed map.  This  system (aka  ?fruit
carts?)  extends  the  TRIPS  architecture  into  the
realm of continuous understanding.  That is, when
state-of-the-art  dialogue systems listen,  they typi-
cally wait for the end of the utterance before decid-
ing what to do.  People on the other hand do not
wait in this way ? they can act on partial informa-
tion as  it  becomes available.   A classic example
comes  from  M.  Tanenhaus  and  colleagues  at
Rochester: when presented with several objects of
various colors and told to ?click on the yel-?, people
will already tend to be looking relatively more at the
yellow object(s) even before the word ?yellow? has
been completed.  To achieve this type of interactivi-
ty with a dialogue system ? at least at the level of
two or three words at a time, if not parts of words ?
imposes some interesting challenges. For example:
1. Information must flow asynchronously between
dialogue components, so that actions can be trig-
gered based on partial utterances even while the
understanding continues
2. There must be reasonable representations of in-
complete information ? not just ?incomplete sen-
tence?,  but  specifying what  is  present  already
and perhaps what may potentially follow
3. Speech  recognition,  utterance  segmentation,
parsing, interpretation, discourse reasoning, and
actions must all be able to happen in real time
The fruit carts system consists of two main compo-
nents:  first,  a  graphical  interface implemented on
Windows  2000  using  the  .NET  framework,  and
connected to  a  high-quality  eyetracker;  second,  a
TRIPS-driven spoken dialogue interface implement-
ed primarily in LISP.   The actions in this domain
are as follows:
1. Select an object (?take the large plain square?)
2. Move it (?move it to central park?)
3. Rotate  it  (?and then turn  it  left  a  bit  ?  that's
good?)
4. Paint it (?and that one needs to be purple?)
5. Fill it (?and there's a grapefruit inside it?)
Figure 1 shows an example screenshot from the
fruit carts visual display. The natural language in-
teraction  is  designed to  handle  various  ways  of
speaking,  including conventional  definite  descrip-
tions (?move the large square to central park?) and
more interactive language such as (?up towards the
flag pole ? right a bit ? more ? um- stop there.?)
6 Conclusion
In this brief paper,  we have described some of
the recent progress on the TRIPS platform.  In par-
ticular we have focused on two systems developed
in TRIPS: a spoken dialogue interface to a mixed-
initiative purchasing assistant, and a spoken inter-
face for exploring continuous understanding in an
object-placement task.  In  both  cases  the  systems
make use of reusable components ? for input and
output  such as  parsing and speech synthesis,  and
also for dialogue functionality such as mapping be-
tween language,  abstract  semantics,  and  specific
representations for each domain.
References 
Aist,  G.  2004.  Speech,  gaze,  and  mouse  data  from
choosing,  placing,  painting,  rotating,  and  filling
(virtual) vending carts. International Committee for
Co-ordination  and  Standardisation  of  Speech
Databases  (COCOSDA)  2004  Workshop,  Jeju  Is-
land, Korea, October 4, 2004. 
Aist, G.S., Allen, J., and Galescu, L. 2004. Expanding
the linguistic coverage of a spoken dialogue system
by mining human-human dialogue for new sentences
with familiar meanings. Member Abstract, 26th An-
nual  Meeting  of  the  Cognitive  Science  Society,
Chicago, August 5-7, 2004. 
James Allen, Nate Blaylock, and George Ferguson. A
problem-solving model for collaborative agents.  In
First International Joint Conference on Autonomous
Agents and Multiagent Systems, Bologna, Italy, July
15-19 2002. 
George  Ferguson,  James  F.  Allen,  Nate  J.  Blaylock,
Donna K. Byron, Nate W. Chambers, Myrsolava O.
Dzikovska, Lucian Galescu, Xipeng Shen, Robert S.
Swier, and Mary D. Swift.  The Medication Advisor
Project: Preliminary Report, Technical Report 776,
Computer  Science  Dept.,  University  of  Rochester,
May 2002. 
Mark,  B.,  and  Perrault,  R.  (principal  investigators).
2004.  Website for Cognitive Assistant  that  Learns
and Organizes. http://www.ai.sri.com/project/CALO
88
Stochastic Language Generation in a Dialogue System:
Toward a Domain Independent Generator
Nathanael Chambers and James Allen
Institute for Human and Machine Cognition
40 South Alcaniz Street
Pensacola, FL 32502
{nchambers,jallen}@ihmc.us
Abstract
Until recently, surface generation in dialogue
systems has served the purpose of simply pro-
viding a backend to other areas of research.
The generation component of such systems
usually consists of templates and canned text,
providing inflexible, unnatural output. To
make matters worse, the resources are typi-
cally specific to the domain in question and
not portable to new tasks. In contrast, domain-
independent generation systems typically re-
quire large grammars, full lexicons, complex
collocational information, and much more.
Furthermore, these frameworks have primar-
ily been applied to text applications and it is
not clear that the same systems could perform
well in a dialogue application. This paper
explores the feasibility of adapting such sys-
tems to create a domain-independent genera-
tion component useful for dialogue systems. It
utilizes the domain independent semantic form
of The Rochester Interactive Planning System
(TRIPS) with a domain independent stochas-
tic surface generation module. We show that
a written text language model can be used
to predict dialogue utterances from an over-
generated word forest. We also present results
from a human oriented evaluation in an emer-
gency planning domain.
1 Introduction
This paper takes steps toward three surface genera-
tion goals in dialogue systems; to create a domain-
independent surface generator, to create a surface gen-
erator that reduces dependence on large and/or domain-
specific resources by using out of domain language mod-
els, and to create an effective human-like surface genera-
tor.
Natural Language systems are relatively young and
most of today?s architectures are designed and tested on
specific domains. It is becoming increasingly desirable
to build components that are domain-independent and re-
quire a small amount of time to instantiate. Unfortu-
nately, when components are tailored to a specific do-
main, it requires a complete overhaul to use the archi-
tecture in a new domain.
While dialogue systems have found success in many
areas, the backend of these systems, Natural Language
Generation (NLG), has largely been ignored and used
solely to show the progress of other components. How-
ever, it is now important to generate not just content-rich
utterances, but also natural utterances that do not inter-
fere with the dialogue. Easy to build template-based NLG
components can usually satisfy the content requirement,
but their static, inflexible forms rarely facilitate an effec-
tive human oriented dialogue system.
Natural surface generation requires hand-crafted lexi-
cons, grammars, ontologies, and much more to be suc-
cessful. The time required to create a simple surface gen-
eration component is small, but the time required to cre-
ate even a mildly natural component is very large. Lan-
guage modeling offers hope that the information encoded
in these grammars and lexicons is implicitly present in
spoken and written text. There have been many advances
with stochastic approaches in areas that have taken ad-
vantage of the large corpora of available newswire, such
as Machine Translation (MT). If newswire text (which
makes up much of the available English corpora) can
be applied to dialogue, we could depend less on hand-
crafted grammars and domain-specific resources.
This paper describes an approach to surface genera-
tion in dialogue systems that uses out of domain language
models; a model based on newswire text and a model
based on spoken dialogue transcripts. We also describe
how this approach fits with a domain independent logical
form being used for interpretation in TRIPS. Our anal-
ysis of this approach shows that newswire corpora can
generate not only the semantic content in its output, but
also shows that it can be integrated successfully into a di-
alogue system, resulting in only a slight decrease in nat-
uralness as judged by human evaluators.
This paper begins with a description of previous sur-
face generation work. Section 3 describes the stochastic
algorithm used from the Machine Translation (MT) sys-
tem, HALogen, including differences in dialogue versus
newswire text. Section 4 describes the domain indepen-
dence of the logical form in TRIPS and how indepen-
dence is preserved in translating into the stochastic com-
ponent. Section 5 describes our evaluation including the
language models and the domain we used for evaluation.
Finally, we present the results and discussion in section
6.
2 Background
Template-based approaches have been widely used for
surface generation. This has traditionally been the
case because the many other areas of NLP research
(speech recognition, parsing, knowledge representation,
etc.) within a dialogue system require an output form to
indicate the algorithms are functional. Templates are cre-
ated very cheaply, but provide a rigid, inflexible output
and poor text quality. See Reiter (Reiter, 1995) for a full
discussion of templates. Dialogue systems particularly
suffer as understanding is very dependent on the natural-
ness of the output.
Rule-based generation has developed as an alternative
to templates. Publicly available packages for this type
of generation take strides toward independent generation.
However, a significant amount of linguistic information
is usually needed in order to generate a modest utterance.
This kind of detail is not available to most domain in-
dependent dialogue systems. A smaller, domain-specific
rule-based approach is difficult to port to new domains.
The corpus-based approach to surface generation does
not use large linguistic databases but rather depends on
language modeling of corpora to predict correct and nat-
ural utterances. The approach is attractive in comparison
to templates and rule-based approaches because the lan-
guage models implicitly encode the natural ordering of
English. Recently, the results from corpus-based surface
generation in dialogue systems have been within specific
domains, the vast majority of which have used the Air
Travel Domain with Air Travel corpora.
Ratnaparkhi (Ratnaparkhi, 2000; Ratnaparkhi, 2002)
and Oh and Rudnicky (Oh and Rudnicky, 2000) both
studied surface generators for the air travel domain. Their
input semantic form is a set of attribute-value pairs that
are specific to the airline reservation task. The language
models were standard n-gram approaches that depended
on a tagged air travel corpus for the attribute types. Both
groups ran human evaluations; Ratnaparkhi studied a 2
subject evaluation (with marks of OK,Good,Bad) and Oh
and Rudnicky studied 12 subjects that compared the out-
put between a template generator and the corpus-based
approach. The latter showed no significant difference.
Most recently, Chen et al utilized FERGUS (Banga-
lore and Rambow, 2000) and attempted to make it more
domain independent in (Chen et al, 2002). There are
two stochastic processes in FERGUS; a tree chooser that
maps an input syntactic tree to a TAG tree, and a trigram
language model that chooses the best sentence in the lat-
tice. They found that a domain-specific corpus performs
better than a Wall Street Journal (WSJ) corpus for the tri-
gram LM. Work was done to try and use an independent
LM, but (Rambow et al, 2001) found interrogatives to
be unrepresented by a WSJ model and fell back on air
travel models. This problem was not discussed in (Chen
et al, 2002). Perhaps automatically extracted trees from
the corpora are able to create many good and few bad
possibilities that the LM might choose.
(Chen et al, 2002) is the first paper to this author?s
knowledge that attempts to create a stochastic domain in-
dependent generator for dialogue systems. One of the
main differences between FERGUS and this paper?s ap-
proach is that the input to FERGUS is a deep syntactic
tree. Our approach integrates semantic input, reducing
the need for large linguistic databases and allowing the
LM to choose the correct forms. We are also unique in
that we are intentionally using two out-of-domain lan-
guage models. Most of the work on FERGUS and the
previous surface generation evaluations in dialogue sys-
tems are dependent on English syntax and word choice
within the air travel domain. The final generation sys-
tem cannot be ported to a new domain without further
effort. By creating grammar rules that convert a seman-
tic form, some of these restrictions can be removed. The
next section describes our stochastic approach and how
it was modified from machine translation to spoken dia-
logue.
3 Stochastic Generation (HALogen)
We used the HALogen framework (Langkilde-Geary,
2002) for our surface generation. HALogen was origi-
nally created for a domain within MT and is a sentence
planner and a surface realizer. Analysis and MT appli-
cations can be found in (Langkilde and Knight, 1998;
Knight and Langkilde, 2000).
HALogen accepts a feature-value structure ranging
from high-level semantics to shallow syntax. Figure 1
shows a mixture of both as an example. Given this input,
generation is a two step process. First, the input form
is converted into a word forest (a more efficient repre-
sentation of a word lattice) as described in (Langkilde-
Geary, 2002). Second, the language model chooses the
most probable path through the forest as the output sen-
tence.
(V68753 / move
:TENSE past
:AGENT (V68837 / person
:QUANT three
:NUMBER plural
)
:THEME (V68846 / ambulance
)
)
Figure 1: HALogen input of the sentence Three people
moved the ambulance.
The word forest is created by a series of grammar rules
that are designed to over-generate for a given representa-
tion. As figure 1 shows, there is a lot of syntactic informa-
tion missing. The rules are not concerned with generating
only syntactically correct possibilities, but to generate all
possibilities under every input that is not specified (our
example does not provide a determiner for ambulance,
so the grammar would produce the definite and indefinite
versions). Once the forest is created, the language model
chooses the best path(s) through the forest.
We modified HALogen?s grammar to fit the needs of a
dialogue system while maintaining the same set of roles
and syntactic arguments recognized by the grammar. The
TRIPS Logical Form uses many more roles than HALo-
gen recognizes, but we converted them to the smaller
set. By using HALogen?s set of roles, we can be assured
that our grammar is domain independent from TRIPS.
We did, however, expand the grammar within its cur-
rent roles. For instance, we found the theme role to be
insufficient and changed the grammar to generate more
syntactic constructs (for example, we generate the theme
in both the object and subject positions). We also ex-
panded the production rules for interrogatives and imper-
atives, both of which were sparsely used/tested because
of HALogen?s original use in MT domains.
HALogen is able to expand WordNet word classes into
their lexical items, but due to the difficulty of mapping
the TRIPS word classes to WordNet, our input terms to
HALogen are the desired lexical items instead of word
classes as shown in figure 1. Future work includes link-
ing the grammar to the TRIPS word classes instead of
WordNet.
4 The Dialogue System
We developed our approach within TRIPS, a collab-
orative planning assistant that interacts with a human
user mainly through natural language dialogue, but also
through graphical displays. The system supports many
domains involving planning scenarios, such as a 911 dis-
aster rescue assistant and a medical adviser. TRIPS per-
(define-type LF CONSUME
:semfeatures
(Situation (aspect dynamic) (cause agentive))
:arguments
(AGENT (Phys-obj (intentional +) (origin living)))
(THEME (Phys-obj (form substance))))
Figure 2: LF type definitions for LF CONSUME (from
(Dzikovska et al, 2003))
forms advanced reasoning and NLP tasks including, but
not limited to, interpretation in context, discovering user
intentions, planning, and dialogue management. Lan-
guage generation has largely been ignored in the sys-
tem until recently. As with many dialogue systems, it
has simply been a means to show results in the above
areas through a language back-end. Recently, Stent
(Stent, 1999) did extensive work on dialogue manage-
ment through rule-based generation (Allen et al, 2001).
4.1 Logical Form of Meaning
There are two meaning representations in TRIPS. The
first is a domain independent representation called the
logical form (LF). The second is a domain dependent
knowledge representation (KR). The effort toward creat-
ing the domain independent LF is part of an overall goal
of creating a dialogue system that is easily portable to
new domains. A domain-specific representation is always
needed for reasoning, and mapping rules are created to
map the LF into the KR for each domain. These rules are
easier to create than a new logical representation for each
domain.
Dzikovska, Swift and Allen (Dzikovska et al, 2003)
have built a parser that parses speech utterances into this
domain-independent LF. The LF is very important to this
paper. One of the biggest problems that any surface gen-
eration approach faces is that it takes a lot of work to gen-
erate sentences for one domain. Moving to a new domain
usually involves duplicating much of this work. How-
ever, if we create a surface generator that uses the LF as
input, we have created a surface generator that is able to
generate English in more than one specific domain.
The LF ontology consists of a single-inheritance hi-
erarchy of frame-like LF types that classify entities ac-
cording to their semantics and argument structure. Every
LF type can have a set of thematic arguments with se-
lectional restrictions. The ontology is explicitly designed
to capture the semantic differences that can affect sen-
tence structure, and it draws from many sources including
FRAMENET, EURO-WORDNET, and VERBNET. The
reader is referred to (Dzikovska et al, 2003) for more de-
tails. An example of an LF type definition is shown in
Figure 2.
(SPEECHACT sa1 SA TELL :content V11)
(F V11 (* LF CONSUME take) :AGENT V123
:THEME V433)
:TMA ((:TENSE PAST))
(PRO V123 (:* LF PERSON he) :CONTEXT REL HE)
(A V433 (:* LF DRUG aspirin))
Figure 3: Logical Form for the sentence, he took an as-
pirin.
The parser uses the LF type definitions to build a gen-
eral semantic representation of the input. This is a flat
and unscoped representation of the semantics of the sen-
tence that serves as input to the TRIPS discourse inter-
pretation modules (which perform reference resolution,
disambiguation, intention recognition to produce the fi-
nal intended meaning). Figure 3 gives an example of the
LF representation of the sentence, he took an aspirin. It
can be read as follows: A speech act of type SA TELL
occurred with content being V11, which is a proposition
of type LF CONSUME (more specifically ?take?), with
AGENT V123 and THEME V433. V123 is pronominal
form of type LF PERSON and pro-type HE, and V433 is
an indefinitely specified object that is of type LF DRUG
(more specifically ?aspirin?).
The LF representation serves as the input to our surface
generation grammar after a small conversion. If natural
human quality dialogue can be produced from this LF, not
only has a domain independent generator been created,
but also a generator that shares ontologies and lexicons
with the parser.
4.2 Integrating HALogen into TRIPS
The task of converting our independent Logical Form
(LF) into HALogen?s Abstract Meaning Representation
was relatively straightforward. Several rules were cre-
ated to change LF specific roles into the smaller set of
roles that the surface generation grammar recognizes. LF
roles such as COGNIZER and ENTITY are converted
to AGENT and THEME respectively. Verb properties
represented by TMA are converted into the appropriate
syntactic roles of TENSE, MODALITY, AUXILLARY,
etc. The LF type triple is reduced to just the lexical
item and appropriate determiners are attached when the
LF provides enough information to warrant it. It is best
illustrated by example using our example LF in figure
3. Given these decisions, our example?s conversion be-
comes:
(V11 / TAKE
:TENSE PAST
:AGENT (V123 / HE)
:THEME (V433 / ASPIRIN))
This resulting AMR is the input to HALogen where it is
converted into a word forest using our modified dialogue-
based HALogen grammar. Finally, the language model
chooses the best output.
The above conversion applies to declarative, impera-
tive and interrogative speech acts. These are translated
and generated by the method in section 3. We also take a
similar approach to Stent?s previous work (Stent, 1999)
that generated grounding and turn-taking acts using a
template-based method. These usually short utterances
do not require complex surface generation and are left to
templates for proper production.
5 Evaluation
This paper is evaluating two surface generation design
decisions: the effectiveness of stochastic (word forest
based) surface generation with domain independent lan-
guage models, and the benefits of using dialogue vs.
newswire models. Evaluating any natural language gen-
eration system involves many factors, but we focused on
two of the most important aspects to evaluate, the con-
tent and clarity (naturalness) of the output (English utter-
ances). This section briefly describes previous automatic
evaluation approaches that we are avoiding, followed by
the human evaluation we have performed on our system.
5.1 Automatic Evaluation
Evaluating generation is particularly difficult due to the
diverse amount of correct output that can be generated.
There are many ways to present a given semantic repre-
sentation in English and what determines quality of con-
tent and form are often subjective measures. There are
two general approaches to a surface generation evalua-
tion. The first uses human evaluators to score the out-
put with some pre-defined ranking measure. The second
uses a quantitative automatic approach usually based on
n-gram presence and word ordering. Bangalore et al de-
scribe some of the quantitative measures that have been
used in (Bangalore et al, 2000). Callaway recently used
quantitative measures in an evaluation between symbolic
and stochastic surface generators in (Callaway, 2003).
The most common quantitative measure is Simple
String Accuracy. This metric uses an ideal output string
and compares it to a generated string using a metric that
combines three word error counts; insertion, deletion, and
substitution. One variation on this approach is tree-based
metrics. These attempt to better represent how bad a bad
result is. The tree-based accuracy metrics do not com-
pare two strings directly, but instead build a dependency
tree for the ideal string and attempt to create the same
dependency tree from the generated string. The score is
dependent not only on word choice, but on positioning at
the phrasal level. Finally, the most recent evaluation met-
ric is the Bleu Metric from IBM(Papineni et al, 2001).
Designed for Machine Translation, it scores generated
sentences based on the n-gram appearance from multiple
ideal sentences. This approach provides more than one
possible realization of an LF and compares the generated
sentence to all possibilities.
Unfortunately, the above automatic metrics are very
limited in mimicking human scores. The Bleu metric can
give reasonable scores, but the results are not as good
when only one human translation is available. These
automatic metrics all compare the desired output with
the actual output. We decided to ignore this evaluation
because it is too dependent on syntactic likeness. The
following two sentences represent the same semantic
meaning yet appear very different in structure:
The injured person is still waiting at the hospital.
The person with the injury at the hospital is still waiting.
The scoring metrics would judge very harshly, yet
a human evaluator should see little difference in semantic
content. Clearly, the first is indeed better in naturalness
(closeness to human English dialogue), but both content
and naturalness cannot be measured with the current
quantitative (and many human study) approaches.
Although it is very time consuming, human evalua-
tion continues to be the gold standard for generation
evaluation.
5.2 Evaluation Methodology
Our evaluation does not compare an ideal utterance with
a generated one. We use a real human-human dialogue
transcript and replace every utterance of one of the par-
ticipants with our generated output. The evaluators are
thereby reading a dialogue between a human and a com-
puter generated human, yet it is based on the original
human-human dialogue. Through this approach, we can
present the evaluators with both our generated and the
original transcripts (as the control group). However, they
do not know which is artificial, or even that any of them
are not human to human. The results will give an accurate
portrayal of how well the system generates dialogue. The
two aspects of dialogue that the evaluators were asked to
measure for each utterance were understandability (se-
mantically within context) and naturalness.
There have been many metrics used in the past. Met-
rics range from scoring each utterance with a subjective
score (Good,Bad) to using a numeric scale. Our evalua-
tors use a numeric scale from 0 to 5. The main motivation
for this is so we can establish averages and performance
results more easily. The final step is to obtain a suitable
domain of study outside the typical air travel domain.
5.3 Domain Description and Dialogue Construction
A good dialogue evaluation is one in which all aspects
of a natural dialogue are present and the only aspect that
has been changed is how the surface generation presents
the required information. By replacing one speaker?s
utterances with our generated utterances in a transcript
of a real conversation, we guarantee that grounding and
turn-taking are still present and our evaluation is not hin-
dered by poor dialogue cues. The TRIPS Monroe Corpus
(Stent, 2000) works well for this task.
There are 20 dialogues in the Monroe Corpus. Each
dialogue is a conversation between two English speak-
ers. Twenty different speakers were used to construct the
dialogues. Each participant was given a map of Mon-
roe County, NY and a description of a task that needed
to be solved. There were eight different disaster scenar-
ios ranging from a bomb attack to a broken leg and the
participants were to act as emergency dispatchers (this
domain is often referred to as the 911 Rescue Domain).
One participant U was given control of solving the task,
and the other participant S was told that U had control.
S was to assist U in solving the task. At the end of the
discussion, U was to summarize the final plan they had
created together.
The average dialogue contains approximately 500 ut-
terances. We chose three of the twenty dialogues for our
evaluation. The three were the shorter dialogues in length
(Three of the only four dialogues that are less than 250 ut-
terances long. Many are over 800 utterances.). This was
needed for practical reasons so the evaluators could con-
duct their rankings in a reasonable amount of time and
still give accurate rankings. The U and S speakers for
each dialogue were different.
We replaced the S speaker in each of the dialogues with
generated text, created by the following steps:
? Parse each S utterance into its LF with the TRIPS
parser.
? Convert the LF to the AMR grammar format.
? Send the AMR to HALogen.
? Generate the top sentence from this conversion us-
ing our chosen LM.
We hand-checked for correctness each AMR that is cre-
ated from the LF. The volatile nature of a dialogue system
under development assured us that many of the utterances
were not properly parsed. Any errors in the AMR were
fixed by hand and hand constructed when no parse could
be made. The fixes were done before we tried to generate
the S speaker in the evaluation dialogues.
We are assuming perfect input to generation. This eval-
uation does not evaluate how well the conversion from
the LF to the AMR is performing. Our goal of generat-
ing natural dialogue from a domain-independent LM can
be fully determined by analyzing the stochastic approach
in isolation. Indeed, the goal of a domain independent
generator is somewhat dependent on the conversion from
our domain independent LF, but we found that the errors
from the conversion are not methodological errors. The
errors are simple lexicon and code errors that do not re-
late to domain-specifics. Work is currently underway to
repair such inconsistencies.
Each of the S participant?s non-dialogue-management
utterances were replaced with our generated utterances.
The grounding, turn-taking and acknowledgment utter-
ances were kept in their original form. We plan on gener-
ating these latter speech acts with templates and are only
testing the stochastic generation in this evaluation. The U
speaker remained in its original state. The control groups
will identify any bias that U may have over S (i.e. if U
speaks ?better? than S in general), but testing the genera-
tion with the same speaker allows us to directly compare
our language models.
5.4 Language Model Construction
We evaluated two language models. The first is a news
source model trained on 250 million words with a vocab-
ulary of 65,529 from the WSJ, AP and other online news
sources as built in (Langkilde-Geary, 2002). This model
will be referred to as the WSJ LM. The second language
model was built from the Switchboard Corpus (J. God-
frey, 1992), a corpus of transcribed conversations and not
newswire text. The corpus is comprised of ?spontaneous?
conversations recorded over the phone, including approx-
imately 2 million words with a vocabulary of 20,363.
This model will be referred to as the SB LM. Both mod-
els are trigram, open vocabulary models with Witten-Bell
smoothing. The Switchboard Corpus was used because it
contrasts the newswire corpus in that it is in the genre of
dialogue yet does not include the Monroe Corpus that the
evaluation was conducted on.
5.5 Evaluators
Ten evaluators were chosen, all were college undergradu-
ates between the ages of 18-21. None were linguistics or
computer science majors. Each evaluator received three
transcripts, one from each of our three chosen dialogues.
One of these three was the original human to human di-
alogue. The other two had the S speaker replaced by our
surface generator. Half of the evaluators received gener-
ations using the WSJ LM and the other half received the
SB LM. They ranked each utterance for understandability
and naturalness on scales between 0 and 5. A comparison
of the human and generated utterances is given in figure
8 in the appendix.
Percent Difference between U and S speakers
0 1 2 3 4
understand 0.92 6.03 3.70 0.23 1.74
natural -1.31 -0.26 2.56 1.94 -3.09
5 6 7 8 9
understand 3.91 3.27 2.46 -0.10 14.8
natural 3.60 2.38 -0.26 5.16 13.3
Total Percent Difference
understand 3.24%
natural 1.85%
Figure 4: Difference between the human evaluator scores
for the two original human speakers, U and S. The ten
evaluators are listed by number, 0 to 9. Evaluators rated
the content (understandability) and clarity (naturalness)
of each utterance on a 0-5 scale. S was rated slightly
higher than U.
6 Results
Figure 4 compares the control dialogues as judged by the
human evaluators by giving the percent difference be-
tween the two human speakers. It is apparent that the
U speaker is judged worse than the S speaker in the aver-
age of the three dialogues. We see the S speaker is scored
3.24% higher in understanding and 1.85% higher in nat-
uralness. Due to the nature of the domain, the U speaker
tends to make more requests and short decisions while the
S speaker gives much longer descriptions and reasons for
his/her actions. It is believed the human evaluators tend to
score shorter utterances more harshly because they aren?t
?complete sentences? as most people are used to seeing
in written text. We believe this also explains the discrep-
ancy of evaluator 9?s very high scores for the S speaker.
Evaluator 9 received dialogue 10 as his control dialogue.
Dialogue 10?s S speaker tended to have much longer ut-
terances than any of the other five speakers in the three
dialogues. It is possible that this evaluator judged shorter
utterances more harshly.
Figure 5 shows the comparison between using the two
LMs as well as the human control group. The scores
shown are the average utterance scores over all evalua-
tors and dialogues. The dialogue management (ground-
ing, turn-taking, etc.) utterance scores are not included in
these averages. Since we do not generate these types of
utterances, it would be misleading to include them in our
evaluation. As figure 5 shows, the difference between
the two LMs is small. Both received a lower natural-
ness score than understandability. It is clear that we are
able to generate utterances that are understood, but yet
are slightly less natural than a human speaker.
Figure 6 shows the distribution of speech acts in each
of the 3 evaluation dialogues. Due to the nature of the
Monroe Corpus, there are not many interrogatives or im-
Language Model Comparison
U S U/S difference
WSJ LM
understand 4.67 4.33 -0.34 (?7.28%)
natural 4.49 3.97 -0.52 (?11.58%)
SB LM
understand 4.62 4.30 -0.32 (?6.93%)
natural 4.18 3.84 -0.34 (?8.13%)
HUMAN
understand 4.63 4.78 0.15 (3.24%)
natural 4.33 4.41 0.08 (1.85%)
Figure 5: Average scores (over the 10 evaluators) of un-
derstandability and naturalness with the dialogue man-
agement utterances removed. The first compares the S
speaker generated with the WSJ LM, the second com-
pares the S speaker generated with the SB LM, and the
third is the S speaker using the original human utterances.
peratives. Since the two participants in the dialogues
work together and neither has more information about the
rescue problem than the other, there are not many ques-
tions. Rather, it is mostly declaratives and acknowledg-
ments.
Figure 7 shows the average score given for each speech
act across all evaluators. Note that the numbers are only
for the S speaker in each dialogue because only S was
generated with the surface generator. Since each eval-
uator scored 2 computer dialogues and 1 human (con-
trol) dialogue, the LM numbers are averaged across twice
as many examples. The understandability scores for the
WSJ and SB LMs are relatively the same across all acts,
but naturalness is slightly less in the SB LM. Comparing
the human scores to both out-of-domain LMs, we see that
declaratives averaged almost a 0.5 point loss from the hu-
man control group in both understandability and natural-
ness. Imperatives suffer an even larger decrease with an
approximate 0.7 loss in understandability. The SB LM
actually averaged over 1.0 decrease in naturalness. The
interrogatives ranged from a 0.5 to 0 loss.
6.1 Discussion
We can conclude from figure 5 that the evaluators were
relatively consistent among each other in rating under-
standability, but not as much so with naturalness. The
comparison between the WSJ and SB LMs is inconclu-
sive because we see in figure 5 that even though the evalu-
ators gave the WSJ utterances higher absolute scores than
the SB utterances, the percent difference from how they
ranked the human U speaker is lower. The fact that it
is inconclusive is somewhat surprising because intuition
leads us to believe that the dialogue-based SB would per-
form better than the newswire-based WSJ. One reason
may be because the nature of the Monroe Corpus does
not include many dialogue specific acts such as questions
and imperatives. However, declaratives are well repre-
sented and we can conclude that the newswire WSJ LM
is as effective as the dialogue SB model for generating
dialogue declaratives. Also, it is of note that the WSJ LM
out-performed the SB LM in naturalness for most speech
act types (as seen in figure 7) as well.
The main result from this work is that an out-of-
domain language model cannot only be used in a stochas-
tic dialogue generation system, but the large amount of
available newswire can also be effectively utilized. We
found only a 7.28% decrease in understandability and an
11.58% decrease in naturalness using our newswire LM.
This result is exciting. These percentages correspond to
ranking an utterance 4.64 and 4.42 instead of a perfect
5.00 and 5.00. The reader is encouraged to look at the
output of the generation in the appendix, figure 8.
6.2 Future Work
We have created a new grammar to generate from the LF
that recognizes the full set of thematic roles. In addition,
we have linked our dialogue system?s lexicon to the gen-
eration module instead of WordNet, resulting in a fully
integrated component to be ported to new domains with
little effort. It remains to run an evaluation of this design.
Also, stochastic generation favors other avenues of
generation research, such as user adaptation. Work is be-
ing done to adapt to the specific vocabulary of the human
user using dynamic language models. We hope to cre-
ate an adaptive, natural generation component from this
effort.
Finally, we are looking into random weighting ap-
proaches for the generation grammar rules and resulting
word forest in order to create dynamic surface generation.
One of the problems of template-based approaches is that
the generation is too static. Our corpus-based approach
solves much of the problem, but there is still a degree of
?sameness? that is generated among the utterances.
7 Conclusion
We have shown that steps toward a domain-independent
NLG component of a dialogue system can be taken
through a corpus-based approach. By depending on a
domain-independent semantic input in combination with
a grammar that over-generates possible English utter-
ances and a newswire language model to choose the best,
we have shown that it is possible to generate content rich
and natural utterances. We report results in a new, richer
domain for stochastic generation research and show our
approach resulting in only an 11.6% decrease in natural-
ness when compared to a human speaker.
Dialogue Mgmt. Declarative Imperative YN-Question WH-Question
Dialogue 1 45 75 10 7 3
Dialogue 2 49 84 4 17 8
Dialogue 3 57 81 7 1 1
Figure 6: The number of types of speech acts in each of the three dialogues.
Dialogue Mgmt. Declarative Imperative YN-Question WH-Question
WSJ LM
und 4.92 4.34 3.83 4.39 4.78
nat 4.87 3.96 3.73 3.82 4.11
SB LM
und 4.63 4.33 4.03 4.31 4.89
nat 4.59 3.87 3.21 4.00 3.33
HUMAN
und 4.73 4.79 4.71 4.76 4.83
nat 4.74 4.41 4.32 4.51 4.83
Figure 7: Comparison of speech act scores of the S speaker. The numbers are averages over the evaluators? scores on
a 0-5 scale.
8 Acknowledgments
We give thanks to Lucian Galescu, Irene Langkilde-
Geary and Amanda Stent for helpful comments and sug-
gestions on previous drafts of this paper. This work was
supported in part by ONR grant 5-23236.
References
J. Allen, G. Ferguson, and A. Stent. 2001. An architec-
ture for more realistic conversational systems. In Pro-
ceedings of Intelligent User Interfaces 2001 (IUI-01),
Santa Fe, NM, January.
S. Bangalore and O. Rambow. 2000. Exploiting a proba-
bilistic hierarchical model for generation. In Proceed-
ings of the 18th International Conference on Computa-
tional Linguistics (COLING 2000), Saarbrucken, Ger-
many.
S. Bangalore, O. Rambow, and S. Whittaker. 2000. Eval-
uation metrics for generation. In Proceedings of the 1st
International Conference on Natural Language Gener-
ation (INLG 2000), Mitzpe Ramon, Israel.
C. Callaway. 2003. Evaluating coverage for large sym-
bolic nlg grammars. In IJCAI, pages 811?817, Aca-
pulco, Mexico.
J. Chen, S. Bangalore, O. Rambow, and M. Walker.
2002. Towards automatic generation of natural lan-
guage generation systems. In Proceedings of the 19th
International Conference on Computational Linguis-
tics (COLING 2002), Taipei, Taiwan.
M. Dzikovska, M. Swift, and J. Allen. 2003. Construct-
ing custom semantic representations from a generic
lexicon. In 5th International Workshop on Computa-
tional Semantics.
J. McDaniel J. Godfrey, E. Holliman. 1992. Switch-
board: Telephone speech corpus for research and de-
velopment. In ICASSP, pages 517?520, San Francisco,
CA.
K. Knight and I. Langkilde. 2000. Preserving ambigui-
ties in generation via automata intersection. In Amer-
ican Association for Artificial Intelligence conference
(AAAI).
I. Langkilde and K. Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proceed-
ings of the ACL/COLING-98, Montreal, Quebec.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator. In International Natural Language
Generation Conference (INLG).
A. Oh and A. Rudnicky. 2000. Stochastic language gen-
eration for spoken dialogue systems. In ANLP/NAACL
2000 Workshop on Conversational Systems, pages 27?
32, May.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Research Report RC22176, IBM, Septem-
ber.
O. Rambow, S. Bangalore, , and M. Walker. 2001. Nat-
ural language generation in dialog systems. In First
International Conference on Human Language Tech-
nology Research (HLT2001).
A. Ratnaparkhi. 2000. Trainable methods for surface
natural language generation. In 1st Meeting of the
North American Chapter of the Association of Com-
putational Linguistics (NAACL 2000).
A. Ratnaparkhi. 2002. Trainable approaches to surface
natural language generation and their application to
conversational dialog systems. Computer, Speech, &
Language.
E. Reiter. 1995. Nlg vs. templates. In Fifth European
Workshop on Natural-Language Generation.
A. Stent. 1999. Content planning and generation in
continuous-speech spoken dialog systems. In KI?99
workshop.
A. Stent. 2000. The monroe corpus. Research Report
728, Computer Science Dept., University of Rochester,
March. 99-2.
9 Appendix
7 right so we have bunch of roads that are out
*7 we have a bunch of roads that are out
8 a bunch of electric lines that are down
*8 bunch of electric lines that are down
9 and we need to fix them
*9 and we need to fix them
10 lets see
*10 let us see
11 and one of the electric lines is across the road and we need to fix that
*11 one electric lines are across the road and we need to fix that immediately
13 it is across
*13 it is across
14 its at the intersection of three eighty three and two fifty two a just
*14 it was at the intersection of three eighty three and two fifty two as
16 so
*16 so
18 yeah so i want so we need to send an electric crew
*18 yeah so we need to send electric crews
19 i guess theres only one set of electric crews
*19 i guess there is one set of electric crews
20 uh send them there to shut off the power
*20 send them the power to shut off in there
22 and that should take about twenty minutes
*22 twenty minutes minutes and that should take
23 um not going to worry about travel time perhaps
*23 perhaps we will not travel time worry
24 and then after that i would send the airport road crew to the same location
*24 i would send the airport crew fixed the road to the same location
28 i guess
*28 i guess
29 but they can shut off the power from an intersection
*29 they can shut the power of an intersection off
31 um before that
*31 before that
32 okay so thats one location
*32 okay so that is one location
33 and its going to take them four hours to fix the road
*33 and they will take four hours to fix the roads
35 and then after that we can send an electric crew to um restore the lines
*35 and then we can send electric crews to restore the lines
36 which takes two hours
*36 that takes two hours
38 six plus twenty minutes yeah
*38 six minutes plus twenty minutes
Figure 8: A comparison of the original human and our generated utterances, part of dialogue three in the Monroe
Corpus (just the S speaker). The starred numbers are inserted into the dialogue to provide a side by side comparison of
the quality of our generation. Starred utterances are generated by the approach described in this paper. The evaluators
did not receive such a dialogue. All human or all generation was presented as the S speaker to each evaluator.
Real-Time Stochastic Language Generation for Dialogue Systems
Nathanael Chambers
Florida Institute for Human and Machine Cognition
40 South Alcaniz Street
Pensacola, FL 32502
nchambers@ihmc.us
Abstract
This paper describes Acorn, a sentence planner and
surface realizer for dialogue systems. Improve-
ments to previous stochastic word-forest based ap-
proaches are described, countering recent criticism
of this class of algorithms for their slow speed.
An evaluation of the approach with semantic in-
put shows runtimes of a fraction of a second and
presents results that suggest it is also portable
across domains.
1 Introduction
This paper describes Acorn, a real-time sentence planner and
surface realizer for dialogue systems that is independent of
a specific domain. Acorn is based on a two-phased gram-
mar and stochastic approach, such as the HALogen system
[Langkilde-Geary, 2002], but offers several improvements to
make it more realistic for dialogue use. The first is to offer
an algorithm for trickle-down features that passes head/foot
features through the grammar as the initial word forest is cre-
ated, allowing the grammar to broadly represent phenomena
such as wh-movement. The second is to more tightly link the
grammar to a lexicon and represent syntactic properties such
as number, person, and tense to constrain the over-generation
process. Lastly, efficiency improvements are described which
further decrease the runtime of the system, allowing Acorn to
be used in a real-time dialogue context. It is named Acorn,
based on the word forests that are created and searched.
The task of Natural Language Generation is frequently
split into three somewhat disjoint steps: document planning,
microplanning (reference and sentence planning) and surface
realization. Document planning is a more reduced task in di-
alogue, mainly involving content determination since there is
no need for a document. Since the system follows a notion
of discourse, content determination is typically performed by
some reasoner external to generation, such as a Task Man-
ager. This paper addresses the sentence planning and sur-
face realization steps, assuming that content determination
and referential generation has already occurred and is rep-
resented in a high-level semantics.
This stochastic approach involves two phases; the first uses
a grammar to over-generate the possible realizations of an
input form into a word forest, and the second uses a lan-
guage model to choose the preferred path through the forest.
This approach is attractive to dialogue systems because it of-
fers flexibility and adaptivity that cannot be achieved through
most symbolic systems. By over-generating possible utter-
ances, the (sometimes dynamic) language models can decide
which is more natural in the current context. Other advan-
tages include domain independence and an under-specified
input. The main disadvantages most often cited include a
very slow runtime and the inability to capture complex lin-
guistic constraints, such as wh-movement. The latter is a side
effect of the word-forest creation algorithm and a solution to
broaden the coverage of language is presented in this paper.
The issue of runtime is critical to dialogue.
Slow runtime is a two-fold problem: the word-forest that
is generated is extremely large and often not linguistically
constrained, and second, the algorithm has not been effi-
ciently implemented. These issues must be addressed be-
fore stochastic approaches can be suited for dialogue. Langk-
ilde [Langkilde, 2000] provides an evaluation of coverage of
HALogen and shows runtimes around 28 seconds for sen-
tences with average lengths of 22 words. Callaway [Call-
away, 2003] later commented on the runtime that HALogen is
anywhere from 6.5 to 16 times slower than the symbolic real-
izer FUF/SURGE (which may also be too slow for dialogue).
This paper shows that more work can be done in stochastic
generation to reduce the runtime by constraining the gram-
mar and making simple algorithm improvements. Runtimes
of only a fraction of one second are presented.
The next section provides a brief background on stochas-
tic generation, followed by a description of Acorn in section
3. The description presents several new grammar additions to
broaden language coverage, including a mechanism, called
trickle-down features, for representing head and foot features
in the grammar. Section 4 describes the evaluation of Acorn,
as well as the results concerning domain independence and
the overall runtime. A brief discussion and related work fol-
lows the evaluation.
2 Background
The task of Content Determination is typically relegated to a
module outside of the Generation component, such as with
a Task Manager or other reasoning components. This leaves
the tasks of Sentence Planning and Surface Realization as the
main steps in dialogue generation, and this paper is describing
a module that performs both. The task of referential genera-
tion is not addressed, and it is assumed that each logical input
is a single utterance, thus removing the need for multiple sen-
tence generation.
Traditionally, surface realization has been performed
through templates or more complex syntactic grammars, such
as the FUF/SURGE system [Elhadad and Robin, 1996].
Template-based approaches produce inflexible output that
must be changed in every new domain to which the system is
ported. Symbolic approaches produce linguistically correct
utterances, but require a syntactic input and typically have
runtimes that are impractical for dialogue. Requiring word
choice to be finished beforehand, including most syntactic de-
cisions, puts a heavy burden on dialogue system designers.
Stochastic approaches have recently provided a new
method of reducing the need for syntactic input and produce
flexible generation in dialogue. HALogen [Langkilde-Geary,
2002] was one of the first stochastic generation systems, pro-
viding a two-phased approach that allowed the system de-
signer to use an under-specified input. The first phase uses
a hand written grammar that over-generates possible word
orderings into a word forest. The second phase uses an n-
gram language model to choose the highest probability path
through the forest, returning this path as the generated sen-
tence. This approach was first used in a dialogue system in
[Chambers and Allen, 2004] as an attempt to create a domain
independent surface realizer. A human evaluation showed a
slight decline in naturalness when moved to a new domain.
The stochastic approach was shown in [Langkilde, 2000] to
produce good coverage of the Penn Treebank, but its runtime
was significantly slow and others have suggested the stochas-
tic approach is not feasible for dialogue.
3 Acorn: System Description
3.1 Input Form
The input to Acorn is a semantic feature-value form rooted on
the type of speech act. On the top level, the :speechact feature
gives the type (i.e. sa tell, sa yn-question, sa accept, etc.),
the :terms feature gives the list of semantic, content bear-
ing terms, and the :root feature gives the variable of the root
term in the utterance. Other features are allowed and often
required, such as a :focus for wh-questions. Each term in the
:terms list is a feature-value structure based on thematic roles,
as used in many other representations (e.g. Verbnet [Kipper
et al, 2000]). This utterance input is a syntactically modified
version of the domain independent Logical Form described in
[Dzikovska et al, 2003].
Each term is specified by the features: :indicator, :class,
optional :lex, and any other relevant thematic roles (e.g.
:agent, :theme, etc.). The :indicator indicates the type or
function of the term and takes the values THE, A, F, PRO, and
QUANTITY-TERM. THE represents a grounded object in the
discourse, A represents an abstract object, F is a functional
operator, PRO is used for references, and QUANTITY-TERM
represents quantities expressed in various scales. There are
other indicators, but the details are beyond the scope of this
paper. The :class specifies the semantic class of the term, and
<UTTERANCE> ::=
(utt :speechact <act> :root <variable>
<FEATURE-VALUE>*
:terms (<TERM>*))
<TERM> ::=
(<variable> :indicator <indicator>
:class <class> <FEATURE-VALUE>*)
<FEATURE-VALUE> ::=
<keyword> <value>
Figure 1: BNF for the input to Acorn. A keyword is a symbol
preceded by a colon, and a value is any valid symbol, variable,
or list.
(utt :speechact sa tell :root v8069 :terms
((v8324 :indicator speechact :class sa tell :content v8069)
(v8069 :indicator f :class want :lex want :theme v8173
:experiencer v7970 :tense present)
(v7970 :indicator pro :class person :lex i :context-rel i)
(v8173 :indicator a :class computer :lex computer
:assoc-with v8161)
(v8161 :indicator quantity-term :class speed-unit
:lex gigahertz :quantity v8225)
(v8225 :indicator quantity-term :class number :value 2.4)))
Figure 2: Input to Acorn for the utterance, ?I want a 2.4 ghz
computer.? This input provides the lexical items for the utter-
ance, but these are typically absent in most cases.
the :lex is the root lexical item for the term. Lex is an optional
feature and is created from the :class if it is not present in the
input. Figure 1 gives the specification of the input, and fig-
ure 2 shows an example input to Acorn for the utterance, ?I
want a 2.4 gigahertz computer?. Appendix A provides further
examples of both semantic and lexical inputs.
3.2 Grammar Rules
The grammar rules in Acorn convert the input utterance into
word orderings by matching keywords (features) in each
term. A unique aspect of Acorn is that the utterance level
features can also be matched at any time. It is often neces-
sary to write a rule based on the current speech act type. The
left-hand side (LHS) of a rule showing both options is given
here:
(grule focus
(:subject ?s)
:g (:speechact ?act sa_tell)
>>
...)
Each rule matches keywords in its LHS to the current term
and binds the values of the keywords in the term to the vari-
ables in the LHS. In the above example, the variable ?s would
be bound to the subject of the term, and the variable ?act is
bound to the top-level :speechact value. A LHS element that
is preceded by the :g symbol indicates a top-level (global)
feature. In this example, the value sa tell is also specified as
a requirement before the rule can match.
When matched, the right-hand side (RHS) offers several
different options of processing. As in HALogen, the recast-
ing (changing a keyword to a new keyword, such as convert-
ing a semantic role into a syntactic one), substitution (remov-
ing a keyword and its value, or just changing its value), and
ordering rules (specifying phrasal and word-level ordering in
the word forest) are supported. Two additional rules are sup-
ported in Acorn that are able to handle wh-movement and
other head features. The first is called empty-creation and its
complement is filling. In order to effectively use these rules,
a method of passing head and/or foot features is needed. The
following describes trickle-down features, followed by a de-
scription of the empty-creation and filling rules.
Trickle-Down Features
A drawback of the grammar phase is that all features in the
terms must be explicitly coded in the rules, otherwise they are
discarded when ordering rules are applied. Using a simple
example of subject-object placement, the following ordering
rule places the subject in front of the verb, and the object
behind.
(grule subject
(:subject ?s)
(:object ?o)
>>
(-> (?s) (?rest) (?o)))
Three new branches are created in the forest, one each for
(?s), (?rest), and (?o). This rule creates a branch in the word
forest that is a conjunct of three non-terminal nodes:
N3 -> N4 N5 N6
Processing of the (?s) and (?o) branches is restarted at the top
of the grammar, but they do not contain any features (the ?rest
variable is a catch-all variable that represents all features not
matched in the LHS). Indeed, it is possible to write rules with
a list of optional features, and include them in the RHS:
(grule subject
(:subject ?s)
(:object ?o)
&keep &optional
(:gap ?g)
>>
(-> (?s :gap ?g) (?rest) (?o :gap ?g)))
However, this quickly leads to bloated rules and can slow
the matching procedure considerably. It is very intuitive to
keep features like head and foot features hidden from the
grammar writer as much as possible. This is accomplished
through what we are calling trickle-down features. The syn-
tax for these special case features includes an asterisk before
the name, as in :*gap. The result of using these features is to
get the effect of the latter rule with the ease of use in the for-
mer rule. It essentially trickles down the features until their
appropriate place in the input utterance is found. Figure 3
shows the feature ?searching? for its correct path. One use of
this is shown in the following examples of the empty-creation
and filling rules.
G4
*gap G4
*gap G4
*gap G4
*gap G4
"now""did" "buy""they" "what"
*gap G4
*gap G4
*gap G4
1
2
3 4
5 6 8
7
Figure 3: A graphical representation of trickle down features.
The gap head feature can be seen percolating to each node,
finding its true path (1->2->6) to the wh-term what, and link-
ing the filler with the gap (6->G4).
Empty-Creation
When building the word forest, we often need to create a
gap node that will be filled later by movement phenomena,
such as in wh-questions. The content of the node may not
be known, but through empty-creation, we can instantiate a
variable and link it to the current location in the word forest.
This variable can then be attached to a special trickle-down
feature which is implicitly passed through the grammar. The
following is an example of an empty-creation rule:
(grule wh-question
(:root ?r +) ;; root term
:g (:speechact ?s sa_wh-question)
((gentemp "?GAP") ?wh-gap)
>>
(g-> ?wh-gap)
(-> ?wh-gap (?rest :*gap ?wh-gap)))
The first half of the RHS (the g-> rule) creates a global
variable and binds a new word forest node label to it. This
label is then used in the second half of the RHS where the
node is inserted into the word forest, and as of now, is empty.
The variable is then passed as a trickle-down feature :*gap to
the current term using the ?rest catch-all variable. This rule
is applied to node 1 in figure 3, creating gap node G4 and the
?rest node 2, passing the :*gap through the forest.
Filling
Filling rules perform the wh-movement needed in wh-
questions and many complement structures. Filling in the
context of Acorn can be seen as binding a gap variable that
has already been created through an empty-creation rule. The
following is an example filling rule that completes the above
wh-gap example.
(grule gap-fill
(:indicator ?i wh-term)
(:*gap ?gap)
>>
(b-> ?gap (?rest)))
This rule checks that the current term is a wh-term that has
a gap head feature. The RHS (the b-> rule) binds the current
term to the gap that has already been created, filling the empty
node in the word forest. The Filling rule essentially grafts a
branch onto a location in the word forest that has previously
been created by an Empty-Creation rule. The dotted line in
figure 3 is created by such a Filling rule.
3.3 Grammar Over-Generation
One of the main attractions of the two-phased approach is
that the grammar in the first phase can be left linguistically
unconstrained and over-generates many possibilities for an
input utterance. However, the statistical second phase may
then be over-burdened with the task of searching it. The con-
verse problem arises when the first stage is too constrained
and does not produce enough realizations to be natural and
flexible, perhaps removing the need for a stochastic phase en-
tirely. There needs to be a balance between the two stages.
The processing time is also critical in that over-generation
can take too much time to be useful for dialogue.
The grammar used in HALogen largely relied on the over-
generation first phase to ensure full coverage of the output.
It also reduced the number of rules in the grammar. Subject-
verb agreement was loosely enforced, particularly with sub-
ject number. Also, singular and plural nouns were both gener-
ated when the input was unspecified, doubling the size of the
noun phrase possibilities. One of the biggest over-generations
was in morphology. HALogen has its own morphology gen-
erator that relies on over-generating algorithms rather than
a lexicon to morph words. The typical word forest then
contains many unknown words that are ignored during the
stochastic search, but which explode the size of the word for-
est. Lastly, modifiers are over-generated to appear both in
front of and behind the head words.
Our approach removes the above over-generation and links
a lexicon to the grammar for morphology. Subject-verb
agreement is enforced where possible without dramatically
increasing the grammar size, nouns are only made plural
when the input specifies so (under the assumption that the
input would contain such semantically critical information),
and modifiers are placed in specific locations on certain
phrases (i.e. adjectives are always premodifiers for nouns,
complements of infinitive verbs are postmodifiers, etc.).
These changes greatly reduce the runtime of the first phase
and directly affect the runtime of the second phase by creating
smaller word forests.
3.4 Algorithm
Forest Creation
Word forest creation begins with the input utterance, such as
the one in figure 2. The top level utterance features are stored
in a global feature list, easily accessed by the grammar rules
if need be. The :root feature points to the root semantic term
given in the list of :terms. This root term is then processed,
beginning at the top of the grammar.
The grammar is pre-processed and each rule is indexed in
a hash table of features according to the least popular feature
in the rule. For example, if a rule has two features, :theme
and :agent, and :agent only appears in 8 rules while :theme
(v87 :indicator f :class purchase
        :subject v91 :object v93)
(v91 ... :position subject) (v87 :indicator f :class purchase) (v93 ... :position object)
"she" "bought" "computer""a"
Figure 4: A word forest created from the Acorn grammar.
appears in 14, the rule will be added to the list of rules in
the :agent bin. During processing of an input term, all of
the term?s features are extracted and the rules under each fea-
ture in the hash table are merged into an ordered subset of
the full grammar. This process differs from HALogen and
its successors by vastly limiting the number of rules that are
checked against each input. Instead of checking 250 rules, we
may only check the relevant 20 rules. After a grammar rule
matches, the index is queried again with the new term(s) from
the RHS of the rule. A new subset of the grammar is created
and used to continue processing through the grammar.
RHS expansions create (1) ordering constraints, (2) new
branches, and (3) feature modifications to the current term.
Options (1) and (2) are typically done with ordering rules
such as the following RHS:
(-> (?s :position subject)
(?rest) (?o :position object))
The variables are either bound from the LHS conditions, or
are unbound (conditions that follow the &optional indicator
in the LHS) and ignored during RHS expansion. The ?rest
variable is a special case variable which refers to the current
term and its features that do not appear in the LHS (by default,
features in the LHS that are matched are removed from the
term, unless they follow a &keep indicator). In the above
example, there will be a new conjunction branch with three
child nodes in the word forest, as shown in figure 4.
When this rule is matched, the ?s node will bind its vari-
able that must point to one of the terms in the input utterance?s
:terms list. Processing will now begin with that term, attach-
ing any features in the RHS to it (in this example, :position
subject), at the top of the grammar. Once completed, pro-
cessing will continue with the current term (?rest) until the
grammar is exhausted. Finally, the third term (?o ...) will be-
gin at the top of the grammar. As discussed in section 3.2,
any trickle-down features in the current term are appended to
the three terms when processing begins/continues on each of
them.
A term attempts to match each rule in the grammar until
a RHS creates a leaf node. This is accomplished by a RHS
expansion into an initial atom that is a string. Finally, inline
functions are allowed to be used in the grammar. The follow-
ing example calls the function stringify and its returned value
is bound to the ?str variable. These calls are typically used to
access the lexicon.
(grule stringify
(:lex ?lex)
;; convert lexical item to string
((stringify ?lex) ?str)
&optional
(:cat ?cat)
>>
(-> (?str :cat ?cat)))
PathFinder
The PathFinder module of Acorn is the second stage, respon-
sible for determining the most likely path through the forest.
In this stage, the hypotheses from the grammar are analyzed
and the top word ordering is chosen based on n-gram stochas-
tic models derived from corpora.
The algorithm we implemented in PathFinder is largely the
same as the one described in [Langkilde, 2000]. It is a dy-
namic programming algorithm that stores the top m phrases
at each decision point based on the leading and trailing words
in the phrase. When dealing with n-grams, we only need
to keep track of the first n ? 1 and the last n ? 1 words in
each phrase. Our approach not only tracks these features as
Langkilde calls them, but PathFinder also sorts the top m
phrases and prunes any duplicates. Pruning duplicates of-
fers an advantage in runtime when the phrases are merged
with neighboring phrases. The complexity analysis is still
O(m?m) = O(m2), but in practice, pruning phrases reduces
the number of phrases to some number less than m.
The largest change to the algorithm is that we added dy-
namic interpolation of language models. PathFinder can load
any number of models and interpolate them together during
n-gram analysis using an input set of weights. PathFinder
also has the capability to use feature-based models and word
history models.
Feature models, such as part of speech n-grams, model the
features1 of forest leaves instead of the lexical items. The
Forest Creation stage is able to output features in addition to
lexical items, as seen in the RHS of this forest leaf:
N6 :POS NN :BASE COMPUTER -> "COMPUTERS"
There are two ?features? on this leaf, pos and base. Parame-
ters can be passed to PathFinder that command it to use the
features instead of the RHS string when applying a language
model to the forest. This option is not evaluated in this paper,
but is a promising option for future work.
Word history models keep track of the current discourse
and monitor word usage, providing a history of word choice
and calculating a unigram probability for each word. The
PathFinder is updated on each utterance in the dialogue and
applies a decaying word history approach, similar to the work
in [Clarkson and Robinson, 1997]. This model is not evalu-
ated in this paper, but is useful in portraying the breadth of
coverage that a stochastic phase can provide to dialogue.
1Here we refer to features in the grammar phase, as in feature-
values. These are not to be confused with the features of Langkilde
in the forest search phase.
:action :co-theme :property
:addressee :cognizer :purpose
:affected :compared-to :rank
:agent :cost :result
:along-with :effect :sit-val
:associated :entity :state
:attribute :event-relative :theme
:beneficiary :experiencer :time-duration-rel
:cause :of :value
:center :patient
Figure 5: The main semantic features in Acorn?s grammar.
4 Evaluation
The three factors that are most important in evaluating dia-
logue generation is portability, coverage, and speed. Other
factors include naturalness, flexibility, and many more, but
the above three are evaluated in this paper to address con-
cerns of domain independent generation and real-time dia-
logue. During one?s efforts to address the latter concern by
constraining the size of the word forest, it is very easy to lose
the former.
4.1 The Grammar
Acorn?s grammar contains 189 rules and is heavily seman-
tic based, although the semantic features and concepts are
transformed into syntactic features before word ordering is
decided. It is possible to input a syntactic utterance, but this
evaluation is only concerned with semantic input. The gram-
mar was created within the context of a computer purchasing
domain in which the dialogue system is a collaborative assis-
tant that helps the user define and purchase a computer. We
had a corpus of 216 utterances from developers of the system
who created their own mock dialogues. The grammar was
constructed mainly based on these parsed utterances. Other
domains such as an underwater robotic mine search and a
database query interface were used to represent as many se-
mantic roles as possible. The list of the main semantic fea-
tures in Acorn?s grammar is provided in figure 5.
4.2 Evaluation Methodology
Each utterance that was able to be parsed in our target dia-
logues was automatically transformed into the input syntax of
Acorn. These inputs were pushed through Acorn, resulting in
a single, top ranked utterance. This utterance was compared
to the target utterance using the Generation String Accuracy
metric. This metric compares a target string to the generated
string and counts the number of word movements (M), sub-
stitutions (S), deletions (D), and insertions (I) (not counting
deletions and insertions implicitly included in movements).
The metric is given below (L is the number of tokens in the
target string):
1?
M + I +D + S
L
(1)
Before comparison, all contractions were split into single
lexical items to prevent the metric from penalizing seman-
tically similar phrases (e.g. aren?t to are not). The Simple
Utterance Lengths
number of words 1-2 3-5 6-9 10-
Number of utterances 661 177 109 39
Figure 6: Number of utterances of each word length. The ma-
jority are grounding/acknowledgements (661 utterances out
of 986). We only evaluated those of length 3 or more, 325
utterances.
String Accuracy metric was also applied to provide compar-
ison against studies that may not use the Generation Metric;
however, the Generation Metric intuitively repairs some of
the former?s failings, namely double penalization for word
movement. More on these and other metrics can be found in
[Bangalore et al, 2000].
4.3 Domain Independent Evaluation
Acorn was evaluated using the Monroe Corpus [Stent, 2000],
a collection of 20 dialogues. Each dialogue is a conversation
between two English speakers who were given a map of Mon-
roe County, NY and a description of a task that needed to be
solved. There were eight different disaster scenarios ranging
from a bomb attack to a broken leg, and the participants were
to act as emergency dispatchers. It is a significantly different
domain from computer purchasing and was chosen because
it offers a corpus that has been parsed by our parser and thus
has readily available logical forms for input to Acorn. The
length of utterances are shown in figure 6.
The four dialogues that had most recently been updated
to our logical form definitions were chosen for the evalua-
tion. The remaining sixteen are used by PathFinder as a bi-
gram language model of the domain?s dialogue. Two series
of tests were run. The first includes the lexical items as input
to Acorn and the second only includes the ontology concepts.
Generation String Accuracy is used to judge the output of
the system against the original utterances in the Monroe dia-
logues. While there have been other generation metrics that
have been proposed, such as the Bleu Metric [Papineni et al,
2001], the Generation String Accuracy metric still provides
a measure of system improvement and a comparison against
other systems. Bleu requires more than one correct output
option to be of worthwhile (?quantity leads to quality?), so is
not as applicable with only one target utterance.
4.4 Domain Specific Evaluation
In order to compare the domain independent evaluation with
a domain specific evaluation, the same evaluation described
in 4.2 was used on the computer purchasing corpus that in-
cludes the logical forms on which Acorn?s grammar is based.
As described in 4.1, the domain is an assistant that collabora-
tively purchases computers online for the user. There are 132
utterances of length three or more in this corpus. The n-gram
models were automatically generated using a hand formed
word grammar of sample sentences. Both Simple and Gen-
eration String Accuracy were used to compare the output of
Acorn to the target utterances in the corpus.
Domain Independent: Monroe Rescue
Simple String Accuracy
Baseline Random Path Final
Lexical Items 0.28 0.55 0.67
Semantic Concepts N/A 0.38 0.59
Generation String Accuracy
Baseline Random Path Final
Lexical Items 0.28 0.59 0.70
Semantic Concepts N/A 0.40 0.62
Figure 7: The Simple and Generation String Accuracy results
of Acorn in the Monroe domain. The two baseline metrics
and the final Acorn scores are given.
4.5 Baselines
Two baselines were included in the evaluation as compara-
tive measures. The first is named simply, baseline, and is a
random ordering of the lexical inputs to Acorn. Instead of
using a grammar to choose the ordering of the input lexical
items, the baseline is a simple procedure which traverses the
input terms, outputting each lexical item as it comes across
them. When there are multiple modifiers on a term, the order
of which to follow first is randomly chosen. This baseline is
only run when lexical items are provided in the input.
The second baseline is called Random Path and serves as
a baseline before the second phase of Acorn. A random path
through the resulting word forest of the first phase of Acorn
is extracted and compared against the target utterance. This
allows us to evaluate the usefulness of the second stochas-
tic phase. Both these baselines are included in the following
results.
4.6 Results
Two different tests were performed. The first included lexical
choice in the input utterances and the second included only
the ontology concepts. The accuracy scores for the Monroe
domain are shown in figure 7. A semantic input with all lex-
ical items specified scored an average of 0.70 (or 70%) on
325 input utterances. A purely semantic input with just the
ontology classes scored 0.62 (or 62%).
The results from Acorn in the Computer Purchasing Do-
main are shown in figure 8. Both the semantic and lexical
evaluations were run, resulting in an average score of 0.85
(85%) and 0.69 (69%) respectively.
In order to judge usefulness for a real-time dialogue sys-
tem, the runtime for both phases of Acorn was recorded for
each utterance. We also ran HALogen for comparison. Since
its grammar is significantly different from Acorn?s, the out-
put from HALogen is not relevant since little time was spent
in conforming its grammar to our logical form; however, the
runtimes are useful for comparison. The times for both Acorn
and HALogen are shown in figure 9. With a purely seman-
tic input, Acorn took 0.16 seconds to build a forest and 0.21
seconds to rank it for a total time of 0.37 seconds. HALogen
took a total time of 19.29 seconds. HALogen runs quicker
when lexical choice is performed ahead of time, finishing in
Domain Specific: Computer Purchasing
Simple String Accuracy
Baseline Random Path Final
Lexical Items 0.28 0.66 0.82
Semantic Concepts N/A 0.47 0.67
Generation String Accuracy
Baseline Random Path Final
Lexical Items 0.28 0.69 0.85
Semantic Concepts N/A 0.49 0.69
Figure 8: The Simple and Generation String Accuracy results
of Acorn in the Monroe domain. The two baseline metrics
and the final Acorn scores are given.
System Runtime
Build Rank Total Runtime
Acorn Lexical 0.06s 0.00s 0.06s
Acorn Semantic 0.16s 0.21s 0.37s
HALogen Lexical 2.26s 0.47s 2.73s
HALogen Semantic 11.51s 7.78s 19.29s
Figure 9: A comparison of runtimes (in seconds) between
Acorn and HALogen. Both the lexical item and the semantic
concept input are shown.
2.73 seconds. The reason is mainly due to its over-generation
of noun plurals, verb person and number, and morphology.
Finally, the runtime improvement of using the grammar
rule indexing algorithm was analyzed. All utterances of word
length five or more with correct parses were chosen from the
dialogues to create forests of sufficient size, resulting in 192
tests. Figure 10 shows the average forest building time with
the indexing algorithm versus the old approach of checking
each grammar rule individually. A 30% improvement was
achieved.
5 Discussion
While it is difficult to quantify, the implementation of trickle-
down features and Empty-Creation and Filling rules accom-
modate well the construction of a grammar that can capture
head/foot features. The forest creation algorithm of HALo-
gen and others is much too cumbersome to implement within,
and representing lexical movement is impossible without it.
The above result of 62% coverage in a new domain is com-
Build Forest Runtime
Normal Grammar 0.30s
Indexed Grammar 0.21s
% Improvement 30%
Figure 10: Runtimes of a sequential grammar rule search for
matching rules versus the rule indexing approach described
in this paper. The average runtime for 192 word forests is
shown.
parable, and arguably better than those given in Langkilde
[Langkilde-Geary, 2002]. This paper uses a semantic ut-
terance input which is most similar to the Min spec test of
Langkilde. The Min spec actually included both the lexical
choice and the surface syntactic roles (such as logical-subject,
instead of theme or agent), resulting in a Simple String Ac-
curacy of 55.3%. Acorn?s input is even more abstract by only
including the semantic roles. Its lexical input, most similar
to the Min spec, but still more abstract with thematic roles,
received 70%. This comparison should only be taken at face
value since dialogue utterances are shorter than the WSJ, but
it provides assurance that a constrained grammar can produce
good output even with a more abstract input. It must also be
noted that the String Accuracy approaches do not take into ac-
count synonyms and paraphrases that are semantically equiv-
alent.
These results also evaluate the amount of effect the
stochastic phase of this approach has on the overall results.
Figure 7 shows that the average random path through the
word forest (the result of the first grammar-based phase) was
only 0.40 (40%). After PathFinder chooses the most prob-
able path, the average is 0.62 (62%). We can conclude that
the grammar is still over-generating possible realizations and
that this approach does require the second stochastic phase to
choose a realization based on previously seen corpora.
The difference between the results in the known domain
(computer purchasing) and the new domain (monroe rescue)
is 85% to 70% (69% to 62% without lexical items). While
the difference is too great to claim domain independence on
a semantic input, one of the main advantages of the over-
generation grammar is that it requires less work to construct a
new grammar when domains are switched. Here we see 70%
achieved for zero invested time. A study that analyzes the
time it takes a programmer to reach 85% has yet to be done.
The runtime improvement of our approach is more drastic
than originally thought possible. An average runtime of 0.37
seconds is decidedly within the time constraints of an effec-
tive dialogue system. While the 30% improvement in gram-
mar indexing is also significant, the larger gains appear to
be results of finer morphology and person/number agreement
between verbs and their subjects. Compared with 19.29 sec-
onds of the previous implementation, it shows that a middle
ground between over-generation and statistical determination
is a viable solution.
Finally, more work is needed to produce better output. The
majority of errors in this approach are modifier placement
choices. Without a formal grammar, the final placement de-
cisions are ultimately decided by an n-gram language model,
resulting in short-sighted decisions. Even though 85% from
a semantic input is a good result, modifiers tend to be the one
area that falls behind. Several examples of this can be seen in
Appendix B where some poor generations are shown.
6 Related Work
Stochastic work on the FERGUS system [Chen et al, 2002]
uses a TAG grammar to produce a word lattice of possible
realizations. The lattice is traversed to find the most likely
path. The work in [Chen et al, 2002] generated sentences in
0.28 seconds for an Air-Travel Domain. This paper differs
in that the input to FERGUS is a shallow syntactic tree, con-
taining all lexemes and function words. In addition, surface
syntax trees were mapped one-to-one with each template in
the Air-Travel domain. There was little, if any, flexibility in
the semantic input. This paper presents a result of 0.37 sec-
onds that includes both the sentence planner, surface realizer,
and a grammar that generates multiple realizations based on
both syntax and semantics.
Work was done on the Oxygen system [Habash, 2000] to
improve the speed of the two-phased Nitrogen generator, a
predecessor to HALogen. The work pre-compiled a declar-
ative grammar into a functional program, thus removing the
need to match rules during forest creation. This paper differs
in that similar performance was achieved without the need
for pre-compiling nor a more complex grammar syntax. This
paper also described lexical movement and trickle down fea-
tures not supported in Oxygen.
Chambers [Chambers and Allen, 2004] used HALogen in
a dialogue system and performed a human evaluation of the
mixed syntax/semantic input. Their input converted their do-
main independent logical form into the HALogen input. This
work differs in that we obviously did not use the HALo-
gen system, but implemented a more efficient two-phased ap-
proach. The work by Chambers and Allen did not analyze
runtime, perform sentence planning (not a full semantic in-
put), nor provide results from the common String Accuracy
metrics for comparison to other approaches.
7 Conclusion
Stochastic approaches to natural language processing are of-
ten criticized for being too slow, particularly in recent at-
tempts in language generation. This paper describes Acorn,
a system that generates dialogue utterances in an average
of 0.37 seconds. The approach and its additional advances
in word forest creation were described, such as a technique
called trickle-down features that allow a grammar to pass
head/foot features through a generation input, enabling lan-
guage phenomena such as wh-movement to be represented.
The grammar syntax and an evaluation of the coverage in an
unknown domain were presented. The coverage is compa-
rable and the runtime drastically out-performs previous ap-
proaches.
A Example Semantic and Lexical Input
Below is an example utterance from the Monroe corpus and
its purely semantic and lexical input to Acorn. In this exam-
ple, only the words have, helicopter, and Strong Memorial are
absent in the semantic input. The resulting generation output
from Acorn is also shown.
Original utterance:
?and i also have a helicopter at strong memorial?
Semantic Input to Acorn:
((utt :speechact sa tell :mods v05 :saterm v88 :terms
((v88 :indicator speechact :class sa tell :content v27
:mods v05)
(v05 :indicator f :class conjunct :lex and :of v88)
(v27 :indicator f :class have :co-theme v63 :theme v09
:mods v64 :mods v23 :tense present)
(v09 :indicator pro :class person :context-rel i)
(v23 :indicator f :class additive :lex also :of v27)
(v63 :indicator a :class air-vehicle)
(v64 :indicator f :class spatial-loc :lex at :of v27 :val v75)
(v75 :indicator the :class facility
:name-of (strong memorial)))))
Lexical Input to Acorn:
((utt :speechact sa tell :mods v05 :saterm v88 :terms
((v88 :indicator speechact :class sa tell :content v27
:mods v05)
(v05 :indicator f :class conjunct :lex and :of v88)
(v27 :indicator f :class have :lex have :co-theme v63
:theme v09 :mods v64 :mods v23 :tense present)
(v09 :indicator pro :class person :lex i :context-rel i)
(v23 :indicator f :class additive :lex also :of v27)
(v63 :indicator a :class air-vehicle :lex helicopter)
(v64 :indicator f :class spatial-loc :lex at :of v27 :val v75)
(v75 :indicator the :class facility :lex strong-memorial
:name-of (strong memorial)))))
Acorn Generation:
?and i have a helicopter also at strong memorial?
B Example Poor Output
Below are some target and generated utterances from Acorn,
illustrating several common errors, and are not examples of
success. The first utterance is the real target one, and the
second is the Acorn generated utterance.
1. ?i think i have a disability with maps?
?i think i have disability with maps?
2. ?they should have stayed in front of the tv?
?in a front of the tv should stay they?
3. ?and i also have a helicopter at strong memorial?
?and i have a helicopter also at strong memorial?
4. ?i can?t see it on the map?
?i can not on the map see it?
5. ?probably all of them are hospitals?
?probably hospitals are all them?
6. ?are you talking to me?
?are you talking me?
7. ?and there are three people on a stretcher at the airport ?
?and three people on a stretcher are at the airport?
8. ?then there?s one stretcher patient at the mall?
?then stretcher one patient is at the mall?
9. ?so that guy should just walk to the hospital?
?so that guy should walk to the hospital just?
10. ?i think that?s a very good plan?
?i think that is very good plan?
C Example Good Output
Below are a list of target utterances that Acorn matched ex-
actly, word for word. It is obviously not a complete list.
1. ?i?m not doing this on purpose?
2. ?we can bring it to strong memorial?
3. ?it?s on elmwood and mount hope ?
4. ?so the heart attack person can?t go there?
5. ?and bring them to saint mary?s?
6. ?do you have any suggestions??
7. ?we can put him in one ambulance?
8. ?because we have only six wounded?
9. ?i think that?s a good idea?
10. ?and the other one is at the airport?
11. ?what can i say??
References
[Bangalore et al, 2000] Srinivas Bangalore, Owen Rambow,
and Steve Whittaker. Evaluation metrics for generation. In
INLG, Saarbrucken, Germany, August 2000.
[Callaway, 2003] Charles Callaway. Evaluating coverage for
large symbolic nlg grammars. In IJCAI, Acapulco, Mex-
ico, August 2003.
[Chambers and Allen, 2004] Nathanael Chambers and
James Allen. Stochastic language generation in a dialogue
system: Toward a domain independent generator. In
Proceedings of the 5th SIGdial Workshop on Discourse
and Dialogue, Boston, USA, May 2004.
[Chen et al, 2002] John Chen, Srinivas Bangalore, Owen
Rambow, and Marilyn A. Walker. Towards automatic gen-
eration of nautral language generation systems. In COL-
ING, Taipei, Taiwan, 2002.
[Clarkson and Robinson, 1997] P.R. Clarkson and A.J.
Robinson. Language model adaptation using mixtures
and an exponentially decaying cache. In Proceedings of
ICASSP-97, pages II:799?802, 1997.
[Dzikovska et al, 2003] M. Dzikovska, M. Swift, and
J. Allen. Constructing custom semantic representations
from a generic lexicon. In 5th International Workshop on
Computational Semantics, 2003.
[Elhadad and Robin, 1996] M. Elhadad and J. Robin. An
overview of surge: A reusable comprehensive syntactic
realization component. Tech Report 96-03, Ben Gurion
University, Beer Sheva, Israel, 1996.
[Habash, 2000] Nizar Habash. Oxygen: A language inde-
pendent linearization engine. In AMTA-2000, Cuernavaca,
Mexico, October 2000.
[Kipper et al, 2000] Karin Kipper, Hoa Trang Dang, and
Martha Palmer. Class-based construction of a verb lexi-
con. In Proceedings of the 17th National Conference on
Artificial Intelligence, Austin, TX, 2000.
[Langkilde-Geary, 2002] Irene Langkilde-Geary. An empir-
ical verification of coverage and correctness for a general-
purpose sentence generator. In INLG, New York, 2002.
[Langkilde, 2000] Irene Langkilde. Forest-based statistical
sentence generation. In NAACL, 2000.
[Papineni et al, 2001] K. Papineni, S. Roukos, T. Ward, and
W. Zhu. Bleu: a method for automatic evaluation of
machine translation. Research Report RC22176, IBM,
September 2001.
[Stent, 2000] A. Stent. The monroe corpus. Research Report
728, Computer Science Dept., University of Rochester,
March 2000. 99-2.
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1797?1807,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Event Schema Induction with a Probabilistic Entity-Driven Model
Nathanael Chambers
United States Naval Academy
Annapolis, MD 21402
nchamber@usna.edu
Abstract
Event schema induction is the task of learning
high-level representations of complex events
(e.g., a bombing) and their entity roles (e.g.,
perpetrator and victim) from unlabeled text.
Event schemas have important connections to
early NLP research on frames and scripts,
as well as modern applications like template
extraction. Recent research suggests event
schemas can be learned from raw text. In-
spired by a pipelined learner based on named
entity coreference, this paper presents the first
generative model for schema induction that in-
tegrates coreference chains into learning. Our
generative model is conceptually simpler than
the pipelined approach and requires far less
training data. It also provides an interesting
contrast with a recent HMM-based model. We
evaluate on a common dataset for template
schema extraction. Our generative model
matches the pipeline?s performance, and out-
performs the HMM by 7 F1 points (20%).
1 Introduction
Early research in language understanding focused
on high-level semantic representations to drive their
models. Many proposals, such as frames and scripts,
used rich event schemas to model the situations de-
scribed in text. While the field has since focused on
more shallow approaches, recent work on schema
induction shows that event schemas might be learn-
able from raw text. This paper continues the trend,
addressing the question, can event schemas be in-
duced from raw text without prior knowledge? We
present a new generative model for event schemas,
and it produces state-of-the-art induction results, in-
cluding a 7 F1 point gain over a different generative
proposal developed in parallel with this work.
Event schemas are unique from most work in in-
formation extraction (IE). Current relation discovery
(Banko et al, 2007a; Carlson et al, 2010b) focuses
on atomic facts and relations. Event schemas build
relations into coherent event structures, often called
templates in IE. For instance, an election template
jointly connects that obama won a presidential elec-
tion with romney was the defeated, the election oc-
curred in 2012, and the popular vote was 50-48. The
entities in these relations fill specific semantic roles,
as in this template schema:
Template Schema for Elections
(events: nominate, vote, elect, win, declare, concede)
Date: Timestamp
Winner: Person
Loser: Person
Position: Occupation
Vote: Number
Traditionally, template extractors assume fore-
knowledge of the event schemas. They know a Win-
ner exists, and research focuses on supervised learn-
ing to extract winners from text. This paper focuses
on the other side of the supervision spectrum. The
learner receives no human input, and it first induces
a schema before extracting instances of it.
Our proposed model contributes to a growing
line of research in schema induction. The majority
of previous work relies on ad-hoc clustering algo-
rithms (Filatova et al, 2006; Sekine, 2006; Cham-
bers and Jurafsky, 2011). Chambers and Jurafsky
is a pipelined approach, learning events first, and
later learning syntactic patterns as fillers. It requires
1797
several ad-hoc metrics and parameters, and it lacks
the benefits of a formal model. However, central to
their algorithm is the use of coreferring entity men-
tions to knit events and entities together into an event
schema. We adapt this entity-driven approach to a
single model that requires fewer parameters and far
less training data. Further, experiments show state-
of-the-art performance.
Other research conducted at the time of this pa-
per also proposes a generative model for schema in-
duction (Cheung et al, 2013). Theirs is not entity-
based, but instead uses a sequence model (HMM-
based) of verb clauses. These two papers thus pro-
vide a unique opportunity to compare two very dif-
ferent views of document structure. One is entity-
driven, modeling an entity?s role by its coreference
chain. The other is clause-driven, classifying indi-
vidual clauses based on text sequence. Each model
makes unique assumptions, providing an interest-
ing contrast. Our entity model outperforms by 7 F1
points on a common extraction task.
The rest of the paper describes in detail our
main contributions: (1) the first entity-based gen-
erative model for schema induction, (2) a direct
pipeline/formal model comparison, (3) results im-
proving state-of-the-art performance by 20%, and
(4) schema induction from the smallest amount of
training data to date.
2 Previous Work
Unsupervised learning for information extraction
usually learns binary relations and atomic facts.
Models can learn relations like Person is married to
Person without labeled data (Banko et al, 2007b), or
rely on seed examples for ontology induction (dog is
a mammal) and attribute extraction (dogs have tails)
(Carlson et al, 2010b; Carlson et al, 2010a; Huang
and Riloff, 2010; Durme and Pasca, 2008). These do
not typically capture the deeper connections mod-
eled by event schemas.
Algorithms that do focus on event schema extrac-
tion typically require both the schemas and labeled
corpora, such as rule-based approaches (Chinchor
et al, 1993; Rau et al, 1992) and modern super-
vised classifiers (Freitag, 1998; Chieu et al, 2003;
Bunescu and Mooney, 2004; Patwardhan and Riloff,
2009; Huang and Riloff, 2011). Classifiers rely on
the labeled examples? surrounding context for fea-
tures (Maslennikov and Chua, 2007). Weakly su-
pervised learning removes some of the need for la-
beled data, but most still require the event schemas.
One common approach is to begin with unlabeled,
but clustered event-specific documents, and extract
common word patterns as extractors (Riloff and
Schmelzenbach, 1998; Sudo et al, 2003; Riloff et
al., 2005; Filatova et al, 2006; Patwardhan and
Riloff, 2007; Chen et al, 2011). Bootstrapping with
seed examples of known slot fillers has been shown
to be effective (Yangarber et al, 2000; Surdeanu et
al., 2006).
Shinyama and Sekine (2006) presented unre-
stricted relation discovery to discover relations in
unlabeled documents. Their algorithm used redun-
dant documents (e.g., all describe Hurricane Ivan)
to observe repeated proper nouns. The approach re-
quires many documents about the exact same event
instance, and relations are binary (not schemas) over
repeated named entities. Our model instead learns
schemas from documents with mixed topics that
don?t describe the same event, so repeated proper
nouns are less helpful.
Chen et al (2011) perform relation extraction
with no supervision on earthquake and finance do-
mains. Theirs is a generative model that represents
relations as predicate/argument pairs. As with oth-
ers, training data is pre-clustered by event type and
there is no schema connection between relations.
This paper builds the most on Chambers and Ju-
rafsky (2011). They learned event schemas with a
three-stage clustering algorithm that included a re-
quirement to retrieve extra training data. This paper
removes many of these complexities. We present
a formal model that uniquely models coreference
chains. Advantages include a joint clustering of
events and entities, and a formal probabilistic inter-
pretation of the resulting schemas. We achieve better
performance, and do so with far less training data.
Cheung et al (2013) is most related as a genera-
tive formulation of schema induction. They propose
an HMM-based model over latent event variables,
where each variable generates the observed clauses.
Latent schema variables generate the event vari-
ables (in the spirit of preliminary work by O?Connor
(2012)). There is no notion of an entity, so learning
uses text mentions and relies on the local HMM win-
1798
message: id dev-muc3-0112 (bellcore, mitre)
incident: date 10 mar 89
incident: location peru: huanuco, ambo (town)
incident: type bombing
incident: stage accomplished
incident: instrument explosive: ?-?
perp: individual ?shining path members?
perp: organization ?shining path?
Figure 1: A subset of the slots in a MUC-4 template.
dow for event transitions. Their model was created
in parallel with our work, and provides a nice con-
trast in both approach and results. Ours outperforms
their model by 20% on a MUC-4 evaluation.
In summary, this paper extends most previous
work on event schema induction by removing the
supervision. Of the recent ?unsupervised? work, we
present the first entity-driven generative model, and
we experiment on a mixed-domain corpus.
3 Dataset: The MUC-4 Corpus
The corpus from the Message Understanding Con-
ference (MUC-4) serves as the challenge text (Sund-
heim, 1991), and will ground discussion of our
model. MUC-4 is also used by the closest previ-
ous work. It contains Latin American newswire
about terrorism events, and it provides a set of
hand-constructed event schemas that are tradition-
ally called template schemas. It also maps labeled
templates to the text, providing a dataset for tem-
plate extraction evaluations. Until very recently,
only extraction has been evaluated. We too evalu-
ate our model through extraction, but we also com-
pare our learned schemas to the hand-created tem-
plate schemas. An example of a filled in MUC-4
template is given in Figure 1.
The MUC-4 corpus defines six template types:
Attack, Kidnapping, Bombing, Arson, Robbery,
and Forced Work Stoppage. Documents are often
labeled with more than one template and type. Many
include multiple events at different times in different
locations. The corpus is particularly challenging be-
cause template schemas are inter-mixed and entities
can play multiple roles across instances.
The training corpus contains 1300 documents,
733 of which are labeled with at least one schema.
567 documents are not labeled with any schemas.
These unlabeled documents are articles that report
on non-specific political events and speeches. They
make the corpus particularly challenging. The de-
velopment and test sets each contain 200 documents.
4 A Generative Model for Event Schemas
This paper?s model is an entity-based approach, sim-
ilar in motivation to Haghighi and Klein (2010) and
the pipelined induction of Chambers and Jurafsky
(2011). Coreference resolution guides the learning
by providing a set of pre-resolved entities. Each
entity receives a schema role label, so it allows all
mentions of the entity to inform that role choice.
This important constraint links coreferring mentions
to the same schema role, and distinguishes our ap-
proach from others (Cheung et al, 2013).
4.1 Illustration
The model represents a document as a set of enti-
ties. An entity is a set of entity mentions clustered
by coreference resolution. We will use the following
two sentences for illustration:
A truck bomb exploded near the embassy.
Three militia planted it, and then they fled.
This text contains five entity mentions. A perfect
coreference resolution system will resolve these five
mentions into three entities:
Entity Mentions Entities Roles
a truck bomb (a truck bomb, it) Instrument
the embassy (the embassy) Target
three militia (three militia, they) Perpetrator
it
they
The schema roles, or template slots, are the type
of target knowledge we want to learn. Each en-
tity will be labeled with both a slot variable s and
a template variable t (e.g., the s=perpetrator of a
t=bombing). The lexical context of the entity men-
tions guides the learning model to this end.
4.2 Definitions
A document d ? D is represented as a set of entities
Ed. Each entity e ? Ed is a triple: e = (h,M,F )
1. he is the canonical word for the entity (typically
the first mention?s head word)
1799
Text
A truck bomb exploded near the embassy.
Three militia planted it, and then they fled.
Entity Representation
entity 1: h = bomb, F = {PHYS-OBJ},
M = { (p=explode, d=subject-explode)
(p=plant, d=object-plant) }
entity 2: h = militia, F = {PERSON, ORG},
M = { (p=plant, d=subject-plant),
(p=flee, subject-flee) }
entity 3: h = embassy, F = {PHYS-OBJ, ORG},
M = { (p=explode, d=prep near-explode) }
Figure 2: Example text mapped to our entities.
2. Me is a set of entity mentions m ? Me. Each
mention is a pairm = (p, d): the predicate, and
the typed dependency from the predicate to the
mention (e.g., push and subject-push).
3. Fe is a set of binary entity features. This paper
only uses named entity types as features, but
generalizes to other features as well.
A document is thus reduced to its entities, their
grammatical contexts, and entity features. Figure 2
continues our example using this formulation. he is
chosen to be e?s longest non-pronoun mention m ?
Me. Mentions are labeled with NER and WordNet
synsets to create an entity?s features Fe ? {Person,
Org, Loc, Event, Time, Object, Other}. We use the
Stanford NLP toolkit to parse, extract typed depen-
dencies, label with NER, and run coreference.
4.3 The Generative Models
Similar to topics in LDA, each document d in our
model has a corresponding multinomial over schema
types ?d, drawn from a Dirichlet. For each entity in
the document, a hidden variable t is drawn accord-
ing to ?d. These t variables represent the high level
schema types, such as bombing or kidnapping. The
predicates associated with each of the entity?s men-
tions are then drawn from the schema?s multinomial
over predicates Pt. The variable t also generates
a hidden variable s from its distribution over slots,
such as perpetrator and victim. Finally, the entity?s
canonical head word is generated from ?s, all entity
mentions? typed dependencies from ?s, and named
entity types from ?s.
The most important characteristic of this model
is the separation of event words from the lexical
properties of specific entity mentions. The schema
type variables t only model the distribution of event
words (bomb, plant, defuse), but the slot variables
s model the syntax (subject-bomb, subject-plant,
object-arrest) and entity words (suspect, terrorist,
man). This allows the high-level schemas to first se-
lect predicates, and then forces predicate arguments
to prefer slots that are in the parent schema type.
Formally, a document d receives a labeling Zd
where each entity e ? Ed is labeled Zd,e = (t, s)
with a schema type t and a slot s. The joint distribu-
tion of a document and labeling is then as follows:
P (d, Zd) =
?
e?Ed
P (t|?)? P (s|t)
?
?
e?Ed
P (he|s)
?
?
e?Ed
?
f?Fe
P (f |s)
?
?
e?Ed
?
m?Me
P (dm|s) ? P (pm|t) (1)
The plate diagram for the model is given in Fig-
ure 3. The darker circles correspond to the observed
entity components in Figure 2. We assume the fol-
lowing generative process for a document d:
Generate ?d from Dir(?)
for each schema type t = 1...m do
Generate Pt from Dir(?)
for each slot st = 1...k do
Generate ?s from Dir(?)
Generate ?s from Dir(?)
Generate ?s from Dir(?)
for each entity e ? Ed do
Generate schema type t from Multinomial(?d)
Generate slot s from UniformDist(k)
Generate head word h from Multinomial(?s)
for each mention m ?Me do
Generate predicate token p from Multinomial(Pt)
Generate typed dependency d from Multinomial(?s)
for each entity type i = 1..|Fe| do
Generate entity type f from Multinomial(?s)
The number of schema types m and the number
of slots per schema k are chosen based on training
set performance.
1800
s
h d
?
f
? ? ? k
EMF
t
v
V mkxm
DE
p
P
NAMED ENTITIES
ENTITY MENTIONSENTITY FEATURESENTITY HEAD
DOCUMENTS
Figure 3: The full plate diagram for the event schema
model. Hyper-parameters are omitted for readability.
The Flat Relation Model
We also experiment with a Flat Relation Model that
removes the hidden t variables, ignoring schema
types. Figure 4 visually compares this flat model
with the full model. We found that the predicate
distribution Pt hurts performance in a flat model.
Predicates are more informative at the higher level,
but less so for slots where syntax is more important.
We thus removed Pt from the model, and everything
else remains the same. This flat model now learns
a large set of k slots S that aren?t connected by a
high-level schema variable. Each slot s ? S has a
corresponding triple of multinomials (h,M,F ) sim-
ilar to above: (1) a multinomial over the head men-
tions ?s, (2) a multinomial over the grammatical re-
lations of the entity mentions ?s, and (3) a multino-
mial over the entity features ?s. For each entity in
a document, a hidden slot s ? S is first drawn from
?, and then the observed entity (h,M,F ) is drawn
according to the multinomials (?s, ?s, ?s). We later
evaluate this flat model to show the benefit of added
schema structure.
4.4 Inference
We use collapsed Gibbs sampling for inference,
sampling the latent variables te,d and se,d in se-
s
h
d
? k EM
Ff?
s h
d
? k EM
F
f
?
? mxm
?
t
Figure 4: Simplified plate diagrams comparing the flat
relation model to the full template model. The observed
f ? F variables are not included for clarity.
quence conditioned on a full setting of all the other
variables (Griffiths and Steyvers, 2004). Initial pa-
rameter values are set by randomly setting t and s
variables from the uniform distribution over schema
types and slots, then computing the other parameter
values based on these initial settings. The hyperpa-
rameters for the dirichlet distributions were chosen
from a small grid search (see Experiments).
Beyond standard inference, we added one con-
straint to the model that favors grammatical distri-
butions ?s that do not contain conflicts. The subject
and direct object of a verb should not both receive
high probability mass under the same schema slot
?s. For instance, the victim of a kidnapping should
not favor both the subject and object of a single verb.
Semantic roles should (typically) select one syntac-
tic slot, so this constraint encourages that behavior.
During sampling of se,d, we use a penalty factor ?
to make conflicting relations less likely. Formally,
P (se,d = s|?, he, Fe,Me) = ? iff there exists an
m ? Me such that P (m|?s) < P (inv(m)|?s) and
P (inv(m)|?s) > 0.1, where inv(m) = object if
m = subject and vice versa. Otherwise, the proba-
bility is computed as normal. We normalize the dis-
tributions after penalties are computed.
4.5 Entity Extraction for Template Filling
Inducing event schemas is only one benefit of the
model. The learned model can also extract spe-
cific instances of the learned schemas without ad-
1801
ditional complexity. To evaluate the effectiveness of
the model, we apply the model to perform standard
template extraction on MUC-4. Previous MUC-4
induction required an extraction algorithm separate
from induction because induction created hard clus-
ters (Chambers and Jurafsky, 2011). Cluster scores
don?t have a natural interpretation, so extraction re-
quired several parameters/thresholds to tune. Our
model instead simply relies on model inference.
We run inference as described above and each en-
tity receives a template label te,d and a template slot
label se,d. These labels are the extractions, and it re-
quires no other parameters. The model thus requires
far less machinery than a pipeline, and the exper-
iments below further show that this simpler model
outperforms the pipeline.
Beyond parameters, the question of ?irrelevant?
documents is a concern in MUC-4. Approximately
half the corpus are documents that are not labeled
with a template, so past algorithms required extra
processing stages to filter out these irrelevant doc-
uments. Patwardhan and Riloff (2009) and Cham-
bers and Jurafsky (2011) make initial decisions as to
whether they should extract or not from a document.
Huang and Riloff (2011) use a genre detector for this
problem. Even the generative HMM-based model of
Cheung et al (Cheung et al, 2013) requires an ex-
tra filtering parameter. Our formal model is unique
in not requiring additional effort. Ours is the only
approach that doesn?t require document filtering.
5 Evaluation Setup
Evaluating on MUC-4 has a diverse history that
complicates comparison. The following balances
comparison against previous work and enables fu-
ture comparison to our results.
5.1 Template Schema Slots
Most systems do not evaluate performance on all
MUC-4 template slots. They instead focus on four
main slots, ignoring the parameterized slots that in-
volve deeper reasoning (such as ?stage of execution?
and ?effect of incident?). The four slots and example
entity fillers are shown here:
Perpetrator: Shining Path members
Victim: Sergio Horna
Target: public facilities
Instrument: explosives
We also focus only on these four slots. We merged
MUC?s two perpetrator slots (individuals and orgs)
into one gold Perpetrator. Previous work has both
split the two and merged the two. We merge them
because the distinction between an individual and
an organization is often subtle and not practically
important to analysts. This is also consistent with
the most recent event schema induction in Chambers
and Jurafsky (2011) and Cheung et al (2013).
One peculiarity in MUC-4 is that some templates
are labeled as optional (i.e., all its slots are optional),
and some required templates contain optional slots
(i.e., a subset of slots are optional). We ignore
both optional templates and specific optional slots
when computing recall, as in previous work (Pat-
wardhan and Riloff, 2007; Patwardhan and Riloff,
2009; Chambers and Jurafsky, 2011).
Comparison between the extracted strings and the
gold template strings uses head word scoring. We
do not use gold parses for the text, so head words
are defined simply as the rightmost word in the noun
phrase. The exception is when the extracted phrase
is of the form ?A of B?, then the rightmost word in
?A? is used as the head. This is again consistent with
previous work1. The standard evaluation metrics are
precision, recall, and F1 score.
5.2 Mapping Learned Slots
Induced schemas need to map to gold schemas be-
fore evaluation. Which learned slots correspond to
MUC-4 slots? There are two methods of mapping.
The first ignores the schema type variables t, and
simply finds the best performing s variable for each
gold template slot2. We call this the slot-only map-
ping evaluation. The second approach is to map each
template variable t to the best gold template type g,
and limit the slot mapping so that only the slots un-
der t can map to slots under g. We call this the tem-
plate mapping evaluation. The slot-only mapping
can result in higher scores since it is not constrained
to preserve schema structure in the mapping.
Chambers and Jurafsky (2011) used template
mapping in their evaluation. Cheung et al (2013)
used slot-only mapping. We run both evaluations in
this paper and separately compare both.
1Personal communications with Patwardhan and Riloff
2bombing-victim is a template slot distinct from kidnap-
victim. Both need to be mapped.
1802
6 Experiments
We use the Stanford CoreNLP toolkit for text pro-
cessing and parsing. We developed the models on
the 1300 document MUC-4 training set. We then
learned once on the entire 1700 training/dev/test set,
and report extraction numbers from the inferred la-
bels on the 200 document test set. Each experiment
was repeated 10 times. Reported numbers are aver-
aged across these runs.
There are two structure variables for the model:
the number of schema types and the number of slots
under each type. We searched for the optimal values
on the training set before evaluating on test. The
hyperparameters for all evaluations were set to ? =
? = ? = ? = 1, ? = .1 based on a grid search.
6.1 Template Schema Induction
The first evaluation compares the learned schemas
to the gold schemas in MUC-4.
Since most previous work assumes this knowl-
edge ahead of time, we align our schemas with the
main MUC-4 template types to measure quality. We
inspected the learned event schemas that mapped to
MUC-4 schemas based on the template mapping ex-
traction evaluation.
Figure 5 shows some of the learned distribu-
tions for two mapped schemas: kidnappings and
bombings. The predicate distribution for each event
schema is shown, as well as the top 5 head words
and grammatical relations for each slot. The words
and events that were jointly learned in these exam-
ples appear quite accurate. The bombing and kidnap
schemas learned all of the equivalent MUC-4 gold
slots. Interestingly, our model also learned Loca-
tions and Times as important entities that appear in
the text. These entities are not traditionally included
in the MUC-4 extraction task.
Figure 6 lists the MUC-4 slots that we did and
did not learn for the four most prevelant types. We
report 71% recall, with almost all errors due to the
model?s failure to learn about arsons. Arson tem-
plates only occur in 40 articles, much less than the
200 bombing and over 400 attack. We show below
that overall extraction performs well despite this.
The learned distributions for Attack end up extract-
ing Arson perpetrators and Arson victims in the ac-
tual extraction evaluation.
Bomb Kidnap Attack Arson
Perpetrator X X X x
Victim X X X X
Target X - X x
Instrument X - x x
Location X X X X
Date/Time X X X x
Figure 6: The MUC-4 gold slots that were learned. The
bottom two are not in the traditional evaluation, but were
learned by our model nonetheless.
Evaluation: Template Mapping
Prec Recall F1
C & J 2011 .48 .25 .33
Formal Template Model .42 .27 .33
Table 1: MUC-4 extraction with template mapping. A
learned schema first maps to a gold MUC template.
Learned slots can then only map to slots in that template.
6.2 Extraction Experiments
We now present the full extraction experiment that
is traditionally used for evaluating MUC-4 per-
formance. Although our learned schemas closely
match gold schemas, extraction depends on how
well the model can extract from diverse lexical con-
texts. We ran inference on the full training and test
sets, and used the inferred labels as schema labels.
These labels were mapped and evaluated against the
gold MUC-4 labels as discussed in Section 5.
Performance is compared to two state-of-the-art
induction systems. Since these previous two mod-
els used different methods to map their learned
schemas, we compare separately. Table 1 shows the
template mapping evaluation with Chambers and Ju-
rafsky (C&J). Table 2 shows the slot-only mapping
evaluation with Cheung et al
Our model achieves an F1 score comparable to
C&J, and 20% higher than Cheung et al Part of the
greater increase over Cheung et al is the mapping
difference. For each MUC-4 type, such as bombing,
any four learned slots can map to the four MUC-
4 bombing slots. There is no constraint that the
learned slots must come from the same schema type.
The more strict template mapping (Table 1) ensures
that entire schema types are mapped together, and it
reduces our performance from .41 to .33.
1803
Kidnapping Entities
Victim (Person 88%)
businessman object-kidnap
citizen object-release
Soares prep of-kidnapping
Kent possessive-release
hostage object-found
Perpetrator (Person 62%, Org 30%)
guerrilla subject-kidnap
ELN subject-hold
group subject-attack
extraditables subject-demand
man subject-announce
Date (TimeDate 89%)
TIME tmod-kidnap
February prep on-kidnap
hours tmod-release
morning prep on-release
night tmod-take
Bombing Entities
Victim (Person 86%, Location 8%)
person object-kill
guerrilla object-wound
soldier subject-die
man subject-blow up
civilian subject-try
Physical Target (Object 65%, Event 42%)
building object-destroy
office object-damage
explosive object-use
station and-office
vehicle prep of-number
Instrument (Event 56%, Object 39%)
bomb subject-explode
explosion subject-occur
attack object-cause
charge object-place
device subject-destroy
Figure 5: Select distributions for two learned events. Left columns are head word distributions ?, right columns are
syntactic relation distributions ?, and entity types in parentheses are the learned ?. Most probable words are shown.
Evaluation: Slot-Only Mapping
Prec Recall F1
Cheung et al 2013 .32 .37 .34
Flat Relation Model .26 .45 .33
Formal Template Model .41 .41 .41
Table 2: MUC-4 extraction with slot-only mapping. Any
learned slot is allowed to map to any gold slot.
Entity Role Performance
Prec Recall F1
Perpetrator .40 .20 .26
Victim .42 .31 .34
Target .38 .28 .31
Instrument .57 .39 .45
Table 3: Results for each MUC-4 template slot using the
template-mapping evaluation.
The macro-level F1 scores can be broken down
into individual slot performance. Table 3 shows
these results ranging from .26 to .45. The Instrument
role proves easiest to learn, consistent with C&J.
A large portion of MUC-4 includes irrelevant
documents. Cheung et al (2013) evaluated their
model without irrelevant documents in the test set
that to see how performance is affected. We com-
pare against their numbers in Table 4. Results are
closer now with ours outperforming .46 to .43 F1.
This suggests that the HMM-based approach stum-
bles more on spurious documents, but performs bet-
ter on relevant ones.
Gold Document Evaluation
Prec Recall F1
Cheung et al 2013 .41 .44 .43
Formal Template Model .49 .43 .46
Table 4: Full MUC-4 extraction with gold document clas-
sification. These results ignore false positives extracted
from ?irrelevant? documents in the test set.
6.3 Model Ablation
Table 2 shows that the flat relation model (no latent
type variables t) is inferior to the full schema model.
F1 drops 20% without the explicit modeling of both
schema types t and their entity slots s. The entity
features Fe are less important. Experiments with-
out them show a slight drop in performance (2 F1
points), small enough that they could be removed for
efficiency. However, it is extremely useful to learn
slots with NER labels like Person or Location.
Finally, we experimented without the sub-
ject/object constraint (Section 4.4). Performance
drops 5-10% depending on the number of schemas
learned. Anecdotally, it merges too many schema
slots that should be separate. We recommend using
this constraint as it has little impact on CPU time.
6.4 Extension: Reduce Training Size
One of the main benefits of this generative model
appears to be the reduction in training data. The
pipelined approach in C&J required an information
retrieval stage to bring in hundreds of other docu-
1804
ments from an external corpus. This paper?s genera-
tive model doesn?t require such a stage.
We thus attempted to induce and extract event
schemas from just the 200 test set documents, with
no training or development data. We repeated this
experiment 30 times and averaged the results, setting
the number of templates t = 20 and slots s = 10 as
in the main experiment. The resulting F1 score for
the template-mapping evaluation fell to 0.27 from
the full data experiment of 0.33 F1. Adding more
training documents in another experiment did not
significantly increase performance over 0.27 until
all training and development documents were in-
cluded. This could be explained by the develop-
ment set being more similar to the test set than train-
ing. We did not investigate further to prevent over-
experimentation on test.
7 Discussion
Our model is one of the first generative formula-
tions of schema induction. It produces state-of-the-
art performance on a traditional extraction task, and
performs with less training data as well as a more
complex pipelined approach. Further, our unique
entity-driven approach outperforms an HMM-based
model developed in parallel to this work.
Our entity-driven proposal is strongly influenced
by the ideas in the pipeline model of Chambers and
Jurafsky (2011). Coreference chains have been used
in a variety of learning tasks, such as narrative learn-
ing and summarization. Here we are the first to show
how it can be used for schema induction in a proba-
bilistic model, connecting predicates across a docu-
ment in a way that is otherwise difficult to represent.
The models perform similarly, but ours also includes
significant benefits like a reduction in complexity,
reproducibility, and a large reduction in training data
requirements.
This paper also implies that learning and ex-
traction need not be independent algorithms. Our
model?s inference procedure to learn schemas is the
same one that labels text for extraction. C&J re-
quired 3-4 separate pipelined steps. Cheung et al
(2013) required specific cutoffs for document classi-
fication before extraction. Not only does our model
perform well, but it does so without these steps.
Highlighted here are key differences between this
proposal and the HMM-based model of Cheung et
al. (2013). One of the HMM strengths is the in-
clusion of sequence-based knowledge. Each slot la-
bel is influenced by the previous label in the text,
encouraging syntactic arguments of a predicate to
choose the same schema. This knowledge is only
loosely present in our document distribution ?. Che-
ung et al also include a hidden event variable be-
tween the template and slot variables. Our model
collapses this event variable and makes fewer depen-
dency assumptions. This difference requires further
investigation as it is unclear if it provides valuable
information, or too much complexity.
We also note a warning for future work on proper
evaluation methodology. This task is particularly
difficult to compare to other models due to its
combination of both induction and then extraction.
There are many ways to map induced schemas to
gold answers, and this paper illustrates how ex-
traction performance is significantly affected by the
choice. We suggest the template-mapping evalua-
tion to preserve learned structure.
Finally, these induced results are far behind su-
pervised learning (Huang and Riloff, 2011). There
is ample room for improvement and future research
in event schema induction.
Acknowledgments
This work was partially supported by a grant from
the Office of Naval Research. It was also sup-
ported, in part, by the Johns Hopkins Human Lan-
guage Technology Center of Excellence. Any opin-
ions, findings, and conclusions or recommendations
expressed in this material are those of the author.
Thanks to Eric Wang for his insights into Bayesian
modeling, Brendan O?Connor for his efforts on nor-
malizing MUC-4 evaluation details, Frank Ferraro
and Benjamin Van Durme for helpful conversations,
and to the reviewers for insightful feedback.
1805
References
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007a. Learning
relations from the web. In Proceedings of the Interna-
tional Joint Conferences on Artificial Intelligence (IJ-
CAI).
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007b. Open in-
formation extraction from the web. In Proceedings of
the International Joint Conferences on Artificial Intel-
ligence (IJCAI).
Razvan Bunescu and Raymond Mooney. 2004. Collec-
tive information extraction with relational markov net-
works. In Proceedings of the Association for Compu-
tational Linguistics (ACL), pages 438?445.
Andrew Carlson, J. Betteridge, B. Kisiel, B. Settles,
E.R. Hruschka Jr., and T.M. Mitchell. 2010a. To-
ward an architecture for never-ending language learn-
ing. In Proceedings of the Conference on Artificial
Intelligence (AAAI).
Andrew Carlson, J. Betteridge, R.C. Wang, E.R. Hr-
uschka Jr., and T.M. Mitchell. 2010b. Coupled semi-
supervised learning for information extraction. In Pro-
ceedings of the ACM International Conference on Web
Search and Data Mining (WSDM).
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the Association for Computational Lin-
guistics.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL).
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies.
Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.
2003. Closing the gap: Learning-based information
extraction rivaling knowledge-engineering methods.
In Proceedings of the Association for Computational
Linguistics (ACL).
Nancy Chinchor, David Lewis, and Lynette Hirschman.
1993. Evaluating message understanding systems: an
analysis of the third message understanding confer-
ence. Computational Linguistics, 19:3:409?449.
Benjamin Van Durme and Marius Pasca. 2008. Finding
cars, goddesses and enzymes: Parametrizable acquisi-
tion of labeled instances for open-domain information
extraction. In Proceedings of the 23rd Annual Con-
ference on Artificial Intelligence (AAAI-2008), pages
1243?1248.
Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen
McKeown. 2006. Automatic creation of domain tem-
plates. In Proceedings of the Association for Compu-
tational Linguistics (ACL).
Dayne Freitag. 1998. Toward general-purpose learning
for information extraction. In Proceedings of the As-
sociation for Computational Linguistics (ACL), pages
404?408.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. In Proceedings of the National Academy of
Sciences of the United States of America, pages 5228?
5235.
Aria Haghighi and Dan Klein. 2010. An entity-level ap-
proach to information extraction. In Proceedings of
the Association for Computational Linguistics (ACL).
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the Association for Com-
putational Linguistics (ACL).
Ruihong Huang and Ellen Riloff. 2011. Peeling back the
layers: Detecting event role fillers in secondary con-
texts. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics (ACL).
Mstislav Maslennikov and Tat-Seng Chua. 2007. Auto-
matic acquisition of domain knowledge for informa-
tion extraction. In Proceedings of the Association for
Computational Linguistics (ACL).
Brendan O?Connor. 2012. Learning frames from text
with an unsupervised latent variable model. Technical
report, Carnegie Mellon University.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
ie with semantic affinity patterns and relevant regions.
In Proceedings of the Conference on Empirical Meth-
ods on Natural Language Processing (EMNLP).
Siddharth Patwardhan and Ellen Riloff. 2009. A unified
model of phrasal and sentential evidence for informa-
tion extraction. In Proceedings of the Conference on
Empirical Methods on Natural Language Processing
(EMNLP).
Lisa Rau, George Krupka, Paul Jacobs, Ira Sider, and
Lois Childs. 1992. Ge nltoolset: Muc-4 test results
and analysis. In Proceedings of the Message Under-
standing Conference (MUC-4), pages 94?99.
Ellen Riloff and Mark Schmelzenbach. 1998. An em-
pirical approach to conceptual case frame acquisition.
In Proceedings of the Sixth Workshop on Very Large
Corpora.
Ellen Riloff, Janyce Wiebe, and William Phillips. 2005.
Exploiting subjectivity classification to improve infor-
mation extraction. In Proceedings of AAAI-05.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of the Joint Conference of the
1806
International Committee on Computational Linguis-
tics and the Association for Computational Linguis-
tics, pages 731?738.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
ie using unrestricted relation discovery. In Proceed-
ings of NAACL.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL), pages 224?231.
Beth M. Sundheim. 1991. Third message understand-
ing evaluation and conference (muc-3): Phase 1 status
report. In Proceedings of the Message Understanding
Conference.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006.
A hybrid approach for the acquisition of information
extraction patterns. In Proceedings of the EACL Work-
shop on Adaptive Text Extraction and Mining.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisi-
tion of domain knowledge for information extraction.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING), pages 940?
946.
1807
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 603?612,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Learning for Microblogs with Distant Supervision:
Political Forecasting with Twitter
Micol Marchetti-Bowick
Microsoft Corporation
475 Brannan Street
San Francisco, CA 94122
micolmb@microsoft.com
Nathanael Chambers
Department of Computer Science
United States Naval Academy
Annapolis, MD 21409
nchamber@usna.edu
Abstract
Microblogging websites such as Twitter
offer a wealth of insight into a popu-
lation?s current mood. Automated ap-
proaches to identify general sentiment to-
ward a particular topic often perform two
steps: Topic Identification and Sentiment
Analysis. Topic Identification first identi-
fies tweets that are relevant to a desired
topic (e.g., a politician or event), and Sen-
timent Analysis extracts each tweet?s atti-
tude toward the topic. Many techniques for
Topic Identification simply involve select-
ing tweets using a keyword search. Here,
we present an approach that instead uses
distant supervision to train a classifier on
the tweets returned by the search. We show
that distant supervision leads to improved
performance in the Topic Identification task
as well in the downstream Sentiment Anal-
ysis stage. We then use a system that incor-
porates distant supervision into both stages
to analyze the sentiment toward President
Obama expressed in a dataset of tweets.
Our results better correlate with Gallup?s
Presidential Job Approval polls than pre-
vious work. Finally, we discover a sur-
prising baseline that outperforms previous
work without a Topic Identification stage.
1 Introduction
Social networks and blogs contain a wealth of
data about how the general public views products,
campaigns, events, and people. Automated algo-
rithms can use this data to provide instant feed-
back on what people are saying about a topic.
Two challenges in building such algorithms are
(1) identifying topic-relevant posts, and (2) iden-
tifying the attitude of each post toward the topic.
This paper studies distant supervision (Mintz et
al., 2009) as a solution to both challenges. We
apply our approach to the problem of predicting
Presidential Job Approval polls from Twitter data,
and we present results that improve on previous
work in this area. We also present a novel base-
line that performs remarkably well without using
topic identification.
Topic identification is the task of identifying
text that discusses a topic of interest. Most pre-
vious work on microblogs uses simple keyword
searches to find topic-relevant tweets on the as-
sumption that short tweets do not need more so-
phisticated processing. For instance, searches for
the name ?Obama? have been assumed to return
a representative set of tweets about the U.S. Pres-
ident (O?Connor et al 2010). One of the main
contributions of this paper is to show that keyword
search can lead to noisy results, and that the same
keywords can instead be used in a distantly super-
vised framework to yield improved performance.
Distant supervision uses noisy signals in text
as positive labels to train classifiers. For in-
stance, the token ?Obama? can be used to iden-
tify a series of tweets that discuss U.S. President
Barack Obama. Although searching for token
matches can return false positives, using the re-
sulting tweets as positive training examples pro-
vides supervision from a distance. This paper ex-
periments with several diverse sets of keywords
to train distantly supervised classifiers for topic
identification. We evaluate each classifier on a
hand-labeled dataset of political and apolitical
tweets, and demonstrate an improvement in F1
score over simple keyword search (.39 to .90 in
the best case). We also make available the first la-
beled dataset for topic identification in politics to
encourage future work.
Sentiment analysis encompasses a broad field
of research, but most microblog work focuses
on two moods: positive and negative sentiment.
603
Algorithms to identify these moods range from
matching words in a sentiment lexicon to training
classifiers with a hand-labeled corpus. Since la-
beling corpora is expensive, recent work on Twit-
ter uses emoticons (i.e., ASCII smiley faces such
as :-( and :-)) as noisy labels in tweets for distant
supervision (Pak and Paroubek, 2010; Davidov et
al., 2010; Kouloumpis et al 2011). This paper
presents new analysis of the downstream effects
of topic identification on sentiment classifiers and
their application to political forecasting.
Interest in measuring the political mood of
a country has recently grown (O?Connor et al
2010; Tumasjan et al 2010; Gonzalez-Bailon et
al., 2010; Carvalho et al 2011; Tan et al 2011).
Here we compare our sentiment results to Presi-
dential Job Approval polls and show that the sen-
timent scores produced by our system are posi-
tively correlated with both the Approval and Dis-
approval job ratings.
In this paper we present a method for cou-
pling two distantly supervised algorithms for
topic identification and sentiment classification on
Twitter. In Section 4, we describe our approach to
topic identification and present a new annotated
corpus of political tweets for future study. In Sec-
tion 5, we apply distant supervision to sentiment
analysis. Finally, Section 6 discusses our sys-
tem?s performance on modeling Presidential Job
Approval ratings from Twitter data.
2 Previous Work
The past several years have seen sentiment anal-
ysis grow into a diverse research area. The idea
of sentiment applied to microblogging domains is
relatively new, but there are numerous recent pub-
lications on the subject. Since this paper focuses
on the microblog setting, we concentrate on these
contributions here.
The most straightforward approach to senti-
ment analysis is using a sentiment lexicon to la-
bel tweets based on how many sentiment words
appear. This approach tends to be used by appli-
cations that measure the general mood of a popu-
lation. O?Connor et al(2010) use a ratio of posi-
tive and negative word counts on Twitter, Kramer
(2010) counts lexicon words on Facebook, and
Thelwall (2011) uses the publicly available Sen-
tiStrength algorithm to make weighted counts of
keywords based on predefined polarity strengths.
In contrast to lexicons, many approaches in-
stead focus on ways to train supervised classi-
fiers. However, labeled data is expensive to cre-
ate, and examples of Twitter classifiers trained on
hand-labeled data are few (Jiang et al 2011). In-
stead, distant supervision has grown in popular-
ity. These algorithms use emoticons to serve as
semantic indicators for sentiment. For instance,
a sad face (e.g., :-() serves as a noisy label for a
negative mood. Read (2005) was the first to sug-
gest emoticons for UseNet data, followed by Go
et al(Go et al 2009) on Twitter, and many others
since (Bifet and Frank, 2010; Pak and Paroubek,
2010; Davidov et al 2010; Kouloumpis et al
2011). Hashtags (e.g., #cool and #happy) have
also been used as noisy sentiment labels (Davi-
dov et al 2010; Kouloumpis et al 2011). Fi-
nally, multiple models can be blended into a sin-
gle classifier (Barbosa and Feng, 2010). Here, we
adopt the emoticon algorithm for sentiment analy-
sis, and evaluate it on a specific domain (politics).
Topic identification in Twitter has received
much less attention than sentiment analysis. The
majority of approaches simply select a single
keyword (e.g., ?Obama?) to represent their topic
(e.g., ?US President?) and retrieve all tweets that
contain the word (O?Connor et al 2010; Tumas-
jan et al 2010; Tan et al 2011). The underlying
assumption is that the keyword is precise, and due
to the vast number of tweets, the search will re-
turn a large enough dataset to measure sentiment
toward that topic. In this work, we instead use
a distantly supervised system similar in spirit to
those recently applied to sentiment analysis.
Finally, we evaluate the approaches presented
in this paper on the domain of politics. Tumasjan
et al(2010) showed that the results of a recent
German election could be predicted through fre-
quency counts with remarkable accuracy. Most
similar to this paper is that of O?Connor et al
(2010), in which tweets relating to President
Obama are retrieved with a keyword search and
a sentiment lexicon is used to measure overall
approval. This extracted approval ratio is then
compared to Gallup?s Presidential Job Approval
polling data. We directly compare their results
with various distantly supervised approaches.
3 Datasets
The experiments in this paper use seven months of
tweets from Twitter (www.twitter.com) collected
604
between June 1, 2009 and December 31, 2009.
The corpus contains over 476 million tweets la-
beled with usernames and timestamps, collected
through Twitter?s ?spritzer? API without keyword
filtering. Tweets are aligned with polling data in
Section 6 using their timestamps.
The full system is evaluated against the pub-
licly available daily Presidential Job Approval
polling data from Gallup1. Every day, Gallup asks
1,500 adults in the United States about whether
they approve or disapprove of ?the job Presi-
dent Obama is doing as president.? The results
are compiled into two trend lines for Approval
and Disapproval ratings, as shown in Figure 1.
We compare our positive and negative sentiment
scores against these two trends.
4 Topic Identification
This section addresses the task of Topic Identi-
fication in the context of microblogs. While the
general field of topic identification is broad, its
use on microblogs has been somewhat limited.
Previous work on the political domain simply uses
keywords to identify topic-specific tweets (e.g.,
O?Connor et al(2010) use ?Obama? to find pres-
idential tweets). This section shows that distant
supervision can use the same keywords to build a
classifier that is much more robust to noise than
approaches that use pure keyword search.
4.1 Distant Supervision
Distant supervision uses noisy signals to identify
positive examples of a topic in the face of unla-
beled data. As described in Section 2, recent sen-
timent analysis work has applied distant supervi-
sion using emoticons as the signals. The approach
extracts tweets with ASCII smiley faces (e.g., :)
and ;)) and builds classifiers trained on these pos-
itive examples. We apply distant supervision to
topic identification and evaluate its effectiveness
on this subtask.
As with sentiment analysis, we need to collect
positive and negative examples of tweets about
the target topic. Instead of emoticons, we extract
positive tweets containing one or more predefined
keywords. Negative tweets are randomly chosen
from the corpus. Examples of positive and neg-
ative tweets that can be used to train a classifier
based on the keyword ?Obama? are given here:
1http://gallup.com/poll/113980/gallup-daily-obama-job-
approval.aspx
ID Type Keywords
PC-1 Obama obama
PC-2 General republican, democrat, senate,
congress, government
PC-3 Topic health care, economy, tax cuts,
tea party, bailout, sotomayor
PC-4 Politician obama, biden, mccain, reed,
pelosi, clinton, palin
PC-5 Ideology liberal, conservative, progres-
sive, socialist, capitalist
Table 1: The keywords used to select positive training
sets for each political classifier (a subset of all PC-3
and PC-5 keywords are shown to conserve space).
positive: LOL, obama made a bears refer-
ence in green bay. uh oh.
negative: New blog up! It regards the new
iPhone 3G S: <URL>
We then use these automatically extracted
datasets to train a multinomial Naive Bayes classi-
fier. Before feature collection, the text is normal-
ized as follows: (a) all links to photos (twitpics)
are replaced with a single generic token, (b) all
non-twitpic URLs are replaced with a token, (c)
all user references (e.g., @MyFriendBob) are col-
lapsed, (d) all numbers are collapsed to INT, (e)
tokens containing the same letter twice or more
in a row are condensed to a two-letter string (e.g.
the word ahhhhh becomes ahh), (f) lowercase the
text and insert spaces between words and punctu-
ation. The text of each tweet is then tokenized,
and the tokens are used to collect unigram and bi-
gram features. All features that occur fewer than
10 times in the training corpus are ignored.
Finally, after training a classifier on this dataset,
every tweet in the corpus is classified as either
positive (i.e., relevant to the topic) or negative
(i.e., irrelevant). The positive tweets are then sent
to the second sentiment analysis stage.
4.2 Keyword Selection
Keywords are the input to our proposed distantly
supervised system, and of course, the input to pre-
vious work that relies on keyword search. We
evaluate classifiers based on different keywords to
measure the effects of keyword selection.
O?Connor et al(2010) used the keywords
?Obama? and ?McCain?, and Tumasjan et al
(2010) simply extracted tweets containing Ger-
many?s political party names. Both approaches
extracted matching tweets, considered them rele-
605
Gallup Daily Obama Job Approval Ratings
Figure 1: Gallup presidential job Approval and Disapproval ratings measured between June and Dec 2009.
vant (correctly, in many cases), and applied sen-
timent analysis. However, different keywords
may result in very different extractions. We in-
stead attempted to build a generic ?political? topic
classifier. To do this, we experimented with the
five different sets of keywords shown in Table 1.
For each set, we extracted all tweets matching
one or more keywords, and created a balanced
positive/negative training set by then selecting
negative examples randomly from non-matching
tweets. A couple examples of ideology (PC-5) ex-
tractions are shown here:
You often hear of deontologist libertarians
and utilitarian liberals but are there any
Aristotelian socialists?
<url> - Then, slather on a liberal amount
of plaster, sand down smooth, and paint
however you want. I hope this helps!
The second tweet is an example of the noisy
nature of keyword extraction. Most extractions
are accurate, but different keywords retrieve very
different sets of tweets. Examples for the political
topics (PC-3) are shown here:
RT @PoliticalMath: hope the president?s
health care predictions <url> are better
than his stimulus predictions <url>
@adamjschmidt You mean we could have
chosen health care for every man woman
and child in America or the Iraq war?
Each keyword set builds a classifier using the ap-
proach described in Section 4.1.
4.3 Labeled Datasets
In order to evaluate distant supervision against
keyword search, we created two new labeled
datasets of political and apolitical tweets.
The Political Dataset is an amalgamation of all
four keyword extractions (PC-1 is a subset of PC-
4) listed in Table 1. It consists of 2,000 tweets ran-
domly chosen from the keyword searches of PC-
2, PC-3, PC-4, and PC-5 with 500 tweets from
each. This combined dataset enables an evalua-
tion of how well each classifier can identify tweets
from other classifiers. The General Dataset con-
tains 2,000 random tweets from the entire corpus.
This dataset alws us to evaluate how well clas-
sifiers identify political tweets in the wild.
This paper?s authors initially annotated the
same 200 tweets in the General Dataset to com-
pute inter-annotator agreement. The Kappa was
0.66, which is typically considered good agree-
ment. Most disagreements occurred over tweets
about money and the economy. We then split the
remaining portions of the two datasets between
the two annotators. The Political Dataset con-
tains 1,691 political and 309 apolitical tweets, and
the General Dataset contains 28 political tweets
and 1,978 apolitical tweets. These two datasets of
2000 tweets each are publicly available for future
evaluation and comparison to this work2.
4.4 Experiments
Our first experiment addresses the question of
keyword variance. We measure performance on
the Political Dataset, a combination of all of our
proposed political keywords. Each keyword set
contributed to 25% of the dataset, so the eval-
uation measures the extent to which a classifier
identifies other keyword tweets. We classified
the 2000 tweets with the five distantly supervised
classifiers and the one ?Obama? keyword extrac-
tor from O?Connor et al(2010).
Results are shown on the left side of Figure 2.
Precision and recall calculate correct identifica-
tion of the political label. The five distantly super-
vised approaches perform similarly, and show re-
markable robustness despite their different train-
ing sets. In contrast, the keyword extractor only
2http://www.usna.edu/cs/nchamber/data/twitter
606
Figure 2: Five distantly supervised classifiers and the Obama keyword classifier. Left panel: the Political Dataset
of political tweets. Right panel: the General Dataset representative of Twitter as a whole.
captures about a quarter of the political tweets.
PC-1 is the distantly supervised analog to the
Obama keyword extractor, and we see that dis-
tant supervision increases its F1 score dramati-
cally from 0.39 to 0.90.
The second evaluation addresses the question
of classifier performance on Twitter as a whole,
not just on a political dataset. We evaluate on the
General Dataset just as on the Political Dataset.
Results are shown on the right side of Figure 2.
Most tweets posted to Twitter are not about pol-
itics, so the apolitical label dominates this more
representative dataset. Again, the five distant
supervision classifiers have similar results. The
Obama keyword search has the highest precision,
but drastically sacrifices recall. Four of the five
classifiers outperform keyword search in F1 score.
4.5 Discussion
The Political Dataset results show that distant su-
pervision adds robustness to a keyword search.
The distantly supervised ?Obama? classifier (PC-
1) improved the basic ?Obama? keyword search
by 0.51 absolute F1 points. Furthermore, dis-
tant supervision doesn?t require additional human
input, but simply adds a trained classifier. Two
example tweets that an Obama keyword search
misses but that its distantly supervised analog
captures are shown here:
Why does Congress get to opt out of the
Obummercare and we can?t. A company
gets fined if they don?t comply. Kiss free-
dom goodbye.
I agree with the lady from california, I am
sixty six years old and for the first time in
my life I am ashamed of our government.
These results also illustrate that distant supervi-
sion allows for flexibility in construction of the
classifier. Different keywords show little change
in classifier performance.
The General Dataset experiment evaluates clas-
sifier performance in the wild. The keyword ap-
proach again scores below those trained on noisy
labels. It classifies most tweets as apolitical and
thus achieves very low recall for tweets that are
actually about politics. On the other hand, distant
supervision creates classifiers that over-extract
political tweets. This is a result of using balanced
datasets in training; such effects can be mitigated
by changing the training balance. Even so, four
of the five distantly trained classifiers score higher
than the raw keyword approach. The only under-
performer was PC-1, which suggests that when
building a classifier for a relatively broad topic
like politics, a variety of keywords is important.
The next section takes the output from our clas-
sifiers (i.e., our topic-relevant tweets) and eval-
uates a fully automated sentiment analysis algo-
rithm against real-world polling data.
5 Targeted Sentiment Analysis
The previous section evaluated algorithms that
extract topic-relevant tweets. We now evaluate
methods to distill the overall sentiment that they
express. This section compares two common ap-
proaches to sentiment analysis.
We first replicated the technique used in
O?Connor et al(2010), in which a lexicon of pos-
itive and negative sentiment words called Opin-
607
ionFinder (Wilson and Hoffmann, 2005) is used
to evaluate the sentiment of each tweet (others
have used similar lexicons (Kramer, 2010; Thel-
wall et al 2010)). We evaluate our full distantly
supervised approach to theirs. We also experi-
mented with SentiStrength, a lexicon-based pro-
gram built to identify sentiment in online com-
ments of the social media website, MySpace.
Though MySpace is close in genre to Twitter, we
did not observe a performance gain. All reported
results thus use OpinionFinder to facilitate a more
accurate comparison with previous work.
Second, we built a distantly supervised system
using tweets containing emoticons as done in pre-
vious work (Read, 2005; Go et al 2009; Bifet and
Frank, 2010; Pak and Paroubek, 2010; Davidov
et al 2010; Kouloumpis et al 2011). Although
distant supervision has previously been shown to
outperform sentiment lexicons, these evaluations
do not consider the extra topic identification step.
5.1 Sentiment Lexicon
The OpinionFinder lexicon is a list of 2,304 pos-
itive and 4,151 negative sentiment terms (Wilson
and Hoffmann, 2005). We ignore neutral words
in the lexicon and we do not differentiate between
weak and strong sentiment words. A tweet is la-
beled positive if it contains any positive terms, and
negative if it contains any negative terms. A tweet
can be marked as both positive and negative, and
if a tweet contains words in neither category, it
is marked neutral. This procedure is the same as
used by O?Connor et al(2010). The sentiment
scores Spos and Sneg for a given set of N tweets
are calculated as follows:
Spos =
?
x 1{xlabel = positive}
N
(1)
Spos =
?
x 1{xlabel = negative}
N
(2)
where 1{xlabel = positive} is 1 if the tweet x is
labeled positive, and N is the number of tweets in
the corpus. For the sake of comparison, we also
calculate a sentiment ratio as done in O?Connor
et al(2010):
Sratio =
?
x 1{xlabel = positive}?
x 1{xlabel = negative}
(3)
5.2 Distant Supervision
To build a trained classifier, we automatically gen-
erated a positive training set by searching for
tweets that contain at least one positive emoti-
con and no negative emoticons. We generated a
negative training set using an analogous process.
The emoticon symbols used for positive sentiment
were :) =) :-) :] =] :-] :} :o) :D =D :-D :P =P
:-P C:. Negative emoticons were :( =( :-( :[ =[
:-[ :{ :-c :c} D: D= :S :/ =/ :-/ :?( : (. Using this
data, we train a multinomial Naive Bayes classi-
fier using the same method used for the political
classifiers described in Section 4.1. This classifier
is then used to label topic-specific tweets as ex-
pressing positive or negative sentiment. Finally,
the three overall sentiment scores Spos, Sneg, and
Sratio are calculated from the results.
6 Predicting Approval Polls
This section uses the two-stage Targeted Senti-
ment Analysis system described above in a real-
world setting. We analyze the sentiment of Twit-
ter users toward U.S. President Barack Obama.
This allows us to both evaluate distant supervision
against previous work on the topic, and demon-
strate a practical application of the approach.
6.1 Experiment Setup
The following experiments combine both topic
identification and sentiment analysis. The previ-
ous sections described six topic identification ap-
proaches, and two sentiment analysis approaches.
We evaluate all combinations of these systems,
and compare their final sentiment scores for each
day in the nearly seven-month period over which
our dataset spans.
Gallup?s Daily Job Approval reports two num-
bers: Approval and Disapproval. We calculate in-
dividual sentiment scores Spos and Sneg for each
day, and compare the two sets of trends using
Pearson?s correlation coefficient. O?Connor et al
do not explicitly evaluate these two, but instead
use the ratio Sratio. We also calculate this daily
ratio from Gallup for comparison purposes by di-
viding the Approval by the Disapproval.
6.2 Results and Discussion
The first set of results uses the lexicon-based clas-
sifier for sentiment analysis and compares the dif-
ferent topic identification approaches. The first
table in Table 2 reports Pearson?s correlation co-
efficient with Gallup?s Approval and Disapproval
ratings. Regardless of the Topic classifier, all
608
Sentiment Lexicon
Topic Classifier Approval Disapproval
keyword -0.22 0.42
PC-1 -0.65 0.71
PC-2 -0.61 0.71
PC-3 -0.51 0.65
PC-4 -0.49 0.60
PC-5 -0.65 0.74
Distantly Supervised Sentiment
Topic Classifier Approval Disapproval
keyword 0.27 0.38
PC-1 0.71 0.73
PC-2 0.33 0.46
PC-3 0.05 0.31
PC-4 0.08 0.26
PC-5 0.54 0.62
Table 2: Correlation between Gallup polling data and
the extracted sentiment with a lexicon (trends shown
in Figure 3) and distant supervision (Figure 4).
Sentiment Lexicon
keyword PC-1 PC-2 PC-3 PC-4 PC-5
.22 .63 .46 .33 .27 .61
Distantly Supervised Sentiment
keyword PC-1 PC-2 PC-3 PC-4 PC-5
.40 .64 .46 .30 .28 .60
Table 3: Correlation between Gallup Approval / Dis-
approval ratio and extracted sentiment ratio scores.
systems inversely correlate with Presidential Ap-
proval. However, they correlate well with Dis-
approval. Figure 3 graphically shows the trend
lines for the keyword and the distantly supervised
system PC-1. The visualization illustrates how
the keyword-based approach is highly influenced
by day-by-day changes, whereas PC-1 displays a
much smoother trend.
The second set of results uses distant supervi-
sion for sentiment analysis and again varies the
topic identification approach. The second table
in Table 2 gives the correlation numbers and Fig-
ure 4 shows the keyword and PC-1 trend lines.The
results are widely better than when a lexicon is
used for sentiment analysis. Approval is no longer
inversely correlated, and two of the distantly su-
pervised systems strongly correlate (PC-1, PC-5).
The best performing system (PC-1) used dis-
tant supervision for both topic identification and
sentiment analysis. Pearson?s correlation coeffi-
cient for this approach is 0.71 with Approval and
0.73 with Disapproval.
Finally, we compute the ratio Sratio between
the positive and negative sentiment scores (Equa-
tion 3) to compare to O?Connor et al(2010). Ta-
ble 3 shows the results. The distantly supervised
topic identification algorithms show little change
between a sentiment lexicon or a classifier. How-
ever, O?Connor et als keyword approach im-
proves when used with a distantly supervised sen-
timent classifier (.22 to .40). Merging Approval
and Disapproval into one ratio appears to mask
the sentiment lexicon?s poor correlation with Ap-
proval. The ratio may not be an ideal evalua-
tion metric for this reason. Real-world interest in
Presidential Approval ratings desire separate Ap-
proval and Disapproval scores, as Gallup reports.
Our results (Table 2) show that distant supervi-
sion avoids a negative correlation with Approval,
but the ratio hides this important advantage.
One reason the ratio may mask the negative
Approval correlation is because tweets are often
classified as both positive and negative by a lexi-
con (Section 5.1). This could explain the behav-
ior seen in Figure 3 in which both the positive and
negative sentiment scores rise over time. How-
ever, further experimentation did not rectify this
pattern. We revised Spos and Sneg to make binary
decisions for a lexicon: a tweet is labeled posi-
tive if it strictly contains more positive words than
negative (and vice versa). Correlation showed lit-
tle change. Approval was still negatively corre-
lated, Disapproval positive (although less so in
both), and the ratio scores actually dropped fur-
ther. The sentiment ratio continued to hide the
poor Approval performance by a lexicon.
6.3 New Baseline: Topic-Neutral Sentiment
Distant supervision for sentiment analysis outper-
forms that with a sentiment lexicon (Table 2).
Distant supervision for topic identification further
improves the results (PC-1 v. keyword). The
best system uses distant supervision in both stages
(PC-1 with distantly supervised sentiment), out-
performing the purely keyword-based algorithm
of O?Connor et al(2010). However, the question
of how important topic identification is has not yet
been addressed here or in the literature.
Both O?Connor et al(2010) and Tumasjan et
al. (2010) created joint systems with two topic
identification and sentiment analysis stages. But
609
Sentiment Lexicon
Figure 3: Presidential job approval and disapproval calculated using two different topic identification techniques,
and using a sentiment lexicon for sentiment analysis. Gallup polling results are shown in black.
Distantly Supervised Sentiment
Figure 4: Presidential job approval sentiment scores calculated using two different topic identification techniques,
and using the emoticon classifier for sentiment analysis. Gallup polling results are shown in black.
Topic-Neutral Sentiment
Figure 5: Presidential job approval sentiment scores calculated using the entire twitter corpus, with two different
techniques for sentiment analysis. Gallup polling results are shown in black for comparison.
610
Topic-Neutral Sentiment
Algorithm Approval Disapproval
Distant Sup. 0.69 0.74
Keyword Lexicon -0.63 0.69
Table 4: Pearson?s correlation coefficient of Sentiment
Analysis without Topic Identification.
what if the topic identification step were removed
and sentiment analysis instead run on the entire
Twitter corpus? To answer this question, we
ran the distantly supervised emoticon classifier to
classify all tweets in the 7 months of Twitter data.
For each day, we computed the positive and neg-
ative sentiment scores as above. The evaluation is
identical, except for the removal of topic identifi-
cation. Correlation results are shown in Table 4.
This baseline parallels the results seen when
topic identification is used: the sentiment lexi-
con is again inversely correlated with Approval,
and distant supervision outperforms the lexicon
approach in both ratings. This is not surpris-
ing given previous distantly supervised work on
sentiment analysis (Go et al 2009; Davidov et
al., 2010; Kouloumpis et al 2011). However,
our distant supervision also performs as well as
the best performing topic-specific system. The
best performing topic classifier, PC-1, correlated
with Approval with r=0.71 (0.69 here) and Dis-
approval with r=0.73 (0.74 here). Computing
overall sentiment on Twitter performs as well as
political-specific sentiment. This unintuitive re-
sult suggests a new baseline that all topic-based
systems should compute.
7 Discussion
This paper introduces a new methodology for
gleaning topic-specific sentiment information.
We highlight four main contributions here.
First, this work is one of the first to evaluate
distant supervision for topic identification. All
five political classifiers outperformed the lexicon-
driven keyword equivalent that has been widely
used in the past. Our model achieved .90 F1 com-
pared to the keyword .39 F1 on our political tweet
dataset. On twitter as a whole, distant supervision
increased F1 by over 100%. The results also sug-
gest that performance is relatively insensitive to
the specific choice of seed keywords that are used
to select the training set for the political classifier.
Second, the sentiment analysis experiments
build upon what has recently been shown in the
literature: distant supervision with emoticons is
a valuable methodology. We also expand upon
prior work by discovering drastic performance
differences between positive and negative lexi-
con words. The OpinionFinder lexicon failed
to correlate (inversely) with Gallup?s Approval
polls, whereas a distantly trained classifier cor-
related strongly with both Approval and Disap-
proval (Pearson?s .71 and .73). We only tested
OpinionFinder and SentiStrength, so it is possible
that another lexicon might perform better. How-
ever, our results suggest that lexicons vary in their
quality across sentiment, and distant supervision
may provide more robustness.
Third, our results outperform previous work on
Presidential Job Approval prediction (O?Connor
et al 2010). We presented two novel approaches
to the domain: a coupled distantly supervised sys-
tem, and a topic-neutral baseline, both of which
outperform previous results. In fact, the baseline
surprisingly matches or outperforms the more so-
phisticated approaches that use topic identifica-
tion. The baseline correlates .69 with Approval
and .74 with Disapproval. This suggests a new
baseline that should be used in all topic-specific
sentiment applications.
Fourth, we described and made available two
new annotated datasets of political tweets to facil-
itate future work in this area.
Finally, Twitter users are not a representative
sample of the U.S. population, yet the high corre-
lation between political sentiment on Twitter and
Gallup ratings makes these results all the more
intriguing for polling methodologies. Our spe-
cific 7-month period of time differs from previous
work, and thus we hesitate to draw strong con-
clusions from our comparisons or to extend im-
plications to non-political domains. Future work
should further investigate distant supervision as a
tool to assist topic detection in microblogs.
Acknowledgments
We thank Jure Leskovec for the Twitter data,
Brendan O?Connor for open and frank correspon-
dence, and the reviewers for helpful suggestions.
611
References
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics (COL-
ING 2010).
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Lecture
Notes in Computer Science, volume 6332, pages 1?
15.
Paula Carvalho, Luis Sarmento, Jorge Teixeira, and
Mario J. Silva. 2011. Liars and saviors in a senti-
ment annotated corpus of comments to political de-
bates. In Proceedings of the Association for Com-
putational Linguistics (ACL-2011), pages 564?568.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING 2010).
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervi-
sion. Technical report.
Sandra Gonzalez-Bailon, Rafael E. Banchs, and An-
dreas Kaltenbrunner. 2010. Emotional reactions
and the pulse of public opinion: Measuring the im-
pact of political events on the sentiment of online
discussions. Technical report.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the Associ-
ation for Computational Linguistics (ACL-2011).
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In Proceedings of the Fifth
International AAAI Conference on Weblogs and So-
cial Media.
Adam D. I. Kramer. 2010. An unobtrusive behavioral
model of ?gross national happiness?. In Proceed-
ings of the 28th International Conference on Human
Factors in Computing Systems (CHI 2010).
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, ACL
?09, pages 1003?1011.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
AAAI Conference on Weblogs and Social Media.
Alexander Pak and Patrick Paroubek. 2010. Twitter
as a corpus for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh International
Conference On Language Resources and Evalua-
tion (LREC).
Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. In Proceedings of the ACL Stu-
dent Research Workshop (ACL-2005).
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment
analysis incorporating social networks. In Pro-
ceedings of the 17th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment
strength detection in short informal text. Journal of
the American Society for Information Science and
Technology, 61(12):2544?2558.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2011. Sentiment in twitter events. Jour-
nal of the American Society for Information Science
and Technology, 62(2):406?418.
Andranik Tumasjan, Timm O. Sprenger, Philipp G.
Sandner, and Isabell M. Welpe. 2010. Election
forecasts with twitter: How 140 characters reflect
the political landscape. Social Science Computer
Review.
J.; Wilson, T.; Wiebe and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the Conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing.
612
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 445?453,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Improving the Use of Pseudo-Words for Evaluating
Selectional Preferences
Nathanael Chambers and Dan Jurafsky
Department of Computer Science
Stanford University
{natec,jurafsky}@stanford.edu
Abstract
This paper improves the use of pseudo-
words as an evaluation framework for
selectional preferences. While pseudo-
words originally evaluated word sense
disambiguation, they are now commonly
used to evaluate selectional preferences. A
selectional preference model ranks a set of
possible arguments for a verb by their se-
mantic fit to the verb. Pseudo-words serve
as a proxy evaluation for these decisions.
The evaluation takes an argument of a verb
like drive (e.g. car), pairs it with an al-
ternative word (e.g. car/rock), and asks a
model to identify the original. This pa-
per studies two main aspects of pseudo-
word creation that affect performance re-
sults. (1) Pseudo-word evaluations often
evaluate only a subset of the words. We
show that selectional preferences should
instead be evaluated on the data in its en-
tirety. (2) Different approaches to select-
ing partner words can produce overly op-
timistic evaluations. We offer suggestions
to address these factors and present a sim-
ple baseline that outperforms the state-of-
the-art by 13% absolute on a newspaper
domain.
1 Introduction
For many natural language processing (NLP)
tasks, particularly those involving meaning, cre-
ating labeled test data is difficult or expensive.
One way to mitigate this problem is with pseudo-
words, a method for automatically creating test
corpora without human labeling, originally pro-
posed for word sense disambiguation (Gale et al,
1992; Schutze, 1992). While pseudo-words are
now less often used for word sense disambigation,
they are a common way to evaluate selectional
preferences, models that measure the strength of
association between a predicate and its argument
filler, e.g., that the noun lunch is a likely object
of eat. Selectional preferences are useful for NLP
tasks such as parsing and semantic role labeling
(Zapirain et al, 2009). Since evaluating them in
isolation is difficult without labeled data, pseudo-
word evaluations can be an attractive evaluation
framework.
Pseudo-word evaluations are currently used to
evaluate a variety of language modeling tasks
(Erk, 2007; Bergsma et al, 2008). However,
evaluation design varies across research groups.
This paper studies the evaluation itself, showing
how choices can lead to overly optimistic results
if the evaluation is not designed carefully. We
show in this paper that current methods of apply-
ing pseudo-words to selectional preferences vary
greatly, and suggest improvements.
A pseudo-word is the concatenation of two
words (e.g. house/car). One word is the orig-
inal in a document, and the second is the con-
founder. Consider the following example of ap-
plying pseudo-words to the selectional restrictions
of the verb focus:
Original: This story focuses on the campaign.
Test: This story/part focuses on the campaign/meeting.
In the original sentence, focus has two arguments:
a subject story and an object campaign. In the test
sentence, each argument of the verb is replaced by
pseudo-words. A model is evaluated by its success
at determining which of the two arguments is the
original word.
Two problems exist in the current use of
445
pseudo-words to evaluate selectional preferences.
First, selectional preferences historically focus on
subsets of data such as unseen words or words in
certain frequency ranges. While work on unseen
data is important, evaluating on the entire dataset
provides an accurate picture of a model?s overall
performance. Most other NLP tasks today evalu-
ate all test examples in a corpus. We will show
that seen arguments actually dominate newspaper
articles, and thus propose creating test sets that in-
clude all verb-argument examples to avoid artifi-
cial evaluations.
Second, pseudo-word evaluations vary in how
they choose confounders. Previous work has at-
tempted to maintain a similar corpus frequency
to the original, but it is not clear how best to do
this, nor how it affects the task?s difficulty. We
argue in favor of using nearest-neighbor frequen-
cies and show how using random confounders pro-
duces overly optimistic results.
Finally, we present a surprisingly simple base-
line that outperforms the state-of-the-art and is far
less memory and computationally intensive. It
outperforms current similarity-based approaches
by over 13% when the test set includes all of the
data. We conclude with a suggested backoff model
based on this baseline.
2 History of Pseudo-Word
Disambiguation
Pseudo-words were introduced simultaneously by
two papers studying statistical approaches to word
sense disambiguation (WSD). Schu?tze (1992)
simply called the words, ?artificial ambiguous
words?, but Gale et al (1992) proposed the suc-
cinct name, pseudo-word. Both papers cited the
sparsity and difficulty of creating large labeled
datasets as the motivation behind pseudo-words.
Gale et al selected unambiguous words from the
corpus and paired them with random words from
different thesaurus categories. Schu?tze paired his
words with confounders that were ?comparable in
frequency? and ?distinct semantically?. Gale et
al.?s pseudo-word term continues today, as does
Schu?tze?s frequency approach to selecting the con-
founder.
Pereira et al (1993) soon followed with a selec-
tional preference proposal that focused on a lan-
guage model?s effectiveness on unseen data. The
work studied clustering approaches to assist in
similarity decisions, predicting which of two verbs
was the correct predicate for a given noun object.
One verb v was the original from the source doc-
ument, and the other v? was randomly generated.
This was the first use of such verb-noun pairs, as
well as the first to test only on unseen pairs.
Several papers followed with differing methods
of choosing a test pair (v, n) and its confounder
v?. Dagan et al (1999) tested all unseen (v, n)
occurrences of the most frequent 1000 verbs in
his corpus. They then sorted verbs by corpus fre-
quency and chose the neighboring verb v? of v
as the confounder to ensure the closest frequency
match possible. Rooth et al (1999) tested 3000
random (v, n) pairs, but required the verbs and
nouns to appear between 30 and 3000 times in
training. They also chose confounders randomly
so that the new pair was unseen.
Keller and Lapata (2003) specifically addressed
the impact of unseen data by using the web to first
?see? the data. They evaluated unseen pseudo-
words by attempting to first observe them in a
larger corpus (the Web). One modeling difference
was to disambiguate the nouns as selectional pref-
erences instead of the verbs. Given a test pair
(v, n) and its confounder (v, n?), they used web
searches such as ?v Det n? to make the decision.
Results beat or matched current results at the time.
We present a similarly motivated, but new web-
based approach later.
Very recent work with pseudo-words (Erk,
2007; Bergsma et al, 2008) further blurs the lines
between what is included in training and test data,
using frequency-based and semantic-based rea-
sons for deciding what is included. We discuss
this further in section 5.
As can be seen, there are two main factors when
devising a pseudo-word evaluation for selectional
preferences: (1) choosing (v, n) pairs from the test
set, and (2) choosing the confounding n? (or v?).
The confounder has not been looked at in detail
and as best we can tell, these factors have var-
ied significantly. Many times the choices are well
motivated based on the paper?s goals, but in other
cases the motivation is unclear.
3 How Frequent is Unseen Data?
Most NLP tasks evaluate their entire datasets, but
as described above, most selectional preference
evaluations have focused only on unseen data.
This section investigates the extent of unseen ex-
amples in a typical training/testing environment
446
of newspaper articles. The results show that even
with a small training size, seen examples dominate
the data. We argue that, absent a system?s need for
specialized performance on unseen data, a repre-
sentative test set should include the dataset in its
entirety.
3.1 Unseen Data Experiment
We use the New York Times (NYT) and Associ-
ated Press (APW) sections of the Gigaword Cor-
pus (Graff, 2002), as well as the British National
Corpus (BNC) (Burnard, 1995) for our analysis.
Parsing and SRL evaluations often focus on news-
paper articles and Gigaword is large enough to
facilitate analysis over varying amounts of train-
ing data. We parsed the data with the Stan-
ford Parser1 into dependency graphs. Let (vd, n)
be a verb v with grammatical dependency d ?
{subject, object, prep} filled by noun n. Pairs
(vd, n) are chosen by extracting every such depen-
dency in the graphs, setting the head predicate as
v and the head word of the dependent d as n. All
prepositions are condensed into prep.
We randomly selected documents from the year
2001 in the NYT portion of the corpus as devel-
opment and test sets. Training data for APW and
NYT include all years 1994-2006 (minus NYT de-
velopment and test documents). We also identified
and removed duplicate documents2. The BNC in
its entirety is also used for training as a single data
point. We then record every seen (vd, n) pair dur-
ing training that is seen two or more times3 and
then count the number of unseen pairs in the NYT
development set (1455 tests).
Figure 1 plots the percentage of unseen argu-
ments against training size when trained on either
NYT or APW (the APW portion is smaller in total
size, and the smaller BNC is provided for com-
parison). The first point on each line (the high-
est points) contains approximately the same num-
ber of words as the BNC (100 million). Initially,
about one third of the arguments are unseen, but
that percentage quickly falls close to 10% as ad-
ditional training is included. This suggests that an
evaluation focusing only on unseen data is not rep-
resentative, potentially missing up to 90% of the
data.
1http://nlp.stanford.edu/software/lex-parser.shtml
2Any two documents whose first two paragraphs in the
corpus files are identical.
3Our results are thus conservative, as including all single
occurrences would achieve even smaller unseen percentages.
0 2 4 6 8 10 120
5
10
15
20
25
30
35
40
45
Number of Tokens in Training (hundred millions)
Per
cen
t Un
see
n
Unseen Arguments in NYT Dev
 
  BNC AP NYT Google
Figure 1: Percentage of NYT development set
that is unseen when trained on varying amounts of
data. The two lines represent training with NYT or
APW data. The APW set is smaller in size from
the NYT. The dotted line uses Google n-grams as
training. The x-axis represents tokens ? 108.
0 2 4 6 8 10 120
5
10
15
20
25
30
35
40
Number of Tokens in Training (hundred millions)
Per
cen
t Un
see
n
Unseen Arguments by Type
 
  Preps Subjects Objects
Figure 2: Percentage of subject/object/preposition
arguments in the NYT development set that is un-
seen when trained on varying amounts of NYT
data. The x-axis represents tokens ? 108.
447
The third line across the bottom of the figure is
the number of unseen pairs using Google n-gram
data as proxy argument counts. Creating argu-
ment counts from n-gram counts is described in
detail below in section 5.2. We include these Web
counts to illustrate how an openly available source
of counts affects unseen arguments. Finally, fig-
ure 2 compares which dependency types are seen
the least in training. Prepositions have the largest
unseen percentage, but not surprisingly, also make
up less of the training examples overall.
In order to analyze why pairs are unseen, we an-
alyzed the distribution of rare words across unseen
and seen examples. To define rare nouns, we order
head words by their individual corpus frequencies.
A noun is rare if it occurs in the lowest 10% of the
list. We similarly define rare verbs over their or-
dered frequencies (we count verb lemmas, and do
not include the syntactic relations). Corpus counts
covered 2 years of the AP section, and we used
the development set of the NYT section to extract
the seen and unseen pairs. Figure 3 shows the per-
centage of rare nouns and verbs that occur in un-
seen and seen pairs. 24.6% of the verbs in un-
seen pairs are rare, compared to only 4.5% in seen
pairs. The distribution of rare nouns is less con-
trastive: 13.3% vs 8.9%. This suggests that many
unseen pairs are unseen mainly because they con-
tain low-frequency verbs, rather than because of
containing low-frequency argument heads.
Given the large amount of seen data, we be-
lieve evaluations should include all data examples
to best represent the corpus. We describe our full
evaluation results and include a comparison of dif-
ferent training sizes below.
4 How to Select a Confounder
Given a test set S of pairs (vd, n) ? S, we now ad-
dress how best to select a confounder n?. Work in
WSD has shown that confounder choice can make
the pseudo-disambiguation task significantly eas-
ier. Gaustad (2001) showed that human-generated
pseudo-words are more difficult to classify than
random choices. Nakov and Hearst (2003) further
illustrated how random confounders are easier to
identify than those selected from semantically am-
biguous, yet related concepts. Our approach eval-
uates selectional preferences, not WSD, but our re-
sults complement these findings.
We identified three methods of confounder se-
lection based on varying levels of corpus fre-
verbs nouns
Unseen Tests
Seen Tests
Distribution of Rare Verbs and Nouns in Tests
Per
cen
t Ra
re W
ord
s
0
5
10
15
20
25
30
Figure 3: Comparison between seen and unseen
tests (verb,relation,noun). 24.6% of unseen tests
have rare verbs, compared to just 4.5% in seen
tests. The rare nouns are more evenly distributed
across the tests.
quency: (1) choose a random noun, (2) choose a
random noun from a frequency bucket similar to
the original noun?s frequency, and (3) select the
nearest neighbor, the noun with frequency clos-
est to the original. These methods evaluate the
range of choices used in previous work. Our ex-
periments compare the three.
5 Models
5.1 A New Baseline
The analysis of unseen slots suggests a baseline
that is surprisingly obvious, yet to our knowledge,
has not yet been evaluated. Part of the reason
is that early work in pseudo-word disambiguation
explicitly tested only unseen pairs4. Our evalua-
tion will include seen data, and since our analysis
suggests that up to 90% is seen, a strong baseline
should address this seen portion.
4Recent work does include some seen data. Bergsma et
al. (2008) test pairs that fall below a mutual information
threshold (might include some seen pairs), and Erk (2007)
selects a subset of roles in FrameNet (Baker et al, 1998) to
test and uses all labeled instances within this subset (unclear
what portion of subset of data is seen). Neither evaluates all
of the seen data, however.
448
We propose a conditional probability baseline:
P (n|vd) =
{
C(vd,n)
C(vd,?)
if C(vd, n) > 0
0 otherwise
where C(vd, n) is the number of times the head
word n was seen as an argument to the pred-
icate v, and C(vd, ?) is the number of times
vd was seen with any argument. Given a test
(vd, n) and its confounder (vd, n?), choose n if
P (n|vd) > P (n?|vd), and n? otherwise. If
P (n|vd) = P (n?|vd), randomly choose one.
Lapata et al (1999) showed that corpus fre-
quency and conditional probability correlate with
human decisions of adjective-noun plausibility,
and Dagan et al (1999) appear to propose a very
similar baseline for verb-noun selectional prefer-
ences, but the paper evaluates unseen data, and so
the conditional probability model is not studied.
We later analyze this baseline against a more
complicated smoothing approach.
5.2 A Web Baseline
If conditional probability is a reasonable baseline,
better performance may just require more data.
Keller and Lapata (2003) proposed using the web
for this task, querying for specific phrases like
?Verb Det N? to find syntactic objects. Such a web
corpus would be attractive, but we?d like to find
subjects and prepositional objects as well as ob-
jects, and also ideally we don?t want to limit our-
selves to patterns. Since parsing the web is unre-
alistic, a reasonable compromise is to make rough
counts when pairs of words occur in close proxim-
ity to each other.
Using the Google n-gram corpus, we recorded
all verb-noun co-occurrences, defined by appear-
ing in any order in the same n-gram, up to and
including 5-grams. For instance, the test pair
(throwsubject, ball) is considered seen if there ex-
ists an n-gram such that throw and ball are both
included. We count all such occurrences for all
verb-noun pairs. We also avoided over-counting
co-occurrences in lower order n-grams that appear
again in 4 or 5-grams. This crude method of count-
ing has obvious drawbacks. Subjects are not dis-
tinguished from objects and nouns may not be ac-
tual arguments of the verb. However, it is a simple
baseline to implement with these freely available
counts.
Thus, we use conditional probability as de-
fined in the previous section, but define the count
C(vd, n) as the number of times v and n (ignoring
d) appear in the same n-gram.
5.3 Smoothing Model
We implemented the current state-of-the-art
smoothing model of Erk (2007). The model is
based on the idea that the arguments of a particular
verb slot tend to be similar to each other. Given
two potential arguments for a verb, the correct
one should correlate higher with the arguments ob-
served with the verb during training.
Formally, given a verb v and a grammatical de-
pendency d, the score for a noun n is defined:
Svd(n) =
?
w?Seen(vd)
sim(n,w) ? C(vd, w) (1)
where sim(n,w) is a noun-noun similarity score,
Seen(vd) is the set of seen head words filling the
slot vd during training, and C(vd, n) is the num-
ber of times the noun n was seen filling the slot vd
The similarity score sim(n,w) can thus be one of
many vector-based similarity metrics5. We eval-
uate both Jaccard and Cosine similarity scores in
this paper, but the difference between the two is
small.
6 Experiments
Our training data is the NYT section of the Gi-
gaword Corpus, parsed into dependency graphs.
We extract all (vd, n) pairs from the graph, as de-
scribed in section 3. We randomly chose 9 docu-
ments from the year 2001 for a development set,
and 41 documents for testing. The test set con-
sisted of 6767 (vd, n) pairs. All verbs and nouns
are stemmed, and the development and test docu-
ments were isolated from training.
6.1 Varying Training Size
We repeated the experiments with three different
training sizes to analyze the effect data size has on
performance:
? Train x1: Year 2001 of the NYT portion of
the Gigaword Corpus. After removing du-
plicate documents, it contains approximately
110 million tokens, comparable to the 100
million tokens in the BNC corpus.
5A similar type of smoothing was proposed in earlier
work by Dagan et al (1999). A noun is represented by a
vector of verb slots and the number of times it is observed
filling each slot.
449
? Train x2: Years 2001 and 2002 of the NYT
portion of the Gigaword Corpus, containing
approximately 225 million tokens.
? Train x10: The entire NYT portion of Giga-
word (approximately 1.2 billion tokens). It is
an order of magnitude larger than Train x1.
6.2 Varying the Confounder
We generated three different confounder sets
based on word corpus frequency from the 41 test
documents. Frequency was determined by count-
ing all tokens with noun POS tags. As motivated
in section 4, we use the following approaches:
? Random: choose a random confounder from
the set of nouns that fall within some broad
corpus frequency range. We set our range to
eliminate (approximately) the top 100 most
frequent nouns, but otherwise arbitrarily set
the lower range as previous work seems to
do. The final range was [30, 400000].
? Buckets: all nouns are bucketed based on
their corpus frequencies6. Given a test pair
(vd, n), choose the bucket in which n belongs
and randomly select a confounder n? from
that bucket.
? Neighbor: sort all seen nouns by frequency
and choose the confounder n? that is the near-
est neighbor of n with greater frequency.
6.3 Model Implementation
None of the models can make a decision if they
identically score both potential arguments (most
often true when both arguments were not seen with
the verb in training). As a result, we extend all
models to randomly guess (50% performance) on
pairs they cannot answer.
The conditional probability is reported as Base-
line. For the web baseline (reported as Google),
we stemmed all words in the Google n-grams and
counted every verb v and noun n that appear in
Gigaword. Given two nouns, the noun with the
higher co-occurrence count with the verb is cho-
sen. As with the other models, if the two nouns
have the same counts, it randomly guesses.
The smoothing model is named Erk in the re-
sults with both Jaccard and Cosine as the simi-
larity metric. Due to the large vector representa-
tions of the nouns, it is computationally wise to
6We used frequency buckets of 4, 10, 25, 200, 1000,
>1000. Adding more buckets moves the evaluation closer
to Neighbor, less is closer to Random.
trim their vectors, but also important to do so for
best performance. A noun?s representative vector
consists of verb slots and the number of times the
noun was seen in each slot. We removed any verb
slot not seen more than x times, where x varied
based on all three factors: the dataset, confounder
choice, and similarity metric. We optimized x
on the development data with a linear search, and
used that cutoff on each test. Finally, we trimmed
any vectors over 2000 in size to reduce the com-
putational complexity. Removing this strict cutoff
appears to have little effect on the results.
Finally, we report backoff scores for Google and
Erk. These consist of always choosing the Base-
line if it returns an answer (not a guessed unseen
answer), and then backing off to the Google/Erk
result for Baseline unknowns. These are labeled
Backoff Google and Backoff Erk.
7 Results
Results are given for the two dimensions: con-
founder choice and training size. Statistical sig-
nificance tests were calculated using the approx-
imate randomization test (Yeh, 2000) with 1000
iterations.
Figure 4 shows the performance change over the
different confounder methods. Train x2 was used
for training. Each model follows the same pro-
gression: it performs extremely well on the ran-
dom test set, worse on buckets, and the lowest on
the nearest neighbor. The conditional probability
Baseline falls from 91.5 to 79.5, a 12% absolute
drop from completely random to neighboring fre-
quency. The Erk smoothing model falls 27% from
93.9 to 68.1. The Google model generally per-
forms the worst on all sets, but its 74.3% perfor-
mance with random confounders is significantly
better than a 50-50 random choice. This is no-
table since the Google model only requires n-gram
counts to implement. The Backoff Erk model is
the best, using the Baseline for the majority of
decisions and backing off to the Erk smoothing
model when the Baseline cannot answer.
Figure 5 (shown on the next page) varies the
training size. We show results for both Bucket Fre-
quencies and Neighbor Frequencies. The only dif-
ference between columns is the amount of training
data. As expected, the Baseline improves as the
training size is increased. The Erk model, some-
what surprisingly, shows no continual gain with
more training data. The Jaccard and Cosine simi-
450
Varying the Confounder Frequency
Random Buckets Neighbor
Baseline 91.5 89.1 79.5
Erk-Jaccard 93.9* 82.7* 68.1*
Erk-Cosine 91.2 81.8* 65.3*
Google 74.3* 70.4* 59.4*
Backoff Erk 96.6* 91.8* 80.8*
Backoff Goog 92.7? 89.7 79.8
Figure 4: Trained on two years of NYT data (Train
x2). Accuracy of the models on the same NYT test
documents, but with three different ways of choos-
ing the confounders. * indicates statistical signifi-
cance with the column?s Baseline at the p < 0.01
level, ? at p < 0.05. Random is overly optimistic,
reporting performance far above more conserva-
tive (selective) confounder choices.
Baseline Details
Train Train x2 Train x10
Precision 96.1 95.5* 95.0?
Accuracy 78.2 82.0* 88.1*
Accuracy +50% 87.5 89.1* 91.7*
Figure 6: Results from the buckets confounder test
set. Baseline precision, accuracy (the same as re-
call), and accuracy when you randomly guess the
tests that Baseline does not answer. All numbers
are statistically significant * with p-value < 0.01
from the number to their left.
larity scores perform similarly in their model. The
Baseline achieves the highest accuracies (91.7%
and 81.2%) with Train x10, outperforming the best
Erk model by 5.2% and 13.1% absolute on buck-
ets and nearest neighbor respectively. The back-
off models improve the baseline by just under 1%.
The Google n-gram backoff model is almost as
good as backing off to the Erk smoothing model.
Finally, figure 6 shows the Baseline?s precision
and overall accuracy. Accuracy is the same as
recall when the model does not guess between
pseudo words that have the same conditional prob-
abilities. Accuracy +50% (the full Baseline in
all other figures) shows the gain from randomly
choosing one of the two words when uncertain.
Precision is extremely high.
8 Discussion
Confounder Choice: Performance is strongly in-
fluenced by the method used when choosing con-
founders. This is consistent with findings for
WSD that corpus frequency choices alter the task
(Gaustad, 2001; Nakov and Hearst, 2003). Our
results show the gradation of performance as one
moves across the spectrum from completely ran-
dom to closest in frequency. The Erk model
dropped 27%, Google 15%, and our baseline 12%.
The overly optimistic performance on random data
suggests using the nearest neighbor approach for
experiments. Nearest neighbor avoids evaluating
on ?easy? datasets, and our baseline (at 79.5%)
still provides room for improvement. But perhaps
just as important, the nearest neighbor approach
facilitates the most reproducibile results in exper-
iments since there is little ambiguity in how the
confounder is selected.
Realistic Confounders: Despite its over-
optimism, the random approach to confounder se-
lection may be the correct approach in some cir-
cumstances. For some tasks that need selectional
preferences, random confounders may be more re-
alistic. It?s possible, for example, that the options
in a PP-attachment task might be distributed more
like the random rather than nearest neighbor mod-
els. In any case, this is difficult to decide without
a specific application in mind. Absent such spe-
cific motiviation, a nearest neighbor approach is
the most conservative, and has the advantage of
creating a reproducible experiment, whereas ran-
dom choice can vary across design.
Training Size: Training data improves the con-
ditional probability baseline, but does not help the
smoothing model. Figure 5 shows a lack of im-
provement across training sizes for both jaccard
and cosine implementations of the Erk model. The
Train x1 size is approximately the same size used
in Erk (2007), although on a different corpus. We
optimized argument cutoffs for each training size,
but the model still appears to suffer from addi-
tional noise that the conditional probability base-
line does not. This may suggest that observing a
test argument with a verb in training is more re-
liable than a smoothing model that compares all
training arguments against that test example.
High Precision Baseline: Our conditional
probability baseline is very precise. It outper-
forms the smoothed similarity based Erk model
and gives high results across tests. The only com-
bination when Erk is better is when the training
data includes just one year (one twelfth of the
NYT section) and the confounder is chosen com-
451
Varying the Training Size
Bucket Frequency Neighbor Frequency
Train x1 Train x2 Train x10 Train x1 Train x2 Train x10
Baseline 87.5 89.1 91.7 78.4 79.5 81.2
Erk-Jaccard 86.5* 82.7* 83.1* 66.8* 68.1* 65.5*
Erk-Cosine 82.1* 81.8* 81.1* 66.1* 65.3* 65.7*
Google - - 70.4* - - 59.4*
Backoff Erk 92.6* 91.8* 92.6* 79.4* 80.8* 81.7*
Backoff Google 88.6 89.7 91.9? 78.7 79.8 81.2
Figure 5: Accuracy of varying NYT training sizes. The left and right tables represent two confounder
choices: choose the confounder with frequency buckets, and choose by nearest frequency neighbor.
Trainx1 starts with year 2001 of NYT data, Trainx2 doubles the size, and Trainx10 is 10 times larger. *
indicates statistical significance with the column?s Baseline at the p < 0.01 level, ? at p < 0.05.
pletely randomly. These results appear consistent
with Erk (2007) because that work used the BNC
corpus (the same size as one year of our data) and
Erk chose confounders randomly within a broad
frequency range. Our reported results include ev-
ery (vd, n) in the data, not a subset of particu-
lar semantic roles. Our reported 93.9% for Erk-
Jaccard is also significantly higher than their re-
ported 81.4%, but this could be due to the random
choices we made for confounders, or most likely
corpus differences between Gigaword and the sub-
set of FrameNet they evaluated.
Ultimately we have found that complex models
for selectional preferences may not be necessary,
depending on the task. The higher computational
needs of smoothing approaches are best for back-
ing off when unseen data is encountered. Condi-
tional probability is the best choice for seen exam-
ples. Further, analysis of the data shows that as
more training data is made available, the seen ex-
amples make up a much larger portion of the test
data. Conditional probability is thus a very strong
starting point if selectional preferences are an in-
ternal piece to a larger application, such as seman-
tic role labeling or parsing.
Perhaps most important, these results illustrate
the disparity in performance that can come about
when designing a pseudo-word disambiguation
evaluation. It is crucially important to be clear
during evaluations about how the confounder was
generated. We suggest the approach of sorting
nouns by frequency and using a neighbor as the
confounder. This will also help avoid evaluations
that produce overly optimistic results.
9 Conclusion
Current performance on various natural language
tasks is being judged and published based on
pseudo-word evaluations. It is thus important
to have a clear understanding of the evaluation?s
characteristics. We have shown that the evalu-
ation is strongly affected by confounder choice,
suggesting a nearest frequency neighbor approach
to provide the most reproducible performance and
avoid overly optimistic results. We have shown
that evaluating entire documents instead of sub-
sets of the data produces vastly different results.
We presented a conditional probability baseline
that is both novel to the pseudo-word disambigua-
tion task and strongly outperforms state-of-the-art
models on entire documents. We hope this pro-
vides a new reference point to the pseudo-word
disambiguation task, and enables selectional pref-
erence models whose performance on the task
similarly transfers to larger NLP applications.
Acknowledgments
This work was supported by the National Science
Foundation IIS-0811974, and the Air Force Re-
search Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181. Any opinions, ndings,
and conclusion or recommendations expressed in
this material are those of the authors and do not
necessarily reect the view of the AFRL. Thanks
to Sebastian Pado?, the Stanford NLP Group, and
the anonymous reviewers for very helpful sugges-
tions.
452
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Christian
Boitet and Pete Whitelock, editors, ACL-98, pages
86?90, San Francisco, California. Morgan Kauf-
mann Publishers.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional prefer-
ence from unlabeled text. In Empirical Methods in
Natural Language Processing, pages 59?68, Hon-
olulu, Hawaii.
Lou Burnard. 1995. User Reference Guide for the
British National Corpus. Oxford University Press,
Oxford.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. Machine Learning, 34(1):43?
69.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, Prague, Czech Republic.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Work on statistical methods for
word sense disambiguation. In AAAI Fall Sympo-
sium on Probabilistic Approaches to Natural Lan-
guage, pages 54?60.
Tanja Gaustad. 2001. Statistical corpus-based word
sense disambiguation: Pseudowords vs. real am-
biguous words. In 39th Annual Meeting of the Asso-
ciation for Computational Linguistics - Student Re-
search Workshop.
David Graff. 2002. English Gigaword. Linguistic
Data Consortium.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3):459?484.
Maria Lapata, Scott McDonald, and Frank Keller.
1999. Determinants of adjective-noun plausibility.
In European Chapter of the Association for Compu-
tational Linguistics (EACL).
Preslav I. Nakov and Marti A. Hearst. 2003. Category-
based pseudowords. In Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 67?69, Edmonton, Canada.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
31st Annual Meeting of the Association for Com-
putational Linguistics, pages 183?190, Columbus,
Ohio.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
37th Annual Meeting of the Association for Compu-
tational Linguistics, pages 104?111.
Hinrich Schutze. 1992. Context space. In AAAI Fall
Symposium on Probabilistic Approaches to Natural
Language, pages 113?120.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Inter-
national Conference on Computational Linguistics
(COLING).
Beat Zapirain, Eneko Agirre, and Llus Mrquez. 2009.
Generalizing over lexical features: Selectional pref-
erences for semantic role classification. In Joint
Conference of the 47th Annual Meeting of the As-
sociation for Computational Linguistics and the
4th International Joint Conference on Natural Lan-
guage Processing, Singapore.
453
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 976?986,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Template-Based Information Extraction without the Templates
Nathanael Chambers and Dan Jurafsky
Department of Computer Science
Stanford University
{natec,jurafsky}@stanford.edu
Abstract
Standard algorithms for template-based in-
formation extraction (IE) require predefined
template schemas, and often labeled data,
to learn to extract their slot fillers (e.g., an
embassy is the Target of a Bombing tem-
plate). This paper describes an approach to
template-based IE that removes this require-
ment and performs extraction without know-
ing the template structure in advance. Our al-
gorithm instead learns the template structure
automatically from raw text, inducing tem-
plate schemas as sets of linked events (e.g.,
bombings include detonate, set off, and de-
stroy events) associated with semantic roles.
We also solve the standard IE task, using the
induced syntactic patterns to extract role fillers
from specific documents. We evaluate on the
MUC-4 terrorism dataset and show that we in-
duce template structure very similar to hand-
created gold structure, and we extract role
fillers with an F1 score of .40, approaching
the performance of algorithms that require full
knowledge of the templates.
1 Introduction
A template defines a specific type of event (e.g.,
a bombing) with a set of semantic roles (or slots)
for the typical entities involved in such an event
(e.g., perpetrator, target, instrument). In contrast to
work in relation discovery that focuses on learning
atomic facts (Banko et al, 2007a; Carlson et al,
2010), templates can extract a richer representation
of a particular domain. However, unlike relation dis-
covery, most template-based IE approaches assume
foreknowledge of the domain?s templates. Very little
work addresses how to learn the template structure
itself. Our goal in this paper is to perform the stan-
dard template filling task, but to first automatically
induce the templates from an unlabeled corpus.
There are many ways to represent events, rang-
ing from role-based representations such as frames
(Baker et al, 1998) to sequential events in scripts
(Schank and Abelson, 1977) and narrative schemas
(Chambers and Jurafsky, 2009; Kasch and Oates,
2010). Our approach learns narrative-like knowl-
edge in the form of IE templates; we learn sets of
related events and semantic roles, as shown in this
sample output from our system:
Bombing Template
{detonate, blow up, plant, explode, defuse, destroy}
Perpetrator: Person who detonates, plants, blows up
Instrument: Object that is planted, detonated, defused
Target: Object that is destroyed, is blown up
A semantic role, such as target, is a cluster of syn-
tactic functions of the template?s event words (e.g.,
the objects of detonate and explode). Our goal is
to characterize a domain by learning this template
structure completely automatically. We learn tem-
plates by first clustering event words based on their
proximity in a training corpus. We then use a novel
approach to role induction that clusters the syntactic
functions of these events based on selectional prefer-
ences and coreferring arguments. The induced roles
are template-specific (e.g., perpetrator), not univer-
sal (e.g., agent or patient) or verb-specific.
After learning a domain?s template schemas, we
perform the standard IE task of role filling from in-
dividual documents, for example:
Perpetrator: guerrillas
Instrument: dynamite
Target: embassy
976
This extraction stage identifies entities using the
learned syntactic functions of our roles. We evalu-
ate on the MUC-4 terrorism corpus with results ap-
proaching those of supervised systems.
The core of this paper focuses on how to char-
acterize a domain-specific corpus by learning rich
template structure. We describe how to first expand
the small corpus? size, how to cluster its events, and
finally how to induce semantic roles. Section 5 then
describes the extraction algorithm, followed by eval-
uations against previous work in section 6 and 7.
2 Previous Work
Many template extraction algorithms require full
knowledge of the templates and labeled corpora,
such as in rule-based systems (Chinchor et al, 1993;
Rau et al, 1992) and modern supervised classi-
fiers (Freitag, 1998; Chieu et al, 2003; Bunescu
and Mooney, 2004; Patwardhan and Riloff, 2009).
Classifiers rely on the labeled examples? surround-
ing context for features such as nearby tokens, doc-
ument position, syntax, named entities, semantic
classes, and discourse relations (Maslennikov and
Chua, 2007). Ji and Grishman (2008) also supple-
mented labeled with unlabeled data.
Weakly supervised approaches remove some of
the need for fully labeled data. Most still require the
templates and their slots. One common approach is
to begin with unlabeled, but clustered event-specific
documents, and extract common word patterns as
extractors (Riloff and Schmelzenbach, 1998; Sudo
et al, 2003; Riloff et al, 2005; Patwardhan and
Riloff, 2007). Filatova et al (2006) integrate named
entities into pattern learning (PERSON won) to ap-
proximate unknown semantic roles. Bootstrapping
with seed examples of known slot fillers has been
shown to be effective (Surdeanu et al, 2006; Yan-
garber et al, 2000). In contrast, this paper removes
these data assumptions, learning instead from a cor-
pus of unknown events and unclustered documents,
without seed examples.
Shinyama and Sekine (2006) describe an ap-
proach to template learning without labeled data.
They present unrestricted relation discovery as a
means of discovering relations in unlabeled docu-
ments, and extract their fillers. Central to the al-
gorithm is collecting multiple documents describ-
ing the same exact event (e.g. Hurricane Ivan), and
observing repeated word patterns across documents
connecting the same proper nouns. Learned patterns
represent binary relations, and they show how to
construct tables of extracted entities for these rela-
tions. Our approach draws on this idea of using un-
labeled documents to discover relations in text, and
of defining semantic roles by sets of entities. How-
ever, the limitations to their approach are that (1)
redundant documents about specific events are re-
quired, (2) relations are binary, and (3) only slots
with named entities are learned. We will extend
their work by showing how to learn without these
assumptions, obviating the need for redundant doc-
uments, and learning templates with any type and
any number of slots.
Large-scale learning of scripts and narrative
schemas also captures template-like knowledge
from unlabeled text (Chambers and Jurafsky, 2008;
Kasch and Oates, 2010). Scripts are sets of re-
lated event words and semantic roles learned by
linking syntactic functions with coreferring argu-
ments. While they learn interesting event structure,
the structures are limited to frequent topics in a large
corpus. We borrow ideas from this work as well, but
our goal is to instead characterize a specific domain
with limited data. Further, we are the first to apply
this knowledge to the IE task of filling in template
mentions in documents.
In summary, our work extends previous work on
unsupervised IE in a number of ways. We are the
first to learn MUC-4 templates, and we are the first
to extract entities without knowing how many tem-
plates exist, without examples of slot fillers, and
without event-clustered documents.
3 The Domain and its Templates
Our goal is to learn the general event structure of
a domain, and then extract the instances of each
learned event. In order to measure performance
in both tasks (learning structure and extracting in-
stances), we use the terrorism corpus of MUC-4
(Sundheim, 1991) as our target domain. This cor-
pus was chosen because it is annotated with tem-
plates that describe all of the entities involved in
each event. An example snippet from a bombing
document is given here:
977
The terrorists used explosives against the
town hall. El Comercio reported that alleged
Shining Path members also attacked public fa-
cilities in huarpacha, Ambo, tomayquichua,
and kichki. Municipal official Sergio Horna
was seriously wounded in an explosion in
Ambo.
The entities from this document fill the following
slots in a MUC-4 bombing template.
Perp: Shining Path members Victim: Sergio Horna
Target: public facilities Instrument: explosives
We focus on these four string-based slots1 from
the MUC-4 corpus, as is standard in this task. The
corpus consists of 1300 documents, 733 of which
are labeled with at least one template. There are six
types of templates, but only four are modestly fre-
quent: bombing (208 docs), kidnap (83 docs), attack
(479 docs), and arson (40 docs). 567 documents do
not have any templates. Our learning algorithm does
not know which documents contain (or do not con-
tain) which templates. After learning event words
that represent templates, we induce their slots, not
knowing a priori how many there are, and then fill
them in by extracting entities as in the standard task.
In our example above, the three bold verbs (use, at-
tack, wound) indicate the Bombing template, and
their syntactic arguments fill its slots.
4 Learning Templates from Raw Text
Our goal is to learn templates that characterize a
domain as described in unclustered, unlabeled doc-
uments. This presents a two-fold problem to the
learner: it does not know how many events exist, and
it does not know which documents describe which
event (some may describe multiple events). We ap-
proach this problem with a three step process: (1)
cluster the domain?s event patterns to approximate
the template topics, (2) build a new corpus specific to
each cluster by retrieving documents from a larger
unrelated corpus, (3) induce each template?s slots
using its new (larger) corpus of documents.
4.1 Clustering Events to Learn Templates
We cluster event patterns to create templates. An
event pattern is either (1) a verb, (2) a noun in Word-
1There are two Perpetrator slots in MUC-4: Organization
and Individual. We consider their union as a single slot.
Net under the Event synset, or (3) a verb and the
head word of its syntactic object. Examples of each
include (1) ?explode?, (2) ?explosion?, and (3) ?ex-
plode:bomb?. We also tag the corpus with an NER
system and allow patterns to include named entity
types, e.g., ?kidnap:PERSON?. These patterns are
crucially needed later to learn a template?s slots.
However, we first need an algorithm to cluster these
patterns to learn the domain?s core events. We con-
sider two unsupervised algorithms: Latent Dirichlet
Allocation (LDA) (Blei et al, 2003), and agglomer-
ative clustering based on word distance.
4.1.1 LDA for Unknown Data
LDA is a probabilistic model that treats documents
as mixtures of topics. It learns topics as discrete
distributions (multinomials) over the event patterns,
and thus meets our needs as it clusters patterns based
on co-occurrence in documents. The algorithm re-
quires the number of topics to be known ahead of
time, but in practice this number is set relatively high
and the resulting topics are still useful. Our best per-
forming LDA model used 200 topics. We had mixed
success with LDA though, and ultimately found our
next approach performed slightly better on the doc-
ument classification evaluation.
4.1.2 Clustering on Event Distance
Agglomerative clustering does not require fore-
knowledge of the templates, but its success relies on
how event pattern similarity is determined.
Ideally, we want to learn that detonate and destroy
belong in the same cluster representing a bombing.
Vector-based approaches are often adopted to rep-
resent words as feature vectors and compute their
distance with cosine similarity. Unfortunately, these
approaches typically learn clusters of synonymous
words that can miss detonate and destroy. Our
goal is to instead capture world knowledge of co-
occuring events. We thus adopt an assumption that
closeness in the world is reflected by closeness in a
text?s discourse. We hypothesize that two patterns
are related if they occur near each other in a docu-
ment more often than chance.
Let g(wi, wj) be the distance between two events
(1 if in the same sentence, 2 in neighboring, etc). Let
Cdist(wi, wj) be the distance-weighted frequency of
978
kidnap: kidnap, kidnap:PER, abduct, release, kidnap-
ping, ransom, robbery, registration
bombing: explode, blow up, locate, place:bomb, det-
onate, damage, explosion, cause, damage, ...
attack: kill, shoot down, down, kill:civilian, kill:PER,
kill:soldier, kill:member, killing, shoot:PER, wave, ...
arson: burn, search, burning, clip, collaborate, ...
Figure 1: The 4 clusters mapped to MUC-4 templates.
two events occurring together:
Cdist(wi, wj) =
?
d?D
?
wi,wj?d
1? log4(g(wi, wj)) (1)
where d is a document in the set of all documents
D. The base 4 logarithm discounts neighboring sen-
tences by 0.5 and within the same sentence scores 1.
Using this definition of distance, pointwise mutual
information measures our similarity of two events:
pmi(wi, wj) = Pdist(wi, wj)/(P (wi)P (wj)) (2)
P (wi) =
C(wi)
?
j C(wj)
(3)
Pdist(wi, wj) =
Cdist(wi, wj)
?
k
?
l Cdist(wk, wl)
(4)
We run agglomerative clustering with pmi over
all event patterns. Merging decisions use the average
link score between all new links across two clusters.
As with all clustering algorithms, a stopping crite-
rion is needed. We continue merging clusters un-
til any single cluster grows beyond m patterns. We
briefly inspected the clustering process and chose
m = 40 to prevent learned scenarios from intuitively
growing too large and ambiguous. Post-evaluation
analysis shows that this value has wide flexibility.
For example, the Kidnap and Arson clusters are un-
changed in 30 < m < 80, and Bombing unchanged
in 30 < m < 50. Figure 1 shows 3 clusters (of 77
learned) that characterize the main template types.
4.2 Information Retrieval for Templates
Learning a domain often suffers from a lack of train-
ing data. The previous section clustered events from
the MUC-4 corpus, but its 1300 documents do not
provide enough examples of verbs and argument
counts to further learn the semantic roles in each
cluster. Our solution is to assemble a larger IR-
corpus of documents for each cluster. For exam-
ple, MUC-4 labels 83 documents with Kidnap, but
our learned cluster (kidnap, abduct, release, ...) re-
trieved 3954 documents from a general corpus.
We use the Associated Press and New York Times
sections of the Gigaword Corpus (Graff, 2002) as
our general corpus. These sections include approxi-
mately 3.5 million news articles spanning 12 years.
Our retrieval algorithm retrieves documents that
score highly with a cluster?s tokens. The docu-
ment score is defined by two common metrics: word
match, and word coverage. A document?s match
score is defined as the average number of times the
words in cluster c appear in document d:
avgm(d, c) =
?
w?c
?
t?d 1{w = t}
|c|
(5)
We define word coverage as the number of seen
cluster words. Coverage penalizes documents that
score highly by repeating a single cluster word a lot.
We only score a document if its coverage, cvg(d, c),
is at least 3 words (or less for tiny clusters):
ir(d, c) =
{
avgm(d, c) if cvg(d, c) > min(3, |c|/4)
0 otherwise
A document d is retrieved for a cluster c if
ir(d, c) > 0.4. Finally, we emphasize precision
by pruning away 50% of a cluster?s retrieved doc-
uments that are farthest in distance from the mean
document of the retrieved set. Distance is the co-
sine similarity between bag-of-words vector repre-
sentations. The confidence value of 0.4 was chosen
from a manual inspection among a single cluster?s
retrieved documents. Pruning 50% was arbitrarily
chosen to improve precision, and we did not exper-
iment with other quantities. A search for optimum
parameter values may lead to better results.
4.3 Inducing Semantic Roles (Slots)
Having successfully clustered event words and re-
trieved an IR-corpus for each cluster, we now ad-
dress the problem of inducing semantic roles. Our
learned roles will then extract entities in the next sec-
tion and we will evaluate their per-role accuracy.
Most work on unsupervised role induction fo-
cuses on learning verb-specific roles, starting with
seed examples (Swier and Stevenson, 2004; He and
979
Gildea, 2006) and/or knowing the number of roles
(Grenager and Manning, 2006; Lang and Lapata,
2010). Our previous work (Chambers and Juraf-
sky, 2009) learned situation-specific roles over nar-
rative schemas, similar to frame roles in FrameNet
(Baker et al, 1998). Schemas link the syntactic rela-
tions of verbs by clustering them based on observing
coreferring arguments in those positions. This paper
extends this intuition by introducing a new vector-
based approach to coreference similarity.
4.3.1 Syntactic Relations as Roles
We learn the roles of cluster C by clustering the syn-
tactic relations RC of its words. Consider the fol-
lowing example:
C = {go off, explode, set off, damage, destroy}
RC = {go off:s, go off:p in, explode:s, set off:s}
where verb:s is the verb?s subject, :o the object, and
p in a preposition. We ideally want to cluster RC as:
bomb = {go off:s, explode:s, set off:o, destroy:s}
suspect = {set off:s}
target = {go off:p in, destroy:o}
We want to cluster all subjects, objects, and
prepositions. Passive voice is normalized to active2.
We adopt two views of relation similarity:
coreferring arguments and selectional preferences.
Chambers and Jurafsky (2008) observed that core-
ferring arguments suggest a semantic relation be-
tween two predicates. In the sentence, he ran and
then he fell, the subjects of run and fall corefer, and
so they likely belong to the same scenario-specific
semantic role. We applied this idea to a new vec-
tor similarity framework. We represent a relation
as a vector of all relations with which their argu-
ments coreferred. For instance, arguments of the
relation go off:s were seen coreferring with men-
tions in plant:o, set off:o and injure:s. We represent
go off:s as a vector of these relation counts, calling
this its coref vector representation.
Selectional preferences (SPs) are also useful in
measuring similarity (Erk and Pado, 2008). A re-
lation can be represented as a vector of its observed
arguments during training. The SPs for go off:s in
our data include {bomb, device, charge, explosion}.
We measure similarity using cosine similarity be-
tween the vectors in both approaches. However,
2We use the Stanford Parser at nlp.stanford.edu/software
coreference and SPs measure different types of sim-
ilarity. Coreference is a looser narrative similarity
(bombings cause injuries), while SPs capture syn-
onymy (plant and place have similar arguments). We
observed that many narrative relations are not syn-
onymous, and vice versa. We thus take the max-
imum of either cosine score as our final similarity
metric between two relations. We then back off to
the average of the two cosine scores if the max is not
confident (less than 0.7); the average penalizes the
pair. We chose the value of 0.7 from a grid search to
optimize extraction results on the training set.
4.3.2 Clustering Syntactic Functions
We use agglomerative clustering with the above
pairwise similarity metric. Cluster similarity is the
average link score over all new links crossing two
clusters. We include the following sparsity penalty
r(ca, cb) if there are too few links between clusters
ca and cb.
score(ca, cb) =
?
wi?ca
?
wj?cb
sim(wi, wj)?r(ca, cb) (6)
r(ca, cb) =
?
wi?ca
?
wj?cb
1{sim(wi, wj) > 0}
?
wi?ca
?
wj?cb
1
(7)
This penalizes clusters from merging when they
share only a few high scoring edges. Clustering
stops when the merged cluster scores drop below
a threshold optimized to extraction performance on
the training data.
We also begin with two assumptions about syntac-
tic functions and semantic roles. The first assumes
that the subject and object of a verb carry different
semantic roles. For instance, the subject of sell fills
a different role (Seller) than the object (Good). The
second assumption is that each semantic role has a
high-level entity type. For instance, the subject of
sell is a Person or Organization, and the object is a
Physical Object.
We implement the first assumption as a constraint
in the clustering algorithm, preventing two clusters
from merging if their union contains the same verb?s
subject and object.
We implement the second assumption by auto-
matically labeling each syntactic function with a role
type based on its observed arguments. The role types
are broad general classes: Person/Org, Physical Ob-
ject, or Other. A syntactic function is labeled as a
980
Bombing Template (MUC-4)
Perpetrator Person/Org who detonates, blows up, plants,
hurls, stages, is detained, is suspected, is blamed on,
launches
Instrument A physical object that is exploded, explodes, is
hurled, causes, goes off, is planted, damages, is set off, is
defused
Target A physical object that is damaged, is destroyed, is
exploded at, is damaged, is thrown at, is hit, is struck
Police Person/Org who raids, questions, discovers, investi-
gates, defuses, arrests
N/A A physical object that is blown up, destroys
Attack/Shooting Template (MUC-4)
Perpetrator Person/Org who assassinates, patrols, am-
bushes, raids, shoots, is linked to
Victim Person/Org who is assassinated, is toppled, is gunned
down, is executed, is evacuated
Target Person/Org who is hit, is struck, is downed, is set fire
to, is blown up, surrounded
Instrument A physical object that is fired, injures, downs, is
set off, is exploded
Kidnap Template (MUC-4)
Perpetrator Person/Org who releases, abducts, kidnaps,
ambushes, holds, forces, captures, is imprisoned, frees
Target Person/Org who is kidnapped, is released, is freed,
escapes, disappears, travels, is harmed, is threatened
Police Person/Org who rules out, negotiates, condemns, is
pressured, finds, arrests, combs
Weapons Smuggling Template (NEW)
Perpetrator Person/Org who smuggles, is seized from, is
captured, is detained
Police Person/Org who raids, seizes, captures, confiscates,
detains, investigates
Instrument A physical object that is smuggled, is seized, is
confiscated, is transported
Election Template (NEW)
Voter Person/Org who chooses, is intimidated, favors, is ap-
pealed to, turns out
Government Person/Org who authorizes, is chosen, blames,
authorizes, denies
Candidate Person/Org who resigns, unites, advocates, ma-
nipulates, pledges, is blamed
Figure 2: Five learned example templates. All knowledge except the template/role names (e.g., ?Victim?) is learned.
class if 20% of its arguments appear under the cor-
responding WordNet synset3, or if the NER system
labels them as such. Once labeled by type, we sep-
arately cluster the syntactic functions for each role
type. For instance, Person functions are clustered
separate from Physical Object functions. Figure 2
shows some of the resulting roles.
Finally, since agglomerative clustering makes
hard decisions, related events to a template may have
been excluded in the initial event clustering stage.
To address this problem, we identify the 200 nearby
events to each event cluster. These are simply the
top scoring event patterns with the cluster?s original
events. We add their syntactic functions to their best
matching roles. This expands the coverage of each
learned role. Varying the 200 amount does not lead
to wide variation in extraction performance. Once
induced, the roles are evaluated by their entity ex-
traction performance in Section 5.
4.4 Template Evaluation
We now compare our learned templates to those
hand-created by human annotators for the MUC-4
terrorism corpus. The corpus contains 6 template
3Physical objects are defined as non-person physical objects
Bombing Kidnap Attack Arson
Perpetrator x x x x
Victim x x x x
Target x x x
Instrument x x
Figure 3: Slots in the hand-crafted MUC-4 templates.
types, but two of them occur in only 4 and 14 of the
1300 training documents. We thus only evaluate the
4 main templates (bombing, kidnapping, attack, and
arson). The gold slots are shown in figure 3.
We evaluate the four learned templates that score
highest in the document classification evaluation
(to be described in section 5.1), aligned with their
MUC-4 types. Figure 2 shows three of our four tem-
plates, and two brand new ones that our algorithm
learned. Of the four templates, we learned 12 of the
13 semantic roles as created for MUC. In addition,
we learned a new role not in MUC for bombings,
kidnappings, and arson: the Police or Authorities
role. The annotators chose not to include this in their
labeling, but this knowledge is clearly relevant when
understanding such events, so we consider it correct.
There is one additional Bombing and one Arson role
that does not align with MUC-4, marked incorrect.
981
We thus report 92% slot recall, and precision as 14
of 16 (88%) learned slots.
We only measure agreement with the MUC tem-
plate schemas, but our system learns other events as
well. We show two such examples in figure 2: the
Weapons Smuggling and Election Templates.
5 Information Extraction: Slot Filling
We now present how to apply our learned templates
to information extraction. This section will describe
how to extract slot fillers using our templates, but
without knowing which templates are correct.
We could simply use a standard IE approach, for
example, creating seed words for our new learned
templates. But instead, we propose a new method
that obviates the need for even a limited human la-
beling of seed sets. We consider each learned se-
mantic role as a potential slot, and we extract slot
fillers using the syntactic functions that were previ-
ously learned. Thus, the learned syntactic patterns
(e.g., the subject of release) serve the dual purpose
of both inducing the template slots, and extracting
appropriate slot fillers from text.
5.1 Document Classification
A document is labeled for a template if two different
conditions are met: (1) it contains at least one trig-
ger phrase, and (2) its average per-token conditional
probability meets a strict threshold.
Both conditions require a definition of the condi-
tional probability of a template given a token. The
conditional is defined as the token?s importance rel-
ative to its uniqueness across all templates. This
is not the usual conditional probability definition as
IR-corpora are different sizes.
P (t|w) =
PIRt(w)?
s?T PIRs(w)
(8)
where PIRt(w) is the probability of pattern w in the
IR-corpus of template t.
PIRt(w) =
Ct(w)
?
v Ct(v)
(9)
where Ct(w) is the number of times word w appears
in the IR-corpus of template t. A template?s trigger
words are defined as words satisfying P (t|w) > 0.2.
Kidnap Bomb Attack Arson
Precision .64 .83 .66 .30
Recall .54 .63 .35 1.0
F1 .58 .72 .46 .46
Figure 4: Document classification results on test.
Trigger phrases are thus template-specific patterns
that are highly indicative of that template.
After identifying triggers, we use the above defi-
nition to score a document with a template. A doc-
ument is labeled with a template if it contains at
least one trigger, and its average word probability
is greater than a parameter optimized on the training
set. A document can be (and often is) labeled with
multiple templates.
Finally, we label the sentences that contain trig-
gers and use them for extraction in section 5.2.
5.1.1 Experiment: Document Classification
The MUC-4 corpus links templates to documents,
allowing us to evaluate our document labels. We
treat each link as a gold label (kidnap, bomb, or
attack) for that document, and documents can have
multiple labels. Our learned clusters naturally do not
have MUC labels, so we report results on the four
clusters that score highest with each label.
Figure 4 shows the document classification
scores. The bombing template performs best with
an F1 score of .72. Arson occurs very few times,
and Attack is lower because it is essentially an ag-
glomeration of diverse events (discussed later).
5.2 Entity Extraction
Once documents are labeled with templates, we next
extract entities into the template slots. Extraction oc-
curs in the trigger sentences from the previous sec-
tion. The extraction process is two-fold:
1. Extract all NPs that are arguments of patterns in the
template?s induced roles.
2. Extract NPs whose heads are observed frequently
with one of the roles (e.g., ?bomb? is seen with In-
strument relations in figure 2).
Take the following MUC-4 sentence as an example:
The two bombs were planted with the exclusive
purpose of intimidating the owners of...
982
The verb plant is in our learned bombing cluster, so
step (1) will extract its passive subject bombs and
map it to the correct instrument role (see figure 2).
The human target, owners, is missed because intim-
idate was not learned. However, if owner is in the
selectional preferences of the learned ?human target?
role, step (2) correctly extracts it into that role.
These are two different, but complementary,
views of semantic roles. The first is that a role is de-
fined by the set of syntactic relations that describe it.
Thus, we find all role relations and save their argu-
ments (pattern extraction). The second view is that
a role is defined by the arguments that fill it. Thus,
we extract all arguments that filled a role in training,
regardless of their current syntactic environment.
Finally, we filter extractions whose WordNet or
named entity label does not match the learned slot?s
type (e.g., a Location does not match a Person).
6 Standard Evaluation
We trained on the 1300 documents in the MUC-4
corpus and tested on the 200 document TST3 and
TST4 test set. We evaluate the four string-based
slots: perpetrator, physical target, human target, and
instrument. We merge MUC?s two perpetrator slots
(individuals and orgs) into one gold Perpetrator slot.
As in Patwardhan and Riloff (2007; 2009), we ig-
nore missed optional slots in computing recall. We
induced clusters in training, performed IR, and in-
duced the slots. We then extracted entities from the
test documents as described in section 5.2.
The standard evaluation for this corpus is to report
the F1 score for slot type accuracy, ignoring the tem-
plate type. For instance, a perpetrator of a bombing
and a perpetrator of an attack are treated the same.
This allows supervised classifiers to train on all per-
petrators at once, rather than template-specific learn-
ers. Although not ideal for our learning goals, we
report it for comparison against previous work.
Several supervised approaches have presented re-
sults on MUC-4, but unfortunately we cannot com-
pare against them. Maslennikov and Chua (2006;
2007) evaluated a random subset of test (they report
.60 and .63 F1), and Xiao et al (2004) did not eval-
uate all slot types (they report .57 F1).
Figure 5 thus shows our results with previous
work that is comparable: the fully supervised and
P R F1
Patwardhan & Riloff-09 : Supervised 48 59 53
Patwardhan & Riloff-07 : Weak-Sup 42 48 44
Our Results (1 attack) 48 25 33
Our Results (5 attack) 44 36 40
Figure 5: MUC-4 extraction, ignoring template type.
F1 Score Kidnap Bomb Arson Attack
Results .53 .43 .42 .16 / .25
Figure 6: Performance of individual templates. Attack
compares our 1 vs 5 best templates.
weakly supervised approaches of Patwardhan and
Riloff (2009; 2007). We give two numbers for our
system: mapping one learned template to Attack,
and mapping five. Our learned templates for Attack
have a different granularity than MUC-4. Rather
than one broad Attack type, we learn several: Shoot-
ing, Murder, Coup, General Injury, and Pipeline At-
tack. We see these subtypes as strengths of our al-
gorithm, but it misses the MUC-4 granularity of At-
tack. We thus show results when we apply the best
five learned templates to Attack, rather than just one.
The final F1 with these Attack subtypes is .40.
Our precision is as good as (and our F1 score near)
two algorithms that require knowledge of the tem-
plates and/or labeled data. Our algorithm instead
learned this knowledge without such supervision.
7 Specific Evaluation
In order to more precisely evaluate each learned
template, we also evaluated per-template perfor-
mance. Instead of merging all slots across all tem-
plate types, we score the slots within each template
type. This is a stricter evaluation than Section 6; for
example, bombing victims assigned to attacks were
previously deemed correct4.
Figure 6 gives our results. Three of the four tem-
plates score at or above .42 F1, showing that our
lower score from the previous section is mainly due
to the Attack template. Arson also unexpectedly
4We do not address the task of template instance identifica-
tion (e.g., splitting two bombings into separate instances). This
requires deeper discourse analysis not addressed by this paper.
983
Precision Recall F1
Kidnap .82 .47 .60 (+.07)
Bomb .60 .36 .45 (+.02)
Arson 1.0 .29 .44 (+.02)
Attack .36 .09 .15 (0.0)
Figure 7: Performance of each template type, but only
evaluated on documents labeled with each type. All oth-
ers are removed from test. The parentheses indicate F1
gain over evaluating on all test documents (figure 6).
scored well. It only occurs in 40 documents overall,
suggesting our algorithm works with little evidence.
Per-template performace is good, and our .40
overall score from the previous section illustrates
that we perform quite well in comparison to the .44-
.53 range of weakly and fully supervised results.
These evaluations use the standard TST3 and
TST4 test sets, including the documents that are not
labeled with any templates. 74 of the 200 test doc-
uments are unlabeled. In order to determine where
the system?s false positives originate, we also mea-
sure performance only on the 126 test documents
that have at least one template. Figure 7 presents the
results on this subset. Kidnap improves most signifi-
cantly in F1 score (7 F1 points absolute), but the oth-
ers only change slightly. Most of the false positives
in the system thus do not originate from the unla-
beled documents (the 74 unlabeled), but rather from
extracting incorrect entities from correctly identified
documents (the 126 labeled).
8 Discussion
Template-based IE systems typically assume knowl-
edge of the domain and its templates. We began
by showing that domain knowledge isn?t necessar-
ily required; we learned the MUC-4 template struc-
ture with surprising accuracy, learning new seman-
tic roles and several new template structures. We
are the first to our knowledge to automatically in-
duce MUC-4 templates. It is possible to take these
learned slots and use a previous approach to IE (such
as seed-based bootstrapping), but we presented an
algorithm that instead uses our learned syntactic pat-
terns. We achieved results with comparable preci-
sion, and an F1 score of .40 that approaches prior
algorithms that rely on hand-crafted knowledge.
The extraction results are encouraging, but the
template induction itself is a central contribution of
this work. Knowledge induction plays an important
role in moving to new domains and assisting users
who may not know what a corpus contains. Re-
cent work in Open IE learns atomic relations (Banko
et al, 2007b), but little work focuses on structured
scenarios. We learned more templates than just the
main MUC-4 templates. A user who seeks to know
what information is in a body of text would instantly
recognize these as key templates, and could then ex-
tract the central entities.
We hope to address in the future how the al-
gorithm?s unsupervised nature hurts recall. With-
out labeled or seed examples, it does not learn as
many patterns or robust classifiers as supervised ap-
proaches. We will investigate new text sources and
algorithms to try and capture more knowledge. The
final experiment in figure 7 shows that perhaps new
work should first focus on pattern learning and entity
extraction, rather than document identification.
Finally, while our pipelined approach (template
induction with an IR stage followed by entity ex-
traction) has the advantages of flexibility in devel-
opment and efficiency, it does involve a number
of parameters. We believe the IR parameters are
quite robust, and did not heavily focus on improving
this stage, but the two clustering steps during tem-
plate induction require parameters to control stop-
ping conditions and word filtering. While all learn-
ing algorithms require parameters, we think it is im-
portant for future work to focus on removing some
of these to help the algorithm be even more robust to
new domains and genres.
Acknowledgments
This work was supported by the National Science
Foundation IIS-0811974, and this material is also
based upon work supported by the Air Force Re-
search Laboratory (AFRL) under prime contract no.
FA8750-09-C-0181. Any opinions, findings, and
conclusion or recommendations expressed in this
material are those of the authors and do not necessar-
ily reflect the view of the Air Force Research Labo-
ratory (AFRL). Thanks to the Stanford NLP Group
and reviewers for helpful suggestions.
984
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Christian
Boitet and Pete Whitelock, editors, ACL-98, pages 86?
90, San Francisco, California. Morgan Kaufmann Pub-
lishers.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007a. Learning
relations from the web. In Proceedings of the Interna-
tional Joint Conferences on Artificial Intelligence (IJ-
CAI).
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007b. Open in-
formation extraction from the web. In Proceedings of
the International Joint Conferences on Artificial Intel-
ligence (IJCAI).
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent dirichlet alocation. Journal of Machine Learning
Research.
Razvan Bunescu and Raymond Mooney. 2004. Collec-
tive information extraction with relational markov net-
works. In Proceedings of the Association of Computa-
tional Linguistics (ACL), pages 438?445.
Andrew Carlson, J. Betteridge, R.C. Wang, E.R. Hr-
uschka Jr., and T.M. Mitchell. 2010. Coupled semi-
supervised learning for information extraction. In Pro-
ceedings of the ACM International Conference on Web
Search and Data Mining (WSDM).
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings of the Association of Computational Linguistics
(ACL), Hawaii, USA.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proceedings of the Association of Computa-
tional Linguistics (ACL), Columbus, Ohio.
Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.
2003. Closing the gap: Learning-based information
extraction rivaling knowledge-engineering methods.
In Proceedings of the Association of Computational
Linguistics (ACL).
Nancy Chinchor, David Lewis, and Lynette Hirschman.
1993. Evaluating message understanding systems: an
analysis of the third message understanding confer-
ence. Computational Linguistics, 19:3:409?449.
Katrin Erk and Sebastian Pado. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods on Natural Language Processing (EMNLP).
Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen
McKeown. 2006. Automatic creation of domain tem-
plates. In Proceedings of the Association of Computa-
tional Linguistics (ACL).
Dayne Freitag. 1998. Toward general-purpose learning
for information extraction. In Proceedings of the As-
sociation of Computational Linguistics (ACL), pages
404?408.
David Graff. 2002. English gigaword. Linguistic Data
Consortium.
Trond Grenager and Christopher D. Manning. 2006. Un-
supervised discovery of a statistical verb lexicon. In
Proceedings of the the 2006 Conference on Empirical
Methods on Natural Language Processing (EMNLP).
Shan He and Daniel Gildea. 2006. Self-training and
co-training for semantic role labeling: Primary report.
Technical Report 891, University of Rochester.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through unsupervised cross-document infer-
ence. In Proceedings of the Association of Compu-
tational Linguistics (ACL).
Niels Kasch and Tim Oates. 2010. Mining script-like
structures from the web. In Proceedings of NAACL
HLT, pages 34?42.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In Proceedings of the North
American Association of Computational Linguistics.
Mstislav Maslennikov and Tat-Seng Chua. 2007. Auto-
matic acquisition of domain knowledge for informa-
tion extraction. In Proceedings of the Association of
Computational Linguistics (ACL).
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
ie with semantic affinity patterns and relevant regions.
In Proceedings of the 2007 Conference on Empirical
Methods on Natural Language Processing (EMNLP).
Siddharth Patwardhan and Ellen Riloff. 2009. A unified
model of phrasal and sentential evidence for informa-
tion extraction. In Proceedings of the 2009 Conference
on Empirical Methods on Natural Language Process-
ing (EMNLP).
Lisa Rau, George Krupka, Paul Jacobs, Ira Sider, and
Lois Childs. 1992. Ge nltoolset: Muc-4 test results
and analysis. In Proceedings of the Message Under-
standing Conference (MUC-4), pages 94?99.
Ellen Riloff and Mark Schmelzenbach. 1998. An em-
pirical approach to conceptual case frame acquisition.
In Proceedings of the Sixth Workshop on Very Large
Corpora.
Ellen Riloff, Janyce Wiebe, and William Phillips. 2005.
Exploiting subjectivity classification to improve infor-
mation extraction. In Proceedings of AAAI-05.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding. Lawrence Erlbaum.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
ie using unrestricted relation discovery. In Proceed-
ings of NAACL.
985
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of the Association of Computational Linguis-
tics (ACL), pages 224?231.
Beth M. Sundheim. 1991. Third message understand-
ing evaluation and conference (muc-3): Phase 1 status
report. In Proceedings of the Message Understanding
Conference.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006.
A hybrid approach for the acquisition of information
extraction patterns. In Proceedings of the EACL Work-
shop on Adaptive Text Extraction and Mining.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In Proceedings of
the 2004 Conference on Empirical Methods on Nat-
ural Language Processing (EMNLP).
Jing Xiao, Tat-Seng Chua, and Hang Cui. 2004. Cas-
cading use of soft and hard matching pattern rules
for weakly supervised information extraction. In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING).
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
COLING, pages 940?946.
986
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 98?106,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Labeling Documents with Timestamps:
Learning from their Time Expressions
Nathanael Chambers
Department of Computer Science
United States Naval Academy
nchamber@usna.edu
Abstract
Temporal reasoners for document understand-
ing typically assume that a document?s cre-
ation date is known. Algorithms to ground
relative time expressions and order events of-
ten rely on this timestamp to assist the learner.
Unfortunately, the timestamp is not always
known, particularly on the Web. This pa-
per addresses the task of automatic document
timestamping, presenting two new models that
incorporate rich linguistic features about time.
The first is a discriminative classifier with
new features extracted from the text?s time
expressions (e.g., ?since 1999?). This model
alone improves on previous generative mod-
els by 77%. The second model learns prob-
abilistic constraints between time expressions
and the unknown document time. Imposing
these learned constraints on the discriminative
model further improves its accuracy. Finally,
we present a new experiment design that facil-
itates easier comparison by future work.
1 Introduction
This paper addresses a relatively new task in
the NLP community: automatic document dating.
Given a document with unknown origins, what char-
acteristics of its text indicate the year in which the
document was written? This paper proposes a learn-
ing approach that builds constraints from a docu-
ment?s use of time expressions, and combines them
with a new discriminative classifier that greatly im-
proves previous work.
The temporal reasoning community has long de-
pended on document timestamps to ground rela-
tive time expressions and events (Mani and Wilson,
2000; Llido? et al, 2001). For instance, consider
the following passage from the TimeBank corpus
(Pustejovsky et al, 2003):
And while there was no profit this year from
discontinued operations, last year they con-
tributed 34 million, before tax.
Reconstructing the timeline of events from this doc-
ument requires extensive temporal knowledge, most
notably, the document?s creation date to ground its
relative expressions (e.g., this year = 2012). Not
only did the latest TempEval competitions (Verha-
gen et al, 2007; Verhagen et al, 2009) include
tasks to link events to the (known) document cre-
ation time, but state-of-the-art event-event ordering
algorithms also rely on these timestamps (Chambers
and Jurafsky, 2008; Yoshikawa et al, 2009). This
knowledge is assumed to be available, but unfortu-
nately this is not often the case, particularly on the
Web.
Document timestamps are growing in importance
to the information retrieval (IR) and management
communities as well. Several IR applications de-
pend on knowledge of when documents were posted,
such as computing document relevance (Li and
Croft, 2003; Dakka et al, 2008) and labeling search
queries with temporal profiles (Diaz and Jones,
2004; Zhang et al, 2009). Dating documents is sim-
ilarly important to processing historical and heritage
collections of text. Some of the early work that moti-
vates this paper arose from the goal of automatically
grounding documents in their historical contexts (de
Jong et al, 2005; Kanhabua and Norvag, 2008; Ku-
mar et al, 2011). This paper builds on their work
98
by incorporating more linguistic knowledge and ex-
plicit reasoning into the learner.
The first part of this paper describes a novel learn-
ing approach to document dating, presenting a dis-
criminative model and rich linguistic features that
have not been applied to document dating. Further,
we introduce new features specific to absolute time
expressions. Our model outperforms the generative
models of previous work by 77%.
The second half of this paper describes a novel
learning algorithm that orders time expressions
against the unknown timestamp. For instance, the
phrase the second quarter of 1999 might be labeled
as being before the timestamp. These labels impose
constraints on the possible timestamp and narrow
down its range of valid dates. We combine these
constraints with our discriminative learner and see
another relative improvement in accuracy by 9%.
2 Previous Work
Most work on dating documents has come from the
IR and knowledge management communities inter-
ested in dating documents with unknown origins.
de Jong et al (2005) was among the first to auto-
matically label documents with dates. They learned
unigram language models (LMs) for specific time
periods and scored articles with log-likelihood ra-
tio scores. Kanhabua and Norvag (2008; 2009) ex-
tended this approach with the same model, but ex-
panded its unigrams with POS tags, collocations,
and tf-idf scores. They also integrated search engine
results as features, but did not see an improvement.
Both works evaluated on the news genre.
Recent work by Kumar et al (2011) focused on
dating Gutenberg short stories. As above, they
learned unigram LMs, but instead measured the KL-
divergence between a document and a time period?s
LM. Our proposed models differ from this work
by applying rich linguistic features, discriminative
models, and by focusing on how time expressions
improve accuracy. We also study the news genre.
The only work we are aware of within the NLP
community is that of Dalli and Wilks (2006). They
computed probability distributions over different
time periods (e.g., months and years) for each ob-
served token. The work is similar to the above IR
work in its bag of words approach to classification.
They focused on finding words that show periodic
spikes (defined by the word?s standard deviation in
its distribution over time), weighted with inverse
document frequency scores. They evaluated on a
subset of the Gigaword Corpus (Graff, 2002).
The experimental setup in the above work (except
Kumar et al who focus on fiction) all train on news
articles from a particular time period, and test on ar-
ticles in the same time period. This leads to possi-
ble overlap of training and testing data, particularly
since news is often reprinted across agencies the
same day. In fact, one of the systems in Kanhabua
and Norvag (2008) simply searches for one training
document that best matches a test document, and as-
signs its timestamp. We intentionally deviate from
this experimental design and instead create tempo-
rally disjoint train/test sets (see Section 5).
Finally, we extend this previous work by focusing
on aspects of language not yet addressed for docu-
ment dating: linguistic structure and absolute time
expressions. The majority of articles in our dataset
contain time expressions (e.g., the year 1998), yet
these have not been incorporated into the models de-
spite their obvious connection to the article?s times-
tamp. This paper first describes how to include
time expressions as traditional features, and then
describes a more sophisticated temporal reasoning
component that naturally fits into our classifier.
3 Timestamp Classifiers
Labeling documents with timestamps is similar to
topic classification, but instead of choosing from
topics, we choose the most likely year (or other
granularity) in which it was written. We thus begin
with a bag-of-words approach, reproducing the gen-
erative model used by both de Jong (2005) and Kan-
habua and Norvag (2008; 2009). The subsequent
sections then introduce our novel classifiers and
temporal reasoners to compare against this model.
3.1 Language Models
The model of de Jong et al (2005) uses the nor-
malized log-likelihood ratio (NLLR) to score doc-
uments. It weights tokens by the ratio of their prob-
ability in a specific year to their probability over the
entire corpus. The model thus requires an LM for
each year and an LM for the entire corpus:
99
NLLR(D,Y ) =
?
w?D
P (w|D) ? log(
P (w|Y )
P (w|C)
) (1)
where D is the target document, Y is the time span
(e.g., a year), and C is the distribution of words in
the corpus across all years. A document is labeled
with the year that satisfies argmaxYNLLR(D,Y ).
They adapted this model from earlier work in the
IR community (Kraaij, 2004). We apply Dirichlet-
smoothing to the language models (as in de Jong et
al.), although the exact choice of ? did not signifi-
cantly alter the results, most likely due to the large
size of our training corpus. Kanhabua and Norvag
added an entropy factor to the summation, but we
did not see an improvement in our experiments.
The unigrams w are lowercased tokens. We will
refer to this de Jong et al model as the Unigram
NLLR. Follow-up work by Kanhabua and Norvag
(2008) applied two filtering techniques to the uni-
grams in the model:
1. Word Classes: include only nouns, verbs, and
adjectives as labeled by a POS tagger
2. IDF Filter: include only the top-ranked terms
by tf-idf score
We also tested with these filters, choosing a cut-
off for the top-ranked terms that optimized perfor-
mance on our development data. We also stemmed
the words as Kanhabua and Norvag suggest. This
model is the Filtered NLLR.
Kanhabua and Norvag also explored what they
termed collocation features, but lacking details on
how collocations were included (or learned), we
could not reproduce this for comparison. How-
ever, we instead propose using NER labels to ex-
tract what may have counted as collocations in their
data. Named entities are important to document dat-
ing due to the nature of people and places coming in
and out of the news at precise moments in time. We
compare the NER features against the Unigram and
Filtered NLLR models in our final experiments.
3.2 Discriminative Models
In addition to reproducing the models from previous
work, we also trained a new discriminative version
with the same features. We used a MaxEnt model
and evaluated with the same filtering methods based
on POS tags and tf-idf scores. The model performed
best on the development data without any filtering
or stemming. The final results (Section 6) only use
the lowercased unigrams. Ultimately, this MaxEnt
model vastly outperforms these NLLR models.
3.3 Models with Time Expressions
The above language modeling and MaxEnt ap-
proaches are token-based classifiers that one could
apply to any topic classification domain. Barring
other knowledge, the learners solely rely on the ob-
served frequencies of unigrams in order to decide
which class is most likely. However, document dat-
ing is not just a simple topic classification applica-
tion, but rather relates to temporal phenomena that
is often explicitly described in the text itself. Lan-
guage contains words and phrases that discuss the
very time periods we aim to recover. These expres-
sions should be better incorporated into the learner.
3.3.1 Motivation
Let the following snippet serve as a text example
with an ambiguous creation time:
Then there?s the fund-raiser at the American
Museum of Natural History, which plans to
welcome about 1,500 guests paying $1,000 to
$5,000. Their tickets will entitle them to a pre-
view of...the new Hayden Planetarium.
Without extremely detailed knowledge about the
American Museum of Natural History, the events
discussed here are difficult to place in time, let alne
when the author reported it. However, time expres-
sions are sometimes included, and the last sentence
in the original text contains a helpful relative clause:
Their tickets will entitle them to a preview
of...the new Hayden Planetarium, which does
not officially open until February 2000.
This one clause is more valuable than the rest of
the document, allowing us to infer that the docu-
ment?s timestamp is before February, 2000. An ed-
ucated guess might surmise the article appeared in
the year prior, 1999, which is the correct year. At
the very least, this clause should eliminate all years
after 2000 from consideration. Previous work on
document dating does not integrate this information
except to include the unigram ?2000? in the model.
100
This paper discusses two complementary ways to
learn and reason about this information. The first
is to simply add richer time-based features into the
model. The second is to build separate learners that
can assign probabilities to entire ranges of dates,
such as all years following 2000 in the example
above. We begin with the feature-based model.
3.3.2 Time Features
To our knowledge, the following time features
have not been used in a document dating setting.
We use the freely available Stanford Parser and NER
system1 to generate the syntactic interpretation for
these features. We then train a MaxEnt classifier and
compare against previous work.
Typed Dependency: The most basic time feature is
including governors of year mentions and the rela-
tion between them. This covers important contexts
that determine the semantics of the time frame, like
prepositions. For example, consider the following
context for the mention 1997:
Torre, who watched the Kansas City Royals
beat the Yankees, 13-6, on Friday for the first
time since 1997.
The resulting feature is ?since pobj 1997?.
Typed Dependency POS: Similar to Typed Depen-
dency, this feature uses POS tags of the dependency
relation?s governor. The feature from the previous
example is now ?PP pobj 1997?. This generalizes
the features to capture time expressions with prepo-
sitions, as noun modifiers, or other constructs.
Verb Tense: An important syntactic feature for tem-
poral positioning is the tense of the verb that domi-
nates the time expression. A past tense verb situates
the phrase in 2003 differently than one in the future.
We traverse the sentence?s parse tree until a gover-
nor with a VB* tag is found, and determine its tense
through hand constructed rules based on the struc-
ture of the parent VP. The verb tense feature takes a
value of past, present, future, or undetermined.
Verb Path: The verb path feature is the dependency
path from the nearest verb to the year expression.
The following snippet will include the feature, ?ex-
pected prep in pobj 2002?.
1http://nlp.stanford.edu/software
Finance Article from Jan. 2002
Text Snippet Relation to 2002
...started a hedge fund before the
market peaked in 2000.
before
The peak in economic activity was
the 4th quarter of 1999.
before
...might have difficulty in the latter
part of 2002.
simultaneous
Figure 1: Three year mentions and their relation to the
document creation year. Relations can be correctly iden-
tified for training using known document timestamps.
Supervising them is Vice President Hu Jintao,
who appears to be Jiang?s favored successor if
he retires from leadership as expected in 2002.
Named Entities: Although not directly related to
time expressions, we also include n-grams of tokens
that are labeled by an NER system using Person, Or-
ganization, or Location. People and places are often
discussed during specific time periods, particularly
in the news genre. Collecting named entity mentions
will differentiate between an article discussing a bill
and one discussing the US President, Bill Clinton.
We extract NER features as sequences of uninter-
rupted tokens labeled with the same NER tag, ignor-
ing unigrams (since unigrams are already included
in the base model). Using the Verb Path example
above, the bigram feature Hu Jintao is included.
4 Learning Time Constraints
This section departs from the above document clas-
sifiers and instead classifies individual emphyear
mentions. The goal is to automatically learn tem-
poral constraints on the document?s timestamp.
Instead of predicting a single year for a document,
a temporal constraint predicts a range of years. Each
time mention, such as ?not since 2009?, is a con-
straint representing its relation to the document?s
timestamp. For example, the mentioned year ?2009?
must occur before the year of document creation.
This section builds a classifier to label time mentions
with their relations (e.g., before, after, or simultane-
ous with the document?s timestamp), enabling these
mentions to constrain the document classifiers de-
scribed above. Figure 1 gives an example of time
mentions and the desired labels we wish to learn.
To better motivate the need for constraints, let
101
1995 1996 1997 1998 1999 2000 2001 2004 20050
0.05
0.1
0.15
0.2
Pro
bab
ility
Year Class
Figure 2: Distribution over years for a single document
as output by a MaxEnt classifier.
Figure 2 illustrate a typical distribution output by a
document classifier for a training document. Two
of the years appear likely (1999 and 2001), how-
ever, the document contains a time expression that
seems to impose a strict constraint that should elim-
inate 2001 from consideration:
Their tickets will entitle them to a preview
of...the new Hayden Planetarium, which does
not officially open until February 2000.
The clause until February 2000 in a present tense
context may not definitively identify the document?s
timestamp (1999 is a good guess), but as discussed
earlier, it should remove all future years beyond
2000 from consideration. We thus want to impose
a constraint based on this phrase that says, loosely,
?this document was likely written before 2000?.
The document classifiers described in previous
sections cannot capture such ordering information.
Our new time features in Section 3.3.2 add richer
time information (such as until pobj 2000 and open
prep until pobj 2000), but they compete with many
other features that can mislead the final classifica-
tion. An independent constraint learner may push
the document classifier in the right direction.
4.1 Constraint Types
We learn several types of constraints between each
year mention and the document?s timestamp. Year
mentions are defined as tokens with exactly four
digits, numerically between 1900 and 2100. Let T
be the document timestamp?s year, and M the year
mention. We define three core relations:
1. Before Timestamp: M < T
2. After Timestamp: M > T
3. Same as Timestamp: M == T
We also experiment with 7 fine-grained relations:
1. One year Before Timestamp: M == T ? 1
2. Two years Before Timestamp: M == T ? 2
3. Three+ years Before Timestamp: M < T ? 2
4. One year After Timestamp: M == T + 1
5. Two years After Timestamp: M == T + 2
6. Three+ years After Timestamp: M > T + 2
7. Same Year and Timestamp: M == T
Obviously the more fine-grained a relation, the bet-
ter it can inform a classifier. We experiment with
these two granularities to compare performance.
The learning process is a typical training envi-
ronment where year mentions are treated as labeled
training examples. Labels for year mentions are
automatically computed by comparing the actual
timestamp of the training document (all documents
in Gigaword have dates) with the integer value of
the year token. For example, a document written in
1997 might contain the phrase, ?in the year 2000?.
The year token (2000) is thus three+ years after the
timestamp (1997). We use this relation for the year
mention as a labeled training example.
Ultimately, we want to use similar syntactic con-
structs in training so that ?in the year 2000? and ?in
the year 2003? mutually inform each other. We thus
compute the label for each time expression, and re-
place the integer year with the generic YEAR token
to generalize mentions. The text for this example be-
comes ?in the year YEAR? (labeled as three+ years
after). We train a MaxEnt model on each year men-
tion, to be described next. Table 2 gives the overall
counts for the core relations in our training data. The
vast majority of year mentions are references to the
future (e.g. after the timestamp).
4.2 Constraint Learner
The features we use to classify year mentions are
given in Table 1. The same time features in the docu-
ment classifier of Section 3.3.2 are included, as well
as several others specific to this constraint task.
We use a MaxEnt classifier trained on the individ-
ual year mentions. Documents often contain multi-
ple (and different) year mentions; all are included in
training and testing. This classifier labels mentions
with relations, but in order to influence the document
classifier, we need to map the relations to individual
102
Time Constraint Features
Typed Dep. Same as Section 3.3.2
Verb Tense Same as Section 3.3.2
Verb Path Same as Section 3.3.2
Decade The decade of the year mention
Bag of Words Unigrams in the year?s sentence
n-gram The 4-gram and 3-gram that end
with the year
n-gram POS The 4-gram and 3-gram of POS tags
that end with the year
Table 1: Features used to classify year expressions.
Constraint Count
After Timestamp 1,203,010
Before Timestamp 168,185
Same as Timestamp 141,201
Table 2: Training size of year mentions (and their relation
to the document timestamp) in Gigaword?s NYT section.
year predictions. Let Td be the set of mentions in
document d. We represent a MaxEnt classifier by
PY (R|t) for a time mention t ? Td and possible re-
lations R. We map this distribution over relations to
a distribution over years by defining Pyear(Y |d):
Pyear(y|d) =
1
Z(Td)
?
t?Td
PY (rel(val(t)? y)|t) (2)
rel(x) =
?
?
?
before if x < 0
after if x > 0
simultaneous otherwise
(3)
where val(t) is the integer year of the year mention
andZ(Td) is the partition function. The rel(val(t)?
y) function simply determines if the year mention t
(e.g., 2003) is before, after, or overlaps the year we
are predicting for the document?s unknown times-
tamp y. We use a similar function for the seven fine-
grained relations. Figure 3 visually illustrates how
Pyear(y|d) is constructed from three year mentions.
4.3 Joint Classifier
Finally, given the document classifiers of Section 3
and the constraint classifier just defined in Section 4,
we create a joint model combining the two with the
following linear interpolation:
P (y|d) = ?Pdoc(y|d) + (1? ?)Pyear(y|d) (4)
where y is a year, and d is the document. ? was set
to 0.35 by maximizing accuracy on the dev set. See
0 0.2 0.4 0.6 0.8 10.515
0.52
0.525
0.53
0.535
0.54
0.545
Lambda Value
Acc
urac
y
Lambda Parameter Accuracy
Figure 4: Development set accuracy and ? values.
Figure 4. This optimal ? = .35 weights the con-
straint classifier higher than the document classifier.
5 Datasets
This paper uses the New York Times section of the
Gigaword Corpus (Graff, 2002) for evaluation. Most
previous work on document dating evaluates on the
news genre, so we maintain the pattern for consis-
tency. Unfortunately, we cannot compare to these
previous experiments because of differing evalua-
tion setups. Dalli and Wilks (2006) is most similar in
their use of Gigaword, but they chose a random set
of documents that cannot be reproduced. We instead
define specific segments of the corpus for evaluation.
The main goal for this experiment setup was to es-
tablish specific training, development, and test sets.
One of the potential difficulties in testing with news
articles is that the same story is often reprinted with
very minimal (or no) changes. Over 10% of the doc-
uments in the New York Times section of the Giga-
word Corpus are exact or approximate duplicates of
another document in the corpus2. A training set for
document dating must not include duplicates from
the test set.
We adopt the intuition behind the experimen-
tal setup used in other NLP domains, like parsing,
where the entire test set is from a contiguous sec-
tion of the corpus (as opposed to randomly selected
examples across the corpus). As the parsing com-
munity trains on sections 2-21 of the Penn Treebank
(Marcus et al, 1993) and tests on section 23, we cre-
ate Gigaword sections by isolating specific months.
2Approximate duplicate is defined as an article whose first
two sentences exactly match the first two of another article.
Only the second matched document is counted as a duplicate.
103
Year Distributions for Three Time Expressions
97 98 99 00 01 02 03 04 0596
PY(y | "peaked in 2000")
PY(y | "was the quarter of 1999")
PY(y | "will have difficulty in part of 2003")
Final Distribution  -  Pyear(y|d)
0.2
0.0
0.2
0.0
0.2
0.0
0.2
0.0
Figure 3: Three year mentions in a document and the distributions output by the learner. The document is from 2002.
The dots indicate the before, same, and after relation probabilities. The combination of three constraints results in a
final distribution that gives the years 2001 and 2002 the highest probability. This distribution can help a document
classifier make a more informed final decision.
Training Jan-May and Sep-Dec
Development July
Testing June and August
In other words, the development set includes docu-
ments from July 1995, July 1996, July 1997, etc. We
chose the dev/test sets to be in the middle of the year
so that the training set includes documents on both
temporal sides of the test articles. We include years
1995-2001 and 2004-2006, but skip 2002 and 2003
due to their abnormally small size compared to the
other years.
Finally, we experiment in a balanced data set-
ting, training and testing on the same number
of documents from each year. The test set in-
cludes 11,300 documents in each year (months
June and August) for a total of 113,000 test doc-
uments. The development set includes 7,300
from July of each year. Training includes ap-
proximately 75,000 documents in each year with
some years slightly less than 75,000 due to their
smaller size in the corpus. The total number of
training documents for the 10 evaluated years is
725,468. The full list of documents is online at
www.usna.edu/Users/cs/nchamber/data/timestamp.
6 Experiments and Results
We experiment on the Gigaword corpus as described
in Section 5. Documents are tokenized and parsed
with the Stanford Parser. The year in the times-
tamp is retrieved from the document?s Gigaword ID
which contains the year and day the article was re-
trieved. Year mentions are extracted from docu-
ments by matching all tokens with exactly four digits
whose integer is in the range of 1900 and 2100.
The MaxEnt classifiers are also from the Stanford
toolkit, and both the document and year mention
classifiers use its default settings (quadratic prior).
The ? factor in the joint classifier is optimized on
the development set as described in Section 4.3. We
also found that dev results improved when training
ignores the border months of Jan, Feb, and Dec. The
features described in this paper were selected solely
by studying performance on the development set.
The final reported results come from running on the
test set once at the end of this study.
Table 3 shows the results on the Test set for all
document classifiers. We measure accuracy to com-
pare overall performance since the test set is a bal-
anced set (each year has the same number of test
documents). Unigram NLLR and Filtered NLLR
are the language model implementations of previ-
ous work as described in Section 3.1. MaxEnt Un-
igram is our new discriminative model for this task.
MaxEnt Time is the discriminative model with rich
time features (but not NER) as described in Section
3.3.2 (Time+NER includes NER). Finally, the Joint
model is the combined document and year mention
classifiers as described in Section 4.3. Table 4 shows
the F1 scores of the Joint model by year.
Our new MaxEnt model outperforms previous
work by 55% relative accuracy. Incorporating time
features further improves the relative accuracy by
104
Model Overall Accuracy
Random Guess 10.0%
Unigram NLLR 24.1%
Filtered NLLR 29.1%
MaxEnt Unigram 45.1%
MaxEnt Time 48.3%
MaxEnt Time+NER 51.4%
Joint 53.4%
Table 3: Performance as measured by accuracy. The pre-
dicted year must exactly match the actual year.
95 96 97 98 99 00 01 02
P .57 .49 .52 .48 .47 .51 .51 .59
R .54 .56 .62 .44 .48 .48 .46 .57
F1 .55 .52 .57 .46 .48 .49 .48 .58
Table 4: Yearly results for the Joint model. 2005/06 are
omitted due to space, with F1 .56 and .63, respectively.
7%, and adding NER by another 6%. Total relative
improvement in accuracy is thus almost 77% from
the Time+NER model over Filtered NLLR. Further,
the temporal constraint model increases this best
classifier by another 3.9%. All improvements are
statistically significant (p < 0.000001, McNemar?s
test, 2-tailed). Table 6 shows that performance in-
creased most on the documents that contain at least
one year mention (60% of the corpus).
Finally, Table 5 shows the results of the tempo-
ral constraint classifiers on year mentions. Not sur-
prisingly, the fine-grained performance is quite a bit
lower than the core relations. The full Joint results
in Table 3 use the three core relations, but the seven
fine-grained relations give approximately the same
results. Its lower accuracy is mitigated by the finer
granularity (i.e., the majority class basline is lower).
7 Discussion
The main contribution of this paper is the discrimi-
native model (54% improvement) and a new set of
P R F1
Before Timestamp .95 .98 .96
Same as Timestamp .73 .57 .64
After Timestamp .84 .81 .82
Overall Accuracy 92.2%
Fine-Grained Accuracy 70.1%
Table 5: Precision, recall, and F1 for the core relations.
Accuracy for both core and fine-grained.
All With Year Mentions
MaxEnt Unigram 45.1% 46.1%
MaxEnt Time+NER 51.4% 54.3%
Joint 53.4% 57.7%
Table 6: Accuracy on all documents and documents with
at least one year mention (about 60% of the corpus).
features for document dating (14% improvement).
Such a large performance boost makes clear that the
log likelihood and entropy approaches from previ-
ous work are not as effective as discriminative mod-
els on a large training corpus. Further, token-based
features do not capture the implicit references to
time in language. Our richer syntax-based features
only apply to year mentions, but this small textual
phenomena leads to a surprising 13% relative im-
provement in accuracy. Table 6 shows that a signif-
icant chunk of this improvement comes from docu-
ments containing year mentions, as expected.
The year constraint learner also improved perfor-
mance. Although most of its features are in the doc-
ument classifier, by learning constraints it captures a
different picture of time that a traditional document
classifier does not address. Combining this picture
with the document classifier leads to another 3.9%
relative improvement. Although we focused on year
mentions here, there are several avenues for future
study, including explorations of how other types of
time expressions might inform the task. These con-
straints might also have applications to the ordering
tasks of recent TempEval competitions.
Finally, we presented a new evaluation setup for
this task. Previous work depended on having train-
ing documents in the same week and day of the test
documents. We argued that this may not be an ap-
propriate assumption in some domains, and particu-
larly problematic for the news genre. Our proposed
evaluation setup instead separates training and test-
ing data across months. The results show that log-
likelihood ratio scores do not work as well in this
environment. We hope our explicit train/test envi-
ronment encourages future comparison and progress
on document dating.
Acknowledgments
Many thanks to Stephen Guo and Dan Jurafsky for
early ideas and studies on this topic.
105
References
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal or-
dering. In Proceedings of the Conference on Em-
pirical Methods on Natural Language Processing
(EMNLP), Hawaii, USA.
W. Dakka, L. Gravano, and P. G. Ipeirotis. 2008. An-
swering general time sensitive queries. In Proceedings
of the 17th International ACM Conference on Informa-
tion and Knowledge Management, pages 1437?1438.
Angelo Dalli and Yorick Wilks. 2006. Automatic dat-
ing of documents and temporal text classification. In
Proceedings of the Workshop on Annotating and Rea-
soning about Time and Events, pages 17?22.
Franciska de Jong, Henning Rode, and Djoerd Hiemstra.
2005. Temporal language models for the disclosure of
historical text. In Humanities, computers and cultural
heritage: Proceedings of the XVIth International Con-
ference of the Association for History and Computing
(AHC 2005).
Fernando Diaz and Rosie Jones. 2004. Using temporal
profiles of queries for precision prediction. In Pro-
ceedings of the 27th Annual International ACM Spe-
cial Interest Group on Information Retrieval Confer-
ence.
David Graff. 2002. English Gigaword. Linguistic Data
Consortium.
Nattiya Kanhabua and Kjetil Norvag. 2008. Improv-
ing temporal language models for determining time of
non-timestamped documents. In Proceedings of the
12th European conference on Research and Advanced
Technology for Digital Libraries.
Nattiya Kanhabua and Kjetil Norvag. 2009. Using tem-
poral language models for document dating. Lecture
Notes in Computer Science: machine learning and
knowledge discovery in databases, 5782.
W. Kraaij. 2004. Variations on language modeling
for information retrieval. Ph.D. thesis, University of
Twente.
Abhimanu Kumar, Matthew Lease, and Jason Baldridge.
2011. Supervised language modeling for temporal res-
olution of texts. In Proceedings of CIKM.
Xiaoyan Li and W. Bruce Croft. 2003. Time-based lan-
guage models. In Proceedings of the twelfth interna-
tional conference on Information and knowledge man-
agement.
Dolores M. Llido?, Rafael Llavori, and Maria? J. Aram-
buru. 2001. Extracting temporal references to assign
document event-time periods. In Proceedings of the
12th International Conference on Database and Ex-
pert Systems Applications.
Inderjeet Mani and George Wilson. 2000. Robust tempo-
ral processing of news. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew
See, David Day, Lisa Ferro, Robert Gaizauskas, Mar-
cia Lazo, Andrea Setzer, and Beth Sundheim. 2003.
The timebank corpus. Corpus Linguistics, pages 647?
656.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Workshop on Semantic Evalu-
ations.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James Puste-
jovsky. 2009. The tempeval challenge: identifying
temporal relations in text. Special Issue: Computa-
tional Semantic Analysis of Language: SemEval-2007
and Beyond, 43(2):161?179.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identify-
ing temporal relations with markov logic. In Proceed-
ings of the Association for Computational Linguistics
(ACL).
Ruiqiang Zhang, Yi Chang, Zhaohui Zheng, Donald
Metzler, and Jian yun Nie. 2009. Search result
re-ranking by feedback control adjustment for time-
sensitive query. In Proceedings of the 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
106
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 501?506,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
An Annotation Framework for Dense Event Ordering
Taylor Cassidy
IBM Research
taylor.cassidy.ctr@mail.mil
Bill McDowell
Carnegie Mellon University
forkunited@gmail.com
Nathanael Chambers
US Naval Academy
nchamber@usna.edu
Steven Bethard
Univ. of Alabama at Birmingham
bethard@cis.uab.edu
Abstract
Today?s event ordering research is heav-
ily dependent on annotated corpora. Cur-
rent corpora influence shared evaluations
and drive algorithm development. Partly
due to this dependence, most research fo-
cuses on partial orderings of a document?s
events. For instance, the TempEval com-
petitions and the TimeBank only annotate
small portions of the event graph, focusing
on the most salient events or on specific
types of event pairs (e.g., only events in the
same sentence). Deeper temporal reason-
ers struggle with this sparsity because the
entire temporal picture is not represented.
This paper proposes a new annotation pro-
cess with a mechanism to force annotators
to label connected graphs. It generates 10
times more relations per document than the
TimeBank, and our TimeBank-Dense cor-
pus is larger than all current corpora. We
hope this process and its dense corpus en-
courages research on new global models
with deeper reasoning.
1 Introduction
The TimeBank Corpus (Pustejovsky et al, 2003)
ushered in a wave of data-driven event ordering
research. It provided for a common dataset of re-
lations between events and time expressions that
allowed the community to compare approaches.
Later corpora and competitions have based their
tasks on the TimeBank setup. This paper ad-
dresses one of its shortcomings: sparse annotation.
We describe a new annotation framework (and a
TimeBank-Dense corpus) that we believe is needed
to fulfill the data needs of deeper reasoners.
The TimeBank includes a small subset of all
possible relations in its documents. The annota-
tors were instructed to label relations critical to the
document?s understanding. The result is a sparse la-
beling that leaves much of the document unlabeled.
The TempEval contests have largely followed suit
and focused on specific types of event pairs. For
instance, TempEval (Verhagen et al, 2007) only
labeled relations between events that syntactically
dominated each other. This paper is the first attempt
to annotate a document?s entire temporal graph.
A consequence of focusing on all relations is a
shift from the traditional classification task, where
the system is given a pair of events and asked only
to label the type of relation, to an identification task,
where the system must determine for itself which
events in the document to pair up. For example, in
TempEval-1 and 2 (Verhagen et al, 2007; Verha-
gen et al, 2010), systems were given event pairs
in specific syntactic positions: events and times in
the same noun phrase, main events in consecutive
sentences, etc. We now aim for a shift in the com-
munity wherein all pairs are considered candidates
for temporal ordering, allowing researchers to ask
questions such as: how must algorithms adapt to
label the complete graph of pairs, and if the more
difficult and ambiguous event pairs are included,
how must feature-based learners change?
We are not the first to propose these questions,
but this paper is the first to directly propose the
means by which they can be addressed. The stated
goal of TempEval-3 (UzZaman et al, 2013) was to
focus on relation identification instead of classifica-
tion, but the training and evaluation data followed
the TimeBank approach where only a subset of
event pairs were labeled. As a result, many systems
focused on classification, with the top system clas-
sifying pairs in only three syntactic constructions
501
There were four or five people inside, 
and they just started firing 
 
Ms. Sanders was hit several times and 
was  pronounced dead at the scene. 
 
The other customers fled, and the 
police said it did not appear that anyone 
else was injured. 
There were four or five people inside, 
and they just started firing 
 
Ms. Sanders was hit several times and 
was pronounced dead at the scene. 
 
The other customers fled, and the 
police said it did not appear that anyone 
else was injured. 
Current Systems & Evaluations This Proposal 
Figure 1: A TimeBank annotated document is on the left, and this paper?s TimeBank-Dense annotation is
on the right. Solid arrows indicate BEFORE relations and dotted arrows indicate INCLUDED IN relations.
(Bethard, 2013). We describe the first annotation
framework that forces annotators to annotate all
pairs
1
. With this new process, we created a dense
ordering of document events that can properly eval-
uate both relation identification and relation anno-
tation. Figure 1 illustrates one document before
and after our new annotations.
2 Previous Annotation Work
The majority of corpora and competitions for event
ordering contain sparse annotations. Annotators for
the original TimeBank (Pustejovsky et al, 2003)
only annotated relations judged to be salient by
the annotator. Subsequent TempEval competitions
(Verhagen et al, 2007; Verhagen et al, 2010; Uz-
Zaman et al, 2013) mostly relied on the TimeBank,
but also aimed to improve coverage by annotating
relations between all events and times in the same
sentence. However, event tokens that were men-
tioned fewer than 20 times were excluded and only
one TempEval task considered relations between
events in different sentences. In practical terms, the
resulting evaluations remained sparse.
A major dilemma underlying these sparse tasks
is that the unlabeled event/time pairs are ambigu-
ous. Each unlabeled pair holds 3 possibilities:
1. The annotator looked at the pair of events and
decided that no temporal relation exists.
2. The annotator did not look at the pair of
events, so a relation may or may not exist.
3. The annotator failed to look at the pair of
events, so a single relation may exist.
Training and evaluation of temporal reasoners is
hampered by this ambiguity. To combat this, our
1
As discussed below, all pairs in a given window size.
Events Times Rels R
TimeBank 7935 1414 6418 0.7
Bramsen 2006 627 ? 615 1.0
TempEval-07 6832 1249 5790 0.7
TempEval-10 5688 2117 4907 0.6
TempEval-13 11145 2078 11098 0.8
Kolomiyets-12 1233 ? 1139 0.9
Do 2012
2
324 232 3132 5.6
This work 1729 289 12715 6.3
Table 1: Events, times, relations and the ratio of
relations to events + times (R) in various corpora.
annotation adopts the VAGUE relation introduced
by TempEval 2007, and our approach forces anno-
tators to use it. This is the only work that includes
such a mechanism.
This paper is not the first to look into more dense
annotations. Bramsen et al (2006) annotated multi-
sentence segments of text to build directed acyclic
graphs. Kolomiyets et al (2012) annotated ?tem-
poral dependency structures?, though they only
focused on relations between pairs of events. Do
et al (2012) produced the densest annotation, but
?the annotator was not required to annotate all pairs
of event mentions, but as many as possible?. The
current paper takes a different tack to annotation
by requiring annotators to label every possible pair
of events/times in a given window. Thus this work
is the first annotation effort that can guarantee its
event/time graph to be strongly connected.
Table 1 compares the size and density of our
corpus to others. Ours is the densest and it contains
the largest number of temporal relations.
2
Do et al (2012) reports 6264 relations, but this includes
both the relations and their inverses. We thus halve the count
502
3 A Framework for Dense Annotation
Frameworks for annotating text typically have two
independent facets: (1) the practical means of how
to label the text, and (2) the higher-level rules about
when something should be labeled. The first is
often accomplished through a markup language,
and we follow prior work in adopting TimeML here.
The second facet is the focus of this paper: when
should an annotator label an ordering relation?
Our proposal starts with documents that have al-
ready been annotated with events, time expressions,
and document creation times (DCT). The following
sentence serves as our motivating example:
Police confirmed Friday that the body
found along a highway in San Juan be-
longed to Jorge Hernandez.
This sentence is represented by a 4 node graph (3
events and 1 time). In a completely annotated graph
it would have 6 edges (relations) connecting the
nodes. In the TimeBank, from which this sentence
is drawn, only 3 of the 6 edges are labeled.
The impact of these annotation decisions (i.e.,
when to annotate a relation) can be significant. In
this example, a learner must somehow deal with
the 3 unlabeled edges. One option is to assume that
they are vague or ambiguous. However, all 6 edges
have clear well-defined ordering relations:
belonged BEFORE confirmed
belonged BEFORE found
found BEFORE confirmed
belonged BEFORE Friday
confirmed IS INCLUDED IN Friday
found IS INCLUDED IN Friday
3
Learning algorithms handle these unlabeled
edges by making incorrect assumptions, or by ig-
noring large parts of the temporal graph. Sev-
eral models with rich temporal reasoners have
been published, but since they require more con-
nected graphs, improvement over pairwise classi-
fiers have been minimal (Chambers and Jurafsky,
2008; Yoshikawa et al, 2009). This paper thus
proposes an annotation process that builds denser
graphs with formal properties that learners can rely
on, such as locally complete subgraphs.
3.1 Ensuring Dense Graphs
While the ideal goal is to create a complete graph,
the time it would take to hand-label n(n ? 1)/2
for accurate comparison to other corpora.
3
Revealed by the previous sentence (not shown here).
edges is prohibitive. We approximate completeness
by creating locally complete graphs over neigh-
boring sentences. The resulting event graph for a
document is strongly connected, but not complete.
Specifically, the following edge types are included:
1. Event-Event, Event-Time, and Time-Time
pairs in the same sentence
2. Event-Event, Event-Time, and Time-Time
pairs between the current and next sentence
3. Event-DCT pairs for every event in the text
4. Time-DCT pairs for every time expression in
the text
Our process requires annotators to annotate the
above edge types, enforced via an annotation tool.
We describe the relation set and this tool next.
3.1.1 Temporal Relations
The TimeBank corpus uses 14 relations based on
the Allen interval relations. The TempEval contests
have used a small set of relations (TempEval-1) and
the larger set of 14 relations (TempEval-3). Pub-
lished work has mirrored this trend, and different
groups focus on different aspects of the semantics.
We chose a middle ground between coarse and
fine-grained distinctions for annotation, settling on
6 relations: before, after, includes, is included, si-
multaneous, and vague. We do not adopt a more
fine-grained set because we annotate pairs that are
far more ambiguous than those considered in previ-
ous efforts. Decisions between relations like before
and immediately before can complicate an already
difficult task. The added benefit of a corpus (or
working system) that makes fine-grained distinc-
tions is also not clear. We lean toward higher an-
notator agreement with relations that have greater
separation between their semantics
4
.
3.1.2 Enforcing Annotation
Imposing the above rules on annotators requires
automated assistance. We built a new tool that
reads TimeML formatted text, and computes the
set of required edges. Annotators are prompted to
assign a label for each edge, and skipping edges is
prohibited.
5
The tool is unique in that it includes
a transitive reasoner that infers relations based on
the annotator?s latest annotations. For example,
4
For instance, a relation like starts is a special case of in-
cludes if events are viewed as open intervals, and immediately
before is a special case of before. We avoid this overlap and
only use includes and before
5
Note that annotators are presented with pairs in order from
document start to finish, starting with the first two events.
503
if event e
1
IS INCLUDED in t
1
, and t
1
BEFORE
e
2
, the tool automatically labels e
1
BEFORE e
2
.
The transitivity inference is run after each input
label, and the human annotator cannot override
the inferences. This prohibits the annotator from
entering edges that break transitivity. As a result,
several properties are ensured through this process:
the graph (1) is a strongly connected graph, (2) is
consistent with no contradictions, and (3) has all
required edges labeled. These 3 properties are new
to all current ordering corpora.
3.2 Annotation Guidelines
Since the annotation tool frees the annotators from
the decision of when to label an edge, the focus is
now what to label each edge. This section describes
the guidelines for dense annotation.
The 80% confidence rule: The decision to label
an edge as VAGUE instead of a defined temporal
relation is critical. We adopted an 80% rule that in-
structed annotators to choose a specific non-vague
relation if they are 80% confident that it was the
writer?s intent that a reader infer that relation. By
not requiring 100% confidence, we allow for alter-
native interpretations that conflict with the chosen
edge label as long as that alternative is sufficiently
unlikely. In practice, annotators had different inter-
pretations of what constitutes 80% certainty, and
this generated much discussion. We mitigated these
disagreements with the following rule.
Majority annotator agreement: An edge?s la-
bel is the relation that received a majority of an-
notator votes, otherwise it is marked VAGUE. If a
document has 2 annotators, both have to agree on
the relation or it is labeled VAGUE. A document
with 3 annotators requires 2 to agree. This agree-
ment rule acts as a check to our 80% confidence
rule, backing off to VAGUE when decisions are un-
certain (arguably, this is the definition of VAGUE).
We also encouraged consistent labelings with
guidelines inspired by Bethard and Martin (2008).
Modal and conditional events: interpreted with
a possible worlds analysis. The core event was
treated as having occurred, whether or not the text
implied that it had occurred. For example,
They [EVENT expect] him to [EVENT
cut] costs throughout the organization.
This event pair is ordered (expect before cut) since
the expectation occurs before the cutting (in the
possible world where the cutting occurs). Negated
events and hypotheticals are treated similarly. One
assumes the event does occur, and all other events
are ordered accordingly. Negated states like ?is not
anticipating? are interpreted as though the antici-
pation occurs, and surrounding events are ordered
with regard to its presumed temporal span.
Aspectual Events: annotated as IS INCLUDED
in their event arguments. For instance, events that
describe the manner in which another event is per-
formed are considered encompassed by the broader
event. Consider the following example:
The move may [EVENT help] [EVENT
prevent] Martin Ackerman from making
a run at the computer-services concern.
This event pair is assigned the relation (help IS IN-
CLUDED in prevent) because the help event is not
meaningful on its own. It describes the proportion
of the preventing accounted for by the move. In
TimeBank, the intentional action class is used in-
stead of the aspectual class in this case, but we still
consider it covered by this guideline.
Events that attribute a property: to a person
or event are interpreted to end when the entity ends.
For instance, ?the talk is nonsense? evokes a non-
sense event with an end point that coincides with
the end of the talk.
Time Expressions: the words now and today
were given ?long now? interpretations if the words
could be replaced with nowadays and not change
the meaning of their sentences. The time?s dura-
tion starts sometime in the past and INCLUDES the
DCT. If nowadays is not suitable, then the now was
INCLUDED IN the DCT.
Generic Events: can be ordered with respect to
each other, but must be VAGUE with respect to
nearby non-generic events.
4 TimeBank-Dense: corpus statistics
We chose a subset of TimeBank documents for our
new corpus: TimeBank-Dense. This provided an
initial labeling of events and time expressions. Us-
ing the tool described above, we annotated 36 ran-
dom documents with at least two annotators each.
These 36 were annotated with 4 times as many
relations as the entire 183 document TimeBank.
The four authors of this paper were the four an-
notators. All four annotated the same initial docu-
ment, conflicts and disagreements were discussed,
504
Annotated Relation Count
BEFORE 2590 INCLUDES 836
AFTER 2104 INCLUDED IN 1060
SIMULTAN. 215 VAGUE 5910
Total Relations: 12715
Table 2: Relation counts in TimeBank-Dense.
and guidelines were updated accordingly. The rest
of the documents were then annotated indepen-
dently. Document annotation was not random, but
we mixed pairs of authors where time constraints al-
lowed. Table 2 shows the relation counts in the final
corpus, and Table 3 gives the annotator agreement.
We show precision (holding one annotation as gold)
and kappa computed on the 4 types of pairs from
section 3.1. Micro-averaged precision was 65.1%,
compared to TimeBank?s 77%. Kappa ranged from
.56-.64, a slight drop from TimeBank?s .71.
The vague relation makes up 46% of the rela-
tions. This is the first empirical count of how many
temporal relations in news articles are truly vague.
Our lower agreement is likely due to the more
difficult task. Table 5 breaks down the individual
disagreements. The most frequent pertained to the
VAGUE relation. Practically speaking, VAGUE was
applied to the final graph if either annotator chose
it. This seems appropriate since a disagreement be-
tween annotators implies that the relation is vague.
The following example illustrates the difficulty
of labeling edges with a VAGUE relation:
No one was hurt, but firefighters or-
dered the evacuation of nearby homes
and said they?ll monitor the ground.
Both annotators chose VAGUE to label ordered and
said because the order is unclear. However, they
disagreed on evacuation with monitor. One chose
VAGUE, but the other chose IS INCLUDED. There is
a valid interpretation where a monitoring process
has already begun, and continues after the evacua-
tion. This interpretation reached 80% confidence
for one annotator, but not the other. In the face of
such a disagreement, the pair is labeled VAGUE.
How often do these disagreements occur? Ta-
ble 4 shows the 3 sources: (1) mutual vague: anno-
tators agree it is vague, (2) partial vague: one anno-
tator chooses vague, but the other does not, and (3)
no vague: annotators choose conflicting non-vague
relations. Only 17% of these disagreements are due
to hard conflicts (no vague). The released corpus
includes these 3 fine-grained VAGUE relations.
Annotators # Links Prec Kappa
A and B 9282 .65 .56
A and D 1605 .72 .63
B and D 279 .70 .64
C and D 1549 .65 .57
Table 3: Agreement between different annotators.
# Vague
Mutual VAGUE 1657 (28%)
Partial VAGUE 3234 (55%)
No VAGUE 1019 (17%)
Table 4: VAGUE relation origins. Partial vague:
one annotator does not choose vague. No vague:
neither annotator chooses vague.
b a i ii s v
b 1776 22 88 37 21 192
a 17 1444 32 102 9 155
i 71 34 642 45 23 191
ii 81 76 40 826 31 230
s 12 8 25 28 147 29
v 500 441 289 356 64 1197
Table 5: Relation agreement between the two main
annotators. Most disagreements involved VAGUE.
5 Conclusion
We described our annotation framework that pro-
duces corpora with formal guarantees about the an-
notated graph?s structure. Both the annotation tool
and the new TimeBank-Dense corpus are publicly
available.
6
This is the first corpus with guarantees
of connectedness, consistency, and a semantics for
unlabeled edges. We hope to encourage a shift in
the temporal ordering community to consider the
entire document when making local decisions. Fur-
ther work is needed to handle difficult pairs with
the VAGUE relation. We look forward to evaluating
new algorithms on this dense corpus.
Acknowledgments
This work was supported, in part, by the Johns
Hopkins Human Language Technology Center of
Excellence. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors. We also give thanks
to Benjamin Van Durme for assistance and insight.
6
http://www.usna.edu/Users/cs/nchamber/caevo/
505
References
Steven Bethard, William J Corvey, Sara Klingenstein,
and James H Martin. 2008. Building a corpus of
temporal-causal structure. In LREC.
Steven Bethard. 2013. Cleartk-timeml: A minimal-
ist approach to tempeval 2013. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 2: Proceedings of the Seventh
International Workshop on Semantic Evaluation (Se-
mEval 2013), pages 10?14, Atlanta, Georgia, USA,
June. Association for Computational Linguistics.
P. Bramsen, P. Deshpande, Y.K. Lee, and R. Barzilay.
2006. Inducing temporal graphs. In Proceedings of
the 2006 Conference on Empirical Methods in Natu-
ral Language Processing, pages 189?198. ACL.
N. Chambers and D. Jurafsky. 2008. Jointly com-
bining implicit constraints improves temporal order-
ing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
698?706. ACL.
Quang Do, Wei Lu, and Dan Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 677?687, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2012. Extracting narrative time-
lines as temporal dependency structures. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 88?97, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003. The timebank corpus. In Corpus
linguistics, volume 2003, page 40.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, pages
75?80. Association for Computational Linguistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62. As-
sociation for Computational Linguistics.
K. Yoshikawa, S. Riedel, M. Asahara, and Y. Mat-
sumoto. 2009. Jointly identifying temporal rela-
tions with Markov Logic. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
405?413. ACL.
506
Using Query Patterns to Learn the Duration of Events
Andrey Gusev Nathanael Chambers Pranav Khaitan Divye Khilnani
Steven Bethard Dan Jurafsky
Department of Computer Science, Stanford University
{agusev,nc,pranavkh,divyeraj,bethard,jurafsky}@cs.stanford.edu
Abstract
We present the first approach to learning the durations of events without annotated training data,
employing web query patterns to infer duration distributions. For example, we learn that ?war?
lasts years or decades, while ?look? lasts seconds or minutes. Learning aspectual information is an
important goal for computational semantics and duration information may help enable rich document
understanding. We first describe and improve a supervised baseline that relies on event duration
annotations. We then show how web queries for linguistic patterns can help learn the duration of
events without labeled data, producing fine-grained duration judgments that surpass the supervised
system. We evaluate on the TimeBank duration corpus, and also investigate how an event?s participants
(arguments) effect its duration using a corpus collected through Amazon?s Mechanical Turk. We make
available a new database of events and their duration distributions for use in research involving the
temporal and aspectual properties of events.
1 Introduction
Bridging the gap between lexical knowledge and world knowledge is crucial for achieving natural language
understanding. For example, knowing whether a nominal is a person or organization and whether a person
is male or female substantially improves coreference resolution, even when such knowledge is gathered
through noisy unsupervised approaches (Bergsma, 2005; Haghighi and Klein, 2009). However, existing
algorithms and resources for such semantic knowledge have focused primarily on static properties of
nominals (e.g. gender or entity type), not dynamic properties of verbs and events.
This paper shows how to learn one such property: the typical duration of events. Since an event?s
duration is highly dependent on context, our algorithm models this aspectual property as a distribution
over durations rather than a single mean duration. For example, a ?war? typically lasts years, sometimes
months, but almost never seconds, while ?look? typically lasts seconds or minutes, but rarely years or
decades. Our approach uses web queries to model an event?s typical distribution in the real world.
Learning such rich aspectual properties of events is an important area for computational semantics,
and should enrich applications like event coreference (e.g., Chen and Ji, 2009) in much the same way
that gender has benefited nominal coreference systems. Event durations are also key to building event
timelines and other deeper temporal understandings of a text (Verhagen et al, 2007; Pustejovsky and
Verhagen, 2009).
The contributions of this work are:
? Demonstrating how to acquire event duration distributions by querying the web with patterns.
? Showing that a system that predicts event durations based only on our web count distributions can
outperform a supervised system that requires manually annotated training data.
? Making available an event duration lexicon with duration distributions for common English events.
We first review previous work and describe our re-implementation and augmentation of the latest
supervised system for predicting event durations. Next, we present our approach to learning event
distributions based on web counts. We then evaluate both of these models on an existing annotated corpus
of event durations and make comparisons to durations we collected using Amazon?s Mechanical Turk.
Finally, we present a generated database of event durations.
145
2 Previous Work
Early work on extracting event properties focused on linguistic aspect, for example, automatically
distinguishing culminated events that have an end point from non-culminated events that do not (Siegel
and McKeown, 2000). The more fine-grained task of predicting the duration of events was first proposed
by Pan et al (2006), who annotated each event in a small section of the TimeBank (Pustejovsky et al,
2003) with duration lower and upper bounds. They then trained support vector machines on their annotated
corpus for two prediction tasks: less-than-a-day vs. more-than-a-day, and bins like seconds, minutes,
hours, etc. Their models used features like bags of words, heads of syntactic subjects and objects, and
WordNet hypernyms of the events. This work provides a valuable resource in its annotated corpus and is
also a good baseline. We replicate their work and also add new features as described below.
Our approach to the duration problem is inspired by the standard use of web patterns for the acquisition
of relational lexical knowledge. Hearst (1998) first observed that a phrase like ?. . . algae, such as
Gelidium. . . ? indicates that ?Gelidium? is a type of ?algae?, and so hypernym-hyponym relations can
be identified by querying a text collection with patterns like ?such <noun> as <noun>? and ?<noun> ,
including <noun>?. A wide variety of pattern-based work followed, including the application of the idea
in VerbOcean to acquire aspects and temporal structure such as happens-before, using patterns like ?to
<verb> and then <verb>? (Chklovski and Pantel, 2004).
More recent work has learned nominal gender and animacy by matching patterns like ?<noun> *
himself? and ?<noun> and her? to a corpus of Web n-grams (Bergsma, 2005; Ji and Lin, 2009). Phrases like
?John Joseph?, which were observed often with masculine pronouns and never with feminine or neuter
pronouns, can thus have their gender identified as masculine. Ji and Lin found that such web-counts can
predict person names as well as a fully supervised named entity recognition system.
Our goal, then, is to integrate these two strands in the literature, applying pattern/web approaches to
the task of estimating event durations. One difference from previous work is the distributional nature of
the extracted knowledge. In the time domain, unlike in most previous relation-extraction domains, there is
rarely a single correct answer: ?war? may last months, years or decades, though years is the most likely.
Our goal is thus to produce a distribution over durations rather than a single mean duration.
3 Duration Prediction Tasks
In both our supervised and unsupervised models, we consider two types of event duration predictions: a
coarse-grained task in which we only want to know whether the event lasts more or less than a day, and a
fine-grained task in which we want to know whether the event lasts seconds, minutes, hours, days, weeks,
months or years. These two duration prediction tasks were originally suggested by Pan et al (2006), based
on their annotation of a subset of newspaper articles in the Timebank corpus (Pustejovsky et al, 2003).
Events were annotated with a minimum and maximum duration like the following:
? 5 minutes ? 1 hour: A Brooklyn woman who was watching her clothes dry in a laundromat.
? 1 week ? 3 months: Eileen Collins will be named commander of the Space Shuttle mission.
? 3 days ? 2 months: President Clinton says he is committed to a possible strike against Iraq. . .
Pan et al suggested the coarse-grained binary classification task because they found that the mean event
durations from their annotations were distributed bimodally across the corpus, roughly split into short
events (less than a day) and long events (more than a day). The fine-grained classification task provides
additional information beyond this simple two way distinction.
For both tasks, we must convert the minimum/maximum duration annotations into single labels. We
follow Pan et al (2006) and take the arithmetic mean of the minimum and maximum durations in seconds.
For example, in the first event above, 5 minutes would be converted into 300 seconds, 1 hour would be
converted into 3600 seconds, the resulting mean would be 1950 seconds, and therefore this event would
be labeled less-than-a-day for the coarse-grained task, and minutes for the fine-grained task. These labels
can then be used directly to train and evaluate our models.
146
4 Supervised Approach
Before describing our query-based approach, we describe our baseline, a replication and extension of the
supervised system from Pan et al (2006). We first briefly describe their features, which are shared across
the coarse and fine-grained tasks, and then suggest new features.
4.1 Pan et. al. Features
The Pan et al (2006) system included the following features which we also replicate:
Event Properties: The event token, lemma and part of speech (POS) tag.
Bag of Words: The n tokens to the left and right of the event word. However, because Pan et al
found that n = 0 performed best, we omit this feature.
Subject and Object: The head word of the syntactic subject and object of the event, along with their
lemmas and POS tags. Subjects and objects provide important context. For example, ?saw Europe? lasts
for weeks or months while ?saw the goal? lasts only seconds.
Hypernyms: WordNet hypernyms for the event, its subject and its object. Starting from the first
synset of each lemma, three hypernyms were extracted from the WordNet hierarchy. Hypernyms can help
cluster similar events together. For example, the event plan had three hypernym ancestors as features:
idea, content and cognition.
4.2 New Features
We present results for our implementation of the Pan et al (2006) system in Section 8. However, we also
implemented additional features.
Event Attributes: Timebank annotates individual events with four attributes: the event word?s tense
(past, present, future, none), aspect (e.g., progressive), modality (e.g., could, would, can, etc.), and event
class (occurrence, aspectual, state, etc.). We use each of these as a feature in our classifier. The aspect and
tense of the event, in particular, are well known indicators of the temporal shape of events (Vendler, 1976).
Named Entity Classes: Pan et al found the subject and object of the events to be useful features,
helping to identify the particular sense of the event. We used a named entity recognizer to add more
information about the subjects and objects, labeling them as persons, organizations, locations, or other.
Typed Dependencies: We coded aspects of the subcategorization frame of a predicate, such as
transitivity, or the presence of prepositional objects or adverbial modifiers, by adding a binary feature
for each typed dependency1 seen with a verb or noun. We experimented with including the head of the
argument itself, but results were best when only the dependency type was included.
Reporting Verbs: Many of the events in Timebank are reporting verbs (say, report, reply, etc.). We
used a list of reporting verbs to identify these events with a binary feature.
4.3 Classifier
Both the Pan et al feature set and our extended feature set were used to train supervised classifiers for the
two event duration prediction tasks. We experimented with naive bayes, logistic regression, maximum
entropy and support vector machine classifiers, but as discussed in Section 8, the maximum entropy model
performed best in cross-validations on the training data.
5 Unsupervised Approach
While supervised learning is effective for many NLP tasks, it is sensitive to the amount of available
training data. Unfortunately, the training data for event durations is very small, consisting of only 58 news
articles (Pan et al, 2006), and labeling further data is quite expensive. This motivates our desire to find an
1We parsed the documents into typed dependencies with the Stanford Parser (Klein and Manning, 2003).
147
approach that does not rely on labeled data, but instead utilizes the large amounts of text available on the
Web to search for duration-specific patterns. This section describes our web-based approach to learning
event durations.
5.1 Web Query Patterns
Temporal properties of events are often described explicitly in language-specific constructions which can
help us infer an event?s duration. Consider the following two sentences from our corpus:
? Many spend hours surfing the Internet.
? The answer is coming up in a few minutes.
These sentences explicitly describe the duration of the events. In the first, the dominating clause spend
hours tells us how long surfing the Internet lasts (hours, not seconds), and in the second, the preposition
attachment serves a similar role. These examples are very rare in the corpus, but as can be seen, are
extremely informative when present. We developed several such informative patterns, and searched the
Web to find instances of them being used with our target events.
For each pattern described below, we use Yahoo! to search for the patterns occurring with our events.
We collect the total hit counts and use them as indicators of duration. The Yahoo! search API returns two
numbers for a query: totalhits and deephits. The former excludes duplicate pages and limits the number
of documents per domain while the latter includes all duplicates. We take the sum of these two numbers
as our count (this worked better than either of the two individually on the training data and provides
a balance between the benefits of each estimate) and normalize the results as described in Section 5.2.
Queries are submitted as complete phrases with quotation marks, so the results only include exact phrase
matches. This greatly reduces the number of hits, but results in more precise distributions.
5.1.1 Coarse-Grained Patterns
The coarse grained task is a binary decision: less than a day or more than a day. We can model this
task directly by looking for constructions that can only be used with events that take less than a day.
The adverb yesterday fills this role nicely; an event modified by yesterday strongly implies that it took
place within a single day?s time. For example, ?shares closed at $18 yesterday? implies that the closing
happened in less than a day. We thus consider the following two query patterns:
? <eventpast> yesterday
? <eventpastp> yesterday
where <eventpast> is the past tense (preterite) form of the event (e.g., ran), and <eventpastp> is the past
progressive form of the event (e.g., was running).
5.1.2 Fine-Grained Patterns
For the fine-grained task, we need patterns that can identify when an event falls into any of the various
buckets: seconds, minutes, hours, etc. Thus, our fine-grained patterns are parameterized both by the event
and by the bucket of interest. We use the following patterns inspired in part by Dowty (1979):
1. <eventpast> for * <bucket>
2. <eventpastp> for * <bucket>
3. spent * <bucket> <eventger>
where <eventpast> and <eventpastp> are defined as above, <eventger> is the gerund form of the event (e.g.,
running), and the wildcard ?*? can match any single token2.
The following three patterns ultimately did not improve the system?s performance on the training data:
4. <eventpast> in * <bucket>
5. takes * <bucket> to <event>
6. <eventpast> last <bucket>
Pattern 4 returned a lot of hits, but had low precision as it picked up many non-durative expressions.
Pattern 5 was very precise but typically returned few hits, and pattern 6 worked for, e.g., last week, but did
not work for shorter durations. All reported systems use patterns 1-3 and do not include 4-6.
2We experimented with varying numbers of wildcards but found little difference in performance on the training data.
148
!"
!#$"
!#%"
!#&"
!#'"
!#("
!#)"
*+
,-
./
*"
0
1.
23
+*
"
4-
25
*"
/6
7*
"
8
++
9*
"
0
-.
34
*"
7+
65
*"
/+
,6
/+
*"
(a) ?was saying for <bucket>?
!"
!#$"
!#%"
!#&"
!#'"
!#("
!#)"
*+
,-
./
*"
0
1.
23
+*
"
4-
25
*"
/6
7*
"
8
++
9*
"
0
-.
34
*"
7+
65
*"
/+
,6
/+
*"
(b) ?for <bucket>?
!"
!#$"
!#%"
!#&"
!#'"
!#("
!#)"
*+
,-
./
*"
0
1.
23
+*
"
4-
25
*"
/6
7*
"
8
++
9*
"
0
-.
34
*"
7+
65
*"
/+
,6
/+
*"
(c) (a) counts divided by (b) counts
Figure 1: Normalizing the distribution for the pattern ?was saying for <bucket>?.
We also tried adding subjects and/or objects to the patterns when they were present for an event.
However, we found that the benefit of the extra context was outweighed by the significantly fewer hits that
resulted. We implemented several backoff approaches that removed the subject and object from the query,
however, the counts from these backoff approaches were less reliable than just using the base event.
5.2 Predicting Durations from Patterns
To predict the duration of an event from the above patterns, we first insert the event into each pattern
template and query the web to see how often the filled template occurs. These counts form a distribution
over each of the bins of interest, e.g., in the fine-grained task we have counts for seconds, minutes, hours,
etc. We discard pattern distributions with very low total counts, and normalize the remaining pattern
distributions based on the frequency with which the pattern occurs in general. Finally, we uniformly
merge the distributions from all patterns, and use the resulting distribution to select a duration label for
the event. The following sections detail this process.
5.2.1 Coarse-Grained Prediction
For the coarse-grained task of less than a day vs. more than a day, we collect counts using the two
yesterday patterns described above. We then normalize these counts by the count of the event?s occurrence
in general. For example, given the event run, we query for ?ran yesterday? and divide by the count of
?ran?. This gives us the probability of seeing yesterday given that we saw ran. We average the probabilities
from the two yesterday patterns, and classify an event as lasting less than a day if its average probability
exceeds a threshold t. We optimized t to our training set (t = .002). This basically says that if an event
occurs with yesterday more than 0.2% of the time, we will assume that the event lasts less than a day.
5.2.2 Fine-Grained Prediction
As with the coarse-grained task, our fine-grained approach begins by collecting counts using the three
fine-grained patterns discussed above. Since each fine-grained pattern has both an <event> and a <bucket>
slot to be filled, for a single event and a single pattern, we end up making 8 queries to cover each of the 8
buckets: seconds, minutes, hours, days, weeks, months, years and decades. After these queries, we have a
pattern-specific distribution of counts over the various buckets, a coarse measure of the types of durations
that might be appropriate to this event. Figure 1(a) shows an example of such a distribution.
As can be seen in Figure 1(a), this initial distribution can be skewed in various ways ? in this case,
years is given far too much mass. This is because in addition to the single event interpretation of words
like ?saying?, there are iterative or habitual interpretations (Moens and Steedman, 1988; Frawley, 1992).
Iterative events occur repeatedly over a period of time, e.g., ?he?s been saying for years that. . . ? The two
interpretations are apparent in the raw distributions of smile and run in Figure 2. The large peak at years
for run shows that it is common to say someone ?was running for years.? Conversely, it is less common to
say someone ?was smiling for years,? so the distribution for smile is less biased towards years.
149
        

	
A
B
C
D 
	AB
CDBEA	FB
Figure 2: Two double peaked distributions.
Coverage of Fine-Grained Query Patterns
Number of Patterns Total Events Precision
At least one 1359 (81.7%) 57.3
At least two 1142 (68.6%) 58.6
All three 428 (25.7%) 65.7
Figure 3: The number of events that match n fine-
grained patterns and the pattern precision on these
events. The training set consists of 1664 events.
While the problem of distinguishing single events from iterative events is out of the scope of this paper
(though an interesting avenue for future research), we can partially address the problem by recognizing
that some buckets are simply more frequent in text than others. For example, Figure 1(b) shows that it is
by far more common to see ?for <bucket>? filled with years than with any other duration unit. Thus, for
each bucket, we divide the counts collected with the event patterns by the counts we get for the pattern
without the event3. Essentially, this gives us for each bucket the probability of the event given that bucket.
Figure 1(c) shows that the resulting normalized distribution fits our intution of how long ?saying? should
last much better than the raw counts: seconds and minutes have much more of the mass now.
After normalizing an event?s counts for each pattern, we combine the distributions from the three
different patterns if their hit counts pass certain confidence thresholds. The total hit count for each pattern
must exceed a minimum threshold tmin = 100 and not exceed a maximum threshold tmax = 100, 000
(both thresholds were optimized on the training data). The former avoids building distributions from a
sparse number of hits, and the latter avoids classifying generic and polysemous events like ?to make? that
return a large number of hits. We found such events to produce generic distributions that do not help in
classification. If all three patterns pass our confidence thresholds, we merge the pattern distributions by
summing them bucket-wise together and renormalizing the resulting distribution to sum to 1. Merging the
patterns mitigates the noise from any single pattern.
To predict the event?s duration, we then select the bucket with the highest smoothed score:
score(bi) = bi?1 + bi + bi+1
where bi is a duration bucket and 0 < i < 9. We define b0 = b9 = 0. In other words, the score of the
minute bucket is the sum of three buckets: second, minute and hour. This parallels the smoothing of the
evaluation metric introduced by (Pan et al, 2006) which we also adopt for evaluation in Section 7.
In the case that fewer than three of our patterns matched, we backoff to the majority class (months for
fine-grained, and more-than-a-day for coarse-grained). We experimented with only requiring one or two
patterns to match, but found the best results on training when requiring all three. Figure 3 shows the large
jump in precision when all three are required. The evaluation is discussed in Section 7.
5.2.3 Coarse-Grained Prediction via Fine-Grained Prediction
We can also use the distributions collected from the fine-grained task to predict coarse-grained labels. We
use the above approach and return less than a day if the selected fine-grained bucket was seconds, minutes
or hours, and more than a day otherwise. We also tried summing over the duration buckets: p(seconds) +
p(minutes) + p(hours) for less than day and p(days) + p(weeks) + p(months) + p(years) + p(decades) for
more than a day, but the simpler approach outperformed these summations in training.
3We also explored normalizing not by the global distribution on the Web, but by the average of the distributions of all the
events in our dataset. However, on the training data, using the global distribution performed better.
150
6 Datasets
6.1 Timebank Duration
As described in Section 3, Pan et al (2006) labeled 58 documents with event durations. We follow their
method of isolating the 10 WSJ articles as a separate test set which we call TestWSJ (147 events). For
the remaining 48 documents, they split the 2132 event instances into a Train and Test set with 1705 and
427 events respectively. Their split was conducted over the bag of events, so their train and test sets may
include events that came from the same document. Their particular split was unavailable.
We instead use a document-split that divides the two sets into bins of documents. Each document?s
entire set of events is assigned to either the training set or the test set, so we do not mix events across
sets. Since documents often repeat mentions of events, this split is more conservative by not mixing test
mentions with the training set. Train, Test, and TestWSJ contain 1664 events (714 unique verbs), 471 events
(274 unique), and 147 events (84 unique) respectively. For each base verb, we created queries as described
in Section 5.1.2. The train/test split is available at http://cs.stanford.edu/people/agusev/durations/.
6.2 Mechanical Turk Dataset
We also collected event durations from Amazon?s Mechanical Turk (MTurk), an online marketplace from
Amazon where requesters can find workers to solve Human Intelligence Tasks (HITs) for small amounts
of money. Prior work has shown that human judgments from MTurk can often be as reliable as trained
annotators (Snow et al, 2008) or subjects in controlled lab studies (Munro et al, 2010), particularly when
judgments are aggregated over many MTurk workers (?Turkers?). Our motivation for using Turkers is to
better analyze system errors. For example, if we give humans an event in isolation (no sentence context),
how well can they guess the durations assigned by the Pan et. al. annotators? This measures how big the
gap is between a system that looks only at the event, and a system that integrates all available context.
To collect event durations from MTurk, we presented Turkers with an event from the TimeBank (a
superset of the events annotated by Pan et al (2006)) and asked them to decide whether the event was most
likely to take seconds, minutes, hours, days, weeks, months, years or decades. We had events annotated
in two different contexts: in isolation, where only the event itself was given (e.g., ?allocated?), and in
subject-object context, where a minimal phrase including the event and its subject and object was given
(e.g., ?the mayor allocated funds?). In both types of tasks, we asked 10 Turkers to label each event,
and they were paid $0.0025 for each annotation ($0.05 for a block of 20 events). To filter out obvious
spammers, we added a test item randomly to each block, e.g., adding the event ?minutes? and rejecting
work from Turkers who labeled this anything other than the duration minutes.
The resulting annotations give duration distributions for each of our events. For example, when
presented the event ?remodeling?, 1 Turker responded with days, 6 with weeks, 2 with months and 1
with years. These annotations suggest that we generally expect ?remodeling? to take weeks, but it may
sometimes take more or less. To produce a single fine-grained label from these distributions, we take the
duration bin with the largest number of Turker annotations, e.g. for ?remodeling?, we would produce the
label weeks. To produce a single coarse-grained label, we use the label less-than-a-day if the fine-grained
label was seconds, minutes or hours and more-than-a-day otherwise.
7 Experiment Setup
As discussed in Section 3, we convert the minimum and maximum duration annotations into labels by
converting each to seconds using ISO standards and calculating the arithmetic mean. If the mean is
? 86400 seconds, it is considered less-than-a-day for the coarse-grained task. The fine-grained buckets
are similarly calculated, e.g., X is labeled days if 86400 < X ? 604800. The Pan et al (2006) evaluation
does not include a decades bucket, but our system still uses ?decades? in its queries.
We optimized all parameters of both the supervised and unsupervised systems on the training set, only
running on test after selecting our best performing model. We compare to the majority class as a baseline,
151
Coarse-Grained
Test TestWSJ
Supervised, Pan 73.3 73.5
Supervised, all 73.0 74.8
Fine-Grained
Test TestWSJ
Supervised, Pan 62.2 61.9
Supervised, all 62.4 66.0
Figure 4: Accuracies of the supervised maximum entropy classifiers with two different feature sets.
Coarse-Grained
Test TestWSJ
Majority class 62.4 57.1
Supervised, all 73.0* 74.8*
Web counts, yesterday 70.7* 74.8*
Web counts, buckets 72.4* 73.5*
Fine-Grained
Test TestWSJ
Majority class 59.2 52.4
Supervised, all 62.4 66.0?
Web counts, buckets 66.5* 68.7*
Figure 5: System accuracy compared against supervised and majority class. * indicates statistical
significance (McNemar?s Test, two-tailed) against majority class at the p < 0.01 level, ? at p < 0.05
tagging all events as more-than-a-day in the coarse-grained task and months in the fine-grained task.
To evaluate our models, we use simple accuracy on the coarse-grained task, and approximate agreement
matching as in Pan et al (2006) on the fine-grained task. In this approximate agreement, a guess is
considered correct if it chooses either the gold label or its immediate neighbor (e.g., hours is correct if
minutes, hours or days is the gold class). Pan et al use this approach since human labeling agreement is
low (44.4%) on the exact agreement fine-grained task.
8 Results
Figure 4 compares the performance of our two supervised models; the reimplementation of Pan et al
(2006) (Supervised, Pan), and our improved model with new features (Supervised, all). The new model
performs similarily to the Pan model on the in-domain Test set, but better on the out-of-domain financial
news articles in the TestWSJ test. On the latter, the new model improves over Pan et al by 1.3% absolute
on the coarse-grained task, and by 4.1% absolute on the fine-grained task. We report results from the
maximum entropy model as it slightly outperformed the naive bayes and support vector machine models4.
We compare these supervised results against our web-based unsupervised systems in Figure 5. For the
coarse-grained task, we have two web count systems described in Section 5: one based on the yesterday
patterns (Web counts, yesterday), and one based on first gathering the fine-grained bucket counts and
then converting those to coarse-grained labels (Web counts, buckets). Generally, these models perform
within 1-2% of the supervised model on the coarse-grained task, though the yesterday-based classifier
exactly matches the supervised system?s performance on the TestWSJ data. The supervised system?s
higher results are not statistically significant against our web-based systems.
For the fine-grained task, Figure 5 compares our web counts algorithm based on duration distributions
(Section 5) to the baseline and supervised systems. Our web counts approach outperforms the best
supervised system by 4.1% absolute on the Test set and by 2.7% absolute on the out-of-domain TestWSJ.
To get an idea of how much the subject/object context could help predict event duration if integrated
perfectly, we evaluated the Mechanical Turk annotations against the Pan et. al. annotated dataset using
approximate agreement as described in Section 7. Figure 6 gives the performance of the Turkers given
two types of context: just the event itself (Event only), and the event plus its subject and/or object (Event
and args). Turkers performed below the majority class baseline when given only the event, but generally
above the baseline when given the subject and object, improving up to 20% over the event-only condition.
Figure 7 shows examples of events with different learned durations.
4This differs from Pan et al who found support vector machines to be the best classifier.
152
Mechanical Turk Accuracy
Coarse Fine
Test WSJ Test WSJ
Majority class 62.4 57.1 59.2 52.4
Event only 52.0 49.4 42.1 43.8
Event and args 65.0 70.1 56.7 59.9
Figure 6: Accuracy of Mechanical Turkers
against Pan et. al. annotations.
Learned Examples
talk to tourism leaders minutes
driving hours
shut down the supply route days
travel weeks
the downturn across Asia months
build a museum years
Figure 7: Examples of web query durations.
9 Discussion
Our novel approach to learning event durations showed 4.1% and 2.7% absolute gains over a state-of-the-
art supervised classifier. Although the gain is not statistically significant, these results nonetheless suggest
that we are learning as much about event durations from the web counts as we are currently able to learn
with our improvements to Pan et al?s (2006) supervised system. This is encouraging because it indicates
that we may not need extensive manual annotations to acquire event durations. Further, our final query
system achieves these results with only the event word, and without considering the subject, object or
other types of context.
Despite the fact that we saw little gains in performance when including subjects and objects in our
query patterns, the Mechanical Turk evaluation suggests that more information may still be gleaned from
the additional context. Giving Turkers the subject and object improved their label accuracy by 10-20%
absolute. This suggests that finding a way to include subjects and objects in the web queries, for example
by using thesauri to generate related queries, is a valuable line of research for future work.
Finally, these MTurk experiments suggest that classifying events for duration out of context is a
difficult task. Pan et al (2006) reported 0.88 annotator agreement on the coarse-grained task when given
the entire document context. Out of context, given just the event word, our Turkers only achieved 52%
and 49% accuracy. Not surprisingly, the task is more difficult without the document. Our system, however,
was also only given the event word, but it was able achieve over 70% in accuracy. This suggests that rich
language understanding is often needed to correctly label an event for duration, but in the absence of such
understanding, modeling the duration by web counts appears to be a practical and useful alternative.
10 A Database of Event Durations
Given the strong performance of our model on duration classification, we are releasing a database of
events and their normalized duration distributions, as predicted by our bucket-based fine-grained model.
We extracted the 1000 most frequent verbs from a newspaper corpus (the NYT portion of Gigaword
Graff (2002)) with the 10 most frequent grammatical objects of each verb. These 10, 000 events and their
duration distributions are available at http://cs.stanford.edu/people/agusev/durations/.
Acknowledgements
Thanks to Chris Manning and the anonymous reviewers for insightful comments and feedback. This
research draws on data provided by Yahoo!, Inc., through its Yahoo! Search Services offering. We
gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-
C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those
of the author(s) and do not necessarily reflect the view of DARPA, AFRL, or the US government.
153
References
Bergsma, S. (2005). Automatic acquisition of gender information for anaphora resolution. In Advances
in Artificial Intelligence, Volume 3501 of Lecture Notes in Computer Science, pp. 342?353. Springer
Berlin / Heidelberg.
Chen, Z. and H. Ji (2009). Graph-based event coreference resolution. In Proceedings of the Workshop on
Graph-based Methods for Natural Language Processing (TextGraphs-4), Singapore, pp. 54?57. ACL.
Chklovski, T. and P. Pantel (2004). Verbocean: Mining the web for fine-grained semantic verb relations.
In D. Lin and D. Wu (Eds.), Proceedings of EMNLP 2004, Barcelona, Spain, pp. 33?40.
Dowty, D. R. (1979). Word Meaning and Montague Grammar. Kluwer Academic Publishers.
Frawley, W. (1992). Linguistic Semantics. Routledge.
Graff, D. (2002). English Gigaword. Linguistic Data Consortium.
Haghighi, A. and D. Klein (2009). Simple coreference resolution with rich syntactic and semantic features.
In Proceedings of EMNLP-2009, Singapore, pp. 1152?1161.
Hearst, M. A. (1998). Automated discovery of wordnet relations. In WordNet: An Electronic Lexical
Database. MIT Press.
Ji, H. and D. Lin (2009). Gender and animacy knowledge discovery from web-scale n-grams for
unsupervised person mention detection. In Proceedings of the Pacific Asia Conference on Language,
Information and Computation.
Klein, D. and C. D. Manning (2003). Accurate unlexicalized parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics, Sapporo, Japan, pp. 423?430.
Moens, M. and M. Steedman (1988). Temporal ontology in natural language. Computational Linguis-
tics 2(14), 15?21.
Munro, R., S. Bethard, V. Kuperman, V. T. Lai, R. Melnick, C. Potts, T. Schnoebelen, and H. Tily (2010).
Crowdsourcing and language studies: the new generation of linguistic data. In Proceedings of the
NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk,
Los Angeles, pp. 122?130.
Pan, F., R. Mulkar, and J. Hobbs (2006). Learning event durations from event descriptions. In Proceedings
of COLING-ACL.
Pustejovsky, J., P. Hanks, R. Sauri, A. See, D. Day, L. Ferro, R. Gaizauskas, M. Lazo, A. Setzer, and
B. Sundheim (2003). The timebank corpus. Corpus Linguistics, 647?656.
Pustejovsky, J. and M. Verhagen (2009). Semeval-2010 task 13: Evaluating events, time expressions, and
temporal relations (tempeval-2). In Proceedings of the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-2009), Boulder, Colorado, pp. 112?116.
Siegel, E. V. and K. R. McKeown (2000). Learning methods to combine linguistic indicators: improving
aspectual classification and revealing linguistic insights. Computational Linguistics 26(4), 595?628.
Snow, R., B. O?Connor, D. Jurafsky, and A. Ng (2008). Cheap and fast ? but is it good? evaluating
non-expert annotations for natural language tasks. In Proceedings of EMNLP-2008, Hawaii.
Vendler, Z. (1976). Verbs and times. Linguistics in Philosophy, 97?121.
Verhagen, M., R. Gaizauskas, F. Schilder, M. Hepple, G. Katz, and J. Pustejovsky (2007). Semeval-2007
task 15: Tempeval temporal relation identification. In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, pp. 75?80.
154
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 28?34,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Stanford?s Multi-Pass Sieve Coreference Resolution System at the
CoNLL-2011 Shared Task
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers,
Mihai Surdeanu, Dan Jurafsky
Stanford NLP Group
Stanford University, Stanford, CA 94305
{heeyoung,peirsman,angelx,natec,mihais,jurafsky}@stanford.edu
Abstract
This paper details the coreference resolution
system submitted by Stanford at the CoNLL-
2011 shared task. Our system is a collection
of deterministic coreference resolution mod-
els that incorporate lexical, syntactic, seman-
tic, and discourse information. All these mod-
els use global document-level information by
sharing mention attributes, such as gender and
number, across mentions in the same cluster.
We participated in both the open and closed
tracks and submitted results using both pre-
dicted and gold mentions. Our system was
ranked first in both tracks, with a score of 57.8
in the closed track and 58.3 in the open track.
1 Introduction
This paper describes the coreference resolution sys-
tem used by Stanford at the CoNLL-2011 shared
task (Pradhan et al, 2011). Our system extends
the multi-pass sieve system of Raghunathan et
al. (2010), which applies tiers of deterministic coref-
erence models one at a time from highest to lowest
precision. Each tier builds on the entity clusters con-
structed by previous models in the sieve, guarantee-
ing that stronger features are given precedence over
weaker ones. Furthermore, this model propagates
global information by sharing attributes (e.g., gender
and number) across mentions in the same cluster.
We made three considerable extensions to the
Raghunathan et al (2010) model. First, we added
five additional sieves, the majority of which address
the semantic similarity between mentions, e.g., us-
ing WordNet distance, and shallow discourse under-
standing, e.g., linking speakers to compatible pro-
nouns. Second, we incorporated a mention detection
sieve at the beginning of the processing flow. This
sieve filters our syntactic constituents unlikely to be
mentions using a simple set of rules on top of the
syntactic analysis of text. And lastly, we added a
post-processing step, which guarantees that the out-
put of our system is compatible with the shared task
and OntoNotes specifications (Hovy et al, 2006;
Pradhan et al, 2007).
Using this system, we participated in both the
closed1 and open2 tracks, using both predicted and
gold mentions. Using predicted mentions, our sys-
tem had an overall score of 57.8 in the closed track
and 58.3 in the open track. These were the top scores
in both tracks. Using gold mentions, our system
scored 60.7 in the closed track in 61.4 in the open
track.
We describe the architecture of our entire system
in Section 2. In Section 3 we show the results of sev-
eral experiments, which compare the impact of the
various features in our system, and analyze the per-
formance drop as we switch from gold mentions and
annotations (named entity mentions and parse trees)
to predicted information. We also report in this sec-
tion our official results in the testing partition.
1Only the provided data can be used, i.e., WordNet and gen-
der gazetteer.
2Any external knowledge source can be used. We used
additional animacy, gender, demonym, and country and states
gazetteers.
28
2 System Architecture
Our system consists of three main stages: mention
detection, followed by coreference resolution, and
finally, post-processing. In the first stage, mentions
are extracted and relevant information about men-
tions, e.g., gender and number, is prepared for the
next step. The second stage implements the ac-
tual coreference resolution of the identified men-
tions. Sieves in this stage are sorted from highest
to lowest precision. For example, the first sieve (i.e.,
highest precision) requires an exact string match be-
tween a mention and its antecedent, whereas the
last one (i.e., lowest precision) implements pronom-
inal coreference resolution. Post-processing is per-
formed to adjust our output to the task specific con-
straints, e.g., removing singletons.
It is important to note that the first system stage,
i.e., the mention detection sieve, favors recall heav-
ily, whereas the second stage, which includes the ac-
tual coreference resolution sieves, is precision ori-
ented. Our results show that this design lead to
state-of-the-art performance despite the simplicity
of the individual components. This strategy has
been successfully used before for information ex-
traction, e.g., in the BioNLP 2009 event extraction
shared task (Kim et al, 2009), several of the top sys-
tems had a first high-recall component to identify
event anchors, followed by high-precision classi-
fiers, which identified event arguments and removed
unlikely event candidates (Bjo?rne et al, 2009). In
the coreference resolution space, several works have
shown that applying a list of rules from highest to
lowest precision is beneficial for coreference reso-
lution (Baldwin, 1997; Raghunathan el al., 2010).
However, we believe we are the first to show that this
high-recall/high-precision strategy yields competi-
tive results for the complete task of coreference res-
olution, i.e., including mention detection and both
nominal and pronominal coreference.
2.1 Mention Detection Sieve
In our particular setup, the recall of the mention de-
tection component is more important than its preci-
sion, because any missed mentions are guaranteed
to affect the final score, but spurious mentions may
not impact the overall score if they are left as sin-
gletons, which are discarded by our post-processing
step. Therefore, our mention detection algorithm fo-
cuses on attaining high recall rather than high preci-
sion. We achieve our goal based on the list of sieves
sorted by recall (from highest to lowest). Each sieve
uses syntactic parse trees, identified named entity
mentions, and a few manually written patterns based
on heuristics and OntoNotes specifications (Hovy et
al., 2006; Pradhan et al, 2007). In the first and
highest recall sieve, we mark all noun phrase (NP),
possessive pronoun, and named entity mentions in
each sentence as candidate mentions. In the follow-
ing sieves, we remove from this set al mentions that
match any of the exclusion rules below:
1. We remove a mention if a larger mention with
the same head word exists, e.g., we remove The
five insurance companies in The five insurance
companies approved to be established this time.
2. We discard numeric entities such as percents,
money, cardinals, and quantities, e.g., 9%,
$10, 000, Tens of thousands, 100 miles.
3. We remove mentions with partitive or quanti-
fier expressions, e.g., a total of 177 projects.
4. We remove pleonastic it pronouns, detected us-
ing a set of known expressions, e.g., It is possi-
ble that.
5. We discard adjectival forms of nations, e.g.,
American.
6. We remove stop words in a predetermined list
of 8 words, e.g., there, ltd., hmm.
Note that the above rules extract both mentions in
appositive and copulative relations, e.g., [[Yongkang
Zhou] , the general manager] or [Mr. Savoca] had
been [a consultant. . . ]. These relations are not an-
notated in the OntoNotes corpus, e.g., in the text
[[Yongkang Zhou] , the general manager], only the
larger mention is annotated. However, appositive
and copulative relations provide useful (and highly
precise) information to our coreference sieves. For
this reason, we keep these mentions as candidates,
and remove them later during post-processing.
2.2 Mention Processing
Once mentions are extracted, we sort them by sen-
tence number, and left-to-right breadth-first traversal
29
order in syntactic trees in the same sentence (Hobbs,
1977). We select for resolution only the first men-
tions in each cluster,3 for two reasons: (a) the first
mention tends to be better defined (Fox, 1993),
which provides a richer environment for feature ex-
traction; and (b) it has fewer antecedent candidates,
which means fewer opportunities to make a mis-
take. For example, given the following ordered list
of mentions, {m11, m
2
2, m
2
3, m
3
4, m
1
5, m
2
6}, where
the subscript indicates textual order and the super-
script indicates cluster id, our model will attempt
to resolve only m22 and m
3
4. Furthermore, we dis-
card first mentions that start with indefinite pronouns
(e.g., some, other) or indefinite articles (e.g., a, an)
if they have no antecedents that have the exact same
string extents.
For each selected mention mi, all previous men-
tions mi?1, . . . , m1 become antecedent candidates.
All sieves traverse the candidate list until they find
a coreferent antecedent according to their criteria
or reach the end of the list. Crucially, when com-
paring two mentions, our approach uses informa-
tion from the entire clusters that contain these men-
tions instead of using just information local to the
corresponding mentions. Specifically, mentions in
a cluster share their attributes (e.g., number, gen-
der, animacy) between them so coreference decision
are better informed. For example, if a cluster con-
tains two mentions: a group of students, which is
singular, and five students, which is plural,
the number attribute of the entire cluster becomes
singular or plural, which allows it to match
other mentions that are both singular and plural.
Please see (Raghunathan et al, 2010) for more de-
tails.
2.3 Coreference Resolution Sieves
2.3.1 Core System
The core of our coreference resolution system is
an incremental extension of the system described in
Raghunathan et al (2010). Our core model includes
two new sieves that address nominal mentions and
are inserted based on their precision in a held-out
corpus (see Table 1 for the complete list of sieves
deployed in our system). Since these two sieves use
3We initialize the clusters as singletons and grow them pro-
gressively in each sieve.
Ordered sieves
1. Mention Detection Sieve
2. Discourse Processing Sieve
3. Exact String Match Sieve
4. Relaxed String Match Sieve
5. Precise Constructs Sieve (e.g., appositives)
6-8. Strict Head Matching Sieves A-C
9. Proper Head Word Match Sieve
10. Alias Sieve
11. Relaxed Head Matching Sieve
12. Lexical Chain Sieve
13. Pronouns Sieve
Table 1: The sieves in our system; sieves new to this pa-
per are in bold.
simple lexical constraints without semantic informa-
tion, we consider them part of the baseline model.
Relaxed String Match: This sieve considers two
nominal mentions as coreferent if the strings ob-
tained by dropping the text following their head
words are identical, e.g., [Clinton] and [Clinton,
whose term ends in January].
Proper Head Word Match: This sieve marks two
mentions headed by proper nouns as coreferent if
they have the same head word and satisfy the fol-
lowing constraints:
Not i-within-i - same as Raghunathan et al (2010).
No location mismatches - the modifiers of two men-
tions cannot contain different location named entities,
other proper nouns, or spatial modifiers. For example,
[Lebanon] and [southern Lebanon] are not coreferent.
No numeric mismatches - the second mention cannot
have a number that does not appear in the antecedent, e.g.,
[people] and [around 200 people] are not coreferent.
In addition to the above, a few more rules are
added to get better performance for predicted men-
tions.
Pronoun distance - sentence distance between a pronoun
and its antecedent cannot be larger than 3.
Bare plurals - bare plurals are generic and cannot have a
coreferent antecedent.
2.3.2 Semantic-Similarity Sieves
We first extend the above system with two
new sieves that exploit semantics from WordNet,
Wikipedia infoboxes, and Freebase records, drawing
on previous coreference work using these databases
(Ng & Cardie, 2002; Daume? & Marcu, 2005;
Ponzetto & Strube, 2006; Ng, 2007; Yang & Su,
30
2007; Bengston & Roth, 2008; Huang et al, 2009;
inter alia). Since the input to a sieve is a collection of
mention clusters built by the previous (more precise)
sieves, we need to link mention clusters (rather than
individual mentions) to records in these three knowl-
edge bases. The following steps generate a query for
these resources from a mention cluster.
First, we select the most representative mention
in a cluster by preferring mentions headed by proper
nouns to mentions headed by common nouns, and
nominal mentions to pronominal ones. In case of
ties, we select the longer string. For example, the
mention selected from the cluster {President George
W. Bush, president, he} is President George W.
Bush. Second, if this mention returns nothing from
the knowledge bases, we implement the following
query relaxation algorithm: (a) remove the text fol-
lowing the mention head word; (b) select the lowest
noun phrase (NP) in the parse tree that includes the
mention head word; (c) use the longest proper noun
(NNP*) sequence that ends with the head word; (d)
select the head word. For example, the query pres-
ident Bill Clinton, whose term ends in January is
successively changed to president Bill Clinton, then
Bill Clinton, and finally Clinton. If multiple records
are returned, we keep the top two for Wikipedia and
Freebase, and all synsets for WordNet.
Alias Sieve
This sieve addresses name aliases, which are de-
tected as follows. Two mentions headed by proper
nouns are marked as aliases (and stored in the same
entity cluster) if they appear in the same Wikipedia
infobox or Freebase record in either the ?name? or
?alias? field, or they appear in the same synset in
WordNet. As an example, this sieve correctly de-
tects America Online and AOL as aliases. We also
tested the utility of Wikipedia categories, but found
little gain over morpho-syntactic features.
Lexical Chain Sieve
This sieve marks two nominal mentions as coref-
erent if they are linked by a WordNet lexical chain
that traverses hypernymy or synonymy relations. We
use all synsets for each mention, but restrict it to
mentions that are at most three sentences apart, and
lexical chains of length at most four. This sieve cor-
rectly links Britain with country, and plane with air-
craft.
To increase the precision of the above two sieves,
we use additional constraints before two mentions
can match: attribute agreement (number, gender, an-
imacy, named entity labels), no i-within-i, no loca-
tion or numeric mismatches (as in Section 2.3.1),
and we do not use the abstract entity synset in Word-
Net, except in chains that include ?organization?.
2.3.3 Discourse Processing Sieve
This sieve matches speakers to compatible pro-
nouns, using shallow discourse understanding to
handle quotations and conversation transcripts. Al-
though more complex discourse constraints have
been proposed, it has been difficult to show improve-
ments (Tetreault & Allen, 2003; 2004).
We begin by identifying speakers within text. In
non-conversational text, we use a simple heuristic
that searches for the subjects of reporting verbs (e.g.,
say) in the same sentence or neighboring sentences
to a quotation. In conversational text, speaker infor-
mation is provided in the dataset.
The extracted speakers then allow us to imple-
ment the following sieve heuristics:
? ?I?s4 assigned to the same speaker are coreferent.
? ?you?s with the same speaker are coreferent.
? The speaker and ?I?s in her text are coreferent.
For example, I, my, and she in the following sen-
tence are coreferent: ?[I] voted for [Nader] because
[he] was most aligned with [my] values,? [she] said.
In addition to the above sieve, we impose speaker
constraints on decisions made by subsequent sieves:
? The speaker and a mention which is not ?I? in the
speaker?s utterance cannot be coreferent.
? Two ?I?s (or two ?you?s, or two ?we?s) assigned to
different speakers cannot be coreferent.
? Two different person pronouns by the same speaker
cannot be coreferent.
? Nominal mentions cannot be coreferent with ?I?,
?you?, or ?we? in the same turn or quotation.
? In conversations, ?you? can corefer only with the
previous speaker.
For example, [my] and [he] are not coreferent in the
above example (third constraint).
4We define ?I? as ?I?, ?my?, ?me?, or ?mine?, ?we? as first
person plural pronouns, and ?you? as second person pronouns.
31
Annotations Coref R P F1
Gold Before 92.8 37.7 53.6
Gold After 75.1 70.1 72.6
Not gold Before 87.9 35.6 50.7
Not gold After 71.7 68.4 70.0
Table 2: Performance of the mention detection compo-
nent, before and after coreference resolution, with both
gold and actual linguistic annotations.
2.4 Post Processing
To guarantee that the output of our system matches
the shared task requirements and the OntoNotes
annotation specification, we implement two post-
processing steps:
? We discard singleton clusters.
? We discard the mention that appears later in
text in appositive and copulative relations. For
example, in the text [[Yongkang Zhou] , the
general manager] or [Mr. Savoca] had been
[a consultant. . . ], the mentions Yongkang Zhou
and a consultant. . . are removed in this stage.
3 Results and Discussion
Table 2 shows the performance of our mention de-
tection algorithm. We show results before and after
coreference resolution and post-processing (when
singleton mentions are removed). We also list re-
sults with gold and predicted linguistic annotations
(i.e., syntactic parses and named entity recognition).
The table shows that the recall of our approach is
92.8% (if gold annotations are used) or 87.9% (with
predicted annotations). In both cases, precision is
low because our algorithm generates many spurious
mentions due to its local nature. However, as the ta-
ble indicates, many of these mentions are removed
during post-processing, because they are assigned
to singleton clusters during coreference resolution.
The two main causes for our recall errors are lack
of recognition of event mentions (e.g., verbal men-
tions such as growing) and parsing errors. Parsing
errors often introduce incorrect mention boundaries,
which yield both recall and precision errors. For
example, our system generates the predicted men-
tion, the working meeting of the ?863 Program? to-
day, for the gold mention the working meeting of the
?863 Program?. Due to this boundary mismatch,
all mentions found to be coreferent with this pre-
dicted mention are counted as precision errors, and
all mentions in the same coreference cluster with the
gold mention are counted as recall errors.
Table 3 lists the results of our end-to-end system
on the development partition. ?External Resources?,
which were used only in the open track, includes: (a)
a hand-built list of genders of first names that we cre-
ated, incorporating frequent names from census lists
and other sources, (b) an animacy list (Ji and Lin,
2009), (c) a country and state gazetteer, and (d) a de-
monym list. ?Discourse? stands for the sieve intro-
duced in Section 2.3.3. ?Semantics? stands for the
sieves presented in Section 2.3.2. The table shows
that the discourse sieve yields an improvement of
almost 2 points to the overall score (row 1 versus
3), and external resources contribute 0.5 points. On
the other hand, the semantic sieves do not help (row
3 versus 4). The latter result contradicts our initial
experiments, where we measured a minor improve-
ment when these sieves were enabled and gold men-
tions were used. Our hypothesis is that, when pre-
dicted mentions are used, the semantic sieves are
more likely to link spurious mentions to existing
clusters, thus introducing precision errors. This sug-
gests that a different tuning of the sieve parameters
is required for the predicted mention scenario. For
this reason, we did not use the semantic sieves for
our submission. Hence, rows 2 and 3 in the table
show the performance of our official submission in
the development set, in the closed and open tracks
respectively.
The last three rows in Table 3 give insight on the
impact of gold information. This analysis indicates
that using gold linguistic annotation yields an im-
provement of only 2 points. This implies that the
quality of current linguistic processors is sufficient
for the task of coreference resolution. On the other
hand, using gold mentions raises the overall score by
15 points. This clearly indicates that pipeline archi-
tectures where mentions are identified first are inad-
equate for this task, and that coreference resolution
might benefit from the joint modeling of mentions
and coreference chains.
Finally, Table 4 lists our results on the held-out
testing partition. Note that in this dataset, the gold
mentions included singletons and generic mentions
32
Components MUC B3 CEAFE BLANC
ER D S GA GM R P F1 R P F1 R P F1 R P F1 avg F1?
58.8 56.5 57.6 68.0 68.7 68.4 44.8 47.1 45.9 68.8 73.5 70.9 57.3?
59.1 57.5 58.3 69.2 71.0 70.1 46.5 48.1 47.3 72.2 78.1 74.8 58.6? ?
60.1 59.5 59.8 69.5 71.9 70.7 46.5 47.1 46.8 73.8 78.6 76.0 59.1? ? ?
60.3 58.5 59.4 69.9 71.1 70.5 45.6 47.3 46.4 73.9 78.2 75.8 58.8? ? ?
63.8 61.5 62.7 71.4 72.3 71.9 47.1 49.5 48.3 75.6 79.6 77.5 61.0? ? ?
73.6 90.0 81.0 69.8 89.2 78.3 79.4 52.5 63.2 79.1 89.2 83.2 74.2? ? ? ?
74.0 90.1 81.3 70.2 89.3 78.6 79.7 53.1 63.7 79.5 89.6 83.6 74.5
Table 3: Comparison between various configurations of our system. ER, D, S stand for External Resources, Discourse,
and Semantics sieves. GA and GM stand for Gold Annotations, and Gold Mentions. The top part of the table shows
results using only predicted annotations and mentions, whereas the bottom part shows results of experiments with gold
information. Avg F1 is the arithmetic mean of MUC, B3, and CEAFE. We used the development partition for these
experiments.
MUC B3 CEAFE BLANC
Track Gold Mention Boundaries R P F1 R P F1 R P F1 R P F1 avg F1
Close Not Gold 61.8 57.5 59.6 68.4 68.2 68.3 43.4 47.8 45.5 70.6 76.2 73.0 57.8
Open Not Gold 62.8 59.3 61.0 68.9 69.0 68.9 43.3 46.8 45.0 71.9 76.6 74.0 58.3
Close Gold 65.9 62.1 63.9 69.5 70.6 70.0 46.3 50.5 48.3 72.0 78.6 74.8 60.7
Open Gold 66.9 63.9 65.4 70.1 71.5 70.8 46.3 49.6 47.9 73.4 79.0 75.8 61.4
Table 4: Results on the official test set.
as well, whereas in development (lines 6 and 7 in Ta-
ble 3), gold mentions included only mentions part of
an actual coreference chain. This explains the large
difference between, say, line 6 in Table 3 and line 4
in Table 4.
Our scores are comparable to previously reported
state-of-the-art results for coreference resolution
with predicted mentions. For example, Haghighi
and Klein (2010) compare four state-of-the-art sys-
tems on three different corpora and report B3 scores
between 63 and 77 points. While the corpora used
in (Haghighi and Klein, 2010) are different from the
one in this shared task, our result of 68 B3 suggests
that our system?s performance is competitive. In this
task, our submissions in both the open and the closed
track obtained the highest scores.
4 Conclusion
In this work we showed how a competitive end-to-
end coreference resolution system can be built using
only deterministic models (or sieves). Our approach
starts with a high-recall mention detection compo-
nent, which identifies mentions using only syntactic
information and named entity boundaries, followed
by a battery of high-precision deterministic corefer-
ence sieves, applied one at a time from highest to
lowest precision. These models incorporate lexical,
syntactic, semantic, and discourse information, and
have access to document-level information (i.e., we
share mention attributes across clusters as they are
built). For this shared task, we extended our ex-
isting system with new sieves that model shallow
discourse (i.e., speaker identification) and seman-
tics (lexical chains and alias detection). Our results
demonstrate that, despite their simplicity, determin-
istic models for coreference resolution obtain com-
petitive results, e.g., we obtained the highest scores
in both the closed and open tracks (57.8 and 58.3
respectively). The code used for this shared task is
publicly released.5
Acknowledgments
We thank the shared task organizers for their effort.
This material is based upon work supported by
the Air Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusion or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the view of the Air
Force Research Laboratory (AFRL).
5See http://nlp.stanford.edu/software/
dcoref.shtml for the standalone coreference resolution
system and http://nlp.stanford.edu/software/
corenlp.shtml for Stanford?s suite of natural language
processing tools, which includes this coreference resolution
system.
33
References
B. Baldwin. 1997. CogNIAC: high precision corefer-
ence with limited knowledge and linguistic resources.
In Proceedings of a Workshop on Operational Factors
in Practical, Robust Anaphora Resolution for Unre-
stricted Texts.
E. Bengston & D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting Complex Biological Events with Rich Graph-
Based Feature Sets. Proceedings of the Workshop on
BioNLP: Shared Task.
H. Daume? III and D. Marcu. 2005. A large-scale ex-
ploration of effective global features for a joint entity
detection and tracking model. In EMNLP-HLT.
B. A. Fox 1993. Discourse structure and anaphora:
written and conversational English. Cambridge Uni-
versity Press.
A. Haghighi and D. Klein. 2010. Coreference resolution
in a modular, entity-centered model. In Proc. of HLT-
NAACL.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R.
Weischedel 2006. OntoNotes: The 90% Solution. In
HLT/NAACL.
Z. Huang, G. Zeng, W. Xu, and A. Celikyilmaz 2009.
Accurate semantic class classifier for coreference res-
olution. In EMNLP.
J.R. Hobbs. 1977. Resolving pronoun references. Lin-
gua.
H. Ji and D. Lin. 2009. Gender and animacy knowl-
edge discovery from web-scale n-grams for unsuper-
vised person mention detection. In PACLIC.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of the BioNLP?09 Shared Task on Event Extrac-
tion. Proceedings of the NAACL-HLT 2009 Work-
shop on Natural Language Processing in Biomedicine
(BioNLP?09).
V. Ng 2007. Semantic Class Induction and Coreference
Resolution. In ACL.
V. Ng and C. Cardie. 2002. Improving Machine Learn-
ing Approaches to Coreference Resolution. in ACL
2002
S. Ponzetto and M. Strube. 2006. Exploiting semantic
role labeling, Wordnet and Wikipedia for coreference
resolution. Proceedings of NAACL.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Indentifying Entities and Events
in OntoNotes. In Proceedings of the IEEE Interna-
tional Conference on Semantic Computing (ICSC).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning (CoNLL 2011).
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning 2010.
A Multi-Pass Sieve for Coreference Resolution. In
EMNLP.
J. Tetreault and J. Allen. 2003. An Empirical Evalua-
tion of Pronoun Resolution and Clausal Structure. In
Proceedings of the 2003 International Symposium on
Reference Resolution.
J. Tetreault and J. Allen. 2004. Dialogue Structure and
Pronoun Resolution. In DAARC.
X. Yang and J. Su. 2007. Coreference Resolution Us-
ing Semantic Relatedness Information from Automat-
ically Discovered Patterns. In ACL.
34
