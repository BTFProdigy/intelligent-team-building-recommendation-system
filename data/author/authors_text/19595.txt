Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 520?529, Dublin, Ireland, August 23-29 2014.
Skill Inference with Personal and Skill Connections
Zhongqing Wang
?
, Shoushan Li
??
, Hanxiao Shi
?
, and Guodong Zhou
?
?
Natural Language Processing Lab, School of Computer Science and Technology,
Soochow University, China
?
School of Computer Science and Information Engineering,
Zhejiang Gongshang University, China
{wangzq.antony, shoushan.li}@gmail.com
hxshi@mail.zjgsu.edu.cn, gdzhou@suda.edu.cn
Abstract
Personal skill information on social media is at the core of many interesting applications. In
this paper, we propose a factor graph based approach to automatically infer skills from per-
sonal profile incorporated with both personal and skill connections. We first extract personal
connections with similar academic and business background (e.g. co-major, co-university, and
co-corporation). We then extract skill connections between skills from the same person. To well
integrate various kinds of connections, we propose a joint prediction factor graph (JPFG) model
to collectively infer personal skills with help of personal connection factor, skill connection fac-
tor, besides the normal textual attributes. Evaluation on a large-scale dataset from LinkedIn.com
validates the effectiveness of our approach.
1 Introduction
With the large amount of user-generated content (UGC) published online every day in the context of
social networks (Tan et al., 2011; Luo et al., 2013), such online social networks (e.g., Twitter, Facebook,
and LinkedIn) have significantly enlarged our social circles and much affected our everyday life. One
popular and important type of UGC is the personal profile, where people post their detailed information,
such as education, experience and other personal information, on online portals. Social websites like
Facebook.com and LinkedIn.com have created a viable business as profile portals, with the popularity
and success largely attributed to their comprehensive personal profiles.
Obviously, online personal profiles can help people connect with others of similar backgrounds and
provide valuable resources for businesses, especially for personnel resource managers to find talents
(Yang et al., 2011a; Guy et al., 2010). In the profiles, the personal skill information is the most impor-
tant aspect to reflect the expertise of a person. However, few social platforms allow users to manually
attach such personal skill information into their personal profiles. For example, in our collected dataset,
91.8% skills appear less than 10 times. Even the distribution of the top 10 frequently occurring skills is
asymmetric, and only 43.1% people attach skills on their profiles. For this regard, it is highly desirable
to develop reliable methods to automatically infer personal skills for personal profiles.
Although it is straightforward to recast skill inference as a standard text classification problem, i.e.,
predicting the skills with the profile text alone, personal profiles usually are poorly organized, even with
critical information missing. Thus, it is challenging to infer skills given the limited information from
the profile texts. We propose two assumptions to address above challenges by incorporating additional
connection information between persons and skills:
? People are always connected to others with similar academic and business backgrounds (e.g. co-
major, co-corporation). For example if there is co-major, co-university, or co-corporation rela-
tionship between two persons, it is very likely that they may share similar skills. Therefore, it is
reasonable to resort to personal connection information to improve the performance of skill infer-
ence.
*corresponding author
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
520
? One person tends to have some related skills. For example, it is very likely that C++, C, and Python
programming languages may co-occur in the one?s profile, i.e., if a person has skill C++, it is highly
possible that he would have the skills such as C or Python. Thus, it is useful to integrate skill
connection information when inferring personal skills.
Based on these assumptions, we propose a Joint Prediction Factor Graph (JPFG) model, which collec-
tively predicts personal skills with help of both personal and skill connections. In particular, the JPFG
model provides a general framework to integrate three kinds of knowledge, i.e. local textual attribute
functions of an individual person, personal connection factors between persons, and skill connection fac-
tors between skills, in collectively inferring personal skills. Specially, we extract personal connections
with similar academic and business background (e.g. co-major, co-corporation). We then extract skill
connections between skills from same person. Evaluation on a large-scale data set from LinkedIn.com
indicates that our JPFG model can significantly improve the performance of personal skill inference.
The remainder of this paper is structured as follows. We review the related work in Section 2. In Sec-
tion 3, we introduce the data collection. In Section 4, we give the problem definition and some analysis
on the task of personal skill reference. In Section 5, we propose the JPFG model and corresponding
algorithms for parameter estimation and prediction. In Section 6, we present our experimental results. In
Section 7, we summarize our work and discuss future directions.
2 Related Works
In this section, we briefly review related studies in expert finding, social tag suggestion and factor graph
model.
2.1 Expert Finding
Expert finding aims to find right persons with appropriate skills or knowledge, i.e. ?Who are the experts
on topic X?? TREC-2005 and TREC-2006 have provided a common platform for researchers to empiri-
cally evaluate methods and techniques on expert finding (Soboroff et al, 2006; Zhang et al., 2007a).
In the literature, expert finding tends to consider each skill individually and seeks the most authority
experts for each skill. Thus, expert finding is always considered as a ranking process, i.e., ranking the
experts from the candidates who are most suitable for the skill (Balog and Rijke, 2007). For example,
Campbell et al. (2003) investigated the issue of expert finding in an email network. They utilized the
link between email authors and receivers to improve the expert finding performance.
Besides that link structure-based algorithms, such as PageRank and HITS, are employed to analyze
the relationship of the link-relationship graph, social networks are utilized to improve the performance
of expert finding. Zhang et al. (2007a) proposed a unified propagation-based approach to address the
issue of expert finding in a social network, considering both personal local and network information (e.g.
the relationship between persons).
Expert finding is in nature different from skill inference. Our study predicts various skills attachable to
a person collectively with both personal and skill connections among people. One distinguishing charac-
teristics of our study is that several skills from a person are simultaneously modeled and the relationship
among these skills is fully leveraged in the inference.
2.2 Social Tag Suggestion
Social tag suggestion aims to extract proper tags from social media and can thus help people organize
their information in an unconstrained manner (Ohkura et al., 2006; Si et al., 2010). Ohkura et al. (2006)
created a multi-tagger to determine whether a particular tag from a candidate tag list should be attached
to a weblog. Lappas et al. (2011) proposed a social endorsement-based approach to generate social tags
from Twitter.com and Flickr.com where various kinds of information in recommendations and comments
are used. Liu et al. (2012) propose a probabilistic model to connect the semantic relations between words
and tags of microblog, and takes the social network structure as regularization. Li et al., (2012) propose
to model context-aware relations of tags for suggestion by regarding resource content as context of tags.
521
Different from above researches, our study is forced on skill inference instead of traditional tag sugges-
tion. Basically, the social connections in skill inference are much different from those in social tagging.
In our study, we use co-major, co-title and other academic and business relationships to build the social
connections. Meanwhile, there are also few researches concern to propose a joint model to leverage both
personal and skill connections.
2.3 Factor Graph Model
Among various approaches investigated in social networks in the last several years (Leskovec et al.,
2010; Lu et al., 2010; Lampos et al., 2013; Guo et al., 2013), Factor Graph Model (FGM) becomes an
effective way to represent and optimize the relationship in social networks (Dong et al., 2012; Yang et
al., 2012b) via a graph structure. Tang et al. (2011a) and Zhuang et al. (2012) formalized the problem
of social relationship learning as a semi-supervised framework, and proposed Partially-labeled Pairwise
Factor Graph Model (PLP-FGM) for inferring the types of social ties. Tang et al. (2013) further proposed
a factor graph based distributed learning method to construct a conformity influence model and formalize
the effects of social conformity in a probabilistic way.
Different from previous studies, this paper proposes a pairwise factor graph model to collectively infer
personal skills with both social connection factor and skill connection factor.
3 Data Construction
We collect our data set from LinkedIn.com. It contains a large number of personal profiles generated by
users, containing various kinds of information, such as personal Summary, Experience, Education, and
Skills & Expertise. We do not collect personal names in public profiles to protect people?s privacy.
The dataset contains 7,381 personal profiles, among which only 3,182 profiles (43.1% of all the pro-
files) show the Skills & Expertise field. In this study, we adopt only these profiles in all our experiments.
As a result, we get 6,863 skills in total, among which 6,299 skills (91.8% of them) appear less than 10
times. Among the remaining 564 skills, we select top 10 frequently occurring skills as the candidate
personal skills in this study (Since the remaining 554 skills only appear less than 250 times in total, it is
difficult to build an effective classifier for them). Table 1 illustrates the statistics.
Skill Number Ratio
Semiconductors 948 0.298
IC 369 0.116
Thin Films 328 0.103
Characterization 326 0.102
CMOS 311 0.098
Matlab 287 0.090
Microsoft Office 283 0.089
Manufacturing 278 0.087
Design of Experiments 262 0.082
Semiconductor Industry 250 0.079
Table 1: The distribution of the candidate personal skills
From Table 1, we can see that the skill distribution in the personal profiles is asymmetric. For example,
the Semiconductor skill occurs about 1,000 times, taking 29.8%, while the Semiconductor Industry skill
occurs 250 times only, taking 7.9%.
4 Problem Definition and Analysis
Before presenting our approach for skill inference, we first give the definition of the problem, and convey
a series of discoveries we observed from the data.
522
4.1 Problem Definition
We first introduce some necessary definitions and then formulate of the problem.
Definition 1: Skill inference. In principle, we cast skill inference as a skill prediction problem. Since
one person might have several skills, we build several vectors for a person and each vector is designed to
determine whether the corresponding skill is appropriate for the person or not (?Positive? means that the
person has the target skill, whereas ?Negative? stands for the opposite). Note that the number of vectors
for a person is equal to the number of candidate skills. For example, suppose we have m persons and
n candidate skills in the dataset, we totally build vectors to represent if these skills are attached in these
persons? profiles.
Definition 2: Textual information. We use texts of Summary and Experience as the textual information
for our research. Texts of Summary and Experience are unstructured information, while texts of Skills
& Expertise are structured information. However, some skills in the Skill & Expertise fields may not be
mentioned in the Summary and Experience fields.
Definition 3: Personal connections. We can explicitly extract four kinds of personal relationships
between two persons from the Education and Experience fields, as follows:
? co major, which denotes that two persons have the same major at school
? co univ, which denotes that two persons graduated from the same university
? co title, which denotes that two persons have the same title in a corporation.
? co corp, which denotes that two persons work in the same corporation.
Definition 4: Skill connections. We extract skill connections from same person. That is, if two vectors
are from the same person with different skills, we consider these two vectors share skill connections (e.g.
John has IC and Thin Films skills).
Learn task: Given the textual information of each profile, the personal connections between pro-
files, and skill connections of skill from same persons, the goal is to infer the skill through the above
information.
To learn the skill inference model, there are several requirements. First, the skills of persons are related
to multiple factors, e.g., network structure, personal connections, and skill connections, it is important to
find a unified model which is able to incorporate all the information together. Second, the algorithm to
learn the inference model should be efficient. In practice, the scale of the social network might be very
large.
4.2 Statistics and Observations
In the following, we give some statistics and observations on personal and skill connections.
Figure 1: The statistic of personal connection edges in our dataset
Statistics of personal connections: Figure 1 gives the statistics of personal connection edges. It
shows that with 3,182 profiles, there exist 332,390 personal connection edges. Besides, among all the
523
four relations, co major, co unvi, co title, and co corp occupy 11.7%, 40.0%, 17.7% and 30.6% respec-
tively.
Observations of skills connections: To validate the tendency of a person sharing similar skills, we
use PMI (Point-wise Mutual Information) to measure the co-occurrence between two skills. As a popular
way to measure the co-occurrence between a pair (Turney, 2002), PMI is calculated as follows:
PMI(i, j) = log
(
N
P (i&j)
P (i)P (j)
)
(1)
N is the number of profiles, P (i&j) denotes the probability of the skills (i.e., i and j) co-occurrence in
a person?s profile, while P (i) denotes the probability of the skill i appearing in a person?s profile.
Skill i Skill j PMI
C COMS 1.711
Thin Films Characterization 1.624
Thin Films Design of Experiments 1.543
Semiconductor Industry IC 1.345
Semiconductor Industry Design of Experiments 1.345
IC Microsoft Office -2.390
CMOS Microsoft Office -2.627
Semiconductor Industry Matlab -3.112
Average PMI score 0.190
Table 2: The top-5 and bottom-3 co-occurred skill pairs with their PMI scores
Table 2 lists the top-5 and bottom-3 co-occurred skill pairs with their PMI scores, together with the
average PMI score. From this table, we can see that if two skills are related, e.g., ?IC? and ?CMOS?,
these two skills tend to co-occur in one person?s profile, vice versa.
5 Joint Prediction Factor Graph Model
In this section, we propose a Joint Prediction Factor Graph (JPFG) model for learning and predicting the
skills with personal and skill connection information besides local textual information.
5.1 Model
We formalize the problem of skill prediction using a pairwise factor graph model, and our basic idea of
defining the correlations is to use different types of factor functions (i.e., personal connection factor, and
skill connection factor). Here, the objective function P
?
(Y |X,G) is defined based on the joint probability
of the factor functions, and the problem of collective skill inference model learning is cast as learning
model parameters ? that maximizes the joint probability of skills based on the input continuous dynamic
network.
Since directly maximizing the conditional probability P
?
(Y |X,G) is often intractable, we factorize
the ?global? probability as a product of ?local? factor functions, each of which depends on a subset of
the variables in the graph (Tang et al., 2013). In particular, we use three kinds of functions to represent
the local textual information of the vector (local textual attribute function), personal connection informa-
tion between vectors (personal connection factor) and skill connection information between skills (skill
connection factor), respectively. We now briefly introduce the ways to define the above three functions.
Local textual attribute functions f(x
ij
, y
i
)
j
: It denotes the attribute value associated with each
person i. Here, we define the local textual attribute as a feature (Lafferty et al., 2001) and accumulate all
the attribute functions to obtain local entropy for a person:
1
Z
1
exp
(
?
i
?
k
?
k
f
k
(x
ik
, y
i
)
)
(2)
524
Where ?
k
is the function weight, representing the influence degree of the attribute k. For simplicity, we
use word unigrams of a text as the basic textual attributes.
Personal connection factor function g(y
i
, y
j
) : For the personal correlation factor function, we
define it through the pairwise network structure. That is, if a person i and another person j have a
personal relationship, we define a personal connection factor function as follows:
g(y
i
, y
j
) = exp
{
?
ij
(y
i
? y
j
)
2
}
(3)
The personal connections are defined Section 4, i.e., co major, co univ, co title, and co corp. We define
that if two persons have at least one personal connection edge, they have a personal relationship. In
addition, ?
ij
is the weight of the function, representing the influence degree of i on j.
Skill connection factor function h(y
i
, y
j
): For the skill connection factor function, we define it
through the pairwise network structure. That is, if vector i and vector j are from the same person with
different skills, we define their skill connection influence factor function as follows:
h(y
i
, y
j
) = exp
{
?
ij
(y
i
? y
j
)
2
}
(4)
Where ?
ij
is the function weight, representing the influence degree of i on j.
By the above defined correlations, we can construct the graphical structure in the factor model. Ac-
cording to the Hammersley-Clifford theorem (Hammersley and Clifford, 1971), we integrate all the factor
functions and obtain the following log-likelihood objective function:
L(?) = log
?
P (Y |X,G)
=
1
Z
1
?
i
?
k
?
k
f
k
(x
ik
, y
i
)
+
1
Z
2
?
i
?
j?NB(i)
exp
{
?
ij
(y
i
? y
j
)
2
}
+
1
Z
3
?
i
?
k?SAME(i)
exp
{
?
ik
(y
i
? y
k
)
2
}
? logZ
(5)
Where (i, j) is a pair derived from the input network, Z = Z
1
Z
2
Z
3
is a normalization factor and
? = ({?}, {?}, {?}) indicates a parameter configuration, NB(i) denotes the set of social relationship
neighbors nodes of i (personal connection), and SAME(i) denotes the set of the node with the same
person of i (skill connection).
5.2 Learning and Prediction
Model Learning: Learning of the factor model is to find the best configuration for free parameters
? = ({?}, {?}, {?}) that maximizes the log likelihood objective function L(?).
?
?
= argmaxL(?) (6)
As the network structure in a social network can be arbitrary (e.g. possible of containing cycles), we
use the Loopy Belief Propagation (LBP) algorithm (Tang et al., 2011a) to approximate the marginal
distribution. To explain how we learn the parameters, we can get the gradient of each ?
k
with regard to
the objective function (Eq. 5), taking ? (the weight of the personal connection factor function g(y
i
, y
j
))
as an example:
L(?)
?
k
= E[g(i, j)] + E
?k
P (Y |X,G)
[g(i, j)] (7)
Where E[g(i, j)] is the expectation of factor function g(i, j) given the data distribution in the input
network and E
?k
P (Y |X,G)
[g(i, j)] represents the expectation under the distribution learned by the model,
i.e., P (y
i
|X,G) .
With the marginal probabilities, the gradient is obtained by summing up all triads (similar gradients
can be derived for parameter ?
k
and ?
ij
). It is worth noting that we need to perform the LBP process
525
twice in each iteration. The first run to estimate the marginal distribution of unknown variables y
i
=? and
the second one is to estimate the marginal distribution over all pairs. Finally, with the obtained gradient,
we update each parameter with a learning rate ?.
Skill Prediction: We can see that in the learning process, additional loopy belief propagation is used
to infer the label of unknown relationships. After learning, all unknown skills are assigned with labels
that maximize the marginal probabilities (Tang et al., 2011b), i.e.,
Y
?
= argmaxL(Y |X,G, ?) (8)
6 Experimentation
In this section, we first introduce the experimental setting, and then evaluate the performance of our
proposed JPFG model with both personal and skill connection information.
6.1 Experimental Setting
As described in Section 3, the experimental data are collected from LinkedIn.com. With top 10 frequently
used skills as candidate skills in all our experiments, we randomly select 2,000 profiles as training data
and 1,000 profiles as testing data.
Though positive and negative samples of each skill are imbalanced (In this paper, the number of the
negative samples is much larger than that of the positive samples), we select balanced testing and training
samples for each skill. Following models are implemented and compared.
? Keyword, for each profile, we consider the profile attached with the skill, only if the text of the skill
appears on the profile article with textual information.
? MaxEnt, which first uses local textual information as features to train a maximum entropy (ME)
classification model, and then employs the classification model to predict the skills in the testing
data set. The ME algorithm is implemented with the mallet toolkit
1
.
? JPFG, exactly our proposed model, which jointly predicts personal skills with local textual infor-
mation, personal connection and skill connection.
For performance evaluation, we adopt Precision (P.), Recall (R.) and F1-Measure (F1.).
6.2 Comparison with Baselines
Our first group of experiments is to investigate whether the JPFG model is able to improve skill inference
and whether the personal and skill connections are useful. The experimental results are shown in Table
3. From the table we can find that as some skills may not be mentioned on the Summary and Experience
fields directly, the performance of the Keyword approach is far from satisfaction. As incorporating
personal and skill connections, the JPFG model yields a much higher F1-measure, which improves the
performance with about 6.8% gain than the MaxEnt model.
6.3 Performance of JPFG with Different Training Data Sizes
After we evaluate the effective of the JPFG model with the large-scale training data, we carry out ex-
periments to test the effect of the JPFG model with different training data sizes. Experiment results are
shown in Figure 3. It shows that the JPFG model with both personal and skill connections always out-
perform the two baseline models. Impressively, our JPFG model using 20% training data outperforms
MaxEnt using 100% training data.
1
http://mallet.cs.umass.edu/
526
Figure 2: The performance of different methods for skill inference
Figure 3: The performance of JPFG with different training data sizes
6.4 Connections Contribution Analysis
Personal connections and skill connections can be also used to build the factor graph models to infer the
skills. We therefore want to compare our JPFG model with the factor graph model with only consider
the personal connections or skill connections, and analysis the contribution of each kinds of connection.
Specifically, MaxEnt-Personal employs the personal connections as additional features incorporated with
textual features to build the maximum entropy classification. FGM-Personal is a simplified version of
the JPFG model, which only employs textual attribute functions and personal connection factor functions
to build the factor graph model. Likewise, FGM-Skill only employs textual attribute functions and skill
connection factor functions to build the factor graph model. Table 3 shows the experiment results.
System P. R. F1.
MaxEnt 0.744 0.797 0.769
MaxEnt-Personal 0.758 0.812 0.783
FGM-Personal 0.765 0.817 0.790
FGM-Skill 0.704 0.967 0.815
JPFG 0.780 0.905 0.837
Table 3: The contribution of connections
From Table 3, we can observe that, 1) Both FGM-Personal and FGM-Skill outperform the baseline
527
MaxEnt approach. It shows that both personal connections and skill connections are helpful for skill
inference; 2) MaxEnt-Personal and FGM-Personal outperform the baseline MaxEnt approach, it show
that personal connections are helpful for inferring skills, and as considering the global optimization,
FGM-Personal is more effective; 3) FGM-Skill built on the skill connections is more effective than
MaxEnt-Personal and FGM-Personal, it show that skill connections are more useful than personal con-
nections; 4) JPFG model outperforms both FGM-Personal and FGM-Skill, it suggests that we should
incorporate both personal and skill connections to the factor graph model when we infer the skills from
profile.
7 Conclusion
In this study, we propose a novel task named personal skill inference, which aims to determine whether a
person takes a specific skill or not. To address this task, we propose a joint prediction factor graph model
with help of both personal and skill connections besides local textual information. Evaluation on a large-
scale dataset shows that our joint model performs much better than several baselines. In particular, it
shows that the performance on personal skill inference can be greatly improved by incorporating skill
connection information.
The general idea of exploring personal and skill connections to help predict people?s skills represents
an interesting research direction in social networking, which has many potential applications. Besides,
as skill information of a person is normally incomplete and fuzzy, how to better infer personal skills with
weakly labeled information is challenging.
Acknowledgements
This research work is supported by the National Natural Science Foundation of China (No. 61273320,
No. 61331011, and No. 61375073), National High-tech Research and Development Program of China
(No. 2012AA011102), Zhejiang Provincial Natural Science Foundation of China (No. LY13F020007),
the Humanity and Social Science on Young Fund of the Ministry of Education (No. 12YJC630170).
We thank Dr. Jie Tang and Honglei Zhuang for providing their software and useful suggestions about
PGM. We thank Prof. Deyi Xiong for helpful discussions, and we acknowledge Dr. Xinfang Liu, and
Yunxia Xue for corpus construction and insightful comments. We also thank anonymous reviewers for
their valuable suggestions and comments.
References
Balog K and M. Rijke. 2007. Determining Expert Profiles (With an Application to Expert Finding). In Proceedings
of IJCAI-07.
Campbell C, P. Maglio, A. Cozzi, and B. Dom. 2003. Expertise Identification Using Email Communications. In
Proceedings of CIKM-03.
Dong Y., J. Tang, S. Wu, J. Tian, N. Chawla, J. Rao, and H. Cao. 2012. Link Prediction and Recommendation
across Heterogeneous Social Networks. In Proceedings of ICDM-12.
Guo W., H. Li, H. Ji, and M. Diab. 2013. Linking Tweets to News: A Framework to Enrich Short Text Data in
Social Media. In Proceedings of ACL-13 .
Guy I., N. Zwerdling, I. Ronen, D. Carmel, and E. Uziel. 2010. Social Media Recommendation based on People
and Tags. In Proceedings of SIGIR-10 .
Hammersley J. and P. Clifford. 1971. Markov Field on Finite Graphs and Lattices, Unpublished manuscript.
Helic D. and M. Strohmaier. 2011. Building Directories for Social Tagging Systems. In Proceedings of CIKM-
2011.
Lafferty J, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting
and Labeling Sequence Data. In Proceedings of ICML-01.
528
Lampos V., D. Preo?iuc-Pietro, and T. Cohn. 2013. A User-centric Model of Voting Intention from Social Media.
In Proceedings of ACL-13.
Lappas T., K. Punera, and T. Sarlos. 2011. Mining Tags Using Social Endorsement Networks. In Proceedings of
SIGIR-11.
Li H., Z. Liu, and M. Sun. 2012. Random Walks on Context-Aware Relation Graphs for Ranking Social Tags. In
Proceedings of COLING-12.
Liu Z., X. Chen, and M. Sun. 2011. A Simple Word Trigger Method for Social Tag Suggestion. In Proceedings of
EMNLP-2011.
Liu Z., C. Tu, and M. Sun. 2012. Tag Dispatch Model with Social Network Regularization for Microblog User Tag
Suggestion. In Proceedings of COLING-12.
Lu Y., and P. Tsaparas, A. 2010. Ntoulas and L. Polanyi. 2010. Exploiting Social Context for Review Quality
Prediction. In Proceedings of WWW-10.
Luo T., J. Tang, J. Hopcroft, Z. Fang, and X. Ding. 2013. Learning to Predict Reciprocity and Triadic Closure in
Social Networks. ACM Transactions on Knowledge Discovery from Data. vol.7(2), Article No. 5.
Murphy K., Y. Weiss, and M. Jordan. 1999. Loopy Belief Propagation for Approximate Inference: An Empirical
Study. In Proceedings of UAI-99 .
Ohkura T., Y. Kiyota and H. Nakagawa. 2006. Browsing System for Weblog Articles based on Automated Folk-
sonomy. In Proceedings of WWW-06.
Si X., Z. Liu, and M. Sun. 2010. Explore the Structure of Social Tags by Subsumption Relations. In Proceedings
of COLING-10.
Soboroff I., A. Vries and N. Craswell. 2006. Overview of the TREC 2006 Enterprise Track In Proceedings of
TREC-06.
Turney P. 2002. Thumbs up or Thumbs down? Semantic Orientation Applied to Unsupervised Classification of
reviews. In Proceedings of ACL-02.
Tan C., L. Lee, J. Tang, L. Jiang, M. Zhou, and P. Li. 2011. User-Level Sentiment Analysis Incorporating Social
Networks. In Proceedings of KDD-11.
Tang W., H. Zhuang, and J. Tang. 2011a. Learning to Infer Social Ties in Large Networks. In Proceedings of
ECML/PKDD-11.
Tang J., Y. Zhang, J. Sun, J. Rao, W. Yu, Y. Chen, and A. Fong. 2011b. Quantitative Study of Individual Emotional
States in Social Networks. IEEE Transactions on Affective Computing. vol.3(2), Pages 132-144.
Tang J., S. Wu, J. Sun, and H. Su. 2012. Cross-domain Collaboration Recommendation. In Proceedings of KDD-
12.
Tang J., S. Wu, and J. Sun. 2013. Confluence: Conformity Influence in Large Social Networks. In Proceedings of
KDD-13.
Xing E, M. Jordan, and S. Russell. 2003. A Generalized Mean Field Algorithm for Variational Inference in Expo-
nential Families. In Proceedings of UAI-03.
Yang S., B. Long, A. Smola, N. Sadagopan, Z. Zheng, and H. Zha. 2011a. Like like alike - Joint Friendship and
Interest Propagation in Social Networks. In Proceedings of WWW-11.
Yang Z., K. Cai, J. Tang, L. Zhang, Z. Su, and J. Li. 2011b. Social Context Summarization. In Proceedings of
SIGIR-11.
Zhang J., J. Tang, and J. Li. 2007a. Expert Finding in A Social Network. In Proceedings of the Twelfth Database
Systems for Advanced Applications (DASFAA-2007).
Zhang J., M. Ackerman, and L. Adamic. 2007b. Expertise Networks in Online Communities: Structure and Algo-
rithms. In Proceedings of TREC-07.
Zhuang H, J. Tang, W. Tang, T. Lou, A. Chin, and X. Wang. 2012. Actively Learning to Infer Social Ties. In
Proceedings of Data Mining and Knowledge Discovery (DMKD-12), vol.25 (2), pages 270-297.
529
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 715?725,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Collective Personal Profile Summarization with Social Networks 
 
 
Zhongqing Wang, Shoushan Li*, Kong Fang, and Guodong Zhou 
Natural Language Processing Lab, School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China  
{wangzq.antony, shoushan.li}@gmail.com,  
{kongfang, gdzhou}@suda.edu.cn 
 
  
 
Abstract 
Personal profile information on social media 
like LinkedIn.com and Facebook.com is at the 
core of many interesting applications, such as 
talent recommendation and contextual advertis-
ing. However, personal profiles usually lack or-
ganization confronted with the large amount of 
available information. Therefore, it is always a 
challenge for people to find desired information 
from them. In this paper, we address the task of 
personal profile summarization by leveraging 
both personal profile textual information and so-
cial networks. Here, using social networks is 
motivated by the intuition that, people with 
similar academic, business or social connections 
(e.g. co-major, co-university, and co-
corporation) tend to have similar experience and 
summaries. To achieve the learning process, we 
propose a collective factor graph (CoFG) model 
to incorporate all these resources of knowledge 
to summarize personal profiles with local textual 
attribute functions and social connection factors. 
Extensive evaluation on a large-scale dataset 
from LinkedIn.com demonstrates the effective-
ness of the proposed approach.* 
1 Introduction 
Web 2.0 has empowered people to actively interact 
with each other, forming social networks around 
mutually interesting information and publishing a 
large amount of useful user-generated content 
(UGC) online (Lappas et al, 2011; Tan et al, 
2011). One popular and important type of UGC is 
the personal profile, where people post detailed 
                                                 
* Corresponding author 
information on online portals about their education, 
experiences and other personal information. Social 
websites like Facebook.com and LinkedIn.com 
have created a viable business as profile portals, 
with the popularity and success partially attributed 
to their comprehensive personal profiles. 
Generally, online personal profiles provide val-
uable resources for businesses, especially for hu-
man resource managers to find talents, and help 
people connect with others of similar backgrounds 
(Yang et al, 2011a; Guy et al, 2010). However, as 
there is always large-scale information of experi-
ence and education fields, it is hardly for us to find 
useful information from the profile. Therefore, it is 
always a challenge for people to find desired in-
formation from them. For this regard, it is highly 
desirable to develop reliable methods to generate a 
summary of a person through his profile automati-
cally.  
To the best of our knowledge, this is the first re-
search that explores automatic summarization of 
personal profiles in social media. A straightfor-
ward approach is to consider personal profile 
summarization as a traditional document summari-
zation problem, which treating each personal pro-
file independently and generate a summary for 
each personal profile individually. For example, 
the well-known extraction and ranking approaches 
(e.g. PageRank, HITS) extract a certain amount of 
important sentences from a document according to 
some ranking measurements to form a summary 
(Wan and Yang, 2008; Wan, 2011).  
However, such straightforward approaches are 
not sufficient to benefit from the carrier of person-
al profiles. As the centroid of social networking, 
people are usually connected to others with similar 
715
background in social media (e.g. co-major, co-
corporation). Therefore, it is reasonable to lever-
age social connection to improve the performance 
of profile summarizing. For example if there are 
co-major, co-university, co-corporation or other 
academic and business relationships between two 
persons, we consider them sharing similar experi-
ence and having similar summaries. 
The remaining challenge is how to incorporate 
both the profile textual information and the con-
nection knowledge in the social networks. In this 
study, we propose a collective factor graph model 
(CoFG) to summarize the text of personal profile 
in social networks with local textual information 
and social connection information. The CoFG 
framework utilizes both the local textual attribute 
functions of an individual person and the social 
connection factor between different persons to col-
lectively summarize personal profile on one person. 
In this study, we treat the profile summarization 
as a supervised learning task. Specifically, we 
model each sentence of the profile as a vector. In 
the training phase, we use the vectors with the so-
cial connection between each person to build the 
CoFG model; while in the testing phase, we per-
form collective inference for the importance of 
each sentence and select a subset of sentences as 
the summary according to the trained model. Eval-
uation on a large-scale data from LinkedIn.com 
indicates that our proposed joint model and social 
connection information improve the performance 
of profile summarization. 
The remainder of our paper is structured as fol-
lows. We go over the related work in Section 2. In 
Section 3, we introduce the data we collected from 
LinkedIn.com and the annotated corpus we con-
structed. In Section 4, we present some motiva-
tional analysis. In Section 5, we explain our pro-
posed model and describe algorithms for parame-
ter estimation and prediction. In Section 6, we pre-
sent our experimental results. We sum up our work 
and discuss future directions in Section 7. 
2 Related Work 
In this section, we will introduce the related work 
on the traditional topic-based summarization, so-
cial-based summarization and factor graph model 
respectively. 
2.1 Topic-based Summarization 
Generally, traditional topic-based summarization 
can be categorized into two categories: extractive 
(Radev et al, 2004) and abstractive (Radev and 
McKeown, 1998) summarization. The former se-
lects a subset of sentences from original docu-
ment(s) to form a summary; the latter reorganizes 
some sentences to form a summary where several 
complex technologies, such as information fusion, 
sentence compression and reformulation are nec-
essarily employed (Wan and Yang, 2008; Celiky-
ilmaz and Hakkani-Tur, 2011; Wang and Zhou, 
2012). This study focuses on extractive summari-
zation.  
Radev et al (2004) proposed a centroid-based 
method to rank the sentences in a document set, 
using various kinds of features, such as the cluster 
centroid, position and TF-IDF features. Ryang and 
Abekawa (2012) proposed a reinforcement learn-
ing approach on text summarization, which models 
the summarization within a reinforcement learn-
ing-based framework.  
Compared to unsupervised approaches, super-
vised learning for summarization is relatively rare. 
A typical work is Shen et al, (2007) which present 
a Conditional Random Fields (CRF) based frame-
work to treat the summarization task as a sequence 
labeling problem. However, different from all ex-
isting studies, our work is the first attempt to con-
sider both textual information and social relation-
ship information for supervised summarization. 
2.2 Social-based Summarization 
As web 2.0 has empowered people to actively in-
teract with each other, studies focusing on social 
media have attracted much attention recently 
(Meeder et al, 2011; Rosenthal and McKeown, 
2011; Yang et al, 2011a). Social-based summari-
zation is exactly a special case of summarization 
where the social connection is employed to help 
obtaining the summarization. Although topic-
based summarization has been extensively studied, 
studies on social-based summarization are relative 
new and rare.  
Hu et al, (2011) proposed an unsupervised Pag-
eRank-based social summarization approach by 
incorporating both document context and user con-
text in the sentence evaluation process. Meng et al, 
(2012) proposed a unified optimization framework 
to produce opinion summaries of tweets through 
716
integrating information from dimensions of topic, 
opinion and insight, as well as other factors (e.g. 
topic relevancy, redundancy and language styles). 
Unlike all the above studies, this paper focuses 
on a novel task, profile summarization. Further-
more, we employ many other kinds of social in-
formation in profiles, such as co-major, and co-
corporation between two people. They are shown 
to be very effective for profile summarization.  
2.3 Factor Graph Model 
As social network has been investigated for sever-
al years (Leskovec et al, 2010; Tan et al, 2011; 
Lu et al, 2010; Guy et al, 2010) and Factor Graph 
Model (FGM) is a popular approach to describe 
the relationship of social network (Tang et al, 
2011a; Zhuang et al, 2012). Factor Graph Model 
builds a graph to represent the relationship of 
nodes on the social networks, and the factor func-
tions are always considered to represent the rela-
tionship of the nodes. 
Tang et al (2011a) and Zhuang et al (2012) 
formalized the problem of social relationship 
learning into a semi-supervised framework, and 
proposed Partially-labeled Pairwise Factor Graph 
Model (PLP-FGM) for learning to infer the type of 
social ties. Dong et al (2012) gave a formal defini-
tion of link recommendation across heterogeneous 
networks, and proposed a ranking factor graph 
model (RFG) for predicting links in social net-
works, which effectively improves the predictive 
performance. Yang et al, (2011b) generated sum-
maries by modeling tweets and social contexts into 
a dual wing factor graph (DWFG), which utilized 
the mutual reinforcement between Web documents 
and their associated social contexts.  
Different from all above researches, this paper 
proposes a pair-wise factor graph model to collec-
tively utilize both textual information and social 
connection factor to generate summary of profile. 
3 Data Collection and Statistics   
The personal profile summarization is a novel task 
and there exists no related data for accessing this 
issue. Therefore, in this study, we collect a data set 
containing personal summaries with the corre-
sponding knowledge, such as the self-introduction 
and personal profiles. In this section, we will in-
troduce this data set in detail. 
3.1 Data Collection  
We collect our data set from LinkedIn.com1 . It 
contains a large number of personal profiles gen-
erated by users, containing various kinds of infor-
mation, such as personal overview, summary, edu-
cation, experience, projects and skills.  
 
John Smith2  
Overview 
Current Applied Researcher at Apple Inc. 
Previous 
Senior Research Scientist at IBM 
? 
Education 
MIT, 
Georgia Institute of Technology,   
? 
Summary 
Machine learning researcher and engineer on 
many fields: 
Query understanding. Automatic Information 
extraction? 
Experience 
Applied Researcher 
Apple Inc., September 2012 ~  
Query recognition and relevance 
? 
Education 
MIT 
Ph.D., Electrical Engineering, 2002 ? 2008 
? 
Figure 1: An example of a profile webpage from 
LinkedIn.com 
 
In this study, the data set is crawled in the fol-
lowing ways. To begin with, 10 random people?s 
public profiles are selected as seed profiles, and 
then the profiles from their ?People Also Viewed? 
field were collected. The data is composed of 
3,182 public profiles3 in total. We do not collect 
personal names in public profiles to protect peo-
ple?s privacy. Figure 1 shows an example of a per-
son?s profile from LinkedIn.com. The profile in-
cludes following fields: 
? Overview: It gives a structure description of a 
person?s general information, such as cur-
rent/previous position and workplace, brief 
                                                 
1 http://www.linkedin.com 
2 The information of the example is a pseudo one. 
3 We collect all the data from LinkedIn.com at Dec 17, 
2012.  
717
education background and general technical 
background.  
? Summary: It summarizes a person?s work, 
experience and education.  
? Experience: It details a person?s work experi-
ence.  
? Education: It details a person?s education 
background.  
Among these fields, the Overview is required 
and the others are optional, such as Project, 
Course and Interest groups. However, compared 
with Overview, Summary, Experience, Education 
fields, they seem to be less important for summari-
zation of personal profiles. Thus, we ignore them 
in our study. 
3.2 Data Statistics of Major Fields 
We collected 3,182 personal profiles from 
LinkedIn.com. Table 1 shows the statistics of ma-
jor fields in our data collection. 
 
Field 
#Non-empty 
fields 
Average 
field 
length 
Overview 3,182 45.1 
Summary 921 25.8 
Experience 3,148 192.1 
Education 2,932 33.6 
Table 1: Statistics of major fields in our data set, i.e. the 
number of non-empty fields and the average length for 
each field 
 
From Table 1, we can see that, 
? The information of each profile is incom-
plete and inconsistent, That is, not all kinds 
of fields are available in each personal?s 
profile.  
? Most people provide their experience and 
education information. However, the Sum-
mary fields are popularly missing (Only 
about 30% of people provide it). This is 
mainly because writing summary is nor-
mally more difficult than other fields. 
Therefore, it is highly desirable to develop 
reliable automatic methods to generate a 
summary of a person through his/her pro-
file. 
? The length of the Experience field is the 
longest one, and work experience always 
could represent general information of 
people.  
3.3 Corpus Construction and Annotation  
Among the 921 profiles that contain the summary, 
we manually select 497 profiles with high quality 
summary to construct the corpus for our research. 
These high-quality summaries are all written by 
the authors themselves. Here, the quality is meas-
ured by manually checking that whether they are 
well capable of summarizing their profiles. That is, 
they are written carefully, and could give an over-
view of a person and represent the education and 
experience information of a person. 
After carefully seeing the profiles, we observe 
that the Experience field contains the most abun-
dant information of a person. Thus, we treat the 
text of Experience field as the source of summary 
for each profile. Besides, we collect social context 
information from Education and Experience field, 
and these social contexts are including by 
LinkedIn explicitly. Table 2 shows the average 
length of summary and experience fields we used 
for evaluating our summarization approach.  
 
Field 
Average 
length 
Summary 
(the summary of the 
profile) 
37.2 
Experience 
(the source text for the 
summarizing) 
372.0 
Table 2: Average length of the high-quality summary  
and corresponding experience fields 
 
From Table 2, we can see that,  
? Compared with the average length of 25.8 
in Table 1, summaries of high quality have 
longer length because they contain more in-
formation of the profiles.  
? The compression ratio of our proposed cor-
pus is 0.1 (37.2/372.0).  
4 Motivation and Analysis 
In this section, we propose the motivation of social 
connection to address the task of personal profile 
summarization. To preliminarily support the moti-
vation, some statistics of the social connection are 
provided. 
718
 Figure 2: An example of personal profile network.  
Red is for female, blue is for male, and the dotted line 
means the social connection between two persons. 
 
We first describe the social connections which 
we used. Figure 2 shows an example of social 
connection between people from the profiles of 
LinkedIn. We find that people are sometimes con-
nected by several social connections. For example, 
John and Lucy are connected by co_unvi relation-
ship, while Lily and Linda are connected by 
co_corp relationship. From LinkedIn, four kinds of 
social relationship between people are extracted 
from the Education field and Experience field. 
They are: 
? co_major denotes that two persons have the 
same major at school 
? co_univ denotes that two persons are graduat-
ed from the same university 
? co_title denotes that two persons have the 
same title at corporation. 
? co_corp denotes that two persons work at the 
same corporation. 
Our basic motivation of using social connection 
lies in the fact that ?connected? people will tend to 
hold related experience and similar summaries.  
We then give the statistics of edges of social 
connection. Table 3 shows basic statistics across 
these edges. From Table 3, we can see that the 
number of users is 497 while the number of social 
connection edges is 14,307. The latter is much 
larger than the former. The number of the edges 
from Education field is similar with the number of 
the edges from Experience filed. Among all the 
relationships, co_unvi is the most common one.  
 
 Numbers 
# users 497 
co_major 1,288 
co_unvi 6,015 
# education field 7,303 
co_title 3,228 
co_corp 3,776 
# experience field 7,004 
# total edges 14,307 
Table 3: The statistic of edges for our main datasets 
5 Collective Factor Graph Model 
In this section, we propose a collective factor 
graph (CoFG) model for learning and summarizing 
the text of personal profile with local textual in-
formation and social connection. 
5.1 Overview of Our Framework 
To generate summaries for profiles, a straightfor-
ward approach is to treat each personal profile in-
dependently and generating a summary for each 
personal profile individually. As we mentioned on 
Section 3.3, we use the sentences of Experience 
field as a text document and consider it as the 
source of summary for each profile. 
Instead, we formalize the problem of personal 
profile summarization in a pair-wise factor graph 
model and propose an approach referred to as 
Loopy Belief Propagation algorithm to learn the 
model for generating the summary of the profile. 
Our basic idea is to define the correlations using 
different types of factor functions. An objective 
function is defined based on the joint probability 
of the factor functions. Thus, the problem of col-
lective personal profile summarization model 
learning is cast as learning model parameters that 
maximizes the joint probability of the input con-
tinuous dynamic network. 
The overview of the proposed method is a su-
pervised framework (as shown in Figure 3).  First, 
we treat each sentence of the training data and test-
ing data as vectors with textual information (local 
textual attribute functions); Second, all the vectors 
are connected by social connection relationships 
(social connection factors) and we model these 
vectors and their relationships into the collective 
factor graph; third, we propose Loopy Belief Prop-
 
John 
Antony 
   Bill 
Lily  
Lucy  
       Linda 
 
 
 
 
 
co_major 
co_univ 
co_corp 
co_corp 
co_title 
co_title 
co_major 
co_univ 
719
agation algorithm to learn the model and predict 
the sentences of testing data; finally, we select a 
subset of sentences of each testing profile as the 
summary according to the models with top-n pre-
diction score. Thus, the core issues of our frame-
work are 1) how to define the collective factor 
graph model to connection profiles with social 
connection; 2) how to learn and predict the pro-
posed CoFG model; 3) how to predict the sentenc-
es from the testing data with the proposed CoFG 
model, and generate the summary by the predict 
scores. We will discuss these issues on the follow-
ing subsections. 
 
 
Figure 3: The overview of our proposed framework 
 
5.2 Model Definition 
Formally, given a network ( , , , )L UG V S S X? , 
each sentence 
is  is associated with an attribute 
vector 
ix  of the profile and a label iy  indicating 
whether the sentence is selected as a summary of 
the profile (The value of 
iy  is binary. 1 means that 
the sentence is selected as a summary sentence, 
whereas 0 stands for the opposite). V denotes the 
authors of the profiles, LS  denotes the labeled 
training data, and US denotes the unlabeled testing 
data. Let { }iX x? and { }iY y? . Then, we have the 
following formulation 
         
? ? ? ? ? ?? ?
, || , ,
P X G Y P YP Y X G P X G?
             (1) 
Here, G denotes all forms of network infor-
mation. This probabilistic formulation indicates 
that labels of skills depend on not only local at-
tributes X, but also the structure of the network G. 
According to Bayes? rule, we have 
         ? ? ? ? ? ?? ?
? ? ? ?
, |
| ,
,
                  | |
P X G Y P Y
P Y X G
P X G
P X Y P Y G
?
?
             (2) 
Where ( | )P Y G represents the probability of labels 
given the structure of the network and ( | )P X Y  
denotes the probability of generating attributes X
associated to their labels Y . We assume that the 
generative probability of attributes given the label 
of each edge is conditionally independent, thus we 
have 
? ? ? ? ? ?| , | |i iiP Y X G P Y G P x y? ?
    (3) 
Where ( | )i iP x y  is the probability of generating 
attributes 
ix given the label iy . Now, the problem 
becomes how to instantiate the probability 
( | )P Y G and ( | )i iP x y . We model them in a Mar-
kov random field, and thus according to the Ham-
mersley-Clifford theorem (Hammersley and 
Clifford, 1971), the two probabilities can be in-
stantiated as follows: 
? ? ? ?
11
1| exp ,
d
i i j j ij i
j
P x y f x yZ ??
? ?? ? ?? ??
       (4) 
? ? ? ?
( )2
1| exp ,
i j NB i
P Y G g i jZ ?
? ?? ? ?? ?? ?
       (5) 
                       
Where 
1 2 and Z Z  are normalization factors. Eq. 4 
indicates that we define an attribute function 
( , )i if x y  for each attribute ijx
 associated with 
sentence
is . j?  is the weight of the j
th attribute. Eq. 
5 represents that we define a set of correlation fac-
tor functions ( , )g i j  over each pair ( , )i j in the 
network. ( )NB i  denotes the set of social relation-
ship neighbors nodes of i.  
 
 
Training  
Set 
 
  
  
Social  
Connection 
Social  
Connection 
Testing  
Set 
  Sentence Scoring 
  Sentence Selection 
 Summarized Profile 
Profiles 
Profiles 
Collective Factor Graph 
Modeling 
  
720
 1 
3 
2 
  
  
 
 
 
  
 
 
 
 
 
 
 
f (v1,y1) 
y
2
 
y
1
 y3 
y
4
 
y
5
 
y
6
 
 S
1
 
 S
2
 
S
3
 
S
4
 
S
5
 
S
6
 
f (v
1
,y
2
) 
f (v
6
,y
6
) 
 
CoFG model 
Nodes of sentences 
with different people 
y1=0 
y
2
=1 
y
3
=1 
y
4
=0 
y
6
=? 
y
5
=? g (y
3
,y
5
) 
Figure 4: Graph representation of CoFG 
The left figure shows the personal profile network. Each dotted line indicates a social connection. Each dotted 
square denotes a person, and the grey square denotes the sentence selected in the summary, and the white square 
denotes a sentence that is not selected as the summary.. 
The right figure shows the CoFG model derived from left figure. Each eclipse denotes a sentence vector of a 
person, and each circle indicates the hidden variable yi. f(vi,yi) indicates the attribute factor function. g(yi,yj) indi-
cates the social connection factor function. 
 
4 
5 
6 
  
  
co_major 
co_corp 
  
Person A 
Person B 
Person C 
We now briefly introduce possible ways to de-
fine the attribute functions{ ( , )}ij i jf x y
, and factor 
function ( , )g i j  .  
Local textual attribute functions{ ( , )}ij i jf x y
: 
It denotes the attribute value associated with each 
sentence i. We define the local textual attribute as 
a feature (Lafferty et al, 2001). We can accumu-
late all the attribute functions and obtain local en-
tropy for a person: 
? ?
1
1 exp ,k k ik i
i k
f x yZ ?
? ?? ?? ???
              (6) 
The textual attributes include following features 
(Shen et al, 2007; Yang et al, 2011b):  
1) BOW: the bag-of-words of each sentence, we 
use unigram features as the basic textual fea-
tures for each sentence.  
2) Length: the number of terms of each sentence. 
3) Topic_words: these are the most frequent 
words in the sentence after the stop words are 
removed. 
4) PageRank_scores: as shown in the related 
work section, a document can be treated as a 
graph and applying a graph-based ranking al-
gorithm (Wan and Yang., 2008). We thus use 
the PageRank score to reflect the importance 
of each sentence. 
Social connection factor function ( , )i jg y y
: 
For the social correlation factor function, we de-
fine it through the pairwise network structure. That 
is, if the person of sentence i and the person of 
sentence j have a social relationship, a factor func-
tion for this social connection is defined (Tang et 
al., 2011a; Tang et al, 2011b), i.e., 
? ? ? ?? ?2, expi j ij i jg y y y y?? ?         (7) 
The person-person social relationships are de-
fined on Section 4, e.g. co_major, co_univ, co_title, 
and co_corp. We define that if two persons have at 
least one social connection edge, they have a so-
cial relationship. In addition, 
ij?  is the weight of 
the function, representing the influence degree of i 
on j. 
To better understand our model, one example of 
factor decomposition is given in Figure 4. In this 
example, there are six sentences from three pro-
files. Among them, four sentences are labeled (two 
are labeled with the category of ?1?, i.e,  1y ?  and 
the other two are labeled with the category of ?0?, 
i.e., 0y ? ) and two sentences are unlabeled (they 
are represented by y=?). We have six attribute 
functions. For example, 
1( , )if v y  denotes the set 
721
of local textual attribute functions of 
iy . We also 
have five pairwise relationships (e.g.,
2 4( , )y y ,
3 5( , )y y ) based on the structure of the input per-
sonal profile social network. For example, 
3 5( , )g y y  denotes social connection between 3y  
and 
5y , while they share the co_major relationship 
on the left figure. 
5.3 Model Learning 
We now address the problem of estimating the free 
parameters. The objective of learning the CoFG 
model is to estimate a parameter configuration 
({ },{ })? ? ??  to maximize the log-likelihood ob-
jective function ( ) log ( | , )L P Y X G?? ? , i.e., 
? ?* argmax L? ??                     (9) 
To solve the objective function, we adopt a gra-
dient descent method. We use ?  (the weight of 
the social connection factor function ( , )i jg y y
) as 
the example to explain how we learn the parame-
ters (the algorithm also applies to tune ?  by simp-
ly replacing ? with? ). Specifically, we first write 
the gradient of each 
k? with regard to the objective 
function (Eq. 9) :  
  ? ? ? ? ? ?( | , ), ,kP Y X G
k
L E g i j E g i j?
?
? ? ? ? ? ? ?? ? ? ?
   (10) 
Where [ ( , )]E g i j is the expectation of factor 
function ( , )g i j  given the data distribution (essen-
tially it can be considered as the average value of 
the factor function ( , )g i j over all pair in the train-
ing data); and 
( | , ) [ ( , )]k Y X GPE g i j?
is the expectation of 
factor function ( , )g i j under the distribution 
( | , )kP Y X G?
given by the estimated model. A 
similar gradient can be derived for parameter
ja
. 
We approximate the marginal distribution
( | , ) [ ( , )]k Y X GPE g i j?
 using LBP (Tang et al, 2011; 
Zhuang et al, 2012). With the marginal probabili-
ties, the gradient can be obtained by summing over 
all triads. It is worth noting that we need to per-
form the LBP process twice for each iteration: one 
is to estimate the marginal distribution of unknown 
variables ?iy ?  and the other is to estimate the 
marginal distribution over all pairs. In this way, 
the algorithm essentially performs a transfer learn-
ing over the complete network. Finally, with the 
obtained gradient, we update each parameter with 
a learning rate? . The learning algorithm is sum-
marized in Figure 5. 
 
Input: Network G , Learning rate ?   
Output: Estimated parameters ?   
Initialize 0? ?   
Repreat 
1) Perform LBP to calculate the 
marginal distribution of unknown 
variables, i.e., ? ?| ,i iP y x G   
2) Perform LBP to calculate the 
marginal distribution of each  
variables, i.e., ? ?( , ), | ,i j i jP y y X G  
3) Calculate the gradient of 
k? ac-
cording to Eq. 10 (for a  with a 
similar formula) 
4) Update parameter ?  with the 
learning rate ?  
               
? ?
new old
L ?? ? ? ?? ?  
Until Convergence 
Figure 5: The Learning Algorithm for CoFG model 
 
5.4 Model Prediction and Summary Gener-
ated 
We can see that in the learning process, the learn-
ing algorithm uses an additional loopy belief prop-
agation to infer the label of unknown relationships. 
With the estimated parameter ? , the summariza-
tion process is to find the most likely configuration 
of Y  for a given profile. This can be obtained by  
? ?* argmax | , ,Y L Y X G ??              (11) 
Finally, we select a subset of sentences of each 
testing profile as the summary according to the 
trained models with top-n prediction scores by *Y   
(Tang et al, 2011b; Dong et al 2012).  
6 Experimentation 
In this section, we describe the settings of our ex-
periment and present the experimental results of 
our proposed CoFG model. 
722
6.1 Experiment Settings 
In the experiment, we use the corpus collected 
from LinkedIn.com that contains 497 profiles (see 
more details in Section 3). The existing summaries 
in these profiles are served as the reference sum-
mary (the standard answers). As discussed in sub-
section 3.3, the average length of summary is 
about 40 words. Thus, we extract 40 words to con-
struct the summary for each profile. We use 200 
personal profiles as the testing data, and the re-
maining ones as the training data. 
We use the ROUGE-1.5.5 (Lin and Hovy, 2004) 
toolkit for evaluation, a popular tool that has been 
widely adopted by several evaluations such as 
DUC and TAC (Wan and Yang, 2008; Wan, 2011). 
We provide four of the ROUGE F-measure scores 
in the experimental results: ROUGE-2 (bigram-
based), ROUGE-L (based on longest common 
subsequences), ROUGE-W (based on weighted 
longest common subsequence, weight=1.2), and 
ROUGE-SU4 (based on skip bigram with a maxi-
mum skip distance of 4).  
6.2 Experimental Results 
We compare the proposed CoFG approach with 
three baselines illustrated as follows: 
? Random: we randomly select sentences of 
each profile to generate the summary for the 
profile. 
? HITS: we employ the HITS algorithm to per-
form profile summarization (Wan and Yang, 
2008). In detail, we first consider the words as 
hubs the sentences as authorities; Then, we 
rank the sentences with the authorities? scores 
for each profile individually; Finally, the 
highest ranked sentences are chosen to consti-
tute the summary. 
? PageRank: we employ the PageRank algo-
rithm to perform profile summarization (Wan 
and Yang, 2008). In detail, we first connect 
the sentences of the profile with cosine text-
based similar measure to construct a graph; 
Then, we apply PageRank algorithm to rank 
the sentence through the graph for each pro-
file individually; Finally, the highest ranked 
sentences are chosen to constitute the sum-
mary.  
?  MaxEnt: as a supervised learning approach, 
maximum entropy uses textual attribute as 
features to train a classification model. Then, 
the classification model is employed to pre-
dict which sentences can be selected to gener-
ate the summary. For the implementation of 
MaxEnt, we employ the tool of mallent 
toolkits4. 
Table 4 shows the comparison results of our ap-
proach (CoFG) and the baseline approaches. From 
Table 4, we can see that 1) either HITS or Pag-
eRank outperforms the approach of  random selec-
tion; 2) The supervised approach i.e. MaxEnt, out-
performs both the HITS algorithm and the Pag-
eRank approach; 3) CoFG model performs best 
and it greatly outperforms both the unsupervised 
and supervised learning baseline approaches in 
terms of the ROUGE-2 F-measure score. This re-
sult verifies the effectiveness of considering the 
social connection between the sentences in differ-
ent profiles, 
Figure 6 shows the performance of our proposed 
CoFG model with different sizes of training data. 
From Figure 6, we can see that CoFG model with 
social connection always performs better than 
MaxEnt, and the performance of our approach de-
scends slowly when the training dataset becomes 
small. Specifically, the performance of CoFG us-
ing only 10% training data achieves better perfor-
mance than MaxEnt using 100% training data. 
 
                                                 
4 http://mallet.cs.umass.edu/ 
 ROUGE-2 ROUGE-L ROUGE-W ROUGE-SU4 
Random 0.0219 0.1363 0.0831 0.0288 
HITS 0.0295 0.1499 0.0905 0.0355 
PageRank 0.0307 0.1574 0.0944 0.0383 
MaxEnt 0.0349 0.1659 0.0995 0.0377 
CoFG 0.0383 0.1696 0.1015 0.0415 
Table 4: Performances of different approaches to profile summarization in terms of different measurements 
723
 
Figure 6:  The performance of CoFG with different 
training data size 
 
Table 5 shows the contribution of the social 
edges with CoFG. Specifically, CoFG is our pro-
posed approach with both education and experi-
ence information, CoFG-edu means that the CoFG 
model considers the social edges of education field 
(co_major, co_univ) only, and CoFG-exp means 
that the CoFG model considers the social edges of 
work experience field (co_title, co_corp) only. 
MaxEnt can be considered as using textual infor-
mation only. 
 
 ROUGE-2 
MaxEnt 0.0349 
CoFG 0.0383 
CoFG-edu 0.0382 
CoFG-exp 0.0381 
Table 5: ROUGE-2 F-Measure score of the contribu-
tion of social edges 
 
From Table 5, we can see that all of our pro-
posed approaches, i.e., CoFG-edu, CoFG-exp, and 
CoFG, outperform the baseline approach, i.e., 
MaxEnt. However, the performance of CoFG-edu, 
CoFG-exp and CoFG are similar. This result is 
mainly due to the fact that the information of so-
cial connection is redundant. For example, two 
persons who are connected by co_major (educa-
tion field) might also be connected by co_corp 
(experience field).  
7 Conclusion and Future Work 
In this paper, we present a novel task named pro-
file summarization and propose a novel approach 
called collective factor graph model to address this 
task. One distinguishing feature of the proposed 
approach lies in its incorporating the social con-
nection. Empirical studies demonstrate that the 
social connection is effective for profile summari-
zation, which enables our approach outperform 
some competitive supervised and unsupervised 
baselines. 
The main contribution of this paper is to explore 
social context information to help generate the 
summary of the profiles, which represents an in-
teresting research direction in social network min-
ing. In the future work, we will explore more kinds 
of social context information and investigate better 
ways of incorporating them into profile summari-
zation and a wider range of social network mining. 
 
Acknowledgments 
 
This research work is supported by the National 
Natural Science Foundation of China 
(No.61273320, No.61272257, No.61331011 and 
No.61375073), and National High-tech Research 
and Development Program of China 
(No.2012AA011102). 
We thank Dr. Jie Tang and Honglei Zhuang for 
providing their software and useful suggestions 
about PGM. We acknowledge Dr. Xinfang Liu, 
Yunxia Xue and Yulai Shen for corpus construc-
tion and insightful comments. We also thank 
anonymous reviewers for their valuable sugges-
tions and comments.  
References  
Baeza-Yates R. and B. Ribeiro-Neto. 1999. Modern 
Information Retrieval. ACM Press and Addison Wes-
ley, 1999 
Celikyilmaz A. and D. Hakkani-Tur. 2011. Discovery 
of Topically Coherent Sentences for Extractive 
Summarization. In Proceeding of ACL-11. 
Dong Y., J. Tang, S. Wu, J. Tian, N. Chawla, J. Rao, 
and H. Cao. 2012. Link Prediction and Recommen-
dation across Heterogeneous Social Networks. In 
Proceedings of ICDM-12. 
Elson D., N. Dames and K. McKeown. 2010. Extracting 
Social Networks from Literary Fiction. In Proceed-
ing of ACL-10. 
Erkan G. and D. Radev. 2004. LexPageRank: Prestige 
in Multi-document Text Summarization. In Proceed-
ings of EMNLP-04. 
Guy I., N. Zwerdling, I.  Ronen, D. Carmel, E. Uziel. 
2010. Social Media Recommendation based on Peo-
ple and Tags. In Proceeding of SIGIR-10. 
0.030
0.032
0.034
0.036
0.038
0.040
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
R
O
U
G
E
-2
 
size of training data 
PageRank MaxEnt CFG
724
Hammersley J. and P. Clifford. 1971. Markov Field on 
Finite Graphs and Lattices, Unpublished manuscript. 
1971. 
Hu P., C. Sun, L. Wu, D. Ji and C. Teng. 1011. Social 
Summarization via Automatically Discovered Social 
Context. In Proceeding of IJCNLP-11. 
Lafferty J, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of ICML-01. 
Lappas T., K. Punera and T. Sarlos. 2011. Mining Tags 
Using Social Endorsement Networks. In Proceeding 
of SIGIR-11. 
Leskovec J., D. Huttenlocher and J. Kleinberg. 2010. 
Predicting Positive and Negative Links in Online So-
cial Networks. In Proceedings of WWW-10. 
Lin, C. 2004. ROUGE: a Package for Automatic Evalu-
ation of Summaries. In Proceedings of ACL-04 
Workshop on Text Summarization Branches Out. 
Lu Y., P. Tsaparas, A. Ntoulas and L. Polanyi. 2010. 
Exploiting Social Context for Review Quality Pre-
diction. In Proceeding of WWW-10. 
Meng X?F. Wei? X. Liu? M. Zhou? S. Li and H. 
Wang. 2012. Entity-Centric Topic-Oriented Opinion 
Summarization in Twitter. In Proceeding of KDD-12.  
Murphy K., Y. Weiss, and M. Jordan. 1999. Loopy Be-
lief Propagation for Approximate Inference: An Em-
pirical Study. In Proceedings of UAI-99. 
Radev D. and K. McKeown. 1998. Generating Natural 
Language Summaries from Multiple On-line Sources. 
Computational Linguistics, 24(3):469?500. 
Radev D., H. Jing, M. Stys, and D. Tam. 2004. Cen-
troid-based Summarization of Multiple Documents. 
Information Processing and Management. 40 (2004), 
919-938. 
Rosenthal S. and K. McKeown. 2011. Age Prediction in 
Blogs: A Study of Style, Content, and OnlineBehav-
ior in Pre- and Post-Social Media Generations. In 
Proceeding of ACL-11. 
Ryang S. and T. Abekawa. 2012. Framework of Auto-
matic Text Summarization Using Reinforcement 
Learning. In Proceeding of EMNLP-2012. 
Shen D., J. Sun, H. Li, Q. Yang and Zheng Chen. 2007. 
Document Summarization using Conditional Ran-
dom Fields. In Proceeding of IJCAI-07. 
Tan C., L. Lee, J. Tang, L. Jiang, M. Zhou and P. Li. 
2011. User-Level Sentiment Analysis Incorporating 
Social Networks. In Proceedings of KDD-11. 
Tang W., H. Zhuang, and J. Tang. 2011a. Learning to 
Infer Social Ties in Large Networks. In Proceedings 
of ECML/PKDD-11. 
Tang J., Y. Zhang, J. Sun, J. Rao, W. Yu, Y. Chen, and 
A. Fong. 2011b. Quantitative Study of Individual 
Emotional States in Social Networks. IEEE Transac-
tions on Affective Computing. vol.3(2), Pages 132-
144. 
Wan X. and J. Yang. 2008. Multi-document Summari-
zation using Cluster-based Link Analysis. In Pro-
ceedings of SIGIR-08. 
Wan X. 2011. Using Bilingual Information for Cross-
Language Document Summarization. In Proceedings 
of ACL-11. 
Wang H. and G. Zhou. 2012. Toward a Unified Frame-
work for Standard and Update Multi-Document 
Summarization. ACM Transactions on Asian Lan-
guage Information Processing. vol.11(2). 
Xing E, M. Jordan, and S. Russell. 2003. A Generalized 
Mean Field Algorithm for Variational Inference in 
Exponential Families. In Proceedings of UAI-03. 
Yang S., B. Long, A. Smola, N. Sadagopan, Z. Zheng 
and H. Zha. 2011a. Like like alike ? Joint Friend-
ship and Interest Propagation in Social Networks. In 
Proceeding of WWW-11. 
Yang Z., K. Cai, J. Tang, L. Zhang, Z. Su and J. Li. 
2011b. Social Context Summarization. In Proceed-
ing of SIGIR-11. 
Zhuang H, J. Tang, W. Tang, T. Lou, A. Chin, and X. 
Wang. 2012. Actively Learning to Infer Social Ties. 
In Proceedings of Data Mining and Knowledge Dis-
covery (DMKD-12), vol.25 (2), pages 270-297. 
725
