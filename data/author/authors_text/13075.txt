Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 952?961, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Exploring Topic Coherence over many models and many topics
Keith Stevens1,2 Philip Kegelmeyer3 David Andrzejewski2 David Buttler2
1University of California Los Angeles; Los Angeles , California, USA
2Lawrence Livermore National Lab; Livermore, California, USA
3Sandia National Lab; Livermore, California, USA
{stevens35,andrzejewski1,buttler1}@llnl.gov
wpk@sandia.gov
Abstract
We apply two new automated semantic eval-
uations to three distinct latent topic models.
Both metrics have been shown to align with
human evaluations and provide a balance be-
tween internal measures of information gain
and comparisons to human ratings of coher-
ent topics. We improve upon the measures
by introducing new aggregate measures that
allows for comparing complete topic models.
We further compare the automated measures
to other metrics for topic models, compar-
ison to manually crafted semantic tests and
document classification. Our experiments re-
veal that LDA and LSA each have different
strengths; LDA best learns descriptive topics
while LSA is best at creating a compact se-
mantic representation of documents and words
in a corpus.
1 Introduction
Topic models learn bags of related words from large
corpora without any supervision. Based on the
words used within a document, they mine topic level
relations by assuming that a single document cov-
ers a small set of concise topics. Once learned,
these topics often correlate well with human con-
cepts. For example, one model might produce topics
that cover ideas such as government affairs, sports,
and movies. With these unsupervised methods, we
can extract useful semantic information in a variety
of tasks that depend on identifying unique topics or
concepts, such as distributional semantics (Jurgens
and Stevens, 2010), word sense induction (Van de
Cruys and Apidianaki, 2011; Brody and Lapata,
2009), and information retrieval (Andrzejewski and
Buttler, 2011).
When using a topic model, we are primarily con-
cerned with the degree to which the learned top-
ics match human judgments and help us differen-
tiate between ideas. But until recently, the evalua-
tion of these models has been ad hoc and applica-
tion specific. Evaluations have ranged from fully
automated intrinsic evaluations to manually crafted
extrinsic evaluations. Previous extrinsic evaluations
have used the learned topics to compactly represent
a small fixed vocabulary and compared this distribu-
tional space to human judgments of similarity (Jur-
gens and Stevens, 2010). But these evaluations are
hand constructed and often costly to perform for
domain-specific topics. Conversely, intrinsic mea-
sures have evaluated the amount of information en-
coded by the topics, where perplexity is one com-
mon example(Wallach et al 2009), however, Chang
et al(2009) found that these intrinsic measures do
not always correlate with semantically interpretable
topics. Furthermore, few evaluations have used the
same metrics to compare distinct approaches such
as Latent Dirichlet Allocation (LDA) (Blei et al
2003), Latent Semantic Analysis (LSA) (Landauer
and Dutnais, 1997), and Non-negative Matrix Fac-
torization (NMF) (Lee and Seung, 2000). This has
made it difficult to know which method is most use-
ful for a given application, or in terms of extracting
useful topics.
We now provide a comprehensive and automated
evaluation of these three distinct models (LDA,
LSA, NMF), for automatically learning semantic
topics. While these models have seen significant im-
provements, they still represent the core differences
between each approach to modeling topics. For our
evaluation, we use two recent automated coherence
measures (Mimno et al 2011; Newman et al 2010)
952
originally designed for LDA that bridge the gap be-
tween comparisons to human judgments and intrin-
sic measures such as perplexity. We consider several
key questions:
1. How many topics should be learned?
2. How many learned topics are useful?
3. How do these topics relate to often used semantic tests?
4. How well do these topics identify similar documents?
We begin by summarizing the three topic mod-
els and highlighting their key differences. We then
describe the two metrics. Afterwards, we focus on
a series of experiments that address our four key
questions and finally conclude with some overall re-
marks.
2 Topic Models
We evaluate three latent factor models that have seen
widespread usage:
1. Latent Dirichlet Allocation
2. Latent Semantic Analysis with Singular Value De-
composition
3. Latent Semantic Analysis with Non-negative Ma-
trix Factorization
Each of these models were designed with differ-
ent goals and are supported by different statistical
theories. We consider both LSA models as topic
models as they have been used in a variety of sim-
ilar contexts such as distributional similarity (Jur-
gens and Stevens, 2010) and word sense induction
(Van de Cruys and Apidianaki, 2011; Brody and
Lapata, 2009). We evaluate these distinct models
on two shared tasks (1) grouping together similar
words while separating unrelated words and (2) dis-
tinguishing between documents focusing on differ-
ent concepts.
We distill the different models into a shared repre-
sentation consisting of two sets of learned relations:
how words interact with topics and how topics inter-
act with documents. For a corpus withD documents
and V words, we denote these relations in terms of
T topics as
(1) a V ? T matrix, W , that indicates the strength
each word has in each topic, and
(2) a T ? D matrix, H , that indicates the strength
each topic has in each document.
T serves as a common parameter to each model.
2.1 Latent Dirichlet Allocation
Latent Dirichlet Allocation (Blei et al 2003) learns
the relationships between words, topics, and docu-
ments by assuming documents are generated by a
particular probabilistic model. It first assumes that
there are a fixed set of topics, T used throughout the
corpus, and each topic z is associated with a multi-
nomial distribution over the vocabulary?z , which is
drawn from a Dirichlet prior Dir(?). A given docu-
ment Di is then generated by the following process
1. Choose ?i ? Dir(?), a topic distribution for Di
2. For each word wj ? Di:
(a) Select a topic zj ? ?i
(b) Select the word wj ? ?zj
In this model, the ? distributions represent the
probability of each topic appearing in each docu-
ment and the ? distributions represent the proba-
bility of words being used for each topic. These
two sets of distributions correspond to our H and W
matrices, respectively. The process above defines a
generative model; given the observed corpus, we use
collapsed Gibbs sampling implementation found in
Mallet1 to infer the values of the latent variables ?
and ? (Griffiths and Steyvers, 2004). The model re-
lies only on two additional hyper parameters, ? and
?, that guide the distributions.
2.2 Latent Semantic Analysis
Latent Semantic Analysis (Landauer and Dutnais,
1997; Landauer et al 1998) learns topics by first
forming a traditional term by document matrix used
in information retrieval and then smoothing the
counts to enhance the weight of informative words.
Based on the original LSA model, we use the Log-
Entropy transform. LSA then decomposes this
smoothed, term by document matrix in order to gen-
eralize observed relations between words and docu-
ments. For both LSA models, we used implementa-
tions found in the S-Space package.2
Traditionally, LSA has used the Singular Value
Decomposition, but we also consider Non-negative
Matrix Factorization as we?ve seen NMF applied
in similar situations (Pauca et al 2004) and others
1http://mallet.cs.umass.edu/
2https://github.com/fozziethebeat/S-Space
953
Model Label Top Words UMass UCI
High Quality Topics
LDA interview told asked wanted interview people made thought time called knew -2.52 1.29wine wine wines bottle grapes made winery cabernet grape pinot red -1.97 1.30
NMF grilling grilled sweet spicy fried pork dish shrimp menu dishes sauce -1.01 1.98cloning embryonic cloned embryo human research stem embryos cell cloning cells -1.84 1.46
SVD cooking sauce food restaurant water oil salt chicken pepper wine cup -1.87 -1.21stocks fund funds investors weapons stocks mutual stock movie film show -2.30 -1.88
Low Quality Topics
LDA rates 10-yr rate 3-month percent 6-month bds bd 30-yr funds robot -1.94 -12.32charity fund contributions .com family apartment charities rent 22d children assistance -2.43 -8.88
NMF plants stem fruitful stems trunk fruiting currants branches fence currant espalier -3.12 -12.59farming buzzards groundhog prune hoof pruned pruning vines wheelbarrow tree clematis -1.90 -12.56
SVD city building city area buildings p.m. floors house listed eat-in a.m. -2.70 -8.03time p.m. system study a.m. office political found school night yesterday -1.67 -7.02
Table 1: Top 10 words from several high and low quality topics when ordered by the UCI Coherence
Measure. Topic labels were chosen in an ad hoc manner only to briefly summarize the topic?s focus.
have found a connection between NMF and Proba-
bilistic Latent Semantic Analysis (Ding et al 2008),
an extension to LSA.We later refer to these two LSA
models simply as SVD and NMF to emphasize the
difference in factorization method.
Singular Value Decomposition decomposes M
into three smaller matrices
M = U?V T
and minimizes Frobenius norm of M ?s reconstruc-
tion error with the constraint that the rows of U and
V are orthonormal eigenvectors. Interestingly, the
decomposition is agnostic to the number of desired
dimensions. Instead, the rows and columns in U and
V T are ordered based on their descriptive power, i.e.
how well they remove noise, which is encoded by
the diagonal singular value matrix ?. As such, re-
duction is done by retaining the first T rows and
columns from U and V T . For our generalization,
we use W = U? and H = ?V T . We note that
values in U and V T can be both negative and pos-
itive, preventing a straightforward interpretation as
unnormalized probabilities
Non-negative Matrix Factorization also factor-
izes M by minimizing the reconstruction error, but
with only one constraint: the decomposed matrices
consist of only non-negative values. In this respect,
we can consider it to be learning an unnormalized
probability distributions over topics. We use the
original Euclidean least squares definition of NMF3.
Formally, NMF is defined as
M = WH
where H and W map directly onto our generaliza-
tion. As in the original NMF work, we learn these
unnormalized probabilities by initializing each set of
probabilities at random and update them according
to the following iterative update rules
W = W MHTWHHT H = H
WTM
WTWH
3 Coherence Measures
Topic Coherence measures score a single topic by
measuring the degree of semantic similarity between
high scoring words in the topic. These measure-
ments help distinguish between topics that are se-
mantically interpretable topics and topics that are ar-
tifacts of statistical inference, see Table 1 for exam-
ples ordered by the UCI measure. For our evalua-
tions, we consider two new coherence measures de-
signed for LDA, both of which have been shown to
match well with human judgements of topic quality:
(1) The UCI measure (Newman et al 2010) and (2)
The UMass measure (Mimno et al 2011).
Both measures compute the coherence of a topic
as the sum of pairwise distributional similarity
3We note that the alternative KL-Divergence form of NMF
has been directly linked to PLSA (Ding et al 2008)
954
scores over the set of topic words, V . We generalize
this as
coherence(V ) =
?
(vi,vj)?V
score(vi, vj , )
where V is a set of word describing the topic and 
indicates a smoothing factor which guarantees that
score returns real numbers. (We will be exploring
the effect of the choice of ; the original authors used
 = 1.)
The UCI metric defines a word pair?s score to
be the pointwise mutual information (PMI) between
two words, i.e.
score(vi, vj , ) = log
p(vi, vj) + 
p(vi)p(vj)
The word probabilities are computed by counting
word co-occurrence frequencies in a sliding window
over an external corpus, such as Wikipedia. To some
degree, this metric can be thought of as an external
comparison to known semantic evaluations.
The UMass metric defines the score to be based
on document co-occurrence:
score(vi, vj , ) = log
D(vi, vj) + 
D(vj)
whereD(x, y) counts the number of documents con-
taining words x and y and D(x) counts the num-
ber of documents containing x. Significantly, the
UMass metric computes these counts over the orig-
inal corpus used to train the topic models, rather
than an external corpus. This metric is more intrin-
sic in nature. It attempts to confirm that the models
learned data known to be in the corpus.
4 Evaluation
We evaluate the quality of our three topic models
(LDA, SVD, and NMF) with three experiments. We
focus first on evaluating aggregate coherence meth-
ods for a complete topic model and consider the
differences between each model as we learn an in-
creasing number of topics. Secondly, we compare
coherence scores to previous semantic evaluations.
Lastly, we use the learned topics in a classifica-
tion task and evaluate whether or not coherent top-
ics are equally informative when discriminating be-
tween documents.
For all our experiments, we trained our models on
92,600 New York Times articles from 2003 (Sand-
haus, 2008). For all articles, we removed stop words
and any words occurring less than 200 times in the
corpus, which left 35,836 unique tokens. All doc-
uments were tokenized with OpenNLP?s MaxEnt4
tokenizer. For the UCI measure, we compute the
PMI between words using a 20 word sliding win-
dow passed over the WaCkypedia corpus (Baroni et
al., 2009). In all experiments, we compute the co-
herence with the top 10 words from each topic that
had the highest weight, in terms of LDA and NMF
this corresponds with a high probability of the term
describing the topic but for SVD there is no clear
semantic interpretation.
4.1 Aggregate methods for topic coherence
Before we can compare topic models, we require an
aggregate measure that represents the quality of a
complete model, rather than individual topics. We
consider two aggregates methods: (1) the average
coherence of all topics and (2) the entropy of the co-
herence for all topics. The average coherence pro-
vides a quick summarization of a model?s quality
whereas the entropy provides an alternate summa-
rization that differentiates between two interesting
situations. Since entropy measures the complexity
of a probability distribution, it can easily differenti-
ate between uniform distributions and multimodal,
distributions. This distinction is relevant when users
prefer to have roughly uniform topic quality instead
of a wide gap between high- and low-quality topics,
or vice versa. We compute the entropy by dropping
the log and  factor from each scoring function.
Figure 1 shows the average coherence scores for
each model as we vary the number of topics. These
average scores indicate some simple relationships
between the models: LDA and NMF have approx-
imately the same performance and both models are
consistently better than SVD. All of the models
quickly reach a stable average score at around 100
topics. This initially suggests that learning more
4http://incubator.apache.org/opennlp/
955
Number of topics
Avera
ge To
pic Co
heren
ce
?5
?4
?3
?2
?1
0
100 200 300 400 500
(a) UMass
Number of topics
Avera
ge To
pic Co
heren
ce
?10
?8
?6
?4
?2
0
2
100 200 300 400 500
MethodLDANMFSVD
(b) UCI
Figure 1: Average Topic Coherence for each model
Number of topics
Cohe
rence
 Entro
py
0
1
2
3
4
5
6
7
100 200 300 400 500
(a) UMass
Number of topics
Cohe
rence
 Entro
py
0
1
2
3
4
5
6
7
100 200 300 400 500
MethodLDANMFSVD
(b) UCI
Figure 2: Entropy of the Topic Coherence for each model
topics neither increases or decreases the quality of
the model, but Figure 2 indicates otherwise. While
the entropy for the UMass score stays stable for all
models, NMF produces erratic entropy results under
the UCI score as we learn more topics. As entropy is
higher for even distributions and lower for all other
distributions, these results suggest that the NMF is
learning topics with drastically different levels of
quality, i.e. some with high quality and some with
very low quality, but the average coherence over all
topics do not account for this.
Low quality topics may be composed of highly
unrelated words that can?t be fit into another topic,
and in this case, our smoothing factor, , may be ar-
tificially increasing the score for unrelated words.
Following the practice of the original use of these
metrics, in Figures 1 and 2 we set  = 1. In Fig-
ure 3, we consider  = 10?12, which should sig-
nificantly reduce the score for completely unrelated
words. Here, we see a significant change in the per-
formance of NMF, the average coherence decreases
dramatically as we learn more topics. Similarly, per-
formance of SVD drops dramatically and well below
the other models. In figure 4 we lastly compute the
average coherence using only the top 10% most co-
herence topics with  = 10?12. Here, NMF again
performs on par with LDA. With the top 10% topics
still having a high average coherence but the full set
956
Number of topics
Avera
ge To
pic Co
heren
ce
?5
?4
?3
?2
?1
0
100 200 300 400 500
(a) UMass
Number of topics
Avera
ge To
pic Co
heren
ce
?10
?8
?6
?4
?2
0
2
100 200 300 400 500
MethodLDANMFSVD
(b) UCI
Figure 3: Average Topic Coherence with  = 10?12
Number of topics
Avera
ge Co
heren
ce of 
top 10
%
?5
?4
?3
?2
?1
0
100 200 300 400 500
(a) UMass
Number of topics
Avera
ge Co
heren
ce of 
top 10
%
?10
?8
?6
?4
?2
0
2
100 200 300 400 500
MethodLDANMFSVD
(b) UCI
Figure 4: Average Topic Coherence of the top 10% topics with  = 10?12
of topics having a low coherence, NMF appears to
be learning more low quality topics once it?s learned
the first 100 topics, whereas LDA learns fewer low
quality topics in general.
4.2 Word Similarity Tasks
The initial evaluations for each coherence mea-
sure asked human judges to directly evaluate top-
ics (Newman et al 2010; Mimno et al 2011). We
expand upon this comparison to human judgments
by considering word similarity tasks that have of-
ten been used to evaluate distributional semantic
spaces (Jurgens and Stevens, 2010). Here, we use
the learned topics as generalized semantics describ-
ing our knowledge about words. If a model?s topics
generalize the knowledge accurately, we would ex-
pect similar words, such as ?cat? and ?dog?, to be
represented with a similar set of topics. Rather than
evaluating individual topics, this similarity task con-
siders the knowledge within the entire set of topics,
the topics act as more compact representation for the
known words in a corpus.
We use the Rubenstein and Goodenough (1965)
and Finkelstein et al(2002) word similarity tasks.
In each task, human judges were asked to evaluate
the similarity or relatedness between different sets of
word pairs. Fifty-One Evaluators for the Rubenstein
and Goodenough (1965) dataset were given 65 pairs
957
Tscor
e
0.0
0.1
0.2
0.3
0.4
0.5
0.6
100 200 300 400 500
modelLDANMFSVD
(a) Rubenstein & Goodenough
T
scor
e
0.0
0.1
0.2
0.3
0.4
0.5
100 200 300 400 500
modelLDANMFSVD
(b) Wordsim 353/Finklestein et. al.
Figure 5: Word Similarity Evaluations for each model
Topics
Corre
lation
0.0
0.2
0.4
0.6
100 200 300 400 500
(a) UMass
Topics
Corre
lation
0.0
0.2
0.4
0.6
100 200 300 400 500
modelLDANMFSVD
(b) UCI
Figure 7: Correlation between topic coherence and topic ranking in classification
of words and asked to rate their similarity on a scale
from 0 to 4, where a higher score indicates a more
similar word pair. Finkelstein et al(2002) broadens
the word similarity evaluation and asked 13 to 16
different subjects to rate 353 word pairs on a scale
from 0 to 10 based on their relatedness, where relat-
edness includes similarity and other semantic rela-
tions. We can evaluate each topic model by comput-
ing the cosine similarity between each pair of words
in the evaluate set and then compare the model?s
ratings to the human ratings by ranked correlation.
A high correlation signifies that the topics closely
model human judgments.
Figure 5 displays the results. SVD and LDA
both surpass NMF on the Rubenstein & Goode-
nough test while SVD is clearly the best model on
the Finklestein et. al test. While our first experi-
ment showed that SVDwas the worst model in terms
of topic coherence scores, this experiment indicates
that SVD provides an accurate, stable, and reliable
approximation to human judgements of similarity
and relatedness between word pairs in comparison
to other topic models.
4.3 Coherence versus Classification
For our final experiment, we examine the relation-
ship between topic coherence and classification ac-
curacy for each topic model. We suspect that highly
958
score
Corre
lation
0.01
0.02
0.03
0.04
0.05
0.06
0.07
?25 ?20 ?15 ?10 ?5
(a) UMass
score
Corre
lation
0.01
0.02
0.03
0.04
0.05
0.06
0.07
?30 ?20 ?10 0
modelLDANMFSVD
(b) UCI
Figure 8: Comparison between topic coherence and topic rank with 500 topics
Topics
Accu
racy
20
30
40
50
60
70
80
100 200 300 400 500
ModelLDANMFSVD
Figure 6: Classification accuracy for each model
coherent topics, and coherent topic models, will per-
form better for classification. We address this ques-
tion by performing a document classification task
using the topic representations of documents as in-
put features and examine the relationship between
topic coherence and the usefulness of the corre-
sponding feature for classification.
We trained each topic model with all 92,600 New
York Times articles as before. We use the sec-
tion labels provided for each article as class labels,
where each label indicates the on-line section(s) un-
der which the article was published and should thus
be related to the topics contained in each article. To
reduce the noise in our data set we narrow down the
articles to those that have only one label and whose
label is applied to at least 2000 documents. This re-
sults in 57,696 articles with label distributions listed
in Table 2. We then represent each document using
columns in the topic by document matrix H learned
for each topic model.
Label Count Label Count
New York and Region 11219 U.S. 3675
Paid Death Notices 11152 Arts 3437
Opinion 8038 World 3330
Business 7494 Style 2137
Sports 7214
Table 2: Section label counts for New York Times
articles used for classification
For each topic model trained on N topics, we
performed stratified 10-fold cross-validation on the
57,696 labeled articles. In each fold, we build an
automatically-sized bagged ensemble of unpruned
CART-style decision trees(Banfield et al 2007) on
90% of the dataset5, use that ensemble to assign la-
bels to the other 10%, and measure the accuracy of
that assignment. Figure 6 shows the average classifi-
cation accuracy over all ten folds for each model. In-
terestingly, SVD has slightly, but statistically signif-
icantly, higher accuracy results than both NMF and
LDA. Furthermore, performance quickly increases
5The precise choice of the classifier scheme matters little, as
long as it is accurate, speedy, and robust to label noise; all of
which is true of the choice here.
959
and plateaus with well under 50 topics.
Our bagged decision trees can also determine the
importance of each feature during classification. We
evaluate the strength of each topic during classifi-
cation by tracking the number of times each node
in our decision trees observe each topic, please see
(Caruana et al 2006) for more details. Figure 8 plot
the relationship between this feature ranking and the
topic coherence for each topic when training LDA,
SVD, and NMF on 500 topics. Most topics for each
model provide little classification information, but
SVD shows a much higher rank for several topics
with a relatively higher coherence score. Interest-
ingly, for all models, the most coherent topics are not
the most informative. Figure 7 plots a more compact
view of this same relationship: the Spearman rank
correlation between classification feature rank and
topic coherence. NMF shows the highest correlation
between rank and coherence, but none of the mod-
els show a high correlation when using more than
100 topics. SVD has the lowest correlation, which
is probably due to the model?s overall low coherence
yet high classification accuracy.
5 Discussion and Conclusion
Through our experiments, we made several excit-
ing and interesting discoveries. First, we discov-
ered that the coherence metrics depend heavily on
the smoothing factor . The original value, 1.0 cre-
ated a positive bias towards NMF from both met-
rics even when NMF generated incoherent topics.
The high smoothing factor also gave a significant in-
crease to SVD scores. We suspect that this was not
an issue in previous studies with the coherence mea-
sures as LDA prefers to form topics from words that
co-occur frequently, whereas NMF and SVD have
no such preferences and often create low quality top-
ics from completely unrelated words. Therefore, we
suggest a smaller  value in general.
We also found that the UCI measure often agreed
with the UMass measure, but the UCI-entropy ag-
gregate method induced more separation between
LSA, SVD, and NMF in terms of topic coherence.
This measure also revealed the importance of the
smoothing factor for topic coherence measures.
With respects to human judgements, we found
that coherence scores do not always indicate a bet-
ter representation of distributional information. The
SVD model consistently out performed both LDA
and NMF models, which each had higher coherence
scores, when attempting to predict human judge-
ments of similarity.
Lastly, we found all models capable of producing
topics that improved document classification. At the
same time, SVD provided the most information dur-
ing classification and outperformed the other mod-
els, which again had more coherent topics. Our com-
parison between topic coherence scores and feature
importance in classification revealed that relatively
high quality topics, but not the most coherent topics,
drive most of the classification decisions, and most
topics do not affect the accuracy.
Overall, we see that each topic model paradigm
has it?s own strengths and weaknesses. Latent Se-
mantic Analysis with Singular Value Decomposition
fails to form individual topics that aggregate similar
words, but it does remarkably well when consider-
ing all the learned topics as similar words develop
a similar topic representation. These topics simi-
larly perform well during classification. Conversely,
both Non Negative Matrix factorization and Latent
Dirichlet Allocation learn concise and coherent top-
ics and achieved similar performance on our evalua-
tions. However, NMF learns more incoherent topics
than LDA and SVD. For applications in which a hu-
man end-user will interact with learned topics, the
flexibility of LDA and the coherence advantages of
LDA warrant strong consideration. All of code for
this work will be made available through an open
source project.6
6 Acknowledgments
This work was performed under the auspices of
the U.S. Department of Energy by Lawrence Liv-
ermore National Laboratory under Contract DE-
AC52-07NA27344 (LLNL-CONF-522871) and by
Sandia National Laboratory under Contract DE-
AC04-94AL85000.
References
David Andrzejewski and David Buttler. 2011. Latent
topic feedback for information retrieval. In Proceed-
6https://github.com/fozziethebeat/TopicModelComparison
960
ings of the 17th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, KDD
?11, pages 600?608, New York, NY, USA. ACM.
Robert E. Banfield, Lawrence O. Hall, Kevin W. Bowyer,
andW. Philip Kegelmeyer. 2007. A comparison of de-
cision tree ensemble creation techniques. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
29(1):173?180, January.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alcation. J. Mach. Learn.
Res., 3:993?1022, March.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ?09, pages 103?
111, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Rich Caruana, Mohamed Elhawary, Art Munson, Mirek
Riedewald, Daria Sorokina, Daniel Fink, Wesley M.
Hochachka, and Steve Kelling. 2006. Mining cit-
izen science data to predict orevalence of wild bird
species. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ?06, pages 909?915, New York,
NY, USA. ACM.
Jonathan Chang, Sean Gerrish, Chong Wang, and
David M Blei. 2009. Reading tea leaves : How hu-
mans interpret topic models. New York, 31:1?9.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Comput. Stat.
Data Anal., 52:3913?3927, April.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: the concept
revisited. ACM Trans. Inf. Syst., 20:116?131, January.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228?5235, April.
David Jurgens and Keith Stevens. 2010. The s-space
package: an open source package for word space mod-
els. In Proceedings of the ACL 2010 System Demon-
strations, ACLDemos ?10, pages 30?35, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Thomas K Landauer and Susan T. Dutnais. 1997. A so-
lution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, pages 211?240.
Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.
1998. An Introduction to Latent Semantic Analysis.
Discourse Processes, (25):259?284.
Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In In
NIPS, pages 556?562. MIT Press.
David Mimno, Hanna Wallach, Edmund Talley, Miriam
Leenders, and Andrew McCallum. 2011. Optimizing
semantic coherence in topic models. In Proceedings of
the 2011 Conference on Emperical Methods in Natu-
ral Language Processing, pages 262?272, Edinburgh,
Scotland, UK. Association of Computational Linguis-
tics.
David Newman, Youn Noh, Edmund Talley, Sarvnaz
Karimi, and Timothy Baldwin. 2010. Evaluating topic
models for digital libraries. In Proceedings of the 10th
annual joint conference on Digital libraries, JCDL
?10, pages 215?224, New York, NY, USA. ACM.
V Paul Pauca, Farial Shahnaz, Michael W Berry, and
Robert J Plemmons, 2004. Text mining using nonnega-
tive matrix factorizations, volume 54, pages 452?456.
SIAM.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8:627?633, October.
Evan Sandhaus. 2008. The New York Times Annotated
Corpus.
Tim Van de Cruys and Marianna Apidianaki. 2011. La-
tent semantic word sense induction and disambigua-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 1476?1485, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In Proceedings of the 26th International Con-
ference on Machine Learning (ICML). Omnipress.
961
Proceedings of the ACL 2010 Conference Short Papers, pages 156?161,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Coreference Resolution with Reconcile
Veselin Stoyanov
Center for Language
and Speech Processing
Johns Hopkins Univ.
Baltimore, MD
ves@cs.jhu.edu
Claire Cardie
Department of
Computer Science
Cornell University
Ithaca, NY
cardie@cs.cornell.edu
Nathan Gilbert
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT
ngilbert@cs.utah.edu
riloff@cs.utah.edu
David Buttler
David Hysom
Lawrence Livermore
National Laboratory
Livermore, CA
buttler1@llnl.gov
hysom1@llnl.gov
Abstract
Despite the existence of several noun phrase coref-
erence resolution data sets as well as several for-
mal evaluations on the task, it remains frustratingly
difficult to compare results across different corefer-
ence resolution systems. This is due to the high cost
of implementing a complete end-to-end coreference
resolution system, which often forces researchers
to substitute available gold-standard information in
lieu of implementing a module that would compute
that information. Unfortunately, this leads to incon-
sistent and often unrealistic evaluation scenarios.
With the aim to facilitate consistent and realis-
tic experimental evaluations in coreference resolu-
tion, we present Reconcile, an infrastructure for the
development of learning-based noun phrase (NP)
coreference resolution systems. Reconcile is de-
signed to facilitate the rapid creation of corefer-
ence resolution systems, easy implementation of
new feature sets and approaches to coreference res-
olution, and empirical evaluation of coreference re-
solvers across a variety of benchmark data sets and
standard scoring metrics. We describe Reconcile
and present experimental results showing that Rec-
oncile can be used to create a coreference resolver
that achieves performance comparable to state-of-
the-art systems on six benchmark data sets.
1 Introduction
Noun phrase coreference resolution (or simply
coreference resolution) is the problem of identi-
fying all noun phrases (NPs) that refer to the same
entity in a text. The problem of coreference res-
olution is fundamental in the field of natural lan-
guage processing (NLP) because of its usefulness
for other NLP tasks, as well as the theoretical in-
terest in understanding the computational mech-
anisms involved in government, binding and lin-
guistic reference.
Several formal evaluations have been conducted
for the coreference resolution task (e.g., MUC-6
(1995), ACE NIST (2004)), and the data sets cre-
ated for these evaluations have become standard
benchmarks in the field (e.g., MUC and ACE data
sets). However, it is still frustratingly difficult to
compare results across different coreference res-
olution systems. Reported coreference resolu-
tion scores vary wildly across data sets, evaluation
metrics, and system configurations.
We believe that one root cause of these dispar-
ities is the high cost of implementing an end-to-
end coreference resolution system. Coreference
resolution is a complex problem, and successful
systems must tackle a variety of non-trivial sub-
problems that are central to the coreference task ?
e.g., mention/markable detection, anaphor identi-
fication ? and that require substantial implemen-
tation efforts. As a result, many researchers ex-
ploit gold-standard annotations, when available, as
a substitute for component technologies to solve
these subproblems. For example, many published
research results use gold standard annotations to
identify NPs (substituting for mention/markable
detection), to distinguish anaphoric NPs from non-
anaphoric NPs (substituting for anaphoricity de-
termination), to identify named entities (substitut-
ing for named entity recognition), and to identify
the semantic types of NPs (substituting for seman-
tic class identification). Unfortunately, the use of
gold standard annotations for key/critical compo-
nent technologies leads to an unrealistic evalua-
tion setting, and makes it impossible to directly
compare results against coreference resolvers that
solve all of these subproblems from scratch.
Comparison of coreference resolvers is further
hindered by the use of several competing (and
non-trivial) evaluation measures, and data sets that
have substantially different task definitions and
annotation formats. Additionally, coreference res-
olution is a pervasive problem in NLP and many
NLP applications could benefit from an effective
coreference resolver that can be easily configured
and customized.
To address these issues, we have created a plat-
form for coreference resolution, called Reconcile,
that can serve as a software infrastructure to sup-
port the creation of, experimentation with, and
evaluation of coreference resolvers. Reconcile
was designed with the following seven desiderata
in mind:
? implement the basic underlying software ar-
156
chitecture of contemporary state-of-the-art
learning-based coreference resolution sys-
tems;
? support experimentation on most of the stan-
dard coreference resolution data sets;
? implement most popular coreference resolu-
tion scoring metrics;
? exhibit state-of-the-art coreference resolution
performance (i.e., it can be configured to cre-
ate a resolver that achieves performance close
to the best reported results);
? can be easily extended with new methods and
features;
? is relatively fast and easy to configure and
run;
? has a set of pre-built resolvers that can be
used as black-box coreference resolution sys-
tems.
While several other coreference resolution sys-
tems are publicly available (e.g., Poesio and
Kabadjov (2004), Qiu et al (2004) and Versley et
al. (2008)), none meets all seven of these desider-
ata (see Related Work). Reconcile is a modular
software platform that abstracts the basic archi-
tecture of most contemporary supervised learning-
based coreference resolution systems (e.g., Soon
et al (2001), Ng and Cardie (2002), Bengtson and
Roth (2008)) and achieves performance compara-
ble to the state-of-the-art on several benchmark
data sets. Additionally, Reconcile can be eas-
ily reconfigured to use different algorithms, fea-
tures, preprocessing elements, evaluation settings
and metrics.
In the rest of this paper, we review related work
(Section 2), describe Reconcile?s organization and
components (Section 3) and show experimental re-
sults for Reconcile on six data sets and two evalu-
ation metrics (Section 4).
2 Related Work
Several coreference resolution systems are cur-
rently publicly available. JavaRap (Qiu et al,
2004) is an implementation of the Lappin and
Leass? (1994) Resolution of Anaphora Procedure
(RAP). JavaRap resolves only pronouns and, thus,
it is not directly comparable to Reconcile. GuiTaR
(Poesio and Kabadjov, 2004) and BART (Versley
et al, 2008) (which can be considered a succes-
sor of GuiTaR) are both modular systems that tar-
get the full coreference resolution task. As such,
both systems come close to meeting the majority
of the desiderata set forth in Section 1. BART,
in particular, can be considered an alternative to
Reconcile, although we believe that Reconcile?s
approach is more flexible than BART?s. In addi-
tion, the architecture and system components of
Reconcile (including a comprehensive set of fea-
tures that draw on the expertise of state-of-the-art
supervised learning approaches, such as Bengtson
and Roth (2008)) result in performance closer to
the state-of-the-art.
Coreference resolution has received much re-
search attention, resulting in an array of ap-
proaches, algorithms and features. Reconcile
is modeled after typical supervised learning ap-
proaches to coreference resolution (e.g. the archi-
tecture introduced by Soon et al (2001)) because
of the popularity and relatively good performance
of these systems.
However, there have been other approaches
to coreference resolution, including unsupervised
and semi-supervised approaches (e.g. Haghighi
and Klein (2007)), structured approaches (e.g.
McCallum and Wellner (2004) and Finley and
Joachims (2005)), competition approaches (e.g.
Yang et al (2003)) and a bell-tree search approach
(Luo et al (2004)). Most of these approaches rely
on some notion of pairwise feature-based similar-
ity and can be directly implemented in Reconcile.
3 System Description
Reconcile was designed to be a research testbed
capable of implementing most current approaches
to coreference resolution. Reconcile is written in
Java, to be portable across platforms, and was de-
signed to be easily reconfigurable with respect to
subcomponents, feature sets, parameter settings,
etc.
Reconcile?s architecture is illustrated in Figure
1. For simplicity, Figure 1 shows Reconcile?s op-
eration during the classification phase (i.e., assum-
ing that a trained classifier is present).
The basic architecture of the system includes
five major steps. Starting with a corpus of docu-
ments together with a manually annotated corefer-
ence resolution answer key1, Reconcile performs
1Only required during training.
157
Figure 1: The Reconcile classification architecture.
the following steps, in order:
1. Preprocessing. All documents are passed
through a series of (external) linguistic pro-
cessors such as tokenizers, part-of-speech
taggers, syntactic parsers, etc. These com-
ponents produce annotations of the text. Ta-
ble 1 lists the preprocessors currently inter-
faced in Reconcile. Note that Reconcile in-
cludes several in-house NP detectors, that
conform to the different data sets? defini-
tions of what constitutes a NP (e.g., MUC
vs. ACE). All of the extractors utilize a syn-
tactic parse of the text and the output of a
Named Entity (NE) extractor, but extract dif-
ferent constructs as specialized in the corre-
sponding definition. The NP extractors suc-
cessfully recognize about 95% of the NPs in
the MUC and ACE gold standards.
2. Feature generation. Using annotations pro-
duced during preprocessing, Reconcile pro-
duces feature vectors for pairs of NPs. For
example, a feature might denote whether the
two NPs agree in number, or whether they
have any words in common. Reconcile in-
cludes over 80 features, inspired by other suc-
cessful coreference resolution systems such
as Soon et al (2001) and Ng and Cardie
(2002).
3. Classification. Reconcile learns a classifier
that operates on feature vectors representing
Task Systems
Sentence UIUC (CC Group, 2009)
splitter OpenNLP (Baldridge, J., 2005)
Tokenizer OpenNLP (Baldridge, J., 2005)
POS OpenNLP (Baldridge, J., 2005)
Tagger + the two parsers below
Parser Stanford (Klein and Manning, 2003)
Berkeley (Petrov and Klein, 2007)
Dep. parser Stanford (Klein and Manning, 2003)
NE OpenNLP (Baldridge, J., 2005)
Recognizer Stanford (Finkel et al, 2005)
NP Detector In-house
Table 1: Preprocessing components available in
Reconcile.
pairs of NPs and it is trained to assign a score
indicating the likelihood that the NPs in the
pair are coreferent.
4. Clustering. A clustering algorithm consoli-
dates the predictions output by the classifier
and forms the final set of coreference clusters
(chains).2
5. Scoring. Finally, during testing Reconcile
runs scoring algorithms that compare the
chains produced by the system to the gold-
standard chains in the answer key.
Each of the five steps above can invoke differ-
ent components. Reconcile?s modularity makes it
2Some structured coreference resolution algorithms (e.g.,
McCallum and Wellner (2004) and Finley and Joachims
(2005)) combine the classification and clustering steps above.
Reconcile can easily accommodate this modification.
158
Step Available modules
Classification various learners in the Weka toolkit
libSVM (Chang and Lin, 2001)
SVMlight (Joachims, 2002)
Clustering Single-link
Best-First
Most Recent First
Scoring MUC score (Vilain et al, 1995)
B3 score (Bagga and Baldwin, 1998)
CEAF score (Luo, 2005)
Table 2: Available implementations for different
modules available in Reconcile.
easy for new components to be implemented and
existing ones to be removed or replaced. Recon-
cile?s standard distribution comes with a compre-
hensive set of implemented components ? those
available for steps 2?5 are shown in Table 2. Rec-
oncile contains over 38,000 lines of original Java
code. Only about 15% of the code is concerned
with running existing components in the prepro-
cessing step, while the rest deals with NP extrac-
tion, implementations of features, clustering algo-
rithms and scorers. More details about Recon-
cile?s architecture and available components and
features can be found in Stoyanov et al (2010).
4 Evaluation
4.1 Data Sets
Reconcile incorporates the six most commonly
used coreference resolution data sets, two from the
MUC conferences (MUC-6, 1995; MUC-7, 1997)
and four from the ACE Program (NIST, 2004).
For ACE, we incorporate only the newswire por-
tion. When available, Reconcile employs the stan-
dard test/train split. Otherwise, we randomly split
the data into a training and test set following a
70/30 ratio. Performance is evaluated according
to the B3 and MUC scoring metrics.
4.2 The Reconcile2010 Configuration
Reconcile can be easily configured with differ-
ent algorithms for markable detection, anaphoric-
ity determination, feature extraction, etc., and run
against several scoring metrics. For the purpose of
this sample evaluation, we create only one partic-
ular instantiation of Reconcile, which we will call
Reconcile2010 to differentiate it from the general
platform. Reconcile2010 is configured using the
following components:
1. Preprocessing
(a) Sentence Splitter: OpenNLP
(b) Tokenizer: OpenNLP
(c) POS Tagger: OpenNLP
(d) Parser: Berkeley
(e) Named Entity Recognizer: Stanford
2. Feature Set - A hand-selected subset of 60 out of the
more than 80 features available. The features were se-
lected to include most of the features from Soon et al
Soon et al (2001), Ng and Cardie (2002) and Bengtson
and Roth (2008).
3. Classifier - Averaged Perceptron
4. Clustering - Single-link - Positive decision threshold
was tuned by cross validation of the training set.
4.3 Experimental Results
The first two rows of Table 3 show the perfor-
mance of Reconcile2010. For all data sets, B3
scores are higher than MUC scores. The MUC
score is highest for the MUC6 data set, while B3
scores are higher for the ACE data sets as com-
pared to the MUC data sets.
Due to the difficulties outlined in Section 1,
results for Reconcile presented here are directly
comparable only to a limited number of scores
reported in the literature. The bottom three
rows of Table 3 list these comparable scores,
which show that Reconcile2010 exhibits state-of-
the-art performance for supervised learning-based
coreference resolvers. A more detailed study of
Reconcile-based coreference resolution systems
in different evaluation scenarios can be found in
Stoyanov et al (2009).
5 Conclusions
Reconcile is a general architecture for coreference
resolution that can be used to easily create various
coreference resolvers. Reconcile provides broad
support for experimentation in coreference reso-
lution, including implementation of the basic ar-
chitecture of contemporary state-of-the-art coref-
erence systems and a variety of individual mod-
ules employed in these systems. Additionally,
Reconcile handles all of the formatting and scor-
ing peculiarities of the most widely used coref-
erence resolution data sets (those created as part
of the MUC and ACE conferences) and, thus,
allows for easy implementation and evaluation
across these data sets. We hope that Reconcile
will support experimental research in coreference
resolution and provide a state-of-the-art corefer-
ence resolver for both researchers and application
developers. We believe that in this way Recon-
cile will facilitate meaningful and consistent com-
parisons of coreference resolution systems. The
full Reconcile release is available for download at
http://www.cs.utah.edu/nlp/reconcile/.
159
System Score Data sets
MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05
Reconcile2010
MUC 68.50 62.80 65.99 67.87 62.03 67.41
B3 70.88 65.86 78.29 79.39 76.50 73.71
Soon et al (2001) MUC 62.6 60.4 ? ? ? ?
Ng and Cardie (2002) MUC 70.4 63.4 ? ? ? ?
Yang et al (2003) MUC 71.3 60.2 ? ? ? ?
Table 3: Scores for Reconcile on six data sets and scores for comparable coreference systems.
Acknowledgments
This research was supported in part by the Na-
tional Science Foundation under Grant # 0937060
to the Computing Research Association for the
CIFellows Project, Lawrence Livermore National
Laboratory subcontract B573245, Department of
Homeland Security Grant N0014-07-1-0152, and
Air Force Contract FA8750-09-C-0172 under the
DARPA Machine Reading Program.
The authors would like to thank the anonymous
reviewers for their useful comments.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In Linguistic Coreference Workshop
at the Language Resources and Evaluation Conference.
Baldridge, J. 2005. The OpenNLP project.
http://opennlp.sourceforge.net/.
E. Bengtson and D. Roth. 2008. Understanding the value of
features for coreference resolution. In Proceedings of the
2008 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
CC Group. 2009. Sentence Segmentation Tool.
http://l2r.cs.uiuc.edu/ cogcomp/atool.php?tkey=SS.
C. Chang and C. Lin. 2001. LIBSVM: a Li-
brary for Support Vector Machines. Available at
http://www.csie.ntu.edu.tw/cjlin/libsvm.
J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating
Non-local Information into Information Extraction Sys-
tems by Gibbs Sampling. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics and
44th Annual Meeting of the ACL.
T. Finley and T. Joachims. 2005. Supervised clustering with
support vector machines. In Proceedings of the Twenty-
second International Conference on Machine Learning
(ICML 2005).
A. Haghighi and D. Klein. 2007. Unsupervised Coreference
Resolution in a Nonparametric Bayesian Model. In Pro-
ceedings of the 45th Annual Meeting of the ACL.
T. Joachims. 2002. SVMLight, http://svmlight.joachims.org.
D. Klein and C. Manning. 2003. Fast Exact Inference with
a Factored Model for Natural Language Parsing. In Ad-
vances in Neural Information Processing (NIPS 2003).
S. Lappin and H. Leass. 1994. An algorithm for pronom-
inal anaphora resolution. Computational Linguistics,
20(4):535?561.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous coreference
resolution algorithm based on the bell tree. In Proceed-
ings of the 42nd Annual Meeting of the ACL.
X. Luo. 2005. On Coreference Resolution Performance
Metrics. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP).
A. McCallum and B. Wellner. 2004. Conditional Models
of Identity Uncertainty with Application to Noun Coref-
erence. In Advances in Neural Information Processing
(NIPS 2004).
MUC-6. 1995. Coreference Task Definition. In Proceedings
of the Sixth Message Understanding Conference (MUC-
6).
MUC-7. 1997. Coreference Task Definition. In Proceed-
ings of the Seventh Message Understanding Conference
(MUC-7).
V. Ng and C. Cardie. 2002. Improving Machine Learning
Approaches to Coreference Resolution. In Proceedings of
the 40th Annual Meeting of the ACL.
NIST. 2004. The ACE Evaluation Plan. NIST.
S. Petrov and D. Klein. 2007. Improved Inference for Un-
lexicalized Parsing. In Proceedings of the Joint Meeting
of the Human Language Technology Conference and the
North American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2007).
M. Poesio and M. Kabadjov. 2004. A general-purpose,
off-the-shelf anaphora resolution module: implementation
and preliminary evaluation. In Proceedings of the Lan-
guage Resources and Evaluation Conference.
L. Qiu, M.-Y. Kan, and T.-S. Chua. 2004. A public reference
implementation of the rap anaphora resolution algorithm.
In Proceedings of the Language Resources and Evaluation
Conference.
W. Soon, H. Ng, and D. Lim. 2001. A Machine Learning Ap-
proach to Coreference of Noun Phrases. Computational
Linguistics, 27(4):521?541.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009. Co-
nundrums in noun phrase coreference resolution: Mak-
ing sense of the state-of-the-art. In Proceedings of
ACL/IJCNLP.
160
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler, and
D. Hysom. 2010. Reconcile: A coreference resolution
research platform. Technical report, Cornell University.
Y. Versley, S. Ponzetto, M. Poesio, V. Eidelman, A. Jern,
J. Smith, X. Yang, and A. Moschitti. 2008. BART: A
modular toolkit for coreference resolution. In Proceed-
ings of the Language Resources and Evaluation Confer-
ence.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A Model-Theoretic Coreference
Scoring Theme. In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6).
X. Yang, G. Zhou, J. Su, and C. Tan. 2003. Coreference
resolution using competition learning approach. In Pro-
ceedings of the 41st Annual Meeting of the ACL.
161
