Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 451?458, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Matching Inconsistently Spelled Names in Automatic Speech Recognizer
Output for Information Retrieval
Hema Raghavan and James Allan
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003, USA
{hema,allan}@cs.umass.edu
Abstract
Many proper names are spelled inconsis-
tently in speech recognizer output, posing
a problem for applications where locating
mentions of named entities is critical. We
model the distortion in the spelling of a
name due to the speech recognizer as the
effect of a noisy channel. The models fol-
low the framework of the IBM translation
models. The model is trained using a par-
allel text of closed caption and automatic
speech recognition output. We also test a
string edit distance based method. The ef-
fectiveness of these models is evaluated on
a name query retrieval task. Our methods
result in a 60% improvement in F1. We
also demonstrate why the problem has not
been critical in TREC and TDT tasks.
1 Introduction
Proper names are key to our understanding of topics
in news. For example, to determine that a news
story is on the 2004 elections in the United States,
the words President Bush, John Kerry and USA
are necessary features of the story. In other words,
names of people, places and organizations are key
entities of a news story. For many tasks, like in
topic detection and tracking (TDT), the entities
form an important feature for distinguishing topics
from one another. For example, it is the people
that distinguish stories on the 2004 election from
stories on the 2000 U.S election. Names, especially
rare and foreign ones are a problem for automatic
speech recognition (ASR) systems as they are often
out of vocabulary (OOV) i.e., they do not exist in
the lexicon of the ASR system. An OOV word is
replaced with the most similar word in the lexicon
of the speech recognizer. Sometimes, even if a
name is in the lexicon of the speech recognizer, it
may have multiple spelling variants. The following
is a sample ASR snippet from the TDT3 1 corpus
that demonstrates how the same entity may have
different spellings even within the same snippet of
ASR text.
...newspaper quotes qaddafi is saying they?ll
turn them over but only if they?re allowed ..leader
moammar gadhafi says he doesn?t want an interna-
tional confrontation over the suspects in the..
In this work, we aim to find methods by which to
cluster or group names in ASR text. We evaluate
a variety of techniques that range from a simple
string-edit distance model to generative models
using both intrinsic and extrinsic evaluations. We
get statistically significant improvements in results
for ad-hoc retrieval when the query is just the name
of a person. We also explain why the problem
of misspelled proper names in ASR has not been
an issue in the TREC spoken document retrieval
(SDR) track or in topic detection and tracking
(TDT). We demonstrate how the problem would be
of significance when the query is short, containing
mainly names with little or no context.
1http:///www.ldc.upenn.edu/projects/tdt3/
451
2 Related Work
That names can be spelled differently is a prob-
lem that has been addressed by the database com-
munity in great detail. They found that the prob-
lem was rising in significance with the increasing
interest in reconciling different databases. Differ-
ences in names due to spelling errors, spelling vari-
ants and transliteration errors have been dealt with
by different kinds of approximate string matching
techniques like Soundex, Phonix, and String Edit
distance (James C. French, 1997; Zobel and Dart,
1996). The nature of the problem is identical when
the domain consists of databases of documents but
in order to apply techniques that were developed for
names by the database community one would have
to first detect names in the corpus, and then normal-
ize them to some canonical form. This is the ap-
proach taken by Raghavan and Allan (Raghavan and
Allan, 2004) who showed that normalizing names
using Soundex codes resulted in a 10% improvement
on the TDT3 Story Link Detection Task. They tested
their method on newswire stories only. Their diffi-
culty in applying Soundex to the ASR documents
was that detecting names in ASR is too error prone
for their methods to be useful (Miller et al, 2000).
Spoken document retrieval was a track at the
TREC-6,7 and 8 (Voorhees and Harman, 1997;
Voorhees and Harman, 1998; Voorhees and Harman,
1999) conferences. At the TREC-8 SDR track the
conclusion was that ASR is not really an issue for
ad hoc retrieval. However, the queries in those tracks
were not centered on any entity. The TREC-8 pro-
ceedings also acknowledge that mean average preci-
sion dropped as named entity word error rate (NE-
WER) increased. A typical speech recognizer has a
lexicon of about 60K and for this size of a lexicon,
about 10% of the person names are out of vocabu-
lary (OOV).
The problem of alternate spellings of names has
also been explored by the cross lingual information
retrieval community (Virga and Khudanpur, 2003;
AbdulJaleel and Larkey, 2003). The problem with
names in machine translated text is quite similar to
the problem with names in ASR text, except that the
errors caused by a speech recognizer are often pho-
netic confusions, which is not necessarily the case
for machine translation errors. Spelling errors of
names in machine translated text are typically con-
sistent. A given word in the source language always
translates to the same word in the target language for
a given machine translation system. As seen earlier,
ASR systems do not exhibit such consistency.
Another problem that resembles the one we are
addressing in this paper is that of spelling correc-
tion. Spelling correction has been tackled in several
different ways (Durham et al, 1983), in some cases
with the use of contextual cues (Golding and Roth,
1999) and in some cases it has been modeled as a
?noisy channel problem? (Kernighan et al, 1990).
The latter approach is similar to ours because we
also approach the problem of spelling variations due
to speech recognizer errors as analogous to the er-
rors caused by a noisy channel. However, spelling
correction methods must rectify human errors (ty-
pographic errors and common confusions) whereas
speech recognizer errors are different.
Additionally, the argument that Jon Smith and
John Smythe may genuinely be different people and
should not be considered to be the same entity is
more of a cross-document co-reference problem.
The problem we are attempting to solve in this
paper is one of grouping names that ?sound like?
each other together, without considering the prob-
lem of cross document co-reference. For example,
the name Lewinsky has 199 occurrences in the TDT3
corpus, and also appears as Lewinski (1324 times),
and Lewenskey (171 times). Most of these occur-
rences refer to Monica Lewinsky. The aim is to
group all these variants together, without taking into
consideration which ones refer to the same person.
We then measure the effectiveness of our methods
on various retrieval tasks.
Perhaps the most similar work from the point of
view of the task is work in word spotting in audio
output (Amir et al, 2001). The queries are single
words and the task is to locate their mention in au-
dio. The starting point in that work is however, a
phonetic transcript of the audio signal and the em-
phasis is not on locating names. Our starting point
is automatic speech recognizer output, and we aim
to locate names in particular.
3 Our Approaches
In this section we explain the techniques by which
we group names together. One method uses string
edit distance to group names that are variants of each
452
other. The other techniques are some of the possible
generative models suitable to this task.
An equivalence class is defined as a group of
names such that any two names in that class are vari-
ants of each other and such that there exist no two
names from different equivalence classes that are
variants of each other. An equivalence class is rep-
resented as a set of names enclosed in curly braces
as {name-1 name-2 ...}
Four of our models are trained on a parallel text
of ASR and manual transcripts (or closed caption
depending on availability) in order to learn a proba-
bilistic model of ASR errors. The parallel text con-
sists of pairs of sentences: sentences from the ASR
output and the corresponding manual transcripts.
This is a common technique in machine translation
for which the IBM translation models are popular
methods (Brown et al, 1993).
As a convention, we use uppercase letters to de-
note ASR output and lowercase for manual tran-
scriptions. Given an input of parallel text of ASR
and manual transcriptions, the model learns a prob-
abilistic dictionary. The dictionary contains pairs
of closed caption and ASR words and the probabil-
ity that the closed caption word is generated from a
given word in ASR. Thus, the model might learn a
high probability for P(CAT|kate).
3.1 Overview of Methods
We generate equivalence classes of names by clus-
tering a list of names. The algorithm draws links be-
tween pairs of words and then clusters the words into
equivalence classes such that if a and b are linked
and b and c are linked then a, b and c are in the same
equivalence class. Links between words are gener-
ated in five different ways described below.
In the first of our methods we align manual tran-
scripts and ASR sentences using the IBM transla-
tion model (Brown et al, 1993) to obtain a proba-
bilistic dictionary. We give details of the translation
model in section 3.2. Names are grouped such that
if P(CAT|kate) is high (above some threshold) then
there is a link between CAT and kate. This is called
the Simple Aligned method. Some sample pairs of
words obtained by this technique are shown in fig-
ure 1.
We can also ask a human to create a list of equiv-
alence classes of names. We describe our method
african AFRICA albania ALBANIAN
alex ALEC cardoso CARDOZO
ann ANNE ching CHIANG
Figure 1: Example of pairs of words obtained by
Simple Aligned
of obtaining such a list in section 4. This method is
called the Supervised method.
Given a list of equivalence classes, pairs of names
that go together can easily be generated such that for
each pair, both words are obtained from the same
equivalence class. In this way equivalence classes
of names obtained from the Simple Aligned and Su-
pervised methods can be used to create a list of pairs
of names that form parallel text to train a charac-
ter level machine translation model. We would ex-
pect this model to learn a high probability for simi-
lar sounding alphabets, e.g., a high probability for
P (C|k). Depending on where the training set of
pairs of names for this method comes from, we get
two possible systems. These are called the Gener-
ative Unsupervised method and Generative Super-
vised method respectively. Note that the Genera-
tive Unsupervised method is not completely unsu-
pervised; we still need the parallel text of ASR and
manual transcripts, but we don?t need a human to
do the added grouping of names into equivalence
classes. A character level translation model helps
us generalize better to unseen words.
We also grouped together names that differ by a
string edit distance of one, giving a fifth system. In
particular, we use the Levenshtein distance (Lev-
enshtein, 1966), that is the number of insertions,
deletions and substitutions needed to convert one
string to the other. Many methods employed by the
database community build on string edit distance.
The method works well but has some disadvantages.
Consider a user who types in a query containing a
name such that the spelling, as typed by the user,
never occurs in the corpus. To employ string edit
distance, one would have to compare the query name
against all the words in the vocabulary of the cor-
pus to find the most similar strings. With a gener-
ative model, only the query needs to be expanded
using the translation model, thereby speeding up the
search process. The string edit distance model on the
453
other hand, is completely unsupervised and needs no
training in the form of parallel text. Both methods
have their advantages and disadvantages, and the use
of one method over the other is situation dependent.
3.2 Details
To learn alignments, translation probabilities, etc in
the first method we used work that has been done in
statistical machine translation (Brown et al, 1993),
where the translation process is considered to be
equivalent to a corruption of the source language text
to the target language text due to a noisy channel.
We can similarly consider that an ASR system cor-
rupts the spelling of a name as a result of a noisy
channel. To obtain the closed caption word c, of an
ASR word a, we want to find the string for which
the probability P (c|a) is highest. This is modeled as
P (c|a) = P (c)P (a|c)P (a) (1)
For a given name a, since P (a) is constant, the
problem reduces to one of maximizing P (c)P (a|c).
P (c) is called the language model. We need
to model P (a|c) as opposed to directly modeling
P (c|a) so that our model assigns more probability
to well formed English names.
Given a pair of sentences (c, a), an alignment
A(c, a) is defined as the mapping from the words
in c to the words in a. If there are l closed caption
words and m ASR words, there are 2lm alignments
in A(c, a). l ? A(c, a) can be denoted as a series
lm1 = l1, l2...lm where lj = i means that a word in
position j of the ASR string is aligned with a word in
position i of the closed caption string. Then P (a|c)
is computed as follows:
P (a|c) =
?
l
P (a, l|c)
P (a, l|c) = P (m|c)
m
?
j
P (lj |lj?11 , a
j?1
1 , m, e)
?P (aj |lj1, a
j?1
1 , m, c) (2)
where aj is a word in position j of the string a, and
aj1 is the series a1...aj . The model is generative in
the following way: we first choose for each word in
the closed caption string the number of ASR words
that will be connected to it, then we pick the identity
of those ASR words and finally we pick the actual
positions that these words will occupy. There are
five different IBM translation models (Brown et al,
1993). Models 3 and 4 build on the above equations,
and also incorporate the notion of fertility. Fertility
takes into account that a given word in closed cap-
tion may be omitted by an ASR system, or one word
may result in two or more, like Iraq? I ROCK (This
is a true example). The models are trained using Ex-
pectation Maximization. Further details are in the
original paper (Brown et al, 1993).
The IBM models have shown good performance
in machine translation, and especially so within cer-
tain families of languages, for example in translating
between French and English or between Sinhalese
and Tamil (Brown et al, 1993; Weerasinghe, 2004).
Pairs of closed caption and ASR sentences or words
(as the case may be) are akin to a pair of closely re-
lated languages.
For the Generative Unsupervised and Generative
Supervised methods, we use the same models, but in
this case the training set consists of pairs of words
obtained from the ASR and closed caption text as
opposed to sentences. In other words, the place of
words in the previous case is taken by characters.
Modeling fertility, etc, again fits very well in this
case. For example the terminal character e is often
dropped in ASR, and a single o in closed caption
may result in a double o in ASR or vice versa.
4 Experimental Set Up
4.1 Corpora
For experiments in this paper we used the TREC-6
and TREC-7 SDR track data (Voorhees and Harman,
1998). We also used the TDT2 and TDT3 corpora.
For TREC-6 we had the ASR output provided by
NIST (WER 34%). The TREC-7 corpus consists of
the output of the Dragon systems speech recognizer
(WER 29.5%). For the TDT sources we had the
ASR output of the BBN Byblos Speech recognizer
provided by the LDC. NIST provides human gener-
ated transcripts for the TREC corpora and LDC pro-
vides closed caption quality transcripts with a WER
of 14.5% for the TDT corpora. There are 3943,
23282, 1819 and 2866 ASR documents in the TDT2
TDT3, TREC-6 and TREC-7 corpora respectively.
454
4.2 Intrinsic Evaluation
The Paice evaluation (Paice, 1996) for stemming al-
gorithms (algorithms that reduce a word to its mor-
phological root), attempts to compare the equiva-
lence classes generated by our methods with human
judgments.
The Paice evaluation measures the performance
of a stemmer based on its understemming and over-
stemming indices (UI and OI respectively). UI
measures the total number of missed links between
words and OI measures the total number of false
alarm links. A perfect stemmer would have a UI and
OI value of zero.
We obtained a list of names to be grouped into
equivalence classes in the following way. We did
not use a named entity tagger on the corpus because
named entity taggers typically have very high word
error rates for ASR text (Bikel et al, 1999). Instead
we ran the Unix spell command on the corpus and
used the list of rejected words as the list of names
for the annotators to group into equivalence classes.
These 296 OOV words are taken to correspond to
the names in the corpus. We then obtained the set of
ground-truth equivalence classes by a method simi-
lar to Paice.
A group of undergraduate students was hired. The
list of names was provided to each student in a text
editor in alphabetical order. The purpose as ex-
plained to them was to group together names that
were alternate spellings of similar sounding names
together. The student was instructed to go through
the list systematically, and for each word to look
at the previous 10 words, as well as the following
10 words to see if there were any other variants. If
there was a word or a group where the current word
was likely to fit in, they were asked to cut the word
and paste it into the appropriate group. In this way,
groups were created such that no word could belong
to more than one group. The annotators were also
asked to mark the words that were indeed names. Of
the 296 OOV words, 292 were found to be actual
names.
4.3 Extrinsic evaluation
In addition to the Paice evaluation we propose two
extrinsic or task based evaluations for our methods.
In the first task, given a name as a query, we aim to
Query Equivalence class
1: {christy christie}
2: {christina christine}
3: {toney toni}
4: {michelle michel mitchell}
5: {columbia colombia colombian}
Figure 2: Some sample query equivalence classes
find all documents that have a mention of that name
or any of its variants. In order to obtain queries
and relevance judgments for this task we arbitrar-
ily chose 35 groups of names from the ground-truth
set of equivalence classes. The TDT3 corpus was
chosen to be the test corpus for this task. Hence we
eliminated those words that had no occurrence in the
TDT3 corpus from the 35 groups of names giving a
total of 76 names. Each of the 76 words formed a
query. For each name query we consider all docu-
ments that contain a mention of any of the names in
the equivalence class of the query as relevant to that
query. In this way we obtained relevance judgments
for the name query task. Some sample queries are
shown in figure 2. We use F1 (harmonic mean of the
precision and recall) as a measure of performance.
Our extrinsic evaluation is spoken document re-
trieval. The queries on the TREC-6 and TREC-7
corpora are standard TREC spoken document re-
trieval track queries. For the TDT2 corpus we use
one randomly chosen document from each topic as
the query. This document is like a long query with
plenty of entities and plenty of contextual informa-
tion. For the TDT3 corpus we use the topic de-
scriptions as provided by the LDC as the queries.
The LDC topic descriptions discuss the events that
describe a topic and the key entities and locations
involved in the event. These are representative of
shorter queries, rich in entities. LDC has provided
relevance judgments for both the TDT2 and TDT3
corpora. Mean average precision was used as the
measure of evaluation.
4.4 Implementation Details
We use GIZA++ (Och and Ney, 2003) to train the
machine translation system and the ISI ReWrite
Decoder (ISI, 2001) to do the actual translations.
The decoder takes as input the models learned by
455
GIZA++ and a sentence from the foreign language.
It can output the top n translations of the input sen-
tence. The ReWrite decoder can translate using IBM
Model-3 or Model-4. We found Model 3 to have
lower perplexity and hence chose it for our experi-
ments. In order to build the language model P (c),
we used the CMU Language Modeling toolkit 2.
All retrieval experiments were performed using the
LEMUR 3 toolkit, and using the traditional vector
space model. In the traditional vector space model
queries and documents are represented as vectors of
words. Each word in the vector is weighted using
a product of term frequency and inverse document
frequency. The similarity between a query and a
document is measured using the cosine of the angle
between the query and document vectors.
The Simple Aligned and Generative Unsuper-
vised methods require a parallel corpus of ASR and
closed caption for training. For the name query task
we used TDT2, TREC-6 and TREC-7 to train these
methods and TDT3 as the test corpus.
The Supervised and Generative Supervised meth-
ods require a human to provide pairs of words that
are variants of each other. We filtered out those
words from the human generated list of equivalence
classes that occurred exclusively in the test corpus
and in no other corpus. This is equivalent to asking
a human to group words in the training corpus. Sim-
ilarly we trained the Simple Aligned and Generative
Unsupervised models using ASR and closed caption
text from all other sources except those in the test
set.
The models were trained similarly for the SDR
experiments. The models were tested on each of
the four corpora in turn, and in each case they were
trained on everything but the test corpus.
5 Results
5.1 Intrinsic Experiments
Table 1 shows how the different methods perform on
the intrinsic evaluation. We also show the UI and OI
values for methods that use string edit distances of
2, 3, 4 and 5. Note that the Supervised method is the
ground truth for this evaluation, and hence it has a UI
and OI value of zero. A string edit distance of 1 has
2http://mi.eng.cam.ac.uk/prc14/toolkit documentation.html
3http://www.cs.cmu.edu/lemur
Method UI OI
Simple Aligned 0.236 0.004
Supervised 0 0
Gen Sup 0.393 0.023
Gen Uns 0.351 0.003
Str. Ed. (1) 0.229 0.000
Str. Ed. (2) 0.083 0.003
Str. Ed. (3) 0.039 0.001
Str. Ed. (4) 0.031 0.124
Str. Ed. (5) 0.023 0.336
Table 1: Understemming and Overstemming indices
for each of the methods (lower is better)
the lowest OI value, meaning there are very few false
alarms. Higher string edit distances have lower UI
values, with an increase in OI. We will interpret the
UI and OI values again after observing performance
on the retrieval tasks, so as to interpret the impact of
missed links and false alarm links for retrieval.
5.2 Name Query Retrieval experiments
The results of our experiments on the name query
task are given in table 2. We report both Macro
and Micro averaged (averaged over the equivalence
classes of the queries) F1 measures. They do not dif-
fer much since the equivalence classes have almost
the same number (2-3) of names.
From table 2, all methods improve the baseline
F1 score significantly (statistical significance mea-
sured using a two tailed t-test with a confidence of
95%). In general, the Simple Aligned, Generative
Unsupervised and string edit distance methods are
the best performing for this task. The string edit
distance improves the baseline by over 60%. The
Supervised method is also not as good as the other
four of our methods as it does not generalize well to
names that occur exclusively in the test set.
String edit distance performs very well on cer-
tain equivalence classes of names. For example, on
the equivalence class {Seigal, Segal, Siegal, Siegel}
the precision and recall are 100% each since all of
the words in the equivalence class differ from each
other by a string edit distance of one. In the case of
the equivalence class {Lewenskey Lewinski Lewin-
sky}, the term Lewenskey has a string edit distance
of 2 (greater than one) from the other two members,
456
Method Micro avg Micro avg Micro Macro avg Macro avg Macro
Recall Precision F1 Recall Precision F1
Baseline 0.401 1 0.573 0.400 1 0.571
Simple Aligned 0.632 0.933 0.754 0.608 0.925 0.734
Sup 0.477 0.961 0.638 0.463 0.960 0.625
Gen Sup 0.530 0.937 0.677 0.517 0.938 0.667
Gen Uns 0.590 0.921 0.720 0.576 0.913 0.706
Str. Ed 0.752 0.867 0.806 0.751 0.871 0.807
Table 2: Results on the Name Query Retrieval task
Lewinsky and Lewinski. The equivalence class of
{John Jon Joan} has very low precision and recall.
This is because both John and Jon differ by a string
edit distance of one from so many other names in the
corpus, such as Jong, resulting in lowered precision.
The Simple Aligned method fails on names it has
not seen in the training set. However, for cases
like {Greensborough Greensboro} the link between
these two names is detected using the simple aligned
method and by no other. The generative methods can
detect variations in spelling due to similar sounding
alphabets. For example it can detect the link be-
tween Sydney and Sidney. The generative models
were also able to learn that c and k are substitutable
for each other. Therefore these models could detect
the links between the words in the equivalence class
{Katherine Kathryn Catherine}.
The Simple Aligned model performs well on the
extrinsic evaluations although it has a high OI value.
The intrinsic evaluations use judgments by humans.
The Simple Aligned method would conflate Kofi and
Copy into one class if that was a genuine ASR error
and the alignment was correct, but these two words
would not be conflated into the same equivalence
class by our annotators and would actually count
as a false alarm on the intrinsic evaluations. There-
fore, although the OI is high for the Simple Aligned
Method, on closer examination we found that some
of the false alarms were actually representative of
ASR errors.
5.3 Spoken Document Retrieval
We now move on to discuss results on the SDR task.
For TDT3 we got statistically significant improve-
ments (an improvement in mean average precision
from 0.715 to 0.757) over the baseline using string
edit distance. On the remaining corpora we got little
or no improvement by our methods. We proceed to
explain why this is the case for each of the corpora.
The TREC-7 corpus has only 5 queries with a
mention of a name resulting in hardly any gains
overall. Similar was the case for TREC-6. Again
in the case of the TDT2 corpus, since we used en-
tire documents as stories, there are enough words in
the query that a few recognition errors can be toler-
ated and therefore traditional retrieval is good for the
task. There is evidence from previous TREC tracks
(Voorhees and Harman, 1999) that shorter queries
result in a decrease in retrieval performance and
hence we see some improvements for TDT3. Be-
sides, the TDT3 queries were rich in names.
We wanted to check how our methods performed
on outputs of different ASR systems. Spoken doc-
ument retrieval on the TREC-7 data with the out-
put of Dragon systems, which has a word error rate
of 29.5%, results in an improvement of 6% using
the Simple Aligned method. The NIST-B2 system
with a higher WER (46.6%) has an improvement in
Mean Average Precision of 6.5%. Similarly with the
CUHTK (WER 35.6%) and NIST-B1 (WER 33.8%)
and Sheffield (WER 24.6 %) systems we obtained
improvements of 1.6%, 0.39% and 0.05% respec-
tively using the Simple Aligned method. Thus, with
increasing WER, the named entity word error rate
increases significantly, and therefore the benefits of
our method are more apparent in such situations.
6 Discussion and Conclusions
We showed (both intrinsically and extrinsically) that
string edit distance is an effective technique for lo-
cating name variants. We also developed a set of
generative models and showed that they are almost
457
as effective at name finding and document retrieval,
but are probably more efficient than string edit dis-
tance. The generative models need to be trained on
parallel text and therefore require human effort for
training the models. The advantage of one method
over the other is dependent on the size of the corpus
and the availability of resources.
The problem has not been of significance in previ-
ous TREC tasks or in TDT, because we have always
escaped the problem of misspelled names by virtue
of the nature of those tasks. In the TREC tasks very
few queries are centered on an entity. In all the TDT
tasks, one is usually required to compare entire sto-
ries with each other. A story is long enough that
there are enough words that are in the vocabulary
(just like a very long query) or that are correctly rec-
ognized, that the ASR errors do not really matter.
Therefore, the TDT tasks also do not suffer as a re-
sult of these ASR errors.
We can improve and apply our methods to other
domains like Switchboard data (Godfrey et al,
1992). Our methods also generalize well across lan-
guages since there are no language specific tech-
niques employed.
7 Acknowledgements
This work was supported in part by the Center
for Intelligent Information Retrieval and in part by
SPAWARSYSCEN-SD grant number N66001-02-1-
8903. Any opinions, findings and conclusions or
recommendations expressed in this material are the
author(s) and do not necessarily reflect those of the
sponsor.
References
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statistical
transliteration for english-arabic cross language information
retrieval. In Proceedings of the 12th CIKM conference,
pages 139?146. ACM Press.
Arnon Amir, Alon Efrat, and Savitha Srinivasan. 2001. Ad-
vances in phonetic word spotting. In CIKM ?01: Proceed-
ings of the tenth international conference on Information and
knowledge management, pages 580?582, New York, NY,
USA. ACM Press.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s in a
name. Machine Learning, 34(1-3):211?231.
P. F. Brown, Steven A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Lingustics, 19(2):263?311.
Ivor Durham, David A. Lamb, and James B. Saxe. 1983.
Spelling correction in user interfaces. Commun. ACM,
26(10):764?773.
J. Godfrey, E. Holiman, and J. McDaniel. 1992. Switchboard:
Telephone speech corpus for research and development. In
Proceedings of the International Conference on Acoustics,
Speech and Signa Processing pp. I-517-520, 1992, pages
517?520.
Andrew R. Golding and Dan Roth. 1999. A winnow-based
approach to context-sensitive spelling correction. Machine
Learning, 34(1-3):107?130.
2001. ISI rewrite decoder, http://www.isi.edu/licensed-
sw/rewrite-decoder/.
Allison L. Powell James C. French. 1997. Applications of ap-
proximate word matching in information retrieval. In Pro-
ceedings of the Sixth CIKM Conference.
Mark D. Kernighan, Kenneth W. Church, , and William A. Gale.
1990. A spelling correction program based on a noisy chan-
nel model. In Proceedings of COLING-90, pages 205?210.
V. I. Levenshtein. 1966. Binary codes capable of correcing
deletions,insertions and reversals. Phs. Dokl., 6:707?710.
David Miller, Richard Schwartz, Ralph Weischedel, and Re-
becca Stone. 2000. Named entity extraction from broadcast
news.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Chris D. Paice. 1996. Method for evaluation of stemming al-
gorithms based on error counting. JASIS, 47(8):632?649.
Hema Raghavan and James Allan. 2004. Using soundex codes
for indexing names in asr documents. In Proceedings of the
HLT NAACL Workshop on Interdisciplinary Approaches to
Speech Indexing and Retrieval.
Paola Virga and Sanjeev Khudanpur. 2003. Transliteration of
proper names in cross-language applications. In Proceed-
ings of the 26th ACM SIGIR conference, pages 365?366.
ACM Press.
E. M. Voorhees and D. K. Harman, editors. 1997. The Sixth
Text REtrieval Conference (TREC 6). NIST.
E. M. Voorhees and D. K. Harman, editors. 1998. The Seventh
Text REtrieval Conference (TREC 7). NIST.
E. M. Voorhees and D. K. Harman, editors. 1999. The Eighth
Text REtrieval Conference (TREC 8). NIST.
Ruvan Weerasinghe. 2004. A statistical machine translation
approach to Sinhala Tamil language translation. In SCALLA
2004.
Justin Zobel and Philip W. Dart. 1996. Phonetic string match-
ing: Lessons from information retrieval. In Proceedings of
the 19th ACM SIGIR Conference,(Special Issue of the SIGIR
Forum), pages 166?172.
458
Using Soundex Codes for Indexing Names in ASR documents
Hema Raghavan
hema@cs.umass.edu
James Allan
allan@cs.umass.edu
Abstract
In this paper we highlight the problems that
arise due to variations of spellings of names
that occur in text, as a result of which links be-
tween two pieces of text where the same name
is spelt differently may be missed. The problem
is particularly pronounced in the case of ASR
text. We propose the use of approximate string
matching techniques to normalize names in or-
der to overcome the problem. We show how we
could achieve an improvement if we could tag
names with reasonable accuracy in ASR.
1 Introduction
Proper names are often key to our understanding of the
information conveyed by a document. This is particu-
larly the case when the domain is news. For example, a
document with several mentions of George W. Bush, Dick
Cheney, Baghdad and Saddam Hussein, gives us a good
sense of what the contents of the document may be. In
comparison, other regular English words like death, scud
and missiles, may be good indicators of more general top-
ics like war, but may not give us any indication of the
exact event being discussed. Linking stories that discuss
the same event, like the Attack on Iraq is very useful for
a news filtering systems. When topics are primarily de-
termined by specific events, it is easy to see why names
of entities- people places and organizations, play such a
critical role in discriminating between events that discuss
a topic.
However, when one considers a real life scenario
where news is from different media (print and broad-
cast) and in many different languages, proper names
pose many different problems. The problem with proper
names is that they often have different spelling variations.
For example, the names Arafat, Araafat, and Arafaat may
all refer to the same entity. Human beings can also vary
in their spellings of a named entity. Besides that, the out-
put of ASR and Machine Translation systems can also re-
sult in different spelling variations of a name. Such slight
spelling variations may be acceptable and discernible by
humans, but for a machine they are harder to match. A
user who issues a query with the term Arafat in it may
never find a document that discusses Araafat, using cur-
rent TF-IDF matching techniques, even though the docu-
ment may be pertinent to his or her query. Although this
loss may not be critical to some applications, one cannot
assume that the problem does not exist. The problem has
been addressed by the data-base community in the past by
the use of approximate string matching techniques, but in
pure-text, we have the added problem of detecting names.
In this paper, we demonstrate with examples how
sometimes we may not be able to draw connections be-
tween two pieces of text without the use of approximate
string matching techniques. We indicate the problems we
encounter while detecting names, and propose ways to
address those issues. In the discussion of previous work
in the next section we describe some tasks that use ASR
output, and which may have been benefited by the use
of approximate string matching techniques. We describe
some preliminary experiments and their results. We then
discuss the bottlenecks, in the proposed methodology,
and how they may be overcome.
2 Past Work
2.1 Stemming
Stemming (Porter, 1980; Krovetz, 1993) is a method in
which the corpus is processed so that semantically and
morphologically related words are reduced to a common
stem. Thus, race, racing, and racer are all reduced to a
single root ? race. Stemming has been found to be ef-
fective for Information Retrieval, TDT and other related
tasks. Current stemming algorithms work only for regu-
lar English words and not names. In this paper we look
at addressing the problem of grouping together and nor-
malizing proper names in the same way that stemming
groups together regular English words.
2.2 Approximate String Matching
There has been some past work (French et al, 1997; Zo-
bel and Dart, 1996) that has addressed the problem that
proper names can have different spellings. Each of those
works, however, only addresses the question of how ef-
fectively one can match a name to its spelling variants.
They measure their performance in terms of the preci-
sion and recall with which they are able to retrieve other
names which are variants of a given query name. Essen-
tially, the primary motivation of those works was in find-
ing good approximate string matching techniques. Those
techniques are directly applicable only in applications
that retrieve tuples from a database record.
However, there is no work that evaluates the effec-
tiveness of approximate string matching techniques for
names in an information retrieval or related task. We
know of no work that attempts to detect names automati-
cally, and then index names that should go together, in the
same way that words of the same stem class are indexed
by one common term.
2.3 The TREC SDR and the TDT Link Detection
tasks
A single news-source may spell all mentions of a given
name identically. However, this consistency is lost when
there are multiple sources of news, where sources span
languages and modes (broadcast and print). The TDT3
corpus (ldc, 2003) is representative of such real-life data.
The corpus consists of English, Arabic and Mandarin
print and broadcast news. ASR output is used in the case
of the broadcast sources and in the case of non-English
stories machine translated output is used for comparing
stories. For both ASR systems and Machine Transla-
tion systems, proper names are often out-of-vocabulary
(OOV). A typical speech recognizer has a lexicon of
about 60K, and for a lexicon of this size about 10% of
the person names are OOV. The OOV problem is usually
solved by the use of transliteration and other such tech-
niques. A breakdown of the OOV rates for names for
different lexicon sizes is given in (Miller et al, 2000).
We believe the problem of spelling errors is of impor-
tance when one wants to index and retrieve ASR docu-
ments. For example, Monica Lewinsky is commonly re-
ferred to in the TDT3 corpus. The corpus has closed- cap-
tion transcripts for TV broadcasts. Closed caption suf-
fers from typing errors. The name Lewinsky is also often
misspelt as Lewinskey in the closed caption text. In the
ASR text some of the variants that appear are Lewenskey,
Linski, Lansky and Lewinsky. This example is typical,
with the errors in the closed caption text highlighting how
humans themselves can vary in their spelling of a name
and the errors in ASR demonstrating how a single ASR
system can output different spellings for the same name.
The ASR errors are largely because ASR systems rely
on phonemes for OOV words, and each of the different
variations in the spellings of the same name is probably
a result of different pronounciations and other such fac-
tors. The result of an ASR system then, is several dif-
ferent spelling variations of each name. It is easy to see
why it would help considerably to group names that refer
to the same entity together, and index them as one en-
tity. We can exploit the fact that these different spelling
variations of a given name exhibit strong similarity us-
ing approximate string matching techniques. We propose
that in certain domains, where the issue that proper names
exist with many different variations is dominant, the use
of approximate string matching techniques to determine
which names refer to the same entity will help improve
the accuracy with which we can detect links between sto-
ries. Figure 1 shows a snippet of closed caption text and
its ASR counterpart. The names Lewinskey and Tripp
are misspelt in the ASR text. The two documents how-
ever have high similarity, because of the other words that
the ASR system gets right. Allan (Allan, 2002) showed
how ASR errors can cause misses in TDT tasks, and can
sometimes be beneficial, resulting in a minimal average
impact on performance in TDT. In the case of Spoken
Document Retrieval (Garofolo et al, 2000) also it was
found that a few ASR errors per document did not re-
sult in a big difference to performance as long as we get
a reasonable percentage of the words right. Of course,
factors such as the length of the two pieces of text being
compared make a difference. Barnett et al(Barnett et al,
1997), showed how short queries were affected consid-
erably by Word Error rate. ASR errors may not cause a
significant drop in performance for any of the Topic De-
tection and Tracking tasks. But, consider a system where
retrieving all documents mentioning Lewinskey and Tripp
is critical, and it is not unrealistic to assume there exist
systems with such needs, the ASR document in the above
mentioned example would be left out. We therefore, be-
lieve that the problem we are addressing in this paper is
an important one. The preliminary experiments in this
paper, which are on the TDT corpus, only highlight how
our approach can help.
3 Story Link Detection
3.1 Task Definition
The Story Link Detection Task is key to all the other tasks
in TDT. The system is handed a set of story pairs, and
for each pair it is asked to judge whether both the stories
discuss the same topic or different topics. In addition to
a YES/NO decision the system is also expected to output
a confidence score, where a low confidence score implies
that the system is more in favor of the NO decision.
3.2 Our Approach
Simply stated our approach to the SLD task, is to use ap-
proximate string matching techniques to compare entities
between two pieces of text. The two pieces of text may be
a query and a document, or two documents, depending on
the task. We first need to identify entities in the two doc-
uments. There exist several techniques to automatically
identify names. For properly punctuated text, heuristics
like capitalization work sufficiently well. However, for
ASR text we often do not have sentence boundaries or
even punctuation. Hence we rely on a Hidden Markov
Model based named entity recognizer (Bikel et al, 1999)
for our task.
A simple strategy that incorporates an approximate
string matching technique is to first preprocess the cor-
pus, and then normalize all mentions of a named entity
to a given canonical form, where the canonical form is
independent of mentions of other entities in the two doc-
uments being compared. Soundex, Phonix, and other
such codes offer us a means of normalizing a word to
its phonetic form. The Soundex code is a combination of
the first letter of the word and a three digit code which
is representative of its phonetic sound. Hence, similar
sounding names like ?Lewinskey? and ?Lewinsky? are
both reduced to the same soundex code ?l520?. We can
pre-process the corpus so that all the named entities are
replaced by their Soundex codes. We then compute the
similarity between documents in the new corpus as op-
posed to the old one, using conventional similarity met-
rics like Cosine or TF-IDF.
4 Experimental Set up
4.1 Data
The corpus (ldc, 2003) has 67111 documents from mul-
tiple sources of news in multiple languages (English
Chinese and Arabic) and media (broadcast news and
newswire). The English sources are Associated Press and
New York Times, PRI, Voice of America etc. For the
broadcast news sources we have ASR output and for TV
we have both ASR output as well as closed caption data.
Additionally we have the following Mandarin news-wire,
web and broadcast sources - Xinhua news, Zaobao, and
Voice of America (Mandarin). For all the Mandarin doc-
uments we have the original documents in the native lan-
guage as well the English output of Systran- a machine
translation system. The data has been collected by LDC
by sampling from the above mentioned sources in the pe-
riod from October to December 1998.
The LDC has annotated 60 topics in the TDT3 corpus.
A topic is determined by an event. For example topic
30001 is the Cambodian Government Coalition. Each
topic has key entities associated with it and a description
of the topic. A subset of the documents are annotated as
being on-topic or not according to a well formed strategy
as defined by the LDC.
4.2 Story Link Detection
To compute the similarity of two documents, that is, the
YES/NO decision threshold, we used the the traditional
cosine similarity metric. To give some leverage to doc-
uments that were very similar even before named entity
normalization, we average the similarity scores between
documents before and after the named entities have been
normalized by their Soundex codes as follows:
Sim(D
1
; D
2
) =
1
2
(Cos(D
1
; D
2
) + Cos(D
0
1
; D
0
2
)) (1)
Where D
1
and D
2
are the original documents and D0
1
and D0
2
are the documents after the names have been nor-
malized.
4.3 Evaluation
An ROC curve is plotted by making a parameter sweep of
the YES/NO decision thresholds, and plotting the Misses
and False Alarms at each point. At each point the cost
is computed using the following empirically determined
formula (Fiscus et al, 1998).
C
det
= 0:02P (miss) + 0:098P (fa) (2)
This cost function is standard across all tasks. The point
of minimum cost serves as the comparison between vari-
ous systems.
5 Results
We tested our idea on the TDT3 corpus for the Story Link
Detection Task, using the Cosine similarity metric, and
found that performance actually degraded. On investiga-
tion we found that the named entity recognizer performs
poorly on Machine Translated and ASR source data. Our
named entity recognizer relies considerably on sentence
structure, to make its predictions. Machine translated out-
put often lacks grammatical structure, and ASR output
does not have punctuation, which results in a lot of named
entity tagging errors.
We therefore decided to test our idea for newswire text.
We created our own test set of 4752 pairs of stories from
newswire sources. This test set was created by randomly
picking on and off-topic stories for each topic using the
same policy as employed by the LDC (Fiscus, 2003). On
these pairs, we obtained about 10% improvement (Fig-
ure 2), suggesting that there is merit in Soundex normal-
ization of names. However, the problem of poor named
entity recognition is a bottle-neck for ASR. We discuss
1
2
5
10
20
40
60
80
90
.01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90
M
is
s 
pr
ob
ab
ilit
y 
(in
 %
)
False Alarms probability (in %)
SLD using soundex codes on Newswire-Newswire pairs
Random Performance
Using Soundex
TW Min DET Norm(Cost) = 0.1588
Baseline
TW Min DET Norm(Cost) = 0.1709
Figure 1: Story Link Detection performance
alternative strategies of how to deal with this, and other
ways of using approximate string matching in the next
section.
6 Alternative strategies
6.1 To not use an entity recognizer
We were not able to benefit from our approach on the
ASR documents because of the poor performance of the
named entity recognizer on those types of document.
An example of a randomly picked named entity tagged
ASR document is given below. The tagging errors are
underlined.
< DOC >
< DOCNO > CNN19981001:0130:0000 < /DOCNO >
< TEXT >
< ENAMEX TYPE=?ORGANIZATION? >
BUDGET SURPLUS < /ENAMEX> AND FIGHTING
OVER WHETHER IT?S GOING DOOR POCKETS WILL
TELL YOU THE < ENAMEX TYPE=?ORGANIZATION?
> VEHICLES CLIMBED DATES THEREAFTER <
/ENAMEX > AND IF YOU?RE REQUIRED TO PAY
CHILD SUPPORT INFORMATION THAT YOUR
JOB AND COME AND ADDRESS NOW PART
HAVE < ENAMEX TYPE=?ORGANIZATION? >
A NATIONAL REGISTRY THE HEADLINE < /ENAMEX
> NEWS I?M < ENAMEX TYPE=?PERSON?>KIMBERLY
KENNEDY </ENAMEX> THOSE STORIES IN A MO-
MENT BUT FIRST </TEXT></DOC >
We need a better performing recognizer, but that may
be hard. Instead we might be able to use other informa-
tion from the speech recognizer to overcome this prob-
lem. We did not have confidence scores for the words in
the ASR output. If we had had that information, or if we
were able to obtain information about which words were
OOV, we could possibly index all words with low confi-
dence scores or all OOV words by their Soundex codes.
Or else, one could normalize all words in the ASR out-
put, that are not part of the regular English vocabulary by
their Soundex codes.
6.2 Other ways of grouping entities
Another direction of research to pursue is the way in
which approximate string matching is used to compare
documents. The way we used approximate string match-
ing in this paper was fairly simple. However, it loses
out on some names that ought to go together particularly
when two names differ in their first alphabet - for example
Katherine and Catherine. The Soundex codes are k365
and c365 respectively. This is by virtue of the nature of
the Soundex code of word.
There are other ways to compute the similarity be-
tween two documents like the Levenshtein distance or
edit distance which is a measure of the number of string
edit operations required to convert one string to the other.
The words Katherine and Catherine have an edit distance
of 1. Given two documents D
1
and D
2
, we can compute
the distance between them by computing the distance be-
tween all pairs of names that occur in the two documents,
and using the distances to group entities and finally to find
the similarity of the two documents. However this means
that each entity in D
1
has to be compared to all entities in
D
1
and D
2
. Besides, this method brings with it the ques-
tion of how to use the distances between the names so as
to group together similar names. This method is probably
a good direction for future research, because the Leven-
shtein distance could possibly be a better string matching
technique. Another plausible strategy would be to use the
edit-distance of the Soundex codes of the names, when
comparing documents. Katherine and Catherine would
have a distance of 1 in this case too.
Using cross document coreference resolution tech-
niques to find equivalence classes of entities would be yet
another alternative approach. In Cross document corefer-
ence, two mentions of the same name, may or may not be
included in the same group depending on whether or not
the context of the two mentions is the same or is different.
7 Conclusions and Future Directions
In this paper we highlighted an important problem that
occurs with names in ASR text. We showed how a name
may be spelt differently by humans. In ASR the same
name had many more different spellings.
We proposed a simple indexing strategy for names,
wherein a name was indexed by its Soundex code. We
found that our strategy did not work for ASR, but the
problem was not with the approach, but because we could
not do a good job of identifying names in ASR text.If
we could detect names with reasonable accuracy in ASR
text we should be able to achieve reasonable improve-
ment. We did not have a named entity recognizer that
performed well on ASR text. We therefore verified our
idea on news-wire text, which is grammatical, well punc-
tuated text. In the news-wire domain, in spite of there be-
ing reasonable consistency in spellings of names, we get
about 10% improvement in minimum cost, and a consis-
tent improvement at all points in the ROC curve. Hence,
a simple technique like Soundex served as a useful nor-
malization technique for names. We proposed alternative
mechanisms that could be applied to ASR text, wherein
all OOV words could be normalized by their Soundex
codes. We also outlined further directions for research in
the way that approximate string matching may be used.
We think the general results of past works that has con-
sidered the problems due to ASR errors to be insignificant
cannot be assumed to transfer across to other problems.
There will arise situations when this problem is material
and research needs to be done in this direction.
8 Acknowledgements
This work was supported in part by the Cen-
ter for Intelligent Information Retrieval, in part by
SPAWARSYSCEN-SD grant number N66001-02-1-
8903. Any opinions, findings and conclusions or recom-
mendations expressed in this material are the author(s)
and do not necessarily reflect those of the sponsor.
References
James Allan. 2002. Detecting and tracking topics in
broadcast news,att speechdays 2002.
James Barnett, Steve Anderson, John Broglio, Mona
Singh, R. Hudson, and S. W. Kuo. 1997. Experiments
in spoken queries for document retrieval. In Proc. Eu-
rospeech ?97, pages 1323?1326, Rhodes, Greece.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34(1-3):211?231.
J. Fiscus, G. Doddington, J. Garofolo, and A. Martin.
1998. Nist?s 1998 topic detection and tracking eval-
uation.
John Fiscus. 2003. Personal communication.
J. C. French, A. L. Powell, and E. Schulman. 1997. Ap-
plications of approximate word matching in informa-
tion retrieval. In Proceedings of the Sixth International
Conference on Knowledge and Information Manage-
ment, pages 9?15, New York, NY. ACM Press.
J. Garofolo, G. Auzanne, and E. Voorhees. 2000. The
trec spoken document retrieval track: A success story.
R. Krovetz. 1993. Viewing Morphology as an Inference
Process,. In Proceedings of the Sixteenth Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 191?203.
2003. http://www.ldc.upenn.edu/tdt/.
David Miller, Richard Schwartz, Ralph Weischedel, and
Rebecca Stone. 2000. Named entity extraction from
broadcast news.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program.
J. Zobel and P. W. Dart. 1996. Phonetic string match-
ing: Lessons from information retrieval. In H.-P. Frei,
D. Harman, P. Scha?ble, and R. Wilkinson, editors,
Proceedings of the 19th International Conference on
Research and Development in Information Retrieval,
pages 166?172, Zurich, Switzerland. ACM Press.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1660?1669, Dublin, Ireland, August 23-29 2014.
Query-Focused Opinion Summarization for User-Generated Content
Lu Wang
1
Hema Raghavan
2
Claire Cardie
1
Vittorio Castelli
3
1
Department of Computer Science, Cornell University, Ithaca, NY 14853, USA
{luwang, cardie}@cs.cornell.edu
2
LinkedIn, CA, USA
hraghavan@linkedin.com
3
IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA
vittorio@us.ibm.com
Abstract
We present a submodular function-based framework for query-focused opinion summarization. Within our
framework, relevance ordering produced by a statistical ranker, and information coverage with respect to
topic distribution and diverse viewpoints are both encoded as submodular functions. Dispersion functions
are utilized to minimize the redundancy. We are the first to evaluate different metrics of text similarity for
submodularity-based summarization methods. By experimenting on community QA and blog summariza-
tion, we show that our system outperforms state-of-the-art approaches in both automatic evaluation and
human evaluation. A human evaluation task is conducted on Amazon Mechanical Turk with scale, and
shows that our systems are able to generate summaries of high overall quality and information diversity.
1 Introduction
Social media forums, such as social networks, blogs, newsgroups, and community question answering
(QA), offer avenues for people to express their opinions as well collect other people?s thoughts on topics
as diverse as health, politics and software (Liu et al., 2008). However, digesting the large amount of
information in long threads on newsgroups, or even knowing which threads to pay attention to, can be
overwhelming. A text-based summary that highlights the diversity of opinions on a given topic can
lighten this information overload. In this work, we design a submodular function-based framework for
opinion summarization on community question answering and blog data.
Question: What is the long term effect of piracy on the music and film industry?
Best Answer: Rising costs for movies and music. ... If they sell less, they need to raise the price to make up for what they lost. The
other thing will be music and movies with less quality. ...
Other Answers:
Ans1: Its bad... really bad. (Just watch this movie and you will find out ... Piracy causes rappers to appear on your computer).
Ans2: By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies.
If they can?t protect their copyrights, they can?t continue to do business. ...
Ans4: It is forcing them to rework their business model, which is a good thing. In short, I don?t think the music industry in particular
will ever enjoy the huge profits of the 90?s. ...
Ans6: Please-People in those businesses make millions of dollars as it is!! I don?t think piracy hurts them at all!!!
Figure 1: Example discussion on Yahoo! Answers. Besides the best answer, other answers also contain
relevant information (in italics). For example, the sentence in blue has a contrasting viewpoint compared
to the other answers.
Opinion summarization has previously been applied to restricted domains, such as product reviews (Hu
and Liu, 2004; Lerman et al., 2009) and news (Stoyanov and Cardie, 2006), where the output summary
is either presented in a structured way with respect to each aspect of the product or organized along
contrastive viewpoints. Unlike those works, we address user generated online data: community QA and
blogs. These forums use a substantially less formal language than news articles, and at the same time
address a much broader spectrum of topics than product reviews. As a result, they present new challenges
for automatic summarization. For example, Figure 1 illustrates a sample question from Yahoo! Answers
1
along with the answers from different users. The question receives more than one answer, and one of
them is selected as the ?best answer? by the asker or other participants. In general, answers from other
users also provide relevant information. While community QA successfully pools rich knowledge from
the wisdom of the crowd, users might need to seine through numerous posts to extract the information
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://answers.yahoo.com/
1660
they need. Hence, it would be beneficial to summarize answers automatically and present the summaries
to users who ask similar questions in the future. In this work, we aim to return a summary that encapsu-
lates different perspectives for a given opinion question and a set of relevant answers or documents.
In our work we assume that there is a central topic (or query) on which a user is seeking diverse opin-
ions. We predict query-relevance through automatically learned statistical rankers. Our ranking function
not only aims to find sentences that are on the topic of the query but also ones that are ?opinionated?
through the use of several features that indicate subjectivity and sentiment. The relevance score is en-
coded in a submodular function. Diversity is accounted for by a dispersion function that maximizes the
pairwise distance between the pairs of sentences selected.
Our chief contributions are:
(1) We develop a submodular function-based framework for query-focused opinion summarization. To
the best of our knowledge, this is the first time that submodular functions have been used to support
opinion summarization. We test our framework on two tasks: summarizing opinionated sentences in
community QA (Yahoo! Answers) and blogs (TAC-2008 corpus). Human evaluation using Amazon Me-
chanical Turk shows that our system generates the best summary 57.1% of the time. On the other hand,
the best answer picked by Yahoo! users is chosen only 31.9% of the time. We also obtain significant
higher Pyramid F1 score on the blog task as compared to the system of Lin and Bilmes (2011).
(2) Within our summarization framework, the statistically learned sentence relevance is included as part
of our objective function, whereas previous work on submodular summarization (Lin and Bilmes, 2011)
only uses ngram overlap for query relevance. Additionally, we use Latent Dirichlet Allocation (Blei et
al., 2003) to model the topic structure of the sentences, and induce clusterings according to the learned
topics. Therefore, our system is capable of generating summaries with broader topic coverage.
(3) Furthermore, we are the first to study how different metrics for computing text similarity or dis-
similarity affect the quality of submodularity-based summarization methods. We show empirically that
lexical representation-based similarity, such as TFIDF scores, uniformly outperforms semantic similar-
ity computed with WordNet. Moreover, when measuring the summary diversity, topical representation
is marginally better than lexical representation, and both of them beats semantic representation.
2 Related Work
Our work falls in the realm of query-focused summarization, where a user asks a question and the sys-
tem generates a summary of the answers containing pertinent and diverse information. A wide range
of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Car-
bonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model
over queries and documents (Daum?e and Marcu, 2006). Most work only implicitly penalizes summary
redundancy, e.g. by downweighting the importance of words that are already selected.
Encouraging diversity of a summary has recently been addressed through submodular functions, which
have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al.,
2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the
query information (when available) or else use simple ngram matching between the query and sentences.
In contrast, we propose to optimize an objective function that addresses both relevance and diversity.
Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004;
Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editori-
als (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a struc-
tured way based on product features or contrastive standpoints. Our work is more related to opinion
summarization on user-generated content, such as community QA. Liu et al. (2008) manually construct
taxonomies for questions in community QA. Summaries are generated by clustering sentences according
to their polarity based on a small dictionary. Tomasoni and Huang (2010) introduce coverage and quality
constraints on the sentences, and utilize an integer linear programming framework to select sentences.
3 Submodular Opinion Summarization
In this section, we describe how query-focused opinion summarization can be addressed by submodular
functions combined with dispersion functions. We first define our problem. Then we introduce the
1661
Basic Features Sentiment Features
- answer position in all answers/sentence position in blog - number/portion of sentiment words from a lexicon (Section 3.2)
- length of the answer/sentence - if contains sentiment words with the same polarity as
- length is less than 5 words sentiment words in query
Query-Sentence Overlap Features Query-Independent Features
- unigram/bigram TF/TFIDF similarity with query - unigram/bigram TFIDF similarity with cluster centroid
- number of key phrases in the query that appear in the - sumBasic score (Nenkova and Vanderwende, 2005)
sentence. A model similar to that described in - number of topic signature words (Lin and Hovy, 2000)
(Luo et al., 2013) was applied to detect key phrases. - JS divergence with cluster
Table 1: Features used for candidate ranking. We use them for ranking answers in both community QA
and blogs.
components of our objective function (Sections 3.1?3.3). The full objective function is presented in
Section 3.4. Lastly, we describe a greedy algorithm with constant factor approximation to the optimal
solution for generating summaries (Section 3.5).
A set of documents or answers to be summarized are first split into a set of individual sentences
V = {s
1
, ? ? ? , s
n
}. Our problem is to select a subset S ? V that maximizes a given objective function
f : 2
V
? R within a length constraint: S
?
= argmax
S?V
f(S), subject to | S |? c. | S | is the length of
the summary S, and c is the length limit.
Definition 1 A function f : 2
V
? R is submodular iff for all s ? V and every S ? S
?
? V , it satisfies
f(S ? {s})? f(S) ? f(S
?
? {s})? f(S
?
).
Previous submodularity-based summarization work assumes this diminishing return property makes
submodular functions a natural fit for summarization and achieves state-of-the-art results on various
datasets. In this paper, we follow the same assumption and work with non-decreasing submodular func-
tions. Nevertheless, they have limitations, one of which is that functions well suited to modeling diversity
are not submodular. Recently, Dasgupta et al. (2013) proved that diversity can nonetheless be encoded
in well-designed dispersion functions which still maintain a constant factor approximation when solved
by a greedy algorithm.
Based on these considerations, we propose an objective function f(S) mainly considering three as-
pects: relevance (Section 3.1), coverage (Section 3.2), and non-redundancy (Section 3.3). Relevance
and coverage are encoded in a non-decreasing submodular function, and non-redundancy is enforced by
maximizing the dispersion function.
3.1 Relevance Function
We first utilize statistical rankers to produce a preference ordering of the candidate answers or sentences.
We choose ListNet (Cao et al., 2007), which has been shown to be effective in many information retrieval
tasks, as our ranker. We use the implementation from Ranklib (Dang, 2011).
Features used in the ranking algorithm are summarized in Table 1. All features are normalized by
standardization. Due to the length limit, we cannot provide the full results on feature evaluation. Never-
theless, we find that ranking candidates by TFIDF similarity or key phrases overlapping with the query
can produce comparable results with using the full feature set (see Section 5).
We take the ranks output by the ranker, and define the relevance of the current summary S as: r(S) =
?
|S|
i
?
rank
?1
i
, where rank
i
is the rank of sentence s
i
in V . For QA answer ranking, sentences from the
same answer have the same ranking. The function r(S) is our first submodular function.
3.2 Coverage Functions
Topic Coverage. This function is designed to capture the idea that a comprehensive opinion sum-
mary should provide thoughts on distinct aspects. Topic models such as Latent Dirichlet Allocation
(LDA) (Blei et al., 2003) and its variants are able to discover hidden topics or aspects of document col-
lections, and thus afford a natural way to cluster texts according to their topics. Recent work (Xie and
Xing, 2013) shows the effectiveness of utilizing topic models for newsgroup document clustering. We
first learn an LDA model from the data, and treat each topic as a cluster. We estimate a sentence-topic
distribution
~
? for each sentence, and assign the sentence to the cluster k corresponding to the mode of the
distribution (i.e., k = argmax
i
?
i
). This naive approach produces comparable clustering performance to
the state-of-the-art according to (Xie and Xing, 2013). T is defined as the clustering induced by our algo-
rithm on the set V . The topic coverage of the current summary S is defined as t(S) =
?
T?T
?
|S ? T |.
1662
From the concavity of the square root it follows that sets S with uniform coverages of topics are preferred
to sets with skewed coverage.
Authorship Coverage. This term encourages the summarization algorithm to select sentences from
different authors. Let A be the clustering induced by the sentence to author relation. In community
QA, sentences from the answers given by the same user belong to the same cluster. Similarly, sentences
from blogs with the same author are in the same cluster. The authorship score is defined as a(S) =
?
A?A
?
|S ?A|.
Polarity Coverage. The polarity score encourages the selection of summaries that cover both positive
and negative opinions. We categorize each sentence simply by counting the number of polarized words
given by our lexicon. A sentence belongs to a positive cluster if it has more positive words than negative
ones, and vice versa. If any negator co-occurs with a sentiment word (e.g. within a window of size 5),
the sentiment is reversed.
2
The polarity clustering P thus have two clusters corresponding to positive
and negative opinions. The score is defined as p(S) =
?
P?P
?
| S ? P |. Our lexicon consists of
MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and
Sebastiani, 2006). Words with conflicting sentiments from different lexicons are removed.
Content Coverage. Similarly to Lin and Bilmes (2011) and Dasgupta et al. (2013), we use the following
function to measure content coverage of the current summary S: c(S) =
?
v?V
min(cov(v, S), ? ?
cov(v, V )), where cov(v, S) =
?
u?S
sim(v, u). We experiment with two types of similarity functions.
One is a Cosine TFIDF similarity score. The other is a WordNet-based semantic similarity score between
pairwise dependency relations from two sentences (Dasgupta et al., 2013). Specifically, sim
Sem
(v, u) =
?
rel
i
?v,rel
j
?u
WN(a
i
, a
j
) ?WN(b
i
, b
j
), where rel
i
= (a
i
, b
i
), rel
j
= (a
j
, b
j
), WN(w
i
, w
j
) is the
shortest path length. All scores are scaled onto [0, 1].
3.3 Dispersion Function
Summaries should contain as little redundant information as possible. We achieve this by adding an
additional term to the objective function, encoded by a dispersion function. Given a set of sentences
S, a complete graph is constructed with each sentence in S as a node. The weight of each edge (u, v)
is their dissimilarity d
?
(u, v). Then the distance between any pair of u and v, d(u, v), is defined as the
total weight of the shortest path connecting u and v.
3
We experiment with two forms of dispersion
function (Dasgupta et al., 2013): (1) h
sum
=
?
u,v?V,u6=v
d(u, v), and (2) h
min
= min
u,v?V,u6=v
d(u, v).
Then we need to define the dissimilarity function d
?
(?, ?). There are different ways to measure the
dissimilarity between sentences (Mihalcea et al., 2006; Agirre et al., 2012). In this work, we experiment
with three types of dissimilarity functions.
Lexical Dissimilarity. This function is based on the well-known Cosine similarity score using TFIDF
weights. Let sim
tfidf
(u, v) be the Cosine similarity between u and v, then we have d
?
Lex
(u, v) =
1? sim
tfidf
(u, v).
Semantic Dissimilarity. This function is based on the semantic meaning embedded in the dependency
relations. d
?
Sem
(u, v) = 1 ? sim
Sem
(v, u), where sim
Sem
(v, u) is the semantic similarity used in
content coverage measurement in Section 3.2.
Topical Dissimilarity. We propose a novel dissimilarity measure based on topic models. Celikyilmaz
et al. (2010) show that estimating the similarity between query and passages by using topic structures
can help improve the retrieval performance. As discussed in the topic coverage in Section 3.2, each
sentence is represented by its sentence-topic distributions estimated by LDA. For candidate sentence u
and v, let their topic distributions be P
u
and P
v
. Then the dissimilarity between u and v can be defined
as: d
?
Topic
(u, v) = JSD(P
u
||P
v
) =
1
2
(
?
i
P
u
(i) log
2
P
u
(i)
P
a
(i)
+
?
i
P
v
(i) log
2
P
v
(i)
P
a
(i)
) where P
a
(i) =
1
2
(P
u
(i) + P
v
(i)).
3.4 Full Objective Function
The objective function takes the interpolation of the submodular functions and dispersion function:
F(S) = r(S) + ?t(S) + ?a(S) + ?p(S) + ?c(S) + ?h(S). (1)
2
There exists a large amount of work on determining the polarity of a sentence (Pang and Lee, 2008) which can be employed
for polarity clustering in this work. We decide to focus on summarization, and estimate sentence polarity through sentiment
word summation (Yu and Hatzivassiloglou, 2003), though we do not distinguish different sentiment words.
3
This definition of distance is used to produce theoretical guarantees for the greedy algorithm described in Section 3.5.
1663
The coefficients ?, ?, ?, ?, ? are non-negative real numbers and can be tuned on a development set.
4
Notice that each summand except h(S) is a non-decreasing, non-negative, and submodular function,
and summation preserves monotonicity, non-negativity, and submodularity. Dispersion function h(s) is
either h
sum
or h
min
as introduced previously.
3.5 Summary Generation via Greedy Algorithm
Generating the summary that maximizes our objective function in Equation 1 is NP-hard (Chandra and
Halld?orsson, 1996). We choose to use a greedy algorithm that guarantees to obtain a constant factor ap-
proximation to the optimal solution (Nemhauser et al., 1978; Dasgupta et al., 2013). Concretely, starting
with an empty set, for each iteration, we add a new sentence so that the current summary achieves the
maximum value of the objective function. In addition to the theoretical guarantee, existing work (Mc-
Donald, 2007) has empirically shown that classical greedy algorithms usually works near-optimally.
4 Experimental Setup
4.1 Opinion Question Identification
We first build a classifier to automatically detect opinion oriented questions in Community QA; questions
in the blog dataset are all opinionated. Our opinion question classifier is trained on two opinion question
datasets: (1) the first, from Li et al. (2008a), contains 646 opinionated and 332 objective questions; (2)
the second dataset, from Amiri et al. (2013), consists of 317 implicit opinion questions, such as ?What
can you do to help environment??, and 317 objective questions. We train a RBF kernel based SVM
classifier to identify opinion questions, which achieves F1 scores of 0.79 and 0.80 on the two datasets
when evaluated using 10-fold cross-validation (the best F1 scores reported are 0.75 and 0.79).
4.2 Datasets
Community QA Summarization: Yahoo! Answers. We use the Yahoo! Answers dataset from Yahoo!
Webscope
TM
program,
5
which contains 3,895,407 questions. We first run the opinion question classifier
to identify the opinion questions. For summarization purpose, we require each question having at least 5
answers, with the average length of answers larger than 20 words. This results in 130,609 questions.
To make a compelling task, we reserve questions with an average length of answers larger than 50
words as our test set for both ranking and summarization; all the other questions are used for training. As
a result, we have 92,109 questions in the training set for learning the statistical ranker, and 38,500 in the
test set. The category distribution of training and test questions (Yahoo! Answers organizes the questions
into predefined categories) are similar. 10,000 questions from the training set are further reserved as the
development set. Each question in the Yahoo! Answers dataset has a user-voted best answer. These best
answers are used to train the statistical ranker that predicts relevance. Separate topic models are learned
for each category, where the category tag is provided by Yahoo! Answer.
Blog Summarization: TAC 2008. We use the TAC 2008 corpus (Dang, 2008), which consists of 25
topics. 23 of them are provided with human labeled nuggets, which TAC used in human evaluation. TAC
also provides snippets (i.e., sentences) that are frequently retrieved by participant systems or identified
as relevant by human annotators. We do not assume those snippets are known to any of our systems.
4.3 Comparisons
For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and
(2) the systems from Lin and Bilmes (2011) with and without query information. The sentence clustering
process in Lin and Bilmes (2011) is done by using CLUTO (Karypis, 2003). For the implementation of
systems in Lin and Bilmes (2011) and Dasgupta et al. (2013), we always use the parameters reported to
have the best performance in their work.
For cQA summarization, we use the best answer voted by the user as a baseline. Note that this is a
strong baseline since all the other systems are unaware of which answer is the best. For blog summa-
rization, we have three additional baselines ? the best systems in TAC 2008 (Kim et al., 2008; Li et al.,
2008b), top sentences returned by our ranker, a baseline produced by TFIDF similarity and a lexicon
4
The values for the coefficients are 5.0, 1.0, 10.0, 5.0, 10.0 for ?, ?, ?, ?, ?, respectively, as tuned on the development set.
5
http://sandbox.yahoo.com/
1664
(henceforth called TFIDF+Lexicon). In TFIDF+Lexicon, sentences are ranked by the TFIDF similar-
ity with the query, and then sentences with sentiment words are selected in sequence. This baseline aims
to show the performance when we only have access to lexicons without using a learning algorithm.
5 Results
5.1 Evaluating the Ranker
We evaluate our ranker (described in Section 3.1) on the task of best answer prediction. Table 2 compares
the average precision and mean reciprocal rank (MRR) of our method to those of three baselines, (1)
where answers are ranked randomly (Baseline (Random)), (2) by length (Baseline (Length)), and (3)
by Jensen Shannon Divergence (JSD) with all answers. We expect that the best answer is the one that
covers the most information, which is likely to have a smaller JSD. Therefore, we use JSD to rank
answers in the ascending order. Table 2 manifests that our ranker outperforms all the other methods.
Baseline (Random) Baseline (Length) JSD Ranker (ListNet)
Avg Precision 0.1305 0.2834 0.4000 0.5336
MRR 0.3403 0.4889 0.5909 0.6496
Table 2: Performance for best answer prediction. Our ranker outperforms the three baselines.
5.2 Community QA Summarization
Automatic Evaluation. Since human written abstracts are not available for the Yahoo! Answers dataset,
we adopt the Jensen-Shannon divergence (JSD) to measure the summary quality. Intuitively, a smaller
JSD implies that the summary covers more of the content in the answer set. Louis and Nenkova (2013)
report that JSD has a strong negative correlation (Spearman correlation = ?0.737) with the overall
summary quality for multi-document summarization (MDS) on news articles and blogs. Our task is
similar to MDS. Meanwhile, the average JSD of the best answers in our test set is smaller than that of
the other answers (0.39 vs. 0.49), with an average length of 103 words compared with 67 words for the
other answers. Also, on the blog task (Section 5.3), the top two systems by JSD also have the top two
ROUGE scores (a common metric for summarization evaluation when human-constructed summaries
are available). Thus, we conjecture that JSD is a good metric for community QA summaries.
Table 3 (left) shows that our system using a content coverage function based on Cosine using TFIDF
weights, and a dispersion function (h
sum
) based on lexicon dissimilarity and 100 topics, outperforms all
of the compared approaches (paired-t test, p < 0.05). The topic number is tuned on the development set,
and we find that varying the number of topics does not impact performance too much. Meanwhile, both
our system and Dasgupta et al. (2013) produce better JSD scores than the two variants of the Lin and
Bilmes (2011) system, which implies the effectiveness of the dispersion function. We further examine the
effectiveness of each component that contributes to the objective function (Section 3.4), and the results
are shown in Table 3 (right).
Length
100 200
Best answer 0.3858 -
Lin and Bilmes (2011) 0.3398 0.2008
Lin and Bilmes (2011) + q 0.3379 0.1988
Dasgupta et al. (2013) 0.3316 0.1939
Our system 0.3017 0.1758
JSD
100
JSD
200
Rel(evance) 0.3424 0.2053
Rel + Aut(hor) 0.3375 0.2040
Rel + Aut + TM (Topic Models) 0.3366 0.2033
Rel + Aut + TM + Pol(arity) 0.3309 0.1983
Rel + Aut + TM + Pol + Cont(ent Coverage) 0.3102 0.1851
Rel + Aut + TM + Pol + Cont + Disp(ersion) 0.3017 0.1758
Table 3: [Left] Summaries evaluated by Jensen-Shannon divergence (JSD) on Yahoo Answer for sum-
maries of 100 words and 200 words. The average length of the best answer is 102.70. [Right] Value
addition of each component in the objective function. The JSD on each line is statistically significantly
lower than the JSD on the previous (? = 0.05).
Human Evaluation. Human evaluation for Yahoo! Answers is carried out on Amazon Mechanical Turk
6
with carefully designed tasks (or ?HITs?). Turkers are presented summaries from different systems in a
random order, and asked to provide two rankings, one for overall quality and the other for information
diversity. We indicate that informativeness and non-redundancy are desirable for quality; however, Turk-
ers are allowed to consider other desiderata, such as coherence or responsiveness, and write down those
when they submit the answers. Here we believe that ranking the summaries is easier than evaluating each
summary in isolation (Lerman et al., 2009).
6
https://www.mturk.com/mturk/
1665
We randomly select 100 questions from our test set, each of which is evaluated by 4 distinct Turkers
located in United States. 40 HITs are thus created, each containing 10 different questions. Four system
summaries (best answer, Dasgupta et al. (2013), and our system with 100 and 200 words respectively) are
displayed along with one noisy summary (i.e. irrelevant to the question) per question in random order.
7
We reject Turkers? HITs if they rank the noisy summary higher than any other. Two duplicate questions
are added to test intra-annotator agreement. We reject HITs if Turkers produced inconsistent rankings
for both duplicate questions. A total of 137 submissions of which 40 HITs pass the above quality filters.
Turkers of all accepted submissions report themselves as native English speakers. An inter-rater agree-
ment of Fleiss? ? of 0.28 (fair agreement (Landis and Koch, 1977)) is computed for quality ranking and
? is 0.43 (moderate agreement) for diversity ranking. Table 4 shows the percentage of times a particular
method is picked as the best summary, and the macro-/micro-average rank of a method, for both overall
quality and information diversity. Macro-average is computed by first averaging the ranks per question
and then averaging across all questions.
For overall quality, our system with a 200 word limit is selected as the best in 44.6% of the evaluations.
It outperforms the best answer (31.9%) significantly, which suggests that our system summary covers rel-
evant information that is not contained in the best answer. Our system with a length constraint of 100
words is chosen as the best for quality 12.5% times while that of Dasgupta et al. (2013) is chosen 11.0%
of the time. Our system is also voted as the best summary for diversity in 78.7% of the evaluations. More
interestingly, both of our systems, with 100 words and 200 words, outperform the best answer and Das-
gupta et al. (2013) for average ranking (both overall quality and information diversity) significantly by
using Wilcoxon signed-rank test (p < 0.05). When we check the reasons given by Turkers, we found that
people usually prefer our summaries due to ?helpful suggestions that covered many options? or being
?balanced with different opinions?. When Turks prefer the best answers, they mostly stress on coherence
and responsiveness. Sample summaries from all the systems are displayed in Figure 2.
Length of Summary Overall Quality Information Diversity
% Average Rank % Average Rank
Best Macro Micro Best Macro Micro
Best answer 102.70 31.9% 2.68 2.69 9.6% 3.27 3.29
Dasgupta et al. (2013)
100
11.0% 2.84 2.83 5.0% 2.95 2.94
Our system 12.5% 2.50
?
2.50
?
6.7% 2.43
?
2.43
?
Our system 200 44.6% 1.98
?
1.98
?
78.7% 1.35
?
1.34
?
Table 4: Human evaluation on Yahoo! Answer Data. Boldface implies statistically significance com-
pared to other results in the same columns using paired-t test. Both of our systems are ranked higher
(i.e. numbers in bold with
?
) than the best answers voted by Yahoo! users and system summaries from
Dasgupta et al. (2013).
Question: What is the long term effect of piracy on the music and film industry?
Dasgupta et al. (2013) (Qty Rank=2.75 Div. Rank=2.5):
?In short, I don?t think the music industry in particular will ever enjoy the huge profits of the 90?s.
?Please-People in those businesses make millions of dollars as it is !! I don?t think piracy hurts them at all !!!
?The other thing will be music and movies with less quality.
?Its a big gray area, I dont see anything wrong with burning a mix cd or a cd for a friend so long as youre not selling them for profit.
?By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies.
Our system (100 words) (Qty Rank=2.25 Div. Rank=2.25):
?Rising costs for movies and music. The other thing will be music and movies with less quality.
?Now, with piracy, there isn?t the willingness to take chances.
?But it?s also like the person put the effort into it and they aren?t getting paid. It?s a big gray area, I don?t see anything wrong with burning a mix cd
or a cd for a friend so long as you?re not selling them for profit.
?It is forcing them to rework their business model, which is a good thing.
Our system (200 words) (Qty. Rank=2.25, Div Rank=1.25):
?Rising costs for movies and music. The other thing will be music and movies with less quality.
?Now, with piracy, there isn?t the willingness to take chances. American Idol is the result of this. .... The real problem here is that the mainstream
music will become even tighter. Record labels will not won?t to go far from what is currently like by the majority.
?I hate when people who have billions of dollars whine about not having more money. But it?s also like the person put the effort into it and they
aren?t getting paid ... I don?t see anything wrong with burning a mix cd or a cd for a friend ....
?It is forcing them to rework their business model, which is a good thing.
?By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies.
Figure 2: Sample summaries from Dasgupta et al. (2013), and our systems (100 words and 200 words).
Sentences from separate bullets (?) are partial answers from different users.
7
Note that we aim to compare results with the gold-standard best answers of about 100 words. The evaluation of the
200-word summaries is provided only as an additional data-point.
1666
5.3 Blog Summarization
Automatic Evaluation. We use the ROUGE (Lin and Hovy, 2003) software with standard options to
automatically evaluate summaries with reference to the human labeled nuggets as those are available
for this task. ROUGE-2 measures bigram overlap and ROUGE-SU4 measures the overlap of unigram
and skip-bigram separated by up to four words. We use the ranker trained on Yahoo! data to produce
relevance ordering, and adopt the system parameters from Section 5.2. Table 5 (left) shows that our
system outperforms the best system in TAC?08 with highest ROUGE-2 score (Kim et al., 2008), the two
baselines (TFIDF+Lexicon, and our ranker), Lin and Bilmes (2011), and Dasgupta et al. (2013).
ROUGE-2 ROUGE-SU4 JSD
Best system in TAC?08 0.2923 0.3766 0.3286
TFIDF + Lexicon 0.3069 0.3876 0.2429
Ranker (ListNet) 0.3200 0.3960 0.2293
Lin and Bilmes (2011) 0.2732 0.3582 0.2330
Lin and Bilmes (2011) + q 0.2852 0.3700 0.2349
Dasgupta et al. (2013) 0.2618 0.3500 0.2370
Our system 0.3234 0.3978 0.2258
Pyramid F-score
Best system in TAC?08 0.2225
Lin and Bilmes (2011) 0.2790
Our system 0.3620
Table 5: Results on TAC?08 dataset. [Left] Our system has significant better ROUGE scores than all
the other systems except our ranker (paired-t test, p < 0.05). We also achieve the best JS divergence.
[Right] Human evaluation with Pyramid F-score. Our system significantly outperforms the others.
Human Evaluation. For human evaluation, we use the standard Pyramid F-score used in the TAC?08
opinion summarization track with ? = 3 (Dang, 2008). In the TAC task, systems are allowed to return up
to 7,000 non-white characters for each question. Since the TAC metric favors recall we do not produce
summaries shorter than 7,000 characters. We ask two human judges to evaluate our system along with
the one that got the highest Pyramid F-score in the TAC?08 and Lin and Bilmes (2011). Cohen?s ? for
inter-annotator agreement is 0.68 (substantial). While we did not explicitly evaluate non-redundancy,
both of our judges report that our system summaries contain less redundant information.
5.4 Further Discussion
Yahoo! Answer
DISPERSION
sum
DISPERSION
min
DISSIMI Cont
tfidf
Cont
sem
Cont
tfidf
Cont
sem
Semantic 0.3143 0.324 3 0.3129 0.3232
Topical 0.3101 0.3202 0.3106 0.3209
Lexical 0.3017 0.3147 0.3071 0.3172
TAC 2008
DISPERSION
sum
DISPERSION
min
DISSIMI Cont
tfidf
Cont
sem
Cont
tfidf
Cont
sem
Semantic 0.2216 0.2169 0.2772 0.2579
Topical 0.2128 0.2090 0.3234 0.3056
Lexical 0.2167 0.2129 0.3117 0.3160
Table 6: Effect of different dispersion functions, content coverage, and dissimilarity metrics on our
system. [Left] JSD values for different combinations on Yahoo! data, using LDA with 100 topics.
All systems are significantly different from each other at significance level ? = 0.05. Systems using
summation of distances for dispersion function (h
sum
) uniformly outperform the ones using minimum
distance (h
min
). [Right] ROUGE scores of different choices for TAC 2008 data. All systems use LDA
with 40 topics. The parameters of our systems are adopted from the ones tuned on Yahoo! Answers.
Given that the text similarity metrics and dispersion functions play important roles in the framework,
we further study the effectiveness of different content coverage functions (Cosine using TFIDF vs. Se-
mantic), dispersion functions (h
sum
vs. h
min
), and dissimilarity metrics used in dispersion functions
(Semantic vs. Topical vs. Lexical). Results on Yahoo! Answer (Table 6 (left)) show that systems using
summation of distances for dispersion functions (h
sum
) uniformly outperform the ones using minimum
distance (h
min
). Meanwhile, Cosine using TFIDF is better at measuring content coverage than WordNet-
based semantic measurement, and this may due to the limited coverage of WordNet on verbs. This is also
true for dissimilarity metrics. Results on blog data (Table 6 (right)), however, show that using minimum
distance for dispersion produces better results. This indicates that optimal dispersion function varies by
genre. Topical-based dissimilarity also marginally outperforms the other two metrics in blog data.
6 Conclusion
We propose a submodular function-based opinion summarization framework. Tested on community QA
and blog summarization, our approach outperforms state-of-the-art methods that are also based on sub-
modularity in both automatic evaluation and human evaluation. Our framework is capable of including
statistically learned sentence relevance and encouraging the summary to cover diverse topics. We also
study different metrics on text similarity estimation and their effect on summarization.
1667
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on seman-
tic textual similarity. In Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 385?393, Montr?eal, Canada, 7-8 June. Association for Computational Linguistics.
Hadi Amiri, Zheng-Jun Zha, and Tat-Seng Chua. 2013. A pattern matching based model for implicit opinion
question identification. In AAAI. AAAI Press.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res.,
3:993?1022, March.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: From pairwise approach
to listwise approach. In Proceedings of the 24th International Conference on Machine Learning, ICML ?07,
pages 129?136, New York, NY, USA. ACM.
Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ?98, pages 335?336, New York, NY, USA. ACM.
Asli Celikyilmaz, Dilek Hakkani-Tur, and Gokhan Tur. 2010. Lda based similarity modeling for question answer-
ing. In Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, SS ?10, pages 1?9, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Barun Chandra and Magn?us M. Halld?orsson. 1996. Facility dispersion and remote subgraphs. In Proceedings
of the 5th Scandinavian Workshop on Algorithm Theory, SWAT ?96, pages 53?65, London, UK, UK. Springer-
Verlag.
Hoa Tran Dang. 2008. Overview of the tac 2008 opinion question answering and summarization tasks. In Proc.
TAC 2008.
Van Dang. 2011. RankLib. http://www.cs.umass.edu/?vdang/ranklib.html.
Anirban Dasgupta, Ravi Kumar, and Sujith Ravi. 2013. Summarization through submodularity and dispersion.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1014?1022, Sofia, Bulgaria, August. Association for Computational Linguistics.
Hal Daum?e, III and Daniel Marcu. 2006. Bayesian query-focused summarization. In Proceedings of the 21st
International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for
Computational Linguistics, ACL-44, pages 305?312, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion
mining. In In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06, pages
417?422.
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ?04, pages 168?177, New
York, NY, USA. ACM.
George Karypis. 2003. CLUTO - a clustering toolkit. Technical Report #02-017, November.
Hyun Duk Kim, Dae Hoon Park, V.G.Vinod Vydiswaran, and ChengXiang Zhai. 2008. Opinion summarization
using entity features and probabilistic sentence coherence optimization: Uiuc at tac 2008 opinion summarization
pilot. In Proc. TAC 2008.
J R Landis and G G Koch. 1977. The measurement of observer agreement for categorical data. Biometrics,
33(1):159?174.
Kevin Lerman, Sasha Blair-Goldensohn, and Ryan McDonald. 2009. Sentiment summarization: Evaluating and
learning user preferences. In Proceedings of the 12th Conference of the European Chapter of the Association for
Computational Linguistics, EACL ?09, pages 514?522, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Baoli Li, Yandong Liu, and Eugene Agichtein. 2008a. Cocqa: Co-training over questions and answers with an
application to predicting question subjectivity orientation. In EMNLP, pages 937?946.
Wenjie Li, You Ouyang, Yi Hu, and Furu Wei. 2008b. Polyu at tac 2008. In Proc. TAC 2008.
1668
Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -
Volume 1, HLT ?11, pages 510?520, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization.
COLING ?00, pages 495?501, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1, pages 71?78.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin, Dingyi Han, and Yong Yu. 2008. Understanding and sum-
marizing answers in community-based question answering services. In Proceedings of the 22Nd International
Conference on Computational Linguistics - Volume 1, COLING ?08, pages 497?504, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Annie Louis and Ani Nenkova. 2013. Automatically assessing machine summary content without a gold standard.
Comput. Linguist., 39(2):267?300, June.
Xiaoqiang Luo, Hema Raghavan, Vittorio Castelli, Sameer Maskey, and Radu Florian. 2013. Finding what matters
in questions. In HLT-NAACL, pages 878?887.
Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. ECIR?07,
pages 557?564, Berlin, Heidelberg. Springer-Verlag.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of
text semantic similarity. In Proceedings of the 21st National Conference on Artificial Intelligence - Volume 1,
AAAI?06, pages 775?780. AAAI Press.
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. 1978. An analysis of approximations for maximizing submod-
ular set functionsI. Mathematical Programming, 14(1):265?294, December.
Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-101.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju. 2010. Summarizing contrastive viewpoints in opinionated
text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP
?10, pages 66?76, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ruben Sipos, Pannaga Shivaswamy, and Thorsten Joachims. 2012. Large-margin learning of submodular summa-
rization models. EACL ?12, pages 224?233, Stroudsburg, PA, USA. Association for Computational Linguistics.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A
Computer Approach to Content Analysis. MIT Press, Cambridge, MA.
Veselin Stoyanov and Claire Cardie. 2006. Partially supervised coreference resolution for opinion summarization
through structured rule learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?06, pages 336?344, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Mattia Tomasoni and Minlie Huang. 2010. Metadata-aware measures for answer summarization in community
question answering. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 760?769, Stroudsburg, PA, USA. Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in
Natural Language Processing, HLT ?05, pages 347?354, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Pengtao Xie and Eric Xing. 2013. Integrating document clustering and topic modeling. In Proceedings of the
Twenty-Ninth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-13), pages 694?
703, Corvallis, Oregon. AUAI Press.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sentences. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
1669
Proceedings of NAACL-HLT 2013, pages 878?887,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Finding What Matters in Questions
Xiaoqiang Luo, Hema Raghavan, Vittorio Castelli, Sameer Maskey and Radu Florian
IBM T.J. Watson Research Center
1101 Kitchawan Road, Yorktown Heights, NY 10598
{xiaoluo,hraghav,vittorio,smaskey,raduf}@us.ibm.com
Abstract
In natural language question answering (QA)
systems, questions often contain terms and
phrases that are critically important for re-
trieving or finding answers from documents.
We present a learnable system that can ex-
tract and rank these terms and phrases (dubbed
mandatory matching phrases or MMPs), and
demonstrate their utility in a QA system on In-
ternet discussion forum data sets. The system
relies on deep syntactic and semantic analysis
of questions only and is independent of rele-
vant documents. Our proposed model can pre-
dict MMPs with high accuracy. When used in
a QA system features derived from the MMP
model improve performance significantly over
a state-of-the-art baseline. The final QA sys-
tem was the best performing system in the
DARPA BOLT-IR evaluation.
1 Introduction
In most question answering (QA) systems and
search engines term-weights are assigned in a con-
text independent fashion using simple TF-IDF like
models (Robertson and Walker, 1994; Ponte and
Croft, 1998). Even the more recent advances
in information retrieval techniques for query term
weighting (Bendersky et al, 2010; Bendersky, 2011)
typically rely on bag-of-words models and cor-
pus statistics, such as inverse-document-frequency
(IDF), to assign weights to terms in questions. While
such solutions may work for keyword queries of the
type common on search engines such as Google,
they do not exploit syntactic and semantic informa-
tion when it comes to well formed natural language
questions. In this paper we propose a new model
that identifies important terms and phrases in a natu-
ral language question, providing better query analy-
sis that ultimately leads to significant improvements
in a QA system.
To motivate the work presented here, consider the
query ?How does one apply for a New York day care
license??. A bag-of-words model would likely as-
sign a high score to ?New licenses for day care cen-
ters in York county, PA? because of high word over-
lap, but it does not answer the question, and also
the state is wrong. A matching component that uses
the phrases ?New York,? ?day care,? and ?license?
is likely to do better. However, a better matching
component will understand that in the context of this
query all three phrases ?New York,? ?day care? and
?license? are important, and that ?New York? needs
to modify ?day care.? A snippet that does not con-
tain1 these important phrases, is unlikely an answer.
We call these important phrases mandatory match-
ing phrases (MMPs).
In this paper, we explore deep syntactic and se-
mantic analyses of questions to determine and rank
MMPs. Unlike existing work (Zhao and Callan,
2010; Bendersky et al, 2010; Bendersky, 2011),
where term/concept weights are learned from a set
of questions and judged documents based on corpus-
based statistics, we annotate questions and build a
trainable system to select and score MMPs. This
model relies heavily on existing syntactic parsers
and semantic-oriented named-entity recognizers, but
does not need question answer pairs. This is espe-
1
?contain? here means semantic equivalence or entailment,
not necessarily the exact words or phrases.
878
cially attractive at the initial system-building stage
when no or little answer data is available.
The main contributions of this paper are: firstly,
we propose a framework to select and rank impor-
tant question phrases (MMPs) for question answer-
ing in Section 3. This framework seamlessly incor-
porates lexical, syntactic and semantic information,
resulting in an MMP prediction F-measure as high
as 88.6%. Secondly, we show that features derived
from identified MMPs improve significantly a rele-
vance classification model, in Section 4.2. Thirdly,
we show that using the improved relevance model
into our QA system results in a statistically signifi-
cant 5 point improvement in F-measure, in Section
5. This finding is further corroborated by the results
on the official 2012 BOLT IR (IR, 2012) task where
the combined system yielded the best performance
in the evaluation.
2 Related Work
Popular information retrieval systems like
BM25 (Robertson and Walker, 1994) and language
models (Ponte and Croft, 1998) use unsupervised
techniques based on corpus statistics for term
weighting. Many of these techniques are variants
of the one proposed by (Luhn, 1958). Recently,
several researchers have studied approaches for term
weighting using supervised learning techniques.
However, much of this research has focused on
information retrieval task rather than on question
answering problems of the nature addressed in
this paper. (Bendersky and Croft, 2008) restricted
themselves to predicting key noun phrases, which
is perhaps sufficient for a retrieval task. However,
for questions like ?Find comments about how
American hedge funds legally avoid taxes,? the verb
?avoid? is perhaps as important as the noun phrase
?American hedge funds? and ?taxes?. Works like
that of (Lease et al, 2009) and (Zhao and Callan,
2010) predict importance at the word level. While
word level importance is perhaps sufficient for
an IR task, predicting the importance of phrases,
especially those derived from a parse tree, gives
a much richer representation that might also be
useful for better question understanding and thus
generate more relevant answers. Both (Lease et al,
2009; Zhao and Callan, 2010) propose supervised
methods that learn from a large set of queries and
relevance judgments on their answers. While this is
possible in a TREC Ad-hoc-retrieval-like task, such
a large training corpus of question-answer pairs is
unavailable for most scenarios. (Monz, 2007) learns
term weights for the IR component of a question
answering task. His work unlike ours does not aim
to find the answers to the questions.
Most QA systems in the literature have dealt
with answering factoid questions, where the an-
swer is a noun phrase in response to questions of
the form ?Who,? ?Where,? ?When.? Most sys-
tems have a question analysis component that rep-
resents the question as syntactic relations in a parse
or as deep semantic relations in a handcrafted on-
tology (Hermjakob et al, 2000; Chu-carroll et al,
2003; Moldovan et al, 2003). In addition certain
systems (Bunescu and Huang, 2010) aim to find the
?focus? of the question, that is, the noun-phrases in
the question that would co-refer with answers. Ad-
ditionally, much past work has focused on finding
the lexical answer type (Pinchak, 2006; Li and Roth,
2002). Since these papers considered a small num-
ber of answer types, rules over the detected relations
and answer types could be applied to find the rel-
evant answer. However, since our system answers
non-factoid questions that can have answer of arbi-
trary types, we want to use as few rules as possible.
The MMPs therefore become a critical component
of our system, both for question analysis and for rel-
evance detection.
3 Question Data and MMP Model
To train the MMP model, we first create a set of
questions and label their MMPs. The labeled data
is then used to train a statistical model to predict
MMPs for new questions as discussed next.
3.1 Question Corpus
We use a subset of the DARPA BOLT corpus (see
Section 5.1) containing forum postings in English.
Four annotators use a search tool to explore this
document collection. They can perform keyword
searches and retrieve forum threads from which they
generate questions. The program participants de-
cided a basic set of question types that are out-of-
scope of the current research agenda. Accordingly,
879
annotators cannot generate questions (1) that require
reasoning or calculation over the data to compute
the answers; (2) that are vague or ambiguous; (3)
that can be broken into multiple disjoint questions;
(4) that are multiple choice questions; (5) that are
factoid questions?the kinds that have already been
well studied in TREC (Voorhees, 2004). Any other
kind of question is allowed. Two other annotators,
who have neither browsed the corpus nor generated
the questions, mark selected spans of the questions
into one of two categories?MMP-Must and MMP-
maybe. The annotation tool allows arbitrary spans
to be highlighted and the annotators are instructed to
select spans corresponding to the smallest semantic
units. The phrases that are very likely to appear con-
tiguously in a relevant answer are marked as MMP-
Must. Annotators can mark multiple spans per ques-
tion, but not overlapping spans. We generated 201
annotated questions using this process.
Figure 1 contains an example, where ?American,?
?hedge fund,? and ?legally avoid taxes? are required
elements to find answers and are thus marked as
MMP-Musts (signified by enclosing rectangles). We
purposely annotate MMPs at the word level and not
in the parse tree, because this requires minimal lin-
guistic knowledge. We do, however, employ an
automatic procedure to attach MMPs to parse tree
nodes when generating MMP training instances.
3.2 MMP Training
Questions annotated in Section 3.1 are first pro-
cessed by an information extraction (IE) pipeline
consisting of syntactic parsing, mention detection
and coreference resolution (Florian et al, 2004; Luo
et al, 2004; Luo and Zitouni, 2005). After IE, we
have access to the syntactic structure represented by
a parse tree and semantic information represented
by coreferenced mentions (including those of named
entities).
To take advantage of the availability of the syn-
tactic and semantic information, we first attach the
MMP annotations to parse tree nodes of a question,
and, if necessary, we augment the parse tree.
There are several reasons why we want to embed
the MMPs into a parse tree. First, many constituents
in parse trees correspond to important phrases we
want to capture, especially proper names. Second,
after an MMP is attached to a tree node, the problem
VP0
VB
Find comments
NNS
about
IN
SBAR
PP
VP
how
WRB NNP NN
legally taxes
NNS
funds
RB NNS
NP
S
avoidhedge
NP
WHADVP
VBP
 American
GPE
NP NP1
NP
2
Figure 1: MMPs are aligned with tree nodes: MMPs
are shown in rectangular boxes along with their aligned
nodes (with slanted labels); augmented parse tree nodes
(i.e., NP1, NP2) in dashed nodes. Dotted edges under
NP0 are the structure before the tree is augmented.
of predicting MMPs reduces to classifying parse tree
nodes, and syntactic information can be naturally
built into the MMP classifier. Lastly, and more im-
portantly, associating MMPs with tree nodes opens
the door to explore features derived from the syn-
tactic parse tree. For instance, it is easy to read
bilexical dependencies from a parse tree (provided
that head information is propagated); with MMPs
aligned with the parse tree, bilexical dependencies
can be ranked by examining whether or not an MMP
phrase is a head or a dependent. This way, not
only are the dependencies in a question captured, but
MMP scores or ranks can be propagated to depen-
dencies as well. We will discuss more how MMP
features are computed in Section 4.2.2.
Annotators can mark MMPs that are not perfectly
aligned with a tree node. Hence, care has to be taken
when generating MMP training instances. As an ex-
ample, In Figure 1, ?American? and ?hedge funds?
are marked as two separate MMPs, but the Penn-
Tree-style parse tree has a flat ?NP0? constituent
spanning directly on ?American hedge fund,? illus-
trated in Figure 1 as dotted edges.
To anchor MMPs in the parse tree, we augment
it by combining the IE output and the MMP anno-
tation. In the aforementioned example, ?American?
is a named mention with the entity type GPE (geo-
political entity) and there is no non-terminal node
spanning it: so, a new node ?NP1? is created; ?hedge
funds? is marked as an MMP: so, a second node
(?NP2?) is created to anchor it.
880
A training instance for building the MMP model
is defined as a span along with an MMP label. For
instance, ?hedge funds? in Figure 1 will generate a
positive training instance as ?(5,6), +1?, where
(5,6) is the span of ?hedge funds? in the question
sentence, and +1 signifies that it is a positive train-
ing instance. For the purpose of this paper we use
only binary labels, mapping all MMP-Must to +1
and MMP-Skip and MMP-Maybe to ?1.
Formally, we use the following procedure to gen-
erate training instances:
Algorithm 1 Pseudo code to generate MMP training
instances.
Input: An input question tree with detected men-
tions and marked MMPs
Output: A list of MMP training instances
1: Foreach mention m in the question
2: if no node spans m, and m does not cross bracket
3: Find lowest node N dominating m
4: Insert a child node of N that spans exactly m
5: Foreach mention p in marked MMPs
6: Find lowest non-terminal Np dominating p
7: Generate a positive training example for Np
8: Mark Np as visited
9: Recursively generate instances for Np?s children
10: Generate a negative training instance for all un-
visited nodes in Step 5-9
Steps 1 to 4 augment the question tree by creating
a node for each named mention, provided that no ex-
isting node spans exactly the mention and the men-
tion does not cross-bracket tree constituents. Steps 5
to 8 generate positive training instances for marked
MMPs; step 9 recursively generates positive training
instances 2 for tree nodes dominated by Np, where
Np is the lowest non-terminal node dominating the
marked MMP p.
After MMP training instances are generated we
design and compute features for each instance, and
use them to train a classifier.
3.3 MMP Features and Classifier
We compute four types of features that will be used
in a statistical classifier. These features are designed
to characterize a phrase from the lexical, syntactic,
2One exception to this step is that if a node spans a single
stop word, then a negative training instance is generated.
semantic and corpus-level aspect. The weights asso-
ciated with these features are automatically learned
from training data.
We will use ?(NP1 American)? in Figure 1 as the
running example below.
Lexical Features: Lexical features are motivated by
the observation that spellings in English sometimes
offer important cues about word significance. For
example, an all-capitalized word often signifies an
acronym; an all-digit word in a question is likely a
year, etc. We compute the following lexical features
for a candidate MMP:
CaseFeatures: is the first word of an MMP
upper-case? Is it all capital letters? Does it contain
numeric letters? For ?(NP American)? in Figure 1,
the upper-case feature fires.
CommonQWord: Does the MMP contain question
words, including ?What,? ?When,? ?Who,? etc.
Syntactic Features: The second group of features
are computed from syntactic parse trees after anno-
tated MMPs are aligned with question parse-trees
as described previously.
PhraseLabel: this feature returns the phrasal label
of the MMP. For ?(NP American)? in Figure 1, the
feature value is ?NP.? This captures that an NP is
more likely an MMP than, say, an ADVP.
NPUnique: this Boolean feature fires if a phrase
is the only NP in a question, indicating that this
constituent probably should be matched. For ?(NP
American),? the feature value would be false.
PosOfPTN: these features characterize the position
of the parse tree node to which an MMP is anchored.
They compute: (1) the position of the left-most
word of the node; (2) whether the left-most word is
the beginning of the question; (3) the depth of the
anchoring node, defined as the length of the path to
the root node. For ?(NP American)? in Figure 1, the
features state that it is the 5th word in the sentence;
it is not the first word of the sentence; and the depth
of the node is 6 (where root has depth 0).
PhrLenToQLenRatio: This feature computes the
number of words in an MMP, and its relative ratio to
the sentence length. This feature controls the length
of MMPs at decoding time, since most of MMPs
are short.
Semantic Features (NETypes): The third group of
features are computed from named entities and aim
to capture semantic information. The feature tests if
881
a phrase is or contains a named entity, and, if this
is the case, the value is the entity type. For ?(NP
American)? in Figure 1, the feature value would be
?GPE.?
Corpus-based Features ( AvgCorpusIDF): This
group of features computes the average of the IDFs
of the words in this phrase. From the corpus IDF,
we also compute the ratio between the number of
stop words and the total number of words in the
MMP, and use it as another feature.
3.4 MMP Classification Results
We now show that we can reliably predict MMPs of
questions. We split our set of 201 annotated ques-
tions into a training set consisting of 174 questions
and a test set with the remaining 27 questions. We
use the procedure and features described in Sec-
tion 3 to train a logistic regression binary classifier
using WEKA. Then, the trained MMP classifier is
applied to the test set question trees. Since the class
bias is quite skewed (only 16% of the phrases are
marked as MMP-Must) we also use re-sampling at
training time to balance the prior probability of the
two classes. At testing time, a parser and a men-
tion detection algorithm (Florian et al, 2004; Luo et
al., 2004; Luo and Zitouni, 2005) are run on each
question. The detected mentions are then used to
augment the question parse trees. The MMP classi-
fier achieves an 88.6% F-measure (cf. Table 1, with
91.6% precision). This is a respectable number, con-
sidering the limited amount of training data. We ex-
perimented with decision trees and bagging as well
but found logistic regression to work the best.
Feature P R F1
AvgCorpusIDF 0.849 0.634 0.725
+NPUnique 0.868 0.634 0.732
+NETypes 0.867 0.662 0.750
+PhraseLabel 0.890 0.705 0.783
+CaseFeatures 0.829 0.820 0.824
+PosOfPTN 0.911 0.852 0.880
+PhrLenToQLenRatio 0.915 0.855 0.883
+commonQWord 0.916 0.858 0.886
Table 1: The performances of the MMP classifier while
incrementally adding features.
The examples in Table 2 illustrate the top three
MMPs produced by the model on two questions.
These results are encouraging: in the first exam-
ple the word AIDS is clearly the most ?important?
word, but IDF alone is not adequate to place it in the
top since AIDS is also a common verb (words are
lower-cased before IDF look-up). Similarly, in the
third example, the phrase ?the causes? has a much
higher MMP score than the phrase ?the concerns?
(MMP score of 0.109), even though the words ?con-
cerns? has a slightly higher IDF, 2.80, than the word
?causes?(2.68). However, in this question, under-
standing that the word ?causes? is critical to the
meaning of the question is critical and is captured
by the MMP model.
We analyzed feature importance for MMP classi-
fication by incrementally adding each feature group
to the model. The result is tabulated in Table 1. Not
surprisingly, syntactical (i.e., ?NPUnique,? ?Phrase-
Label? and ?PosOfPTN?) and semantic features
(i.e., ?NETypes?) are complementary to the corpus-
based statistics features (i.e., average IDF). Lexical
features also improve recall: the addition of ?Case-
Features? boosts the F-measure by 4 points. At first
sight, it is surprising that the feature group ?PosOf-
PTN,? which characterize the position of a candi-
date MMP relative to the sentence and relative to the
parse tree, has such a large impact?it improves the
F-measure by 5.6 points. However, a cursory brows-
ing of the training questions reveals that most MMPs
are short and concentrate towards the end of the sen-
tence. So this feature group helps by directing the
model to predict MMPs at the end of the sentence
and to prefer short phrases versus long ones.
4 Relevance Model with MMPs
We now validate our second hypothesis that MMPs
are effective for open domain question answering.
We demonstrate this through the improvement in
performance on relevance prediction. More specif-
ically, given a natural language question, the task
is one of finding relevant sentences in posts on on-
line forums. The relevance prediction component
is critical for question answering as has been seen
in TREC(Ittycheriah and Roukos, 2001) and more
recently in the Jeopardy challenge(Gondek et al,
2012). The improved relevance model further im-
proves our question answering system as seen in
Section 5.
882
Question Top 3 MMPs MMP-
score
Top words
by IDF
List statistics about changes in the de-
mographics of AIDS.
1: AIDS 0.955 demographics
2: changes 0.525 AIDS
3: the demographics 0.349 statistics
What are the concerns about the
causes of autism?
1: autism 0.989 autism
2: the causes 0.422 concerns
3: the causes of autism 0.362 causes
Table 2: Example questions and the top-3 phrases ranked by the MMP model.
4.1 Data for Relevance Model
The data to train and test the relevance model is ob-
tained as follows. First, a rudimentary version (i.e.,
key word search) of a QA system using Lucene is
built. The Lucene index comprised of a large num-
ber of threads in online forums released to the par-
ticipants of the BOLT-IR task(IR, 2012) for devel-
opment of our systems. The corpus is described in
more detail in Sec. 5. Top snippets returned by the
search engine are judged for relevancy by our an-
notators. The initial (small) batch of data is used
to train a relevance model which is deployed in the
system. The new model is in turn used to create
more answers for new questions. When more data
is collected, the relevance model is retrained and re-
deployed to collect more data. The process is iter-
ated for several months, and at the end of this pro-
cess, a total of 390 training questions are created and
about 28,915 snippets are judged by human annota-
tors, out of which about 6,528 are relevant answers.
These question-answers pairs are used to train the fi-
nal relevance model used in our question-answering
system. A separate held-out test set of 59 questions
is created and its system output is also judged by hu-
mans. This data set is our test set.
4.2 Relevance Prediction
A key component in our question-answering sys-
tem is the snippet relevance model, which is used
to compute the probability that a snippet is relevant
to a question. The relevance model is a conditional
distribution P (r|q, s;D), where r is a binary ran-
dom variable indicating if the candidate snippet s is
relevant to the question q. D is the document where
the snippet s is found.
In our question answering system, MMPs ex-
tracted from questions are used to compute the fea-
tures for the relevance model. To test their effective-
ness, we conduct a controlled experiment by com-
paring the system with MMP features with 2 base-
lines: (1) a system without MMP features; (2) a
baseline with each word as an MMP and the word?s
IDF as the MMP score.
4.2.1 Baseline Features
We list the features used in our baseline system,
where no MMP feature is used. The features can
be categorized into the following types. (1) Text
Match Features: One set of features are the cosine
scores between different representations of the query
and the snippet. In one version the query and snip-
pet words are used as is; in another version the query
and snippet are stemmed using porter stemmer; in
yet another the words are morphed to their roots by
a table extracted from WordNet. We also compute
the inclusion scores (the proportion of query words
found in the snippet) and other word overlap fea-
tures. (2) Answer Type Features: The top 3 pre-
dictions of a statistical classifier trained to predict
answer categories were used as features. (3) Men-
tion Match Features compute whether a named en-
tity in the query occurs in the snippet. The matching
takes into consideration the results from within and
cross document coreference resolution components
for nominal and pronominal mentions. (4) Event
match features use several hand-crafted dictionar-
ies containing terms exclusive to various types of
events like ?violence?, ?legal?, ?election?. Accord-
ingly a set of features that take a value of ?1? if
both the query and snippet contain the same event
type were designed. (5) Snippet Statistics: Several
features based on snippet length, the position of the
snippet in the post etc were created.
883
4.2.2 Features Derived from MMP
The MMPs extracted from questions are used to
compute features in the following ways.
As MMPs are aligned with a question?s syntactic
tree, they can be used to find answers by matching
a question constituent with that of a candidate snip-
pet. The MMP model also returns a score for each
phrase, which can be used to compute the degree to
which a question matches a candidate snippet.
In this section, we use s = wn1 to denote a snip-
pet with words w1, w2, ? ? ? , wn, and m to denote
a phrase from the MMP model along with a score
M(m). The features are listed below:
HardMatch: Let I(m ? s) be a 1 or 0 function
indicating if a snippet contains the MMP m, then
the hard match score is computed as:
HM(q, s) =
?
m?q M(m)I(m ? s)
?
m?q M(m)
.
SoftLMMatch: The SoftLMMatch score is a
language-model (LM) based score, similar to that
used in (Bendersky and Croft, 2008), except that
MMPs play the role of concepts. The snippet-side
language model score LM(v|s) is computed as:
LM(v|s) =
?n
i=1 I(wi = v) + 0.05
n + 0.05|V | ,
where wi is the ith in snippet s; I(wi = v) is an
indicator function, taking value 1 if wi is v and 0
otherwise; |V | is the vocabulary size.
The soft match score between a question q and a
snippet s is then:
SM(q, s) =
?
m?q
(
M(m)?w?m LM(w|s)
)
?
m?q M(m)
,
where m ? q denotes all MMPs in question q, and
similarly, w ? m signifying words in m.
MMPInclScore: An MMP m?s inclusion score is:
IS(m, s) =
?
w?m I(l(w, s) > ?)IDF (w)
?
w?m IDF (w)
,
where w ? m are the words in m; I(?) is the in-
dicator function taking value 1 when the argument
is true and 0 otherwise; ? is a constant threshold;
IDF (w) is the IDF of word w. l(w, s) is the sim-
ilarity of word w to the snippet s as: l(w, s) =
maxv?sJW (w, v), where JW (w, v) is the Jaro
Winkler similarity score between words w and v.
The MMP weighted inclusion score between the
question q and snippet s is computed as:
IS(q, s) =
?
m?q M(m)IS(m, s)
?
m?q M(m)
MMPRankDep: This feature, RD(q, s) first tests
if there exists a matched bilexcial dependency be-
tween q and s; if yes, it further tests if the head or
dependent in the matched dependency is the head of
any MMP.
Let m(i) be the ith ranked MMP; let ?wh, wd|q?
and ?uh, ud|s? be bilexical dependencies from q and
s, respectively, where wh and uh are the heads and
wd and ud are the dependents; let EQ(w, u) be a
function testing if the question word w and snip-
pet word u are a match. In our implementation,
EQ(w, u) is true if either w and u are exactly the
same, or their morphs are the same, or they head
the same entity, or their synset in WordNet overlap.
With these notations, RD(q, s) is true if and only if
EQ(wh, uh) ? EQ(wd, ud) ?wh ? m(i) ?wd ? m(j)
is true for some ?wh, wd|q?, for some ?uh, ud|s? and
for some i and j.
EQ(wh, uh)?EQ(wd, ud) requires that the ques-
tion dependency ?wh, wd|q? and the snippet depen-
dency ?uh, ud|s? match; wh ? m(i) ?wd ? m(j) re-
quires that the head word and dependent word are in
the ith-rank and jth rank MMP, respectively. There-
fore, RD(q, s) is a dependency feature enhanced
with MMPs.
To test the effectiveness of the MMP features, we
trained 3 snippet classifiers on the data described
in Section 4.1: one baseline system without MMP
features (henceforth ?no-MMP?); a second baseline
with words as MMPs and their IDFs as the scores
in the MMP model(henceforth ?IDF-as-MMP?); the
third system uses the MMPs generated by the model
from Section 3 and all MMP features described in
this section. We used two types of classifiers: deci-
sion tree (DTree) and logistic regression (Logit).
The classification results on a set of 59 questions
disjoint from the training set are shown in Table 3.
The numbers in the table are F-measure on answer
snippets (or positive snippets). Within a machine
884
Learner
Model DTree Logit
noMMP 0.426 0.458
IDF-as-MMP 0.413 0.455
MMP 0.451 0.470
Table 3: F-measure for Relevance Prediction.
learning method, the model with MMP features is
always the best. Between the two classifiers, the lo-
gistic regression models are consistently better than
the decision tree ones. The results show that MMP
features are very helpful to the relevance model.
5 End-to-End System Results
The question-answering system is used in the 2012
BOLT IR evaluation (IR, 2012). The task is to an-
swer questions against a corpus of posts collected
from Internet discussion forums in 3 languages:
Arabic, Chinese and English. There are 499K, 449K
and 262K threads in each of these languages. The
Arabic and Chinese posts were first translated into
English before being processed. We now describe
our experiments on the set of 59 questions devel-
oped internally and demonstrate the effectiveness of
an MMP based relevance model in the end-to-end
system. In the next subsection we discuss our per-
formance in the BOLT-IR evaluation done by NIST
for DARPA.
We now briefly describe the question-answering
system we developed for the DARPA BOLT IR task,
where we applied the MMP classifier and its fea-
tures. Users submit questions to the system in natu-
ral language; the BOLT program mandates that these
questions comply with the restrictions described in
Section 3.1. Questions are analyzed by a query pre-
processing stage that includes our MMP extraction
classifier. The preprocessed queries are converted
to search queries. These are sent to an Indri-based
search engine (Strohman et al, 2005), which re-
turns candidate passages, typically spanning numer-
ous sentences. Each sentence of the retrieved pas-
sages is analyzed by a relevance detection module,
consisting of a statistical classifier that uses, among
others, features computed from the MMPs extracted
from the questions. Sentences or spans that are
deemed relevant to the question by the relevance de-
tection module are further grouped into equivalence
classes that provide different information about the
answers. The system generates a single answer for
each equivalence class, since elements of the same
class are redundant with respect to each other. The
elements of each equivalence class are converted
into citations that support the corresponding answer.
The ultimate goal of the MMP model is to im-
prove the performance of our question-answering
system. To test the effectiveness of the MMP model,
we contrast the model trained in Section3 with an
IDF baseline, where each non-stop word in a ques-
tion is an MMP and its score is the corpus IDF. The
IDF baseline is what a typical question answering
system would do in absence of deep question analy-
sis. To have a fair comparison, the two systems are
tested on the same set of 59 questions as the rele-
vance model.
The results of the IDF baseline and MMP system
are tabulated in Table 4. Note that the recalls are
less than 1.0 because (1) annotated snippets come
from both systems; (2) the annotation is done for all
snippets in a window surrounding system snippets.
As can be seen from Table 4, the MMP system is
about 5 points better than the baseline system. The
precision is notably better by 2 points, and the re-
call is far better (by 7.7%) than that of the baseline.
We also compute the question-level F-measures and
conduct a Wilcoxon signed-rank test for paired sam-
ples. The test indicates that the MMP system is bet-
ter than the baseline system at p < 0.00066. There-
fore, the MMP system has a clear advantage over the
baseline system.
System Prec Recall F1
baseline .4228 .3679 .3935
MMP .4425 .4452 .4438
Table 4: End-to-End system result on 59 questions.
5.1 BOLT Evaluation Results
The BOLT evaluation consists of 146 questions,
mostly event- or topic- related, e.g., ?What are peo-
ple saying about the ending of NASA?s space shuttle
program??. A system answer, if correct, is mapped
manually to a facet, which is one semantic unit that
answers the question. For each question, facets
are collected across all participants? submission. A
885
facet-based F-measure is computed for each partic-
ipating site. The recall from which the official F-
measure is computed is weighted by snippet cita-
tions (a citation is a reference to the original docu-
ment that supports the correct facet). In other words,
a snippet with more citations leads to a higher recall
than one with less citations. The performances of
4 participating sites are listed in Table 5. Note that
the F-measure is weighted and is not necessarily a
number between the precision and the recall.
Facet Metric
Site Precision Recall (Weighted) F
SITE 1 0.2713 0.1595 0.1713
SITE 2 0.1500 0.1316 0.1109
SITE 3 0.1935 0.2481 0.1734
Ours 0.2729 0.2195 0.2046
Table 5: Official BOLT 2012 IR evaluation results.
.
Among 4 participating sites, our system has the
highest performance. SITE 1 has about the same
level of precision, with lower recall, while SITE 3
has the best recall, but lower precision. The results
validate that the MMP question analysis technique
presented in this paper is quite effective.
6 Conclusions
We propose a framework to select and rank manda-
tory matching phrases (MMP) for question answer-
ing. The framework makes full use of the lexical,
syntactic and semantic information in a question and
does not require answer data.
The proposed MMP framework is tested at 3 lev-
els in a full QA system and is shown to be very effec-
tive to improve its performance: first, we show that
it is possible to reliably predict MMPs from ques-
tions alone: the MMP classifier can achieve an F-
measure as high as 88.6%; second, phrases proposed
by the MMP model are incorporated into a snippet
relevance model and we show that it improves its
performance; third, the MMP framework is used in
an question answering system which achieved the
best performance in the official 2012 BOLT IR (IR,
2012) evaluation.
Acknowledgments
This work was partially supported by the Defense
Advanced Research Projects Agency under contract
No. HR0011-12-C-0015. The views and findings
contained in this material are those of the authors
and do not necessarily reflect the position or policy
of the U.S. government and no official endorsement
should be inferred.
References
Michael Bendersky and W. Bruce Croft. 2008. Discov-
ering key concepts in verbose queries. Proceedings of
the 31st annual international ACM SIGIR conference
on research and development in information retrieval
- SIGIR ?08, page 491.
Michael Bendersky, Donald Metzler, and W. Bruce Croft.
2010. Learning concept importance using a weighted
dependence model. Proceedings of the third ACM in-
ternational conference on Web search and data mining
- WSDM ?10, page 31.
Michael Bendersky. 2011. Parameterized concept
weighting in verbose queries. Proceedings of the 34th
annual international ACM SIGIR conference on re-
search and development in information retrieval.
Razvan Bunescu and Yunfeng Huang. 2010. Towards a
general model of answer typing: Question focus iden-
tification. In Proceedings of the 11th International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing).
Jennifer Chu-carroll, John Prager, Christopher Welty,
Krzysztof Czuba, and David Ferrucci. 2003. A multi-
strategy and multi-source approach to question an-
swering. In In Proceedings of Text REtrieval Confer-
ence.
R Florian, H Hassan, A Ittycheriah, H Jing, N Kamb-
hatla, X Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 1?8, Boston, Massachusetts, USA, May 2
- May 7. Association for Computational Linguistics.
D. C. Gondek, A. Lally, A. Kalyanpur, J. W. Murdock,
P. A. Duboue, L. Zhang, Y. Pan, Z. M. Qiu, and
C. Welty. 2012. A framework for merging and rank-
ing of answers in DeepQA. IBM Journal of Research
and Development, 56(3.4):14:1 ?14:12, may-june.
Ulf Hermjakob, Eduard H. Hovy, and Chin yew Lin.
2000. Knowledge-based question answering. In In
Proceedings of the 6th World Multiconference on Sys-
tems, Cybernetics and Informatics (SCI-2002, pages
772?781.
886
BOLT IR. 2012. Broad operational language translation
(BOLT). www.darpa.mil/Our_Work/I2O/
Programs/Broad_Operational_Language_
Translat%ion_(BOLT).aspx. [Online; ac-
cessed 10-Dec-2012].
Abraham Ittycheriah and Salim Roukos. 2001. IBM?s
statistical question answering system - TREC-11. In
Proceedings of the Text REtrieval Conference.
Matthew Lease, James Allan, and W. Bruce Croft. 2009.
Advances in Information Retrieval, volume 5478 of
Lecture Notes in Computer Science. Springer Berlin
Heidelberg, Berlin, Heidelberg, April.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of the 19th international confer-
ence on Computational linguistics - Volume 1, COL-
ING ?02, pages 1?7, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
H. P. Luhn. 1958. A business intelligence system. IBM
J. Res. Dev., 2(4):314?319, October.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-
lingual coreference resolution with syntactic fea-
tures. In Proc. of Human Language Technology
(HLT)/Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proc. of ACL.
Dan Moldovan, Christine Clark, Sanda Harabagiu, and
Steve Maiorano. 2003. Cogex: a logic prover for
question answering. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ?03, pages 87?
93.
Christof Monz. 2007. Model tree learning for query
term weighting in question answering. In Proceed-
ings of the 29th European conference on IR re-
search, ECIR?07, pages 589?596, Berlin, Heidelberg.
Springer-Verlag.
Christopher Pinchak. 2006. A probabilistic answer type
model. In In EACL, pages 393?400.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st annual international ACM SIGIR
conference on research and development in informa-
tion retrieval, SIGIR ?98, pages 275?281, New York,
NY, USA. ACM.
S. E. Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of
the 17th annual international ACM SIGIR conference
on research and development in information retrieval,
SIGIR ?94, pages 232?241, New York, NY, USA.
Springer-Verlag New York, Inc.
Trevor Strohman, Donald Metzler, Howard Turtle, and
W. Bruce Croft. 2005. Indri: a language-model based
search engine for complex queries. Technical report,
in Proceedings of the International Conference on In-
telligent Analysis.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In TREC.
Le Zhao and Jamie Callan. 2010. Term necessity predic-
tion. Proceedings of the 19th ACM international con-
ference on Information and knowledge management -
CIKM ?10, page 259.
887
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1384?1394,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Sentence Compression Based Framework to Query-Focused
Multi-Document Summarization
Lu Wang1 Hema Raghavan2 Vittorio Castelli2 Radu Florian2 Claire Cardie1
1Department of Computer Science, Cornell University, Ithaca, NY 14853, USA
{luwang, cardie}@cs.cornell.edu
2IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA
{hraghav, vittorio, raduf}@us.ibm.com
Abstract
We consider the problem of using sentence
compression techniques to facilitate query-
focused multi-document summarization. We
present a sentence-compression-based frame-
work for the task, and design a series of
learning-based compression models built on
parse trees. An innovative beam search de-
coder is proposed to efficiently find highly
probable compressions. Under this frame-
work, we show how to integrate various in-
dicative metrics such as linguistic motivation
and query relevance into the compression pro-
cess by deriving a novel formulation of a com-
pression scoring function. Our best model
achieves statistically significant improvement
over the state-of-the-art systems on several
metrics (e.g. 8.0% and 5.4% improvements in
ROUGE-2 respectively) for the DUC 2006 and
2007 summarization task.
1 Introduction
The explosion of the Internet clearly warrants
the development of techniques for organizing and
presenting information to users in an effective
way. Query-focused multi-document summariza-
tion (MDS) methods have been proposed as one
such technique and have attracted significant at-
tention in recent years. The goal of query-focused
MDS is to synthesize a brief (often fixed-length)
and well-organized summary from a set of topic-
related documents that answer a complex ques-
tion or address a topic statement. The result-
ing summaries, in turn, can support a number of
information analysis applications including open-
ended question answering, recommender systems,
and summarization of search engine results. As
further evidence of its importance, the Document
Understanding Conference (DUC) has used query-
focused MDS as its main task since 2004 to foster
new research on automatic summarization in the
context of users? needs.
To date, most top-performing systems for
multi-document summarization?whether query-
specific or not?remain largely extractive: their
summaries are comprised exclusively of sen-
tences selected directly from the documents
to be summarized (Erkan and Radev, 2004;
Haghighi and Vanderwende, 2009; Celikyilmaz
and Hakkani-Tu?r, 2011). Despite their simplicity,
extractive approaches have some disadvantages.
First, lengthy sentences that are partly relevant
are either excluded from the summary or (if se-
lected) can block the selection of other important
sentences, due to summary length constraints.
In addition, when people write summaries, they
tend to abstract the content and seldom use
entire sentences taken verbatim from the original
documents. In news articles, for example, most
sentences are lengthy and contain both potentially
useful information for a summary as well as un-
necessary details that are better omitted. Consider
the following DUC query as input for a MDS
system:1 ?In what ways have stolen artworks
been recovered? How often are suspects arrested
or prosecuted for the thefts?? One manually gen-
erated summary includes the following sentence
but removes the bracketed words in gray:
A man suspected of stealing a million-dollar collection
of [hundreds of ancient] Nepalese and Tibetan art objects in
New York [11 years ago] was arrested [Thursday at his South
Los Angeles home, where he had been hiding the antiquities,
police said].
In this example, the compressed sentence is rela-
1From DUC 2005, query for topic d422g.
1384
tively more succinct and readable than the origi-
nal (e.g. in terms of Flesch-Kincaid Reading Ease
Score (Kincaid et al, 1975)). Likewise, removing
information irrelevant to the query (e.g. ?11 years
ago?, ?police said?) is crucial for query-focused
MDS.
Sentence compression techniques (Knight and
Marcu, 2000; Clarke and Lapata, 2008) are the
standard for producing a compact and grammat-
ical version of a sentence while preserving rel-
evance, and prior research (e.g. Lin (2003)) has
demonstrated their potential usefulness for generic
document summarization. Similarly, strides have
been made to incorporate sentence compression
into query-focused MDS systems (Zajic et al,
2006). Most attempts, however, fail to produce
better results than those of the best systems built
on pure extraction-based approaches that use no
sentence compression.
In this paper we investigate the role of sentence
compression techniques for query-focused MDS.
We extend existing work in the area first by inves-
tigating the role of learning-based sentence com-
pression techniques. In addition, we design three
types of approaches to sentence-compression?
rule-based, sequence-based and tree-based?and
examine them within our compression-based
framework for query-specific MDS. Our top-
performing sentence compression algorithm in-
corporates measures of query relevance, con-
tent importance, redundancy and language qual-
ity, among others. Our tree-based methods rely on
a scoring function that allows for easy and flexi-
ble tailoring of sentence compression to the sum-
marization task, ultimately resulting in significant
improvements for MDS, while at the same time
remaining competitive with existing methods in
terms of sentence compression, as discussed next.
We evaluate the summarization models on
the standard Document Understanding Confer-
ence (DUC) 2006 and 2007 corpora 2 for query-
focused MDS and find that all of our compression-
based summarization models achieve statistically
significantly better performance than the best
DUC 2006 systems. Our best-performing sys-
tem yields an 11.02 ROUGE-2 score (Lin and
Hovy, 2003), a 8.0% improvement over the best
reported score (10.2 (Davis et al, 2012)) on the
2We believe that we can easily adapt our system for tasks
(e.g. TAC-08?s opinion summarization or TAC-09?s update
summarization) or domains (e.g. web pages or wikipedia
pages). We reserve that for future work.
DUC 2006 dataset, and an 13.49 ROUGE-2, a
5.4% improvement over the best score in DUC
2007 (12.8 (Davis et al, 2012)). We also ob-
serve substantial improvements over previous sys-
tems w.r.t. the manual Pyramid (Nenkova and
Passonneau, 2004) evaluation measure (26.4 vs.
22.9 (Jagarlamudi et al, 2006)); human annota-
tors furthermore rate our system-generated sum-
maries as having less redundancy and compara-
ble quality w.r.t. other linguistic quality metrics.
With these results we believe we are the first
to successfully show that sentence compression
can provide statistically significant improvements
over pure extraction-based approaches for query-
focused MDS.
2 Related Work
Existing research on query-focused multi-
document summarization (MDS) largely relies
on extractive approaches, where systems usually
take as input a set of documents and select
the top relevant sentences for inclusion in the
final summary. A wide range of methods have
been employed for this task. For unsupervised
methods, sentence importance can be estimated
by calculating topic signature words (Lin and
Hovy, 2000; Conroy et al, 2006), combining
query similarity and document centrality within
a graph-based model (Otterbacher et al, 2005),
or using a Bayesian model with sophisticated
inference (Daume? and Marcu, 2006). Davis et
al. (2012) first learn the term weights by Latent
Semantic Analysis, and then greedily select
sentences that cover the maximum combined
weights. Supervised approaches have mainly
focused on applying discriminative learning for
ranking sentences (Fuentes et al, 2007). Lin and
Bilmes (2011) use a class of carefully designed
submodular functions to reward the diversity of
the summaries and select sentences greedily.
Our work is more related to the less studied
area of sentence compression as applied to (sin-
gle) document summarization. Zajic et al (2006)
tackle the query-focused MDS problem using a
compress-first strategy: they develop heuristics to
generate multiple alternative compressions of all
sentences in the original document; these then be-
come the candidates for extraction. This approach,
however, does not outperform some extraction-
based approaches. A similar idea has been stud-
ied for MDS (Lin, 2003; Gillick and Favre, 2009),
1385
but limited improvement is observed over extrac-
tive baselines with simple compression rules. Fi-
nally, although learning-based compression meth-
ods are promising (Martins and Smith, 2009;
Berg-Kirkpatrick et al, 2011), it is unclear how
well they handle issues of redundancy.
Our research is also inspired by probabilis-
tic sentence-compression approaches, such as the
noisy-channel model (Knight and Marcu, 2000;
Turner and Charniak, 2005), and its extension via
synchronous context-free grammars (SCFG) (Aho
and Ullman, 1969; Lewis and Stearns, 1968) for
robust probability estimation (Galley and McKe-
own, 2007). Rather than attempt to derive a new
parse tree like Knight and Marcu (2000) and Gal-
ley and McKeown (2007), we learn to safely re-
move a set of constituents in our parse tree-based
compression model while preserving grammati-
cal structure and essential content. Sentence-level
compression has also been examined via a dis-
criminative model McDonald (2006), and Clarke
and Lapata (2008) also incorporate discourse in-
formation by using integer linear programming.
3 The Framework
We now present our query-focused MDS frame-
work consisting of three steps: Sentence Rank-
ing, Sentence Compression and Post-processing.
First, sentence ranking determines the importance
of each sentence given the query. Then, a sen-
tence compressor iteratively generates the most
likely succinct versions of the ranked sentences,
which are cumulatively added to the summary, un-
til a length limit is reached. Finally, the post-
processing stage applies coreference resolution
and sentence reordering to build the summary.
Sentence Ranking. This stage aims to rank sen-
tences in order of relevance to the query. Un-
surprisingly, ranking algorithms have been suc-
cessfully applied to this task. We experimented
with two of them ? Support Vector Regres-
sion (SVR) (Mozer et al, 1997) and Lamb-
daMART (Burges et al, 2007). The former
has been used previously for MDS (Ouyang et
al., 2011). LambdaMart on the other hand has
shown considerable success in information re-
trieval tasks (Burges, 2010); we are the first to
apply it to summarization. For training, we use
40 topics (i.e. queries) from the DUC 2005 cor-
pus (Dang, 2005) along with their manually gener-
ated abstracts. As in previous work (Shen and Li,
Basic Features
relative/absolute position
is among the first 1/3/5 sentences?
number of words (with/without stopwords)
number of words more than 5/10 (with/without stopwords)
Query-Relevant Features
unigram/bigram/skip bigram (at most four words apart) overlap
unigram/bigram TF/TF-IDF similarity
mention overlap
subject/object/indirect object overlap
semantic role overlap
relation overlap
Query-Independent Features
average/total unigram/bigram IDF/TF-IDF
unigram/bigram TF/TF-IDF similarity with the centroid of the cluster
average/sum of sumBasic/SumFocus (Toutanova et al, 2007)
average/sum of mutual information
average/sum of number of topic signature words (Lin and Hovy, 2000)
basic/improved sentence scorers from Conroy et al (2006)
Content Features
contains verb/web link/phone number?
contains/portion of words between parentheses
Table 1: Sentence-level features for sentence ranking.
2011; Ouyang et al, 2011), we use the ROUGE-
2 score, which measures bigram overlap between
a sentence and the abstracts, as the objective for
regression.
While space limitations preclude a longer dis-
cussion of the full feature set (ref. Table 1), we
describe next the query-relevant features used for
sentence ranking as these are the most impor-
tant for our summarization setting. The goal of
this feature subset is to determine the similarity
between the query and each candidate sentence.
When computing similarity, we remove stopwords
as well as the words ?discuss, describe, specify,
explain, identify, include, involve, note? that are
adopted and extended from Conroy et al (2006).
Then we conduct simple query expansion based
on the title of the topic and cross-document coref-
erence resolution. Specifically, we first add the
words from the topic title to the query. And for
each mention in the query, we add other mentions
within the set of documents that corefer with this
mention. Finally, we compute two versions of the
features?one based on the original query and an-
other on the expanded one. We also derive the
semantic role overlap and relation instance over-
lap between the query and each sentence. Cross-
document coreference resolution, semantic role la-
beling and relation extraction are accomplished
via the methods described in Section 5.
Sentence Compression. As the main focus of
this paper, we propose three types of compression
methods, described in detail in Section 4 below.
Post-processing. Post-processing performs
coreference resolution and sentence ordering.
1386
Basic Features Syntactic Tree Features
first 1/3/5 tokens (toks)? POS tag
last 1/3/5 toks? parent/grandparent label
first letter/all letters capitalized? leftmost child of parent?
is negation? second leftmost child of parent?
is stopword? is headword?
Dependency Tree Features in NP/VP/ADVP/ADJP chunk?
dependency relation (dep rel) Semantic Features
parent/grandparent dep rel is a predicate?
is the root? semantic role label
has a depth larger than 3/5?
Rule-Based Features
For each rule in Table 2 , we construct a corresponding feature to
indicate whether the token is identified by the rule.
Table 3: Token-level features for sequence-based com-
pression.
We replace each pronoun with its referent unless
they appear in the same sentence. For sentence
ordering, each compressed sentence is assigned
to the most similar (tf-idf) query sentence. Then
a Chronological Ordering algorithm (Barzilay et
al., 2002) sorts the sentences for each query based
first on the time stamp, and then the position in
the source document.
4 Sentence Compression
Sentence compression is typically formulated as
the problem of removing secondary information
from a sentence while maintaining its grammati-
cality and semantic structure (Knight and Marcu,
2000; McDonald, 2006; Galley and McKeown,
2007; Clarke and Lapata, 2008). We leave other
rewrite operations, such as paraphrasing and re-
ordering, for future work. Below we describe
the sentence compression approaches developed
in this research: RULE-BASED COMPRESSION,
SEQUENCE-BASED COMPRESSION, and TREE-
BASED COMPRESSION.
4.1 Rule-based Compression
Turner and Charniak (2005) have shown that ap-
plying hand-crafted rules for trimming sentences
can improve both content and linguistic qual-
ity. Our rule-based approach extends existing
work (Conroy et al, 2006; Toutanova et al, 2007)
to create the linguistically-motivated compression
rules of Table 2. To avoid ill-formed output, we
disallow compressions of more than 10 words by
each rule.
4.2 Sequence-based Compression
As in McDonald (2006) and Clarke and Lapata
(2008), our sequence-based compression model
makes a binary ?keep-or-delete? decision for each
word in the sentence. In contrast, however, we
Figure 1: Diagram of tree-based compression. The
nodes to be dropped are grayed out. In this example,
the root of the gray subtree (a ?PP?) would be labeled
REMOVE. Its siblings and parent are labeled RETAIN
and PARTIAL, respectively. The trimmed tree is real-
ized as ?Malaria causes millions of deaths.?
view compression as a sequential tagging problem
and make use of linear-chain Conditional Ran-
dom Fields (CRFs) (Lafferty et al, 2001) to se-
lect the most likely compression. We represent
each sentence as a sequence of tokens, X =
x0x1 . . . xn, and generate a sequence of labels,
Y = y0y1 . . . yn, that encode which tokens are
kept, using a BIO label format: {B-RETAIN de-
notes the beginning of a retained sequence, I-
RETAIN indicates tokens ?inside? the retained se-
quence, O marks tokens to be removed}.
The CRF model is built using the features
shown in Table 3. ?Dependency Tree Features?
encode the grammatical relations in which each
word is involved as a dependent. For the ?Syntac-
tic Tree?, ?Dependency Tree? and ?Rule-Based?
features, we also include features for the two
words that precede and the two that follow the cur-
rent word. Detailed descriptions of the training
data and experimental setup are in Section 5.
During inference, we find the maximally likely
sequence Y according to a CRF with parameter
? (Y = argmaxY ? P (Y ?|X; ?)), while simulta-
neously enforcing the rules of Table 2 to reduce
the hypothesis space and encourage grammatical
compression. To do this, we encode these rules as
features for each token, and whenever these fea-
ture functions fire, we restrict the possible label
for that token to ?O?.
4.3 Tree-based Compression
Our tree-based compression methods are in line
with syntax-driven approaches (Galley and McK-
eown, 2007), where operations are carried out
on parse tree constituents. Unlike previous
work (Knight and Marcu, 2000; Galley and McK-
eown, 2007), we do not produce a new parse tree,
1387
Rule Example
Header [MOSCOW , October 19 ( Xinhua ) ?] Russian federal troops Tuesday continued...
Relative dates ...Centers for Disease Control confirmed [Tuesday] that there was...
Intra-sentential attribution ...fueling the La Nina weather phenomenon, [the U.N. weather agency said].
Lead adverbials [Interestingly], while the Democrats tend to talk about...
Noun appositives Wayne County Prosecutor [John O?Hara] wanted to send a message...
Nonrestrictive relative clause Putin, [who was born on October 7, 1952 in Leningrad], was elected in the presidential election...
Adverbial clausal modifiers [Starting in 1998], California will require 2 per cent of a manufacturer...
(Lead sentence) [Given the short time], car makers see electric vehicles as...
Within Parentheses ...to Christian home schoolers in the early 1990s [(www.homecomputermarket.com)].
Table 2: Linguistically-motivated rules for sentence compression. The grayed-out words in brackets are removed.
but focus on learning to identify the proper set of
constituents to be removed. In particular, when a
node is dropped from the tree, all words it sub-
sumes will be deleted from the sentence.
Formally, given a parse tree T of the sentence
to be compressed and a tree traversal algorithm,
T can be presented as a list of ordered constituent
nodes, T = t0t1 . . . tm. Our objective is to find a
set of labels, L = l0l1 . . . lm, where li ? {RETAIN,
REMOVE, PARTIAL}. RETAIN (RET) and RE-
MOVE (REM) denote whether the node ti is re-
tained or removed. PARTIAL (PAR) means ti is
partly removed, i.e. at least one child subtree of ti
is dropped.
Labels are identified, in order, according to the
tree traversal algorithm. Every node label needs
to be compatible with the labeling history: given
a node ti, and a set of labels l0 . . . li?1 predicted
for nodes t0 . . . ti?1, li =RET or li =REM is com-
patible with the history when all children of ti are
labeled as RET or REM, respectively; li =PAR is
compatible when ti has at least two descendents
tj and tk (j < i and k < i), one of which is
RETained and the other, REMoved. As such, the
root of the gray subtree in Figure 1 is labeled as
REM; its left siblings as RET; its parent as PAR.
As the space of possible compressions is expo-
nential in the number of leaves in the parse tree,
instead of looking for the globally optimal solu-
tion, we use beam search to find a set of highly
likely compressions and employ a language model
trained on a large corpus for evaluation.
A Beam Search Decoder. The beam search de-
coder (see Algorithm 1) takes as input the sen-
tence?s parse tree T = t0t1 . . . tm, an order-
ing O for traversing T (e.g. postorder) as a se-
quence of nodes in T , the set L of possible
node labels, a scoring function S for evaluat-
ing each sentence compression hypothesis, and
a beam size N . Specifically, O is a permuta-
tion on the set {0, 1, . . . ,m}?each element an
index onto T . Following O, T is re-ordered as
tO0tO1 . . . tOm , and the decoder considers each or-
dered constituent tOi in turn. In iteration i, all
existing sentence compression hypotheses are ex-
panded by one node, tOi , labeling it with all com-
patible labels. The new hypotheses (usually sub-
sentences) are ranked by the scorer S and the top
N are preserved to be extended in the next itera-
tion. See Figure 2 for an example.
Input : parse tree T , ordering O = O0O1 . . . Om,
L ={RET, REM, PAR}, hypothesis scorer S,
beam size N
Output: N best compressions
stack? ? (empty set);
foreach node tOi in T = tO0 . . . tOm doif i == 0 (first node visited) then
foreach label lO0 in L donewHypothesis h? ? [lO0 ];put h? into Stack;
end
else
newStack? ? (empty set);
foreach hypothesis h in stack do
foreach label lOi in L doif lOi is compatible thennewHypothesis h? ? h + [lOi ];put h? into newStack;
end
end
end
stack? newStack;
end
Apply S to sort hypotheses in stack in descending
order;
Keep the N best hypotheses in stack;
end
Algorithm 1: Beam search decoder.
Our BASIC Tree-based Compression in-
stantiates the beam search decoder with
postorder traversal and a hypothesis scorer
that takes a possible sentence compression?
a sequence of nodes (e.g. tO0 . . . tOk ) and
their labels (e.g. lO0 . . . lOk )?and returns?k
j=1 logP (lOj |tOj ) (denoted later as
ScoreBasic). The probability is estimated by
a Maximum Entropy classifier (Berger et al,
1388
Figure 2: Example of beam search decoding. For
postorder traversal, the three nodes are visited in a
bottom-up order. The associated compression hypothe-
ses (boxed) are ranked based on the scores in parenthe-
ses. Beam scores for other nodes are omitted.
Basic Features Syntactic Tree Features
projection falls w/in first 1/3/5 toks?? constituent label
projection falls w/in last 1/3/5 toks?? parent left/right sibling label
subsumes first 1/3/5 toks?? grandparent left/right sibling label
subsumes last 1/3/5 toks?? is leftmost child of parent?
number of words larger than 5/10?? is second leftmost child of parent?
is leaf node?? is head node of parent?
is root of parsing tree?? label of its head node
has word with first letter capitalized? has a depth greater than 3/5/10?
has word with all letters capitalized? Dependency Tree Features
has negation? dep rel of head node?
has stopwords? dep rel of parent?s head node?
Semantic Features dep rel of grandparent?s head node?
the head node has predicate? contain root of dep tree??
semantic roles of head node has a depth larger than 3/5??
Rule-Based Features
For each rule in Table 2 , we construct a corresponding feature to indicate
whether the token is identified by the rule.
Table 4: Constituent-level features for tree-based com-
pression. ? or ? denote features that are concatenated
with every Syntactic Tree feature to compose a new
one.
1996) trained at the constituent level using the
features in Table 4. We also apply the rules of
Table 2 during the decoding process. Concretely,
if the words subsumed by a node are identified
by any rule, we only consider REM as the node?s
label.
Given the N -best compressions from the de-
coder, we evaluate the yield of the trimmed trees
using a language model trained on the Giga-
word (Graff, 2003) corpus and return the compres-
sion with the highest probability. Thus, the de-
coder is quite flexible ? its learned scoring func-
tion allows us to incorporate features salient for
sentence compression while its language model
guarantees the linguistic quality of the compressed
string. In the sections below we consider addi-
tional improvements.
4.3.1 Improving Beam Search
CONTEXT-aware search is based on the intu-
ition that predictions on preceding context can
be leveraged to facilitate the prediction of the
current node. For example, parent nodes with
children that have all been removed (retained)
should have a label of REM (RET). In light of
this, we encode these contextual predictions as
additional features of S, that is, ALL-CHILDREN-
REMOVED/RETAINED, ANY-LEFTSIBLING-
REMOVED/RETAINED/PARTLY REMOVED,
LABEL-OF-LEFT-SIBLING/HEAD-NODE.
HEAD-driven search modifies the BASIC pos-
torder tree traversal by visiting the head node first
at each level, leaving other orders unchanged. In
a nutshell, if the head node is dropped, then its
modifiers need not be preserved. We adopt the
same features as CONTEXT-aware search, but re-
move those involving left siblings. We also add
one more feature: LABEL-OF-THE-HEAD-NODE-
IT-MODIFIES.
4.3.2 Task-Specific Sentence Compression
The current scorer ScoreBasic is still fairly naive
in that it focuses only on features of the sen-
tence to be compressed. However extra-sentential
knowledge can also be important for query-
focused MDS. For example, information regard-
ing relevance to the query might lead the de-
coder to produce compressions better suited for
the summary. Towards this goal, we construct
a compression scoring function?the multi-scorer
(MULTI)?that allows the incorporation of mul-
tiple task-specific scorers. Given a hypothesis at
any stage of decoding, which yields a sequence of
words W = w0w1...wj , we propose the following
component scorers.
Query Relevance. Query information ought to
guide the compressor to identify the relevant con-
tent. The query Q is expanded as described in
Section 3. Let |W ? Q| denote the number of
unique overlapping words betweenW andQ, then
scoreq = |W ?Q|/|W |.
Importance. A query-independent impor-
tance score is defined as the average Sum-
Basic (Toutanova et al, 2007) value in W ,
i.e. scoreim =?ji=1 SumBasic(wi)/|W |.
Language Model. We let scorelm be the proba-
bility of W computed by a language model.
Cross-Sentence Redundancy. To encourage di-
versified content, we define a redundancy score to
discount replicated content: scorered = 1? |W ?
C|/|W |, whereC is the words already selected for
the summary.
1389
The multi-scorer is defined as a linear
combination of the component scorers: Let
~? = (?0, . . . , ?4), 0 ? ?i ? 1, ????score =
(scoreBasic, scoreq, scoreim, scorelm, scorered),
S = scoremulti = ~? ? ????score (1)
The parameters ~? are tuned on a held-out tuning
set by grid search. We linearly normalize the score
of each metric, where the minimum and maximum
values are estimated from the tuning data.
5 Experimental Setup
We evaluate our methods on the DUC 2005, 2006
and 2007 datasets (Dang, 2005; Dang, 2006;
Dang, 2007), each of which is a collection of
newswire articles. 50 complex queries (topics) are
provided for DUC 2005 and 2006, 35 are collected
for DUC 2007 main task. Relevant documents for
each query are provided along with 4 to 9 human
MDS abstracts. The task is to generate a summary
within 250 words to address the query. We split
DUC 2005 into two parts: 40 topics to train the
sentence ranking models, and 10 for ranking algo-
rithm selection and parameter tuning for the multi-
scorer. DUC 2006 and DUC 2007 are reserved as
held out test sets.
Sentence Compression. The dataset
from Clarke and Lapata (2008) is used to
train the CRF and MaxEnt classifiers (Section 4).
It includes 82 newswire articles with one manually
produced compression aligned to each sentence.
Preprocessing. Documents are processed by a
full NLP pipeline, including token and sentence
segmentation, parsing, semantic role labeling,
and an information extraction pipeline consist-
ing of mention detection, NP coreference, cross-
document resolution, and relation detection (Flo-
rian et al, 2004; Luo et al, 2004; Luo and Zitouni,
2005).
Learning for Sentence Ranking and Compres-
sion. We use Weka (Hall et al, 2009) to train a
support vector regressor and experiment with var-
ious rankers in RankLib (Dang, 2011)3. As Lamb-
daMART has an edge over other rankers on the
held-out dataset, we selected it to produce ranked
sentences for further processing. For sequence-
based compression using CRFs, we employ Mal-
let (McCallum, 2002) and integrate the Table 2
rules during inference. NLTK (Bird et al, 2009)
3Default parameters are used. If an algorithm needs a val-
idation set, we use 10 out of 40 topics.
MaxEnt classifiers are used for tree-based com-
pression. Beam size is fixed at 2000.4 Sen-
tence compressions are evaluated by a 5-gram lan-
guage model trained on Gigaword (Graff, 2003)
by SRILM (Stolcke, 2002).
6 Results
The results in Table 5 use the official ROUGE soft-
ware with standard options5 and report ROUGE-
2 (R-2) (measures bigram overlap) and ROUGE-
SU4 (R-SU4) (measures unigram and skip-bigram
separated by up to four words). We compare our
sentence-compression-based methods to the best
performing systems based on ROUGE in DUC
2006 and 2007 (Jagarlamudi et al, 2006; Pingali
et al, 2007), system by Davis et al (2012) that
report the best R-2 score on DUC 2006 and 2007
thus far, and to the purely extractive methods of
SVR and LambdaMART.
Our sentence-compression-based systems
(marked with ?) show statistically significant
improvements over pure extractive summarization
for both R-2 and R-SU4 (paired t-test, p < 0.01).
This means our systems can effectively remove
redundancy within the summary through compres-
sion. Furthermore, our HEAD-driven beam search
method with MULTI-scorer beats all systems on
DUC 20066 and all systems on DUC 2007 except
the best system in terms of R-2 (p < 0.01). Its
R-SU4 score is also significantly (p < 0.01)
better than extractive methods, rule-based and
sequence-based compression methods on both
DUC 2006 and 2007. Moreover, our systems with
learning-based compression have considerable
compression rates, indicating their capability to
remove superfluous words as well as improve
summary quality.
Human Evaluation. The Pyramid (Nenkova
and Passonneau, 2004) evaluation was developed
to manually assess how many relevant facts or
Summarization Content Units (SCUs) are cap-
tured by system summaries. We ask a professional
annotator (who is not one of the authors, is highly
experienced in annotating for various NLP tasks,
and is fluent in English) to carry out a Pyramid
evaluation on 10 randomly selected topics from
4We looked at various beam sizes on the heldout data, and
observed that the performance peaks around this value.
5ROUGE-1.5.5.pl -n 4 -w 1.2 -m -2 4 -u -c 95 -r 1000 -f
A -p 0.5 -t 0 -a -d
6The system output from Davis et al (2012) is not avail-
able, so significance tests are not conducted on it.
1390
DUC 2006 DUC 2007
System C Rate R-2 R-SU4 C Rate R-2 R-SU4
Best DUC system ? 9.56 15.53 ? 12.62 17.90
Davis et al (2012) ? 10.2 15.2 ? 12.8 17.5
SVR 100% 7.78 13.02 100% 9.53 14.69
LambdaMART 100% 9.84 14.63 100% 12.34 15.62
Rule-based 78.99% 10.62 ?? 15.73 ? 78.11% 13.18? 18.15?
Sequence 76.34% 10.49 ? 15.60 ? 77.20% 13.25? 18.23?
Tree (BASIC + ScoreBasic) 70.48% 10.49 ? 15.86 ? 69.27% 13.00? 18.29?
Tree (CONTEXT + ScoreBasic) 65.21% 10.55 ?? 16.10 ? 63.44% 12.75 18.07?
Tree (HEAD + ScoreBasic) 66.70% 10.66 ?? 16.18 ? 65.05% 12.93 18.15?
Tree (HEAD + MULTI) 70.20% 11.02 ?? 16.25 ? 73.40% 13.49? 18.46?
Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
preserved. R-2 (ROUGE-2) and R-SU4 (ROUGE-SU4) scores are multiplied by 100. ??? indicates that data is
unavailable. BASIC, CONTEXT and HEAD represent the basic beam search decoder, context-aware and head-driven
search extensions respectively. ScoreBasic and MULTI refer to the type of scorer used. Statistically significant
improvements (p < 0.01) over the best system in DUC 06 and 07 are marked with ?. ? indicates statistical
significance (p < 0.01) over extractive approaches (SVR or LambdaMART). HEAD + MULTI outperforms all the
other extract- and compression-based systems in R-2.
System Pyr Gra Non-Red Ref Foc Coh
Best DUC system (ROUGE) 22.9?8.2 3.5?0.9 3.5?1.0 3.5?1.1 3.6?1.0 2.9?1.1
Best DUC system (LQ) ? 4.0?0.8 4.2?0.7 3.8?0.7 3.6?0.9 3.4?0.9
Our System 26.4?10.3 3.0?0.9 4.0?1.1 3.6?1.0 3.4?0.9 2.8?1.0
Table 6: Human evaluation on our multi-scorer based system, Jagarlamudi et al (2006) (Best DUC system
(ROUGE)), and Lacatusu et al (2006) (Best DUC system (LQ)). Our system can synthesize more relevant content
according to Pyramid (?100). We also examine linguistic quality (LQ) in Grammaticality (Gra), Non-redundancy
(Non-Red), Referential clarity (Ref), Focus (Foc), and Structure and Coherence (Coh) like Dang (2006), each rated
from 1 (very poor) to 5 (very good). Our system has better non-redundancy than Jagarlamudi et al (2006) and is
comparable to Jagarlamudi et al (2006) and Lacatusu et al (2006) in other metrics except grammaticality.
the DUC 2006 task with gold-standard SCU an-
notation in abstracts. The Pyramid score (see Ta-
ble 6) is re-calculated for the system with best
ROUGE scores in DUC 2006 (Jagarlamudi et al,
2006) along with our system by the same annota-
tor to make a meaningful comparison.
We further evaluate the linguistic quality (LQ)
of the summaries for the same 10 topics in ac-
cordance with the measurement in Dang (2006).
Four native speakers who are undergraduate stu-
dents in computer science (none are authors) per-
formed the task, We compare our system based
on HEAD-driven beam search with MULTI-scorer
to the best systems in DUC 2006 achieving top
ROUGE scores (Jagarlamudi et al, 2006) (Best
DUC system (ROUGE)) and top linguistic quality
scores (Lacatusu et al, 2006) (Best DUC system
(LQ))7. The average score and standard deviation
for each metric is displayed in Table 6. Our sys-
tem achieves a higher Pyramid score, an indication
that it captures more of the salient facts. We also
7Lacatusu et al (2006) obtain the best scores in three lin-
guistic quality metrics (i.e. grammaticality, focus, structure
and coherence), and overall responsiveness on DUC 2006.
attain better non-redundancy than Jagarlamudi et
al. (2006), meaning that human raters perceive
less replicative content in our summaries. Scores
for other metrics are comparable to Jagarlamudi
et al (2006) and Lacatusu et al (2006), which
either uses minimal non-learning-based compres-
sion rules or is a pure extractive system. However,
our compression system sometimes generates less
grammatical sentences, and those are mostly due
to parsing errors. For example, parsing a clause
starting with a past tense verb as an adverbial
clausal modifier can lead to an ill-formed com-
pression. Those issues can be addressed by an-
alyzing k-best parse trees and we leave it in the
future work. A sample summary from our multi-
scorer based system is in Figure 3.
Sentence Compression Evaluation. We
also evaluate sentence compression separately
on (Clarke and Lapata, 2008), adopting the same
partitions as (Martins and Smith, 2009), i.e. 1, 188
sentences for training and 441 for testing. Our
compression models are compared with Hedge
Trimmer (Dorr et al, 2003), a discriminative
model proposed by McDonald (2006) and a
1391
System C Rate Uni-Prec Uni-Rec Uni-F1 Rel-F1
HedgeTrimmer 57.64% 0.72 0.65 0.64 0.50
McDonald (2006) 70.95% 0.77 0.78 0.77 0.55
Martins and Smith (2009) 71.35% 0.77 0.78 0.77 0.56
Rule-based 87.65% 0.74 0.91 0.80 0.63
Sequence 70.79% 0.77 0.80 0.76 0.58
Tree (BASIC) 69.65% 0.77 0.79 0.75 0.56
Tree (CONTEXT) 67.01% 0.79 0.78 0.76 0.57
Tree (HEAD) 68.06% 0.79 0.80 0.77 0.59
Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
all use single-scorer. Our context-aware and head-driven tree-based approaches outperform all the other systems
significantly (p < 0.01) in precision (Uni-Prec) without sacrificing the recalls (i.e. there is no statistically signifi-
cant difference between our models and McDonald (2006) / M & S (2009) with p > 0.05). Italicized numbers for
unigram F1 (Uni-F1) are statistically indistinguishable (p > 0.05). Our head-driven tree-based approach also pro-
duces significantly better grammatical relations F1 scores (Rel-F1) than all the other systems except the rule-based
method (p < 0.01).
Topic D0626H: How were the bombings of the US em-
bassies in Kenya and Tanzania conducted? What terror-
ist groups and individuals were responsible? How and
where were the attacks planned?
WASHINGTON, August 13 (Xinhua) ? President Bill
Clinton Thursday condemned terrorist bomb attacks at
U.S. embassies in Kenya and Tanzania and vowed to find
the bombers and bring them to justice. Clinton met with
his top aides Wednesday in the White House to assess the
situation following the twin bombings at U.S. embassies
in Kenya and Tanzania, which have killed more than 250
people and injured over 5,000, most of them Kenyans and
Tanzanians. Local sources said the plan to bomb U.S. em-
bassies in Kenya and Tanzania took three months to com-
plete and bombers destined for Kenya were dispatched
through Somali and Rwanda. FBI Director Louis Freeh,
Attorney General Janet Reno and other senior U.S. gov-
ernment officials will hold a news conference at 1 p.m.
EDT (1700GMT) at FBI headquarters in Washington ?to
announce developments in the investigation of the bomb-
ings of the U.S. embassies in Kenya and Tanzania,? the
FBI said in a statement. ...
Figure 3: Part of the summary generated by the multi-
scorer based summarizer for topic D0626H (DUC
2006). Grayed out words are removed. Query-
irrelevant phrases, such as temporal information or
source of the news, have been removed.
dependency-tree based compressor (Martins and
Smith, 2009)8. We adopt the metrics in Martins
and Smith (2009) to measure the unigram-level
macro precision, recall, and F1-measure with
respect to human annotated compression. In
addition, we also compute the F1 scores of
grammatical relations which are annotated by
RASP (Briscoe and Carroll, 2002) according
to Clarke and Lapata (2008).
In Table 7, our context-aware and head-driven
tree-based compression systems show statistically
significantly (p < 0.01) higher precisions (Uni-
8Thanks to Andre? F.T. Martins for system outputs.
Prec) than all the other systems, without decreas-
ing the recalls (Uni-Rec) significantly (p > 0.05)
based on a paired t-test. Unigram F1 scores (Uni-
F1) in italics indicate that the corresponding sys-
tems are not statistically distinguishable (p >
0.05). For grammatical relation evaluation, our
head-driven tree-based system obtains statistically
significantly (p < 0.01) better F1 score (Rel-F1
than all the other systems except the rule-based
system).
7 Conclusion
We have presented a framework for query-focused
multi-document summarization based on sentence
compression. We propose three types of com-
pression approaches. Our tree-based compres-
sion method can easily incorporate measures of
query relevance, content importance, redundancy
and language quality into the compression pro-
cess. By testing on a standard dataset using the
automatic metric ROUGE, our models show sub-
stantial improvement over pure extraction-based
methods and state-of-the-art systems. Our best
system also yields better results for human eval-
uation based on Pyramid and achieves comparable
linguistic quality scores.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation Grant IIS-0968450 and a gift
from Boeing. We thank Ding-Jung Han, Young-
Suk Lee, Xiaoqiang Luo, Sameer Maskey, Myle
Ott, Salim Roukos, Yiye Ruan, Ming Tan, Todd
Ward, Bowen Zhou, and the ACL reviewers for
valuable suggestions and advice on various as-
pects of this work.
1392
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax directed
translations and the pushdown assembler. J. Comput. Syst.
Sci., 3(1):37?56.
Regina Barzilay, Noemie Elhadad, and Kathleen R. McKe-
own. 2002. Inferring strategies for sentence ordering in
multidocument news summarization. J. Artif. Int. Res.,
17(1):35?55, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011.
Jointly learning to extract and compress. ACL ?11, pages
481?490, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput. Linguist.,
22(1):39?71, March.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural
Language Processing with Python. O?Reilly Media.
T. Briscoe and J. Carroll. 2002. Robust accurate statistical
annotation of general text.
Christopher J.C. Burges, Robert Ragno, and Quoc Viet Le.
2007. Learning to rank with nonsmooth cost functions. In
B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Advances
in Neural Information Processing Systems 19, pages 193?
200. MIT Press, Cambridge, MA.
Christopher J. C. Burges. 2010. From RankNet to Lamb-
daRank to LambdaMART: An overview. Technical report,
Microsoft Research.
Asli Celikyilmaz and Dilek Hakkani-Tu?r. 2011. Discovery
of topically coherent sentences for extractive summariza-
tion. ACL ?11, pages 491?499, Stroudsburg, PA, USA.
Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global inference
for sentence compression an integer linear programming
approach. J. Artif. Int. Res., 31(1):399?429, March.
John M. Conroy, Judith D. Schlesinger, Dianne P. O?Leary,
and Jade Goldstein, 2006. Back to Basics: CLASSY 2006.
U.S. National Inst. of Standards and Technology.
Hoa T. Dang. 2005. Overview of DUC 2005. In Document
Understanding Conference.
Hoa Tran Dang. 2006. Overview of DUC 2006. In
Proc. Document Understanding Workshop, page 10 pages.
NIST.
Hoa T. Dang. 2007. Overview of DUC 2007. In Document
Understanding Conference.
Van Dang. 2011. RankLib. Online.
Hal Daume?, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. ACL ?06, pages 305?312,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sashka T. Davis, John M. Conroy, and Judith D. Schlesinger.
2012. Occams - an optimal combinatorial covering algo-
rithm for multi-document summarization. In ICDM Work-
shops, pages 454?463.
Bonnie J Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: a parse-and-trim approach to headline
generation. In Proceedings of the HLT-NAACL 03 on
Text summarization workshop - Volume 5, HLT-NAACL-
DUC ?03, pages 1 ? 8, Stroudsburg, PA, USA. Association
for Computational Linguistics, Association for Computa-
tional Linguistics.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank: graph-
based lexical centrality as salience in text summarization.
J. Artif. Int. Res., 22(1):457?479, December.
Radu Florian, Hany Hassan, Abraham Ittycheriah, Hongyan
Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov,
and Salim Roukos. 2004. A statistical model for multilin-
gual entity detection and tracking. In HLT-NAACL, pages
1?8.
Maria Fuentes, Enrique Alfonseca, and Horacio Rodr??guez.
2007. Support vector machines for query-focused sum-
marization trained and evaluated on pyramid data. In Pro-
ceedings of the 45th Annual Meeting of the ACL on In-
teractive Poster and Demonstration Sessions, ACL ?07,
pages 57?60, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lexicalized
Markov grammars for sentence compression. NAACL
?07, pages 180?187, Rochester, New York, April. Asso-
ciation for Computational Linguistics.
Dan Gillick and Benoit Favre. 2009. A scalable global model
for summarization. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Process-
ing, ILP ?09, pages 10?18, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
David Graff. 2003. English Gigaword.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summarization.
NAACL ?09, pages 362?370, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. 2009.
The weka data mining software: an update. SIGKDD Ex-
plor. Newsl., 11(1):10?18, November.
Jagadeesh Jagarlamudi, Prasad Pingali, and Vasudeva Varma,
2006. Query Independent Sentence Scoring approach to
DUC 2006.
J. Peter Kincaid, Robert P. Fishburne, Richard L. Rogers, and
Brad S. Chissom. 1975. Derivation of New Readability
Formulas (Automated Readability Index, Fog Count and
Flesch Reading Ease Formula) for Navy Enlisted Person-
nel. Technical report, February.
Kevin Knight and Daniel Marcu. 2000. Statistics-based sum-
marization - step one: Sentence compression. AAAI ?00,
pages 703?710. AAAI Press.
Finley Lacatusu, Andrew Hickl, Kirk Roberts, Ying Shi,
Jeremy Bensley, Bryan Rink, Patrick Wang, and Lara Tay-
lor, 2006. LCCs gistexter at duc 2006: Multi-strategy
multi-document summarization.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
1393
Proceedings of the Eighteenth International Conference
on Machine Learning, ICML ?01, pages 282?289, San
Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
P. M. Lewis, II and R. E. Stearns. 1968. Syntax-directed
transduction. J. ACM, 15(3):465?488, July.
Hui Lin and Jeff Bilmes. 2011. A class of submodular func-
tions for document summarization. In Proceedings of the
49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume 1,
HLT ?11, pages 510?520, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2000. The automated ac-
quisition of topic signatures for text summarization. In
Proceedings of the 18th conference on Computational
linguistics - Volume 1, COLING ?00, pages 495?501,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic eval-
uation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1,
pages 71?78.
Chin-Yew Lin. 2003. Improving summarization perfor-
mance by sentence compression: a pilot study. In Pro-
ceedings of the sixth international workshop on Informa-
tion retrieval with Asian languages - Volume 11, AsianIR
?03, pages 1?8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual
coreference resolution with syntactic features. In
HLT/EMNLP.
Xiaoqiang Luo, Abraham Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based on
the bell tree. In ACL, pages 135?142.
Andre? F. T. Martins and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and com-
pression. In Proceedings of the Workshop on Integer Lin-
ear Programming for Natural Langauge Processing, ILP
?09, pages 1?9, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.cs.umass.edu.
Ryan McDonald. 2006. Discriminative Sentence Compres-
sion with Soft Syntactic Constraints. In Proceedings of
the 11th?EACL, Trento, Italy, April.
Michael Mozer, Michael I. Jordan, and Thomas Petsche, ed-
itors. 1997. Advances in Neural Information Processing
Systems 9, NIPS, Denver, CO, USA, December 2-5, 1996.
MIT Press.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluating
content selection in summarization: The pyramid method.
In Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, HLT-NAACL 2004: Main Proceedings, pages 145?
152, Boston, Massachusetts, USA, May 2 - May 7. Asso-
ciation for Computational Linguistics.
Jahna Otterbacher, Gu?nes? Erkan, and Dragomir R. Radev.
2005. Using random walks for question-focused sentence
retrieval. In Proceedings of the conference on Human
Language Technology and Empirical Methods in Natural
Language Processing, HLT ?05, pages 915?922, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.
You Ouyang, Wenjie Li, Sujian Li, and Qin Lu. 2011.
Applying regression models to query-focused multi-
document summarization. Inf. Process. Manage.,
47(2):227?237, March.
Prasad Pingali, Rahul K, and Vasudeva Varma, 2007. IIIT
Hyderabad at DUC 2007. U.S. National Inst. of Standards
and Technology.
Chao Shen and Tao Li. 2011. Learning to rank for query-
focused multi-document summarization. In Diane J.
Cook, Jian Pei, Wei Wang 0010, Osmar R. Zaane, and
Xindong Wu, editors, ICDM, pages 626?634. IEEE.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of ICSLP, volume 2,
pages 901?904, Denver, USA.
Kristina Toutanova, Chris Brockett, Michael Gamon, Ja-
gadeesh Jagarlamudi, Hisami Suzuki, and Lucy Vander-
wende. 2007. The PYTHY Summarization System: Mi-
crosoft Research at DUC 2007. In Proc. of DUC.
Jenine Turner and Eugene Charniak. 2005. Supervised and
unsupervised learning for sentence compression. ACL
?05, pages 290?297, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Zajic, Bonnie J Dorr, Jimmy Lin, and R. Schwartz.
2006. Sentence compression as a component of a multi-
document summarization system. Proceedings of the
2006 Document Understanding Workshop, New York.
1394
