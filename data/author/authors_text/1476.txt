Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 471?478,
New York, June 2006. c?2006 Association for Computational Linguistics
Cross Linguistic Name Matching in English and Arabic: A ?One to 
Many Mapping? Extension of the Levenshtein Edit Distance Algorithm 
Dr. Andrew T. Freeman, Dr. Sherri L. Condon and  
Christopher M. Ackerman 
The Mitre Corporation 
7525 Colshire Dr 
McLean, Va 22102-7505 
{afreeman, scondon, cackerman}@mitre.org 
Abstract 
This paper presents a solution to the prob-
lem of matching personal names in Eng-
lish to the same names represented in 
Arabic script.  Standard string comparison 
measures perform poorly on this task due 
to varying transliteration conventions in 
both languages and the fact that Arabic 
script does not usually represent short 
vowels.  Significant improvement is 
achieved by augmenting the classic 
Levenshtein edit-distance algorithm with 
character equivalency classes.  
1 Introduction to the problem 
Personal names are problematic for all language 
technology that processes linguistic content, espe-
cially in applications such as information retrieval, 
document clustering, entity extraction, and transla-
tion.  Name matching is not a trivial problem even 
within a language because names have more than 
one part, including titles, nicknames, and qualifiers 
such as Jr. or II.  Across documents, instances of 
the name might not include the same name parts, 
and within documents, the second or third mention 
of a name will often have only one salient part.  In 
multilingual applications, the problem is compli-
cated by the fact that when a name is represented 
in a script different from its native script, there 
may be several alternative representations for each 
phoneme, leading to large number of potential 
variants for multi-part names. 
A good example of the problem is the name of 
the current leader of Libya.  In Arabic, there is 
only one way to write the consonants and long 
vowels of any person?s name, and the current 
leader of Libya?s name in un-vocalized Arabic text 
can only be written as ???? ???????.  In English, 
his name has many common representations.  Ta-
ble 1 documents the top five hits returned from a 
web search at www.google.com, using various 
English spellings of the name.   
 
       Version            Occurrences 
Muammar Gaddafi  43,500 
Muammar Qaddafi  35,900 
Moammar Gadhafi  34,100 
Muammar Qadhafi  15,000 
Muammar al Qadhafi 11,500 
 
Table 1.  Qadhafy?s names in English 
 
Part of this variation is due to the lack of an 
English phoneme corresponding to the Standard 
Arabic phoneme /q/.  The problem is further com-
pounded by the fact that in many dialects spoken in 
the Arabic-speaking world, including Libya, this 
phoneme is pronounced as [g]. 
The engineering problem is how one reliably 
matches all versions of a particular name in lan-
guage A to all possible versions of the same name 
in language B.  Most solutions employ standard 
string similarity measures, which require the 
names to be represented in a common character 
set.  The solution presented here exploits translit-
eration conventions in normalization procedures 
and equivalence mappings for the standard Leven-
shtein distance measure.  
2 Fuzzy string matching 
The term fuzzy matching is used to describe 
methods that match strings based on similarity 
rather than identity.  Common fuzzy matching 
techniques include edit distance, n-gram matching, 
and normalization procedures such as Soundex.  
471
This section surveys methods and tools currently 
used for fuzzy matching.   
2.1 Soundex 
 Patented in 1918 by Odell and Russell the 
Soundex algorithm was designed to find spelling 
variations of names.  Soundex represents classes of 
sounds that can be lumped together.  The precise 
classes and algorithm are shown below in figures 1 
and 2.  
 
Code:   0 1         2       3       4     5      6 
Letters: aeiouy bp    cgjkq   dt      l    mn     r 
 hw fv     sxz 
Figure 1: Soundex phonetic codes 
 
1. Replace all but the first letter of the string by its 
phonetic code. 
2. Eliminate any adjacent repetitions of codes. 
3. Eliminate all occurrences of code 0, i.e. eliminate 
all vowels. 
4. Return the first four characters of the resulting 
string. 
5. Examples: Patrick = P362, Peter  = P36, Peterson = 
P3625 
Figure 2: The Soundex algorithm 
 
The examples in figure 2 demonstrate that 
many different names can appear to match each 
other when using the Soundex algorithm. 
2.2 Levenshtein  Edit Distance 
The Levenshtein algorithm is a string edit-
distance algorithm.  A very comprehensive and 
accessible explanation of the Levenshtein algo-
rithm is available on the web at 
http://www.merriampark.com/ld.htm.    
The Levenshtein algorithm measures the edit 
distance where edit distance is defined as the num-
ber of insertions, deletions or substitutions required 
to make the two strings match.  A score of zero 
represents a perfect match.  
With two strings, string s of size m and string t 
of size n, the algorithm has O(nm) time and space 
complexity.  A matrix is constructed with n rows 
and m columns.  The function e(si,tj) where si is a 
character in the string s, and tj is a character in 
string t returns a 0 if the two characters are equal 
and a 1 otherwise.  The algorithm can be repre-
sented compactly with the recurrence relation 
shown in figure 3. 
 
Figure 3. Recurrence relation for Levenshtein edit distance  
 
A simple ?fuzzy-match? algorithm can be cre-
ated by dividing the Levenshtein edit distance 
score by the length of the shortest (or longest) 
string, subtracting this number from one, and set-
ting a threshold score that must be achieved in or-
der for the strings to be considered a match.  In this 
simple approach, longer pairs of strings are more 
likely to be matched than shorter pairs of strings 
with the same number of different characters.   
2.3 Editex 
The Editex algorithm is described by Zobel and 
Dart (1996).  It combines a Soundex style algo-
rithm with Levenshtein by replacing the e(si,tj) 
function of Levenshtein with a function r(si,tj).  
The function r(si,tj) returns 0 if the two letters are 
identical, 1 if they belong to the same letter group 
and 2 otherwise.  The full algorithm with the letter 
groups is shown in figures 4 and 5.  The Editex 
algorithm neutralizes the h and w.  This shows up 
in the algorithm description as d(si-1,si).  It is the 
same as r(si,tj), with two exceptions.  It compares 
letters of the same string rather than letters from 
the different strings.  The other difference is that if 
si-1 is h or w, and si-1?si, then d(si-1,si) is one. 
 
Figure 4: Recurrence relation for Editex edit distance 
   0        1     2      3   4    5    6     7        8      9 
for each i from 0 to |s| 
 for each j from 0 to |t| 
levenshtein(0; 0) = 0 
levenshtein(i; 0) = i 
levenshtein(0;j) = j 
levenshtein (i;j) = 
 min[levenshtein (i ? 1; j) + 1; 
levenshtein(i; j ? 1) + 1; 
levenshtein(i ? 1; j ? 1) + 
e(si; tj )] 
for each i from 0 to |s| 
 for each j from 0 to |t| 
editex(0; 0) = 0 
editex(i; 0) = editex(i ? 1; 0) + d(si?1; si) 
editex(0; j) = editex(0; j ? 1) + d(tj?1; tj ) 
editex(i; j) = min[editex (i ? 1; j) +  
d(si?1; si); 
ediext(i; j ? 1) + d(tj?1; tj); 
editex(i ? 1; j ? 1) + r(si; tj )] 
 
472
aeiouy   bp   ckq  dt   lr  mn  gj   fpv    sxz   csz 
Figure 5: Editex letter groups 
Zobel and Dart (1996) discuss several en-
hancements to the Soundex and Levenshtein string 
matching algorithms.  One enhancement is what 
they call ?tapering.?  Tapering involves weighting 
mismatches at the beginning of the word with a 
higher score than mismatches towards the end of 
the word.  The other enhancement is what they call 
phonometric methods, in which the input strings 
are mapped to pronunciation based phonemic rep-
resentations.  The edit distance algorithm is then 
applied to the phonemic representations of the 
strings.   
Zobel and Dart report that the Editex algorithm 
performed significantly better than alternatives 
they tested, including Soundex, Levenshtein edit 
distance, algorithms based on counting common n-
gram sequences, and about ten permutations of 
tapering and phoneme based enhancements to as-
sorted combinations of Soundex, n-gram counting 
and Levenshtein.  
2.4 SecondString 
SecondString, described by Cohen, Ravikumar 
and Fienberg (2003) is an open-source library of 
string-matching algorithms implemented in Java.  
It is freely available at the web site 
http://secondstring.sourceforge.net.   
The SecondString library offers a wide assort-
ment of string matching algorithms, both those 
based on the ?edit distance? algorithm, and those 
based on other string matching algorithms.  Sec-
ondString also provides tools for combining 
matching algorithms to produce hybrid-matching 
algorithms, tools for training on string matching 
metrics and tools for matching on tokens within 
strings for multi-token strings.  
3  Baseline task 
An initial set of identical names in English and 
Arabic script were obtained from 106 Arabic texts 
and 105 English texts in a corpus of newswire arti-
cles.  We extracted 408 names from the English 
language articles and 255 names from the Arabic 
language articles.  Manual cross-script matching 
identified 29 names common to both lists.    
For a baseline measure, we matched the entire 
list of names from the Arabic language texts 
against the entire list of English language names 
using algorithms from the SecondString toolkit.  
The Arabic names were transliterated using the 
computer program Artrans produced by Basis 
(2004).  
For each of these string matching metrics, the 
matching threshold was empirically set to a value 
that would return some matches, but minimized 
false matches.  The Levenshtein ?edit-distance? 
algorithm returns a simple integer indicating the 
number of edits required to make the two strings 
match.  We normalized this number by using the 
formula ???
?
???
?
+
?
ts
tsnLevenshtei ),(1 , where any pair 
of strings with a fuzzy match score less than 0.875 
was not considered to be a match.  The intent of 
dividing by the length of both names is to mini-
mize the weight of a mismatched character in 
longer strings. 
For the purposes of defining recall and preci-
sion, we ignored all issues dealing with the fact 
that many English names correctly matched more 
than one Arabic name, and that many Arabic 
names correctly matched more than one English 
name.  The number of correct matches is the num-
ber of correct matches for each Arabic name, 
summed across all Arabic names having one or 
more matches.  Recall R is defined as the number 
of correctly matched English names divided by the 
number of available correct English matches in the 
test set.  Precision P is defined as the total number 
of correct names returned by the algorithm divided 
by the total number of names returned.  The F-
score is
( )
RP
PR
+
?2 . 
Figure 5 shows the results obtained from the 
four algorithms that were tested.  Smith-Waterman 
is based on Levenshtein edit-distance algorithm, 
with some parameterization of the gap score.  
SLIM is an iterative statistical learning algorithm 
based on a variety of estimation-maximization in 
which a Levenshtein edit-distance matrix is itera-
tively processed to find the statistical probabilities 
of the overlap between two strings.  Jaro is a type 
n-gram algorithm which measures the number and 
the order of the common characters between two 
strings. Needleman-Wunsch from Cohen et al?s 
(2003) SecondString Java code library is the Java 
implementation referred to as ?Levenshtein edit 
473
distance? in this report.  The Levenshtein algo-
rithms clearly out performed the other metrics.   
 
Algorithm Recall Precision F-score 
Smith Waterman 14/29 14/18 0.5957 
SLIM 3/29 3/8 0.1622 
Jaro 8/29 8/11 0.4 
NeedlemanWunsch  19/29 19/23 0.7308 
Figure 5: Comparison of string similarity metrics 
4 Motivation of enhancements  
One insight is that each letter in an Arabic 
name has more than one possible letter in its Eng-
lish representation.  For instance, the first letter of 
former Egyptian president Gamal Abd Al-Nasser?s 
first name is written with the Arabic letter  ??, 
which in most other dialects of Arabic is pro-
nounced either as [??] or [?], most closely resem-
bling the English pronunciation of the letter ?j?.  
As previously noted, ?? has the received pronun-
ciation of [q], but in many dialects it is pronounced 
as [g], just like the Egyptian pronunciation of Nas-
ser?s first name Gamal.  The conclusion is that 
there is no principled way to predict a single repre-
sentation in English for an Arabic letter. 
Similarly, Arabic representations of non-native 
names are not entirely predictable.  Accented syl-
lables will be given a long vowel, but in longer 
names, different writers will place the long vowels 
showing the accented syllables in different places.  
We observed six different ways to represent the 
name Milosevic in Arabic.  
The full set of insights and ?real-world? knowl-
edge of the craft for representing foreign names in 
Arabic and English is summarized in figure 6.  
These rules are based on first author Dr. Andrew 
Freeman?s1 experience with reading and translating 
Arabic language texts for more than 16 years. 
1) The hamza (?) and the ?ayn (?) will 
often appear in English language texts 
as an apostrophe or as the vowel that 
follows. 
2) Names not native to Arabic will have a 
long vowel or diphthong for accented 
syllables represented by ?w,? ?y? or ?A.  
3) The high front un-rounded diphthong 
(?i,? ?ay?, ?igh?) found in non-Arabic 
names will often be represented with an 
alif-yaa (???) sequence in the Arabic 
                                                          
1 Dr. Freeman?s PhD dissertation was on Arabic dialectology.  
script. 
4) The back rounded diphthongs, (ow, au, 
oo) will be represented with a single 
?waw? in Arabic. 
5) The Roman scripts letters ?p? and ?v? 
are represented by ?b? and ?f? in Arabic.  
The English letter ?x? will appear as the 
sequence ?ks? in Arabic  
6) Silent letters, such as final ?e? and in-
ternal ?gh? in English names will not 
appear in the Arabic script. 
7) Doubled English letters will not be rep-
resented in the Arabic script. 
8) Many Arabic names will not have any 
short vowels represented. 
9) The ?ch? in the English name ?Richard? 
will be represented with the two charac-
ter sequence ?t? (?) and ?sh? (?).  The 
name ?Buchanan? will be represented in 
Arabic with the letter ?k? (?). 
Figure 6: Rules for Arabic and English representations 
5 Implementation of the enhancements 
5.1 Character Equivalence Classes (CEQ):  
The implementation of the enhancements has 
six parts.  We replaced the comparison for the 
character match in the Levenshtein algorithm with 
a function Ar(si, tj ) that returns zero if the character 
tj from the English string is in the match set for the 
Arabic character si;, otherwise it returns a one.   
 
 
Figure 7: Cross linguistic Levenshtein 
 
String similarity measures require the strings to 
have the same character set, and we chose to use 
transliterated Arabic so that investigators who 
could not read Arabic script could still view and 
understand the results.  The full set of transliterated 
Arabic equivalence classes is shown in Figure 8.  
The set was intentionally designed to handle Ara-
bic text transliterated into either the Buckwalter 
for each i from 0 to |s| 
for each j from 0 to |t| 
levenshtein(0; 0) = 0 
levenshtein(i; 0) = i 
levenshtein(0;j) = j 
levenshtein (i;j) = 
min 
   [levenshtein (i ? 1; j) + 1; 
   levenshtein(i; j ? 1) + 1; 
   levenshtein(i ? 1; j ? 1) + Ar(si; tj )] 
474
transliteration (Buckwalter, 2002) or the default 
setting of the transliteration software developed by 
Basis Technology (Basis, 2004). 
5.2 Normalizing the Arabic string 
The settings used with the Basis Artrans trans-
literation tool transforms certain Arabic letters into 
English digraphs with the appropriate two charac-
ters from the following set: (kh, sh, th, dh).  The 
Buckwalter transliteration method requires a one-
to-one and recoverable mapping from the Arabic 
script to the transliterated script.  We transformed 
these characters into the Basis representation with 
regular expressions.  These regular expressions are 
shown in figure 9 as perl script. 
Translit-
eration 
English equivalence class Arabic 
letter 
'  ',a ,A,e,E,i,I,o,O,u,U  ? 
| ',a ,A,e ,E,i ,I,o ,O,u ,U ? 
> ',a ,A,e ,E,i ,I,o ,O,u ,U ? 
& ',a ,A,e ,E,i ,I,o ,O,u ,U ? 
< ',a ,A,e ,E,i ,I,o ,O,u ,U ? 
} ',a ,A,e ,E,i ,I,o ,O,u ,U ? 
A ',a ,A,e ,E,i ,I,o ,O,u ,U ? 
b b ,B,p ,P,v,V ? 
p a ,e ? 
+ a ,e ? 
t t,T ? 
v t ,T ? 
j j,J,g,G ?? 
H h, H ?? 
x k, K ??  
d d, D ? 
* d, D ? 
r r, R ? 
z z, Z ? 
s s, S,c, C ? 
$ s, S ? 
S s, S ? 
D d, D ? 
T t, T ? 
Z z, Z,d, D ?  
E ',`,c,a,A,e,E,i,I,o,O,u,U ? 
` ',`,c,a,A,e,E,i,I,o,O,u,U ? 
g g, G ?  
f f, F,v, V ? 
q q, Q, g, G,k, K ?  
k k, K,c, C,S, s ? 
l l, L ?  
m m, M ?  
n n, N ?  
h h, H ?? 
w w, W,u, u,o, O, 0 ?  
y y, Y, i, I, e, E, ,j, J ? 
Y a, A,e, E,i, I, o,O,u, U ? 
a a, e ?? 
i i, e ??  
u u, o ?? 
Figure 8: Arabic to English character equivalence sets 
 
 
Figure 9. Normalizing the Arabic 
5.3 Normalizing the English string 
Normalization enhancements were aimed at 
making the English string more closely match the 
transliterated form of the Arabic string.  These cor-
respond to points 2 through 7 of the list in Figure 
6.  The perl code that implemented these transfor-
mations is shown in figure 10.  
Figure 10. Normalizing the English 
5.4 Normalizing the vowel representations  
Normalization of the vowel representations is 
based on two observations that correspond to 
points 2 and 8 of Figure 6.  Figure 11 shows some 
English names represented in Arabic transliterated 
using the Buckwalter transliteration method. 
Name in English Name in Arabic Arabic  
transliteration 
Bill Clinton ??? ??????? byl klyntwn 
Colin Powell ????? ???? kwlyn bAwl 
$s2 =~ s/(a|e|i|A|E|I)(e|i|y)/y/g; 
# hi dipthongs go to y in Arabic 
$s2 =~ s/(e|a|o)(u|w|o)/w/g;  
 # lo dipthongs go to w in Arabic 
$s2 =~ s/(P|p)h/f/g;  # ph -> f in Arabic 
$s2 =~ s/(S|s)ch/sh/g; # sch is sh 
$s2 =~ s/(C|c)h/tsh/g; # ch is tsh or k ,  
# we catch the "k" on the pass 
$s2 =~ s/-//g; # eliminate all hyphens 
$s2 =~ s/x/ks/g; # x->ks in Arabic 
$s2 =~ s/e( |$)/$1/g; # the silent final e  
$s2 =~ s/(\S)\1/$1/g; # eliminate duplicates 
$s2 =~ s/(\S)gh/$1/g; # eliminate silent gh  
$s2 =~ s/\s//g;  # eliminate white space 
$s2 =~ s/(\.|,|;)//g; # eliminate punctuation
$s1 =~ s/\$/sh/g; #  normalize Buckwalter 
$s1 =~ s/v/th/g; # normalize Buckwalter 
$s1 =~ s/\*/dh/g; # normalize Buckwalter 
$s1 =~ s/x/kh/g; # normalize Buckwalter 
$s1 =~ s/(F|K|N|o|~)//g; #  remove case vowels,  
# the shadda and the sukuun 
$s1 =~ s/\'aa/\|/g; # normalize basis w/  
# Buckwalter madda 
$s1 =~ s/(U|W|I|A)/A/g; # normalize  hamza 
$s1 =~ s/_//; # eliminate underscores 
$s1 =~ s/\s//g; # eliminate white space 
475
Richard Cheney ??????? ????? rytshArd 
tshyny 
Figure 11. English names as represented in Arabic 
All full, accented vowels are represented in the 
Arabic as a long vowel or diphthong.  This vowel 
or diphthong will appear in the transliterated un-
vocalized text as either a ?w,? ?y? or ?A.?  Unac-
cented short vowels such as the ?e? found in the 
second syllable of ?Powell? are not represented in 
Arabic.  Contrast figure 11 with the data in figure 
12. 
 
Name in 
Arabic 
Arabic  
transliteration Name in English 
 ?????
????? ??? 
mSTfY Alshykh 
dyb 
Mustafa al Sheikh 
Deeb 
???? ???? mHmd EATf Muhammad Atef 
???? ????? Hsny mbArk Hosni Mubarak 
Figure 12. Arabic names as represented in English 
The Arabic only has the lengtheners ?y?, ?w?, 
or ?A? where there are lexically determined long 
vowels or diphthongs in Arabic.  The English rep-
resentation of these names must contain a vowel 
for every syllable.  The edit-distance score for 
matching ?Muhammad? with ?mHmd? will fail 
since only 4 out of 7 characters match.  Lowering 
the match threshold will raise the recall score while 
lowering the precision score.  Stripping all vowels 
from both strings will raise the precision on the 
matches for Arabic names in English, but will 
lower the precision for English names in Arabic. 
 Figure 13.  Algorithm for retaining matching vowels  
 
The algorithm presented in figure 13 retains 
only those vowels that are represented in both 
strings.  The algorithm is a variant of a sorted file 
merge. 
5.5 Normalizing ?ch? representations with a 
separate pass 
This enhancement requires a separate pass.  The 
name ?Buchanan? is represented in Arabic as ?by-
wkAnAn? and ?Richard? is ?rytshArd.?  Thus, 
whichever choice the software makes for the cor-
rect value of the English substring ?ch,? it will 
choose incorrectly some significant number of 
times.  In one pass, every ?ch? in the English string 
gets mapped to ?tsh.?  In a separate pass, every 
?ch? in the English string is transformed into a ?k.? 
5.6 Light Stemming 
The light stemming performed here was to re-
move the first letter of the transliterated Arabic 
name if it matched the prefixes ?b,? ?l? or ?w? and 
run the algorithm another time if the match score 
was below the match threshold but above another 
lower threshold.  The first two items are preposi-
tions that attach to any noun.  The third is a con-
junction that attaches to any word.  Full stemming 
for Arabic is a separate and non-trivial problem. 
6 Results 
The algorithm with all enhancements was im-
plemented in perl and in Java.  Figure 14 presents 
the results of the enhanced algorithm on the origi-
nal baseline as compared with the baseline algo-
rithm.  The enhancements improved the F-score by 
22%.   
Algorithm Recall Precision F-score 
Baseline  19/29 19/23 0.7308 
Enhancements 29/29 29/32 0.9508 
Figure 14. Enhanced edit distance on original data set 
6.1 Results with a larger data set 
After trying the algorithm out on a couple more 
?toy? data sets with similar results, we used a more 
realistic data set, which I will call the TDT data 
set.  This data set was composed of 577 Arabic 
names and 968 English names that had been manu-
ally extracted from approximately 250 Arabic and 
English news articles on common topics in a NIST 
TDT corpus.  There are 272 common names.  The 
number of strings on the English side that correctly 
For  each i from 0 to min(|Estring|, |Astring|),  
each j from 0 to min(|Estring|, |Astring|) 
if Astringi equals Estringj 
       Outstringi = Estringi increment i and j 
if vowel(Astringi) and vowel(Estringj) 
       Outstringi = Estringi increment i and j 
if  not vowel(Astringi) and vowel(Estringj) 
       increment j but not i 
       if j < |Estring| 
           Outstringi = Estring; increment i and j 
otherwise 
      Outstringi = Estringi; increment i and j 
Finally if there is anything left of Estring,  
strip all vowels from what is left 
append Estring to end of Outstring
476
match an Arabic language string is 591.  The actual 
number of matches in the set is 641, since many 
Arabic strings match to the same set of English 
names.  For instance, ?Edmond Pope? has nine 
variants in English and six variants in Arabic.  This 
gives 36 correct matches for the six Arabic spell-
ings of Edmond Pope. 
We varied the match threshold for various 
combinations of the described enhancements.  The 
plots of the F-score, precision and recall from these 
experiments using the TDT data set are shown in 
figures 15, 16, and 17. 
7 Discussion 
Figure 15 shows that simply adding the ?char-
acter equivalency classes? (CEQ) to the baseline 
algorithm boosts the F-score from around 48% to 
around 72%.  Adding all other enhancements to the 
baseline algorithm, without adding CEQ only im-
proves the f-score marginally.  Combining these 
same enhancements with the CEQ raises the f-
score by roughly 7% to almost 80%.   
When including CEQ, the algorithm has a peak 
performance with a threshold near 85%.  When 
CEQ is not included, the algorithm has a peak per-
formance when the match threshold is around 70%.  
The baseline algorithm will declare that the strings 
match at a cutoff of 70%.  Because we are normal-
izing by dividing by the lengths of both strings, 
this allows strings to match when half of their let-
ters do not match.  The CEQ forces a structure 
onto which characters are an allowable mismatch 
before the threshold is applied.  This apparently 
leads to a reduction in the number allowable mis-
matches when the match threshold is tested.  
The time and space complexity of the baseline 
Levenshtein algorithm is a function of the length of 
the two input strings, being |s| * |t|.  This makes the 
time complexity (N2) where N is the size of the 
average input string.  The enhancements described 
here add to the time complexity.  The increase is 
an average two or three extra compares per charac-
ter and thus can be factored out of any equation.  
The new time complexity is K(|s|*|t|) where K >= 
3. 
What we do here is the opposite of the approach 
taken by the Soundex and Editex algorithms.  They 
try to reduce the complexity by collapsing groups 
of characters into a single super-class of characters.  
The algorithm here does some of that with the 
steps that normalize the strings.  However, the 
largest boost in performance is with CEQ, which 
expands the number of allowable cross-language 
matches for many characters. 
One could expect that increasing the allowable 
number of matches would over-generate, raising 
the recall while lowering the precision.   
Referring to Figure 8, we see that?s ome Arabic 
graphemes map to overlapping sets of characters in 
the English language strings.   
Arabic ?? can be realized, as either [j] or [g], 
and one of the reflexes in English for Arabic ? can 
be [g] as well.  How do we differentiate the one 
from the other?  Quite simply, the Arabic input is 
not random data.  Those dialects that produce ? as 
a [g] will as a rule not produce ?? as [g] and vice 
versa.  The Arabic pronunciation of the string de-
termines the correct alternation of the two charac-
ters for us as it is written in English.  On a string-
by-string basis, it is very unlikely that the two rep-
resentations will conflict.  The numbers show that 
by adding CEQ, the baseline algorithm?s recall at 
threshold of 72.5%, goes from 57% to around 67% 
at a threshold of 85% for Arabic to English cross-
linguistic name matching.  Combining all of the 
enhancements raises the recall at a threshold of 
85%, to 82%.  As previously noted, augmenting 
the baseline algorithm with all enhancements ex-
cept CEQ, does improve the performance dramati-
cally.  CEQ combines well with the other 
enhancements.   
It is true that there is room for a lot improve-
ment with an f-score of 80%.  However, anyone 
doing cross-linguistic name matches would proba-
bly benefit by implementing some form of the 
character equivalence classes detailed here. 
477
Figure 15: F-score by match threshold
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.875 0.85 0.825 0.8 0.775 0.75 0.725 0.7
Threshold
F-score
F all enh F str norm
F vowel dance F String Norm & Vowel Dance
F all No Eq Class F baseline
F Eq Class only F EC StrN VowD  
Figure 16: Recall by threshold
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.875 0.85 0.825 0.8 0.775 0.75 0.725 0.7
Threshold
R
R baseline R StrNrm & Vow elD R eq class only
R EC StrN Vow D R all
 
Figure 17: Precision by threshold
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.875 0.85 0.825 0.8 0.775 0.75 0.725 0.7
Threshold
P
P  eq class only P all P baseline P all No Eq Class
 
References 
Basis Technology.  2004.  Arabic Transliteration Mod-
ule.  Artrans documentation.  (The documentation  is 
available for download at 
http://www.basistech.com/arabic-editor.) 
Bilenko, Mikael, Mooney, Ray, Cohen, William W., 
Ravikumar, Pradeep and Fienberg, Steve. 2003. 
Adaptive Name-Matching. in Information Integration 
in IEEE Intelligent Systems, 18(5): 16-23. 
Buckwalter, Tim. 2002.  Arabic Transliteration. 
http://www.qamus.org/transliteration.htm. 
Cohen, William W., Ravikumar, Pradeep and Fienberg, 
Steve. 2003.  A Comparison of String Distance Met-
rics for Name-Matching Tasks. IIWeb 2003: 73-78.  
Jackson, Peter and Moulinier, Isabelle. 2002 . Natural 
Language Processing for Online Applications: Text 
Retrieval, Extraction, and Categorization (Natural 
Language Processing, 5). John Benjamins Publish-
ing. 
Jurafsky, Daniel, and James H. Martin. 2000. Speech 
and Language Processing: An Introduction to Natu-
ral Language Processing, Speech Recognition, and 
Computational Linguistics. Prentice-Hall. 
Knuth, Donald E. 1973. The Art of Computer Pro-
gramming, Volume 3: Sorting and Searching. . Addi-
son-Wesley Publishing Company,  
Ukonnen, E. 1992. Approximate string-matching with 
q-grams and maximal matches.  Theoretical Com-
puter Science, 92: 191-211. 
Wright, W.  1967.  A Grammar of the Arabic Language. 
Cambridge.  Cambridge University Press.      
Zobel, Justin and Dart, Philip. 1996.  Phonetic string 
matching: Lessons from information retrieval.in  
Proceedings of the Eighteenth ACM SIGIR Interna-
tional Conference on Research and Development in 
Information Retrieval, Zurich, Switzerland, August 
1996, pp. 166-173.  
478
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 299?300,
New York City, June 2006. c?2006 Association for Computational Linguistics
1. What?s in a Name: Current Methods, Applications, and Evaluation
in Multilingual Name Search and Matching
Sherri Condon and Keith J. Miller, MITRE
Names of people, places, and organizations have unique linguistic properties, and they typically require
special treatment in automatic processes. Appropriate processing of names is essential to achieve high-
quality information extraction, speech recognition, machine translation, and information management, yet
most HLT applications provide limited specialized processing of names. Variation in the forms of names can
make it difficult to retrieve names from data sources, to perform co-reference resolution across documents,
or to associate instances of names with their representations in gazetteers and lexicons. Name matching
has become critical in government contexts for checking watchlists and maintaining tax, health, and So-
cial Security records. In commercial contexts, name matching is essential in credit, insurance, and legal
applications.
This tutorial will focus on personal names, with special attention given to Arabic names, though it will
be clear that much of the material applies to other languages and to names of places and organizations. Case
studies will be used to illustrate problems and approaches to solutions. Arabic names illustrate many of the
issues encountered in multilingual name matching, among which are complex name structures and spelling
variation due to morphophonemic alternation and competing transliteration conventions.
1.1 Tutorial Outline
1. Name matching across languages, scripts, and cultures
? Survey of problems using Arabic case study
* Name parts and structure (titles, initials, particles, prefixes, suffixes, nicknames,
tribal names)
* Transliteration complications (segmentation, ambiguity, incompleteness, dialect
variation, acoustic mismatches, competing standards)
* Other difficulties presented by personal names
? Survey of approaches to solutions, advantages/disadvantages of each:
* SOUNDEX, generic string matching (Levenshtein, n-gram, Jaro-Winkler),
* Variant generation (pattern matching, dictionaries, gazetteers),
* Normalization (morphological analysis, rewriting, ?deep? structures)
* Intelligent-search algorithms that incorporate linguistic knowledge in selection of
string-similarity measures, parameters, and lists
? Matching across scripts
* Methods for data acquisition
* Transliteration
* Phonological interlingua
2. Evaluation of Name Search and Matching Systems
? Development of ground-truth sets
* Human adjudication
* Estimation techniques
? Case study: adjudication exercises
? Issues in establishing ground truth: different truth for different applications
? Metrics (precision, recall, F scores, others)
? Case study comparing matching systems for Romanized Arabic names (based on MITRE
evaluation of 9 name matching products)
? Inter-adjudicator agreement
? Performance and other considerations
299
1.2 Target Audience
This tutorial is intended for those with interest in information retrieval and entity extraction, identity reso-
lution, Arabic computational linguistics, and related language-processing applications. As a relatively un-
studied domain, name matching is a promising area for innovation and for researchers seeking new projects.
Keith J. Miller received his Ph.D. in Computational Linguistics from Georgetown University. He spent
several years working on various large-scale name matching systems. His current research activities cen-
ter around multicultural name matching, machine translation, embedded HLT systems, and component and
system-level evaluation of systems involving HLT components.
Sherri Condon received her Ph.D. in Linguistics from the University of Texas at Austin. In addition to
several years of work in multilingual name matching and cross script name matching, she is a researcher in
discourse/dialogue, entity extraction, and evaluation of machine translation and dialogue systems.
300
Sharing Problems and Solutions for Machine Translation of
Spoken and Written Interaction
Sherri Condon             Keith Miller
The MITRE Corporation
7515 Colshire Drive
McLean, VA 22102-7508
{scondon, keith}@mitre.org
Abstract
Examples from chat interaction are
presented to demonstrate that machine
translation of written interaction
shares many problems with translation
of spoken interaction. The potential
for common solutions to the problems
is illustrated by describing operations
that normalize and tag input before
translation.  Segmenting utterances
into small translation units and
processing short turns separately  are
also motivated using data from chat.
1 Introduction
The informal, dialogic character of oral
interaction imposes demands on translation
systems that are not encountered in well-formed,
monologic texts.  These differences make it
appear that any similarities between the machine
translation of text and speech will be limited to
core translation components, as opposed to pre-
and post-processing operations that are linked to
the medium.
In this paper, we demonstrate that many
challenges of translating spoken interaction are
also encountered in translating written
interaction such as chat or instant messaging.
Consequently, it is proposed that solutions
developed for these common problems can be
shared by researchers engaged in applying
machine translation technologies to both types
of interaction.  Specifically, preprocessing
operations can address many of the problems
that make dialogic interaction difficult to
translate in both spoken and written media.
After surveying the challenges that are
shared in machine translation of spoken and
written interaction, we identify several areas in
which preprocessing solutions have been
proposed that could be fruitfully adopted for
either spoken or written input.  The speech
recognition problem of discriminating out of
vocabulary words from unrecognized
vocabulary words is equivalent to the problem
of discriminating novel forms that emerge in
chat environments from words that are
unrecognized due to nonstandard spellings.  We
suggest that a solution based on templates like
those used in example-based translation could
be a useful approach to the problem for both
spoken and written input.  Similarly, other
preprocessing operations that tag input for
special processing can be used to facilitate
translation of problematic phenomena such as
discourse markers and vocatives.  Finally, we
explore the possibility that the complexity of
translating interaction can be reduced by
translating smaller packages of input and
exploiting participants? strategies for packaging
certain discourse functions in smaller turn units.
2 Challenges for translation of
spoken and written interaction
In illustrating the problems for machine
translation that are shared by both spoken and
written interactions, we take for granted that
readers are aware of examples that occur in
spoken interaction because these are available in
the literature and from direct observation of
personal experience.  Therefore, we focus on
providing examples of written interaction to
demonstrate that the same kinds of challenges
arise in translation of chat and instant messages.
Most of the examples we present are taken from
logs of chat interactions collected from 10 chat
channels in 8 languages during July of 2001.
The examples are presented exactly as they
appeared in the logs.
                                            Association for Computational Linguistics.
                          Algorithms and Systems, Philadelphia, July 2002, pp. 93-100.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
2.1 Ellipsis and fragments
The elliptical and fragmentary quality of
ordinary spoken dialogue is well-known and is
characteristic of chat interaction, too, as in (1).
(1) a. faut voir
b. voir koi?
The French expression il faut ?it is necessary? is
used without the pleonastic pronoun il, and the
verb voir ?to see? is used without a direct object
expressing what it is necessary to see.  The
writer may have intended to use voir
intransitively, as in the English expression we?ll
have to see, but the interlocutor who responded
with (1b) asks ?to see what?? and omits both the
pronoun and the verb faut.  (Creative spelling
such as the convention that replaces the qu in
quoi with k in koi is discussed in section 3.3.)
Though it is unlikely that preprocessing
operations will be able to add information that is
missing from fragments and elliptical
expressions, these problems interact with
preprocessing operations such as segmentation
of units for translation (see 2.9).
2.2 High function/low content terms
Spoken interaction is replete with formulaic
expressions that serve significant interactional
functions, but have little or no internal structure.
These include greetings, leave-takings,
affirmations, negations, and other interjections,
some of which are illustrated in (2).
(2) a. re esselamu aleyk?m
b. in like califonia
c. jose?lets make 1
The expression re is a conventional greeting in
chat interaction with the function re-greet, as in
hello again.  In (2a) it is used on a Turkish chat
channel preceding a greeting borrowed from
Arabic with the literal meaning ?peace to you.?
The example in (2b) demonstrates that chat
interaction includes expressions such as like that
are usually associated exclusively with speech.
Like and discourse markers such as well, now, so
and anyway occur frequently in chat interaction.
Wiebe et al (1995) identify discourse markers
as a major area of difficulty for translation of
spoken interaction.  Many discourse markers are
homophonous and/or homographic with lexical
items that have very different meanings, and the
need to disambiguate polysemous words has
received much attention in the language
processing and machine translation literature.
The use of vocative proper names illustrated
in (2c) is frequent in chat interaction, where
participants? nicknames are used to direct
messages to specific interlocutors.  In a small
sample of 76 messages from our chat logs, 31%
included vocative uses of participants?
nicknames.  In speech and in chat interaction
like (2), where punctuation is unpredictable (see
3.2), capitalization cannot be relied on to
identify proper names.  The complexity of
translating proper names has also received
considerable attention in the machine translation
research community, and translation of proper
names has been proposed as an evaluation
measure for machine translation (Papineni et al,
2002; Vanni and Miller, 2002).
2.3 Vagueness
Though vagueness is a problem in all language
use, Wiebe et al (1995) identify it as a major
problem for translation of spoken interaction,
citing metaphorical expressions such as de alli,
literally, ?from there,? translated as after that.
More pervasive are the deixis and vagueness
that result from the shared online context that
participants in real time interaction can rely on,
compared to communication environments in
which relevant context must be encoded in the
text itself.   Researchers have demonstrated the
increased explicitness and structural complexity
of asynchronous interaction, in which delays
between the transmission of messages preclude
immediate feedback, compared to synchronous
interaction, in which it is expected that messages
will be responded to immediately (Chafe, 1982;
Condon and Cech, forthcoming; Sotillo, 2000).
Similarly, Popowich et al (2000) report that a
high degree of semantic vagueness is a problem
for translating closed captions because the visual
context provided by the television screen
supplies missing details.
2.4 Anaphora
Another consequence of the synchronous
communication environments in written and
spoken interaction is the high frequency of
pronouns and deictic forms.  Wiebe et al (1995)
report that 64% of utterances in a corpus of
spoken Spanish contained pronominals
compared to 48% of sentences in a written
corpus.  Similarly, numerous studies have
demonstrated the high frequency of personal
pronouns, especially first person pronouns, in
chat interaction compared to other types of
written texts (Ferrara, Brunner and Whittemore,
1991; Flanagan 1996; Yates, 1996).  Pronouns
are particularly problematic for translation when
the target language makes distinctions such as
gender that are not present in the source
language.  To determine the appropriate
inflection, the antecedent of the pronoun must
be identified, and resolution of pronoun
antecedents is another thorny problem that has
attracted much attention from researchers in
machine translation.
2.5  Juncture
Along with the liberties that participants in chat
interaction take with spelling and punctuation
conventions (see 3.1,2), they also deliberately
(and undoubtedly sometimes accidentally) omit
spaces between words, as in (3).
(3) a. selamunaleykum  (= selamun aleykum)
b. aleykumselam   (= aleykum selam)
The Turkish ?peace to you? greeting in (3a) is a
variant of (2a) and is usually represented as two
words, though the merged forms in (3a) and in
the conventional reply (3b) occur several times
in a sample of our corpus.  Consequently, one of
the basic challenges for speech recognition,
identification of word boundaries, is also a
problem in chat interaction.
2.6 Colloquial terms, idioms and slang
Wiebe et al (1995) use the term conventional
constructions to refer to idiosyncratic
collocations and tense/aspect usage. Colloquial
or idiomatic usage complicates translation of
both spoken and chat interaction, though it is
less frequent in formal writing.  (4) provides
some examples from chat.
(4) a. have a ball, y?all
b. do u sleep there n stuff
Like discourse markers, expressions such as
have a ball in (4a) and [and] stuff in (4b), have
both compositional and idiomatic meanings,
which causes ambiguity that must be resolved.
2.7 Code-switching
Code-switching is common in multilingual
speech communities, and the participants in
communication environments like chat tend to
be multilingual.  The Turkish to English switch
in (5a) illustrates.
(5) a. anlamami istedigin seyi anlamadim sorry
b. salam mon frere
In (5b) from the #paris chat channel, Arabic
salam ?peace? is used as a greeting.  Not only
are these switches problematic for translation
engines designed to map a single source
language into a single target language, but also
translation into a single language eliminates the
sociolinguistic and pragmatic effects of code-
switching.
2.8  Language play
Another consequence of the informal contexts in
which speech and chat interaction occur is the
playful use of language for entertainment, and in
online environments like chat, where fun often
is the primary attraction,  humor and language
play are valued (Danet et al, 1995).  In addition
to play with identity and typographic symbols,
which have become conventional in chat
interaction, novel games like (6) emerge.
(6) a. wew
b. wiw
c. wow
(6) is part of a sequence on a Cebuano language
channel in which the game seems to be to
produce consonant frames with different vowels.
It ocurred after another game in which
participants inserted a vocative use of baby (as
in hey baby) into almost every message, often
accompanied by additional codeswitching into
English, and finally prompting the protest, ?you
guys have been on that baby thing for ages.?
2.9 Segmentation of translation units
Just as spoken interaction does not include clear
delimiters for word boundaries, it also lacks
conventional means of marking larger units of
discourse that would be analogous to sentence
punctuation in written texts.  Similarly, though
chat interaction is written, punctuation is often
inconsistent or absent.  For example, vocative
nicknames used to address messages to specific
participants may not be separated from the
remainder of the message by any punctuation or
they may be separated by commas, colons,
ellipses, parentheses, brackets, and emoticons.
The same range of possibilities occurs for
punctuation between sentences, which is
frequently absent.   Consequently, it is difficult
to segment input into consistent units that
translation components can anticipate.
3 Analogous Challenges in Spoken
and Written Interaction
Another set of problems that arise in translation
of written interaction are not found in spoken
interaction because they involve the typographic
symbols that render language in written form.
However, most of the problems have analogies
in spoken interaction, just as lack of punctuation
in writing causes the same juncture and
segmentation problems encountered in speech.
Most of the challenges in Section 2 represent
problems for translation from the source
language to the target language, whereas the
challenges in this section primarily complicate
the problem of recognizing the source message.
3.1 Unintentional misspellings and
typographical errors
Nonstandard spellings occur so frequently in
chat interaction that it is difficult to find
examples that do not contain them, as (2b)
illustrates above.  Online interaction also
contains many deliberate misspellings that are
discussed in the next section.  In addition to
misspellings like (2b) and (7a), we classify as
typographic errors the many instances like (7b)
in which participants fail to punctuate
contractions (though these may be deliberate).
(7) a. hi evenybody
b. bon jai mon vrai nick crisse
?good, I have my true nickname crisse?
Unlike the English contraction I?ve, the French
contraction j?ai ?I have? is not optional:  it is
always spelled j?ai and neither je ai nor jai exist
in the French language.  This kind of
misspelling is analogous to mispronunciations
and speech errors like slips of the tongue in
speech, though clearly these anomalous forms
are much more frequent in chat interaction than
in speech.
Another type of problem is the failure to use
diacritic symbols associated with letters in some
orthographic systems.  For example, in French
the letter ?a? without an accent represents the 3rd
person singular present tense form of the verb
?have,? while the form ? is the preposition ?to.?
Both of these forms are pronounced the same,
but in other cases the diacritic signifies a change
in both pronunciation and meaning.  For
example, marche is the 3rd person singular
present tense form of the French verb marcher
?to work, go? and is pronounced like English
marsh with one syllable, but march? ?market? is
a noun pronounced with a second syllable [e].
Consequently, the failure to follow orthographic
conventions, creates homographs that present
the same identification and ambiguity problems
as homophones do in speech.
3.2 Creative spelling, rebus, and
abbreviations
Online interaction is famous for the creative and
playful conventions that have emerged for
frequently used expressions, and though most of
these originated in English, it is now possible to
observe French mdr (mort de rire ?dying of
laughter?) with English lol (laughing out loud)
or amha (? mon humble avis ?in my humble
opinion?) like English imho (in my humble
opinion) and even Portuguese vc (voce ?you?).
Like the nonstandard spellings that are
unintentional, these deliberate departures from
convention are so frequent that we have already
seen several instances of rebus forms in (2c) and
(4b) and the replacement of Romance qu by k in
(1). Other examples include English pls "please"
and ur ?your,? Turkish slm (selam ?peace?) and
the French forms in (8).
(8) a. ah wi snooppy ?   "ah yes, snooppy?"
b. et c pa toi     "and it is not you"
In (8a) oui ?yes? is spelled wi, which reflects the
pronunciation, and in (8b) the rebus form c
represents c?est ?it is,? both pronounced [se],
while pas is also spelled as pronounced, without
the silent s.  In the creative and rebus spellings,
the nonstandard forms typically reflect the
pronunciation of the word and the pronunciation
is often a reduced one, as in hiya ?hi you? and
cyah ?see you.? Consequently, these forms can
be viewed as analogous to the variation in
speech that is caused by use of reduced forms.
Alternatively, these representations might be
viewed as analogous to the out of vocabulary
words that plague current speech recognizers.
3.3 Register and dialect differences
Chat interaction is subject to the same kinds of
register and dialect variation that occurs in
speech.  For example (2a) employs a form of the
standard Turkish greeting esselamu aleyk?m that
is closer to the original Arabic because it uses
the Arabic definite article es-, though the umlaut
is not Arabic.  In contrast the variant in (3a),
selamun aleykum, employs the Turkish suffix on
selam, but omits the umlaut.  (8) illustrates other
variants that occurred in a sample of chat from
the #ankara channel.
(9) a. selamun aleyk?m
b. selam?n aleykum
c. Selammmmmmmmm
d. selamlar
e. selam all
Another example is the variable use of ne in
French constructions with negative polarity.
Though formal French uses ne before the verb
and pas after the verb for sentence negation,
most varieties omit the ne in everyday contexts,
as observed in (8b), where the absence of both
ne and the s on pas creates serious problems for
any translation engine that expects negation to
appear as ne and pas in French.  This variation
combines with a creative spelling based on
reduction to produce examples like (10).
(10)  shuis pa interess?   ?I am not interested?
The standard form of (10) is je ne suis pas
interess?, but in casual speech, ne is dropped,
the vowel in je is omitted and the two adjacent
fricatives merge to produce the sound that is
typically spelled sh in English (though it is
usually spelled ch in French).
3.4 Emotives and repeated letters
Two challenges for speech recognition are non-
lexical sounds such as laughter or grunts and the
distortions of pronunciation that are caused by
emphasis, fatigue, or emotions such as anger
and boredom.  These complications have
analogies in written interaction when
participants attempt to render the same sounds
orthographically, producing forms like those in
(9c) and (10).
(10) a. merhabaaaaaaaaaaaaaaaaaaaaaa
 b. ewww
c. eeeeeeeeeeeeeeeeeeeeeeeeeee
 d. hehehe
In (10a) the final syllable of the Turkish greeting
merhaba is lengthened in the same way that it
would be in an enthusiastic and expansive
greeting, and (10b) effectively communicates a
typical expression of disgust.  Laughter is
rendered in a variety of ways including ha ha,
heh heh, and (10d).  The variability of spellings
in these cases resembles the variability of
nonverbal sounds in speech.
3.5 Emoticons
Another way that chat participants express
emotion is by using emoticons and messages
that consist entirely of punctuation, as in (11).
(11) a. hey Pipes` >:) how u doing?
b. o)))*******
c. !!!!
 d. ????
Like the emotives and repeated letters described
in 3.4, these can be viewed as analogous to the
non-lexical sounds that occur in speech.
However, they are probably more easily
identified because they are drawn from a very
limited set of symbols.
4 Sharing solutions to shared
problems
Because machine translations of spoken and
written interaction share so many challenges, it
is likely that solutions to the problems might
also be shared in ways that will allow research
on the newer phenomenon of written interaction
to benefit from the years of experience with
spoken interaction.  Conversely, approaches to
written interaction, not biased by previous
efforts, can provide fresh perspectives on
familiar problems.  We present some examples
in which there appears to be strong potential for
this kind of mutual benefit, drawing on our
efforts to improve the performance of TrIM,
MITRE?s Translingual Instant Messenger
prototype.  TrIM is an instant messaging
environment in which participants are able to
interact by reading and typing in their own
preferred languages.  The system translates each
user?s messages into the language of the other
participants and displays both the source
language and target language versions.
TrIM?s translation services are provided by
the CyberTrans system, which provides a
common interface for various commercial text
translation systems and several types of text
documents (e.g. e-mail, web, FrameMaker).  It
incorporates text normalization tools that can
improve the quality of the input text and thus the
resultant translation.  Specifically, preprocessing
systems provide special handling for
punctuation and normalize spelling, such as
adding diacritics that have been omitted.
4.1 Spelling and recognition problems
Closer consideration of the problems created by
nonstandard spellings  reveals the strong
similarities between the complexity of speech
recognition and recognition of written words in
?noisy? communication environments such as
chat.  In both cases, there is a need to
discriminate between words that are not
recognized because they are not in the system
and words that are in the system, but are not
recognized for other reasons, such as variation
in phonetic form or a problem in the recognition
process.  Two properties of chat interaction
make this problem as serious for identifying
written words as it is for spoken input.  First,
though a much larger vocabulary can be
maintained in digital memory than in the models
of speech recognition systems, the creativity and
innovation that is valued in chat environments
provides a constant source of new vocabulary:
it is guaranteed that there will always be out of
vocabulary (OOV) words.  Second,  the high
frequency of intentional and unintentional
departures from standard spelling matches the
variability of speech and makes it essential that
the system be able to normalize spellings so that
messages are not obscured by large numbers of
unidentified words.
A variety of methods have been proposed in
the speech recognition literature for detecting
OOV words. Fetter (1998) reviews four
approaches to the problem and observes that
they can be classified in two broad groups:
explicit acoustic and language models of OOV
words ?compete against models of in-
vocabulary words during a word-based
search?Implicit models use information
derived from other recognition parameters to
compute the likelihood of OOV-words? (Fetter,
1998: 104).  In the latter group, he classifies
approaches that use confidence measures, online
garbage modeling in keyword spotting, and the
use of an additional phoneme recognizer
running in parallel to a word recognizer. These
approaches might be adapted to the problem of
discriminating misspelled and OOV words in
chat interaction, just as approaches to spelling
correction might provide alternative solutions to
the analogous problem in speech recognition.
For example, models of OOV words might
compete with models of in-vocabulary
recognition errors using Brill and Moore?s
(2000) error model for noisy channel spelling
correction that takes into account the
probabilities of errors occurring in specific
positions in the word.  By modeling recognition
errors, the model captures the stochastic
properties of both the language and the
individual recognition system.
Because our goal is not only recognizing,
but also translating messages, we are especially
interested in solutions that will facilitate the
translation system and process.  Consequently,
solutions based on modeling the contexts of
OOV word use and the contexts of nonstandard
spellings seem most promising.  For example, it
would be worth exploring whether the templates
used in example-based translation could be used
to model these contexts.
4.2 Preprocessing for special cases
Seligman (2000) observes that current spoken
language translation systems use very different
methods for recognizing phones, words, and
syntactic structures, and he envisions systems in
which these processes are integrated, proposing
alternatives that range from architectures which
support a common central data structure to
grammars whose terminal symbols are phones.
The latter approach appears to be too narrow
because it precludes the possibility of
employing preprocessing operations that
structure input to facilitate translation.
The success of TrIm and CyberTrans
suggests that preprocessing operations offer
useful approaches to the challenges we have
identified.  For example, a preprocessing system
in CyberTrans identifies words which are likely
to be missing diacritic symbols and inserts them
before the input is sent to the translation
engines.  As a result, the chat message in (12a)
is correctly translated as (12b) rather than (12c)
or (12d), which are the results from two systems
that did not benefit from preprocessing.
(12) a. et la ca va mieux
 b. and there that is better
c. and Ca is better
 d. and the ca goes better
The French form la is the feminine definite
article, whereas the form l? is the demonstrative
deictic ?there.?  CyberTrans recognized that the
form should be l? and that ca should be ?a.
Another example concerns the problems of
forms such as discourse markers, vocatives, and
greetings.  (13) shows that when the discourse
marker well is separated by a comma, as in
(13a), it is correctly translated as a discourse
marker in (13b), whereas without the comma in
(13c), the translation uses puits, the word that
would be used if referring to a water well.
(13) a. Well, I'm feeling great!
b. Bien, je me sens grand!
c. well aren't we happy today?
d. puits ne sommes-nous pas heureux
aujourd'hui?
Therefore, the comma served as a signal to the
system to translate the discourse marker
differently, and clearly, a preprocessing
operation that identifies and tags items like
discourse markers can facilitate their translation.
4.3 Segmentation of translation units
Though clause or sentence units are not clearly
marked in speech, the grammars on which
analysis and translation rely typically operate
with the clause or sentence as the primary unit
of analysis.  Consequently, the issue of
segmenting speech into sentence-like units has
received considerable attention in the speech
translation community, especially the possibility
that prosodic information can be used to identify
appropriate boundaries (Kompe, 1997; Shriberg
et al, 2000).  Other efforts identify lexical items
with high probabilities of occurring at sentence
boundaries and incorporate probabilistic models,
taking into account acoustic features (Lavie et
al. 1996) or part of speech information
(Zechner, 2001).
In contrast, some researchers have proposed
that identifying sentence boundaries is less
important than finding appropriate packaging
sizes for translation.  For example, Seligman
(2000) reports that pause units divided a corpus
into units that were smaller than sentence units,
but could be parsed and yielded understandable
translations when translated by hand.  Popowich
et al (2000) deliberately segment closed caption
input into small packages for translation,
claiming that it provides a basis for handling the
vocatives, false starts, tag questions, and other
non-canonical structures encountered in
captions.  They also claim that the procedure
reduces the scope of translation failures and
constrains the generation problem.  Since much
interaction is already fragmented, processing
that relies on smaller units rather than sentences
seems worth investigating.
A related possibility is that much of the
problematic input is already packaged in small
units of short turns or messages.  Yu et al
(2000) report that 54% of turns with a duration
of 0.7 seconds or less in their data consisted of
either yeah or mmhmm and about 70% of the
turns contained the same 40 words or phrasal
expressions.  They took advantage of these facts
by building a language model tailored
specifically for short turns.
In a pilot study, we examined a small
sample of chat data in order to determine
whether short messages were more likely to
contain the problematic features described in
sections 2 and 3.  Of 76 chat messages, 46 or
61% were 3 units or less, where units were
delineated by space and punctuation (without
separation of contractions) and emoticons
counted as a unit.  The frequency of items such
as greetings, vocatives and acronyms was
counted for each message size.  Of 8 greetings
in the corpus, 7 occurred in messages of 3 or
fewer words, and all 9 greeting-plus-vocative
structures occurred in messages of 3 or fewer
words.  Messages with 3 or fewer words also
contained 11 of the 14 emotives in the corpus
(including emoticons), 8 of the 10 messages
with repeated letters, 3 of the 3 acronyms, 2 of
the 3 discourse markers, and both of the
interjections in the corpus.  These results
support the claim that much of the problematic
usage in chat interaction is limited to short turns
that can be identified and processed separately.
5 Conclusions
This paper demonstrates that many of the
problems which complicate translation of
spoken interaction are shared by written
interaction such as chat and instant messaging.
It is proposed that solutions developed to solve
similar problems in the two communication
environments can be profitably shared, and
several examples are presented where mutually
beneficial approaches might be developed.
Specifically, we noted that the problem of
discriminating unrecognized OOV words  from
in-vocabularly words in spoken interaction is
analogous to the problem of discriminating
unrecognized OOV words from misspelled
words in written interaction.  We suggested that
some of the same methods used in spelling
correction might be adapted to speech
recognition, especially language models that
incorporate probabilities of errors in specific
positions in the word.  We also observed the
potential of preprocessing operations that
structure input for translation systems to allow
special treatment of problematic language,
including the possibility that much complexity
can be avoided by processing and translating
smaller units separately.  We look forward to
exploring these possibilities in future work.
References
Brill, Eric and Moore, Robert C.  2000. An improved
error model for noisy channel spelling
correction.  Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics.
Chafe, Wallace. 1982. Integration and involvement in
speaking, writing, and oral literature.  In Spoken
and Written Language:  Exploring Orality and
Literacy, Deborah Tannen (Ed.), Norwoord, NJ:
Ablex, pp. 35-53.
Condon, Sherri and Cech, Claude.  (Forthcoming)
Discourse management in three modalities.  In
Computer-Mediated Conversation, Susan
Herring (Ed.), Hampton Press.
Danet, Brenda, Ruedenberg-Wright, Lucia, and
Rosenbaum-Tamari, Yehudit. 1995. Hmmm...
Where's that smoke coming from?" Writing,
Play and Performance on Internet Relay. Journal
of Computer-Mediated Communication, 1 (2).
Ferrara, Kathleen, Brunner, Hans, and Whittemore,
Greg.  1991.  Interactive written discourse as an
emergent register.  Written Communication, 8
(1), 8-34.
Fetter, Pablo. 1998.  Detection and transcription of
OOV words.  Verbmobil Technical Report 231.
Flanagan, Mary.  1996.  Two years online:
Experiences, challenges and trends.  Expanding
MT Horizons:  Proceedings of the Second
Conference of the Association for Machine
translation in the Americas, 2-5 October,  pp.
192-197.
Kompe, Ralf. 1997. Prosody in Speech
Understanding Systems.  Berlin:  Springer.
Lavie, Alon, Gates, Donna, Coccaro, Noah and
Levin, Lori.  1996. Input segmentation of
spontaneous speech in Janus: A speech-to-
speech translation system. Proceedings of the
ECAI 96, Budapest, Hungary.
Papineni, K., Roukos, S., Ward, T., Henderson, J.,
and Reeder, Florence. 2002. Corpus-Based
comprehensive and diagnostic MT evaluation :
Initial Arabic, Chinese, French, and Spanish
results. Proceedings of the Human Language
Technology Conference.  San Diego, California.
Popowich, Fred, McFetridge, Paul, Turcato, Davide,
and Toole, Janine. 2000. Machine translation of
closed captions.  Machine Translation, 15, 311-
341.
Shriberg, Elizabeth, Stolcke, Andreas, Hakkani-Tur,
Dilek, and Tur, Gokhan. 2000.  Prosody-based
automatic segmentation of speech into sentences
and topics.  Speech Communication 32(1-2).
Seligman, Mark.  2000.  Nine issues in speech
translation.  Machine Translation, 15, 149-185.
Sotillo, Susana M. 2000. Discourse functions and
syntactic complexity in synchronous and
asynchronous communication. Language
Learning & Technology, 4 (1), pp. 82-119.
Vanni, Michelle. and Miller, Keith.  2002. Scaling
the ISLE framework: Use of existing corpus
resources for validation of MT evaluation
metrics across languages. In Proceedings of
LREC 2002.  Las Plamas, Canary Islands, Spain.
Wiebe, Janice, Farwell, David, Villa, Daniel, Chen,
J-L, Sinclaire, R., Sandgren, Thorsten, Stein, G.,
Zarazua, David, and Ohara, Tom. 1995.
ARTWORK:  Discourse Processing in Machine
Translation of Dialog.   Final Report Year 1.
New Mexico State University:  Computing
Research Lab.  http://crl.NMSU.Edu/Research/
Projects/artwork/index.html
Yates, Simeon. 1996.  Oral and written linguistic
aspects of computer conferencing:  A corpus
based study. In Computer-Mediated
Communication:  Linguistic, Social and Cross-
Cultural Perspectives., Susan Herring (Ed.),
Philadelphia:  John Benjamins, pp. 29-46.
Yu, Hua, Tomokiyo, Takashi, Wang, Zhirong, and
Waibel, Alex.  2000.  New developments in
automatic meeting transcription.  International
Conference on Speech and Language Processing,
Beijing, China.
Zechner, Klaus. 2001. Automatic Generation of
Concise Summaries of Spoken Dialogues in
Unrestricted Domains. Proceedings of the 24th
ACM-SIGIR International Conference on
Research and Development in Information
Retrieval, New Orleans, Louisiana.
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 152?160,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Name Matching Between Chinese and Roman Scripts:                       
Machine Complements Human 
 
Ken Samuel, Alan Rubenstein, Sherri Condon, and Alex Yeh 
The MITRE Corporation; M/S H305; 7515 Colshire Drive; McLean, Virginia 22102-7508 
samuel@mitre.org, rubenstein@mitre.org, scondon@mitre.org, and asy@mitre.org 
 
  
Abstract 
There are generally many ways to translite-
rate a name from one language script into 
another. The resulting ambiguity can make it 
very difficult to ?untransliterate? a name by 
reverse engineering the process. In this paper, 
we present a highly successful cross-script 
name matching system that we developed by 
combining the creativity of human intuition 
with the power of machine learning. Our sys-
tem determines whether a name in Roman 
script and a name in Chinese script match 
each other with an F-score of 96%. In addi-
tion, for name pairs that satisfy a computa-
tional test, the F-score is 98%. 
 
1 Introduction 
There are generally many ways to transliterate a 
person?s name from one language script into 
another. For example, writers have transliterated 
the Arabic name, ??????, into Roman characters 
in at least 13 ways, such as Al Choukri, Ash-
shukri, and al-Schoukri. This ambiguity can 
make it very difficult to ?untransliterate? a name 
by reverse engineering the process. 
We focused on a task that is related to transli-
teration. Cross-script name matching aims to de-
termine whether a given name part in Roman 
script matches a given name part in Chinese 
(Mandarin) script,1 where a name part is a single 
?word? in a person?s name (such as a surname), 
and two names match if one is a transliteration of 
the other.2 Cross-script name matching has many 
                                                 
1 In this paper, we often use the word ?Roman? to refer to 
?Roman script?, and similarly, ?Chinese? usually stands 
for ?Chinese script?. 
2 Sometimes a third script comes between the Roman and 
Chinese versions of the name. For example, a Roman 
name might be transliterated into Arabic, which is then 
transliterated into Chinese, or an Arabic name could be 
transliterated into Roman and Chinese independently. 
applications, such as identity matching, improv-
ing search engines, and aligning parallel corpora. 
We combine a) the creative power of human 
intuition, which can come up with clever ideas 
and b) the computational power of machine 
learning, which can analyze large quantities of 
data. Wan and Verspoor (1998) provided the 
human intuition by designing an algorithm to 
divide names into pieces that are just the right 
size for Roman-Chinese name matching (Section 
2.2.). Armed with Wan and Verspoor?s algo-
rithm, a machine learning approach analyzes 
hundreds of thousands of matched name pairs to 
build a Roman-Chinese name matching system 
(Section 3). 
Our experimental results are in Section 4. The 
system correctly determines whether a Roman 
name and a Chinese name match each other with 
F = 96.5%.3 And F = 97.6% for name pairs that 
satisfy the Perfect Alignment hypothesis condi-
tion, which is defined in Section 2.2. 
 
2 Related Work 
Wan and Verspoor?s (1998) work had a great 
impact on our research, and we explain how we 
use it in Section 2.2. In Section 2.1, we identify 
other related work. 
2.1 Chinese-English Name Matching 
Condon et al (2006) wrote a paper about the 
challenges of matching names across Roman and 
Chinese scripts. In Section 6 of their paper, they 
offered an overview of several papers related to 
Roman-Chinese name matching. (Cohen et al, 
2003; Gao et al, 2004;  Goto et al, 2003; Jung et 
al., 2000; Kang and Choi, 2000; Knight and 
Graehl, 1997; Kondrak, 2000; Kondrak and 
Dorr, 2004; Li et al, 2004; Meng et al, 2001; Oh 
                                                 
3 F stands for F-score, which is a popular evaluation metric. 
(Andrade et al, 2009) 
152
and Choi, 2006; Virga and Khudanpur, 2003; 
Wellner et al, 2005; Winkler, 2002) 
The Levenshtein algorithm is a popular way to 
compute string edit distance. (Levenshtein, 1966) 
It can quantify the similarity between two names. 
However, this algorithm does not work when the 
names are written in different scripts. So Free-
man et al (2006) developed a strategy for Ro-
man-Arabic  string matching that uses equiva-
lence classes of characters to normalize the 
names so that Levenshtein?s method can be ap-
plied.  Later, Mani et al (2006) transformed that 
system from Roman-Arabic to Roman-Chinese 
name matching and extended the Levenshtein 
approach, attaining F = 85.2%. Then when they 
trained a machine learning algorithm on the out-
put, the performance improved to F = 93.1% 
 Mani et al also tried applying a phonological 
alignment system (Kondrak, 2000) to the Ro-
man-Chinese name matching task, and they re-
ported an F-score of 91.2%. However, when they 
trained a machine learning approach on that sys-
tem?s output, the F-score was only 90.6%.  
It is important to recognize that it would be in-
appropriate to present a side-by-side comparison 
between Mani?s work and ours (F = 96.5%), be-
cause there are many differences, such as the 
data that was used for evaluation. 
2.2 Subsyllable Units 
Transliteration is usually based on the way 
names are pronounced.4 However, each character 
in a Roman name generally corresponds to a sin-
gle phoneme, while a Chinese character (CC) 
generally corresponds to a subsyllable unit 
(SSU). A phoneme is the smallest meaningful 
unit of sound, and a subsyllable unit is a se-
quence of one to three phonemes that conform to 
the following three constraints. (Wan and Vers-
poor, 1998) 
                                                 
4  Of course, there are exceptions. For example, when a 
name happens to be a word, sometimes that name is trans-
lated (rather than transliterated) into the other language. 
But our experimental results suggest that the exceptions 
are quite rare. 
(1) There is exactly one vowel phoneme.5 
(2) At most, one consonant phoneme may pre-
cede the vowel phoneme. 
(3) The vowel phoneme may be followed by, at 
most, one nasal phoneme.6 
Consider the example in Table 1. The name 
?Albertson? consists of eight phonemes in three 
syllables.7 The last syllable, SAHN, satisfies the 
definition of SSU, and the other two are split into 
smaller pieces, resulting in a total of five SSUs. 
There are also five CCs in the Chinese version,  
?????. We note that the fourth and sixth rows 
in the table show similarities in their pronuncia-
tions. For example, the first SSU, AE, sounds 
like the first CC, /a/. And, although the sounds 
are not always identical, such as BER and /pei/, 
Wan and Verspoor claimed that these SSU-CC 
correspondences can be generalized in the fol-
lowing way: 
Perfect Alignment (PA) hypothesis 
If a Roman name corresponds to a sequence of n 
SSUs, S1, S2, ..., Sn, and the Chinese form of that 
name is a sequence of n CCs, C1, C2, ..., Cn, then 
Ci matches Si for all 1 ? i ? n. 
In Section 4, we show that the PA hypothesis 
works very well. However, it is not uncommon 
to have more SSUs than CCs in a matching name 
pair, in which case, the PA hypothesis does not 
apply. Often this happens because an SSU is left 
out of the Chinese transliteration, perhaps be-
cause it is a sound that is not common in Chi-
nese. For example, suppose ?Carlberg? (KAA, 
R,L,BER,G) is transliterated as ???? . In 
this example, the SSU, R, does not corres-
pond to any of the CCs. We generalize this 
phenomenon with another hypothesis:  
SSUs Deletion (SSUD) hypothesis 
If a Roman name corresponds to a sequence of 
n+k  SSUs (k>0), S1, S2, ..., Sn+k, and the Chinese 
form of that name is a sequence of n CCs, C1, C2, 
..., Cn, then, for some set of k Si?s, if those SSUs 
are removed from the sequence of SSUs, then the 
PA hypothesis holds. 
And in the case where the number of CCs is 
greater than the number of SSUs, we make the 
                                                 
5 Wan and Verspoor treat the phoneme, /?r/, as in Albertson, 
as a vowel phoneme. 
6 The nasal phonemes are /n/ and /?/, as in ?nothing?. 
7 To represent phonemes, we use two different standards in 
this paper. The symbols between slashes (like /?r/) are in 
the IPA format (International Phonetic Association, 
1999). And the phonemes written in capital letters (like 
ER) are in the ARPABET format (Klatt, 1990). 
Roman Characters: Albertson 
Phonemes: AE,L,B,ER,T,S,AH,N 
Syllables: AEL,BERT,SAHN 
Subsyllable Units: AE,L,BER,T,SAHN 
Chinese: ????? 
Chinese Phonemes: /a/,/?r/,/pei/,/t
h?/,/su?/ 
Table 1. Subsyllable Units 
153
corresponding CCs Deletion (CCD) hypothesis. 
In the next section, we show how we utilize these 
hypotheses. 
 
3 Machine Learning 
We designed a machine learning algorithm to 
establish a mapping between SSUs and CCs. In 
Section 3.1, we show how our system can do 
Roman-Chinese name matching, and then we 
present the training procedure in Section 3.2. 
3.1 Application Mode 
Given a Roman-Chinese name pair, our system 
computes a match score, which is a number be-
tween 0 and 1 that is meant to represent the like-
lihood that two names match.  This is accom-
plished via the process presented in Figure 1. 
Starting in the upper-left node of the diagram 
with a Roman name and a Chinese name, the 
system determines how the Roman name should 
be pronounced by running it through the Festival 
system. (Black et al, 1999) Next, two algorithms 
designed by Wan and Verspoor (1998) join the 
phonemes to form syllables and divide the syl-
lables into SSUs.8 If the number of SSUs is equal 
to the number of characters in the Chinese 
name,9 we apply the PA hypothesis to align each 
SSU with a CC.  
The system computes a match score using a 
data structure called the SSU-CC matrix (subsyl-
lable unit ? Chinese character matrix), which has 
a nonnegative number for each SSU-CC pair, 
and this value should represent the strength of 
the correspondence between the SSU and the 
CC. Table 2 shows an example of an SSU-CC 
matrix. With this matrix, the name pair <Albert, 
????> receives a relatively high match score,  
because the SSUs in Albert are AE, L, BER, and 
T, and the numbers in the SSU-CC matrix for 
<AE,?>, <L,?>, <BER,?> and <T,?> are 2, 2, 
3, and 2, respectively.10 Alternatively, the system 
assigns a very low match score to <Albert,         
????>, because the values of <AE,?>, <L,?>, 
<BER,?>, and <T,?> are all 0. 
3.2 Training Mode 
To generate an SSU-CC matrix, we train our sys-
tem on a corpus of Roman-Chinese name pairs 
                                                 
8  This procedure passes through three separate modules, 
each of which introduces errors, so we would expect the 
system to suffer from compounding errors. However, the 
excellent evaluation results in Section 4 suggest  other-
wise. This may be because the system encounters the 
same kinds of errors during training that it sees in the ap-
plication mode, so perhaps it can learn to compensate for 
them. 
9 Section 3.3 discusses the procedure used when these num-
bers are not equal. 
10 The equation used to derive the match score from these 
values can be found in Section 5. 
 
Figure 2. Training Mode 
 
Figure 1. Application Mode 
 
A 
E 
B 
E 
R 
E 
H G 
K 
A 
A L 
L 
A 
H 
N 
L 
I 
Y 
N 
A 
H R 
S 
A 
H 
N T 
? 0 0 0 0 0 0 1 0 0 0 0 0 
? 0 0 0 0 0 0 0 1 0 0 0 0 
? 0 0 0 0 1 0 0 0 0 0 0 0 
? 0 0 1 0 0 0 0 0 0 0 0 0 
? 0 0 1 0 0 0 0 0 0 0 0 0 
? 0 0 0 0 0 0 0 0 1 0 0 0 
? 0 0 0 0 0 2 0 0 0 1 0 0 
? 0 0 0 0 0 0 0 0 0 0 1 0 
? 0 0 0 0 0 0 0 0 0 0 0 2 
? 0 3 0 0 0 0 0 0 0 0 0 0 
? 0 0 0 0 0 0 1 0 0 0 0 0 
? 0 0 0 1 0 0 0 0 0 0 0 0 
? 2 0 0 0 0 0 0 0 0 0 0 0 
Table 2. SSU-CC Matrix #1 
 
154
that match. Figure 2 shows a diagram of the 
training system. The procedure for transforming 
the Roman name into a sequence of SSUs is 
identical to that presented in Section 3.1. Then, if 
the number of SSUs is the same as the number of 
CCs,9 we apply the PA hypothesis to pair the 
SSUs with the CCs. For example, the third name 
pair in Table 3 has three SSU-CC pairs: <KAA,
?>, <R,?>, and <LIY,?>. So the system mod-
ifies the SSU-CC matrix by adding 1 to each cell 
that corresponds to one of these SSU-CC pairs. 
Training on the five name pairs in Table 3 pro-
duces the SSU-CC matrix in Table 2. 
3.3 Imperfect Alignment 
The system makes two passes through the train-
ing data. In the first pass, whenever the PA hypo-
thesis does not apply to a name pair (because the 
number of SSUs differs from the number of 
CCs), that name pair is skipped.  
Then, in the second pass, the system builds 
another SSU-CC matrix. The procedure for 
processing each name pair that satisfies the PA 
hypothesis?s condition is exactly the same as in 
the first pass (Section 3.2). But the other name 
pairs require the SSUD hypothesis or the CCD 
hypothesis to delete SSUs or CCs. For a given 
Roman-Chinese name pair:  
where D is the set of all deletion sets that make 
the PA hypothesis applicable. Note that the size 
of D grows exponentially as the difference be-
tween the number of SSUs and CCs grows. 
As an example, consider adding the name pair 
<Carlberg, ????> to the data in Table 3. Carl-
berg has five SSUs: KAA,R,L,BER,G, but ???-
? has only four CCs. So the PA hypothesis is not 
applicable, and the system ignores this name pair 
in the first pass. Table 2 shows the values in Ma-
trix #1 when it is completed. 
In the second pass, we must apply the SSUD 
hypothesis to <Carlberg, ????> by deleting 
one of the SSUs. There are five ways to do this, 
as shown in the five rows of Table 4. (For in-
stance, the last row represents the case where G 
is deleted ? the SSU-CC pairs are <KAA,?>, 
<R,?>, <L,?>, <BER,?>, and <G,?>.11) 
Each of the five options are evaluated using 
the values in Matrix #1 (Table 2) to produce the 
scores in the second column of Table 4. Then the 
                                                 
11 The ? represents a deleted SSU. We include a row and 
column named ? in Matrix #2 to record values for the 
cases in which the SSUs and CCs are deleted. 
For every d in D: 
Temporarily make the deletions in d. 
Evaluate the resulting name pair with Matrix #1. 
Scale the evaluation scores of the d?s to sum to 1. 
For every d in D: 
Temporarily make the deletions in d. 
For every SSU-CC pair, ssu-cc, in the result: 
Add d?s scaled score to cell [ssu,cc] in Matrix #2. 
Example # 1 2 3 4 5 
Roman 
Characters 
Albert Albertson Carly Elena Ellenberg 
Subsyllable 
Units 
AE,L,BER,T AE,L,BER,T,SAHN KAA,R,LIY EH,LAHN,NAH EH,LAHN,BER,G 
Chinese 
Characters 
???? ????? ??? ??? ???? 
Table 3. Training Data 
CCs Score Scaled Score 
????? 0.00 0.00 
????? 0.90 0.54 
????? 0.76 0.46 
????? 0.00 0.00 
????? 0.00 0.00 
Table 4. Subsyllable Unit Deletion 
 
 
? 
B 
E 
R G 
K 
A 
A L R ... 
?  0.00 0.00 0.00 0.46 0.54  
? 0.00 0.00 0.00 2.00 0.00 0.00  
? 0.00 0.00 0.00 0.00 2.54 1.46  
? 0.00 4.00 0.00 0.00 0.00 0.00  
? 0.00 0.00 2.00 0.00 0.00 0.00  
...        
Table 5. SSU-CC Matrix #2 
 
155
system scales the scores to sum to 1, as shown in 
the third column, and it uses those values as 
weights to determine how much impact each of 
the five options has on the second matrix. Table 
5 shows part of Matrix #2. 
In application mode, when the system encoun-
ters a name pair that does not satisfy the PA hy-
pothesis?s condition it tries all possible deletion 
sets and selects the one that produces the highest 
match score. 
3.4 Considering Context 
It might be easier to estimate the likelihood that 
an SSU-CC pair is a match by using information 
found in surrounding SSU-CC pairs, such as the 
SSU that follows a given SSU-CC pair. We do 
this by increasing the number of columns in the 
SSU-CC matrix to separate the examples based 
on the surrounding context. 
For example, in Table 2, we cannot determine 
whether LAHN should map to ? or ?. But the 
SSU that follows LAHN clears up the ambiguity, 
because when LAHN immediately precedes 
BER, it maps to  ?, but when it is followed by 
NAH, it corresponds to ?. Table 6 displays a 
portion of the SSU-CC matrix that accounts for 
the contextual information provided by the SSU 
that follows an SSU-CC pair. 
3.5 The Threshold 
Given an SSU-CC name pair, the system produc-
es a number between 0 and 1. But in order to 
evaluate the system in terms of precision, recall, 
and F-score, we need the system to return a yes 
(a match) or no (not a match) response. So we 
use a threshold value to separate those two cases.  
The threshold value can be manually selected 
by a human, but this is often difficult to do effec-
tively. So we developed the following automated 
approach to choose the threshold. After the train-
ing phase finishes developing Matrix #2, the sys-
tem processes the training data12 one more time. 
                                                 
12 We tried selecting the threshold with data that was not 
used in training, and we found no statistically significant 
improvement. 
But this time it runs in application mode (Section 
3.1), computing a match score for each training 
example. Then the system considers all possible 
ways to separate the yes and no responses with a 
threshold, selecting the threshold value that is the 
most effective on the training data. 
Building the SSU-CC matrices does not re-
quire any negative examples (name pairs that do 
not match). However, we do require negative 
examples in order to determine the threshold and 
to evaluate the system. Our technique for gene-
rating negative examples involves randomly 
rearranging the names in the data.13 
 
4 Evaluation of the System 
We ran several experiments to test our system 
under a variety of different conditions. After de-
scribing our data and experimental method, we 
present some of our most interesting experimen-
tal results. 
We used a set of nearly 500,000 Roman-
Chinese person name pairs collected from Xin-
hua News Agency newswire texts. (Huang, 
2005) Table 7 shows the distribution of the data 
based on alignment. Note that the PA hypothesis 
applies to more than 60% of the data. 
We used the popular 10-fold cross validation 
approach 14  to obtain ten different evaluation 
scores. For each experiment we present the aver-
age of these scores. 
Our system?s precision (P), recall (R), and F-
score (F) are: P = 98.19%, R = 94.83%, and F = 
96.48%. These scores are much better than we 
originally expected to see for the challenging 
task of Roman-Chinese name matching.  
Table 8 shows P, R, and F for subsets of the 
test data, organized by the number of SSUs mi-
                                                 
13 Unfortunately, there is no standard way to generate nega-
tive examples. 
14 The data is divided into ten subsets of approximately the 
same size, testing the system on each subset when trained 
on the other nine. 
 
LAHN 
(BER) 
LAHN 
(NAH) 
BER 
(G) 
BER 
(T) 
? 1 0 0 0 
? 0 0 1 2 
? 0 1 0 0 
Table 6. Considering Context 
 
Alignment % of Data 
#SSUs - #CCs ? 3 1.62% 
#SSUs - #CCs = 2 6.66% 
#SSUs - #CCs = 1 20.00% 
#SSUs - #CCs = 0 60.60% 
#SSUs - #CCs = -1 10.48% 
#SSUs - #CCs = -2 0.61% 
#SSUs - #CCs ? -3 0.02% 
Table 7. Statistics of the Data 
156
nus the number of CCs in the name pairs. The 
differences between scores in adjacent rows of 
each column are statistically significant.15  Per-
fectly aligned name pairs proved to be the ea-
siest, with F = 97.55%, but the system was also 
very successful on the examples with the number 
of SSUs and the number of CCs differing by one 
(F = 96.08% and F = 97.37%). These three cases 
account for more than 91% of the positive exam-
ples in our data set. (See Table 7.) 
4.1 Deletion Hypotheses 
We ran tests to determine whether the second 
pass through the training data (in which the 
SSUD and CCD hypotheses are applied) is effec-
tive. Table 9 shows the results on the complete 
set of test data, and all of the differences between 
the scores are statistically significant.  
The first row of Table 9 presents F when the 
system made only one pass through the training 
data. The second row?s experiments utilized the 
CCD hypothesis but ignored examples with more 
SSUs than CCs during training. For the third 
row, we used the SSUD hypothesis, but not the 
CCD hypothesis, and the last row corresponds to 
system runs that used all of the training exam-
ples. From these results, it is clear that both of 
the deletion hypotheses are useful, particularly 
the SSUD hypothesis. 
4.2 Context 
In Section 3.4, we suggested that contextual in-
formation might be useful. So we ran some tests, 
obtaining the results shown in Table 10. For the 
second row, we used no contextual information. 
Row 5 corresponds to the case where we gave 
the system access to the SSU immediately fol-
lowing the SSU-CC pair being analyzed. In row 
                                                 
15 We use the homoscedastic t test (?Student?s t Test?, 2009) 
to decide whether the difference between two results is 
statistically significant. 
6?s experiment, we used the SSU immediately 
preceding the SSU-CC pair under consideration, 
and row 7 corresponds to system runs that ac-
counted for both surrounding SSUs. 
We also tried simplifying the contextual in-
formation to boolean values that specify whether 
an SSU-CC pair is at a boundary of its name or 
not, and rows 1, 3, and 4 of Table 10 show those 
results. ?Left Border? is true if and only if the 
SSU-CC pair is at the beginning of its name, 
?Right Border? is true if and only if the SSU-CC 
pair is at the end of its name, and ?Both Borders? 
is true if and only if the SSU-CC pair is at the 
beginning or end of its name. All differences in 
the table are statistically significant, except for 
those between rows 2, 3, and 4. These results 
suggest that the right border provides no useful 
information, even if the left border is also in-
cluded in the SSU-CC matrix. But when the 
SSU-CC matrix only accounted for the left bor-
der, the F-score was significantly higher than the 
baseline. Providing more specific information in 
the form of SSUs actually made the scores go 
down significantly. 
4.3 Sparse Data 
We were initially surprised to discover that using 
the rich information in the surrounding SSUs 
made the results worse. The explanation for this 
is that adding contextual information increases 
the size of the SSU-CC matrix, and so several of 
the numbers in the matrix become smaller. (For 
example, compare the values in the ?BER? col-
umns in Table 2 and Table 6.) This means that 
the system might have been suffering from a 
sparse data problem, which is a situation where 
there are not enough training examples to distin-
guish correct answers from incorrect answers, 
and so incorrect answers can appear to be correct 
by random chance.  
There are two factors that can contribute to a 
sparse data problem. One is the amount of train-
ing data available ? as the quantity of training 
data increases, the sparse data problem becomes 
less severe. The other factor is the complexity of 
Alignment P R F 
#SSUs - #CCs ? 3 72.38% 94.02% 81.79% 
#SSUs - #CCs = 2 95.26% 92.67% 93.95% 
#SSUs - #CCs = 1 99.07% 93.27% 96.08% 
#SSUs - #CCs = 0 99.87% 95.33% 97.55% 
#SSUs - #CCs = -1 98.33% 96.42% 97.37% 
#SSUs - #CCs = -2 73.80% 94.98% 83.04% 
#SSUs - #CCs ? -3 7.54% 78.04% 13.71% 
Table 8. Varying Alignment of Name Pairs 
# Contextual Information F 
1 Left Border 96.48% 
2 No Context 96.25% 
3 Both Borders 96.24% 
4 Right Border 96.19% 
5 Next SSU 87.53% 
6 Previous SSU 85.89% 
7 Previous SSU and Next SSU 47.89% 
Table 10. Evaluation with Context 
Hypotheses F 
PA 75.25% 
PA & CCD 83.74% 
PA & SSUD 92.86% 
PA & CCD & SSUD 96.48% 
Table 9. Varying the Training Data 
 
157
the learned model ? as the model becomes more 
complex, the sparse data problem worsens. 
Our system?s model is the SSU-CC matrix, 
and a reasonable measure of the its complexity is 
the number of entries in the matrix. The second 
column of Table 11 shows the number of SSU-
CC pairs in training divided by the number of 
cells in the SSU-CC matrix. These ratios are 
quite low, suggesting that there is a sparse data 
problem. Even without using any context, there 
are nearly 8 cells for each SSU-CC pair, on aver-
age.16  
It might be more reasonable to ignore cells 
with extremely low values, since we can assume 
that these values are effectively zero. The third 
column of Table 11 only counts cells that have 
values above 10-7. The numbers in that column 
look better, as the ratio of cells to training pairs 
is better than 1:4 when no context is used. How-
ever, when using the previous SSU, there are still 
more cells than training pairs.  
Another standard way to test for sparse data is 
to compare the system?s results as a function of 
the quantity of training data. As the amount of 
training data increases, we expect the F-score to 
rise, until there is so much training data that the 
F-score is at its optimal value.17 Figure 3 shows 
the results of all of the context experiments that 
we ran, varying the amount of training data. 
(90% of the training data was used to get the F-
scores in Table 10.) The t test tells us that ?No 
Context? is the only curve that does not increase 
significantly on the right end. This suggests that 
all of the other curves might continue increasing 
if we used more training data. So even the ?Both 
SSUs? case could potentially achieve a competi-
tive score, given enough training examples. Also, 
                                                 
16 It is true that a name pair can have multiple SSU-CC 
pairs, but even if the average number of SSU-CC pairs per 
name pair is as high as 8 (and it is not), one training name 
pair per SSU-CC matrix cell is still insufficient. 
17 Note that this value may not be 100%, because there are 
factors that can make perfection difficult to achieve, such 
as errors in the data. 
more training data could produce higher scores 
than 96.48%. 
5 Summary 
We designed a system that achieved an F-score 
of 96.48%, and F = 97.55% on the 60.61% of the 
data that satisfies the PA hypothesis?s condition.  
Due to the paper length restriction, we can on-
ly provide short summaries of the other experi-
ments that that we ran. 
1) We experimentally compared six different 
equations for computing match scores and 
found that the best of them is an arithmetic 
or geometric average of Prob(SSU|CC) and 
Prob(CC|SSU).  
2) We attempted to make use of two simple 
handcrafted rules, but they caused the sys-
tem?s performance to drop significantly. 
3) We compared two approaches for automati-
cally computing the pronunciation of a Ro-
man name and found that using the Festival 
system (Black et al, 1999) alone is just as ef-
fective as using the CMU Pronunciation Dic-
tionary (CMUdict, 1997) supplemented by 
Festival. 
4) We tried computing the threshold value with 
data that was not used in training the system. 
However, this failed to improve the system?s 
performance significantly. 
 
6 Future Work 
There are so many things that we still want to do, 
including: 
1. modifying our system for the task of 
transliteration (Section 6.1),  
2. running fair comparisons between our 
work and related research, 
3. using Levenshtein?s algorithm (Levensh-
tein, 1966) to implement the SSUD and 
Contextual Info. All Cells  Cells > 10-7  
No Context 0.128 4.35 
Right Border 0.071 3.45 
Left Border 0.069 3.45 
Both Borders 0.040 3.13 
Next SSU 0.002 1.12 
Previous SSU 0.001 0.78 
Both SSUs far less far less 
Table 11. Num. SSU-CC Pairs  per Matrix Cell 
 
Figure 3. Testing for Sparse Data 
 
  
40%
50%
60%
70%
80%
90%
100%
10% 20% 30% 40% 50% 60% 70% 80% 90%
F
-S
c
o
re
Training Set Size (% of available data)
Left Border Next SSU
No Context Previous SSU
Right Border Both SSUs
Both Borders
158
CCD hypotheses, instead of exhaustively 
evaluating all possible deletion sets (Sec-
tion 3.3),18  
4. developing a standard methodology for 
creating negative examples,  
5. when using contextual information, split-
ting rows or columns of the SSU-CC 
matrix only when they are ambiguous 
according to a metric such as Informa-
tion Gain (Section 3.4),19 
6. combining our system with other Ro-
man-Chinese name matching systems in 
a voting structure (Van Halteren, Zavrel, 
and Daelemans, 1998), 
7. independently evaluating the modules 
that determine pronunciation, construct 
syllables, and separate subsyllable units 
(Section 3),  
8. converting phonemes into feature vectors 
(Aberdeen, 2006),  
9. modifying our methodology to apply it 
to other similar languages, such as Japa-
nese, Korean, Vietnamese, and Ha-
waiian.  
10. manually creating rules based on infor-
mation in the SSU-CC matrix, and  
11. utilizing graphemic information. 
6.1 Transliteration 
We would like to modify our system to enable 
it to transliterate a given Roman name into Chi-
nese in the following way. First, the system 
computes the SSUs as in Section 3.1. Then it 
produces a match score for every possible se-
quence of CCs that has the same length as the 
sequence of SSUs, returning all of the CC se-
quences with match scores that satisfy a prede-
termined threshold restriction. 
For example, in a preliminary experiment, 
given the Roman name Ellen, the matcher pro-
duced the transliterations below, with the match 
scores in parentheses.20 
 ? ?  (0.32) 
 ? ?  (0.14) 
 ? ?  (0.11)  
 ? ?  (0.05) 
                                                 
18 We thank a reviewer for suggesting this method of im-
proving efficiency. 
19 We thank a reviewer for this clever way to control the 
size of the SSU-CC matrix when context is considered. 
20 A manually-set threshold of 0.05 was used in this experi-
ment. 
Based on our data, the first and fourth results 
are true transliterations of Ellen, and the only 
true transliteration that failed to make the list is 
??. 
 
7 Conclusion 
There was a time when computational linguistics 
research rarely used machine learning. Research-
ers developed programs and then showed how 
they could successfully handle a few examples, 
knowing that their programs were unable to ge-
neralize much further. Then the language com-
munity became aware of the advantages of ma-
chine learning, and statistical systems almost 
completely took over the field. Researchers 
solved all kinds of problems by tapping into the 
computer?s power to process huge corpora of 
data. But eventually, the machine learning sys-
tems reached their limits. 
We believe that, in the future, the most suc-
cessful systems will be those developed by 
people cooperating with machines. Such systems 
can solve problems by combining the computer?s 
ability to process massive quantities of data with 
the human?s ability to intuitively come up with 
new ideas. 
Our system is a success story of human-
computer cooperation. The computer tirelessly 
processes hundreds of thousands of training ex-
amples to generate the SSU-CC matrix. But it 
cannot work at all without the insights of Wan 
and Verspoor. And together, they made a system 
that is successful more than 96% of the time. 
 
References 
Aberdeen, J. (2006) ?geometric-featurechart-jsa-
20060616.xls?. Unpublished. 
Andrade, Miguel. Smith, S. Paul. Cowlisha, Mike F. 
Gantner, Zeno. O?Brien, Philip. Farmbrough, Rich. 
et al ?F1 Score.? (2009) Wikipedia: The Free En-
cyclopedia.  http://en.wikipedia.org/wiki/F-score. 
Black, Alan W. Taylor, Paul. Caley, Richard. (1999) 
The Festival Speech Synthesis System: System Do-
cumentation. Centre for Speech Technology Re-
search (CSTR). The University of Edinburgh. 
http://www.cstr.ed.ac.uk/projects/festival/manual 
CMUdict. (1997) The CMU Pronouncing Dictionary. 
v0.6. The Carnegie Mellon Speech Group. 
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.  
Cohen, W. Ravikumar, P. Fienberg, S. (2003) ?A 
Comparison of String Distance Metrics for Name-
159
Matching Tasks.? Proceedings of the IJCAI-03 
Workshop on Information Integration on the Web. 
Eds. Kambhampati, S. Knoblock, C. 73-78. 
Condon, Sherri. Aberdeen, John. Albin, Matthew. 
Freeman, Andy. Mani, Inderjeet. Rubenstein, Alan. 
Sarver, Keri. Sexton, Mike. Yeh, Alex. (2006) 
?Multilingual Name Matching Mid-Year Status 
Report.? 
Condon, S. Freeman, A. Rubenstein, A. Yeh, A. 
(2006) ?Strategies for Chinese Name Matching.? 
Freeman, A. Condon, S. Ackermann, C. (2006) 
"Cross Linguistic Name Matching in English and 
Arabic: A ?One to Many Mapping? Extension of 
the Levenshtein Edit Distance Algorithm." Pro-
ceedings of NAACL/HLT. 
Gao, W. Wong, K. Lam, W. (2004) ?Phoneme-Based 
Transliteration of Foreign Names for OOV Prob-
lem.? Proceedings of the First International Joint 
Conference on Natural Language Processing. 
Goto, I. Kato, N. Uratani, N. Ehara, T. (2003) ?Trans-
literation Considering Context Information Based 
on the Maximum Entropy Method.? Proceedings 
of MT-Summit IX. 
Huang, Shudong. (2005) ?LDC2005T34: Chinese <-> 
English Named Entity Lists v 1.0.? Linguistics Da-
ta Consortium. Philadelphia, Pennsylvania.  ISBN 
#1-58563-368-2. http://www.ldc.upenn.edu/Cata 
log/CatalogEntry.jsp?catalogId=LDC2005T34. 
International Phonetic Association. (1999) Handbook 
of the International Phonetic Association : A Guide 
to the Use of the International Phonetic Alphabet. 
Cambridge University Press, UK. ISBN 
0521652367. http://www.cambridge.org/uk/cata 
logue/catalogue.asp?isbn=0521652367.  
Jung, S. Hong, S. Paek, E. (2000) ?An English to Ko-
rean Transliteration Model of Extended Markov 
Window.? Proceedings of COLING. 
Kang, B.J. Choi, K.S. (2000) ?Automatic Translitera-
tion and Back-Transliteration by Decision Tree 
Learning.? Proceedings of the 2nd International 
Conference on Language Resources and Evalua-
tion. 
Klatt, D.H. (1990) ?Review of the ARPA Speech Un-
derstanding Project.? Readings in Speech Recogni-
tion. Morgan Kaufmann Publishers Inc. San Fran-
cisco, CA. ISBN 1-55860-124-4.  554-575. 
Knight, K. Graehl, J. (1997) ?Machine Translitera-
tion.? Proceedings of the Conference of the Asso-
ciation for Computational Linguistics (ACL). 
Kondrak, G. (2000) ?A New Algorithm for the 
Alignment of Phonetic Sequences.? Proceedings of 
the First Meeting of the North American Chapter 
of the Association for Computational Linguistics 
(NAACL). Seattle, Washington. 288-295. 
Kondrak, G. Dorr, B. (2004) ?Identification of Con-
fusable Drug Names: A New Approach and Evalu-
ation Methodology.? Proceedings of the Twentieth 
International Conference on Computational Lin-
guistics (COLING). 952-958. 
 Levenshtein, V.I. (1966) ?Binary Codes Capable of 
Correcting Deletions, Insertions and Reversals.? 
Sov. Phys. Dokl. 6. 707-710. 
Li, H. Zhang, M. Su, J. (2004) ?A Joint Source-
Channel Model for Machine Transliteration.? Pro-
ceedings of ACL 2004. 
Mani, Inderjeet. Yeh, Alexander. Condon, Sherri. 
(2006) "Machine Learning from String Edit Dis-
tance and Phonological Similarity." 
Meng, H. Lo, W. Chen, B. Tang, T. (2001) ?Generat-
ing Phonetic Cognates to Handle Named Entities in 
English-Chinese Cross-Language Spoken Docu-
ment Retrieval.? Proceedings of ASRU. 
Oh, Jong-Hoon. Choi, Key-Sun. (2006) ?An Ensem-
ble of Transliteration Models for Information Re-
trieval.? Information Processing & Management. 
42(4). 980-1002. 
 ?Student?s t Test.? (2009) Wikipedia: The Free En-
cyclopedia. http://en.wikipedia.org/wiki/T_test# 
Equal_sample_sizes.2C_equal_variance. 
Van Halteren, H., Zavrel, J. Daelemans, W. (1998) 
?Improving Data Driven Word-Class Tagging by 
System Combination.? Proceedings of the 36th 
Annual Meeting of the Association for Computa-
tional Linguistics and the 17th International Con-
ference on Computational Linguistics. Montr?al, 
Qu?bec, Canada. 491-497. 
Virga, P. Khudanpur, S. (2003) ?Transliteration of 
Proper Names in Cross-Lingual Information Re-
trieval.? Proceedings of the ACL Workshop on 
Multi-lingual Named Entity Recognition. 
Wan, Stephen. Verspoor, Cornelia Maria. (1998). 
"Automatic English-Chinese Name Transliteration 
for Development of Multilingual Resources." Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics. Montr?al, 
Qu?bec, Canada.                     
Wellner, B. Castano, J. Pustejovsky, J. (2005) ?Adap-
tive String Similarity Metrics for Biomedical Ref-
erence Resolution.? Proceedings of the ACL-ISMB 
Workshop on Linking Biological Literature, Ontol-
ogies, and Databases: Mining Biological Seman-
tics. 9-16. http://www.cs.brandeis.edu/~wellner/ 
pubs/Wellner-StringSim-BioLINK.pdf. 
Winkler, W. ?Methods for Record Linkage and Baye-
sian Networks.? (2002) Proceedings of the Section 
on Survey Research Methods, American Statistical 
Association. http://www.census.gov/srd/www/ 
byyear.html. 
160
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 2?9
Manchester, August 2008
Learning to Match Names Across Languages 
Inderjeet Mani 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730, USA 
imani@mitre.org 
Alex Yeh 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730, USA 
asy@mitre.org 
Sherri Condon 
The MITRE Corporation 
7515 Colshire Drive 
McLean, VA 22102, USA 
scondon@mitre.org 
 
Abstract 
We report on research on matching 
names in different scripts across languag-
es. We explore two trainable approaches 
based on comparing pronunciations. The 
first, a cross-lingual approach, uses an 
automatic name-matching program that 
exploits rules based on phonological 
comparisons of the two languages carried 
out by humans. The second, monolingual 
approach, relies only on automatic com-
parison of the phonological representa-
tions of each pair. Alignments produced 
by each approach are fed to a machine 
learning algorithm. Results show that the 
monolingual approach results in ma-
chine-learning based comparison of per-
son-names in English and Chinese at an 
accuracy of over 97.0 F-measure. 
1 Introduction 
The problem of matching pairs of names which 
may have different spellings or segmentation 
arises in a variety of common settings, including 
integration or linking database records, mapping 
from text to structured data (e.g., phonebooks, 
gazetteers, and biological databases), and text to 
text comparison (for information retrieval, 
clustering, summarization, coreference, etc.).  
For named entity recognition, a name from a 
gazetteer or dictionary may be matched against 
text input; even within monolingual applications, 
the forms of these names might differ. In multi-
document summarization, a name may have 
different forms across different sources. Systems 
                                                 
? 2008 The MITRE Corporation.  All rights reserved. Licensed for 
use in the proceedings of the Workshop on Multi-source, Multilin-
gual Information Extraction and Summarization (MIMIES2) at 
COLING?2008. 
that address this problem must be able to handle 
variant spellings, as well as abbreviations, 
missing or additional name parts, and different 
orderings of name parts.  
In multilingual settings, where the names 
being compared can occur in different scripts in 
different languages, the problem becomes 
relevant to additional practical applications, 
including both multilingual information retrieval 
and machine translation. Here special challenges 
are posed by the fact that there usually aren?t 
one-to-one correspondences between sounds 
across languages. Thus the name Stewart, 
pronounced   / s t u w ? r t / in IPA, can be 
mapped to Mandarin ????? ?, which is 
Pinyin ?si tu er te?, pronounced /s i t? u a ? t? e/, 
and the name Elizabeth / I l I z ? b ? ?/ can map 
to ??????, which is Pinyin ?yi li sha bai?, 
pronounced /I l I ? ? p aI/. Further, in a given 
writing system, there may not be a one-to-one 
correspondence between orthography and sound, 
a well-known case in point being English. In 
addition, there may be a variety of variant forms, 
including dialectical variants, (e.g., Bourguiba 
can map to Abu Ruqayba), orthographic 
conventions (e.g., Anglophone Wasim can map 
to Francophone Ouassime), and differences in 
name segmentation (Abd Al Rahman can map to 
Abdurrahman).  Given the high degree of 
variation and noise in the data, approaches based 
on machine learning are needed. 
The considerable differences in possible 
spellings of a name also call for approaches 
which can compare names based on 
pronunciation. Recent work has developed 
pronunciation-based models for name 
comparison, e.g., (Sproat, Tao and Zhai 2006) 
(Tao et al 2006). This paper explores trainable 
pronunciation-based models further.  
2
Table 1: Matching ?Ashburton? and ?????? 
Consider the problem of matching Chinese 
script names against their English (Pinyin) Ro-
manizations. Chinese script has nearly 50,000 
characters in all, with around 5,000 characters in 
use by the well-educated. However, there are 
only about 1,600 Pinyin syllables when tones are 
counted, and as few as 400 when they aren?t. 
This results in multiple Chinese script represen-
tations for a given Roman form name and many 
Chinese characters that map to the same Pinyin 
forms. In addition, one can find multiple Roman 
forms for many names in Chinese script, and 
multiple Pinyin representations for a Chinese 
script representation.  
In developing a multilingual approach that can 
match names from any pair of languages, we 
compare an approach that relies strictly on mo-
nolingual knowledge for each language, specifi-
cally, grapheme-to-phoneme rules for each lan-
guage, with a method that relies on cross-lingual 
rules which in effect map between graphemic 
and/or phonemic representations for the specific 
pair of languages.  
The monolingual approach requires finding 
data on the phonemic representations of a name 
in a given language, which (as we describe in 
Section 4) may be harder than finding more 
graphemic representations. But once the 
phonemic representation is found for names in a 
given language, then as one adds more languages 
to a system, no more work needs to be done in 
that given language. In contrast, with the cross-
lingual approach, whenever a new language is 
added, one needs to  go over all the existing 
languages already in the system and compare 
each of them with the new language to develop 
cross-lingual rules for each such language pair. 
The engineering of such rules requires bilingual 
expertise, and knowledge of differences between 
language pairs. The cross-lingual approach is 
thus more expensive to develop, especially for 
applications which require coverage of a large 
number of languages. 
Our paper investigates whether we can address 
the name-matching problem without requiring 
such a knowledge-rich approach, by carrying out 
a comparison of the performance of the two 
approaches. We present results of large-scale 
machine-learning for matching personal names 
in Chinese and English, along with some 
preliminary results for English and Urdu. 
2 Basic Approaches 
2.1 Cross-Lingual Approach 
Our cross-lingual approach (called MLEV) is 
based on (Freeman et al 2006), who used a 
modified Levenshtein string edit-distance 
algorithm to match Arabic script person names 
against their corresponding English versions. The 
Levenshtein edit-distance algorithm counts the 
minimum number of insertions, deletions or 
substitutions required to make a pair of strings  
match. Freeman et al (2006) used (1) insights 
about phonological differences between the two  
languages to create rules for equivalence classes 
of characters that are treated as identical in the 
computation of edit-distance and (2) the use of 
normalization rules applied to the English and 
transliterated Arabic names based on mappings 
between characters in the respective writing 
systems. For example, characters corresponding 
to low diphthongs in English are normalized as 
?w?, the transliteration for the Arabic 
???character, while high diphthongs are mapped 
to ?y?, the transliteration for the Arabic ??? 
character.   
Table 1 shows the representation and 
comparison of a Roman-Chinese name pair 
(shown in the title) obtained from the Linguistic 
Data Consortium?s LDC Chinese-English name 
pairs corpus (LDC 2005T34). This corpus 
provides name part pairs, the first element in 
English (Roman characters) and the second in 
Chinese characters, created by the LDC from 
Xinhua Newswire's proper name and who's who 
databases. The name part can be a first, middle 
or last name. We compare the English form of 
the name with a Pinyin Romanization of the 
Chinese. (Since the Chinese is being compared 
with English, which is toneless, the tone part of 
Pinyin is being ignored throughout this paper.) 
For this study, the Levenshtein edit-distance 
score (where a perfect match scores zero) is 
 Roman Chinese (Pinyin) Alignment Score 
LEV ashburton ashenbodu |   a   s   h   b   u   r   t   o   n   | 
|   a   s   h   e   n   b  o  d    u   | 
0.67 
MLEV ashburton ashenbodu |  a   s   h   -   -   b   u   r    t   o   n  | 
|  a   s   h   e   n   b   o   -   d   u   -  | 
0.72 
MALINE asVburton aseCnpotu |   a   sV  -   b   <   u   r   t   o   |   n 
|   a   s   eC  n   p   o   -   t   u   |   - 
0.48 
3
normalized to a similarity score as in (Freeman et 
al. 2006), where the score ranges from 0 to 1, 
with 1 being a perfect match. This edit-distance 
score is shown in the LEV row. 
The MLEV row, under the Chinese Name 
column, shows an ?Englishized? normalization 
of the Pinyin for Ashburton. Certain characters or 
character sequences in Pinyin are pronounced 
differently than in English. We therefore apply 
certain transforms to the Pinyin; for example, the 
following substitutions are applied at the start of 
a Pinyin syllable, which makes it easier for an 
English speaker to see how to pronounce it and 
renders the Pinyin more similar to English 
orthography: ?u:? (umlaut ?u?) => ?u?, ?zh? => 
?j?, ?c? => ?ts?, and ?q? => ?ch? (so the Pinyin 
?Qian? is more or less pronounced as if it were 
spelled as ?Chian?, etc.). The MLEV algorithm 
uses equivalence classes that allow ?o? and ?u? 
to match, which results in a higher score than the 
generic score using the LEV method.  
2.2 Monolingual Approach 
Instead of relying on rules that require extensive 
knowledge of differences between a language 
pair2, the monolingual approach first builds pho-
nemic representations for each name, and then 
aligns them. Earlier research by (Kondrak 2000) 
used dynamic programming to align strings of 
phonemes, representing the phonemes as vectors 
of phonological features, which are associated 
with scores to produce similarity values. His 
program ALINE includes a ?skip? function in the 
alignment operations that can be exploited for 
handling epenthetic segments, and in addition to 
1:1 alignments, it also handles 1:2 and 2:1 
alignments. In this research, we made extensive 
modifications to ALINE to add the phonological 
features for languages like Chinese and Arabic 
and to normalize the similarity scores, producing 
a system called MALINE. 
In Table 1, the MALINE row3 shows that the 
English name has a palato-alveolar modification 
                                                 
                                                                         
2As (Freeman et al, 2006) point out, these insights are 
not easy to come by: ?These rules are based on first 
author Dr. Andrew Freeman?s experience with read-
ing and translating Arabic language texts for more 
than 16 years? (Freeman et al, 2006, p. 474). 
3For the MALINE row in Table 1, the ALINE docu-
mentation explains the notation as follows: ?every 
phonetic symbol is represented by a single lowercase 
letter followed by zero or more uppercase letters. The 
initial lowercase letter is the base letter most similar 
to the sound represented by the phonetic symbol. The 
remaining uppercase letters stand for the feature mod-
on the ?s? (expressed as ?sV?), so that we get the 
sound corresponding to ?sh?; the Pinyin name 
inserts a centered ?e? vowel, and devoices the 
bilabial plosive /b/ to /p/. There are actually 
sixteen different Chinese ?pinyinizations? of 
Ashburton, according to our data prepared from 
the LDC corpus.  
3 Experimental Setup 
3.1 Machine Learning Framework 
Neither of the two basic approaches described so 
far use machine learning. Our machine learning 
framework is based on learning from alignments 
produced by either approach. To view the learn-
ing problem as one amenable to a statistical clas-
sifier, we need to generate labeled feature vectors 
so that each feature vector includes an additional 
class feature that can have the value ?true? or 
?false.? Given a set of such labeled feature vec-
tors as training data, the classifier builds a model 
which is then used to classify unlabeled feature 
vectors with the right labels. 
A given set of attested name pairs constitutes a 
set of positive examples. To create negative 
pairs, we have found that randomly selecting 
elements that haven?t been paired will create 
negative examples in which the pairs of elements 
being compared are so different that they can be 
trivially separated from the positive examples. 
The experiments reported here used the MLEV 
score as a threshold to select negatives, so that 
examples below the threshold are excluded. As 
the threshold is raised, the negative examples 
should become harder to discriminate from 
positives (with the harder problems mirroring 
some of the ?confusable name? characteristics of 
the real-world name-matching problems this 
technology is aimed at). Positive examples below 
the threshold are also eliminated. Other criteria, 
including a MALINE score, could be used, but 
the MLEV scores seemed adequate for these 
preliminary experiments.  
Raising the threshold reduces the number of 
negative examples. It is highly desirable to 
balance the number of positive and negative 
examples in training, to avoid the learning being 
 
ifiers which alter the sound defined by the base letter. 
By default, the output contains the alignments togeth-
er with the overall similarity scores. The aligned sub-
sequences are delimited by '|' signs. The '<' sign signi-
fies that the previous phonetic segment has been 
aligned with two segments in the other sequence, a 
case of compression/expansion. The '-' sign denotes a 
?skip?, a case of insertion/deletion.?  
4
biased by a skewed distribution. However, when 
one starts with a balanced distribution of positive 
and negatives, and then excludes a number of 
negative examples below the threshold, a 
corresponding number of positive examples must 
also be removed to preserve the balance. Thus, 
raising the threshold reduces the size of the 
training data. Machine learning algorithms, 
however, can benefit from more training data.  
Therefore, in the experiments below, thresholds 
which provided woefully inadequate training set 
sizes were eliminated.  
One can think of both the machine learning 
method and the basic name comparison methods 
(MLEV and MALINE) as taking each pair of 
names with a known label and returning a 
system-assigned class for that pair. Precision, 
Recall, and F-Measure can be defined in an 
identical manner for both machine learning and 
basic name comparison methods. In such a 
scheme, a threshold on the similarity score is 
used to determine whether the basic comparison 
match is a positive match or not. Learning the 
best threshold for a dataset can be determined by 
searching over different values for the threshold.  
In short, the methodology employed for this 
study involves two types of thresholds: the 
MLEV threshold used to identify negative 
examples and the threshold that is applied to the 
basic comparison methods, MLEV and 
MALINE, to identify matches. To avoid 
confusion, the term negative threshold refers to 
the former, while the term positive threshold is 
used for the latter. 
The basic comparison methods were used as 
baselines in this research. To be able to provide a 
fair basic comparison score at each negative 
threshold, we ?trained? each basic comparison 
matcher at twenty different positive thresholds 
on the same training set used by the learner.  For 
each negative threshold, we picked the positive 
threshold that gave the best performance on the 
training data, and used that to score the matcher 
on the same test data as used by the learner.  
3.2 Feature Extraction 
Consider the MLEV alignment in Table 1. It can 
be seen that the first three characters are matched 
identically across both strings; after that, we get 
an ?e? inserted, an ?n? inserted, a ?b? matched 
identically, a ?u? matched to an ?o?, a ?r? de-
leted, a ?t? matched to a ?d?, an ?o? matched to a 
?u?, and an ?n? deleted. The match unigrams are 
thus ?a:a?, ?s:s?, ?h:h?, ?-:e?, ?-:n?, ?b:b?, ?u:o?, 
?r:-?, ?t:d?, ?o:u?, and ?n:-?. Match bigrams 
were generated by considering any insertion, de-
letion, and (non-identical) substitution unigram, 
and noting the unigram, if any, to its left, pre-
pending that left unigram to it (delimited by a 
comma). Thus, the match bigrams in the above 
example include ?h:h,-:e?, ?-:e,-:n?, ?b:b,u:o?, 
?u:o,r:-?, ?r:-,t:d?, ?t:d,o:u?, ?o:u,n:-?.  
These match unigram and match bigram 
features are generated from just a single MLEV 
match. The composite feature set is the union of 
the complete match unigram and bigram feature 
sets. Given the composite feature set, each match 
pair is turned into a feature vector consisting of 
the following features: string1, string2, the match 
score according to each of the basic comparison 
matchers (MLEV and MALINE), and the 
Boolean value of each feature in the composite 
feature set. 
3.3 Data Set 
Our data is a (roughly 470,000 pair) subset of the 
Chinese-English personal name pairs in LDC 
2005T34. About 150,000 of the pairs had more 
than 1 way to pronounce the English and/or Chi-
nese. For these, to keep the size of the experi-
ments manageable from the point of view of 
training the learners, one pronunciation was ran-
domly chosen as the one to use. (Even with this 
restriction, a minimum negative threshold results 
in over half a million examples). Chinese charac-
ters were mapped into Hanyu Pinyin representa-
tions, which are used for MLEV alignment and 
string comparisons.   Since the input to MALINE 
uses a phonemic representation that encodes 
phonemic features in one or more letters, both 
Pinyin and English forms were mapped into the 
MALINE notation.   
There are a number of slightly varying ways to 
map Pinyin into an international pronunciation 
system like IPA. For example, (Wikipedia 2006) 
and (Salafra 2006) have mappings that differ 
from each other and also each of these two 
sources have changed its mapping over time. We 
used a version of Salafra from 2006 (but we 
ignored the ejectives). For English, the CMU 
pronouncing dictionary (CMU 2008) provided 
phonemic representations that were then mapped 
into the MALINE notation. The dictionary had 
entries for 12% of our data set. For the names not 
in the CMU dictionary, a simple grapheme to 
phoneme script provided an approximate 
phonemic form. We did not use a monolingual 
mapping of Chinese characters (Mandarin 
pronunciation) into IPA because we did not find 
any. 
5
60
65
70
75
80
85
90
95
100
105
0 0.2 0.4 0.6 0.8
M
X
C
MB
XB
CB
Note that we could insist that all pairs in our 
dataset be distinct, requiring that there be exactly 
one match for each Roman name and exactly one 
match for each Pinyin name. This in our view is 
unrealistic, since large corpora will be skewed 
towards names which tend to occur frequently 
(e.g., international figures in news) and occur 
with multiple translations.  We included attested 
match pairs in our test corpora, regardless of the 
number of matches that were associated with a 
member of the pair. 
4 Results 
A variety of machine learning algorithms were 
tested. Results are reported, unless otherwise in-
dicated, using SVM Lite, a Support Vector Ma-
chine (SVM4) classifier5 that scales well to large 
data sets.  
Testing with SVM Lite was done with a 90/10 
train-test split. Further testing was carried out 
with the weka SMO SVM classifier, which used 
built-in cross-validation. Although the latter clas-
sifier didn?t scale to the larger data sets we used, 
it did show that cross-validation didn?t change 
the basic results for the data sets it was tried on.  
4.1 Machine Learning with Different Fea-
ture Sets 
Figure 1:  F-measure with Different Fea-
ture Sets 
Figure 1 shows the F-measure of learning for 
monolingual features (M, based on MALINE), 
cross-lingual features (X, based on MLEV), and 
a combined feature set (C) of both types of fea-
tures6 at different negative thresholds (shown on 
the horizontal axis). Baselines are shown with 
the suffix B, e.g., the basic MALINE without 
learning is MB. When using both monolingual 
and cross-lingual features (C), the baseline (CB) 
                                                 
                                                
4We used a linear kernel function in our SVM expe-
riments; using polynomial or radial basis kernels did 
not improve performance. 
5 From svmlight.joachims.org. 
6In Figure 1, the X curve is more or less under the C 
curve. 
is set to a system response of ?true? only when 
both the MALINE and MLEV baseline systems 
by themselves respond ?true?. Table 2 shows the 
number of examples at each negative threshold 
and the Precision and Recall for these methods, 
along with baselines using the basic methods 
shown in square brackets. 
The results show that the learning method (i) 
outperforms the baselines (basic methods), and 
(ii) the gap between learning and basic compari-
son widens as the problem becomes harder (i.e., 
as the threshold is raised). 
For separate monolingual and cross-lingual 
learning, the increase in accuracy of the learning 
over the baseline (non-learning) results7 was sta-
tistically significant at all negative thresholds 
except 0.6 and 0.7. For learning with combined 
monolingual and cross-lingual features (C), the 
increase over the baseline (non-learning) com-
bined results was statistically significant at all 
negative thresholds except for 0.7. 
In comparing the mono-lingual and cross-
lingual learning approaches, however, the only 
statistically significant differences were that the 
cross-lingual features were more accurate than 
the monolingual features at the 0 to 0.4 negative 
thresholds. This suggests that (iii) the mono-
lingual learning approach is as viable as the 
cross-lingual one as the problem of confusable 
names becomes harder.  
However, using the combined learning ap-
proach (C) is better than using either one. Learn-
ing accuracy with both monolingual and cross-
lingual features is statistically significantly better 
than learning with monolingual features at the 
0.0 to 0.4 negative thresholds, and better than 
learning with cross-lingual features at the 0.0 to 
0.2, and 0.4 negative thresholds. 
 
7Statistical significance between F-measures is not 
directly computable since the overall F-measure is not 
an average of the F-measures of the data samples. 
Instead, we checked the statistical significance of the 
increase in accuracy (accuracy is not shown for rea-
sons of space) due to learning over the baseline. The 
statistical significance test was done by assuming that 
the accuracy scores were binomials that were approx-
imately Gaussian. When the Gaussian approximation 
assumption failed (due to the binomial being too 
skewed), a looser, more general bound was used 
(Chebyshev?s inequality, which applies to all proba-
bility distributions). All statistically significant differ-
ences are at the 1% level (2-sided). 
6
4.2 Feature Set Analyses 
The unigram features reflect common correspon-
dences between Chinese and English pronuncia-
tion.  For example, (Sproat, Tao and Zhai 2006) 
note that Chinese /l/ is often associated with Eng-
lish /r/, and the feature l:r is among the most fre-
quent unigram mappings in both the MLEV and 
MALINE alignments. At a frequency of 103,361, 
it is the most frequent unigram feature in the 
MLEV mappings, and it is the third most fre-
quent unigram feature in the MALINE align-
ments (56,780). 
Systematic correspondences among plosives 
are also captured in the MALINE unigram map-
pings.  The unaspirated voiceless Chinese plo-
sives /p,t,k/ contrast with aspirated plosives 
/p?,t?,k?/, whereas the English voiceless plosives 
(which are aspirated in predictable environments) 
contrast with voiced plosives /b,d,g/.  As a result, 
English /b,d,g/ phonemes are usually translite-
rated using Chinese characters that are pro-
nounced /p,t,k/, while English /p,t,k/ phonemes 
usually correspond to Chinese /p?,t?,k?/. The ex-
amples of Stewart and Elizabeth in Section 1 
illustrate the correspondence of English /t/ and 
Chinese / t?/ and of English /b/ with Chinese /p/ 
respectively. All six of the unigram features that  
result from these correspondences occur among 
the 20 most frequent in the MALINE alignments, 
ranging in frequency from 23,602 to 53,535. 
 
 
Neg-
ative 
Thre-
shold 
Exam-
ples 
Monolingual  (M) Cross-Lingual (X) Combined (C) 
  P R P R P R 
0 538,621 94.69 
[90.6] 
95.73 
[91.0] 
96.5 
[90.0] 
97.15 
[93.4] 
97.13 
[90.8] 
97.65 
[91.0] 
0.1 307,066 95.28 
[87.1] 
96.23 
[83.4] 
98.06 
[89.2] 
98.25 
[89.9] 
98.4 
[87.6] 
98.64 
[84.1] 
0.2 282,214 95.82 
[86.2] 
96.63 
[84.4] 
97.91 
[88.4] 
98.41 
[90.3] 
98.26 
[86.7] 
98.82 
[84.7] 
0.3 183,188 95.79 
[80.6] 
96.92 
[85.3] 
98.18 
[86.3] 
98.8 
[90.7] 
98.24 
[80.6] 
99.27 
[84.8] 
0.4 72,176 96.31 
[77.1] 
98.69 
[82.3] 
97.89 
[91.8] 
99.61 
[86.2] 
98.91 
[77.1] 
99.64 
[80.9] 
0.5 17,914 94.62 
[64.6] 
98.63 
[84.3] 
99.44 
[89.4] 
100.0 
[91.9] 
99.46 
[63.8] 
99.89 
[84.7] 
0.6 2,954 94.94 
[66.1] 
100 
[77.0] 
98.0 
[85.2] 
98.66 
[92.8] 
99.37 
[61.3] 
100.0 
[73.1] 
0.7 362 95.24 
[52.8] 
100 
[100.0] 
94.74 
[78.9] 
100.0 
[78.9] 
100.0 
[47.2] 
94.74 
[100.0] 
Table 2:  Precision and Recall with Different Feature Sets 
(Baseline scores in square brackets) 
 
4.3 Comparison with other Learners 
To compare with other machine learning tools, 
we used the WEKA toolkit (from 
www.weka.net.nz). Table 3 shows the compar-
isons on the MLEV data for a fixed size at one 
threshold. Except for SVM Light, the results 
are based on 10-fold cross validation.  The 
other classifiers appear to perform relatively 
worse at that setting for the MLEV data, but 
the differences in accuracy are not statistically 
significant even at the 5% level. A large con-
tributor to the lack of significance is the small 
test set size of 66 pairs (10% of 660 examples) 
used in the SVM Light test. 
4.4 Other Language Pairs 
Some earlier experiments for Arabic-Roman 
comparisons were carried out using a Condi-
tional Random Field learner (CRF), using the 
Carafe toolkit (from source-
forge.net/projects/carafe). The method com-
putes its own Levenshtein edit-distance scores, 
and learns edit-distance costs from that. The 
scores obtained, on average, had only a .6 cor-
relation with the basic comparison Levenshtein 
scores. However, these experiments did not 
return accuracy results, as ground-truth data 
was not specified for this task. 
7
Several preliminary machine learning expe-
riments were also carried out on Urdu-Roman 
comparisons. The data used were Urdu data 
extracted from a parallel corpus recently pro-
duced by the LDC (LCTL_Urdu.20060408).  
The results are shown in Table 4. Here a .55 
MALINE score and a .85 MLEV score were 
used for selecting positive examples by basic 
comparison, and negative examples were se-
lected at random. Here the MALINE method 
(row 1) using the weka SMO SVM made use 
of a threshold based on a MALINE score. In 
these earlier experiments, machine learning 
does not really improve the system perfor-
mance (F-measure decreases with learning on 
one test and only increases by 0.1% on the 
other test). However, since these earlier expe-
riments did not benefit from the use of differ-
ent negative thresholds, there was no control 
over problem difficulty.  
5 Related Work 
While there is a substantial literature employ-
ing learning techniques for record linkage 
based on the theory developed by Fellegi and 
Sunter (1969), researchers have only recently 
developed applications that focus on name 
strings and that employ methods which do not 
require features to be independent (Cohen and 
Richman 2002). Ristad and Yianilos (1997) 
have developed a generative model for learn-
ing string-edit distance that learns the cost of 
different edit operations during string align-
ment. Bilenko and Mooney (2003) extend Ris-
tad?s approach to include gap penalties (where 
the gaps are contiguous sequences of mis-
matched characters) and compare this genera-
tive approach with a vector similarity approach 
that doesn?t carry out alignment. McCallum et 
al. (2005) use Conditional Random Fields 
(CRFs) to learn edit costs, arguing in favor of 
discriminative training approaches and against 
generative approaches, based in part on the 
fact that the latter approaches ?cannot benefit 
from negative evidence from pairs of strings 
that (while partially overlapping) should be 
considered dissimilar?. Such CRFs model the 
conditional probability of a label sequence (an 
alignment of two strings) given a sequence of 
observations (the strings).  
A related thread of research is work on au-
tomatic transliteration, where training sets are 
typically used to compute probabilities for 
mappings in weighted finite state transducers 
(Al-Onaizan and Knight 2002; Gao et al 2004) 
or source-channel models (Knight and Graehl 
1997; Li et al 2004). (Sproat et al 2006) have 
compared names from comparable and con-
temporaneous English and Chinese texts, scor-
ing matches by training a learning algorithm to 
compare the phonemic representations of the 
names in the pair, in addition to taking into 
account the frequency distribution of the pair 
over time.  (Tao et al 2006) obtain similar re-
sults using frequency and a similarity score 
based on a phonetic cost matrix 
The above approaches have all developed 
special-purpose machine-learning architectures 
to address the matching of string sequences. 
They take pairs of strings that haven?t been 
aligned, and learn costs or mappings from 
them, and once trained, search for the best 
match given the learned representation  
 
Positive  
Threshold
Examples Method P R F Accuracy 
.65 660 SVM Light 90.62 87.88 89.22 89.39   
.65 660 WEKA SMO 80.6 83.3 81.92 81.66 
.65 660 AdaBoost M1 84.9 78.5 81.57 82.27 
Table 3: Comparison of Different Classifiers 
 
Method Positive 
Threshold 
Examples P R F 
WEKA SMO .55 (MALINE) 206 (MALINE) 84.8 [81.5] 86.4 [93.3] 85.6 [87.0] 
WEKA SMO .85 (MLEV) 584 (MLEV) 89.9 [93.2] 94.7 [91.2] 92.3 [92.2] 
Table 4: Urdu-Roman Name Matching Results with Random Negatives 
(Baseline scores in square brackets) 
 
8
Our approach, by contrast, takes pairs of 
strings along with an alignment, and using fea-
tures derived from the alignments, trains a learn-
er to derive the best match given the features. 
This offers the advantage of modularity, in that 
any type of alignment model can be combined 
with SVMs or other classifiers (we have pre-
ferred SVMs since they offer discriminative 
training). Our approach allows leveraging of any 
existing alignments, which can lead to starting 
the learning from a higher baseline and less train-
ing data to get to the same level of performance. 
Since the learner itself doesn?t compute the 
alignments, the disadvantage of our approach is 
the need to engineer features that communicate 
important aspects of the alignment to the learner.  
In addition, our approach, as with McCallum 
et al (2005), allows one to take advantage of 
both positive and negative training examples, 
rather than positive ones alone. Our data genera-
tion strategy has the advantage of generating 
negative examples so as to vary the difficulty of 
the problem, allowing for more fine-grained per-
formance measures. Metrics based on such a 
control are likely to be useful in understanding 
how well a name-matching system will work in 
particular applications, especially those involving 
confusable names. 
6 Conclusion 
The work presented here has established a 
framework for application of machine learning 
techniques to multilingual name matching.  The 
results show that machine learning dramatically 
outperforms basic comparison methods, with F-
measures as high as 97.0 on the most difficult 
problems. This approach is being embedded in a 
larger system that matches full names using a 
vetted database of full-name matches for evalua-
tion.  
So far, we have confined ourselves to minimal 
feature engineering. Future work will investigate 
a more abstract set of phonemic features. We 
also hope to leverage ongoing work on harvest-
ing name pairs from web resources, in addition 
applying them to less commonly taught languag-
es, as and when appropriate resources for them 
become available. 
References 
Al-Onaizan, Y. and K. Knight, K. 2002. Machine 
Transliteration of Names in Arabic Text. Proceed-
ings of the ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Bilenko, M. and Mooney, R.J. 2003. Adaptive dupli-
cate detection using learnable string similarity 
measures. In Proc. of SIGKDD-2003. 
CMU. 2008. The CMU Pronouncing 
nary. ftp://ftp.cs.cmu.edu/project/speech/dict/ 
Cohen, W. W., and Richman, J. 2002. Learning to 
match and cluster large high-dimensional data sets 
for data integration. In Proceedings of The Eighth 
ACM SIGKDD International Conference on Know-
ledge Discovery and Data Mining (KDD-2002). 
Fellegi, I. and Sunter, A.  1969. A theory for record 
linkage.  Journal of the American Statistical Socie-
ty, 64:1183-1210, 1969. 
Freeman, A., Condon, S. and Ackermann, C. 2006. 
Cross Linguistic Name Matching in English and 
Arabic. Proceedings of HLT. 
Gao, W., Wong, K., and Lam, W. 2004. Phoneme-
based transliteration of foreign names for OOV 
problem.  In Proceedings of First International 
Joint Conference on Natural Language Processing. 
Kondrak, G. 2000. A New Algorithm for the Align-
ment of Phonetic Sequences. Proceedings of the 
First Meeting of the North American Chapter of 
the Association for Computational Linguistics 
(ANLP-NAACL 2000),  288-295.  
Knight, K. and Graehl, J., 1997. Machine Translitera-
tion, In Proceedings of the Conference of the Asso-
ciation for Computation Linguistics (ACL). 
Li, H., Zhang, M., & Su, J. 2004. A joint source-
channel model for machine transliteration.  In Pro-
ceedings of Conference of the Association for 
Computation Linguistics (ACL). 
McCallum, A., Bellare, K. and Pereira, F. 2005. A 
Conditional Random Field for Discriminatively-
trained Finite-state String Edit Distance. Confe-
rence on Uncertainty in AI (UAI). 
Ristad, E. S. and Yianilos, P. N. 1998.  Learning 
string edit distance.  IEEE Transactions on Pattern 
Recognition and Machine Intelligence. 
Salafra. 2006. http://www.safalra.com /science 
/linguistics/pinyin-pronunciation/ 
Sproat, R., Tao, T. and Zhai, C. 2006.  Named Entity 
Transliteration with Comparable Corpora.  In Pro-
ceedings of the Conference of the Association for 
Computational Linguistics.  New York. 
Tao, T., Yoon, S. Fister, A., Sproat, R. and Zhai, C. 
2006.  Unsupervised Named Entity Transliteration 
Using Temporal and Phonetic Correlation.  In Pro-
ceedings of the ACL Empirical Methods in Natural 
Language Processing Workshop. 
Wikipedia. 2006. http://en.wikipedia.org/wiki/Pinyin 
9
