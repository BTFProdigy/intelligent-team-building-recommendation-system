Direct Orthographical Mapping for Machine Transliteration  
ZHANG Min          LI Haizhou        SU Jian 
 
Institute for Infocomm Research  
21 Heng Mui Keng Terrace, Singapore 11961 
{mzhang, hli, sujian}@i2r.a-star.edu.sg 
 
Abstract 
Machine transliteration/back-transliteration plays 
an important role in many multilingual speech and 
language applications. In this paper, a novel 
framework for machine transliteration/back-
transliteration that allows us to carry out direct 
orthographical mapping (DOM) between two 
different languages is presented. Under this 
framework, a joint source-channel transliteration 
model, also called n-gram transliteration model (n-
gram TM), is further proposed to model the 
transliteration process. We evaluate the proposed 
methods through several transliteration/back-
transliteration experiments for English/Chinese and 
English/Japanese language pairs. Our study reveals 
that the proposed method not only reduces an 
extensive system development effort but also 
improves the transliteration accuracy significantly.  
1 Introduction 
Many technical terms and proper names, such as 
personal, location and organization names, are 
translated from one language into another language 
with approximate phonetic equivalents. The 
phonetic translation from the native language to 
foreign language is defined as transliteration; 
conversely, the process of recalling a word in 
native language from a transliteration is defined as 
back-transliteration. For example, English name 
?Smith? and ????  (pinyin 1 : Shi-Mi-Si)? in 
Chinese form a pair of transliteration and back-
transliteration. In many natural language 
processing tasks, such as multilingual named entity 
and term processing, machine translation, corpus 
alignment, cross lingual information retrieval and 
automatic bilingual dictionary compilation, 
automatic name transliteration has become an 
indispensable component. 
Recent efforts are reported for several language 
pairs, such as English/Chinese (Meng et al, 2001; 
Virga et al, 2003; Lee et al, 2003; Gao et al, 
2004; Guo et al, 2004), English/Japanese (Knight 
et al, 1998; Brill et al, 2001; Bilac et al, 2004), 
                                                     
1 Pinyin is the standard Romanization of Chinese. 
English/Korean (Oh et al, 2002; Sung et al, 
2000), and English/Arabic (Yaser et al, 2002). 
Most of the reported works utilize a phonetic clue 
to resolve the transliteration through a multiple 
step phonemic mapping where algorithms, such as 
dictionary lookup, rule-based and machine 
learning-based approaches, have been well 
explored. 
In this paper, we will discuss the limitation of 
the previous works and present a novel framework 
for machine transliteration. The new framework 
carries out the transliteration by direct 
orthographical mapping (DOM) without any 
intermediate phonemic mapping. Under this 
framework, we further propose a joint source-
channel transliteration mode (n-gram TM) as an 
alternative machine learning-based approach to 
model the source-target word orthographic 
association. Without the loss of generality, we 
evaluate the performance of the proposed method 
for English/Chinese and English/Japanese pairs. 
An experiment that compares the proposed method 
with several state-of-art approaches is also 
presented. The results reveal that our method 
outperforms other previous methods significantly. 
The reminder of the paper is organized as 
follows. Section 2 reviews the previous work. In 
section 3, the DOM framework and n-gram TM 
model are formulated. Section 4 describes the 
evaluation results and compares our method with 
other reported work. Finally, we conclude the 
study with some discussions. 
2 Previous Work 
The topic of machine transliteration has been 
studied extensively for several different language 
pairs, and many techniques have been proposed. 
To better understand the nature of the problem, we 
review the previous work from two different 
viewpoints: the transliteration framework and the 
transliteration model. The transliteration model is 
built to capture the knowledge of bilingual 
phonetic association and subsequently is applied to 
the transliteration process. 
2.1 Transliteration Framework 
The phoneme-based approach has received 
remarkable attention in the previous works (Meng 
et al, 2001; Virga et al, 2003; Knight et al, 1998; 
Oh et al, 2002; Sung et al, 2000; Yaser et al, 
2002; Lee et al, 2003). In general, this approach 
includes the following three intermediate 
phonemic/orthographical mapping steps: 
1) Conversion of a source language word into 
its phonemic representation (grapheme-to-
phoneme conversion, or G2P); 
2) Transformation of the source language 
phonemic representation to the target 
language phonemic representation; 
3) Generation of target language orthography 
from its phonemic representation (phoneme-
to-grapheme conversion, or P2G). 
To achieve phonetic equivalent transliteration, 
phoneme-based approach has become the most 
popular approach. However, the success of 
phoneme-based approach is limited by the 
following constraints: 
1) Grapheme-to-phoneme conversion, 
originated from text-to-speech (TTS) 
research, is far from perfect (The 
Onomastica Consortium, 1995), especially 
for the name of different language origins. 
2) Cross-lingual phonemic mapping presents a 
great challenge due to phonemic divergence 
between some language pairs, such as 
Chinese/English, Japanese/English (Wan 
and Verspoor, 1998; Meng et al, 2001). 
3) The conversion of phoneme-to-grapheme 
introduces yet another level of imprecision, 
esp. for the ideographic language, such as 
Chinese.  Virga and Khudanpur (2003) 
reported 8.3% absolute accuracy drops when 
converting from Pinyin to Chinese character. 
The three error-prone steps as stated above lead 
to an inferior overall system performance. The 
complication of multiple steps and introduction of 
intermediate phonemes also incur high cost in 
system development when moving from one 
language pair to another, because we have to work 
on language specific ad-hoc phonic rules. 
2.2 Transliteration Model 
Transliteration model is a knowledge base to 
support the execution of transliteration strategy. To 
build the knowledge base, machine learning or 
rule-based algorithms are adopted in phoneme-
based approach. For instance, noisy-channel model 
(NCM) (Virga et al, 2003; Lee et al, 2003), 
HMM (Sung et al, 2000), decision tree (Kang et 
al., 2000), transformation-based learning (Meng et 
al., 2001), statistical machine transliteration model 
(Lee et al, 2003), finite state transducers (Knight 
et al, 1998) and rule-based approach (Wan et al, 
1998; Oh et al, 2002). It is observed that the 
reported transliteration models share a common 
strategy, that is:  
1) To model the transformation rules; 
2) To model the target language; 
3) To model the above both; 
However, the modeling of different knowledge 
is always done independently. For example, NCM 
and HMM (Virga et al, 2003; Lee et al, 2003; 
Sung et al, 2000) model the transformation 
mapping rules and the target language separately; 
decision tree (Kang et al, 2000), transformation-
based learning (Meng et al, 2001), finite state 
transducers (Knight et al, 1998) and statistical 
machine transliteration model (Lee et al, 2003) 
only model the transformation rules.  
3 Direct Orthographical Mapping 
To overcome the limitation of phoneme-based 
approach, we propose a unified framework for 
machine transliteration, direct orthographical 
mapping (DOM). The DOM framework tries to 
model phonetic equivalent association by fully 
exploring the orthographical contextual 
information and the orthographical mapping. 
Under the DOM framework, we propose a joint 
source-channel transliteration model (n-gram TM) 
to capture the source-target word orthographical 
mapping relation and the contextual information. 
Unlike the noisy-channel model, the joint source-
channel model does not try to capture how the 
source names can be mapped to the target names, 
but rather how both source and target names can be 
generated simultaneously. 
The proposed framework is applicable to all 
language pairs. For simplicity, in this section, we 
take English/Chinese pair as example in the 
formulation, where E2C refers to English to 
Chinese transliteration and C2E  refers to Chinese 
to English back-transliteration. 
3.1 Transliteration Pair and Alignment 
Suppose that we have an English name 
1... ...i mx x x? =  and a Chinese transliteration 
1... ...i ny y y? = where ix are English letters and 
jy are Chinese characters. The English name ?  
and its Chinese Transliteration ?  can be 
segmented into a series of substrings: 
1 2... Ke e e? =  and 1 2... Kc c c? =  ( min( , )k m n< ). 
We call the substring as transliteration unit and 
each English transliteration unit ie  is aligned with 
a corresponding Chinese transliteration unit ic  to 
form a transliteration pair. An alignment between 
? and ?  is defined as ?  with 
1 1 1, ,e c e c< > =< >  
2 2 2, ,e c e c< > =< >  ?  
and , ,K K Ke c e c< > =< > . A transliteration pair 
ice >< ,  represents a two-way mapping between 
ie  and  ic . A unit could be a Chinese character or 
a monograph, a digraph or a trigraph and so on for 
English. For example, ??|a ?|b ?|ru ?|zzo? is 
one alignment of Chinese-English word pair  ??
???? and ?abruzzo?. 
3.2 DOM Transliteration Framework 
By the definition of ? , ?  and ? , the E2C 
transliteration can be formulated as 
),,(maxarg
)),,(maxarg(maxarg
),,(maxarg
),(maxarg
,
???
???
???
???
??
??
??
?
P
P
P
P
=
?
=
=
?
         (1)  
Similarly the C2E back-transliteration as 
,
arg max ( , , )P
? ?
? ? ? ??                      (2) 
To reduce the computational complexity, in eqn. 
(1), common practice is to replace the summation 
with maximization. 
The eqn. (1) and (2) formulate the DOM 
transliteration framework. ),,( ???P is the joint 
probability of ? , ?  and ? , whose definition 
depends on the transliteration model which will be 
discussed in the next two subsections. Unlike the 
phoneme-based approach, DOM does not need to 
explicitly model any phonetic information of either 
source or target language. Assuming sufficient 
training corpus, DOM transliteration framework is 
to capture the phonetic equivalents through 
orthographic mapping or transliteration 
pair ice >< , . By eliminating the potential 
imprecision introduced through a multiple-step 
phonetic mapping in the phoneme-based approach, 
DOM is expected to outperform. In contrast to 
phoneme-based approach, DOM is purely data-
driven, therefore can be extended across different 
language pairs easily. 
3.3 n-gram TM under DOM 
Given ? and ? , the joint probability of 
),,( ???P  is the probability of alignment ? , 
which can be formulated as follows: 
?
=
?><><=
=
K
k
k
k ceceP
PPP
1
1
1 ),|,(
)(*)|,(),,( ???????
 (3) 
In eqn. (3), the transliteration pair is used as the 
token to derive n-gram statistics, so we call the 
model as n-gram TM transliteration model.  
 
 
Figure 1. System structure of DOM 
The above block diagram illustrates typical 
system structure of DOM. The training of n-gram 
TM model is discussed in section 3.5. Given a 
language pair, the bidirectional transliterations can 
be achieved with the same n-gram TM and using 
the same decoder.  
3.4 DOM: n-gram TM vs. NCM 
Noisy-channel model (NCM) has been well 
studied in the phoneme-based approach. Let?s take 
E2C as an example to look into a bigram case to 
see what n-gram TM and NCM present to us under 
DOM. We have 
?
=
??
=
K
k
kkkk ccPceP
PPP
1
1 )|(*)|(
)(*)|,(),,( ???????
        (4) 
?
=
?><><?
=
K
k
kk ceceP
PPP
1
1 ),|,(
)(*)|,(),,( ???????
         (5) 
where eqn. (4) and (5) are the bigram version of 
NCM and n-gram TM under DOM, respectively. 
The formulation of eqn. (4) could be interpreted as 
a HMM that has Chinese units as its hidden states 
and English transliteration units as the observations 
(Rabiner, 1989). Indeed, NCM consists of two 
models; one is the channel model or transliteration 
model, ?
=
K
k
kk ceP
1
)|( , which tries to estimate the 
mapping probability between the two units; 
DOM Framework 
Name in 
Language A 
Bi-directional 
Decoder
Name in 
Language B 
 
n-gram 
TM
another is the source model or language model, 
?
=
?
K
k
kk ccP
1
1 )|( , which tries to estimate the 
generative probability of the Chinese name, given 
the sequence of Chinese transliteration units. 
Unlike NCM, n-gram TM model does not try to 
capture how source names can be mapped into 
target names, but rather how source and target 
names can be generated simultaneously. 
We can also study the two models from the 
contextual information usage viewpoint. One finds 
that eqn. (4) can be approximated by eqn. (5). 
)|(*)|(
),|(*),,|(
),|,(
1
11
1
?
??
?
?
><><=
><><
kkkk
kkkkk
kk
ccPceP
cecPceceP
ceceP
  (6) 
Eqn. (6) shows us that the context information 
1, ?>< kce and 1?ke  are absent in the channel 
model and source model of NCM, respectively. In 
this way, one could argue that n-gram TM model 
captures more context information than traditional 
NCM model. With adequate and sufficient training 
data, n-gram TM is expected to outperform NCM 
in the decoding.  
3.5 Transliteration Alignment Training 
For the n-gram TM model training, the bilingual 
name corpus needs to be aligned firstly at the 
transliteration unit level. The maximum likelihood 
approach, through EM algorithm (Dempster et al, 
1977) is employed to infer such an alignment. 
The aligning process is different from that of 
transliteration given in eqn. (1) or (2), here we 
have a fixed bilingual entries, ? and ? . The 
aligning process is just to find the alignment 
segmentation ? between the two strings that 
maximizes the joint probability: 
),,(maxarg ????
?
P=   (7) 
Kneser-Ney smoothing algorithm (Chen et al, 
1998) is applied to smooth the probability 
distribution. NCM model training is carried out in 
the similar way to n-gram TM. The difference 
between the two models lies in eqn (4) and (5). 
3.6 Decoding Issue 
The decoder searches for the most probabilistic 
path of transliteration pairs, given the word in 
source language, by resolving different 
combinations of alignments. Rather than Viterbi 
algorithm, we use stack decoder (Schwartz et al, 
1990) to get N-best results for further processing or 
as output for other applications. 
4 The Experiments  
4.1 Testing Environments 
We evaluate our method through several 
experiments for two language pairs: 
English/Chinese and English/Japanese.  
For English/Chinese language pair, we use a 
database from the bilingual dictionary ?Chinese 
Transliteration of Foreign Personal Names? 
(Xinhua, 1992). The database includes a collection 
of 37,694 unique English entries and their official 
Chinese transliteration. The listing includes 
personal names of English, French, and many other 
origins. The following results for this language pair 
are estimated by 13-fold cross validation for more 
accurate. We report two types of error rates: word 
error rate and character error rate. In word error 
rate, a word is considered correct only if an exact 
match happens between transliteration and the 
reference. The character error rate is the sum of 
deletion, insertion and substitution errors. Only the 
top choice in N-best results is used for character 
error rate reporting. 
For English/Japanese language pair, we use the 
same database as that in the literature (Bilac et al, 
2004) 2 . The database includes 7,021 Japanese 
words in katakana together with their English 
translation extracted from the EDICT dictionary3. 
714 tokens of these entries are withheld for 
evaluation. Only word error rate is reported for this 
language pair. 
4.2 Modeling 
The alignment is done fully automatically along 
with the n-gram TM training process. 
 
# close set bilingual entries (full data)  37,694 
# unique Chinese transliteration (close) 28,632 
# training entries for open test 34,777 
# test entries for open test 2,896 
# unique transliteration pairs  T 5,640 
# total transliteration pairs TW  119,364
# unique English units E 3,683 
# unique Chinese units C 374 
# bigram TM ),|,( 1?><>< kk ceceP  38,655 
# NCM Chinese bigram )|( 1?kk ccP  12,742 
Table 1. Modeling statistics (E-C) 
Table 1 reports statistics in the model training 
for English/Chinese pair, and table 2 is for 
English/Japanese pair. 
                                                     
2 We thank Mr. Slaven Bilac for letting us use his 
testing setup as a reference.  
3 ftp://ftp.cc.monash.edu.au/pub/nihongo/. 
# close set bilingual entries (full data)  7,021 
# training entries for open test 6,307 
# test entries for open test 714 
# unique transliteration pairs  T 2,173 
# total transliteration pairs TW  28,366
# unique English units E 1,216 
# unique Japanese units J 276 
# bigram TM 1( , | , )k kP e j e j ?< > < >  9,754 
Table 2. Modeling statistics (E-J) 
4.3 E2C Transliteration 
In this experiment, we conduct both open and 
closed tests for n-gram TM and NCM models 
under DOM paradigm. Results are reported in 
Table 3 and Table 4. 
 
 open 
(word) 
open 
(char) 
Closed 
(word) 
closed 
(char) 
1-gram 45.6% 21.1% 44.8% 20.4% 
2-gram 31.6% 13.6% 10.8% 4.7% 
3-gram 29.9% 10.8% 1.6% 0.8% 
Table 3. E2C error rates for n-gram TM tests.  
 open 
(word) 
open 
(char) 
closed 
(word) 
closed 
(char) 
1-gram 47.3% 23.9% 46.9% 22.1% 
2-gram 39.6% 20.0% 16.4% 10.9% 
3-gram 39.0% 18.8% 7.8% 1.9% 
Table 4. E2C error rates for NCM tests 
Not surprisingly, the result shows that n-gram 
TM, which benefits from the joint source-channel 
model coupling both source and target contextual 
information into the model, is superior to NCM in 
all the test cases. 
4.4 C2E Back-Transliteration 
The C2E back-transliteration is more 
challenging than E2C transliteration. Experiment 
results are reported in Table 5. As expected, C2E 
error rate is much higher than that of E2C.  
 
 open 
(word) 
Open 
(letter) 
closed 
(word) 
closed 
(letter) 
1 gram 82.3% 28.2% 81% 27.7% 
2 gram 63.8% 20.1% 40.4% 12.3% 
3 gram 62.1% 19.6% 14.7% 5.0% 
Table 5. C2E error rate for 3-gram TM tests 
Table 6 reports the N-best word error rates for 
both E2C and C2E which implies the potential of 
error reduction by using secondary knowledge 
source, such as table looking-up. The N-best error 
rates are also reduced greatly at 10-best level. 
 
 E2C 
open 
E2C 
closed 
C2E 
open 
C2E 
Closed 
1-best 29.9% 1.6% 62.1% 14.7% 
5-best 8.2% 0.94% 43.3% 5.2% 
10-best 5.4% 0.90% 24.6% 4.8% 
Table 6. N-best word error rates for 3-gram TM  
4.5 Discussions of DOM 
Due to lack of standard data sets, the DOM 
framework is unable to make a straightforward 
comparison with other approaches. Nevertheless, 
we list some reported studies on other databases of 
E2C tasks in Table 7 and those of C2E tasks in 
Table 8 for reference purpose. In Table 7, the 
reference data are extracted from Table 1 and 3 of 
(Virga et al, 2003), where only character and 
Pinyin error rates are reported. The first 4 setups 
by Virga et al all adopted the phoneme-based 
approach. In table 8, the reference data are 
extracted from Table 2 and Figure 4 of (Guo et al, 
2004), where word error rates are reported. 
 
System Trainin
g size 
Test 
size 
Pinyin 
errors 
Char 
errors 
Meng et 
al. 
2,233 1,541 52.5% N/A 
Small MT 2,233 1,541 50.8% 57.4% 
Big MT 3,625 250 49.1% 57.4% 
Huge MT 
(Big MT) 
309,019 3,122 42.5% N/A 
3-gram 
TM/DOM 
34,777 2,896 <10.8% 10.8% 
3-gram 
NCM/DOM 
34,777 2,896 <18.8% 18.8% 
Table 7. Performance Comparison of E2C 
Since we have obtained results in character 
already and the character to Pinyin mapping is one-
to-one in the 374 legitimate Chinese characters for 
transliteration in our implementation, we expect 
less Pinyin error than character error in Table 7.  
 
 Training  
size 
Test 
size 
1-best 10-best 
Guo et al 424,788 500 >82.0% >50.0% 
3-gram 
TM/DOM 
34,777 2,896 62.1% 24.6% 
Table 8. Performance Comparison of C2E 
For E2C, Table 7 shows that even with an 8 
times larger database than ours, Huge MT (Big 
MT) test case who reports the best performance 
still generates 3 times Pinyin error rate than ours. 
For C2E, Table 8 shows that even with only 9 
percent training set, our approach can still make 20 
percent absolute word error rate reduction.  Thus, 
although the experiment are done in different 
environments, to some extend, Table 7 and Table 8 
reveal that the n-gram TM/DOM outperforms other 
techniques for the case of English/Chinese 
transliteration/back-transliteration significantly.  
4.6 English/Japanese Transliteration 
In this experiment, we conduct both open and 
closed tests for n-gram TM on English/Japanese 
transliteration and back-transliteration. We use the 
same training and testing setups as those in (Bilac 
et al, 2004). 
Table 9 reports the results from three different 
transliteration mechanisms. Case 1 is the 3-gram 
TM under DOM; Case 2 is Case 1 integrated with 
a dictionary lookup validation process during 
decoding; Case 3 is extracted from (Bilac et al, 
2004). Similar to English/Chinese transliteration, 
one can find that J2E back-transliteration is more 
challenging than E2J transliteration in both open 
and closed cases. It is also found that word error 
rates are reduced greatly at 10-best level.  
(Bilac et al, 2004) proposed a hybrid-method of 
grapheme-based and phoneme-based for J2E back-
transliteration, where the whole EDICT dictionary, 
including the test set, is used to train a LM. A LM 
unit is a word itself. In this way, the dictionary is 
used as a lookup table in the decoding process to 
help identify a valid choice among candidates.  To 
establish comparison, we also integrate the 
dictionary lookup processing with the decoder, 
which is referred as Case 2 in Table 9. It is found 
that Case 2 presents a error reduction of 
43.8%=(14.6-8.2)/14.6% for word over to those 
reported in (Bilac et al, 2004). Furthermore, the n-
gram TM/DOM approach is rather straightforward 
in implementation where direct orthographical 
mapping could potentially handle Japanese 
transliteration of names of different language 
origins, while the issues with non-English terms 
are reported in (Bilac et al, 2004). 
 
The DOM framework shows us a great 
improvement in performance with n-gram TM 
being the most successful implementation. 
Nevertheless, NCM presents another successful 
implementation of DOM framework. The n-gram 
TM and NCM under direct orthographic mapping 
(DOM) paradigm simplify the process and reduce 
the chances of conversion errors. The experiments 
also show that even with much less training data, 
DOM are still much more superior performance 
than the state of art solutions. 
5 Conclusions 
In this paper, we propose a new framework, 
direct orthographical mapping (DOM) for machine 
transliteration and back-transliteration. Under the 
DOM framework, we further propose a joint 
source-channel transliteration model, also called n-
gram TM. We also implement the NCM model 
under DOM for reference. We use EM algorithm 
as an unsupervised training approach to train the n-
gram TM and NCM. The proposed methods are 
tested on an English-Chinese name corpus and 
English-Japanese katakana word pair extracted 
from EDICT dictionary. The data-driven and one-
step mapping strategies greatly reduce the 
development efforts of machine transliteration 
systems and improve accuracy significantly over 
earlier reported results. We also find the back-
transliteration is more challenging than the 
transliteration. 
The DOM framework demonstrates several 
unique edges over phoneme-based approach:  
   
English-Japanese 
Transliteration 
Japanese-English 
Back-transliteration 
 
open test closed 
test 
open test closed 
test 
1-best 40.5% 13.5% 62.8% 17.9% Case 1: 3-gram 
TM/DOM 10-best 13.2% 0.8% 17.9% 2.1% 
1-best 5.4% 0.7% 8.2% 1.2% Case 2: 3-gram 
TM/DOM with 
dictionary lookup 
10-best 0.7% 0% 1.7% 0.3% 
1-best N/A N/A 14.6% N/A Case 3: Bilac et al, 
2004 10-best N/A N/A 2.2% N/A 
Table 9. Experiment results of English-Japanese Transliteration
1) By skipping the intermediate phonemic 
interpretation, the transliteration error rate is 
reduced significantly; 
2) Transliteration models under DOM are data-
driven. Assuming sufficient training corpus, 
the modeling approach applies to different 
language pairs; 
3) DOM presents a paradigm shift for machine 
transliteration, that provides a platform for 
implementation of many other transliteration 
models; 
The n-gram TM is a successful implementation 
of DOM framework due to the following aspects: 
1) N-gram TM captures contextual information 
in both source and target languages jointly; 
unlike the phoneme-based approach, the 
modeling of transformation rules and target 
language is tightly coupled in n-gram TM 
model. 
2) As n-gram TM uses transliteration pair as 
modeling unit, the same model applies to bi-
directional transliteration; 
3) The bilingual aligning process is integrated 
into the decoding process in n-gram TM, 
which allows us to achieve a joint 
optimization of alignment and transliteration 
automatically. Hence manual pre-alignment 
is unnecessary. 
Named entities are sometimes translated in 
combination of transliteration and meanings. As 
the proposed framework allows direct 
orthographical mapping, we are extending our 
approach to handle such name translation. We also 
extending our method to handle the disorder and 
fertility issues in named entity translation. 
References  
Chun-Jen Lee and Jason S. Chang,  2003. Acquisition of 
English-Chinese Transliteration Word Pairs from 
Parallel-Aligned Texts using a Statistical Machine 
Translation Model, Proceedings of HLT-NAACL 
Workshop: Building and Using parallel Texts Data 
Driven Machine Translation and Beyond, 2003, 
Edmonton, pp. 96-103 
Dempster, A.P., N.M. Laird and D.B.Rubin, 1977. 
Maximum likelihood from incomplete data via the 
EM algorithm, J. Roy. Stat. Soc., Ser. B. Vol. 39 
Eric Brill, Garry Kacmarcik and Chris Brockrtt, 2001. 
Automatically Harvesting Katakana-English Term 
Pairs from Search Engine Query Logs. Proceeding of 
NLPRS?01 
Helen M. Meng, Wai-Kit Lo, Berlin Chen and Karen 
Tang. 2001. Generate Phonetic Cognates to Handle 
Name Entities in English-Chinese cross-language 
spoken document retrieval, Proceedings of ASRU 
2001 
Jong-Hoon Oh and Key-Sun Choi, 2002. An English-
Korean Transliteration Model Using Pronunciation 
and Contextual Rules, Proceedings of COLING 2000 
Kang B.J. and Key-Sun Choi, 2000. Automatic 
Transliteration and Back-transliteration by Decision 
Tree Learning, Proceedings of the 2nd International 
Conference on Language Resources and Evaluation, 
Athens, Greece 
K. Knight and J. Graehl. 1998. Machine Transliteration, 
Computational Linguistics, Vol 24, No. 4 
Paola Virga, Sanjeev Khudanpur, 2003. Transliteration 
of Proper Names in Cross-lingual Information 
Retrieval. Proceedings of ACL 2003 workshop 
MLNER, 2003 
Rabiner, Lawrence R. 1989, A tutorial on hidden 
Markov models and selected applications in speech 
recognition, Proceedings of the IEEE 77(2) 
Schwartz, R. and Chow Y. L., 1990. The N-best 
algorithm: An efficient and Exact procedure for 
finding the N most likely sentence hypothesis, 
Proceedings of ICASSP 1990, Albuquerque, pp. 81-
84 
Slaven Bilac and Hozumi Tanaka, 2004. Improving 
Back-Transliteration by Combining Information 
Sources. Proceedings of IJCNLP-04, Haian, pp. 542-
547  
Stephen Wan and Cornelia Maria Verspoor, 1998. 
Automatic English-Chinese name transliteration for 
development of multilingual resources. Proceedings 
of COLING-ACL?98  
Stanley F. Chen and Joshua Goodman, 1998. An 
Empirical Study of Smoothing Techniques for 
Language Modeling, TR-10-98, Computer Science 
Group, Harvard Universituy. 1998 
Sung Young Jung, Sung Lim Hong and Eunok Paek, 
2000. An English to Korean Transliteration Model of 
Extended Markov Window, Proceedings of COLING 
2000 
The Onomastica Consortium, 1995. The Onomastica 
interlanguage pronunciation lexicon, Proceedings of 
EuroSpeech, Madrid, Spain, pp829-832 
Wei Gao, Kam-Fai Wong and Wai Lam, 2004. 
Phoneme-based Transliteration of Foreign Names 
for OOV Problems. Proceedings of IJCLNP-04, 
Hainan, pp 374-381  
Xinhua News Agency, 1992. Chinese transliteration of 
foreign personal names, The Commercial Press 
Yaser Al-Onaizan and Kevin Knight, 2002. Translating 
named entities using monolingual and bilingual 
resources. Proceedings of the 40th ACL, 
Philadelphia, 2002, pp. 400-408 
Yuqing Guo and  Haifeng Wang, 2004. Chinese-to-
English Backward Machine Transliteration. 
Companion Volume to the Proceedings of IJCNLP-
04, Hainan, pp 17-20 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 105?112
Manchester, August 2008
Regenerating Hypotheses for Statistical Machine Translation 
Boxing Chen, Min Zhang, Aiti Aw and Haizhou Li 
Department of Human Language Technology 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, 119613, Singapore 
{bxchen, mzhang, aaiti, hli}@i2r.a-star.edu.sg 
 Abstract 
This paper studies three techniques that 
improve the quality of N-best hypotheses 
through additional regeneration process. 
Unlike the multi-system consensus ap-
proach where multiple translation sys-
tems are used, our improvement is 
achieved through the expansion of the N-
best hypotheses from a single system. We 
explore three different methods to im-
plement the regeneration process: re-
decoding, n-gram expansion, and confu-
sion network-based regeneration. Ex-
periments on Chinese-to-English NIST 
and IWSLT tasks show that all three 
methods obtain consistent improvements. 
Moreover, the combination of the three 
strategies achieves further improvements 
and outperforms the baseline by 0.81 
BLEU-score on IWSLT?06, 0.57 on 
NIST?03, 0.61 on NIST?05 test set re-
spectively. 
1 Introduction 
State-of-the-art Statistical Machine Translation 
(SMT) systems usually adopt a two-pass search 
strategy (Och, 2003; Koehn, et al, 2003) as 
shown in Figure 1. In the first pass, a decoding 
algorithm is applied to generate an N-best list of 
translation hypotheses, while in the second pass, 
the final translation is selected by rescoring and 
re-ranking the N-best translations through addi-
tional feature functions. The fundamental as-
sumption behind using a second pass is that the 
generated N-best list may contain better transla-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
tions than the best choice found by the decoder. 
Therefore, the performance of a two-pass SMT 
system can be improved from two aspects, i.e. 
scoring models and the quality of the N-best hy-
potheses. 
Rescoring pass improves the performance of 
machine translation by enhancing the scoring 
models with more global sophisticated and dis-
criminative feature functions. The idea for apply-
ing two passes instead of one is that some global 
feature functions cannot be easily decomposed 
into local scores and computed during decoding. 
Furthermore, rescoring allows some feature func-
tions, such as word and n-gram posterior prob-
abilities, to be estimated on the N-best list (Uef-
fing, 2003; Chen et al, 2005; Zens and Ney, 
2006). 
In this two-pass method, translation perform-
ance hinges on the N-best hypotheses that are 
generated in the first pass (since rescoring occurs 
on these), so adding the translation candidates 
generated by other MT systems to these hypothe-
ses could potentially improve the performance. 
This technique is called system combination 
(Bangalore et al, 2001; Matusov et al, 2006; 
Sim et al, 2007; Rosti et al, 2007a; Rosti et al, 
2007b). 
We have instead chosen to regenerate new hy-
potheses from the original N-best list, a tech-
nique which we call regeneration. Regeneration 
is an intermediate pass between decoding and 
rescoring as depicted in Figure 2. Given the 
original N-best list (N-best1) generated by the 
decoder, this regeneration pass creates new trans-
lation hypotheses from this list to form another 
N-best list (N-best2). These two N-best lists are 
then combined and given to the rescoring pass to 
derive the best translation. 
We implement three methods to regenerate 
new hypotheses: re-decoding, n-gram expansion 
and confusion network. Re-decoding (Rosti et al, 
2007a) based regeneration re-decodes the source 
sentence using original LM as well as new trans-
105
lation and reordering models that are trained on 
the source-to-target N-best translations generated 
in the first pass. N-gram expansion (Chen et al, 
2007) regenerates more hypotheses by continu-
ously expanding the partial hypotheses through 
an n-gram language model trained on the original 
N-best translations. And confusion network gen-
erates new hypotheses based on confusion net-
work decoding (Matusov et al, 2006), where the 
confusion network is built on the original N-best 
translations. 
Confusion network and re-decoding have been 
well studied in the combination of different MT 
systems (Bangalore et al, 2001; Matusov et al, 
2006; Sim et al, 2007; Rosti et al, 2007a; Rosti 
et al, 2007b). Researchers have used confusion 
network to compute consensus translations from 
the outputs of different MT systems and improve 
the performance over each single systems. (Rosti 
et al, 2007a) also used re-decoding to do system 
combination by extracting sentence-specific 
phrase translation tables from the outputs of dif-
ferent MT systems and running a phrase-based 
decoding with this new translation table. Finally, 
N-gram expansion method (Chen et al, 2007) 
collects sub-strings occurring in the N-best list to 
produce alternative translations. 
This work demonstrates that a state-of-the-art 
MT system can be further improved by means of 
regeneration which expands its own N-best 
translations other than taking the translation can-
didates from the other MT systems. 
 
Figure 1: Structure of a typical two-pass ma-
chine translation system. N-best translations are 
generated by the decoder and the 1-best transla-
tion is returned after rescored with additional 
feature functions. 
 
Figure 2: Structure of a three-pass machine 
translation system with the new regeneration 
pass. The original N-best translations list (N-
best1) is expanded to generate a new N-best 
translations list (N-best2) before the rescoring 
pass. 
2 SMT Process 
Phrase-based statistical machine translation sys-
tems are usually modeled through a log-linear 
framework (Och and Ney, 2002). By introducing 
the hidden word alignment variable a  (Brown et 
al., 1993), the optimal translation can be 
searched for based on the following criterion: 
*
1,
arg max( ( , , ))
M
m mme a
e h?== e f a?             (1) 
where  is a string of phrases in the target lan-
guage, 
e
f
f a
 is the source language string of 
phrases,  h e  are feature functions, 
weights 
( , , )m
m? are typically optimized to maximize 
the scoring function (Och, 2003). 
Our MT baseline system is based on Moses 
decoder (Koehn et al, 2007) with word align-
ment obtained from GIZA++ (Och et al, 2003). 
The translation model (TM), lexicalized word 
reordering model (RM) are trained using the 
tools provided in the open source Moses package. 
Language model (LM) is trained with SRILM 
toolkit (Stolcke, 2002) with modified Kneser-
Ney smoothing method (Chen and Goodman, 
1998). 
3 Regeneration Methods 
Given the original N-best translations, regenera-
tion pass is to generate M new target translations 
which are not seen in the original N-best choices. 
3.1 Regeneration with Re-decoding 
One way of regeneration is by running the de-
coding again to obtain new hypotheses through a 
re-decoding process (Rosti et al, 2007a). In this 
work, the same decoder (Moses) is used to pro-
duce the new M-best translations using a new 
translation model and reordering model trained 
over the word-aligned source input and original 
N-best target hypotheses. Although the target-to-
source phrase alignments are available in the 
original N-best hypotheses, to enlarge the differ-
ence between the new M-best translations and 
the original N-best translations, we re-align the 
words using GIZA++. 
Weights of the decoder are re-optimized by 
the tool in the Moses package over the develop-
ment set. The process of such a re-decoding is 
summarized as follows: 
106
1. Run GIZA++ to align the words between the 
source input and target N-best translations; 
2. Train translation and reordering model; 
3. Optimize the weights of the decoder with 
the new models; 
4. Decode the source input by using new mod-
els and new weights to generate N+M dis-
tinct translations (?distinct? here refers to 
the target language string only, not consider-
ing the phrase segmentation, etc.); 
5. Output M-best translations which are not 
seen in the original N-best translations. 
Re-decoding on test set follows the same steps, 
but without the tuning step, step 3. 
3.2 Regeneration with N-gram Expansion 
N-gram expansion (Chen et al, 2007) combines 
the sub-strings occurred in the original N-best 
translations to generate new hypotheses. Firstly, 
all n-grams from the original N-best translations 
are collected. Then the partial hypotheses are 
continuously expanded by appending a word 
through the n-grams collected in the first step. 
We explain this method in more detail using the 
following example. 
Suppose we have four original hypotheses 
shown in Figure 3. Firstly, we collect all the 3-
grams from the original hypotheses. The first n-
grams of all original entries in the N-best list are 
set as the initial partial hypotheses. They are: it's 
5 minutes, it is 5, it?s about 5 and i walk 5. Then 
the expansion of a partial hypothesis starts by 
computing the set of n-grams matching its last n-
1 words. As shown in Figure 4, the n-gram 5 
minutes on matches the last two words of the 
partial hypothesis it?s about 5 minutes. So the 
hypothesis is expanded to it?s about 5 minutes on. 
The expansion continues until the partial hy-
pothesis ends with a special end-of-sentence 
symbol that occurs at the end of all N-best strings. 
Figure 5 shows some new hypotheses that are 
generated from the example in Figure 3. This is 
an example excerpted from our development data. 
One reference is also given in Figure 5; the first 
new generated hypothesis is equal to this refer-
ence.  But unfortunately, there is no such hy-
pothesis in the original N-best translations. 
During the new hypotheses generation, the 
translation outputs of a given source sentence are 
computed through a beam-search algorithm with 
a log-linear combination of the feature functions. 
In addition to n-gram frequency and n-gram pos-
terior probability which have been used in (Chen 
et al, 2007), we also used language model, di-
rect/inverse IBM model 1, and word penalty in 
this work. The size of the beam is set to N+M, to 
ensure more than M new hypotheses are gener-
ated. 
 
 
Original 
hypotheses 
1. it's 5 minutes on foot . 
2. it is 5 minutes on foot . 
3. it?s about 5 minutes? to walk . 
4. i walk 5 minutes . 
 
n-grams 
it's 5 minutes, 5 minutes on, ??
on foot ., about 5 minutes ?? 
5 minutes . 
 
Figure 3: Example of original hypotheses and 3-
grams collected from them. 
 
partial hyp. it?s about 5 minutes  
n-gram +                    5    minutes    on
new partial hyp. it?s about 5 minutes on
 
Figure 4: Expanding a partial hypothesis via a 
matching n-gram. 
 
 
New 
hypotheses
it?s about 5 minutes on foot . 
it's 5 minutes . 
i walk 5 minutes on foot . 
?? 
Reference it's about five minutes on foot . 
 
Figure 5: New generated hypotheses through n-
gram expansion and one reference. 
3.3 Regeneration with Confusion Network 
Confusion network based regeneration builds a 
confusion network over the original N-best hy-
potheses, and then extracts M-best hypotheses 
from it. The word order in the N-best translations 
could be very different, so we need to choose a 
hypothesis with the ?most correct? word order as 
the confusion network skeleton (alignment refer-
ence), then align and reorder other hypotheses in 
this word order. 
Some previous work compute the consensus 
translation under MT system combination, which 
differ from ours in the way of choosing the skele-
ton and aligning the words. Matusov et al (2006) 
let every hypothesis play the role of the skeleton 
once and used GIZA++ to get word alignment. 
Bangalore et al (2001), Sim et al (2007), Rosti 
et al (2007a), and Rosti et al (2007b) chose the 
hypothesis that best agrees with other hypotheses 
on average as the skeleton. Bangalore et al 
(2001) used a WER based alignment and Sim et 
al. (2007), Rosti et al (2007a), and Rosti et al 
(2007b) used minimum Translation Error Rate 
107
(TER) based alignment to build the confusion 
network. 
1. it?s 5 minutes on foot .  
Original 
hypotheses
2. it is 5 minutes on foot . 
Choosing alignment reference: Since the N-
best translations are ranked, choosing the first 
best hypothesis as the skeleton is straightforward 
in our work. 
3. it?s about 5 minutes? to walk . 
4. i walk 5 minutes . ?  it?s 5 minutes on foot . 
Alignments it 5 minutes on foot . is 
Aligning words: As a confusion network can be 
easily built from a one-to-one alignment, we de-
velop our algorithm based on the one-to-one as-
sumption and use competitive linking algorithm 
(Melamed, 2000) for our word alignment. Firstly, 
an association score is computed for every possi-
ble word pair from the skeleton and sentence to 
be aligned. Then a greedy algorithm is applied to 
select the best word-alignment. In this paper, we 
use a linear combination of multiple association 
scores, as suggested in (Kraif and Chen, 2004). 
As the two sentences to be aligned are in the 
same language, the association scores are com-
puted on the following four clues. They are cog-
nate (S
aboutit?s 5 minutes? to walk .
1), word class (S2), synonyms (S3), and 
position difference (S4). The four scores are line-
arly combined with empirically determined 
weights as shown is Equation 2. 
4
1
( , )j i k k
k
S f e S?
=
= ??                  (2) 
Reordering words: After word alignment, the 
words in all other hypotheses are reordered to 
match the word order of the skeleton. The 
aligned words are reordered according to their 
alignment indices. The unaligned words are reor-
dered in two strategies: moved with its previous 
word or next word. In this work, additional ex-
periments suggested that moving the unaligned 
word with its previous word achieve better per-
formance. In the case that the first word is un-
aligned, it will be moved with its next word. 
Each word is assigned a score based on a simple 
voting scheme. Figure 6 shows an example of 
creating a confusion network. 
Extracting M-best translations: New transla-
tions are extracted from the confusion network. 
We again use beam-search algorithm to derive 
new hypotheses.  The same feature functions 
proposed in Section 3.2 are used to score the par-
tial hypotheses. Moreover, we also use position 
based word probability (i.e. in Figure 6, the 
words in position 5, ?on? scored a probability of 
0.5, and ?? ? scored a probability of 0.25) as a 
feature function. Figure 6 shows some examples 
of new hypotheses generated through confusion 
network regeneration. 
 
 
i 5 minutes ?  walk .   ?  it?s 5 minutes on foot . 
Confusion 
network 
it is 5 minutes on foot .
it?s about 5 minutes? to walk .?i  5 minutes ?  walk . 
 1. it's about five minutes on foot . 
New 2. it about five minutes on foot . 
hypotheses 3. it's about five minutes on walk . 
4. i about 5 minutes to work . 
 
Figure 6: Example of creating a confusion net-
work from the word alignments, and new hy-
potheses generated through the confusion net-
work. The sentence in bold is the alignment ref-
erence. 
4 Rescoring model 
Since the final N+M-best hypotheses are pro-
duced either from different methods or same de-
coder with different models, local feature func-
tions of each hypothesis are not directly compa-
rable, and thus inadequate for rescoring. We 
hence exploit rich global feature functions in the 
rescoring models to compensate the loss of local 
feature functions. We apply the following 10 fea-
ture functions and optimize the weight of each 
feature function using the tool in Moses package. 
? direct and inverse IBM model 1 and 3 
? association score, i.e. hyper-geometric distri-
bution probabilities and mutual information 
? lexicalized word/block reordering rules 
(Chen et al, 2006) 
? 6-gram target LM 
? 8-gram target word-class based LM, word-
classes are clustered by GIZA++ 
? length ratio between source and target sen-
tence 
? question feature (Chen et al, 2005) 
? linear sum of n-grams relative frequencies 
within N-best translations (Chen et al, 2005) 
? n-gram posterior probabilities within the N-
best translations (Zens and Ney, 2006) 
? sentence length posterior probabilities (Zens 
and Ney, 2006) 
108
5 Experiments data Chinese English 
5.1 Tasks 
We carried out two sets of experiments on two 
different datasets. One is in spoken language 
domain while the other is on newswire corpus. 
Both experiments are on Chinese-to-English 
translation. 
Experiments on spoken language domain were 
carried out on the Basic Traveling Expression 
Corpus (BTEC) (Takezawa et al, 2002) Chi-
nese- to-English data augmented with HIT-
corpus1. BTEC is a multilingual speech corpus 
which contains sentences spoken by tourists. 40K 
sentence-pairs are used in our experiment. HIT-
corpus is a balanced corpus and has 500K sen-
tence-pairs in total. We selected 360K sentence-
pairs that are more similar to BTEC data accord-
ing to its sub-topic. Additionally, the English 
sentences of Tanaka corpus2 were also used to 
train our LM. We ran experiments on an 
IWSLT 3  challenge track which uses IWSLT-
20064 DEV clean text set as development set and 
IWSLT-2006 TEST clean text as test set. Table 1 
summarizes the statistics of the training, dev and 
test data for IWSLT task. 
Experiments on newswire domain were car-
ried out on the FBIS5 corpus. We used NIST6 
2002 MT evaluation test set as our development 
set, and the NIST 2003, 2005 test sets as our test 
sets. Table 2 summarizes the statistics of the 
training, dev and test data for NIST task. 
 
data Chinese English
Sentences 406,122 
Words 4,443K 4,591K
 
Train 
Vocabulary 69,989 61,087 
Sentences 489 489?7Dev.  
Words 5,896 45,449 
Sentences 500 500?7Test 
Words 6,296 51,227 
Sentences - 155K Additional 
target data Words - 1.7M 
 
Table 1: Statistics of training, development and 
test data for IWSLT task. 
                                                 
1 http://mitlab.hit.edu.cn/
2 http://www.csse.monash.edu.au/~jwb/tanakacorpus.html 
3 International Workshop for Spoken Language Trans-
lation 
4 http:// www.slc.atr.jp/IWSLT2006/ 
5 LDC2003E14 
6 http://www.nist.gov/speech/tests/mt/ 
Sentences 238,761  
Train Words 7.0M 8.9M 
Vocabulary 56,223 63,941 
Sentences 878 878?4 NIST 02 
(dev) Words 23,248 108,616
Sentences 919 919?4 NIST 03 
(test) Words 25,820 116,547
Sentences 1,082 1,082?4NIST 05 
(test) Words 30,544 141,915
Sentences - 2.2M Additional
target data Words - 61.5M 
 
Table 2: Statistics of training, development and 
test data for NIST task. 
 
Dev set Test set   
System #hypo BLEU NIST BLEU NIST
1-best - 29.98 7.468 29.10 7.103
RESC1 1,200 31.60 7.657 30.42 7.165
RD 1,200 32.46 7.664 30.95 7.175
NE 1,200 32.58 7.660 31.02 7.178
CN 1,200 32.33 7.671 30.82 7.200
RESC2 2,000 31.72 7.659 30.55 7.166
32.98 7.673 31.36 7.202COMB 2,000
 
Table 3: Translation performances (BLEU% and 
NIST scores) of IWSLT task: decoder (1-best), 
rescoring on original 1,200 N-best (RESC1) and 
2,000 N-best hypotheses (RESC2), re-decoding 
(RD), n-gram expansion (NE), confusion net-
work (CN) and combination of all hypotheses 
(COMB). 
5.2 Results 
We set N = 800 and M = 400 for IWSLT task, i.e. 
800 distinct translations for each source input are 
extracted from the decoder and used for regen-
eration; and 400 new hypotheses are generated 
for each regeneration system: re-decoding (RD), 
n-gram expansion (NE) and confusion network 
(CN). System COMB combines the original N-
best and the three regenerated M-best hypotheses 
lists (totally, 2,000 distinct hypotheses: 800 + 
3?400). Then each system computes the 1-best 
translation through rescoring and re-ranking its 
hypotheses list. For comparison purpose, the per-
formance of rescoring on two sets of original N-
best translations are also computed and they are 
applied based on 1,200 (RESC1) and 2,000 
(RESC2) distinct hypotheses extracted from the 
decoder.  For NIST task, we set N = 1,600, and 
M = 800, thus, RESC2 and COMB compute 1-
109
NIST?02 (dev) NIST?03 (test) NIST?05 (test)  
System 
 
#hypo BLEU NIST BLEU NIST BLEU NIST 
1-best 1 27.67 8.498 26.68 8.271 24.82 7.856 
RESC1 2,400 28.13 8.519 27.09 8.312 25.29 7.868 
RD 2,400 28.46 8.518 27.34 8.320 25.54 7.897 
NE 2,400 28.52 8.539 27.47 8.329 25.65 7.907 
CN 2,400 28.40 8.545 27.30 8.332 25.54 7.913 
RESC2 4,000 28.27 8.522 27.21 8.320 25.43 7.875 
COMB 4,000 28.92 8.602 27.78 8.401 26.04 7.994 
 
Table 4: Translation performances (BLEU% and NIST scores) of NIST task: decoder (1-best), rescoring 
on original 2,400 N-best (RESC1) and 4,000 N-best hypotheses (RESC2), re-decoding (RD), n-gram 
expansion (NE), confusion network (CN) and combination of all hypotheses (COMB). 
 
Reference No tax is needed for this item . Thank you . 
RESC2 you don't have to do not need to pay duty on this . thank you . 
1 
COMB (RD) not need to pay duty on this . thank you . 
Reference Certainly . The fitting room is over there . Please come with me . 
RESC2 the fitting room is over there . can you come with me . 
2 
COMB (NE) yes , you can . the fitting room is over there . please come with me . 
Reference OK . I will bring it to you in five minutes . 
RESC2 a good five minutes , we will give you . 
3 
COMB (CN) ok . after five minutes , i will give it to you . 
 
Table 5: Translations output by system RESC2 and COMB on IWSLT task (case-insensitive). 
best from 4,000 (1,600 + 3 800) distinct hy-
potheses. 
?
Our evaluation metrics are BLEU (Papineni et 
al., 2002) and NIST, which are to perform case-
insensitive matching of n-grams up to n = 4. The 
translation performance of IWSLT task and 
NIST task is reported in Tables 3 and 4 respec-
tively. The row ?1-best? reports the scores of the 
translations produced by the decoder. The col-
umn ?#hypo? means the size of the N-best hy-
potheses involved in rescoring. Note that on top 
of the same global feature functions as men-
tioned in Section 4, the local feature functions 
used during decoding were also involved in res-
coring RESC1 and RESC2. 
First of all, we note that both BLEU and NIST 
scores of the first decoding step were improved 
through rescoring. If rescoring was applied after 
regeneration on the N+M best lists, additional 
improvements were gained for all the develop-
ment and test sets on all three regeneration sys-
tems. Absolute improvement on BLEU score of 
0.4-0.6 on IWSLT?06 test set and 0.25-0.35 on 
NIST test sets were obtained when compared 
with system RESC1. Comparing the performance 
of three regeneration methods, we can see that 
re-decoding and confusion network based 
method achieved very similar improvement; 
while n-gram expansion based regeneration ob-
tained slightly better improvement than the other 
two methods. Combining all regenerated hy-
potheses with the original hypotheses further in-
creased the scores on both tasks. Compared with 
RESC2, system COMB obtained absolute im-
provement of 0.81 (31.36 ? 30.55) BLEU score 
on IWSLT?06 test set, 0.57 (27.28 ? 27.21) 
BLEU score on NIST?03 and 0.61 (26.04 ? 25.43) 
BLEU score on NIST?05 respectively. 
We further illustrate the effectiveness of the 
regeneration mechanism using some translation 
examples obtained from system RESC2 and 
COMB as shown in Table 5. 
6 Discussion 
To better interpret the performance improvement; 
first let us check if the regeneration pass has pro-
duced better hypotheses. We computed the oracle 
scores on all four 1,200-best lists in IWSLT task. 
The oracle chooses the translation with the low-
est word error rate (WER) with respect to the 
references in all cases. The results are reported in 
Table 6.  It is worth noticing that the first 800-
best (original N-best) hypotheses are the same in 
110
all four lists, with differences found only in the 
remaining 400 hypotheses (M-best). The consis-
tent improvement of oracle scores shows that the 
tra
theses contain better ones 
than the original ones. 
 
nslation candidates have been really improved. 
From another viewpoint, Table 7 shows the 
number of translations generated by each method 
in the final translation output (translations of 
COMB). After re-ranking N+3M entries, it is 
observed that more than 25% (e.g. for IWSLT?06 
test set, (50+74+39)/500=32.6%; NIST?03 test 
set, (77+85+68)/919=25.1%; NIST?05 test set, 
(95+110+82)/1082=26.5%) of best scored out-
puts were generated by the regeneration pass, 
showing that new generated translations are quite 
often the rescoring winner. This also proved that 
the new-generated hypo
List BLEU NIST WER PER
M  oses 46.10 8.765 36.29 30.94
RD 46.91 8.764 35.29 30.62
NE 46.95 8.811 36.05 30.72
Dev. 
CN 46.85 8.769 36.17 30.83
M  oses 45.09 8.403 37.07 32.04
RD 45.67 8.418 36.50 31.82
NE 45.82 8.481 36.44 31.70
Test 
CN 45.68 8.471 36.55 31.81
 
Table 6: Oracle scores (BLEU%, NIST, WER% 
and PER%) on IWSLT task 1,200-best lists of 
four systems: decoder (Moses), re-decoding 
(RD), n-gram expansion (NE) and confusion 
etwork (CN). 
 
n
# sentence   
Set Tot. Orig. RD NE CN
Dev 489 325 52 76 36IWSLT 
Test 500 337 50 74 39
NIST 02 878 613 92 100 73
NIST 03 919 689 77 85 68
NIST 
NIST 05 1082 795 95 110 82
 
Table 7: Number of translations generated by 
each method in the final translation output of 
system COMB: decoder (Orig.), re-decoding 
(RD), n-gram expansion (NE) and confusion 
network (CN). ?Tot.? is the size of the dev/test 
set. 
ities of words occur in the N-
best translations. 
n, and confusion 
ne
ree methods further im-
pr
the N-
best list through hypotheses regeneration. 
S. 
, pages 351?354. Madonna di 
P. 
ation. Com-
B.
Federico. 2005. The ITC-irst SMT System for 
 
Then, let us consider each single regeneration 
method to understand why regeneration can pro-
duce better hypotheses. Re-decoding may intro-
duce new and better phrase-pairs which are ex-
tracted from the N-best hypotheses to the transla-
tion model thus generate better hypotheses. N-
gram expansion can (almost) fully exploit the 
search space of target strings, which can be gen-
erated by an n-gram LM. As a result, it can pro-
duce alternative translations which contain word 
re-orderings and phrase structures not considered 
by the search algorithm of the decoder (Chen, et 
al., 2007). Confusion network based regeneration 
reinforces the word choice by considering the 
posterior probabil
7 Conclusions 
In this paper, we proposed a novel three-pass 
SMT framework against the typical two-pass 
system. This framework enhanced the quality of 
the translation candidates generated by our pro-
posed regeneration pass and improved the final 
translation performance. Three regeneration 
methods were introduced, namely, re-decoding, 
word-based n-gram expansio
twork based regeneration.  
Experiments were based on the state-of-the-art 
phrase-based decoder and carried out on the 
IWSLT and NIST Chinese-to-English task. We 
showed that all three methods improved the per-
formance with the n-gram expansion method 
achieving the greatest improvement. Moreover, 
the combination of the th
oves the performance. 
We conclude that translation performance can 
be improved by increasing the potential of trans-
lation candidates to contain better translations. 
We have presented an alternative solution to 
ameliorate the quality of translation candidates in 
a way that differs from system combination 
which takes translations from other MT systems. 
We demonstrated that the translation perform-
ance could be self-boosted by expanding 
References 
Bangalore, G. Bordel, and G. Riccardi. 2001. 
Computing consensus translation from multiple 
machine translation systems. In Proceeding of 
IEEE workshop on Automatic Speech Recognition 
and Understanding
Campiglio, Italy. 
F. Brown, V. J. Della Pietra, S. A. Della Pietra & R. 
L. Mercer. 1993. The Mathematics of Statistical 
Machine Translation: Parameter Estim
putational Linguistics, 19(2) 263-312. 
 Chen, R. Cattoni, N. Bertoldi, M. Cettolo and M. 
111
IWSLT-2005. In Proceeding of IWSLT-2005, 
pp.98-104, Pittsburgh, USA, October. 
B. Chen, M. Cettolo and M. Federico. 2006. Reorder-
ing Rules for Phrase-based Statistical Machine 
Translation. In Proceeding of IWSLT-2006, Kyoto, 
Japan. 
B. Chen, M. Federico and M. Cettolo. 2007. Better N-
best Translations through Generative n-gram Lan-
guage Models. In Proceeding of MT Summit XI. 
Copenhagen, Denmark.  
S. F. Chen and J. T. Goodman. 1998. An Empirical 
Study of Smoothing Techniques for Language 
Modeling. Technical Report TR-10-98, Computer 
Science Group, Harvard University. 
P. Koehn, F. J. Och and D. Marcu. 2003. Statistical 
Phrase-based Translation. In Proceedings of 
HLT/NAACL, pp 127-133, Edmonton, Canada. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. 
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin 
and E. Herbst. 2007. Moses: Open Source Toolkit 
for Statistical Machine Translation. In Proceeding 
of ACL-2007, pp. 177-180, Prague, Czech Republic. 
O. Kraif, B. Chen. 2004. Combining clues for lexical 
level aligning using the Null hypothesis approach. 
In Proceeding of COLING-2004, Geneva, pp. 
1261-1264.  
E. Matusov, N. Ueffing, and H. Ney. 2006. Comput-
ing consensus translation from multiple machine 
translation systems using enhanced hypotheses 
alignment. In Proceeding of EACL-2006, Trento, 
Italy.  
I. D. Melamed. 2000. Models of translational equiva-
lence among words. Computational Linguistics, 
26(2), pp. 221-249. 
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of ACL-
2003. Sapporo, Japan. 
F. J. Och and H. Ney. 2003. A Systematic Compari-
son of Various Statistical Alignment Models. 
Computational Linguistics, 29(1), pp. 19-51. 
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceeding of ACL-2002. 
A. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas, R. 
Schwartz and B. Dorr. 2007a. Combining Outputs 
from Multiple Machine Translation Systems.  In 
Proceeding of NAACL-HLT-2007, pp. 228-235. 
Rochester, NY. 
A. Rosti, S. Matsoukas and R. Schwartz. 2007b. Im-
proved Word-Level System Combination for Ma-
chine Translation. In Proceeding of ACL-2007, 
Prague. 
K. C. Sim, W. J. Byrne, M. J.F. Gales, H. Sahbi, and 
P. C. Woodland. 2007. Consensus network decod-
ing for statistical machine translation system com-
bination. In Proceeding of  ICASSP-2007. 
A. Stolcke. 2002. SRILM - an extensible language 
modelling toolkit. In Proceeding of ICSLP-2002. 
901-904. 
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, 
and S. Yamamoto. 2002. Toward a broad-coverage 
bilingual corpus for speech translation of travel 
conversations in the real world. In Proceeding of 
LREC-2002, Las Palmas de Gran Canaria, Spain. 
R. Zens and H. Ney. 2006. N-gram Posterior Prob-
abilities for Statistical Machine Translation. In 
Proceeding of HLT-NAACL Workshop on SMT, pp. 
72-77, NY. 
 
112
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1009?1016
Manchester, August 2008
Linguistically Annotated BTG for Statistical Machine Translation
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li
Human Language Technology
Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore 119613
{dyxiong, mzhang, aaiti}@i2r.a-star.edu.sg
Abstract
Bracketing Transduction Grammar (BTG)
is a natural choice for effective integration
of desired linguistic knowledge into sta-
tistical machine translation (SMT). In this
paper, we propose a Linguistically Anno-
tated BTG (LABTG) for SMT. It conveys
linguistic knowledge of source-side syn-
tax structures to BTG hierarchical struc-
tures through linguistic annotation. From
the linguistically annotated data, we learn
annotated BTG rules and train linguisti-
cally motivated phrase translation model
and reordering model. We also present an
annotation algorithm that captures syntac-
tic information for BTG nodes. The ex-
periments show that the LABTG approach
significantly outperforms a baseline BTG-
based system and a state-of-the-art phrase-
based system on the NISTMT-05 Chinese-
to-English translation task. Moreover, we
empirically demonstrate that the proposed
method achieves better translation selec-
tion and phrase reordering.
1 Introduction
Formal grammar used in statistical machine trans-
lation (SMT), such as Bracketing Transduction
Grammar (BTG) proposed by (Wu, 1997) and the
synchronous CFG presented by (Chiang, 2005),
provides a natural platform for integrating lin-
guistic knowledge into SMT because hierarchical
structures produced by the formal grammar resem-
ble linguistic structures.1 Chiang (2005) attempts
to integrate linguistic information into his formally
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1We inherit the definitions of formal and linguistic from
(Chiang, 2005) which makes a distinction between formally
syntax-based SMT and linguistically syntax-based SMT.
syntax-based system by adding a constituent fea-
ture. Unfortunately, the linguistic feature does not
show significant improvement on the test set. In
this paper, we further this effort by integrating lin-
guistic knowledge into BTG.
We want to augment BTG?s formal structures
with linguistic structures since they are both hier-
archical. In particular, our goal is to learn a more
linguistically meaningful BTG from real-world bi-
texts by projecting linguistic structures onto BTG
formal structures. In doing so, we hope to (1)
maintain the strength of phrase-based approach
since phrases are still used on BTG leaf nodes; (2)
obtain a tight integration of linguistic knowledge in
the translation model; (3) and finally avoid induc-
ing a complicated linguistic synchronous grammar
with expensive computation. The challenge, of
course, is that BTG hierarchical structures are not
always aligned with the linguistic structures in the
syntactic parse trees of source or target language.
Along this line, we propose a novel approach:
Linguistically Annotated BTG (LABTG) for SMT.
The LABTG annotates BTG rules with linguistic
elements that are learned from syntactic parse trees
on the source side through an annotation algo-
rithm, which is capable of labelling both syntactic
and non-syntactic phrases. The linguistic elements
extracted from parse trees capture both internal
lexical content and external context of phrases.
With these linguistic annotations, we expect the
LABTG to address two traditional issues of stan-
dard phrase-based SMT (Koehn et al, 2003) in a
more effective manner. They are (1) phrase trans-
lation: translating phrases according to their con-
texts; (2) phrase reordering: incorporating richer
linguistic features for better reordering.
The proposed LABTG displays two unique
characteristics when compared with BTG-based
SMT (Wu, 1996; Xiong et al, 2006). The first
is that two linguistically-informed sub-models are
introduced for better phrase translation and re-
ordering: annotated phrase translation model and
1009
annotated reordering model. The second is that
our proposed annotation algorithm and scheme are
capable of conveying linguistic knowledge from
source-side syntax structures to BTG structures.
We describe the LABTG model and the annota-
tion algorithm in Section 4. To better explain the
LABTG model, we establish a unified framework
of BTG-based SMT in Section 3. We conduct
a series of experiments to study the effect of the
LABTG in Section 5.
2 Related Work
There have been various efforts to integrate lin-
guistic knowledge into SMT systems, either from
the target side (Marcu et al, 2006; Hassan et al,
2007; Zollmann and Venugopal, 2006), the source
side (Quirk et al, 2005; Liu et al, 2006; Huang
et al, 2006) or both sides (Eisner, 2003; Ding et
al., 2005; Koehn and Hoang, 2007), just to name a
few. LABTG can be considered as, but not limited
to, a new attempt that enriches translation model
with source-side linguistic annotations.
(Huang and Knight, 2006) and (Hassan et al,
2007) introduce relabeling and supertagging on the
target side, respectively. The former re-annotates
syntactified phrases to learn grammatical distinc-
tions while the latter supertags standard plain
phrases, both applied on the target side. The differ-
ence between their work and LABTG is significant
because we annotate standard plain phrases using
linguistic elements on the source side. Compared
with the target side annotation which improves flu-
ency and grammaticality of translation output, lin-
guistic annotation on the source side helps to im-
prove translation adequacy.
Recently, some researchers have extended and
created several variations of BTG/ITG. Zhang et
al. (2005) propose lexicalized ITG for better word
alignment. Xiong et al (2006) demonstrate that
their MEBTG, a BTG variation with MaxEnt-
based reordering model, can improve phrase re-
ordering significantly. Similarly, Setiawan et al
(2007) use an enhanced BTG variation with func-
tion words for reordering. LABTG differs from
these BTG variations in that the latter does not use
any external linguistic knowledge.
Zhang et al (2007) describe a phrase reorder-
ing model based on BTG-style rules which inte-
grates source-side syntactic knowledge. Our an-
notated reordering model of LABTG differs from
their work in two key aspects. Firstly, we al-
low any phrase reorderings while they only reorder
syntactic phrases. In their model, only syntactic
phrases can use linguistic knowledge from parse
trees for reordering while non-syntactic phrases
are combined monotonously with a constant re-
ordering score since no syntactic knowledge can
be used at all. Our proposed LABTG successfully
overcomes this limitation by supporting linguis-
tic annotation on both syntactic and non-syntactic
phrases. Moreover, we show that excluding non-
syntactic phrase from reordering does hurt the
performance. Secondly, we use richer linguistic
knowledge in reordering, including head words
and syntactic labels of context nodes, when com-
pared with their model. Our experiments show that
these additional information can improve reorder-
ing.
3 BTG Based SMT
We establish a unified framework for BTG-based
SMT in this section. There are two kinds of rules
in BTG, lexical rules (denoted as rl) and merging
rules (denoted as rm):
r
l
: A ? x/y
and
r
m
: A ? [A
l
, A
r
]|?A
l
, A
r
?
Lexical rules translate source phrase x into target
phrase y and generate a leaf node A in BTG tree.
Merging rules combine left and right neighboring
phrases A
l
and A
r
into a larger phrase A in an or-
der o ? {straight, inverted}.
We define a BTG derivation D as a sequence
of independent applications of lexical and merging
rules (D = ?rl
1..n
l
, r
m
1..n
m
?). Given a source sen-
tence, the decoding task of BTG-based SMT is to
find a best derivation, which yields the best trans-
lation.
Similar to (Xiong et al, 2006), we can as-
sign a probability to each rule using a log-linear
model with different features and corresponding ?
weights, then multiply them to obtain P (D). For
convenience of notation and keeping in line with
the common understanding of standard phrase-
based model, here we re-organize these features
into translation model (P
T
), reordering model
(P
R
) and target language model (P
L
) as follows
P (D) = P
T
(r
l
1..n
l
) ? P
R
(r
m
1..n
m
)
?
R
?P
L
(e)
?
L
? exp(|e|)
?
w (1)
where exp(|e|) is the word penalty.
1010
The translation model is defined as:
P
T
(r
l
1..n
l
) =
n
l
?
i=1
P (r
l
i
)
P (r
l
) = p(x|y)
?
1
? p(y|x)
?
2
? p
lex
(x|y)
?
3
?p
lex
(y|x)
?
4
? exp(1)
?
5 (2)
where p(?) represent the phrase translation proba-
bilities in both directions, p
lex
(?) denote the lexi-
cal translation probabilities in both directions, and
exp(1) is the phrase penalty.
Similarly, the reordering model is defined on the
merging rules as follows
P
R
(r
m
1..n
m
) =
n
m
?
i=1
P (r
m
i
) (3)
In the original BTGmodel (Wu, 1996), P (rm) was
actually a prior probability which can be set based
on the order preference of the language pairs. In
MEBTG (Xiong et al, 2006), however, the prob-
ability is calculated more sophisticatedly using a
MaxEnt-based classification model with boundary
words as its features.
4 Linguistically Annotated BTG Based
SMT
We extend the original BTG into the linguistically
annotated BTG by adding linguistic annotations
from source-side parse trees to both BTG lexical
rules and merging rules. Before we elaborate how
the LABTG extends the baseline, we introduce an-
notated BTG rules.
In the LABTG, both lexical rules and merging
rules are annotated with linguistic elements as fol-
lows
ar
l
: A
a
? x#a/y
and
ar
m
: A
a
? [A
a
l
l
, A
a
r
r
]|?A
a
l
l
, A
a
r
r
?
The annotation a comprises three linguistic ele-
ments from source-side syntactic parse tree: (1)
head word hw, (2) the part-of-speech (POS) tag
ht of head word and (3) syntactic label sl. In an-
notated lexical rules, the three elements are com-
bined together and then attached to x as an anno-
tation unit. In annotated merging rules, each node
involved in merging is annotated with these three
elements individually.
There are various ways to learn the annotated
rules from training data. The straight-forward way
is to first generate the best BTG tree for each sen-
tence pair using the way of (Wu, 1997), then an-
notate each BTG node with linguistic elements
by projecting source-side syntax tree to BTG tree,
and finally extract rules from these annotated BTG
trees. This way restricts learning space to only the
best BTG trees2, and leads to the loss of many use-
ful annotated rules.
Therefore, we use an alternative way to extract
the annotated rules as illustrated below. Firstly, we
run GIZA++ (Och and Ney, 2000) on the train-
ing corpus in both directions and then apply the
ogrow-diag-finalp refinement rule (Koehn et al,
2003) to obtain many-to-many word alignments.
Secondly, we extract bilingual phrases from the
word-aligned corpus, then annotate their source
sides with linguistic elements to obtain the an-
notated lexical rules.3 Finally, we learn reorder-
ing examples (Xiong et al, 2006), annotate their
two neighboring sub-phrases and whole phrases,
and then generalize them in the annotated merging
rules. Although this alternative way may also miss
reorderings due to word alignment errors, it is still
more flexible and robust than the straight-forward
one, and can learn more annotated BTG rules with-
out constructing BTG trees explicitly.
4.1 LABTG Annotation Algorithm
During the process of rule learning and decod-
ing, we need to annotate bilingual phrases or BTG
nodes generated by the decoder given a source
sentence together with its parse tree. Since both
phrases and BTG nodes can be projected to a span
on the source sentence, we run our annotation al-
gorithm on source-side spans and then assign an-
notation results to the corresponding phrases or
nodes. If the span is exactly covered by a single
subtree in the source-side parse tree, it is called
syntactic span, otherwise non-syntactic span.
One of the challenges in this annotation algorithm
is that BTG nodes (or phrases) are not always cov-
ering syntactic span, in other words, are not always
aligned to constituent nodes in the source-side tree.
To solve this problem, we use heuristic rules to
generate pseudo head word and composite label
which consists of syntactic labels of three relevant
constituents for the non-syntactic span.
The annotation algorithm is shown in Fig. 1.
For a syntactic span, the annotation is trivial. An-
notation elements directly come from the subtree
that exactly covers the span. For a non-syntactic
2Producing BTG forest for each sentence pair is very time-
consuming.
3This makes the number of extracted annotated lexical
rules proportional to that of bilingual phrases.
1011
1: Annotator (span s = ?i, j?, source-side parse tree t)
2: if s is a syntactic span then
3: Find the subtree c in t which exactly covers s
4: s.a := {c.hw, c.ht, c.sl}
5: else
6: Find the smallest subtree c? subsuming s in t
7: if c?.hw ? s then
8: s.a.hw := c?.hw and s.a.ht := c?.ht
9: else
10: Find the word w ? s which is nearest to c?.hw
11: s.a.hw := w and s.a.ht := w.t /*w.t is the POS
tag of w*/
12: end if
13: Find the left context node ln of s in c?
14: Find the right context node rn of s in c?
15: s.a.sl := ln.sl-c?.sl-rn.sl
16: end if
Figure 1: The LABTG Annotation Algorithm.
span, the process is much complicated. Firstly,
we need to locate the smallest subtree c? subsum-
ing the span (line 6). Secondly, we try to identify
the head word/tag of the span (line 7-12) by us-
ing c??s head word hw directly if it is within the
span. Otherwise, the word within the span which
is nearest to hw will be assigned as the head word
of the span. Finally, we determine the composite
label of the span (line 13-15), which is formulated
as L-C-R. L/R refers to the syntactic label of the
left/right context node of s which is a sub-node of
c
?
. There are different ways to define the context
node of a span in the source-side parse tree. It can
be the closest neighboring node or the boundary
node which is the highest leftmost/rightmost sub-
node of c? not overlapping the span. If there is no
such context node (the span s is exactly aligned to
the left/right boundary of c?), L/R will be set to
NULL. C is the label of c?. L, R and C together
define the external syntactic context of s.
Fig. 2 shows a syntactic parse tree for a Chinese
sentence, with head word annotated for each inter-
nal node.4 Some sample annotations are given in
Table 1. We also show different composite labels
for non-syntactic spans with different definitions
of their context nodes. sl
1
is obtained when the
boundary node is defined as the context node while
sl
2
is obtained when the closest neighboring node
is defined as the context node.
4.2 LABTG Model
To better model annotated rules, the LABTG con-
tributes two significant modifications to formula
(1). First is the annotated phrase translation model
4In this paper, we use phrase labels from the Penn Chinese
Treebank (Xue et al, 2005).
IP(??)
?
?
?
?
?
H
H
H
H
H
NP(??)
?
?
H
H
NP(??)
NR
??1
Tibet
NP(??)
?H
NN
??2
financial
NN
??3
work
VP(??)
?
?
?
?
?
H
H
H
H
H
VV
??4
gain
AS
?5
NP(??)
?
?
H
H
ADJP(??)
JJ
??6
remarkable
NP(??)
NN
?7?
achievement
Figure 2: A syntactic parse tree with head word
annotated for each internal node. The superscripts
of leaf nodes denote their surface positions from
left to right.
span hw ht sl
1
(boundary node) sl
2
(neighboring node)
?1, 2? ?? NN NULL-NP-NN NULL-NP-NN
?2, 3? ?? NN NP NP
?2, 4? ?? VV NP-IP-NP NP-IP-AS
?3, 4? ?? VV NP-IP-NP NN-IP-AS
Table 1: Annotation samples according to the tree
shown in Fig. 2. hw/ht represents head word/tag,
respectively. sl means the syntactic label.
with source side linguistically enhanced to replace
the standard phrase translation model, and second
is the additional MaxEnt-based reordering model
that uses linguistic annotations as features. The
LABTG model is formulated as follows
P (D) = P
T
a
(ar
l
1..n
l
) ? P
R
b
(r
m
1..n
m
)
?
R
b
?P
R
a
(ar
m
1..n
m
)
?
R
a
? P
L
(e)
?
L
? exp(|e|)
?
w (4)
Here P
T
a
is the annotated phrase translation
model, P
R
b
is the reordering model from MEBTG
using boundary words as features and P
R
a
is the
annotated reordering model using linguistic anno-
tations of nodes as features.
Annotated Phrase Translation Model The
annotated phrase translation model P
T
a
is sim-
ilar to formula (2) except that phrase transla-
tion probabilities on both directions are p(x#a|y)
and p(y|x#a) respectively, instead of p(x|y) and
p(y|x). By introducing annotations into the trans-
lation model, we integrate linguistic knowledge
into the statistical selection of target equivalents.
Annotated Reordering Model The annotated
reordering model P
R
a
is a MaxEnt-based classi-
fication model which uses linguistic elements of
each annotated node as its features. The model can
be formulated as
P
R
a
(ar
m
) = p
?
(o|A
a
, A
a
l
l
, A
a
r
r
)
1012
=exp(
?
i
?
i
h
i
(o,A
a
, A
a
l
l
, A
a
r
r
))
?
o
exp(
?
i
?
i
h
i
(o,A
a
, A
a
l
l
, A
a
r
r
))
where the functions h
i
? {0, 1} are reordering fea-
tures and ?
i
are weights of these features.
Each merging rule involves 3 nodes
(Aa, Aal
l
, A
a
r
r
) and each node has 3 linguistic
elements (hw, ht, sl). Therefore, the model has 9
features in total. Taking the left node Aal
l
as an
example, the model could use its head word w as
feature as follows
h
i
(o,A
a
, A
a
l
l
, A
a
r
r
) =
{
1, A
a
l
l
.hw = w, o = straight
0, otherwise
4.3 Training
To train the annotated translation model, firstly we
extract all annotated lexical rules from source-side
parsed, word-aligned training data. Then we es-
timate the annotated phrase translation probabili-
ties p(x#a|y) and p(y|x#a) using relative counts
from all collected annotated lexical rules. For ex-
ample, p(y|x#a) can be calculated as follows
p(y|x#a) =
count(x#a, y)
?
y
count(x#a, y)
One might think that linguistic annotations would
cause serious data sparseness problem and the
probabilities should be smoothed. However, ac-
cording to our statistics (described in the next sec-
tion), the differences in annotations for the same
source phrase x are not so diverse. So we take
a direct backoff strategy to map unseen annotated
lexical rules to their un-annotated versions on the
fly during decoding, which is detailed in the next
subsection.
To train the annotated reordering model, we
generate all annotated reordering examples, then
obtain features using linguistic elements of these
examples, and finally estimate feature weights
based on the maximum entropy principle.
4.4 Decoding
A CKY-style decoder with beam search is devel-
oped, similar to (Xiong et al, 2006). Each in-
put source sentence is firstly parsed to obtain its
syntactic tree. Then the CKY-style decoder tries
to generate the best annotated BTG tree using the
trained annotated lexical and merging rules. We
store all annotated lexical rules and their proba-
bilities in a standard phrase table ?, where source
phrases are augmented with annotations. During
the application of annotated lexical rules, we la-
bel each source phrase x with linguistic annota-
tion a through the annotation algorithm given the
source-side parse tree, and retrieve x#a from ?.
In the case of unseen combination x#a, we map
it to x and lookup x in the phrase table so that we
can use the un-annotated lexical rule A ? x/y.
We set p(y|x) = max
a
?
p(y|x#a
?
) and p(x|y) =
max
a
?
p(x#a
?
|y) where (x, a?, y) ? ?. When two
neighboring nodes are merged in a specific order,
the two reordering models, P
R
b
and P
R
a
, will eval-
uate this merging independently with individual
scores. The former uses boundary words as fea-
tures while the latter uses the linguistic elements
as features, annotated on the BTG nodes through
the annotation algorithm according to the source-
side parse tree.
5 Experiments and Analysis
In this section we conducted a number of ex-
periments to demonstrate the competitiveness of
the proposed LABTG based SMT when compared
with two baseline systems: Moses (Koehn et al,
2007), a state-of-the-art phrase-based system and
MEBTG (Xiong et al, 2006), a BTG based sys-
tem. We also investigated the impact of differ-
ent annotation schemes on the LABTG model and
studied the effect of annotated phrase translation
model and annotated reordering model on transla-
tion selection and phrase reordering respectively.
All experiments were carried out on the Chinese-
to-English translation task of the NISTMT-05 with
case-sensitive BLEU scores reported.
The systems were trained on the FBIS cor-
pus. We removed 15,250 sentences, for which
the Chinese parser (Xiong et al, 2005) failed to
produce syntactic parse trees. The parser was
trained on the Penn Chinese Treebank with a F1
score of 79.4%. From the remaining FBIS corpus
(224, 165 sentence pairs), we obtained 4.55M stan-
dard bilingual phrases (including 2.75M source
phrases) for the baseline systems and 4.65M an-
notated lexical rules (including 3.13M annotated
source phrases augmented with linguistic anno-
tations) for the LABTG system using the algo-
rithm mentioned above. These statistics reveal
that there are 1.14 (3.13M/2.75M) annotations per
source phrase, which means our annotation algo-
rithm does not increase the number of extracted
rules exponentially.
We extracted 2.8M reordering examples, from
1013
System BLEU
Moses 0.2386
MEBTG 0.2498
LABTG 0.2667
Table 2: LABTG vs. Moses and MEBTG.
which we generated 114.8K reordering features for
the reordering model P
R
b
(shared by both MEBTG
and LABTG systems) using the right boundary
words of phrases and 85K features for the anno-
tated reordering model P
R
a
(only included in the
LABTG system) using linguistic annotations. We
ran the MaxEnt toolkit (Zhang, 2004) to tune re-
ordering feature weights with iteration number be-
ing set to 100 and Gaussian prior to 1 to avoid over-
fitting.
We built our four-gram language model using
Xinhua section of the English Gigaword corpus
(181.1M words) with the SRILM toolkit (Stolcke,
2002). For the efficiency of minimum-error-rate
training (Och, 2003), we built our development set
(580 sentences) using sentences not exceeding 50
characters from the NIST MT-02 evaluation test
data.
5.1 LABTG vs. phrase-based SMT and
BTG-based SMT
We compared the LABTG system with two base-
line systems. The results are given in Table 2.
The LABTG outperforms Moses and MEBTG by
2.81 and 1.69 absolute BLEU points, respectively.
These significant improvements indicate that BTG
formal structures can be successfully extended
with linguistic knowledge extracted from syntac-
tic structures without losing the strength of phrase-
based method.
5.2 The Effect of Different Annotation
Schemes
A great amount of linguistic knowledge is con-
veyed through the syntactic label sl. To obtain
this label, we tag syntactic BTG node with single
label C from its corresponding constituent in the
source-side parse tree while annotate non-syntactic
BTG node with composite label formulated as L-
C-R. We conducted experiments to study the effect
of different annotation schemes on the LABTG
model by comparing three different annotation
schemes for non-syntactic BTG node: (1) using
single label C from its corresponding smallest sub-
tree c? (C), (2) constructing composite label using
Annotation scheme BLEU
C 0.2626
N-C-N 0.2591
B-C-B 0.2667
Annotating syntactic nodes with com-
posite label
0.2464
Table 3: Comparison of different annotation
schemes.
neighboring node as context node (N-C-N), and (3)
constructing composite label using boundary node
as context node (B-C-B). The results are shown in
Table 3.
On the one hand, linguistic annotation provides
additional information for LABTG, transferring
knowledge from source-side linguistic structures
to BTG formal structures. On the other hand, how-
ever, it is also a constraint on LABTG, guiding the
annotated translation model and reordering model
to the selection of target alernatives and reorder-
ing patterns, respectively. A tight constraint al-
ways means that annotations are too specific, al-
though they incorporate rich knowledge. Too spe-
cific annotations are more sensitive to parse errors,
and easier to make the model lose correct transla-
tions or use wrong reordering patterns. That is the
reason why the annotation scheme ?N-C-N? and
?Annotating syntactic nodes with composite label?
5 both hurt the performance. Conversely, a loose
constraint means that annotations are too generic
and have less knowledge incorporated. The an-
notation scheme ?C? is such a scheme with loose
constraint and less knowledge.
Therefore, an ideal annotation scheme should
not be too specific or too generic. The annota-
tion scheme ?B-C-B? achieves a reasonable bal-
ance between knowledge incorporation and con-
straint, which obtains the best performance. There-
fore we choose boundary node as context node for
label annotation of non-syntactic BTG nodes in ex-
periments described later.
5.3 The Effect of Annotated Translation
Model
To investigate the effect of the annotated transla-
tion model on translation selection, we compared
the standard phrase translation model P
T
used
in MEBTG with the annotated phrase translation
5In this annotation scheme, we produce composite label
L-C-R for both syntactic and non-syntactic BTG nodes. For
syntactic node, sibling node is used as context node while for
non-syntactic node, boundary node is used as context node.
1014
Translation model BLEU
P
T
0.2498
P
T
a
0.2581
P
T
a
(-NULL) 0.2548
Table 4: The effect of annotated translation model.
model P
T
a
. The experiment results are shown in
Table 4. The significant improvement in the BLEU
score indicates that the annotated translation model
helps to select better translation options.
Our study on translation output shows that anno-
tating phrases with source-side linguistic elements
can provide at least two kinds of information for
translation model to improve the adequacy: cate-
gory and context. The category knowledge of a
phrase can be used to select its appropriate trans-
lation related to its category. For example, Chi-
nese phrase ??? can be translated into ?value? if
it is a verb or ?at/on? if it is a proposition. How-
ever, the baseline BTG-based system always se-
lects the proposition translation even if it is a verb
because the language model probability for propo-
sition translation is higher than that of verb trans-
lation. This wrong translation of content words is
similar to the incorrect omission reported in (Och
et al, 2003), which both hurt translation adequacy.
The annotated translation model can avoid wrong
translation by filtering out phrase candidates with
unmatched categories.
The context information (provided by context
node) is also quite useful for translation selection.
Even the ?NULL? context, which we used in label
annotation to indicate that a phrase is located at the
boundary of a constituent, provides some informa-
tion, such as, transitive or intransitive attribute of
a verb phrase. The last row of Tabel 4 shows that
if we remove ?NULL? in label annotation, the per-
formance is degraded. (Huang and Knight, 2006)
also reported similar result by using sisterhood an-
notation on the target side.
5.4 The Effect of Annotated Reordering
Model
To investigate the effect of the annotated reorder-
ing model, we integrate P
R
a
with various settings
in MEBTG while keeping its original phrase trans-
lation model P
T
and reordering model P
R
b
un-
changed. We augment P
R
a
?s feature pool incre-
mentally: firstly using only single labels 6(SL)
6For non-syntactic node, we only use the single label C,
without constructing composite label L-C-R.
Reordering Configuration BLEU
P
R
b
0.2498
P
R
b
+ P
R
a
(SL) 0.2588
P
R
b
+ P
R
a
(+BNL) 0.2627
P
R
b
+ P
R
a
(+BNL+HWT) 0.2652
P
R
b
+ P
R
a
(SL+BNL+HWT): only al-
lowed syntactic phrase reordering
0.2512
Table 5: The effect of annotated reordering model.
as features (132 features in total), then construct-
ing composite labels for non-syntactic phrases
(+BNL) (6.7K features), and finally introducing
head words into the feature pool (+BNL+HWT)
(85K features). This series of experiments demon-
strate the impact and degree of contribution made
by each feature for reordering. We also conducted
experiments to investigate the effect of restrict-
ing reordering to syntactic phrases using the best
reordering feature set (SL+BNL+HWT) for P
R
a
.
The experimental results are presented in Table 2,
from which we have the following observations:
(1) Source-side syntactic labels (SL) capture re-
ordering patterns between source structures and
their target counterparts. Even when the base-
line feature set SL with only 132 features is used
for P
R
a
, the BLEU score improves from 0.2498
to 0.2588. This is because most of the frequent
reordering patterns between Chinese and English
have been captured using syntactic labels. For ex-
ample, the pre-verbal modifier PP in Chinese is
translated into post-verbal counterpart in English.
This reordering can be described by a rule with an
inverted order: V P ? ?PP, V P ?, and captured
by our syntactic reordering features.
(2) Context information, provided by labels of
context nodes (BNL) and head word/tag pairs
(HWT), also improves phrase reordering. Produc-
ing composite labels for non-syntactic BTG nodes
(+BNL) and integrating head word/tag pairs into
P
R
a
as reordering features (+BNL+HWT) are both
effective, indicating that context information com-
plements syntactic label for capturing reordering
patterns.
(3) Restricting phrase reordering to syntactic
phrases is harmful. The BLEU score plummets
from 0.2652 to 0.2512.
6 Conclusions
In this paper, we have presented a Linguistically
Annotated BTG based approach to effectively in-
tegrate linguistic knowledge into SMT by merging
1015
source-side linguistic structures with BTG hierar-
chical structures. The LABTG brings BTG-based
SMT towards linguistically syntax-based SMT and
narrows the linguistic gap between them. Our
experimental results show that the LABTG sig-
nificantly outperforms the state-of-the-art phrase-
based SMT and the baseline BTG-based SMT. The
proposed method also offers better translation se-
lection and phrase reordering by introducing the
annotated phrase translation model and the anno-
tated reordering model with linguistic annotations.
We conclude that (1) source-side syntactic in-
formation can improve translation adequacy; (2)
linguistic annotations of BTG nodes well capture
reordering patterns between source structures and
their target counterparts; (3) integration of linguis-
tic knowledge into SMT should be carefully con-
ducted so that the incorporated knowledge could
not have negative constraints on the model7.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of ACL
2005.
Yuan Ding and Martha Palmer. 2005. Machine Transla-
tion Using Probabilistic Synchronous Dependency Inser-
tion Grammars. In Proceedings of ACL 2005.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proceedings of ACL 2003.
Hany Hassan, Khalil Sima?an and Andy Way. 2007. Su-
pertagged Phrase-based Statistical Machine Translation.
In Proceedings of ACL 2007.
Bryant Huang, Kevi Knight. 2006. Relabeling Syntax Trees
to Improve Syntax-Based Machine Translation Quality. In
Proceedings of NAACL-HLT 2006.
Liang Huang, Kevi Knight and Aravind Joshi. 2006. Statisti-
cal Syntax-directed Translation with Extended Domain of
Locality. In Proceedings of AMTA 2006.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings of
HLT/NAACL.
Philipp Koehn, Hieu Hoang. 2007. Factored Translation
Models. In Proceedings of EMNLP 2007.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris
Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst.
2007. Moses: Open Source Toolkit for Statistical Machine
Translation. ACL 2007, demonstration session, Prague,
Czech Republic, June 2007.
7For example, the annotation scheme ?N-C-N? incorpo-
rates rich syntactic knowledge, but also tightens the constraint
on the model, which therefore loses robustness.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Translation.
In Proceedings of ACL-COLING 2006.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical Machine Translation
with Syntactified Target Language Phraases. In Proceed-
ings of EMNLP.
Franz Josef Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of ACL 2000.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of ACL
2003.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin
Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin,
Dragomir Radev. 2003. Final Report of Johns Hopkins
2003 SummerWorkshop on Syntax for Statistical Machine
Translation.
Chris Quirk, Arul Menezes and Colin Cherry. 2005. Depen-
dency Treelet Translation: Syntactically Informed Phrasal
SMT. In Proceedings of ACL 2005.
Hendra Setiawan, Min-Yen Kan and Haizhou Li. 2007. Or-
dering Phrases with Function Words. In Proceedings of
ACL 2007.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of International Con-
ference on Spoken Language Processing, volume 2, pages
901-904.
Dekai Wu. 1996. A Polynomial-Time Algorithm for Statisti-
cal Machine Translation. In Proceedings of ACL 1996.
Dekai Wu. 1997. Stochastic Inversion Transduction Gram-
mars and Bilingual Parsing of Parallel Corpora. Computa-
tional Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang
Qian. 2005. Parsing the Penn Chinese Treebank with Se-
mantic Knowledge. In Proceedings of IJCNLP, Jeju Is-
land, Korea.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum En-
tropy Based Phrase Reordering Model for Statistical Ma-
chine Translation. In Proceedings of ACL-COLING 2006.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer.
2005. The Penn Chinese TreeBank: Phrase Structure An-
notation of a Large Corpus. Natural Language Engineer-
ing, 11(2):207-238.
Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou. 2007.
Phrase Reordering Model Integrating Syntactic Knowl-
edge for SMT. In Proceedings of EMNLP-CoNLL 2007.
Hao Zhang and Daniel Gildea. 2005. Stochastic Lexicalized
Inversion Transduction Grammar for Alignment. In Pro-
ceedings of ACL 2005.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
Augmented Machine Translation via Chart Parsing. In
NAACL 2006 - Workshop on statistical machine transla-
tion, New York. June 4-9.
1016
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1097?1104
Manchester, August 2008
Grammar Comparison Study for Translational Equivalence 
Modeling and Statistical Machine Translation 
Min Zhang1,  Hongfei Jiang2,  Haizhou Li1,  Aiti Aw1  and  Sheng Li2 
1Institute for Infocomm Research, Singapore 
2Harbin Institute of Technology, China 
{mzhang, hli, aaiti}@i2r.a-star.edu.sg 
{hfjiang, lisheng}@mtlab.hit.edu.cn 
 
Abstract 
This paper presents a general platform, 
namely synchronous tree sequence sub-
stitution grammar (STSSG), for the 
grammar comparison study in Transla-
tional Equivalence Modeling (TEM) and 
Statistical Machine Translation (SMT). 
Under the STSSG platform, we compare 
the expressive abilities of various gram-
mars through synchronous parsing and a 
real translation platform on a variety of 
Chinese-English bilingual corpora. Ex-
perimental results show that the STSSG 
is able to better explain the data in paral-
lel corpora than other grammars. Our 
study further finds that the complexity of 
structure divergence is much higher than 
suggested in literature, which imposes a 
big challenge to syntactic transformation-
based SMT. 
1 Introduction 
Translational equivalence is a mathematical rela-
tion that holds between linguistic expressions 
with the same meaning (Wellington et al, 2006).  
The common explicit representations of this rela-
tion are word alignments, phrase alignments and 
structure alignments between bilingual sentences. 
Translational Equivalence Modeling (TEM) is a 
process to describe and build these alignments 
using mathematical models. Thus, the study of 
TEM is highly relevant to Statistical Machine 
Translation (SMT). 
Grammar is the most important infrastructure 
for TEM and SMT since translation models? ex-
pressive and generative abilities are mainly de-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
termined by the grammar. Many grammars, such 
as finite-state grammars (FSG), bracket/inversion 
transduction grammars (BTG/ITG) (Wu, 1997), 
context-free grammar (CFG), tree substitution 
grammar (TSG) (Comon et al, 2007) and their 
synchronous versions, have been explored in 
SMT. Based on these grammars, a great number 
of SMT models have been recently proposed, 
including string-to-string model (Synchronous 
FSG) (Brown et al, 1993; Koehn et al, 2003), 
tree-to-string model (TSG-string) (Huang et al, 
2006; Liu et al, 2006; Liu et al, 2007), string-to-
tree model (string-CFG/TSG) (Yamada and 
Knight, 2001; Galley et al, 2006; Marcu et al, 
2006), tree-to-tree model (Synchronous 
CFG/TSG, Data-Oriented Translation) (Chiang, 
2005; Cowan et al, 2006; Eisner, 2003; Ding and 
Palmer, 2005; Zhang et al, 2007; Bod, 2007; 
Quirk wt al., 2005; Poutsma, 2000; Hearne and 
Way, 2003) and so on. 
Although many achievements have been ob-
tained by these advances, it is still unclear which 
of these important pursuits is able to best explain 
human translation data, as each has its advan-
tages and disadvantages. Therefore, it has great 
meaning in both theory and practice to do com-
parison studies among these grammars and SMT 
models to see which of them are capable of better 
describing parallel translation data. This is a fun-
damental issue worth exploring in multilingual 
information processing. However, little effort in 
previous work has been put in this point. To ad-
dress this issue, in this paper we define a general 
platform, namely synchronous tree sequence 
substitution grammar (STSSG), for the compari-
son studies. The STSSG can be seen as a gener-
alization of Synchronous TSG (STSG) by replac-
ing elementary tree (a single subtree used in 
STSG) with contiguous tree sequence as the ba-
sic translation unit. As a result, most of previous 
grammars used in SMT can be interpreted as the 
reduced versions of the STSSG. Under the 
STSSG platform, we compare the expressive 
1097
abilities of various grammars and translation 
models through linguistically-based synchronous 
parsing and a real translation platform. By syn-
chronous parsing, we aim to study which gram-
mar can well explain translation data (i.e. transla-
tional equivalence alignment) while by the real 
translation platform, we expect to investigate 
which model can achieve better translation per-
formance. In addition, we also measure the im-
pact of various factors in this study, including the 
genera of corpora (newspaper domain via spoken 
domain), the accuracy of word alignments and 
syntax parsing (automatically vs. manually).  
We report our experimental settings, experi-
mental results and our findings in detail in the 
rest of the paper, which is organized as follows: 
Section 2 reviews previous work. Section 3 
elaborates the general framework while Section 4 
reports the experimental results. Finally, we con-
clude our work in Section 5. 
2 Previous Work 
There are only a few of previous work related to 
the study of translation grammar comparison. 
Fox (2002) is the first to look at how well pro-
posed translation models fit actual translation 
data empirically. She examined the issue of 
phrasal cohesion between English and French 
and discovered that while there is less cohesion 
than one might desire, there is still a large 
amount of regularity in the constructions where 
breakdowns occur. This suggests that reordering 
words by phrasal movement is a reasonable strat-
egy (Fox, 2002). She has also examined the dif-
ferences in cohesion between Treebank-style 
parse trees, trees with flattened verb phrases, and 
dependency structures. Their experimental re-
sults indicate that the highest degree of cohesion 
is present in dependency structures. 
Motivated by the same problem raised by Fox 
(2002), Galley et al (2004) study what rule can 
better explain human translation data. They first 
propose a theory that gives formal semantics to 
word-level alignments defined over parallel cor-
pora, and then use the theory to introduce a linear 
algorithm that is used to derive from word-
aligned, parallel corpora the minimal set of syn-
tactically motivated transformation rules to ex-
plain human translation data. Their basic idea is 
to create transformation rules that condition on 
larger fragments of tree structure. Their experi-
mental results suggest that their proposed rules 
provide a good, realistic indicator of the com-
plexities inherent in translation than SCFG. 
Wellington et al (2006) describes their study 
of the patterns of translational equivalence exhib-
ited by a variety of bilingual/monolingual bitexts. 
They empirically measure the lower bounds on 
alignment failure rates with and without gaps 
under the constraints of word alignment alone or 
with one or both side parse trees. Their study 
finds surprisingly many examples of translational 
equivalence that could not be analyzed using bi-
nary-branching structures without discontinuities. 
Thus, they claim that the complexity of these 
patterns in every bitext is higher than suggested 
in the literature. In addition, they suggest that the 
low coverage rates without gaps under the con-
straints of independently generated monolingual 
parse trees might be the main reason why ?syn-
tactic? constraints have not yet increased the ac-
curacy of SMT systems. However, they find that 
simply allowing a single gap in bilingual phrases 
or other types of constituent can improve cover-
age dramatically. 
DeNeefe et al (2007) compares the strengths 
and weaknesses of a syntax-based MT model 
with a phrase-based MT model from the view-
points of translational equivalence extraction 
methods and coverage. They find that there are 
surprising differences in phrasal coverage ? nei-
ther is merely a superset of the other. They also 
investigate the reason why some phrase pairs are 
not learned by the syntax-based model. They fur-
ther propose several solutions and evaluate on 
the syntax-based extraction techniques in light of 
phrase pairs captured and translation accuracy. 
Finally, significant performance improvement is 
reported using their solutions. 
Different from previous work discussed above, 
this paper mainly focuses on the expressive abil-
ity comparison studies among different gram-
mars and models through synchronous parsing 
and a real SMT platform. Fox (2002), Galley et 
al (2004) and Wellington et al (2006) examine 
TEM only. DeNeefe et al (2007) only compares 
the strengths and weaknesses of a syntax-based 
MT model with a phrase-based MT model. 
3 The General Platform: the STSSG 
In this section, we first define the STSSG plat-
form in Subsection 3.1, and then explain why it 
is a general framework that can cover most of 
previous syntax-based translation grammars and 
models in Subsection 3.2. In Subsection 3.3 and 
3.4, we discuss the STSSG-based SMT and syn-
chronous parsing, which are used to compare 
different grammars and translation models. 
1098
1( )
IT e
1( )
JT f
A
 
 
Figure 1.  A word-aligned parse tree pairs of a Chi-
nese sentence and its English translation 
 
 
 
Figure 2. Two examples of translation rules 
3.1 Definition of the STSSG 
The STSSG is an extension of the STSG by us-
ing tree sequences (rather than elementary trees) 
as the basic translation unit. A STSSG is a septet 
, , , , ,,t t ts s sG N N S S P? ?=< > , where: 
z s?  and t?  are source and target terminal 
alphabets (POSs or lexical words), respec-
tively, and 
z sN  and tN are source and target non-
terminal alphabets (linguistic phrase tag, i.e. 
NP/VP?), respectively, and 
z s sS N?  and t tS N?  are the source and tar-
get start symbols (roots of source and target 
parse trees), and 
z P is a production rule set. 
A grammar rule ir  in the STSSG is an aligned 
tree sequence pair, < s? , t? , A  >, where s? and 
t?  are tree sequences of source side and target 
sides, respectively, and A is the alignments be-
tween leaf nodes of two tree sequences. Here, the 
key concept of ?tree sequence? refers to an or-
dered subtree sequence covering a consecutive 
tree fragment in a complete parse tree. The leaf 
nodes of a subtree in a tree sequence can be ei-
ther non-terminal symbols or terminal symbols. 
Fig. 2 shows two STSSG rules extracted from 
the aligned tree pair shown in Fig. 1, where 1r is 
also a STSG rule.  
In the STSSG, a translational equivalence is 
modeled as a tree sequence pair while MT is 
viewed as a tree sequence substitution process. 
From the definition of ?tree sequence?, we can 
see that a subtree in a tree sequence is a so-called 
elementary tree used in TSG. This suggests that 
SCFG and STSG are only a subset of STSSG 
and SCFG is a subset of STSG. The next subsec-
tion discusses how to configure the STSSG to 
implement the other two simplified grammars. 
This is the reason why we call the STSSG a gen-
eral framework for synchronous grammar-based 
translation modeling. 
It is worth noting that, from rule rewriting 
viewpoint, STSSG can be thought of as a re-
stricted version of synchronous multi-component 
TAGs (Schuler et al, 2000) although TAG is 
more powerful than TSG due to the additional 
operation ?adjunctions?. The synchronous multi-
component TAG can also rewrite several non-
terminals in one step of derivation. The differ-
ence between them is that the rewriting sites (i.e. 
the substitution nodes) must be contiguous in 
STSSG. In addition, STSSG is also related to 
tree automata (Comon et al, 2007). However, the 
discussion on the theoretical relation and com-
parison between them is out of the scope of the 
paper. In this paper, we focus on the comparison 
study of SMT grammars using the STSSG plat-
form. 
3.2 Rule Extraction and Grammar Con-
figuration 
All the STSSG mapping rules are extracted from 
bi-parsed trees. Our rule extraction algorithm is 
an extension of that presented at (Chiang, 2005; 
Liu et al, 2006; Zhang et al, 2007). We modify 
their tree-to-tree/string rule extraction algorithms 
to extract tree-sequence-to-tree-sequence rules. 
Our rules2 are extracted in two steps: 
                                                 
2  We classify the rules into two categories: initial 
rules, whose leaf nodes must be terminals, and ab-
1099
1) Extracting initial rules from bi-parsed trees. 
This is rather straightforward. We first generate 
all fully lexicalized source and target tree se-
quences (whose leaf nodes must be lexical words) 
using a DP algorithm and then iterate over all 
generated source and target sequence pairs. If 
their word alignments are all within the scope of 
the current tree sequence pair, then the current 
tree sequence pair is an initial rule. 
2) Extracting abstract rules from the extracted 
initial rules. The idea behind is that we generate 
an abstract rule from a ?big? initial rule by re-
moving one or more ?small? initial rules from 
the ?big? one, where the ?small? ones must be a 
sub-graph of the ?big? one. Please refer to 
(Chiang, 2005; Liu et al, 2006; Zhang et al, 
2007) for the implementation details. 
As indicated before (Chiang, 2005; Zhang et 
al., 2007), the above scheme generates a very 
large number of rules, which not only makes the 
system too complicated but also introduces too 
many undesirable ambiguities. To control the 
overall model complexity, we introduce the fol-
lowing parameters: 
1) The maximal numbers of trees in the source 
and target tree sequences: s? and t? . 
2) The maximal tree heights in the source and 
target tree sequences: s? and t? . 
3) The maximal numbers of non-terminal leaf 
nodes in the source and target tree sequences: 
s? and t? . 
Now let us see how to implement other mod-
els in relation to STSSG based the STSSG 
through configuring the above parameters. 
1) STSG-based tree-to-tree model (Zhang et 
al., 2007; Bod, 2007) when s? = t? =1. 
2) SCFG-based tree-to-tree model when s? = 
t? =1 and s? = t? =2. 
3) Phrase-based translation model only (no re-
ordering model) when s? = t? =0 and s? = t? =1. 
4) TSG-CFG-based tree-to-string model (Liu 
et al, 2006) when s? = t? =1, t? =2 and ignore 
phrase tags in target side.  
5) CFG-TSG-based string-to-tree model (Gal-
ley et al, 2006) when s? = t? =1and s? =2. 
6) TSSG-CFG-based tree-sequence-to-string 
model (Liu et al, 2007) when t? =2 and ignore 
phrase tags in target side. 
                                                                          
stract rule that having at least one non-terminal leaf 
node. 
From the above definitions, we can see that all 
of previous related models/grammars can be can 
be interpreted as the reduced versions of the 
STSSG. This is the reason why we use the 
STSSG as a general platform for our model and 
grammar comparison studies. 
3.3 Model Training and Decoder for SMT 
We use the tree sequence mapping rules to model 
the translation process. Given the source parse 
tree 1( )
JT f , there are multiple derivations3 that 
could lead to the same target tree 1( )
IT e , the 
mapping probability 1 1( ( ) | ( ))
I JrP T e T f is ob-
tained by summing over the probabilities of all 
derivations. The probability of each derivation?  
is given by the product of the probabilities of all 
the rules ( )ip r  used in the derivation (here we 
assume that a rule is applied independently in a 
derivation). 
1 1 1 1( | ) ( ( ) | ( ))
                  = ( )
i
I J I J
i
r
r rP e f P T e T f
p r
? ??
=
??           (1) 
The model is implemented under log-linear 
framework. We use seven basic features that are 
analogous to the commonly used features in 
phrase-based systems (Koehn, 2004): 1) bidirec-
tional rule mapping probabilities; 2) bidirectional 
lexical translation probabilities; 3) the target lan-
guage model; 4) the number of rules used and 5) 
the number of target words. Besides, we define 
two new features: 1) the number of lexical words 
in a rule to control the model?s preference for 
lexicalized rules over un-lexicalized rules and 2) 
the average tree height in a rule to balance the 
usage of hierarchical rules and more flat rules. 
The overall training process is similar to the 
process in the phrase-based system (koehn et al, 
2007): word alignment, rule extraction, feature 
extraction and probability calculation and feature 
weight tuning. 
Given 1( )
JT f , the decoder is to find the best 
derivation ?  that generates < 1( )JT f , 1( )IT e >.  
1
1
1 1
,
? arg max ( ( ) | ( ))
  arg max ( )
I
I
i
I J
e
i
e r
re P T e T f
p r
? ??
=
? ?              (2) 
By default, same as other SMT decoder, here 
we use Viterbi derivation in Eq (2) instead of the 
                                                 
3 A derivation is a sequence of tree sequence rules that 
maps a source parse tree to its target one. 
1100
summing probabilities in Eq (3). This is to make 
the decoder speed not too slow. The decoder is a 
standard span-based chart parser together with a 
function for mapping the source derivations to 
the target ones. To speed up the decoder, we util-
ize several thresholds to limit the search beams 
for each span, such as the number of rules used 
and the number of hypotheses generated. 
3.4 Synchronous Parsing   
A synchronous parser is an algorithm that can 
infer the syntactic structure of each component 
text in a multitext and simultaneously infer the 
correspondence relation between these structures. 
When a parser?s input can have fewer dimen-
sions than the parser?s grammar, we call it a 
translator. When a parser?s grammar can have 
fewer dimensions than the parser?s input, we call 
it a synchronizer (Melamed, 2004). Therefore, 
synchronous parsing and MT are closed to each 
other. In this paper, we use synchronous parsing 
to compare the ability of different grammars in 
translational equivalence modeling.  
Given a bilingual sentence pair 1
Jf and 1
Ie , the 
synchronous parser is to find a derivation ?  that 
generates < 1( )
JT f , 1( )
IT e >. Our synchronous 
parser is similar to the synchronous CKY parser 
presented at (Melamed, 2004). The difference is 
that we implement it based on our STSSG de-
coder. Therefore, in nature the parser is a stan-
dard synchronous chart parser but constrained by 
the rules of the STSSG grammar. In our imple-
mentation, we simply use our decoder to simu-
late the bilingual parser: 1) for each sentence pair, 
we extract one model; 2) we use the model and 
the decoder to translate the source sentence of 
the given sentence pair; 3) if the target sentence 
is successfully generated by the decoder, then we 
say the symphonious parsing is successful. 
Please note that the synchronous parsing is con-
sidered as successful once the last words in the 
source and target sentences are covered by the 
decoder even if there is no a complete target 
parse tree generated (it may be a tree sequence). 
This is because our study only concerns whether 
all translational equivalences are linked together 
by the synchronous parser correctly. 
4 Experiments 
4.1 Experimental Settings 
Synchronous parsing settings: Our experiments 
of synchronous parsing are carried on three Chi-
nese-to-English bilingual corpora: the FBIS cor-
pus, the IWSLT 2007 training set and the HIT 
Corpus. The FBIS data is a collection of trans-
lated newswire documents published by major 
news agencies from three representative loca-
tions: Beijing, Taipei and Hongkong. The 
IWSLT data is a multilingual speech corpus on 
travel domain while the HIT corpus consists of 
example sentences of a Chinese-English diction-
ary. The first two corpora are sentence-aligned 
while the HIT corpus is a manually bi-parsed 
corpus with manually annotated word alignments. 
We use the three corpora to study whether the 
models? expressive abilities are domain depend-
ent and how the performance of word alignment 
and parsing affect the ability of translation mod-
els. We selected 2000 sentence pairs from each 
individual corpus for the comparison study of 
translational equivalence modeling. Table 1 
gives descriptive statistics of the tree data set. 
 
 Chinese English 
FBIS 48,331 59,788 
IWSLT  17,667 18,427 
HIT 18,215 20,266 
 
Table 1. # of words of experimental data 
for synchronous parsing (there are 2k sen-
tence pairs in each individual corpus) 
 
In the synchronous parsing experiments, we 
compared three synchronous grammars: SCFG, 
STSG and STSSG using the STSSG platform. 
We use the same settings except the following 
parameters (please refer to Subsection 3.2 for 
their definitions): s? = t? =1, s? = t? =2 for 
SCFG ; s? = t? =1 and s? = t? =6 for STSG; 
s? = t? = 4 and s? = t? =6 for STSSG. We iter-
ate over each sentence pair in the three corpora 
with the following process: 
1) to used Stanford parser (Klein and Manning, 
2003) to parse bilingual sentences separately,  
this means that our study is based on the Penn 
Treebank style grammar.  
2) to extract SCFG, STSG and STSSG rules 
form each sentence pair, respectively; 
3) to do synchronous parsing using the exacted 
rules.  
Finally, we can calculate the successful rate of 
the synchronous parsing on each corpus. 
SMT evaluation settings: For the SMT ex-
periments, we trained the translation model on 
the FBIS corpus (7.2M (Chinese)+9.2M(English) 
words) and trained a 4-gram language model on 
1101
the Xinhua portion of the English Gigaword cor-
pus (181M words) using the SRILM Toolkits 
(Stolcke, 2002) with modified Kneser-Ney 
smoothing (Chen and Goodman, 1998). We used 
these sentences with less than 50 characters from 
the NIST MT-2002 test set as our development 
set and the NIST MT-2005 test set as our test set. 
We used the Stanford parser (Klein and Manning, 
2003) to parse bilingual sentences on the training 
set and Chinese sentences on the development 
and test sets. The evaluation metric is case-
sensitive BLEU-4 (Papineni et al, 2002). We 
used GIZA++ and the heuristics ?grow-diag-
final? to generate m-to-n word alignments. For 
the MER training, we modified Koehn?s MER 
trainer (Koehn, 2004) for our STSSG-based sys-
tem. For significance test, we used Zhang et als 
implementation (Zhang et al 2004). We com-
pared four SMT systems: Moses (Koehn et al, 
2007), SCFG-based, STSG-based and STSSG-
based tree-to-tree translation models. For Moses, 
we used its default settings. For the others, we 
implemented them on the STSSG platform by 
adopting the same settings as used in the syn-
chronous parsing. We optimized the decoding 
parameters on the development sets empirically. 
4.2 Experimental Results  
 
 SCFG STSG STSSG 
FBIS 7 (0.35%) 143 (7.15%) 388 (19.4%) 
IWSLT 171 (8.6%) 1179 (58.9%) 1708 (85.4%)
HIT 65 (3.23%) 1133 (56.6%) 1532 (76.6%)
 
Table 2. Successful rates (numbers inside 
bracket) of synchronous parsing over 2,000 
sentence pairs, where the integers outside 
bracket are the numbers of successfully-
parsed sentence pairs 
 
Table 2 reports the experimental results of syn-
chronous parsing. It shows that: 
1) As an extension of STSG/SCFG, STSSG 
outperforms STSG and SCFG consistently in the 
three data sets. The significant difference sug-
gests that the STSSG is much more effective in 
modeling translational equivalences and structure 
divergences. The reason is simply because the 
STSSG uses tree sequences as the basic transla-
tion unit so that it can model non-syntactic 
phrase equivalence with structure information 
and handle structure reordering in a large span.  
2) STSG shows much better performance than 
SCFG. It is mainly due to that STSG allow mul-
tiple level tree nodes operation and reordering in 
a larger span than SCFG. It reconfirms that only 
allowing sibling nodes reordering as done in 
SCFG may be inadequate for translational equiva-
lence modeling (Galley et al, 2004)4.  
3) All the three models on the FBIS corpus 
show much lower performance than that on the 
other two corpora. The main reason, as shown in 
Table 1, is that the sentences in the FBIS corpus 
are much longer than that in the other corpus, so 
their syntactic structures are significantly more 
complicated than the other two. In addition, al-
though tree sequences are utilized, STSSG show 
much lower performance in the FBIS corpus. 
This implies that the complexity of structure di-
vergence between two languages is higher than 
suggested in literature (Fox, 2002; Galley et al, 
2004). Therefore, structure divergence is still a 
big challenge to translational equivalence model-
ing when using syntactic structure mapping. 
4) The HIT corpus does not show better per-
formance than the IWSLT corpus although the 
HIT corpus is manually annotated with parse 
trees and word alignments. In order to study 
whether high performance word alignment and 
parsing results can help synchronous parsing, we 
do several cross validations and report the ex-
perimental results in Table 3. 
 
 Gold Word Alignment 
Automatic 
Word Align-
ment 
 Gold Parse 3.2/56.6/76.6 2.9/57.7/80.9
 Automatic  
Parse 3.2/55.6/76.0 2.9/54.2/78.8
 
Table 3. Successful rates (SCFG/STSG/ 
STSSG)(%) with regards to different word 
alignments and parse trees  on the HIT corpus 
 
Table 3 compares the performance of syn-
chronous parsing on the HIT corpus when using 
gold and automatic parser and word alignment. It 
is surprised that gold word alignments and parse 
trees do not help and even decrease the perform-
ance slightly. Our analysis further finds that 
                                                 
4 This claim is mainly hold for linguistically-informed 
SCFG since formal SCFG and BTG already showed 
much better performance in the formally syntax-based 
translation framework (Chiang, 2005). This is because 
the formal syntax is learned from phrase translational 
equivalences directly without relying on any linguistic 
theory (Chiang, 2005). Thus, it may not suffer from 
the issues of non-isomorphic structure alignment and 
non-syntactic phrase usage heavily (Wellington et al, 
2006). 
1102
more than 90% sentence pairs out of all the sen-
tence pairs that can be successfully bi-parsed are 
in common in the four experiments. This sug-
gests that the STSSG/STSG (SCFG achieves too 
much lower performance) and our rule extraction 
algorithm are robust in dealing with the errors 
introduced by the word alignment and parsing 
programs. If a parser, for example, makes a sys-
tematic error, we expect to learn a rule that can 
nevertheless be systematically used to model cor-
rect translational equivalence. Our error analysis 
on the three corpora shows that most of the fail-
ures of synchronous parsing are due to the struc-
ture divergence (i.e. the nature of non-
isomorphic structure mapping) and the long dis-
tance dependence in the syntactic structures.  
 
 SCFG Moses STSG STSSG
BLEU(%) 22.72 23.86 24.71 26.07 
 
     Table 3. Performance comparison of dif-
ferent grammars on FBIS corpus 
 
Table 3 compares different grammars in terms 
of translation performance. It shows that: 
1) The same as synchronous parsing, the 
STSSG-based model statistically significantly 
outperforms (p < 0.01) previous phrase-based and 
linguistically syntax-based methods. This empiri-
cally verifies the effect of the tree-sequence-based 
grammar for statistical machine translation.  
2) Both STSSG and STSG outperform Moses 
significantly and STSSG clearly outperforms 
STSG, which suggest that: 
z The linguistically motivated structure fea-
tures are still useful for SMT, which can be cap-
tured by the two syntax-based grammars through 
tree node operations. 
z STSSG is much more effective in utiliz-
ing linguistic structures than STSG since it uses 
tree sequence as the basic translation unit. This 
enables STSSG not only to handle structure reor-
derings by tree node operations in a larger span, 
but also to capture non-syntactic phrases with syn-
tactic information, and hence giving the grammar 
more expressive power. 
3) The linguistic-based SCFG shows much 
lower performance. This is largely because SCFG 
only allows sibling nodes reordering and fails to 
utilize both non-syntactic phrases and those syn-
tactic phrases that cannot be covered by a single 
CFG rule. It thereby suggests that SCFG is less 
effective in modelling parse tree structure trans-
fer.  
The above two experimental results show that 
STSSG achieves significant improvements over 
the other two grammars in terms of synchronous 
parsing?s successful rate and translation Bleu 
score. 
5 Conclusions 
Grammar is the fundamental infrastructure in 
translational equivalence modeling and statistical 
machine translation since grammar formalizes 
what kind of rule to be learned from a parallel 
text. In this paper, we first present a general plat-
form STSSG and demonstrate that a number of 
synchronous grammars and SMT models can be 
easily implemented based on the platform. We 
then compare the expressive abilities of different 
grammars on the platform using synchronous 
parsing and statistical machine translation. Our 
experimental results show that STSSG can better 
explain the data in parallel corpora than the other 
two synchronous grammars. We further finds 
that, although syntactic structure features are 
helpful in modeling translational equivalence, the 
complexity of structure divergence is much 
higher than suggested in literature, which im-
poses a big challenge to syntactic transformation-
based SMT. This may explain why traditional 
syntactic constraints in SMT do not yield much 
performance improvement over robust phrase-
substitution models. 
The fundamental assumption underlying much 
recent work on syntax-based modeling, which is 
considered to be one of next technology break-
throughs in SMT, is that translational equiva-
lence can be well modeled by structural trans-
formation. However, as discussed in prior arts 
(Galley et al, 2004) and this paper, linguisti-
cally-informed SCFG is an inadequate model for 
parallel corpora due to its nature that only allow-
ing child-node reorderings. Although STSG 
shows much better performance than SCFG, its 
two major limitations are that it only allows 
structure distortion operated on a single sub-tree 
and cannot model non-syntactic phrases. STSSG 
extends STSG by using tree sequence as the ba-
sic translation unit. This gives the grammar much 
more expressive power.  
There are many open issues in the syntactic 
transformation-based SMT due to the divergence 
nature between bilingual structure mappings. We 
find that structural divergences are more serious 
than suggested in the literature (Fox, 2002; Gal-
lery et al, 2004) or what we expected when sen-
tences are longer. We will continue to investigate 
1103
whether and how parallel corpora can be well 
modeled by syntactic structure mappings.   
References 
Rens Bod. 2007. Unsupervised Syntax-Based Ma-
chine Translation: The Contribution of Discon-
tinuous Phrases. MT-Summmit-07. 51-56.  
Peter F. Brown, S. A. Della Pietra, V. J. Della Pietra, 
and R. L. Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311. 
S. F. Chen and J. Goodman. 1998. An empirical study 
of smoothing techniques for language modeling. 
Technical Report TR-10-98, Harvard University 
Center for Research in Computing Technology. 
David Chiang. 2005. A hierarchical phrase-based 
model for SMT. ACL-05. 263-270. 
H. Comon, M. Dauchet, R. Gilleron, F. Jacquemard, 
D. Lugiez, S. Tison, and M. Tommasi. 2007. Tree 
automata techniques and applications. Available at: 
http://tata.gforge.inria.fr/. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree trans-
lation. EMNLP-06. 232-241. 
S. DeNeefe, K. Knight, W. Wang and D. Marcu. 2007. 
What Can Syntax-based MT Learn from Phrase-
based MT? EMNLP-CoNLL-07. 755-763 
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency 
insertion grammars. ACL-05. 541-548. 
Bonnie J. Dorr (1994). Machine Translation Diver-
gences: A formal description and proposed solu-
tion. Computational Linguistics, 20(4): 597-633 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for MT. ACL-03 (companion volume). 
Heidi J. Fox. 2002. Phrasal Cohesion and Statistical 
Machine Translation. EMNLP-2002. 304-311  
Michel Galley, J. Graehl, K. Knight, D. Marcu, S. 
DeNeefe, W. Wang and I. Thayer. 2006. Scalable 
Inference and Training of Context-Rich Syntactic 
Translation Models. COLING-ACL-06. 961-968 
M. Galley, M. Hopkins, K. Knight and D. Marcu. 
2004. What?s in a translation rule? HLT-NAACL. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06 (poster). 
Mary Hearne and Andy Way. 2003. Seeing the wood 
for the trees: data-oriented translation. MT Sum-
mit IX, 165-172. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. ACL-03. 423-430. 
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statis-
tical phrase-based translation. HLT-NAACL-03. 
127-133. 
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. AMTA-04, 115-124. 
Philipp Koehn, H. Hoang, A. Birch, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. 
Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. 
Constantin and E. Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07 (poster) 77-180. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation Rules. 
ACL-07. 704-711. 
Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 
2006. SPMT: Statistical Machine Translation with 
Syntactified Target Language Phrases. EMNLP-06. 
44-52. 
I. Dan Melamed. 2004. Statistical machine translation 
by parsing. ACL-04. 653-660. 
K. Papineni, Salim Roukos, ToddWard and Wei-Jing 
Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation. ACL-02. 311-318. 
Arjen Poutsma. 2000. Data-oriented translation. 
COLING-2000. 635-641 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. ACL-05. 271-279. 
William Schuler, David Chiang and Mark Dras. 2000. 
Multi-Component TAG and Notions of Formal 
Power. ACL-2000. 448-455 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904. 
Benjamin Wellington, Sonjia Waxmonsky and I. Dan 
Melamed. 2006. Empirical Lower Bounds on the 
Complexity of Translational Equivalence. COL-
ING-ACL-06. 977-984. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377-403. 
K. Yamada and Kevin Knight. 2001. A syntax-based 
statistical translation model. ACL-01. 523-530. 
M. Zhang, H. Jiang, A. Aw, J. Sun, S. Li and C. Tan. 
2007. A Tree-to-Tree Alignment-based Model for 
SMT. MT-Summit-07. 535-542. 
Y. Zhang, S. Vogel and A. Waibel. 2004. Interpreting 
BLEU/NIST scores: How much improvement do 
we need to have a better system? LREC-04.  
1104
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 810?818, Prague, June 2007. c?2007 Association for Computational Linguistics
A Statistical Language Modeling Approach to
Lattice-Based Spoken Document Retrieval
Tee Kiah Chia? Haizhou Li? Hwee Tou Ng?
?Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
{chiateek,nght}@comp.nus.edu.sg
?Institute for Infocomm Research
21 Heng Mui Keng Terrace
Singapore 119613
hli@i2r.a-star.edu.sg
Abstract
Speech recognition transcripts are far from
perfect; they are not of sufficient quality to
be useful on their own for spoken document
retrieval. This is especially the case for con-
versational speech. Recent efforts have tried
to overcome this issue by using statistics
from speech lattices instead of only the 1-
best transcripts; however, these efforts have
invariably used the classical vector space re-
trieval model. This paper presents a novel
approach to lattice-based spoken document
retrieval using statistical language models: a
statistical model is estimated for each doc-
ument, and probabilities derived from the
document models are directly used to mea-
sure relevance. Experimental results show
that the lattice-based language modeling
method outperforms both the language mod-
eling retrieval method using only the 1-best
transcripts, as well as a recently proposed
lattice-based vector space retrieval method.
1 Introduction
Information retrieval (IR) is the task of ranking a
collection of documents according to an estimate of
their relevance to a query. With the recent growth
in the amount of speech recordings in the form of
voice mails, news broadcasts, and so forth, the task
of spoken document retrieval (SDR) ? information
retrieval in which the document collection is in the
form of speech recordings ? is becoming increas-
ingly important.
SDR on broadcast news corpora has been
?deemed to be a solved problem?, due to the fact that
the performance of retrieval engines working on 1-
best automatic speech recognition (ASR) transcripts
was found to be ?virtually the same as their perfor-
mance on the human reference transcripts? (NIST,
2000). However, this is still not the case for SDR
on data which are more challenging, such as conver-
sational speech in noisy environments, as the 1-best
transcripts of these data contain too many recogni-
tion errors to be useful for retrieval. One way to
ameliorate this problem is to work with not just one
ASR hypothesis for each utterance, but multiple hy-
potheses presented in a lattice data structure. A lat-
tice is a connected directed acyclic graph in which
each edge is labeled with a term hypothesis and a
likelihood value (James, 1995); each path through a
lattice gives a hypothesis of the sequence of terms
spoken in the utterance.
Each lattice can be viewed as a statistical model
of the possible transcripts of an utterance (given the
speech recognizer?s state of knowledge); thus, an
IR model based on statistical inference will seem
to be a more natural and more principled approach
to lattice-based SDR. This paper thus proposes a
lattice-based SDR method based on the statistical
language modeling approach of Song and Croft
(1999). In this method, the expected word count ?
the mean number of occurrences of a word given
a lattice?s statistical model ? is computed for each
word in each lattice. Using these expected counts,
a statistical language model is estimated for each
spoken document, and a document?s relevance to a
query is computed as a probability under this model.
810
The rest of this paper is organized as follows. In
Section 2 we review related work in the areas of
speech processing and IR. Section 3 describes our
proposed method as well as the baseline methods.
Details of the experimental setup are given in Sec-
tion 4, and experimental results are in Section 5. Fi-
nally, Section 6 concludes our discussions and out-
lines our future work.
2 Related Work
2.1 Lattices for Spoken Document Retrieval
James and Young (1994) first introduced the lattice
as a representation for indexing spoken documents,
as part of a method for vocabulary-independent key-
word spotting. The lattice representation was later
applied to the task of spoken document retrieval
by James (1995): James counted how many times
each query word occurred in each phone lattice with
a sufficiently high normalized log likelihood, and
these counts were then used in retrieval under a vec-
tor space model with tf ? idf weighting. Jones et al
(1996) combined retrieval from phone lattices using
variations of James? method with retrieval from 1-
best word transcripts to achieve better results.
Since then, a number of different methods for
SDR using lattices have been proposed. For in-
stance, Siegler (1999) used word lattices instead of
phone lattices as the basis of retrieval, and gener-
alized the tf ? idf formalism to allow uncertainty
in word counts. Chelba and Acero (2005) prepro-
cessed lattices into more compact Position Specific
Posterior Lattices (PSPL), and computed an aggre-
gate score for each document based on the poste-
rior probability of edges and the proximity of search
terms in the document. Mamou et al (2006) con-
verted each lattice into a word confusion network
(Mangu et al, 2000), and estimated the inverse doc-
ument frequency (idf ) of each word t as the ratio of
the total number of words in the document collection
to the total number of occurrences of t.
Despite the differences in the details, the above
lattice-based SDR methods have all been based on
the classical vector space retrieval model with tf ?idf
weighting.
2.2 Expected Counts from Lattices
A speech recognizer generates a 1-best transcript
of a spoken document by considering possible tran-
scripts of the document, and then selecting the tran-
script with the highest probability. However, unlike
a text document, such a 1-best transcript is likely to
be inexact due to speech recognition errors. To rep-
resent the uncertainty in speech recognition, and to
incorporate information from multiple transcription
hypotheses rather than only the 1-best, it is desirable
to use expected word counts from lattices output by
a speech recognizer.
In the context of spoken document search, Siegler
(1999) described expected word counts and for-
mulated a way to estimate expected word counts
from lattices based on the relative ranks of word
hypothesis probabilities; Chelba and Acero (2005)
used a more explicit formula for computing word
counts based on summing edge posterior probabili-
ties in lattices; Saraclar and Sproat (2004) performed
word-spotting in speech lattices by looking for word
occurrences whose expected counts were above a
certain threshold; and Yu et al (2005) searched for
phrases in spoken documents using a similar mea-
sure, the expected word relevance.
Expected counts have also been used to sum-
marize the phonotactics of a speech recording rep-
resented in a lattice: Hatch et al (2005) per-
formed speaker recognition by computing the ex-
pected counts of phone bigrams in a phone lattice,
and estimating an unsmoothed probability distribu-
tion of phone bigrams.
Although many uses of expected counts have been
studied, the use of statistical language models built
from expected word counts has not been well ex-
plored.
2.3 Retrieval via Statistical Language
Modeling
Finally, the statistical language modeling approach
to retrieval was used by Ponte and Croft (1998) for
IR with text documents, and it was shown to outper-
form the tf ? idf approach for this task; this method
was further improved on in Song and Croft (1999).
Chen et al (2004) applied Song and Croft?s method
to Mandarin spoken document retrieval using 1-best
ASR transcripts. In this task, it was also shown to
811
outperform tf ? idf . Thus, the statistical language
modeling approach to retrieval has been shown to be
superior to the vector space approach for both these
IR tasks.
2.4 Contributions of Our Work
The main contributions of our work include
? extending the language modeling IR approach
from text-based retrieval to lattice-based spo-
ken document retrieval; and
? formulating a method for building a statistical
language model based on expected word counts
derived from lattices.
Our method is motivated by the success of the sta-
tistical retrieval framework over the vector space ap-
proach with tf ? idf for text-based IR, as well as
for spoken document retrieval via 1-best transcripts.
Our use of expected counts differs from Saraclar and
Sproat (2004) in that we estimate probability mod-
els from the expected counts. Conceptually, our
method is close to that of Hatch et al (2005), as
both methods build a language model to summa-
rize the content of a spoken document represented
in a lattice. In practice, our method differs from
Hatch et al (2005)?s in many ways: first, we derive
word statistics for representing semantics, instead of
phone bigram statistics for representing phonotac-
tics; second, we introduce a smoothing mechanism
(Zhai and Lafferty, 2004) to the language model that
is specific for information retrieval.
3 Methods
We now describe the formulation of three different
SDR methods: a baseline statistical retrieval method
which works on 1-best transcripts, our proposed sta-
tistical lattice-based SDR method, as well as a pre-
viously published vector space lattice-based SDR
method.
3.1 Baseline Statistical Retrieval Method
Our baseline retrieval method is motivated by Song
and Croft (1999), and uses the language model
smoothing methods of Zhai and Lafferty (2004).
This method is used to perform retrieval on the docu-
ments? 1-best ASR transcripts and reference human
transcripts.
Let C be the collection of documents to retrieve
from. For each document d contained in C, and each
query q, the relevance of d to q can be defined as
Pr(d | q). This probability cannot be computed di-
rectly, but under the assumption that the prior Pr(d)
is uniform over all documents in C, we see that
Pr(d | q) = Pr(q | d) Pr(d)Pr(q) ? Pr(q | d);
This means that ranking documents by Pr(d | q) is
equivalent to ranking them by Pr(q | d), and thus
Pr(q | d) can be used to measure relevance (Berger
and Lafferty, 1999).
Now express q as a series of words drawn from
a vocabulary V = {w1, w2, ? ? ?wV }; that is, q =
q1q2 ? ? ? qK , where K is the number of words in the
query, and qi ? V for 1 ? i ? K. Then given
a unigram model derived from d which assigns a
probability Pr(w | d) to each word w in V , we can
compute Pr(q | d) as follows:
Pr(q | d) = Pr(q1q2 ? ? ? qK | d)
=
K
?
i=1
Pr(qi | d)
=
?
w?V ,
C(w|q)>0
Pr(w|d)C(w|q) (1)
where C(w | q) is the word count of w in q.
Before using Equation 1, we must estimate a uni-
gram model from d: that is, an assignment of proba-
bilities Pr(w | d) for all w ? V . One way to do this
is to use a maximum likelihood estimate (MLE) ? an
assignment of Pr(w | d) for all w which maximizes
the probability of generating d. The MLE is given
by the equation
Pr mle(w | d) =
C(w | d)
|d|
where C(w | d) is the number of occurrences of
w in d, and |d| is the total number of words in d.
However, using this formula means we will get a
value of zero for Pr(q | d) if even a single query
word qi is not found in d. To overcome this problem,
we smooth the model by assigning some probability
mass to such unseen words. Specifically, we adopt
812
a two-stage smoothing method (Zhai and Lafferty,
2004):
Pr(w | d) = (1 ? ?)C(w | d) + ?Pr(w | C)|d| + ?
+?Pr(w | U) (2)
Here, U denotes a background language model, and
? > 0 and ? ? (0, 1) are parameters to the smooth-
ing procedure. This is a combination of Bayesian
smoothing using Dirichlet priors (MacKay and Peto,
1984) and Jelinek-Mercer smoothing (Jelinek and
Mercer, 1980).
The parameter ? can be set empirically according
to the nature of the queries. For the parameter ?, we
adopt the estimation procedure of Zhai and Lafferty
(2004): we maximize the leave-one-out log likeli-
hood of the document collection, namely
??1(? | C) =
?
d?C
?
w?V
C(w | d)
log
(
C(w | d) ? 1 + ?Pr(w | C)
|d| ? 1 + ?
)
(3)
by using Newton?s method to solve the equation
???1(? | C) = 0
3.2 Our Proposed Statistical Lattice-Based
Retrieval Method
We now propose our lattice-based retrieval method.
In contrast to the above baseline method, our pro-
posed method works on the lattice representation of
spoken documents, as generated by a speech recog-
nizer.
First, each spoken document is divided into M
short speech segments. A speech recognizer then
generates a lattice for each speech segment. As
previously stated, a lattice is a connected directed
acyclic graph with edges labeled with word hypothe-
ses and likelihoods. Thus, each path through the lat-
tice contains a hypothesis of the series of words spo-
ken in this speech segment, t = t1t2 ? ? ? tN , along
with acoustic probabilities Pr(o1 | t1), Pr(o2 | t2),
? ? ? Pr(oN | tN ), where oi denotes the acoustic
observations for the time interval of the word ti
hypothesized by the speech recognizer. Let o =
o1o2 ? ? ? oN denote the acoustic observations for the
entire speech segment; then
Pr(o | t) =
N
?
i=1
Pr(oi | ti)
We then rescore each lattice with an n-gram lan-
guage model. Effectively, this means multiplying
the acoustic probabilities with n-gram probabilities:
Pr(t,o) = Pr(o | t) Pr(t)
=
N
?
i=1
Pr(oi | ti) Pr(ti | ti?n+1 ? ? ? ti?1)
This produces an expanded lattice in which paths
(hypotheses) are weighted by their posterior proba-
bilities rather than their acoustic likelihoods: specif-
ically, by Pr(t,o) ? Pr(t | o) rather than Pr(o | t)
(Odell, 1995). The lattice is then pruned, by remov-
ing those paths in the lattice whose log posterior
probabilities ? to be precise, whose ? ln Pr(t | o)
? are not within a threshold ? of the best path?s log
posterior probability (in our implementation, ? =
10000.5).
Next, we compute the expected count of each
word in each document. For each word w and each
document d comprised of M speech segments rep-
resented by M acoustic observations o(1), o(2), ? ? ?
o(M), the expected count of w in d is
E[C(w | d)] =
M
?
j=1
?
t
C(w | t) Pr(t | o(j))
where C(w | t) is the word count of w in the hy-
pothesized transcript t. We can also analogously
compute the expected document length:
E[|d|] =
M
?
j=1
?
t
|t|Pr(t | o(j))
where |t| denotes the number of words in t.
We now replace C(w | d) and |d| in Equation 2
with E[C(w | d)] and E[|d|]; thus
Pr(w | d) = (1 ? ?)E[C(w | d)] + ?Pr(w | C)E[|d|] + ?
+?Pr(w | U) (4)
In addition, we also modify the procedure for
estimating ?, by replacing C(w | d) and
813
? 0.528
? 0.472
?
 0.580
3 0.404
? 0.016
?
 0.764
? 0.236
?
 0.764
?q 0.099
?- 0.071
?? 0.066
?
? 0.673
? 0.327
Figure 1: Example of a word confusion network
|d| in Equation 3 with
?
E[C(w | d)] + 12
?
and
?
w?V
?
E[C(w | d)] + 12
?
respectively. The prob-
ability estimates from Equation 4 can then be sub-
stituted into Equation 1 to yield relevance scores.
3.3 Baseline tf ? idf Lattice-Based Retrieval
Method
As a further comparison, we also implemented
Mamou et al (2006)?s vector space retrieval method
(without query refinement via lexical affinities). In
this method, each document d is represented as
a word confusion network (WCN) (Mangu et al,
2000) ? a simplified lattice which can be viewed as
a sequence of confusion sets c1, c2, c3, ? ? ? . Each ci
corresponds approximately to a time interval in the
spoken document and contains a group of word hy-
potheses, and each word w in this group of hypothe-
ses is labeled with the probability Pr(w | ci,d) ? the
probability that w was spoken in the time interval of
ci. A confusion set may also give a probability for
Pr(? | ci,d), the probability that no word was spo-
ken in the time of ci. Figure 1 gives an example of a
WCN.
Mamou et al?s retrieval method proceeds as fol-
lows. First, the documents are divided into speech
segments, lattices are generated from the speech seg-
ments, and the lattices are pruned according to the
path probability threshold ?, as described in Sec-
tion 3.2. The lattice for each speech segment is then
converted into a WCN according to the algorithm
of Mangu et al (2000). The WCNs for the speech
segments in each document are then concatenated to
form a single WCN per document.
Now, to retrieve documents in response to a query
q, the method computes, for each document d ? C
and each word w ? V ,
? the ?document length? |d|, computed as the
number of confusion sets in the WCN of d;
? the ?average document length? avdl, computed
as
avdl = 1|C|
?
d??C
?
?d?
?
? ;
? the ?document term frequency? C?(w | d),
computed as
C?(w|d) =
?
c?occ(w,d)
(brank(w|c,d)?Pr(w|c,d))
where occ(w,d) is the set of confusion sets
in d?s WCN which contain w as a hypothe-
sis, rank(w | c,d) is the rank of w in terms
of probability within the confusion set c, and
(b1, b2, b3, ? ? ? ) = (10, 9, 8, 7, 6, 5, 4, 3, 2, 1,
0, 0, 0, ? ? ? ) is a boosting vector which serves
to discard all but the top 10 hypotheses, and
gives more weight to higher-ranked word hy-
potheses;
? the query term frequency C(w | q), which is
simply the word count of w in q; and
? the ?inverse document frequency? idf(w),
computed as
idf(w) = log OOw
where
Ow =
?
d?C
?
c?occ(w,d)
Pr(w | c,d)
O =
?
w??V
Ow?
With these, the relevance of d to q is computed as
(Carmel et al, 2001)
rel(d,q) =
P
w?V C
?(w | d) ? C(w | q) ? idf(w)
p
0.8 ? avdl + 0.2 ? |d|
4 Experiments
4.1 Document Collection
To evaluate our proposed retrieval method, we per-
formed experiments using the Hub5 Mandarin train-
ing corpus released by the Linguistic Data Consor-
tium (LDC98T26). This is a conversational tele-
phone speech corpus which is 17 hours long, and
814
contains recordings of 42 telephone calls corre-
sponding to approximately 600Kb of transcribed
Mandarin text. Each conversation has been broken
up into speech segments of less than 8 seconds each.
As the telephone calls in LDC98T26 have not
been divided neatly into ?documents?, we had to
choose a suitable unit of retrieval which could serve
as a ?document?. An entire conversation would be
too long for such a purpose, while a speech segment
or speaker turn would be too short. We decided to
use 12 -minute time windows with 50% overlap as re-
trieval units, following Abberley et al (1999) and
Tuerk et al (2001). The 42 telephone conversations
were thus divided into 4,312 retrieval units (?doc-
uments?). Each document comprises multiple con-
secutive speech segments.
4.2 Queries and Ground Truth Relevance
Judgements
We then formulated 18 queries (14 test queries, 4
development queries) to issue on the document col-
lection. Each query was comprised of one or more
written Chinese keywords. We then obtained ground
truth relevance judgements by manually examining
each of the 4,312 documents to see if it is relevant
to the topic of each query. The number of retrieval
units relevant to each query was found to range from
4 to 990. The complete list of queries and the num-
ber of documents relevant to each query are given in
Table 1.
4.3 Preprocessing of Documents and Queries
Next, we processed the document collection with a
speech recognizer. For this task we used the Abacus
system (Hon et al, 1994), a large vocabulary contin-
uous speech recognizer which contains a triphone-
based acoustic system and a frame-synchronized
search algorithm for effective word decoding. Each
Mandarin syllable was modeled by one to four tri-
phone models. Acoustic models were trained from
a corpus of 200 hours of telephony speech from
500 speakers sampled at 8kHz. For each speech
frame, we extracted a 39-dimensional feature vec-
tor consisting of 12 MFCCs and normalized en-
ergy, and their first and second order derivatives.
Sentence-based cepstral mean subtraction was ap-
plied for acoustic normalization both in the training
and testing. Each triphone was modeled by a left-
Test queries
Topic Keywords # relevant
documents
Contact information ??,Rh,??,??,v 103
Chicago z? 15
The weather ?,?,y,FZ,Z,?O,?,8?, 117
?,?,?,??
Housing matters 2,,?,2,?,?,?2, 354
??,y?,2?,?
Studies, academia ?,?,A,,?,Wt,'V, 990
1,I,?,D,3
Litigation F,F,K?,?? 31
Raising children B/,/,	?,?,???,m, 334
m?,E?
Christian churches s?, ,?,??,s?,??,?, 78
L?
Floods vy,?,?,y 4
Clothing q,?,?,F,?:g,g, 28
:F,?q,
Eating out ?,j,iq,?j,>0,,? 57
Playing sports KE,??,?|E,\E 24
Dealings with banks Uq,|,,?,TQ 54
Computers and ?,??,?G 175
software
Development queries
Topic Keywords # relevant
documents
Passport and visa ?L,?y,??,C?,I,#? 143
matters
Washington D. C. ?? 15
Working life ??,,K?,{,?*,??,??, 509
??,l,??,??,3/,?,?
1996 Olympics ???,?}L 8
Table 1: List of test and development queries
to-right 3-state hidden Markov model (HMM), each
state having 16 Gaussian mixture components. In
total, we built 1,923 untied within-syllable triphone
models for 43 Mandarin phonemes, as well as 3 si-
lence models. The search algorithm was supported
by a loop grammar of over 80,000 words.
We processed the speech segments in our collec-
tion corpus, to generate lattices incorporating acous-
tic likelihoods but not n-gram model probabilities.
We then rescored the lattices using a backoff tri-
815
gram language model interpolated in equal propor-
tions from two trigram models:
? a model built from the TDT-2, TDT-3, and
TDT-4 Mandarin news broadcast transcripts
(about 58Mb of text)
? a model built from corpora of transcripts of
conversations, comprised of a 320Kb subset of
the Callhome Mandarin corpus (LDC96T16)
and the CSTSC-Flight corpus from the Chinese
Corpus Consortium (950Kb)
The unigram counts from this model were also used
as the background language model U in Equations 2
and 4.
The reference transcripts, queries, and trigram
model training data were all segmented into words
using Low et al (2005)?s Chinese word segmenter,
trained on the Microsoft Research (MSR) corpus,
with the speech recognizer?s vocabulary used as an
external dictionary. The 1-best ASR transcripts were
decoded from the rescored lattices.
Lattice rescoring, trigram model building, WCN
generation, and computation of expected word
counts were done using the SRILM toolkit (Stolcke,
2002), while lattice pruning was done with the help
of the AT&T FSM Library (Mohri et al, 1998).
We also computed the character error rate (CER)
and syllable error rate (SER) of the 1-best tran-
scripts, and the lattice oracle CER, for one of
the telephone conversations in the speech corpus
(ma_4160). The CER was found to be 69%, the
SER 63%, and the oracle CER 29%.
4.4 Retrieval and Evaluation
We then performed retrieval on the document col-
lection using the algorithms in Section 3, using the
reference transcripts, the 1-best ASR transcripts, lat-
tices, and WCNs. We set ? = 0.1, which was sug-
gested by Zhai and Lafferty (2004) to give good re-
trieval performance for keyword queries.
The results of retrieval were checked against the
ground truth relevance judgements, and evaluated in
terms of the non-interpolated mean average preci-
sion (MAP):
MAP = 1L
L
?
i=1
?
?
1
Ri
Ri
?
j=1
j
ri,j
?
?
Retrieval Retrieval MAP for MAP for
method source development test
queries queries
Statistical Reference 0.5052 0.4798
transcripts
Statistical 1-best 0.1251 0.1364
transcripts
Vector space Lattices, 0.1685 0.1599
tf ? idf ? = 27, 500
Statistical Lattices, 0.2180 0.2154
? = 65, 000
Table 2: Summary of experimental results
where L denotes the total number of queries, Ri the
total number of documents relevant to the ith query,
and ri,j the position of the jth relevant document
in the ranked list output by the retrieval method for
query i.
For the lattice-based retrieval methods, we per-
formed retrieval with the development queries using
several values of ? between 0 and 100,000, and then
used the value of ? with the best MAP to do retrieval
with the test queries.
5 Experimental Results
The results of our experiments are summarized
in Table 2; the MAP of the two lattice-based
retrieval methods, Mamou et al (2006)?s vector
space method and our proposed statistical retrieval
method, are shown in Figure 2 and Figure 3 respec-
tively.
The results show that, for the vector space re-
trieval method, the MAP of the development queries
is highest at ? = 27, 500, at which point the MAP
for the test queries is 0.1599; and for our proposed
method, the MAP for the development queries is
highest at ? = 65, 000, and at this point the MAP
for the test queries reaches 0.2154.
As can be seen, the performance of our statistical
lattice-based method shows a marked improvement
over the MAP of 0.1364 achieved using only the 1-
best ASR transcripts, and indeed a one-tailed Stu-
dent?s t-test shows that this improvement is statisti-
cally significant at the 99.5% confidence level. The
statistical method also yields better performance
than Mamou et al?s vector space method ? a t-test
816
For 4 development queries
 0.12
 0.13
 0.14
 0.15
 0.16
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 0  20000  40000  60000  80000  100000
M
AP
? (max. log probability difference of paths)
Retrieval using word probabilities from word confusion networks
? = 27,500
For 14 test queries
 0.13
 0.14
 0.15
 0.16
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 0  20000  40000  60000  80000  100000
M
AP
? (max. log probability difference of paths)
Retrieval using word probabilities from word confusion networks
? = 27,500
Figure 2: MAP of Mamou et al (2006)?s vector
space method for lattice-based retrieval, at various
pruning thresholds ?
shows the performance difference to be statistically
significant at the 97.5% confidence level.
6 Conclusions and Future Work
We have presented a method for performing spo-
ken document retrieval using lattices which is based
on a statistical language modeling retrieval frame-
work. Results show that our new method can sig-
nificantly improve the retrieval MAP compared to
using only the 1-best ASR transcripts. Also, our
proposed retrieval method has been shown to out-
perform Mamou et al (2006)?s vector space lattice-
based retrieval method.
Besides the better empirical performance, our
method also has other advantages over Mamou et
al.?s vector space method. For one, our method com-
putes expected word counts directly from rescored
lattices, and does not require an additional step to
For 4 development queries
 0.12
 0.13
 0.14
 0.15
 0.16
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 0  20000  40000  60000  80000  100000
M
AP
? (max. log probability difference of paths)
Retrieval using expected counts from lattices
Retrieval using 1?best transcripts
? = 65,000
For 14 test queries
 0.13
 0.14
 0.15
 0.16
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 0  20000  40000  60000  80000  100000
M
AP
? (max. log probability difference of paths)
Retrieval using expected counts from lattices
Retrieval using 1?best transcripts
? = 65,000
Figure 3: MAP of our proposed statistical method
for lattice-based retrieval, at various pruning thresh-
olds ?
convert lattices lossily to WCNs. Furthermore, our
method uses all the hypotheses in each lattice, rather
than just the top 10 word hypotheses at each time
interval. Most importantly, our method provides
a more natural and more principled approach to
lattice-based spoken document retrieval based on a
sound statistical foundation, by harnessing the fact
that lattices are themselves statistical models; the
statistical approach also means that our method can
be more easily augmented with additional statistical
knowledge sources in a principled way.
For future work, we plan to test our proposed
method on English speech corpora, and with larger-
scale retrieval tasks involving more queries and
more documents. We would like to extend our
method to other speech processing tasks, such as
spoken document classification and example-based
spoken document retrieval as well.
817
References
Dave Abberley, David Kirby, Steve Renals, and Tony
Robinson. 1999. The THISL broadcast news retrieval
system. In Proceedings of ESCA ETRW Workshop on
Accessing Information in Spoken Audio, pages 14?19.
Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proceedings of SI-
GIR 1999, pages 222?229.
David Carmel, Einat Amitay, Miki Herscovici, Yoelle
Maarek, Yael Petruschka, and Aya Soffer. 2001. Juru
at TREC 10 ? Experiments with index pruning. In
Proceedings of the Tenth Text Retrieval Conference
(TREC-10), pages 228?236.
Ciprian Chelba and Alex Acero. 2005. Position specific
posterior lattices for indexing speech. In Proceedings
of ACL 2005, pages 443?450.
Berlin Chen, Hsin-min Wang, and Lin-shan Lee. 2004. A
discriminative HMM/n-gram-based retrieval approach
for Mandarin spoken documents. ACM Transactions
on Asian Language Information Processing, 3(2):128?
145.
Andrew O. Hatch, Barbara Peskin, and Andreas Stol-
cke. 2005. Improved phonetic speaker recognition us-
ing lattice decoding. In Proceedings of IEEE ICASSP
2005, 1:169?172.
Hsiao-Wuen Hon, Baosheng Yuan, Yen-Lu Chow, S.
Narayan, and Kai-Fu Lee. 1994. Towards large vocab-
ulary Mandarin Chinese speech recognition. In Pro-
ceedings of IEEE ICASSP 1994, 1:545?548.
David Anthony James and Steve J. Young. 1994. A
fast lattice-based approach to vocabulary independent
wordspotting. In Proceedings of ICASSP 1994, 1:377?
380.
David Anthony James. 1995. The Application of Classi-
cal Information Retrieval Techniques to Spoken Docu-
ments. Ph. D. thesis, University of Cambridge.
Frederick Jelinek and Robert L. Mercer. 1980. Interpo-
lated estimation of Markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
tern Recognition in Practice, pages 381?397.
Gareth J. F. Jones, Jonathan T. Foote, Karen Sp?rck
Jones, and Steve J. Young. 1996. Retrieving spoken
documents by combining multiple index sources. In
Proceedings of SIGIR 1996, pages 30?38.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A
maximum entropy approach to Chinese word segmen-
tation. In Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing, pages 161?
164.
David J. C. MacKay and Linda C. Bauman Peto. 1994,
A hierarchical Dirichlet language model. Natural Lan-
guage Engineering, 1(3):1?19.
Jonathan Mamou, David Carmel, and Ron Hoory. 2006.
Spoken document retrieval from call-center conversa-
tions. In Proceedings of SIGIR 2006, pages 51?58.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Ri-
ley. 1998. A rational design for a weighted finite-state
transducer library. Lecture Notes in Computer Science,
1436:144?158.
National Institute of Standards and Technol-
ogy. 2000. TREC-9 SDR Track web site.
www.nist.gov/speech/tests/sdr/sdr2000/sdr2000.htm.
Julian James Odell. 1995. The Use of Context in Large
Vocabulary Speech Recognition. Ph. D. thesis, Cam-
bridge University Engineering Department.
Jay M. Ponte and W. Bruce Croft. 1998. A language mod-
eling approach to information retrieval. In Proceedings
of SIGIR 1998, pages 275?281.
Murat Saraclar and Richard Sproat. 2004. Lattice-based
search for spoken utterance retrieval. In Proceedings
of HLT-NAACL 2004, pages 129?136.
Matthew A. Siegler. 1999. Integration of Continuous
Speech Recognition and Information Retrieval for Mu-
tually Optimal Performance. Ph. D. thesis, Carnegie
Mellon University.
Fei Song and W. Bruce Croft. 1999. A general lan-
guage model for information retrieval. In Proceedings
of CIKM 1999, pages 316?321.
Andreas Stolcke. 2002. SRILM ? An extensible language
modeling toolkit. In Proceedings of ICSLP, 2:901?
904.
Andy Tuerk, Sue E. Johnson, Pierre Jourlin, Karen
Sp?rck Jones, and Philip C. Woodland. 2001. The
Cambridge University multimedia document retrieval
demo system. International Journal of Speech Tech-
nology, 4(3?4):241?250.
Peng Yu, Kaijiang Chen, Lie Lu, and Frank Seide.
2005. Searching the audio notebook: keyword
search in recorded conversations. In Proceedings of
HLT/EMNLP 2005, pages 947?954.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to in-
formation retrieval. ACM Transactions on Information
Systems, 22(2):179?214.
818
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 698?707,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Tree Kernel-based SVM with Structured Syntactic Know-
ledge for BTG-based Phrase Reordering 
 
 
Min Zhang             Haizhou Li  
Institute for Infocomm Research  
1 Fusionopolis Way,#21-01 Connexis (South Tower) 
Singapore 138632 
{mzhang,hli}@i2r.a-star.edu.sg 
 
 
  
 
Abstract 
Structured syntactic knowledge is important 
for phrase reordering. This paper proposes us-
ing convolution tree kernel over source parse 
tree to model structured syntactic knowledge 
for BTG-based phrase reordering in the con-
text of statistical machine translation. Our 
study reveals that the structured syntactic fea-
tures over the source phrases are very effective 
for BTG constraint-based phrase reordering 
and those features can be well captured by the 
tree kernel. We further combine the structured 
features and other commonly-used linear fea-
tures into a composite kernel. Experimental re-
sults on the NIST MT-2005 Chinese-English 
translation tasks show that our proposed 
phrase reordering model statistically signifi-
cantly outperforms the baseline methods. 
1 Introduction  
Phrase-based method (Koehn et al, 2003; Och 
and Ney, 2004; Koehn et al, 2007) and syntax-
based method (Wu, 1997; Yamada and Knight, 
2001; Eisner, 2003; Chiang, 2005; Cowan et al, 
2006; Marcu et al, 2006; Liu et al, 2007; Zhang 
et al, 2007c, 2008a, 2008b; Shen et al, 2008; Mi 
and Huang, 2008) represent the state-of-the-art 
technologies in statistical machine translation 
(SMT). As the two technologies are complemen-
tary in many ways, an interesting research topic 
is how to combine the strengths of the two me-
thods. Many research efforts have been made to 
address this issue, which can be summarized into 
two ideas. One is to add syntax into phrase-based 
model while another one is to enhance syntax-
based model to handle non-syntactic phrases. In 
this paper, we bring forward the first idea by 
studying the issue of how to utilize structured 
syntactic features for phrase reordering in a 
phrase-based SMT system with BTG (Bracketing 
Transduction Grammar) constraints (Wu, 1997). 
Word and phrase reordering is a crucial com-
ponent in a SMT system. In syntax-based method, 
word reordering is implicitly addressed by trans-
lation rules, thus the performance is subject to 
parsing errors to a large extent (zhang et al, 
2007a) and the impact of syntax on reordering is 
difficult to single out (Li et al, 2007). In phrase-
based method, local word reordering1 can be ef-
fectively captured by phrase pairs directly while 
local phrase reordering is explicitly modeled by 
phrase reordering model and distortion model. 
Recently, many phrase reordering methods have 
been proposed, ranging from simple distance-
based distortion model (Koehn  et al, 2003; Och 
and Ney, 2004), flat reordering model (Wu, 1997; 
Zens et al, 2004), lexicalized reordering model 
(Tillmann, 2004; Kumar and Byrne, 2005), to 
hierarchical phrase-based model (Chiang, 2005; 
Setiawan et al, 2007) and classifier-based reor-
dering model with linear features (Zens and Ney, 
2006; Xiong et al, 2006; Zhang et al, 2007a; 
Xiong et al, 2008). However, one of the major 
limitations of these advances is the structured 
syntactic knowledge, which is important to glob-
al reordering (Li et al, 2007; Elming, 2008), has 
not been well exploited. This makes the phrase-
based method particularly weak in handling 
global phrase reordering. From machine learning 
viewpoint (Vapnik, 1995), it is computationally 
infeasible to explicitly generate features involv-
ing structured information in many NLP applica-
                                                 
1 This paper follows the term convention of global reorder-
ing and local reordering of Li et al (2007), between which 
the distinction is solely defined by reordering distance 
(whether beyond four source words) (Li et al, 2007). 
698
tions. For example, one cannot enumerate effi-
ciently all the sub-tree features for a full parse 
tree. This would be the reason why structured 
features are not fully utilized in previous statis-
tical feature-based phrase reordering model. 
Thanks to the nice property of kernel-based 
machine learning method that can implicitly ex-
plore (structured) features in a high dimensional 
feature space (Vapnik, 1995), in this paper we 
propose using convolution tree kernel (Haussler, 
1999; Collins and Duffy, 2001) to explore the 
structured syntactic knowledge for phrase reor-
dering and further combine the tree kernel with 
other diverse linear features into a composite 
kernel to strengthen the model?s predictive abili-
ty. Indeed, using tree kernel methods to mine 
structured knowledge has shown success in some 
NLP applications like parsing (Collins and Duffy, 
2001), semantic role labeling (Moschitti, 2004; 
Zhang et al, 2007b), relation extraction (Zhang 
et al, 2006), pronoun resolution (Yang et al, 
2006) and question classification (Zhang and 
Lee, 2003). However, to our knowledge, such 
technique still remains unexplored for phrase 
reordering. 
In this paper, we look into the phrase reorder-
ing problem in two aspects: 1) how to model and 
optimize structured features, and 2) how to com-
bine the structured features with other linear fea-
tures and further integrate them into the log-
linear model-based translation framework. Our 
study shows that: 1) the structured syntactic fea-
tures are very useful and 2) our kernel-based 
model can well explore diverse knowledge, in-
cluding previously-used linear features and the 
structured syntactic features, for phrase reorder-
ing. Our model displays one advantage over the 
previous work that it is able to utilize the struc-
tured syntactic features without the need for ex-
tensive feature engineering in decoding a parse 
tree into a set of linear syntactic features. 
To have a more insightful evaluation, we de-
sign three experiments with three different eval-
uation metrics. Experimental results on the NIST 
MT-2005 Chinese-English translation tasks show 
that our method statistically significantly outper-
forms the baseline methods in term of the three 
different evaluation metrics. 
The rest of the paper is organized as follows. 
Section 2 introduces the baseline method of 
BTG-based phrase translation method while sec-
tion 3 discusses the proposed method in detail. 
The experimental results are reported and dis-
cussed in section 4. Finally, we conclude the pa-
per in section 5. 
2 Baseline System and Method 
We use the MaxEnt-based BTG translation sys-
tem (Xiong et al, 2006) as our baseline. It is a 
phrase-based SMT system with BTG reordering 
constraint. The system uses the BTG lexical 
translation rules ( ? ? ?/? ) to translate the 
source phrase ?  into target phrase ? , and the 
BTG merging rules (? ? ??, ??| ? ?, ? ? ) to 
combine two neighboring phrases with a straight 
or inverted order. In the translation model, the 
BTG lexical rules are weighted with several fea-
tures, such as phrase translation, word penalty 
and language models, in a log-linear form. With 
the BTG constraint, the reordering model ? is 
defined on the two neighboring phrases ??  and 
?? and their order ? ? ?????????, ????????? as 
follows: 
? ? f(?, ??, ??)                                  (1) 
In the baseline system, a MaxEnt-based clas-
sifier with boundary words of the two neighbor-
ing phrases as features is used to model the 
merging/reordering order. The baseline MaxEnt-
based reordering model is formulized as follows: 
? ? ??(?|??, ??) ? ???(? ????(?,?
?,??))?
? ???(? ????(?,??,??))??     (2) 
where the functions  ??(?, ??, ??) ? ?0,1?  are 
model feature functions using the boundary 
words of the two neighboring phrases as features, 
and ?? are feature weights that are trained based 
on the MaxEnt-based criteria. 
3 Tree Kernel-based Phrase Reordering 
Model  
3.1 Kernel-based Classifier Solution to 
Phrase Reordering 
In this paper, phrase reordering is recast as a 
classification issue as done in previous work 
(Xiong et al, 2006 & 2008; Zhang et al, 2007a). 
In training, we use a machine learning algorithm 
training on the annotated phrase reordering in-
stances that are automatically extracted from 
word-aligned, source sentence parsed training 
corpus, to learn a classifier. In testing (decoding), 
the learned classifier is applied to two adjacent 
source phrases to decide whether they should be 
merged (straight) or reordered (inverted) and 
what their probabilities are, and then these prob-
abilities are used as one feature in the log-linear 
model in a phrase-based decoder. 
In addition to the previously-used linear fea-
tures, we are more interested in the value of 
structured syntax in phrase reordering and how 
to capture it using kernel methods. However, not 
699
all classifiers are able to work with kernel me-
thods. Only those dot-product-based classifiers 
can work with kernels by replacing the dot prod-
uct with a kernel function, where the kernel func-
tion is able to directly calculate the similarity 
between two (structured) objects without enume-
rating them into linear feature vectors. In this 
paper, we select SVM as our classifier. In this 
section, we first define the structured syntactic 
features and introduce the commonly used linear 
features, and then discuss how to utilize these 
features by kernel methods together SVM for 
phrase reordering 
3.2 Structured Syntactic Features 
A reordering instance ? ? ???, ??? (see Eq.1) in 
this paper refers to two adjacent source phrases 
??  and ?? to be translated. The structured syn-
tactic feature spaces of a reordering instance are 
defined as the portion of a parse tree of the 
source sentence that at least covers the span of 
the reordering instance (i.e. the two neighboring 
phrases). The syntactic features are defined as all 
 
T1) Minimum Sub-Tree (MST) 
                               
T2) Minimum Sub-Structure (MSS)                T4) Chunking Tree (CT) 
 
 
 
T3) Context-sensitive Minimum Sub-Structure (CMSS) 
 
Figure 1. Different representations of structured syntactic features of a reordering instance in the example 
sentence excerpted from our training corpus ????/build  ??/scale??/mighty ?/of ??/various 
types ??/qualified personnel  ??/contingent ??/above all  ??/urgently  ??/necessary ??
/central authorities  ??/overall  ??/planning?(To build a mighty contingent of qualified personnel of 
various types, it is necessary, above all, for the central authorities to make overall planning.) ?, where ??
?/various types ??/qualified personnel  ??/contingent (contingent of qualified personnel of various 
types)? is the 1st/left phrase and ???/above all  ??/urgent  ??/necessary (it is necessary, above all, 
?)? is the 2nd/right phrase. Note that different function tags are attached to the grammar tag of each inter-
nal node. 
700
the possible subtrees in the structured feature 
spaces. We can see that the structured feature 
spaces and their features are encapsulated by a 
full parse tree of source sentences. Thus, it is 
critical to understand which portion of a parse 
tree (i.e. structured feature space) is the most ef-
fective to represent a reordering instance. Moti-
vated by the work of (Zhang et al, 2006), we 
here examine four cases that contain different 
sub-structures as shown in Fig. 1. 
 
(1) Minimum Sub-Tree (MST): the sub-tree 
rooted by the nearest common ancestor of the 
two phrases. This feature records the minimum 
sub-structure covering the two phrases and its 
left and right contexts as shown in Fig 1.T1. 
(2) Minimum Sub-Structure (MSS): the smal-
lest common sub-structure covering the two 
phrases. It is enclosed by the shortest path link-
ing the two phrases. Thus, its leaf nodes exactly 
consist of all the phrasal words. 
(3) Context-sensitive Minimum Sub-Structure 
(CMSS): the MSS extending with the 1st left 
sibling node of the left phrase and the 1st right 
sibling node of the right phrase and their descen-
dants. If sibling is unavailable, then we move to 
the parent of current node and repeat the same 
process until the sibling is available or the root of 
the MST is reached. 
(4) Chunking Tree (CT): the base phrase list 
extracted from the MSS. We prune out all the 
internal structures of the MSS and only keep the 
root node and the base phrase list for generating 
the chunking tree. 
Fig. 1 illustrates the different representations 
of an example reordering instance. T1 is the MST 
for the example instance, where the sub-structure 
circled by a dotted line is the MSS, which is also 
shown in T2 for clarity. We can see that the MSS 
is a subset of the MST. By T2 we would like to 
evaluate whether the structured information is 
effective for phrase reordering while by compar-
ing the system performance when using T1 and 
T2, we would like to evaluate whether the struc-
tured context information embedded in the MST 
is useful to phrase reordering. T3 is the CMSS, 
where the two sub-structures circled by dotted 
lines are included as the context to T2 and make 
T3 limited context-sensitive. This is to evaluate 
whether the limited context information in the 
CMSS is helpful. By comparing the performance 
of T1 and T3, we would like to see whether the 
larger context in T1 is a noisy feature. T4 is the 
CT, where only the basic structured information 
is kept. By comparing the performance of T2 and 
T4, we would like to study whether the high-level 
structured syntactic features in T2 are useful to 
phrase reordering. 
After defining the four structured feature 
spaces, we further partition each feature space 
into five parts according to their functionalities. 
Because it only makes sense to evaluate two par-
titions of the same functionality between two 
reordering instances, the feature space partition 
leads to a more precise similarity calculation. As 
shown in Fig 1, all the internal nodes in each par-
tition are labeled with a unique function tag in 
the following way: 
? Left Context (-lc): nodes in this partition 
do not cover any phrase word and they are 
all in the left of the left phrase. 
? Right Context (-rc): nodes in this partition 
do not cover any phrase word and they are 
all in the right of the right phrase. 
? Left Phrase (-lp): nodes in this partition 
only cover the first phrase and/or its left 
context. 
? Right Phrase (-rp): nodes in this partition 
only cover the second phrase and/or its right 
context. 
? Shared Part (-sp): nodes in this partition at 
least cover both of the two phrases partially. 
No lexical word is tagged since it is not a part 
of the structured features, and therefore not par-
ticipating in the tree kernel computing. 
3.3 Linear Features 
In our study, we define the following lexicalized 
linear features which are easily to be extracted 
and integrated to our composite kernel: 
? Leftmost and rightmost boundary words of 
the left and right source phrases 
? Leftmost and rightmost boundary words of 
the left and right target phrases 
? Internal words of the four phrases (exclud-
ing boundary words) 
? Target language model (LM) score differ-
ence  (monotone-inverted) 
In total, we arrive at 13 features, including 8 
boundary word features, 4 (kinds of) internal 
word features and 1 LM feature. The first 12 fea-
tures have been proven useful (Xiong et al, 
2006; Zhang et al, 2007a) to phrase reordering. 
LM score is certainly a strong evidence for mod-
eling word orders and lexical selection. Although 
it is already used as a standalone feature in the 
log-linear model, we also would like to explicitly 
re-optimize it together with other reordering fea-
tures in our reordering model. 
701
3.4 Tree Kernel, Composite Kernel and In-
tegrating into our Reordering Model 
As discussed before, we use convolution tree 
kernel to capture the structured syntactic feature 
implicitly by directly computing similarity be-
tween the parse-tree representations of two reor-
dering instances with explicitly enumerating all 
the features one by one. In convolution tree ker-
nel (Collins and Duffy, 2001), a parse tree T  is 
implicitly represented by a vector of integer 
counts of each sub-tree type (regardless of its 
ancestors): 
 
( )T? = (# subtree1(T), ?, # subtreen(T))  
where # subtreei(T) is the occurrence number of 
the ith sub-tree type (subtreei) in T. Since the 
number of different sub-trees is exponential with 
the parse tree size, it is computationally infeasi-
ble to directly use the feature vector ( )T? . To 
solve this computational issue, Collins and Duffy 
(2001) proposed the following parse tree kernel 
to calculate the dot product between the above 
high dimensional vectors implicitly. 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
 ( ) ( )
 ( , )
(( ) ( ))
i isubtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
=< >
=
= ?
?? ? ?
? ?
 
where N1 and N2 are the sets of nodes in trees T1 
and T2, respectively, and ( )
isubtree
I n  is a function 
that is 1 iff the subtreei occurs with root at node n 
and zero otherwise, and 1 2( , )n n?  is the number of 
the common subtrees rooted at n1 and n2, i.e., 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? = ??  
1 2( , )n n? can be further computed efficiently by 
the following recursive rules: 
Rule 1: if the productions (CFG rules) at 1n  and 
2n  are different, 1 2( , ) 0n n? = ; 
Rule 2: else if both 1n  and 2n  are pre-terminals 
(POS tags), 1 2( , ) 1n n ?? = ? ; 
Rule 3: else,  
1( )
1 2 1 21
( , ) (1 ( ( , ), ( , )))
nc n
j
n n ch n j ch n j?
=
? = + ?? ,  
where 1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and ? (0< ? <1) is the de-
cay factor in order to make the kernel value less 
variable with respect to the subtree sizes. In ad-
dition, the recursive Rule 3 holds because given 
two nodes with the same children, one can con-
struct common sub-trees using these children and 
common sub-trees of further offspring. The time 
complexity for computing this kernel is
1 2(| | | |)O N N? and in practice in near to linear 
computational time without the need of enume-
rating all subtree features.  
In our study, the linear feature-based similarity 
is simply calculated using dot-product. We then 
define the following composite kernel to com-
bine the structured features-based and the linear 
features-based similarities:  
 
??(??, ??) ? ? ? ??(??, ??) ? (1 ? ?) ? ??(??, ??) (3) 
 
where Kt is the tree kernel over the structured 
features and Kl is the linear kernel (dot-product) 
over the linear features. The composite kernel Kc 
is a linear combination of the two individual ker-
nels, where the coefficient ? is set to its default 
value 0.3 as that in Moschitti (2004)?s implemen-
tation. The kernels return the similarities be-
tween two reordering instances based on their 
features used. Our basic assumption is, the more 
similar the two reordering instances of x1 and x2 
are, the more chance they share the same order. 
Now let us see how to integrate the kernel 
functions into SVM. The linear classifier learned 
by SVM is formulized as: 
( ) sgn( )i i iif x y a x x b= ? +?                    (4) 
where ia is the weight of a support vector ix (i.e., 
a support reordering instance ?? ? ???, ???in our 
study), iy  is its class label (1:  ????????  or -
1: ???????? in our study) and b is the intercept 
of the hyperplane. An input reordering instance x
is classified as positive (negative) if ( )f x >0 (
( )f x <0). 
Based on the linear classifier, a kernelized 
SVM can be easily implemented by simply re-
placing the dot product ix x? in Eq (4) with a 
kernel function ( , )iK x x . Thus, the kernelized 
SVM classifier is formulated as: 
( ) sgn( ( , ) )i i iif x y a K x x b= +?                 (5) 
where ( , )iK x x is either ( , )c iK x x , ( , )t iK x x or  
( , )l iK x x in our study. Following Eq (1), our 
reordering model (implemented by the kerne-
lized SVM) can be formulized as follows: 
 
? ? f(?, ??, ??) ? ????(?|? ? ???, ???) 
? ???(? (?????)?, ??) ? ?)? )                   (6)  
 
A reordering instance x is classified as straight 
(or inverted) if ????(?|?) ? 0 (or ????(?|?) ?
0). Eq (6) and Eq (2) show the difference be-
tween our kernalized SVM-based reordering 
702
model and the MaxEnt-based reordering model. 
The main difference between them lies in that 
our model is able to utilize structured syntactic 
features by kernalized SVM while the previous 
work can only use lexicalized word features by 
MaxEnt-based classifier.  
Finally, because the return value of  
????(?|?)  is a distance function rather than a 
probability, we use a sigmoid function to convert 
????(?|?) to a posterior probability as shown 
using the following to functions and apply it as 
one feature to the log-linear model in the decod-
ing.  
( | )
1
( | )
1 svmp o x
P straight x
e?
=
+
    and  
( | )
1
( | )
1 svmp o x
P inverted x
e
=
+
 
where straight represents a positive instance and 
inverted represents a negative instance. 
4 Experiments and Discussion 
4.1 Experimental Settings 
Basic Settings: we evaluate our method on Chi-
nese-English translation task. We use the FBIS 
corpus as training set, the NIST MT-2002 test set 
as development (dev) set and the NIST MT-2005 
test set as test set. The Stanford parser (Klein and 
Manning, 2003) is used to parse Chinese sen-
tences on the training, dev and test sets. GIZA++ 
(Och and Ney, 2004) and the heuristics ?grow-
diag-final-and? are used to generate m-to-n word 
alignments. The translation model is trained on 
the FBIS corpus and a 4-gram language model is 
trained on the Xinhua portion of the English Gi-
gaword corpus using the SRILM Toolkits 
(Stolcke, 2002) with modified Kneser-Ney 
smoothing (Kenser and Ney, 1995). For the 
MER training (Och, 2003), we modify Koehn?s 
MER trainer (Koehn, 2004) to train our system. 
For significance test, we use Zhang et als im-
plementation (Zhang et al 2004). 
Baseline Systems: we set three baseline sys-
tems: B1) Moses (Koehn et al, 2007) that uses 
lexicalized unigram reordering model to predict 
three orientations: monotone, swap and discon-
tinuous; B2) MaxEnt-based reordering model 
with lexical boundary word features only (Xiong 
et al, 2006); B3) Linguistically annotated reor-
dering model for BTG-based (LABTG) SMT 
(Xiong et al, 2008). For Moses, we used the de-
fault settings. We build a CKY-style decoder and 
integrate the corresponding reordering modelling 
methods into the decoder to implement the 2nd 
and the 3rd baseline systems and our system. Ex-
cept reordering models, all the four systems use 
the same features in translation model, language 
model and distortion model as Moses in the log-
linear framework. We tune the four systems us-
ing the strategies as discussed previously in this 
section. 
Reordering Model Training: we extract all 
reordering instances from the m-to-n word-
aligned training corpus. The reordering instances 
include the two source phrases, two target phras-
es, order label and its corresponding parse tree. 
We generate the boundary word features from 
the extracted reordering instances in the same 
way as discussed in Xiong et al (2006) and use 
Zhang?s MaxEnt Tools 2  to train a reordering 
model for the 2nd baseline system. Similarly, we 
use the algorithm 1 in Xiong et al (2008) to ex-
tract features and use the same MaxEnt Tools to 
train a reordering model for the 3rd baseline sys-
tem. Based on the extracted reordering instances, 
we generate the four structured features and the 
linear features, and then use the Tree Kernel 
Tools (Moschitti, 2004) to train our kernel-based 
reordering model (linear, tree and composite). 
Experimental Design and Evaluation Met-
rics: we design three experiments and evaluate 
them using three metrics.  
 Classification-based: in the first experiment, 
we extract all reordering instances and their fea-
tures from the dev and test sets, and then use the 
reordering models trained on the training set to 
classify (label) those instances extracted from the 
dev and test sets. In this way, we can isolate the 
reordering problem from the influence of others, 
such as translation model, pruning and decoding 
strategies, to better examine the reordering mod-
els? ability and to give analytical insights into the 
features. Classification Accuracy (CAcc), the 
percentage of the correctly labeled instances over 
all trials, is used as the evaluation metric.  
Forced decoding3-based and normal decoding-
based: the two experiments evaluate the reorder-
ing models through a real SMT system. The 
reordering model and the language model are the 
same in the two experiments. However, in forced 
decoding, we train two translation models, one 
using training data only while another using both 
                                                 
2 http://homepages.inf.ed.ac.uk/s0450736/maxent.html 
3 A normal SMT decoder filters a translation model accord-
ing to the source sentences, whereas in forced decoding, a 
translation model is filtered based on both source sentence 
and target references. In other words, in forced decoding, 
the decoder is forced to use those phrases whose translations 
are already in the references. 
703
training, dev and test data. By forced decoding, 
we aim to isolate the reordering problem from 
those of OOV and lexical selections resulting 
from imperfect translation model in the context 
of a real SMT task. Besides the the case-sensitive 
BLEU-4 (Papineni et al, 2002) used in the two 
experiments, we design another evaluation me-
trics Reordering Accuracy (RAcc) for forced de-
coding evaluation. RAcc is the percentage of the 
adjacent word pairs with correct word order 4 
over all words in one-best translation results. 
Similar to BLEU score, we also use the similar 
Brevity Penalty BP (Papineni et al, 2002) to pe-
nalize the short translations in computing RAcc. 
Finally, please note for the three evaluation me-
trics, the higher values represent better perfor-
mance. 
 
Feature Spaces CAcc (%) 
Dev Test 
Minimum Sub-Tree (MST) 89.87 89.92
Minimum Sub-Structure (MSS) 87.95 87.88
Context-Sensitive MSS (CMSS) 89.11 89.01
Chunking Tree (CT) 86.17 86.21
Linear Features (Kl) 90.79 90.46
Kl w/o using LM feature (Kl-LM) 84.24 84.06
Composite Kernel (Kc: MST+Kl) 92.98 92.67
MST w/o the 5 function tags 86.94 87.03
All are straight (monotonic) 78.92 78.67
 
Table 1: Performance of our methods on the 
dev and test sets with different feature combi-
nations 
4.2 Experimental Results 
Classification of Instances: Table 1 reports the 
performance of our defined four structured fea-
tures, linear feature and the composite kernel. 
The results are summarized as follows. 
The last row reports the performance without 
using any reordering features. We just suppose 
that all the translations are monotonic, no reor-
dering happens. The CAccs of 78.92% and 78.67% 
serve as the bottom line in our study. Compared 
with the bottom line, the tree kernels over the 4 
structured features are very effective for phrase 
                                                 
4 An adjacent word pair wiwi+1 in a translation have correct 
word order if and only if wi appears before wi+1 in transla-
tion references. Note than the two words may not be adja-
cent in the references even if they have correct word order. 
reordering since only structured information is 
used in the tree kernel5. 
The CTs performs the worst among the 4 
structured features. This suggests that the middle 
and high-level structures beyond base phrases are 
very useful for phrase reordering. The MSSs 
show lower performance than the CMSSs and 
the MSTs achieve the best performance. This 
clearly indicates that the structured context in-
formation is useful for phrase reordering. For this 
reason, the subsequent discussions are focused 
on the MSTs, unless otherwise specified. The 
MSSs without using the 5 function tags perform 
much worse than the original ones. This suggests 
that the partitions of the structured feature spaces 
are very helpful, which can effectively avoid the 
undesired matching between partitions of differ-
ent functionalities. Comparison of Kl and Kl-LM 
shows the LM plays an important role in phrase 
reordering. The composite kernel (Kc) performs 
much better than the two individual kernels. This 
suggests that the structured and linear features 
are complementary and the composite kernel can 
well integrate them for phrase reordering. 
 
Methods CAcc (%) 
Dev Test 
Minimum Sub-Tree (MST) 89.87 89.92
Linear Features (Kl) 90.79 90.46
Composite Kernel (Kc: MST+Kl) 92.98 92.67
MaxEnt+boundary word (B2) 88.33 86.97
MaxEnt+linguistic features (B3_1) 84.83 83.92
MaxEnt+LABTG (B3: B2+ B3_1) 88.82 88.18
 
Table 2: Performance comparison of different me-
thods 
 
Table 2 compares the performance of the base-
line methods with ours. Comparison between 
B3_1 and MST clearly demonstrates that the 
structured syntactic features are much more ef-
fective than the linear syntactic features that are 
manually extracted via heuristics. It also suggests 
that the tree kernel can well capture the struc-
tured features implicitly. Kl outperforms B2. This 
is mainly due to the contribution of LM features. 
B2 (MaxEnt-based) significantly outperforms Kl-
LM in Table 1 (SVM-based). This suggests that 
phrase reordering may not be a good linearly bi-
nary-separable task if only boundary word fea-
tures are used. Our composite kernel (Kc) signifi-
cantly outperforms LABTG (B3). This mainly 
                                                 
5 The tree kernel algorithm only compares internal struc-
tures. It does not concern any lexical leaf nodes.   
704
attributes to the contributions of structured syn-
tactic features, LM and the tree kernel. 
 
Forced Decoding: Table 3 compares the per-
formance of our composite kernel with that of 
the LABTG (Baseline 3) in forced decoding. As 
discussed before, here we try two translation 
models.  
The composite kernel outperforms the 
LABTG in all test cases. This further validates 
the effectiveness of the kernel methods in phrase 
reordering. There are still around 30% words 
reordered incorrectly even if we use the transla-
tion model trained on both training, dev and test 
sets. This reveals the limitations of current SMT 
modeling methods and suggests interesting fu-
ture work in this area. The source language 
OOV6 rate in forced decoding (13.6%) is much 
higher that in normal decoding (6.22%, see table 
4). This is mainly due to the fact that the phrase 
table in forced decoding is filtered out based on 
both source and target languages while in normal 
decoding it is based on source language only. As 
a result, more phrases are filtered out in the 
forced decoding. There is 1.4% OOV even if the 
translation model is trained on the test set. This is 
due to the incorrect word alignment, large-span 
word alignment and different English tokeniza-
tion strategies used in BLEU-scoring tool and 
ours. 
 
Methods Test Set (%) 
RAcc OOV BLEU
Composite Kernel (Kc) 
  +translation model on 
   Training, dev and test 
51.03 
72.67 
 
13.6 
1.41 
 
38.56 
62.87 
 
MaxEnt+LABTG (B3) 
  +translation model on  
    training, dev and test 
48.96 
71.45 
 
13.6 
1.41 
 
37.32 
62.14 
 
 
Table 3: Performance comparison of forced de-
coding 
 
Methods Test Set 
 BLEU(%) OOV(%)
Composite Kernel (Kc) 27.65 6.26 
Moses (B1) 25.71 6.17 
MaxEnt+boundary word(B2) 25.99 6.22 
MaxEnt+LABTG (B3) 26.63 6.22 
 
Table 4: Performance comparison 
 
                                                 
6 OOV means a source words has no any English translation 
according to the translation model. OOV rate is the percent-
age of the number of OOV words over all the source words.  
Normal Decoding/Translation: Table 4 reports 
the translation performance of our system and 
the three baseline systems. 
Moses (B1) and the MaxEnt-based boundary 
word model (B2) achieve comparable perfor-
mance. This means the lexicalized orientation-
based reordering model in Moses performs simi-
larly to the boundary word-based reordering 
model since the two models are both lexical 
word-based. However, theoretically, the Max-
Ent-based model may suffer less from data 
sparseness issue since it does not depends on 
internal phrasal words and uses MaxEnt to op-
timize feature weights while the orientation-
based model uses relative frequency of the entire 
phrases to compute the posterior probabilities. s. 
The MaxEnt-based LABTG model significantly 
outperforms (p<0.05) the MaxEnt-based boun-
dary word model and the lexicalized orientation-
based reordering model. This indicates that the 
linearly linguistically syntactic information is a 
useful feature to phrase reordering. 
Our composite kernel-based model signifi-
cantly outperforms (p<0.01) the three baseline 
methods. This again proves that the structured 
syntactic features are much more effective than 
the linear syntactic features for phrase reordering 
and the tree kernel method can well capture the 
informative structured features. The four me-
thods show very slight difference in OOV rates. 
This is mainly due to the difference in implemen-
tation detail, such as different OOV penalties and 
other pruning thresholds.  
5 Conclusion and Future Work 
Structured syntactic knowledge is very useful to 
phrase reordering. This paper provides insights 
into how the structured feature can be used for 
phrase reordering. In previous work, the struc-
tured features are selected manually by heuristics 
and represented by a linear feature vector. This 
may largely compromise the contribution of the 
structured features to phrase reordering. Thanks 
to the nice properties of kernel-based learning 
method and SVM classifier, we propose leverag-
ing on the kernelized SVM learning algorithm to 
address the problem. Specifically, we propose 
using convolution tree kernel to capture the 
structured features and design a composite kernel 
to combine the structured features and other li-
near features for phrase reordering. The tree ker-
nel is able to directly take the structured reorder-
ing instances as inputs and compute their similar-
ities without enumerating them into a set of liner 
705
features. In addition, we also study how to find 
the optimal structured feature space and how to 
partition the structured feature spaces according 
to their functionalities. Finally, we evaluate our 
method on the NIST MT-2005 Chinese-English 
translation tasks. To provide insights into the 
model, we design three kinds of experiments to-
gether with three different evaluation metrics. 
Experimental results show that the structured 
features are very effective and our composite 
kernel can well capture both the structured and 
the linear features without the need for extensive 
feature engineering. It also shows that our me-
thod significantly outperforms the baseline me-
thods. 
The tree kernel-based phrase reordering me-
thod is not only applicable to adjacent phrases. It 
is able to work with any long phrase pairs with 
gap of any length in-between. We will study this 
case in the near future. We would also like to use 
one individual tree kernel for one partition in a 
structured feature space. In doing so, we are able 
to give different weights to different partitions 
according to their functionalities and contribu-
tions. Note that these weights can be automati-
cally tuned and optimized easily against a dev 
set. 
References 
David Chiang. 2005. A hierarchical phrase-based 
model for SMT. ACL-05. 263-270. 
Michael Collins and N. Duffy. 2001. Convolution 
Kernels for Natural Language. NIPS-2001. 
M. R. Costa-juss? and J.A.R. Fonollosa. 2006. Statis-
tical Machine Reordering. EMNLP-06. 70-76. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree trans-
lation. EMNLP-06. 232-241. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for MT. ACL-03 (companion volume). 
Jakob Elming. 2008. Syntactic Reordering Integrated 
with Phrase-Based SMT. COLING-08. 209-216. 
Michel Galley and Christopher D. Manning. 2008. A 
Simple and Effective Hierarchical Phrase Reorder-
ing Model. EMNLP-08. 848-856. 
David Haussler. 1999. Convolution Kernels on Dis-
crete Structures. TR UCS-CRL-99-10. 
T. Joachims. 1998. Text Categorization with SVM: 
learning with many relevant features. ECML-98. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. ACL-03. 423-430. 
Reinhard Kenser and Hermann Ney. 1995. Improved 
backing-off for M-gram language modeling. 
ICASSP-95, 181-184 
Philipp Koehn, F. Och and D. Marcu. 2003. Statistical 
phrase-based translation. HLT-NAACL-03.  
Philipp Koehn, H. Hoang, A. Birch, C. C.-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-
ran, R. Zens, C. Dyer, O. Bojar, A. Constantin and 
E. Herbst. 2007. Moses: Open Source Toolkit for 
SMT. ACL-07 (poster). 77-180. 
Shankar Kumar and William Byrne. 2005. Local 
Phrase Reordering Models for Statistical Machine 
Translation. HLT-EMNLP-2005. 161-168. 
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou, 
Minghui Li and Yi Guan. 2007. A Probabilistic 
Approach to Syntax-based Reordering for Statis-
tical Machine Translation. ACL-07. 720-727. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation 
Rules. ACL-07. 704-711. 
Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 
2006. SPMT: SMT with Syntactified Target Lan-
guage Phrases. EMNLP-06. 44-52. 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. EMNLP-08. 206-214. 
Alessandro Moschitti. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. ACL-04. 
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto 
and Kazuteru Ohashi. 2006. A Clustered Global 
Phrase Reordering Model for Statistical Machine 
Translation. COLING-ACL-06. 713-720. 
Franz J. Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statis-
tical machine translation. ACL-02. 295-302. 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz J. Och and H. Ney. 2003. A Systematic Com-
parison of Various Statistical Alignment Methods. 
Computational Linguistics, 29(1):20-51. 
Franz J. Och and H. Ney. 2004. The alignment tem-
plate approach to statistical machine translation. 
Computational Linguistics, 30(4):417-449. 
Kishore Papineni, S. Roukos, T. and W. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. ACL-02. 311-318. 
Hendra Setiawan, Min-Yen Kan and Haizhou Li. 
2007. Ordering Phrases with Function Words. 
ACL-07. 712-719. 
Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A 
New String-to-Dependency Machine Translation 
Algorithm with a Target Dependency Language 
Model. ACL-HLT-08. 577-585. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904. 
Christoph Tillmann. 2004. A Unigram Orientation 
Model for Statistical Machine Translation. HLT-
NAACL-04 (short paper). 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer. 
706
Chao Wang, M. Collins and P. Koehn. 2007. Chinese 
Syntactic Reordering for Statistical Machine 
Translation. EMNLP-CONLL-07. 734-745. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpo-
ra. Computational Linguistics, 23(3):377-403. 
Fei Xia and Michael McCord. 2004. Improving a Sta-
tistical MT System with Automatically Learned 
Rewrite Patterns. COLING-04. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
SMT. COLING-ACL-06. 521?528. 
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li. 
2008. A Linguistically Annotated Reordering Mod-
el for BTG-based Statistical Machine Translation. 
ACL-HLT-08 (short paper). 149-152. 
Kenji Yamada and K. Knight. 2001. A syntax-based 
statistical translation model. ACL-01. 523-530. 
Xiaofeng Yang, Jian Su and Chew Lim Tan. 2006. 
Kernel-Based Pronoun Resolution with Structured 
Syntactic Knowledge. COLING-ACL-06. 41-48. 
Richard Zens, H. Ney, T. Watanabe and E. Sumita. 
2004. Reordering Constraints for Phrase-Based 
Statistical Machine Translation. COLING-04. 
Richard Zens and Hermann Ney. 2006. Discrimina-
tive Reordering Models for Statistical Machine 
Translation. WSMT-2006. 
Dell Zhang and W. Lee. 2003. Question classification 
using support vector machines. SIGIR-03. 
Min Zhang, Jie Zhang, Jian Su and GuoDong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Fea-
tures. COLING-ACL-06. 825-832. 
Dongdong Zhang, M. Li, C.H. Li and M. Zhou. 
2007a. Phrase Reordering Model Integrating Syn-
tactic Knowledge for SMT. EMNLP-CONLL-07. 
533-540. 
Min Zhang, W. Che, A. Aw, C. Tan, G. Zhou, T. Liu 
and S. Li. 2007b. A Grammar-driven Convolution 
Tree Kernel for Semantic Role Classification. 
ACL-07. 200-207. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007c. A Tree-to-Tree 
Alignment-based Model for Statistical Machine 
Translation.MT-Summit-07. 535-542 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Haizhou Li, 
Chew Lim Tan and Chew Lim Tan and Sheng Li. 
2008a. A Tree Sequence Alignment-based Tree-to-
Tree Translation Model. ACL-HLT-08. 559-567. 
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, 
Sheng Li. 2008b. Grammar Comparison Study for 
Translational Equivalence Modeling and Statistic-
al Machine Translation. COLING-08. 1097-1104 
Ying Zhang, Stephan Vogel and Alex Waibel. 2004. 
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? 
LREC-04. 2051-2054. 
707
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1037?1045,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Fast Translation Rule Matching for Syntax-based Statistical 
Machine Translation 
 
 
 
Hui Zhang1, 2   Min Zhang1   Haizhou Li1   Chew Lim Tan2    
1Institute for Infocomm Research                    2National University of Singapore                    
zhangh1982@gmail.com   {mzhang, hli}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sg 
 
 
 
 
 
 
 
Abstract 
In a linguistically-motivated syntax-based trans-
lation system, the entire translation process is 
normally carried out in two steps, translation 
rule matching and target sentence decoding us-
ing the matched rules. Both steps are very time-
consuming due to the tremendous number of 
translation rules, the exhaustive search in trans-
lation rule matching and the complex nature of 
the translation task itself. In this paper, we pro-
pose a hyper-tree-based fast algorithm for trans-
lation rule matching. Experimental results on 
the NIST MT-2003 Chinese-English translation 
task show that our algorithm is at least 19 times 
faster in rule matching and is able to help to 
save 57% of overall translation time over previ-
ous methods when using large fragment transla-
tion rules. 
1 Introduction 
Recently linguistically-motivated syntax-based 
translation method has achieved great success in 
statistical machine translation (SMT) (Galley et al, 
2004; Liu et al, 2006, 2007; Zhang et al, 2007, 
2008a; Mi et al, 2008; Mi and Huang 2008; 
Zhang et al, 2009). It translates a source sentence 
to its target one in two steps by using structured 
translation rules. In the first step, which is called 
translation rule matching step, all the applicable1 
translation rules are extracted from the entire rule 
set by matching the source parse tree/forest. The 
second step is to decode the source sentence into 
its target one using the extracted translation rules. 
Both of the two steps are very time-consuming 
due to the exponential number of translation rules 
and the complex nature of machine translation as 
                                                           
1 Given a source structure (either a parse tree or a parse 
forest), a translation rule is applicable if and only if the 
left hand side of the translation rule exactly matches a 
tree fragment of the given source structure. 
an NP-hard search problem (Knight, 1999). In the 
SMT research community, the second step has 
been well studied and many methods have been 
proposed to speed up the decoding process, such 
as node-based or span-based beam search with 
different pruning strategies (Liu et al, 2006; 
Zhang et al, 2008a, 2008b) and cube pruning 
(Huang and Chiang, 2007; Mi et al, 2008). How-
ever, the first step attracts less attention. The pre-
vious solution to this problem is to do exhaustive 
searching with heuristics on each tree/forest node 
or on each source span. This solution becomes 
computationally infeasible when it is applied to 
packed forests with loose pruning threshold or rule 
sets with large tree fragments of large rule height 
and width. This not only overloads the translation 
process but also compromises the translation per-
formance since as shown in our experiments the 
large tree fragment rules are also very useful.  
To solve the above issue, in this paper, we pro-
pose a hyper-tree-based fast algorithm for transla-
tion rule matching. Our solution includes two 
steps. In the first step, all the translation rules are 
re-organized using our proposed hyper-tree struc-
ture, which is a compact representation of the en-
tire translation rule set, in order to make the com-
mon parts of translation rules shared as much as 
possible. This enables the common parts of differ-
ent translation rules to be visited only once in rule 
matching. Please note that the first step can be 
easily done off-line very fast. As a result, it does 
not consume real translation time. In the second 
step, we design a recursive algorithm to traverse 
the hyper-tree structure and the input source forest 
in a top-down manner to do the rule matching be-
tween them. As we will show later, the hyper-tree 
structure and the recursive algorithm significantly 
improve the speed of the rule matching and the 
entire translation process compared with previous 
methods. 
With the proposed algorithm, we are able to 
carry out experiments with very loose pruning 
1037
thresholds and larger tree fragment rules effi-
ciently. Experimental results on the NIST MT-
2003 Chinese-English translation task shows that 
our algorithm is 19 times faster in rule matching 
and is able to save 57% of overall translation time 
over previous methods when using large fragment 
translation rules with height up to 5. It also shows 
that the larger rules with height of up to 5 signifi-
cantly outperforms the rules with height of up to 3 
by around 1 BLEU score. 
The rest of this paper is organized as follows. 
Section 2 introduces the syntax-based translation 
system that we are working on. Section 3 reviews 
the previous work. Section 4 explains our solution 
while section 5 reports the experimental results. 
Section 6 concludes the paper. 
2 Syntax-based Translation 
This section briefly introduces the forest/tree-
based tree-to-string translation model which 
serves as the translation platform in this paper. 
2.1 Tree-to-string model 
   
                                                    
 
                                                      
XNA declaration is related to some regulation 
 
Figure 1. A tree-to-string translation process. 
 
The tree-to-string model (Galley et al 2004; Liu et 
al. 2006) views the translation as a structure map-
ping process, which first breaks the source syntax 
tree into many tree fragments and then maps each 
tree fragment into its corresponding target transla-
tion using translation rules, finally combines these 
target translations into a complete sentence. Fig. 1 
illustrates this process. In real translation, the 
number of possible tree fragment segmentations 
for a given input tree is exponential in the number 
of tree nodes.  
2.2 Forest-based translation 
To overcome parse error for SMT, Mi and Huang 
(2008) propose forest-based translation by using a 
packed forest instead of a single syntax tree as the 
translation input. A packed forest (Tomita 1987; 
Klein and Manning, 2001; Huang and Chiang, 
2005) is a compact representation of many possi-
ble parse trees of a sentence, which can be for-
mally described as a triple , where V is 
the set of non-terminal nodes, E is the set of hy-
per-edges and S is a sentence represented as an 
ordered word sequence. A hyper-edge in a packed 
forest is a group of edges in a tree which connects 
a father node to all its children nodes, representing 
a CFG-based parse rule. Fig. 2 is a packed forest 
incorporating two parse trees T1 and T2 of a sen-
tence as shown in Fig. 3 and Fig. 4. Given a hy-
per-edge e, let h be its father node, then we say 
that e is attached to h. 
A non-terminal node in a packed forest can be 
represented as ?label [start, stop]?, where ?label? 
is its syntax category and ?[start, stop]? is the 
range of words it covers. For example, the node in 
Fig. 5 pointed by the dark arrow is labelled as 
?NP[3,4]?, where NP is its label and [3,4] means 
that it covers the span from the 3rd word to the 4th  
word. In forest-based translation, rule matching is 
much more complicated than the tree-based one.  
 
 
 
Figure 2. A packed forest 
 
Zhang et al (2009) reduce the tree sequence 
problem into tree problem by introducing virtual 
node and related forest conversion algorithms, so 
1038
the algorithm proposed in this paper is also appli-
cable to the tree sequence-based models. 
 
     
 
Figure 3. Tree 1 (T1)      Figure 4. Tree 2 (T2) 
3 Matching Methods in Previous Work  
In this section, we discuss the two typical rule 
matching algorithms used in previous work. 
3.1 Exhaustive search by tree fragments 
This method generates all possible tree fragments 
rooted by each node in the source parse tree or 
forest, and then matches all the generated tree 
fragments against the source parts (left hand side) 
of translation rules to extract the useful rules 
(Zhang et al, 2008a).  
 
 
 
Figure 5. Node NP[3,4] in packed forest 
 
 
 
Figure 6. Candidate fragments on NP[3,4] 
For example, if we want to extract useful rules 
for node NP[3,4] in Fig 5, we have to generate all 
the tree fragments rooted at node NP[3,4] as 
shown in Fig 6, and then query each fragment in 
the rule set. Let  be a node in the packed forest, 
 represents the number of possible tree frag-
ments rooted at node , then we have: 
 
 
?
?? ?? ??? 
??? ????????
 ???? ?? ?
? ?? ? ?????????? 
???????? ?? ?
 
 
 
The above equation shows that the number of 
tree fragments is exponential to the span size, the 
height and the number of hyper-edges it covers. In 
a real system, one can use heuristics, e.g. the max-
imum number of nodes and the maximum height 
of fragment, to limit the number of possible frag-
ments. However, these heuristics are very subjec-
tive and hard to optimize. In addition, they may 
filter out some ?good? fragments.  
3.2 Exhaustive search by rules 
This method does not generate any source tree 
fragments. Instead, it does top-down recursive 
matching from each node one-by-one with each 
translation rule in the rule set (Mi and Huang 
2008). 
For example, given a translation rule with its 
left hand side as shown in Fig. 7, the rule match-
ing between the given rule and the node IP[1,4] in 
Fig. 2 can be done as follows.  
1. Decompose the left hand side of the transla-
tion rule as shown in Fig. 7 into a sequence of hy-
per-edges in top-down, left-to-right order as fol-
lows: 
IP => NP VP;  NP => NP NP;  NP => NN; 
NN => ?? 
 
 
 
Figure 7. The left hand side of a rule 
 
2. Pattern match these hyper-edges(rule) one-
by-one in top-down left-to-right order from node 
IP[1,4]. If there is a continuous path in the forest 
matching all of these hyper-edges in order, then 
we can say that the rule is useful and matchable 
1039
with the tree fragment covered by the continuous 
path. The following illustrates the matching steps: 
1. Match hyper-edge ?IP => NP VP? with node 
IP[1,4]. There are two hyper-edges in the forest 
matching it: ?IP[1,4] => NP[1,1] VP[2,4]? and 
?IP[1,4] => NP[1,2] VP [3,4]?, which generates 
two candidate paths. 
2. Since hyper-edge ?NP => NP NP? fails to 
match NP[1,1], the path initiated with ?IP[1,4] => 
NP[1,1] VP[2,4]? is pruned out. 
3.  Since there is a hyper-edge ?NP[1,2] => 
NP[1,1] NP[2,2]? matching ?NP => NP NP? on 
NP[1,2], then continue for further matching. 
4. Since ?NP=>NN? on NP[2,2] matches 
?NP[2,2] => NN[2,2]?, then continue for further 
matching. 
5. ?NN=>??? on NN[2,2] matches ?NN[2,2] 
=>??? and it is the last hyper-edge in the input 
rules. Finally, there is one continuous path suc-
cessfully matching the left hand side of the input 
rule.  
This method is able to avoid the exponential 
problem of the first method as described in the 
previous subsection. However, it has to do one-by-
one pattern matching for each rule on each node. 
When the rule set is very large (indeed it is very 
large in the forest-based model even with a small 
training set), it becomes very slow, and even much 
slower than the first method. 
4 The Proposed Hyper-tree-based Rule 
Matching Algorithm 
In this section, we first explain the motivation why 
we re-organize the translation rule sets, and then 
elaborate how to re-organize the translation rules 
using our proposed hyper-tree structure. Finally 
we discuss the top-down rule matching algorithm 
between forest and hyper-tree.  
4.1 Motivation 
 
 
              Figure 8.  Two rules? left hand side 
 
 
Figure 9. Common part of the two rules? left hand  
sides in Figure 8 
 
Fig. 9 shows the common part of the left hand 
sides of two translation rules as shown in Fig. 8. 
In previous rule matching algorithm, the common 
parts are matched as many times as they appear in 
the rule set, which reduces the rule matching 
speed significantly. This motivates us to propose 
the hyper-tree structure and the rule matching al-
gorithm to make the common parts shared by mul-
tiple translation rules to be visited only once in the 
entire rule matching process. 
4.2 Hyper-node, hyper-path and hyper-tree 
A hyper-tree is a compact representation of a 
group of tree translation rules with common parts 
shared. It consists of a set of hyper-nodes with 
edges connecting different hyper-nodes into a big 
tree. A hyper-tree is constructed from the transla-
tion rule sets in two steps: 
1) Convert each tree translation rule into a hy-
per-path; 
2) Construct the hyper-tree by incrementally 
adding each individual hyper-path into the 
hyper-tree. 
A tree rule can be converted into a hyper-path 
without losing information. Fig. 10 demonstrates 
the conversion process:  
1) We first fill the rule tree with virtual nodes  
to make all its leaves have the same depth 
to the root; 
2) We then group all the nodes in the same 
tree level to form a single hyper-node, 
where we use a comma as a delimiter to 
separate the tree nodes with different father 
nodes; 
3) A hyper-path is a set of hyper-nodes linked 
in a top-down manner. 
The commas and virtual nodes  are introduced 
to help to recover the original tree from the hyper-
path. Given a tree node in a hyper-node, if there 
are n commas before it, then its father node is the 
(n+1)th tree node in the father hyper-node. If we 
could find father node for each node in hyper-
nodes, then it is straightforward to recover the 
original tree from the hyper-path by just adding 
the edges between original father and children 
nodes except the virtual node .  
1040
After converting each tree rule into a hyper-
path, we can organize the entire rule set into a big 
hyper-tree as shown in Figure 11. The concept of 
hyper-path and hyper-tree could be viewed as an 
extension of the "prefix merging" ideas for CFG 
rules (Klein and Manning 2001). 
 
         
 
 
 
 
Figure 10. Convert tree to hyper-path 
 
 
 
Figure 11. A hyper-tree example 
 
Algorithm 1 shows how to organize the rule set 
into a big hyper-tree. The general process is that 
for each rule we convert it into a hyper-path and 
then add the hyper-path into a hyper-tree incre-
mentally. However, there are many different hy-
per-trees generated given a big rule set. We then 
introduce a TOP label as the root node to link all 
the individual hyper-trees to a single big hyper-
tree. Algorithm 2 shows the process of adding a 
hyper-path into a hyper-tree. Given a hyper-path, 
we do a top-down matching between the hyper-
tree and the input hyper-path from root hyper-
node until a leaf hyper-node is reached or there is 
no matching hyper-node at some level found. 
Then we add the remaining unmatchable part of 
the input hyper-path as the descendants of the last 
matchable hyper-node. 
Please note that in Fig. 10 and Fig. 11, we ig-
nore the target side (right hand side) of translation 
rules for easy discussion. Indeed, we can easily 
represent all the complete translation rules (not 
only left hand side) in Fig. 11 by simply adding 
the corresponding rule target sides into each hy-
per-node as done by line 5 of Algorithm 1.  
Any hyper-path from the root to any hyper-
node (not necessarily be a leaf of the hyper-tree) 
in a hyper-tree can represent a tree fragment. As a 
result, the hyper-tree in Fig. 11 can represent up to 
6 candidate tree fragments. It is easy to understand 
that the maximum number of tree fragments that a 
hyper-tree can represent is equal to the number of 
hyper-nodes in it except the root. It is worth not-
ing that a hyper-node in a hyper-tree without any 
target side rule attached means there is no transla-
tion rule corresponding to the tree fragment repre-
sented by the hyper-path from the root to the cur-
rent hyper-node. The compact representation of 
the rule set by hyper-tree enables a fast algorithm 
to do translation rule matching. 
 
Algorithm 1. Compile rule set into hyper-tree 
Input: rule set 
Output: hyper-tree 
 
1.  Initialize hyper-tree as a TOP node  
2.  for  each rule in rule set  do 
3.          Convert the left hand side tree to a hyper-path p 
4.          Add hyper-path p into hyper-tree 
5. Add rule?s right hand side to the leaf hyper-node of  
a hyper-path in the hyper-tree  
6. end for 
 
Algorithm  2. Add hyper-path into hyper-tree 
Input: hyper-path p and hyper-tree t 
Notation:  
   h: the height of hyper-path p 
   p(i) : the hyper-node of ith level (top-down) of p 
   TN: the hyper-node in hyper-tree  
Output: updated hyper-tree t  
 
1. Initialize TN as TOP 
2. for  i := 1 to h  do 
3.       if there is a child c of TN has the same label as p(i)    
              then 
4.             TN := c 
5.       else  
6.             Add a child c to TN, label c as p(i) 
7.             TN := c 
4.3 Translation rule matching between forest 
and hyper-tree 
Given the source parse forest and the translation 
rules represented in the hyper-tree structure, here 
we present a fast matching algorithm to extract so-
called useful translation rules from the entire rule 
set in a top-down manner for each node of the for-
est.  
As shown in Algorithm 3, the general process 
of the matching algorithm is as follows: 
 
1041
Algorithm 3. Rule matching on one node  
Input: hyper-tree T, forest F, and node n 
Notation:   
      FP: a pair <FNS, TN>, FNS is the frontier nodes of      
             matched tree fragment,  
             TN is the hyper-tree node matching it 
      SFP: the queue of FP 
Output: Available rules on node n 
 
1. if there is no child c of TOP having the same label as n      
   then 
2.        Return failure. 
3. else  
4.      Initialize FP as <{n},c> and put it into SFP 
5.      for each FP in SFP do 
6.                 SFP ? PropagateNextLevel(FP.FNS, FP.TN)  
7.      for each FP in SFP do 
8.          if the rule set attached to FP.TN is not empty   
         then 
9.               Add FP to result 
 
Algorithm 4. PropagateNextLevel  
Input: Frontier node sequence FNS, hyper-tree node TN 
Notation: 
           CT: a child node of TN 
                  the number of node sequence (separated by  
                  comma, see Fig 11) in CT is equal to the number  
                  of node in TN.   
           CT(i) : the ith node sequence in hyper-node CT 
           FNS(i): the ith node in FNS 
           TFNS: the temporary set of frontier node sequence 
           RFNS: the result set of frontier node sequence  
           FP:  a pair of frontier node sequence  
                   and hyper-tree node 
           RFP: the result set of FP 
Output: RFP  
 
1. for each child hyper-node CT of TN do 
2.        for i:= 1 to the number of node sequence in CT do 
3.              empty TFNS 
4.              if CT(i) ==  then 
5.                      Add FNS(i) to TFNS. 
6.              else 
7.                   for each hyper-edge e attached to FNS(i) do 
8.                         if e.children match CT(i) then 
9.                                Add e.children to TFNS 
10.              if TFNS is empty then 
11.                      empty RFNS 
12.                      break 
13.              else if i == 1 then  
14.                       RFNS := TFNS 
15.              else  
16.                       RFNS := RFNS  TFNS 
17.        for each FNS in RFNS do 
18.                add <FNS, CT > into RFP 
 
1) For each node n of the source forest if no 
child node of TOP in hyper-tree has the same label 
with it, it means that no rule matches any tree 
fragments rooted at the node n (i.e., no useful 
rules to be used for the node n) (line 1-2) 
2) Otherwise, we match the sub-forest starting 
from the node n against a sub-hyper-tree starting 
from the matchable child node of TOP layer by 
layer in a top-down manner. There may be many 
possible tree fragments rooted at node n and each 
of them may have multiple useful translation rules. 
In our implementation, we maintain a data struc-
ture of FP = <FNS, TN> to record the currently 
matched tree fragment of forest and its corres-
ponding hyper-tree node in the rule set, where 
FNS is the frontier node set of the current tree 
fragment and TN is the hyper-tree node. The data 
structure FP is used to help extract useful transla-
tion rules and is also used for further matching of 
larger tree fragments. Finally, all the FPs for the 
node n are kept in a queue. During the search, the 
queue size is dynamically increased. The matching 
algorithm terminates when all the FPs have been 
visited (line 5-6 and Algorithm 4). 
3) In the final queue, each element (FP) of the 
queue contains the frontier node sequence of the 
matched tree fragment and its corresponding hy-
per-tree node. If the target side of a rule in the hy-
per-tree node is not empty, we just output the 
frontier nodes of the matched tree fragment, its 
root node n and all the useful translation rules for 
later translation process. 
Algorithm 4 describes the detailed process of 
how to propagate the matching process down to 
the next level.  <FNS, TN> is the current level 
frontier node sequence and hyper-tree node. Given 
a child hyper-node CT of TN (line 1), we try to 
find the group of next level frontier node sequence 
to match it (line 2-18). As shown in Fig 11, a hy-
per-node consists of a sequence of node sequence 
with comma as delimiter. For the ith node se-
quence CT(i) in CT, If CT(i) is , that means 
FNS(i) is a leaf/frontier node in the matched tree 
fragment and thus no need to propagate to the next 
level (line 4-5). Otherwise, we try each hyper-
edge e of FNS(i) to see whether its children match 
CT(i), and put the children of the matched hyper-
edge into a temp set TFNS (line 7-9). If the temp 
set is empty, that means the current matching fails 
and no further expansion needs (line 10-12). Oth-
erwise, we integrate current matched children into 
the final group of frontier node sequence (line 13-
16) by Descartes Product ( ). Finally, we con-
struct all the <FNS, TN> pair for next level 
matching (line 17-18). 
It would be interesting to study the time com-
plexity of our Algorithm 3 and 4. Suppose the 
maximum number of children of each hyper-node 
in hyper-tree is N (line 1), the maximum number 
of node sequence in CT is M (line 2), the maxi-
mum number of hyper-edge in each node in 
packed forest is K (line 7), the maximum number 
of hyper-edge with same children representation 
in each node in packed forest is C (i.e. the maxi-
mum size of TFNS in line 16, and the maximum 
complexity of the Descartes Product in line 16 
1042
would be CM), then the time complexity upper-
bound of Algorithm 4 is O(NM(K+CM)). For Al-
gorithm 3, its time complexity is O(RNM(K+CM)), 
where R is the maximum number of tree fragment 
matched in each node.  
5 Experiment 
5.1 Experimental settings 
We carry out experiment on Chinese-English 
NIST evaluation tasks. We use FBIS corpus 
(250K sentence pairs) as training data with the 
source side parsed by a modified Charniak parser 
(Charniak 2000) which can output a packed forest. 
The Charniak Parser is trained on CTB5, tuned on 
301-325 portion, with F1 score of 80.85% on 271-
300 portion. We use GIZA++ (Och and Ney, 2003) 
to do m-to-n word-alignment and adopt heuristic 
?grow-diag-final-and? to do refinement. A 4-gram 
language model is trained on Gigaword 3 Xinhua 
portion by SRILM toolkit (Stolcke, 2002) with 
Kneser-Ney smoothing. We use NIST 2002 as 
development set and NIST 2003 as test set. The 
feature weights are tuned by the modified Koehn?s 
MER (Och, 2003, Koehn, 2007) trainer. We use 
case-sensitive BLEU-4 (Papineni et al, 2002) to 
measure the quality of translation result. Zhang et 
al. 2004?s implementation is used to do significant 
test. 
Following (Mi and Huang 2008), we use viterbi 
algorithm to prune the forest. Instead of using a 
static pruning threshold (Mi and Huang 2008), we 
set the threshold as the distance of the probabili-
ties of the nth best tree and the 1st best tree. It 
means the pruned forest is able to at least keep all 
the top n best trees. However, because of the shar-
ing nature of the packed forest, it may still contain 
a large number of additional trees. Our statistic 
shows that when we set the threshold as the 100th 
best tree, the average number of all possible trees 
in the forest is 1.2*105 after pruning. 
In our experiments, we compare our algorithm 
with the two traditional algorithms as discussed in 
section 3. For the ?Exhaustive search by tree? al-
gorithm, we use a bottom-up dynamic program-
ming algorithm to generate all the candidate tree 
fragments rooted at each node. For the ?Exhaus-
tive search by rule? algorithm, we group all rules 
with the same left hand side in order to remove the 
duplicated matching for the same left hand side 
rules. All these settings aim for fair comparison. 
5.2 Accuracy, speed vs. rule heights 
We first compare the three algorithms? perfor-
mance by setting the maximum rule height from 1 
to 5. We set the forest pruning threshold to the 
100th best parse tree.  
Table 1 compares the speed of the three algo-
rithms. It clearly shows that the speed of both of 
the two traditional algorithms increases dramati-
cally while the speed of our hyper-tree based algo-
rithm is almost linear to the tree height. In the case 
of rule height of 5, the hyper-tree algorithm is at 
least 19 times (9.329/0.486) faster than the two 
traditional algorithms and saves 8.843(9.329 - 
0.486) seconds in rule matching for each sentence 
on average, which contributes 57% (8.843/(9.329 
+ 6.21)) speed improvement to the overall transla-
tion.  
 
H 
Rule Matching 
D Exhaus-
tive 
by tree 
Exhaus-
tive 
by rule 
Hyper- 
tree-
based 
1 0.043 0.077 0.083   2.96 
2 0.047 0.920 0.173   3.56 
3 0.237 9.572 0.358   4.02 
4 2.300 48.90 0.450   5.27 
5 9.329 90.80 0.486   6.21 
 
Table 1. Speed in seconds per sentence vs. rule 
height; ?H? is rule height, ?D? represents the de-
coding time after rule matching 
 
 
Height BLEU 
1 0.1646 
2 0.2498 
3 0.2824 
4 0.2874 
5 0.2925 
Moses 0.2625 
 
Table 2. BLEU vs. rule height 
 
Table 2 reports the BLEU score with different 
rule heights, where Moses, a state-of-the-art 
phrase-based SMT system, serves as the baseline 
system.  It shows the BLEU score consistently 
improves as the rule height increases. In addition, 
one can see that the rules with maximum height of 
5 are able to outperform the rules with maximum 
height of 3 by 1 BLEU score (p<0.05) and signifi-
cantly outperforms Moses by 3 BLEU score 
(p<0.01). To our knowledge, this is the first time 
to report the performance of rules up to height of 5 
for forest-based translation model.  
1043
We also study the distribution of the rules used 
in the 1-best translation output. The results are 
shown in Table 3; we could see something inter-
esting that is as the rule height increases, the total 
number of rules with that height decreases, while 
the percentage of partial-lexicalized increases 
dramatically. And one thing needs to note is the 
percentage of partial-lexicalized rules with height 
of 1 is 0, since there is no partial-lexicalized rule 
with height of 1 in the rule set (the father node of 
a word is a pos tag node).  
 
H Total 
Rule Type Percentage (%) 
F P U 
1 9814   76.58     0 23.42 
2 5289   44.99     46.40 8.60 
3 3925   18.39     77.25 4.35 
4 1810   7.90      87.68 4.41 
5 511    6.46 90.50 3.04 
 
Table 3. statistics of rules used in the 1-best trans-
lation output, ?F? means full-lexicalized, ?P? 
means partial-lexicalized, ?U? means unlexiclaizd. 
5.3 Speed vs. forest pruning threshold 
This section studies the impact of the forest prun-
ing threshold on the rule matching speed when 
setting the maximum rule height to 5. 
 
Threshold 
Rule Matching  
Exhaus-
tive 
by tree 
Exhaus-
tive 
by rule 
Hyper- 
tree- 
based 
1 1.2 23.66 0.171 
10 3.1 36.42 0.234 
50 5.7 66.20 0.405 
100 9.3 90.80 0.486 
200 27.3 104.86 0.598 
500 133.6 148.54 0.873 
 
Table 4. Speed in seconds per sentence vs. for-
est  pruning threshold 
 
In Table 4, we can see that our hyper-tree based 
algorithm is the fastest among the three algorithms 
in all pruning threshold settings and even 150 
times faster than both of the two traditional algo-
rithms with threshold of 500th best. Table 5 shows 
the average number of parse trees embedded in a 
packed forest with different pruning thresholds per 
sentence. We can see that the number of trees in-
creases exponentially when the pruning threshold 
increases linearly. When the threshold is 500th best, 
the average number of trees per sentence is 
1.49*109. However, even in this extreme case, the 
hyper-tree based algorithm is still capable of com-
pleting rule matching within 1 second.  
 
Threshold Number of Trees  
1 1 
10 32 
50 5922 
100 128860 
200 2.75*106 
500 1.49*109 
 
Table 5. Average number of trees in packed 
forest with different pruning threshold. 
5.4 Hyper-tree compression rate 
As we describe in section 4.2, theoretically the 
number of tree fragments that a hyper-tree can 
represent is equal to the number of hyper-nodes in 
it. However, in real rule set, there is no guarantee 
that each tree fragment in the hyper-tree has cor-
responding translation rules. To gain insights into 
how effective the compact representation of the 
hyper-tree and how many hyper-nodes without 
translation rules, we define the compression rate 
as follows.  
 
 
 
 
Table 6 reports the different statistics on the 
rule sets with different maximum rule heights 
ranging from 1 to 5. The reported statistics are the 
number of rules, the number of unique left hand 
side (since there may be more than one rules hav-
ing the same left hand side), the number of hyper-
nodes and the compression rate.  
 
H n_rules n_LHS n_nodes c_rate 
1 21588 10779 10779 100% 
2 141632 51807 51903 99.8% 
3 1.73*106 491268 494919 99.2% 
4 8.65*106 2052731 2083296 98.5% 
5 1.89*107 3966742 4043824 98.1% 
 
Table 6. Statistics of rule set and hyper-tree. ?H? 
is rule height, ?n_rules? is the number of rules, 
?n_LHS? is the number of unique left hand side, 
?n_nodes? is the number of hyper-nodes in hyper-
tree and ?c_rate? is the compression rate. 
 
Table 6 shows that in all the five cases, the 
compression rates of hyper-tree are all more than 
1044
98%. It means that almost all the tree fragments 
embedded in the hyper-tree have corresponding 
translation rules. As a result, we are able to use 
almost only one hyper-edge (i.e. only the frontier 
nodes of a tree fragment without any internal 
nodes) to represent all the rules with the same left 
hand side. This suggests that our hyper-tree is par-
ticularly effective in representing the tree transla-
tion rules compactly. It also shows that there are a 
lot of common parts among different translation 
rules. 
All the experiments reported in this section 
convincingly demonstrate the effectiveness of our 
proposed hyper-tree representation of translation 
rules and the hyper-tree-based rule matching algo-
rithm. 
6 Conclusion   
In this paper2, we propose the concept of hyper-
tree for compact rule representation and a hyper-
tree-based fast algorithm for translation rule 
matching in a forest-based translation system. We 
compare our algorithm with two previous widely-
used rule matching algorithms.  Experimental re-
sults on the NIST Chinese-English MT 2003 eval-
uation data set show the rules with maximum rule 
height of 5 outperform those with height 3 by 1.0 
BLEU and outperform MOSES by 3.0 BLEU. In 
the same test cases, our algorithm is at least 19 
times faster than the two traditional algorithms, 
and contributes 57% speed improvement to the 
overall translation. We also show that in a more 
challenging setting (forest containing 1.49*109 
trees on average) our algorithm is 150 times faster 
than the two traditional algorithms. Finally, we 
show that the hyper-tree structure has more than 
98% compression rate. It means the compact re-
presentation by the hyper-tree is very effective for 
translation rules. 
References  
Eugene Charniak. 2000. A maximum-entropy inspired 
parser. NAACL-00. 
Michel Galley, Mark Hopkins, Kevin Knight and Da-
niel Marcu. 2004. What?s in a translation rule? 
HLT-NAACL-04.  
Liang Huang and David Chiang. 2005. Better k-best 
Parsing. IWPT-05. 
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. ACL-07. 144?151 
                                                           
The corresponding authors of this paper are Hui Zhang 
(zhangh1982@gmail.com) and Min Zhang 
(mzhang@i2r.a-star.edu.sg) 
Dan Klein and Christopher D. Manning. 2001. Parsing 
and Hypergraphs. IWPT-2001. 
Dan Klein and Christopher D. Manning. 2001. Parsing 
with Treebank Grammars: Empirical Bounds, Theo-
retical Models, and the Structure of the Penn Tree-
bank. ACL - 2001. 338-345. 
Kevin Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. CL: J99-4005. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07. 177-180. (poster) 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation Rules. 
ACL-07. 704-711. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. ACL-HLT-08. 192-199. 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. EMNLP-08. 206-214 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics. 29(1) 19-51  
Kishore Papineni, Salim Roukos, ToddWard and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. ACL-02.311-318. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904. 
Masaru Tomita. 1987. An Efficient Augmented-
Context-Free Parsing Algorithm. Computational 
Linguistics 13(1-2): 31-46. 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew 
Lim Tan. 2009. Forest-based Tree Sequence to 
String Translation Model. ACL-IJCNLP-09. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A Tree-to-Tree Align-
ment-based Model for Statistical Machine Transla-
tion. MT-Summit-07. 535-542. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew 
Lim Tan, Sheng Li. 2008a. A Tree Sequence Align-
ment-based Tree-to-Tree Translation Model. ACL-
HLT-08. 559-567. 
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, Sheng 
Li. 2008b. Grammar Comparison Study for Transla-
tional Equivalence Modeling and Statistical Ma-
chine Translation. COLING-08. 1097-1104. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement 
do we need to have a better system? LREC-04 
1045
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1552?1560,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
K-Best Combination of Syntactic Parsers  
 
Hui Zhang1, 2   Min Zhang1   Chew Lim Tan2   Haizhou Li1   
1Institute for Infocomm Research                 2National University of Singapore                    
zhangh1982@gmail.com   {mzhang, hli}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sg 
 
 
 
Abstract 
In this paper, we propose a linear model-based 
general framework to combine k-best parse 
outputs from multiple parsers. The proposed 
framework leverages on the strengths of pre-
vious system combination and re-ranking 
techniques in parsing by integrating them into 
a linear model. As a result, it is able to fully 
utilize both the logarithm of the probability of 
each k-best parse tree from each individual 
parser and any additional useful features. For 
feature weight tuning, we compare the simu-
lated-annealing algorithm and the perceptron 
algorithm. Our experiments are carried out on 
both the Chinese and English Penn Treebank 
syntactic parsing task by combining two state-
of-the-art parsing models, a head-driven lexi-
calized model and a latent-annotation-based 
un-lexicalized model. Experimental results 
show that our F-Scores of 85.45 on Chinese 
and 92.62 on English outperform the previ-
ously best-reported systems by 1.21 and 0.52, 
respectively. 
1 Introduction 
Statistical models have achieved great success in 
language parsing and obtained the state-of-the-
art results in a variety of languages. In general, 
they can be divided into two major categories, 
namely lexicalized models (Collins 1997, 1999; 
Charniak 1997, 2000) and un-lexicalized models 
(Klein and Manning 2003; Matsuzaki et al 2005; 
Petrov et al 2006; Petrov and Klein 2007). In 
lexicalized models, word information play a key 
role in modeling grammar rule generation, while 
un-lexicalized models usually utilize latent in-
formation derived from the parse structure diver-
sity. Although the two models are different from 
each other in essence, both have achieved state-
of-the-art results in a variety of languages and 
are complementary to each other (this will be 
empirically verified later in this paper). There-
fore, it is natural to combine the two models for 
better parsing performance.  
Besides individual parsing models, many sys-
tem combination methods for parsing have been 
proposed (Henderson and Brill 1999; Zeman and 
?abokrtsk? 2005; Sagae and Lavie 2006) and 
promising performance improvements have been 
reported. In addition, parsing re-ranking (Collins 
2000; Riezler et al 2002; Charniak and Johnson 
2005; Huang 2008) has also been shown to be 
another effective technique to improve parsing 
performance. This technique utilizes a bunch of 
linguistic features to re-rank the k-best (Huang 
and Chiang 2005) output on the forest level or 
tree level. In prior work, system combination 
was applied on multiple parsers while re-ranking 
was applied on the k-best outputs of individual 
parsers. 
In this paper, we propose a linear model-based 
general framework for multiple parsers combina-
tion. The proposed framework leverages on the 
strengths of previous system combination and re-
ranking methods and is open to any type of fea-
tures. In particular, it is capable of utilizing the 
logarithm of the parse tree probability from each 
individual parser while previous combination 
methods are unable to use this feature since the 
probabilities from different parsers are not com-
parable. In addition, we experiment on k-best 
combination while previous methods are only 
verified on 1-best combination. Finally, we apply 
our method in combining outputs from both the 
lexicalized and un-lexicalized parsers while pre-
vious methods only carry out experiments on 
multiple lexicalized parsers. We also compare 
two learning algorithms in tuning the feature 
weights for the linear model. 
We perform extensive experiments on the 
Chinese and English Penn Treebank corpus. Ex-
perimental results show that our final results, an 
F-Score of 92.62 on English and 85.45 on Chi-
nese, outperform the previously best-reported 
systems by 0.52 point and 1.21 point, respec-
tively. This convincingly demonstrates the effec-
tiveness of our proposed framework. Our study 
also shows that the simulated-annealing algo-
rithm (Kirkpatrick et al 1983) is more effective 
1552
than the perceptron algorithm (Collins 2002) for 
feature weight tuning. 
The rest of this paper is organized as follows. 
Section 2 briefly reviews related work. Section 3 
discusses our method while section 4 presents 
the feature weight tuning algorithm. In Section 5, 
we report our experimental results and then con-
clude in Section 6. 
2 Related Work  
As discussed in the previous section, system 
combination and re-ranking are two techniques 
to improve parsing performance by post-
processing parsers? k-best outputs.  
Regarding the system combination study, 
Henderson and Brill (1999) propose two parser 
combination schemes, one that selects an entire 
tree from one of the parsers, and one that builds a 
new tree by selecting constituents suggested by 
the initial trees. According to the second scheme, 
it breaks each parse tree into constituents, calcu-
lates the count of each constituent, then applies 
the majority voting to decide which constituent 
would appear in the final tree. Sagae and Lavie 
(2006) improve this second scheme by introduc-
ing a threshold for the constituent count, and 
search for the tree with the largest number of 
count from all the possible constituent combina-
tion. Zeman and ?abokrtsk? (2005) study four 
combination techniques, including voting, stack-
ing, unbalanced combining and switching, for 
constituent selection on Czech dependency pars-
ing. Promising results have been reported in all 
the above three prior work. Henderson and Brill 
(1999) combine three parsers and obtained an F1 
score of 90.6, which is better than the score of 
88.6 obtained by the best individual parser as 
reported in their paper. Sagae and Lavie (2006) 
combine 5 parsers to obtain a score of 92.1, 
while they report a score of 91.0 for the best sin-
gle parser in their paper. Finally, Zeman and 
?abokrtsk? (2005) reports great improvements 
over each individual parsers and show that a 
parser with very low accuracy can also help to 
improve the performance of a highly accurate 
parser. However, there are two major limitations 
in these prior works. First, only one-best output 
from each individual parsers are utilized. Second, 
none of these works uses the parse probability of 
each parse tree output from the individual parser.  
Regarding the parser re-ranking, Collins (2000) 
proposes a dozen of feature types to re-rank k-
best outputs of a single head-driven parser. He 
uses these feature types to extract around half a 
million different features on the training set, and 
then examine two loss functions, MRF and 
Boosting, to do feature selection. Charniak and 
Johnson (2005) generate a more accurate k-best 
output and adopt MaxEnt method to estimate the 
feature weights for more than one million fea-
tures extracted from the training set. Huang 
(2008) further improves the re-ranking work of 
Charniak and Johnson (2005) by re-ranking on 
packed forest, which could potentially incorpo-
rate exponential number of k-best list. The re-
ranking techniques also achieve great improve-
ment over the original individual parser. Collins 
(2002) improves the F1 score from 88.2% to 
89.7%, while Charniak and Johnson (2005) im-
prove from 90.3% to 91.4%. This latter work 
was then further improved by Huang (2008) to 
91.7%, by utilizing the benefit of forest structure. 
However, one of the limitations of these tech-
niques is the huge number of features which 
makes the training very expensive and inefficient 
in space and memory usage.  
3 K-best Combination of Lexicalized 
and Un-Lexicalized Parsers with 
Model Probabilities 
In this section, we first introduce our proposed k-
best combination framework. Then we apply this 
framework to the combination of two state-of-
the-art lexicalized and un-lexicalized parsers 
with an additional feature inspired by traditional 
combination techniques. 
3.1 K-best Combination Framework 
Our proposed framework consists of the follow-
ing steps: 
1) Given an input sentence and N different 
parsers, each parser generates K-best parse 
trees. 
2) We combine the N*K output trees and 
remove any duplicates to obtain M unique 
tress. 
3) For each of the M unique trees, we re-
evaluate it with all the N models which are 
used by the N parsers. It is worth noting 
that this is the key point (i.e. one of the 
major advantages) of our method since 
some parse trees are only generated from 
one or I (I<N) parsers. For example, if a 
tree is only generated from head-driven 
lexicalized model, then it only has the 
head-driven model score. Now we re-
evaluate it with the latent-annotation un-
lexicalized model to reflect the latent-
1553
annotation model?s confidence for this 
tree. This enables our method to effec-
tively utilize the confidence measure of all 
the individual models without any bias. 
Without this re-evaluation step, the previ-
ous combination methods are unable to 
utilize the various model scores. 
4) Besides model scores, we also compute 
some additional feature scores for each 
tree, such as the widely-used ?constituent 
count? feature. 
5) Then we adopt the linear model to balance 
and combine these feature scores and gen-
erate an overall score for each parse tree.  
6) Finally we re-rank the M best trees and 
output the one with the highest score. 
 
 
? ? ? ? ? ?
? ?  
 
The above is the linear function used in our 
method, where t is the tree to be evaluated,  to 
 are the model confidence scores (in this paper, 
we use logarithm of the parse tree probability) 
from the N models,  to  are their weights, 
?  to ?  are the L additional features, ?  to ?  
are their weights.  
In this paper, we employ two individual pars-
ing model scores and only one additional feature. 
Let  be the head-driven model score,  be the 
latent-annotation model score, ?  be the consti-
tuent count feature and ?  is the weight of fea-
ture ? .  
3.2 Confidences of Lexicalized and Un-
lexicalized Model 
The term ?confidence? was used in prior parser 
combination studies to refer to the accuracy of 
each individual parser. This reflects how much 
we can trust the parse output of each parser. In 
this paper, we use the term ?confidence? to refer 
to the logarithm of the tree probability computed 
by each model, which is a direct measurement of 
the model?s confidence on the target tree being 
the best or correct parse output. In fact, the fea-
ture weight ? in our linear model functions simi-
larly as the traditional ?confidence?. However, 
we do not directly use parser?s accuracy as its 
value. Instead we tune it automatically on devel-
opment set to optimize it against the parsing per-
formance directly. In the following, we introduce 
the state-of-the-art head-driven lexicalized and 
latent-annotation un-lexicalized models (which 
are used as two individual models in this paper), 
and describe how they compute the tree probabil-
ity briefly. 
Head-driven model is one of the most repre-
sentative lexicalized models. It attaches the head 
word to each non-terminal and views the genera-
tion of each rule as a Markov process first from 
father to head child, and then to the head child?s 
left and right siblings. 
Take following rule r as example,  
 
 
 
 is the rule?s left hand side (i.e. father label), 
 is the head child,  is M?s left sibling and  
is M?s right sibling. Let h be M?s head word, the 
probability of this rule is 
 
 
 
The probability of a tree is just the product of the 
probabilities of all the rules in it. The above is 
the general framework of head-driven model. For 
a specific model, there may be some additional 
features and modification. For example, the 
model2 in Collins (1999) introduces sub-
categorization and model3 introduces gap as ad-
ditional features. Charniak (2000)?s model intro-
duces pre-terminal as additional features. 
The latent-annotation model (Matsuzaki et al 
2005; Petrov et al 2006) is one of the most ef-
fective un-lexicalized models. Briefly speaking, 
latent-annotation model views each non-terminal 
in the Treebank as a non-terminal followed by a 
set of latent variables, and uses EM algorithms to 
automatically learn the latent variables? probabil-
ity functions to maximize the probability of the 
given training data. Take the following binarized 
rule as example, 
 
 
 
could be viewed as the set of rules  
 
 
 
The process of computing the probability of a 
normal tree is to first binarized all the rules in it, 
and then replace each rule to the corresponding 
set of rules with latent variables. Now the pre-
vious tree becomes a packed forest (Klein and 
Manning 2001; Petrov et al 2007) in the latent-
annotation model, and its probability is the inside 
probability of the root node. This model is quite 
different from the head-driven model in which 
1554
the probability of a tree is just the product all the 
rules? probability. 
3.3 Constituent Counts 
Besides the two model scores, we also adopt 
constituent count as an additional feature in-
spired by (Henderson and Brill 1999) and (Sagae 
and Lavie 2006). A constituent is a non-terminal 
node covering a special span. For example, 
?NP[2,4]? means a constituent labelled as ?NP? 
which covers the span from the second word to 
the fourth word. If we have 100 trees and NP[2,4] 
appears in 60 of them, then its constituent count 
is 60. For each tree, its constituent count is the 
sum of all the counts of its constituent. However, 
as suggested in (Sagae and Lavie 2006), this fea-
ture favours precision over recall. To solve this 
issue, Sagae and Lavie (2006) use a threshold to 
balance them. For any constituent, we calculate 
its count if and only if it appears more than X 
times in the k-best trees; otherwise we set it as 0. 
In this paper, we normalize this feature by divid-
ing the constituent count by the number of k-best. 
Note that the threshold value and the additional 
feature value are not independent. Once the 
threshold changes, the feature value has to be re-
calculated. 
In conclusion, we have four parameters to es-
timate: two model score weights, one additional 
feature weight and a threshold for the additional 
feature.  
4 Parameter Estimation  
We adopt the minimum error rate principle to 
tune the feature weights by minimizing the error 
rate (i.e. maximizing the F1 score) on the devel-
opment set. In our study, we implement and 
compare two algorithms, the simulated-annealing 
style algorithm and the average perceptron algo-
rithm. 
4.1 Simulated Annealing 
Simulated-annealing algorithm has been proved 
to be a powerful and efficient algorithm in solv-
ing NP problem (?ern? 1985). Fig 1 is the pseu-
do code of the simulated-annealing algorithm 
that we apply.   
In a single iteration (line 4-11), the simulated 
algorithm selects some random points (the Mar-
kov link) for hill climbing. However, it accepts 
some bad points with a threshold probability 
controlled by the annealing temperature (line 7-
10). The hill climbing nature gives this algorithm 
the ability of converging at local maximal point 
and the random nature offers it the chance to 
jump from some local maximal points to global 
maximal point. We do a slight modification to 
save the best parameter so far across all the fi-
nished iterations and let it be the initial point for 
upcoming iterations (line 12-17). 
RandomNeighbour(p) is the function to gener-
ate a random neighbor for the p (the four-tuple 
parameter to be estimated). F1(p) is the function 
to calculate the F1 score over the entire test set. 
Given a fixed parameter p, it selects the candi-
date tree with best score for each sentence and 
computes the F1 score with the PARSEVAL me-
trics. 
 
Pseudo code 1. Simulated-annealing algorithm 
Input: k-best trees combined from two model output 
Notation:  
   p: the current parameter value 
   F1(p): the F1 score with the parameter value p 
   TMF: the max F1 score of each iteration 
   TMp: the optimal parameter value during iteration 
   MaxF1: the max F1 score on dev set 
   Rp: the parameter value which maximizes the F1 score 
of the dev set 
   T: annealing temperature 
   L: length of Markov link 
Output: Rp 
 
1. MaxF1:= 0, Rp:= (0,0,0,0), T:=1, L=100 // initialize 
2. Repeat                                                       // iteration 
3.      TMp :=Rp 
4.      for  i := 1 to L  do 
5.            p := RandomNeighbour(TMp) 
6.            d= F1(p)- TMF 
7.            if d>0 or exp(d/T) > random[0,1) then  
8.                  TMF:=F1(p) 
9.                  TMp:=p 
10.            end if 
11.      end for 
12.      if TMF > MaxF1 then 
13.            MaxF:=TMF 
14.            Rp:=TMp 
15.      else  
16.            TMp:=Rp 
17.      end if 
18.      T=T*0.9 
19. Until convergence 
 
Fig 1. Simulated Annealing Algorithm 
4.2 Averaged Perceptron 
Another algorithm we apply is the averaged per-
ceptron algorithm. Fig 2 is the pseudo code of 
this algorithm. Averaged perceptron is an online 
algorithm. It iterates through each instance. In 
each instance, it selects the candidate answer 
with the maximum function score. Then it up-
dates the weight by the margin of feature value 
between the select answer and the oracle answer 
(line 5-9). After each iteration, it does average to 
generate a new weight (line 10). The averaged 
1555
perceptron has a solid theoretical fundamental 
and was proved to be effective across a variety of 
NLP tasks (Collins 2002). 
However, it needs a slightly modification to 
adapt to our problem. Since the threshold and the 
constituent count are not independent, they are 
not linear separable. In this case, the perceptron 
algorithm cannot be guaranteed to converge. To 
solve this issue, we introduce an outer loop (line 
2) to iterate through the value range of threshold 
with a fixed step length and in the inner loop we 
use perceptron to estimate the other three para-
meters. Finally we select the final parameter 
which has maximum F1 score across all the itera-
tion (line 14-17). 
 
Pseudo code 2. Averaged perceptron algorithm 
Input: k-best trees combined from two model output 
Notation:  
   MaxF1, Rp: already defined in pseudo code 1 
   T: the max number of iterations 
   I: the number of instances 
   Threshold: the threshold for constituent count 
   w: the three feature weights other than threshold 
   ?: the candidate tree with max function score given a 
fixed weight w 
   ?: the candidate tree with the max F1 score (since the 
oracle tree may not appeared in our candidate set, 
we choose this one as the pseudo orcale tree) 
   : the set of candidate tree for ith sentence 
Output: Rp 
 
1. MaxF1:=0, T=30 
2. for  Threshold :=0 to 1 with step 0.01 do  
3.     Initialize w 
4.     for iter : 1 to T do 
5.           for  i := 1 to I  do 
6.               ? ?????????  
7.               ? ?  
8.               ?:= w 
9.           end for  
10.           ? ??I???I  
11.           if converged  then break 
12.     end for 
13.     p := (Threshold, w) 
14.     if F1(p) > MaxF1 then 
15.         MaxF1 := F1(p) 
16.         Rp:=p 
17.     end if 
18. end for 
 
Fig 2. Averaged Perceptron Algorithm 
5 Experiments 
We evaluate our method on both Chinese and 
English syntactic parsing task with the standard 
division on Chinese Penn Treebank Version 5.0 
and WSJ English Treebank 3.0 (Marcus et al 
1993) as shown in Table 1.  
We use Satoshi Sekine and Michael Collins? 
EVALB script modified by David Ellis for accu-
racy evaluation. We use Charniak?s parser 
(Charniak 2000) and Berkeley?s parser (Petrov 
and Klein 2007) as the two individual parsers, 
where Charniak?s parser represents the best per-
formance of the lexicalized model and the Berke-
ley?s parser represents the best performance of 
the un-lexicalized model. We retrain both of 
them according to the division in Table. 1. The 
number of EM iteration process for Berkeley?s 
parser is set to 5 on English and 6 on Chinese. 
Both the Charniak?s parser and Berkeley?s parser 
provide function to evaluate an input parse tree?s 
probability and output the logarithm of the prob-
ability. 
 
        Div. 
Lang. Train Dev Test 
English Sec.02-21 Sec. 22 Sec. 23 
 
Chinese 
Art. 
001-270, 
400-1151 
Art. 
301-325 
Art. 
271-300 
 
          Table 1. Data division 
5.1 Effectiveness of our Combination Me-
thod 
This sub-section examines the effectiveness of 
our proposed methods. The experiment is set up 
as follows: 1) for each sentence in the dev and 
test sets, we generate 50-best from Charniak?s 
parser (Charniak 2000) and Berkeley?s parser 
(Petrov and Klein 2007), respectively; 2) the two 
50-best trees are merged together and duplication 
was removed; 3) we tune the parameters on the 
dev set and test on the test set. (Without specific 
statement, we use simulated-annealing as default 
weight tuning algorithm.)  
The results are shown in Table 2 and Table 3. 
?P? means precision, ?R? means recall and ?F? is 
the F1-measure (all is in % percentage metrics); 
?Charniak? represents the parser of (Charniak 
2000), ?Berkeley? represents the parser of (Pe-
trov and Klein 2007), ?Comb.? represents the 
combination of the two parsers. 
 
         parser 
accuracy Charniak Berkeley Comb. 
<=40 
words 
P 85.20 86.65 90.44 
R 83.70 84.18 85.96 
F 84.44 85.40 88.15 
All 
P 82.07 84.63 87.76 
R 79.66 81.69 83.27 
F 80.85 83.13 85.45 
 
Table 2. Results on Chinese 
1556
         parser 
accuracy Charniak Berkeley Comb. 
<=40 
words 
P 90.45 90.27 92.36 
R 90.14 89.76 91.42 
F 90.30 90.02 91.89 
All 
P 89.86 89.77 91.89 
R 89.53 89.26 90.97 
F 89.70 89.51 91.43 
 
Table 3. Results on English 
 
From Table 2 and Table 3, we can see our me-
thod outperforms the single systems in all test 
cases with all the three evaluation metrics. Using 
the entire Chinese test set, our method improves 
the performance by 2.3 (85.45-83.13) point in 
F1-Score, representing 13.8% error rate reduc-
tion. Using the entire English test set, our method 
improves the performance by 1.7 (91.43-89.70) 
point in F1-Score, representing 16.5% error rate 
reduction. These improvements convincingly 
demonstrate the effectiveness of our method. 
5.2 Effectiveness of K 
Fig 3 and Fig. 4 show the relationship between 
F1 score and the number of K-best used when 
doing combination on Chinese and English re-
spectively.  
From Fig 3 and Fig. 4, we could see that the 
F1 score first increases with the increasing of K 
(there are some vibration points, this may due to 
statistical noise) and reach the peak when K is 
around 30-50, then it starts to drop.  It shows that 
k-best list did provide more information than 
one-best and thus can help improve the accuracy; 
however more k-best list may also contain more 
noises and these noises may hurt the final com-
bination quality. 
 
 
 
       Fig 3. F1-measure vs. K on Chinese 
 
 
 
       Fig 4. F1-measure vs. K on English 
5.3 Diversity on the K-best Output of the 
Head-driven and Latent-annotation-
driven Model  
In this subsection, we examine how different of 
the 50-best trees generated from Charnriak?s 
parser (head-driven model) (Charnriak, 2000) 
and Berkeley?s parser (latent-annotation model) 
(Petrov and Klein, 2007).   
Table 4 reports the statistics on the 50-best 
output for Chinese and English test set. Since for 
some short sentences the parser cannot generate 
up to 50 best trees, the average number of trees is 
less than 50 for each sentence. Each cell reports 
the total number of trees generated over the en-
tire test set followed by the average count for 
each sentence in bracket. ?Total? means simply 
combine the number of trees from the two pars-
ers while ?Unique? means the number after re-
moving the duplicated trees for each sentence. In 
the last row, we report the averaged redundant 
rate for each sentence, which is derived by divid-
ing the figures in the row ?Duplicated? by those 
in the row ?Total?. 
 
 Chinese English 
Charniak 14577 (41.9) 120438 (49.9) 
Berkeley 14524 (41.7) 114299 (47.3) 
Total 29101 (83.6) 234737 (97.2) 
Unique 27747 (79.7) 221633 (91.7) 
Duplicated 1354 (3.9) 13104 (5.4) 
Redundant rate 4.65% 5.58% 
 
          Table 4. The statistics on the 50-best out-
put for Chinese and English test set.  
 
The small redundant rate clearly suggests that 
the two parsing models are quite different and 
are complementary to each other.  
1557
         parser 
Oracle Charniak Berkeley Comb. 
Chinese 
P 88.95 90.07 92.45 
R 86.51 87.12 89.67 
F 87.71 88.57 91.03 
English 
P 97.06 95.86 98.10 
R 96.57 95.53 97.68 
F 96.82 95.70 97.89 
 
Table 5. The oracle over 50-best output for in-
dividual parser and our method 
 
The k-best oracle score is the upper bound of 
the quality of the k-best trees. Table 5 reports the 
oracle score for the 50-best of the two individual 
parsers and our method.  Similar to Table 4, Ta-
ble 5 shows again that the two models are com-
plementary to each other and our method is able 
to take the strength of the two models. 
5.4 Effectiveness of Model Confidence 
One of the advantages of our method that we 
claim is that we can utilize the feature of the 
model confidence score (logarithm of the parse 
tree probability). 
Table 6 shows that all the three features con-
tribute to the final accuracy improvement. Even 
if we only use the ?B+C? confidence scores, it 
also outperforms the baseline individual parser 
(as reported in Table 2 and Table 3) greatly. All 
these together clearly verify the effective of the 
model confidence feature and our method can 
effectively utilize this feature. 
 
         Feat.  
Lang    I B+C B+C+I 
Chinese 82.34 84.67 85.45 
English 90.20 91.02 91.43 
 
Table 6. F1 score on 50-best combination with 
different feature configuration. ?I? means the 
constituent count, ?B? means Berkeley parser 
confidence score and ?C? means Charniak parser 
confidence score. 
5.5 Comparison of the Weight Tuning Al-
gorithms 
In this sub-section, we compare the two weight 
tuning algorithms on 50-best combination tasks 
on both Chinese and English. Dan Bikel?s ran-
domized parsing evaluation comparator (Bikel 
2004) was used to do significant test on precision 
and recall metrics. The results are shown in Ta-
ble 7.  
We can see, simulated annealing outperforms 
the averaged perceptron significantly in both 
precision (p<0.005) and recall (p<0.05) metrics 
of Chinese task and precision (p<0.005) metric 
of English task. Though averaged perceptron got 
slightly better recall score on English task, it is 
not significant according to the p-value (p>0.2). 
From table 8, we could see the simulated an-
nealing algorithm is around 2-4 times slower 
than averaged perceptron algorithm. 
 
         Algo. 
Lang SA. AP. P-value 
Chinese 
P 87.76 86.85 0.003 
R 83.27 82.90 0.030 
English 
P 91.89 91.72 0.004 
R 90.97 91.02 0.205 
 
Table 7. Precision and Recall score on 50-best 
combination by the two parameter estimation 
algorithms with significant test; ?SA.? is simu-
lated annealing, ?AP.? is averaged perceptron, 
?P-value? is the significant test p-value. 
 
           Algo. 
Lang 
Simulated 
Annealing 
Averaged 
Perceptron 
Chinese 2.3 0.6 
English 12 6 
  
   Table 8. Time taken (in minutes) on 50-best 
combination of the two parameter estimation 
algorithms 
5.6 Performance-Enhanced Individual  
Parsers on English  
For Charniak?s lexicalized parser, there are two 
techniques to improve its performance. One is re-
ranking as explained in section 2. The other is 
the self-training (McClosky et al 2006) which 
first parses and reranks the NANC corpus, and 
then use them as additional training data to re-
train the model. In this sub-section, we apply our 
method to combine the Berkeley parser and the 
enhanced Charniak parser by using the new 
model confidence score output from the en-
hanced Charniak parser.  
Table 9 and Table 10 show that the Charniak 
parser enhanced by re-ranking and self-training 
is able to help to further improve the perfor-
mance of our method. This is because that the 
enhanced Charniak parser provides more accu-
rate model confidence score.  
 
1558
         parser 
accuracy reranking Comb. baseline 
<=40 
words 
P 92.34 93.41 92.36 
R 91.61 92.15 91.42 
F 91.97 92.77 91.89 
All 
P 91.78 92.92 91.89 
R 91.03 91.70 90.97 
F 91.40 92.30 91.43 
 
Table 9. Performance with Charniak parser 
enhanced by re-ranking; ?baseline? is the per-
formance of the combination of Table 3. 
 
         parser 
accuracy 
self-train+ 
reranking Comb. baseline 
<=40 
words 
P 92.87 93.69 92.36 
R 92.12 92.44 91.42 
F 92.49 93.06 91.89 
All 
P 92.41 93.25 91.89 
R 91.64 92.00 90.97 
F 92.02 92.62 91.43 
 
 Table 10. Performance with Charniak parser 
enhanced by re-ranking plus self-training 
5.7 Comparison with Other State-of-the-art 
Results 
Table 11 and table 12 compare our method with 
the other state-of-the-art methods; we use I, B, R, 
S and C to denote individual model (Charniak 
2000; Collins 2000; Bod 2003; Petrov and Klein 
2007), bilingual-constrained model (Burkett and 
Klein 2008)1, re-ranking model (Charniak and 
Johnson 2005, Huang 2008), self-training model 
(David McClosky 2006) and combination model 
(Sagae and Lavie 2006) respectively. The two 
tables clearly show that our method advance the 
state-of-the-art results on both Chinese and Eng-
lish syntax parsing. 
 
System  F1-Measure 
I 
Charniak (2000) 80.85 
Petrov and Klein (2007) 83.13 
B Burkett and Klein (2008)1 84.24 
C Our method 85.45 
 
Table 11. Accuracy comparison on Chinese 
 
                                                           
1 Burkett and Klein (2008) use the additional know-
ledge from Chinese-English parallel Treebank to im-
prove Chinese parsing accuracy. 
System  F1-Measure 
I 
Petrov and Klein (2007) 89.5 
Charniak (2000) 89.7 
Bod (2003) 90.7 
R 
Collins (2000) 89.7 
Charniak and Johnson (2005) 91.4 
Huang (2008) 91.7 
S David McClosky (2006) 92.1 
C 
Sagae and Lavie (2006) 92.1 
Our method 92.6 
 
  Table 12. Accuracy comparison on English. 
6 Conclusions   
In this paper2, we propose a linear model-based 
general framework for multiple parser combina-
tion. Compared with previous methods, our me-
thod is able to use diverse features, including 
logarithm of the parse tree probability calculated 
by the individual systems. We verify our method 
by combining the two representative parsing 
models, lexicalized model and un-lexicalized 
model, on both Chinese and English. Experimen-
tal results show our method is very effective and 
advance the state-of-the-art results on both Chi-
nese and English syntax parsing. In the future, 
we will explore more features and study the for-
est-based combination methods for syntactic 
parsing. 
Acknowledgement  
We would like to thank Prof. Hwee Tou Ng for 
his help and support; Prof. Charniak for his sug-
gestion on doing the experiments with the self-
trained parser and David McCloksy for his help 
on the self-trained model; Yee Seng Chan and 
the anonymous reviewers for their valuable 
comments. 
References  
Dan  Bikel. 2004. On the Parameter Space of Genera-
tive Lexicalized Statistical Parsing Models. Ph.D. 
Thesis, University of Pennsylvania 2004. 
Rens Bod. 2003. An efficient implementation of a new 
DOP model. EACL-04. 
David Burkett and Dan Klein. 2008. Two Languages 
are Better than One (for Syntactic Parsing). 
EMNLP-08. 
                                                           
The corresponding authors of this paper are Hui 
Zhang (zhangh1982@gmail.com) and Min Zhang 
(mzhang@i2r.a-star.edu.sg) 
1559
V ?ern? 1985. Thermodynamical approach to the 
travelling salesman problem: an efficient simula-
tion algorithm. Journal of Optimization Theory and 
Applications, 45:41-51.1985. 
Eugene Charniak. 1997. Statistical parsing with a 
context-free grammar and word statistics. AAAI-
97, pages 598-603. 
Eugene Charniak. 2000. A maximum-entropy-inspired 
parser. NAACL-2000. 
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative 
reranking. ACL-05, Pages 173-180. 
Michael Collins. 1997. Three generative, lexicalised 
models for statistical parsing. ACL-97, pages 16-
23.  
Michael Collins.1999. Head-drivenstatistical models 
for natural language parsing. Doctoral Disserta-
tion, Dept. of Computer and Information Science, 
University of Pennsylvania, Philadelphia 1999. 
Michael Collins. 2000. Discriminative reranking for 
natural language parsing. ICML-00, pages 175-
182. 
Michael Collins. 2002. Discriminative training me-
thods for hidden markov models: Theory and expe-
riments with perceptron algorithms. EMNLP-02. 
Liang Huang. 2008. Forest Reranking: Discriminative 
Parsing with Non-Local Features. ACL-HLT-08, 
pages 586-594. 
Liang Huang and David Chiang. 2005. Better k-best 
Parsing. IWPT-05. 
S. Kirkpatrick, C. D. Gelatt, Jr. and M. P. Vecchi. 
1983. Optimization by Simulated Annealing. 
Science. New Series 220 (4598): 671-680. 
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. IWPT-01. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. ACL-03, pages 423-
430. 
John Henderson and Eric Brill. 1999. Exploiting di-
versity in natural language processing: combining 
parsers. EMNLP-99. 
Mitchell P. Marcus, Beatrice Santorini and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19:313-330. 
Takuya Matsuzaki. Yusuke Miyao and Jun'ichi Tsujii. 
2005. Probabilistic CFG with latent annotations. 
ACL-05, pages 75-82. 
David McClosky, Eugene Charniak and Mark John-
son. 2006. Effective self-training for parsing. 
NAACL-06, pages 152-159. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. COLING-ACL-06, 
pages 443-440. 
Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. HLT-NAACL-07, pages 
401-411. 
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, 
Richard Crouch, John T. III Maxwell and Mark 
Johnson. 2002. Parsing the wall street journal us-
ing a lexical-functional grammar and discrimina-
tive estimation techniques. ACL-02, pages 271?
278.  
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. HLT-NAACL-06, pages 129-
132. 
Daniel Zeman and Zden?k ?abokrtsk?. Improving 
Parsing Accuracy by Combining Diverse Depen-
dency Parsers. IWPT-05. 
 
1560
Phrase-Based Statistical Machine Translation:
A Level of Detail Approach
Hendra Setiawan1,2, Haizhou Li1, Min Zhang1, and Beng Chin Ooi2
1 Institute for Infocomm Research,
21 Heng Mui Keng Terrace,
Singapore 119613
{stuhs, hli, mzhang}@i2r.a-star.edu.sg
2 School of Computing,
National University of Singapore,
Singapore 117543
{hendrase, ooibc}@comp.nus.edu.sg
Abstract. The merit of phrase-based statistical machine translation is
often reduced by the complexity to construct it. In this paper, we ad-
dress some issues in phrase-based statistical machine translation, namely:
the size of the phrase translation table, the use of underlying transla-
tion model probability and the length of the phrase unit. We present
Level-Of-Detail (LOD) approach, an agglomerative approach for learn-
ing phrase-level alignment. Our experiments show that LOD approach
significantly improves the performance of the word-based approach. LOD
demonstrates a clear advantage that the phrase translation table grows
only sub-linearly over the maximum phrase length, while having a per-
formance comparable to those of other phrase-based approaches.
1 Introduction
Early approach to statistical machine translation relies on the word-based trans-
lation model to describe the translation process [1]. However, the underlying as-
sumption of word-to-word translation often fails to capture all properties of the
language, i.e. the existence of the phrase where a group of words often function
together as a unit. Many researchers have proposed to move from the word-based
to the phrase-based translation model [2] [3] [4]. A phrase-based approach offers
many advantages as a phrase translation captures word context and local re-
ordering inherently [3]. It has become popular in statistical machine translation
applications.
There are typically two groups of approaches to constructing the phrase-
based model. The first group learns phrase translation directly from the sen-
tence pair. It learns both word and phrase units simultaneously. Although these
approaches appear intuitive, it usually suffers from a prohibitive computational
cost. It might have to consider all possible multi-word sequences as phrase can-
didates and all possible pairings as phrase translations at the same time.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 576?587, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Phrase-Based Statistical Machine Translation: A Level of Detail Approach 577
The second group of approaches learns phrase translations through word-level
alignment: alignment template [2] and projection extension [6], just to name a
few. In general, these approaches take the word-level alignment, a by-product of
the word-based translation model, as their input and then utilize a heuristic mea-
surement to learn the phrase translation. The heuristic measurement contains
all possible configurations of word-level alignment on a phrase translation.
It is noted that the underlying word-level alignment is just an approximation
to the exact alignment. The approximation is reflected by a probability produced
by the word-based translation model. The majority of approaches do not make
use of this probability, whereas it may provide a valuable clue leading to a better
phrase translation from a statistical point of view. Koehn, et. al [8] compared the
representative of both groups and reported that learning phrase translation using
a simple heuristic from word alignment yields a better translation performance
than learning phrase translation directly from the sentence pair.
Many approaches try to learn all phrase translations in one step, either di-
rectly from the sentence pair or through word alignment. As a result, they may
encounter a huge amount of phrase translation candidates at once. Usually, they
limit the maximum phrase length to reduce the choice of candidates. Although
this method is sufficient to satisfy the computational requirement, it comes with
the cost of not finding the good phrases longer than the imposed limit. Addition-
ally, to reduce the candidates, those approaches use a threshold to separate good
phrase translation from the rest. The threshold is ad-hoc and often not capable
of making a clear separation. Therefore, the use of threshold often comes with
the cost of the inclusion of undesired phrase translations and the absence of good
phrase translations in the phrase translation table. The cost may be reflected
from the size of the phrase translation table that often grows almost linearly over
the phrase length limit [6][8]. The growth implies a non-intuitive behavior: two
phrases with different length introduce an equal number of additional entries to
the phrase translation table. As longer phrases occur less often, there should be
fewer entries introduced into the phrase translation table.
We propose an agglomerative approach to learn phrase translations. Our
approach is motivated by the second group, which is to learn phrase translation
through word-alignment, while addressing the common issues: the size of the
phrase translation table, the use of underlying translation model probability
and the length of the phrase unit.
Only a few approaches move away from one-step learning. Melamed [13]
presented an agglomerative approach to learn the phrases progressively from
a parallel corpus by using sub-phrase bigram statistics. Moore [14] proposed
a similar approach which identifies the phrase candidates by parsing the raw
training data. Our idea differs from these approaches in that we look into the
association of the alignments rather than the association of the words to discover
the phrases.
In this paper, we propose the Level of Detail (LOD) approach for learning
of phrase translations in phrase-based statistical machine translation. Section 2
discusses the background and motivation and then formulates the LOD approach
578 H. Setiawan et al
while section 3 describes the learning process in details. Section 4 describes
the experimental results. In this section, we compare LOD with state-of-the-art
word-based approach in translation tasks. Finally, section 5 concludes this paper
by providing some discussion in comparison with other related works.
2 Statistical Machine Translation: A Level of Detail
2.1 Motivation and Background
It is often not intuitive to model the translation of a phrase using the word-based
translation model. First, the literal translation of phrase constituents is often in-
appropriate from a linguistic point of view. The word-based translation model
treats a phrase as a multi-word. One such example is the case where a phrase
appears as an idiom. The translation of an idiom cannot be synthesized from
the literal translation of its constituents but rather from the semantic trans-
lation of the whole. Besides, the literal translation of an idiom detracts from
the intended meaning. In one such example, the literal translation of French
?manger sur le pouce? is ?to eat on the thumb?. This detracts from the correct
translation ?to grab a bite to eat ?. In addition, to produce the correct trans-
lation, the word-based translation model might have to learn that ?manger?
is translated as ?eat? or ?pouce? is translated as ?thumb?. Although it may
serve the translation purpose, it will introduce many non-intuitive entries to the
dictionary.
Second, even if it is possible to translate a phrase verbatim, modeling phrase
translation using the word-based translation model suffers from a disadvantage:
the number of word alignments required to synthesize the phrase translation is
large. It requires four word alignments to model the translation between ?une
minute de silence? and ?one minute of silence?, whereas one phrase alignment
is adequate. The introduction of more alignments also implies the requirement
to estimate more parameters for the translation model. The implication often
comes with the cost of learning wrong word alignments.
Third, a phrase often constitutes some spurious words. The word-based trans-
lation model often has trouble in modeling spurious words, such as function
words. Function words may appear freely in any position and often may not
be translated to any word. We observe that many of these function words ap-
pear inside a phrase. It is beneficial to realize these spurious words inside a
phrase unit so as to improve statistical machine translation performance and
also to remove the necessity to model them explicitly. All these suggest that,
ideally, a phrase translation should be realized as a phrase alignment, where
the lexical correspondence is established on phrase level rather than on its word
constituents.
The discussion above suggests that phrase-based translation is a wise choice.
Practically, as a phrase is not a well defined lexical entry, a mechanism is needed
to judge what constitutes a phrase in the context of statistical machine transla-
tion. In this paper, we advocate an approach to look into the phrase discovery
process at different level of details. The level of detail refers to the size of a
Phrase-Based Statistical Machine Translation: A Level of Detail Approach 579
phrase unit. At its finest level of detail, a phrase translation uses the word-based
translation model where a phrase is modeled through its word constituent. At
a coarser level of detail, a sub-phrase unit is introduced as a sequence of words,
making it a constituent of the phrase. The coarsest level of detail refers to the
status of a phrase where all word constituents converge into a whole unit.
Our Level-Of-Detail (LOD) approach views the problem of phrase-based
translation modeling through a LOD process. It starts from the finest word-
level alignment and transforms the phrase translation into its coarsest level of
detail.
2.2 Formulation
Let < e, f > be a sentence pair of two sequences of words with e as an English
sentence and f as its translation in French1. Let < e?, f? > represents the same
sentence pair but with the phrase as its atomic unit rather than the word. To
generalize the notation, we treat word and phrase unit similarly by considering
a word as a phrase of length one. Therefore, < e, f > hereafter will be referred as
< e?, f? >(0), which represents the finest level of detail, and < e?, f? > as < e?, f? >(N),
which represents the coarsest level of detail. Let each tuple in the sentence pair
of any level of detail n, < e?, f? >(n) be e?(n) = {e?(n)0 , e?
(n)
1 , . . . , e?
(n)
i , . . . , e?
(n)
l(n)} and
f?
(n)
= {f? (n)0 , f?
(n)
1 , . . . , f?
(n)
j , . . . , f?
(n)
m(n)} where e?
(n)
0 ,f?
(n)
0 represent the special token
NULL as suggested in [1] and l(n),m(n) represent the length of the corresponding
sentence. Let T (n) be a set of alignment defined over the sentence pair < e?, f? >(n)
with t(n)ij = [e?
(n)
i , f?
(n)
j ] as its member. The superscript in all notations denotes
the level of detail where 0 represents the finest and N represents the coarsest
level of detail.
LOD algorithm iteratively transforms < e?, f? >(0) to < e?, f? >(N) through
re-alignment of phrases and re-estimation of phrase translation probability. At
n-th iteration, LOD harvests all bi-directional alignments from the sentence pair
< e?, f? >(n). The alignment is obtained by a typical word-based translation model,
such as the IBM model, while treating a sub-phrase at n-th iteration as a word.
We refer to those alignments as B(n), a pool of sub-phrase alignments unique to
the particular iteration. Afterwards, LOD generates all possible phrase alignment
candidates C(n) for a coarser level of detail from these sub-phrase alignments.
A resulting phrase alignment candidate is basically a joining of two adjacent
sub-phrase alignments subject to a certain criterion. It represents the future
coarser level alignment. Up to this point, two sets of alignment are obtained
over< e?, f? >(n): a pool of sub-phrase alignments B(n) at the current level and a
pool of phrase alignment candidates C(n) at a coarser level. From these two sets
of alignments B(n) ?C(n), we would like to derive a new set of alignments T (n+1)
that best describes the training corpus with the re-estimated statistics obtained
at n-th iteration. LOD constructs < e?, f? >(n+1) from the new set of alignment.
Algorithm 1 provides the general overview of LOD algorithm.
1 Subsequently, we will refer e as source sentence and f as target sentence, but the
term does not always reflect the translation direction.
580 H. Setiawan et al
Algorithm 1. An overview of LOD approach in learning phrase translation. The LOD
approach takes a sentence pair at its finest level of detail as its input, learns the phrase-
level alignment iteratively and outputs the same sentence pair at its coarsest level of
detail along with its phrase translation table.
input ?e?, f??(0)
for n = 0 to (N ? 1) do
- Generate bi-directional sub-phrase level alignments B(n) from ?e?, f??(n)
- Identify phrase-level alignment candidates C(n) from B(n)
- Estimate the alignment probability in B(n) and C(n)
- Learn coarser level alignment T (n+1) from B(n) ? C(n) and construct ?e?, f??(n+1)
output ?e?, f??(N) and T (N)
3 Learning Phrase Translation
In this section, we discuss the steps of LOD algorithm in detail. As presented
in Algorithm 1, moving from one level of alignment to its coarser level, LOD
follows four simple steps:
1. Generation of bi-directional sub-phrase level alignments 2
2. Identification of phrase level alignment candidates
3. Estimation of alignment probability
4. Learning coarser level alignment
3.1 Generation of Bi-directional Sub-phrase Level Alignments
LOD follows the common practice to utilize the IBM translation model for learn-
ing the phrase translation. That is to harvest all alignments from both translation
directions. For the sake of clarity, LOD defines the following notation for these
alignments, as follows:
Let ? (n)ef : e?
(n)
i ?? f?
(n)
j be an alignment function represents all alignments
from translating the source English sentence to the target French sentence, and
? (n)fe : f?
(n)
j ?? e?
(n)
i be the reversed translation direction. Then, bi-directional
sub-phrase alignment B(n) includes all possible alignment by both functions:
B(n) = {t(n)ij = [e?
(n)
i , f?
(n)
j ]|(?
(n)
ef (e?
(n)
i ) = f?
(n)
j ) ? (?
(n)
fe (f?
(n)
j ) = e?
(n)
i )}
Let us denote NULL alignments, N (n), a subset of alignments in B(n) in
which the special token NULL is involved.
2 The process starts with word level alignment. A word here is also referred to as a
sub-phrase.
Phrase-Based Statistical Machine Translation: A Level of Detail Approach 581
3.2 Identification of Phrase Alignment Candidates
LOD applies a simple heuristic to identify a phrase alignment candidate. First,
LOD considers every combination of two distinct sub-phrase alignments and as-
sesses its candidacy. Here, we define a phrase alignment candidate < t(n)ij , t
(n)
i?j? >?
C(n) as follows:
Let < t(n)ij , t
(n)
i?j? > be a set of two tuples, where t
(n)
ij ? B(n) and t
(n)
i?j? ? B(n).
Then < t(n)ij , t
(n)
i?j? > is a phrase aligment candidate if and only if
1. not ((i, i?)= 0) or (|i ? i?| = 1)
2. not ((t(n)ij ? N (n)) and (t
(n)
i?j? ? N (n)))
In the definition above, the first clause defines a candidate as a set of two whose
source sub-phrases are adjacent. The second clause forbids the consideration of
two NULL alignments.
As LOD considers only two alignments for each phrase alignment candidate,
it implies that, at the n-th iteration, the length of the longest possible phrase
is bounded by 2n. Apparently, we do not have to examine sub-phrase alignment
trunks of more than two sub-phrases because the iteration process guarantees
LOD to explore phrases of any length given sufficient iteration. This way, the
search space at each iteration can be manageable at each iteration.
3.3 Estimation of Alignment Probability
Joining the alignment set B(n) derived in Section 3.1 and the coarser level align-
ment C(n) derived in Section 3.2, we form a candidate alignment set B(n) ? C(n).
Assuming that there are two alignments x ? B(n), y ? B(n), and a candidate
alignment < x, y >? C(n), we derive the probability p(x) and p(y) from the
statistics as the count of x and y normalized by the number of alignments in the
corpus, and we derive the joint probability p(< x, y >) in a similar way.
If there is a genuine association between the two alignments, x and y, then
we expect that p(< x, y >)  p(x)p(y). If there is no interesting relationship
between x and y, then p(< x, y >) ? p(x)p(y) where we say that x and y are
independent. If x and y are in a complementary relationship, then we expect to
see that p(< x, y >)  p(x)p(y). These statistics allow us to discover a genuine
sub-phrase association.
The probability is estimated by the count of observed events normalized by
the corpus size. Note that the alignment from the IBM translation model is
derived using a Viterbi-like decoding scheme. Each observed event is counted as
one. This is referred to as hard-counting. As the alignment is done according to
probability distribution, another way of counting the event is to use the fractional
count that can be derived from the translation model. We refer to it as soft-
counting.
3.4 Learning a Coarser Level Alignment
From section 3.1 to 3.3, we have prepared all the necessary alignments with their
probability estimates. The next step is to re-align < e?, f? >(n) into < e?, f? >(n+1)
582 H. Setiawan et al
using alignment phrases in B(n) ? C(n) with their newly estimated probability
distribution. The re-alignment is considered as a constrained search process. Let
p(t(n)ij ) be the probability of a phrase alignment t
(n)
ij ? (B(n) ? C(n)) as defined
in Section 3.3, T (n) be the potential new alignment sequence for < e?, f? >(n), we
have the likelihood for T (n) as
log P (< e?, f? >(n) |T (n)) =
?
t(n)ij ?T
(n)
log p(t(n)ij ) (1)
The constrained search is to decode an alignment sequence that produces
the highest likelihood possible in the current iteration, subject to the following
constraints:
1. to preserve the phrase ordering of the source and target languages
2. to preserve the completeness of word or phrase coverage in the sentence pair
3. to ensure the mutual exclusion between alignments (except for the special
NULL tokens)
The constrained search can be formulated as follows:
T (n+1) = argmax
?T (n)
log P (< e?, f? >(n) |T (n)) (2)
In Eq.(2), we have T (n+1) as the best alignment sequence to re-align sentence
pair < e?, f? >(n) to < e?, f? >(n+1) .
The constraints are to ensure that the search leads to a valid alignment re-
sult. The search is essentially a decoding process, which traverses the sentence
pair along the source language and explores all the possible phrase alignments
with the target language. In practice, LOD tries to find a phrase translation
table that maximizes Eq.(2) as formulated in Algorithm 2. As the existing align-
ment for < e?, f? >(n) in the n-th iteration is a valid alignment subject to three
Algorithm 2. A stack decoding algorithm to explore the best alignment path between
source and target languages by considering all alignment candidates in B(n) ? C(n) at
n-th iteration.
1. Initialize a lattice of l(n) slots for l(n) sub-phrase in source language.
2. Starting from i=1, for all phrases in source language ei;
1) Register all the alignments t(n)ij that map source phrases ending with ei,
including ei itself, into slot i in the lattice;
2) Register the probability of alignment p(t(n)ij ) together with
the alignment entry t(n)ij
3) Repeat 1) and 2) until i=l(n)
3. Apply stack decoding [15] process to find the top n-best paths subject to the
three constraints. During the decoding processing, the extension of partial path
is subject to a connectivity test to enforce the three constraints.
4. Output the top best alignment result as the final result.
Phrase-Based Statistical Machine Translation: A Level of Detail Approach 583
constraints, it also serves as one resolution to the search. In the worst case, if the
constrained search can not discover any new alignment other than the existing
one, then the existing alignment in the current iteration will stand through the
next iteration.
In Algorithm 2, we establish the lattice along the source language. In the
case of English to French translation, we follow the phrases in the English order.
However, it can be done along the target language as well since our approach
follows a symmetric many-to-many word alignment strategy.
This step ends with the promotion of all phrase alignment candidates in the
best alignment sequence T (n+1). The promotion includes the merging of the two
sub-phrase alignments and the concerning sub-phrases. The merged unit will be
considered as a unit in the next iteration.
4 Experiments
The objective of our experiments is to validate our LOD approach in ma-
chine translation task. Additionally, we are interested in investigating the fol-
lowing: the effect of soft-counting in probability estimation, and the behav-
ior of LOD approach in every iteration, in terms of the length of the phrase
unit and the size of the phrase translation table. We report all our experi-
ments using BLEU metrics [10]. Furthermore, we report confidence intervals
with 95% statistical significance level of each experiments, as suggested by
Koehn [16].
We validate our approach through several experiments using English and
French language pairs from the Hansard corpus. We restrict the sentence length
to at most 20 words to obtain around 110 thousands sentence pairs. Then we
randomly select around 10 thousands sentence pair as our own testing set. In
total, the French corpus consists of 994,564 words and 29,360 unique words; while
the English corpus consists of 1,055,167 words and 20,138 unique words. Our
experiment is conducted on both English-to-French (e2f) and French-to-English
(f2e) tasks under open testing set-up. We use these available tools: GIZA++3
for word-based IBM 4 model training and ISI ReWrite4 for translation test. For
measuring the BLEU score and deriving the confidence intervals, we use the
publicly available tools5.
4.1 Soft-Counting vs. Hard-Counting
Table 1 summarizes our experiments in analyzing the effect of soft-counting
and hard-counting in the probability estimation on the BLEU score. Case I
demonstrates the BLEU score of the experiment using the underlying transla-
tion model probability or soft-counting, while Case II demonstrates the score of
3 http://www.fjoch.com/
4 http://www.isi.edu/licensed-sw/rewrite-decoder/
5 http://www.nist.gov/speech/tests/mt/resources/scoring.htm and
http://projectile.is.cs.cmu.edu/research/public/tools/bootStrap/tutorial.htm
584 H. Setiawan et al
Table 1. Summary of experiment showing the contribution of using the translation
model probability. The experiments are conducted on English-to-French task. Case I
indicates the BLEU score of the LOD approach using soft-counting whereas Case II
indicates the BLEU score of hard-counting. The value in the column indicates the
BLEU score. The range inside the bracket indicates the confidence intervals with 95%
statistical significance level.
iteration Case I Case II
1 29.60 (29.01-30.14) 28.80 (28.20-29.38)
2 30.72 (30.09-31.29) 30.11 (29.48-30.67)
3 31.52 (30.87-32.06) 30.70 (30.05-31.32)
4 31.93 (31.28-32.50) 30.93 (30.30-31.51)
5 31.90 (31.45-32.68) 31.07 (30.39-31.62)
hard-counting. The experimental results suggest that the use of the underlying
translation model probability is beneficial as it gives consistently higher BLEU
scores in all the iterations. The comparison using paired bootstrap resampling
[16] also confirms the conclusion.
4.2 LOD Behavior over Iteration
Table 2 summarizes the performance of our LOD approach for the first 10 itera-
tions in comparison with the baseline IBM 4 word-based approach. The results
show that the LOD approach produces a significant improvement over IBM 4
consistently. The first iteration yields the biggest improvement. We achieve an
absolute BLEU score improvement of 5.01 for the English-to-French task and
5.48 for the French-to-English task from the first iteration. The subsequent im-
provement is obtained by performing more iterations and capturing longer phrase
translation, however, the improvement gained is less significant compared to that
of the first iteration.
Table 2 also summarizes the maximum phrase length and the behavior of
the phrase translation table: its size and its increment over iteration. It shows
that the phrase length is soft-constrained by the maximum likelihood criterion
in Eq. (2) rather than limited. As iteration goes on, longer phrases are learnt
but their probabilities are less probable than shorter one. Consequently, longer
phrases introduce fewer entries to the phrase translation table. Table 2 captures
the behavior of the phrase translation table. The first iteration contributes the
highest increment of 12.5 % to the phrase translation table while the accumulated
increment of table size up to 10th iteration only contributes 27.5% increment
over the original size. It suggests that as iteration goes and longer phrases are
captured, fewer additional entries are introduced to the phrase translation table.
The results also show the growth of the size of the phrase translation table is
sub-linear and it converges after reasonable number of iterations. This represents
a clear advantage of LOD over other related work [6][8].
Phrase-Based Statistical Machine Translation: A Level of Detail Approach 585
Table 2. Summary of experiments showing the behavior of LOD approach and the
characteristics of the phrase translation table in each iteration. The table shows the
translation performance of the word-based IBM 4 approach and the first 10 iteration of
LOD approach in BLEU score. The value in the columns indicate the BLEU score while
the range inside the bracket represents the confidence intervals with 95% statistical
significance level. The table also shows the trend of the phrase translation table: the
maximum phrase length, its size, and its increase over iterations.
Max Table BLEU with confidence intervals
Iteration Phrase Size Increase
Length e2f f2e
IBM 4 1 216,852 - 24.59 (24.12-25.21) 26.76 (26.15-27.33)
1 2 244,097 27,245 29.60 (29.01-30.14) 32.24 (31.58-32.83)
2 4 258,734 14,637 30.72 (30.09-31.29) 32.93 (32.28-33.57)
3 7 266,209 7,475 31.52 (30.87-32.06) 33.88 (33.22-34.49)
4 7 270,531 4,322 31.93 (31.28-32.50) 34.14 (33.46-34.76)
5 10 271,793 1,262 31.90 (31.45-32.68) 34.26 (33.56-34.93)
6 11 273,589 1,796 32.14 (31.48-32.72) 34.50 (33.78-35.16)
7 12 274,641 1,052 32.09 (31.43-32.68) 34.55 (33.81-35.18)
8 12 275,399 758 32.07 (31.39-32.60) 34.43 (33.71-35.09)
9 13 275,595 196 31.98 (31.32-32.55) 34.65 (33.93-35.29)
10 14 276,508 913 32.22 (31.55-32.79) 34.61 (33.91-35.26)
5 Discussion
In this paper, we propose LOD approach to phrase-based statistical machine
translation. The LOD approach addresses three issues in the phrase-based trans-
lation framework: the size of phrase translation table, the use of underlying
translation model probability and the length of the phrase unit.
In terms of the size of the phrase translation table, our LOD approach
presents a sub-linear growth of the phrase translation table. It demonstrates a
clear advantage over other reported attempts, such as in [6][8] where the phrase
translation table grows almost linearly over the phrase length limit. The LOD
approach manages the phrase translation table size in a systematic way as a
result of the incorporation of maximum likelihood criterion into the phrase dis-
covery process.
In terms of the use of underlying translation model probability, we propose
to use soft-counting instead of hard-counting in the re-estimation processing of
probability estimation. In the projection extension algorithm [6], the phrases are
learnt based on the presence of alignment in certain configurations. In alignment
template[2], two phrases are considered to be translation of each other, if the
word alignments exist within the phrases and not to the words outside. Both
methods are based on hard-counting of translation event. Our experiment results
suggest the use of soft-counting.
586 H. Setiawan et al
In terms of the length of the phrase unit, we move away from the window-like
limit for phrase candidacy [4][9]. The LOD approach is shown to be more flexible
in capturing phrases of different length. It gradually explores longer phrases as
iteration goes, leading any reasonable length given sufficient iteration as long as
they are statistically credible.
It is known that statistical machine translation relies very much on the
training corpus. A larger phrase translation table means more training data
are needed for the translation model to be statistically significant. In this paper,
we successfully introduce the LOD approach to control the process of new phrase
discovery process. The results are encouraging.
References
1. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert
L. Mercer. 1993. The mathematics of statistical machine translation: parameter
estimation. Computational Linguistics, 19(2), pp. 263-311.
2. Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment
models for statistical machine translation. In Proc of the Joint SIGDAT Conference
on Empirical Methods in Natural Language Processing and Very Large Corpora,
pp. 20-28, University of Maryland, College Park, MD, June.
3. Franz Josef Och and Hermann Ney. 2000. A Comparison of alignment models for
statistical machine translation. In Proc of the 18th International Conference of
Computational Linguistics, Saarbruken, Germany, July.
4. Daniel Marcu and William Wong. 2002. A phrase-Based, joint probability model for
statistical machine translation. In Proc. of the Conference on Empirical Methods
in Natural Language Processing, pp. 133-139, Philadelphia, PA, July.
5. Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word
alignment in statistical translation, Proc. of COLING ?96: The 16th International
Conference of Computational Linguistics. pp. 836-841. Copenhagen, Denmark.
6. Christoph Tillmann. 2003. A projection extension algorithm for statistical machine
translation. in Proc. of the Conference on Empirical Methods in Natural Language
Processing, Sapporo, Japan.
7. Ying Zhang, Stephan Vogel, Alex Waibel. 2003. Integrated phrase segmentation
and alignment algorithm for statistical machine translation. in Proc. of the Confer-
ence on Natural Language Processing and Knowledge Engineering, Beijing, China.
8. Philipp Koehn, Franz Josef Och, Daniel Marcu. 2003. Statistical Phrase-based
Translation. In Proc. of the Human Language Technology Conference, pp. 127-
133, Edmonton, Canada, May/June.
9. Ashish Venugopal, Stephan Vogel, Alex Waibel. 2004. Effective phrase translation
extraction from alignment models. in Proc. of 41st Annual Meeting of Association
of Computational Linguistics, pp. 319-326, Sapporo, Japan, July.
10. K. Papineni, S. Roukos, T. Ward and W. J. Zhu. 2001. BLEU: A method for
automatic evaluation of machine translation. Technical Report RC22176 (W0109-
022), IBM Research Report.
11. G. Doddington. 2002. Automatic evaluation of machine translation quality using
N-gram co-occurence statistics. In Proc. of the Conference on Human Language
Technology, pp. 138-135, San Diego, CA, USA.
Phrase-Based Statistical Machine Translation: A Level of Detail Approach 587
12. Richard Zens, Hermann Ney. 2004. Improvements in phrase-Based statistical ma-
chine translation. in Proc. of Conference on Human Language Technology, pp.
257-264, Boston, MA, USA.
13. I. D. Melamed. 1997. Automatic discovery of non-compositional compounds in par-
allel data. In Proc. of 2nd Conference on Empirical Methods in Natural Language
Processing, Provicence, RI.
14. Robert C Moore. 2001. Towards a simple and accurate statistical approach to
learning translation relationships among words. In Proc of Workshop on Data-
driven Machine Translation, 39th Annual Meeting and 10th Conference of the
European Chapter, Association for Computational Linguistics, pp. 79-86, Toulouse,
France.
15. R Schwartz and Y. L. Chow . 1990. The N-best algorithm: An efficient and exact
procedure for finding the N most likely sentence hypothesis. In Proc. of ICASSP
1990, pp. 81-84. Albuquerque, CA.
16. Philipp Koehn. 2004. Statistical significance tests for machine translation evalua-
tion. In Proc. of the 2004 Conference on Empirical Methods in Natural Language
Processing, pp. 388-395.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 600 ? 611, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A Phrase-Based Context-Dependent Joint Probability 
Model for Named Entity Translation 
Min Zhang1, Haizhou Li1, Jian Su1, and Hendra Setiawan1,2 
1
 Institute for Infocomm Research,  
21 Heng Mui Keng Terrace, Singapore 119613 
{mzhang, hli, sujian, stuhs}@i2r.a-star.edu.sg 
2
 Department of Computer Science,  
National University of Singapore, Singapore, 117543 
hendrase@comp.nus.edu.sg 
Abstract. We propose a phrase-based context-dependent joint probability 
model for Named Entity (NE) translation. Our proposed model consists of a 
lexical mapping model and a permutation model. Target phrases are generated 
by the context-dependent lexical mapping model, and word reordering is per-
formed by the permutation model at the phrase level. We also present a two-
step search to decode the best result from the models. Our proposed model is 
evaluated on the LDC Chinese-English NE translation corpus. The experiment 
results show that our proposed model is high effective for NE translation.  
1   Introduction 
A Named Entity (NE) is essentially a proper noun phrase. Automatic NE translation is 
an indispensable component of cross-lingual applications such as machine translation 
and cross-lingual information retrieval and extraction.  
NE is translated by a combination of meaning translation and/or phoneme trans-
literation [1]. NE transliteration has been given much attention in the literature. 
Many attempts, including phoneme and grapheme-based methods, various machine 
learning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM) 
[4], have been made recently to tackle the issue of NE transliteration. However, 
only a few works have been reported in NE translation. Chen et al [1] proposed a 
frequency-based approach to learn formulation and transformation rules for multi-
lingual Named Entities (NEs). Al-Onaizan and Knight [5] investigated the transla-
tion of Arabic NEs to English using monolingual and bilingual resources. Huang et 
al. [6] described an approach to translate rarely occurring NEs by combining pho-
netic and semantic similarities. In this paper, we pay special attention to the issue of 
NE translation.  
Although NE translation is less sophisticated than machine translation (MT) in gen-
eral, to some extent, the issues in NE translation are similar to those in MT. Its chal-
lenges lie in not only the ambiguity in lexical mapping such as <?(Fu),Deputy> and 
<?(Fu),Vice> in Fig.1 in the next page, but also the position permutation and fertility 
of words. Fig.1 illustrates two excerpts of NE translation from the LDC corpus [7]: 
 A Phrase-Based Context-Dependent Joint Probability Model 601 
(a) Regional office of science and technology for Africa 
 
??(FeiZhou) ??(DiQu) ??(KeJi) ???(BanShiChu) 
 
 
(b) Deputy chief of staff to office of the vice president 
 
   ?(Fu) ??(ZongTong) ???(BanGongShi)?(Fu)??(ZhuRen) 
Fig. 1. Example bitexts with alignment 
where the italic word is the Chinese pinyin transcription. 
Inspired by the JSCM model for NE transliteration [4] and the success of statistical 
phrase-based MT research [8-12], in this paper we propose a phrase-based context-
dependent joint probability model for NE translation. It decomposes the NE transla-
tion problem into two cascaded steps: 
1)  Lexical mapping step, using the phrase-based context-dependent joint prob-
ability model, where the appropriate lexical item in the target language is 
chosen for each lexical item in the source language;  
2)  Reordering step, using the phrase-based n-gram permutation model, where 
the chosen lexical items are re-arranged in a meaningful and grammatical 
order of target language.  
A two-step decoding algorithm is also presented to allow for effective search of the 
best result in each of the steps. 
The layout of the paper is as follows. Section 2 introduces the proposed model. In 
Section 3 and 4, the training and decoding algorithms are discussed. Section 5 reports 
the experimental results. In Section 6, we compare our model with the other relevant 
existing models. Finally, we conclude the study in Section 7. 
2   The Proposed Model 
We present our method by starting with a definition of translation unit in Section 2.1, 
followed by the formulation of the lexical mapping model and the permutation model 
in Section 2.2. 
2.1   Defining Translation Unit 
Phrase level translation models in statistical MT have demonstrated significant im-
provement in translation quality by addressing the problem of local re-ordering across 
language boundaries [8-12]. Thus we also adopt the same concept of phrase used in 
statistical phrase-based MT [9,11,12] as the basic NE translation unit to address the 
problems of word fertility and local re-ordering within phrase.  
Suppose that we have Chinese as the source language 1 1... ...=
J
j Jc c c c and Eng-
lish as the target language 1 1... ...
I
i Ie e e e=  in an NE translation 1 1( , )J Ic e , where 
602 M. Zhang et al 
1
J
jc c?  and 1
I
ie e?  are Chinese and English words respectively. Given a directed 
word alignment A :{ 1 1?J Ic e , 1 1?I Je c }, the set of the bilingual phrase pairs ?  is 
defined as follows: 
2 2
1 11 1
1 2 1 2
( , , )={ ( , ) :
                        { ... }, { ... }:
                          }              
j iJ I
j ic e c e
j j j i i i j i
vice versa
?
? ? ? ? ? ?
?
A
A
                   (1)  
The above definition means that two phrases are considered to be translations of 
each other, if the words are aligned exclusively within the phrase pair, and not to the 
words outside [9,11,12]. The phrases have to be contiguous and a null phrase is not 
allowed. 
Suppose that the NE pair 1 1( , )J Ic e  is segmented into X phrase pairs ( 1Xc% , 1Xe% ) ac-
cording to the phrase pair set ? , where 1
Xe% is reordered so that the phrase alignment 
is in monotone order, i.e., xc% is aligned ?% %x xc e For simplicity, we denote by 
,? =< >% %x x xc e  the xth phrase pair in ( 1Xc% , 1Xe% ) = 1... ...x X? ? ? , ? ? ?x . 
2.2   Lexical Mapping Model and Permutation Model 
Given the phrase pair set ? , an NE pair ( 1Jc , 1Ie ) can be rewritten as ( 1Xc% , 1Xe% ) = 
1... ...x X? ? ? = 1X? . Let us describe a Chinese to English (C2E) bilingual training 
corpus as the output of a generative stochastic process: 
 
 
(1) Initialize queue Qc and  Qe as empty sequences; 
(2) Select a phrase pair ,x x xc e? =< >% %  according to the probability distribu-
tion 11( | )xxp ?? ? , remove x?  from ? ; 
(3) Append the phrase xc%  to Qc and append the phrase xe%  to Qe; 
(4) Repeat steps 2) and 3) until ? is empty; 
(5) Reorder all phrases in Qe according to the probability distribution of the 
permutation model; 
(6) Output Qe and Qc . 
 
As 11( | )xxp ?? ?  is typically obtained from a source-ordered aligned bilingual 
corpus, reordering is needed only for the target language. According to this generative 
story, the joint probability of the NE pair ( 1Jc , 1Ie ) can then be obtained by summing 
the probabilities over all possible ways of generating various sets of ? and all possi-
ble permutations that can arrive at ( 1
J
c , 1
I
e ).  This joint probability can be formulated 
 A Phrase-Based Context-Dependent Joint Probability Model 603 
in Eq.(2). Here we assume that the generation of the set ? and the reordering process 
are modeled by n-order Markov models, and the reordering process is independent of 
the source word position. 
1
1 1 1 1 1
1
1
1
( , )= { ( ) * ( | )}
       {( ( | ))* ( | )}
?
?
?
?
=
?
? ??
?
? ?
%
% %X
J I X I X
X
kx X
x x n k
x
p c e p p e e
p p e e
                     (2) 
1
1 1
1
( | ) ( | )xX
x x n
X
kk X
k k k
x
p e e p e e ?
?
=
? ?% % % %                                                      (3) 
where 
1
% Xkke  stands for one of the permutational sequences of 1% Xe  that can yield 1Ie  
by linearly joining all phrases, i.e., 
11
= % XkI ke e ().  The generative process, as formu-
lated above, does not try to capture how the source NE is mapped into the target NE, 
but rather how the source and target translation units can be generated simultaneously 
in the source order and how the target NE can be constructed by reordering the target 
phrases, 1% Xe .  
In essence, our proposed model consists of two sub-models: a lexical mapping 
model (LMM), characterized by 1( | )xx x np ??? ? , that models the monotonic genera-
tive process of phrase pairs; and a permutation model (PM), characterized by 
1( | )x
x x n
k
k kp e e ?
?
% % , that models the permutation process for reordering of the target 
language. The LMM in this paper is among the first attempts to introduce context-
dependent lexical mapping into statistical MT (Och et al, 2003). The PM here is also 
different from the widely used position-based distortion model in that it models 
phrase connectivity instead of position distortion. Although PM functions as an n-
gram language model, it only models the ordering connectivity between target lan-
guage phrases, i.e., it is not in charge of target word selection. 
Since the proposed model is phrase-based and we use conditional joint probability 
in LMM and use context-dependent n-gram in PM, we call the proposed model a 
phrase-based context-dependent joint probability model. 
3   Training 
Following the modeling strategy discussed above, the training process consists of 
three steps: phrase alignment, reordering of corpus, and learning statistical parameters 
for lexical mapping and permutation models. 
3.1   Acquiring Phrase Pairs 
To reduce vocabulary size and avoid sparseness, we constrain the phrase length to up 
to three words and the lower-frequency phrase pairs are pruned out for accurate 
604 M. Zhang et al 
phrase-alignment1. Given a word alignment corpus which can be obtained by means 
of the publicly available GIZA++ toolkit [15], it is very straightforward to construct 
the phrase-alignment corpus by incrementally traversing the word-aligned NE from 
left to right2. The set of resulting phrase pairs forms a lexical mapping table.  
3.2   Reordering Corpus 
The context-dependent lexical mapping model assumes monotonic alignment in the 
bilingual training corpus. Thus, the phrase aligned corpus needs to be reordered so 
that it is in either source-ordered or target-ordered alignment. We choose to reorder 
the target phrases to follow the source order. Only in this way can we use the lexical 
mapping model to describe the monotonic generative process and leave the reordering 
of target translation units to the permutation model.  
3.3   Training LMM and PM  
According to Eq. (2), the lexical mapping model (LMM) and the permutation 
model (PM) can be interpreted as a kind of n-gram Markov model. The phrase pair is 
the basic token of LMM and the target phrase is the basic token of PM. A bilingual 
corpus aligned in the source language order is used to train LMM, and a target lan-
guage corpus with phrase segmentation in their original word order is used to train 
PM. Given the two corpora, we use the SRILM Toolkit [13] to train the two n-gram 
models. 
4   Decoding 
The proposed modeling framework allows LMM and PM decoding to cascade as in 
Fig.2.  
 
Fig. 2. A cascaded decoding strategy 
The two-step operation is formulated by Eq.(4) and Eq.(5). Here, the probability 
summation as in Eq.(2) is replaced with maximization to reduce the computational 
complexity: 
1
1
1
? arg max{ ( | )}
X
X x
x x n
x
e p ?
?
?
=
= ? ??%                                                 (4) 
                                                          
1
  Koehn et. al. [12] found that that in MT learning phrases longer than three words and learning 
phrases from high-accuracy word-alignment does not have strong impact on performance. 
2
  For the details of the algorithm to acquire phrase alignment from word alignment, please refer 
to the section 2.2 & 3.2 in [9] and the section 3.1 in [12].  
%1?Xe
LMM 
Decoder
PM 
Decoder
1
Jc 1Ie
 A Phrase-Based Context-Dependent Joint Probability Model 605 
1
1
1
? arg max{ ( | )}x
x x n
X
kI
k k
x
e p e e ?
??
=
= ? % %                                                 (5) 
LMM decoding: Given the input 1
Jc , the LMM decoder searches for the most prob-
able phrase pair set ? in the source order using Eq.(4). Since this is a monotone 
search problem, we use a stack decoder [14,18] to arrive at the n-best results. 
PM decoding: Given the translation phrase sequence 1?
Xe% from the LMM decoder, 
the PM decoder searches for the best phrase order that gives the highest n-gram score 
by using Eq.(5) in the search space ? , which is all the !X  permutations of the all 
phrases in 1?
Xe% . This is a non-monotone search problem. 
The PM decoder conducts a time-synchronized search from left to right, where time 
clocking is synchronized over the number of phrases covered by the current partial 
path. To reduce the search space, we prune the partial paths along the way.  Two par-
tial paths are considered identical if they satisfy the following both conditions: 
1) They cover the same set of phrases regardless of the phrase order; 
2) The last n-1 phrases and their ordering are identical, where n is the order 
of the n-gram permutation model. 
For any two identical partial paths, only the path with higher n-gram score is retained. 
According to Eq. (5), the above pruning strategy is risk-free because the two partial 
paths cover the exact same portion of input phrases and the n-gram histories for the 
next input phrases in the two partial paths are also identical. 
It is also noteworthy that the decoder only needs to perform / 2X  expansions as 
after / 2X  expansions, all combinations of / 2X  phrases would have been explored 
already. Therefore, after / 2X  expansions, we only need to combine the correspond-
ing two partial paths to make up the entire input phrases, then select the path with 
highest n-gram score as the best translation output. 
Let us examine the number of paths that the PM decoder has to traverse. The prun-
ing reduces the search space by a factor of !Z , from !
( )!
Z
XP
X
X Z
=
?
 
to
!
! ( )!
Z
XC
X
Z X Z
=
? ?
, where Z is the number of phrases in a partial path. 
Since X ZX
Z
XC C
?
= , the maximum number of paths that we have to traverse is / 2XXC . 
For instance, when 10X = , the permutation decoder traverses 510 252C =  paths 
instead of the 510 30, 240P = in an exhausted search. 
By cascading the translation and permutation steps, we greatly reduce the search 
space. In LMM decoding, the traditional stack decoder for monotone search is very 
fast. In PM decoding, since most of NE is less than 10 phrases, the permutation de-
coder only needs to explore at most 510 252C =  living paths due to our risk-free prun-
ing strategy. 
606 M. Zhang et al 
5   Experiments 
5.1   Experimental Setting and Modeling 
All the experiments are conducted on the LDC Chinese-English NE translation corpus 
[7]. The LDC corpus consists of a large number of Chinese-Latin language NE en-
tries. Table 1 reports the statistics of the entire corpus. Because person and place 
names in this corpus are translated via transliteration, we only extract the categories 
of organization, industry, press, international organization, and others to form a cor-
pus subset for our NE translation experiment, as indicated in bold in Table 1. As the 
corpus is in its beta release, there are still many undesired entries in it. We performed 
a quick proofreading to correct some errors and remove the following types of entries:  
1) The duplicate entry; 
2) The entry of single Chinese or English word;  
3) The entries whose English translation contains two or more non-English words. 
We also segment the Chinese translation into a word sequence. Finally, we obtain a 
corpus of 74,606 unique bilingual entries, which are randomly partitioned into 10 
equal parts for 10-fold cross validation.  
Table 1.  Statistics of the LDC Corpus 
# of Entries  
Category C2E E2C 
Person 486,212 572,213 
Place 276,382 298,993 
Who-is-Who 30,028 36,881 
Organization 30,800 37,145 
Industry 54,747 58,468 
Press 29,757 32,922 
Int?l Org 7,040 7,040 
Others 13,007 14,066 
As indicated in Section 1, although MT is more difficult than NE translation, they 
both have many properties in common, such as lexical mapping ambiguity and permu-
tation/distortion. Therefore, to establish a comparison, we use the publicly available 
statistical MT training and decoding tools, which can represent the state-of-the-art of 
statistical phrase-based MT research, to carry out the same NE translation experiments 
as reference cases. All the experiments conducted in this paper are listed as follow: 
1) IBM method C: word-based IBM Model 4 trained by GIZA++3 [15] and ISI 
Decoder4 [14,16]; 
                                                          
3
 http://www.fjoch.com/ 
4
 http://www.isi.edu/natural-language/software/decoder/manual.html 
 A Phrase-Based Context-Dependent Joint Probability Model 607 
2) IBM method D:   phrase-based IBM Model 4 trained by GIZA++ on phrase-
aligned corpus and ISI Decoder working on phrase-segmented testing corpus. 
3) Koehn method: Koehn et al?s phrase-based model [12] and PHARAOH5 de-
coder6; 
4) Our method: phrase-based bi-gram LMM and bi-gram PM, and our two-step 
decoder. 
To make an accurate comparison, all the above three phrase-based models are 
trained on the same phrase-segmented and aligned corpus, and tested on the same 
phrase-segmented corpus. ISI Decoder carries out a greedy search, and PHARAOH is 
a beam-search stack decoder. To optimize their performances, the two decoders are 
allowed to do unlimited reordering without penalty. We train trigram language mod-
els in the first three experiments and bi-gram models in the forth experiment. 
5.2   NE Translation 
Table 2 and Table 3 report the performance of the four methods on the LDC NE 
translation corpus. The results are interpreted in different scoring measures, which 
allow us to compare the performances from different viewpoints.   
? ACC reports the accuracy of the exact;  
? WER reports the word error rate;  
? PER is the position-independent, or ?bag-of-words? word error rate;  
? BLEU score measures n-gram precision [19] 
? NIST score [20] is a weighted n-gram precision.  
Please note that WER and PER are error rates, the lower numbers represent better 
results. For others, the higher numbers represents the better results. 
Table 2.  E2C NE translation performance (%) 
 IBM 
  method C 
IBM 
  method D 
Koehn  
method 
Our  
method 
ACC 24.5 36.3 47.1 51.5 
WER 51.0 38.5 32.5 26.6 
PER 48.5 36.2 26.8 16.3 
BLEU 29.9 41.8 51.2 56.1 O
p e
n
 
te
s t
 
NIST 7.2 8.6 9.3 10.2 
ACC 51.1 78.9 88.2 90.9 
WER 34.1 12.8 6.3 4.3 
PER 31.5 9.5 4.1 2.7 
BLEU 54.7 80.9 89.1 91.9 
E2
C 
Cl
o s
e d
 
te
s t
 
NIST 11.1 14.2 14.7 14.8 
                                                          
5
 http://www.isi.edu/licensed-sw/pharaoh/ 
6
 http://www.isi.edu/licensed-sw/pharaoh/manual-v1.2.ps 
608 M. Zhang et al 
Table 3.  C2E NE translation performance (%) 
 IBM 
  method C 
IBM 
  method D 
Koehn  
method 
Our  
method 
ACC 13.4 21.8 31.2 36.1 
WER 60.8 45.8 41.3 38.9 
PER 49.6 38.2 32.6 26.6 
BLEU 25.1 49.8 52.9 54.1 o
p e
n  
te
s t
 
NIST 5.94 8.21 8.91 9.25 
ACC 34.3 69.5 79.2 81.3 
WER 48.2 23.6 11.3 9.2 
PER 35.7 14.7 8.7 6.2 
BLEU 42.5 76.2 85.7 88.0 
C2
E 
c l
o s
e d
 te
s t 
NIST 8.7 12.7 13.8 14.4 
Table 2 & 3 show that our method outperforms the other three methods consis-
tently in all cases and by all scores. IBM method D gives better performance than 
IBM method C, simply because it uses phrase as the translation unit instead of single 
word. Koehn et al?s phrase-based model [12] and IBM phrase-based Model 4 used in 
IBM method D are very similar in modeling. They both use context-independent 
lexical mapping model, distortion model and trigram target language model. The 
reason why Koehn method outperforms IBM method D may be due to the different 
decoding strategy. However, we still need further investigation to understand why 
Koehn method outperforms IBM method D significantly. It may also be due to the 
different LM training toolkits used in the two experiments. 
Our method tops the performance among the four experiments. The significant po-
sition-independent word error rate (PER) reduction shows that our context-dependent 
joint probability lexical mapping model is quite effective in target word selection 
compared with the other context-free conditional probability lexical model together 
with target word n-gram language model. 
Table 4. Step by step top-1 performance (%) 
 
 
LMM decoder  
 
LMM+PM decoder  
 
E2C 
 
59.9 
 
51.5 
C2E 40.5 36.1 
Table 4 studies the performance of the decoder by steps. The LMM decoder col-
umn reports the top-1 ?bag-of-words? accuracy of the LMM decoder regardless of 
word order. This is the upper bound of accuracy that the PM decoder can achieve. The 
LMM+PM decoder column shows the combined performance of two steps, where we 
 A Phrase-Based Context-Dependent Joint Probability Model 609 
measure the top-1 LMM+PM accuracy by taking top-1 LMM decoding results as 
input. It is found that the PM decoder is surprisingly effective in that it perfectly reor-
ders 85.9% (51.5/59.9) and 89.1% (36.1 /40.5) target languages in E2C and C2E 
translation respectively. 
All the experiments above recommend that our method is an effective solution for 
NE translation. 
6   Related Work 
Since our method has benefited from the JSCM of Li et al [4] and statistical MT 
research [8-12], let us compare our study with the previous related work. 
The n-gram JSCM was proposed for machine transliteration by Li et al [4]. It cou-
ples the source and channel constraints into a generative model to directly estimate 
the joint probability of source and target algnment using n-gram statistics. It was 
shown that JSCM captures rich contextual information that is present in a bilingual 
corpus to model the monotonic generative process of sequential data. In this point, our 
LMM model is the same as JSCM. The only difference is that in machine translitera-
tion Li et al [4] use phoneme unit as the basic modeling unit and our LMM is phrase-
based.  
In our study, we enhance the LMM with the PM to account for the word reorder-
ing issue in NE translation, so our model is capable of modeling the non-monotone 
problem. In contrast, JSCM only models the monotone problem. 
Both rule-based [1] and statistical model-based [5,6] methods have been proposed 
to address the NE translation problem. The model-based methods mostly are based on 
conditional probability under the noisy-channel framework [8]. Now let?s review the 
different modeling methods: 
1) As far as lexical choice issue is concerned, the noisy-channel model, repre-
sented by IBM Model 1-5 [8], models lexical dependency using a context-free 
conditional probability. Marcu and Wong [10] proposed a phrase-based con-
text-free joint probability model for lexical mapping. In contrast, our LMM 
models lexical dependency using n-order bilingual contextual information.  
2) Another characteristic of our method lies in its modeling and search strat-
egy.  NE translation and MT are usually viewed as a non-monotone search 
problem and it is well-known that a non-monotone search is exponentially 
more complex than a monotone search. Thus, we propose the two separated 
models and the two-step search, so that the lexical mapping issue can be re-
solved by monotone search. This results in a large improvement on transla-
tion selection. 
3) In addition, instead of the position-based distortion model [8-12], we use the 
n-gram permutation model to account for word reordering. A risk-free de-
coder is also proposed for the permutation model.  
One may argue that our proposed model bears a strong resemblance to IBM Model 
1: a position-independent translation model and a language model on target sentence 
without explicit distortion modeling. Let us discuss the major differences between 
them: 
610 M. Zhang et al 
1) Our LMM models the lexical mapping and target word selection using a con-
text-dependent joint probability while IBM Model 1 using a context-
independent conditional probability and a target n-gram language model. 
2) Our LMM carries out the target word selection and our PM only models the 
target word connectivity while the language model in IBM Model 1 performs 
the function of target word selection. 
Alternatively, finite-state automata (FSA) for statistical MT were previous sug-
gested for decoding using contextual information [21,22]. Bangalore and Riccardi 
[21] proposed a phrase-based variable length n-gram model followed by a reordering 
scheme for spoken language translation. However, their re-ordering scheme was not 
evaluated by empirical experiments.  
7   Conclusions 
In this paper, we propose a new model for NE translation. We present the training and 
decoding methods for the proposed model. We also compare the proposed method 
with related work. Empirical experiments show that our method outperforms the pre-
vious methods significantly in all test cases. We conclude that our method works 
more effectively and efficiently in NE translation than previous work does.  
Our method does well in NE translation, which is relatively less sophisticated in 
terms of word distortion. We expect to improve its permutation model by integrating 
a distortion model to account for larger sentence structure and apply to machine trans-
lation study. 
Acknowledgments 
We would like to thank the anonymous reviews for their invaluable suggestions on 
our original manuscript. 
References 
1. Hsin-Hsi Chen, Changhua Yang and Ying Lin. 2003. Learning Formulation and Trans-
formation Rules for Multilingual NEs. Proceedings of the ACL 2003 Workshop on 
MMLNER 
2. K. Knight and J. Graehl. 1998. Machine Transliteration. Computational Linguistics, 24(4) 
3. Jong-Hoon Oh and Key-Sun Choi, 2002. An English-Korean Transliteration Model Using 
Pronunciation and Contextual Rules. Proceedings of COLING 2002 
4. Haizhou Li, Ming Zhang and Jian Su. 2004. A Joint Source-Channel Model for Machine 
Transliteration. Proceedings of the 42th ACL, Barcelona,  160-167 
5. Y. Al-Onaizan and K. Knight, 2002. Translating named entities using monolingual and bi-
lingual resources. Proceedings of the 40th ACL, Philadelphia,  400-408 
6. Fei Huang, S. Vogel and A. Waibel, 2004. Improving NE Translation Combining Phonetic 
and Semantic Similarities. Proceedings of HLT-NAACL-2004 
7. LDC2003E01, 2003. http://www.ldc.upenn.edu/ 
 A Phrase-Based Context-Dependent Joint Probability Model 611 
8. P.F. Brown, S.A.D. Pietra, V.J.D. Pietra and R.L. Mercer.1993. The mathematics of statis-
tical machine translation. Computational Linguistics,19(2):263-313 
9. Richard Zens and Hermann Ney. 2004. Improvements in Phrase-Based Statistical Machine 
Translation. Proceedings of HLT-NAACL-2004 
10. D. Marcu and W. Wong. 2002. A Phrase-based, Joint Probability Model for Statistical 
Machine Translation. Proceedings of EMNLP-2002 
11. Franz Joseh Och, C. Tillmann and H. Ney. 1999. Improved Alignment Models for Statisti-
cal Machine Translation. Proceedings of Joint Workshop on EMNLP and Very Large Cor-
pus: 20-28 
12. P. Koehn, F. J. Och and D. Marcu. 2003. Statistical Phrase-based Translation. Proceedings 
of HLT-2003 
13. A. Stolcke. 2002. SRILM -- An Extensible Language Modeling Toolkit. Proceedings of 
ICSLP-2002, vol. 2, 901-904, Denver. 
14. U. Germann, M. Jahr, K. Knight, D. Marcu and K. Yamada. 2001. Fast Decoding and Op-
timal Decoding for Machine Translation. Proceedings of ACL-2001 
15. Franz Joseh Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical 
Alignment Models. Computational Linguistics, 29(1):19-51 
16. U. Germann. 2003. Greedy Decoding for Statistical Machine Translation in Almost Linear 
Time. Proceedings of HLT-NAACL-2003 
17. Christoph Tillmann and Hermann Ney. 2003. Word Reordering and a Dynamic Program-
ming Beam Search Algorithm for Statistical Machine Translation. Computational Linguis-
tics, 29(1):97-133 
18. R. Schwartz and Y. L. Chow. 1990. The N-best algorithm: An efficient and Exact procedure 
for finding the N most likely sentence hypothesis, Proceedings of ICASSP 1990, 81-84 
19. K. Papineni, S. Roukos, T. Ward and W. J. Zhu. 2001. BLEU: a method for automatic 
evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Re-
search Report. 
20. G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram 
co-occurrence statistics. Proceedings of ARPA Workshop on HLT 
21. S. Bangalore and G. Riccardi, 2000, Stochastic Finite State Models for Spoken Language 
Machine Translation, Workshop on Embedded MT System 
22. Stephan Kanthak and Hermann Hey, 2004. FSA: An Efficient and Flexiable C++ Tookkit 
for Finite State Automata Using On-Demand Computation, Proceedings of ACL-2004 
  
Name Origin Recognition Using Maximum Entropy Model  
and Diverse Features 
Min Zhang1, Chengjie Sun2, Haizhou Li1, Aiti Aw1, Chew Lim Tan3, Xiaolong Wang2 
1Institute for Infocomm 
Research, Singapore 
{mzhang,hli,aaiti} 
@i2r.a-star.edu.sg 
2Harbin Institute of 
Technology, China 
{cjsun,wangxl} 
@insun.hit.edu.cn 
 
3National University of 
Singapore, Singapore 
tancl@comp.
nus.edu.sg 
Abstract 
Name origin recognition is to identify the 
source language of a personal or location 
name.  Some early work used either rule-
based or statistical methods with single 
knowledge source. In this paper, we cast the 
name origin recognition as a multi-class 
classification problem and approach the 
problem using Maximum Entropy method. 
In doing so, we investigate the use of differ-
ent features, including phonetic rules, n-
gram statistics and character position infor-
mation for name origin recognition. Ex-
periments on a publicly available personal 
name database show that the proposed ap-
proach achieves an overall accuracy of 
98.44% for names written in English and 
98.10% for names written in Chinese, which 
are significantly and consistently better than 
those in reported work.  
1 Introduction 
Many technical terms and proper names, such as 
personal, location and organization names, are 
translated from one language into another with 
approximate phonetic equivalents. The phonetic 
translation practice is referred to as transliteration; 
conversely, the process of recovering a word in its 
native language from a transliteration is called as 
back-transliteration (Zhang et al 2004; Knight 
and Graehl, 1998).  For example, English name 
?Smith? and ????  (Pinyin 1 : Shi-Mi-Si)? in 
                                                 
1 Hanyu Pinyin, or Pinyin in short, is the standard romaniza-
tion system of Chinese. In this paper, Pinyin is given next to 
Chinese form a pair of transliteration and back-
transliteration. In many natural language process-
ing tasks, such as machine translation and cross-
lingual information retrieval, automatic name 
transliteration has become an indispensable com-
ponent.  
Name origin refers to the source language of a 
name where it originates from. For example, the 
origin of the English name ?Smith? and its Chi-
nese transliteration ???? (Shi-Mi-Si)? is Eng-
lish, while both ?Tokyo? and ??? (Dong-Jing)? 
are of Japanese origin. Following are examples of 
different origins of a collection of English-Chinese 
transliterations. 
 
English: Richard-??? (Li-Cha-De) 
Hackensack-????(Ha-Ken-
Sa-Ke) 
Chinese: Wen JiaBao-???(Wen-Jia-
Bao) 
ShenZhen???(Shen-Zhen) 
Japanese: Matsumoto-?? (Song-Ben) 
Hokkaido-???(Bei-Hai-Dao) 
Korean: Roh MooHyun-???(Lu-Wu-
Xuan) 
Taejon-??(Da-Tian) 
Vietnamese: Phan Van Khai-???(Pan-
Wen-Kai) 
Hanoi-??(He-Nei) 
 
In the case of machine transliteration, the name 
origins dictate the way we re-write a foreign word. 
For example, given a name written in English or 
Chinese for which we do not have a translation in 
                                                                            
Chinese characters in round brackets for ease of reading. 
56
  
a English-Chinese dictionary, we first have to de-
cide whether the name is of Chinese, Japanese, 
Korean or some European/English origins. Then 
we follow the transliteration rules implied by the 
origin of the source name. Although all English 
personal names are rendered in 26 letters, they 
may come from different romanization systems. 
Each romanization system has its own rewriting 
rules. English name ?Smith? could be directly 
transliterated into Chinese as ????(Shi-Mi-Si)? 
since it follows the English phonetic rules, while 
the Chinese translation of Japanese name ?Koi-
zumi? becomes ???(Xiao-Quan)? following the 
Japanese phonetic rules. The name origins are 
equally important in back-transliteration practice. 
Li et al (2007) incorporated name origin recogni-
tion to improve the performance of personal name 
transliteration. Besides multilingual processing, 
the name origin also provides useful semantic in-
formation (regional and language information) for 
common NLP tasks, such as co-reference resolu-
tion and name entity recognition. 
Unfortunately, little attention has been given to 
name origin recognition (NOR) so far in the litera-
ture. In this paper, we are interested in two kinds 
of name origin recognition: the origin of names 
written in English (ENOR) and the origin of 
names written in Chinese (CNOR). For ENOR, 
the origins include English (Eng), Japanese (Jap), 
Chinese Mandarin Pinyin (Man) and Chinese Can-
tonese Jyutping (Can). For CNOR, they include 
three origins: Chinese (Chi, for both Mandarin and 
Cantonese), Japanese and English (refer to Latin-
scripted language). 
Unlike previous work (Qu and Grefenstette, 
2004; Li et al, 2006; Li et al, 2007) where NOR 
was formulated with a generative model, we re-
gard the NOR task as a classification problem. We 
further propose using a discriminative learning 
algorithm (Maximum Entropy model: MaxEnt) to 
solve the problem. To draw direct comparison, we 
conduct experiments on the same personal name 
corpora as that in the previous work by Li et al 
(2006). We show that the MaxEnt method effec-
tively incorporates diverse features and outper-
forms previous methods consistently across all test 
cases. 
The rest of the paper is organized as follows: in 
section 2, we review the previous work. Section 3 
elaborates our proposed approach and the features. 
Section 4 presents our experimental setup and re-
ports our experimental results. Finally, we con-
clude the work in section 5. 
2 Related Work 
Most of previous work focuses mainly on ENOR 
although same methods can be extended to CNOR. 
We notice that there are two informative clues that 
used in previous work in ENOR. One is the lexical 
structure of a romanization system, for example, 
Hanyu Pinyin, Mandarin Wade-Giles, Japanese 
Hepbrun or Korean Yale, each has a finite set of 
syllable inventory (Li et al, 2006). Another is the 
phonetic and phonotactic structure of a language, 
such as phonetic composition, syllable structure. 
For example, English has unique consonant 
clusters such as /str/ and /ks/ which Chinese, 
Japanese and Korean (CJK) do not have. 
Considering the NOR solutions by the use of these 
two clues, we can roughly group them into two 
categories: rule-based methods (for solutions 
based on lexical structures) and statistical methods 
(for solutions based on phonotactic structures). 
Rule-based Method  
Kuo and Yang (2004) proposed using a rule-
based method to recognize different romanization 
system for Chinese only. The left-to-right longest 
match-based lexical segmentation was used to 
parse a test word. The romanization system is con-
firmed if it gives rise to a successful parse of the 
test word. This kind of approach (Qu and Grefen-
stette, 2004) is suitable for romanization systems 
that have a finite set of discriminative syllable in-
ventory, such as Pinyin for Chinese Mandarin. For 
the general tasks of identifying the language origin 
and romanization system, rule based approach 
sounds less attractive because not all languages 
have a finite set of discriminative syllable inven-
tory. 
Statistical Method 
1) N-gram Sum Method (SUM): Qu and Gre-
fenstette (2004) proposed a NOR identifier using a 
trigram language model (Cavnar and Trenkle, 
1994) to distinguish personal names of three lan-
guage origins, namely Chinese, Japanese and Eng-
lish. In their work, the training set includes 11,416 
Chinese name entries, 83,295 Japanese name en-
tries and 88,000 English name entries. However, 
the trigram is defined as the joint probabil-
57
  
ity 1 2( )i i ip c c c? ? for 3-character 1 2i i ic c c? ?  rather than 
the commonly used conditional probabil-
ity 1 2( | )i i ip c c c? ? . Therefore, the so-called trigram 
in Qu and Grefenstette (2004) is basically a sub-
string unigram probability, which we refer to as 
the n-gram (n-character) sum model (SUM) in this 
paper. Suppose that we have the unigram count 
1 2( )i i iC c c c? ? for character substring 1 2i i ic c c? ? , the 
unigram is then computed as: 
1 2
1 2
1 2
1 2,
( )
( )
( )
i i i
i i i
i i i
i i ii c c c
C c c c
p c c c
C c c c
? ?
? ?
? ?
? ?
= ?           (1) 
which is the count of character substring 1 2i i ic c c? ?  
normalized by the sum of all 3-character string 
counts in the name list for the language of interest.  
For origin recognition of Japanese names, this 
method works well with an accuracy of 92%. 
However, for English and Chinese, the results are 
far behind with a reported accuracy of 87% and 
70% respectively. 
2) N-gram Perplexity Method (PP): Li et al 
(2006) proposed using n-gram character perplexity 
cPP  to identify the origin of a Latin-scripted name. 
Using bigram, the cPP is defined as: 
1
1 log ( | )
2
Nc
i i 1ic
p c cN
cPP
?
=
? ?
=   (2) 
where cN is the total number of characters in the 
test name, ic is the i
th character in the test name. 
1( | )i ip c c ? is the bigram probability which is 
learned from each name list respectively. As a 
function of model, cPP  measures how good the 
model matches the test data. Therefore, cPP can be 
used to measure how good a test name matches a 
training set. A test name is identified to belong to 
a language if the language model gives rise to the 
minimum perplexity. Li et al (2006) shown that 
the PP method gives much better performance 
than the SUM method. This may be due to the fact 
that the PP measures the normalized conditional 
probability rather than the sum of joint probability. 
Thus, the PP method has a clearer mathematical 
interpretation than the SUM method. 
The statistical methods attempt to overcome the 
shortcoming of rule-based method, but they suffer 
from data sparseness, especially when dealing 
with a large character set, such as in Chinese (our 
experiments will demonstrate this point empiri-
cally). In this paper, we propose using Maximum 
Entropy (MaxEnt) model as a general framework 
for both ENOR and CNOR. We explore and inte-
grate multiple features into the discriminative clas-
sifier and use a common dataset for benchmarking. 
Experimental results show that the MaxEnt model 
effectively incorporates diverse features to demon-
strate competitive performance.   
3 MaxEnt Model and Features 
3.1 MaxEnt Model for NOR 
The principle of maximum entropy (MaxEnt) 
model is that given a collection of facts, choose a 
model consistent with all the facts, but otherwise 
as uniform as possible (Berger et al, 1996). Max-
Ent model is known to easily combine diverse fea-
tures. For this reason, it has been widely adopted 
in many natural language processing tasks. The 
MaxEnt model is defined as: 
( , )
1
1
( | ) j i
K
f c x
i j
j
p c x
Z
?
=
= ?           (3) 
      ( , )
1 1 1
( | ) j i
KN N
f c x
i j
i i j
Z p c x ?
= = =
= =? ??          (4) 
where ic is the outcome label, x is the given obser-
vation, also referred to as an instance. Z is a nor-
malization factor. N  is the number of outcome 
labels, the number of language origins  in our case. 
1 2, , , Kf f fL are feature functions and 
1 2, , , K? ? ?L are the model parameters. Each pa-
rameter corresponds to exactly one feature and can 
be viewed as a ?weight? for the corresponding fea-
ture.  
In the NOR task, c is the name origin label; x is 
a personal name, if is a feature function. All fea-
tures used in the MaxEnt model in this paper are 
binary. For example: 
 
1,    " "& (" ")
( , )
0,  j
if c Eng x contains str
f c x
otherwise
=?
= ??
 
In our implementation, we used Zhang?s maxi-
mum entropy package2. 
3.2 Features 
Let us use English name ?Smith? to illustrate the 
features that we define. All characters in a name 
                                                 
2 http://homepages.inf.ed.ac.uk/s0450736/maxent.html 
58
  
are first converted into upper case for ENOR be-
fore feature extraction. 
N-gram Features: N-gram features are de-
signed to capture both phonetic and orthographic 
structure information for ENOR and orthographic 
information only for CNOR. This is motivated by 
the facts that: 1) names written in English but from 
non-English origins follow different phonetic rules 
from the English one; they also manifest different 
character usage in orthographic form; 2) names 
written in Chinese follows the same pronunciation 
rules (Pinyin), but the usage of Chinese characters 
is distinguishable between different language ori-
gins as reported in Table 2 of (Li et al, 2007).  
The N-gram related features include: 
1) FUni: character unigram <S, M, I, T, H> 
2) FBi: character bigram <SM, MI, IT, TH> 
3) FTri: character trigram <SMI, MIT, ITH > 
Position Specific n-gram Features: We in-
clude position information into the n-gram fea-
tures. This is mainly to differentiate surname from 
given name in recognizing the origin of CJK per-
sonal names written in Chinese. For example, the 
position specific n-gram features of a Chinese 
name ????(Wen-Jia-Bao)? are as follows: 
1) FPUni: position specific unigram  
<0?(Wen), 1?(Jia), 2?(Bao)> 
2) FPBi: position specific bigram  
<0??(Wen-Jia), 1??(Jia-Bao)> 
3) FPTri: position specific trigram  
<0???(Wen-Jia-Bao)> 
Phonetic Rule-based Features: These features 
are inspired by the rule-based methods (Kuo and 
Yang, 2004; Qu and Grefenstette, 2004) that check 
whether an English name is a sequence of sylla-
bles of CJK languages in ENOR task. We use the 
following two features in ENOR task as well. 
1) FMan: a Boolean feature to indicate 
whether a name is a sequence of Chinese 
Mandarin Pinyin.   
2) FCan: a Boolean feature to indicate whether 
a name is a sequence of Cantonese Jyutping. 
Other Features:  
1) FLen: the number of Chinese characters in a 
given name. This feature is for CNOR only.  
The numbers of Chinese characters in per-
sonal names vary with their origins. For ex-
ample, Chinese and Korean names usually 
consist of 2 to 3 Chinese characters while 
Japanese names can have up to 4 or 5 Chi-
nese characters 
2) FFre: the frequency of n-gram in a given 
name. This feature is for ENOR only. In 
CJK names, some consonants or vowels 
usually repeat in a name as the result of the 
regular syllable structure. For example, in 
the Chinese name ?Zhang Wanxiang?, the 
bigram ?an? appears three times 
Please note that the trigram and position spe-
cific trigram features are not used in CNOR due to 
anticipated data sparseness in CNOR3.  
4 Experiments 
We conduct the experiments to validate the effec-
tiveness of the proposed method for both ENOR 
and CNOR tasks. 
4.1 Experimental Setting 
 
Origin #  entries Romanization System 
Eng4 88,799 English 
Man5 115,879 Pinyin 
Can 115,739 Jyutping 
Jap6 123,239 Hepburn 
 
Table 1: DE: Latin-scripted personal name corpus for 
ENOR 
 
 
Origin #  entries 
Eng7 37,644 
Chi8 29,795 
Jap9 33,897 
 
Table 2: DC: Personal name corpus written in Chinese 
characters for CNOR 
 
                                                 
3 In the test set of CNOR, 1080 out of 2980 names of Chinese 
origin do not consist of any bigrams learnt from training data, 
while 2888 out of 2980 names do not consist of any learnt 
trigrams. This is not surprising as most of Chinese names only 
have two or three Chinese characters and in our open testing, 
the train set is exclusive of all entries in the test set.  
4 http://www.census.gov/genealogy/names/ 
5 http://technology.chtsai.org/namelist/  
6 http://www.csse.monash.edu.au/~jwb/enamdict_doc.html 
7 Xinhua News Agency (1992)  
8 http://www.ldc.upenn.edu LDC2005T34 
9 www.cjk.org 
59
  
Datasets: We prepare two data sets which are col-
lected from publicly accessible sources: DE and DC 
for the ENOR and CNOR experiment respectively. 
DE is the one used in (Li et al, 2006), consisting of 
personal names of Japanese (Jap), Chinese (Man), 
Cantonese (Can) and English (Eng) origins. DC 
consists of personal names of Japanese (Jap), Chi-
nese (Chi, including both Mandarin and Canton-
ese) and English (Eng) origins. Table 1 and Table 
2 list their details. In the experiments, 90% of en-
tries in Table 1 (DE) and Table 2 (DC) are ran-
domly selected for training and the remaining 10% 
are kept for testing for each language origin. Col-
umns 2 and 3 in Tables 7 and 8 list the numbers of 
entries in the training and test sets.  
 
Evaluation Methods: Accuracy is usually used to 
evaluate the recognition performance (Qu and 
Gregory, 2004; Li et al, 2006; Li et al, 2007). 
However, as we know, the individual accuracy 
used before only reflects the performance of recall 
and does not give a whole picture about a multi-
class classification task. Instead, we use precision 
(P), recall (R) and F-measure (F) to evaluate the 
performance of each origin. In addition, an overall 
accuracy (Acc) is also given to describe the whole 
performance. The P, R, F and Acc are calculated 
as following: 
 
#        
#          
correctly recognized entries of the given origin
P
entries recognized as the given origin by the system
=
 
 
#        
#      
correctly recognized entries of the given origin
R
entries of the given origin
=
 
 
2PR
F
P R
=
+
     #     
#   
all correctly recognized entries
Acc
all entries
=
 
4.2 Experimental Results and Analysis 
Table 3 reports the experimental results of ENOR. 
It shows that the MaxEnt approach achieves the 
best result of 98.44% in overall accuracy when 
combining all the diverse features as listed in Sub-
section 3.2. Table 3 also measures the contribu-
tions of different features for ENOR by gradually 
incorporating the feature set. It shows that:  
1) All individual features are useful since the 
performance increases consistently when 
more features are being introduced. 
2) Bigram feature presents the most informa-
tive feature that gives rise to the highest 
performance gain, while the trigram feature  
further boosts performance too. 
3) MaxEnt method can integrate the advan-
tages of previous rule-based and statistical 
methods and easily integrate other features. 
 
F
ea
tu
re
s 
O
ri
gi
n 
P(
%)
    
R(
%)
 
F 
Ac
c(%
) 
Eng 91.40 80.76 85.75
Man 83.05 81.90 82.47
Can 81.13 82.76 81.94
FUni 
Jap 87.31 94.11 90.58
85.29
Eng 97.54 91.10 94.21
Man 97.51 98.10 97.81
Can 97.68 98.05 97.86
+FBi 
Jap 94.62 98.24 96.39
96.72
Eng 97.71 93.79 95.71
Man 98.94 99.37 99.16
Can 99.12 99.19 99.15
+FTri 
Jap 96.19 98.52 97.34
97.97
Eng 97.53 94.64 96.06
Man 99.21 99.43 99.32
Can 99.41 99.24 99.33
+FPUni 
Jap 96.48 98.49 97.47
98.16
Eng 97.68 94.98 96.31
Man 99.32 99.50 99.41
Can 99.53 99.34 99.44
+FPBi 
Jap 96.59 98.52 97.55
98.28
Eng 97.62 94.97 96.27
Man 99.34 99.58 99.46
Can 99.63 99.37 99.50
+FPTri 
Jap 96.61 98.45 97.52
98.30
Eng 97.74 95.06 96.38
Man 99.37 99.59 99.48
Can 99.61 99.41 99.51
+FFre 
Jap 96.66 98.56 97.60
98.35
Eng 97.82 95.11 96.45
Man 99.52 99.68 99.60
Can 99.71 99.59 99.65
 + FMan 
+ FCan 
Jap 96.69 98.59 97.63
98.44
 
Table 3: Contribution of each feature for ENOR 
 
 
60
  
Features Eng Jap Man Can 
FMan -0.357 0.069 0.072 -0.709 
FCan -0.424 -0.062 -0.775 0.066 
 
Table 4: Features weights in ENOR task. 
 
F
ea
tu
re
 
O
ri
gi
n 
P(
%
) 
R(
%
) 
F 
   A
cc(
%
) 
Eng 97.89 98.43 98.16
Chi 95.80 95.03 95.42FUni 
Jap 96.96 97.05 97.00
96.97 
Eng 96.99 98.27 97.63
Chi 96.86 92.11 94.43+FBi 
Jap 95.04 97.73 96.36
96.28 
Eng 97.35 98.38 97.86
Chi 97.29 95.00 96.13+FLen 
Jap 96.78 97.64 97.21
97.14 
Eng 97.74 98.65 98.19
Chi 97.65 96.34 96.99+FPUni 
Jap 97.91 98.05 97.98
97.77 
Eng 97.50 98.43 97.96
Chi 97.61 96.04 96.82+FPBi 
Jap 97.59 97.94 97.76
97.56 
Eng 98.08 99.04 98.56
Chi 97.57 96.88 97.22
FUni 
+FLen 
+ 
FPUni Jap 98.58 98.11 98.34
98.10 
 
Table 5: Contribution of each feature for CNOR 
 
Table 4 reports the feature weights of two fea-
tures ?FMan? and ?FCan? with regard to different 
origins in ENOR task. It shows that ?FCan? has 
positive weight only for origin ?Can? while 
?FMan? has positive weights for both origins 
?Man? and ?Jap?, although the weight for ?Man? 
is higher. This agrees with our observation that the 
two features favor origins ?Man? or ?Can?. The 
feature weights also reflect the fact that some 
Japanese names can be successfully parsed by the 
Chinese Mandarin Pinyin system due to their simi-
lar syllable structure. For example, the Japanese 
name ?Tanaka Miho? is also a sequence of Chi-
nese Pinyin: ?Ta-na-ka Mi-ho?.  
Table 5 reports the contributions of different 
features in CNOR task by gradually incorporating 
the feature set. It shows that:  
1) Unigram features are the most informative 
2) Bigram features degrade performance. This 
is largely due to the data sparseness prob-
lem as discussed in Section 3.2.   
3) FLen is also useful that confirms our intui-
tion about name length. 
Finally the combination of the above three use-
ful features achieves the best performance of 
98.10% in overall accuracy for CNOR as in the 
last row of Table 5. 
In Tables 3 and 5, the effectiveness of each fea-
ture may be affected by the order in which the fea-
tures are incorporated, i.e., the features that are 
added at a later stage may be underestimated. 
Thus, we conduct another experiment using "all-
but-one" strategy to further examine the effective-
ness of each kind of features. Each time, one type 
of the n-gram (n=1, 2, 3) features (including or-
thographic n-gram, position-specific and n-gram 
frequency features) is removed from the whole 
feature set. The results are shown in Table 6. 
 
F
ea
tu
re
s 
O
ri
gi
n 
P(
%)
 
R(
%)
 
F 
Ac
c(%
) 
Eng 97.81 95.01 96.39
Man 99.41 99.58 99.49
Can 99.53 99.48 99.50
w/o 
Uni-
gram 
Jap 96.63 98.52 97.57
98.34
Eng 97.34 95.17 96.24
Man 99.30 99.48 99.39
Can 99.54 99.33 99.43
w/o Bi-
gram 
Jap 96.73 98.32 97.52
98.26
Eng 97.57 94.10 95.80
Man 98.98 99.23 99.10
Can 99.20 99.08 99.14
w/o 
Tri-
gram 
Jap 96.06 98.42 97.23
97.94
 
Table 6: Effect of n-gram feature for ENOR 
 
Table 6 reveals that removing trigram features 
affects the performance most. This suggests that 
trigram features are much more effective for 
ENOR than other two types of features. It also 
shows that trigram features in ENOR does not suf-
fer from the data sparseness issue. 
As observed in Table 5, in CNOR task, 93.96% 
61
  
accuracy is obtained when removing unigram fea-
tures, which is much lower than 98.10% when bi-
gram features are removed. This suggests that uni-
gram features are very useful in CNOR, which is 
mainly due to the data sparseness problem that 
bigram features may have encountered. 
4.3 Model Complexity and Data Sparseness 
Table 7 (ENOR) and Table 8 (CNOR) compare 
our MaxEnt model with the SUM model (Qu and 
Gregory, 2004) and the PP model (Li et al, 2006). 
All the experiments are conducted on the same 
data sets as described in section 4.1. Tables 7 and 
8 show that the proposed MaxEnt model outper-
forms other models. The results are statistically 
significant ( 2? test with p<0.01) and consistent 
across all tests. 
Model Complexity: 
We look into the complexity of the models and 
their effects. Tables 7 and 8 summarize the overall 
accuracy of three models. Table 9 reports the 
numbers of parameters in each of the models. We 
are especially interested in a comparison between 
the MaxEnt and PP models because their perform-
ance is close.  We observe that, using trigram fea-
tures, the MaxEnt model has many more parame-
ters than the PP model does. Therefore, it is not 
surprising if the MaxEnt model outperforms when 
more training data are available. However, the ex-
periment results also show that the MaxEnt model 
consistently outperforms the PP model even with 
the same size of training data. This is largely at-
tributed to the fact that MaxEnt incorporates more 
robust features than the PP model does, such as 
rule-based, length of names features.  
One also notices that PP clearly outperforms 
SUM by using the same number of parameters in 
ENOR and shows comparable performance in 
CNOR tasks. Note that SUM and PP are different 
in two areas: one is the PP model employs word 
length normalization while SUM doesn?t; another 
that the PP model uses n-gram conditional prob-
ability while SUM uses n-character joint probabil-
ity. We believe that the improved performance of 
PP model can be attributed to the effect of usage 
of conditional probability, rather than length nor-
malization since length normalization does not 
change the order of probabilities. 
Data Sparesness: 
We understand that we can only assess the ef-
fectiveness of a feature when sufficient statistics is 
available. In CNOR (see Table 8), we note that the 
Chinese transliterations of English origin use only 
377 Chinese characters, so data sparseness is not a 
big issue. Therefore, bigram SUM and bigram PP 
methods easily achieve good performance for Eng-
lish origin. However, for Japanese origin (repre-
sented by 1413 Chinese characters) and Chinese 
origin (represented by 2319 Chinese characters), 
the data sparseness becomes acute and causes per-
formance degradation in SUM and PP models. We 
are glad to find that MaxEnt still maintains a good 
performance benefiting from other robust features. 
Table 10 compares the overall accuracy of the 
three methods using unigram and bigram features 
in CNOR task, respectively. It shows that the 
MaxEnt method achieves best performance. An-
other interesting finding is that unigram features 
perform better than bigram features for PP and  
MaxEnt models, which shows that  data sparseness 
remains an issue even for MaxEnt model.  
5 Conclusion 
We propose using MaxEnt model to explore di-
verse features for name origin recognition. Ex-
periment results show that our method is more ef-
fective than previously reported methods. Our 
contributions include: 
1) Cast the name origin recognition problem as 
a multi-class classification task and propose 
a MaxEnt solution to it; 
2) Explore and integrate diverse features for 
name origin recognition and propose the 
most effective feature sets for ENOR and 
for CNOR 
In the future, we hope to integrate our name 
origin recognition method with a machine translit-
eration engine to further improve transliteration 
performance. We also hope to study the issue of 
name origin recognition in context of sentence and 
use contextual words as additional features. 
References 
Adam L. Berger, Stephen A. Della Pietra and Vincent J. 
Della Pietra. 1996. A Maximum Entropy Approach 
to Natural Language Processing. Computational Lin-
guistics. 22(1):39?71. 
William B. Cavnar and John M. Trenkle. 1994. Ngram 
based text categorization. In 3rd Annual Symposium 
62
  
on Document Analysis and Information Retrieval, 
275?282. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics. 24(4), 
599-612. 
Jin-Shea Kuo and Ying-Kuei Yan. 2004. Generating 
Paired Transliterated-Cognates Using Multiple Pro-
nunciation Characteristics from Web Corpora. PA-
CLIC 18, December 8th-10th, Waseda University, 
Tokyo, Japan, 275?282. 
Haizhou Li, Shuanhu Bai and Jin-Shea Kuo. 2006. 
Transliteration. Advances in Chinese Spoken Lan-
guage Processing. World Scientific Publishing Com-
pany, USA, 341?364. 
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo and Minghui 
Dong. 2007. Semantic Transliteration of Personal 
Names. ACL-2007. 120?127. 
Xinhua News Agency. 1992. Chinese Transliteration of 
Foreign Personal Names. The Commercial Press  
Yan Qu and Gregory Grefenstette. 2004. Finding ideo-
graphic representations of Japanese names written in 
Latin script via language identification and corpus 
validation. ACL-2004. 183?190. 
Min Zhang, Jian Su and Haizhou Li. 2004. Direct Or-
thographical Mapping for Machine Translation. 
COLING-2004. 716-722. 
 
Trigram SUM Trigram PP MaxEnt Origin # training 
entries 
# test 
entries P (%) R(%) F P(%) R(%) F P(%) R(%) F 
Eng 79,920 8,879 94.66 72.50 82.11 95.84 94.72 95.28 97.82 95.11 96.45
Man 104,291 11,588 86.79 94.87 90.65 98.99 98.33 98.66 99.52 99.68 99.60
Can 104,165 11,574 90.03 93.87 91.91 96.17 99.67 97.89 99.71 99.59 99.65
Jap 110,951 12,324 89.17 92.84 90.96 98.20 96.29 97.24 96.69 98.59 97.63
Overall Acc (%) 89.57 97.39 98.44 
Table 7: Benchmarking different methods in ENOR task 
Bigram SUM  Bigram PP  MaxEnt Origin # training 
entries 
# test 
entries P(%) R(%) F P(%) R(%) F P(%) R(%) F 
Eng 37,644 3,765 95.94 98.65 97.28 97.58 97.61 97.60 98.08 99.04 98.56 
Chi 29,795 2,980 96.26 87.35 91.59 95.10 87.35 91.06 97.57 96.88 97.22 
Jap 33,897 3,390 93.01 97.67 95.28 90.94 97.43 94.07 98.58 98.11 98.34 
Overall Acc (%) 95.00 94.53 98.10 
Table 8: Benchmarking different methods in CNOR task 
# of parameters for ENOR # of parameters for CNOR 
Methods 
Trigram Unigram Bigram 
MaxEnt  124,692 13,496  182,116 
PP 16,851 4,045 86,490 
SUM  16,851 4,045 86,490 
 
Table 9: Numbers of parameters used in different methods 
 
 SUM PP MaxEnt 
Unigram Features 90.55 97.09 98.10 
Bigram Features 95.00 94.53 97.56 
 
Table 10: Overall accuracy using unigram and bigram features in CNOR task 
63
Multi-View Co-training of Transliteration Model 
 
Jin-Shea Kuo Haizhou Li 
Chung-Hwa Telecomm. 
Laboratories, Taiwan 
d8807302@gmail.com 
Institute for Infocomm Research, 
Singapore 119613 
hli@i2r.a-star.edu.sg 
 
Abstract 
This paper discusses a new approach to 
training of transliteration model from 
unlabeled data for transliteration extraction. 
We start with an inquiry into the 
formulation of transliteration model by 
considering different transliteration 
strategies as a multi-view problem, where 
each view exploits a natural division of 
transliteration features, such as phoneme-
based, grapheme-based or hybrid features. 
Then we introduce a multi-view Co-
training algorithm, which leverages 
compatible and partially uncorrelated 
information across different views to 
effectively boost the model from unlabeled 
data. Applying this algorithm to 
transliteration extraction, the results show 
that it not only circumvents the need of data 
labeling, but also achieves performance 
close to that of supervised learning, where 
manual labeling is required for all training 
samples. 
1 Introduction 
Named entities are important content words in text 
documents. In many applications, such as cross-
language information retrieval (Meng et al, 2001; 
Virga and Khudanpur, 2003) and machine 
translation (Knight and Graehl, 1998; Chen et al, 
2006), one of the fundamental tasks is to identify 
these words. Imported foreign proper names 
constitute a good portion of such words, which are 
newly translated into Chinese by transliteration. 
Transliteration is a process of translating a foreign 
word into the native language by preserving its 
pronunciation in the original language, otherwise 
known as translation-by-sound.  
As new words emerge everyday, no lexicon is 
able to cover all transliterations. It is desirable to 
find ways to harvest transliterations from real 
world corpora. In this paper, we are interested in 
the learning of English to Chinese (E-C) 
transliteration model for transliteration extraction 
from the Web. 
A statistical transliteration model is typically 
trained on a large amount of transliteration pairs, 
also referred to a bilingual corpus. The 
correspondence between a transliteration pair may 
be described by the mapping of different basic 
pronunciation units (BPUs) such as phoneme-
based1, or grapheme-based one, or both. We can 
see each type of BPU mapping as a natural division 
of transliteration features, which represents a view 
to the phonetic mapping problem. By using 
different BPUs, we approach the transliteration 
modeling and extraction problems from different 
views.  
This paper is organized as follows. In Section 2, 
we briefly introduce previous work. In Section 3, 
we conduct an inquiry into the formulation of 
transliteration model or phonetic similarity model 
(PSM) and consider it as a multi-view problem. In 
Section 4, we propose a multi-view Co-training 
strategy for PSM training and transliteration 
extraction. In Section 5, we study the effectiveness 
of proposed algorithms. Finally, we conclude in 
Section 6. 
2 Related Work 
Studies on transliteration have been focused on 
transliteration modeling and transliteration 
extraction. The transliteration modeling approach 
deduces either phoneme-based or grapheme-based 
mapping rules using a generative model that is 
                                                 
1 Both phoneme and syllable based approaches are 
referred to as phoneme-based in this paper. 
373
trained from a large bilingual corpus. Most of the 
works are devoted to phoneme-based transliteration 
modeling (Knight and Graehl, 1998; Lee, 1999). 
Suppose that EW is an English word and CW is its 
Chinese transliteration. EW and CW form an E-C 
transliteration pair. The phoneme-based approach 
first converts EW into an intermediate phonemic 
representation p, and then converts p into its 
Chinese counterpart CW. The idea is to transform 
both source and target words into comparable 
phonemes so that the phonetic similarity between 
two words can be measured easily.  
Recently the grapheme-based approach has 
attracted much attention. It was proposed by Jeong 
et al (1999), Li et al (2004) and many others (Oh 
et al, 2006b), which is also known as direct 
orthography mapping. It treats the transliteration as 
a statistical machine translation problem under 
monotonic constraint. The idea is to obtain the 
bilingual orthographical correspondence directly to 
reduce the possible errors introduced in multiple 
conversions. However, the grapheme-based 
transliteration model has more parameters than 
phoneme-based one does, thus expects a larger 
training corpus. 
Most of the reported works have been focused 
on either phoneme- or grapheme-based approaches. 
Bilac and Tanaka (2004) and Oh et al (2006a; 
2006b) recently proposed using a mix of phoneme 
and grapheme features, where both features are 
fused into a single learning process. The feature 
fusion was shown to be effective. However, their 
methods hinge on the availability of a labeled 
bilingual corpus. 
In transliteration extraction, mining translations 
or transliterations from the ever-growing 
multilingual Web has become an active research 
topic, for example, by exploring query logs (Brill et 
al., 2001) and parallel (Nie et al, 1999) or 
comparable corpora (Sproat et al, 2006). 
Transliterations in such a live corpus are typically 
unlabeled. For model-based transliteration 
extraction, recent progress in machine learning 
offers different options to exploit unlabeled data, 
that include active learning (Lewis and Catlett, 
1994) and Co-training (Nigam and Ghani, 2000; 
T?r et al 2005). 
Taking the prior work a step forward, this paper 
explores a new way of fusing phoneme and 
grapheme features through a multi-view Co-
training algorithm (Blum and Mitchell, 1998), 
which starts with a small number of labeled data to 
bootstrap a transliteration model to automatically 
harvest transliterations from the Web. 
3 Phonetic Similarity Model with 
Multiple Views 
Machine transliteration can be formulated as a 
generative process, which takes a character string 
in source language as input and generates a 
character string in the target language as output. 
Conceptually, this process can be regarded as a 3-
step decoding: segmentation of both source and 
target strings into basic pronunciation units (BPUs), 
relating the source BPUs with target units by 
resolving different combinations of alignments and 
unit mappings in finding the most probable BPU 
pairs. A BPU can be defined as a phoneme 
sequence, a grapheme sequence, or a part of them. 
A transliteration model establishes the phonetic 
relationship between BPUs in two languages to 
measure their similarity, therefore, it is also known 
as the phonetic similarity model (PSM). 
 To introduce the multi-view concept, we 
illustrate the BPU transfers in Figure 1, where each 
transfer is represented by a direct path with 
different line style. There are altogether four 
different paths: the phoneme-based path V1 
(T1?T2?T3), the grapheme-based path V4 (T4), 
and their variants, V2(T1?T5) and V3(T6?T3). The 
last two paths make use of the intermediate BPU 
mappings between phonemes and graphemes. Each 
of the paths represents a view to the mapping 
problem. Given a labeled bilingual corpus, we are 
able to train a transliteration model for each view 
easily.   
 
 
Figure 1. Multiple views for establishing 
transliteration correspondence. 
 
The E-C transliteration has been studied 
extensively in the paradigm of noisy channel model 
Source 
Phoneme 
Target 
Phoneme 
Source 
Word 
Target  
Word 
T1 
T2 
T4 
T3 
T5 T6 
374
(Manning and Scheutze, 1999), with EW as the 
observation and CW as the input to be recovered. 
Applying Bayes rule, the transliteration can be 
described by Eq. (1),  
( | ) ( )( | ) ,
( )
P EW CW P CWP CW EW
P EW
?=               (1) 
where we need to deal with two probability 
distributions: P(EW|CW), the probability of 
transliterating CW to EW, also known as the unit 
mapping rules, and P(CW), the probability 
distribution of CW, known as the target language 
model. 
Representing EW in English BPU 
sequence 1{ ,... ,... }= m MEP ep ep ep  and CW in 
Chinese one, 1{ ,... ,... }= n NCP cp cp cp , a typical 
transliteration probability can be expressed as, 
 
( | ) ( | ) ( | ) ( | ).P EW CW P EW EP P EP CP P CP CW? ? ?   (2) 
 
The language model, P(CW), can be represented by 
Chinese characters n-gram statistics (Manning and 
Scheutze, 1999) and expressed in Eq. (3). In the 
case of bigram, we have, 
1 1
2
( ) ( ) ( | )
N
n n
n
P CW P c P c c ?
=
? ?          (3) 
We next rewrite Eq. (2) for the four different views 
depicted in Figure 1 in a systematic manner. 
3.1 Phoneme-based Approach 
The phoneme-based approach approximates the 
transliteration probability distribution by 
introducing an intermediate phonemic 
representation. In this way, we convert the words in 
the source language, say 1 2, ... KEW e e e= , into 
English syllables ES , then Chinese syllables CS  
and finally the target language, say Chinese 
1 2, ... KCW c c c=  in sequence. Eq. (2) can be 
rewritten by replacing EP and CP with ES and CS, 
respectively, and expressed by Eq. (4). 
 
( | ) ( | ) ( | ) ( | )P EW CW P EW ES P ES CS PCS CW? ? ?       (4) 
 
The three probabilities correspond to the three-step 
mapping in V1 path.  
The phoneme-based approach suffers from 
multiple step mappings. This could compromise 
overall performance because none of the three 
steps guarantees a perfect conversion.  
3.2 Grapheme-based Approach 
The grapheme-based approach is inspired by the 
transfer model (Vauqois, 1988) in machine 
translation that estimates ( | )P EW CW  directly 
without interlingua representation. This method 
aims to alleviate the imprecision introduced by the 
multiple transfers in phoneme-based approach. 
In practice, a grapheme-based approach converts 
the English graphemes to Chinese graphemes in 
one single step. Suppose that we have 
1 2, ... KEW e e e= and 1 2, ... KCW c c c= where ke  and 
kc are aligned grapheme units.  
Under the noisy channel model, we can estimate 
( | )P EW CW  based on the alignment statistics 
which is similar to the lexical mapping in statistical 
machine translation.  
1
( | ) ( | )K k kkP EW CW P e c=??     (5) 
Eq.(5) is a grapheme-based alternative to Eq.(2).  
3.3 Hybrid Approach 
A tradeoff between the phoneme- and grapheme-
based approaches is to take shortcuts to the 
mapping between phonemes and graphemes of two 
languages via V2 or V3, where only two steps of 
mapping are involved. For V3, we rewrite Eq.(2) as 
Eq. (6): 
 
( | ) ( | ) ( | ),= ?P EW CW P EW CS P CS CW         (6) 
 
where ( | )P EW CS  translates Chinese sounds into 
English words. For V2, we rewrite Eq. (2) as Eq. 
(7): 
 
( | ) ( | ) ( | ),= ?P EW CW P EW ES P ES CW         (7) 
 
where ( | )P ES CW translates Chinese words into 
English sounds. 
Eqs. (4) ? (7) describe the four paths of 
transliteration. In a multi-view problem, one 
partitions the domain?s features into subsets, each 
of which is sufficient for learning the target 
concept. Here the target concept is the label of 
transliteration pair. Given a collection of E-C pair 
candidates, the transliteration extraction task can be 
formulated as a hypothesis test, which makes a 
binary decision as to whether a candidate E-C pair 
is a genuine transliteration pair or not. Given an E-
C pair X={EW,CW}, we have 0H , which 
375
hypothesizes that EW  and CW  form a genuine E-
C pair, and 1H , which hypothesizes otherwise. The 
likelihood ratio is given as 0 1( | ) / ( | )P X H P X H? = , 
where 0( | )P X H and 0( | )P X H  are derived from 
P(EW|CW). By comparing ?  with a threshold ? , 
we make the binary decision as that in (Kuo et al, 
2007).  
As discussed, each view takes a distinct path that 
has its own advantages and disadvantages in terms 
of model expressiveness and complexity. Each 
view represents a weak learner achieving 
moderately good performance towards the target 
concept. Next, we study a multi-view Co-training 
process that leverages the data of different views 
from each other in order to boost the accuracy of a 
PSM model.  
4 Multi-View Learning Framework 
The PSM can be trained in a supervised manner 
using a manually labeled corpus. The advantage of 
supervised learning is that we can establish a model 
quickly as long as labeled data are available. 
However, this method suffers from some practical 
constraints. First, the derived model can only be as 
good as the data it sees. Second, the labeling of 
corpus is labor intensive.  
To circumvent the need of manual labeling, here 
we study three adaptive strategies cast in the 
machine learning framework, namely unsupervised 
learning, Co-training and Co-EM. 
4.1 Unsupervised Learning 
Unsupervised learning minimizes human 
supervision by probabilistically labeling data 
through an Expectation and Maximization (EM) 
(Dempster et al, 1977) process. The unsupervised 
learning strategy can be depicted in Figure 2 by 
taking the dotted path, where the extraction process 
accumulates all the acquired transliteration pairs in 
a repository for training a new PSM. A new PSM is 
in turn used to extract new transliteration pairs. The 
unsupervised learning approach only needs a few 
labeled samples to bootstrap the initial model for 
further extraction. Note that the training samples 
are noisy and hence the quality of initial PSM 
therefore has a direct impact on the final 
performance.  
4.2 Co-training and Co-EM  
The multi-view setting (Muslea et al, 2002) 
applies to learning problems that have a natural 
way to divide their features into different views, 
each of which is sufficient to learn the target 
concept. Blum and Mitchell (1998) proved that for 
a problem with two views, the target concept can 
be learned based on a few labeled and many 
unlabeled examples, provided that the views are 
compatible and uncorrelated. Intuitively, the 
transliteration problem has compatible views. If an 
E-C pair forms a transliteration, then this is true 
across all different views. However, it is arguable 
that the four views in Figure 1 are uncorrelated. 
Studies (Nigam and Ghani, 2000; Muslea et al, 
2002) shown that the views do not have to be 
entirely uncorrelated for Co-training to take effect. 
This motivates our attempt to explore multi-view 
Co-training for learning models in transliteration 
extraction. 
 
  
Figure 2. Diagram of unsupervised/multi-view Co-
training for transliteration extraction. 
 
To simplify the discussion, here we take a two-
view (V1 and V2) example to show how Co-
training can potentially help. To start with, one can 
learn a weak hypothesis PSM1 using V1 based on a 
few labeled examples and then apply PSM1 to all 
unlabeled examples. If the views are uncorrelated, 
or at least partially uncorrelated, these newly 
labeled examples seen from V1 augment the 
training set for V2. These newly labeled examples 
Stop Start 
Iterate 
Final 
PSM 
Initial 
PSM 
Search &  
Ranking 
PSM Learner 
Lexicon The Web 
Training 
Repository 
PSM  
Evaluation & Stop 
Criterion 
Unsupervised 
Co-training 
PSM Learner 1 
Training 
Repository 
PSM Learner n 
376
present new information from the V2 point of view, 
from which one can in turn update the PSM2. As 
the views are compatible, both V1 and V2 label the 
samples consistently according to the same 
probabilistic transliteration criteria. In this way, 
PSMs are boosted each other through such an 
iterative process between two different views.  
 
 
Table 1. Co-training with two learners. 
Extending the two-view to multi-view, one can 
develop multiple learners from several subsets of 
features, each of which approaches the problem 
from a unique perspective, called a view when 
taking the Co-training path in Figure 2. Finally, we 
use outputs from multi-view learners to 
approximate the manual labeling. The multi-view 
learning is similar to unsupervised learning in the 
sense that the learning alleviates the need of 
labeling and starts with very few labeled data. 
However, it is also different from the unsupervised 
learning because the latter does not leverage the 
natural split of compatible and uncorrelated 
features. Two variants of two-view learning 
strategy can be summarized in Table 1 and Table 2, 
where the algorithm in Table 1 is referred to as Co-
training and the one in Table 2 as Co-EM (Nigam 
and Ghani. 2000; Muslea et al, 2002). 
In Co-training, Learners A and B are trained on 
the same training data and updated simultaneously. 
In Co-EM, Learners A and B are trained on labeled 
set predicted by each other?s view, with their 
models being updated in sequence. In other words, 
the Co-EM algorithm interchanges the probabilistic 
labels generated in the view of each other before a 
new EM iteration. In both cases, the unsupervised, 
multi-view algorithms use the hypotheses learned 
to probabilistically label the examples.  
 
 
Table 2. Co-EM with two learners. 
The extension of algorithms in Table 1 and 2 to 
the multi-view transliteration problem is 
straightforward. After an ensemble of learners are 
trained, the overall PSM can be expressed as a 
linear combination of the learners,  
1
( | ) ( | ),n i iiP EW CW w P EW CW==?             (8) 
where iw is the weight of ith learner ( | )iP EW CW , 
which can be learnt by using a development corpus.  
5 Experiments 
To validate the effectiveness of the learning 
framework, we conduct a series of experiments in 
transliteration extraction on a development corpus 
described later. First, we repeat the experiment in 
(Kuo et al, 2006) to train a PSM using PSA and 
GSA feature fusion in a supervised manner, which 
serves as the upper bound of Co-training or Co-EM 
system performance. We then train the PSMs with 
single view V1, V2, V3 and V4 alone in an 
unsupervised manner. The performance achieved 
by each view alone can be considered as the 
baseline for multi-view benchmarking. Then, we 
run two-view Co-training for different 
combinations of views on the same development 
corpus. We expect to see positive effects with the 
multi-view training. Finally, we run the 
experiments using two-view Co-training and Co-
EM and compare the results. 
A 500 MB development corpus is constructed by 
crawling pages from the Web for the experiments. 
We first establish a gold standard for performance 
evaluation by manually labeling the corpus based 
on the following criteria: (i) if an EW is partly 
Given  
a). A small set of labeled samples and a set of 
unlabeled samples. 
b). Learner A is trained on a labeled set to 
predict the labels of the unlabeled data. 
 
1) Loop for k iterations 
a). Learner B is trained on data labeled by 
Learner A to predict the labels of the 
unlabeled data; 
b). Learner A is trained on data labeled  by 
Learner B to predict the labels of the 
unlabeled data;   
2) Combine models from Learners A and B. 
Given: 
a). A small set of labeled samples and a set 
of unlabeled samples. 
b). Two learners A and B are trained on the 
labeled set. 
 
1) Loop for k iterations: 
a). Learners A and B predict the labels of 
the unlabeled data to augment the labeled 
set; 
b). Learners A and B are trained on the 
augmented labeled set.    
2) Combine models from Learners A and B. 
377
translated phonetically and partly translated 
semantically, only the phonetic transliteration 
constituent is extracted to form a transliteration 
pair; (ii) multiple E-C pairs can appear in one 
sentence; (iii) an EW can have multiple valid 
Chinese transliterations and vice versa.  
We first derive 80,094 E-C pair candidates from 
the 500 MB corpus by spotting the co-occurrence 
of English and Chinese words in the same 
sentences. This can be done automatically without 
human intervention. Then, the manual labeling 
process results in 8,898 qualified E-C pairs, also 
referred to as Distinct Qualified Transliteration 
Pairs (DQTPs).  
 To establish comparison, we first train a PSM 
using all 8,898 DQTPs in a supervised manner and 
conduct a closed test as reported in Table 3. We 
further implement three PSM learning strategies 
and conduct a systematic series of experiments by 
following the recognition followed by validation 
strategy proposed in (Kuo et al, 2007). 
 
 Precision Recall F-measure 
Closed test 0.834 0.663 0.739 
Table 3. Performance with PSM trained in the 
supervised manner. 
For performance benchmarking, we define the 
precision as the ratio of extracted number of 
DQTPs over that of total extracted pairs, recall as 
the ratio of extracted number of DQTPs over that 
of total DQTPs, and F-measure as in Eq. (9). They 
are collectively referred to as extraction 
performance. 
2 recall precisionF measure
recall precision
? ?? =
+
            (9) 
5.1 Unsupervised Learning 
As formulated in Section 4.1, first, we derive an 
initial PSM using randomly selected 100 seed 
DQTPs for each learner and simulate the Web-
based learning process: (i) extract E-C pairs using 
the PSM; (ii) add all of the extracted E-C pairs to 
the DQTP pool; (iii) re-estimate the PSM for each 
view by using the updated DQTP pool. This 
process is also known as semi-supervised EM 
(Muslea et al, 2002). 
As shown in Figure 3, the unsupervised learning 
algorithm consistently improves the initial PSM 
using in all four views. To appreciate the 
effectiveness of each view, we report the F-
measures on each individual view V1, V2, V3 and 
V4, as 0.680, 0.620, 0.541 and 0.520, respectively at 
the 6th iteration. We observe that V1, the phoneme-
based path, achieves the best result. 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
1 2 3 4 5 6
#Iteration
F-
me
as
ur
e
Supervised
V1
V2
V3
V4
Figure 3. F-measure over iterations using 
unsupervised learning with individual view. 
5.2 Co-training (CT) 
We report three typical combinations of two co-
working learners or two-view Co-training. Like in 
unsupervised learning, we start with the same 100 
seed DQTPs and an initial PSM model by 
following the algorithm in Table 1 over 6 iterations. 
With two-view Co-training, we obtain 0.726, 
0.705, 0.590 and 0.716 in terms of F-measures for 
V1+V2, V2+V3, V3+V4 and V1+V4 at the 6th 
iteration, as shown in Figure 4. Comparing Figure 
3 and 4, we find that Co-training consistently 
outperforms unsupervised learning by exploiting 
compatible information across different views. The 
V1+V2 Co-training outperforms other Co-training 
combinations, and surprisingly achieves close 
performance to that of supervised learning.  
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
1 2 3 4 5 6
#Iteration
F-
me
as
ur
e
Supervised
V1
V1+V2
V2+V3
V3+V4
V1+V4
 
Figure 4. F-measure over iterations using Co-
training algorithm 
378
5.3 Co-EM (CE) 
Next we start with the same 100 seed DQTPs by 
initializing the training pool and carry out Co-EM 
on the same corpus. We build PSM1 for Learner A 
and PSM2 for Learner B. To start with, PSM1 is 
learnt from the initial labeled set. We then follow 
the algorithm in Table 2 by looping in the 
following two steps over 6 iterations: (i) estimate 
the PSM2 from the samples labeled by Learner A 
(V1) to extract the high confident E-C pairs and 
augment the DQTP pool with the probabilistically 
labeled E-C pairs; (ii) estimate the PSM1 from the 
samples labeled by Learner B (V2) to extract the 
high confident E-C pairs and augment the DQTP 
pool with the probabilistically labeled E-C pairs. 
We report the results in Figure 5. 
 
0.5
0.6
0.7
0.8
1 2 3 4 5 6
#Iteration
F-
me
as
ur
e
Supervised
CT-V1+V2
CE-V1+V2
 
Figure 5. Comparing F-measure over iterations 
between Co-training (CT) and Co-EM (CE). 
 
To summarize, we compare the performance of 
six learning methods studied in this paper in Table 
4. The Co-training and Co-EM learning approaches 
have alleviated the need of manual labeling, yet 
achieving performance close to supervised learning. 
The multi-view learning effectively leverages 
multiple compatible and partially uncorrelated 
views. It reduces the need of labeled samples from 
80,094 to just 100.  
We also compare the multi-view learning 
algorithm with active learning on the same 
development corpus using same features. We 
include the results from previously reported work 
(Kuo et al, 2006) into Table 4 (see Exp. 2) where 
multiple features are fused in a single active 
learning process. In Exp. 2, PSA feature is the 
equivalent of V1 feature in Exp. 4; GSA feature is 
the equivalent of V4 feature in Exp. 4. In Exp. 4, 
we carry out V1+V4 two-view Co-training. It is 
interesting to find that the multi-view learning in 
this paper achieves better results than active 
learning in terms of F-measure while reducing the 
need of manual labeling from 8,191 samples to just 
100.  
 
Exp. Learning algorithm F-measure 
# of 
samples 
to label 
1 Supervised 0.739 80,094 
2 Active Learning 
(Kuo et al, 2006) 0.710 8,191 
3 Unsupervised (V1) 0.680 100 
4 Co-training (V1+V4) 0.716 100 
5 Co-training (V1+V2) 0.726 100 
6 Co-EM (V1+V2) 0.725 100 
Table 4. Comparison of six learning strategies.  
6 Conclusions 
Fusion of phoneme and grapheme features in 
transliteration modeling was studied in many 
previous works. However, it was done through the 
combination of phoneme and grapheme similarity 
scores (Bilac and Tanaka, 2004), or by pooling 
phoneme and grapheme features together into a 
single-view training process (Oh and Choi, 2006b). 
This paper presents a new approach that leverages 
the information across different views to 
effectively boost the learning from unlabeled data. 
We have shown that both Co-training and Co-
EM not only outperform the unsupervised learning 
of single view, but also alleviate the need of data 
labeling. This reaffirms that multi-view is a viable 
solution to the learning of transliteration model and 
hence transliteration extraction. Moving forward, 
we believe that contextual feature in documents 
presents another compatible, uncorrelated, and 
complementary view to the four views. 
We validate the effectiveness of the proposed 
algorithms by conducting experiments on 
transliteration extraction. We hope to extend the 
work further by investigating the possibility of 
applying the multi-view learning algorithms to 
machine translation.  
References 
S. Bilac and H. Tanaka. 2004. Improving back-
transliteration by combining information sources, In 
Proc. of Int?l Joint Conf. on Natural Language 
Processing, pp. 542-547. 
379
S. Blum and T. Mitchell. 1998. Combining Labeled and 
Unlabeled Data with Co-training, In Proc. of 11th 
Conference on Computational Learning Theory, pp. 
92-100. 
E. Brill, G. Kacmarcik and C. Brockett. 2001. 
Automatically Harvesting Katakana-English Term 
Pairs from Search Engine Query Logs, In Proc. of 
Natural Language Processing Pacific Rim 
Symposium (NLPPRS), pp. 393-399. 
H.-H. Chen, W.-C. Lin, C.-H. Yang and W.-H. Lin. 
2006, Translating-Transliterating Named Entities for 
Multilingual Information Access, Journal of the 
American Society for Information Science and 
Technology, 57(5), pp. 645-659. 
A. P. Dempster, N. M. Laird and D. B. Rubin. 1977. 
Maximum Likelihood from Incomplete Data via the 
EM Algorithm, Journal of the Royal Statistical 
Society, Ser. B. Vol. 39, pp. 1-38. 
K. S. Jeong, S. H. Myaeng, J. S. Lee and K.-S. Choi. 
1999. Automatic Identification and Back-
transliteration of Foreign Words for Information 
Retrieval, Information Processing and Management, 
Vol. 35, pp. 523-540. 
K. Knight and J. Graehl. 1998. Machine Transliteration, 
Computational Linguistics, Vol. 24, No. 4, pp. 599-
612. 
J.-S. Kuo, H. Li and Y.-K. Yang. 2006. Learning 
Transliteration Lexicons from the Web, In Proc. of 
44th ACL, pp. 1129-1136. 
J.-S. Kuo, H. Li and Y.-K. Yang. 2007. A Phonetic 
Similarity Model for Automatic Extraction of 
Transliteration Pairs, ACM Transactions on Asian 
Language Information Processing. 6(2), pp. 1-24. 
J.-S. Lee. 1999. An English-Korean Transliteration and 
Retransliteration Model for Cross-Lingual 
Information Retrieval, PhD Thesis, Department of 
Computer Science, KAIST. 
D. D. Lewis and J. Catlett. 1994. Heterogeneous 
Uncertainty Sampling for Supervised Learning, In 
Proc. of Int?l Conference on Machine Learning 
(ICML), pp. 148-156. 
H. Li, M. Zhang and J. Su. 2004. A Joint Source 
Channel Model for Machine Transliteration, In Proc. 
of 42nd ACL, pp. 159-166. 
C. D. Manning and H. Scheutze. 1999. Fundamentals of 
Statistical Natural Language Processing, The MIT 
Press. 
H. M. Meng, W.-K. Lo, B. Chen and T. Tang. 2001. 
Generate Phonetic Cognates to Handle Name Entities 
in English-Chinese Cross-Language Spoken 
Document Retrieval, In Proceedings of Automatic 
Speech Recognition Understanding (ASRU), pp. 311-
314. 
I. Muslea, S. Minton and C. A. Knoblock. 2002. Active 
+ Semi-supervised learning = Robust Multi-View 
Learning, In Proc. of the 9th Int?l Conference on 
Machine Learning, pp. 435-442. 
J.-Y. Nie, P. Isabelle, M. Simard and R. Durand. 1999. 
Cross-language Information Retrieval based on 
Parallel Texts and Automatic Mining of Parallel Text 
from the Web, In Proc. of 22nd ACM SIGIR, pp 74-81. 
K. Nigam and R. Ghani. 2000. Analyzing the 
Effectiveness and Applicability of Co-training, In 
Proc. of the 9th Conference in Information and 
Knowledge and Management, pp. 86-93. 
J.-H. Oh, K.-S. Choi and H. Isahara. 2006a. A Machine 
Transliteration Model based on Graphemes and 
Phonemes, ACM TALIP, Vol. 5, No. 3, pp. 185-208. 
J.-H. Oh and K.-S. Choi. 2006b. An Ensemble of 
Transliteration Models for Information Retrieval, In 
Information Processing and Management, Vol. 42, pp. 
980-1002. 
R. Sproat, T. Tao and C. Zhai. 2006. Named Entity 
Transliteration with Comparable Corpora, In Proc. of 
44th ACL, pp. 73-80. 
G. T?r, D. Hakkani-T?r and R. E. Schapire. 2005. 
Combining Active and Semi-supervised Learning for 
Spoken Language Understanding, Speech 
Communication, 45, pp. 171-186. 
B. Vauqois. 1988. A Survey of Formal Grammars and 
Algorithms for Recognition and Transformation in 
Machine Translation, IFIP Congress-68, reprinted  
TAO: Vingtcinq Ans de Traduction Automatique - 
Analectes in C. Boitet, Ed., Association Champollin, 
Grenoble, pp.201-213 
P. Virga and S. Khudanpur. 2003. Transliteration of 
Proper Names in Cross-Lingual Information Retrieval, 
In Proceedings of 41st ACL Workshop on 
Multilingual and Mixed Language Named Entity 
Recognition, pp. 57-64. 
380
Mining Transliterations from Web Query Results: 
An Incremental Approach 
Jin-Shea Kuo Haizhou Li Chih-Lung Lin 
Chung-Hwa Telecomm.  
Laboratories, Taiwan 
d8807302 @gmail.com 
Institute for Infocomm  
Research, Singapore 119613 
hzli@ieee.com 
Chung Yuan Christian 
 University, Taiwan 
linclr@gmail.com 
 
Abstract 
We study an adaptive learning framework 
for phonetic similarity modeling (PSM) that 
supports the automatic acquisition of trans-
literations by exploiting minimum prior 
knowledge about machine transliteration to 
mine transliterations incrementally from the 
live Web. We formulate an incremental 
learning strategy for the framework based 
on Bayesian theory for PSM adaptation. 
The idea of incremental learning is to bene-
fit from the continuously developing his-
tory to update a static model towards the in-
tended reality. In this way, the learning 
process refines the PSM incrementally 
while constructing a transliteration lexicon 
at the same time on a development corpus. 
We further demonstrate that the proposed 
learning framework is reliably effective in 
mining live transliterations from Web query 
results. 
1 Introduction 
Transliteration is a process of rewriting a word 
from one language into another by preserving its 
pronunciation in its original language, also known 
as translation-by-sound. It usually takes place be-
tween languages with different scripts, for example, 
from English to Chinese, and words, such as proper 
nouns, that do not have ?easy? or semantic transla-
tions. 
The increasing size of multilingual content on 
the Web has made it a live information source rich 
in transliterations. Research on automatic acquisi-
tion of transliteration pairs in batch mode has 
shown promising results (Kuo et al, 2006). In 
dealing with the dynamic growth of the Web, it is 
almost impossible to collect and store all its con-
tents in local storage. Therefore, there is a need to 
develop an incremental learning algorithm to mine 
transliterations in an on-line manner. In general, an 
incremental learning technique is designed for 
adapting a model towards a changing environment. 
We are interested in deducing the incremental 
learning method for automatically constructing an 
English-Chinese (E-C) transliteration lexicon from 
Web query results.  
In the deduction, we start with a phonetic simi-
larity model (PSM), which measures the phonetic 
similarity between words in two different scripts, 
and study the learning mechanism of PSM in both 
batch and incremental modes. The contributions of 
this paper include: (i) the formulation of a batch 
learning framework and an incremental learning 
framework for PSM learning; (ii) a comparative 
study of the batch and incremental unsupervised 
learning strategies. 
In this paper, Section 2 briefly introduces prior 
work related to machine transliteration. In Section 
3, we formulate the PSM and its batch and incre-
mental learning algorithms while in Section 4, we 
discuss the practical issues in implementation. Sec-
tion 5 provides a report on the experiments con-
ducted and finally, we conclude in Section 6. 
2 Related Work 
Much of research on extraction of transliterations 
has been motivated by information retrieval tech-
niques, where attempts to extracting transliteration 
pairs from large bodies of corpora have been made. 
Some have proposed extracting translations from 
parallel or comparable bitexts using co-occurrence 
analysis or a context-vector approach (Fung and 
Yee, 1998; Nie et al, 1999). These methods com-
pare the semantic similarities between source and 
target words without taking their phonetic similari-
ties into account.  
Another direction of research is focused on es-
16
Sixth SIGHAN Workshop on Chinese Language Processing
tablishing the phonetic relationship between trans-
literation pairs. This typically involves the encod-
ing of phoneme- or grapheme-based mapping rules 
using a generative model trained from a large bi-
lingual lexicon. Suppose that EW and CW form an 
E-C transliteration pair. The phoneme-based ap-
proach (Knight & Graehl, 1998) first converts EW 
into an intermediate phonemic representation and 
then converts the phonemic representation into its 
Chinese counterpart CW. The grapheme-based ap-
proach, also known as direct orthographical map-
ping (Li et al, 2004), which treats transliteration as 
a statistical machine translation problem under 
monotonic constraints, has also achieved promising 
results. 
Many efforts have also been channeled to tap-
ping the wealth of the Web for harvesting translit-
eration/translation pairs. These include studying the 
query logs (Brill et al, 2001), unrelated corpora 
(Rapp, 1999), and comparable corpora (Sproat et al 
2006). To establish cross-lingual correspondence in 
the harvest, these algorithms usually rely on one or 
more statistical clues (Lam et al, 2004), such as 
the correlation between word frequencies, and cog-
nates of similar spelling or pronunciations. In doing 
so, two things are needed: first, a robust mecha-
nism that establishes statistical relationships be-
tween bilingual words, such as a phonetic similar-
ity model which is motivated by transliteration 
modeling research; and second, an effective learn-
ing framework that is able to adaptively discover 
new events from the Web.  
In Chinese/Japanese/Korean (CJK) Web pages, 
translated terms are frequently accompanied by 
their original Latin words, with the Latin words 
serving as the appositives of the CJK words. In 
other words, the E-C pairs are always closely col-
located. Inspired by this observation in CJK texts, 
some algorithms were proposed (Kuo et al, 2006) 
to search over the close context of an English word 
in a Chinese predominant bilingual snippet for 
transliteration.  
Unfortunately, many of the reported works have 
not taken the dynamic nature of the Web into ac-
count. In this paper, we study the learning frame-
work of the phonetic similarity model, which 
adopts a transliteration modeling approach for 
transliteration extraction from the Web in an in-
cremental manner.   
3 Phonetic Similarity Model 
Phonetic similarity model (PSM) is a probabilistic 
model that encodes the syllable mapping between 
E-C pairs. Let 1{ ,... ,... }m MES e e e= be a sequence of 
English syllables derived from EW and 
1{ ,... ,... }n NCS s s s=  be the sequence of Chinese syl-
lables derived from CW, represented by a Chinese 
character string 1,... ,...,n NCW w w w? . If each of the 
English syllables is drawn from a vocabulary of X 
entries, 1{ ,..., }m Ie x x? , and each of the Chinese 
syllable from a vocabulary of Y entries, 
1{ ,..., }n Js y y? , then the E-C transliteration can be 
considered as a generative process formulated by 
the noisy channel model, which recovers the input 
CW from the observed output EW. Applying 
Bayesian rule, we have Eq. (1), where ( | )P EW CW  
is estimated to characterize the noisy channel, 
known as the transliteration probability and 
( )P CW  is a language model to characterize the 
source language.  
( | ) ( | ) ( ) / ( )P CW EW P EW CW P CW P EW= . (1) 
Following the translation-by-sound principle, 
( | )P EW CW can be approximated by the phonetic 
probability ( | )P ES CS , which is given by Eq. (2).  
( | ) max ( , | ),P ES CS P ES CS
???
= ?     (2) 
where ?  is the set of all possible alignment paths 
between ES and CS. To find the best alignment 
path ? , one can resort to a dynamic warping algo-
rithm (Myers and Rabiner, 1981). Assuming condi-
tional independence of syllables in ES and CS, we 
have 
1
( | ) ( | )
k k
K
m nk
P ES CS P e s==?  where k is the 
index of alignment. We rewrite Eq.(1) as, 
( | ) ( | ) ( ) / ( )P CW EW P ES CS P CW P EW? .  (3) 
The language model ( )P CW in Eq.(3) can be repre-
sented by the n-gram statistics of the Chinese char-
acters derived from a monolingual corpus. Using 
bigram to approximate the n-gram model, we have 
1 12
( ) ( ) ( | )
N
n nn
P CW P w P w w ?=? ? .  (4) 
Removing ( )P EW  from Eq.(3) which is not a func-
tion of CW, a PSM ? now consists of both 
( | )P ES CS and ( )P CW  parameters (Kuo et al, 
2007). We now look into the mathematic formula-
tion for the learning of ( | )P ES CS  parameters from 
a bilingual transliteration lexicon.  
3.1 Batch Learning of PSM  
17
Sixth SIGHAN Workshop on Chinese Language Processing
A collection of manually selected or automatically 
extracted E-C pairs can form a transliteration lexi-
con. Given such a lexicon for training, the PSM 
parameters can be estimated in a batch mode. An 
initial PSM is bootstrapped using limited prior 
knowledge such as a small amount of translitera-
tions, which may be obtained by exploiting co-
occurrence information (Sproat et al, 2006). Then 
we align the E-C pairs using the PSM ? and derive 
syllable mapping statistics.  
Suppose that we have the event counts ,i jc =  
( , )m i n jcount e x s y= = , and ( )j n jc count s y= =  for a 
given transliteration lexicon D with alignments ? . 
We would like to find the parameters 
|i jP = ( | )m i n jP e x s y= = , ,m ne s< >?? , that maxi-
mize the probability, 
,
|
( , | ) ( | ) i j
c
m n j i i j
P D P e s P?? ? = =? ? ? ,       (5) 
where |{ , 1,..., , 1,..., }i jP i I j J? = = = , with maximum 
likelihood estimation (MLE) criteria, subject to the 
constraints of | 1,i jiP j= ?? . Rewriting Eq.(5) in 
log-likelihood ( LL )  
, |
( , | )
log ( | ) logm n i j i j
j i
LL D
P e s c P?
? ?
= =? ??                 (6) 
It is described as the cross-entropy of the true data 
distribution ,i jc with regard to the PSM model. 
Given an alignment ??? , the MLE estimate of 
PSM is: 
| , /i j i j jP c c= .              (7) 
With a new PSM, one is able to arrive at a new 
alignment. This is formulated as an expectation-
maximization (EM) process (Dempster, 1977), 
which assumes that there exists a mappingD?? , 
where ?  is introduced as the latent information, 
also known as missing data in the EM literature. 
The EM algorithm maximizes the likelihood prob-
ability ( | )P D ?  over ?  by exploiting 
( | ) ( , | )P D P D?? = ? ?? .  
The EM process guarantees non-decreasing like-
lihood probability ( | )P D ? through multiple EM 
steps until it converges. In the E-step, we derive the 
event counts ,i jc  and jc  by force-aligning all the 
E-C pairs in the training lexicon D  using a PSM. 
In the M-step, we estimate the PSM parameters ?  
by Eq.(7). The EM process also serves as a refining 
process to obtain the best alignment between the E-
C syllables. In each EM cycle, the model is updated 
after observing the whole corpus D . An EM cycle 
is also called an iteration in batch learning. The 
batch learning process is described as follows and 
depicted in Figure 1. 
 
 
Figure 1. Batch learning of PSM   
 
Batch Learning Algorithm: 
Start: Bootstrap PSM parameters |i jP using prior 
phonetic mapping knowledge; 
E-Step: Force-align corpus D  using |i jP  to obtain 
?  and hence the counts of ,i jc  and jc ; 
M-Step: Re-estimate | , /i j i j jP c c=  using the counts 
from E-Step; 
Iterate: Repeat E-Step and M-Step until ( | )P D ?  
converges; 
3.2 Incremental Learning of PSM  
In batch learning all the training samples have to be 
collected in advance. In a dynamically changing 
environment, such as the Web, new samples always 
appear and it is impossible to collect all of them. 
Incremental learning (Zavaliagkos, 1995) is de-
vised to achieve rapid adaptation towards the work-
ing environment by updating the model as learning 
samples arrive in sequence. It is believed that if the 
statistics for the E-step are incrementally collected 
and the parameters are frequently estimated, incre-
mental learning converges quicker because the in-
formation from the new data contributes to the pa-
rameter estimation more effectively than the batch 
algorithm does (Gotoh et al, 1998). In incremental 
learning, the model is typically updated progres-
sively as the training samples become available and 
the number of incremental samples may vary from 
as few as one to as many as they are available. In 
the extreme case where all the learning samples are 
Iterate 
Initial 
PSM 
E-Step 
 
Training 
Corpus 
M-Step Final PSM 
18
Sixth SIGHAN Workshop on Chinese Language Processing
available and the updating is done after observing 
all of them, the incremental learning becomes batch 
learning. Therefore, the batch learning can be con-
sidered as a special case of the incremental learning. 
The incremental learning can be formulated 
through maximum a posteriori (MAP) framework, 
also known as Bayesian learning, where we assume 
that the parameters ?  are random variables subject 
to a prior distribution. A possible candidate for the 
prior distribution of |i jP  is the Dirichlet density 
over each of the parameters |i jP (Bacchiani et al, 
2006). Let |{ , 1,..., }j i jP i I? = = , we introduce, 
| 1
|( ) ,i j
h
j i ji
P P j? ?? ? ?? ,   (8) 
where | 1i ji h =? , and ? , which can be empirically 
set, is a positive scalar. Assuming H is the set of 
hyperparameters, we have as many hyperparame-
ters |i jh H? as the parameters |i jP . The probability 
of generating the aligned transliteration lexicon is 
obtained by integrating over the parameter space, 
( ) ( | ) ( )P D P D P d= ? ? ?? . 
This integration can be easily written down in a 
closed form due to the conjugacy between Dirichlet 
distribution | 1| i j
h
i ji
P ? ??  and the multinomial dis-
tribution ,
|
i jc
i i j
P? . Instead of finding ?  that 
maximizes ( | )P D ? with MLE, we maximize a 
posteriori (MAP) probability as follows: 
argmax ( | ) argmax ( | ) ( ) / ( )
argmax ( | ) ( ) (9)
MAP P D P D P P D
P D P
? ?
?
? = ? = ? ?
= ? ?
The MAP solution uses a distribution to model the 
uncertainty of the parameters ? , while the MLE 
gives a point estimation (Jelinek, 1990; MacKay, 
1994). We rewrite Eq.(9) as Eq.(10) using Eq.(5) 
and Eq.(8).  
, | 1
|argmax i j i j
j
c hmap
j i ji
P ?+ ?
?
? ? ?                        (10) 
Eq.(10) can be seen as a Dirichlet function of ?  
given H , or a multinomial function of H given ? . 
With given prior H , the MAP estimation is there-
fore similar to the MLE problem which is to find 
the mode of the kernel density in Eq.(10).  
| | |(1 )i j i j i jP h f? ?= + ? ,             (11) 
where | , /i j i j jf c c= , ,/( )i ji c? ? ?= +? . 
One can find that ?  serves as a weighting factor 
between the prior and the current observations. The 
difference between MLE and MAP strategy lies in 
the fact that MAP introduces prior knowledge into 
the parameter updating formula. Eq.(11) assumes 
that the prior parameters H  are known and static 
while the training samples are available all at once.  
The idea of incremental learning is to benefit 
from the continuously developing history to update 
the static model towards the intended reality. As is 
often the case, the Web query results in an on-line 
application arrive in sequence. It is of practical use 
to devise such an incremental mechanism that 
adapts both parameters and the prior knowledge 
over time. The quasi-Bayesian (QB) learning 
method offers a solution to it (Bai and Li, 2006). 
Let?s break up a training corpus D into a se-
quence of sample subsets 1 2{ , ,..., }TD D D D=  and 
denote an accumulated sample subset ( )tD =  
1 2{ , ,..., },1tD D D t T? ?  as an incremental corpus. 
Therefore, we have ( )TD D= . The QB method ap-
proximates the posterior probability ( 1)( | )tP D ??  
by the closest tractable prior density ( 1)( | )tP H ??  
with ( 1)tH ? evolved from historical corpus ( 1)tD ? ,  
( 1)
,
( ) ( )
( 1)
1
|1
argmax ( | )
argmax ( | ) ( | )
argmax , .
t
i j i
t t
QB
t
t
I c h
i ji
P D
P D P D
P j?
?
?
?
?
+ ?
=?
? = ?
? ? ?
= ??
          (12) 
QB estimation offers a recursive learning 
mechanism. Starting with a hyperparameter set 
(0)H  and a corpus subset 1D , we estimate (1)H  and 
(1)
QB? , then (2)H  and (2)QB?  and so on until ( )tH  and 
( )t
QB?  as observed samples arrive in sequence. The 
updating of parameters can be iterated between the 
reproducible prior and posterior estimates as in Eq. 
(13) and Eq. (14). Assuming T ?? , we have the 
following: 
 
Incremental Learning Algorithm: 
Start: Bootstrap (0)QB?  and (0)H using prior phonetic 
mapping knowledge and set 1t = ; 
E-Step: Force-align corpus subset tD  using ( 1)tQB?? , 
compute the event counts ( ),ti jc  and reproduce prior 
parameters ( 1) ( )t tH H? ? . 
( ) ( 1) ( )
,| | /
t t t
i ji j i jh h c
?= + ?           (13) 
19
Sixth SIGHAN Workshop on Chinese Language Processing
M-Step: Re-estimate parameters ( )( ) tt QBH ??  and 
|i jP  using the counts from E-Step. 
( ) ( ) ( )
| | |/
t t t
i j i j i ji
P h h= ?           (14) 
EM cycle: Repeat E-Step and M-Step until 
( )( | )tP D?  converges. 
Iterate: Repeat T EM cycles covering the entire 
data set D in an iteration. 
 
The algorithm updates the PSM as training sam-
ples become available. The scalar factor ?  can be 
seen as a forgetting factor. When ?  is big, the up-
date of hyperparameters favors the prior. Otherwise, 
current observation is given more attention. As for 
the sample subset size | |tD , if we set | | 100tD = , 
each EM cycle updates ?  after observing every 
100 samples. To be comparable with batch learning, 
we define an iteration here to be a sequence of EM 
cycles that covers the whole corpus D. If corpus D 
has a fixed size ( )| |TD , an iteration means T EM 
cycles in incremental learning.  
4 Mining Transliterations from the Web 
Since the Web is dynamically changing and new 
transliterations come out all the time, it is better to 
mine transliterations from the Web in an incre-
mental way. Words transliterated by closely ob-
serving common guidelines are referred to as regu-
lar transliterations. However, in Web publishing, 
translators in different regions may not observe the 
same guidelines. Sometimes they skew the translit-
erations in different ways to introduce semantic 
implications, also known as wordplay, resulting in 
casual transliterations. Casual transliteration leads 
to multiple Chinese transliteration variants for the 
same English word. For example, ?Disney? may be 
transliterated into ????/Di-Shi-Ni/1?, ????
/Di-Si-Nai/? and ????/Di-Si-Nai/?.  
Suppose that a sufficiently large, manually vali-
dated transliteration lexicon is available, a PSM 
can be built in a supervised manner. However, this 
method hinges on the availability of such a lexicon.  
Even if a lexicon is available, the derived model 
can only be as good as what the training lexicon 
offers. New transliterations, such as casual ones, 
may not be well handled. It is desirable to adapt the 
PSM as new transliterations become available, also 
                                                 
1 The Chinese words are romanized in Hanyu Pinyin. 
referred to as the learning-at-work mechanism. 
Some solutions have been proposed recently along 
this direction (Kuo et al, 2006). However, the ef-
fort was mainly devoted to mitigating the need of 
manual labeling. A dynamic learning-at-work 
mechanism for mining transliterations has not been 
well studied. 
Here we are interested in an unsupervised learn-
ing process, in which we adapt the PSM as we ex-
tract transliterations. The learning-at-work frame-
work is illustrated in Figure 2. As opposed to a 
manually labeled training corpus in Figure 1, we 
insert into the EM process an automatic translitera-
tion extraction mechanism, search and rank, as 
shown in the left panel of Figure 2. The search and 
rank shortlists a set of transliterations from the 
Web query results or bilingual snippets. 
 
 
Figure 2. Diagram of unsupervised transliteration 
extraction ? learning-at-work. 
4.1 Search and Rank 
We obtain bilingual snippets from the Web by 
iteratively submitting queries to the Web search 
engines (Brin and Page, 1998). Qualified sentences 
are extracted from the results of each query. Each 
qualified sentence has at least one English word.  
Given a qualified sentence, first, the competing 
Chinese transliteration candidates are denoted as a 
set ? , from which we would like to pick the most 
likely one. Second, we would like to know if there 
is indeed a Chinese transliteration CW in the close 
context of the English word EW. 
We propose ranking the candidates using the 
PSM model to find the most likely CW for a given 
EW. The CW candidate that gives the highest poste-
rior probability is considered the most probable 
Final 
PSM 
Initial PSM 
E-Step 
M-Step 
The Web 
 
Transliterations 
Search and 
Rank 
Final  
Lexicon 
Iterate 
20
Sixth SIGHAN Workshop on Chinese Language Processing
candidate CW ? .  
argmax ( | )
argmax ( | ) ( )
CW
CW
CW P CW EW
P ES CS P CW
??
??
? =
?           (15) 
The next step is to examine if CW ?  and EW indeed 
form a genuine E-C pair. We define the confidence 
of the E-C pair as the posterior odds similar to that 
in a hypothesis test under the Bayesian interpreta-
tion. We have 0H , which hypothesizes that 
CW ? and EW  form an E-C pair, and 1H , which 
hypothesizes otherwise, and use posterior odd ?  
(Kuo et al, 2006) for hypothesis tests. 
Our search and rank formulation can be seen as 
an extension to a prior work (Brill et al, 2001). 
The posterior odd ?  is used as the confidence 
score so that E-C pairs extracted from different 
contexts can be directly compared. In practice, we 
set a threshold for ?  to decide on a cutoff point for 
E-C pairs short-listing. In this way, the search and 
rank is able to retrieve a collection of translitera-
tions from the Web given a PSM. 
4.2 Unsupervised Learning Strategy 
Now we can carry out PSM learning as formulated 
in Section 3 using the transliterations as if they 
were manually validated. By unsupervised batch 
learning, we mean to re-estimate the PSM after 
search and rank over the whole database, i.e., in 
each iteration. Just as in supervised learning, one 
can expect the PSM performance to improve over 
multiple iterations. We report the F-measure at 
each iteration. The extracted transliterations also 
form a new training corpus in next iteration. 
In contrast to the batch learning, incremental 
learning updates the PSM parameters as the train-
ing samples arrive in sequence. This is especially 
useful in Web mining. With the QB incremental 
optimization, one can think of an EM process that 
continuously re-estimates PSM parameters as the 
Web crawler discovers new ?territories?. In this 
way, the search and rank process gathers qualified 
training samples tD after crawling a portion of the 
Web. Note that the incremental EM process up-
dates parameters more often than batch learning 
does. To evaluate performance of both learning, we 
define an iteration to be T EM cycles in incre-
mental learning on a training corpus ( )TD D=  as 
discussed in Section 3.2.  
5 Experiments 
To obtain the ground truth for performance evalua-
tion, each possible transliteration pair is manually 
checked based on the following criteria: (i) only the 
phonetic transliteration is extracted to form a trans-
literation pair; (ii) multiple E-C pairs may appear in 
one sentence; (iii) an EW can have multiple valid 
Chinese transliterations and vice versa. The valida-
tion process results in a collection of qualified E-C 
pairs, also referred to as distinct qualified translit-
eration pairs (DQTPs), which form a translitera-
tion lexicon. 
To simulate the dynamic Web, we collected a 
Web corpus, which consists of about 500 MB of 
Web pages, referred to as SET1. From SET1, 
80,094 qualified sentences were automatically ex-
tracted and 8,898 DQTPs were further selected 
with manual validation.  
To establish a reference for performance bench-
marking, we first initialize a PSM, referred to as 
seed PSM hereafter, using randomly selected 100 
seed DQTPs. By exploiting the seed PSM on all 
8,898 DQTPs, we train a PSM in a supervised 
batch mode and improve the PSM on SET1 after 
each iteration. The performance defined below in 
precision, recall and F-measure in the 6th iteration 
is reported in Table 1 and the F-measure is also 
shown in Figure 3.  
# _ /# _ ,
# _ /# _ ,
2 /( )
precision extracted DQTPs extracted pairs
recall extracted DQTPs total DQTPs
F measure recall precision recall precision
=
=
? = ? ? +
  
 
 Precision Recall F-measure 
Closed-test 0.834 0.663 0.739 
Table 1. The performance achieved by supervised 
batch learning on SET1. 
 
We use this closed test (supervised batch learning) 
as the reference point for unsupervised experiments. 
Next we further implement two PSM learning 
strategies, namely unsupervised batch and unsu-
pervised incremental learning. 
5.1 Unsupervised Batch Learning 
We begin with the same seed PSM. However, we 
use transliterations that are extracted automatically 
instead of manually validated DQTPs for training. 
Note that the transliterations are extracted and col-
lected at the end of each iteration. It may differ 
from one iteration to another. After re-estimating 
21
Sixth SIGHAN Workshop on Chinese Language Processing
the PSM in each iteration, we evaluate performance 
on SET1. 
Comparing the two batch mode learning strate-
gies in Figure 3, it is observed that learning sub-
stantially improves the seed PSM after the first it-
eration. Without surprise, the supervised learning 
consistently outperforms the unsupervised one, 
which reaches a plateau at 0.679 F-measure. This 
performance is considered as the baseline for com-
parison in this paper. The unsupervised batch learn-
ing presented here is similar to that in (Kuo et al, 
2006).  
0.45
0.55
0.65
0.75
1 2 3 4 5 6
#Iteration
F-
m
ea
su
re
Supervised Batch
Unsupervised Batch
U-Incremental (100)
U-Incremental (5,000)
 
Figure 3. Comparison of F-measure over iterations 
(U-Incremental: Unsupervised Incremental). 
5.2 Unsupervised Incremental Learning 
We now formulate an on-line2 unsupervised incre-
mental learning algorithm: 
(i) Start with the seed PSM, set 1t = ; 
(ii) Extract | |tD  quasi-transliterations pairs fol-
lowed by E-Step in incremental learning algo-
rithm; 
(iii) Re-estimate PSM using | |tD  (M-Step), 1t t= + ; 
(iv) Repeat (ii) and (iii) to crawl over a corpus. 
 
To simulate the on-line incremental learning just 
described, we train and test on SET1 because of the 
availability of gold standard and comparison with 
performance by batch mode. We empirically set 
0.5? =  and study different | |tD settings. An itera-
tion is defined as multiple cycles of steps (ii)-(iii) 
that screen through the whole SET1 once. We run 
multiple iterations. 
The performance of incremental learning with 
| | 100tD = and | | 5,000tD = are reported in Figure 3. 
It is observed that incremental learning benefits 
from more frequent PSM updating. With | | 100tD = , 
it not only attains good F-measure in the first itera-
                                                 
2 In an actual on-line environment, we are not supposed to 
store documents, thus no iteration can take place. 
tion, but also outperforms that of unsupervised 
batch learning along the EM process. The PSM 
updating becomes less frequent for larger | |tD . 
When | |tD  is set to be the whole corpus, then in-
cremental learning becomes a batch mode learning, 
which is evidenced by | | 5,000tD =  and it performs 
close to the batch mode learning. The experiments 
in Figure 3 are considered closed tests. Next we 
move on to an actual on-line experiment. 
5.3 Learning from the Live Web  
In practice, it is possible to extract bilingual snip-
pets of interest by repeatedly submitting queries to 
the Web. With the learning-at-work mechanism, 
we can mine the query results for up-to-date trans-
literations in an on-line environment. For example, 
by submitting ?Amy? to search engines, we may 
get ?Amy-??/Ai-Mi/? and, as a by-product, ?Jes-
sica-???/Jie-Xi-Ka/? as well. In this way, new 
queries can be generated iteratively, thus new pairs 
are discovered.  
Following the unsupervised incremental learning 
algorithm, we start the crawling with the same seed 
PSM as in Section 5.2. We adapt the PSM as every 
100 quasi-transliterations are extracted, i.e. 
| | 100tD = . The crawling stops after accumulating 
67,944 Web pages, where there are 100 snippets at 
most in a page, with 2,122,026 qualified sentences. 
We obtain 123,215 distinct E-C pairs when the 
crawling stops. For comparison, we also carry out 
unsupervised batch learning over the same 
2,122,026 qualified sentences in a single iteration 
under such an on-line environment. As the gold 
standard for this live corpus is not available, we 
randomly select 500 quasi-transliteration pairs for 
manual checking of precision (see Table 2). It is 
found that incremental learning is more productive 
than batch learning in discovering transliteration 
pairs. This finding is consistent with the test results 
on SET1. 
 
 Unsupervised Batch 
Unsupervised 
Incremental  
#distinct E-C pairs 67,708 123,215 
Estimated Precision 0.758 0.768 
Table 2. Comparison between the unsupervised 
batch and incremental learning from live Web. 
 
The live Web corpus was used in transliteration 
extraction using active learning (Kuo et al, 2006). 
22
Sixth SIGHAN Workshop on Chinese Language Processing
Kuo et al reported slightly better performance by 
annotating some samples manually and adapting 
the learning process in a batch manner. However, it 
is apparent that, in an on-line environment, the un-
supervised learning is more suitable for discovering 
knowledge without resorting to human annotation; 
incremental learning is desirable as it does not re-
quire storing all documents in advance.  
6 Conclusions 
We have proposed a learning framework for min-
ing E-C transliterations using bilingual snippets 
from a live Web corpus. In this learning-at-work 
framework, we formulate the PSM learning method 
and study strategies for PSM learning in both batch 
and incremental manners. The batch mode learning 
benefits from multiple iterations for improving per-
formance, while the unsupervised incremental one, 
which does not require all the training data to be 
available in advance, adapts to the dynamically 
changing environment easily without compromis-
ing the performance. Unsupervised incremental 
learning provides a practical and effective solution 
to transliteration extraction from query results, 
which can be easily extended to other Web mining 
applications.  
References 
S. Bai and H. Li. 2006. Bayesian Learning of N-gram 
Statistical Language Modeling, In Proc. of ICASSP, 
pp. 1045-1048. 
M. Bacchiani, B. Roark, M. Riley and R. Sproat. 2006. 
MAP Adaptation of Stochastic Grammars, Computer 
Speech and Language, 20(1), pp. 41-68. 
E. Brill, G. Kacmarcik, C. Brockett. 2001. Automati-
cally Harvesting Katakana-English Term Pairs from 
Search Engine Query Logs, In Proc. of NLPPRS, pp. 
393-399. 
S. Brin and L. Page. 1998. The Anatomy of a Large-
scale Hypertextual Web Search Engine, In Proc. of 7th 
WWW, pp. 107-117. 
A. P. Dempster, N. M. Laird and D. B. Rubin. 1977. 
Maximum Likelihood from Incomplete Data via the 
EM Algorithm, Journal of the Royal Statistical Soci-
ety, Ser. B. Vol. 39, pp. 1-38. 
P. Fung and L.-Y. Yee. 1998. An IR Approach for 
Translating New Words from Nonparallel, Compara-
ble Texts, In Proc. of 17th COLING and 36th ACL, pp. 
414-420. 
Y. Gotoh, M. M. Hochberg and H. F. Silverman. 1998. 
Efficient Training Algorithms for HMM?s Using In-
cremental Estimation, IEEE Trans. on Speech and 
Audio Processing, 6(6), pp. 539-547. 
F. Jelinek. 1999. Self-organized Language Modeling for 
Speech Recognition, Readings in speech recognition, 
Morgan Kaufmann, pp. 450-506. 
D. Jurafsky and J. H. Martin. 2000. Speech and Lan-
guage Processing, pp. 102-120, Prentice-Hall, New 
Jersey. 
K. Knight and J. Graehl. 1998. Machine Transliteration, 
Computational Linguistics, 24(4), pp. 599-612. 
J.-S. Kuo, H. Li and Y.-K. Yang. 2006. Learning Trans-
literation Lexicons from the Web, In Proc. of 44th 
ACL, pp. 1129-1136. 
J.-S. Kuo, H. Li and Y.-K. Yang. 2007. A Phonetic 
Similarity Model for Automatic Extraction of Trans-
literation Pairs, ACM TALIP, 6(2), pp. 1-24.  
H. Li, M. Zhang and J. Su. 2004. A Joint Source Chan-
nel Model for Machine Transliteration, In Proc. of 
42nd ACL, pp. 159-166. 
W. Lam, R.-Z. Huang and P.-S. Cheung. 2004. Learning 
Phonetic Similarity for Matching Named Entity 
Translations and Mining New Translations, In Proc. 
of 27th ACM SIGIR, pp. 289-296. 
D. MacKay and L. Peto. 1994. A Hierarchical Dirichlet 
Language Model, Natural Language Engineering, 
1(3), pp.1-19.  
C. S. Myers and L. R. Rabiner. 1981. A Comparative 
Study of Several Dynamic Time-warping Algorithms 
for Connected word Recognition, The Bell System 
Technical Journal, 60(7), pp. 1389-1409. 
J.-Y. Nie, P. Isabelle, M. Simard and R. Durand. 1999. 
Cross-language Information Retrieval based on Paral-
lel Texts and Automatic Mining of Parallel Text from 
the Web, In Proc. of 22nd ACM SIGIR, pp. 74-81. 
R. Rapp. 1999. Automatic Identification of Word Trans-
lations from Unrelated English and German Corpora, 
In Proc. of 37th ACL, pp. 519-526. 
R. Sproat, T. Tao and C. Zhai. 2006. Named Entity 
Transliteration with Comparable Corpora, In Proc. of 
44th ACL, pp. 73-80. 
G. Zavaliagkos, R. Schwartz, and J. Makhoul. 1995. 
Batch, Incremental and Instantaneous Adaptation 
Techniques for Speech Recognition, In Proc. of 
ICASSP, pp. 676-679. 
23
Sixth SIGHAN Workshop on Chinese Language Processing
A Joint Source-Channel Model for Machine Transliteration 
Li Haizhou, Zhang Min, Su Jian 
 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, Singapore 119613 
{hli,sujian,mzhang}@i2r.a-star.edu.sg 
 
Abstract 
Most foreign names are transliterated into 
Chinese, Japanese or Korean with 
approximate phonetic equivalents. The 
transliteration is usually achieved through 
intermediate phonemic mapping. This 
paper presents a new framework that 
allows direct orthographical mapping 
(DOM) between two different languages, 
through a joint source-channel model, also 
called n-gram transliteration model (TM). 
With the n-gram TM model, we automate 
the orthographic alignment process to 
derive the aligned transliteration units from 
a bilingual dictionary. The n-gram TM 
under the DOM framework greatly reduces 
system development effort and provides a 
quantum leap in improvement in 
transliteration accuracy over that of other 
state-of-the-art machine learning 
algorithms. The modeling framework is 
validated through several experiments for 
English-Chinese language pair.  
1 Introduction 
In applications such as cross-lingual information 
retrieval (CLIR) and machine translation, there is 
an increasing need to translate out-of-vocabulary 
words from one language to another, especially 
from alphabet language to Chinese, Japanese or 
Korean.  Proper names of English, French, 
German, Russian, Spanish and Arabic origins 
constitute a good portion of out-of-vocabulary 
words. They are translated through transliteration, 
the method of translating into another language by 
preserving how words sound in their original 
languages. For writing foreign names in Chinese, 
transliteration always follows  the original 
romanization. Therefore, any foreign name will 
have only one Pinyin (romanization of Chinese) 
and thus in Chinese characters. 
In this paper, we focus on automatic Chinese 
transliteration of foreign alphabet names. Because 
some alphabet writing systems use various 
diacritical marks, we find it more practical to write 
names containing such diacriticals as they are 
rendered in English. Therefore, we refer all 
foreign-Chinese transliteration to English-Chinese 
transliteration, or E2C.  
Transliterating English names into Chinese is 
not straightforward. However, recalling the 
original from Chinese transliteration is even more 
challenging as the E2C transliteration may have 
lost some original phonemic evidences. The 
Chinese-English backward transliteration process 
is also called back-transliteration, or C2E (Knight 
& Graehl, 1998).  
In machine transliteration, the noisy channel 
model (NCM), based on a phoneme-based 
approach, has recently received considerable 
attention (Meng et al 2001; Jung et al 2000; Virga 
& Khudanpur, 2003; Knight & Graehl, 1998). In 
this paper we discuss the limitations of such an 
approach and address its problems by firstly 
proposing a paradigm that allows direct 
orthographic mapping (DOM), secondly further 
proposing a joint source-channel model as a 
realization of DOM. Two other machine learning 
techniques, NCM and ID3 (Quinlan, 1993) 
decision tree, also are implemented under DOM as 
reference to compare with the proposed n-gram 
TM. 
This paper is organized as follows: In section 2, 
we present the transliteration problems. In section 
3, a joint source-channel model is formulated. In 
section 4, several experiments are carried out to 
study different aspects of proposed algorithm. In 
section 5, we relate our algorithms to other 
reported work. Finally, we conclude the study with 
some discussions. 
2 Problems in transliteration 
Transliteration is a process that takes a character 
string in source language as input and generates a 
character string in the target language as output. 
The process can be seen conceptually as two levels 
of decoding: segmentation of the source string into 
transliteration units; and relating the source 
language transliteration units with units in the 
target language, by resolving different 
combinations of alignments and unit mappings. A 
unit could be a Chinese character or a monograph, 
a digraph or a trigraph and so on for English. 
2.1 Phoneme-based approach 
The problems of English-Chinese transliteration 
have been studied extensively in the paradigm of 
noisy channel model (NCM). For a given English 
name E as the observed channel output, one seeks 
a posteriori the most likely Chinese transliteration 
C that maximizes P(C|E). Applying Bayes rule, it 
means to find C to maximize 
 
P(E,C) = P(E | C)*P(C)                       (1) 
 
with equivalent effect. To do so, we are left with 
modeling two probability distributions: P(E|C), the 
probability of transliterating C to E through a noisy 
channel, which is also called transformation rules, 
and P(C), the probability distribution of source, 
which reflects what is considered good Chinese 
transliteration in general. Likewise, in C2E back-
transliteration, we would find E that maximizes 
 
P(E,C) = P(C | E)*P(E)                       (2) 
 
for a given Chinese name.  
In eqn (1) and (2), P(C) and P(E) are usually 
estimated using n-gram language models (Jelinek, 
1991). Inspired by research results of grapheme-to-
phoneme research in speech synthesis literature, 
many have suggested phoneme-based approaches 
to resolving P(E|C) and P(C|E), which 
approximates the probability distribution by 
introducing a phonemic representation. In this way, 
we convert the names in the source language, say 
E, into an intermediate phonemic representation P, 
and then convert the phonemic representation into 
the target language, say Chinese C. In E2C 
transliteration, the phoneme-based approach can be 
formulated as P(C|E) = P(C|P)P(P|E) and 
conversely we have P(E|C) = P(E|P)P(P|C) for 
C2E back-transliteration.  
Several phoneme-based techniques have been 
proposed in the recent past for machine 
transliteration using transformation-based learning 
algorithm (Meng et al 2001; Jung et al 2000; 
Virga & Khudanpur, 2003) and using finite state 
transducer that implements transformation rules 
(Knight & Graehl, 1998), where both handcrafted 
and data-driven transformation rules have been 
studied.  
However, the phoneme-based approaches are 
limited by two major constraints, which could 
compromise transliterating precision, especially in 
English-Chinese transliteration: 
1) Latin-alphabet foreign names are of different 
origins. For instance, French has different phonic 
rules from those of English.  The phoneme-based 
approach requires derivation of proper phonemic 
representation for names of different origins. One 
may need to prepare multiple language-dependent 
grapheme-to-phoneme (G2P) conversion systems 
accordingly, and that is not easy to achieve (The 
Onomastica Consortium, 1995). For example, 
/Lafontant/ is transliterated into ???(La-Feng-
Tang) while /Constant/ becomes ????(Kang-
Si-Tan-Te)? where syllable /-tant/ in the two 
names are transliterated differently depending on 
the names? language of origin.  
2) Suppose that language dependent grapheme-
to-phoneme systems are attainable, obtaining 
Chinese orthography will need two further steps: a) 
conversion from generic phonemic representation 
to Chinese Pinyin; b) conversion from Pinyin to 
Chinese characters. Each step introduces a level of 
imprecision. Virga and Khudanpur (2003) reported 
8.3% absolute accuracy drops when converting 
from Pinyin to Chinese characters, due to 
homophone confusion. Unlike Japanese katakana 
or Korean alphabet, Chinese characters are more 
ideographic than phonetic. To arrive at an 
appropriate Chinese transliteration, one cannot rely 
solely on the intermediate phonemic representation.  
2.2 Useful orthographic context  
To illustrate the importance of contextual 
information in transliteration, let?s take name 
/Minahan/ as an example, the correct segmentation 
should be /Mi-na-han/, to be transliterated as ?-
?-? (Pinyin: Mi-Na-Han).  
 
English /mi- -na- -han/ 
Chinese ? ? ? 
Pinyin Mi Nan Han 
 
However, a possible segmentation /Min-ah-an/ 
could lead to an undesirable syllabication of ?-
?-? (Pinyin: Min-A-An).  
 
English /min- -ah- -an/ 
Chinese ? ? ? 
Pinyin Min A An 
 
According to the transliteration guidelines, a 
wise segmentation can be reached only after 
exploring the combination of the left and right 
context of transliteration units. From the 
computational point of view, this strongly suggests 
using a contextual n-gram as the knowledge base 
for the alignment decision.  
Another example will show us how one-to-many 
mappings could be resolved by context. Let?s take 
another name /Smith/ as an example. Although we 
can arrive at an obvious segmentation /s-mi-th/, 
there are three Chinese characters for each of /s-/, 
/-mi-/ and /-th/. Furthermore, /s-/ and /-th/ 
correspond to overlapping characters as well, as 
shown next. 
 
English /s- -mi- -th/ 
Chinese 1 ? ? ? 
Chinese 2 ? ? ? 
Chinese 3 ? ? ? 
 
A human translator will use transliteration rules 
between English syllable sequence  and Chinese 
character sequence to obtain the best mapping ?-
?-?, as indicated in italic in the table above.  
To address the issues in transliteration, we 
propose a direct orthographic mapping (DOM) 
framework through a joint source-channel model 
by fully exploring orthographic contextual 
information, aiming at alleviating the imprecision 
introduced by the multiple-step phoneme-based 
approach.  
3 Joint source-channel model 
In view of the close coupling of the source and 
target transliteration units, we propose to estimate 
P(E,C) by a joint source-channel model, or n-gram 
transliteration model (TM). For K aligned 
transliteration units, we have 
 
)...,,...,(),( 2121 KK ccceeePCEP =  
   ),...,,,( 21 KcececeP ><><><= (3) 
   ?
=
?><><=
K
k
k
k ceceP
1
1
1 ),|,(       
 
which provides an alternative to the phoneme-
based approach for resolving eqn. (1) and (2) by 
eliminating the intermediate phonemic 
representation. 
Unlike the noisy-channel model, the joint 
source-channel model does not try to capture how 
source names can be mapped to target names, but 
rather how source and target names can be 
generated simultaneously. In other words, we 
estimate a joint probability model that can be 
easily marginalized in order to yield conditional 
probability models for both transliteration and 
back-transliteration. 
Suppose that we have an English name 
mxxx ...21=?  and a Chinese transliteration 
nyyy ...21=? where ix are letters and jy are 
Chinese characters. Oftentimes, the number of 
letters is different from the number of Chinese 
characters. A Chinese character may correspond to 
a letter substring in English or vice versa.  
 
mii xxxxxxx ...... 21321 ++  
 
 
 
nj yyyy ......21  
 
where there exists an alignment  ?  with 
 
>=<>< 111 ,, yxce  
>=<>< 2322 ,, yxxce  ? 
 
and >=<>< nmK yxce ,, . A transliteration unit 
correspondence >< ce,  is called a transliteration 
pair. Then, the E2C transliteration can be 
formulated as 
 
),,(maxarg
,
????
??
P=  (4) 
 
and similarly the C2E back-transliteration as 
 
),,(maxarg
,
????
??
P=  (5) 
 
An n-gram transliteration model is defined as the 
conditional probability, or transliteration 
probability, of a transliteration pair kce >< ,  
depending on its immediate n predecessor pairs: 
 
      ),,(),( ???PCEP =  
?
=
?
+?><><=
K
k
k
nkk ceceP
1
1
1),|,(        (6) 
 
3.1 Transliteration alignment 
A bilingual dictionary contains entries mapping 
English names to their respective Chinese 
transliterations. Like many other solutions in 
computational linguistics, it is possible to 
automatically analyze the bilingual dictionary to 
acquire knowledge in order to map new English 
names to Chinese and vice versa. Based on the 
transliteration formulation above, a transliteration 
model can be built with transliteration unit?s n-
gram statistics. To obtain the statistics, the 
bilingual dictionary needs to be aligned. The 
maximum likelihood approach, through EM 
algorithm (Dempster, 1977), allows us to infer 
such an alignment easily as described in the table 
below. 
 
 
 
 
 
 
 
 
 
 
 
 
The aligning process is different from that of 
transliteration given in eqn. (4) or (5) in that, here 
we have fixed bilingual entries, ? and ? . The 
aligning process is just to find the alignment 
segmentation ? between the two strings that 
maximizes the joint probability: 
),,(maxarg ????
?
P=   (7) 
A set of transliteration pairs that is derived from 
the aligning process forms a transliteration table, 
which is in turn used in the transliteration 
decoding.  As the decoder is bounded by this table, 
it is important to make sure that the training 
database covers as much as possible the potential 
transliteration patterns. Here are some examples of 
resulting alignment pairs. 
 
?|s  ?|l ?|t ?|d 
?|k ?|b ?|g ?|r  
?|ll ?|c  ?|ro  ?|ri  
?|man  ?|m  ?|p  ?|de  
?|ra  ?|le  ?|a  ?|ber  
?|la  ?|son  ?|ton  ?|tt  
?|re  ?|co  ?|o  ?|e  
?|ma  ?|ley  ?|li  ?|mer 
 
Knowing that the training data set will never be 
sufficient for every n-gram unit, different 
smoothing approaches are applied, for example, by 
using backoff or class-based models, which can be 
found in statistical language modeling literatures 
(Jelinek, 1991). 
3.2 DOM: n-gram TM vs. NCM 
Although in the literature, most noisy channel 
models (NCM) are studied under phoneme-based 
paradigm for machine transliteration, NCM can 
also be realized under direct orthographic mapping 
(DOM). Next, let?s look into a bigram case to see 
what n-gram TM and NCM present to us. For E2C 
conversion, re-writing eqn (1) and eqn (6) , we 
have 
?
=
??
K
k
kkkk ccPcePP
1
1)|()|(),,( ???       (8) 
),,( ???P ),|,( 1
1
?
=
><><?? kkK
k
ceceP   (9)      
                
The formulation of eqn. (8) could be interpreted 
as a hidden Markov model with Chinese characters 
as its hidden states and English transliteration units 
as the observations (Rabiner, 1989). The number 
of parameters in the bigram TM is potentially 2T , 
while in the noisy channel model (NCM) it?s 
2CT + , where T  is the number of transliteration 
pairs and C is the number of Chinese 
transliteration units. In eqn. (9), the current 
transliteration depends on both Chinese and 
English transliteration history while in eqn. (8), it 
depends only on the previous Chinese unit. 
As 22 CTT +>> , an n-gram TM gives a finer 
description than that of NCM. The actual size of 
models largely depends on the availability of 
training data. In Table 1, one can get an idea of 
how they unfold in a real scenario. With 
adequately sufficient training data, n-gram TM is 
expected to outperform NCM in the decoding. A 
perplexity study in section 4.1 will look at the 
model from another perspective. 
4 The experiments1 
We use a database from the bilingual dictionary 
?Chinese Transliteration of Foreign Personal 
Names? which was edited by Xinhua News 
Agency and was considered the de facto standard 
of personal name transliteration in today?s Chinese 
press. The database includes a collection of 37,694 
unique English entries and their official Chinese 
transliteration. The listing includes personal names 
of English, French, Spanish, German, Arabic, 
Russian and many other origins. 
The database is initially randomly distributed 
into 13 subsets. In the open test, one subset is 
withheld for testing while the remaining 12 subsets 
are used as the training materials. This process is 
repeated 13 times to yield an average result, which 
is called the 13-fold open test. After experiments, 
we found that each of the 13-fold open tests gave 
consistent error rates with less than 1% deviation. 
Therefore, for simplicity, we randomly select one 
of the 13 subsets, which consists of 2896 entries, 
as the standard open test set to report results. In the 
close test, all data entries are used for training and 
testing.  
                                                     
1 demo at http://nlp.i2r.a-star.edu.sg/demo.htm 
The Expectation-Maximization algorithm 
1. Bootstrap initial random alignment 
2. Expectation: Update n-gram statistics to 
estimate probability distribution 
3. Maximization: Apply the n-gram TM to 
obtain new alignment 
4. Go to step 2 until the alignment converges 
5. Derive a list transliteration units from final 
       alignment as transliteration table 
4.1 Modeling 
The alignment of transliteration units is done 
fully automatically along with the n-gram TM 
training process. To model the boundary effects, 
we introduce two extra units <s> and </s> for start 
and end of each name in both languages. The EM 
iteration converges at 8th round when no further 
alignment changes are reported. Next are some 
statistics as a result of the model training: 
 
# close set bilingual entries (full data)  37,694 
# unique Chinese transliteration (close) 28,632 
# training entries for open test 34,777 
# test entries for open test 2,896 
# unique transliteration pairs  T 5,640 
# total transliteration pairs TW  119,364
# unique English units E 3,683 
# unique Chinese units C 374 
# bigram TM ),|,( 1?><>< kk ceceP  38,655 
# NCM Chinese bigram )|( 1?kk ccP  12,742 
Table 1. Modeling statistics 
The most common metric for evaluating an n-
gram model is the probability that the model 
assigns to test data, or perplexity (Jelinek, 1991). 
For a test set W composed of V names, where each 
name has been aligned into a sequence of 
transliteration pair tokens, we can calculate the 
probability of test set 
?
=
=
V
v
vvvPWp
1
),,()( ??? by applying the n-gram 
models to the token sequence. The cross-entropy 
)(WH p  of a model on data W is defined as 
)(log1)( 2 WpW
WH
T
p ?=  where TW is the total 
number of aligned transliteration pair tokens in the 
data W. The perplexity )(WPPp of a model is the 
reciprocal of the average probability assigned by 
the model to each aligned pair in the test set W 
as )(2)( WHp pWPP = . 
Clearly, lower perplexity means that the model 
describes better the data. It is easy to understand 
that closed test always gives lower perplexity than 
open test.  
 
 
 
 
 
 
 TM 
open  
NCM 
open 
TM 
closed 
NCM 
closed 
1-gram 670 729 655 716 
2-gram 324 512 151 210 
3-gram 306 487 68 127 
Table 2. Perplexity study of bilingual database 
We have the perplexity reported in Table 2 on 
the aligned bilingual dictionary, a database of 
119,364 aligned tokens. The NCM perplexity is 
computed using n-gram equivalents of eqn. (8) for 
E2C transliteration, while TM perplexity is based 
on those of eqn (9) which applies to both E2C and 
C2E. It is shown that TM consistently gives lower 
perplexity than NCM in open and closed tests. We 
have good reason to expect TM to provide better 
transliteration results which we expect to be 
confirmed later in the experiments. 
The Viterbi algorithm produces the best 
sequence by maximizing the overall probability, 
),,( ???P . In CLIR or multilingual corpus 
alignment (Virga and Khudanpur, 2003), N-best 
results will be very helpful to increase chances of 
correct hits. In this paper, we adopted an N-best 
stack decoder (Schwartz and Chow, 1990) in both 
TM and NCM experiments to search for N-best 
results. The algorithm also allows us to apply 
higher order n-gram such as trigram in the search. 
4.2 E2C transliteration 
In this experiment, we conduct both open and 
closed tests for TM and NCM models under DOM 
paradigm. Results are reported in Table 3 and 
Table 4.  
 open 
(word) 
open 
(char) 
closed 
(word) 
closed 
(char) 
1-gram 45.6% 21.1% 44.8% 20.4% 
2-gram 31.6% 13.6% 10.8% 4.7% 
3-gram 29.9% 10.8% 1.6% 0.8% 
Table 3. E2C error rates for n-gram TM tests.  
 open 
(word)
open 
(char) 
closed 
(word) 
closed 
(char) 
1-gram 47.3% 23.9% 46.9% 22.1% 
2-gram 39.6% 20.0% 16.4% 10.9% 
3-gram 39.0% 18.8% 7.8% 1.9% 
Table 4. E2C error rates for n-gram NCM tests 
In word error report, a word is considered 
correct only if an exact match happens between 
transliteration and the reference. The character 
error rate is the sum of deletion, insertion and 
substitution errors. Only the top choice in N-best 
results is used for error rate reporting. Not 
surprisingly, one can see that n-gram TM, which 
benefits from the joint source-channel model 
coupling both source and target contextual 
information into the model, is superior to NCM in 
all the test cases.  
4.3 C2E back-transliteration 
The C2E back-transliteration is more 
challenging than E2C transliteration. Not many 
studies have been reported in this area. It is 
common that multiple English names are mapped 
into the same Chinese transliteration. In Table 1, 
we see only 28,632 unique Chinese transliterations 
exist for 37,694 English entries, meaning that some 
phonemic evidence is lost in the process of 
transliteration. To better understand the task, let?s 
compare the complexity of the two languages 
presented in the bilingual dictionary.  
Table 1 also shows that the 5,640 transliteration 
pairs are cross mappings between 3,683 English 
and 374 Chinese units. In order words, on average, 
for each English unit, we have 1.53 = 5,640/3,683 
Chinese correspondences. In contrast, for each 
Chinese unit, we have 15.1 = 5,640/374 English 
back-transliteration units! Confusion is increased 
tenfold going backward.  
The difficulty of back-transliteration is also 
reflected by the perplexity of the languages as in 
Table 5. Based on the same alignment 
tokenization, we estimate the monolingual 
language perplexity for Chinese and English 
independently using the n-gram language models 
)|( 1 1
?
+?
k
nkk ccP  and )|(
1
1
?
+?
k
nkk eeP . Without 
surprise, Chinese names have much lower 
perplexity than English names thanks to fewer 
Chinese units. This contributes to the success of 
E2C but presents a great challenge to C2E back-
transliteration. 
 
 1-gram 2-gram 3-gram 
Chinese 207/206 97/86 79/45 
English 710/706 265/152 234/67 
Table 5 language perplexity comparison 
(open/closed test) 
 open 
(word) 
open 
(letter) 
closed 
(word) 
closed 
(letter) 
1 gram 82.3% 28.2% 81% 27.7% 
2 gram 63.8% 20.1% 40.4% 12.3% 
3 gram 62.1% 19.6% 14.7% 5.0% 
Table 6. C2E error rate for n-gram TM tests 
 E2C 
open 
E2C 
closed 
C2E 
open 
C2E 
closed 
1-best 29.9% 1.6% 62.1% 14.7% 
5-best 8.2% 0.94% 43.3% 5.2% 
10-best 5.4% 0.90% 24.6% 4.8% 
Table 7. N-best word error rates for 3-gram TM 
tests 
A back-transliteration is considered correct if it 
falls within the multiple valid orthographically 
correct options. Experiment results are reported in 
Table 6. As expected, C2E error rate is much 
higher than that of E2C. 
In this paper, the n-gram TM model serves as the 
sole knowledge source for transliteration. 
However, if secondary knowledge, such as a 
lookup table of valid target transliterations, is 
available, it can help reduce error rate by 
discarding invalid transliterations top-down the N 
choices. In Table 7, the word error rates for both 
E2C and C2E are reported which imply potential 
error reduction by secondary knowledge source. 
The N-best error rates are reduced significantly at 
10-best level as reported in Table 7. 
5 Discussions 
It would be interesting to relate n-gram TM to 
other related framework. 
5.1 DOM: n-gram TM vs. ID3 
In section 4, one observes that contextual 
information in both source and target languages is 
essential. To capture them in the modeling, one 
could think of decision tree, another popular 
machine learning approach. Under the DOM 
framework, here is the first attempt to apply 
decision tree in E2C and C2E transliteration. 
With the decision tree, given a fixed size 
learning vector, we used top-down induction trees 
to predict the corresponding output. Here we 
implement ID3 (Quinlan, 1993) algorithm to 
construct the decision tree which contains 
questions and return values at terminal nodes. 
Similar to n-gram TM, for unseen names in open 
test, ID3 has backoff smoothing, which lies on the 
default case which returns the most probable value 
as its best guess for a partial tree path according to 
the learning set.  
In the case of E2C transliteration, we form a 
learning vector of 6 attributes by combining 2 left 
and 2 right letters around the letter of focus ke  and 
1 previous Chinese unit 1?kc . The process is 
illustrated in Table 8, where both English and 
Chinese contexts are used to infer a Chinese 
character. Similarly, 4 attributes combining 1 left, 
1 centre and 1 right Chinese character and 1 
previous English unit are used for the learning 
vector in C2E test. An aligned bilingual dictionary 
is needed to build the decision tree.  
To minimize the effects from alignment 
variation, we use the same alignment results from 
section 4. Two trees are built for two directions, 
E2C and C2E. The results are compared with those 
3-gram TM  in Table 9. 
 
2?ke  1?ke  ke  1+ke  2+ke  1?kc   kc
_ _ N I C _ > ?
_ N I C E ? > _ 
N I C E _ _ > ?
I C E _ _ ? > _ 
Table 8. E2C transliteration using ID3 decision 
tree  for transliterating Nice to               
?? (?|NI ?|CE)  
 open  closed  
ID3 E2C  39.1% 9.7% 
3-gram TM E2C 29.9% 1.6% 
ID3 C2E 63.3% 38.4% 
3-gram TM C2E 62.1% 14.7% 
Table 9. Word error rate ID3 vs. 3-gram TM 
One observes that n-gram TM consistently 
outperforms ID3 decision tree in all tests. Three 
factors could have contributed:  
 
1) English transliteration unit size ranges from 1 
letter to 7 letters. The fixed size windows in ID3 
obviously find difficult to capture the dynamics of 
various ranges.  n-gram TM seems to have better 
captured the dynamics of transliteration units;  
2) The backoff smoothing of n-gram TM is more 
effective than that of ID3;  
3) Unlike n-gram TM, ID3 requires a separate 
aligning process for bilingual dictionary. The 
resulting alignment may not be optimal for tree 
construction.  Nevertheless, ID3 presents another 
successful implementation of DOM framework.  
 
5.2 DOM vs. phoneme-based approach 
Due to lack of standard data sets, it is difficult to 
compare the performance of the n-gram TM to that 
of other approaches. For reference purpose, we list 
some reported studies on other databases of E2C 
transliteration tasks in Table 10. As in the 
references, only character and Pinyin error rates 
are reported, we only include our character and 
Pinyin error rates for easy reference. The reference 
data are extracted from Table 1 and 3 of (Virga and 
Khudanpur 2003). As we have not found any C2E 
result in the literature, only E2C results are 
compared here. 
The first 4 setups by Virga et alall adopted the 
phoneme-based approach in the following steps:  
 
1) English name to English phonemes; 
2) English phonemes to Chinese Pinyin;  
3) Chinese Pinyin to Chinese characters. 
 
It is obvious that the n-gram TM compares 
favorably to other techniques. n-gram TM presents 
an error reduction of 74.6%=(42.5-10.8)/42.5% for 
Pinyin over the best reported result, Huge MT (Big 
MT) test case, which is noteworthy.  
The DOM framework shows a quantum leap in 
performance with n-gram TM being the most 
successful implementation. The n-gram TM and 
ID3 under direct orthographic mapping (DOM) 
paradigm simplify the process and reduce the 
chances of conversion errors. As a result, n-gram 
TM and ID3 do not generate Chinese Pinyin as 
intermediate results. It is noted that in the 374 
legitimate Chinese characters for transliteration, 
character to Pinyin mapping is unique while Pinyin 
to character mapping could be one to many. Since 
we have obtained results in character already, we 
expect less Pinyin error than character error should 
a character-to-Pinyin mapping be needed. 
 
System Trainin
g size 
Test 
size 
Pinyin 
errors 
Char 
errors 
Meng et al2,233 1,541 52.5% N/A 
Small MT 2,233 1,541 50.8% 57.4% 
Big MT 3,625 250 49.1% 57.4% 
Huge MT 
(Big MT) 
309,01
9 
3,122 42.5% N/A 
3-gram 
TM/DOM 
34,777 2,896 < 10.8% 10.8% 
ID3/DOM 34,777 2,896 < 15.6% 15.6% 
Table 10. Performance reference in recent 
studies 
6 Conclusions 
In this paper, we propose a new framework 
(DOM) for transliteration. n-gram TM is a 
successful realization of DOM paradigm. It 
generates probabilistic orthographic transformation 
rules using a data driven approach. By skipping the 
intermediate phonemic interpretation, the 
transliteration error rate is reduced significantly. 
Furthermore, the bilingual aligning process is 
integrated into the decoding process in n-gram TM, 
which allows us to achieve a joint optimization of 
alignment and transliteration automatically. Unlike 
other related work where pre-alignment is needed, 
the new framework greatly reduces the 
development efforts of machine transliteration 
systems. Although the framework is implemented 
on an English-Chinese personal name data set, 
without loss of generality, it well applies to 
transliteration of other language pairs such as 
English/Korean and English/Japanese. 
It is noted that place and company names are 
sometimes translated in combination of 
transliteration and meanings, for example, 
/Victoria-Fall/ becomes ? ? ? ? ? ? 
(Pinyin:Wei Duo Li Ya Pu Bu). As the proposed 
framework allows direct orthographical mapping, 
it can also be easily extended to handle such name 
translation. We expect to see the proposed model 
to be further explored in other related areas. 
References  
Dempster, A.P., N.M. Laird and D.B.Rubin, 1977. 
Maximum likelihood from incomplete data via 
the EM algorithm, J. Roy. Stat. Soc., Ser. B. Vol. 
39, pp138 
Helen M. Meng, Wai-Kit Lo, Berlin Chen and 
Karen Tang. 2001. Generate Phonetic Cognates 
to Handle Name Entities in English-Chinese 
cross-language spoken document retrieval, 
ASRU 2001 
Jelinek, F. 1991, Self-organized language 
modeling for speech recognition, In Waibel, A. 
and Lee K.F. (eds), Readings in Speech 
Recognition, Morgan Kaufmann., San Mateo, 
CA 
K. Knight and J. Graehl. 1998. Machine 
Transliteration, Computational Linguistics 24(4) 
Paola Virga, Sanjeev Khudanpur, 2003. 
Transliteration of Proper Names in Cross-
lingual Information Retrieval. ACL 2003 
workshop MLNER 
Quinlan J. R. 1993, C4.5 Programs for machine 
learning, Morgan Kaufmann , San Mateo, CA  
Rabiner, Lawrence R. 1989, A tutorial on hidden 
Markov models and selected applications in 
speech recognition, Proceedings of the IEEE 
77(2) 
Schwartz, R. and Chow Y. L., 1990, The N-best 
algorithm: An efficient and Exact procedure for 
finding the N most likely sentence hypothesis, 
Proceedings of ICASSP 1990, Albuquerque, pp 
81-84 
Sung Young Jung, Sung Lim Hong and Eunok 
Paek, 2000, An English to Korean 
Transliteration Model of Extended Markov 
Window, Proceedings of COLING 
The Onomastica Consortium, 1995. The 
Onomastica interlanguage pronunciation 
lexicon, Proceedings of EuroSpeech, Madrid, 
Spain, Vol. 1, pp829-832 
Xinhua News Agency, 1992, Chinese 
transliteration of foreign personal names, The 
Commercial Press 
 
Proceedings of the 43rd Annual Meeting of the ACL, pages 515?522,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Phonotactic Language Model for Spoken Language Identification 
 Haizhou Li and Bin Ma 
Institute for Infocomm Research 
Singapore 119613 
{hli,mabin}@i2r.a-star.edu.sg 
 
Abstract 
We have established a phonotactic lan-
guage model as the solution to spoken 
language identification (LID). In this 
framework, we define a single set of 
acoustic tokens to represent the acoustic 
activities in the world?s spoken languages. 
A voice tokenizer converts a spoken 
document into a text-like document of 
acoustic tokens. Thus a spoken document 
can be represented by a count vector of 
acoustic tokens and token n-grams in the 
vector space. We apply latent semantic 
analysis to the vectors, in the same way 
that it is applied in information retrieval, 
in order to capture salient phonotactics 
present in spoken documents. The vector 
space modeling of spoken utterances con-
stitutes a paradigm shift in LID technol-
ogy and has proven to be very successful. 
It presents a 12.4% error rate reduction 
over one of the best reported results on 
the 1996 NIST Language Recognition 
Evaluation database. 
1 Introduction 
Spoken language and written language are similar 
in many ways. Therefore, much of the research in 
spoken language identification, LID, has been in-
spired by text-categorization methodology. Both 
text and voice are generated from language de-
pendent vocabulary. For example, both can be seen 
as stochastic time-sequences corrupted by a chan-
nel noise. The n-gram language model has 
achieved equal amounts of success in both tasks, 
e.g. n-character slice for text categorization by lan-
guage (Cavnar and Trenkle, 1994) and Phone Rec-
ognition followed by n-gram Language Modeling, 
or PRLM (Zissman, 1996) .  
Orthographic forms of language, ranging from 
Latin alphabet to Cyrillic script to Chinese charac-
ters, are far more unique to the language than their 
phonetic counterparts. From the speech production 
point of view, thousands of spoken languages from 
all over the world are phonetically articulated us-
ing only a few hundred distinctive sounds or pho-
nemes (Hieronymus, 1994). In other words, 
common sounds are shared considerably across 
different spoken languages. In addition, spoken 
documents1, in the form of digitized wave files, are 
far less structured than written documents and need 
to be treated with techniques that go beyond the 
bounds of written language. All of this makes the 
identification of spoken language based on pho-
netic units much more challenging than the identi-
fication of written language. In fact, the challenge 
of LID is inter-disciplinary, involving digital signal 
processing, speech recognition and natural lan-
guage processing.  
In general, a LID system usually has three fun-
damental components as follows:  
1) A voice tokenizer which segments incoming 
voice feature frames and associates the seg-
ments with acoustic or phonetic labels, called 
tokens; 
2) A statistical language model which captures 
language dependent phonetic and phonotactic 
information from the sequences of tokens; 
3) A language classifier which identifies the lan-
guage based on discriminatory characteristics 
of acoustic score from the voice tokenizer and 
phonotactic score from the language model.  
In this paper, we present a novel solution to the 
three problems, focusing on the second and third 
problems from a computational linguistic perspec-
tive. The paper is organized as follows: In Section 
2, we summarize relevant existing approaches to 
the LID task. We highlight the shortcomings of 
existing approaches and our attempts to address the 
                                                          
1 A spoken utterance is regarded as a spoken document in this 
paper. 
515
issues. In Section 3 we propose the bag-of-sounds 
paradigm to turn the LID task into a typical text 
categorization problem. In Section 4, we study the 
effects of different settings in experiments on the 
1996 NIST Language Recognition Evaluation 
(LRE) database2. In Section 5, we conclude our 
study and discuss future work. 
2 Related Work 
Formal evaluations conducted by the National In-
stitute of Science and Technology (NIST) in recent 
years demonstrated that the most successful ap-
proach to LID used the phonotactic content of the 
voice signal to discriminate between a set of lan-
guages (Singer et al, 2003). We briefly discuss 
previous work cast in the formalism mentioned 
above: tokenization, statistical language modeling, 
and language identification. A typical LID system 
is illustrated in Figure 1 (Zissman, 1996), where 
language dependent voice tokenizers (VT) and lan-
guage models (LM) are deployed in the Parallel 
PRLM architecture, or P-PRLM. 
 
 
Figure 1.  L monolingual phoneme recognition 
front-ends are used in parallel to tokenize the input 
utterance, which is analyzed by LMs to predict the 
spoken language 
2.1 Voice Tokenization 
A voice tokenizer is a speech recognizer that 
converts a spoken document into a sequence of 
tokens. As illustrated in Figure 2, a token can be of 
different sizes, ranging from a speech feature 
frame, to a phoneme, to a lexical word. A token is 
defined to describe a distinct acoustic/phonetic 
activity. In early research, low level spectral 
                                                          
2 http://www.nist.gov/speech/tests/ 
frames, which are assumed to be independent of 
each other, were used as a set of prototypical spec-
tra for each language (Sugiyama, 1991). By adopt-
ing hidden Markov models, people moved beyond 
low-level spectral analysis towards modeling a 
frame sequence into a larger unit such as a pho-
neme and even a lexical word.  
Since the lexical word is language specific, the 
phoneme becomes the natural choice when build-
ing a language-independent voice tokenization 
front-end. Previous studies show that parallel lan-
guage-dependent phoneme tokenizers effectively 
serve as the tokenization front-ends with P-PRLM 
being the typical example. However, a language-
independent phoneme set has not been explored 
yet experimentally. In this paper, we would like to 
explore the potential of voice tokenization using a 
unified phoneme set. 
 
 
Figure 2 Tokenization at different resolutions 
2.2 n-gram Language Model 
With the sequence of tokens, we are able to es-
timate an n-gram language model (LM) from the 
statistics. It is generally agreed that phonotactics, 
i.e. the rules governing the phone/phonemes se-
quences admissible in a language, carry more lan-
guage discriminative information than the 
phonemes themselves. An n-gram LM over the 
tokens describes well n-local phonotactics among 
neighboring tokens. While some systems model 
the phonotactics at the frame level (Torres-
Carrasquillo et al, 2002), others have proposed P-
PRLM. The latter has become one of the most 
promising solutions so far (Zissman, 1996).  
  A variety of cues can be used by humans and 
machines to distinguish one language from another. 
These cues include phonology, prosody, morphol-
ogy, and syntax in the context of an utterance. 
VT-1: Chinese 
VT-2: English 
VT-L: French 
 
 
 LM-L: French 
LM-1 ? LM-L 
 
 
 LM-L: French 
LM-1 ? LM-L 
 
 
 LM-L: French 
LM-1 ? LM-L 
language classifier 
spoken utterance  
hypothesized
language
word 
phoneme 
frame
516
However, global phonotactic cues at the level of 
utterance or spoken document remains unexplored 
in previous work. In this paper, we pay special at-
tention to it. A spoken language always contains a 
set of high frequency function words, prefixes, and 
suffixes, which are realized as phonetic token sub-
strings in the spoken document. Individually, those 
substrings may be shared across languages. How-
ever, the pattern of their co-occurrences discrimi-
nates one language from another.  
Perceptual experiments have shown (Mut-
husamy, 1994) that with adequate training, human 
listeners? language identification ability increases 
when given longer excerpts of speech.  Experi-
ments have also shown that increased exposure to 
each language and longer training sessions im-
prove listeners? language identification perform-
ance. Although it is not entirely clear how human 
listeners make use of the high-order phonotac-
tic/prosodic cues present in longer spans of a spo-
ken document, strong evidence shows that 
phonotactics over larger context provides valuable 
LID cues beyond n-gram, which will be further 
attested by our experiments in Section 4. 
2.3 Language Classifier 
The task of a language classifier is to make 
good use of the LID cues that are encoded in the 
model l?  to hypothesize from among L lan-
guages, ? , as the one that is actually spoken in a 
spoken document O. The LID model 
l?
l?  in P-
PRLM refers to extracted information from acous-
tic model and n-gram LM for language l.  We have 
and { ,AM } LLMl l l? ? ?=  ( 1,..., )l l? ?? = . A maxi-
mum-likelihood classifier can be formulated as 
follows: 
( ) (
? arg max ( / )
arg max / , /
l
l
AM LM
l l
l T
l P O
P O T P T
?
? ?
??
?? ??
=
? ? )
)
      (1) 
The exact computation in Eq.(1) involves sum-
ming over all possible decoding of token se-
quences T given O. In many implementations, 
it is approximated by the maximum over all se-
quences in the sum by finding the most likely to-
ken sequence, , for each language l, using the 
Viterbi algorithm: 
??
l?T
( ) (? ? ?arg max[ / , / ]AM LMl l l l
l
l P O T P T? ?
??
?         (2) 
Intuitively, individual sounds are heavily shared 
among different spoken languages due to the com-
mon speech production mechanism of humans. 
Thus, the acoustic score has little language dis-
criminative ability. Many experiments (Yan and 
Barnard, 1995; Zissman, 1996) have further at-
tested that the n-gram LM score provides more 
language discriminative information than their 
acoustic counterparts. In Figure 1, the decoding of 
voice tokenization is governed by the acoustic 
model AMl? to arrive at an acoustic score ( )?/ , AMl lP O T ?  and a token sequence . The n-
gram LM derives the n-local phonotactic score 
l?T
( )? / LMl lP T ? from the language model LMl? .  
Clearly, the n-gram LM suffers the major short-
coming of having not exploited the global phono-
tactics in the larger context of a spoken utterance. 
Speech recognition researchers have so far chosen 
to only use n-gram local statistics for primarily 
pragmatic reasons, as this n-gram is easier to attain. 
In this work, a language independent voice tokeni-
zation front-end is proposed, that uses a unified 
acoustic model  AM?  instead of multiple language 
dependent acoustic models AMl? .  The n-gram 
LM LMl? is generalized to model both local and 
global phonotactics. 
3 Bag-of-Sounds Paradigm 
The bag-of-sounds concept is analogous to the 
bag-of-words paradigm originally formulated in 
the context of information retrieval (IR) and text 
categorization (TC) (Salton 1971; Berry et al, 
1995; Chu-Caroll and Carpenter, 1999). One focus 
of IR is to extract informative features for docu-
ment representation. The bag-of-words paradigm 
represents a document as a vector of counts. It is 
believed that it is not just the words, but also the 
co-occurrence of words that distinguish semantic 
domains of text documents.   
Similarly, it is generally believed in LID that, al-
though the sounds of different spoken languages 
overlap considerably, the phonotactics differenti-
ates one language from another. Therefore, one can 
easily draw the analogy between an acoustic token 
in bag-of-sounds and a word in bag-of-words. 
Unlike words in a text document, the phonotactic 
information that distinguishes spoken languages is 
517
concealed in the sound waves of spoken languages. 
After transcribing a spoken document into a text 
like document of tokens, many IR or TC tech-
niques can then be readily applied. 
It is beyond the scope of this paper to discuss 
what would be a good voice tokenizer. We adopt 
phoneme size language-independent acoustic to-
kens to form a unified acoustic vocabulary in our 
voice tokenizer. Readers are referred to (Ma et al, 
2005) for details of acoustic modeling. 
3.1 Vector Space Modeling 
In human languages, some words invariably occur 
more frequently than others. One of the most 
common ways of expressing this idea is known as 
Zipf?s Law (Zipf, 1949). This law states that there 
is always a set of words which dominates most of 
the other words of the language in terms of their 
frequency of use. This is true both of written words 
and of spoken words. The short-term, or local pho-
notactics, is devised to describe Zipf?s Law.  
The local phonotactic constraints can be typi-
cally described by the token n-grams, or phoneme 
n-grams as in (Ng et al, 2000), which represents 
short-term statistics such as lexical constraints. 
Suppose that we have a token sequence, t1 t2 t3 t4. 
We derive the unigram statistics from the token 
sequence itself. We derive the bigram statistics 
from t1(t2) t2(t3) t3(t4) t4(#) where the token vo-
cabulary is expanded over the token?s right context. 
Similarly, we derive the trigram statistics from the 
t1(#,t2) t2(t1,t3) t3(t2,t4) t4(t3,#) to account for left 
and right contexts. The # sign is a place holder for 
free context. In the interest of manageability, we 
propose to use up to token trigram. In this way, for 
an acoustic system of Y  tokens, we have poten-
tially bigram and Y trigram in the vocabulary.  2Y 3
Meanwhile, motivated by the ideas of having 
both short-term and long-term phonotactic statis-
tics, we propose to derive global phonotactics in-
formation to account for long-term phonotactics: 
The global phonotactic constraint is the high-
order statistics of n-grams. It represents document 
level long-term phonotactics such as co-
occurrences of n-grams. By representing a spoken 
document as a count vector of n-grams, also called 
bag-of-sounds vector, it is possible to explore the 
relations and higher-order statistics among the di-
verse n-grams through latent semantic analysis 
(LSA).  
It is often advantageous to weight the raw 
counts to refine the contribution of each n-gram to 
LID. We begin by normalizing the vectors repre-
senting the spoken document by making each vec-
tor of unit length. Our second weighting is based 
on the notion that an n-gram that only occurs in a 
few languages is more discriminative than an n-
gram that occurs in nearly every document. We use 
the inverse-document frequency (idf) weighting 
scheme (Spark Jones, 1972), in which a word is 
weighted inversely to the number of documents in 
which it occurs, by means of 
( ) log / ( )idf w D d w=  , where w is a word in the 
vocabulary of W token n-grams. D is the total num-
ber of documents in the training corpus from L lan-
guages. Since each language has at least one 
document in the training corpus, we have D L? . 
is the number of documents containing the 
word w. Letting be the count of word w in 
document d, we have the weighted count as 
( )d w
,w dc
2 1/ 2
, , ,
1
( ) /( )w d w d w d
w W
c c idf w c ?
?? ?
? = ? ?  (3) 
and a vector to represent 
document d. A corpus is then represented by a 
term-document matrix
1, 2, ,{ , ,..., }
T
d d d W dc c c c? ? ?=
1 2{ , ,..., }DH c c c= of W D? .  
3.2 Latent Semantic Analysis 
The fundamental idea in LSA is to reduce the 
dimension of a document vector, W to Q, where 
Q W<< and Q D<<  , by projecting the problem 
into the space spanned by the rows of the closest 
rank-Q matrix to H in the Frobenius norm (Deer-
wester et al 1990).  Through singular value de-
composition (SVD) of H, we construct a modified 
matrix HQ from the Q-largest singular values: 
T
Q Q Q QH U S V=                         (4) 
QU is a W Q? left singular matrix with rows 
,1wu w W? ? QS; is a Q Q?  diagonal matrix of Q-
largest singular values of H; is QV D Q? right sin-
gular matrix with rows , 1 . dv d D? ?
With the SVD, we project the D document vec-
tors in H into a reduced space  , referred to as 
Q-space in the rest of this paper. A test document 
of unknown language ID is mapped to a 
pseudo-document in the Q-space by matrix  
QV
pc
pv QU
518
1T
p p p Qc v c U S
?? = Q   (5) 
After SVD, it is straightforward to arrive at a 
natural metric for the closeness between two spo-
ken documents  and in Q-space instead of  
their original W-dimensional space  and . 
iv jv
ic jc
( , ) cos( , )
|| || || ||
T
i j
i j i j
i j
v v
g c c v v
v v
?? = ?    (6) 
( , )i jg c c  indicates the similarity between two vec-
tors, which can be transformed to a distance meas-
ure . 1( , ) cos ( , )i j i jk c c g c c
?=
In the forced-choice classification, a test docu-
ment, supposedly monolingual, is classified into 
one of the L languages. Note that the test document 
is unknown to the H matrix. We assume consis-
tency between the test document?s intrinsic phono-
tactic pattern and one of the D patterns, that is 
extracted from the training data and is presented in 
the H matrix, so that the SVD matrices still apply 
to the test document, and Eq.(5) still holds for di-
mension reduction. 
3.3 Bag-of-Sounds Language Classifier 
The bag-of-sounds phonotactic LM benefits from 
several properties of vector space modeling and 
LSA.  
1) It allows for representing a spoken document 
as a vector of n-gram features, such as unigram, 
bigram, trigram, and the mixture of them; 
2) It provides a well-defined distance metric for 
measurement of phonotactic distance between 
spoken documents;  
3) It processes spoken documents in a lower di-
mensional Q-space, that makes the bag-of-
sounds phonotactic language modeling, LMl? , 
and classification computationally manageable. 
Suppose we have only one prototypical vector 
and its projection in the Q-space to represent 
language l. Applying LSA to the term-document 
matrix
lc lv
:H W L? , a minimum distance classifier is 
formulated: 
? arg min ( , )p l
l
l k v
??
= v    (7) 
In Eq.(7), is the Q-space projection of , a test 
document. 
pv pc
Apparently, it is very restrictive for each lan-
guage to have just one prototypical vector, also 
referred to as a centroid. The pattern of language 
distribution is inherently multi-modal, so it is 
unlikely well fitted by a single vector. One solution 
to this problem is to span the language space with 
multiple vectors. Applying LSA to a term-
document matrix :H W L?? , where L L as-
suming each language l is represented by a set of 
M vectors, 
M? = ?
l? , a new classifier, using k-nearest 
neighboring rule (Duda and Hart, 1973) , is formu-
lated, named k-nearest classifier (KNC): 
? arg min ( , )
l
p l
l l
l k
?
??? ??
= v v?               (8) 
where l? is the set of k-nearest-neighbor to  and  pv
l l? ? ? . 
Among many ways to derive the M centroid vec-
tors, here is one option. Suppose that we have a set 
of training documents Dl for language l , as subset 
of corpus ? ,  and . To derive 
the M vectors, we choose to carry out vector quan-
tization (VQ) to partition D
lD ?? 1Ll lD=? = ?
l
l  into M cells Dl,m in the 
Q-space such that 1 ,
M
m l mD D=? =  using similarity 
metric Eq.(6). All the documents in each cell 
,l mD can then be merged to form a super-document, 
which is further projected into a Q-space vector 
. This results in M prototypical centroids 
. Using KNC, a test vector is 
compared with M vectors to arrive at the k-nearest 
neighbors for each language, which can be compu-
tationally expensive when M is large. 
,l mv
, ( 1,...l m l )M??v m =
Alternatively, one can account for multi-modal 
distribution through finite mixture model. A mix-
ture model is to represent the M discrete compo-
nents with soft combination. To extend the KNC 
into a statistical framework, it is necessary to map 
our distance metric Eq.(6) into a probability meas-
ure. One way is for the distance measure to induce 
a family of exponential distributions with pertinent 
marginality constraints. In practice, what we need 
is a reasonable probability distribution, which 
sums to one, to act as a lookup table for the dis-
tance measure. We here choose to use the empiri-
cal multivariate distribution constructed by 
allocating the total probability mass in proportion 
to the distances observed with the training data. In 
short, this reduces the task to a histogram normali-
zation. In this way, we map the distance  
to a conditional probability distribution 
( , )i jk c c
( | )i jp v v  
519
subject to . Now that we are in the 
probability domain, techniques such as mixture 
smoothing can be readily applied to model a lan-
guage class with finer fitting. 
| |
1
( | ) 1i ji p v v
?
= =?
Let?s re-visit the task of L language forced-
choice classification. Similar to KNC, suppose we 
have M centroids  in the Q-
space for each language l. Each centroid represents 
a class.  The class conditional probability can be 
described as a linear combination of
,  ( 1,... )l m lv m?? = M
,( | )i l mp v v : 
,
1
( | ) ( ) ( | )
M
LM
i l l m i l m
m
,p v p v p v?
=
=? v
)
           (9) 
the probability ,( l mp v , functionally serves as a 
mixture weight of ,( | )i l mp v v . Together with a set 
of centroids , ,  ( 1,...l m lv m )?? = ,( | )i l mM p v v
)
and 
,( l mp v  define a mixture model 
LM
l? .  ,( | )i l mp v v  
is estimated by histogram normalization and 
,( l m )p v is estimated under the maximum likelihood 
criteria, , ,( ) /l m m l lp v C= C  , where C  is total 
number of documents in D
l
l, of which C docu-
ments fall into the cell m.  
,m l
An Expectation-Maximization iterative process 
can be devised for training of LMl?  to maximize the 
likelihood Eq.(9) over the entire training corpus: 
| |
1 1
( | ) ( | )
lDL
LM
d l
l d
p p v ?
= =
? ? =??            (10) 
Using the phonotactic LM score ( )? / LMl lP T for 
classification, with T  being represented by the 
bag-of-sounds vector v ,  Eq.(2) can be reformu-
lated as Eq.(11),  named mixture-model classifier 
(MMC): 
?
l?
p
, ,
1
? arg max ( | )
 arg max ( ) ( | )
LM
p l
l
M
l m p l m
l m
l p v
p v p v v
?
??
?? =
=
= ?  (11) 
To establish fair comparison with P-PRLM, as 
shown in Figure 3, we devise our bag-of-sounds 
classifier to solely use the LM score ( )? / LMl lP T ? for classification decision whereas the 
acoustic score ( )?/ , AMl lP O may potentially help 
as reported in (Singer et al, 2003).  
T ?                                                           
 
 
Figure 3.  A bag-of-sounds classifier. A unified 
front-end followed by L parallel bag-of-sounds 
phonotactic LMs. 
4 Experiments 
This section will experimentally analyze the per-
formance of the proposed bag-of-sounds frame-
work using the 1996 NIST Language Recognition 
Evaluation (LRE) data. The database was intended 
to establish a baseline of performance capability 
for language recognition of conversational tele-
phone speech. The database contains recorded 
speech of 12 languages: Arabic, English, Farsi, 
French, German, Hindi, Japanese, Korean, Manda-
rin, Spanish, Tamil and Vietnamese. We use the 
training set and development set from LDC Call-
Friend corpus3 as the training data. Each conversa-
tion is segmented into overlapping sessions of 
about 30 seconds each, resulting in about 12,000 
sessions for each language. The evaluation set con-
sists of 1,492 30-sec sessions, each distributed 
among the various languages of interest. We treat a 
30-sec session as a spoken document in both train-
ing and testing. We report error rates (ER) of the 
1,492 test trials. 
4.1 Effect of Acoustic Vocabulary 
The choice of n-gram affects the performance of 
LID systems. Here we would like to see how a bet-
ter choice of acoustic vocabulary can help convert 
a spoken document into a phonotactically dis-
criminative space. There are two parameters that 
determine the acoustic vocabulary: the choice of 
acoustic token, and the choice of n-grams. In this 
paper, the former concerns the size of an acoustic 
system Y in the unified front-end. It is studied in 
more details in (Ma et al, 2005). We set Y to 32 in 
3 See http://www.ldc.upenn.edu/. The overlap between 1996 
NIST evaluation data and CallFriend database has been re-
moved from training data as suggested in the 2003 NIST LRE 
website http://www.nist.gov/speech/tests/index.htm 
LM
l? LM-L:  French 
Unified VT
1
LM? LM-1: Chinese 
2
LM?  LM-2: English 
Language C
lassifier
spoken utterance
H
ypothesized language 
AM?
520
this experiment; the latter decides what features to 
be included in the vector space. The vector space 
modeling allows for multiple heterogeneous fea-
tures in one vector. We introduce three types of 
acoustic vocabulary (AV) with mixture of token 
unigram, bigram, and trigram:   
a) AV1: 32 broad class phonemes as unigram, 
selected from 12 languages, also referred to as 
P-ASM as detailed in (Ma et al, 2005) 
b) AV2: AV1 augmented by 32  bigrams of 
AV1, amounting to 1,056 tokens 
32?
c) AV3: AV2 augmented by 32  tri-
grams of AV1, amounting to 33,824 tokens 
32 32? ?
 
 AV1 AV2 AV3 
ER % 46.1 32.8 28.3 
Table 1.  Effect of acoustic vocabulary (KNC) 
 
We carry out experiments with KNC classifier 
of 4,800 centroids. Applying k-nearest-neighboring 
rule, k is empirically set to 3. The error rates are 
reported in Table 1 for the experiments over the 
three AV types. It is found that high-order token n-
grams improve LID performance.   This reaffirms 
many previous findings that n-gram phonotactics 
serves as a valuable cue in LID. 
4.2 Effect of Model Size 
As discussed in KNC, one would expect to im-
prove the phonotactic model by using more cen-
troids. Let?s examine how the number of centroid 
vectors M affects the performance of KNC. We set 
the acoustic system size Y to 128, k-nearest to 3, 
and only use token bigrams in the bag-of-sounds 
vector. In Table 2, it is not surprising to find that 
the performance improves as M increases. How-
ever, it is not practical to have large M be-
cause comparisons need to take place in 
each test trial.  
L L M? = ?
 
#M 1,200 2,400 4,800 12,000 
ER % 17.0 15.7 15.4 14.8 
Table 2. Effect of number of centroids (KNC) 
 
To reduce computation, MMC attempts to use 
less number of mixtures M to represent the phono-
tactic space. With the smoothing effect of the mix-
ture model, we expect to use less computation to 
achieve similar performance as KNC. In the ex-
periment reported in Table 3, we find that MMC 
(M=1,024) achieves 14.9% error rate, which al-
most equalizes the best result in the KNC experi-
ment (M=12,000) with much less computation.  
 
#M 4 16 64 256 1,024 
ER % 29.6 26.4 19.7 16.0 14.9 
Table 3. Effect of number of mixtures (MMC) 
4.3 Discussion 
The bag-of-sounds approach has achieved equal 
success in both 1996 and 2003 NIST LRE data-
bases. As more results are published on the 1996 
NIST LRE database, we choose it as the platform 
of comparison. In Table 4, we report the perform-
ance across different approaches in terms of error 
rate for a quick comparison. MMC presents a 
12.4% ER reduction over the best reported result4 
(Torres-Carrasquillo et al, 2002). 
It is interesting to note that the bag-of-sounds 
classifier outperforms its P-PRLM counterpart by a 
wide margin (14.9% vs 22.0%). This is attributed 
to the global phonotactic features in LMl? .  The 
performance gain in (Torres-Carrasquillo et al, 
2002; Singer et al, 2003) was obtained mainly by 
fusing scores from several classifiers, namely 
GMM, P-PRLM and SVM, to benefit from both 
acoustic and language model scores. Noting that 
the bag-of-sounds classifier in this work solely re-
lies on the LM score, it is believed that fusing with 
scores from other classifiers will further boost the 
LID performance.  
 
 ER % 
P-PRLM5 22.0 
P-PRLM + GMM acoustic5 19.5 
P-PRLM + GMM acoustic +  
GMM tokenizer5
17.0 
Bag-of-sounds classifier (MMC) 14.9 
Table 4. Benchmark of different approaches 
 
Besides the error rate reduction, the bag-of-
sounds approach also simplifies the on-line com-
puting procedure over its P-PRLM counterpart. It 
would be interesting to estimate the on-line com-
putational need of MMC. The cost incurred has 
two main components: 1) the construction of the 
                                                          
4 Previous results are also reported in DCF, DET, and equal 
error rate (EER). Comprehensive benchmarking for bag-of-
sounds phonotactic LM will be reported soon. 
5 Results extracted from (Torres-Carrasquillo et al, 2002) 
521
pseudo document vector, as done via Eq.(5); 2) 
vector comparisons. The computing 
cost is estimated to be  per test trial 
(Bellegarda, 2000). For typical values of Q, this 
amounts to less than 0.05 Mflops. While this is 
more expensive than the usual table look-up in 
conventional n-gram LM, the performance im-
provement is able to justify the relatively modest 
computing overhead. 
L L M? = ?
2( )QO
5 Conclusion 
We have proposed a phonotactic LM approach to 
LID problem. The concept of bag-of-sounds is in-
troduced, for the first time, to model phonotactics 
present in a spoken language over a larger context. 
With bag-of-sounds phonotactic LM, a spoken 
document can be treated as a text-like document of 
acoustic tokens. This way, the well-established 
LSA technique can be readily applied. This novel 
approach not only suggests a paradigm shift in LID, 
but also brings 12.4% error rate reduction over one 
of the best reported results on the 1996 NIST LRE 
data. It has proven to be very successful.  
We would like to extend this approach to other 
spoken document categorization tasks. In monolin-
gual spoken document categorization, we suggest 
that the semantic domain can be characterized by 
latent phonotactic features. Thus it is straightfor-
ward to extend the proposed bag-of-sounds frame-
work to spoken document categorization. 
Acknowledgement 
The authors are grateful to Dr. Alvin F. Martin of 
the NIST Speech Group for his advice when pre-
paring the 1996 NIST LRE experiments, to Dr G. 
M. White and Ms Y. Chen of Institute for Info-
comm Research for insightful discussions.  
References  
Jerome R. Bellegarda. 2000. Exploiting latent semantic 
information in statistical language modeling, In Proc. 
of the IEEE, 88(8):1279-1296. 
M. W. Berry, S.T. Dumais and G.W. O?Brien. 1995. 
Using Linear Algebra for intelligent information re-
trieval, SIAM Review, 37(4):573-595. 
William B. Cavnar, and John M. Trenkle. 1994. N-
Gram-Based Text Categorization, In Proc. of 3rd 
Annual Symposium on Document Analysis and In-
formation Retrieval, pp. 161-169. 
Jennifer Chu-Carroll, and Bob Carpenter. 1999. Vector-
based Natural Language Call Routing, Computa-
tional Linguistics, 25(3):361-388. 
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and 
R. Harshman, 1990, Indexing by latent semantic 
analysis, Journal of the American Society for Infor-
matin Science, 41(6):391-407 
Richard O. Duda and Peter E. Hart. 1973. Pattern Clas-
sification and scene analysis. John Wiley & Sons 
James L. Hieronymus. 1994. ASCII Phonetic Symbols 
for the World?s Languages: Worldbet. Technical Re-
port AT&T Bell Labs. 
Spark Jones, K. 1972. A statistical interpretation of 
term specificity and its application in retrieval, Jour-
nal of Documentation, 28:11-20 
Bin Ma, Haizhou Li and Chin-Hui Lee, 2005. An Acous-
tic Segment Modeling Approach to Automatic Lan-
guage Identification, submitted to Interspeech 2005 
Yeshwant K. Muthusamy, Neena Jain, and Ronald A.  
Cole. 1994. Perceptual benchmarks for automatic 
language identification, In Proc. of ICASSP 
Corinna Ng , Ross Wilkinson , Justin Zobel, 2000. 
, Speech Communication, 32(1-2):61-
77 
Ex-
periments in spoken document retrieval using pho-
neme n-grams
G. Salton, 1971. The SMART Retrieval System, Pren-
tice-Hall, Englewood Cliffs, NJ, 1971 
E. Singer, P.A. Torres-Carrasquillo, T.P. Gleason, W.M. 
Campbell and D.A. Reynolds. 2003. Acoustic, Pho-
netic and Discriminative Approaches to Automatic 
language recognition, In Proc. of Eurospeech 
Masahide Sugiyama. 1991. Automatic language recog-
nition using acoustic features, In Proc. of ICASSP. 
Pedro A. Torres-Carrasquillo, Douglas A. Reynolds, 
and J.R. Deller. Jr. 2002. Language identification us-
ing Gaussian Mixture model tokenization, in Proc. of 
ICASSP. 
Yonghong Yan, and Etienne Barnard. 1995. An ap-
proach to automatic language identification based on 
language dependent phone recognition, In Proc. of 
ICASSP. 
George K. Zipf. 1949. Human Behavior and the Princi-
pal of Least effort, an introduction to human ecology. 
Addison-Wesley, Reading, Mass. 
Marc A. Zissman. 1996. Comparison of four ap-
proaches to automatic language identification of 
telephone speech, IEEE Trans. on Speech and Audio 
Processing, 4(1):31-44. 
522
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1129?1136,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Transliteration Lexicons from the Web 
 
Jin-Shea Kuo1, 2 
1Chung-Hwa Telecom. 
Laboratories, Taiwan 
jskuo@cht.com.tw 
Haizhou Li 
Institute for Infocomm 
Research, Singapore  
hzli@ieee.org 
Ying-Kuei Yang2 
2National Taiwan University of 
Science and Technology, Taiwan  
ykyang@mouse.ee. 
ntust.edu.tw 
 
Abstract 
This paper presents an adaptive learning 
framework for Phonetic Similarity 
Modeling (PSM) that supports the 
automatic construction of transliteration 
lexicons. The learning algorithm starts 
with minimum prior knowledge about 
machine transliteration, and acquires 
knowledge iteratively from the Web. We 
study the active learning and the 
unsupervised learning strategies that 
minimize human supervision in terms of 
data labeling. The learning process 
refines the PSM and constructs a 
transliteration lexicon at the same time. 
We evaluate the proposed PSM and its 
learning algorithm through a series of 
systematic experiments, which show that 
the proposed framework is reliably 
effective on two independent databases. 
1 Introduction 
In applications such as cross-lingual information 
retrieval (CLIR) and machine translation (MT), 
there is an increasing need to translate out-of-
vocabulary (OOV) words, for example from an 
alphabetical language to Chinese. Foreign proper 
names constitute a good portion of OOV words, 
which are translated into Chinese through 
transliteration. Transliteration is a process of 
translating a foreign word into a native language 
by preserving its pronunciation in the original 
language, otherwise known as translation-by-
sound.  
MT and CLIR systems rely heavily on 
bilingual lexicons, which are typically compiled 
manually. However, in view of the current 
information explosion, it is labor intensive, if not 
impossible, to compile a complete proper nouns 
lexicon. The Web is growing at a fast pace and is 
providing a live information source that is rich in 
transliterations. This paper presents a novel 
solution for automatically constructing an 
English-Chinese transliteration lexicon from the 
Web. 
Research on automatic transliteration has 
reported promising results for regular 
transliteration (Wan and Verspoor, 1998; Li et al 
2004), where transliterations follow rigid 
guidelines. However, in Web publishing, 
translators in different countries and regions may 
not observe common guidelines. They often 
skew the transliterations in different ways to 
create special meanings to the sound equivalents, 
resulting in casual transliterations. In this case, 
the common generative models (Li et al 2004) 
fail to predict the transliteration most of the time. 
For example, ?Coca Cola? is transliterated into 
? ? ? ? ?  /Ke-Kou-Ke-Le/? as a sound 
equivalent in Chinese, which literately means 
?happiness in the mouth?. In this paper, we are 
interested in constructing lexicons that cover 
both regular and casual transliterations. 
When a new English word is first introduced, 
many transliterations are invented. Most of them 
are casual transliterations because a regular 
transliteration typically does not have many 
variations. After a while, the transliterations 
converge into one or two popular ones. For 
example, ?Taxi? becomes ???  /Di-Shi/? in 
China and ? ? ?  /De-Shi/? in Singapore. 
Therefore, the adequacy of a transliteration entry 
could be judged by its popularity and its 
conformity with the translation-by-sound 
principle. In any case, the phonetic similarity 
should serve as the primary basis of judgment. 
This paper is organized as follows. In Section 
2, we briefly introduce prior works pertaining to 
machine transliteration. In Section 3, we propose 
a phonetic similarity model (PSM) for 
confidence scoring of transliteration. In Section 4, 
we propose an adaptive learning process for 
PSM modeling and lexicon construction. In 
Section 5, we conduct experiments to evaluate 
different adaptive learning strategies. Finally, we 
conclude in Section 6. 
1129
2 Related Work 
In general, studies of transliteration fall into two 
categories: transliteration modeling (TM) and 
extraction of transliteration pairs (EX) from 
corpora.  
The TM approach models phoneme-based or 
grapheme-based mapping rules using a 
generative model that is trained from a large 
bilingual lexicon, with the objective of 
translating unknown words on the fly. The 
efforts are centered on establishing the phonetic 
relationship between transliteration pairs. Most 
of these works are devoted to phoneme1-based 
transliteration modeling (Wan and Verspoor 
1998, Knight and Graehl, 1998). Suppose that 
EW is an English word and CW is its prospective 
Chinese transliteration. The phoneme-based 
approach first converts EW into an intermediate 
phonemic representation P, and then converts the 
phonemic representation into its Chinese 
counterpart CW. In this way, EW and CW form 
an E-C transliteration pair. 
In this approach, we model the transliteration 
using two conditional probabilities, P(CW|P) and 
P(P|EW), in a generative model P(CW|EW) = 
P(CW|P)P(P|EW). Meng (2001) proposed a rule-
based mapping approach. Virga and Khudanpur 
(2003) and Kuo et al(2005) adopted the noisy-
channel modeling framework. Li et al(2004) 
took a different approach by introducing a joint 
source-channel model for direct orthography 
mapping (DOM), which treats transliteration as a 
statistical machine translation problem under 
monotonic constraints. The DOM approach, 
which is a grapheme-based approach, 
significantly outperforms the phoneme-based 
approaches in regular transliterations. It is noted 
that the state-of-the-art accuracy reported by Li 
et al(2004) for regular transliterations of the 
Xinhua database is about 70.1%, which leaves 
much room for improvement if one expects to 
use a generative model to construct a lexicon for 
casual transliterations. 
EX research is motivated by information 
retrieval techniques, where people attempt to 
extract transliteration pairs from corpora. The 
EX approach aims to construct a large and up-to-
date transliteration lexicon from live corpora. 
Towards this objective, some have proposed 
extracting translation pairs from parallel or 
comparable bitext using co-occurrence analysis 
                                               
1 Both phoneme and syllable based approaches are referred 
to as phoneme-based here. 
or a context-vector approach (Fung and Yee, 
1998; Nie et al 1999). These methods compare 
the semantic similarities between words without 
taking their phonetic similarities into accounts. 
Lee and Chang (2003) proposed using a 
probabilistic model to identify E-C pairs from 
aligned sentences using phonetic clues. Lam et al
(2004) proposed using semantic and phonetic 
clues to extract E-C pairs from comparable 
corpora. However, these approaches are subject 
to the availability of parallel or comparable 
bitext. A method that explores non-aligned text 
was proposed by harvesting katakana-English 
pairs from query logs (Brill et al 2001). It was 
discovered that the unsupervised learning of such 
a transliteration model could be overwhelmed by 
noisy data, resulting in a decrease in model 
accuracy.  
Many efforts have been made in using Web-
based resources for harvesting transliteration/ 
translation pairs. These include exploring query 
logs (Brill et al 2001), unrelated corpus (Rapp, 
1999), and parallel or comparable corpus (Fung 
and Yee, 1998; Nie et al 1999; Huang et al
2005). To establish correspondence, these 
algorithms usually rely on one or more statistical 
clues, such as the correlation between word 
frequencies, cognates of similar spelling or 
pronunciations. They include two aspects. First, 
a robust mechanism that establishes statistical 
relationships between bilingual words, such as a 
phonetic similarity model which is motivated by 
the TM research; and second, an effective 
learning framework that is able to adaptively 
discover new events from the Web. In the prior 
work, most of the phonetic similarity models 
were trained on a static lexicon. In this paper, we 
address the EX problem by exploiting a novel 
Web-based resource. We also propose a phonetic 
similarity model that generates confidence scores 
for the validation of E-C pairs. 
In Chinese webpages, translated or 
transliterated terms are frequently accompanied 
by their original Latin words. The latter serve as 
the appositives of the former. A sample search 
result for the query submission ?Kuro? is the 
bilingual snippet2 ?...?? Kuro?? P2P???
????????3 ??? P2P ???????
???? C2C (Content to Community)...?. The 
co-occurrence statistics in such a snippet was 
shown to be useful in constructing a transitive 
translation model (Lu et al 2002). In the 
                                               
2 A bilingual snippet refers to a Chinese predominant text 
with embedded English appositives. 
1130
example above, ?Content to Community? is not a 
transliteration of C2C, but rather an acronym 
expansion, while ??? /Ku-Luo/?, as underlined, 
presents a transliteration for ?Kuro?. What is 
important is that the E-C pairs are always closely 
collocated. Inspired by this observation, we 
propose an algorithm that searches over the close 
context of an English word in a bilingual snippet 
for the word?s transliteration candidates.  
The contributions of this paper include: (i) an 
approach to harvesting real life E-C 
transliteration pairs from the Web; (ii) a phonetic 
similarity model that evaluates the confidence of 
so extracted E-C pair candidates; (iii) a 
comparative study of several machine learning 
strategies. 
3 Phonetic Similarity Model 
English and Chinese have different syllable 
structures. Chinese is a syllabic language where 
each Chinese character is a syllable in either 
consonant-vowel (CV) or consonant-vowel-nasal 
(CVN) structure. A Chinese word consists of a 
sequence of characters, phonetically a sequence 
of syllables. Thus, in first E-C transliteration, it 
is a natural choice to syllabify an English word 
by converting its phoneme sequence into a 
sequence of Chinese-like syllables, and then 
convert it into a sequence of Chinese characters.  
There have been several effective algorithms 
for the syllabification of English words for 
transliteration. Typical syllabification algorithms 
first convert English graphemes to phonemes, 
referred to as the letter-to-sound transformation, 
then syllabify the phoneme sequence into a 
syllable sequence. For this method, a letter-to-
sound conversion is needed (Pagel, 1998; 
Jurafsky, 2000). The phoneme-based 
syllabification algorithm is referred to as PSA. 
Another syllabification technique attempts to 
map the grapheme of an English word to 
syllables directly (Kuo and Yang, 2004). The 
grapheme-based syllabification algorithm is 
referred to as GSA. In general, the size of a 
phoneme inventory is smaller than that of a 
grapheme inventory. The PSA therefore requires 
less training data for statistical modeling (Knight, 
1998); on the other hand, the grapheme-based 
method gets rid of the letter-to-sound conversion, 
which is one of the main causes of transliteration 
errors (Li et al 2004).   
Assuming that Chinese transliterations always 
co-occur in proximity to their original English 
words, we propose a phonetic similarity 
modeling (PSM) that measures the phonetic 
similarity between candidate transliteration pairs. 
In a bilingual snippet, when an English word EW 
is spotted, the method searches for the word?s 
possible Chinese transliteration CW in its 
neighborhood. EW can be a single word or a 
phrase of multiple English words. Next, we 
formulate the PSM and the estimation of its 
parameters.  
3.1 Generative Model 
Let 1{ ,... ,... }m MES es es es= be a sequence of 
English syllables derived from EW, using the 
PSA or GSA approach, and 1{ ,... ,... }n NCS cs cs cs=  
be the sequence of Chinese syllables derived 
from CW, represented by a Chinese character 
string 1,... ,...,n NCW c c c? . EW and CW is a 
transliteration pair. The E-C transliteration can 
be considered a generative process formulated by 
the noisy channel model, with EW as the input 
and CW as the output. ( / )P EW CW  is estimated 
to characterize the noisy channel, known as the 
transliteration probability. ( )P CW  is a language 
model to characterize the source language. 
Applying Bayes? rule, we have 
( / ) ( / ) ( ) / ( )P CW EW P EW CW P CW P EW=   (1) 
Following the translation-by-sound principle, the 
transliteration probability ( / )P EW CW can be 
approximated by the phonetic confusion 
probability ( / )P ES CS , which is given as 
( / ) max ( , / ),P ES CS P ES CSD?F= D   (2) 
where F  is the set of all possible alignment 
paths between ES and CS. It is not trivial to find 
the best alignment path D . One can resort to a 
dynamic programming algorithm. Assuming 
conditional independence of syllables in ES and 
CS, we have 1( / ) ( / )
M
m mmP ES CS p es cs== ?  in a 
special case where M N= . Note that, typically, 
we have N M?  due to syllable elision. We 
introduce a null syllable j  and a dynamic 
warping strategy to evaluate ( / )P ES CS  when 
M N? (Kuo et al 2005). With the phonetic 
approximation, Eq.(1) can be rewritten as 
( / ) ( / ) ( ) / ( )P CW EW P ES CS P CW P EW?     (3) 
The language model in Eq.(3) can be 
represented by Chinese characters n-gram 
statistics. 
1 2 11( ) ( / , ,..., )
N
n n nnP CW p c c c c- -== ?   (4) 
1131
In adopting bigram, Eq.(4) is rewritten as 
1 12( ) ( ) ( / )
N
n nnP CW p c p c c -=? ? . Note that the 
context of EW usually has a number of 
competing Chinese transliteration candidates in a 
set, denoted as W . We rank the candidates by 
Eq.(1) to find the most likely CW for a given EW. 
In this process, ( )P EW  can be ignored because it 
is the same for all CW candidates. The CW 
candidate that gives the highest posterior 
probability is considered the most probable 
candidate CW ? . 
arg max ( / )
arg max ( / ) ( )
CW
CW
CW P CW EW
P ES CS P CW
?W
?W
? =
?  (5) 
However, the most probable CW ?  isn?t 
necessarily the desired transliteration. The next 
step is to examine if CW ?  and EW indeed form a 
genuine E-C pair. We define the confidence of 
the E-C pair as the posterior odds similar to that 
in a hypothesis test under the Bayesian 
interpretation. We have 0H , which hypothesizes 
that CW ? and EW  form an E-C pair, and 1H , 
which hypothesizes otherwise. The posterior 
odds is given as follows,  
0
1
'
( / ) ( / ') ( ')
( / ) ( / ) ( )CW
CW CW
P H EW P ES CS P CW
P H EW P ES CS P CW
s
?W?
= ? ? (6) 
where 'CS is the syllable sequence of CW ? , 
1( / )p H EW  is approximated by the probability 
mass of the competing candidates of CW ? , 
or
'
( / ) ( )CW
CW CW
P ES CS P CW?W?? . The higher the s  
is, the more probable that hypothesis 
0H overtakes 1H . The PSM formulation can be 
seen as an extension to prior work (Brill et al 
2001) in transliteration modeling. We introduce 
the posterior odds s as the confidence score so 
that E-C pairs that are extracted from different 
contexts can be directly compared. In practice, 
we set a threshold for s  to decide a cutoff point 
for E-C pairs short-listing. 
3.2 PSM Estimation 
The PSM parameters are estimated from the 
statistics of a given transliteration lexicon, which 
is a collection of manually selected E-C pairs in 
supervised learning, or a collection of high 
confidence E-C pairs in unsupervised learning. 
An initial PSM is bootstrapped using prior 
knowledge such as rule-based syllable mapping. 
Then we align the E-C pairs with the PSM and 
derive syllable mapping statistics for PSA and 
GSA syllabifications. A final PSM is a linear 
combination of the PSA-based PSM (PSA-PSM) 
and the GSA-based PSM (GSA-PSM). The PSM 
parameter ( / )m np es cs can be estimated by an 
Expectation-Maximization (EM) process 
(Dempster, 1977). In the Expectation step, we 
compute the counts of events such as 
# ,m nes cs< >  and # ncs< >  by force-aligning the 
E-C pairs in the training lexicon Y . In the 
Maximization step, we estimate the PSM 
parameters ( / )m np es cs by  
( / ) # , /#m n m n np es cs es cs cs= < > < > .  (7) 
As the EM process guarantees non-decreasing 
likelihood probability ( / )P ES CS"Y? , we let 
the EM process iterate until ( / )P ES CS"Y?  
converges. The EM process can be thought of as 
a refining process to obtain the best alignment 
between the E-C syllables and at the same time a 
re-estimating process for PSM parameters. It is 
summarized as follows. 
Start: Bootstrap PSM parameters 
( / )m np es cs using prior phonetic mapping 
knowledge 
E-Step: Force-align corpus Y  using existing 
( / )m np es cs  and compute the counts of 
# ,m nes cs< >  and # ncs< > ; 
M-Step: Re-estimate ( / )m np es cs  using the 
counts from E-Step. 
Iterate: Repeat E-Step and M-Step until 
( / )P ES CS"Y?  converges. 
4 Adaptive Learning Framework 
We propose an adaptive learning framework 
under which we learn PSM and harvest E-C pairs 
from the Web at the same time. Conceptually, 
the adaptive learning is carried out as follows. 
We obtain bilingual snippets from the Web by 
iteratively submitting queries to the Web search 
engines (Brin and Page, 1998). For each batch of 
querying, the query results are all normalized to 
plain text, from which we further extract 
qualified sentences. A qualified sentence has at 
least one English word. Under this criterion, a 
collection of qualified sentences can be extracted 
automatically. To label the E-C pairs, each 
qualified sentence is manually checked based on 
the following transliteration criteria: (i) if an EW 
is partly translated phonetically and partly 
translated semantically, only the phonetic 
transliteration constituent is extracted to form a 
1132
transliteration pair; (ii) elision of English sound 
is accepted; (iii) multiple E-C pairs can appear in 
one sentence; (iv) an EW can have multiple valid 
Chinese transliterations and vice versa. The 
validation process results in a collection of 
qualified E-C pairs, also referred to as Distinct 
Qualified Transliteration Pairs (DQTPs).  
As formulated in Section 3, the PSM is trained 
using a training lexicon in a data driven manner. 
It is therefore very important to ensure that in the 
learning process we have prepared a quality 
training lexicon. We establish a baseline system 
using supervised learning. In this approach, we 
use human labeled data to train a model. The 
advantage is that it is able to establish a model 
quickly as long as labeled data are available. 
However, this method also suffers from some 
practical issues. First, the derived model can only 
be as good as the data that it sees. An adaptive 
mechanism is therefore needed for the model to 
acquire new knowledge from the dynamically 
growing Web. Second, a massive annotation of 
database is labor intensive, if not entirely 
impossible.  
To reduce the annotation needed, we discuss 
three adaptive strategies cast in the machine 
learning framework, namely active learning, 
unsupervised learning and active-unsupervised 
learning. The learning strategies can be depicted 
in Figure 1 with their difference being discussed 
next. We also train a baseline system using 
supervised learning approach as a reference point 
for benchmarking purpose. 
4.1 Active Learning 
Active learning is based on the assumption that a 
small number of labeled samples, which are 
DQTPs here, and a large number of unlabeled 
 
 
Figure 1. An adaptive learning framework for 
automatic construction of transliteration lexicon. 
samples are available. This assumption is valid in 
most NLP tasks. In contrast to supervised 
learning, where the entire corpus is labeled 
manually, active learning selects the most useful 
samples for labeling and adds the labeled 
examples to the training set to retrain the model. 
This procedure is repeated until the model 
achieves a certain level of performance. 
Practically, a batch of samples is selected each 
time. This is called batch-based sample selection 
(Lewis and Catlett, 1994), as shown in the search 
and ranking block in Figure 1.  
For an active learning to be effective, we 
propose using three measures to select candidates 
for human labeling. First, we would like to select 
the most uncertain samples that are potentially 
highly informative for the PSM model. The 
informativeness of a sample can be quantified by 
its confidence score s  as in the PSM 
formulation. Ranking the E-C pairs by s  is 
referred to as C-rank. The samples of low C-rank 
are the interesting samples to be labeled. Second, 
we would like to select candidates that are of low 
frequency. Ranking by frequency is called F-
rank. During Web crawling, most of the search 
engines use various strategies to prevent 
spamming and one of fundamental tasks is to 
remove the duplicated Web pages. Therefore, we 
assume that the bilingual snippets are all unique. 
Intuitively, E-C pairs of low frequency indicate 
uncommon events which are of higher interest to 
the model. Third, we would like to select 
samples upon which the PSA-PSM and GSA-
PSM disagree the most. The disagreed upon 
samples represent new knowledge to the PSM. In 
short, we select low C-rank, low F-rank and 
PSM-disagreed samples for labeling because the 
high C-rank, high F-rank and PSM-agreed 
samples are already well known to the model. 
4.2 Unsupervised Learning 
Unsupervised learning skips the human labeling 
step. It minimizes human supervision by 
automatically labeling the data. This can be 
effective if prior knowledge about a task is 
available, for example, if an initial PSM can be 
built based on human crafted phonetic mapping 
rules. This is entirely possible. Kuo et al(2005) 
proposed using a cross-lingual phonetic 
confusion matrix resulting from automatic 
speech recognition to bootstrap an initial PSM 
model. The task of labeling samples is basically 
to distinguish the qualified transliteration pairs 
from the rest. Unlike the sample selection 
method in active learning, here we would like to 
Iterate Start 
Final 
PSM 
Initial 
PSM 
Search &  
Ranking 
 
PSM  
Learning 
Lexicon 
Stop 
The Web 
 
 
Select & 
 Labeling 
Training 
Samples 
Labeled 
Samples 
PSM  
Evaluation & Stop 
Criterion 
1133
select the samples that are of high C-rank and 
high F-rank because they are more likely to be 
the desired transliteration pairs. 
The difference between the active learning and 
the unsupervised learning strategies lies in that 
the former selects samples for human labeling, 
such as in the select & labeling block in Figure 1 
before passing on for PSM learning, while the 
latter selects the samples automatically and 
assumes they are all correct DQTPs. The 
disadvantage of unsupervised learning is that it 
tends to reinforce its existing knowledge rather 
than to discover new events.  
4.3 Active-Unsupervised Learning 
The active learning and the unsupervised 
learning strategies can be complementary. Active 
learning minimizes the labeling effort by 
intelligently short-listing informative and 
representative samples for labeling. It makes sure 
that the PSM learns new and informative 
knowledge over iterations. Unsupervised 
learning effectively exploits the unlabelled data. 
It reinforces the knowledge that PSM has 
acquired and allows PSM to adapt to changes at 
no cost. However, we do not expect 
unsupervised learning to acquire new knowledge 
like active learning does. Intuitively, a better 
solution is to integrate the two strategies into one, 
referred to as the active-unsupervised learning 
strategy. In this strategy, we use active learning 
to select a small amount of informative and 
representative samples for labeling. At the same 
time, we select samples of high confidence score 
from the rest and consider them correct E-C pairs. 
We then merge the labeled set with the high-
confidence set in the PSM re-training.  
5 Experiments 
We first construct a development corpus by 
crawling of webpages. This corpus consists of 
about 500 MB of webpages, called SET1 (Kuo et 
al, 2005). Out of 80,094 qualified sentences, 
8,898 DQTPs are manually extracted from SET1, 
which serve as the gold standard in testing. To 
establish a baseline system, we first train a PSM 
using all 8,898 DQTPs in supervised manner and 
conduct a closed test on SET1 as in Table 1. We 
further implement three PSM learning strategies 
and conduct a systematic series of experiments. 
 
 Precision Recall F-measure 
closed-test 0.79 0.69 0.74 
     Table 1. Supervised learning test on SET1 
5.1 Unsupervised Learning 
We follow the formulation described in 
Section 4.2. First, we derive an initial PSM using 
randomly selected 100 seed DQTPs and simulate 
the Web-based learning process with the SET1: 
(i) select high F-rank and high C-rank E-C pairs 
using PSM, (ii) add the selected E-C pairs to the 
DQTP pool as if they are true DQTPs, and (iii) 
reestimate PSM by using the updated DQTP pool. 
In Figure 2, we report the F-measure over 
iterations. The U_HF curve reflects the learning 
progress of using E-C pairs that occur more than 
once in the SET1 corpus (high F-rank). The 
U_HF_HR curve reflects the learning progress 
using a subset of E-C pairs from U_HF which 
has high posterior odds as defined in Eq.(6). 
Both selection strategies aim to select E-C pairs, 
which are as genuine as possible. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
1 2 3 4 5 6
# Iteration
F-
m
ea
su
re Supervised
U_HF
U_HF_HR
 
Figure 2. F-measure over iterations for 
unsupervised learning on SET1. 
 
We found that both U_HF and U_HF_HR give 
similar results in terms of F-measure. Without 
surprise, more iterations don?t always lead to 
better performance because unsupervised 
learning doesn?t aim to acquiring new knowledge 
over iterations. Nevertheless, unsupervised 
learning improves the initial PSM in the first 
iteration substantially. It can serve as an effective 
PSM adaptation method. 
5.2 Active Learning 
The objective of active learning is to minimize 
human supervision by automatically selecting the 
most informative samples to be labeled. The 
effect of active learning is that it maximizes 
performance improvement with minimum 
annotation effort. Like in unsupervised learning, 
we start with the same 100 seed DQTPs and an 
initial PSM model and carry out experiments on 
SET1: (i) select low F-rank, low C-rank and 
GSA-PSM and PSA-PSM disagreed E-C pairs; 
(ii) label the selected pairs by removing the non-
E-C pairs and add the labeled E-C pairs to the 
DQTP pool, and (iii) reestimate the PSM by 
using the updated DQTP pool.  
1134
To select the samples, we employ 3 different 
strategies: A_LF_LR, where we only select low 
F-rank and low C-rank candidates for labeling. 
A_DIFF, where we only select those that GSA-
PSM and PSA-PSM disagreed upon; and 
A_DIFF_LF_LR, the union of A_LF_LR and 
A_DIFF selections. As shown in Figure 3, the F-
measure of A_DIFF (0.729) and 
A_DIFF_LF_LR (0.731) approximate to that of 
supervised learning 0.735) after four iterations.   
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
1 2 3 4 5 6
# Iteration
F-
me
as
ur
e Supervised
A_LF_LR
A_DIFF
A_DIFF_LF_LR
 
Figure 3. F-measure over iterations for active 
learning on SET1. 
 
With almost identical performance as 
supervised learning, the active learning approach 
has greatly reduced the number of samples for 
manual labeling as reported in Table 2. It is 
found that for active learning to reach the 
performance of supervised learning, A_DIFF is 
the most effective strategy. It reduces the 
labeling effort by 89.0%, from 80,094 samples to 
8,750. 
 
 Sample selection #samples labeled 
A_LF_LR 1,671 
A_DIFF 8,750 Active learning A_DIFF_LF_LR 9,683 
Supervised learning 80,094 
Table 2. Number of total samples for manual 
labeling in 6 iterations of Figure 3. 
5.3 Active Unsupervised Learning 
It would be interesting to study the performance 
of combining unsupervised learning and active 
learning. The experiment is similar to that of 
active learning except that, in step (iii) of active 
learning, we take the unlabeled high confidence 
candidates (high F-rank and high C-rank as in 
U_HF_HR of Section 5.1) as the true labeled 
samples and add into the DQTP pool. The result 
is shown in Figure 4. Although active 
unsupervised learning was reported having 
promising results (Riccardi and Hakkani-Tur, 
2003) in some NLP tasks, it has not been as 
effective as active learning alone in this 
experiment probably due to the fact the 
unlabeled high confidence candidates are still too 
noisy to be informative. 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
1 2 3 4 5 6
# Iteration
F-
me
as
ur
e Supervised
AU_LF_LR
AU_DIFF
AU_DIFF_LF_LR
 
Figure 4. F-measure over iterations for active 
unsupervised learning on SET1. 
5.4 Learning Transliteration Lexicons 
The ultimate objective of building a PSM is to 
extract a transliteration lexicon from the Web by 
iteratively submitting queries and harvesting new 
transliteration pairs from the return results until 
no more new pairs. For example, by submitting 
?Robert? to search engines, we may get ?Robert-
????, ?Richard-??? and ?Charles-???? 
in return. In this way, new queries can be 
generated iteratively, thus new pairs are 
discovered. We pick the best performing SET1-
derived PSM trained using A_DIFF_LF_LR 
active learning strategy and test it on a new 
database SET2 which is obtained in the same 
way as SET1. 
 
 Before  adaptation 
After  
adaptation 
#distinct E-C pairs 137,711 130,456  
Precision 0.777 0.846  
#expected DQTPs 107,001 110,365  
Table 3. SET1-derived PSM adapted towards 
SET2. 
 
SET2 contains 67,944 Web pages amounting 
to 3.17 GB. We extracted 2,122,026 qualified 
sentences from SET2. Using the PSM, we extract 
137,711 distinct E-C pairs. As the gold standard 
for SET2 is unavailable, we randomly select 
1,000 pairs for manual checking. A precision of 
0.777 is reported. In this way, 107,001 DQTPs 
can be expected. We further carry out one 
iteration of unsupervised learning using 
U_HF_HR to adapt the SET1-derived PSM 
towards SET2. The results before and after 
adaptation are reported in Table 3. Like the 
experiment in Section 5.1, the unsupervised 
learning improves the PSM in terms of precision 
significantly. 
1135
6 Conclusions 
We have proposed a framework for harvesting E-
C transliteration lexicons from the Web using 
bilingual snippets. In this framework, we 
formulate the PSM learning and E-C pair 
evaluation methods. We have studied three 
strategies for PSM learning aiming at reducing 
the human supervision.  
The experiments show that unsupervised 
learning is an effective way for rapid PSM 
adaptation while active learning is the most 
effective in achieving high performance. We find 
that the Web is a resourceful live corpus for real 
life E-C transliteration lexicon learning, 
especially for casual transliterations. In this 
paper, we use two Web databases SET1 and 
SET2 for simplicity. The proposed framework 
can be easily extended to an incremental learning 
framework for live databases. This paper has 
focused solely on use of phonetic clues for 
lexicon and PSM learning. We have good reason 
to expect the combining semantic and phonetic 
clues to improve the performance further.  
References 
E. Brill, G. Kacmarcik, C. Brockett. 2001. 
Automatically Harvesting Katakana-English Term 
Pairs from Search Engine Query Logs, In Proc. of 
NLPPRS, pp. 393-399. 
S. Brin and L. Page. 1998. The Anatomy of a Large-
scale Hypertextual Web Search Engine, In Proc. of 
7th WWW, pp. 107-117. 
A. P. Dempster, N. M. Laird and D. B. Rubin. 1977. 
Maximum Likelihood from Incomplete Data via 
the EM Algorithm, Journal of the Royal Statistical 
Society, Ser. B. Vol. 39, pp. 1-38. 
P. Fung and L.-Y. Yee. 1998. An IR Approach for 
Translating New Words from Nonparallel, 
Comparable Texts. In Proc. of 17th COLING and 
36th ACL, pp. 414-420. 
F. Huang, Y. Zhang and Stephan Vogel. 2005. Mining 
Key Phrase Translations from Web Corpora. In 
Proc. of HLT-EMNLP, pp. 483-490. 
D. Jurafsky and J. H. Martin. 2000. Speech and 
Language Processing, pp. 102-120, Prentice-Hall, 
New Jersey. 
K. Knight and J. Graehl. 1998. Machine 
Transliteration, Computational Linguistics, Vol. 24, 
No. 4, pp. 599-612. 
J.-S. Kuo and Y.-K. Yang. 2004. Constructing 
Transliterations Lexicons from Web Corpora, In 
the Companion Volume, 42nd ACL, pp. 102-105. 
J.-S. Kuo and Y.-K. Yang. 2005. Incorporating 
Pronunciation Variation into Extraction of 
Transliterated-term Pairs from Web Corpora, In 
Proc. of ICCC, pp. 131-138. 
C.-J. Lee and J.-S. Chang. 2003. Acquisition of 
English-Chinese Transliterated Word Pairs from 
Parallel-Aligned Texts Using a Statistical Machine 
Transliteration Model, In Proc. of HLT-NAACL 
Workshop Data Driven MT and Beyond, pp. 96-
103. 
D. D. Lewis and J. Catlett. 1994. Heterogeneous 
Uncertainty Sampling for Supervised Learning, In 
Proc. of ICML 1994, pp. 148-156. 
H. Li, M. Zhang and J. Su. 2004. A Joint Source 
Channel Model for Machine Transliteration, In 
Proc. of 42nd ACL, pp. 159-166. 
W. Lam, R.-Z. Huang and P.-S. Cheung. 2004. 
Learning Phonetic Similarity for Matching Named 
Entity Translations and Mining New Translations, 
In Proc. of 27th ACM SIGIR, pp. 289-296. 
W.-H. Lu, L.-F. Chien and H.-J Lee. 2002. 
Translation of Web Queries Using Anchor Text 
Mining, TALIP, Vol. 1, Issue 2, pp. 159- 172. 
H. M. Meng, W.-K. Lo, B. Chen and T. Tang. 2001. 
Generate Phonetic Cognates to Handle Name 
Entities in English-Chinese Cross-Language 
Spoken Document Retrieval, In Proc. of ASRU, pp. 
311-314. 
J.-Y. Nie, P. Isabelle, M. Simard, and R. Durand. 
1999. Cross-language Information Retrieval based 
on Parallel Texts and Automatic Mining of Parallel 
Text from the Web?, In Proc. of 22nd ACM SIGIR, 
pp 74-81. 
V. Pagel, K. Lenzo and A. Black. 1998. Letter to 
Sound Rules for Accented Lexicon Compression, 
In Proc. of ICSLP, pp. 2015-2020. 
R. Rapp. 1999. Automatic Identification of Word 
Translations from Unrelated English and German 
Corpora, In Proc. of 37th ACL, pp. 519-526. 
G. Riccardi and D. Hakkani-T?r. 2003. Active and 
Unsupervised Learning for Automatic Speech 
Recognition. In Proc. of 8th Eurospeech. 
P. Virga and S. Khudanpur. 2003. Transliteration of 
Proper Names in Cross-Lingual Information 
Retrieval, In Proc.  of 41st ACL Workshop on 
Multilingual and Mixed Language Named Entity 
Recognition, pp. 57-64. 
S. Wan and C. M. Verspoor. 1998. Automatic 
English-Chinese Name Transliteration for 
Development of Multilingual Resources, In Proc. of 
17th COLING and 36th ACL, pp.1352-1356. 
1136
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 120?127,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
 
 
 
 
 
 
Abstract 
Words of foreign origin are referred to as 
borrowed words or loanwords. A loanword 
is usually imported to Chinese by phonetic 
transliteration if a translation is not easily 
available. Semantic transliteration is seen 
as a good tradition in introducing foreign 
words to Chinese. Not only does it preserve 
how a word sounds in the source language, 
it also carries forward the word?s original 
semantic attributes. This paper attempts to 
automate the semantic transliteration 
process for the first time. We conduct an 
inquiry into the feasibility of semantic 
transliteration and propose a probabilistic 
model for transliterating personal names in 
Latin script into Chinese. The results show 
that semantic transliteration substantially 
and consistently improves accuracy over 
phonetic transliteration in all the 
experiments. 
1 Introduction 
The study of Chinese transliteration dates back to 
the seventh century when Buddhist scriptures were 
translated into Chinese. The earliest bit of Chinese 
translation theory related to transliteration may be 
the principle of ?Names should follow their 
bearers, while things should follow Chinese.? In 
other words, names should be transliterated, while 
things should be translated according to their 
meanings. The same theory still holds today.  
Transliteration has been practiced in several 
ways, including phonetic transliteration and 
phonetic-semantic transliteration. By phonetic 
transliteration, we mean rewriting a foreign word 
in native grapheme such that its original 
pronunciation is preserved. For example, London 
becomes ??  /Lun-Dun/1 which does not carry 
any clear connotations. Phonetic transliteration 
represents the common practice in transliteration. 
Phonetic-semantic transliteration, hereafter 
referred to as semantic transliteration for short, is 
an advanced translation technique that is 
considered as a recommended translation practice 
for centuries. It translates a foreign word by 
preserving both its original pronunciation and 
meaning. For example, Xu Guangqi 2  translated 
geo- in geometry into Chinese as ??  /Ji-He/, 
which carries the pronunciation of geo- and 
expresses the meaning of ?a science concerned 
with measuring the earth?.  
Many of the loanwords exist in today?s Chinese 
through semantic transliteration, which has been 
well received (Hu and Xu, 2003; Hu, 2004) by the 
people because of many advantages. Here we just 
name a few. (1) It brings in not only the sound, but 
also the meaning that fills in the semantic blank 
left by phonetic transliteration. This also reminds 
people that it is a loanword and avoids misleading; 
(2) It provides etymological clues that make it easy 
to trace back to the root of the words. For example, 
a transliterated Japanese name will maintain its 
Japanese identity in its Chinese appearance; (3) It 
evokes desirable associations, for example, an 
English girl?s name is transliterated with Chinese 
characters that have clear feminine association, 
thus maintaining the gender identity. 
                                                 
1 Hereafter, Chinese characters are also denoted in Pinyin ro-
manization system, for ease of reference.  
2 Xu Quangqi (1562?1633) translated The Original Manu-
script of Geometry to Chinese jointly with Matteo Ricci. 
Semantic Transliteration of Personal Names 
Haizhou Li*,  Khe Chai Sim*,  Jin-Shea Kuo?,  Minghui Dong* 
*Institute for Infocomm Research 
Singapore 119613 
{hli,kcsim,mhdong}@i2r.a-star.edu.sg 
?Chung-Hwa Telecom Laboratories 
Taiwan 
jskuo@cht.com.tw 
120
Unfortunately, most of the reported work in the 
area of machine transliteration has not ventured 
into semantic transliteration yet. The Latin-scripted 
personal names are always assumed to 
homogeneously follow the English phonic rules in 
automatic transliteration (Li et al, 2004). 
Therefore, the same transliteration model is 
applied to all the names indiscriminatively. This 
assumption degrades the performance of 
transliteration because each language has its own 
phonic rule and the Chinese characters to be 
adopted depend on the following semantic 
attributes of a foreign name. 
(1) Language of origin: An English word is not 
necessarily of pure English origin. In English news 
reports about Asian happenings, an English 
personal name may have been originated from 
Chinese, Japanese or Korean. The language origin 
affects the phonic rules and the characters to be 
used in transliteration3. For example, a Japanese 
name Matsumoto should be transliterated as ?? 
/Song-Ben/, instead of ???? /Ma-Ci-Mo-Tuo/ 
as if it were an English name. 
(2) Gender association: A given name typically 
implies a clear gender association in both the 
source and target languages. For example, the 
Chinese transliterations of Alice and Alexandra 
are ??? /Ai-Li-Si/ and ???? /Ya-Li-Shan-
Da/ respectively, showing clear feminine and 
masculine characteristics. Transliterating Alice as 
???  /Ai-Li-Si/ is phonetically correct, but 
semantically inadequate due to an improper gender 
association. 
(3) Surname and given name: The Chinese name 
system is the original pattern of names in Eastern 
Asia such as China, Korea and Vietnam, in which 
a limited number of characters 4  are used for 
surnames while those for given names are less 
restrictive. Even for English names, the character 
set for given name transliterations are different 
from that for surnames. 
Here are two examples of semantic 
transliteration for personal names.  George Bush 
                                                 
3 In the literature (Knight  and  Graehl,1998; Qu et al, 2003), 
translating romanized Japanese or Chinese names to Chinese 
characters is also known as back-transliteration. For simplic-
ity, we consider all conversions from Latin-scripted words to 
Chinese as transliteration in this paper. 
4 The 19 most common surnames cover 55.6% percent of the 
Chinese population (Ning and Ning 1995). 
and Yamamoto Akiko are transliterated into ??  
?? and ??  ???  that arouse to the 
following associations: ??  /Qiao-Zhi/ - male 
given name, English origin; ??  /Bu-Shi/ - 
surname, English origin; ? ?  /Shan-Ben/ - 
surname, Japanese origin; ??? /Ya-Xi-Zi/ - 
female given name, Japanese origin. 
 In Section 2, we summarize the related work. In 
Section 3, we discuss the linguistic feasibility of 
semantic transliteration for personal names. 
Section 4 formulates a probabilistic model for 
semantic transliteration.  Section 5 reports the 
experiments. Finally, we conclude in Section 6. 
2 Related Work 
In general, computational studies of transliteration 
fall into two categories: transliteration modeling 
and extraction of transliteration pairs. In 
transliteration modeling, transliteration rules are 
trained from a large, bilingual transliteration 
lexicon (Lin and Chen, 2002; Oh and Choi, 2005), 
with the objective of translating unknown words 
on the fly in an open, general domain. In the 
extraction of transliterations, data-driven methods 
are adopted to extract actual transliteration pairs 
from a corpus, in an effort to construct a large, up-
to-date transliteration lexicon (Kuo et al, 2006; 
Sproat et al, 2006).  
Phonetic transliteration can be considered as an 
extension to the traditional grapheme-to-phoneme 
(G2P) conversion (Galescu and Allen, 2001), 
which has been a much-researched topic in the 
field of speech processing. If we view the 
grapheme and phoneme as two symbolic 
representations of the same word in two different 
languages, then G2P is a transliteration task by 
itself. Although G2P and phonetic transliteration 
are common in many ways, transliteration has its 
unique challenges, especially as far as E-C 
transliteration is concerned. E-C transliteration is 
the conversion between English graphemes, 
phonetically associated English letters, and 
Chinese graphemes, characters which represent 
ideas or meanings. As a Chinese transliteration can 
arouse to certain connotations, the choice of 
Chinese characters becomes a topic of interest (Xu 
et al, 2006). 
Semantic transliteration can be seen as a subtask 
of statistical machine translation (SMT) with 
121
monotonic word ordering. By treating a 
letter/character as a word and a group of 
letters/characters as a phrase or token unit in SMT, 
one can easily apply the traditional SMT models, 
such as the IBM generative model (Brown et al, 
1993) or the phrase-based translation model (Crego 
et al, 2005) to transliteration. In transliteration, we 
face similar issues as in SMT, such as lexical 
mapping and alignment. However, transliteration is 
also different from general SMT in many ways. 
Unlike SMT where we aim at optimizing the 
semantic transfer, semantic transliteration needs to 
maintain the phonetic equivalence as well. 
In computational linguistic literature, much 
effort has been devoted to phonetic transliteration, 
such as English-Arabic, English-Chinese (Li et al, 
2004), English-Japanese (Knight and Graehl, 
1998) and English-Korean. In G2P studies, Font 
Llitjos and Black (2001) showed how knowledge 
of language of origin may improve conversion 
accuracy. Unfortunately semantic transliteration, 
which is considered as a good tradition in 
translation practice (Hu and Xu, 2003; Hu, 2004), 
has not been adequately addressed computationally 
in the literature. Some recent work (Li et al, 2006; 
Xu et al, 2006) has attempted to introduce 
preference into a probabilistic framework for 
selection of Chinese characters in phonetic 
transliteration. However, there is neither analytical 
result nor semantic-motivated transliteration 
solution being reported. 
3 Feasibility of Semantic Transliteration 
A Latin-scripted personal name is written in letters, 
which represent the pronunciations closely, 
whereas each Chinese character represents not only 
the syllables, but also the semantic associations. 
Thus, character rendering is a vital issue in trans-
literation. Good transliteration adequately projects 
semantic association while an inappropriate one 
may lead to undesirable interpretation. 
Is semantic transliteration possible? Let?s first 
conduct an inquiry into the feasibility of semantic 
transliteration on 3 bilingual name corpora, which 
are summarizied in Table 1 and will be used in 
experiments. E-C corpus is an augmented version 
of Xinhua English to Chinese dictionary  for 
English names (Xinhua, 1992). J-C corpus is a 
romanized Japanese to Chinese dictionary for 
Japanese names. The C-C corpus is a Chinese 
Pinyin to character dictionary for Chinese names. 
The entries are classified into surname, male and 
female given name categories. The E-C corpus also 
contains some entries without gender/surname 
labels, referred to as unclassified. 
 
 E-C J-C5 C-C6 
Surname (S) 12,490 36,352 569,403 
Given name (M) 3,201 35,767 345,044 
Given name (F) 4,275 11,817 122,772 
Unclassified 22,562 - - 
All 42,528 83,936 1,972,851 
Table 1: Number of entries in 3 corpora 
 
Phonetic transliteration has not been a problem 
as Chinese has over 400 unique syllables that are 
enough to approximately transcribe all syllables in 
other languages. Different Chinese characters may 
render into the same syllable and form a range of 
homonyms. Among the homonyms, those arousing 
positive meanings can be used for personal names. 
As discussed elsewhere (Sproat et al, 1996), out of 
several thousand common Chinese characters, a 
subset of a few hundred characters tends to be used 
overwhelmingly for transliterating English names 
to Chinese, e.g. only 731 Chinese characters are 
adopted in the E-C corpus. Although the character 
sets are shared across languages and genders, the 
statistics in Table 2 show that each semantic 
attribute is associated with some unique characters. 
In the C-C corpus, out of the total of 4,507 
characters, only 776 of them are for surnames. It is 
interesting to find that female given names are 
represented by a smaller set of characters than that 
for male across 3 corpora.     
 
 E-C J-C C-C All 
S 327 2,129 776 2,612 (19.2%)
M 504 1,399 4,340 4,995 (20.0%)
F 479 1,178 1,318 2,192 (26.3%)
All 731 (44.2%)
2,533 
(46.2%)
4,507 
(30.0%) 5,779 (53.6%)
Table 2: Chinese character usage in 3 corpora. The 
numbers in brackets indicate the percentage of 
characters that are shared by at least 2 corpora. 
 
Note that the overlap of Chinese characters 
usage across genders is higher than that across 
languages. For instance, there is a 44.2% overlap 
                                                 
5 http://www.cjk.org 
6 http://technology.chtsai.org/namelist 
122
across gender for the transcribed English names; 
but only 19.2% overlap across languages for the 
surnames. 
In summary, the semantic attributes of personal 
names are characterized by the choice of characters, 
and therefore their n-gram statistics as well. If the 
attributes are known in advance, then the semantic 
transliteration is absolutely feasible. We may 
obtain the semantic attributes from the context 
through trigger words. For instance, from ?Mr 
Tony Blair?, we realize ?Tony? is a male given 
name while ?Blair? is a surname; from  ?Japanese 
Prime Minister Koizumi?, we resolve that 
?Koizumi? is a Japanese surname. In the case 
where contextual trigger words are not available, 
we study detecting the semantic attributes from the 
personal names themselves in the next section. 
4 Formulation of Transliteration Model  
Let S and T denote the name written in the source 
and target writing systems respectively. Within a 
probabilistic framework, a transliteration system 
produces the optimum target name, T*, which 
yields the highest posterior probability given the 
source name, S, i.e. 
)|(maxarg* STPT
T ST?
=  (1) 
where ST  is the set of all possible transliterations 
for the source name, S. The alignment between S 
and T is assumed implicit in the above formulation.  
In a standard phonetic transliteration system, 
)|( STP , the posterior probability of the hypothe-
sized transliteration, T, given the source name, S, is 
directly modeled without considering any form of 
semantic information. On the other hand, semantic 
transliteration described in this paper incorporates 
language of origin and gender information to cap-
ture the semantic structure. To do so, )|( STP  is 
rewritten as 
( | )P T S  = ??? GL GL SGLTP, )|,,(  (2) 
 = ??? GL GL SGLPGLSTP, )|,(),,|(  (3) 
where ( | , , )P T S L G  is the transliteration probabil-
ity from source S to target T, given the language of 
origin (L) and gender (G) labels. L  and G denote 
the sets of languages and genders respectively. 
)|,( SGLP  is the probability of the language and 
the gender given the source, S. 
Given the alignment between S and T, the 
transliteration probability given L and G may be 
written as  
),,|( GLSTP = 11 1
1
( | , )
I
i i
i
i
P t T S?
=
?  (4)
 ? 1 1
1
( | , , )
I
i i i i
i
P t t s s? ?
=
?  (5)
where is  and it are the i
th token of S and T respec-
tively and I is the total number of tokens in both S 
and T. kjS  and kjT  represent the sequence of tokens 
( )1, , ,j j ks s s+ K  and ( )1, , ,j j kt t t+ K  respectively. Eq. 
(4) is in fact the n-gram likelihood of the token pair 
,i it s? ?  sequence and Eq. (5) approximates this 
probability using a bigram language model. This 
model is conceptually similar to the joint source-
channel model (Li et al, 2004) where the target to-
ken it  depends on not only its source token is but 
also the history 1it ? and 1is ? . Each character in the 
target name forms a token. To obtain the source 
tokens, the source and target names in the training 
data are aligned using the EM algorithm. This 
yields a set of possible source tokens and a map-
ping between the source and target tokens. During 
testing, each source name is first segmented into 
all possible token sequences given the token set. 
These source token sequences are mapped to the 
target sequences to yield an N-best list of translit-
eration candidates. Each candidate is scored using 
an n-gram language model given by Eqs. (4) or (5). 
As in Eq. (3), the transliteration also greatly 
depends on the prior knowledge, )|,( SGLP . 
When no prior knowledge is available, a uniform 
probability distribution is assumed. By expressing 
)|,( SGLP  in the following form, 
)|(),|()|,( SLPSLGPSGLP =  (6) 
prior knowledge about language and gender may 
be incorporated. For example, if the language of S 
is known as sL , we have 
1
( | )
0
s
s
L L
P L S
L L
=?= ? ??
 (7) 
Similarly, if the gender information for S is known 
as sG , then, 
123
1
( | , )
0
s
s
G G
P G L S
G G
=?= ? ??
 (8) 
Note that personal names have clear semantic 
associations. In the case where the semantic 
attribute information is not available, we propose 
learning semantic information from the names 
themselves. Using Bayes? theorem, we have 
)(
),(),|()|,(
SP
GLPGLSPSGLP =  (9) 
( | , )P S L G  can be modeled using an n-gram lan-
guage model for the letter sequence of all the 
Latin-scripted names in the training set. The prior 
probability, ),( GLP , is typically uniform. )(SP  
does not depend on L and G, thus can be omitted. 
Incorporating )|,( SGLP into Eq. (3) can be 
viewed as performing a soft decision of the 
language and gender semantic attributes. By 
contrast, hard decision may also be performed 
based on maximum likelihood approach: 
arg max ( | )s
L
L P S L
?
=
L
 (10) 
arg max ( | , )s
G
G P S L G
?
=
G
 (11) 
where sL  and sG are the detected language and 
gender of S respectively. Therefore, for hard deci-
sion, )|,( SGLP  is obtained by replacing sL  and 
sG  in Eq. (7) and (8) with sL  and sG respec-
tively. Although hard decision eliminates the need 
to compute the likelihood scores for all possible 
pairs of L and G, the decision errors made in the 
early stage will propagate to the transliteration 
stage. This is potentially bad if a poor detector is 
used (see Table 9 in Section 5.3). 
If we are unable to model the prior knowledge 
of semantic attributes )|,( SGLP , then a more 
general model will be used for ( | , , )P T S L G  by 
dropping the dependency on the information that is 
not available. For example, Eq. (3) is reduced 
to ( | , ) ( | )
L
P T S L P L S?? L  if the gender information 
is missing. Note that when both language and 
gender are unknown, the system simplifies to the 
baseline phonetic transliteration system. 
5 Experiments 
This section presents experiments on database of 3 
language origins (Japanese, Chinese and English) 
and gender information (surname7, male and fe-
male). In the experiments of determining the lan-
guage origin, we used the full data set for the 3 lan-
guages as in shown in Table 1. The training and test 
data for semantic transliteration are the subset of 
Table 1 comprising those with surnames, male and 
female given names labels. In this paper, J, C and 
E stand for Japanese, Chinese and English; S, M 
and F represent Surname, Male and Female given 
names, respectively.  
 
# unique entries L Data set S M F All 
Train 21.7k 5.6k 1.7k 27.1k J 
Test 2.6k 518 276 2.9k 
Train 283 29.6k 9.2k 31.5k C 
Test 283 2.9k 1.2k 3.1k 
Train 12.5k 2.8k 3.8k 18.5k E 
Test 1.4k 367 429 2.1k 
Table 3: Number of unique entries in training and 
test sets, categorized by semantic attributes 
 
Table 3 summarizes the number of unique8 name 
entries used in training and testing. The test sets 
were randomly chosen such that the amount of test 
data is approximately 10-20% of the whole corpus. 
There were no overlapping entries between the 
training and test data. Note that the Chinese sur-
names are typically single characters in a small set; 
we assume there is no unseen surname in the test 
set. All the Chinese surname entries are used for 
both training and testing. 
5.1 Language of Origin 
For each language of origin, a 4-gram language 
model was trained for the letter sequence of the 
source names, with a 1-letter shift. 
 
Japanese Chinese English All 
96.46 96.44 89.90 94.81 
Table 4: Language detection accuracies (%) using 
a 4-gram language model for the letter sequence of 
the source name in Latin script. 
                                                 
7 In this paper, surnames are treated as a special class of gen-
der. Unlike given names, they do not have any gender associa-
tion. Therefore, they fall into a third category which is neither 
male nor female.  
8 By contrast, Table 1 shows the total number of name exam-
ples available. For each unique entry, there may be multiple 
examples. 
124
 
Table 4 shows the language detection accuracies 
for all the 3 languages using Eq. (10). The overall 
detection accuracy is 94.81%. The corresponding 
Equal Error Rate (EER)9 is 4.52%. The detection 
results may be used directly to infer the semantic 
information for transliteration. Alternatively, the 
language model likelihood scores may be 
incorporated into the Bayesian framework to 
improve the transliteration performance, as 
described in Section 4. 
5.2 Gender Association 
Similarly, gender detection 10  was performed by 
training a 4-gram language model for the letter se-
quence of the source names for each language and 
gender pair.  
 
Language Male Female All 
Japanese 90.54 80.43 87.03 
Chinese 64.34 71.66 66.52 
English 75.20 72.26 73.62 
Table 5: Gender detection accuracies (%) using a 
4-gram language model for the letter sequence of 
the source name in Latin script. 
 
Table 5 summarizes the gender detection accura-
cies using Eq. (11) assuming language of origin is 
known, arg max ( | , )s s
G
G P S L L G
?
= =
G
. The overall 
detection accuracies are 87.03%, 66.52% and 
73.62% for Japanese, Chinese and English respec-
tively. The corresponding EER are 13.1%, 21.8% 
and 19.3% respectively. Note that gender detection 
is generally harder than language detection. This is 
because the tokens (syllables) are shared very 
much across gender categories, while they are 
quite different from one language to another.  
5.3 Semantic Transliteration 
The performance was measured using the Mean 
Reciprocal Rank (MRR) metric (Kantor and Voor-
hees, 2000), a measure that is commonly used in 
information retrieval, assuming there is precisely 
one correct answer. Each transliteration system 
generated at most 50-best hypotheses for each 
                                                 
9 EER is defined as the error of false acceptance and false re-
jection when they are equal. 
10 In most writing systems, the ordering of surname and 
given name is known. Therefore, gender detection is 
only performed for male and female classes. 
word when computing MRR. The word and char-
acter accuracies of the top best hypotheses are also 
reported.  
We used the phonetic transliteration system as 
the baseline to study the effects of semantic 
transliteration. The phonetic transliteration system 
was trained by pooling all the available training 
data from all the languages and genders to estimate 
a language model for the source-target token pairs. 
Table 6 compares the MRR performance of the 
baseline system using unigram and bigram 
language models for the source-target token pairs. 
 
 J C E All 
Unigram 0.5109 0.4869 0.2598 0.4443 
Bigram 0.5412 0.5261 0.3395 0.4895 
Table 6:  MRR performance of phonetic translit-
eration for 3 corpora using unigram and bigram 
language models. 
 
The MRR performance for Japanese and Chinese 
is in the range of 0.48-0.55. However, due to the 
small amount of training and test data, the MRR 
performance of the English name transliteration is 
slightly poor (approximately 0.26-0.34). In general, 
a bigram language model gave an overall relative 
improvement of 10.2% over a unigram model.  
 
L G Set J C E 
S 0.5366 0.7426 0.4009 
M 0.5992 0.5184 0.2875 
F 0.4750 0.4945 0.1779 2 2 
All 0.5412 0.5261 0.3395 
S 0.6500 0.7971 0.7178 
M 0.6733 0.5245 0.4978 
F 0.5956 0.5191 0.4115 2 
All 0.6491 0.5404 0.6228 
S 0.6822 0.9969 0.7382 
M 0.7267 0.6466 0.4319 
F 0.5856 0.7844 0.4340 
3 
3 
All 0.6811 0.7075 0.6294 
S 0.6541 0.6733 0.7129 
M 0.6974 0.5362 0.4821 
F 0.5743 0.6574 0.4138 
c c 
All 0.6477 0.5764 0.6168 
Table 7: The effect of language and gender in-
formation on the overall MRR performance of 
transliteration (L=Language, G=Gender, 
2=unknown, 3=known, c=soft decision). 
 
Next, the scenarios with perfect language and/or 
gender information were considered. This com-
125
parison is summarized in Table 7. All the MRR re-
sults are based on transliteration systems using bi-
gram language models. The table clearly shows 
that having perfect knowledge, denoted by ?3?, of 
language and gender helps improve the MRR per-
formance; detecting semantic attributes using soft 
decision, denoted by ?c?, has a clear win over the 
baseline, denoted by ?2?, where semantic informa-
tion is not used. The results strongly recommend 
the use of semantic transliteration for personal 
names in practice. 
Next let?s look into the effects of automatic 
language and gender detection on the performance. 
 
 J C E All 
2 0.5412 0.5261 0.3395 0.4895 
? 0.6292 0.5290 0.5780 0.5734 
c 0.6162 0.5301 0.6088 0.5765 
3 0.6491 0.5404 0.6228 0.5952 
Table 8: The effect of language detection 
schemes on MRR using bigram language models 
and unknown gender information (hereafter, 
2=unknown, 3=known, ?=hard decision, c=soft 
decision). 
 
Table 8 compares the MRR performance of the 
semantic transliteration systems with different 
prior information, using bigram language models. 
Soft decision refers to the incorporation of the lan-
guage model scores into the transliteration process 
to improve the prior knowledge in Bayesian infer-
ence. Overall, both hard and soft decision methods 
gave similar MRR performance of approximately 
0.5750, which was about 17.5% relatively im-
provement compared to the phonetic transliteration 
system with 0.4895 MRR. The hard decision 
scheme owes its surprisingly good performance to 
the high detection accuracies (see Table 4). 
 
 S M F All 
2 0.6825 0.5422 0.5062 0.5952 
? 0.7216 0.4674 0.5162 0.5855 
c 0.7216 0.5473 0.5878 0.6267 
3 0.7216 0.6368 0.6786 0.6812 
Table 9: The effect of gender detection schemes 
on MRR using bigram language  
models with perfect language information. 
 
Similarly, the effect of various gender detection 
methods used to obtain the prior information is 
shown in Table 9. The language information was 
assumed known a-priori. Due to the poorer 
detection accuracy for the Chinese male given 
names (see Table 5), hard decision of gender had 
led to deterioration in MRR performance of the 
male names compared to the case where no prior 
information was assumed. Soft decision of gender 
yielded further gains of 17.1% and 13.9% relative 
improvements for male and female given names 
respectively, over the hard decision method. 
 
Overall Accuracy (%) L G MRR Word Character 
2 2 0.4895 36.87 58.39 
2 0.5952 46.92 65.18 3 3 0.6812 58.16 70.76 
? ? 0.5824 47.09 66.84 
c c 0.6122 49.38 69.21 
Table 10: Overall transliteration performance 
using bigram language model with various lan-
guage and gender information. 
 
Finally, Table 10 compares the performance of 
various semantic transliteration systems using bi-
gram language models. The baseline phonetic 
transliteration system yielded 36.87% and 58.39% 
accuracies at word and character levels respec-
tively; and 0.4895 MRR. It can be conjectured 
from the results that semantic transliteration is sub-
stantially superior to phonetic transliteration. In 
particular, knowing the language information im-
proved the overall MRR performance to 0.5952; 
and with additional gender information, the best 
performance of 0.6812 was obtained. Furthermore, 
both hard and soft decision of semantic informa-
tion improved the performance, with the latter be-
ing substantially better. Both the word and charac-
ter accuracies improvements were consistent and 
have similar trend to that observed for MRR.  
The performance of the semantic transliteration 
using soft decisions (last row of Table 10) 
achieved 25.1%, 33.9%, 18.5% relative improve-
ment in MRR, word and character accuracies 
respectively over that of the phonetic 
transliteration (first row of Table 10). In addition, 
soft decision also presented 5.1%, 4.9% and 3.5% 
relative improvement over hard decision in MRR, 
word and character accuracies respectively. 
5.4 Discussions 
It was found that the performance of the baseline 
phonetic transliteration may be greatly improved 
by incorporating semantic information such as the 
language of origin and gender. Furthermore, it was 
found that the soft decision of language and gender 
126
outperforms the hard decision approach. The soft 
decision method incorporates the semantic scores 
( , | )P L G S with transliteration scores ( | , , )P T S L G , 
involving all possible semantic specific models in 
the decoding process.  
In this paper, there are 9 such models (3 
languages? 3 genders). The hard decision relies on 
Eqs. (10) and (11) to decide language and gender, 
which only involves one semantic specific model 
in the decoding. Neither soft nor hard decision 
requires any prior information about the names. It 
provides substantial performance improvement 
over phonetic transliteration at a reasonable 
computational cost. If the prior semantic 
information is known, e.g. via trigger words, then 
semantic transliteration attains its best performance. 
6 Conclusion 
Transliteration is a difficult, artistic human en-
deavor, as rich as any other creative pursuit. Re-
search on automatic transliteration has reported 
promising results for regular transliteration, where 
transliterations follow certain rules. The generative 
model works well as it is designed to capture regu-
larities in terms of rules or patterns. This paper ex-
tends the research by showing that semantic trans-
literation of personal names is feasible and pro-
vides substantial performance gains over phonetic 
transliteration.  This paper has presented a success-
ful attempt towards semantic transliteration using 
personal name transliteration as a case study. It 
formulates a mathematical framework that incor-
porates explicit semantic information (prior 
knowledge), or implicit one (through soft or hard 
decision) into the transliteration model. Extending 
the framework to machine transliteration of named 
entities in general is a topic for further research. 
References 
Peter F. Brown and Stephen Della Pietra and Vincent J. 
Della Pietra and Robert L. Mercer. 1993, The Mathe-
matics of Statistical Machine Translation: Parameter 
Estimation, Computational Linguistics, 19(2), pp. 
263-311. 
J. M. Crego, M. R. Costa-jussa and J. B. Mario and J. A. 
R. Fonollosa. 2005, N-gram-based versus Phrase-
based Statistical Machine Translation, In Proc. of 
IWSLT, pp. 177-184. 
Ariadna Font Llitjos, Alan W. Black. 2001. Knowledge 
of language origin improves pronunciation accuracy 
of proper names. In Proc. of Eurospeech, Denmark, 
pp 1919-1922. 
Lucian Galescu and James F. Allen. 2001, Bi-
directional Conversion between Graphemes and Pho-
nemes using a Joint N-gram Model, In Proc. 4th ISCA 
Tutorial and Research Workshop on Speech Synthesis, 
Scotland, pp. 103-108. 
Peter Hu, 2004, Adapting English to Chinese, English 
Today, 20(2), pp. 34-39. 
Qingping Hu and Jun Xu, 2003, Semantic Translitera-
tion: A Good Tradition in Translating Foreign Words 
into Chinese Babel: International Journal of Transla-
tion, Babel, 49(4), pp. 310-326. 
Paul B. Kantor and Ellen M. Voorhees, 2000, The 
TREC-5 Confusion Track: Comparing Retrieval 
Methods for Scanned Text. Informational Retrieval, 2, 
pp. 165-176. 
K. Knight and J. Graehl. 1998. Machine Transliteration, 
Computational Linguistics 24(4), pp. 599-612. 
J.-S. Kuo, H. Li and Y.-K. Yang. 2006. Learning Trans-
literation Lexicons from the Web, In Proc. of 44th 
ACL, pp. 1129-1136. 
Haizhou Li, Min Zhang and Jian Su. 2004. A Joint 
Source Channel Model for Machine Transliteration, In 
Proc. of 42nd ACL, pp. 159-166. 
Haizhou Li, Shuanhu Bai, and Jin-Shea Kuo, 2006, 
Transliteration, In Advances in Chinese Spoken Lan-
guage Processing, C.-H. Lee, et al (eds), World Sci-
entific, pp. 341-364. 
Wei-Hao Lin and Hsin-Hsi Chen, 2002, Backward ma-
chine transliteration by learning phonetic similarity, In 
Proc. of CoNLL , pp.139-145. 
Yegao Ning and Yun Ning, 1995, Chinese Personal 
Names, Federal Publications, Singapore. 
Jong-Hoon Oh and Key-Sun Choi. 2005, An Ensemble 
of Grapheme and Phoneme for Machine Translitera-
tion, In Proc. of IJCNLP, pp.450-461. 
Y. Qu, G. Grefenstette and D. A. Evans, 2003, Auto-
matic Transliteration for Japanese-to-English Text Re-
trieval. In Proc. of 26th ACM SIGIR, pp. 353-360. 
Richard Sproat, C. Chih, W. Gale, and N. Chang. 1996. 
A stochastic Finite-state Word-segmentation Algo-
rithm for Chinese, Computational Linguistics, 22(3), 
pp. 377-404. 
Richard Sproat, Tao Tao and ChengXiang Zhai. 2006. 
Named Entity Transliteration with Comparable Cor-
pora, In Proc. of 44th ACL, pp. 73-80. 
Xinhua News Agency, 1992, Chinese Transliteration of 
Foreign Personal Names, The Commercial Press. 
L. Xu, A. Fujii, T. Ishikawa, 2006 Modeling Impression 
in Probabilistic Transliteration into Chinese, In Proc. 
of EMNLP 2006, Sydney,  pp. 242?249. 
127
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 712?719,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Ordering Phrases with Function Words
Hendra Setiawan and Min-Yen Kan
School of Computing
National University of Singapore
Singapore 117543
{hendrase,kanmy}@comp.nus.edu.sg
Haizhou Li
Institute for Infocomm Research
21 Heng Mui Keng Terrace
Singapore 119613
hli@i2r.a-star.edu.sg
Abstract
This paper presents a Function Word cen-
tered, Syntax-based (FWS) solution to ad-
dress phrase ordering in the context of
statistical machine translation (SMT). Mo-
tivated by the observation that function
words often encode grammatical relation-
ship among phrases within a sentence, we
propose a probabilistic synchronous gram-
mar to model the ordering of function words
and their left and right arguments. We im-
prove phrase ordering performance by lexi-
calizing the resulting rules in a small number
of cases corresponding to function words.
The experiments show that the FWS ap-
proach consistently outperforms the base-
line system in ordering function words? ar-
guments and improving translation quality
in both perfect and noisy word alignment
scenarios.
1 Introduction
The focus of this paper is on function words, a class
of words with little intrinsic meaning but is vital in
expressing grammatical relationships among words
within a sentence. Such encoded grammatical infor-
mation, often implicit, makes function words piv-
otal in modeling structural divergences, as project-
ing them in different languages often result in long-
range structural changes to the realized sentences.
Just as a foreign language learner often makes
mistakes in using function words, we observe that
current machine translation (MT) systems often per-
form poorly in ordering function words? arguments;
lexically correct translations often end up reordered
incorrectly. Thus, we are interested in modeling
the structural divergence encoded by such function
words. A key finding of our work is that modeling
the ordering of the dependent arguments of function
words results in better translation quality.
Most current systems use statistical knowledge
obtained from corpora in favor of rich natural lan-
guage knowledge. Instead of using syntactic knowl-
edge to determine function words, we approximate
this by equating the most frequent words as func-
tion words. By explicitly modeling phrase ordering
around these frequent words, we aim to capture the
most important and prevalent ordering productions.
2 Related Work
A good translation should be both faithful with ade-
quate lexical choice to the source language and flu-
ent in its word ordering to the target language. In
pursuit of better translation, phrase-based models
(Och and Ney, 2004) have significantly improved the
quality over classical word-based models (Brown et
al., 1993). These multiword phrasal units contribute
to fluency by inherently capturing intra-phrase re-
ordering. However, despite this progress, inter-
phrase reordering (especially long distance ones)
still poses a great challenge to statistical machine
translation (SMT).
The basic phrase reordering model is a simple
unlexicalized, context-insensitive distortion penalty
model (Koehn et al, 2003). This model assumes
little or no structural divergence between language
pairs, preferring the original, translated order by pe-
nalizing reordering. This simple model works well
when properly coupled with a well-trained language
712
model, but is otherwise impoverished without any
lexical evidence to characterize the reordering.
To address this, lexicalized context-sensitive
models incorporate contextual evidence. The local
prediction model (Tillmann and Zhang, 2005) mod-
els structural divergence as the relative position be-
tween the translation of two neighboring phrases.
Other further generalizations of orientation include
the global prediction model (Nagata et al, 2006) and
distortion model (Al-Onaizan and Papineni, 2006).
However, these models are often fully lexicalized
and sensitive to individual phrases. As a result, they
are not robust to unseen phrases. A careful approx-
imation is vital to avoid data sparseness. Proposals
to alleviate this problem include utilizing bilingual
phrase cluster or words at the phrase boundary (Na-
gata et al, 2006) as the phrase identity.
The benefit of introducing lexical evidence with-
out being fully lexicalized has been demonstrated
by a recent state-of-the-art formally syntax-based
model1, Hiero (Chiang, 2005). Hiero performs
phrase ordering by using linked non-terminal sym-
bols in its synchronous CFG production rules cou-
pled with lexical evidence. However, since it is dif-
ficult to specify a well-defined rule, Hiero has to rely
on weak heuristics (i.e., length-based thresholds) to
extract rules. As a result, Hiero produces grammars
of enormous size. Watanabe et al (2006) further
reduces the grammar?s size by enforcing all rules to
comply with Greibach Normal Form.
Taking the lexicalization an intuitive a step for-
ward, we propose a novel, finer-grained solution
which models the content and context information
encoded by function words - approximated by high
frequency words. Inspired by the success of syntax-
based approaches, we propose a synchronous gram-
mar that accommodates gapping production rules,
while focusing on the statistical modeling in rela-
tion to function words. We refer to our approach
as the Function Word-centered Syntax-based ap-
proach (FWS). Our FWS approach is different from
Hiero in two key aspects. First, we use only a
small set of high frequency lexical items to lexi-
calize non-terminals in the grammar. This results
in a much smaller set of rules compared to Hiero,
1Chiang (2005) used the term ?formal? to indicate the use of
synchronous grammar but without linguistic commitment
,\ 4 ? { j? Q? ? { ?\
a form is a coll. of data entry fields on a page
(((
((((
(((
((((
((













PP
PP
PP
PPP
```
```
```
```
``
Figure 1: A Chinese-English sentence pair.
greatly reducing the computational overhead that
arises when moving from phrase-based to syntax-
based approach. Furthermore, by modeling only
high frequency words, we are able to obtain reliable
statistics even in small datasets. Second, as opposed
to Hiero, where phrase ordering is done implicitly
alongside phrase translation and lexical weighting,
we directly model the reordering process using ori-
entation statistics.
The FWS approach is also akin to (Xiong et al,
2006) in using a synchronous grammar as a reorder-
ing constraint. Instead of using Inversion Transduc-
tion Grammar (ITG) (Wu, 1997) directly, we will
discuss an ITG extension to accommodate gapping.
3 Phrase Ordering around Function
Words
We use the following Chinese (c) to English (e)
translation in Fig.1 as an illustration to conduct an
inquiry to the problem. Note that the sentence trans-
lation requires some translations of English words
to be ordered far from their original position in Chi-
nese. Recovering the correct English ordering re-
quires the inversion of the Chinese postpositional
phrase, followed by the inversion of the first smaller
noun phrase, and finally the inversion of the sec-
ond larger noun phrase. Nevertheless, the correct
ordering can be recovered if the position and the se-
mantic roles of the arguments of the boxed function
words were known. Such a function word centered
approach also hinges on knowing the correct phrase
boundaries for the function words? arguments and
which reorderings are given precedence, in case of
conflicts.
We propose modeling these sources of knowl-
edge using a statistical formalism. It includes 1) a
model to capture bilingual orientations of the left
and right arguments of these function words; 2) a
model to approximate correct reordering sequence;
and 3) a model for finding constituent boundaries of
713
the left and right arguments. Assuming that the most
frequent words in a language are function words,
we can apply orientation statistics associated with
these words to reorder their adjacent left and right
neighbors. We follow the notation in (Nagata et
al., 2006) and define the following bilingual ori-
entation values given two neighboring source (Chi-
nese) phrases: Monotone-Adjacent (MA); Reverse-
Adjacent (RA); Monotone-Gap (MG); and Reverse-
Gap (RG). The first clause (monotone, reverse) in-
dicates whether the target language translation order
follows the source order; the second (adjacent, gap)
indicates whether the source phrases are adjacent or
separated by an intervening phrase on the target side.
Table 1 shows the orientation statistics for several
function words. Note that we separate the statistics
for left and right arguments to account for differ-
ences in argument structures: some function words
take a single argument (e.g., prepositions), while
others take two or more (e.g., copulas). To han-
dle other reordering decisions not explicitly encoded
(i.e., lexicalized) in our FWS model, we introduce a
universal token U , to be used as a backoff statistic
when function words are absent.
For example, orientation statistics for 4 (to be)
overwhelmingly suggests that the English transla-
tion of its surrounding phrases is identical to its Chi-
nese ordering. This reflects the fact that the argu-
ments of copulas in both languages are realized in
the same order. The orientation statistics for post-
position ? (on) suggests inversion which captures
the divergence between Chinese postposition to the
English preposition. Similarly, the dominant orien-
tation for particle { (of) suggests the noun-phrase
shift from modified-modifier to modifier-modified,
which is common when translating Chinese noun
phrases to English.
Taking all parts of the model, which we detail
later, together with the knowledge in Table 1, we
demonstrate the steps taken to translate the exam-
ple in Fig. 2. We highlight the function words with
boxed characters and encapsulate content words as
indexed symbols. As shown, orientation statistics
from function words alone are adequate to recover
the English ordering - in practice, content words also
influence the reordering through a language model.
One can think of the FWS approach as a foreign lan-
guage learner with limited knowledge about Chinese
grammar but fairly knowledgable about the role of
Chinese function words.
,\4 ? { j? Q?? { ?\
X1 4 X2 ? { X3 { X4
HHj
? X2
?
9
XXXXXz
X3 { X5
?
)
XXXXXXXz
X4 { X6
?? ?
X1 4 X7
X1 4 X4 { X3 { ? X2
,\ 4 ?\ { j?Q?? { ? 
a form is a coll. of data entry fields on a page
#1
#2
#3 ? ? ? ? ? ? ? ? ?
Figure 2: In Step 1, function words (boxed char-
acters) and content words (indexed symbols) are
identified. Step 2 reorders phrases according to
knowledge embedded in function words. A new in-
dexed symbol is introduced to indicate previously
reordered phrases for conciseness. Step 3 finally
maps Chinese phrases to their English translation.
4 The FWS Model
We first discuss the extension of standard ITG to
accommodate gapping and then detail the statistical
components of the model later.
4.1 Single Gap ITG (SG-ITG)
The FWS model employs a synchronous grammar
to describe the admissible orderings.
The utility of ITG as a reordering constraint for
most language pairs, is well-known both empirically
(Zens and Ney, 2003) and analytically (Wu, 1997),
however ITG?s straight (monotone) and inverted (re-
verse) rules exhibit strong cohesiveness, which is in-
adequate to express orientations that require gaps.
We propose SG-ITG that follows Wellington et al
(2006)?s suggestion to model at most one gap.
We show the rules for SG-ITG below. Rules 1-
3 are identical to those defined in standard ITG, in
which monotone and reverse orderings are repre-
sented by square and angle brackets, respectively.
714
Rank Word unigram MAL RAL MGL RGL MAR RAR MGR RGR
1 { 0.0580 0.45 0.52 0.01 0.02 0.44 0.52 0.01 0.03
2 ? 0.0507 0.85 0.12 0.02 0.01 0.84 0.12 0.02 0.02
3  0.0550 0.99 0.01 0.00 0.00 0.92 0.08 0.00 0.00
4  0.0155 0.87 0.10 0.02 0.00 0.82 0.12 0.05 0.02
5  0.0153 0.84 0.11 0.01 0.04 0.88 0.11 0.01 0.01
6 Z 0.0138 0.95 0.02 0.01 0.01 0.97 0.02 0.01 0.00
7 ? 0.0123 0.73 0.12 0.10 0.04 0.51 0.14 0.14 0.20
8 ,1 0.0114 0.78 0.12 0.03 0.07 0.86 0.05 0.08 0.01
9 ? 0.0099 0.95 0.02 0.02 0.01 0.96 0.01 0.02 0.01
10 R 0.0091 0.87 0.10 0.01 0.02 0.88 0.10 0.01 0.00
21 4 0.0056 0.85 0.11 0.02 0.02 0.85 0.04 0.09 0.02
37 ? 0.0035 0.33 0.65 0.02 0.01 0.31 0.63 0.03 0.03
- U 0.0002 0.76 0.14 0.06 0.05 0.74 0.13 0.07 0.06
Table 1: Orientation statistics and unigram probability of selected frequent Chinese words in the HIT corpus.
Subscripts L/R refers to lexical unit?s orientation with respect to its left/right neighbor. U is the universal
token used in back-off for N = 128. Dominant orientations of each word are in bold.
(1) X ? c/e
(2) X ? [XX] (3) X ? ?XX?
(4) X? [X X] (5) X? ?X X?
(6) X ? [X ?X] (7) X ? ?X ?X?
SG-ITG introduces two new sets of rules: gap-
ping (Rules 4-5) and dovetailing (Rules 6-7) that
deal specifically with gaps. On the RHS of the gap-
ping rules, a diamond symbol () indicates a gap,
while on the LHS, it emits a superscripted symbol
X to indicate a gapped phrase (plain Xs without
superscripts are thus contiguous phrases). Gaps in
X are eventually filled by actual phrases via dove-
tailing (marked with an ? on the RHS).
Fig.3 illustrates gapping and dovetailing rules
using an example where two Chinese adjectival
phrases are translated into a single English subordi-
nate clause. SG-ITG can generate the correct order-
ing by employing gapping followed by dovetailing,
as shown in the following simplified trace:
X1 ? ? 1997{??, V.1  1997 ?
X2 ? ? 1998{??, V.2  1998 ?
X3 ? [X1 ?X2]
? [ 1997{?? Z 1998{??,
V.1  1997 ? V.2  1998 ]
? 1997{??Z1998{??,
V.1 and V.2 that were released in 1997 and 1998
where X1 and X

2 each generate the translation of
their respective Chinese noun phrase using gapping
and X3 generates the English subclause by dovetail-
ing the two gapped phrases together.
Thus far, the grammar is unlexicalized, and does
1997#q{ ?? Z 1998#q{??
V.1 and V.2 that were released in 1997 and 1998.
!!
!!
!!
((((
((((
(((
((
hhhh
hhhh
hhh
hh
PP
PP
PP
P
Figure 3: An example of an alignment that can be
generated only by allowing gaps.
not incorporate any lexical evidence. Now we mod-
ify the grammar to introduce lexicalized function
words to SG-ITG. In practice, we introduce a new
set of lexicalized non-terminal symbols Yi, i ?
{1...N}, to represent the topN most-frequent words
in the vocabulary; the existing unlexicalized X is
now reserved for content words. This difference
does not inherently affect the structure of the gram-
mar, but rather lexicalizes the statistical model.
In this way, although different Yis follow the same
production rules, they are associated with different
statistics. This is reflected in Rules 8-9. Rule 8 emits
the function word; Rule 9 reorders the arguments
around the function word, resembling our orienta-
tion model (see Section 4.2) where a function word
influences the orientation of its left and right argu-
ments. For clarity, we omit notation that denotes
which rules have been applied (monotone, reverse;
gapping, dovetailing).
(8) Yi? c/e (9) X? XYiX
In practice, we replace Rule 9 with its equivalent
2-normal form set of rules (Rules 10-13). Finally,
we introduce rules to handle back-off (Rules 14-16)
and upgrade (Rule 17). These allow SG-ITG to re-
715
vert function words to normal words and vice versa.
(10) R? YiX (11) L? XYi
(12) X? LX (13) X? XR
(14) Yi? X (15) R? X
(16) L? X (17) X? YU
Back-off rules are needed when the grammar has
to reorder two adjacent function words, where one
set of orientation statistics must take precedence
over the other. The example in Fig.1 illustrates such
a case where the orientation of ? (on) and { (of)
compete for influence. In this case, the grammar
chooses to use{ (of) and reverts the function word
? (on) to the unlexicalized form.
The upgrade rule is used for cases where there are
two adjacent phrases, both of which are not function
words. Upgrading allows either phrase to act as a
function word, making use of the universal word?s
orientation statistics to reorder its neighbor.
4.2 Statistical model
We now formulate the FWS model as a statistical
framework. We replace the deterministic rules in our
SG-ITG grammar with probabilistic ones, elevating
it to a stochastic grammar. In particular, we develop
the three sub models (see Section 3) which influence
the choice of production rules for ordering decision.
These models operate on the 2-norm rules, where the
RHS contains one function word and its argument
(except in the case of the phrase boundary model).
We provide the intuition for these models next, but
their actual form will be discussed in the next section
on training.
1) Orientation Model ori(o|H,Yi): This model
captures the preference of a function word Yi to a
particular orientation o ? {MA,RA,MG,RG} in
reordering its H ? {left, right} argument X . The
parameter H determines which set of Yi?s statistics
to use (left or right); the model consults Yi?s left ori-
entation statistic for Rules 11 and 13 where X pre-
cedes Yi, otherwise Yi?s right orientation statistic is
used for Rules 10 and 12.
2) Preference Model pref(Yi): This model ar-
bitrates reordering in the cases where two function
words are adjacent and the backoff rules have to de-
cide which function word takes precedence, revert-
ing the other to the unlexicalized X form. This
model prefers the function word with higher uni-
gram probability to take the precedence.
3) Phrase BoundaryModel pb(X): This model is
a penalty-based model, favoring the resulting align-
ment that conforms to the source constituent bound-
ary. It penalizes Rule 1 if the terminal rule X
emits a Chinese phrase that violates the boundary
(pb = e?1), otherwise it is inactive (pb = 1).
These three sub models act as features alongside
seven other standard SMT features in a log-linear
model, resulting in the following set of features
{f1, . . . , f10}: f1) orientation ori(o|H,Yi); f2)
preference pref(Yi); f3) phrase boundary pb(X);
f4) language model lm(e); f5 ? f6) phrase trans-
lation score ?(e|c) and its inverse ?(c|e); f7 ? f8)
lexical weight lex(e|c) and its inverse lex(c|e); f9)
word penalty wp; and f10) phrase penalty pp.
The translation is then obtained from the most
probable derivation of the stochastic SG-ITG. The
formula for a single derivation is shown in Eq. (18),
where X1, X2, ..., XL is a sequence of rules with
w(Xl) being the weight of each particular rule Xl.
w(Xl) is estimated through a log-linear model, as
in Eq. (19), with all the abovementioned features
where ?j reflects the contribution of each feature fj .
P (X1, ..., XL) =
?L
l=1
w(Xl)(18)
w(Xl) =
?10
j=1
fj(Xl)
?j(19)
5 Training
We train the orientation and preference models from
statistics of a training corpus. To this end, we first
derive the event counts and then compute the rela-
tive frequency of each event. The remaining phrase
boundary model can be modeled by the output of a
standard text chunker, as in practice it is simply a
constituent boundary detection mechanism together
with a penalty scheme.
The events of interest to the orientation model are
(Yi, o) tuples, where o ? {MA,RA,MG,RG} is
an orientation value of a particular function word
Yi. Note that these tuples are not directly observable
from training data. Hence, we need an algorithm to
derive (Yi, o) tuples from a parallel corpus. Since
both left and right statistics share identical training
steps, thus we omit references to them.
The algorithm to derive (Yi, o) involves several
steps. First, we estimate the bi-directional alignment
716
by running GIZA++ and applying the ?grow-diag-
final? heuristic. Then, the algorithm enumerates all
Yi and determines its orientation o with respect to
its argument X to derive (Yi, o). To determine o,
the algorithm inspects the monotonicity (monotone
or reverse) and adjacency (adjacent or gap) between
Yi?s and X?s alignments.
Monotonicity can be determined by looking at the
Yi?s alignment with respect to the most fine-grained
level of X (i.e., word level alignment). However,
such a heuristic may inaccurately suggest gap ori-
entation. Figure 1 illustrates this problem when de-
riving the orientation for the second{ (of). Look-
ing only at the word alignment of its left argument
? (fields) incorrectly suggests a gapped orientation,
where the alignment of j?Q? (data entry) in-
tervened. It is desirable to look at the alignment of
j?Q?? (data entry fields) at the phrase level,
which suggests the correct adjacent orientation in-
stead.
To address this issue, the algorithm uses gap-
ping conservatively by utilizing the consistency con-
straint (Och and Ney, 2004) to suggest phrase level
alignment of X . The algorithm exhaustively grows
consistent blocks containing the most fine-grained
level of X not including Yi. Subsequently, it merges
each hypothetical argument with the Yi?s alignment.
The algorithm decides that Yi has a gapped orienta-
tion only if all merged blocks violate the consistency
constraint, concluding an adjacent orientation other-
wise.
With the event countsC(Yi, o) of tuple (Yi, o), we
estimate the orientation model for Yi and U using
Eqs. (20) and (21). We also estimate the prefer-
ence model with word unigram counts C(Yi) using
Eqs. (22) and (23), where V indicates the vocabu-
lary size.
ori(o|Yi) = C(Yi, o)/C(Yi, ?), i 6 N(20)
ori(o|U) =
?
i>N
C(Yi, o)/
?
i>N
C(Yi, ?)(21)
pref(Yi) = C(Yi)/C(?), i 6 N(22)
pref(U) = 1/(V ?N)
?
i>N
C(Yi)/C(?)(23)
Samples of these statistics are found in Table 1
and have been used in the running examples. For
instance, the statistic ori(RAL|{) = 0.52, which
is the dominant one, suggests that the grammar in-
versely order {(of)?s left argument; while in our
illustration of backoff rules in Fig.1, the grammar
chooses{(of) to take precedence since pref({) >
pref(?).
6 Decoding
We employ a bottom-up CKY parser with a beam
to find the derivation of a Chinese sentence which
maximizes Eq. (18). The English translation is then
obtained by post-processing the best parse.
We set the beam size to 30 in our experiment and
further constrain reordering to occur within a win-
dow of 10 words. Our decoder also prunes entries
that violate the following constraints: 1) each entry
contains at most one gap; 2) any gapped entries must
be dovetailed at the next level higher; 3) an entry
spanning the whole sentence must not contain gaps.
The score of each newly-created entry is derived
from the scores of its parts accordingly. When scor-
ing entries, we treat gapped entries as contiguous
phrases by ignoring the gap symbol and rely on the
orientation model to penalize such entries. This al-
lows a fair score comparison between gapped and
contiguous entries.
7 Experiments
We would like to study how the FWS model affects
1) the ordering of phrases around function words; 2)
the overall translation quality. We achieve this by
evaluating the FWS model against a baseline system
using two metrics, namely, orientation accuracy and
BLEU respectively.
We define the orientation accuracy of a (function)
word as the accuracy of assigning correct orientation
values to both its left and right arguments. We report
the aggregate for the top 1024 most frequent words;
these words cover 90% of the test set.
We devise a series of experiments and run it in two
scenarios - manual and automatic alignment - to as-
sess the effects of using perfect or real-world input.
We utilize the HIT bilingual computer manual cor-
pus, which has been manually aligned, to perform
Chinese-to-English translation (see Table 2). Man-
ual alignment is essential as we need to measure ori-
entation accuracy with respect to a gold standard.
717
Chinese English
train words 145,731 135,032
(7K sentences) vocabulary 5,267 8,064
dev words 13,986 14,638
(1K sentences) untranslatable 486 (3.47%)
test words 27,732 28,490
(2K sentences) untranslatable 935 (3.37%)
Table 2: Statistics for the HIT corpus.
A language model is trained using the SRILM-
Toolkit, and a text chunker (Chen et al, 2006) is ap-
plied to the Chinese sentences in the test and dev
sets to extract the constituent boundaries necessary
for the phrase boundary model. We run minimum er-
ror rate training on dev set using Chiang?s toolkit to
find a set of parameters that optimizes BLEU score.
7.1 Perfect Lexical Choice
Here, the task is simplified to recovering the correct
order of the English sentence from the scrambled
Chinese order. We trained the orientation model us-
ing manual alignment as input. The aforementioned
decoder is used with phrase translation, lexical map-
ping and penalty features turned off.
Table 4 compares orientation accuracy and BLEU
between our FWS model and the baseline. The
baseline (lm+d) employs a language model and
distortion penalty features, emulating the standard
Pharaoh model. We study the behavior of the
FWS model with different numbers of lexicalized
items N . We start with the language model alone
(N=0) and incrementally add the orientation (+ori),
preference (+ori+pref) and phrase boundary models
(+ori+pref+pb).
As shown, the language model alone is rela-
tively weak, assigning the correct orientation in only
62.28% of the cases. A closer inspection reveals that
the lm component aggressively promotes reverse re-
orderings. Including a distortion penalty model (the
baseline) improves the accuracy to 72.55%. This
trend is also apparent for the BLEU score.
When we incorporate the FSW model, including
just the most frequent word (Y1={), we see im-
provement. This model promotes non-monotone re-
ordering conservatively around Y1 (where the dom-
inant statistic suggests reverse ordering). Increasing
the value of N leads to greater improvement. The
most effective improvement is obtained by increas-
pharaoh (dl=5) 22.44 ? 0.94
+ori 23.80 ? 0.98
+ori+pref 23.85 ? 1.00
+ori+pref+pb 23.86 ? 1.08
Table 3: BLEU score with the 95% confidence in-
tervals based on (Zhang and Vogel, 2004). All im-
provement over the baseline (row 1) are statistically
significant under paired bootstrap resampling.
ing N to 128. Additional (marginal) improvement
is obtained at the expense of modeling an additional
900+ lexical items. We see these results as validat-
ing our claim that modeling the top few most fre-
quent words captures most important and prevalent
ordering productions.
Lastly, we study the effect of the pref and pb fea-
tures. The inclusion of both sub models has little af-
fect on orientation accuracy, but it improves BLEU
consistently (although not significantly). This sug-
gests that both models correct the mistakes made by
the ori model while preserving the gain. They are
not as effective as the addition of the basic orienta-
tion model as they only play a role when two lexi-
calized entries are adjacent.
7.2 Full SMT experiments
Here, all knowledge is automatically trained on the
train set, and as a result, the input word alignment
is noisy. As a baseline, we use the state-of-the-art
phrase-based Pharaoh decoder. For a fair compari-
son, we run minimum error rate training for different
distortion limits from 0 to 10 and report the best pa-
rameter (dl=5) as the baseline.
We use the phrase translation table from the base-
line and perform an identical set of experiments as
the perfect lexical choice scenario, except that we
only report the result for N=128, due to space con-
straint. Table 3 reports the resulting BLEU scores.
As shown, the FWS model improves BLEU score
significantly over the baseline. We observe the same
trend as the one in perfect lexical choice scenario
where top 128 most frequent words provides the ma-
jority of improvement. However, the pb features
yields no noticeable improvement unlike in prefect
lexical choice scenario; this is similar to the findings
in (Koehn et al, 2003).
718
N=0 N=1 N=4 N=16 N=64 N=128 N=256 N=1024
O
ri
en
ta
ti
on
A
cc
.
(%
)
lm+d 72.55
+ori 62.28 76.52 76.58 77.38 77.54 78.17 77.76 78.38
+ori+pref 76.66 76.82 77.57 77.74 78.13 77.94 78.54
+ori+pref+pb 76.70 76.85 77.58 77.70 78.20 77.94 78.56
B
L
E
U
lm+d 75.13
+ori 66.54 77.54 77.57 78.22 78.48 78.76 78.58 79.20
+ori+pref 77.60 77.70 78.29 78.65 78.77 78.70 79.30
+ori+pref+pb 77.69 77.80 78.34 78.65 78.93 78.79 79.30
Table 4: Results using perfect aligned input. Here, (lm+d) is the baseline; (+ori), (+ori+pref) and
(+ori+pref+pb) are different FWS configurations. The results of the model (where N is varied) that fea-
tures the largest gain are bold, whereas the highest score is italicized.
8 Conclusion
In this paper, we present a statistical model to cap-
ture the grammatical information encoded in func-
tion words. Formally, we develop the FunctionWord
Syntax-based (FWS) model, a probabilistic syn-
chronous grammar, to encode the orientation statis-
tics of arguments to function words. Our experimen-
tal results shows that the FWS model significantly
improves the state-of-the-art phrase-based model.
We have touched only the surface benefits of mod-
eling function words. In particular, our proposal is
limited to modeling function words in the source
language. We believe that conditioning on both
source and target pair would result in more fine-
grained, accurate orientation statistics.
From our error analysis, we observe that 1) re-
ordering may span several levels and the preference
model does not handle this phenomena well; 2) cor-
rectly reordered phrases with incorrect boundaries
severely affects BLEU score and the phrase bound-
ary model is inadequate to correct the boundaries es-
pecially for cases of long phrase. In future, we hope
to address these issues while maintaining the bene-
fits offered by modeling function words.
References
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical Lower Bounds on
the Complexity of Translational Equivalence. In
ACL/COLING 2006, pp. 977?984.
Christoph Tillman and Tong Zhang. 2005. A Localized
Prediction Model for Statistical Machine Translation.
In ACL 2005, pp. 557?564.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In ACL
2005, pp. 263?270.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase ReorderingModel for Sta-
tistical Machine Translation. In ACL/COLING 2006,
pp. 521?528.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Translation.
Computational Linguistics, 30(4):417?449.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A Clustered Global
Phrase Reordering Model for Statistical Machine
Translation. In ACL/COLING 2006, pp. 713?720.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, Robert L. Mercer 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In HLT-NAACL
2003, pp. 127?133.
Richard Zens and Hermann Ney. 2003. A Compara-
tive Study on Reordering Constraints in Statistical Ma-
chine Translation. In ACL 2003.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-Right Target Generation for Hierarchi-
cal Phrase-Based Translation. In ACL/COLING 2006,
pp. 777?784.
Wenliang Chen, Yujie Zhang and Hitoshi Isahara 2006.
An Empirical Study of Chinese Chunking In ACL
2006 Poster Sessions, pp. 97?104.
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion Models for Statistical Machine Translation. In
ACL/COLING 2006, pp. 529?536.
Ying Zhang and Stephan Vogel. 2004. Measuring Confi-
dence Intervals for theMachine Translation Evaluation
Metrics. In TMI 2004.
719
Proceedings of ACL-08: HLT, pages 559?567,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Tree Sequence Alignment-based Tree-to-Tree Translation Model 
 
 
Min Zhang1  Hongfei Jiang2  Aiti Aw1  Haizhou Li1  Chew Lim Tan3 and Sheng Li2
1Institute for Infocomm Research 2Harbin Institute of Technology 3National University of Singapore
mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg 
aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn  
hli@i2r.a-star.edu.sg   
 
  
Abstract 
This paper presents a translation model that is 
based on tree sequence alignment, where a tree 
sequence refers to a single sequence of sub-
trees that covers a phrase. The model leverages 
on the strengths of both phrase-based and lin-
guistically syntax-based method. It automati-
cally learns aligned tree sequence pairs with 
mapping probabilities from word-aligned bi-
parsed parallel texts. Compared with previous 
models, it not only captures non-syntactic 
phrases and discontinuous phrases with lin-
guistically structured features, but also sup-
ports multi-level structure reordering of tree 
typology with larger span. This gives our 
model stronger expressive power than other re-
ported models. Experimental results on the 
NIST MT-2005 Chinese-English translation 
task show that our method statistically signifi-
cantly outperforms the baseline systems.  
1 Introduction 
Phrase-based modeling method (Koehn et al, 
2003; Och and Ney, 2004a) is a simple, but power-
ful mechanism to machine translation since it can 
model local reorderings and translations of multi-
word expressions well. However, it cannot handle 
long-distance reorderings properly and does not 
exploit discontinuous phrases and linguistically 
syntactic structure features (Quirk and Menezes, 
2006). Recently, many syntax-based models have 
been proposed to address the above deficiencies 
(Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and 
Palmer, 2005; Quirk et al 2005; Cowan et al, 
2006; Zhang et al, 2007; Bod, 2007; Yamada and 
Knight, 2001; Liu et al, 2006; Liu et al, 2007; 
Gildea, 2003; Poutsma, 2000; Hearne and Way, 
2003). Although good progress has been reported, 
the fundamental issues in applying linguistic syn-
tax to SMT, such as non-isomorphic tree align-
ment, structure reordering and non-syntactic phrase 
modeling, are still worth well studying. 
In this paper, we propose a tree-to-tree transla-
tion model that is based on tree sequence align-
ment. It is designed to combine the strengths of 
phrase-based and syntax-based methods. The pro-
posed model adopts tree sequence 1  as the basic 
translation unit and utilizes tree sequence align-
ments to model the translation process. Therefore, 
it not only describes non-syntactic phrases with 
syntactic structure information, but also supports 
multi-level tree structure reordering in larger span. 
These give our model much more expressive 
power and flexibility than those previous models. 
Experiment results on the NIST MT-2005 Chinese-
English translation task show that our method sig-
nificantly outperforms Moses (Koehn et al, 2007), 
a state-of-the-art phrase-based SMT system, and 
other linguistically syntax-based methods, such as 
SCFG-based and STSG-based methods (Zhang et 
al., 2007). In addition, our study further demon-
strates that 1) structure reordering rules in our 
model are very useful for performance improve-
ment while discontinuous phrase rules have less 
contribution and 2) tree sequence rules are able to 
model non-syntactic phrases with syntactic struc-
ture information, and thus contribute much to the 
performance improvement, but those rules consist-
ing of more than three sub-trees have almost no 
contribution.  
The rest of this paper is organized as follows: 
Section 2 reviews previous work. Section 3 elabo-
                                                          
1 A tree sequence refers to an ordered sub-tree sequence that 
covers a phrase or a consecutive tree fragment in a parse tree. 
It is the same as the concept ?forest? used in Liu et al(2007).  
559
rates the modelling process while Sections 4 and 5 
discuss the training and decoding algorithms. The 
experimental results are reported in Section 6. Fi-
nally, we conclude our work in Section 7. 
2 Related Work 
Many techniques on linguistically syntax-based 
SMT have been proposed in literature. Yamada 
and Knight (2001) use noisy-channel model to 
transfer a target parse tree into a source sentence. 
Eisner (2003) studies how to learn non-isomorphic 
tree-to-tree/string mappings using a STSG. Ding 
and Palmer (2005) propose a syntax-based transla-
tion model based on a probabilistic synchronous 
dependency insertion grammar. Quirk et al (2005) 
propose a dependency treelet-based translation 
model. Cowan et al (2006) propose a feature-
based discriminative model for target language 
syntactic structures prediction, given a source 
parse tree. Huang et al (2006) study a TSG-based 
tree-to-string alignment model. Liu et al (2006) 
propose a tree-to-string model. Zhang et al 
(2007b) present a STSG-based tree-to-tree transla-
tion model. Bod (2007) reports that the unsuper-
vised STSG-based translation model performs 
much better than the supervised one. The motiva-
tion behind all these work is to exploit linguistical-
ly syntactic structure features to model the 
translation process. However, most of them fail to 
utilize non-syntactic phrases well that are proven 
useful in the phrase-based methods (Koehn et al, 
2003). 
The formally syntax-based model for SMT was 
first advocated by Wu (1997). Xiong et al (2006) 
propose a MaxEnt-based reordering model for 
BTG (Wu, 1997) while Setiawan et al (2007) pro-
pose a function word-based reordering model for 
BTG. Chiang (2005)?s hierarchal phrase-based 
model achieves significant performance improve-
ment. However, no further significant improve-
ment is achieved when the model is made sensitive 
to syntactic structures by adding a constituent fea-
ture (Chiang, 2005). 
In the last two years, many research efforts were 
devoted to integrating the strengths of phrase-
based and syntax-based methods. In the following, 
we review four representatives of them.   
1) Hassan et al (2007) integrate supertags (a 
kind of lexicalized syntactic description) into the 
target side of translation model and language mod-
el under the phrase-based translation framework, 
resulting in good performance improvement. How-
ever, neither source side syntactic knowledge nor 
reordering model is further explored.  
2) Galley et al (2006) handle non-syntactic 
phrasal translations by traversing the tree upwards 
until a node that subsumes the phrase is reached. 
This solution requires larger applicability contexts 
(Marcu et al, 2006). However, phrases are utilized 
independently in the phrase-based method without 
depending on any contexts.  
3) Addressing the issues in Galley et al (2006), 
Marcu et al (2006) create an xRS rule headed by a 
pseudo, non-syntactic non-terminal symbol that 
subsumes the phrase and its corresponding multi-
headed syntactic structure; and one sibling xRS 
rule that explains how the pseudo symbol can be 
combined with other genuine non-terminals for 
acquiring the genuine parse trees. The name of the 
pseudo non-terminal is designed to reflect the full 
realization of the corresponding rule. The problem 
in this method is that it neglects alignment consis-
tency in creating sibling rules and the naming me-
chanism faces challenges in describing more 
complicated phenomena (Liu et al, 2007).  
4) Liu et al (2006) treat all bilingual phrases as 
lexicalized tree-to-string rules, including those 
non-syntactic phrases in training corpus. Although 
the solution shows effective empirically, it only 
utilizes the source side syntactic phrases of the in-
put parse tree during decoding. Furthermore, the 
translation probabilities of the bilingual phrases 
and other tree-to-string rules are not compatible 
since they are estimated independently, thus hav-
ing different parameter spaces. To address the 
above problems, Liu et al (2007) propose to use 
forest-to-string rules to enhance the expressive 
power of their tree-to-string model. As is inherent 
in a tree-to-string framework, Liu et al?s method 
defines a kind of auxiliary rules to integrate forest-
to-string rules into tree-to-string models. One prob-
lem of this method is that the auxiliary rules are 
not described by probabilities since they are con-
structed during decoding, rather than learned from 
the training corpus. So, to balance the usage of dif-
ferent kinds of rules, they use a very simple feature 
counting the number of auxiliary rules used in a 
derivation for penalizing the use of forest-to-string 
and auxiliary rules. 
In this paper, an alternative solution is presented 
to combine the strengths of phrase-based and syn-
560
1( )
IT e
1( )
JT f
A
 
 
Figure 1: A word-aligned parse tree pairs of a Chi-
nese sentence and its English translation  
 
 
 
Figure 2: Two Examples of tree sequences 
 
 
 
Figure 3: Two examples of translation rules 
tax-based methods. Unlike previous work, our so-
lution neither requires larger applicability contexts 
(Galley et al, 2006), nor depends on pseudo nodes 
(Marcu et al, 2006) or auxiliary rules (Liu et al, 
2007). We go beyond the single sub-tree mapping 
model to propose a tree sequence alignment-based 
translation model. To the best of our knowledge, 
this is the first attempt to empirically explore the 
tree sequence alignment based model in SMT.  
3 Tree Sequence Alignment Model 
3.1 Tree Sequence Translation Rule   
The leaf nodes of a sub-tree in a tree sequence can 
be either non-terminal symbols (grammar tags) or 
terminal symbols (lexical words). Given a pair of 
source and target parse trees 1( )
JT f and 1( )
IT e  in 
Fig. 1, Fig. 2 illustrates two examples of tree se-
quences derived from the two parse trees. A tree 
sequence translation rule r  is a pair of aligned tree 
sequences r =< 2
1
( )jjTS f , 21( )
i
iTS e , A%  >, where: 
z 2
1
( )jjTS f is a source tree sequence, covering 
the span [ 1 2,j j ] in 1( )
JT f , and 
z 2
1
( )iiTS e is a target one, covering the span 
[ 1 2,i i ] in 1( )
IT e , and 
z A% are the alignments between leaf nodes of 
two tree sequences, satisfying the following 
condition: 1 2 1 2( , ) :i j A i i i j j j? ? ? ? ? ? ?% . 
Fig. 3 shows two rules extracted from the tree pair 
shown in Fig. 1, where r1 is a tree-to-tree rule and 
r2 is a tree sequence-to-tree sequence rule. Ob-
viously, tree sequence rules are more powerful 
than phrases or tree rules as they can capture all 
phrases (including both syntactic and non-syntactic 
phrases) with syntactic structure information and 
allow any tree node operations in a longer span. 
We expect that these properties can well address 
the issues of non-isomorphic structure alignments, 
structure reordering, non-syntactic phrases and 
discontinuous phrases translations. 
3.2 Tree Sequence Translation Model 
Given the source and target sentences 1
Jf and 1
Ie  
and their parse trees 1( )
JT f and 1( )
IT e , the tree 
sequence-to-tree sequence translation model is 
formulated as: 
1 1
1 1
1 1 1 1 1 1
( ), ( )
1 1
( ), ( )
1 1 1
1 1 1 1
( | ) ( , ( ), ( ) | )
( ( ( ) | )
( ( ) | ( ), )
( | ( ), ( ), ))
                
                      
                      
J I
J I
I J I I J J
T f T e
J J
T f T e
I J J
I I J J
r r
r
r
r
P e f P e T e T f f
P T f f
P T e T f f
P e T e T f f
=
=
?
?
?
? (1) 
In our implementation, we have: 
561
1) 1 1( ( ) | ) 1
J JrP T f f ? since we only use the best 
source and target parse tree pairs in training. 
2) 1 1 1 1( | ( ), ( ), ) 1
I I J JrP e T e T f f ? since we just 
output the leaf nodes of 1( )
IT e to generate 1
Ie  
regardless of source side information. 
Since 1( )
JT f contains the information of 1
Jf , 
now we have: 
1 1 1 1 1
1 1
( | ) ( ( ) | ( ), )
                 ( ( ) | ( ))
I J I J J
I J
r r
r
P e f P T e T f f
P T e T f
=
=
           (2) 
By Eq. (2), translation becomes a tree structure 
mapping issue. We model it using our tree se-
quence-based translation rules. Given the source 
parse tree 1( )
JT f , there are multiple derivations 
that could lead to the same target tree 1( )
IT e , the 
mapping probability 1 1( ( ) | ( ))
I JrP T e T f is obtained 
by summing over the probabilities of all deriva-
tions. The probability of each derivation? is given 
as the product of the probabilities of all the rules 
( )ip r  used in the derivation (here we assume that 
a rule is applied independently in a derivation). 
2 2
1 1
1 1 1 1( | ) ( ( ) | ( ))
     = ( : ( ), ( ), )
i
I J I J
i j
i i j
r
r rP e f P T e T f
p r TS e TS f A
? ??
=
< >?? %    (3) 
Eq. (3) formulates the tree sequence alignment-
based translation model. Figs. 1 and 3 show how 
the proposed model works. First, the source sen-
tence is parsed into a source parse tree. Next, the 
source parse tree is detached into two source tree 
sequences (the left hand side of rules in Fig. 3). 
Then the two rules in Fig. 3 are used to map the 
two source tree sequences to two target tree se-
quences, which are then combined to generate a 
target parse tree. Finally, a target translation is 
yielded from the target tree.  
Our model is implemented under log-linear 
framework (Och and Ney, 2002). We use seven 
basic features that are analogous to the commonly 
used features in phrase-based systems (Koehn, 
2004): 1) bidirectional rule mapping probabilities; 
2) bidirectional lexical rule translation probabilities; 
3) the target language model; 4) the number of 
rules used and 5) the number of target words. In 
addition, we define two new features: 1) the num-
ber of lexical words in a rule to control the model?s 
preference for lexicalized rules over un-lexicalized 
rules and 2) the average tree depth in a rule to bal-
ance the usage of hierarchical rules and flat rules. 
Note that we do not distinguish between larger (tal-
ler) and shorter source side tree sequences, i.e. we 
let these rules compete directly with each other. 
4 Rule Extraction 
Rules are extracted from word-aligned, bi-parsed 
sentence pairs 1 1( ), ( ),
J IT f T e A< > , which are 
classified into two categories: 
z initial rule, if all leaf nodes of the rule are 
terminals (i.e. lexical word), and 
z abstract rule, otherwise, i.e. at least one leaf 
node is a non-terminal (POS or phrase tag). 
Given an initial rule 2 2
1 1
( ), ( ),j ij iTS f TS e A< >% , 
its sub initial rule is defined as a triple 
4 4
3 3
?( ), ( ),j ij iTS f TS e A< >  if and only if: 
z 4 4
3 3
?( ), ( ),j ij iTS f TS e A< > is an initial rule. 
z 3 4 3 4( , ) :i j A i i i j j j? ? ? ? ? ? ?% , i.e. 
A? A? %  
z 4
3
( )jjTS f is a sub-graph of 21( )
j
jTS f while  
4
3
( )iiTS e  is a sub-graph of 21( )
i
iTS e . 
Rules are extracted in two steps: 
1) Extracting initial rules first. 
2) Extracting abstract rules from extracted ini-
tial rules with the help of sub initial rules. 
It is straightforward to extract initial rules. We 
first generate all fully lexicalized source and target 
tree sequences using a dynamic programming algo-
rithm and then iterate over all generated source and 
target tree sequence pairs 2 2
1 1
( ), ( )j ij iTS f TS e< > . If 
the condition ? ( , )i j? 1 2 1 2:A i i i j j j? ? ? ? ? ? ? 
is satisfied, the triple 2 2
1 1
( ), ( ),j ij iTS f TS e A< >% is 
an initial rule, where A%  are alignments between 
leaf nodes of 2
1
( )jjTS f  and 21( )
i
iTS e . We then de-
rive abstract rules from initial rules by removing 
one or more of its sub initial rules. The abstract 
rule extraction algorithm presented next is imple-
mented using dynamic programming. Due to space 
limitation, we skip the details here. In order to con-
trol the number of rules, we set three constraints 
for both finally extracted initial and abstract rules:  
1) The depth of a tree in a rule is not greater 
562
than h . 
2) The number of non-terminals as leaf nodes is 
not greater than c . 
3) The tree number in a rule is not greater than d. 
In addition, we limit initial rules to have at most 
seven lexical words as leaf nodes on either side. 
However, in order to extract long-distance reorder-
ing rules, we also generate those initial rules with 
more than seven lexical words for abstract rules 
extraction only (not used in decoding). This makes 
our abstract rules more powerful in handling 
global structure reordering. Moreover, by configur-
ing these parameters we can implement other 
translation models easily: 1) STSG-based model  
when 1d = ; 2) SCFG-based model when 1d =  
and 2h = ; 3) phrase-based translation model only 
(no reordering model) when 0c =  and 1h = . 
 
Algorithm 1: abstract rules extraction 
Input: initial rule set inir  
Output: abstract rule set absr  
1: for each i inir r? , do 
2:    put all sub initial rules of ir  into a set subiniir
3:    for each subset subiniir? ? do 
4:          if there are spans overlapping between 
any two rules in the subset ?  then 
5:                    continue   //go to line 3 
6:           end if  
7:           generate an abstract rule by removing 
the portions covered by ?  from ir  and 
co-indexing the pairs of non-terminals 
that rooting the removed source and 
target parts 
8:           add them into the abstract rule set absr  
9:     end do 
10: end do  
 
5 Decoding 
Given 1( )
JT f , the decoder is to find the best deri-
vation ?  that generates < 1( )JT f , 1( )IT e >.  
1
1
1 1
,
? arg max ( ( ) | ( ))
  arg max ( )
I
I
i
I J
e
i
e r
re P T e T f
p r
? ??
=
? ?              (4) 
Algorithm 2: Tree Sequence-based Decoder 
 Input: 1( )
JT f   Output: 1( )
IT e  
 Data structures: 
1 2[ , ]h j j    To store translations to a span 1 2[ , ]j j  
1: for s = 0 to J -1 do      // s: span length 
2:     for 1j = 1 to J - s , 2j = 1j + s  do  
3:          for each rule r spanning 1 2[ , ]j j  do  
4:               if r  is an initial rule then 
5:                    insert r into 1 2[ , ]h j j  
6:               else 
7:      generate new translations from 
r by replacing non-terminal leaf 
nodes of r with their correspond-
ing spans? translations that are al-
ready translated in previous steps 
8:      insert them into 1 2[ , ]h j j  
9:  end if 
10: end for 
11: end for 
12: end for 
13: output the hypothesis with the highest score  
in [1, ]h J  as the final best translation 
 
The decoder is a span-based beam search to-
gether with a function for mapping the source deri-
vations to the target ones. Algorithm 2 illustrates 
the decoding algorithm. It translates each span ite-
ratively from small one to large one (lines 1-2).  
This strategy can guarantee that when translating 
the current span, all spans smaller than the current 
one have already been translated before if they are 
translatable (line 7). When translating a span, if the 
usable rule is an initial rule, then the tree sequence 
on the target side of the rule is a candidate transla-
tion (lines 4-5). Otherwise, we replace the non-
terminal leaf nodes of the current abstract rule 
with their corresponding spans? translations that 
are already translated in previous steps (line 7). To 
speed up the decoder, we use several thresholds to 
limit search beams for each span:  
1) ? , the maximal number of rules used 
2) ? , the minimal log probability of rules 
3) ? , the maximal number of translations yield  
It is worth noting that the decoder does not force 
a complete target parse tree to be generated. If no 
rules can be used to generate a complete target 
parse tree, the decoder just outputs whatever have 
563
been translated so far monotonically as one hy-
pothesis. 
6 Experiments 
6.1 Experimental Settings 
We conducted Chinese-to-English translation ex-
periments. We trained the translation model on the 
FBIS corpus (7.2M+9.2M words) and trained a 4-
gram language model on the Xinhua portion of the 
English Gigaword corpus (181M words) using the 
SRILM Toolkits (Stolcke, 2002) with modified 
Kneser-Ney smoothing. We used sentences with 
less than 50 characters from the NIST MT-2002 
test set as our development set and the NIST MT-
2005 test set as our test set. We used the Stanford 
parser (Klein and Manning, 2003) to parse bilin-
gual sentences on the training set and Chinese sen-
tences on the development and test sets. The 
evaluation metric is case-sensitive BLEU-4 (Papi-
neni et al, 2002). We used GIZA++ (Och and Ney, 
2004) and the heuristics ?grow-diag-final? to gen-
erate m-to-n word alignments. For the MER train-
ing (Och, 2003), we modified Koehn?s MER 
trainer (Koehn, 2004) for our tree sequence-based 
system. For significance test, we used Zhang et als 
implementation (Zhang et al 2004). 
We set three baseline systems: Moses (Koehn et 
al., 2007), and SCFG-based and STSG-based tree-
to-tree translation models (Zhang et al, 2007). For 
Moses, we used its default settings. For the 
SCFG/STSG and our proposed model, we used the 
same settings except for the parameters d and h  
( 1d = and 2h = for the SCFG; 1d = and 6h = for 
the STSG; 4d =  and 6h = for our model). We 
optimized these parameters on the training and de-
velopment sets: c =3, ? =20, ? =-100 and ? =100. 
6.2 Experimental Results   
We carried out a number of experiments to ex-
amine the proposed tree sequence alignment-based 
translation model. In this subsection, we first re-
port the rule distributions and compare our model 
with the three baseline systems. Then we study the 
model?s expressive ability by comparing the con-
tributions made by different kinds of rules, includ-
ing strict tree sequence rules, non-syntactic phrase 
rules, structure reordering rules and discontinuous 
phrase rules2. Finally, we investigate the impact of 
maximal sub-tree number and sub-tree depth in our 
model. All of the following discussions are held on 
the training and test data. 
 
 
Rule 
 Initial Rules  Abstract Rules  
L P U Total 
BP 322,965 0 0  322,965
TR 443,010 144,459 24,871  612,340
TSR 225,570 103,932 714  330,216
 
Table 1: # of rules used in the testing ( 4d = , h =  6) 
(BP: bilingual phrase (used in Moses), TR: tree rule (on-
ly 1 tree), TSR: tree sequence rule (> 1 tree), L: fully 
lexicalized, P: partially lexicalized, U: unlexicalized) 
 
Table 1 reports the statistics of rules used in the 
experiments. It shows that:  
1) We verify that the BPs are fully covered by 
the initial rules (i.e. lexicalized rules), in which the 
lexicalized TSRs model all non-syntactic phrase 
pairs with rich syntactic information. In addition, 
we find that the number of initial rules is greater 
than that of bilingual phrases. This is because one 
bilingual phrase can be covered by more than one 
initial rule which having different sub-tree struc-
tures. 
2) Abstract rules generalize initial rules to un-
seen data and with structure reordering ability. The 
number of the abstract rule is far less than that of 
the initial rules. This is because leaf nodes of an 
abstract rule can be non-terminals that can 
represent any sub-trees using the non-terminals as 
roots.   
Fig. 4 compares the performance of different 
models. It illustrates that: 
1) Our tree sequence-based model significantly 
outperforms (p < 0.01) previous phrase-based and 
linguistically syntax-based methods. This empirical-
ly verifies the effect of the proposed method. 
2) Both our method and STSG outperform Mos-
es significantly. Our method also clearly outper-
forms STSG. These results suggest that: 
z The linguistically motivated structure features 
are very useful for SMT, which can be cap-
                                                          
2 To be precise, we examine the contributions of strict tree 
sequence rules and single tree rules separately in this section. 
Therefore, unless specified, the term ?tree sequence rules? 
used in this section only refers to the strict tree sequence rules, 
which must contain at least two sub-trees on the source side. 
564
tured by the two syntax-based models through 
tree node operations. 
z Our model is much more effective in utilizing 
linguistic structures than STSG since it uses 
tree sequence as basic translation unit. This 
allows our model not only to handle structure 
reordering by tree node operations in a larger 
span, but also to capture non-syntactic phras-
es, which circumvents previous syntactic 
constraints, thus giving our model more ex-
pressive power. 
3) The linguistically motivated SCFG shows 
much lower performance. This is largely because 
SCFG only allows sibling nodes reordering and fails 
to utilize both non-syntactic phrases and those syn-
tactic phrases that cannot be covered by a single 
CFG rule. It thereby suggests that SCFG is less 
effective in modelling parse tree structure transfer 
between Chinese and English when using Penn 
Treebank style linguistic grammar and under word-
alignment constraints. However, formal SCFG 
show much better performance in the formally syn-
tax-based translation framework (Chiang, 2005). 
This is because the formal syntax is learned from 
phrases directly without relying on any linguistic 
theory (Chiang, 2005). As a result, it is more ro-
bust to the issue of non-syntactic phrase usage and 
non-isomorphic structure alignment.  
24.71
26.07
23.86
22.72
21.5
22.5
23.5
24.5
25.5
26.5
SCFG Moses STSG Ours
BL
EU
(%
)
 
Figure 4: Performance comparison of different methods 
 
Rule  
Type 
TR 
(STSG) 
TR 
+TSR_L 
TR+TSR_L
+TSR_P 
TR 
+TSR 
BLEU(%) 24.71 25.72 25.93 26.07 
 
Table 2: Contributions of TSRs (see Table 1 for the de-
finitions of the abbreviations used in this table) 
 
Table 2 measures the contributions of different 
kinds of tree sequence rules. It suggests that: 
1) All the three kinds of TSRs contribute to the 
performance improvement and their combination 
further improves the performance. It suggests that 
they are complementary to each other since the 
lexicalized TSRs are used to model non-syntactic 
phrases while the other two kinds of TSRs can ge-
neralize the lexicalized rules to unseen phrases. 
2)  The lexicalized TSRs make the major con-
tribution since they can capture non-syntactic 
phrases with syntactic structure features. 
 
Rule Type BLEU (%) 
TR+TSR 26.07 
(TR+TSR) w/o SRR 24.62 
(TR+TSR) w/o DPR 25.78 
 
Table 3: Effect of Structure Reordering Rules (SRR: 
refers to the structure reordering rules that have at least 
two non-terminal leaf nodes with inverted order in the 
source and target sides, which are usually not captured 
by phrase-based models. Note that the reordering be-
tween lexical words and non-terminal leaf nodes is not 
considered here) and Discontinuous Phrase Rules (DPR: 
refers to these rules having at least one non-terminal 
leaf node between two lexicalized leaf nodes) in our 
tree sequence-based model ( 4d =  and 6h = ) 
 
Rule Type # of rules # of rules overlapped 
(Intersection) 
SRR 68,217 18,379 (26.9%) 
DPR 57,244 18,379 (32.1%) 
 
Table 4: numbers of SRR and DPR rules 
 
Table 3 shows the contributions of SRR and 
DPR. It clearly indicates that SRRs are very effec-
tive in reordering structures, which improve per-
formance by 1.45 (26.07-24.62) BLEU score. 
However, DPRs have less impact on performance 
in our tree sequence-based model. This seems in 
contradiction to the previous observations3 in lite-
rature. However, it is not surprising simply be-
cause we use tree sequences as the basic translation 
units. Thereby, our model can capture all phrases. 
In this sense, our model behaves like a phrase-
based model, less sensitive to discontinuous phras-
                                                          
3 Wellington et al (2006) reports that discontinuities are very 
useful for translational equivalence analysis using binary-
branching structures under word alignment and parse tree 
constraints while they are almost of no use if under word 
alignment constraints only. Bod (2007) finds that discontinues 
phrase rules make significant performance improvement in 
linguistically STSG-based SMT models. 
565
es (Wellington et al, 2006). Our additional expe-
riments also verify that discontinuous phrase rules 
are complementary to syntactic phrase rules (Bod, 
2007) while non-syntactic phrase rules may com-
promise the contribution of discontinuous phrase 
rules. Table 4 reports the numbers of these two 
kinds of rules. It shows that around 30% rules are 
shared by the two kinds of rule sets. These over-
lapped rules contain at least two non-terminal leaf 
nodes plus two terminal leaf nodes, which implies 
that longer rules do not affect performance too 
much. 
 
22.07
25.28
26.1425.94 26.02 26.07
21.5
22.5
23.5
24.5
25.5
26.5
1 2 3 4 5 6
BL
EU
(%
)
 
Figure 5: Accuracy changing with different max-
imal tree depths ( h = 1 to 6 when 4d = ) 
 
22.72
24.71
26.0526.03 26.07
25.74
25.2925.2825.2624.78
21.5
22.5
23.5
24.5
25.5
26.5
1 2 3 4 5
B
LE
U
(%
)
 
Figure 6: Accuracy changing with the different maximal 
number of trees in a tree sequence ( d =1 to 5), the upper 
line is for 6h =  while the lower line is for 2h = .  
 
Fig. 5 studies the impact when setting different 
maximal tree depth ( h ) in a rule on the perfor-
mance. It demonstrates that:  
1) Significant performance improvement is 
achieved when the value of h  is increased from 1 
to 2. This can be easily explained by the fact that 
when h = 1, only monotonic search is conducted, 
while h = 2 allows non-terminals to be leaf nodes, 
thus introducing preliminary structure features to 
the search and allowing non-monotonic search. 
2) Internal structures and large span (due to h  
increasing) are also useful as attested by the gain 
of 0.86 (26.14-25.28) Blue score when the value of 
h  increases from 2 to 4. 
Fig. 6 studies the impact on performance by set-
ting different maximal tree number (d) in a rule. It 
further indicates that: 
1) Tree sequence rules (d >1) are useful and 
even more helpful if we limit the tree depth to no 
more than two (lower line, h=2). However, tree 
sequence rules consisting of more than three sub-
trees have almost no contribution to the perform-
ance improvement. This is mainly due to data 
sparseness issue when d >3. 
2) Even if only two-layer sub-trees (lower line) 
are allowed, our method still outperforms STSG 
and Moses when d>1. This further validates the 
effectiveness of our design philosophy of using 
multi-sub-trees as basic translation unit in SMT. 
7 Conclusions and Future Work 
In this paper, we present a tree sequence align-
ment-based translation model to combine the 
strengths of phrase-based and syntax-based me-
thods. The experimental results on the NIST MT-
2005 Chinese-English translation task demonstrate 
the effectiveness of the proposed model. Our study 
also finds that in our model the tree sequence rules 
are very useful since they can model non-syntactic 
phrases and reorderings with rich linguistic struc-
ture features while discontinuous phrases and tree 
sequence rules with more than three sub-trees have 
less impact on performance. 
There are many interesting research topics on 
the tree sequence-based translation model worth 
exploring in the future. The current method ex-
tracts large amount of rules. Many of them are re-
dundant, which make decoding very slow. Thus, 
effective rule optimization and pruning algorithms 
are highly desirable. Ideally, a linguistically and 
empirically motivated theory can be worked out, 
suggesting what kinds of rules should be extracted 
given an input phrase pair. For example, most 
function words and headwords can be kept in ab-
stract rules as features. In addition, word align-
ment is a hard constraint in our rule extraction. We 
will study direct structure alignments to reduce the 
impact of word alignment errors. We are also in-
terested in comparing our method with the forest-
to-string model (Liu et al, 2007). Finally, we 
would also like to study unsupervised learning-
based bilingual parsing for SMT.  
566
 References  
Rens Bod. 2007. Unsupervised Syntax-Based Machine 
Translation: The Contribution of Discontinuous 
Phrases. MT-Summmit-07. 51-56. 
David Chiang. 2005. A hierarchical phrase-based mod-
el for SMT. ACL-05. 263-270. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree transla-
tion. EMNLP-06. 232-241. 
Yuan Ding and Martha Palmer. 2005. Machine transla-
tion using probabilistic synchronous dependency in-
sertion grammars. ACL-05. 541-548. 
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for MT. ACL-03 (companion volume). 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What?s in a translation rule? HLT-
NAACL-04. 
Michel Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang and I. Thayer. 2006. Scalable Infe-
rence and Training of Context-Rich Syntactic 
Translation Models. COLING-ACL-06. 961-968 
Daniel Gildea. 2003. Loosely Tree-Based Alignment for 
Machine Translation. ACL-03. 80-87. 
Jonathan Graehl and Kevin Knight. 2004. Training tree 
transducers. HLT-NAACL-2004. 105-112. 
Mary Hearne and Andy Way. 2003. Seeing the wood for 
the trees: data-oriented translation. MT Summit IX, 
165-172. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06 (poster). 
Dan Klein and Christopher D. Manning. 2003. Accurate 
Unlexicalized Parsing. ACL-03. 423-430. 
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statistic-
al phrase-based translation. HLT-NAACL-03. 127-
133. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
models. AMTA-04, 115-124 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07 (poster) 77-180. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation Rules. 
ACL-07. 704-711. 
Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 
2006. SPMT: Statistical Machine Translation with 
Syntactified Target Language Phrases. EMNLP-06. 
44-52. 
I. Dan Melamed. 2004. Statistical machine translation 
by parsing. ACL-04. 653-660. 
Franz J. Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statistical 
machine translation. ACL-02. 295-302. 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz J. Och and Hermann Ney. 2004a. The alignment 
template approach to statistical machine translation. 
Computational Linguistics, 30(4):417-449. 
Kishore Papineni, Salim Roukos, ToddWard and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. ACL-02. 311-318. 
Arjen Poutsma. 2000. Data-oriented translation. 
COLING-2000. 635-641 
Chris Quirk and Arul Menezes. 2006. Do we need 
phrases? Challenging the conventional wisdom in 
SMT. COLING-ACL-06. 9-16. 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. ACL-05. 271-279. 
Stefan Riezler and John T. Maxwell III. 2006. Gram-
matical Machine Translation. HLT-NAACL-06. 
248-255. 
Hendra Setiawan, Min-Yen Kan and Haizhou Li. 2007. 
Ordering Phrases with Function Words. ACL-7. 
712-719. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. ICSLP-02. 901-904. 
Benjamin Wellington, Sonjia Waxmonsky and I. Dan 
Melamed. 2006. Empirical Lower Bounds on the 
Complexity of Translational Equivalence. COLING-
ACL-06. 977-984. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3):377-403. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
SMT. COLING-ACL-06. 521? 528. 
Kenji Yamada and Kevin Knight. 2001. A syntax-based 
statistical translation model. ACL-01. 523-530. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A Tree-to-Tree Align-
ment-based Model for Statistical Machine Transla-
tion. MT-Summit-07. 535-542. 
Ying Zhang. Stephan Vogel. Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement 
do we need to have a better system? LREC-04. 2051-
2054. 
567
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 149?152,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Linguistically Annotated Reordering Model
for BTG-based Statistical Machine Translation
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li
Human Language Technology
Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore 119613
{dyxiong, mzhang, aaiti, hli}@i2r.a-star.edu.sg
Abstract
In this paper, we propose a linguistically anno-
tated reordering model for BTG-based statis-
tical machine translation. The model incorpo-
rates linguistic knowledge to predict orders for
both syntactic and non-syntactic phrases. The
linguistic knowledge is automatically learned
from source-side parse trees through an an-
notation algorithm. We empirically demon-
strate that the proposed model leads to a sig-
nificant improvement of 1.55% in the BLEU
score over the baseline reordering model on
the NIST MT-05 Chinese-to-English transla-
tion task.
1 Introduction
In recent years, Bracketing Transduction Grammar
(BTG) proposed by (Wu, 1997) has been widely
used in statistical machine translation (SMT). How-
ever, the original BTG does not provide an effec-
tive mechanism to predict the most appropriate or-
ders between two neighboring phrases. To address
this problem, Xiong et al (2006) enhance the BTG
with a maximum entropy (MaxEnt) based reorder-
ing model which uses boundary words of bilingual
phrases as features. Although this model outper-
forms previous unlexicalized models, it does not uti-
lize any linguistically syntactic features, which have
proven useful for phrase reordering (Wang et al,
2007). Zhang et al (2007) integrates source-side
syntactic knowledge into a phrase reordering model
based on BTG-style rules. However, one limita-
tion of this method is that it only reorders syntac-
tic phrases because linguistic knowledge from parse
trees is only carried by syntactic phrases as far as re-
ordering is concerned, while non-syntactic phrases
are combined monotonously with a flat reordering
score.
In this paper, we propose a linguistically anno-
tated reordering model for BTG-based SMT, which
is a significant extension to the work mentioned
above. The new model annotates each BTG node
with linguistic knowledge by projecting source-side
parse trees onto the corresponding binary trees gen-
erated by BTG so that syntactic features can be used
for phrase reordering. Different from (Zhang et al,
2007), our annotation algorithm is able to label both
syntactic and non-syntactic phrases. This enables
our model to reorder any phrases, not limited to syn-
tactic phrases. In addition, other linguistic informa-
tion such as head words, is also used to improve re-
ordering.
The rest of the paper is organized as follows. Sec-
tion 2 briefly describes our baseline system while
Section 3 introduces the linguistically annotated re-
ordering model. Section 4 reports the experiments
on a Chinese-to-English translation task. We con-
clude in Section 5.
2 Baseline SMT System
The baseline system is a phrase-based system which
uses the BTG lexical rules (A ? x/y) to translate
source phrase x into target phrase y and the BTG
merging rules (A ? [A,A]|?A,A?) to combine two
neighboring phrases with a straight or inverted or-
der. The BTG lexical rules are weighted with several
features, such as phrase translation, word penalty
and language models, in a log-linear form. For the
merging rules, a MaxEnt-based reordering model
using boundary words of neighboring phrases as fea-
tures is used to predict the merging order, similar to
(Xiong et al, 2006). We call this reordering model
149
boundary words based reordering model (BWR). In
this paper, we propose to incorporate a linguistically
annotated reordering model into the log-linear trans-
lation model, so as to strengthen the BWR?s phrase
reordering ability. We train all the model scaling fac-
tors on the development set to maximize the BLEU
score. A CKY-style decoder is developed to gener-
ate the best BTG binary tree for each input sentence,
which yields the best translation.
3 Linguistically Annotated Reordering
Model
The linguistically annotated reordering
model (LAR) is a MaxEnt-based classifica-
tion model which predicts the phrase order
o ? {inverted, straight} during the application
of merging rules to combine their left and right
neighboring phrases Al and Ar into a larger phrase
A. 1 The model can be formulated as
LAR = exp(
?
i ?ihi(o,Al, Ar, A))?
o? exp(
?
i ?ihi(o?, Al, Ar, A))
(1)
where the functions hi ? {0, 1} are reordering fea-
tures and ?i are weights of these features. We define
the features as linguistic elements which are anno-
tated for each BTG node through an annotation al-
gorithm, which comprise (1) head word hw, (2) the
part-of-speech (POS) tag ht of head word and (3)
syntactic label sl.
Each merging rule involves 3 nodes (A,Al, Ar)
and each node has 3 linguistic elements (hw, ht, sl).
Therefore, the model has 9 features in total. Taking
the left node Al as an example, the model could use
its head word w as feature as follows
hi(o,A,Al, Ar) =
{ 1, Al.hw = w, o = straight
0, otherwise
3.1 Annotation Algorithm
There are two steps to annotate a phrase or a BTG
node using source-side parse tree information: (1)
determining the span on the source side which is
exactly covered by the node or the phrase, then
(2) annotating the span according to the source-side
parse tree. If the span is exactly covered by a sin-
gle subtree in the source-side parse tree, it is called
1Each phrase is also a node in the BTG tree generated by the
decoder.
1: Annotator (span s = ?i, j?, source-side parse tree t)
2: if s is a syntactic span then
3: Find the subtree c in t which exactly covers s
4: s.{ } := {c.hw, c.ht, c.sl}
5: else
6: Find the smallest subtree c? subsuming s in t
7: if c?.hw ? s then
8: s.hw := c?.hw and s.ht := c?.ht
9: else
10: Find the word w ? s which is nearest to c?.hw
11: s.hw := w and s.ht := w.t /*w.t is the POS
tag of w*/
12: end if
13: Find the left boundary node ln of s in c?
14: Find the right boundary node rn of s in c?
15: s.sl := ln.sl-c?.sl-rn.sl
16: end if
Figure 1: The Annotation Algorithm.
syntactic span, otherwise it is non-syntactic span.
One of the challenges in this annotation algorithm
is that phrases (BTG nodes) are not always cover-
ing syntactic span, in other words, they are not al-
ways aligned to all constituent nodes in the source-
side tree. To solve this problem, we use heuristic
rules to generate pseudo head word and composite
label which consists of syntactic labels of three rel-
evant constituents for the non-syntactic span. In this
way, our annotation algorithm is capable of labelling
both syntactic and non-syntactic phrases and there-
fore providing linguistic information for any phrase
reordering.
The annotation algorithm is shown in Fig. 1. For
a syntactic span, the annotation is trivial. Annotation
elements directly come from the subtree that covers
the span exactly. For a non-syntactic span, the pro-
cess is much complicated. Firstly, we need to locate
the smallest subtree c? subsuming the span (line 6).
Secondly, we try to identify the head word/tag of the
span (line 7-12) by using its head word directly if it
is within the span. Otherwise, the word within the
span which is nearest to hw will be assigned as the
head word of the span. Finally, we determine the
composite label of the span (line 13-15), which is
formulated as L-C-R. L/R means the syntactic label
of the left/right boundary node of s which is the
highest leftmost/rightmost sub-node of c? not over-
lapping the span. If there is no such boundary node
150
IP(??)
??
??
?
HH
HH
H
NP(??)
?? HH
NP(??)
NR
??1
Tibet
NP(??)
? H
NN
??2
financial
NN
??3
work
VP(??)
??
??
?
HH
HH
H
VV
??4
gain
AS
?5
NP(??)
?? HH
ADJP(??)
JJ
??6
remarkable
NP(??)
NN
??7
achievement
Figure 2: A syntactic parse tree with head word annotated
for each internal node. The superscripts of leaf nodes
denote their surface positions from left to right.
span hw ht sl
?1, 2? ?? NN NULL-NP-NN
?2, 3? ?? NN NP
?2, 4? ?? VV NP-IP-NP
?3, 4? ?? VV NP-IP-NP
Table 1: Annotation samples according to the tree shown
in Fig. 2. hw/ht represents the head word/tag, respec-
tively. sl means the syntactic label.
(the span s is exactly aligned to the left/right bound-
ary of c?), L/R will be set to NULL. C is the label of
c?. L, R and C together define the external syntactic
context of s.
Fig. 2 shows a syntactic parse tree for a Chinese
sentence, with head word annotated for each internal
node. Some sample annotations are given in Table 1.
3.2 Training and Decoding
Training an LAR model takes three steps. Firstly, we
extract annotated reordering examples from source-
side parsed, word-aligned bilingual data using the
annotation algorithm and the reordering example
extraction algorithm of (Xiong et al, 2006). We
then generate features using linguistic elements of
these examples and finally estimate feature weights.
This training process flexibly learns rich syntactic
reordering information without explicitly construct-
ing BTG tree or forest for each sentence pair.
During decoding, each input source sentence is
firstly parsed to obtain its syntactic tree. Then the
CKY-style decoder tries to generate the best BTG
tree using the lexical and merging rules. When two
neighboring nodes are merged in a specific order, the
two embedded reordering models, BWR and LAR,
evaluate this merging independently with individual
scores. The former uses boundary words as features
while the latter uses the linguistic elements as fea-
tures, annotated on the BTG nodes through the anno-
tation algorithm according to the source-side parse
tree.
4 Experiments
All experiments in this section were carried out on
the Chinese-to-English translation task of the NIST
MT-05. The baseline system and the new system
with the LAR model were trained on the FBIS cor-
pus. We removed 15,250 sentences, for which the
Chinese parser (Xiong et al, 2005) failed to pro-
duce syntactic parse trees. The parser was trained
on the Penn Chinese Treebank with a F1 score of
79.4%. The remaining FBIS corpus (224,165 sen-
tence pairs) was used to obtain standard bilingual
phrases for the systems.
We extracted 2.8M reordering examples from
these sentences. From these examples, we gener-
ated 114.8K reordering features for the BWR model
using the right boundary words of phrases and 85K
features for the LAR model using linguistic annota-
tions. We ran the MaxEnt toolkit (Zhang, 2004) to
tune reordering feature weights with iteration num-
ber being set to 100 and Gaussian prior to 1 to avoid
overfitting.
We built our four-gram language model using
Xinhua section of the English Gigaword corpus
(181.1M words) with the SRILM toolkit (Stol-
cke, 2002). For the efficiency of minimum-error-
rate training (Och, 2003), we built our development
set (580 sentences) using sentences not exceeding
50 characters from the NIST MT-02 evaluation test
data.
4.1 Results
We compared various reordering configurations in
the baseline system and new system. The base-
line system only has BWR as the reordering model,
while the new system employs two reordering mod-
els: BWR and LAR. For the linguistically anno-
tated reordering model LAR, we augment its feature
pool incrementally: firstly using only single labels
151
2(SL) as features (132 features in total), then con-
structing composite labels for non-syntactic phrases
(+BNL) (6.7K features), and finally introducing
head words and their POS tags into the feature pool
(+BNL+HWT) (85K features). This series of exper-
iments demonstrate the impact and degree of con-
tribution made by each feature for reordering. We
also conducted experiments to investigate the ef-
fect of restricting reordering to syntactic phrases in
the new system using the best reordering feature
set (SL+BNL+HWT) for LAR. The experimental
results (case-sensitive BLEU scores together with
confidence intervals) are presented in Table 2, from
which we have the following observations:
(1) The LAR model improves the performance
statistically significantly. Even we only use the base-
line feature set SL with only 132 features for the
LAR, the BLEU score improves from 0.2497 to
0.2588. This is because most of the frequent reorder-
ing patterns between Chinese and English have been
captured using syntactic labels. For example, the
pre-verbal modifier PP in Chinese is translated into
post-verbal counterpart in English. This reordering
can be described by a rule with an inverted order:
V P ? ?PP, V P ?, and captured by our syntactic
reordering features.
(2) Context information, provided by labels of
boundary nodes (BNL) and head word/tag pairs
(HWT), also improves phrase reordering. Produc-
ing composite labels for non-syntactic BTG nodes
(+BNL) and integrating head word/tag pairs into
the LAR as reordering features (+BNL+HWT) are
both effective, indicating that context information
complements syntactic label for capturing reorder-
ing patterns.
(3) Restricting phrase reordering to syntactic
phrases is harmful. The BLEU score plummets from
0.2652 to 0.2512.
5 Conclusion
In this paper, we have presented a linguistically an-
notated reordering model to effectively integrate lin-
guistic knowledge into phrase reordering by merg-
ing source-side parse trees with BTG binary trees.
Our experimental results show that, on the NIST
2For non-syntactic node, we only use the single label C,
without constructing composite label L-C-R.
Reordering Configuration BLEU (%)
BWR 24.97 ? 0.90
BWR + LAR (SL) 25.88 ? 0.95
BWR + LAR (+BNL) 26.27 ? 0.98
BWR + LAR (+BNL+HWT) 26.52 ? 0.96
Only allowed SPs reordering 25.12 ? 0.87
Table 2: The effect of the linguistically annotated reorder-
ing model. BWR denotes the boundary word based re-
ordering model while LAR denotes the linguistically an-
notated reordering model. (SL) is the baseline feature set,
(+BNL) and (+BNL+HWT) are extended feature sets for
the LAR. SP means syntactic phrase.
MT-05 task of Chinese-to-English translation, the
proposed reordering model leads to BLEU improve-
ment of 1.55%. We believe that our linguistically
annotated reordering model can be further improved
by using better annotation which transfers more
knowledge (morphological, syntactic or semantic)
to the model.
References
Franz Josef Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In Proceedings of ACL 2003.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of International Conference on
Spoken Language Processing, volume 2, pages 901-904.
Chao Wang, Michael Collins and Philipp Koehn. 2007. Chi-
nese Syntactic Reordering for Statistical Machine Transla-
tion. In Proceedings of EMNLP-CoNLL 2007.
Dekai Wu. 1997. Stochastic Inversion Transduction Grammars
and Bilingual Parsing of Parallel Corpora. Computational
Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang
Qian. 2005. Parsing the Penn Chinese Treebank with Se-
mantic Knowledge. In Proceedings of IJCNLP, Jeju Island,
Korea.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum
Entropy Based Phrase Reordering Model for Statistical Ma-
chine Translation. In Proceedings of ACL-COLING 2006.
Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou. 2007.
Phrase Reordering Model Integrating Syntactic Knowledge
for SMT. In Proceedings of EMNLP-CoNLL 2007.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
152
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 157?160,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Exploiting N-best Hypotheses for SMT Self-Enhancement 
 
Boxing Chen, Min Zhang, Aiti Aw and Haizhou Li 
Department of Human Language Technology 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, 119613, Singapore 
{bxchen, mzhang, aaiti, hli}@i2r.a-star.edu.sg 
 
 
 
Abstract 
Word and n-gram posterior probabilities esti-
mated on N-best hypotheses have been used to 
improve the performance of statistical ma-
chine translation (SMT) in a rescoring frame-
work. In this paper, we extend the idea to 
estimate the posterior probabilities on N-best 
hypotheses for translation phrase-pairs, target 
language n-grams, and source word re-
orderings. The SMT system is self-enhanced 
with the posterior knowledge learned from N-
best hypotheses in a re-decoding framework. 
Experiments on NIST Chinese-to-English task 
show performance improvements for all the 
strategies. Moreover, the combination of the 
three strategies achieves further improvements 
and outperforms the baseline by 0.67 BLEU 
score on NIST-2003 set, and 0.64 on NIST-
2005 set, respectively. 
1 Introduction 
State-of-the-art Statistical Machine Translation 
(SMT) systems usually adopt a two-pass search 
strategy. In the first pass, a decoding algorithm is 
applied to generate an N-best list of translation 
hypotheses; while in the second pass, the final 
translation is selected by rescoring and re-ranking 
the N-best hypotheses through additional feature 
functions. In this framework, the N-best hypothe-
ses serve as the candidates for the final translation 
selection in the second pass. 
These N-best hypotheses can also provide useful 
feedback to the MT system as the first decoding 
has discarded many undesirable translation candi-
dates. Thus, the knowledge captured in the N-best 
hypotheses, such as posterior probabilities for 
words, n-grams, phrase-pairs, and source word re-
orderings, etc. is more compatible with the source 
sentences and thus could potentially be used to 
improve the translation performance. 
Word posterior probabilities estimated from the 
N-best hypotheses have been widely used for con-
fidence measure in automatic speech recognition 
(Wessel, 2002) and have also been adopted into 
machine translation. Blatz et al (2003) and Uef-
fing et al (2003) used word posterior probabilities 
to estimate the confidence of machine translation. 
Chen et al (2005), Zens and Ney (2006) reported 
performance improvements by computing target n-
grams posterior probabilities estimated on the N-
best hypotheses in a rescoring framework. Trans-
ductive learning method (Ueffing et al, 2007) 
which repeatedly re-trains the generated source-
target N-best hypotheses with the original training 
data again showed translation performance im-
provement and demonstrated that the translation 
model can be reinforced from N-best hypotheses.  
In this paper, we further exploit the potential of 
the N-best hypotheses and propose several 
schemes to derive the posterior knowledge from 
the N-best hypotheses, in an effort to enhance the 
language model, translation model, and source 
word reordering under a re-decoding framework of 
any phrase-based SMT system. 
2 Self-Enhancement with Posterior 
Knowledge 
The self-enhancement system structure is shown in 
Figure 1. Our baseline system is set up using 
Moses (Koehn et al, 2007), a state-of-the-art 
phrase-base SMT open source package. In the fol-
lowings, we detail the approaches to exploiting the 
three different kinds of posterior knowledge, 
namely, language model, translation model and 
word reordering. 
157
2.1 Language Model 
We consider self-enhancement of language model 
as a language model adaptation problem similar to 
(Nakajima et al, 2002). The original monolingual 
target training data is regarded as general-domain 
data while the test data as a domain-specific data. 
Obviously, the real domain-specific target data 
(test data) is unavailable for training. In this work, 
the N-best hypotheses of the test set are used as a 
quasi-corpus to train a language model. This new 
language model trained on the quasi-corpus is then 
used together with the language model trained on 
the general-domain data (original training data) to 
produce a new list of N-best hypotheses under our 
self-enhancement framework. The feature function 
of the language model 1 1( , )
J I
LMh f e  is a mixture 
model of the two language models as in Equation 1. 
1 1 1 1 2 1( , ) ( ) ( )
J I I I
LM TLM QLMh f e h e h e? ?= +      (1) 
where 1
Jf is the source language words string, 
1
Ie is  the target language words string, TLM is the 
language model trained on target training data, and 
QLM is on the quasi-corpus of N-best hypotheses. 
The mixture model exploits multiple language 
models with weights 1?  and 2?  being optimized 
together with other feature functions. The proce-
dure for self-enhancement of the language model is 
as follows. 
1. Run decoding and extract N-best hypotheses. 
2. Train a new language model (QLM) on the N-
best hypotheses. 
3. Optimize the weights of the decoder which uses 
both original LM (TLM) and the new LM 
(QLM). 
4. Repeat step 1-3 for a fixed number of iterations. 
2.2 Translation Model 
In general, we can safely assume that for a given 
source input, phrase-pairs that appeared in the N-
best hypotheses are better than those that did not. 
We call the former ?good phrase-pairs? and the 
later ?bad phrase-pairs? for the given source input. 
Hypothetically, we can reinforce the translation 
model by appending the ?good phrase-pairs? to the 
original phrase table and changing the probability 
space of the translation model, as phrase-based 
translation probabilities are estimated using rela-
tive frequencies. The new direct phrase-based 
translation probabilities are computed as follows:   
( , ) ( , )( | )
( ) ( )
train nbest
train nbest
N f e N f ep e f
N f N f
+= +
% %% %%% % %       (2) 
where f%  is the source language phrase, e%  is  the 
target language phrase, (.)trainN is the frequencies 
observed in the training data, and (.)nbestN  is the 
frequencies observed in the N-best hypotheses. For 
those phrase-pairs that did not appear in the N-best 
hypotheses list (?bad phrase-pairs?), ( , )nbestN f e% %  
equals 0, but the marginal count of f%  is increased 
by ( )nbestN f% , in this way the phrase-based transla-
tion probabilities of ?bad phrase-pairs? degraded 
when compared with the corresponding probabili-
ties in the original translation model, and that of 
?good phrase-pairs? increased, hence improve the 
translation model. 
The procedure for translation model self-
enhancement can be summarized as follows. 
1. Run decoding and extract N-best hypotheses. 
2. Extract ?good phrase-pairs? according to the 
hypotheses? phrase-alignment information and 
append them to the original phrase table to gen-
erate a new phrase table. 
3. Score the new phrase table to create a new 
translation model. 
4. Optimize the weights of the decoder with the 
above new translation model. 
5. Repeat step 1-4 for a fixed number of iterations. 
2.3 Word Reordering 
Some previous work (Costa-juss? and Fonollosa, 
2006; Li et al, 2007) have shown that reordering a 
source sentence to match the word order in its cor-
 
Figure 1: Self-enhancement system structure, where 
TM is translation model, LM is language model, and 
RM is reordering model. 
158
responding target sentence can produce better 
translations for a phrase-based SMT system. We 
bring this idea forward to our word reordering self-
enhancement framework, which similarly trans-
lates a source sentence (S) to target sentence (T) in 
two stages: S S T?? ? , where S ?  is the reor-
dered source sentence.  
The phrase-alignment information in each hy-
pothesis indicates the word reordering for source 
sentence. We select the word reordering with the 
highest posterior probability as the best word reor-
dering for a given source sentence. Word re-
orderings from different phrase segmentation but 
with same word surface order are merged. The 
posterior probabilities of the word re-orderings are 
computed as in Equation 3. 
1
1 1
( )( | )
J
J J
hyp
N rp r f
N
=                        (3) 
where 1( )
JN r  is the count of word reordering 1
Jr , 
and hypN  is the number of N-best hypotheses.  
The words of the source sentence are then reor-
dered according to their indices in the best selected 
word reordering 1
Jr . The procedure for self-
enhancement of word reordering is as follows. 
1. Run decoding and extract N-best hypotheses. 
2. Select the best word re-orderings according to 
the phrase-alignment information. 
3. Reorder the source sentences according to the 
selected word reordering. 
4. Optimize the weights of the decoder with the 
reordered source sentences. 
5. Repeat step 1-4 for a fixed number of iterations. 
3 Experiments and Results 
Experiments on Chinese-to-English NIST transla-
tion tasks were carried out on the FBIS1 corpus. 
We used NIST 2002 MT evaluation test set as our 
development set, and the NIST 2003, 2005 test sets 
as our test sets as shown in Table 1. 
We determine the number of iteration empiri-
cally by setting it to 10. We then observe the 
BLEU score on the development set for each itera-
tion. The iteration number which achieved the best 
BLEU score on development set is selected as the 
iteration number of iterations for the test set.  
 
                                                          
1 LDC2003E14 
#Running words Data set type 
Chinese English 
parallel 7.0M 8.9M train 
monolingual - 61.5M 
NIST 02 dev 23.2K 108.6K 
NIST 03 test 25.8K 116.5K 
NIST 05 test 30.5K 141.9K 
Table 1: Statistics of training, dev and test sets. Evalua-
tion sets of NIST campaigns include 4 references: total 
numbers of running words are provided in the table. 
 
System #iter. NIST 02 NIST 03 NIST 05
Base - 27.67 26.68 24.82 
TM 4 27.87 26.95 25.05 
LM 6 27.96 27.06 25.07 
WR 6 27.99 27.04 25.11 
Comb 7 28.45 27.35 25.46 
Table 2: BLEU% scores of five systems: decoder (Base), 
self-enhancement on translation model (TM), language 
model (LM), word reordering (WR) and the combina-
tion of TM, LM and WR (Comb). 
 
Further experiments also suggested that, in this 
experiment scenario, setting the size of N-best list 
to 3,000 arrives at the greatest performance im-
provements. Our evaluation metric is BLEU (Pap-
ineni et al, 2002). The translation performance is 
reported in Table 2, where the column ?#iter.? re-
fers to the iteration number where the system 
achieved the best BLEU score on development set. 
Compared with the baseline (?Base? in Table 2), 
all three self-enhancement methods (?TM?, ?LM?, 
and ?WR? in Table 2) consistently improved the 
performance. In general, absolute gains of 0.23- 
0.38 BLEU score were obtained for each method 
on two test sets. While comparing the performance 
among all three methods, we can see that they 
achieved very similar improvement. Combining 
the three methods showed further gains in BLEU 
score. Totally, the combined system outperformed 
the baseline by 0.67 BLEU score on NIST?03, and 
0.64 on NIST?05 test set, respectively. 
4 Discussion 
As posterior knowledge applied in our models are 
posterior probabilities, the main difference be-
tween our work and all previous work is the use of 
knowledge source, where we derive knowledge 
from the N-best hypotheses generated from previ-
ous iteration. 
159
Comparing the work of (Nakajima et al, 2002), 
there is a slight difference between the two models. 
Nakajima et al used only 1-best hypothesis, while 
we use N-best hypotheses of test set as the quasi-
corpus to train the language model. 
In the work of  (Costa-juss? and Fonollosa, 2006;  
Li et al, 2007) which similarly translates a source 
sentence (S) to target sentence (T) in two stages: 
S S T?? ? , they derive S ? from training data; 
while we obtain S ?  based on the occurrence fre-
quency, i.e. posterior probability of each source 
word reordering in the N-best hypotheses list. 
An alternative solution for enhancing the trans-
lation model is through self-training (Ueffing, 
2006; Ueffing et al, 2007) which re-trains the 
source-target N-best hypotheses together with the 
original training data, and thus differs from ours in 
the way of new phrase pairs extraction. We only 
supplement those phrase-pairs appeared in the N-
best hypotheses to the original phrase table. Fur-
ther experiment showed that improvement ob-
tained by self-training method is not as consistent 
on both development and test sets as that by our 
method. One possible reason is that in self-training, 
the entire translation model is adjusted with the 
addition of new phrase-pairs extracted from the 
source-target N-best hypotheses, and hence the 
effect is less predictable. 
5 Conclusions 
To take advantage of the N-best hypotheses, we 
proposed schemes in a re-decoding framework and 
made use of the posterior knowledge learned from 
the N-best hypotheses to improve a phrase-based 
SMT system. The posterior knowledge include 
posterior probabilities for target n-grams, transla-
tion phrase-pairs and source word re-orderings, 
which in turn improve the language model, transla-
tion model, and word reordering respectively. 
Experiments were based on the state-of-the-art 
phrase-based decoder and carried out on NIST 
Chinese-to-English task. It has been shown that all 
three methods improved the performance. More-
over, the combination of all three strategies outper-
forms each individual method and significantly 
outperforms the baseline. We demonstrated that 
the SMT system can be self-enhanced by exploit-
ing useful feedback from the N-best hypotheses 
which are generated by itself. 
References 
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. 
Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 2003. 
Confidence estimation for machine translation. Final 
report, JHU/CLSP Summer Workshop. 
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo and M. 
Federico. 2005. The ITC-irst SMT System for 
IWSLT-2005. In Proceeding of IWSLT-2005, pp.98-
104, Pittsburgh, USA, October. 
M. R. Costa-juss?, J. A. R. Fonollosa. 2006. Statistical 
Machine Reordering. In Proceeding of EMNLP 2006. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, 
R. Zens, C. Dyer, O. Bojar, A. Constantin and E. 
Herbst. 2007. Moses: Open Source Toolkit for Statis-
tical Machine Translation. In Proceedings of ACL-
2007, pp. 177-180, Prague, Czech Republic. 
C.-H. Li, M. Li, D. Zhang, M. Li, M. Zhou and Y. Guan. 
2007.  A Probabilistic Approach to Syntax-based Re-
ordering for Statistical Machine Translation. In Pro-
ceedings of ACL-2007. Prague, Czech Republic. 
H. Nakajima, H. Yamamoto, T. Watanabe. 2002.  Lan-
guage model adaptation with additional text gener-
ated by machine translation. In Proceedings of 
COLING-2002. Volume 1, Pages: 1-7. Taipei. 
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu, 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceeding of ACL-2002, pp. 
311-318. 
N. Ueffing. 2006. Using Monolingual Source-Language 
Data to Improve MT Performance. In Proceedings of 
IWSLT 2006. Kyoto, Japan. November 27-28. 
N. Ueffing, K. Macherey, and H. Ney. 2003. Confi-
dence Measures for Statistical Machine Translation. 
In Proceeding of MT Summit IX, pages 394?401, 
New Orleans, LA, September. 
N. Ueffing, G. Haffari, A. Sarkar. 2007. Transductive 
learning for statistical machine translation. In Pro-
ceedings of ACL-2007, Prague. 
F. Wessel. 2002. Word Posterior Probabilities for Large 
Vocabulary Continuous Speech Recognition. Ph.D. 
thesis, RWTH Aachen University. Aachen, Germany, 
January. 
R. Zens and H. Ney. 2006. N-gram Posterior Probabili-
ties for Statistical Machine Translation. In Proceed-
ings of the HLT-NAACL Workshop on SMT, pp. 72-
77, NY. 
160
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 136?144,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Transliteration Alignment
Vladimir Pervouchine, Haizhou Li
Institute for Infocomm Research
A*STAR, Singapore 138632
{vpervouchine,hli}@i2r.a-star.edu.sg
Bo Lin
School of Computer Engineering
NTU, Singapore 639798
linbo@pmail.ntu.edu.sg
Abstract
This paper studies transliteration align-
ment, its evaluation metrics and applica-
tions. We propose a new evaluation met-
ric, alignment entropy, grounded on the
information theory, to evaluate the align-
ment quality without the need for the gold
standard reference and compare the metric
with F -score. We study the use of phono-
logical features and affinity statistics for
transliteration alignment at phoneme and
grapheme levels. The experiments show
that better alignment consistently leads to
more accurate transliteration. In transliter-
ation modeling application, we achieve a
mean reciprocal rate (MRR) of 0.773 on
Xinhua personal name corpus, a signifi-
cant improvement over other reported re-
sults on the same corpus. In transliteration
validation application, we achieve 4.48%
equal error rate on a large LDC corpus.
1 Introduction
Transliteration is a process of rewriting a word
from a source language to a target language in a
different writing system using the word?s phono-
logical equivalent. The word and its translitera-
tion form a transliteration pair. Many efforts have
been devoted to two areas of studies where there
is a need to establish the correspondence between
graphemes or phonemes between a transliteration
pair, also known as transliteration alignment.
One area is the generative transliteration model-
ing (Knight and Graehl, 1998), which studies how
to convert a word from one language to another us-
ing statistical models. Since the models are trained
on an aligned parallel corpus, the resulting statisti-
cal models can only be as good as the alignment of
the corpus. Another area is the transliteration vali-
dation, which studies the ways to validate translit-
eration pairs. For example Knight and Graehl
(1998) use the lexicon frequency, Qu and Grefen-
stette (2004) use the statistics in a monolingual
corpus and the Web, Kuo et al (2007) use proba-
bilities estimated from the transliteration model to
validate transliteration candidates. In this paper,
we propose using the alignment distance between
the a bilingual pair of words to establish the evi-
dence of transliteration candidacy. An example of
transliteration pair alignment is shown in Figure 1.
e
5
e
1
e
2
e
3
e
4
c
1
c
2
c
3
A   L I  C E
? ? ?
source graphemes
target graphemes
e
1
e
2
e
3
grapheme tokens
Figure 1: An example of grapheme alignment (Al-
ice, ???), where a Chinese grapheme, a char-
acter, is aligned to an English grapheme token.
Like the word alignment in statistical ma-
chine translation (MT), transliteration alignment
becomes one of the important topics in machine
transliteration, which has several unique chal-
lenges. Firstly, the grapheme sequence in a word
is not delimited into grapheme tokens, resulting
in an additional level of complexity. Secondly, to
maintain the phonological equivalence, the align-
ment has to make sense at both grapheme and
phoneme levels of the source and target languages.
This paper reports progress in our ongoing spoken
language translation project, where we are inter-
ested in the alignment problem of personal name
transliteration from English to Chinese.
This paper is organized as follows. In Section 2,
we discuss the prior work. In Section 3, we in-
troduce both statistically and phonologically mo-
tivated alignment techniques and in Section 4 we
advocate an evaluation metric, alignment entropy
that measures the alignment quality. We report the
experiments in Section 5. Finally, we conclude in
Section 6.
136
2 Related Work
A number of transliteration studies have touched
on the alignment issue as a part of the translit-
eration modeling process, where alignment is
needed at levels of graphemes and phonemes. In
their seminal paper Knight and Graehl (1998) de-
scribed a transliteration approach that transfers the
grapheme representation of a word via the pho-
netic representation, which is known as phoneme-
based transliteration technique (Virga and Khu-
danpur, 2003; Meng et al, 2001; Jung et al,
2000; Gao et al, 2004). Another technique is
to directly transfer the grapheme, known as di-
rect orthographic mapping, that was shown to
be simple and effective (Li et al, 2004). Some
other approaches that use both source graphemes
and phonemes were also reported with good per-
formance (Oh and Choi, 2002; Al-Onaizan and
Knight, 2002; Bilac and Tanaka, 2004).
To align a bilingual training corpus, some take a
phonological approach, in which the crafted map-
ping rules encode the prior linguistic knowledge
about the source and target languages directly into
the system (Wan and Verspoor, 1998; Meng et al,
2001; Jiang et al, 2007; Xu et al, 2006). Oth-
ers adopt a statistical approach, in which the affin-
ity between phonemes or graphemes is learned
from the corpus (Gao et al, 2004; AbdulJaleel and
Larkey, 2003; Virga and Khudanpur, 2003).
In the phoneme-based technique where an in-
termediate level of phonetic representation is used
as the pivot, alignment between graphemes and
phonemes of the source and target words is
needed (Oh and Choi, 2005). If source and tar-
get languages have different phoneme sets, align-
ment between the the different phonemes is also
required (Knight and Graehl, 1998). Although
the direct orthographic mapping approach advo-
cates a direct transfer of grapheme at run-time,
we still need to establish the grapheme correspon-
dence at the model training stage, when phoneme
level alignment can help.
It is apparent that the quality of transliteration
alignment of a training corpus has a significant
impact on the resulting transliteration model and
its performance. Although there are many stud-
ies of evaluation metrics of word alignment for
MT (Lambert, 2008), there has been much less re-
ported work on evaluation metrics of translitera-
tion alignment. In MT, the quality of training cor-
pus alignment A is often measured relatively to
the gold standard, or the ground truth alignment
G, which is a manual alignment of the corpus or
a part of it. Three evaluation metrics are used:
precision, recall, and F -score, the latter being a
function of the former two. They indicate how
close the alignment under investigation is to the
gold standard alignment (Mihalcea and Pedersen,
2003). Denoting the number of cross-lingual map-
pings that are common in both A and G as CAG,
the number of cross-lingual mappings in A as CA
and the number of cross-lingual mappings in G as
CG, precision Pr is given as CAG/CA, recall Rc
as CAG/CG and F -score as 2Pr ?Rc/(Pr+Rc).
Note that these metrics hinge on the availability
of the gold standard, which is often not available.
In this paper we propose a novel evaluation metric
for transliteration alignment grounded on the in-
formation theory. One important property of this
metric is that it does not require a gold standard
alignment as a reference. We will also show that
how this metric is used in generative transliteration
modeling and transliteration validation.
3 Transliteration alignment techniques
We assume in this paper that the source language
is English and the target language is Chinese, al-
though the technique is not restricted to English-
Chinese alignment.
Let a word in the source language (English) be
{ei} = {e1 . . . eI} and its transliteration in the
target language (Chinese) be {cj} = {c1 . . . cJ},
ei ? E, cj ? C, and E, C being the English and
Chinese sets of characters, or graphemes, respec-
tively. Aligning {ei} and {cj} means for each tar-
get grapheme token c?j finding a source grapheme
token e?m, which is an English substring in {ei}
that corresponds to cj , as shown in the example in
Figure 1. As Chinese is syllabic, we use a Chinese
character cj as the target grapheme token.
3.1 Grapheme affinity alignment
Given a distance function between graphemes of
the source and target languages d(ei, cj), the prob-
lem of alignment can be formulated as a dynamic
programming problem with the following function
to minimize:
Dij = min(Di?1,j?1 + d(ei, cj),
Di,j?1 + d(?, cj),
Di?1,j + d(ei, ?))
(1)
137
Here the asterisk * denotes a null grapheme that
is introduced to facilitate the alignment between
graphemes of different lengths. The minimum dis-
tance achieved is then given by
D =
I?
i=1
d(ei, c?(i)) (2)
where j = ?(i) is the correspondence between the
source and target graphemes. The alignment can
be performed via the Expectation-Maximization
(EM) by starting with a random initial alignment
and calculating the affinity matrix count(ei, cj)
over the whole parallel corpus, where element
(i, j) is the number of times character ei was
aligned to cj . From the affinity matrix conditional
probabilities P (ei|cj) can be estimated as
P (ei|cj) = count(ei, cj)/
?
j
count(ei, cj) (3)
Alignment j = ?(i) between {ei} and {cj} that
maximizes probability
P =
?
i
P (c?(i)|ei) (4)
is also the same alignment that minimizes align-
ment distance D:
D = ? logP = ?
?
i
logP (c?(i)|ei) (5)
In other words, equations (2) and (5) are the same
when we have the distance function d(ei, cj) =
? logP (cj |ei). Minimizing the overall distance
over a training corpus, we conduct EM iterations
until the convergence is achieved.
This technique solely relies on the affinity
statistics derived from training corpus, thus is
called grapheme affinity alignment. It is also
equally applicable for alignment between a pair of
symbol sequences representing either graphemes
or phonemes. (Gao et al, 2004; AbdulJaleel and
Larkey, 2003; Virga and Khudanpur, 2003).
3.2 Grapheme alignment via phonemes
Transliteration is about finding phonological
equivalent. It is therefore a natural choice to use
the phonetic representation as the pivot. It is
common though that the sound inventory differs
from one language to another, resulting in differ-
ent phonetic representations for source and tar-
get words. Continuing with the earlier example,
?
AE L AH S
A L I C E
AY l i s iz
? ?
graphemes
phonemes
phonemes
graphemes
source
target
Figure 2: An example of English-Chinese translit-
eration alignment via phonetic representations.
Figure 2 shows the correspondence between the
graphemes and phonemes of English word ?Al-
ice? and its Chinese transliteration, with CMU
phoneme set used for English (Chase, 1997) and
IIR phoneme set for Chinese (Li et al, 2007a).
A Chinese character is often mapped to a unique
sequence of Chinese phonemes. Therefore, if
we align English characters {ei} and Chinese
phonemes {cpk} (cpk ? CP set of Chinese
phonemes) well, we almost succeed in aligning
English and Chinese grapheme tokens. Alignment
between {ei} and {cpk} becomes the main task in
this paper.
3.2.1 Phoneme affinity alignment
Let the phonetic transcription of English word
{ei} be {epn}, epn ? EP , where EP is the set of
English phonemes. Alignment between {ei} and
{epn}, as well as between {epn} and {cpk} can
be performed via EM as described above. We esti-
mate conditional probability of Chinese phoneme
cpk after observing English character ei as
P (cpk|ei) =
?
{epn}
P (cpk|epn)P (epn|ei) (6)
We use the distance function between English
graphemes and Chinese phonemes d(ei, cpk) =
? logP (cpk|ei) to perform the initial alignment
between {ei} and {cpk} via dynamic program-
ming, followed by the EM iterations until con-
vergence. The estimates for P (cpk|epn) and
P (epn|ei) are obtained from the affinity matrices:
the former from the alignment of English and Chi-
nese phonetic representations, the latter from the
alignment of English words and their phonetic rep-
resentations.
3.2.2 Phonological alignment
Alignment between the phonetic representations
of source and target words can also be achieved
using the linguistic knowledge of phonetic sim-
ilarity. Oh and Choi (2002) define classes of
138
phonemes and assign various distances between
phonemes of different classes. In contrast, we
make use of phonological descriptors to define the
similarity between phonemes in this paper.
Perhaps the most common way to measure the
phonetic similarity is to compute the distances be-
tween phoneme features (Kessler, 2005). Such
features have been introduced in many ways, such
as perceptual attributes or articulatory attributes.
Recently, Tao et al (2006) and Yoon et al (2007)
have studied the use of phonological features and
manually assigned phonological distance to mea-
sure the similarity of transliterated words for ex-
tracting transliterations from a comparable corpus.
We adopt the binary-valued articulatory at-
tributes as the phonological descriptors, which are
used to describe the CMU and IIR phoneme sets
for English and Chinese Mandarin respectively.
Withgott and Chen (1993) define a feature vec-
tor of phonological descriptors for English sounds.
We extend the idea by defining a 21-element bi-
nary feature vector for each English and Chinese
phoneme. Each element of the feature vector
represents presence or absence of a phonologi-
cal descriptor that differentiates various kinds of
phonemes, e.g. vowels from consonants, front
from back vowels, nasals from fricatives, etc1.
In this way, a phoneme is described by a fea-
ture vector. We express the similarity between
two phonemes by the Hamming distance, also
called the phonological distance, between the two
feature vectors. A difference in one descriptor
between two phonemes increases their distance
by 1. As the descriptors are chosen to differenti-
ate between sounds, the distance between similar
phonemes is low, while that between two very dif-
ferent phonemes, such as a vowel and a consonant,
is high. The null phoneme, added to both English
and Chinese phoneme sets, has a constant distance
to any actual phonemes, which is higher than that
between any two actual phonemes.
We use the phonological distance to perform
the initial alignment between English and Chi-
nese phonetic representations of words. After that
we proceed with recalculation of the distances be-
tween phonemes using the affinity matrix as de-
scribed in Section 3.1 and realign the corpus again.
We continue the iterations until convergence is
1The complete table of English and Chinese phonemes
with their descriptors, as well as the translitera-
tion system demo is available at http://translit.i2r.a-
star.edu.sg/demos/transliteration/
reached. Because of the use of phonological de-
scriptors for the initial alignment, we call this tech-
nique the phonological alignment.
4 Transliteration alignment entropy
Having aligned the graphemes between two lan-
guages, we want to measure how good the align-
ment is. Aligning the graphemes means aligning
the English substrings, called the source grapheme
tokens, to Chinese characters, the target grapheme
tokens. Intuitively, the more consistent the map-
ping is, the better the alignment will be. We can
quantify the consistency of alignment via align-
ment entropy grounded on information theory.
Given a corpus of aligned transliteration pairs,
we calculate count(cj , e?m), the number of times
each Chinese grapheme token (character) cj is
mapped to each English grapheme token e?m. We
use the counts to estimate probabilities
P (e?m, cj) = count(cj , e?m)/
?
m,j
count(cj , e?m)
P (e?m|cj) = count(cj , e?m)/
?
m
count(cj , e?m)
The alignment entropy of the transliteration corpus
is the weighted average of the entropy values for
all Chinese tokens:
H = ?
?
j
P (cj)
?
m
P (e?m|cj) logP (e?m|cj)
= ?
?
m,j
P (e?m, cj) logP (e?m|cj)
(7)
Alignment entropy indicates the uncertainty of
mapping between the English and Chinese tokens
resulting from alignment. We expect and will
show that this estimate is a good indicator of the
alignment quality, and is as effective as the F -
score, but without the need for a gold standard ref-
erence. A lower alignment entropy suggests that
each Chinese token tends to be mapped to fewer
distinct English tokens, reflecting better consis-
tency. We expect a good alignment to have a
sharp cross-lingual mapping with low alignment
entropy.
5 Experiments
We use two transliteration corpora: Xinhua cor-
pus (Xinhua News Agency, 1992) of 37,637
personal name pairs and LDC Chinese-English
139
named entity list LDC2005T34 (Linguistic Data
Consortium, 2005), containing 673,390 personal
name pairs. The LDC corpus is referred to as
LDC05 for short hereafter. For the results to be
comparable with other studies, we follow the same
splitting of Xinhua corpus as that in (Li et al,
2007b) having a training and testing set of 34,777
and 2,896 names respectively. In contrast to the
well edited Xinhua corpus, LDC05 contains erro-
neous entries. We have manually verified and cor-
rected around 240,000 pairs to clean up the corpus.
As a result, we arrive at a set of 560,768 English-
Chinese (EC) pairs that follow the Chinese pho-
netic rules, and a set of 83,403 English-Japanese
Kanji (EJ) pairs, which follow the Japanese pho-
netic rules, and the rest 29,219 pairs (REST) be-
ing labeled as incorrect transliterations. Next we
conduct three experiments to study 1) alignment
entropy vs. F -score, 2) the impact of alignment
quality on transliteration accuracy, and 3) how to
validate transliteration using alignment metrics.
5.1 Alignment entropy vs. F -score
As mentioned earlier, for English-Chinese
grapheme alignment, the main task is to align En-
glish graphemes to Chinese phonemes. Phonetic
transcription for the English names in Xinhua
corpus are obtained by a grapheme-to-phoneme
(G2P) converter (Lenzo, 1997), which generates
phoneme sequence without providing the exact
correspondence between the graphemes and
phonemes. G2P converter is trained on the CMU
dictionary (Lenzo, 2008).
We align English grapheme and phonetic repre-
sentations e? ep with the affinity alignment tech-
nique (Section 3.1) in 3 iterations. We further
align the English and Chinese phonetic represen-
tations ep ? cp via both affinity and phonological
alignment techniques, by carrying out 6 and 7 it-
erations respectively. The alignment methods are
schematically shown in Figure 3.
To study how alignment entropy varies accord-
ing to different quality of alignment, we would
like to have many different alignment results. We
pair the intermediate results from the e ? ep and
ep ? cp alignment iterations (see Figure 3) to
form e ? ep ? cp alignments between English
graphemes and Chinese phonemes and let them
converge through few more iterations, as shown
in Figure 4. In this way, we arrive at a total of 114
phonological and 80 affinity alignments of differ-
ent quality.
{cp
k
}
{e
i
}
English
graphemes
{ep
n
}
English
phonemes
Chinese
phonemes
affinity alignment affinity alignment
e? ep
iteration 1
e? ep
iteration 2
e? ep
iteration 3
ep? cp
iteration 1
ep? cp
iteration 2
...
ep? cp
iteration 6
phonological alignment
ep? cp
iteration 1
ep? cp
iteration 2
...
ep? cp
iteration 7
Figure 3: Aligning English graphemes to
phonemes e?ep and English phonemes to Chinese
phonemes ep?cp. Intermediate e?ep and ep?cp
alignments are used for producing e ? ep ? cp
alignments.
e? ep
alignments
ep? cp
affinity / 
phonological
alignments
iteration 1
iteration 2
iteration 3
iteration 1
iteration 2
iteration n
...
...
calculating
d(e
i
, cp
k
)
affinity
alignment
iteration 1
iteration 2
...
e? ep? cp
etc
Figure 4: Example of aligning English graphemes
to Chinese phonemes. Each combination of e?ep
and ep? cp alignments is used to derive the initial
distance d(ei, cpk), resulting in several e?ep?cp
alignments due to the affinity alignment iterations.
We have manually aligned a random set of
3,000 transliteration pairs from the Xinhua train-
ing set to serve as the gold standard, on which we
calculate the precision, recall and F -score as well
as alignment entropy for each alignment. Each
alignment is reflected as a data point in Figures 5a
and 5b. From the figures, we can observe a clear
correlation between the alignment entropy and F -
score, that validates the effectiveness of alignment
entropy as an evaluation metric. Note that we
don?t need the gold standard reference for report-
ing the alignment entropy.
We also notice that the data points seem to form
clusters inside which the value of F -score changes
insignificantly as the alignment entropy changes.
Further investigation reveals that this could be due
to the limited number of entries in the gold stan-
dard. The 3,000 names in the gold standard are not
enough to effectively reflect the change across dif-
ferent alignments. F -score requires a large gold
standard which is not always available. In con-
trast, because the alignment entropy doesn?t de-
pend on the gold standard, one can easily report
the alignment performance on any unaligned par-
allel corpus.
140
??????
??????
??????
???
??? ??? ??? ???
?? ? ?
???? ?? ? ? ?
?? ??
(a) 80 affinity alignments
??????
??????
??????
???
??? ??? ??? ???
???? ?? ? ? ?
?? ??
?? ? ?
(b) 114 phonological alignments
Figure 5: Correlation between F -score and align-
ment entropy for Xinhua training set algnments.
Results for precision and recall have similar trends
.
5.2 Impact of alignment quality on
transliteration accuracy
We now further study how the alignment affects
the generative transliteration model in the frame-
work of the joint source-channel model (Li et al,
2004). This model performs transliteration by
maximizing the joint probability of the source and
target names P ({ei}, {cj}), where the source and
target names are sequences of English and Chi-
nese grapheme tokens. The joint probability is
expressed as a chain product of a series of condi-
tional probabilities of token pairs P ({ei}, {cj}) =
P ((e?k, ck)|(e?k?1, ck?1)), k = 1 . . . N , where we
limit the history to one preceding pair, resulting in
a bigram model. The conditional probabilities for
token pairs are estimated from the aligned training
corpus. We use this model because it was shown
to be simple yet accurate (Ekbal et al, 2006; Li
et al, 2007b). We train a model for each of the
114 phonological alignments and the 80 affinity
alignments in Section 5.1 and conduct translitera-
tion experiment on the Xinhua test data.
During transliteration, an input English name
is first decoded into a lattice of all possible En-
glish and Chinese grapheme token pairs. Then the
joint source-channel transliteration model is used
to score the lattice to obtain a ranked list ofmmost
likely Chinese transliterations (m-best list).
We measure transliteration accuracy as the
mean reciprocal rank (MRR) (Kantor and
Voorhees, 2000). If there is only one correct
Chinese transliteration of the k-th English word
and it is found at the rk-th position in the m-best
list, its reciprocal rank is 1/rk. If the list contains
no correct transliterations, the reciprocal rank is
0. In case of multiple correct transliterations, we
take the one that gives the highest reciprocal rank.
MRR is the average of the reciprocal ranks across
all words in the test set. It is commonly used as
a measure of transliteration accuracy, and also
allows us to make a direct comparison with other
reported work (Li et al, 2007b).
We take m = 20 and measure MRR on Xinhua
test set for each alignment of Xinhua training set
as described in Section 5.1. We report MRR and
the alignment entropy in Figures 6a and 7a for the
affinity and phonological alignments respectively.
The highest MRR we achieve is 0.771 for affin-
ity alignments and 0.773 for phonological align-
ments. This is a significant improvement over the
MRR of 0.708 reported in (Li et al, 2007b) on the
same data. We also observe that the phonological
alignment technique produces, on average, better
alignments than the affinity alignment technique
in terms of both the alignment entropy and MRR.
We also report the MRR and F -scores for each
alignment in Figures 6b and 7b, from which we
observe that alignment entropy has stronger corre-
lation with MRR than F -score does. The Spear-
man?s rank correlation coefficients are ?0.89 and
?0.88 for data in Figure 6a and 7a respectively.
This once again demonstrates the desired property
of alignment entropy as an evaluation metric of
alignment.
To validate our findings from Xinhua corpus,
we further carry out experiments on the EC set
of LDC05 containing 560,768 entries. We split
the set into 5 almost equal subsets for cross-
validation: in each of 5 experiments one subset is
used for testing and the remaining ones for train-
ing. Since LDC05 contains one-to-many English-
Chinese transliteration pairs, we make sure that an
English name only appears in one subset.
Note that the EC set of LDC05 contains
many names of non-English, and, generally, non-
European origin. This makes the G2P converter
less accurate, as it is trained on an English pho-
netic dictionary. We therefore only apply the affin-
ity alignment technique to align the EC set. We
141
??????
??????
??????
??? ??? ??? ???
MRR
Alignment?entropy
(a) 80 affinity alignments
??????
??????
??????
??? ??? ??? ??? ??? ??? ???
MRR
F?score
(b) 80 affinity alignments
Figure 6: Mean reciprocal ratio on Xinhua test
set vs. alignment entropy and F -score for mod-
els trained with different affinity alignments.
use each iteration of the alignment in the translit-
eration modeling and present the resulting MRR
along with alignment entropy in Figure 8. The
MRR results are the averages of five values pro-
duced in the five-fold cross-validations.
We observe a clear correlation between the
alignment entropy and transliteration accuracy ex-
pressed by MRR on LDC05 corpus, similar to that
on Xinhua corpus, with the Spearman?s rank cor-
relation coefficient of ?0.77. We obtain the high-
est average MRR of 0.720 on the EC set.
5.3 Validating transliteration using
alignment measure
Transliteration validation is a hypothesis test that
decides whether a given transliteration pair is gen-
uine or not. Instead of using the lexicon fre-
quency (Knight and Graehl, 1998) or Web statis-
tics (Qu and Grefenstette, 2004), we propose vali-
dating transliteration pairs according to the align-
ment distance D between the aligned English
graphemes and Chinese phonemes (see equations
(2) and (5)). A distance function d(ei, cpk) is
established from each alignment on the Xinhua
training set as discussed in Section 5.2.
An audit of LDC05 corpus groups the corpus
into three sets: an English-Chinese (EC) set of
560,768 samples, an English-Japanese (EJ) set
of 83,403 samples and the REST set of 29,219
??????
??????
??????
??? ??? ??? ???
MRR
Alignment?entropy
(a) 114 phonological alignments
??????
??????
??????
??? ??? ??? ??? ??? ??? ???
MRR
F?score
(b) 114 phonological alignments
Figure 7: Mean reciprocal ratio on Xinhua test
set vs. alignment entropy and F -score for models
trained with different phonological alignments.
??????
??????
??????
??????
???
??? ??? ??? ???
??
???? ?? ? ? ?
Figure 8: Mean reciprocal ratio vs. alignment en-
tropy for alignments of EC set.
samples that are not transliteration pairs. We
mark the EC name pairs as genuine and the rest
112,622 name pairs that do not follow the Chi-
nese phonetic rules as false transliterations, thus
creating the ground truth labels for an English-
Chinese transliteration validation experiment. In
other words, LDC05 has 560,768 genuine translit-
eration pairs and 112,622 false ones.
We run one iteration of alignment over LDC05
(both genuine and false) with the distance func-
tion d(ei, cpk) derived from the affinity matrix of
one aligned Xinhua training set. In this way, each
transliteration pair in LDC05 provides an align-
ment distance. One can expect that a genuine
transliteration pair typically aligns well, leading
to a low distance, while a false transliteration pair
will do otherwise. To remove the effect of word
length, we normalize the distance by the English
name length, the Chinese phonetic transcription
142
length, and the sum of both, producing score1,
score2 and score3 respectively.
Miss?probability?(%)
F
a
l
s
e
?
a
l
a
r
m
?
p
r
o
b
a
b
i
l
i
t
y
?
(
%
)
2 5
10
1
2
5
1
0
1
20
2
0
score
2
EER:?4.48?%
score
1
EER:?7.13?%
score
3
EER:?4.80?%
(a) DET with score1, score2,
score3.
1 2 5 10
1
2
5
1
0
Miss?probability?(%)
F
a
l
s
e
?
a
l
a
r
m
?
p
r
o
b
a
b
i
l
i
t
y
?
(
%
)
Entropy:?2.396
MRR:?0.773
EER:?4.48?%
Entropy:?2.529
MRR:?0.764
EER:?4.52%
Entropy:?2.625
MRR:?0.754
EER:?4.70%
(b) DET results vs. three different
alignment quality.
Figure 9: Detection error tradeoff (DET) curves
for transliteration validation on LDC05.
We can now classify each LDC05 name pair as
genuine or false by having a hypothesis test. When
the test score is lower than a pre-set threshold, the
name pair is accepted as genuine, otherwise false.
In this way, each pre-set threshold will present two
types of errors, a false alarm and a miss-detect
rate. A common way to present such results is via
the detection error tradeoff (DET) curves, which
show all possible decision points, and the equal er-
ror rate (EER), when false alarm and miss-detect
rates are equal.
Figure 9a shows three DET curves based on
score1, score2 and score3 respectively for one
one alignment solution on the Xinhua training set.
The horizontal axis is the probability of miss-
detecting a genuine transliteration, while the verti-
cal one is the probability of false-alarms. It is clear
that out of the three, score2 gives the best results.
We select the alignments of Xinhua training
set that produce the highest and the lowest MRR.
We also randomly select three other alignments
that produce different MRR values from the pool
of 114 phonological and 80 affinity alignments.
Xinhua train 
set algnment
Alignment entropy 
of Xinhua train set
MRR on Xinhua 
test set
LDC 
classification 
EER, %
1
2
3
4
5
2.396 0.773 4.48
2.529 0.764 4.52
2.586 0.761 4.51
2.621 0.757 4.71
2.625 0.754 4.70
Table 1: Equal error ratio of LDC transliteration
pair validation for different alignments of Xinhua
training set.
We use each alignment to derive distance func-
tion d(ei, cpk). Table 1 shows the EER of LDC05
validation using score2, along with the alignment
entropy of the Xinhua training set that derives
d(ei, cpk), and the MRR on Xinhua test set in the
generative transliteration experiment (see Section
5.2) for all 5 alignments. To avoid cluttering Fig-
ure 9b, we show the DET curves for alignments
1, 2 and 5 only. We observe that distance func-
tion derived from better aligned Xinhua corpus,
as measured by both our alignment entropy met-
ric and MRR, leads to a higher validation accuracy
consistently on LDC05.
6 Conclusions
We conclude that the alignment entropy is a re-
liable indicator of the alignment quality, as con-
firmed by our experiments on both Xinhua and
LDC corpora. Alignment entropy does not re-
quire the gold standard reference, it thus can be
used to evaluate alignments of large transliteration
corpora and is possibly to give more reliable esti-
mate of alignment quality than the F -score metric
as shown in our transliteration experiment.
The alignment quality of training corpus has
a significant impact on the transliteration mod-
els. We achieve the highest MRR of 0.773 on
Xinhua corpus with phonological alignment tech-
nique, which represents a significant performance
gain over other reported results. Phonological
alignment outperforms affinity alignment on clean
database.
We propose using alignment distance to validate
transliterations. A high quality alignment on a
small verified corpus such as Xinhua can be effec-
tively used to validate a large noisy corpus, such
as LDC05. We believe that this property would be
useful in transliteration extraction, cross-lingual
information retrieval applications.
143
References
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Sta-
tistical transliteration for English-Arabic cross lan-
guage information retrieval. In Proc. ACM CIKM.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proc. ACL
Workshop: Computational Apporaches to Semitic
Languages.
Slaven Bilac and Hozumi Tanaka. 2004. A hybrid
back-transliteration system for Japanese. In Proc.
COLING, pages 597?603.
Lin L. Chase. 1997. Error-responsive feedback mech-
anisms for speech recognizers. Ph.D. thesis, CMU.
Asif Ekbal, Sudip Kumar Naskar, and Sivaji Bandy-
opadhyay. 2006. A modified joint source-channel
model for transliteration. In Proc. COLING/ACL,
pages 191?198
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004.
Phoneme-based transliteration of foreign names for
OOV problem. In Proc. IJCNLP, pages 374?381.
Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named entity translation with web min-
ing and transliteration. In IJCAI, pages 1629?1634.
Sung Young Jung, SungLim Hong, and Eunok Paek.
2000. An English to Korean transliteration model of
extended Markov window. In Proc. COLING, vol-
ume 1.
Paul. B. Kantor and Ellen. M. Voorhees. 2000. The
TREC-5 confusion track: comparing retrieval meth-
ods for scanned text. Information Retrieval, 2:165?
176.
Brett Kessler. 2005. Phonetic comparison algo-
rithms. Transactions of the Philological Society,
103(2):243?260.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang. 2007.
A phonetic similarity model for automatic extraction
of transliteration pairs. ACM Trans. Asian Language
Information Processing, 6(2).
Patrik Lambert. 2008. Exploiting lexical informa-
tion and discriminative alignment training in statis-
tical machine translation. Ph.D. thesis, Universitat
Polite`cnica de Catalunya, Barcelona, Spain.
Kevin Lenzo. 1997. t2p: text-to-phoneme converter
builder. http://www.cs.cmu.edu/?lenzo/t2p/.
Kevin Lenzo. 2008. The CMU pronounc-
ing dictionary. http://www.speech.cs.cmu.edu/cgi-
bin/cmudict.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. ACL, pages 159?166.
Haizhou Li, Bin Ma, and Chin-Hui Lee. 2007a. A
vector space modeling approach to spoken language
identification. IEEE Trans. Acoust., Speech, Signal
Process., 15(1):271?284.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui
Dong. 2007b. Semantic transliteration of personal
names. In Proc. ACL, pages 120?127.
Linguistic Data Consortium. 2005. LDC Chinese-
English name entity lists LDC2005T34.
Helen M. Meng, Wai-Kit Lo, Berlin Chen, and Karen
Tang. 2001. Generate phonetic cognates to han-
dle name entities in English-Chinese cross-language
spoken document retrieval. In Proc. ASRU.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Proc. HLT-NAACL,
pages 1?10.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation
and contextual rules. In Proc. COLING 2002.
Jong-Hoon Oh and Key-Sun Choi. 2005. Machine
learning based english-to-korean transliteration us-
ing grapheme and phoneme information. IEICE
Trans. Information and Systems, E88-D(7):1737?
1748.
Yan Qu and Gregory Grefenstette. 2004. Finding ideo-
graphic representations of Japanese names written in
Latin script via language identification and corpus
validation. In Proc. ACL, pages 183?190.
Tao Tao, Su-Youn Yoon, Andrew Fisterd, Richard
Sproat, and ChengXiang Zhai. 2006. Unsupervised
named entity transliteration using temporal and pho-
netic correlation. In Proc. EMNLP, pages 250?257.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. ACL MLNER.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proc. COL-
ING, pages 1352?1356.
M. M. Withgott and F. R. Chen. 1993. Computational
models of American speech. Centre for the study of
language and information.
Xinhua News Agency. 1992. Chinese transliteration
of foreign personal names. The Commercial Press.
LiLi Xu, Atsushi Fujii, and Tetsuya Ishikawa. 2006.
Modeling impression in probabilistic transliteration
into Chinese. In Proc. EMNLP, pages 242?249.
Su-Youn Yoon, Kyoung-Young Kim, and Richard
Sproat. 2007. Multilingual transliteration using fea-
ture based phonetic method. In Proc. ACL, pages
112?119.
144
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 172?180,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Forest-based Tree Sequence to String Translation Model 
 
Hui Zhang1, 2   Min Zhang1   Haizhou Li1   Aiti Aw1   Chew Lim Tan2 
1Institute for Infocomm Research                    2National University of Singapore                    
zhangh1982@gmail.com   {mzhang, hli, aaiti}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sg  
 
 
 
 
Abstract 
This paper proposes a forest-based tree se-
quence to string translation model for syntax- 
based statistical machine translation, which 
automatically learns tree sequence to string 
translation rules from word-aligned source-
side-parsed bilingual texts. The proposed 
model leverages on the strengths of both tree 
sequence-based and forest-based translation 
models. Therefore, it can not only utilize forest 
structure that compactly encodes exponential 
number of parse trees but also capture non-
syntactic translation equivalences with linguis-
tically structured information through tree se-
quence. This makes our model potentially 
more robust to parse errors and structure di-
vergence. Experimental results on the NIST 
MT-2003 Chinese-English translation task 
show that our method statistically significantly 
outperforms the four baseline systems. 
1 Introduction 
Recently syntax-based statistical machine trans-
lation (SMT) methods have achieved very prom-
ising results and attracted more and more inter-
ests in the SMT research community. Fundamen-
tally, syntax-based SMT views translation as a 
structural transformation process. Therefore, 
structure divergence and parse errors are two of 
the major issues that may largely compromise 
the performance of syntax-based SMT (Zhang et 
al., 2008a; Mi et al, 2008).  
Many solutions have been proposed to address 
the above two issues. Among these advances, 
forest-based modeling (Mi et al, 2008; Mi and 
Huang, 2008) and tree sequence-based modeling 
(Liu et al, 2007; Zhang et al, 2008a) are two 
interesting modeling methods with promising 
results reported. Forest-based modeling aims to 
improve translation accuracy through digging the 
potential better parses from n-bests (i.e. forest) 
while tree sequence-based modeling aims to 
model non-syntactic translations with structured 
syntactic knowledge. In nature, the two methods 
would be complementary to each other since 
they manage to solve the negative impacts of 
monolingual parse errors and cross-lingual struc-
ture divergence on translation results from dif-
ferent viewpoints. Therefore, one natural way is 
to combine the strengths of the two modeling 
methods for better performance of syntax-based 
SMT. However, there are many challenges in 
combining the two methods into a single model 
from both theoretical and implementation engi-
neering viewpoints. In theory, one may worry 
about whether the advantage of tree sequence has 
already been covered by forest because forest 
encodes implicitly a huge number of parse trees 
and these parse trees may generate many differ-
ent phrases and structure segmentations given a 
source sentence. In system implementation, the 
exponential combinations of tree sequences with 
forest structures make the rule extraction and 
decoding tasks much more complicated than that 
of the two individual methods.  
In this paper, we propose a forest-based tree 
sequence to string model, which is designed to 
integrate the strengths of the forest-based and the 
tree sequence-based modeling methods. We pre-
sent our solutions that are able to extract transla-
tion rules and decode translation results for our 
model very efficiently. A general, configurable 
platform was designed for our model. With this 
platform, we can easily implement our method 
and many previous syntax-based methods by 
simple parameter setting. We evaluate our 
method on the NIST MT-2003 Chinese-English 
translation tasks. Experimental results show that 
our method significantly outperforms the two 
individual methods and other baseline methods. 
Our study shows that the proposed method is 
able to effectively combine the strengths of the 
forest-based and tree sequence-based methods, 
and thus having great potential to address the 
issues of parse errors and non-syntactic transla-
172
tions resulting from structure divergence. It also 
indicates that tree sequence and forest play dif-
ferent roles and make contributions to our model 
in different ways. 
The remainder of the paper is organized as fol-
lows. Section 2 describes related work while sec-
tion 3 defines our translation model. In section 4 
and section 5, the key rule extraction and decod-
ing algorithms are elaborated. Experimental re-
sults are reported in section 6 and the paper is 
concluded in section 7. 
2 Related work  
As discussed in section 1, two of the major chal-
lenges to syntax-based SMT are structure diver-
gence and parse errors. Many techniques have 
been proposed to address the structure diver-
gence issue while only fewer studies are reported 
in addressing the parse errors in the SMT re-
search community. 
To address structure divergence issue, many 
researchers (Eisner, 2003; Zhang et al, 2007) 
propose using the Synchronous Tree Substitution 
Grammar (STSG) grammar in syntax-based 
SMT since the STSG uses larger tree fragment as 
translation unit. Although promising results have 
been reported, STSG only uses one single sub-
tree as translation unit which is still committed to 
the syntax strictly. Motivated by the fact that 
non-syntactic phrases make non-trivial contribu-
tion to phrase-based SMT, the tree sequence-
based translation model is proposed (Liu et al, 
2007; Zhang et al, 2008a) that uses tree se-
quence as the basic translation unit, rather than 
using single sub-tree as in the STSG. Here, a tree 
sequence refers to a sequence of consecutive 
sub-trees that are embedded in a full parse tree. 
For any given phrase in a sentence, there is at 
least one tree sequence covering it. Thus the tree 
sequence-based model has great potential to ad-
dress the structure divergence issue by using tree 
sequence-based non-syntactic translation rules. 
Liu et al (2007) propose the tree sequence con-
cept and design a tree sequence to string transla-
tion model. Zhang et al (2008a) propose a tree 
sequence-based tree to tree translation model and 
Zhang et al (2008b) demonstrate that the tree 
sequence-based modelling method can well ad-
dress the structure divergence issue for syntax-
based SMT. 
To overcome the parse errors for SMT, Mi et 
al. (2008) propose a forest-based translation 
method that uses forest instead of one best tree as 
translation input, where a forest is a compact rep-
resentation of exponentially number of n-best 
parse trees. Mi and Huang (2008) propose a for-
est-based rule extraction algorithm, which learn 
tree to string rules from source forest and target 
string. By using forest in rule extraction and de-
coding, their methods are able to well address the 
parse error issue. 
From the above discussion, we can see that 
traditional tree sequence-based method uses sin-
gle tree as translation input while the forest-
based model uses single sub-tree as the basic 
translation unit that can only learn tree-to-string 
(Galley et al 2004; Liu et al, 2006) rules. There-
fore, the two methods display different strengths, 
and which would be complementary to each 
other. To integrate their strengths, in this paper, 
we propose a forest-based tree sequence to string 
translation model.  
3 Forest-based tree sequence to string 
model  
In this section, we first explain what a packed 
forest is and then define the concept of the tree 
sequence in the context of forest followed by the 
discussion on our proposed model. 
3.1 Packed Forest 
A packed forest (forest in short) is a special kind 
of hyper-graph (Klein and Manning, 2001; 
Huang and Chiang, 2005), which is used to rep-
resent all derivations (i.e. parse trees) for a given 
sentence under a context free grammar (CFG). A 
forest F is defined as a triple ? ?, ?, ? ?, where 
? is non-terminal node set, ?  is hyper-edge set 
and ? is leaf node set (i.e. all sentence words). A 
forest F satisfies the following two conditions: 
 
1) Each node ?  in ?  should cover a phrase, 
which is a continuous word sub-sequence in ?. 
2) Each hyper-edge ?  in ?  is defined as 
?? ? ?? ??? ? ??, ??? ? ?? ? ??, ?? ? ?? , 
where ?? ? ?? ???  covers a sequence of conti-
nuous and non-overlap phrases, ??  is the father 
node of the children sequence ?? ??? ???. The 
phrase covered by ??  is just the sum of all the 
phrases covered by each child node ??. 
 
We here introduce another concept that is used 
in our subsequent discussions. A complete forest 
CF is a general forest with one additional condi-
tion that there is only one root node N in CF, i.e., 
all nodes except the root N in a CF must have at 
least one father node. 
Fig. 1 is a complete forest while Fig. 7 is a 
non-complete forest due to the virtual node 
?VV+VV? introduced in Fig. 7. Fig. 2 is a hyper-
edge (IP => NP VP) of Fig. 1, where NP covers 
173
the phrase ?Xinhuashe?, VP covers the phrase 
?shengming youguan guiding? and IP covers the 
entire sentence. In Fig.1, only root IP has no fa-
ther node, so it is a complete forest. The two 
parse trees T1 and T2 encoded in Fig. 1 are 
shown separately in Fig. 3 and Fig. 41.  
Different parse tree represents different deri-
vations and explanations for a given sentence. 
For example, for the same input sentence in Fig. 
1, T1 interprets it as ?XNA (Xinhua News 
Agency) declares some regulations.? while T2 
interprets it as ?XNA declaration is related to 
some regulations.?.  
 
 
 
Figure 1. A packed forest for sentence ????
/Xinhuashe ?? /shengming ?? /youguan ??
/guiding? 
             
Figure 2.  A hyper-edge used in Fig. 1 
 
       
 
Figure 3. Tree 1 (T1)            Figure 4. Tree 2 (T2) 
3.2 Tree sequence in packed forest 
Similar to the definition of tree sequence used in 
a single parse tree defined in Liu et al (2007) 
and Zhang et al (2008a), a tree sequence in a 
forest also refers to an ordered sub-tree sequence 
that covers a continuous phrase without overlap-
ping. However, the major difference between 
                                                          
1 Please note that a single tree (as T1 and T2 shown in Fig. 
3 and Fig. 4) is represented by edges instead of hyper-edges. 
A hyper-edge is a group of edges satisfying the 2nd condi-
tion as shown in the forest definition. 
them lies in that the sub-trees of a tree sequence 
in forest may belongs to different single parse 
trees while, in a single parse tree-based model, 
all the sub-trees in a tree sequence are committed 
to the same parse tree.  
The forest-based tree sequence enables our 
model to have the potential of exploring addi-
tional parse trees that may be wrongly pruned out 
by the parser and thus are not encoded in the for-
est. This is because that a tree sequence in a for-
est allows its sub-trees coming from different 
parse trees, where these sub-trees may not be 
merged finally to form a complete parse tree in 
the forest. Take the forest in Fig. 1 as an exam-
ple, where ((VV shengming) (JJ youguan)) is a 
tree sequence that all sub-trees appear in T1 
while ((VV shengming) (VV youguan)) is a tree 
sequence whose sub-trees do not belong to any 
single tree in the forest. But, indeed the two sub-
trees (VV shengming) and (VV youguan) can be 
merged together and further lead to a complete 
single parse tree which may offer a correct inter-
pretation to the input sentence (as shown in Fig. 
5). In addition, please note that, on the other 
hand, more parse trees may introduce more noisy 
structures. In this paper, we leave this problem to 
our model and let the model decide which sub-
structures are noisy features. 
 
          
 
 Figure 5. A parse tree that was wrongly 
pruned out 
 
            
    Figure 6. A tree sequence to string rule 
 
174
A tree-sequence to string translation rule in a 
forest is a triple <L, R, A>, where L is the tree 
sequence in source language, R is the string con-
taining words and variables in target language, 
and A is the alignment between the leaf nodes of 
L and R. This definition is similar to that of (Liu 
et al 2007, Zhang et al 2008a) except our tree-
sequence is defined in forest. The shaded area of 
Fig. 6 exemplifies a tree sequence to string trans-
lation rule in the forest.  
3.3 Forest-based tree-sequence to string 
translation model 
Given a source forest F and target translation TS 
as well as word alignment A, our translation 
model is formulated as: 
  
 Pr??, ??, ?? ? ? ? ????????????? ?,???????, ??,??  
 
By the above Eq., translation becomes a tree 
sequence structure to string mapping issue. Giv-
en the F, TS and A, there are multiple derivations 
that could map F to TS under the constraint A. 
The mapping probability Pr??, ??, ??  in our 
study is obtained by summing over the probabili-
ties of all derivations ?. The probability of each 
derivation ?? is given as the product of the prob-
abilities of all the rules ( )ip r  used in the deriva-
tion (here we assume that each rule is applied 
independently in a derivation). 
Our model is implemented under log-linear 
framework (Och and Ney, 2002). We use seven 
basic features that are analogous to the common-
ly used features in phrase-based systems (Koehn, 
2003): 1) bidirectional rule mapping probabilities, 
2) bidirectional lexical rule translation probabili-
ties, 3) target language model, 4) number of rules 
used and 5) number of target words. In addition, 
we define two new features: 1) number of leaf 
nodes in auxiliary rules (the auxiliary rule will be 
explained later in this paper) and 2) product of 
the probabilities of all hyper-edges of the tree 
sequences in forest. 
4 Training  
This section discusses how to extract our transla-
tion rules given a triple ? ?, ??, ? ? . As we 
know, the traditional tree-to-string rules can be 
easily extracted from ? ?, ??, ? ? using the algo-
rithm of Mi and Huang (2008)2. We would like 
                                                          
2 Mi and Huang (2008) extend the tree-based rule extraction 
algorithm (Galley et al, 2004) to forest-based by introduc-
ing non-deterministic mechanism. Their algorithm consists 
of two steps, minimal rule extraction and composed rule 
generation. 
to leverage on their algorithm in our study. Un-
fortunately, their algorithm is not directly appli-
cable to our problem because tree rules have only 
one root while tree sequence rules have multiple 
roots. This makes the tree sequence rule extrac-
tion very complex due to its interaction with for-
est structure. To address this issue, we introduce 
the concepts of virtual node and virtual hyper-
edge to convert a complete parse forest ?  to a 
non-complete forest ? which is designed to en-
code all the tree sequences that we want. There-
fore, by doing so, the tree sequence rules can be 
extracted from a forest in the following two 
steps: 
1) Convert the complete parse forest ? into a 
non-complete forest ?  in order to cover those 
tree sequences that cannot be covered by a single 
tree node. 
2) Employ the forest-based tree rule extraction 
algorithm (Mi and Huang, 2008) to extract our 
rules from the non-complete forest. 
To facilitate our discussion, here we introduce 
two notations:  
? Alignable: A consecutive source phrase is 
an alignable phrase if and only if it can be 
aligned with at least one consecutive target 
phrase under the word-alignment con-
straint. The covered source span is called 
alignable span. 
? Node sequence: a sequence of nodes (ei-
ther leaf or internal nodes) in a forest cov-
ering a consecutive span. 
Algorithm 1 illustrates the first step of our rule 
extraction algorithm, which is a CKY-style Dy-
namic Programming (DP) algorithm to add vir-
tual nodes into forest. It includes the following 
steps: 
1) We traverse the forest to visit each span in 
bottom-up fashion (line 1-2), 
1.1) for each span [u,v] that is covered by 
single tree nodes3, we put these tree 
nodes into the set NSS(u,v) and go 
back to step 1 (line 4-6). 
1.2) otherwise we concatenate the tree se-
quences of sub-spans to generate the 
set of tree sequences covering the cur-
rent larger span (line 8-13). Then, we 
prune the set of node sequences (line 
14). If this span is alignable, we 
create virtual father nodes and corres-
ponding virtual hyper-edges to link 
the node sequences with the virtual 
father nodes (line 15-20). 
                                                          
3 Note that in a forest, there would be multiple single tree 
nodes covering the same span as shown Fig.1.  
175
2) Finally we obtain a forest with each align-
able span covered by either original tree 
nodes or the newly-created tree sequence 
virtual nodes. 
Theoretically, there is exponential number of 
node sequences in a forest. Take Fig. 7 as an ex-
ample. The NSS of span [1,2] only contains ?NP? 
since it is alignable and covered by the single 
tree node NP. However, span [2,3] cannot be 
covered by any single tree node, so we have to 
create the NSS of span[2,3] by concatenating the 
NSSs of span [2,2] and span [3,3]. Since NSS of 
span [2,2] contains 4 element {?NN?, ?NP?, 
?VV?, ?VP?} and NSS of span [3, 3] also con-
tains 4 element {?VV?, ?VP?, ?JJ?, ?ADJP?}, 
NSS of span [2,3] contains 16=4*4 elements. To 
make the NSS manageable, we prune it with the 
following thresholds: 
? each node sequence should contain less 
than n nodes 
? each node sequence set should contain less 
than m node sequences 
? sort node sequences according to their 
lengths and only keep the k shortest ones 
Each virtual node is simply labeled by the 
concatenation of all its children?s labels as 
shown in Fig. 7. 
 
Algorithm 1. add virtual nodes into forest 
Input: packed forest F, alignment A 
Notation:  
   L: length of source sentence 
   NSS(u,v): the set of node sequences covering span [u,v] 
  VN(ns): virtual father node for node sequence ns. 
Output: modified forest F with virtual nodes 
 
 
1. for length := 0 to L - 1 do 
2.      for start := 1 to L - length do 
3.          stop := start + length 
4.          if span[start, stop] covered by tree nodes then 
5.                for each node n of span [start, stop] do 
6.                    add n into NSS(start, stop) 
7.          else  
8.                for pivot := start to stop - 1 
9.                     for each ns1 in NSS(start, pivot) do 
10.                          for each ns2 in NSS(pivot+1, stop) do 
11.                               create ?? ?? ?1? ?  ?2?  
12.                                if ns is not in NSS(start, stop) then 
13.                                      add ns into NSS(start, stop) 
14.                do pruning on NSS(start, stop) 
15.                if the span[start, stop] is alignable then 
16.                    for each ns of NSS(start, stop) do 
17.                   if node VN(ns) is not in F then 
18.                                add node VN(ns) into F 
19.                          add a hyper-edge h into F,  
20.                          let lhs(h) := VN(ns), rhs(h) := ns 
 
Algorithm 1 outputs a non-complete forest CF 
with each alignable span covered by either tree 
nodes or virtual nodes. Then we can easily ex-
tract our rules from the CF using the tree rule 
extraction algorithm (Mi and Huang, 2008). 
Finally, to calculate rule feature probabilities 
for our model, we need to calculate the fractional 
counts (it is a kind of probability defined in Mi 
and Huang, 2008) of each translation rule in a 
parse forest. In the tree case, we can use the in-
side-outside-based methods (Mi and Huang 
2008) to do it. In the tree sequence case, since 
the previous method cannot be used directly, we 
provide another solution by making an indepen-
dent assumption that each tree in a tree sequence 
is independent to each other. With this assump-
tion, the fractional counts of both tree and tree 
sequence can be calculated as follows: 
 
???? ? ?????????????????   
 
???????? ? ? ????
????????????
? ? ????
??????
? ? ????
??????????????
 
 
where ???? is the fractional counts to be calcu-
lated for rule r, a frag is either lhs(r) (excluding 
virtual nodes and virtual hyper-edges) or any tree 
node in a forest, TOP is the root of the forest, 
??. ? and ??.) are the outside and inside probabil-
ities of nodes, ?????. ? returns the root nodes of a 
tree sequence fragment, ???????. ?  returns the 
leaf nodes of a tree sequence fragment, ???? is 
the hyper-edge probability. 
 
 
 
              Figure 7. A virtual node in forest 
5 Decoding  
We benefit from the same strategy as used in our 
rule extraction algorithm in designing our decod-
ing algorithm, recasting the forest-based tree se-
quence-to-string decoding problem as a forest-
based tree-to-string decoding problem. Our de-
coding algorithm consists of four steps: 
1) Convert the complete parse forest to a non-
complete one by introducing virtual nodes. 
176
2) Convert the non-complete parse forest into 
a translation forest4 ?? by using the translation 
rules and the pattern-matching algorithm pre-
sented in Mi et al (2008). 
3) Prune out redundant nodes and add auxil-
iary hyper-edge into the translation forest for 
those nodes that have either no child or no father. 
By this step, the translation forest ?? becomes a 
complete forest.  
4) Decode the translation forest using our 
translation model and a dynamic search algo-
rithm. 
The process of step 1 is similar to Algorithm 1 
except no alignment constraint used here. This 
may generate a large number of additional virtual 
nodes; however, all redundant nodes will be fil-
tered out in step 3. In step 2, we employ the tree-
to-string pattern match algorithm (Mi et al, 
2008) to convert a parse forest to a translation 
forest. In step 3, all those nodes not covered by 
any translation rules are removed. In addition, 
please note that the translation forest is already 
not a complete forest due to the virtual nodes and 
the pruning of rule-unmatchable nodes. We, 
therefore, propose Algorithm 2 to add auxiliary 
hyper-edges to make the translation forest com-
plete.  
In Algorithm 2, we travel the forest in bottom-
up fashion (line 4-5). For each span, we do: 
1) generate all the NSS for this span (line 7-12)  
2) filter the NSS to a manageable size (line 13) 
3) add auxiliary hyper-edges for the current 
span (line 15-19) if it can be covered by at least 
one single tree node, otherwise go to step 1 . This 
is the key step in our Algorithm 2. For each tree 
node and each node sequences covering the same 
span (stored in the current NSS), if the tree node 
has no children or at least one node in the node 
sequence has no father, we add an auxiliary hy-
per-edge to connect the tree node as father node 
with the node sequence as children. Since Algo-
rithm 2 is DP-based and traverses the forest in a 
bottom-up way, all the nodes in a node sequence 
should already have children node after the lower 
level process in a small span. Finally, we re-build 
the NSS of current span for upper level NSS 
combination use (line 20-22). 
 
 In Fig. 8, the hyper-edge ?IP=>NP VV+VV 
NP? is an auxiliary hyper-edge introduced by 
Algorithm 2. By Algorithm 2, we convert the 
translation forest into a complete translation for-
est. We then use a bottom-up node-based search 
                                                          
4 The concept of translation forest is proposed in Mi et 
al. (2008). It is a forest that consists of only the hyper-
edges induced from translation rules. 
algorithm to do decoding on the complete trans-
lation forest. We also use Cube Pruning algo-
rithm (Huang and Chiang 2007) to speed up the 
translation process. 
 
 
 
Figure 8. Auxiliary hyper-edge in a translation 
forest 
 
Algorithm 2. add auxiliary hyper-edges into mt forest F 
Input:  mt forest F 
Output: complete forest F with auxiliary hyper-edges 
 
1. for i := 1 to L do 
2.      for each node n of span [i, i] do 
3.          add n into NSS(i, i) 
4. for length := 1 to L - 1 do 
5.      for start := 1 to L - length do 
6.          stop := start + length 
7.          for pivot := start to stop-1 do 
8.               for each ns1 in NSS (start, pivot) do 
9.                    for each ns2 in NSS (pivot+1,stop) do 
10.                 create ?? ?? ?1? ?  ?2? 
11.                          if ns is not in NSS(start, stop) then 
12.                                add ns into NSS (start, stop) 
13.           do pruning on NSS(start, stop) 
14.           if there is tree node cover span [start, stop] then 
15.         for each tree node n of span [start,stop] do 
16.                      for each ns of NSS(start, stop) do 
17.                     if node n have no children or  
there is node in ns with no father  
then 
18.                                add auxiliary hyper-edge h into F 
19.                                let lhs(h) := n, rhs(h) := ns 
20.          empty NSS(start, stop) 
21.          for each node n of span [start, stop] do 
22.                 add n into NSS(start, stop) 
6 Experiment 
6.1 Experimental Settings 
We evaluate our method on Chinese-English 
translation task. We use the FBIS corpus as train-
ing set, the NIST MT-2002 test set as develop-
ment (dev) set and the NIST MT-2003 test set as 
test set. We train Charniak?s parser (Charniak 
2000) on CTB5 to do Chinese parsing, and modi-
fy it to output packed forest. We tune the parser 
on section 301-325 and test it on section 271-
300. The F-measure on all sentences is 80.85%. 
A 3-gram language model is trained on the Xin-
177
hua portion of the English Gigaword3 corpus and 
the target side of the FBIS corpus using the 
SRILM Toolkits (Stolcke, 2002) with modified 
Kneser-Ney smoothing (Kenser and Ney, 1995). 
GIZA++ (Och and Ney, 2003) and the heuristics 
?grow-diag-final-and? are used to generate m-to-
n word alignments. For the MER training (Och, 
2003), Koehn?s MER trainer (Koehn, 2007) is 
modified for our system. For significance test, 
we use Zhang et al?s implementation (Zhang et 
al, 2004). Our evaluation metrics is case-
sensitive BLEU-4 (Papineni et al, 2002). 
For parse forest pruning (Mi et al, 2008), we 
utilize the Margin-based pruning algorithm pre-
sented in (Huang, 2008). Different from Mi et al 
(2008) that use a static pruning threshold, our 
threshold is sentence-depended. For each sen-
tence, we compute the Margin between the n-th 
best and the top 1 parse tree, then use the Mar-
gin-based pruning algorithm presented in 
(Huang, 2008) to do pruning. By doing so, we 
can guarantee to use at least all the top n best 
parse trees in the forest. However, please note 
that even after pruning there is still exponential 
number of additional trees embedded in the for-
est because of the sharing structure of forest. 
Other parameters are set as follows: maximum 
number of roots in a tree sequence is 3, maxi-
mum height of a translation rule is 3, maximum 
number of leaf nodes is 7, maximum number of 
node sequences on each span is 10, and maxi-
mum number of rules extracted from one node is 
10000. 
6.2 Experimental Results 
We implement our proposed methods as a gen-
eral, configurable platform for syntax-based 
SMT study. Based on this platform, we are able 
to easily implement most of the state-of-the-art 
syntax-based x-to-string SMT methods via sim-
ple parameter setting. For training, we set forest 
pruning threshold to 1 best for tree-based me-
thods and 100 best for forest-based methods. For 
decoding, we set: 
1) TT2S: tree-based tree-to-string model by 
setting the forest pruning threshold to 1 best and 
the number of sub-trees in a tree sequence to 1. 
2) TTS2S: tree-based tree-sequence to string 
system by setting the forest pruning threshold to 
1 best and the maximum number of sub-trees in a 
tree sequence to 3. 
3) FT2S: forest-based tree-to-string system by 
setting the forest pruning threshold to 500 best, 
the number of sub-trees in a tree sequence to 1. 
4) FTS2S: forest-based tree-sequence to string 
system by setting the forest pruning threshold to 
500 best and the maximum number of sub-trees 
in a tree sequence to 3. 
 
Model BLEU(%) 
Moses 25.68 
TT2S 26.08 
TTS2S 26.95 
FT2S 27.66 
FTS2S 28.83 
 
Table 1. Performance Comparison 
 
We use the first three syntax-based systems 
(TT2S, TTS2S, FT2S) and Moses (Koehn et al, 
2007), the state-of-the-art phrase-based system, 
as our baseline systems. Table 1 compares the 
performance of the five methods, all of which are 
fine-tuned.  It shows that: 
1) FTS2S significantly outperforms (p<0.05) 
FT2S. This shows that tree sequence is very use-
ful to forest-based model. Although a forest can 
cover much more phrases than a single tree does, 
there are still many non-syntactic phrases that 
cannot be captured by a forest due to structure 
divergence issue. On the other hand, tree se-
quence is a good solution to non-syntactic trans-
lation equivalence modeling. This is mainly be-
cause tree sequence rules are only sensitive to 
word alignment while tree rules, even extracted 
from a forest (like in FT2S), are also limited by 
syntax according to grammar parsing rules. 
2) FTS2S shows significant performance im-
provement (p<0.05) over TTS2S due to the con-
tribution of forest. This is mainly due to the fact 
that forest can offer very large number of parse 
trees for rule extraction and decoder. 
3) Our model statistically significantly outper-
forms all the baselines system. This clearly de-
monstrates the effectiveness of our proposed 
model for syntax-based SMT. It also shows that 
the forest-based method and tree sequence-based 
method are complementary to each other and our 
proposed method is able to effectively integrate 
their strengths. 
4) All the four syntax-based systems show bet-
ter performance than Moses and three of them 
significantly outperforms (p<0.05) Moses. This 
suggests that syntax is very useful to SMT and 
translation can be viewed as a structure mapping 
issue as done in the four syntax-based systems. 
Table 2 and Table 3 report the distribution of 
different kinds of translation rules in our model 
(training forest pruning threshold is set to 100 
best) and in our decoding (decoding forest prun-
ing threshold is set to 500 best) for one best 
translation generation. From the two tables, we 
can find that: 
178
Rule Type Tree 
to String 
Tree Sequence 
to String 
L 4,854,406 20,526,674 
P 37,360,684 58,826,261 
U 3,297,302 3,775,734 
All 45,512,392 83,128,669 
 
Table 2. # of rules extracted from training cor-
pus. L means fully lexicalized, P means partially 
lexicalized, U means unlexicalized. 
 
Rule Type Tree 
to String 
Tree Sequence 
to String 
L 10,592 1,161 
P 7,132 742 
U 4,874 278 
All 22,598 2,181 
 
Table 3. # of rules used to generate one-best 
translation result in testing 
 
1) In Table 2, the number of tree sequence 
rules is much larger than that of tree rules al-
though our rule extraction algorithm only ex-
tracts those tree sequence rules over the spans 
that tree rules cannot cover. This suggests that 
the non-syntactic structure mapping is still a big 
challenge to syntax-based SMT. 
2) Table 3 shows that the tree sequence rules 
is around 9% of the tree rules when generating 
the one-best translation. This suggests that 
around 9% of translation equivalences in the test 
set can be better modeled by tree sequence to 
string rules than by tree to string rules. The 9% 
tree sequence rules contribute 1.17 BLEU score 
improvement (28.83-27.66 in Table 1) to FTS2S 
over FT2S.  
3) In Table 3, the fully-lexicalized rules are 
the major part (around 60%), followed by the 
partially-lexicalized (around 35%) and un-
lexicalized (around 15%). However, in Table 2, 
partially-lexicalized rules extracted from training 
corpus are the major part (more than 70%). This 
suggests that most partially-lexicalized rules are 
less effective in our model. This clearly directs 
our future work in model optimization. 
 
BLEU (%)    
N-best \ model FT2S FTS2S 
100 Best 27.40 28.61 
500 Best  27.66 28.83 
2500 Best  27.66 28.96 
5000 Best  27.79 28.89 
 
Table 4. Impact of the forest pruning  
 
Forest pruning is a key step for forest-based 
method. Table 4 reports the performance of the 
two forest-based models using different values of 
the forest pruning threshold for decoding. It 
shows that: 
1) FTS2S significantly outperforms (p<0.05) 
FT2S consistently in all test cases. This again 
demonstrates the effectiveness of our proposed 
model. Even if in the 5000 Best case, tree se-
quence is still able to contribute 1.1 BLEU score 
improvement (28.89-27.79). It indicates the ad-
vantage of tree sequence cannot be covered by 
forest even if we utilize a very large forest.  
2) The BLEU scores are very similar to each 
other when we increase the forest pruning thre-
shold. Moreover, in one case the performance 
even drops. This suggests that although more 
parse trees in a forest can offer more structure 
information, they may also introduce more noise 
that may confuse the decoder. 
7 Conclusion   
In this paper, we propose a forest-based tree-
sequence to string translation model to combine 
the strengths of forest-based methods and tree-
sequence based methods. This enables our model 
to have the great potential to address the issues 
of structure divergence and parse errors for syn-
tax-based SMT. We convert our forest-based tree 
sequence rule extraction and decoding issues to 
tree-based by introducing virtual nodes, virtual 
hyper-edges and auxiliary rules (hyper-edges). In 
our system implementation, we design a general 
and configurable platform for our method, based 
on which we can easily realize many previous 
syntax-based methods. Finally, we examine our 
methods on the FBIS corpus and the NIST MT-
2003 Chinese-English translation task. Experi-
mental results show that our model greatly out-
performs the four baseline systems. Our study 
demonstrates that forest-based method and tree 
sequence-based method are complementary to 
each other and our proposed method is able to 
effectively combine the strengths of the two in-
dividual methods for syntax-based SMT. 
Acknowledgement  
We would like to thank Huang Yun for preparing 
the pictures in this paper; Run Yan for providing 
the java version modified MERT program and 
discussion on the details of MOSES; Mi Haitao 
for his help and discussion on re-implementing 
the FT2S model; Sun Jun and Xiong Deyi for 
their valuable suggestions. 
179
References  
Eugene Charniak. 2000. A maximum-entropy inspired 
parser. NAACL-00. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for MT. ACL-03 (companion volume). 
Michel Galley, Mark Hopkins, Kevin Knight and Da-
niel Marcu. 2004. What?s in a translation rule? 
HLT-NAACL-04. 273-280. 
Liang Huang. 2008. Forest Reranking: Discriminative 
Parsing with Non-Local Features. ACL-HLT-08. 
586-594 
Liang Huang and David Chiang. 2005. Better k-best 
Parsing. IWPT-05. 
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language 
models. ACL-07. 144?151 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06. (poster) 
Reinhard Kenser and Hermann Ney. 1995. Improved 
backing-off for M-gram language modeling. 
ICASSP-95. 181-184 
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. IWPT-2001. 
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statis-
tical phrase-based translation. HLT-NAACL-03. 
127-133. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07. 177-180. (poster) 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation 
Rules. ACL-07. 704-711. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. ACL-HLT-08. 192-199. 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. EMNLP-08. 206-214. 
Franz J. Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statis-
tical machine translation. ACL-02. 295-302. 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Syste-
matic Comparison of Various Statistical Alignment 
Models. Computational Linguistics. 29(1) 19-51.  
Kishore Papineni, Salim Roukos, ToddWard and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. ACL-02. 311-
318. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A Tree-to-Tree 
Alignment-based Model for Statistical Machine 
Translation. MT-Summit-07. 535-542. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan, Sheng Li. 2008a. A Tree Sequence 
Alignment-based Tree-to-Tree Translation Model. 
ACL-HLT-08. 559-567. 
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, 
Sheng Li. 2008b. Grammar Comparison Study for 
Translational Equivalence Modeling and Statistic-
al Machine Translation. COLING-08. 1097-1104. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? 
LREC-04. 2051-2054. 
 
180
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 315?323,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Syntax-Driven Bracketing Model for Phrase-Based Translation
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 South Connexis, Singapore 138632
{dyxiong, mzhang, aaiti, hli}@i2r.a-star.edu.sg
Abstract
Syntactic analysis influences the way in
which the source sentence is translated.
Previous efforts add syntactic constraints
to phrase-based translation by directly
rewarding/punishing a hypothesis when-
ever it matches/violates source-side con-
stituents. We present a new model that
automatically learns syntactic constraints,
including but not limited to constituent
matching/violation, from training corpus.
The model brackets a source phrase as
to whether it satisfies the learnt syntac-
tic constraints. The bracketed phrases are
then translated as a whole unit by the de-
coder. Experimental results and analy-
sis show that the new model outperforms
other previous methods and achieves a
substantial improvement over the baseline
which is not syntactically informed.
1 Introduction
The phrase-based approach is widely adopted in
statistical machine translation (SMT). It segments
a source sentence into a sequence of phrases, then
translates and reorder these phrases in the target.
In such a process, original phrase-based decod-
ing (Koehn et al, 2003) does not take advan-
tage of any linguistic analysis, which, however,
is broadly used in rule-based approaches. Since
it is not linguistically motivated, original phrase-
based decoding might produce ungrammatical or
even wrong translations. Consider the following
Chinese fragment with its parse tree:
Src: [? [[7? 11?]NP [?? [? [?? ?]NP
]PP ]VP ]IP ]VP
Ref: established July 11 as Sailing Festival day
Output: [to/? [?[set up/?? [for/? naviga-
tion/??]] on July 11/7?11?? knots/?]]
The output is generated from a phrase-based sys-
tem which does not involve any syntactic analy-
sis. Here we use ?[]? (straight orientation) and
???? (inverted orientation) to denote the common
structure of the source fragment and its transla-
tion found by the decoder. We can observe that
the decoder inadequately breaks up the second NP
phrase and translates the two words ???? and
??? separately. However, the parse tree of the
source fragment constrains the phrase ??? ??
to be translated as a unit.
Without considering syntactic constraints from
the parse tree, the decoder makes wrong decisions
not only on phrase movement but also on the lex-
ical selection for the multi-meaning word ???1.
To avert such errors, the decoder can fully respect
linguistic structures by only allowing syntactic
constituent translations and reorderings. This, un-
fortunately, significantly jeopardizes performance
(Koehn et al, 2003; Xiong et al, 2008) because by
integrating syntactic constraint into decoding as a
hard constraint, it simply prohibits any other use-
ful non-syntactic translations which violate con-
stituent boundaries.
To better leverage syntactic constraint yet still
allow non-syntactic translations, Chiang (2005)
introduces a count for each hypothesis and ac-
cumulates it whenever the hypothesis exactly
matches syntactic boundaries on the source side.
On the contrary, Marton and Resnik (2008) and
Cherry (2008) accumulate a count whenever hy-
potheses violate constituent boundaries. These
constituent matching/violation counts are used as
a feature in the decoder?s log-linear model and
their weights are tuned via minimal error rate
training (MERT) (Och, 2003). In this way, syn-
tactic constraint is integrated into decoding as a
soft constraint to enable the decoder to reward hy-
potheses that respect syntactic analyses or to pe-
1This word can be translated into ?section?, ?festival?,
and ?knot? in different contexts.
315
nalize hypotheses that violate syntactic structures.
Although experiments show that this con-
stituent matching/violation counting feature
achieves significant improvements on various
language-pairs, one issue is that matching syn-
tactic analysis can not always guarantee a good
translation, and violating syntactic structure does
not always induce a bad translation. Marton and
Resnik (2008) find that some constituency types
favor matching the source parse while others
encourage violations. Therefore it is necessary to
integrate more syntactic constraints into phrase
translation, not just the constraint of constituent
matching/violation.
The other issue is that during decoding we are
more concerned with the question of phrase co-
hesion, i.e. whether the current phrase can be
translated as a unit or not within particular syntac-
tic contexts (Fox, 2002)2, than that of constituent
matching/violation. Phrase cohesion is one of
the main reasons that we introduce syntactic con-
straints (Cherry, 2008). If a source phrase remains
contiguous after translation, we refer this type of
phrase bracketable, otherwise unbracketable. It
is more desirable to translate a bracketable phrase
than an unbracketable one.
In this paper, we propose a syntax-driven brack-
eting (SDB) model to predict whether a phrase
(a sequence of contiguous words) is bracketable
or not using rich syntactic constraints. We parse
the source language sentences in the word-aligned
training corpus. According to the word align-
ments, we define bracketable and unbracketable
instances. For each of these instances, we auto-
matically extract relevant syntactic features from
the source parse tree as bracketing evidences.
Then we tune the weights of these features us-
ing a maximum entropy (ME) trainer. In this way,
we build two bracketing models: 1) a unary SDB
model (UniSDB) which predicts whether an inde-
pendent phrase is bracketable or not; and 2) a bi-
nary SDB model(BiSDB) which predicts whether
two neighboring phrases are bracketable. Similar
to previous methods, our SDB model is integrated
into the decoder?s log-linear model as a feature so
that we can inherit the idea of soft constraints.
In contrast to the constituent matching/violation
counting (CMVC) (Chiang, 2005; Marton and
Resnik, 2008; Cherry, 2008), our SDB model has
2Here we expand the definition of phrase to include both
syntactic and non-syntactic phrases.
the following advantages
? The SDB model automatically learns syntac-
tic constraints from training data while the
CMVC uses manually defined syntactic con-
straints: constituency matching/violation. In
our SDB model, each learned syntactic fea-
ture from bracketing instances can be consid-
ered as a syntactic constraint. Therefore we
can use thousands of syntactic constraints to
guide phrase translation.
? The SDB model maintains and protects the
strength of the phrase-based approach in a
better way than the CMVC does. It is able to
reward non-syntactic translations by assign-
ing an adequate probability to them if these
translations are appropriate to particular syn-
tactic contexts on the source side, rather than
always punish them.
We test our SDB model against the baseline
which doest not use any syntactic constraints on
Chinese-to-English translation. To compare with
the CMVC, we also conduct experiments using
(Marton and Resnik, 2008)?s XP+. The XP+ ac-
cumulates a count for each hypothesis whenever
it violates the boundaries of a constituent with a
label from {NP, VP, CP, IP, PP, ADVP, QP, LCP,
DNP}. The XP+ is the best feature among all fea-
tures that Marton and Resnik use for Chinese-to-
English translation. Our experimental results dis-
play that our SDB model achieves a substantial
improvement over the baseline and significantly
outperforms XP+ according to the BLEU metric
(Papineni et al, 2002). In addition, our analysis
shows further evidences of the performance gain
from a different perspective than that of BLEU.
The paper proceeds as follows. In section 2 we
describe how to learn bracketing instances from
a training corpus. In section 3 we elaborate the
syntax-driven bracketing model, including feature
generation and the integration of the SDB model
into phrase-based SMT. In section 4 and 5, we
present our experiments and analysis. And we fi-
nally conclude in section 6.
2 The Acquisition of Bracketing
Instances
In this section, we formally define the bracket-
ing instance, comprising two types namely binary
bracketing instance and unary bracketing instance.
316
We present an algorithm to automatically ex-
tract these bracketing instances from word-aligned
bilingual corpus where the source language sen-
tences are parsed.
Let c and e be the source sentence and the
target sentence, W be the word alignment be-
tween them, T be the parse tree of c. We
define a binary bracketing instance as a tu-
ple ?b, ?(ci..j), ?(cj+1..k), ?(ci..k)? where b ?
{bracketable, unbracketable}, ci..j and cj+1..k
are two neighboring source phrases and ?(T, s)
(?(s) for short) is a subtree function which returns
the minimal subtree covering the source sequence
s from the source parse tree T . Note that ?(ci..k)
includes both ?(ci..j) and ?(cj+1..k). For the two
neighboring source phrases, the following condi-
tions are satisfied:
?eu..v, ep..q ? e s.t.
?(m,n) ? W, i ? m ? j ? u ? n ? v (1)
?(m,n) ? W, j + 1 ? m ? k ? p ? n ? q (2)
The above (1) means that there exists a target
phrase eu..v aligned to ci..j and (2) denotes a tar-
get phrase ep..q aligned to cj+1..k. If eu..v and
ep..q are neighboring to each other or all words be-
tween the two phrases are aligned to null, we set
b = bracketable, otherwise b = unbracketable.
From a binary bracketing instance, we derive a
unary bracketing instance ?b, ?(ci..k)?, ignoring
the subtrees ?(ci..j) and ?(cj+1..k).
Let n be the number of words of c. If we ex-
tract all potential bracketing instances, there will
be o(n2) unary instances and o(n3) binary in-
stances. To keep the number of bracketing in-
stances tractable, we only record 4 representa-
tive bracketing instances for each index j: 1) the
bracketable instance with the minimal ?(ci..k), 2)
the bracketable instance with the maximal ?(ci..k),
3) the unbracketable instance with the minimal
?(ci..k), and 4) the unbracketable instance with the
maximal ?(ci..k).
Figure 1 shows the algorithm to extract brack-
eting instances. Line 3-11 find all potential brack-
eting instances for each (i, j, k) ? c but only keep
4 bracketing instances for each index j: two min-
imal and two maximal instances. This algorithm
learns binary bracketing instances, from which we
can derive unary bracketing instances.
1: Input: sentence pair (c, e), the parse tree T of c and the
word alignment W between c and e
2: < := ?
3: for each (i, j, k) ? c do
4: if There exist a target phrase eu..v aligned to ci..j and
ep..q aligned to cj+1..k then
5: Get ?(ci..j), ?(cj+1..k), and ?(ci..k)
6: Determine b according to the relationship between
eu..v and ep..q
7: if ?(ci..k) is currently maximal or minimal then
8: Update bracketing instances for index j
9: end if
10: end if
11: end for
12: for each j ? c do
13: < := < ? {bracketing instances from j}
14: end for
15: Output: bracketing instances <
Figure 1: Bracketing Instances Extraction Algo-
rithm.
3 The Syntax-Driven Bracketing Model
3.1 The Model
Our interest is to automatically detect phrase
bracketing using rich contextual information. We
consider this task as a binary-class classification
problem: whether the current source phrase s is
bracketable (b) within particular syntactic contexts
(?(s)). If two neighboring sub-phrases s1 and s2
are given, we can use more inner syntactic con-
texts to complete this binary classification task.
We construct the syntax-driven bracketing
model within the maximum entropy framework. A
unary SDB model is defined as:
PUniSDB(b|?(s), T ) =
exp(?i ?ihi(b, ?(s), T )?
b exp(
?
i ?ihi(b, ?(s), T )
(3)
where hi ? {0, 1} is a binary feature function
which we will describe in the next subsection, and
?i is the weight of hi. Similarly, a binary SDB
model is defined as:
PBiSDB(b|?(s1), ?(s2), ?(s), T ) =
exp(?i ?ihi(b, ?(s1), ?(s2), ?(s), T )?
b exp(
?
i ?ihi(b, ?(s1), ?(s2), ?(s), T )
(4)
The most important advantage of ME-based
SDB model is its capacity of incorporating more
fine-grained contextual features besides the binary
feature that detects constituent boundary violation
or matching. By employing these features, we
can investigate the value of various syntactic con-
straints in phrase translation.
317
jingfang
police
yi fengsuo
block
le baozha
bomb
xianchang
scene
NN NN
NP
VP
ASVVADNN
ADVP
VP
NP
IP
s
s1 s2
Figure 2: Illustration of syntax-driven features
used in SDB. Here we only show the features for
the source phrase s. The triangle, rounded rect-
angle and rectangle denote the rule feature, path
feature and constituent boundary matching feature
respectively.
3.2 Syntax-Driven Features
Let s be the source phrase in question, s1 and s2
be the two neighboring sub-phrases. ?(.) is the
root node of ?(.). The SDB model exploits various
syntactic features as follows.
? Rule Features (RF)
We use the CFG rules of ?(s), ?(s1) and
?(s2) as features. These features capture
syntactic ?horizontal context? which demon-
strates the expansion trend of the source
phrase s, s1 and s2 on the parse tree.
In figure 2, the CFG rule ?ADVP?AD?,
?VP?VV AS NP?, and ?VP?ADVP
VP? are used as features for s1, s2 and s
respectively.
? Path Features (PF)
The tree path ?(s1)..?(s) connecting ?(s1)
and ?(s), ?(s2)..?(s) connecting ?(s2)
and ?(s), and ?(s)..? connecting ?(s) and
the root node ? of the whole parse tree are
used as features. These features provide
syntactic ?vertical context? which shows the
generation history of the source phrases on
the parse tree.
(a) (b) (c)
Figure 3: Three scenarios of the relationship be-
tween phrase boundaries and constituent bound-
aries. The gray circles are constituent boundaries
while the black circles are phrase boundaries.
In figure 2, the path features are ?ADVP
VP?, ?VP VP? and ?VP IP? for s1, s2 and s
respectively.
? Constituent Boundary Matching Features
(CBMF)
These features are to capture the relationship
between a source phrase s and ?(s) or
?(s)?s subtrees. There are three different
scenarios3: 1) exact match, where s exactly
matches the boundaries of ?(s) (figure 3(a)),
2) inside match, where s exactly spans a
sequence of ?(s)?s subtrees (figure 3(b)), and
3) crossing, where s crosses the boundaries
of one or two subtrees of ?(s) (figure 3(c)).
In the case of 1) or 2), we set the value of
this feature to ?(s)-M or ?(s)-I respectively.
When s crosses the boundaries of the sub-
constituent ?l on s?s left, we set the value to
?(?l)-LC; If s crosses the boundaries of the
sub-constituent ?r on s?s right, we set the
value to ?(?r)-RC; If both, we set the value
to ?(?l)-LC-?(?r)-RC.
Let?s revisit the Figure 2. The source
phrase s1 exactly matches the constituent
ADVP, therefore CBMF is ?ADVP-M?. The
source phrase s2 exactly spans two sub-trees
VV and AS of VP, therefore CBMF is
?VP-I?. Finally, the source phrase s cross
boundaries of the lower VP on the right,
therefore CBMF is ?VP-RC?.
3.3 The Integration of the SDB Model into
Phrase-Based SMT
We integrate the SDB model into phrase-based
SMT to help decoder perform syntax-driven
phrase translation. In particular, we add a
3The three scenarios that we define here are similar to
those in (Lu? et al, 2002).
318
new feature into the log-linear translation model:
PSDB(b|T, ?(.)). This feature is computed by the
SDB model described in equation (3) or equation
(4), which estimates a probability that a source
span is to be translated as a unit within partic-
ular syntactic contexts. If a source span can be
translated as a unit, the feature will give a higher
probability even though this span violates bound-
aries of a constituent. Otherwise, a lower proba-
bility is given. Through this additional feature, we
want the decoder to prefer hypotheses that trans-
late source spans which can be translated as a unit,
and avoids translating those which are discontinu-
ous after translation. The weight of this new fea-
ture is tuned via MERT, which measures the extent
to which this feature should be trusted.
In this paper, we implement the SDB model in a
state-of-the-art phrase-based system which adapts
a binary bracketing transduction grammar (BTG)
(Wu, 1997) to phrase translation and reordering,
described in (Xiong et al, 2006). Whenever a
BTG merging rule (s ? [s1 s2] or s ? ?s1 s2?)
is used, the SDB model gives a probability to the
span s covered by the rule, which estimates the
extent to which the span is bracketable. For the
unary SDB model, we only consider the features
from ?(s). For the binary SDB model, we use all
features from ?(s1), ?(s2) and ?(s) since the bi-
nary SDB model is naturally suitable to the binary
BTG rules.
The SDB model, however, is not only limited
to phrase-based SMT using BTG rules. Since it
is applied on a source span each time, any other
hierarchical phrase-based or syntax-based system
that translates source spans recursively or linearly,
can adopt the SDB model.
4 Experiments
We carried out the MT experiments on Chinese-
to-English translation, using (Xiong et al, 2006)?s
system as our baseline system. We modified the
baseline decoder to incorporate our SDB mod-
els as descried in section 3.3. In order to com-
pare with Marton and Resnik?s approach, we also
adapted the baseline decoder to their XP+ feature.
4.1 Experimental Setup
In order to obtain syntactic trees for SDB models
and XP+, we parsed source sentences using a lex-
icalized PCFG parser (Xiong et al, 2005). The
parser was trained on the Penn Chinese Treebank
with an F1 score of 79.4%.
All translation models were trained on the FBIS
corpus. We removed 15,250 sentences, for which
the Chinese parser failed to produce syntactic
parse trees. To obtain word-level alignments, we
ran GIZA++ (Och and Ney, 2000) on the remain-
ing corpus in both directions, and applied the
?grow-diag-final? refinement rule (Koehn et al,
2005) to produce the final many-to-many word
alignments. We built our four-gram language
model using Xinhua section of the English Gi-
gaword corpus (181.1M words) with the SRILM
toolkit (Stolcke, 2002).
For the efficiency of MERT, we built our de-
velopment set (580 sentences) using sentences not
exceeding 50 characters from the NIST MT-02 set.
We evaluated all models on the NIST MT-05 set
using case-sensitive BLEU-4. Statistical signif-
icance in BLEU score differences was tested by
paired bootstrap re-sampling (Koehn, 2004).
4.2 SDB Training
We extracted 6.55M bracketing instances from our
training corpus using the algorithm shown in fig-
ure 1, which contains 4.67M bracketable instances
and 1.89M unbracketable instances. From ex-
tracted bracketing instances we generated syntax-
driven features, which include 73,480 rule fea-
tures, 153,614 path features and 336 constituent
boundary matching features. To tune weights of
features, we ran the MaxEnt toolkit (Zhang, 2004)
with iteration number being set to 100 and Gaus-
sian prior to 1 to avoid overfitting.
4.3 Results
We ran the MERT module with our decoders to
tune the feature weights. The values are shown
in Table 1. The PSDB receives the largest feature
weight, 0.29 for UniSDB and 0.38 for BiSDB, in-
dicating that the SDB models exert a nontrivial im-
pact on decoder.
In Table 2, we present our results. Like (Mar-
ton and Resnik, 2008), we find that the XP+ fea-
ture obtains a significant improvement of 1.08
BLEU over the baseline. However, using all
syntax-driven features described in section 3.2,
our SDB models achieve larger improvements
of up to 1.67 BLEU. The binary SDB (BiSDB)
model statistically significantly outperforms Mar-
ton and Resnik?s XP+ by an absolute improvement
of 0.59 (relatively 2%). It is also marginally better
than the unary SDB model.
319
Features
System P (c|e) P (e|c) Pw(c|e) Pw(e|c) Plm(e) Pr(e) Word Phr. XP+ PSDB
Baseline 0.041 0.030 0.006 0.065 0.20 0.35 0.19 -0.12 ? ?
XP+ 0.002 0.049 0.046 0.044 0.17 0.29 0.16 0.12 -0.12 ?
UniSDB 0.023 0.051 0.055 0.012 0.21 0.20 0.12 0.04 ? 0.29
BiSDB 0.016 0.032 0.027 0.013 0.13 0.23 0.08 0.09 ? 0.38
Table 1: Feature weights obtained by MERT on the development set. The first 4 features are the phrase
translation probabilities in both directions and the lexical translation probabilities in both directions. Plm
= language model; Pr = MaxEnt-based reordering model; Word = word bonus; Phr = phrase bonus.
BLEU-n n-gram Precision
System 4 1 2 3 4 5 6 7 8
Baseline 0.2612 0.71 0.36 0.18 0.10 0.054 0.030 0.016 0.009
XP+ 0.2720** 0.72 0.37 0.19 0.11 0.060 0.035 0.021 0.012
UniSDB 0.2762**+ 0.72 0.37 0.20 0.11 0.062 0.035 0.020 0.011
BiSDB 0.2779**++ 0.72 0.37 0.20 0.11 0.065 0.038 0.022 0.014
Table 2: Results on the test set. **: significantly better than baseline (p < 0.01). + or ++: significantly
better than Marton and Resnik?s XP+ (p < 0.05 or p < 0.01, respectively).
5 Analysis
In this section, we present analysis to perceive the
influence mechanism of the SDB model on phrase
translation by studying the effects of syntax-driven
features and differences of 1-best translation out-
puts.
5.1 Effects of Syntax-Driven Features
We conducted further experiments using individ-
ual syntax-driven features and their combinations.
Table 3 shows the results, from which we have the
following key observations.
? The constituent boundary matching feature
(CBMF) is a very important feature, which
by itself achieves significant improvement
over the baseline (up to 1.13 BLEU). Both
our CBMF and Marton and Resnik?s XP+
feature focus on the relationship between a
source phrase and a constituent. Their signifi-
cant contribution to the improvement implies
that this relationship is an important syntactic
constraint for phrase translation.
? Adding more features, such as path feature
and rule feature, achieves further improve-
ments. This demonstrates the advantage of
using more syntactic constraints in the SDB
model, compared with Marton and Resnik?s
XP+.
BLEU-4
Features UniSDB BiSDB
PF + RF 0.2555 0.2644*@@
PF 0.2596 0.2671**@@
CBMF 0.2678** 0.2725**@
RF + CBMF 0.2737** 0.2780**++@@
PF + CBMF 0.2755**+ 0.2782**++@?
RF + PF + CBMF 0.2762**+ 0.2779**++
Table 3: Results of different feature sets. * or **:
significantly better than baseline (p < 0.05 or p <
0.01, respectively). + or ++: significantly better
than XP+ (p < 0.05 or p < 0.01, respectively).
@?: almost significantly better than its UniSDB
counterpart (p < 0.075). @ or @@: significantly
better than its UniSDB counterpart (p < 0.05 or
p < 0.01, respectively).
? In most cases, the binary SDB is constantly
significantly better than the unary SDB, sug-
gesting that inner contexts are useful in pre-
dicting phrase bracketing.
5.2 Beyond BLEU
We want to further study the happenings after we
integrate the constraint feature (our SDB model
and Marton and Resnik?s XP+) into the log-linear
translation model. In particular, we want to inves-
tigate: to what extent syntactic constraints change
translation outputs? And in what direction the
changes take place? Since BLEU is not sufficient
320
System CCM Rate (%)
Baseline 43.5
XP+ 74.5
BiSDB 72.4
Table 4: Consistent constituent matching rates re-
ported on 1-best translation outputs.
to provide such insights, we introduce a new sta-
tistical metric which measures the proportion of
syntactic constituents 4 whose boundaries are con-
sistently matched by decoder during translation.
This proportion, which we call consistent con-
stituent matching (CCM) rate , reflects the ex-
tent to which the translation output respects the
source parse tree.
In order to calculate this rate, we output transla-
tion results as well as phrase alignments found by
decoders. Then for each multi-branch constituent
cji spanning from i to j on the source side, we
check the following conditions.
? If its boundaries i and j are aligned to phrase
segmentation boundaries found by decoder.
? If all target phrases inside cji ?s target span 5
are aligned to the source phrases within cji
and not to the phrases outside cji .
If both conditions are satisfied, the constituent cji
is consistently matched by decoder.
Table 4 shows the consistent constituent match-
ing rates. Without using any source-side syntac-
tic information, the baseline obtains a low CCM
rate of 43.53%, indicating that the baseline de-
coder violates the source parse tree more than it
respects the source structure. The translation out-
put described in section 1 is actually generated by
the baseline decoder, where the second NP phrase
boundaries are violated.
By integrating syntactic constraints into decod-
ing, we can see that both Marton and Resnik?s
XP+ and our SDB model achieve a significantly
higher constituent matching rate, suggesting that
they are more likely to respect the source struc-
ture. The examples in Table 5 show that the de-
coder is able to generate better translations if it is
4We only consider multi-branch constituents.
5Given a phrase alignment P = {cgf ? eqp}, if the seg-
mentation within cji defined by P is cji = cj1i1 ...c
jk
ik , and
cjrir ? evrur ? P, 1 ? r ? k, we define the target span of c
j
i
as a pair where the first element is min(eu1 ...euk ) and the
second element is max(ev1 ...evk ), similar to (Fox, 2002).
CCM Rates (%)
System <6 6-10 11-15 16-20 >20
XP+ 75.2 70.9 71.0 76.2 82.2
BiSDB 69.3 74.7 74.2 80.0 85.6
Table 6: Consistent constituent matching rates for
structures with different spans.
faithful to the source parse tree by using syntactic
constraints.
We further conducted a deep comparison of
translation outputs of BiSDB vs. XP+ with re-
gard to constituent matching and violation. We
found two significant differences that may explain
why our BiSDB outperforms XP+. First, although
the overall CCM rate of XP+ is higher than that
of BiSDB, BiSDB obtains higher CCM rates for
long-span structures than XP+ does, which are
shown in Table 6. Generally speaking, viola-
tions of long-span constituents have a more neg-
ative impact on performance than short-span vio-
lations if these violations are toxic. This explains
why BiSDB achieves relatively higher precision
improvements for higher n-grams over XP+, as
shown in Table 3.
Second, compared with XP+ that only punishes
constituent boundary violations, our SDB model
is able to encourage violations if these violations
are done on bracketable phrases. We observed in
many cases that by violating constituent bound-
aries BiSDB produces better translations than XP+
does, which on the contrary matches these bound-
aries. Still consider the example shown in section
1. The following translations are found by XP+
and BiSDB respectively.
XP+: [to/? ?[set up/?? [for the/? [naviga-
tion/?? section/?]]] on July 11/7?11??]
BiSDB: [to/? ?[[set up/?? a/?] [marine/??
festival/?]] on July 11/7?11??]
XP+ here matches all constituent boundaries while
BiSDB violates the PP constituent to translate the
non-syntactic phrase ??? ??. Table 7 shows
more examples. From these examples, we clearly
see that appropriate violations are helpful and even
necessary for generating better translations. By
allowing appropriate violations to translate non-
syntactic phrases according to particular syntac-
tic contexts, our SDB model better inherits the
strength of phrase-based approach than XP+.
321
Src: [[? [???????]NP ]PP [?? [??]NP [????]NP ]VP ]VP
Ref: show their loving hearts to people in the Indian Ocean disaster areas
Baseline: ?love/?? [for the/? ?[people/?? [to/?? [own/?? a report/??]]]? ?in/?? the Indian Ocean/?
???]?
XP+: ?[contribute/?? [its/?? [part/?? love/??]]] [for/? ?the people/?? ?in/?? the Indian Ocean/?
????]?
BiSDB: ?[[[contribute/?? its/??] part/??] love/??] [for/? ?the people/?? ?in/?? the Indian Ocean?
????]?
Src: [???? [?]ADVP [?? [[???]QP ??]NP [???]PP]VP]IP [?]PU [????...]IP
Ref: The Pentagon has dispatched 20 airplanes to South Asia, including...
Baseline: [[The Pentagon/???? has sent/???] [?[to/? [[South Asia/?? ,/?] including/????]] [20/?
? plane/???]?]]
XP+: [The Pentagon/???? [has/? [sent/?? [[20/?? planes/???] [to/? South Asia/??]]]]] [,/?
[including/????...]]
BiSDB: [The Pentagon/???? [has sent/??? [[20/?? planes/???] [to/? South Asia/??]]] [,/? [in-
cluding/????...]]
Table 5: Translation examples showing that both XP+ and BiSDB produce better translations than the
baseline, which inappropriately violates constituent boundaries (within underlined phrases).
Src: [[? [[[????????]NP [??]ADJP [??]NP]NP ?]LCP]PP ??]VP
Ref: said after a brief discussion with Powell at the US State Department
XP+: [?after/? ??[a brief/?? meeting/??] [with/? Powell/??]? [in/? the US State Department/???
??]? said/??]
BiSDB: ?said after/??? ?[a brief/?? meeting/??] ? with Powell/??? [at/? the State Department of the
United States/?????]???
Src: [? [[?? [??????]NP]VP]IP]PP [??? [??????]NP]VP
Ref: took a key step towards building future democratic politics
XP+: ?[a/? [key/??? step/???]] ?forward/?? [to/? [a/?? [future/?? political democracy/???
?]]]??
BiSDB: ?[made a/??? [key/??? step/???]] [towards establishing a/??? ?democratic politics/???
? in the future/???]?
Table 7: Translation examples showing that BiSDB produces better translations than XP+ via appropriate
violations of constituent boundaries (within double-underlined phrases).
6 Conclusion
In this paper, we presented a syntax-driven brack-
eting model that automatically learns bracketing
knowledge from training corpus. With this knowl-
edge, the model is able to predict whether source
phrases can be translated together, regardless of
matching or crossing syntactic constituents. We
integrate this model into phrase-based SMT to
increase its capacity of linguistically motivated
translation without undermining its strengths. Ex-
periments show that our model achieves substan-
tial improvements over baseline and significantly
outperforms (Marton and Resnik, 2008)?s XP+.
Compared with previous constituency feature,
our SDB model is capable of incorporating more
syntactic constraints, and rewarding necessary vi-
olations of the source parse tree. Marton and
Resnik (2008) find that their constituent con-
straints are sensitive to language pairs. In the fu-
ture work, we will use other language pairs to test
our models so that we could know whether our
method is language-independent.
References
Colin Cherry. 2008. Cohesive Phrase-based Decoding
for Statistical Machine Translation. In Proceedings
of ACL.
David Chiang. 2005. A Hierarchical Phrase-based
Model for Statistical Machine Translation. In Pro-
ceedings of ACL, pages 263?270.
David Chiang, Yuval Marton and Philip Resnik. 2008.
Online Large-Margin Training of Syntactic and
Structural Translation Features. In Proceedings of
EMNLP.
Heidi J. Fox 2002. Phrasal Cohesion and Statistical
Machine Translation. In Proceedings of EMNLP,
pages 304?311.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of HLT-NAACL.
322
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Eval-
uation. In International Workshop on Spoken Lan-
guage Translation.
Yajuan Lu?, Sheng Li, Tiezhun Zhao and Muyun Yang.
2002. Learning Chinese Bracketing Knowledge
Based on a Bilingual Language Model. In Proceed-
ings of COLING.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrase-Based Transla-
tion. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of
ACL 2000.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatically
Evaluation of Machine Translation. In Proceedings
of ACL.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
Yueliang Qian. 2005. Parsing the Penn Chinese
Treebank with Semantic Knowledge. In Proceed-
ings of IJCNLP, Jeju Island, Korea.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Max-
imum Entropy Based Phrase Reordering Model for
Statistical Machine Translation. In Proceedings of
ACL-COLING 2006.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Linguistically Annotated BTG for Statistical
Machine Translation. In Proceedings of COLING
2008.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
323
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 324?332,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Topological Ordering of Function Words
in Hierarchical Phrase-based Translation
Hendra Setiawan
1
and Min-Yen Kan
2
and Haizhou Li
3
and Philip Resnik
1
1
University of Maryland Institute for Advanced Computer Studies
2
School of Computing, National University of Singapore
3
Human Language Technology, Institute for Infocomm Research, Singapore
{hendra,resnik}@umiacs.umd.edu,
kanmy@comp.nus.edu.sg, hli@i2r.a-star.edu.sg
Abstract
Hierarchical phrase-based models are at-
tractive because they provide a consis-
tent framework within which to character-
ize both local and long-distance reorder-
ings, but they also make it difcult to
distinguish many implausible reorderings
from those that are linguistically plausi-
ble. Rather than appealing to annotation-
driven syntactic modeling, we address this
problem by observing the inuential role
of function words in determining syntac-
tic structure, and introducing soft con-
straints on function word relationships as
part of a standard log-linear hierarchi-
cal phrase-based model. Experimentation
on Chinese-English and Arabic-English
translation demonstrates that the approach
yields signicant gains in performance.
1 Introduction
Hierarchical phrase-based models (Chiang, 2005;
Chiang, 2007) offer a number of attractive bene-
ts in statistical machine translation (SMT), while
maintaining the strengths of phrase-based systems
(Koehn et al, 2003). The most important of these
is the ability to model long-distance reordering ef-
ciently. To model such a reordering, a hierar-
chical phrase-based system demands no additional
parameters, since long and short distance reorder-
ings are modeled identically using synchronous
context free grammar (SCFG) rules. The same
rule, depending on its topological ordering ? i.e.
its position in the hierarchical structure ? can af-
fect both short and long spans of text. Interest-
ingly, hierarchical phrase-based models provide
this benet without making any linguistic commit-
ments beyond the structure of the model.
However, the system's lack of linguistic com-
mitment is also responsible for one of its great-
est drawbacks. In the absence of linguistic knowl-
edge, the system models linguistic structure using
an SCFG that contains only one type of nontermi-
nal symbol
1
. As a result, the system is susceptible
to the overgeneration problem: the grammar may
suggest more reordering choices than appropriate,
and many of those choices lead to ungrammatical
translations.
Chiang (2005) hypothesized that incorrect re-
ordering choices would often correspond to hier-
archical phrases that violate syntactic boundaries
in the source language, and he explored the use
of a ?constituent feature? intended to reward the
application of hierarchical phrases which respect
source language syntactic categories. Although
this did not yield signicant improvements, Mar-
ton and Resnik (2008) and Chiang et al (2008)
extended this approach by introducing soft syn-
tactic constraints similar to the constituent feature,
but more ne-grained and sensitive to distinctions
among syntactic categories; these led to substan-
tial improvements in performance. Zollman et al
(2006) took a complementary approach, constrain-
ing the application of hierarchical rules to respect
syntactic boundaries in the target language syn-
tax. Whether the focus is on constraints from the
source language or the target language, the main
ingredient in both previous approaches is the idea
of constraining the spans of hierarchical phrases to
respect syntactic boundaries.
In this paper, we pursue a different approach
to improving reordering choices in a hierarchical
phrase-based model. Instead of biasing the model
toward hierarchical phrases whose spans respect
syntactic boundaries, we focus on the topologi-
cal ordering of phrases in the hierarchical struc-
ture. We conjecture that since incorrect reorder-
ing choices correspond to incorrect topological or-
derings, boosting the probability of correct topo-
1
In practice, one additional nonterminal symbol is used in
?glue rules?. This is not relevant in the present discussion.
324
logical ordering choices should improve the sys-
tem. Although related to previous proposals (cor-
rect topological orderings lead to correct spans
and vice versa), our proposal incorporates broader
context and is structurally more aware, since we
look at the topological ordering of a phrase relative
to other phrases, rather than modeling additional
properties of a phrase in isolation. In addition, our
proposal requires no monolingual parsing or lin-
guistically informed syntactic modeling for either
the source or target language.
The key to our approach is the observation that
we can approximate the topological ordering of
hierarchical phrases via the topological ordering
of function words. We introduce a statistical re-
ordering model that we call the pairwise domi-
nance model, which characterizes reorderings of
phrases around a pair of function words. In mod-
eling function words, our model can be viewed as
a successor to the function words-centric reorder-
ing model (Setiawan et al, 2007), expanding on
the previous approach by modeling pairs of func-
tion words rather than individual function words
in isolation.
The rest of the paper is organized as follows. In
Section 2, we briey review hierarchical phrase-
based models. In Section 3, we rst describe the
overgeneration problem in more detail with a con-
crete example, and then motivate our idea of us-
ing the topological ordering of function words to
address the problem. In Section 4, we develop
our idea by introducing the pairwise dominance
model, expressing function word relationships in
terms of what we call the the dominance predi-
cate. In Section 5, we describe an algorithm to es-
timate the parameters of the dominance predicate
from parallel text. In Sections 6 and 7, we describe
our experiments, and in Section 8, we analyze the
output of our system and lay out a possible future
direction. Section 9 discusses the relation of our
approach to prior work and Section 10 wraps up
with our conclusions.
2 Hierarchical Phrase-based System
Formally, a hierarchical phrase-based SMT sys-
tem is based on a weighted synchronous context
free grammar (SCFG) with one type of nonter-
minal symbol. Synchronous rules in hierarchical
phrase-based models take the following form:
X ? ??, ?,?? (1)
where X is the nonterminal symbol and ? and ?
are strings that contain the combination of lexical
items and nonterminals in the source and target
languages, respectively. The ? symbol indicates
that nonterminals in ? and ? are synchronized
through co-indexation; i.e., nonterminals with the
same index are aligned. Nonterminal correspon-
dences are strictly one-to-one, and in practice the
number of nonterminals on the right hand side is
constrained to at most two, which must be sepa-
rated by lexical items.
Each rule is associated with a score that is com-
puted via the following log linear formula:
w(X ? ??, ?,??) =
?
i
f?ii (2)
where fi is a feature describing one particular as-
pect of the rule and ?i is the corresponding weight
of that feature. Given e? and f? as the source
and target phrases associated with the rule, typi-
cal features used are rule's translation probability
Ptrans(f? |e?) and its inverse Ptrans(e?|f?), the lexi-
cal probability Plex(f? |e?) and its inverse Plex(e?|f?).
Systems generally also employ a word penalty, a
phrase penalty, and target language model feature.
(See (Chiang, 2005) for more detailed discussion.)
Our pairwise dominance model will be expressed
as an additional rule-level feature in the model.
Translation of a source sentence e using hier-
archical phrase-based models is formulated as a
search for the most probable derivationD? whose
source side is equal to e:
D? = argmax P (D),where source(D)=e.
D = Xi, i ? 1...|D| is a set of rules following a
certain topological ordering, indicated here by the
use of the superscript.
3 Overgeneration and Topological
Ordering of Function Words
The use of only one type of nonterminal allows a
exible permutation of the topological ordering of
the same set of rules, resulting in a huge number of
possible derivations from a given source sentence.
In that respect, the overgeneration problem is not
new to SMT: Bracketing Transduction Grammar
(BTG) (Wu, 1997) uses a single type of nontermi-
nal and is subject to overgeneration problems, as
well.
2
2
Note, however, that overgeneration in BTG can be
viewed as a feature, not a bug, since the formalism was origi-
325
The problem may be less severe in hierarchi-
cal phrase-based MT than in BTG, since lexical
items on the rules' right hand sides often limit the
span of nonterminals. Nonetheless overgeneration
of reorderings is still problematic, as we illustrate
using the hypothetical Chinese-to-English exam-
ple in Fig. 1.
Suppose we want to translate the Chinese sen-
tence in Fig. 1 into English using the following set
of rules:
1. Xa ? ??Z X1, computers andX1?
2. Xb ? ?X14 X2, X1 are X2?
3. Xc ? ?C? , cell phones ?
4. Xd ? ?X1{? , inventions of X1?
5. Xe ? ???- , the last century ?
Co-indexation of nonterminals on the right hand
side is indicated by subscripts, and for our ex-
amples the label of the nonterminal on the left
hand side is used as the rule's unique identier.
To correctly translate the sentence, a hierarchical
phrase-based system needs to model the subject
noun phrase, object noun phrase and copula con-
structions; these are captured by rulesXa,Xd and
Xb respectively, so this set of rules represents a
hierarchical phrase-based system that can be used
to correctly translate the Chinese sentence. Note
that the Chinese word order is correctly preserved
in the subject (Xa) as well as copula constructions
(Xb), and correctly inverted in the object construc-
tion (Xd).
However, although it can generate the correct
translation in Fig. 2, the grammar has no mech-
anism to prevent the generation of an incorrect
translation like the one illustrated in Fig. 3. If
we contrast the topological ordering of the rules
in Fig. 2 and Fig. 3, we observe that the difference
is small but quite signicant. Using precede sym-
bol (?) to indicate the rst operand immediately
dominates the second operand in the hierarchical
structure, the topological orderings in Fig. 2 and
Fig. 3 are Xa ? Xb ? Xc ? Xd ? Xe and
Xd ? Xa ? Xb ? Xc ? Xe, respectively. The
only difference is the topological ordering of Xd:
in Fig. 2, it appears below most of the other hier-
archical phrases, while in Fig. 3, it appears above
all the other hierarchical phrases.
nally introduced for bilingual analysis rather than generation
of translations.
Modeling the topological ordering of hierarchi-
cal phrases is computationally prohibitive, since
there are literally millions of hierarchical rules in
the system's automatically-learned grammar and
millions of possible ways to order their applica-
tion. To avoid this computational problem and
still model the topological ordering, we propose
to use the topological ordering of function words
as a practical approximation. This is motivated by
the fact that function words tend to carry crucial
syntactic information in sentences, serving as the
?glue? for content-bearing phrases. Moreover, the
positional relationships between function words
and content phrases tends to be xed (e.g., in En-
glish, prepositions invariably precede their object
noun phrase), at least for the languages we have
worked with thus far.
In the Chinese sentence above, there are three
function words involved: the conjunctionZ (and),
the copula 4 (are), and the noun phrase marker
{ (of).3 Using the function words as approximate
representations of the rules in which they appear,
the topological ordering of hierarchical phrases in
Fig. 2 is Z(and) ? 4(are) ? {(of), while that
in Fig. 3 is {(of) ? Z(and) ? 4(are).4 We
can distinguish the correct and incorrect reorder-
ing choices by looking at this simple information.
In the correct reordering choice,{(of) appears at
the lower level of the hierarchy while in the incor-
rect one,{(of) appears at the highest level of the
hierarchy.
4 Pairwise Dominance Model
Our example suggests that we may be able to im-
prove the translation model's sensitivity to correct
versus incorrect reordering choices by modeling
the topological ordering of function words. We do
so by introducing a predicate capturing the domi-
nance relationship in a derivation between pairs of
neighboring function words.
5
Let us dene a predicate d(Y ?, Y ??) that takes
two function words as input and outputs one of
3
We use the term ?noun phrase marker? here in a general
sense, meaning that in this example it helps tell us that the
phrase is part of an NP, not as a technical linguistic term. It
serves in other grammatical roles, as well. Disambiguating
the syntactic roles of function words might be a particularly
useful thing to do in the model we are proposing; this is a
question for future research.
4
Note that for expository purposes, we designed our sim-
ple grammar to ensure that these function words appear in
separate rules.
5
Two function words are considered neighbors iff no other
function word appears between them in the source sentence.
326
? Z C? 4 ?{??-
?
XXXXXz
?????9?? ? ?
arecomputers and cell phones inventions of the last century
Figure 1: A running example of Chinese-to-English translation.
Xa???Z Xb, computers andXb?
???Z Xc4 Xd, computers andXc are Xd?
???ZC?4 Xd, computers and cell phones areXd?
???ZC?4 Xe{? , computers and cell phones are inventions ofXe?
???ZC?4??-{? , computers and cell phones are inventions of the last century?
Figure 2: The derivation that leads to the correct translation
Xd??Xa{? , inventions of Xa?
???Z Xb{? , inventions of computers andXb?
???Z Xc4 Xe{? , inventions of computers andXc are Xe?
???ZC?4 Xe{? , inventions of computers and cell phones areXe?
???ZC?4??-{? , inventions of computers and cell phones are the last century?
Figure 3: The derivation that leads to the incorrect translation
four values: {leftFirst, rightFirst, dontCare, nei-
ther}, where Y ? appears to the left of Y ?? in the
source sentence. The value leftFirst indicates that
in the derivation's topological ordering, Y ? pre-
cedes Y ?? (i.e. Y ? dominates Y ?? in the hierarchi-
cal structure), while rightFirst indicates that Y ??
dominates Y ?. In Fig. 2, d(Y ?, Y ??) = leftFirst
for Y ? = the copula 4 (are) and Y ?? = the noun
phrase marker{ (of).
The dontCare and neither values capture two
additional relationships: dontCare indicates that
the topological ordering of the function words is
exible, and neither indicates that the topologi-
cal ordering of the function words is disjoint. The
former is useful in cases where the hierarchical
phrases suggest the same kind of reordering, and
therefore restricting their topological ordering is
not necessary. This is illustrated in Fig. 2 by the
pairZ(and) and the copula 4(are), where putting
either one above the other does not change the -
nal word order. The latter is useful in cases where
the two function words do not share a same parent.
Formally, this model requires several changes in
the design of the hierarchical phrase-based system.
1. To facilitate topological ordering of function
words, the hierarchical phrases must be sub-
categorized with function words. Taking Xb
in Fig. 2 as a case in point, subcategorization
using function words would yield:
6
Xb(4 ?{) ? Xc4 Xd({) (3)
The subcategorization (indicated by the
information in parentheses following the
nonterminal) propagates the function word
4(are) of Xb to the higher level structure to-
gether with the function word {(of) of Xd.
This propagation process generalizes to other
rules by maintaining the ordering of the func-
tion words according to their appearance in
the source sentence. Note that the subcate-
gorized nonterminals often resemble genuine
syntactic categories, for instance X({) can
frequently be interpreted as a noun phrase.
2. To facilitate the computation of the domi-
nance relationship, the coindexing in syn-
chronized rules (indicated by the ? symbol
in Eq. 1) must be expanded to include infor-
mation not only about the nonterminal corre-
spondences but also about the alignment of
the lexical items. For example, adding lexi-
cal alignment information to rule Xd would
yield:
Xd ? ?X1{2?3, inventions3 of2 X1?
(4)
6
The target language side is concealed for clarity.
327
The computation of the dominance relation-
ship using this alignment information will be
discussed in detail in the next section.
Again takingXb in Fig. 2 as a case in point, the
dominance feature takes the following form:
fdom(Xb) ? dom(d(4,{)|4, {)) (5)
dom(d(YL, YR)|YL, YR)) (6)
where the probability of4 ?{ is estimated ac-
cording to the probability of d(4,{).
In practice, both 4(are) and {(of) may ap-
pear together in one same rule. In such a case, a
dominance score is not calculated since the topo-
logical ordering of the two function words is un-
ambiguous. Hence, in our implementation, a
dominance score is only calculated at the points
where the topological ordering of the hierarchical
phrases needs to be resolved, i.e. the two function
words always come from two different hierarchi-
cal phrases.
5 Parameter Estimation
Learning the dominance model involves extract-
ing d values for every pair of neighboring func-
tion words in the training bitext. Such statistics
are not directly observable in parallel corpora, so
estimation is needed. Our estimation method is
based on two facts: (1) the topological ordering
of hierarchical phrases is tightly coupled with the
span of the hierarchical phrases, and (2) the span
of a hierarchical phrase at a higher level is al-
ways a superset of the span of all other hierarchical
phrases at the lower level of its substructure. Thus,
to establish soft estimates of dominance counts,
we utilize alignment information available in the
rule together with the consistent alignment heuris-
tic (Och and Ney, 2004) traditionally used to guess
phrase alignments.
Specically, we dene the span of a function
word as a maximal, consistent alignment in the
source language that either starts from or ends
with the function word. (Requiring that spans be
maximal ensures their uniqueness.) We will re-
fer to such spans as Maximal Consistent Align-
ments (MCA). Note that each function word has
two such Maximal Consistent Alignments: one
that ends with the function word (MCAR)and an-
other that starts from the function word (MCAL).
Y ? Y ?? left- right- dont- nei-
First First Care ther
Z (and) 4 (are) 0.11 0.16 0.68 0.05
4 (are) { (of) 0.57 0.15 0.06 0.22
Table 1: The distribution of the dominance values
of the function words involved in Fig. 1. The value
with the highest probability is in bold.
Given two function words Y ? and Y ??, with Y ?
preceding Y ??, we dene the value of d by exam-
ining the MCAs of the two function words.
d(Y ?, Y ??) =?
?????
?????
leftFirst, Y ? 6? MCAR(Y ??) ? Y ??? MCAL(Y ?)
rightFirst, Y ?? MCAR(Y ??) ? Y ?? 6? MCAL(Y ?)
dontCare, Y ?? MCAR(Y ??) ? Y ??? MCAL(Y ?)
neither, Y ? 6? MCAR(Y ??) ? Y ?? 6? MCAL(Y ?)
(6)
Fig. 4a illustrates the leftFirst dominance value
where the intersection of the MCAs contains only
the second function word ({(of)). Fig. 4b illus-
trates the dontCare value, where the intersection
contains both function words. Similarly, rightFirst
and neither are represented by an intersection that
contains only Y ?, or by an empty intersection, re-
spectively. Once all the d values are counted, the
pairwise dominance model of neighboring func-
tion words can be estimated simply from counts
using maximum likelihood. Table 1 illustrates es-
timated dominance values that correctly resolve
the topological ordering for our running example.
6 Experimental Setup
We tested the effect of introducing the pairwise
dominance model into hierarchical phrase-based
translation on Chinese-to-English and Arabic-to-
English translation tasks, thus studying its effect
in two languages where the use of function words
differs signicantly. Following Setiawan et al
(2007), we identify function words as the N most
frequent words in the corpus, rather than identify-
ing them according to linguistic criteria; this ap-
proximation removes the need for any additional
language-specic resources. We report results
for N = 32, 64, 128, 256, 512, 1024, 2048.7 For
7
We observe that even N = 2048 represents less than
1.5% and 0.8% of the words in the Chinese and Arabic vo-
cabularies, respectively. The validity of the frequency-based
strategy, relative to linguistically-dened function words, is
discussed in Section 8
328
n
a
n
b
j
j
j
z
j
z
j
the last century
of
innovations
are
cell phones
and
computers
?
Z
C
? 4
?
-
 {

?
j
z
j
z
j
j
j
the last century
of
innovations
are
cell phones
and
computers
?
 Z
C
? 4
?
-
{

?
Figure 4: Illustrations for: a) the leftFirst value,
and b) the dontCare value. Thickly bordered
boxes are MCAs of the function words while solid
circles are the alignment points of the function
words. The gray boxes are the intersections of the
two MCAs.
all experiments, we report performance using the
BLEU score (Papineni et al, 2002), and we assess
statistical signicance using the standard boot-
strapping approach introduced by (Koehn, 2004).
Chinese-to-English experiments. We trained
the system on the NIST MT06 Eval corpus ex-
cluding the UN data (approximately 900K sen-
tence pairs). For the language model, we used a 5-
gram model with modied Kneser-Ney smoothing
(Kneser and Ney, 1995) trained on the English side
of our training data as well as portions of the Giga-
word v2 English corpus. We used the NIST MT03
test set as the development set for optimizing inter-
polation weights using minimum error rate train-
ing (MERT; (Och and Ney, 2002)). We carried out
evaluation of the systems on the NIST 2006 eval-
uation test (MT06) and the NIST 2008 evaluation
test (MT08). We segmented Chinese as a prepro-
cessing step using the Harbin segmenter (Zhao et
al., 2001).
Arabic-to-English experiments. We trained
the system on a subset of 950K sentence pairs
from the NIST MT08 training data, selected by
?subsampling? from the full training data using a
method proposed by Kishore Papineni (personal
communication). The subsampling algorithm se-
lects sentence pairs from the training data in a
way that seeks reasonable representation for all n-
grams appearing in the test set. For the language
model, we used a 5-gram model trained on the En-
glish portion of the whole training data plus por-
tions of the Gigaword v2 corpus. We used the
NIST MT03 test set as the development set for
optimizing the interpolation weights using MERT.
We carried out the evaluation of the systems on the
NIST 2006 evaluation set (MT06) and the NIST
2008 evaluation set (MT08). Arabic source text
was preprocessed by separating clitics, the de-
niteness marker, and the future tense marker from
their stems.
7 Experimental Results
Chinese-to-English experiments. Table 2 sum-
marizes the results of our Chinese-to-English ex-
periments. These results conrm that the pairwise
dominance model can signicantly increase per-
formance as measured by the BLEU score, with a
consistent pattern of results across the MT06 and
MT08 test sets. Modeling N = 32 drops the per-
formance marginally below baseline, suggesting
that perhaps there are not enough words for the
pairwise dominance model to work with. Dou-
bling the number of words (N = 64) produces
a small gain, and dening the pairwise dominance
model using N = 128 most frequent words pro-
duces a statistically signicant 1-point gain over
the baseline (p < 0.01). Larger values of N
yield statistically signicant performance above
the baseline, but without further improvements
over N = 128.
Arabic-to-English experiments. Table 3 sum-
marizes the results of our Arabic-to-English ex-
periments. This set of experiments shows a pat-
tern consistent with what we observed in Chinese-
to-English translation, again generally consistent
across MT06 and MT08 test sets although mod-
eling a small number of lexical items (N = 32)
brings a marginal improvement over the baseline.
In addition, we again nd that the pairwise dom-
inance model with N = 128 produces the most
signicant gain over the baseline in the MT06,
although, interestingly, modeling a much larger
number of lexical items (N = 2048) yields the
strongest improvement for the MT08 test set.
329
MT06 MT08
baseline 30.58 24.08
+dom(N = 32) 30.43 23.91
+dom(N = 64) 30.96 24.45
+dom(N = 128) 31.59 24.91
+dom(N = 256) 31.24 24.26
+dom(N = 512) 31.33 24.39
+dom(N = 1024) 31.22 24.79
+dom(N = 2048) 30.75 23.92
Table 2: Experimental results on Chinese-to-
English translation with the pairwise dominance
model (dom) of different N . The baseline (the
rst line) is the original hierarchical phrase-based
system. Statistically signicant results (p < 0.01)
over the baseline are in bold.
MT06 MT08
baseline 41.56 40.06
+dom(N = 32) 41.66 40.26
+dom(N = 64) 42.03 40.73
+dom(N = 128) 42.66 41.08
+dom(N = 256) 42.28 40.69
+dom(N = 512) 41.97 40.95
+dom(N = 1024) 42.05 40.55
+dom(N = 2048) 42.48 41.47
Table 3: Experimental results on Arabic-to-
English translation with the pairwise dominance
model (dom) of different N . The baseline (the
rst line) is the original hierarchical phrase-based
system. Statistically signicant results over the
baseline (p < 0.01) are in bold.
8 Discussion and Future Work
The results in both sets of experiments show con-
sistently that we have achieved a signicant gains
by modeling the topological ordering of function
words. When we visually inspect and compare
the outputs of our system with those of the base-
line, we observe that improved BLEU score often
corresponds to visible improvements in the sub-
jective translation quality. For example, the trans-
lations for the Chinese sentence ?<1 
?2 :3
?4 ?5 ?6 8?7 8 9 ?10 ?11 ?12
?13?, taken from Chinese MT06 test set, are as
follows (co-indexing subscripts represent recon-
structed word alignments):
? baseline: ?military1 intelligence2 un-
der observation8 in5 u.s.6 air raids7 :3 iran4
to9 how11 long12 ?13 ?
? +dom(N=128): ? military1 survey2 :3 how11
long12 iran4 under8 air strikes7 of the u.s6
can9 hold out10 ?13 ?
In addition to some lexical translation errors
(e.g. ?6 should be translated to U.S. Army),
the baseline system also makes mistakes in re-
ordering. The most obvious, perhaps, is its fail-
ure to capture the wh-movement involving the in-
terrogative word ?11 (how); this should move
to the beginning of the translated clause, consis-
tent with English wh-fronting as opposed to Chi-
nese wh in situ. The pairwise dominance model
helps, since the dominance value between the in-
terrogative word and its previous function word,
the modal verb 9(can) in the baseline system's
output, is neither, rather than rightFirst as in the
better translation.
The fact that performance tends to be best us-
ing a frequency threshold of N = 128 strikes
us as intuitively sensible, given what we know
about word frequency rankings.
8
In English,
for example, the most frequent 128 words in-
clude virtually all common conjunctions, deter-
miners, prepositions, auxiliaries, and comple-
mentizers ? the crucial elements of ?syntactic
glue? that characterize the types of linguistic
phrases and the ordering relationships between
them ? and a very small proportion of con-
tent words. Using Adam Kilgarriff's lemma-
tized frequency list from the British National Cor-
pus, http://www.kilgarriff.co.uk/bnc-readme.html,
the most frequent 128 words in English are heav-
ily dominated by determiners, ?functional? ad-
verbs like not and when, ?particle? adverbs like
up, prepositions, pronouns, and conjunctions, with
some arguably ?functional? auxiliary and light
verbs like be, have, do, give, make, take. Con-
tent words are generally limited to a small number
of frequent verbs like think and want and a very
small handful of frequent nouns. In contrast, ranks
129-256 are heavily dominated by the traditional
content-word categories, i.e. nouns, verbs, adjec-
tives and adverbs, with a small number of left-over
function words such as less frequent conjunctions
while, when, and although.
Consistent with these observations for English,
the empirical results for Chinese suggest that our
8
In fact, we initially simply choseN = 128 for our exper-
imentation, and then did runs with alternative N to conrm
our intuitions.
330
approximation of function words using word fre-
quency is reasonable. Using a list of approxi-
mately 900 linguistically identied function words
in Chinese extracted from (Howard, 2002), we ob-
serve that that the performance drops when in-
creasing N above 128 corresponds to a large in-
crease in the number of non-function words used
in the model. For example, with N = 2048, the
proportion of non-function words is 88%, com-
pared to 60% when N = 128.9
One natural extension of this work, therefore,
would be to tighten up our characterization of
function words, whether statistically, distribution-
ally, or simply using manually created resources
that exist for many languages. As a rst step, we
did a version of the Chinese-English experiment
using the list of approximately 900 genuine func-
tion words, testing on the Chinese MT06 set. Per-
haps surprisingly, translation performance, 30.90
BLEU, was around the level we obtained when
using frequency to approximate function words at
N = 64. However, we observe that many of
the words in the linguistically motivated function
word list are quite infrequent; this suggests that
data sparseness may be an additional factor worth
investigating.
Finally, although we believe there are strong
motivations for focusing on the role of function
words in reordering, there may well be value in
extending the dominance model to include content
categories. Verbs and many nouns have subcat-
egorization properties that may inuence phrase
ordering, for example, and this may turn out to ex-
plain the increase in Arabic-English performance
for N = 2048 using the MT08 test set. More gen-
erally, the approach we are taking can be viewed
as a way of selectively lexicalizing the automati-
cally extracted grammar, and there is a large range
of potentially interesting choices in how such lex-
icalization could be done.
9 Related Work
In the introduction, we discussed Chiang's (2005)
constituency feature, related ideas explored by
Marton and Resnik (2008) and Chiang et al
(2008), and the target-side variation investigated
by Zollman et al (2006). These methods differ
from each other mainly in terms of the specic lin-
9
We plan to do corresponding experimentation and anal-
ysis for Arabic once we identify a suitable list of manually
identied function words.
guistic knowledge being used and on which side
the constraints are applied.
Shen et al (2008) proposed to use lin-
guistic knowledge expressed in terms of a de-
pendency grammar, instead of a syntactic con-
stituency grammar. Villar et al (2008) attempted
to use syntactic constituency on both the source
and target languages in the same spirit as the con-
stituency feature, along with some simple pattern-
based heuristics ? an approach also investigated by
Iglesias et al (2009). Aiming at improving the se-
lection of derivations, Zhou et al (2008) proposed
prior derivation models utilizing syntactic annota-
tion of the source language, which can be seen as
smoothing the probabilities of hierarchical phrase
features.
A key point is that the model we have intro-
duced in this paper does not require the linguistic
supervision needed in most of this prior work. We
estimate the parameters of our model from parallel
text without any linguistic annotation. That said,
we would emphasize that our approach is, in fact,
motivated in linguistic terms by the role of func-
tion words in natural language syntax.
10 Conclusion
We have presented a pairwise dominance model
to address reordering issues that are not handled
particularly well by standard hierarchical phrase-
based modeling. In particular, the minimal lin-
guistic commitment in hierarchical phrase-based
models renders them susceptible to overgenera-
tion of reordering choices. Our proposal han-
dles the overgeneration problem by identifying
hierarchical phrases with function words and by
using function word relationships to incorporate
soft constraints on topological orderings. Our
experimental results demonstrate that introducing
the pairwise dominance model into hierarchical
phrase-based modeling improves performance sig-
nicantly in large-scale Chinese-to-English and
Arabic-to-English translation tasks.
Acknowledgments
This research was supported in part by the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-2-001. Any opinions, ndings, conclusions or
recommendations expressed in this paper are those
of the authors and do not necessarily reect the
view of the sponsors.
331
References
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 224?233, Honolulu,
Hawaii, October.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL'05), pages
263?270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jiaying Howard. 2002. A Student Handbook for Chi-
nese Function Words. The Chinese University Press.
Gonzalo Iglesias, Adria de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule ltering by pattern
for efcient hierarchical translation. In Proceedings
of the 12th Conference of the European Chapter of
the Association of Computational Linguistics (to ap-
pear).
R. Kneser and H. Ney. 1995. Improved backing-
off for m-gram language modeling. In Proceed-
ings of IEEE International Conference on Acoustics,
Speech, and Signal Processing95, pages 181?184,
Detroit, MI, May.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 127?133,
Edmonton, Alberta, Canada, May. Association for
Computational Linguistics.
Philipp Koehn. 2004. Statistical signicance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain,
July.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of The 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1003?
1011, Columbus, Ohio, June.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 295?302, Philadelphia,
Pennsylvania, USA, July.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 712?
719, Prague, Czech Republic, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of The 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 577?585, Columbus,
Ohio, June.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing soft syntax features and heuristics for hi-
erarchical phrase based machine translation. Inter-
national Workshop on Spoken Language Translation
2008, pages 190?197, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Sep.
Tiejun Zhao, Yajuan Lv, Jianmin Yao, Hao Yu, Muyun
Yang, and Fang Liu. 2001. Increasing accuracy
of chinese segmentation with strategy of multi-step
processing. Journal of Chinese Information Pro-
cessing (Chinese Version), 1:13?18.
Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntac-
tic parsing and tree kernels. In Proceedings of
the ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
19?27, Columbus, Ohio, June.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York City,
June.
332
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 941?948,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Comparative Study of Hypothesis Alignment and its Improvement 
for Machine Translation System Combination 
Boxing Chen*, Min Zhang, Haizhou Li and Aiti Aw 
 
Institute for Infocomm Research 
1 Fusionopolis Way, 138632 Singapore 
{bxchen, mzhang, hli, aaiti}@i2r.a-star.edu.sg 
 
 
Abstract 
Recently confusion network decoding shows 
the best performance in combining outputs 
from multiple machine translation (MT) sys-
tems. However, overcoming different word 
orders presented in multiple MT systems dur-
ing hypothesis alignment still remains the 
biggest challenge to confusion network-based 
MT system combination. In this paper, we 
compare four commonly used word align-
ment methods, namely GIZA++, TER, CLA 
and IHMM, for hypothesis alignment. Then 
we propose a method to build the confusion 
network from intersection word alignment, 
which utilizes both direct and inverse word 
alignment between the backbone and hypo-
thesis to improve the reliability of hypothesis 
alignment. Experimental results demonstrate 
that the intersection word alignment yields 
consistent performance improvement for all 
four word alignment methods on both Chi-
nese-to-English spoken and written language 
tasks. 
1 Introduction 
Machine translation (MT) system combination 
technique leverages on multiple MT systems to 
achieve better performance by combining their 
outputs. Confusion network based system com-
bination for machine translation has shown 
promising advantage compared with other tech-
niques based system combination, such as sen-
tence level hypothesis selection by voting and 
source sentence re-decoding using the phrases or 
translation models that are learned from the 
source sentences and target hypotheses pairs 
(Rosti et al, 2007a; Huang and Papineni, 2007). 
In general, the confusion network based sys-
tem combination method for MT consists of four 
steps: 1) Backbone selection: to select a back-
bone (also called ?skeleton?) from all hypotheses. 
The backbone defines the word orders of the fi-
nal translation. 2) Hypothesis alignment: to build 
word-alignment between backbone and each hy-
pothesis. 3) Confusion network construction: to 
build a confusion network based on hypothesis 
alignments. 4) Confusion network decoding: to 
decode the best translation from a confusion 
network. Among the four steps, the hypothesis 
alignment presents the biggest challenge to the 
method due to the varying word orders between 
outputs from different MT systems (Rosti et al 
2007). Many techniques have been studied to 
address this issue. Bangalore et al (2001) used 
the edit distance alignment algorithm which is 
extended to multiple strings to build confusion 
network, it only allows monotonic alignment. 
Jayaraman and Lavie (2005) proposed a heuris-
tic-based matching algorithm which allows non-
monotonic alignments to align the words be-
tween the hypotheses. More recently, Matusov et 
al. (2006, 2008) used GIZA++ to produce word 
alignment for hypotheses pairs. Sim et al (2007), 
Rosti et al (2007a), and Rosti et al (2007b) used 
minimum Translation Error Rate (TER) (Snover 
et al, 2006) alignment to build the confusion 
network. Rosti et al (2008) extended TER algo-
rithm which allows a confusion network as the 
reference to compute word alignment. Karakos et 
al. (2008) used ITG-based method for hypothesis 
alignment. Chen et al (2008) used Competitive 
Linking Algorithm (CLA) (Melamed, 2000) to 
align the words to construct confusion network. 
Ayan et al (2008) proposed to improve align-
ment of hypotheses using synonyms as found in 
WordNet (Fellbaum, 1998) and a two-pass 
alignment strategy based on TER word align-
ment approach. He et al (2008) proposed an 
IHMM-based word alignment method which the 
parameters are estimated indirectly from a varie-
ty of sources. 
Although many methods have been attempted, 
no systematic comparison among them has been 
reported. A through and fair comparison among 
them would be of great meaning to the MT sys-
941
tem combination research. In this paper, we im-
plement a confusion network-based decoder. 
Based on this decoder, we compare four com-
monly used word alignment methods (GIZA++, 
TER, CLA and IHMM) for hypothesis alignment 
using the same experimental data and the same 
multiple MT system outputs with similar features 
in terms of translation performance. We conduct 
the comparison study and other experiments in 
this paper on both spoken and newswire do-
mains: Chinese-to-English spoken and written 
language translation tasks. Our comparison 
shows that although the performance differences 
between the four methods are not significant, 
IHMM consistently show slightly better perfor-
mance than other methods. This is mainly due to 
the fact the IHMM is able to explore more know-
ledge sources and Viterbi decoding used in 
IHMM allows more thorough search for the best 
alignment while other methods has to use less 
optimal greedy search.  
In addition, for better performance, instead of 
only using one direction word alignment (n-to-1 
from hypothesis to backbone) as in previous 
work, we propose to use more reliable word 
alignments which are derived from the intersec-
tion of two-direction hypothesis alignment to 
construct confusion network. Experimental re-
sults show that the intersection word alignment-
based method consistently improves the perfor-
mance for all four methods on both spoken and 
written language tasks. 
This paper is organized as follows. Section 2 
presents a standard framework of confusion net-
work based machine translation system combina-
tion. Section 3 introduces four word alignment 
methods, and the algorithm of computing inter-
section word alignment for all four word align-
ment methods. Section 4 describes the experi-
ments setting and results on two translation tasks. 
Section 5 concludes the paper. 
2 Confusion network based system 
combination 
In order to compare different hypothesis align-
ment methods, we implement a confusion net-
work decoding system as follows: 
Backbone selection: in the previous work, 
Matusov et al (2006, 2008) let every hypothesis 
play the role of the backbone (also called ?skele-
ton? or ?alignment reference?) once. We follow 
the work of (Sim et al, 2007; Rosti et al, 2007a; 
Rosti et al, 2007b; He et al, 2008) and choose 
the hypothesis that best agrees with other hypo-
theses on average as the backbone by applying 
Minimum Bayes Risk (MBR) decoding (Kumar 
and Byrne, 2004).  TER score (Snover et al 
2006) is used as the loss function in MBR decod-
ing. Given a hypothesis set H, the backbone can 
be computed using the following equation, where  
( , )TER ? ?  returns the TER score of two hypothes-
es. 
 
?
?arg min ( , )b
E H E H
E TER E E
? ?
= ?           (1) 
Hypothesis alignment: all hypotheses are 
word-aligned to the corresponding backbone in a 
many-to-one manner. We apply four word 
alignment methods: GIZA++-based, TER-based, 
CLA-based, and IHMM-based word alignment 
algorithm. For each method, we will give details 
in the next section. 
Confusion network construction: confusion 
network is built from one-to-one word alignment; 
therefore, we need to normalize the word align-
ment before constructing the confusion network.  
The first normalization operation is removing 
duplicated links, since GIZA++ and IHMM-
based word alignments could be n-to-1 mappings 
between the hypothesis and backbone. Similar to 
the work of (He et al, 2008), we keep the link 
which has the highest similarity measure 
( , )j iS e e?  based on surface matching score, such 
as the length of maximum common subsequence 
(MCS) of the considered word pair. 
2 ( ( , ))
( , )
( ) ( )
j i
j i
j i
len MCS e e
S e e
len e len e
??
? =
? +
          (2) 
where ( , )j iMCS e e?  is the maximum common 
subsequence of word je?  and ie ; (.)len  is a 
function to compute the length of letter sequence. 
The other hypothesis words are set to align to the 
null word. For example, in Figure 1, 1e? and 3e?  
are aligned to the same backbone word 
2e , we 
remove the link between 
2e  and 3e?  if 
3 2 1 2( , ) ( , )S e e S e e? ?< , as shown in Figure 1 (b). 
The second normalization operation is reorder-
ing the hypothesis words to match the word order 
of the backbone. The aligned words are reor-
dered according to their alignment indices. To 
reorder the null-aligned words, we need to first 
insert the null words into the proper position in 
the backbone and then reorder the null-aligned 
hypothesis words to match the nulls on the back-
bone side. Reordering null-aligned words varies 
based to the word alignment method in the pre-
942
vious work. We reorder the null-aligned word 
following the approach of Chen et al (2008) 
with some extension. The null-aligned words are 
reordered with its adjacent word: moving with its 
left word (as Figure 1 (c)) or right word (as Fig-
ure 1 (d)). However, to reduce the possibility of 
breaking a syntactic phrase, we extend to choose 
one of the two above operations depending on 
which one has the higher likelihood with the cur-
rent null-aligned word. It is implemented by 
comparing two association scores based on co-
occurrence frequencies. They are association 
score of the null-aligned word and its left word, 
or the null-aligned word and its right word. We 
use point-wise mutual information (MI) as Equa-
tion 3 to estimate the likelihood. 
 11
1
( )
( , ) log
( ) ( )
i i
i i
i i
p e e
MI e e
p e p e
+
+
+
? ?
? ? =
? ?
              (3) 
where 1( )i ip e e +? ?  is the occurrence probability of 
bigram 1i ie e +? ?  observed in the hypothesis list; 
( )ip e?  and 1( )ip e +?  are probabilities of hypothe-
sis word ie?  and 1ie +?  respectively. 
In example of Figure 1, we choose (c) 
if 2 3 3 4( , ) ( , )MI e e MI e e? ? ? ?> , otherwise, word is 
reordered as (d). 
a 
1e  2e  3e  
 
    
 
1e?  2e?  3e?  4e?  
b 
1e  2e  3e  
 
    
 
1e?  2e?  3e?  4e?  
c 
1e  2e  3e  
 
4e?  1e?  2e?  3e?  
d 
 
1e  2e  3e  
3e?  4e?  1e?  2e?  
 
Figure 1: Example of alignment normalization. 
 
Confusion network decoding: the output 
translations for a given source sentence are ex-
tracted from the confusion network through a 
beam-search algorithm with a log-linear combi-
nation of a set of feature functions. The feature 
functions which are employed in the search 
process are:  
? Language model(s), 
? Direct and inverse IBM model-1, 
? Position-based word posterior probabili-
ties (arc scores of the confusion network), 
? Word penalty, 
? N-gram frequencies (Chen et al, 2005), 
? N-gram posterior probabilities (Zens and 
Ney, 2006). 
The n-grams used in the last two feature func-
tions are collected from the original hypotheses 
list from each single system. The weights of fea-
ture functions are optimized to maximize the 
scoring measure (Och, 2003). 
3 Word alignment algorithms 
We compare four word alignment methods 
which are widely used in confusion network 
based system combination or bilingual parallel 
corpora word alignment. 
3.1 Hypothesis-to-backbone word align-
ment 
GIZA++: Matusov et al (2006, 2008) proposed 
using GIZA++ (Och and Ney, 2003) to align 
words between the backbone and hypothesis. 
This method uses enhanced HMM model boot-
strapped from IBM Model-1 to estimate the 
alignment model. All hypotheses of the whole 
test set are collected to create sentence pairs for 
GIZA++ training. GIZA++ produces hypothesis-
backbone many-to-1 word alignments. 
TER-based: TER-based word alignment 
method (Sim et al, 2007; Rosti et al, 2007a; 
Rosti et al, 2007b) is an extension of multiple 
string matching algorithm based on Levenshtein 
edit distance (Bangalore et al, 2001). The TER 
(translation error rate) score (Snover et al, 2006) 
measures the ratio of minimum number of string 
edits between a hypothesis and reference where 
the edits include insertions, deletions, substitu-
tions and phrase shifts. The hypothesis is modi-
fied to match the reference, where a greedy 
search is used to select the set of shifts because 
an optimal sequence of edits (with shifts) is very 
expensive to find. The best alignment is the one 
that gives the minimum number of translation 
edits.  TER-based method produces 1-to-1 word 
alignments. 
CLA-based: Chen et al (2008) used competi-
tive linking algorithm (CLA) (Melamed, 2000) 
to build confusion network for hypothesis rege-
neration. Firstly, an association score is com-
puted for every possible word pair from the 
backbone and hypothesis to be aligned. Then a 
greedy algorithm is applied to select the best 
word alignment. We compute the association 
score from a linear combination of two clues: 
943
surface similarity computed as Equation (2) and 
position difference based distortion score by fol-
lowing (He et al, 2008). CLA works under a 1-
to-1 assumption, so it produces 1-to-1 word 
alignments. 
IHMM-based: He et al (2008) propose an 
indirect hidden Markov model (IHMM) for hy-
pothesis alignment. Different from traditional 
HMM, this model estimates the parameters indi-
rectly from various sources, such as word seman-
tic similarity, surface similarity and distortion 
penalty, etc. For fair comparison reason, we also 
use the surface similarity computed as Equation 
(2) and position difference based distortion score 
which are used for CLA-based word alignment. 
IHMM-based method produces many-to-1 word 
alignments. 
3.2 Intersection word alignment and its ex-
pansion 
In previous work, Matusov et al (2006, 2008) 
used both direction word alignments to compute 
so-called state occupation probabilities and then 
compute the final word alignment. The other 
work usually used only one direction word 
alignment (many/1-to-1 from hypothesis to 
backbone). In this paper, we use more reliable 
word alignments which are derived from the in-
tersection of both direct (hypothesis-to-backbone) 
and inverse (backbone-to-hypothesis) word 
alignments with heuristic-based expansion which 
is widely used in bilingual word alignment. The 
algorithm includes two steps: 
1) Generate bi-directional word alignments. It 
is straightforward for GIZA++ and IHMM to 
generate bi-directional word alignments. This is 
simply achieved by switching the parameters of 
source and target sentences. Due to the nature of 
greedy search in TER, the bi-directional TER-
based word alignments by switching the parame-
ters of source and target sentences are not neces-
sary exactly the same. For example, in Figure 2, 
the word ?shot? can be aligned to either ?shoot? 
or ?the? as the edit cost of word pair (shot, shoot) 
and (shot, the) are the same when compute the 
minimum-edit-distance for TER score. 
 
 
I shot  killer 
I shoot the killer 
a 
 
I shoot the killer 
I  shot killer 
b 
Figure 2: Example of two directions TER-based 
word alignments. 
 
For CLA word alignment, if we use the same 
association score, direct and inverse CLA word 
alignments should be exactly the same. There-
fore, we use different functions to compute the 
surface similarities, such as using maximum 
common subsequence (MCS) to compute inverse 
word alignment, and using longest matched pre-
fix (LMP) for computing direct word alignment, 
as in Equation (4). 
2 ( ( , ))
( , )
( ) ( )
j i
j i
j i
len LMP e e
S e e
len e len e
??
? =
? +
         (4) 
2) When two word alignments are ready, we 
start from the intersection of the two word 
alignments, and then continuously add new links 
between backbone and hypothesis if and only if 
both of the two words of the new link are un-
aligned and this link exists in the union of two 
word alignments. If there are more than two links 
share a same hypothesis or backbone word and 
also satisfy the constraints, we choose the link 
that with the highest similarity score. For exam-
ple, in Figure 2, since MCS-based similarity 
scores ( , ) ( , )S shot shoot S shot the> , we 
choose alignment (a). 
4  Experiments and results 
4.1 Tasks and single systems 
Experiments are carried out in two domains. One 
is in spoken language domain while the other is 
on newswire corpus. Both experiments are on 
Chinese-to-English translation. 
Experiments on spoken language domain were 
carried out on the Basic Traveling Expression 
Corpus (BTEC) (Takezawa et al, 2002) Chi-
nese- to-English data augmented with HIT-
corpus1. BTEC is a multilingual speech corpus 
which contains sentences spoken by tourists. 
40K sentence-pairs are used in our experiment. 
HIT-corpus is a balanced corpus and has 500K 
sentence-pairs in total. We selected 360K sen-
tence-pairs that are more similar to BTEC data 
according to its sub-topic. Additionally, the Eng-
lish sentences of Tanaka corpus2 were also used 
to train our language model. We ran experiments 
on an IWSLT challenge task which uses IWSLT-
20063 DEV clean text set as development set and 
IWSLT-2006 TEST clean text as test set. 
                                                 
1 http://mitlab.hit.edu.cn/ 
2 http://www.csse.monash.edu.au/~jwb/tanakacorpus.html 
3 http:// www.slc.atr.jp/IWSLT2006/ 
944
Experiments on newswire domain were car-
ried out on the FBIS4 corpus. We used NIST5 
2002 MT evaluation test set as our development 
set, and the NIST 2005 test set as our test set.  
Table 1 summarizes the statistics of the train-
ing, dev and test data for IWSLT and NIST tasks. 
 
task data Ch En 
 
 
 
IWSLT 
Train Sent. 406K 
Words 4.4M 4.6M 
Dev Sent. 489 489?7 
Words 5,896 45,449 
Test Sent. 500 500?7 
Words 6,296 51,227 
Add. Words - 1.7M 
 
 
 
NIST 
Train Sent. 238K 
Words 7.0M 8.9M 
Dev 
2002 
Sent. 878 878?4 
Words 23,248 108,616 
Test 
2005 
Sent. 1,082 1,082?4 
Words 30,544 141,915 
Add. Words - 61.5M 
 
Table 1: Statistics of training, dev and test data 
for IWSLT and NIST tasks. 
 
In both experiments, we used four systems, as 
listed in Table 2,  they are phrase-based system 
Moses (Koehn et al, 2007), hierarchical phrase-
based system (Chiang, 2007), BTG-based lexica-
lized reordering phrase-based system (Xiong et 
al., 2006) and a tree sequence alignment-based 
tree-to-tree translation system (Zhang et al, 
2008). Each system for the same task is trained 
on the same data set. 
4.2 Experiments setting 
For each system, we used the top 10 scored hy-
potheses to build the confusion network. Similar 
to (Rosti et al, 2007a), each word in the hypo-
thesis is assigned with a rank-based score of 
1/ (1 )r+ , where r is the rank of the hypothesis. 
And we assign the same weights to each system. 
For selecting the backbone, only the top hypo-
thesis from each system is considered as a candi-
date for the backbone. 
Concerning the four alignment methods, we 
use the default setting for GIZA++; and use tool-
kit TERCOM (Snover et al, 2006) to compute 
the TER-based word alignment, and also use the 
default setting. For fair comparison reason, we 
                                                 
4 LDC2003E14 
5 http://www.nist.gov/speech/tests/mt/ 
decide to do not use any additional resource, 
such as target language synonym list, IBM model 
lexicon; therefore, only surface similarity is ap-
plied in IHMM-based and CLA-based methods. 
We compute the distortion model by following 
(He et al, 2008) for IHMM and CLA-based me-
thods. The weights for each model are optimized 
on held-out data. 
 
 System Dev Test 
 
IWSLT
Sys1 30.75 27.58 
Sys2 30.74 28.54 
Sys3 29.99 26.91 
Sys4 31.32 27.48 
 
NIST 
Sys1 25.64 23.59 
Sys2 24.70 23.57 
Sys3 25.89 22.02 
Sys4 26.11 21.62 
 
Table 2: Results (BLEU% score) of single sys-
tems involved to system combination. 
4.3 Experiments results 
Our evaluation metric is BLEU (Papineni et al, 
2002), which are to perform case-insensitive 
matching of n-grams up to n = 4.  
Performance comparison of four methods: 
the results based on direct word alignments are 
reported in Table 3, row Best is the best single 
systems? scores; row MBR is the scores of back-
bone; GIZA++, TER, CLA, IHMM stand for 
scores of systems for four word alignment me-
thods. 
z MBR decoding slightly improves the per-
formance over the best single system for both 
tasks. This suggests that the simple voting strate-
gy to select backbone is workable. 
z For both tasks, all methods improve the per-
formance over the backbone. For IWSLT test set, 
the improvements are from 2.06 (CLA, 30.88-
28.82) to 2.52 BLEU-score (IHMM, 31.34-
28.82). For NIST test set, the improvements are 
from 0.63 (TER, 24.31-23.68) to 1.40 BLEU-
score (IHMM, 25.08-23.68). This verifies that 
the confusion network decoding is effective in 
combining outputs from multiple MT systems 
and the four word-alignment methods are also 
workable for hypothesis-to-backbone alignment. 
z For IWSLT task where source sentences are 
shorter (12-13 words per sentence in average), 
the four word alignment methods achieve similar 
performance on both dev and test set. The big-
gest difference is only 0.46 BLEU score (30.88 
for CLA, vs. 31.34 for IHMM). For NIST task 
945
where source sentences are longer (26-28 words 
per sentence in average), the difference is more 
significant. Here IHMM method achieves the 
best performance, followed by GIZA++, CLA 
and TER. IHMM is significantly better than TER 
by 0.77 BLEU-score (from 24.31 to 25.08, 
p<0.05). This is mainly because IHMM exploits 
more knowledge source and Viterbi decoding 
allows more thorough search for the best align-
ment while other methods use less optimal gree-
dy search. Another reason is that TER uses hard 
matching in computing edit distance. 
 
 method Dev Test 
 
 
IWSLT 
Best 31.32 28.54 
MBR 31.40 28.82 
GIZA++ 34.16 31.06 
TER 33.92 30.96 
CLA 33.85 30.88 
IHMM 34.35 31.34 
 
 
NIST 
Best 26.11 23.59 
MBR 26.36 23.68 
GIZA++ 27.58 24.88 
TER 27.15 24.31 
CLA 27.44 24.51
IHMM 27.76 25.08
 
Table 3: Results (BLEU% score) of combined 
systems based on direct word alignments. 
 
Performance improvement by intersection 
word alignment: Table 4 reports the perfor-
mance of the system combinations based on in-
tersection word alignments. It shows that: 
z Comparing Tables 3 and 4, we can see that 
the intersection word alignment-based expansion 
method improves the performance in all the dev 
and test sets for both tasks by 0.2-0.57 BLEU-
score and the improvements are consistent under 
all conditions. This suggests that the intersection 
word alignment-based expansion method is more 
effective than the commonly used direct word-
alignment-based hypothesis alignment method in 
confusion network-based MT system combina-
tion. This is because intersection word align-
ments are more reliable compared with direct 
word alignments, and so for heuristic-based ex-
pansion which is based on the aligned words 
with higher scores. 
z TER-based method achieves the biggest 
performance improvement by 0.4 BLEU-score in 
IWSLT and 0.57 in NIST. Our statistics shows 
that the TER-based word alignment generates 
more inconsistent links between the two-
directional word alignments than other methods. 
This may give the intersection with heuristic-
based expansion method more room to improve 
performance. 
z On the contrast, CLA-based method obtains 
relatively small improvement of 0.26 BLEU-
score in IWSLT and 0.21 in NIST. The reason 
could be that the similarity functions used in the 
two directions are more similar. Therefore, there 
are not so many inconsistent links between the 
two directions. 
z Table 5 shows the number of links modified 
by intersection operation and the BLEU-score 
improvement. We can see that the more the mod-
ified links, the bigger the improvement.  
 
 method Dev Test 
 
 
IWSLT
MBR 31.40 28.82
GIZA++ 34.38 31.40
TER 34.17 31.36
CLA 34.03 31.14
IHMM 34.59 31.74
 
 
NIST 
MBR 26.36 23.68
GIZA++ 27.80 25.11
TER 27.58 24.88
CLA 27.64 24.72
IHMM 27.96 25.37
 
Table 4: Results (BLEU% score) of combined 
systems based on intersection word alignments. 
 
 
 
system 
IWSLT NIST 
Inc. Imp. Inc. Imp.
CLA 1.2K 0.26 9.2K 0.21 
GIZA++ 3.2K 0.36 25.5K 0.23 
IHMM 3.7K 0.40 21.7K 0.29 
TER 4.3K 0.40 40.2K 0.57 
#total links 284K 1,390K 
 
Table 5: Number of modified links and absolute 
BLEU(%) score improvement on test sets. 
 
Effect of fuzzy matching in TER: the pre-
vious work on TER-based word alignment uses 
hard match in counting edits distance. Therefore, 
it is not able to handle cognate words match, 
such as in Figure 2, original TER script count the 
edit cost of (shoot, shot) equals to word pair 
(shot, the). Following (Leusch et al, 2006), we 
modified the TER script to allow fuzzy matching: 
change the substitution cost from 1 for any word 
pair to 
946
 ( , ) 1 ( , )sub j i j iCOST e e S e e? ?= ?              (5) 
which ( , )j iS e e?  is the similarity score based on 
the length of longest matched prefix (LMP) 
computed as in Equation (4).  As a result, the 
fuzzy matching reports 
( , ) 1 (2 3) /(5 4) 1/ 3SubCost shoot shot = ? ? + =  and 
( , ) 1 (2 0) /(5 3) 1SubCost shoot the = ? ? + =  while in 
original TER, both of the two scores are equal to 
1. Since cost of word pair (shoot, shot) is smaller 
than that of word pair (shot, the), word ?shot? 
has higher chance to be aligned to ?shoot? (Fig-
ure 2 (a)) instead of ?the? (Figure 2 (b)). This 
fuzzy matching mechanism is very useful to such 
kind of monolingual alignment task as in hypo-
thesis-to-backbone word alignment since it can 
well model word variances and morphological 
changes. 
Table 6 summaries the results of TER-based 
systems with or without fuzzy matching. We can 
see that the fuzzy matching improves the per-
formance for all cases. This verifies the effect of 
fuzzy matching for TER in monolingual word 
alignment. In addition, the improvement in NIST 
test set (0.36 BLEU-score for direct alignment 
and 0.21 BLEU-score for intersection one) are 
more than that in IWSLT test set (0.15 BLEU-
score for direct alignment and 0.11 BLEU-score 
for intersection one). This is because the sen-
tences of IWSLT test set are much shorter than 
that of NIST test set. 
 
TER-based 
systems 
IWSLT NIST 
Dev Test Dev Test 
Direct align 
+fuzzy match 
33.92 
34.14
30.96 
31.11 
27.15 
27.53
24.31 
24.67
Intersect align 
    +fuzzy match 
34.17 
34.40
31.36 
31.47 
27.58 
27.79
24.88 
25.09
 
Table 6: Results (BLEU% score) of TER-based 
combined systems with or without fuzzy match. 
5 Conclusion 
Confusion-network-based system combination 
shows better performance than other methods in 
combining multiple MT systems? outputs, and 
hypothesis alignment is a key step. In this paper, 
we first compare four word alignment methods 
for hypothesis alignment under the confusion 
network framework. We verify that the confu-
sion network framework is very effective in MT 
system combination and IHMM achieves the best 
performance. Moreover, we propose an intersec-
tion word alignment-based expansion method for 
hypothesis alignment, which is more reliable as it 
leverages on both direct and inverse word align-
ment. Experimental results on Chinese-to-
English spoken and newswire domains show that 
the intersection word alignment-based method 
yields consistent improvements across all four 
word alignment methods. Finally, we evaluate 
the effect of fuzzy matching for TER. 
Theoretically, confusion network decoding is 
still a word-level voting algorithm although it is 
more complicated than other sentence-level vot-
ing algorithms. It changes lexical selection by 
considering the posterior probabilities of words 
in hypothesis lists. Therefore, like other voting 
algorithms, its performance strongly depends on 
the quality of the n-best hypotheses of each sin-
gle system. In some extreme cases, it may not be 
able to improve BLEU-score (Mauser et al, 
2006; Sim et al, 2007). 
 
References  
N. F. Ayan. J. Zheng and W. Wang. 2008. Improving 
Alignments for Better Confusion Networks for 
Combining Machine Translation Systems. In Pro-
ceedings of COLING 2008, pp. 33?40. Manchester, 
Aug. 
S. Bangalore, G. Bordel, and G. Riccardi. 2001. 
Computing consensus translation from multiple 
machine translation systems. In Proceeding of 
IEEE workshop on Automatic Speech Recognition 
and Understanding, pp. 351?354. Madonna di 
Campiglio, Italy. 
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo and M. 
Federico. 2005. The ITC-irst SMT System for 
IWSLT-2005. In Proceeding of IWSLT-2005, 
pp.98-104, Pittsburgh, USA, October. 
B. Chen, M. Zhang, A. Aw and H. Li. 2008. Regene-
rating Hypotheses for Statistical Machine Transla-
tion. In: Proceeding of COLING 2008. pp105-112. 
Manchester, UK. Aug. 
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228. 
C. Fellbaum. editor. 1998. WordNet: An Electronic 
Lexical Database. MIT Press. 
X. He, M. Yang, J. Gao, P. Nguyen, R. Moore, 2008. 
Indirect-HMM-based Hypothesis Alignment for 
Combining Outputs from Machine Translation 
Systems. In Proceeding of EMNLP. Hawaii, US, 
Oct. 
F. Huang and K. Papinent. 2007. Hierarchical System 
Combination for Machine Translation. In Proceed-
ings of the 2007 Joint Conference on Empirical 
Methods in Natural Language Processing and 
947
Computational Natural Language Learning 
(EMNLP-CoNLL?2007), pp. 277 ? 286, Prague, 
Czech Republic, June. 
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching. 
In Proceeding of EAMT. pp.143?152. 
D. Karakos, J. Eisner, S. Khudanpur, and M. Dreyer. 
2008. Machine Translation System Combination 
using ITG-based Alignments. In Proceeding of 
ACL-HLT 2008, pp. 81?84. 
O. Kraif, B. Chen. 2004. Combining clues for lexical 
level aligning using the Null hypothesis approach. 
In: Proceedings of COLING 2004, Geneva, Au-
gust, pp. 1261-1264.  
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-
ran, R. Zens, C. Dyer, O. Bojar, A. Constantin and 
E. Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. In Proceedings of 
ACL-2007. pp. 177-180, Prague, Czech Republic. 
S. Kumar and W. Byrne. 2004. Minimum Bayes Risk 
Decoding for Statistical Machine Translation. In    
Proceedings of HLT-NAACL 2004, May 2004, 
Boston, MA, USA. 
G. Leusch, N. Ueffing and H. Ney. 2006. CDER: Ef-
ficient MT Evaluation Using Block Movements. In 
Proceedings of EACL. pp. 241-248. Trento Italy. 
E. Matusov, N. Ueffing, and H. Ney. 2006. Compu-
ting consensus translation from multiple machine 
translation systems using enhanced hypotheses 
alignment. In Proceeding of EACL, pp. 33-40, 
Trento, Italy, April. 
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D. 
Dechelotte, M. Federico, M. Kolss, Y. Lee, J. B. 
Marino, M. Paulik, S. Roukos, H. Schwenk, and H. 
Ney. System Combination for Machine Translation 
of Spoken and Written Language. IEEE Transac-
tions on Audio, Speech and Language Processing, 
volume 16, number 7, pp. 1222-1237, September. 
A. Mauser, R. Zens, E. Matusov, S. Hasan, and H. 
Ney. 2006. The RWTH Statistical Machine Trans-
lation System for the IWSLT 2006 Evaluation. In 
Proceeding of IWSLT 2006, pp. 103-110, Kyoto, 
Japan, November. 
I. D. Melamed. 2000. Models of translational equiva-
lence among words. Computational Linguistics, 
26(2), pp. 221-249. 
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of ACL-
2003. Sapporo, Japan. 
F. J. Och and H. Ney. 2003. A systematic comparison 
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19-51. 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 
2002. BLEU: a method for automatic evaluation of 
machine translation. In Proceeding of ACL-2002, 
pp. 311-318. 
A. I. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas, R. 
Schwartz and B. Dorr. 2007a. Combining Outputs 
from Multiple Machine Translation Systems.  In 
Proceeding of NAACL-HLT-2007, pp. 228-235. 
Rochester, NY. 
A. I. Rosti, S. Matsoukas and R. Schwartz. 2007b. 
Improved Word-Level System Combination for 
Ma-chine Translation. In Proceeding of ACL-2007, 
Prague. 
A. I. Rosti, B. Zhang, S. Matsoukas, and R. Schwartz. 
2008. Incremental Hypothesis Alignment for 
Building Confusion Networks with Application to 
Machine Translation System Combination, In Pro-
ceeding of the Third ACL Workshop on Statistical 
Machine Translation, pp. 183-186. 
K. C. Sim, W. J. Byrne, M. J.F. Gales, H. Sahbi, and 
P. C. Woodland. 2007. Consensus network decod-
ing for statistical machine translation system com-
bination. In Proceeding of  ICASSP-2007. 
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. 
Makhoul. 2006. A study of translation edit rate 
with targeted human annotation. In Proceeding of 
AMTA. 
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, 
and S. Yamamoto. 2002. Toward a broad-coverage 
bilingual corpus for speech translation of travel 
conversations in the real world. In Proceeding of 
LREC-2002, Las Palmas de Gran Canaria, Spain. 
D. Xiong, Q. Liu and S. Lin. 2006. Maximum Entro-
py Based Phrase Reordering Model for Statistical 
Machine Translation. In Proceeding of ACL-2006. 
pp.521-528.  
R. Zens and H. Ney. 2006. N-gram Posterior Prob-
abilities for Statistical Machine Translation. In 
Proceeding of HLT-NAACL Workshop on SMT, pp. 
72-77, NY. 
M. Zhang, H. Jiang, A. Aw, H. Li, C. L. Tan, and S. 
Li. 2008. A Tree Sequence Alignment-based Tree-
to-Tree Translation Model. In Proceeding of ACL-
2008. Columbus, US. June. 
Y. Zhang, S. Vogel, and A. Waibel 2004. Interpreting 
BLEU/NIST scores: How much improvement do 
we need to have a better system? In Proceedings of 
LREC 2004, pp. 2051-2054. 
                                                 
*  The first author has moved to National Research 
Council, Canada. His current email address is: Box-
ing.Chen@nrc.ca. 
948
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 21?24,
Suntec, Singapore, 3 August 2009. c?2009 ACL and AFNLP
MARS: Multilingual Access and Retrieval System with Enhanced 
Query Translation and Document Retrieval 
 
 
Lianhau Lee, Aiti Aw, Thuy Vu, Sharifah Aljunied Mahani, Min Zhang, Haizhou Li 
Institute for Infocomm Research 
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632 
{lhlee, aaiti, tvu, smaljunied, mzhang, hli} 
@i2r.a-star.edu.sg 
 
  
 
Abstract 
In this paper, we introduce a multilingual ac-
cess and retrieval system with enhanced query 
translation and multilingual document retrieval, 
by mining bilingual terminologies and aligned 
document directly from the set of comparable 
corpora which are to be searched upon by us-
ers. By extracting bilingual terminologies and 
aligning bilingual documents with similar con-
tent prior to the search process provide more 
accurate translated terms for the in-domain 
data and support multilingual retrieval even 
without the use of translation tool during re-
trieval time. This system includes a user-
friendly graphical user interface designed to 
provide navigation and retrieval of information 
in browse mode and search mode respectively.  
1 Introduction 
Query translation is an important step in the 
cross-language information retrieval (CLIR). 
Currently, most of the CLIR system relies on 
various kinds of dictionaries, for example Word-
Nets (Luca and Nurnberger, 2006; Ranieri et al, 
2004), in query translation. Although dictionaries 
can provide effective translation on common 
words or even phrases, they are always limited in 
the coverage. Hence, there is a need to expand 
the existing collections of bilingual terminologies 
through various means. 
Recently, there has been more and more re-
search work focus on bilingual terminology ex-
traction from comparable corpora. Some promis-
ing results have been reported making use of sta-
tistics, linguistics (Sadat et al, 2003), translitera-
tion (Udupa et al, 2008), date information (Tao 
and Zhai, 2005) and document alignment ap-
proach (Talvensaari et al, 2007). 
In this paper, we introduce our Multilingual 
Access and Retrieval System ? MARS which 
addresses the query translation issue by using in-
domain bilingual terminologies extracted directly 
from the comparable corpora which are to be 
accessed by users. And at the same time, bilin-
gual documents are paired up prior to the search 
process based on their content similarities to 
overcome the limitation of traditional keyword 
matching based on the translated terms. These 
would provide better retrieval experiences as not 
only more accurate in-domain translated term 
will be used to retrieve the documents but also 
provide a new perspective of multilingual infor-
mation retrieval to process the time-consuming 
multilingual document matching at the backend. 
The following sections of this paper will de-
scribe the system architecture and the proposed 
functionalities of the MARS system. 
2 MARS System 
The MARS system is designed to enhance query 
translation and document retrieval through min-
ing the underlying multilingual structures of 
comparable corpora via a pivot language. There 
are three reasons for using a pivot language. 
Firstly, it is appropriate to use a universal lan-
guage among potential users of different native 
languages. Secondly, it reduces the backend data 
processing cost by just considering the pair-wise 
relationship between the pivot language and any 
other languages. Lastly, the dictionary resources 
between the pivot language and all the other lan-
guages are more likely to be available than oth-
erwise. 
There are two main parts in this system, 
namely data processing and user interface. The 
data processing is an offline process to mine the 
underlying multilingual structure of the compa-
21
rable corpora to support retrieval. The structure 
of the comparable corpora is presented visually 
in the user interface under browse mode and 
search mode to facilitate navigation and retrieval 
of information respectively. 
3 Data Processing  
For demo purpose, three different language 
newspapers from the year 1995 to 2006 pub-
lished by Singapore Press Holding (SPH), 
namely Strait Times1 (English), ZaoBao2 (Chi-
nese) and Berita Harian3  (Malay), are used as 
comparable corpora. In these particular corpora, 
English is chosen as the pivot language and noun 
terms are chosen as the basic semantic unit as 
they represent a huge amount of significant in-
formation. Our strategy is to organize and ma-
nipulate the corpora in three levels of abstraction 
? clusters, documents and terms. And our key 
task over here is to find the underlying associa-
tions of documents or terminologies in each level 
across different languages. 
First, monolingual documents are grouped into 
clusters by k-means algorithm using simple word 
vectors. Then, monolingual noun terms are ex-
tracted from each cluster using linguistic patterns 
and filtered by occurrence statistics globally 
(within cluster) and locally (within document), so 
that they are good representatives for cluster as a 
whole as well as individual documents (Vu et al, 
2008). The extracted terms are then used in 
document clustering in a new cycle and the 
whole process is repeated until the result con-
verges. 
Next, cluster alignment is carried out between 
the pivot language (English) and the other lan-
guages (Chinese, Malay). Clusters can be con-
ceptualized as the collection of documents with 
the same themes (e.g. finance, politics or sports) 
and their alignments as the correspondents in the 
other languages. Since there may be overlaps 
among themes, e.g. finance and economy, each 
cluster is allowed to align to more than one clus-
ter with varying degree of alignment score. 
After that, document alignment is carried out 
between aligned cluster pairs (Vu et al, 2009). 
Note that the corpora are comparable, thus the 
aligned document pairs are inherently compara-
                                                 
1 http://www.straitstimes.com/ an English news agency in 
Singapore. Source ? Singapore Press Holdings Ltd. 
2 http://www.zaobao.com/ a Chinese news agency in 
Singapore. Source ? Singapore Press Holdings Ltd. 
3 http://cyberita.asia1.com.sg/ a Malay news agency in 
Singapore. Source ? Singapore Press Holdings Ltd. 
ble, i.e. they are similar in contents but not iden-
tical as translation pairs. Also as important to 
note that, document alignment harvested over 
here is independent of user query. In other 
words, document alignment is not simply deter-
mined by mere occurrence of certain keyword 
and its absence does not hinder documents to be 
aligned. Hence mining of document alignment 
beforehand improves document retrieval after-
ward. 
Finally, term alignment is likewise generated 
between aligned document pairs. The aligned 
terms are expected to be in-domain translation 
pairs since they are both derived from documents 
of similar contents, and thus they have similar 
contexts. By making use of the results provided 
by each other, document alignment and term 
alignment can be improved over iterations. 
All the mentioned processes are done offline 
and the results are stored in a relational database 
which will handle online queries generated in the 
user interface later on. 
4 User Interface  
As mentioned, there are two modes provided in 
the user interface to facilitate navigation and re-
trieval of information, namely browse mode and 
search mode. Both modes can be switched sim-
ply by clicking on the respective tabs in the user 
interface. In the following, the functionalities of 
the browse mode and the search mode will be 
explained in details. 
4.1 Browse Mode 
Browse mode provides a means to navigate 
through the complex structures underneath an 
overwhelming data with an easily-understood, 
user-friendly graphical interface. In the figure 1, 
the graph in the browse mode gives an overall 
picture of the distribution of documents in vari-
ous clusters and among the different language 
collections. The outer circles represent the lan-
guage repositories and the inner circles represent 
the clusters. The sizes of the clusters are depend-
ing on the number of contained documents and 
the color represents the dominant theme. The 
labels of the highlighted clusters, characterized 
by a set of five distinguished words, are shown in 
the tooltips next to them. By clicking on a clus-
ter, the links depicting the cluster alignments will 
show up. The links to the clusters in the other 
languages are all propagated through the pivot 
language. 
22
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1 Browse mode in the MARS System 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 2 Search mode in the MARS System 
23
The right hand side of the browse panel pro-
vides the detail information about the selected 
cluster using three sub-panels, i.e. top, middle 
and bottom. The top panel displays a list of ex-
tracted terms from the selected cluster. User may 
narrow down the list of interested terms by using 
the search-text column on top. By clicking on a 
term in the list, its translations in other lan-
guages, if any, will be displayed in the middle 
sub-panel and the document containing the term 
will be listed in the bottom sub-panel. The 
?Search? buttons next to the term translations 
provide a short-cut to jump to the search mode 
with the corresponding term translation being cut 
and pasted over. Last but not least, user may 
simply click on any document listed in the bot-
tom sub-panel to read the content of the docu-
ment and its aligned documents in a pop-up win-
dow. 
4.2 Search Mode 
Search mode provides a means for comprehen-
sive information retrieval. Refer to the figure 2, 
user may enter query in any of the selected lan-
guages to search for documents in all languages. 
The main difference is that query translation is 
done via bilingual terms extracted via the term 
alignment technology discussed earlier. For each 
retrieved document, documents with similar con-
tent in the other languages are also provided to 
supplement the searched results. This enables 
documents which are potentially relevant to the 
users be retrieved as some of these retrieved 
documents may not contain the translated terms 
at all. 
On top of the query translation, other informa-
tion such as related terms and similar terms to 
the query are shown at the tab panel on the right. 
Related terms are terms that correlate statistically 
with the query term and they are arranged by 
cluster, separated by dotted line in the list. Simi-
lar terms are longer terms that contains the query 
term in itself. Both the related terms and the 
similar terms provide user additional hints and 
guides to improve further queries. 
5 Conclusion  
The MARS system is developed to enable user to 
better navigate and search information from mul-
tilingual comparable corpora in a user-friendly 
graphical user interface. Query translation and 
document retrieval is enhanced by utilizing the 
in-domain bilingual terminologies and document 
alignment acquired from the comparable corpora 
itself, without limited by dictionaries and key-
word matching. 
Currently, the system only support simple 
query. Future work will improve on this to allow 
more general query. 
References  
Ernesto William De Luca, and Andreas Nurnberger. 
2006. A Word Sense-Oriented User Interface 
for Interactive Multilingual Text Retrieval, In 
Proceedings of the Workshop Information Re-
trieval, Hildesheim.  
M. Ranieri, E. Pianta, and L. Bentivogli. 2004. 
Browsing Multilingual Information with the 
MultiSemCor Web Interface, In Proceedings of 
the LREC-2004 Workshop ?The amazing utility of 
parallel and comparable corpora?, Lisban, Portu-
gal. 
Fatiha Sadat, Masatoshi Yoshikawa, Shunsuke Ue-
mura. 2003. Learning bilingual translations 
from comparable corpora to cross-language 
information retrieval: hybrid statistics-based 
and linguistics-based approach, In Proceedings 
of the 6th international workshop on Information 
Retrieval with Asian Languages, vol. 1: pp. 57-64. 
 Raghavendra Udupa, K. Saravanan, A. Kumaran, 
Jagadeesh Jagarlamudi. 2008. Mining named en-
tity transliteration equivalents from compara-
ble corpora. In Proceedings of the 17th ACM con-
ference on Information and knowledge manage-
ment. 
Tao Tao, and ChengXiang Zhai. 2005. Mining com-
parable bilingual text corpora for cross-
language information integration. In Proceed-
ings of the 11th ACM SIGKDD international con-
ference on Knowledge discovery in data mining. 
Tuomas Talvensaari, Jorma Laurikkala, Kalervo Jar-
velin, Martti Juhola, Heikki Keskustalo. 2007. 
Creating and exploiting a comparable corpus 
in cross-language information retrieval. ACM 
Transactions on Information System (TOIS), vol. 
25(1):  Article No 4. 
Thuy Vu, Aiti Aw, Min Zhang. 2008. Term extrac-
tion through unithood and termhood unifica-
tion. In Proceedings of the 3rd International Joint 
Conference on Natural Language Processing 
(IJCNLP-08), Hyderabad, India. 
Thuy Vu, Aiti Aw, Min Zhang. 2009. Feature-based 
Method for Document Alignment in Compara-
ble News Corpora. In Proceedings of the 12th 
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL-09), 
Athens, Greece. 
24
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 1?18,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Report of NEWS 2009 Machine Transliteration Shared Task
Haizhou Li?, A Kumaran?, Vladimir Pervouchine? and Min Zhang?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{hli,vpervouchine,mzhang}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
This report documents the details of the
Machine Transliteration Shared Task con-
ducted as a part of the Named Enti-
ties Workshop (NEWS), an ACL-IJCNLP
2009 workshop. The shared task features
machine transliteration of proper names
from English to a set of languages. This
shared task has witnessed enthusiastic par-
ticipation of 31 teams from all over the
world, with diversity of participation for
a given system and wide coverage for a
given language pair (more than a dozen
participants per language pair). Diverse
transliteration methodologies are repre-
sented adequately in the shared task for a
given language pair, thus underscoring the
fact that the workshop may truly indicate
the state of the art in machine transliter-
ation in these language pairs. We mea-
sure and report 6 performance metrics on
the submitted results. We believe that the
shared task has successfully achieved the
following objectives: (i) bringing together
the community of researchers in the area
of Machine Transliteration to focus on var-
ious research avenues, (ii) Calibrating sys-
tems on common corpora, using common
metrics, thus creating a reasonable base-
line for the state-of-the-art of translitera-
tion systems, and (iii) providing a quan-
titative basis for meaningful comparison
and analysis between various algorithmic
approaches used in machine translitera-
tion. We believe that the results of this
shared task would uncover a host of inter-
esting research problems, giving impetus
to research in this significant research area.
1 Introduction
Names play a significant role in many Natural
Language Processing (NLP) and Information Re-
trieval (IR) systems. They have a critical role
in Cross Language Information Retrieval (CLIR)
and Machine Translation (MT) systems as the sys-
tems? performances are shown to positively cor-
relate with the correct conversion of names be-
tween the languages in several studies (Demner-
Fushman and Oard, 2002; Mandl and Womser-
Hacker, 2005; Hermjakob et al, 2008; Udupa et
al., 2009). The traditional source for name equiva-
lence, the bilingual dictionaries ? whether hand-
crafted or statistical ? offer only limited support
as they do not have sufficient coverage of names.
New names are introduced to the vocabulary of a
language every day.
All of the above point to the critical need for ro-
bust Machine Transliteration technology and sys-
tems. This has attracted attention from the re-
search community. Over the last decade scores of
papers on Machine Transliteration have appeared
in the top Computational Linguistics, Information
Retrieval and Data Management conferences, ex-
ploring diverse algorithmic approaches in a wide
variety of different languages (Knight and Graehl,
1998; Li et al, 2004; Zelenko and Aone, 2006;
Sproat et al, 2006; Sherif and Kondrak, 2007;
Hermjakob et al, 2008; Goldwasser and Roth,
2008; Goldberg and Elhadad, 2008; Klementiev
and Roth, 2006). However, there has not been
any coordinated effort in calibrating the state-of-
the-art technical capabilities of machine translit-
eration: the studies explore different algorithmic
approaches in different language pairs and report
their performance in different metrics and tested
on different corpora.
The overarching objective of this shared task
is to drive the machine transliteration technology
forward, to measure and baseline the state-of-the-
1
art and to provide a meaningful comparison be-
tween the most promising algorithmic approaches
in order to stimulate the discussions among the re-
searchers. The NLP community in Asia is espe-
cially interested in transliteration as several major
Asian languages do not use Latin script in their na-
tive writing systems. The Named Entity Workshop
(NEWS 2009) in ACL-IJCNLP 2009 in Singapore
provides an ideal platform for the shared task to
take off. This is precisely what we address in this
shared task on machine transliteration that is con-
ducted as a part of the Named Entity Workshop
(NEWS-2009), an ACL-IJCNLP 2009 workshop.
The shared task aims at achieving the following
objectives:
? Providing a forum to bring together the com-
munity of researchers in the area of Machine
Transliteration to focus on various research
avenues in this important research area.
? Calibrating systems on common hand-crafted
corpora, using common metrics, in many dif-
ferent languages, thus creating a reasonable
baseline for the state-of-the-art of translitera-
tion systems.
? Analysing the results so that a reason-
able comparison of different algorithmic
approaches and their trade-offs (such as,
transliteration quality vs. generality of ap-
proach across languages vs. training data
size, etc.) may be explored.
We believe that a substantial part of what we have
set out to achieve has been accomplished, and we
present this report as a record of the task pro-
cess, system participation and results and our find-
ings. It is our hope that this reporting will generate
lively discussions during the NEWS workshop and
subsequent research in this important area.
This introduction outlines the purpose of the
transliteration shared task conducted as a part of
the NEWS workshop. Section 2 outlines the ma-
chine transliteration task and the corpora used and
Section 3 discusses the metrics chosen for evalua-
tion, along with the rationale for choosing them.
Section 4 sketches the participation. Section 5
presents the results of the shared task and the anal-
ysis of the results. Section 6, summarises the
queries and feedback we have received from the
participants and Section 7 concludes, presenting
some lessons learnt from the current edition of the
shared task, and some ideas we want to pursue
in the future plan for the Machine Transliteration
tasks.
2 Transliteration Shared Task
In this section, we outline the definition of the task,
the process followed and the rationale for the de-
cisions.
2.1 ?Transliteration?: A definition
There exists several terms that are used inter-
changeably in the contemporary research litera-
ture for the conversion of names between two
languages, such as, transliteration, transcription,
and sometimes Romanisation, especially if Latin
scripts are used for target strings (Halpern, 2007).
Our aim is not only at capturing the name con-
version process from a source to a target language,
but also at its ultimate utility for downstream ap-
plications, such as CLIR and MT. We have nar-
rowed down to three specific requirements for the
task, as follows: ?Transliteration is the conver-
sion of a given name in the source language (a
text string in the source writing system or orthog-
raphy) to a name in the target language (another
text string in the target writing system or orthog-
raphy), such that the target language name is:
(i) phonemically equivalent to the source name
(ii) conforms to the phonology of the target lan-
guage and (iii) matches the user intuition of the
equivalent of the source language name in the tar-
get language.?
Given that the phoneme set of languages may
not be exactly the same, the first requirement must
be diluted to ?close to?, instead of ?equivalent?.
The second requirement is needed to ensure that
the target string is a valid string as per the target
language phonology. The third requirement is in-
troduced to produce what a normal user would ex-
pect (at least for the popular names), and in or-
der to make it useful for downstream applications
like MT or CLIR systems. Though the third re-
quirement make systems produce target language
strings that marginally violate the first or second
requirements, it ensures that such transliteration
system is of value to downstream systems. All the
above requirements are implicitly enforced by the
choice of name pairs used to define the training
and test corpora in a given language pair. In cases
where multiple equivalent target language names
are possible for a source language name, we in-
2
clude all of them.
After much debate, we have also retained the
task name as ?transliteration?, though our defi-
nition may be closest to the ?popular transcrip-
tion? (Halpern, 2007), due to the popularity of
term ?Machine Transliteration? among the lan-
guage technology researchers.
2.2 Shared Task Description
The shared task is specified as development of ma-
chine transliteration systems in one or more of the
specified language pairs. Each language pair of
the shared task consists of a source and a target
language, implicitly specifying the transliteration
direction. Training and development data in each
of the language pairs have been made available to
all registered participants for developing a translit-
eration system for that specific language pair using
any approach that they find appropriate.
At the evaluation time, a standard hand-crafted
test set consisting of between 1,000 and 3,000
source names (approximately 10% of the train-
ing data size) have been released, on which the
participants are required to produce a ranked list
of transliteration candidates in the target language
for each source name. The system output is
tested against a reference set (which may include
multiple correct transliterations for some source
names), and the performance of a system is cap-
tured in multiple metrics (defined in Section 3),
each designed to capture a specific performance
dimension.
For every language pair every participant is re-
quired to submit one run (designated as a ?stan-
dard? run) that uses only the data provided by the
NEWS workshop organisers in that language pair,
and no other data or linguistic resources. This
standard run ensures parity between systems and
enables meaningful comparison of performance
of various algorithmic approaches in a given lan-
guage pair. Participants are allowed to submit
more runs (designated as ?non-standard?) for ev-
ery language pair using either data beyond that
provided by the shared task organisers or linguis-
tic resources in a specific language, or both. This
essentially may enable any participant to demon-
strate the limits of performance of their system in
a given language pair.
The shared task timelines provide adequate time
for development, testing (approximately 2 months
after the release of the training data) and the final
result submission (5 days after the release of the
test data).
2.3 Shared Task Corpora
We have had two specific constraints in selecting
languages for the shared task: language diversity
and data availability. To make the shared task in-
teresting and to attract wider participation, it is
important to ensure a reasonable variety among
the languages in terms of linguistic diversity, or-
thography and geography. Clearly, the ability of
procuring and distributing a reasonably large (ap-
proximately 10K paired names for training and
testing together) hand-crafted corpora consisting
primarily of paired names is critical for this pro-
cess. At the end of the planning stage and after
discussion with the data providers, we have cho-
sen the set of 7 languages shown in Table 1 for the
task (Li et al, 2004; Kumaran and Kellner, 2007;
MSRI, 2009; CJKI, 2009).
For all of the languages chosen, we have been
able to procure paired names data between En-
glish and the respective languages and were able
to make them available to the participants. In ad-
dition, we have been able to procure a specific
corpus of about 40K Romanised Japanese names
and their Kanji counterparts, and the correspond-
ing language pair (Japanese names from their Ro-
manised form to Kanji) has been included as one
of the task language pair.
It should be noted here that each corpus has a
definite skew in its characteristics: the names in
the Chinese, Japanese and Korean (CJK) language
corpora are Western names; the Indic languages
(Hindi, Kannada and Tamil) corpora consists of a
mix of Indian and Western names. The Roman-
ised Kanji to Kanji corpus consists only of native
Japanese names. While such characteristics may
have provided us an opportunity to specifically
measure the performance for forward translitera-
tions (in CJK) and backward transliterations (in
Romanised Kanji), we do not highlight such fine
distinctions in this edition.
Finally, it should be noted here that the corpora
procured and released for NEWS 2009 represent
perhaps the most diverse and largest corpora to be
used for any common transliteration tasks today.
3 Evaluation Metrics and Rationale
The participants have been asked to submit re-
sults of one standard and up to four non-standard
3
Source language Target language Data Source Data Size (No. source names) Task IDTraining Development Testing
English Hindi Microsoft Research India 9,975 974 1,000 EnHi
English Tamil Microsoft Research India 7,974 987 1,000 EnTa
English Kannada Microsoft Research India 7,990 968 1,000 EnKa
English Russian Microsoft Research India 5,977 943 1,000 EnRu
English Chinese Institute for Infocomm Research 31,961 2,896 2,896 EnCh
English Korean Hangul CJK Institute 4,785 987 989 EnKo
English Japanese Katakana CJK Institute 23,225 1,492 1,489 EnJa
Japanese name (in English) Japanese Kanji CJK Institute 6,785 1,500 1,500 JnJk
Table 1: Source and target languages for the shared task on transliteration.
runs. Each run contains a ranked list of up to
10 candidate transliterations for each source name.
The submitted results are compared to the ground
truth (reference transliterations) using 6 evaluation
metrics capturing different aspects of translitera-
tion performance. Since a name may have mul-
tiple correct transliterations, all these alternatives
are treated equally in the evaluation, that is, any
of these alternatives is considered as a correct
transliteration, and all candidates matching any of
the reference transliterations are accepted as cor-
rect ones.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
3.1 Word Accuracy in Top-1 (ACC)
Also known as Word Error Rate, it measures cor-
rectness of the first transliteration candidate in the
candidate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ?ri,j : ri,j = ci,1;
0 otherwise
}
(1)
3.2 Fuzziness in Top-1 (Mean F-score)
The mean F-score measures how different, on av-
erage, the top transliteration candidate is from its
closest reference. F-score for each source word
is a function of Precision and Recall and equals 1
when the top candidate matches one of the refer-
ences, and 0 when there are no common characters
between the candidate and any of the references.
Precision and Recall are calculated based on
the length of the Longest Common Subsequence
(LCS) between a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses? etc.)
4
3.3 Mean Reciprocal Rank (MRR)
Measures traditional MRR for any right answer
produced by the system, from among the candi-
dates. 1/MRR tells approximately the average
rank of the correct transliteration. MRR closer to 1
implies that the correct answer is mostly produced
close to the top of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
3.4 MAPref
Measures tightly the precision in the n-best can-
didates for i-th source name, for which reference
transliterations are available. If all of the refer-
ences are produced, then the MAP is 1. Let?s de-
note the number of correct candidates for the i-th
source word in k-best list as num(i, k). MAPref
is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
3.5 MAP10
MAP10 measures the precision in the 10-best can-
didates for i-th source name provided by the can-
didate system. In general, the higher MAP10 is,
the better is the quality of the transliteration sys-
tem in capturing the multiple references.
MAP10 =
1
N
N?
i=1
1
10
(
10?
k=1
num(i, k)
)
(10)
3.6 MAPsys
MAPsys measures the precision in the top Ki-best
candidates produced by the system for i-th source
name, for which ni reference transliterations are
available. This measure allows the systems to pro-
duce variable number of transliterations, based on
their confidence in identifying and producing cor-
rect transliterations.
MAPsys =
1
N
N?
i=1
1
Ki
(
Ki?
k=1
num(i, k)
)
(11)
4 Participation in Shared Task
There have been 31 systems from around the
world that participated in the shared task and sub-
mitted the transliteration results for a common test
data, produced by their systems trained on the
common training corpora.
A few teams have participated in all or almost
all tasks (that is, language pairs); most others par-
ticipated in 3 tasks on average. Each language pair
has attracted on average around 13 teams. The par-
ticipation details are shown in Table 3 and the de-
mographics of the participating teams by country
is shown in Figure 1.
? ? ? ? ? ? ? ? ? ? ??
??
???
???
?????????
????
?????
???
????
????
????????
0 1 2 3 4 5 6 7 8 9 10
Figure 1: Participation by country.
Teams are required to submit at least one stan-
dard run for every task they participated in. In total
104 standard and 86 non-standard runs have been
submitted. Table 2 shows the number of standard
and non-standard runs submitted for each task. It
is clear that the most ?popular? tasks are translit-
eration from English to Hindi and from English to
Chinese, attempted by 21 and 18 participants re-
spectively. Overall, as can be noted from the re-
sults, each task has received significant participa-
tion.
5 Task Results and Analysis
5.1 Standard runs
The 8 individual plots in Figure 2 summarise (for
each task) the results of standard runs via 3 mea-
sured metrics concerning output of at least one
correct candidate per source word, namely, ac-
curacy in top-1, F -score and Mean Reciprocal
Rank (MRR). The plots in Figure 3 summarise (for
each task) the results for 3 metrics on ranked or-
dered transliteration output of the systems, namely
MAPref , MAP10 and MAPsys metrics. All the
results are presented numerically in Tables 8?11,
for all evaluation metrics. These are the official
5
English
to Hindi
English
to Tamil
English
to Kan-
nada
English
to Rus-
sian
English
to Chi-
nese
English
to Ko-
rean
English
to
Japanese
Katakana
Japanese
translit-
erated to
Japanese
Kanji
Language pair code EnHi EnTa EnKa EnRu EnCh EnKo EnJa JnJk
Standard runs 21 13 14 13 18 8 10 7
Non-standard runs 18 5 5 16 20 9 5 8
Table 2: Number of runs submitted for each task. Number of participants coincides with the number of
standard runs submitted.
evaluation results published for this edition of the
transliteration shared task. Note that two teams
have updated their results (after fixing bugs in their
systems) after the deadline; their results are iden-
tified specifically.
We find that two approaches to transliteration
are most popular in the shared task submissions.
One of these approaches is Phrase-based statis-
tical machine transliteration (Finch and Sumita,
2008), an approach initially developed for ma-
chine translation (Koehn et al, 2003). Systems
that adopted this approach are (Song, 2009; Haque
et al, 2009; Noeman, 2009; Rama and Gali, 2009;
Chinnakotla and Damani, 2009).1 The other is
Conditional Random Fields(Lafferty et al, 2001)
(CRF), adopted by (Aramaki and Abekawa, 2009;
Shishtla et al, 2009). With only a few exceptions,
most implementations are based on approaches
that are language-independent. Indeed, many of
the participants fielded their systems on multiple
languages, as can be seen from Table 3.
We also note that combination of several differ-
ent models via re-ranking of their outputs (CRF,
Maximum Entropy Model, Margin Infused Re-
laxed Algorithm) proves to be very successful (Oh
et al, 2009); their system (reported as Team ID
6) produced the best or second-best transliteration
performance consistently across all metrics, in all
tasks, except Japanese back-transliteration. Exam-
ples of other model combinations are (Das et al,
2009).
At least two teams (reported as Team IDs 14
and 27) incorporate language origin detection in
their system (Bose and Sarkar, 2009; Khapra and
Bhattacharyya, 2009). The Indian language cor-
pora contains names of both English and Indic ori-
gin. Khapra and Bhattacharyya (2009) demon-
strate how much the transliteration performance
can be improved when language of origin detec-
1To maintain anonymity, papers of the teams that submit-
ted anonymous results are not cited in this report.
tion is employed, followed by a language-specific
transliteration model for decoding.
Some systems merit specific mention as they
adopt are rather unique approaches. Jiampoja-
marn et al (2009) propose DirectTL discrimina-
tive sequence prediction model that is language-
independent (reported as Team ID 7). Their
transliteration accuracy is among the highest in
several tasks (EnCh, EnHi and EnRu). Zelenko
(2009) present an approach to the transliteration
problem based on Minimum Description Length
(MDL) principle. Freitag and Wang (2009) ap-
proach the problem of transliteration with bidirec-
tional perceptron edit models.
Finally, in Figure 4 we present a plot where
each point represents a standard run by a system,
with different tasks marked with specific shape
and colour. This plot gives a bird-eye-view of
the system performances across two most uncorre-
lated evaluation metrics, namely accuracy in top-1
(ACC) and Mean F -score. Not surprisingly, we
notice very high performance in terms of F -score
for English to Russian transliteration task, likely
because Russian orthography follows pronuncia-
tion very closely, except for characters like soft
and hard signs that can hardly be recovered from
English words.
We also observe that Japanese back-
transliteration has proven to be much harder
than other (forward-transliteration) tasks. In
general, we note that a well-performing translit-
eration system performs well across all metrics.
We are curious about the correlation between
different metrics, and the results (specifically,
the Spearman?s rank correlation coefficient) are
presented below:
? Accuracy in top-1 vs. F -score: 0.40
? Accuracy in top-1 vs. MRR: 0.97
? Accuracy in top-1 vs. MAPref : 0.997
6
? Accuracy in top-1 vs. MAP10: 0.89
? Accuracy in top-1 vs. MAPsys: 0.80
We find that F -score is the most uncorrelated met-
ric: the Spearman?s rank correlation coefficient
between F -score and accuracy in top-1 is 0.40 and
between F -score and MRR it is 0.44. This is likely
because all metrics, except for F -score, are based
on word accuracy, while F -score is based on word
similarity allowing non-matching words to have
scores well above 0.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8Accuracy in top-10
0.10.2
0.30.4
0.50.6
0.70.8
0.91
F-score English to ChineseEnglish to HindiEnglish to TamilEnglish to KannadaEnglish to RussianEnglish to KoreanEnglish to JapaneseJapanese transliterated to Japanese Kanji
Figure 4: Accuracy in top-1 vs. F -score for dif-
ferent tasks.
5.2 Non-standard runs
For the non-standard runs there exist no restric-
tions for the teams on the use of more data or other
linguistic resources. The purpose of non-standard
runs is to see how accurate personal name translit-
eration can be, for a given language pair. The ap-
proaches used in non-standard runs are typical and
may be summarised as follows:
? Dictionary lookup.
? Pronunciation dictionaries to convert words
to their phonetic transcription.
? Additional corpora for training and dictio-
nary lookup, such as LDC English-Chinese
named entity list LDC2005T34 (Linguistic
Data Consortium, 2005).
? Web search, and in particular, Wikipedia
search. First, transliteration candidates are
generated. Then a Web search is performed
to see if any of the candidates appear in the
search results. Based on the results, the can-
didates are re-ranked.
The results are shown in Tables 16?19. For En-
glish to Chinese and English to Russian transliter-
ation tasks the accuracy in top-1 can go as high as
0.909 and 0.955 respectively when Web search is
used to aid transliteration.
5.3 Post-evaluation
Two participants have found a bug in their system
implementation and re-evaluated the results after
the deadline. Their results are marked specifically
in Tables 4?8 and 16.
6 Process Analysis and Fine-tuning
In this section we highlight some of the sugges-
tions and feedback that we have received from the
participants during the course of this shared task.
While a few of them have been implemented in the
current edition, many of these may be considered
in the future editions of the shared task.
More or different languages There is quite a
bit of interest in enhancing the list of language
pairs short-listed. While we are constrained (in
this edition) due to the availability of manually
verified data, certainly more languages will be in-
cluded in the future editions, as some specific data
have already been promised for future editions.
Bidirectional transliteration Many partic-
ipants express interest in transliterations into
English; and this reflexive task will be added in
the future editions. We believe it will encourage
more participation as it will be easy to read and
verify system output in English for those teams
not familiar with the non-English side of the
language.
Forward vs. backward transliteration There
is quite a bit of interest expressed in specifically
separating forward and backward transliteration
tasks. However, such separation requires specific
corpora with known origin for each name pair, and
clearly we are constrained by the availability of
corpora. When corpora is available, the task may
be designated explicitly in future editions.
Number of standard runs The number of stan-
dard runs that may be submitted may be increased
in the future editions, as many participants would
like to submit many standard runs, trained with
different parameters.
7
Errors in training and development corpora
While we have taken all precautions in acquiring
and creating the corpora, some errors still remain.
We thank those who have sent us the errata. How-
ever, since the affected part is less than 0.5% of
the data, we believe that the effect on final results
is minimal. The errata will be made available to
all participants.
7 Conclusions and Future Plans
We are pleased to report a comprehensive cal-
ibration and baselining of machine translitera-
tion apporaches as most state-of-the-art machine
transliteration techniques are represented in the
shared task. The most popular techniques such as
Phrase-Based Machine Transliteration (Koehn et
al., 2003), and Conditional Random Fields (Laf-
ferty et al, 2001) are inspired by recent progress in
machine translation. As the standard runs are lim-
ited by the use of corpus, most of the systems are
implemented under the direct orthographic map-
ping (DOM) framework (Li et al, 2004). While
the standard runs allow us to conduct meaning-
ful comparison across different algorithms, we
recognise that the non-standard runs open up more
opportunities for exploiting larger linguistic cor-
pora. It is also noted that several systems have re-
ported improved performance over any previously
reported results on similar corpora.
NEWS 2009 Shared Task represents a suc-
cessful debut of a community effort in driving
machine transliteration techniques forward. The
overwhelming responses in the first shared task
also warrant continuation of such an effort in fu-
ture ACL or IJCNLP events.
Acknowledgements
The organisers of the NEWS 2009 Shared Task
would like to thank the Institute for Infocomm Re-
search (Singapore), Microsoft Research India and
CJK Institute (Japan) for providing the corpora
and technical support. Without those, the Shared
Task would not be possible. We thank those par-
ticipants who identified errors in the data and sent
us the errata. We want to thank Monojit Choud-
hury for his contribution to metrics defined for the
shared task. We also want to thank the members
of programme committee for their invaluable com-
ments that improve the quality of the shared task
papers. Finally, we wish to thank all the partici-
pants for their active participation that have made
this first machine transliteration shared task a com-
prehensive one.
8
References
Eiji Aramaki and Takeshi Abekawa. 2009. Fast de-
coding and easy implementation: Transliteration as
a sequential labeling. In Proc. ACL/IJCNLP Named
Entities Workshop Shared Task.
Dipankar Bose and Sudeshna Sarkar. 2009. Learn-
ing multi character alignment rules and classifica-
tion of training data for transliteration. In Proc.
ACL/IJCNLP Named Entities Workshop Shared
Task.
Manoj Kumar Chinnakotla and Om P. Damani. 2009.
Experiences with English-Hindi, English-Tamil and
English-Kannada transliteration tasks at NEWS
2009. In Proc. ACL/IJCNLP Named Entities Work-
shop Shared Task.
CJKI. 2009. CJK Institute. http://www.cjk.org/.
Amitava Das, Asif Ekbal, Tapabrata Mondal, and
Sivaji Bandyopadhyay. 2009. English to Hindi
machine transliteration system at NEWS 2009.
In Proc. ACL/IJCNLP Named Entities Workshop
Shared Task.
D. Demner-Fushman and D. W. Oard. 2002. The ef-
fect of bilingual term list size on dictionary-based
cross-language information retrieval. In Proc. 36-th
Hawaii Int?l. Conf. System Sciences, volume 4, page
108.2.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proc. 3rd Int?l.
Joint Conf NLP, volume 1, Hyderabad, India, Jan-
uary.
Dayne Freitag and Zhiqiang Wang. 2009. Name
transliteration with bidirectional perceptron edit
models. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Yoav Goldberg and Michael Elhadad. 2008. Identifica-
tion of transliterated foreign words in Hebrew script.
In Proc. CICLing, volume LNCS 4919, pages 466?
477.
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. In Proc. EMNLP,
pages 353?362.
Jack Halpern. 2007. The challenges and pitfalls
of Arabic romanization and arabization. In Proc.
Workshop on Comp. Approaches to Arabic Script-
based Lang.
Rejwanul Haque, Sandipan Dandapat, Ankit Kumar
Srivastava, Sudip Kumar Naskar, and Andy Way.
2009. English-Hindi transliteration using context-
informed PB-SMT. In Proc. ACL/IJCNLP Named
Entities Workshop Shared Task.
Ulf Hermjakob, Kevin Knight, and Hal Daume?. 2008.
Name translation in statistical machine translation:
Learning when to transliterate. In Proc. ACL,
Columbus, OH, USA, June.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language-independent approach to translit-
eration. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Mitesh Khapra and Pushpak Bhattacharyya. 2009. Im-
proving transliteration accuracy using word-origin
detection and lexicon lookup. In Proc. ACL/IJCNLP
Named Entities Workshop Shared Task.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Proc.
21st Int?l Conf Computational Linguistics and 44th
Annual Meeting of ACL, pages 817?824, Sydney,
Australia, July.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721?722.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. Int?l.
Conf. Machine Learning, pages 282?289.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159?166,
Barcelona, Spain.
Linguistic Data Consortium. 2005. LDC Chinese-
English name entity lists LDC2005T34.
T. Mandl and C. Womser-Hacker. 2005. The effect of
named entities on effectiveness in cross-language in-
formation retrieval evaluation. In Proc. ACM Symp.
Applied Comp., pages 1059?1064.
MSRI. 2009. Microsoft Research India.
http://research.microsoft.com/india.
Sara Noeman. 2009. Language independent translit-
eration system using phrase based SMT approach
on substring. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Tori-
sawa. 2009. Machine transliteration with target-
language grapheme and phoneme: Multi-engine
transliteration approach. In Proc. ACL/IJCNLP
Named Entities Workshop Shared Task.
Taraka Rama and Karthik Gali. 2009. Modeling ma-
chine transliteration as a phrase based statistical ma-
chine translation problem. In Proc. ACL/IJCNLP
Named Entities Workshop Shared Task.
9
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proc. 45th Annual Meeting
of the ACL, pages 944?951, Prague, Czech Repub-
lic, June.
Praneeth Shishtla, V Surya Ganesh, S Sethurama-
lingam, and Vasudeva Varma. 2009. A language-
independent transliteration schema using character
aligned models. In Proc. ACL/IJCNLP Named Enti-
ties Workshop Shared Task.
Yan Song. 2009. Name entities transliteration via
improved statistical translation on character-level
chunks. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable cor-
pora. In Proc. 21st Int?l Conf Computational Lin-
guistics and 44th Annual Meeting of ACL, pages 73?
80, Sydney, Australia.
Raghavendra Udupa, K. Saravanan, Anton Bakalov,
and Abhijit Bhole. 2009. ?They are out there, if
you know where to look?: Mining transliterations
of OOV query terms for cross-language informa-
tion retrieval. In LNCS: Advances in Information
Retrieval, volume 5478, pages 437?448. Springer
Berlin / Heidelberg.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In Proc. EMNLP,
pages 612?617, Sydney, Australia, July.
Dmitry Zelenko. 2009. Combining MDL translitera-
tion training with discriminative modeling. In Proc.
ACL/IJCNLP Named Entities Workshop Shared
Task.
10
Team ID Organisation English to
Hindi
English to
Tamil
English to
Kannada
English to
Russian
English to
Chinese
English
to Ko-
rean
English
to
Japanese
Katakana
Japanese
translit-
erated to
Japanese
Kanji
EnHi EnTa EnKa EnRu EnCh EnKo EnJa JnJk
1 IIT Bombay x x x
2 Institution of Computational
Linguistics Peking Univer-
sity
x
3 University of Tokyo x x x x x x x
4? University of Illinois,
Urbana-Champaign
x x
5 IIT Bombay x x
6 NICT x x x x x x x x
7 University of Alberta x x x x x x
8 x x x x x x x x
9 x x x x x x x x
10 Johns Hopkins University x x x x x
11 x x x
12 x x
13 Jadavpur University x
14 IIIT Hyderabad x
15 x x x
16? ARL-CACI x
17 x x x x x x x x
18 x
19? Chaoyang University of
Technology
x
20 Pondicherry University x x x
21 Microsoft Research x x
22 SRI International x x x x x
23 IBM Cairo TDC x x
24 SRA x x x x x x x x
25 IIT Kharagpur x x x
26 Institute of Software Chinese
Academy of Sciences
x
27 x
28 George Washington Univer-
sity
x
29? x
30 Dublin City University x
31 IIIT x x x x x
Table 3: Participation of teams in different tasks. ?Participants without a system paper.
11
?
??
??
??
??
??
??
??
??
??
?
? ? ?? ?? ? ? ?? ?? ?? ?? ? ?? ?? ?? ?? ?? ? ? ?? ?? ??
???
????
??
Site?ID
(a) English to Hindi
?
??
??
??
??
??
??
??
??
??
?
? ?? ?? ? ?? ?? ? ? ?? ? ?? ? ?? ??
???
????
??
Site?ID
(b) English to Kannada
?
??
??
??
??
??
??
??
??
??
?
? ?? ?? ?? ? ?? ?? ? ? ?? ? ?? ??
???
????
??
Site?ID
(c) English to Tamil
???
????
?? ??
????
????
?
? ? ?? ?? ? ?? ?? ? ?? ? ? ?? ??
???
????
??
Site?ID
(d) English to Russian
???
????
?? ??
????
????
?
? ? ?? ? ? ?? ? ?? ?? ? ? ?? ?? ?? ?? ?? ?? ??
???
????
??
Site?ID
(e) English to Chinese
?
??
??
??
??
??
??
??
??
?? ? ?? ?? ? ? ? ?
???
????
??
Site?ID
(f) English to Korean
?
??
??
??
??
??
??
??
??
??
?
? ?? ?? ? ?? ? ? ?? ?? ?
???
????
??
Site?ID
(g) English to Japanese Katakana
?
??
??
??
??
??
??
??
??
??
?? ?? ? ? ? ? ??
???
????
??
Site?ID
(h) Japanese transliterated to Japanese Kanji
Figure 2: Accuracy in top-1, F -score and MRR for standard runs.
12
?
??
??
??
??
??
??
? ? ?? ?? ? ? ?? ?? ?? ?? ? ?? ?? ?? ?? ?? ? ? ?? ?? ??
?????
????
?????
Site?ID
(a) English to Hindi
?
??
??
??
??
??
??
? ?? ?? ? ?? ?? ? ? ?? ? ?? ? ?? ??
?????
????
?????
Site?ID
(b) English to Kannada
??
??
??
??
??
??
??
? ?? ?? ?? ? ?? ?? ? ? ?? ? ?? ??
?????
????
?????
Site?ID
(c) English to Tamil
?
??
??
??
??
??
??
? ? ?? ?? ? ?? ?? ? ?? ? ? ?? ??
?????
????
?????
Site?ID
(d) English to Russian
?
??
??
??
??
??
??
??
??
? ? ?? ? ? ?? ? ?? ?? ? ? ?? ?? ?? ?? ?? ?? ??
?????
????
?????
Site?ID
(e) English to Chinese
?
??
??
??
??
??
??
?? ? ?? ?? ? ? ? ?
?????
????
?????
Site?ID
(f) English to Korean
?
??
??
??
??
??
??
? ?? ?? ? ?? ? ? ?? ?? ?
?????
????
?????
Site?ID
(g) English to Japanese Katakana
?
??
??
??
??
??
??
?? ?? ? ? ? ? ??
?????
????
?????
Site?ID
(h) Japanese transliterated to Japanese Kanji
Figure 3: MAPref , MAP10 and MAPsys scores for standard runs.
13
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
7 0.498 0.890 0.603 0.488 0.195 0.195 University of Alberta
6 0.483 0.892 0.607 0.477 0.202 0.202 NICT
13 0.471 0.861 0.519 0.463 0.162 0.383 Jadavpur University
14 0.463 0.876 0.573 0.454 0.201 0.201 IIIT Hyderabad
8 0.462 0.876 0.576 0.454 0.189 0.189
1 0.423 0.863 0.544 0.417 0.179 0.202 IIT Bombay
11 0.418 0.879 0.546 0.412 0.183 0.240
21 0.418 0.864 0.522 0.409 0.170 0.170 Microsoft Research
17 0.415 0.858 0.505 0.406 0.164 0.168
24 0.409 0.864 0.527 0.402 0.174 0.176 SRA
5 0.409 0.881 0.546 0.400 0.184 0.184 IIT Bombay
31 0.407 0.877 0.544 0.402 0.195 0.195 IIIT
16 0.406 0.863 0.514 0.397 0.170 0.280 ARL-CACI
30 0.399 0.863 0.488 0.392 0.157 0.157 Dublin City University
10 0.398 0.855 0.515 0.389 0.170 0.170 Johns Hopkins University
25 0.366 0.854 0.493 0.360 0.164 0.164 IIT Kharagpur
3 0.363 0.864 0.503 0.360 0.170 0.170 University of Tokyo
9 0.349 0.829 0.455 0.341 0.151 0.151
22 0.212 0.788 0.317 0.207 0.106 0.106 SRI International
29 0.053 0.664 0.089 0.053 0.037 0.037
20 0.004 0.012 0.004 0.004 0.001 0.004 Pondicherry University
21 0.466 0.881 0.567 0.457 0.183 0.183 Microsoft Research (post-evaluation)
22 0.465 0.886 0.567 0.458 0.185 0.185 SRI International (post-evaluation)
Table 4: Standard runs for English to Hindi task.
TeamID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.474 0.910 0.608 0.465 0.204 0.204 NICT
17 0.436 0.894 0.551 0.427 0.184 0.189
11 0.435 0.902 0.572 0.430 0.195 0.265
31 0.406 0.894 0.542 0.399 0.193 0.193 IIIT
1 0.405 0.892 0.542 0.397 0.181 0.184 IIT Bombay
25 0.404 0.883 0.539 0.398 0.182 0.182 IIT Kharagpur
24 0.374 0.880 0.512 0.369 0.174 0.174 SRA
3 0.365 0.884 0.504 0.360 0.172 0.172 University of Tokyo
8 0.361 0.883 0.510 0.354 0.174 0.174
10 0.327 0.870 0.458 0.317 0.156 0.156 Johns Hopkins University
9 0.316 0.848 0.451 0.307 0.154 0.154
22 0.141 0.760 0.256 0.139 0.090 0.090 SRI International
20 0.061 0.131 0.068 0.059 0.021 0.056 Pondicherry University
22 0.475 0.909 0.581 0.466 0.193 0.193 SRI International (post-evaluation)
Table 5: Standard runs for English to Tamil task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.398 0.880 0.526 0.391 0.178 0.178 NICT
17 0.370 0.867 0.499 0.362 0.170 0.175
11 0.363 0.870 0.482 0.355 0.164 0.218
1 0.360 0.861 0.479 0.351 0.161 0.164 IIT Bombay
31 0.350 0.864 0.482 0.344 0.175 0.175 IIIT
24 0.345 0.854 0.462 0.336 0.157 0.157 SRA
8 0.343 0.855 0.458 0.334 0.155 0.155
5 0.335 0.859 0.453 0.327 0.154 0.154 IIT Bombay
25 0.335 0.856 0.457 0.328 0.154 0.154 IIT Kharagpur
3 0.324 0.856 0.438 0.315 0.148 0.148 University of Tokyo
10 0.235 0.817 0.353 0.229 0.121 0.121 Johns Hopkins University
9 0.177 0.799 0.307 0.178 0.109 0.109
22 0.091 0.735 0.180 0.090 0.064 0.064 SRI International
20 0.004 0.009 0.004 0.004 0.001 0.004 Pondicherry University
22 0.396 0.874 0.494 0.385 0.161 0.161 SRI International (post-evaluation)
Table 6: Standard runs for English to Kannada task.
14
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
7 0.613 0.928 0.696 0.613 0.212 0.212 University of Alberta
6 0.605 0.926 0.701 0.605 0.215 0.215 NICT
17 0.597 0.925 0.691 0.597 0.212 0.255
24 0.566 0.919 0.662 0.566 0.203 0.216 SRA
8 0.564 0.917 0.677 0.564 0.210 0.210
31 0.548 0.916 0.640 0.548 0.210 0.210 IIIT
23 0.545 0.917 0.596 0.545 0.286 0.299 IBM Cairo TDC
3 0.531 0.912 0.635 0.531 0.219 0.219 University of Tokyo
10 0.506 0.901 0.609 0.506 0.204 0.204 Johns Hopkins University
4 0.504 0.909 0.618 0.504 0.193 0.193 University of Illinois, Urbana-Champaign
9 0.500 0.906 0.613 0.500 0.192 0.192
22 0.364 0.876 0.440 0.364 0.136 0.136 SRI International
27 0.354 0.869 0.394 0.354 0.134 0.134
22 0.609 0.928 0.686 0.609 0.209 0.209 SRI International (post-evaluation)
Table 7: Standard runs for English to Russian task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.731 0.895 0.812 0.731 0.246 0.246 NICT
7 0.717 0.890 0.785 0.717 0.237 0.237 University of Alberta
15 0.713 0.883 0.794 0.713 0.241 0.241
8 0.666 0.864 0.765 0.666 0.234 0.234
2 0.652 0.858 0.755 0.652 0.232 0.232 Institution of Computational Linguistics Peking
University China
17 0.646 0.867 0.747 0.646 0.229 0.229
9 0.643 0.854 0.745 0.643 0.228 0.229
18 0.621 0.852 0.718 0.621 0.220 0.222
24 0.619 0.847 0.711 0.619 0.217 0.217 SRA
4 0.607 0.840 0.695 0.607 0.213 0.213 University of Illinois, Urbana-Champaign
3 0.580 0.826 0.653 0.580 0.199 0.199 University of Tokyo
26 0.498 0.786 0.603 0.498 0.187 0.189 Institute of Software Chinese Academy of Sci-
ences
31 0.493 0.804 0.600 0.493 0.192 0.192 IIIT
22 0.468 0.768 0.546 0.468 0.168 0.168 SRI International
28 0.456 0.763 0.587 0.456 0.185 0.185 George Washington University
10 0.450 0.755 0.514 0.450 0.157 0.166 Johns Hopkins University
23 0.411 0.737 0.464 0.411 0.141 0.173 IBM Cairo TDC
19 0.199 0.606 0.229 0.199 0.070 0.070 Chaoyang University of Technology
22 0.671 0.872 0.725 0.672 0.218 0.218 SRI International (post-evaluation)
Table 8: Standard runs for English to Chinese task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.476 0.742 0.596 0.477 0.187 0.199
6 0.473 0.740 0.584 0.473 0.182 0.182 NICT
12 0.451 0.720 0.576 0.451 0.181 0.181
24 0.413 0.702 0.524 0.412 0.165 0.165 SRA
7 0.387 0.693 0.469 0.387 0.146 0.146 University of Alberta
8 0.362 0.662 0.460 0.362 0.144 0.144
9 0.332 0.648 0.425 0.331 0.134 0.135
3 0.170 0.512 0.218 0.170 0.069 0.069 University of Tokyo
Table 9: Standard runs for English to Korean task.
15
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.537 0.858 0.657 0.529 0.223 0.223 NICT
15 0.510 0.838 0.624 0.498 0.209 0.209
17 0.503 0.843 0.627 0.491 0.212 0.212
7 0.500 0.847 0.604 0.487 0.199 0.199 University of Alberta
21 0.465 0.827 0.559 0.454 0.183 0.183 Microsoft Research
3 0.457 0.828 0.576 0.445 0.194 0.194 University of Tokyo
8 0.449 0.816 0.571 0.436 0.192 0.192
24 0.420 0.807 0.541 0.410 0.182 0.184 SRA
12 0.408 0.808 0.537 0.398 0.182 0.182
9 0.406 0.800 0.529 0.393 0.180 0.180
21 0.469 0.834 0.567 0.454 0.186 0.186 Microsoft Research (post-evaluation)
Table 10: Standard runs for English to Japanese Katakana task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
15 0.627 0.763 0.706 0.605 0.292 0.292
17 0.606 0.749 0.695 0.586 0.287 0.288
8 0.596 0.741 0.687 0.575 0.282 0.282
7 0.560 0.730 0.644 0.525 0.244 0.244 University of Alberta
9 0.555 0.708 0.653 0.538 0.261 0.261
6 0.532 0.716 0.583 0.485 0.214 0.218 NICT
24 0.509 0.675 0.600 0.491 0.226 0.226 SRA
Table 11: Standard runs for Japanese Transliterated to Japanese Kanji task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
7 0.509 0.893 0.610 0.498 0.198 0.198 University of Alberta
1 0.487 0.873 0.594 0.481 0.195 0.229 IIT Bombay
6 0.475 0.893 0.601 0.469 0.200 0.200 NICT
6 0.469 0.884 0.581 0.464 0.192 0.193 NICT
6 0.455 0.888 0.575 0.448 0.191 0.191 NICT
5 0.448 0.885 0.570 0.439 0.190 0.190 IIT Bombay
6 0.443 0.879 0.555 0.437 0.184 0.191 NICT
17 0.424 0.862 0.513 0.415 0.166 0.174
30 0.421 0.864 0.519 0.415 0.171 0.171 Dublin City University
30 0.420 0.867 0.519 0.413 0.170 0.170 Dublin City University
30 0.419 0.868 0.464 0.419 0.338 0.338 Dublin City University
16 0.407 0.862 0.528 0.399 0.175 0.289 ARL-CACI
16 0.407 0.862 0.528 0.399 0.175 0.289 ARL-CACI
30 0.407 0.856 0.507 0.399 0.168 0.168 Dublin City University
16 0.400 0.864 0.516 0.391 0.171 0.212 ARL-CACI
13 0.389 0.831 0.487 0.385 0.160 0.328 Jadavpur University
13 0.384 0.828 0.485 0.380 0.160 0.325 Jadavpur University
16 0.273 0.796 0.358 0.266 0.119 0.193 ARL-CACI
Table 12: Non-standard runs for English to Hindi task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.478 0.910 0.606 0.472 0.203 0.203 NICT
6 0.459 0.906 0.583 0.453 0.195 0.196 NICT
6 0.459 0.906 0.583 0.453 0.195 0.196 NICT
6 0.453 0.907 0.584 0.446 0.196 0.196 NICT
17 0.437 0.894 0.555 0.426 0.185 0.193
Table 13: Non-standard runs for English to Tamil task.
16
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.399 0.881 0.522 0.391 0.176 0.176 NICT
6 0.386 0.877 0.503 0.379 0.169 0.169 NICT
6 0.380 0.869 0.488 0.370 0.163 0.163 NICT
17 0.374 0.868 0.502 0.366 0.170 0.176
6 0.373 0.869 0.485 0.362 0.162 0.168 NICT
Table 14: Non-standard runs for English to Kannada task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.955 0.989 0.966 0.955 0.284 0.504
17 0.609 0.928 0.701 0.609 0.214 0.263
7 0.608 0.927 0.694 0.608 0.212 0.212 University of Alberta
7 0.607 0.927 0.690 0.607 0.211 0.211 University of Alberta
6 0.600 0.927 0.634 0.600 0.189 0.189 NICT
6 0.600 0.926 0.699 0.600 0.214 0.214 NICT
7 0.591 0.928 0.679 0.591 0.208 0.208 University of Alberta
6 0.561 0.918 0.595 0.561 0.178 0.182 NICT
6 0.557 0.920 0.596 0.557 0.179 0.233 NICT
23 0.545 0.917 0.618 0.545 0.188 0.206 IBM Cairo TDC
23 0.524 0.913 0.602 0.524 0.184 0.203 IBM Cairo TDC
23 0.524 0.913 0.579 0.524 0.277 0.291 IBM Cairo TDC
4 0.496 0.908 0.613 0.496 0.191 0.191 University of Illinois, Urbana-Champaign
27 0.338 0.872 0.408 0.338 0.128 0.128
27 0.293 0.845 0.325 0.293 0.099 0.099
27 0.162 0.849 0.298 0.162 0.188 0.188
Table 15: Non-standard runs for English to Russian task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.909 0.960 0.933 0.909 0.276 0.276
7 0.746 0.900 0.814 0.746 0.245 0.245 University of Alberta
7 0.734 0.895 0.807 0.734 0.244 0.244 University of Alberta
7 0.732 0.895 0.803 0.732 0.242 0.242 University of Alberta
6 0.731 0.894 0.812 0.731 0.246 0.246 NICT
6 0.715 0.890 0.741 0.715 0.220 0.231 NICT
6 0.699 0.884 0.729 0.699 0.216 0.232 NICT
6 0.684 0.873 0.711 0.684 0.211 0.211 NICT
22 0.663 0.867 0.754 0.663 0.230 0.230 SRI International
17 0.658 0.865 0.752 0.658 0.230 0.230
18 0.587 0.834 0.665 0.587 0.203 0.330
26 0.500 0.786 0.607 0.500 0.189 0.191 Institute of Software Chinese Academy of Sciences
22 0.487 0.787 0.622 0.487 0.196 0.196 SRI International
28 0.462 0.764 0.564 0.462 0.175 0.175 George Washington University
28 0.458 0.763 0.602 0.458 0.191 0.191 George Washington University
23 0.411 0.737 0.464 0.411 0.141 0.173 IBM Cairo TDC
19 0.279 0.668 0.351 0.279 0.110 0.110 Chaoyang University of Technology
28 0.058 0.353 0.269 0.058 0.101 0.101 George Washington University
28 0.050 0.359 0.260 0.050 0.098 0.098 George Washington University
4 0.001 0.249 0.001 0.001 0.000 0.000 University of Illinois, Urbana-Champaign
22 0.674 0.873 0.763 0.674 0.232 0.232 SRI International (post-evaluation)
22 0.500 0.793 0.636 0.500 0.200 0.200 SRI International (post-evaluation)
Table 16: Non-standard runs for English to Chinese task.
17
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.794 0.894 0.836 0.793 0.249 0.323
12 0.785 0.887 0.840 0.785 0.252 0.441
12 0.784 0.889 0.840 0.784 0.252 0.484
12 0.781 0.885 0.839 0.781 0.252 0.460
12 0.740 0.868 0.806 0.740 0.243 0.243
6 0.461 0.737 0.576 0.461 0.180 0.180 NICT
6 0.457 0.734 0.506 0.457 0.153 0.153 NICT
6 0.447 0.718 0.493 0.447 0.149 0.149 NICT
6 0.369 0.679 0.406 0.369 0.123 0.123 NICT
Table 17: Non-standard runs for English to Korean task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.535 0.858 0.656 0.526 0.222 0.222 NICT
6 0.517 0.850 0.567 0.495 0.177 0.188 NICT
6 0.513 0.854 0.567 0.495 0.178 0.178 NICT
7 0.510 0.848 0.614 0.496 0.202 0.202 University of Alberta
6 0.500 0.842 0.547 0.480 0.170 0.196 NICT
Table 18: Non-standard runs for English to Japanese Katakana task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.717 0.818 0.784 0.691 0.319 0.319
17 0.703 0.805 0.768 0.673 0.311 0.311
17 0.698 0.805 0.774 0.676 0.317 0.317
17 0.681 0.790 0.755 0.657 0.308 0.309
6 0.525 0.713 0.607 0.503 0.248 0.249 NICT
6 0.525 0.712 0.606 0.502 0.248 0.248 NICT
6 0.523 0.712 0.572 0.479 0.211 0.213 NICT
6 0.517 0.705 0.603 0.496 0.248 0.249 NICT
Table 19: Non-standard runs for Japanese Transliterated to Japanese Kanji task.
18
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 19?26,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Whitepaper of NEWS 2009 Machine Transliteration Shared Task?
Haizhou Li?, A Kumaran?, Min Zhang? and Vladimir Pervouchine?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
Transliteration is defined as phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is necessary in many applications, such
as machine translation, corpus alignment,
cross-language IR, information extraction
and automatic lexicon acquisition. All
such systems call for high-performance
transliteration, which is the focus of the
shared task in the NEWS 2009 workshop.
The objective of the shared task is to pro-
mote machine transliteration research by
providing a common benchmarking plat-
form for the community to evaluate the
state-of-the-art technologies.
1 Task Description
The task is to develop machine transliteration sys-
tem in one or more of the specified language pairs
being considered for the task. Each language pair
consists of a source and a target language. The
training and development data sets released for
each language pair are to be used for developing
a transliteration system in whatever way that the
participants find appropriate. At the evaluation
time, a test set of source names only would be re-
leased, on which the participants are expected to
produce a ranked list of transliteration candidates
in another language (i.e. n-best transliterations),
and this will be evaluated using common metrics.
For every language pair the participants must sub-
mit one run that uses only the data provided by the
NEWS workshop organisers in a given language
pair (designated as ?standard? runs). Users may
submit more runs (?non-standard?) for each lan-
guage pair that uses other data than those provided
by the NEWS 2009 workshop; such runs would be
evaluated and reported separately.
?http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/
2 Important Dates
Research paper submission deadline 1 May 2009
Shared task
Registration opens 16 Feb 2009
Registration closes 9 Apr 2009
Release Training/Development Data 16 Feb 2009
Release Test Data 10 Apr 2009
Results Submission Due 14 Apr 2009
Results Announcement 29 Apr 2009
Task (short) Papers Due 3 May 2009
For all submissions
Acceptance Notification 1 Jun 2009
Camera-Ready Copy Deadline 7 Jun 2009
Workshop Date 7 Aug 2009
3 Participation
1. Registration (16 Feb 2009)
(a) NEWS Shared Task opens for registra-
tion.
(b) Prospective participants are to register to
the NEWS Workshop homepage.
2. Training & Development Data (16 Feb 2009)
(a) Registered participants are to obtain
training and development data from the
Shared Task organiser and/or the desig-
nated copyright owners of databases.
3. Evaluation Script (16 Mar 2009)
(a) A sample test set and expected user out-
put format are to be released.
(b) An evaluation script, which runs on the
above two, is to be released.
(c) The participants must make sure that
their output is produced in a way that
the evaluation script may run and pro-
duce the expected output.
19
(d) The same script (with held out test data
and the user outputs) would be used for
final evaluation.
4. Test data (10 April 2009)
(a) The test data would be released on 10
Apr 2009, and the participants have a
maximum of 4 days to submit their re-
sults in the expected format.
(b) Only 1 ?standard? run must be submit-
ted from every group on a given lan-
guage pair; more ?non-standard? runs (0
to 4) may be submitted. In total, maxi-
mum 5 runs (1 ?standard? run plus up to
4 ?non-standard? runs) can be submit-
ted from each group on a registered lan-
guage pair.
(c) Any runs that are ?non-standard? must
be tagged as such.
(d) The test set is a list of names in source
language only. Every group will pro-
duce and submit a ranked list of translit-
eration candidates in another language
for each given name in the test set.
Please note that this shared task is a
?transliteration generation? task, i.e.,
given a name in a source language one
is supposed to generate one or more
transliterations in a target language. It
is not the task of ?transliteration discov-
ery?, i.e., given a name in the source lan-
guage and a set of names in the target
language evaluate how to find the ap-
propriate names from the target set that
are transliterations of the given source
name.
5. Results (29 April 2009)
(a) On 29 April 2009, the evaluation results
would be announced and will be made
available on the Workshop website.
(b) Note that only the scores (in respective
metrics) of the participating systems on
each language pairs would be published,
and no explicit ranking of the participat-
ing systems would be published.
(c) Note that this is a shared evaluation task
and not a competition; the results are
meant to be used to evaluate systems on
common data set with common metrics,
and not to rank the participating sys-
tems. While the participants can cite the
performance of their systems (scores on
metrics) from the workshop report, they
should not use any ranking information
in their publications.
(d) Further, all participants should agree not
to reveal identities of other participants
in any of their publications unless you
get permission from the other respective
participants. If the participants want
to remain anonymous in published
results, they should inform the or-
ganisers (mzhang@i2r.a-star.edu.sg,
a.kumaran@microsoft.com), at the time
of registration. Note that the results of
their systems would still be published,
but with the participant identities
masked. As a result, in this case, your
organisation name will still appear in
the web site as one of participants, but it
is not linked explicitly with your results.
6. Short Papers on Task (3 May 2009)
(a) Each submitting site is required to sub-
mit a 4-page system paper (short paper)
for its submissions, including their ap-
proach, data used and the results on ei-
ther test set or development set or by n-
fold cross validation on training set.
(b) All system short papers will be included
in the proceedings. Selected short pa-
pers will be presented orally in the
NEWS 2009 workshop. Reviewers?
comments for all system short papers
and the acceptance notification for the
system short papers for oral presentation
would be announced on 1 June 2009 to-
gether with that of other papers.
(c) All registered participants are required
to register and attend the workshop to
introduce your work.
(d) All paper submission and review
will be managed electronically
through https://www.softconf.com/acl-
ijcnlp09/NEWS/.
4 Languages Involved
The tasks are to transliterate personal names or
place names from a source to a target language as
summarised in Table 1.
20
Source language Target language Data Owner Approx. Data Size Task ID
English Chinese Institute for Infocomm Research 30K EnCh
English Japanese Katakana CJK Institute 25K EnJa
English Korean Hangul CJK Institute 7K EnKo
Japanese name (in English) Japanese Kanji CJK Institute 20K JnJk
English Hindi Microsoft Research India 15K EnHi
English Tamil Microsoft Research India 15K EnTa
English Kannada Microsoft Research India 15K EnKa
English Russian Microsoft Research India 10K EnRu
Table 1: Source and target languages for the shared task on transliteration.
The names given in the training sets for Chi-
nese, Japanese and Korean languages are Western
names and their CJK transliterations; the Japanese
Name (in English)? Japanese Kanji data set con-
sists only of native Japanese names. The Indic data
set (Hindi, Tamil, Kannada) consists of a mix of
Indian and Western names.
English? Chinese
Timothy????
English? Japanese Katakana
Harrington??????
English? Korean Hangul
Bennett ? ??
Japanese name in English? Japanese Kanji
Akihiro???
English? Hindi
San Francisco ? ????????????????
English? Tamil
London ? ??????
English? Kannada
Tokyo ? ??????
English? Russian
Moscow ? ??????
5 Standard Databases
Training Data (Parallel)
Paired names between source and target lan-
guages; size 5K ? 40K.
Training Data is used for training a basic
transliteration system.
Development Data (Parallel)
Paired names between source and target lan-
guages; size 1K ? 2K.
Development Data is in addition to the Train-
ing data, which is used for system fine-tuning
of parameters in case of need. Participants
are allowed to use it as part of training data.
Testing Data
Source names only; size 1K ? 3K.
This is a held-out set, which would be used
for evaluating the quality of the translitera-
tions.
1. Participants will need to obtain licenses from
the respective copyright owners and/or agree
to the terms and conditions of use that are
given on the downloading website (Li et al,
2004; Kumaran and Kellner, 2007; MSRI,
2009; CJKI, 2009). NEWS 2009 will pro-
vide the contact details of each individual
database. The data would be provided in Uni-
code UTF-8 encoding, in XML format; the
results are expected to be submitted in XML
format. The XML formats will be announced
at the workshop website.
2. The data are provided in 3 sets as described
above.
3. Name pairs are distributed as-is, as provided
by the respective creators.
(a) While the databases are mostly man-
ually checked, there may be still in-
consistency (that is, non-standard usage,
region-specific usage, errors, etc.) or in-
completeness (that is, not all right varia-
tions may be covered).
(b) The participants may use any method to
further clean up the data provided.
i. If they are cleaned up manually, we
appeal that such data be provided
back to the organisers for redistri-
bution to all the participating groups
in that language pair; such sharing
benefits all participants, and further
21
ensures that the evaluation provides
normalisation with respect to data
quality.
ii. If automatic cleanup were used,
such cleanup would be considered a
part of the system fielded, and hence
not required to be shared with all
participants.
4. We expect that the participants to use only the
data (parallel names) provided by the Shared
Task for transliteration task for a ?standard?
run to ensure a fair evaluation. One such run
(using only the data provided by the shared
task) is mandatory for all participants for a
given language pair that they participate in.
5. If more data (either parallel names data or
monolingual data) were used, then all such
runs using extra data must be marked as
?non-standard?. For such ?non-standard?
runs, it is required to disclose the size and
characteristics of the data used in the system
paper.
6. A participant may submit a maximum of 5
runs for a given language pair (including the
mandatory 1 ?standard? run).
6 Paper Format
Paper submissions to NEWS 2009 should follow
the ACL-IJCNLP-2009 paper submission policy,
including paper format, blind review policy and ti-
tle and author format convention. Full papers (re-
search paper) are in two-column format without
exceeding eight (8) pages of content plus one extra
page for references and short papers (task paper)
are also in two-column format without exceeding
four (4) pages, including references. Submission
must conform to the official ACL-IJCNLP-2009
style guidelines. For details, please refer to the
website2.
7 Evaluation Metrics
We plan to measure the quality of the translitera-
tion task using the following 6 metrics. We accept
up to 10 output candidates in a ranked list for each
input entry.
Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
2http://www.acl-ijcnlp-2009.org/main/authors/stylefiles/index.html
of these alternatives are considered as a correct
transliteration, and the first correct transliteration
in the ranked list is accepted as a correct hit.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
1. Word Accuracy in Top-1 (ACC) Also
known as Word Error Rate, it measures correct-
ness of the first transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ?ri,j : ri,j = ci,1;
0 otherwise
}
(1)
2. Fuzziness in Top-1 (Mean F-score) The
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.
Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
22
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses? etc.)
3. Mean Reciprocal Rank (MRR) Measures
traditional MRR for any right answer produced by
the system, from among the candidates. 1/MRR
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available. If all of
the references are produced, then the MAP is 1.
Let?s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
5. MAP10 measures the precision in the 10-best
candidates for i-th source name provided by the
candidate system. In general, the higher MAP10
is, the better is the quality of the transliteration
system in capturing the multiple references. Note
that the number of reference transliterations may
be more or less than 10. If the number of refer-
ence transliterations is below 10, then MAP10 can
never be equal to 1. Only if the number of ref-
erence transliterations for every source word is at
least 10, then MAP10 could possibly be equal to 1.
MAP10 =
1
N
N?
i=1
1
10
(
10?
k=1
num(i, k)
)
(10)
Note that in general MAPm measures the ?good-
ness in m-best? candidate list. We use m = 10
because we have asked the systems to produce up
to 10 candidates for every source name in the test
set.
6. MAPsys Measures the precision in the top
Ki-best candidates produced by the system for i-
th source name, for which ni reference translit-
erations are available. This measure allows the
systems to produce variable number of translitera-
tions, based on their confidence in identifying and
producing correct transliterations. If all of the ni
references are produced in the top-ni candidates
(that is, Ki = ni, and all of them are correct), then
the MAPsys is 1.
MAPsys =
1
N
N?
i=1
1
Ki
(
Ki?
k=1
num(i, k)
)
(11)
8 Contact Us
If you have any questions about this share task and
the database, please email to
Dr. Haizhou Li
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
hli@i2r.a-star.edu.sg
Dr. A. Kumaran
Microsoft Research India
Scientia, 196/36, Sadashivnagar 2nd Main
Road
Bangalore 560080 INDIA
a.kumaran@microsoft.com
Mr. Kurt Easterwood
The CJK Dictionary Institute (CJK Data)
Komine Building (3rd & 4th floors)
34-14, 2-chome, Tohoku, Niiza-shi
Saitama 352-0001 JAPAN
akurt@cjki.org
23
References
CJKI. 2009. CJK Institute. http://www.cjk.org/.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721?722.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159?166,
Barcelona, Spain.
MSRI. 2009. Microsoft Research India.
http://research.microsoft.com/india.
Appendix A: Training/Development Data
? File Naming Conventions:
NEWS09 train XXYY nnnn.xml
NEWS09 dev XXYY nnnn.xml
NEWS09 test XXYY nnnn.xml
? XX: Source Language
? YY: Target Language
? nnnn: size of parallel/monolingual
names (?25K?, ?10000?, etc)
? File formats:
All data will be made available in XML for-
mats (Figure 1).
? Data Encoding Formats:
The data will be in Unicode UTF-8 encod-
ing files without byte-order mark, and in the
XML format specified.
Appendix B: Submission of Results
? File Naming Conventions:
NEWS09 result XXYY gggg nn descr.xml
? XX: Source Language
? YY: Target Language
? gggg: Group ID
? nn: run ID. Note that run ID ?1? stands for ?stan-
dard? run where only the provided data are al-
lowed to be used. Run ID ?2?5? means ?non-
standard? run where additional data can be used.
? descr: Description of the run.
? File formats:
All data will be made available in XML formats (Fig-
ure 2).
? Data Encoding Formats:
The results are expected to be submitted in UTF-8 en-
coded files without byte-order mark only, and in the
XML format specified.
24
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationCorpus
CorpusID = "NEWS2009-Train-EnHi-25K"
SourceLang = "English"
TargetLang = "Hindi"
CorpusType = "Train|Dev"
CorpusSize = "25000"
CorpusFormat = "UTF8">
<Name ID=?1?>
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh1_1</TargetName>
<TargetName ID="2">hhhhhh1_2</TargetName>
...
<TargetName ID="n">hhhhhh1_n</TargetName>
</Name>
<Name ID=?2?>
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh2_1</TargetName>
<TargetName ID="2">hhhhhh2_2</TargetName>
...
<TargetName ID="m">hhhhhh2_m</TargetName>
</Name>
...
<!-- rest of the names to follow -->
...
</TransliterationCorpus>
Figure 1: File: NEWS2009 Train EnHi 25K.xml
25
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationTaskResults
SourceLang = "English"
TargetLang = "Hindi"
GroupID = "Trans University"
RunID = "1"
RunType = "Standard"
Comments = "HMM Run with params: alpha=0.8 beta=1.25">
<Name ID="1">
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh11</TargetName>
<TargetName ID="2">hhhhhh12</TargetName>
<TargetName ID="3">hhhhhh13</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
<Name ID="2">
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh21</TargetName>
<TargetName ID="2">hhhhhh22</TargetName>
<TargetName ID="3">hhhhhh23</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
...
<!-- All names in test corpus to follow -->
...
</TransliterationTaskResults>
Figure 2: Example file: NEWS2009 EnHi TUniv 01 StdRunHMMBased.xml
26
Coling 2010: Poster Volume, pages 639?646,
Beijing, August 2010
EM-based Hybrid Model for Bilingual Terminology Extraction 
from Comparable Corpora 
 
Lianhau Lee, Aiti Aw, Min Zhang, Haizhou Li 
Institute for Inforcomm Research 
{lhlee, aaiti, mzhang, hli}@i2r.a-star.edu.sg 
 
 
Abstract 
In this paper, we present an unsuper-
vised hybrid model which combines sta-
tistical, lexical, linguistic, contextual, 
and temporal features in a generic EM-
based framework to harvest bilingual 
terminology from comparable corpora 
through comparable document align-
ment constraint. The model is configur-
able for any language and is extensible 
for additional features. In overall, it pro-
duces considerable improvement in per-
formance over the baseline method. On 
top of that, our model has shown prom-
ising capability to discover new bilin-
gual terminology with limited usage of 
dictionaries. 
1 Introduction 
Bilingual terminology extraction or term align-
ment has been well studied in parallel corpora. 
Due to the coherent nature of parallel corpora, 
various statistical methods, like EM algorithm 
(Brown et. al., 1993) have been proven to be 
effective and have achieved excellent perform-
ance in term of precision and recall. The limita-
tion of parallel corpora in all domains and lan-
guages has led some researchers to explore 
ways to automate the parallel sentence extrac-
tion process from non-parallel corpora 
(Munteanu and Marcu, 2005; Fung and Cheung, 
2004) before proceeding to the usual term 
alignment extraction using the existing tech-
niques for parallel corpora. Nevertheless, the 
coverage is limited since parallel sentences in 
non-parallel corpora are minimal. 
Meanwhile, some researchers have started to 
exploit comparable corpora directly in a new 
manner. The motivations for such an approach 
are obvious: comparable corpora are abundantly 
available, from encyclopedia to daily newspa-
pers, and the human effort is reduced in either 
generating or collecting these corpora. If bilin-
gual terminology can be extracted directly from 
these corpora, evolving or emerging terminol-
ogies can be captured much faster than lexicog-
raphy and this would facilitate many tasks and 
applications in accessing cross-lingual informa-
tion. 
There remain challenges in term alignment 
for comparable corpora. The structures of texts, 
paragraphs and sentences can be very different. 
The similarity of content in two documents var-
ies through they talk about the same subject 
matter. Recent research in using transliteration 
(Udupa et. al., 2008; Knight and Graehl, 1998), 
context information (Morin et. al., 2007; Cao 
and Li, 2002; Fung, 1998), part-of-speech tag-
ging, frequency distribution (Tao and Zhai, 
2005) or some hybrid methods (Klementiev and 
Roth, 2006; Sadat et. al., 2003) have shone 
some light in dealing with comparable corpora. 
In particular, context information seems to be 
popular since it is ubiquitous and can be re-
trieved from corpora easily. 
In this paper, we propose an EM-based hy-
brid model for term alignment to address the 
issue. Through this model, we hope to discover 
new bilingual terminology from comparable 
corpora without supervision. In the following 
sections, the model will be explained in details. 
639
2 System Architecture 
It is expensive and challenging to extract bilin-
gual terminologies from a given set of compa-
rable corpora if they are noisy with very diverse 
topics. Thus the first thing we do is to derive the 
document association relationship between two 
corpora of different languages. To do this, we 
adopt the document alignment approach pro-
posed by Vu et. al. (2009) to harvest compara-
ble news document pairs. Their approach is re-
lying on 3 feature scores, namely Title-n-
Content (TNC), Linguistic Independent Unit 
(LIU), and Monolingual Term Distribution 
(MTD). In the nutshell, they exploit common 
words, numbers and identical strings in titles 
and contents as well as their distribution in time 
domain. Their method is shown to be superior 
to Tao and Zai (2005) which simply make use 
of frequency correlation of words. 
After we have retrieved comparable docu-
ment pairs, we tokenize these documents with 
prominent monolingual noun terms found 
within. We are interested only in noun terms 
since they are more informative and more im-
portantly they are more likely not to be covered 
by dictionary and we hope to find their transla-
tions through comparable bilingual corpora. We 
adopt the approach developed by Vu et. al. 
(2008). They first use the state-of-the-art C/NC-
Value method (Frantzi and Ananiadou, 1998) to 
extract terms based on the global context of the 
corpus, follow by refining the local terms for 
each document with a term re-extraction process 
(TREM) using Viterbi algorithm. 
 
 
Figure 1. The procedure of bilingual terminol-
ogy extraction from comparable documents.  
 
After these preprocesses, we have a set of 
comparable bilingual document pairs and a set 
of prominent monolingual noun terms for each 
monolingual document. The aim of our term 
alignment model is to discover new bilingual 
terminology formed from these monolingual 
terms across aligned document pairs (Figure.1). 
Like other approaches to comparable corpora, 
there exist many challenges in aligning bilingual 
terms due to the presence of noises and the sig-
nificant text-structure disparity across the com-
parable bilingual documents. To overcome this, 
we propose using both corpus-driven and non-
corpus-driven information, from which we draw 
various features and derive our hybrid model. 
These features are used to make initial guess on 
the alignment score of term pair candidates. Fig-
ure 2 shows the overall process of our term 
alignment model on comparable corpora. This 
model is language independent and it comprises 
several main components: 
? EM algorithm 
? Term alignment initialization 
? Mutual information (MI) & TScore res-
coring 
 
Figure 2. Term alignment model.  D = docu-
ment alignment score, L = lexical similarity, N 
= named entity similarity, C = context similar-
ity, T = temporal similarity, R = related term 
similarity. 
640
3 EM Algorithm 
We make two assumptions on the preprocesses 
that the extracted monolingual terms are good 
representatives of their source documents, and 
the document alignment scores derived from 
document alignment process are good indicators 
of how well the contents of various documents 
align. Hence, the logical implication suggests 
that the extracted terms from both well aligned 
documents could well be candidates of aligned 
term pairs. 
By reformulating the state-of-the-art EM-
based word alignment framework IBM model 1 
(Brown et. al., 1993), we can derive a term 
alignment model easily. In IBM word alignment 
model 1, the task is to find word alignment by 
using parallel sentences. In the reformulated 
model for term alignment, parallel sentences are 
replaced by comparable documents, character-
ized by document alignment score and their rep-
resentative monolingual terms. 
The significant advantage over the original 
IBM model 1 is the relaxation of parallel sen-
tences or parallel corpora, by incorporating an 
additional feature of document alignment score. 
We initialize the term alignment score of the 
corresponding term pair candidates with the 
document alignment score to reflect the confi-
dence level of document alignment. Other than 
that, we also employ a collection of feature 
similarity score: lexical similarity, named entity 
similarity, context similarity, temporal similar-
ity, and related term similarity, to term align-
ment initialization. We will explain this further 
in the next section. 
As we know, IBM model 1 will converge to 
the global maximum regardless of the initial 
assignment. This is truly good news for parallel 
corpora, but not for comparable corpora which 
contains a lot of noises. To prevent IBM model 
1 from overfitting, we choose to run ten itera-
tions (each iteration consists of one E-step and 
one M-step) for each cycle of EM in both e-f 
and f-e directions.  
After each cycle of EM process, we simply 
filter off the weak term alignment pairs of both 
directions with a high threshold (0.8) and popu-
late the lexicon database with the remaining 
pairs and use it to start another cycle of EM. 
The process repeats until no new term align-
ment pair is found. The EM algorithm for term 
alignment is shown as follow: 
 
Figure 3. EM algorithm for e-f direction, where 
e[k] = k-th aligned source document, f[k] = k-th 
aligned target document, e[k,i] = i-th term in 
e[k], f[k,j] = j-th term in f[k], a[i,j,k] = probabil-
ity of alignment from f[k,j] to e[k,i], t(f|e) = 
probability of alignment from term e to term f. 
4 Term Alignment Initialization 
We retrieve term alignment candidates by pair-
ing all possible combinations of extracted 
monolingual source terms and target terms 
across the aligned document pairs. Before each 
cycle of EM, we assign an initial term align-
ment score, t(f|e) to each of these term pair can-
didates. Basically, we initialize the term align-
ment score t(f|e) based on document alignment 
score (D), lexical similarity (L), named entity 
similarity (N), context similarity (C), temporal 
similarity (T), and related term similarity (R). 
The similarity calculations of the corpus-driven 
features (D, C, T, R) are derived directly from 
the corpus and require limited lexical resource. 
The non-corpus-driven features (L, N) make use 
of a small word based bilingual dictionary to 
measure their lexical relevancy. That makes our 
model not resource-demanding and it shows that 
our model can work under limited resource 
condition. 
All the above features contribute to the term 
alignment score t(f|e) independently, and we 
formulate their cumulative contributions as the 
following: 
Initialize t(f|e). 
for (iteration = 1 to 10) 
E step 
kjiallfor
ikejkft
ikejkft
kjia
i
,,,
]),[|],[(
]),[|],[(
],,[ ?=
 
M step 
),(,
),(
),(
)|(
),(],,,[),(
],[
,],[
:,,
feallfor
fetcount
fetcount
eft
feallforkjiafetcount
f
fjkf
eike
kji
?
?
=
=
==
 
End for.
641
)|()|()|()|(
)|()|()|(
,:),(
efRefTefCefN
efLEFDeft
FfEeFE
????
???
???
?= ?
??   
where, 
e = source term 
f  = target term 
E  = source document 
F   = target document 
D   = document alignment score 
L   = lexical similarity 
N   = named entity similarity 
C  = context similarity 
T   = temporal similarity 
R   = related term similarity 
  
 (1) 
 
This formula allows us to extend the model with 
additional features without affecting the existing 
configuration. 
4.1 Document Alignment Score (D) 
As explained in the Section 3, the relaxation on 
the requirement of parallel corpora in the new 
EM model leads to the incorporation of 
document alignment score. To indicate the 
confidence level of document alignment, we 
credit every aligned term pair candidate formed 
across the aligned documents with the 
corresponding document alignment score.  
Although it is not necessary, document 
alignment score is first normalized to the range 
of [0,1], with 1 indicates parallel alignment. 
4.2 Lexical Similarity (L) 
We design a simple lexical similarity measure-
ment of two terms based on word translation. 
Term pairs that share more than 50% of word 
translation pairs will be credited with lexical 
similarity of L0, where L0 is configurable con-
tribution weightage of lexical similarity. This 
provides us a primitive hint on term alignment 
without resorting to exhaustive dictionary 
lookup. 
??
? ?=
otherwise
efTifL
efL W
,1
5.0)|(,
)|( 0  
where L0 > 1 and  TW(f|e) is word translation 
score.  
(2)
 
4.3 Named Entity Similarity (N) 
Named entity similarity is a measure of prede-
fined category membership likelihood, such as 
person, location and organization. Term pairs 
that belong to the same NE categories will be 
credited with named entity similarity of N0, 
where N0 is a configurable weightage of named 
entity similarity. We use this similarity score to 
discover bilingual terms of same NE categories, 
yet not covered by bilingual dictionary. 
??
?=
otherwise
matchcategoriesNEifN
efN
,1
,
)|( 0  
where N0 > 1. 
(3)
 
4.4 Context Similarity (C) 
We assume that terms with similar contexts are 
likely to have similar meaning. Thus, we make 
use of context similarity to measure semantic 
similarity. Here, only k nearest content words 
(verbs, nouns, adjectives and adverbs) before or 
after the terms within the sentence boundary are 
considered as its contexts. The following shows 
the calculation of context similarity of two 
terms based on cosine similarity between their 
context frequency vectors before scaling to the 
range of [1, C0], where C0 is a configurable con-
tribution weightage of context similarity. As 
shown in the formula, the t(f?|e?) accounts for 
the translation probability from the source con-
text word to the target context word, hence the 
cosine similarity calculation is carried out in the 
target language domain. 
??
?
??
???
+=
)('
2
)('
2
)('
)('
0
)'()'(
)'|'()'()'(
)1(
1)|(
fcontextfecontexte
fcontextf
econtexte
ffreqefreq
eftffreqefreq
C
efC
 
where C0 > 1. 
(4)
  
4.5 Temporal Similarity (T) 
In temporal similarity, we make use of date in-
formation which is available in some corpus 
(e.g. news). We assume aligned terms are syn-
chronous in time, this is especially true for com-
parable news corpora (Tao and Zai, 2005). We 
642
use Discrete Fourier Transform (DFT) to trans-
form the distribution function of a term in dis-
crete time domain to a representative function in 
discrete frequency domain, which is usually 
known as ?spectrum?. We then calculate the 
power spectrum, which is defined as magnitude 
square of a spectrum. Power spectrum is sensi-
tive to the relative spacing in time (or frequency 
component), yet invariant to the shifting in time, 
thus it is most suitably to be used for pattern 
matching of time distribution. The temporal 
similarity is calculated based on cosine similar-
ity between the power spectrums of the two 
terms before scaling to the range of [1, T0], 
where T0 is a configurable contribution weight-
age of temporal similarity. 
 ( ) 1)(),(cos)1()|( 0 +?= kPkPineTefT fe             (5) 
where T0 > 1 and 
( )
2
1
0
2
2
22
)(
|)}({|)(
)()(
)()(
)(),(cos
?
??
?
?
=
??=
=
=
N
n
kn
N
i
x
xx
kk
k
enonFunctionDistributi
nonFunctionDistributiDFTkP
kvku
kvku
kvkuine
?
 
4.6 Related Term Similarity (R) 
Related terms are terms that correlate statisti-
cally in the same documents and they can be 
found by using mutual information or t-test in 
the monolingual corpus. Basically, related term 
similarity is a measure of related term likeli-
hood. Aligned terms are assumed to have simi-
lar related terms, hence related term similarity 
contributes to semantic similarity. The related 
term similarity is calculated based on weighted 
contribution from the related terms of the source 
term before scaling to the range of [1, R0], 
where R0 is a configurable contribution weight-
age of related terms similarity. 
 
1)|()1()|( 0 +?= efyRsimilaritRefR          (6) 
 where R0 > 1 and 
? ?
?
? ?
?=
Ff eRe
eRe
efvote
efvote
efyRsimilarit
)('
)('
)'|(
)'|(
)|(  
? ?
?
? ???
???
?
?
?
?
=
Ff
eeR
eRe
eeR
eRe
efvote
eeMIfew
efvote
eeMIfew
efvote
}]'{)'([
)("
}]'{)'([
)("
)"|(
)",(),"(
)"|(
)",(),"(
)'|(
 
???
?
???
?=
??
? ???=
)"()(
)",(
log)",(
,1
)()"(,5.1
),"(
epep
eep
eeMI
otherwise
fReTrif
few
 
 vote(f|e?) is initialized to 1 before it is com-
puted iteratively until it converges. R(e) is the 
set of related term of e and Tr(e) is the set of 
translated term of e. 
5 MI & TScore Rescoring 
We design the MI & TScore rescoring process 
to enhance the alignment score t(f|e) of e-f term 
pairs that have significant co-occurrence fre-
quencies in aligned document pairs, based on 
pointwise mutual information and TScore (or 
commonly known as t-test) of the terms. By 
using both measures concurrently, the associa-
tion relationship of a term pair can be assumed 
with higher confidence. On top of that, the asso-
ciation of a term pair can also be suggested by a 
much higher TScore value alone. In this rescor-
ing process, we scale up the alignment score 
t(f|e) of any term pair which is strongly associ-
ated by a constant factor. The following shows 
the mathematical expressions of what has been 
described, with M0 as the configurable scaling 
factor. 
 
Rescoring condition: 
andfeTScoreif 5.2),({[ ?          (7) 
)]','(6.0),(
)()'(
)()'(:)','(
feMIMaxfeMI
ffreqffreqor
efreqefreqfe = =
??  
thenfeTScoreor }5),( ?  
0)|()|( MefTefT ?=  
where M0 > 1 and 
N
fep
fpepfep
feTScore
2
),(
)()(),(
),(
?=  
),( feirNumberOfPaN =  
643
6 Experiment and Evaluation 
We conduct the experiment on articles from 
three newspapers of different languages pub-
lished by Singapore Press Holding (SPH), 
namely Straits Times1 (English), ZaoBao2 (Chi-
nese) and Berita Harian3 (Malay), in June 2006. 
There are 3187 English articles, 4316 Chinese 
articles and 1115 Malay articles. English is cho-
sen to be the source language and the remaining 
two languages as target languages. To analyze 
the effect of the quality of comparable docu-
ment in our term alignment model, we prepare 
two different input sets of document alignment, 
namely golden document alignment and auto-
mated document alignment for each source-
target language pair. The former is retrieved by 
linguistic experts who are requested to read the 
contents of the articles in the source and the tar-
get languages, and then match the articles with 
similar contents (e.g. news coverage on same 
story), while the latter is generated using unsu-
pervised method proposed by Vu et. al. (2009), 
mentioned in Section 2. 
In both cases of document alignments, only 
monolingual noun terms extracted automatically 
by program (Vu et. al., 2008) will be used as 
basic semantic unit. There are 23,107 unique 
English noun terms, 31,944 unique Chinese 
noun terms and 8,938 unique Malay noun terms 
extracted in overall. In average, there are 17.3 
noun term tokens extracted for each English 
document, 16.9 for Chinese document and 13.0 
for Malay document. Also note that the term 
alignment reference list is constructed based on 
these extracted monolingual terms under the 
constraints of document alignment. In other 
words, the linguistic experts are requested to 
match the extracted terms across aligned docu-
ment pairs (for both golden document alignment 
and automated document alignment sets respec-
tively). The numbers of comparable document 
pairs and the corresponding unique term align-
ment reference pairs are shown in Table 2. 
                                                 
1 http://www.straitstimes.com/ an English news 
agency in Singapore. Source ? Singapore Press 
Holdings Ltd. 
2 http://www.zaobao.com/ a Chinese news agency in 
Singapore. Source ? Singapore Press Holdings Ltd. 
3 http://cyberita.asia1.com.sg/ a Malay news agency 
in Singapore. Source ? Singapore Press Holdings 
Ltd. 
In the experiment, we will conduct the named 
entity recognition (NER) by using the devel-
oped system from the Stanford NLP Group, for 
English, and an in-house engine, for Chinese. 
Currently, there is no available NER engine for 
Malay.  
 
Dictionary E-C C-E E-M M-E 
Entry 23,979 71,287 28,496 18,935 
Table 1. Statistics of dictionaries, where E = English, 
C = Chinese, M = Malay. 
 
GoldenDocAlign AutomatedDocAlign 
Corpus Doc 
Align  
Term 
Align Ref 
Doc 
Align 
Term 
Align Ref 
ST-ZB 90 313 899 777 
ST-BH 42 113 475 358 
Table 2. Statistics of comparable document align-
ment pairs and term alignment reference pairs. 
 
For baseline, we make use of IBM model 1, 
modified in the same way which has been de-
scribed in the section 3, except that we treat all 
comparable documents as parallel sentences, i.e. 
document alignment score is 1. Precision and 
recall are used to evaluate the performance of 
the system. To achieve high precision, high 
thresholds are used in the system and they are 
kept constant throughout the experiments for 
consistency. To evaluate the capability of dis-
covering new bilingual terminology, we design 
a novelty metric, which is the ratio of the num-
ber of correct out-of-dictionary term alignment 
over the total number of correct term alignment. 
 
C
N
Novelty
G
C
Recall
T
C
Precision ===         (8) 
where, 
C = total number of correct term alignment result. 
T = total number of term alignment result. 
G = total number of term alignment reference. 
N     = total number of correct term alignment result 
that are out-of-dictionary. 
 
Table 3 shows the evaluation result of term 
alignment using EM algorithm with incremental 
feature setting. The particular order of setting is 
due to the implementation sequences and it is 
not expected to affect the result of analysis. 
We observe that the precision, recall and 
novelty of the system are comparatively higher 
when the golden document alignment is used 
instead of the automated document alignment.  
644
Table 3. Performance of term alignment using EM algorithm with incremental feature setting, where D = 
document alignment, L = lexical similarity, R = related term similarity, M = MI & TScore rescoring, N = 
named entity similarity, C = context similarity, T = temporal similarity.
 
This is expected since the golden document 
alignment provides document pairs with 
stronger semantic bonding. This also suggests 
that improving on the document alignment 
would further improve the term alignment re-
sult. 
It is noteworthy observation that the imple-
mented features improve the system precision 
and recall under various scenarios, although the 
degree of improvement varies from case to case. 
This shows the effectiveness of these features in 
the model.  
On the other hand, the novelty of the system 
is around 40%+ and 50%+ for ST-ZB and ST-
BH respectively (except for the automated 
document alignment in ST-BH scenarios). This 
suggests that the system can discover quite a 
large percentage of the correct bilingual termi-
nologies that do not exist in the lexicon initially. 
Compared with the baseline IBM model 1, 
there is an increase of 14.5% in precision, 
3.51% in recall and 2.9% in novelty for ST-ZB, 
using the golden document alignment. For ST-
BH, there is an even larger increase: 50% in 
precision, 7.96% in recall and 60% in novelty. 
7 Conclusion 
We have proposed an unsupervised EM-based 
hybrid model to extract bilingual terminology 
from comparable corpora through document 
alignment constraint. Our strategy is to make 
use of various information (corpus-driven and 
non-corpus-driven) to make initial guess on the 
semantic bonding of the term alignment candi-
dates before subjecting them to document 
alignment constraint through EM algorithm. 
The hybrid model allows inclusion of additional 
features without reconfigurations on existing 
features, this make it practically attractive. 
Moreover, the proposed system can be easily 
deployed in any language with minimal con-
figurations. 
We have successfully conducted the experi-
ments in English-Chinese and English-Malay 
comparable news corpora. The features em-
ployed in the model have shown incremental 
improvement in performance over the baseline 
method. In particular, the system shows im-
provement in the capability to discover new bi-
lingual terminology from comparable corpora 
even with limited usage of dictionaries. 
From the experiments, we have found that the 
quality of comparable bilingual documents is a 
GoldenDocAlign AutomatedDocAlign corpora Setting 
Precision Recall Novelty Precision Recall Novelty 
IBM 1 75.0% 1.92%  50.0% 22.2% 0.26% 50.0% 
(D) 75.0% 1.92% 50.0% 22.2% 0.26% 50.0% 
(D,L) 81.8% 2.88% 55.6% 33.3% 0.52% 25.0% 
(D,L,R) 81.8% 2.88% 55.6% 33.3% 0.52% 25.0% 
(D,L,R,M) 78.6% 3.51% 63.6% 35.7% 0.64% 40.0% 
(D,L,R,M,N) 88.2% 4.79% 53.3% 35.7% 0.64% 40.0% 
(D,L,R,M,N,C) 89.5% 5.43% 52.9% 33.3% 0.64% 40.0% 
ST-ZB 
(D,L,R,M,N,C,T) 89.5% (17/19) 
5.43% 
(17/313) 
52.9% 
(9/17) 
37.5% 
(6/16) 
0.77% 
(6/777) 
16.7%   
(1/6) 
IBM 1 33.3% 0.89% 0.00% 33.3% 0.78% 0.00% 
(D) 33.3% 0.89% 0.00% 33.3% 0.78% 0.00% 
(D,L) 75.0% 5.31% 50.0% 50.0% 1.94% 0.00% 
(D,L,R) 75.0% 5.31% 50.0% 50.0% 1.94% 0.00% 
(D,L,R,M) 75.0% 5.31% 50.0% 54.5% 2.33% 0.00% 
(D,L,R,M,N) 75.0% 5.31% 50.0% 54.5% 2.33% 0.00% 
(D,L,R,M,N,C) 83.3% 8.85% 60.0% 50.0% 1.94% 0.00% 
ST-BH 
(D,L,R,M,N,C,T) 83.3% (10/12) 
8.85% 
(10/113) 
60.0% 
(6/10) 
50.0% 
(5/10) 
1.94% 
(5/258) 
0.00% 
(0/5) 
645
major limiting factor to achieve good perform-
ance. In future, we want to explore ways to im-
prove on this. 
References 
R. Agrawal, C. Faloutsos, and A. Swami. 1993. Effi-
cient similarity search in sequence databases. In 
Proceedings of the 4th International Conference 
on Foundations of Data Organization and Algo-
rithms. Chicago, United States. 
P. F. Brown, V. S. A. Della Pietra, V. J. Della Pietra, 
and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2): 263-312. 
Yunbo Cao and Hang Li. 2002. Base Noun Phrase 
Translation Using Wed Data and the EM Algo-
rithm, Computational Linguistics, pp.1-7. 
Pascale Fung, 1998. A statistical view on bilingual 
lexicon extraction: From parallel corpora to non-
parallel corpora. Proceedings of AMTA, pp.1-17.  
Pascale Fung and Percy Cheung. 2004. Mining Very-
Non-Parallel Corpora: Parallel Sentence and 
Lexicon Extraction via Bootstrapping and EM, 
Proceedings of EMNLP, pp.57-63. 
Alexandre Klementiev and Dan Roth, 2006. Weakly 
Supervised Named Entity Transliteration and Dis-
covery from Multilingual Comparable Corpora. 
Computational Linguistics, pp. 817-824. 
K. Knight and J. Graehl. 1998. Machine translitera-
tion, Computational Linguistics, 24(4): 599-612.  
E. Morin, B. Daille, K. Takeuchi, K. Kageura. 2007. 
Bilingual Terminology Mining ? Using Brain, not 
brawn comparable corpora, Proceedings of ACL. 
Dragos Stefan Munteanu and Daniel Marcu. 2005. 
Improving Machine Translation Performance by 
Exploiting Non-Parallel Corpora. Computational 
Linguistics, 31(4): 477-504. 
Fatiha Sadat, Masatoshi Yoshikawa, Shunsuke Ue-
mura, 2003. Learning Bilingual Translations from 
Comparable Corpora to Cross-Language Infor-
mation Retrieval: Hybrid Statistics-based and 
Linguistics-based Approach. Proceedings of ACL, 
vol.11, pp.57-64. 
Tao Tao and Chengxiang Zhai. 2005. Mining com-
parable bilingual text corpora for cross-language 
information integration, Proceedings of ACM. 
Raghavendra Udupa, K. Saravanan, A. Kumaran, 
Jagadeesh Jagarlamudi. 2008. Mining named en-
tity transliteration equivalents from comparable 
corpora, Proceedings of ACM. 
Thuy Vu, Aiti Aw, Min Zhang, 2008. Term extrac-
tion through unithood and termhood unification. 
Proceedings of IJCNLP-08, Hyderabad, India. 
Thuy Vu, Aiti Aw, Min Zhang, 2009. Feature-based 
Method for Document Alignment in Comparable 
News Corpora. Proceedings of EACL-09, Athens, 
Greece. 
 
646
Coling 2010: Poster Volume, pages 972?978,
Beijing, August 2010
Improving Name Origin Recognition with Context Features and
Unlabelled Data
Vladimir Pervouchine, Min Zhang, Ming Liu and Haizhou Li
Institute for Infocomm Research, A-STAR
vpervouchine@gmail.com,{mzhang,mliu,hli}@i2r.a-star.edu.sg
Abstract
We demonstrate the use of context fea-
tures, namely, names of places, and un-
labelled data for the detection of per-
sonal name language of origin.
While some early work used either
rule-based methods or n-gram statisti-
cal models to determine the name lan-
guage of origin, we use the discrimi-
native classification maximum entropy
model and view the task as a classifica-
tion task. We perform bootstrapping of
the learning using list of names out of
context but with known origin and then
using expectation-maximisation algo-
rithm to further train the model on
a large corpus of names of unknown
origin but with context features. Us-
ing a relatively small unlabelled cor-
pus we improve the accuracy of name
origin recognition for names written
in Chinese from 82.7% to 85.8%, a
significant reduction in the error rate.
The improvement in F -score for infre-
quent Japanese names is even greater:
from 77.4% without context features to
82.8% with context features.
1 Introduction
Transliteration is a process of rewriting a
word from a source language to a target lan-
guage in a different writing system using the
word?s phonological equivalent. Many techni-
cal terms and proper nouns, such as personal
names, names of places and organisations are
transliterated during translation of a text from
one language to another. A process reverse
to the transliteration, which is recovering a
word in its native language from its translit-
eration in a foreign language, is called back-
transliteration (Knight and Graehl, 1998). In
many natural language processing (NLP) tasks
such as machine translation and cross-lingual
information retrieval, transliteration is an im-
portant component.
Name origin refers to the language of ori-
gin of a name. For example, the origin of En-
glish name ?Smith? and its Chinese transliter-
ation ???? (Shi-Mi-Si)? is English, while
both ?Tokyo? and ??? (Dong-Jing)? are of
Japanese origin.
For machine transliteration the name origins
dictate the way we re-write a foreign name.
For example, given a name written in Chi-
nese for which we do not have a translation
in an English-Chinese dictionary, we first have
to decide whether the name is of Chinese,
Japanese, Korean, English or another origin.
Then we follow the transliteration rules im-
plied by the origin of the name. Although
all English personal names are rendered in
26 letters, they may come from different ro-
manization systems. Each romanisation sys-
972
tem has its own rewriting rules. English name
?Smith? could be directly transliterated into
Chinese as ???? (Shi-Mi-Si)? since it fol-
lows the English phonetic rules, while the Chi-
nese translation of Japanese name ?Koizumi?
becomes ??? (Xiao-Quan)? following the
Japanese phonetic rules. The name origins
are equally important in back-transliteration.
Li et al (2007b) demonstrated that incorpo-
rating name origin recognition (NOR) into a
transliteration system greatly improves the per-
formance of personal name transliteration. Be-
sides multilingual processing, the name origin
also provides useful semantic information (re-
gional and language information) for common
NLP tasks, such as co-reference resolution and
name entity recognition.
Unfortunately, not much attention has been
given to name origin recognition (NOR) so far
in the literature. In this paper, we are inter-
ested in recognition of the origins of names
written in Chinese, which names can be of
three origins: Chinese, Japanese or English,
where ?English? is a rather broad category that
includes other West European and American
names written natively in Latin script.
Unlike previous work (Qu and Grefenstette,
2004; Li et al, 2007a; Li et al, 2007b),
where NOR was formulated with a genera-
tive model, we follow the approach of Zhang
et al (2008) and regard the NOR task as a
classification problem, using a discriminative
learning algorithm for classification. Further-
more, in the training data with names labelled
with their origin is rather limited, whereas
there is vast data from news articles that con-
tains many personal names without any labels
of their origins. In this research we propose
a method to harness the power of the unla-
belled noisy news data by bootstrapping the
learning process with labelled data and then
using the personal name context in the unla-
belled data to improve the NOR model. We
achieve that by using the maximum entropy
model and the expectation-maximisation train-
ing, and demonstrate that our method can sig-
nificantly improve the accuracy of NOR com-
pared to the baseline model trained only from
the labelled data.
The rest of the paper is organised as follows:
in Section 2 we review the previous research.
In Section 3 we present our approach, and in
Section 4 we describe our experimental setup,
the data used and the evaluation method. We
conclude in Section 5.
2 Related research
Most the research up to date focuses primar-
ily on recognition of origin of names written
in Latin script, called English NOR (ENOR),
although the same methods can be extended to
names in Chinese script (CNOR). We notice
that there are two informative clues that used
in previous work in ENOR. One is the lexi-
cal structure of a romanisation system, for ex-
ample, Hanyu Pinyin, Mandarin Wade-Giles,
Japanese Hepbrun or Korean Yale, each has
a finite set of syllable inventory (Li et al,
2007a). Another is the phonetic and phono-
tactic structure of a language, such as phonetic
composition, syllable structure. For example,
English has unique consonant clusters such
as ?str? and ?ks? which Chinese, Japanese
and Korean (CJK) do not have. Consider-
ing the NOR solutions by the use of these
two clues, we can roughly group them into
two categories: rule-based methods (for solu-
tions based on lexical structures) and statisti-
cal methods (for solutions based on phonotac-
tic structures).
Rule-based method Kuo et al (2007) pro-
posed using a rule-based method to recog-
nise different romanisation system for Chinese
only. The left-to-right longest match-based
lexical segmentation was used to parse a test
word. The romanisation system is confirmed
973
if it gives rise to a successful parse of the test
word. This kind of approach (Qu and Grefen-
stette, 2004) is suitable for romanisation sys-
tems that have a finite set of discriminative
syllable inventory, such as Pinyin for Chinese
Mandarin. For the general tasks of identifying
the language origin and romanisation system,
rule based approach sounds less attractive be-
cause not all languages have a finite set of dis-
criminative syllable inventory.
N-gram statistics methods
N-gram sum method Qu and Grefenstette
(2004) proposed a NOR identifier us-
ing a trigram language model (Cavnar
and Trenkle, 1994) to distinguish per-
sonal names of three language origins,
namely Chinese, Japanese and English.
In their work the training set includes
11,416 Chinese, 83,295 Japanese and
88,000 English name entries. How-
ever, the trigram is defined as the joint
probability p(cici?1ci?2) rather than the
commonly used conditional probability
p(ci|ci?1ci?2). Therefore it is basically
a substring unigram probability. For ori-
gin recognition of Japanese names, this
method works well with an accuracy of
92%. However, for English and Chinese,
the results are far behind with a reported
accuracy of 87% and 70% respectively.
N-gram perplexity method Li et al (2007a)
proposed a method of NOR using n-gram
character perplexity PPc to identify the
origin of names written in Latin script.
Using bigrams, the perplexity is defined
as
PPc = 2
?1
Nc
?Nc
i=1 log p(ci|ci?1)
whereNc is the total number of characters
in a given name, ci is the i-th character
in the name and p(ci|ci?1) is the bigram
probability learned from a list of names
of the same origin. Therefore, PPc can
be used to measure how well a new name
fits the model learned from the training
set of names. The origin is assigned ac-
cording to the model that gives the lowest
perplexity value. Li et al (2007a) demon-
strated that using PPc gives much better
performance than with the substring uni-
gram method.
Classification method Zhang et al (2008)
proposed using a discriminative classification
approach and extract features from the names.
They use Maximum Entropy (MaxEnt) model
and a number of features based on n-grams,
character positions, word length as well as
some rule-based phonetic features. They per-
formed both ENOR and CNOR and demon-
strated that their method indeed leads to better
performance in name origin recognition then
the n-gram statistics method. They attribute
that to the fact their model incorporates more
robust features than the n-gram statistics based
models.
In this paper we too follow the discriminat-
ing classification approach, but we add fea-
tures based on the context of a personal name.
These features require the original text with the
names to be available. Our approach closely
models the real-life situation when large cor-
pora of articles with personal names is read-
ily available in the Web, yet the origins of the
names are unknown.
3 Model and training methods
3.1 Maximum entropy model for NOR
The principle of maximum entropy is that
given a collection of facts we should choose
a model that is consistent with all the facts but
otherwise as uniform as possible (Berger et al,
1996). maximum entropy model (MaxEnt) is
known to easily combine diverse features and
974
has been used widely in natural language pro-
cessing research. Given an observation x the
probability of outcome label ci, i = 1 . . . N
given x is given by
p(ci|x) =
1
Z exp
?
?
n?
j=1
?jfj (x, ci)
?
? (1)
where N is the number of the outcome labels,
which is the number of name origins in our
case, n is the number of features, fj are the
feature functions and ?j are the model param-
eters. Each parameter corresponds to exactly
one feature and can be viewed as a ?weight?
for the corresponding feature. Z is the normal-
isation factor given by
Z =
N?
i=1
p(ci|x) (2)
In the problem at hand x is a personal name
and all the features are binary. The features,
also known as contextual predicates, are in the
form
fi(x, c) =
{
1 if c = ci and cp(x) = true
0 otherwise
(3)
where cp is the contextual predicate that maps
a pair (ci, x) to {true, false}.
In our experiments we use Zhang?s maxi-
mum entropy library1.
3.2 Initial training with labelled data and
n-gram features
For the initial training of MaxEnt model we
use labelled data: personal names of Chinese,
Japanese or English origin written in Chinese.
The origin of each name is known. Following
paper by Zhang et al (2008) and their findings
1http://homepages.inf.ed.ac.uk/lzhang10/
maxent toolkit.html
regarding the contribution value of each fea-
ture that they studied, we extract unigram, po-
sitional unigram and word length features. For
example, Chinese name ????? has the fol-
lowing features:
??? (?,0) (?,1) (?,2) 3
We restrict the n-gram features to unigram
only to avoid the data sparseness, because our
data contains a number of Chinese surnames
and given names, which have a length of one
or two characters.
3.3 Further training with unlabelled data
and context features
For further training of MaxEnt model we use
unlabelled data collected from news articles.
The name origin is not known but each per-
sonal name is in a context and is often sur-
rounded by names of places that may give a
hint about the personal name origin. For each
personal name we extract all names of places
in the same paragraph and use them as fea-
tures. If a place name is repeated many times
in the same paragraph we only include it once
in the feature list.
For example, paragraph containing passage
?The U.S. President Barack Obama ...? will
result in two personal names ?Barack? and
?Obama? having ?U.S.? as their context fea-
ture. Due to the diversity of place names we
also attempt to map the names of the places
into the country names. In this case, features
like ?U.S.?, ?USA?, ?America? are manually
substituted with ?USA?. In our experiments we
also try to narrow the place name extraction
to windows of different sizes surrounding the
personal name. The rationale here is that the
closer a place name is to the personal name,
the more likely it has a connection to the ori-
gin of the personal name.
In summary, our algorithm includes two
steps.
975
First, we use the boostrap data and n-gram,
positional n-gram and name length features to
do the initial training (the 0-th iteration) of
MaxEnt model with L-BFGS method (Byrd et
al., 1995). After that we use the model to as-
sign origin labels to names of the training set
of the unlabelled data.
Next, we use both the bootstrap data and
the training set of the unlabelled data, labelled
in the previous step, and add the context fea-
tures to the already used n-gram, positional n-
gram and name length features. Since there is
no context available for the bootstrap data, the
context features for it are missing, which can
be handled by the MaxEnt model. We perform
the Expectation-Maximisation (EM) iterations
by using the mixed data to train the i-th itera-
tion of the MaxEnt model, then use the model
to re-label the training set of the unlabelled
data and repeat the training of the model for
the (i + 1)-st iteration. We stop the iterations
when the ratio of patterns that change the ori-
gin labels becomes less than 0.01%.
4 Experiments
4.1 Corpora
The corpora consists of two datasets. One
dataset, called the ?bootstrap data?, is a set of
Chinese, Japanese and English names written
in Chinese following the respective translitera-
tion rules according to the name origins. The
names are a mixture of full names, first (given)
names and surnames. Table 1 shows the num-
ber of names of each origin. This is the la-
belled data; the origin of each name is known.
The data is used to start the MaxEnt model
training.
The second dataset, called the ?unlabelled
data?, is Chinese, Japanese and English per-
sonal names written in Chinese, which have
been extracted from the news articles col-
lected over 6 months from Xinhua news web-
site. The articles have been processed by an
Origin Number of names
Chinese 52,342
Japanese 26,171
English 26,171
Table 1: Number of names of each origin in
the bootstrap dataset.
automatic part-of-speech (POS) tagger, after
which personal names and names of places
have been manually identified (the latter for
extracting the context features). Normally the
first (given) name and surnames are identi-
fied as two separate personal names. The data
is split into a training set of 27,882 names
with unknown origin and a testing set of 1,476
names whose origin was manually assigned.
We split data in such a way that there is no
overlap between patterns in the training and
testing sets, although there may be overlap be-
tween names. For example, if a name may
be present in both training and testing sets but
in a different context, making the two names
two distinct patterns. The number of names
of each origin in the testing set is shown in
Table 2. As seen from the table, the number
Origin Number of names
Chinese 738
Japanese 369
English 422
Table 2: Number of names of each origin in
the testing dataset.
of Chinese names exceeds the number of En-
glish or Japanese names. This is an expected
consequence of using articles from a Chinese
news agency because many of the articles are
reporting on local affairs. We have manually
removed a number of Chinese name patterns
from the testing set, since the original percent-
age of Chinese names in the articles is about
83%.
976
4.2 Evaluation method
Following Zhang et al (2008) to make
our results comparable to theirs, we eval-
uate our system using precision Po, recall
Ro and F -score Fo for each origin o ?
{?Chinese ?? ?Japanese ?? ?English ??}. Let
the number of correctly recognised names of
a given origin o be ko, and the total number of
names recognised as being of origin o be mo,
while the actual number of names of origin o
be no. Then the precision, recall and F -score
are given as:
Po =
ko
mo
Ro =
ko
no
Fo =
2? Po ?Ro
Po +Ro
We also report the overall accuracy of the sys-
tem (or, rather the overall recall), which is the
ratio of the total number of correctly recog-
nised names to the number of all names:
Acc = kChinese + kJapanese + kEnglishnChinese + nJapanese + nEnglish
4.3 Results
After each iteration of our MaxEnt-based EM
algorithm, we record the number of patterns in
the training set that changed their origin labels,
as well as calculate the precision, recall and
F -score for each origin as well as the overall
accuracy. The results are reported in Tables 3
and 4, where for the sake of brevity the origin
subscripts are ?C?, ?J? and ?W? for Chinese,
Japanese and English name origin respectively.
Compared to the 0-th iteration there is an
significant improvement in accuracy, particu-
larly in recognition of Japanese names, which
are relatively infrequent compared to Chinese
and English ones in the unlabelled training
data. This clearly shows the effectiveness of
our proposed method.
Iteration PC PJ PW RC RJ RW0 0.887 0.724 0.857 0.823 0.911 0.761
1 0.914 0.736 0.875 0.823 0.968 0.775
2 0.910 0.736 0.874 0.823 0.968 0.767
3 0.914 0.737 0.874 0.824 0.973 0.767
4 0.913 0.742 0.875 0.825 0.968 0.778
Table 3: Results of running EM iterations,
original names of the places are kept.
Iteration Acc FC FJ FW
0 0.829 0.854 0.807 0.806
1 0.847 0.866 0.836 0.822
2 0.845 0.864 0.836 0.817
3 0.847 0.867 0.839 0.817
4 0.849 0.867 0.840 0.824
Table 4: Results of running EM iterations,
original names of the places are kept.
5 Conclusions
We propose extension of MaxEnt model for
NOR task by using two types of data for train-
ing: origin-labelled names alone and origin-
unlabelled names in their context surrounding.
We show how to apply a simple EM method to
make use of the contextual words as features,
and improve the NOR accuracy from 82.9%
to 84.9% overall, while for rare names such
as Japanese the effect of using unlabelled data
with context features is even greater.
The purpose of this research is to demon-
strate how the unlabelled data can be used. In
the future we hope to investigate the use of
other context features, as well as to study the
effect of data size on the NOR accuracy im-
provement.
The feature of names? places normally ex-
hibit great variation: one country name may be
spelled in many different ways, and often there
are names of cities etc that surround personal
names. We will explore to normalise names
of places by substituting each name with name
of the country where the place is in the future
work.
977
References
[Berger et al1996] Berger, A., Stephen A.
Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural lan-
guage processing. Computational Linguistics,
22(1):39?71.
[Byrd et al1995] Byrd, R. H., P. Lu, and J. Nocedal.
1995. A limited memory algorithm for bound
constrained optimization. SIAM Journal of Sci-
entific and Statistical Computing, 16(5):1190?
1208.
[Cavnar and Trenkle1994] Cavnar, William B. and
John M. Trenkle. 1994. Ngram based text cat-
egorization. In Proc. 3rd Annual Symposium on
Document Analysis and Information Retrieval,
pages 275?282.
[Knight and Graehl1998] Knight, Kevin and
Jonathan Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(4).
[Kuo et al2007] Kuo, Jin-Shea, Haizhou Li, and
Ying-Kuei Yang. 2007. A phonetic similarity
model for automatic extraction of transliteration
pairs. ACM Transactions on Asian Language In-
formation Processing, 6(2).
[Li et al2007a] Li, Haizhou, Shuanhu Bai, and Jin-
Shea Kuo. 2007a. Transliteration. In Advances
in Chinese Spoken Language Processing, chap-
ter 15, pages 341?364. World Scientific.
[Li et al2007b] Li, Haizhou, Khe Chai Sim, Jin-
Shea Kuo, and Minghui Dong. 2007b. Semantic
transliteration of personal names. In Proc. 45th
Annual Meeting of the ACL, pages 120?127.
[Qu and Grefenstette2004] Qu, Yan and Gregory
Grefenstette. 2004. Finding ideographic rep-
resentations of Japanese names written in Latin
script via language identification and corpus val-
idation. In Proc. 42nd ACL Annual Meeting,
pages 183?190, Barcelona, Spain.
[Zhang et al2008] Zhang, Min, Chengjie Sun,
Haizhou Li, Aiti Aw, Chew Lim Tan, and Xi-
aolong Wang. 2008. Name origin recognition
using maximum entropy model and diverse fea-
tures. In Proc. 3rd Int?l Conf. NLP, pages 56?63.
978
Coling 2010: Poster Volume, pages 1444?1452,
Beijing, August 2010
Machine Transliteration: Leveraging on Third Languages 
Min Zhang          Xiangyu Duan           Vladimir Pervouchine         Haizhou Li 
Institute for Infocomm Research, A-STAR 
{mzhang, xduan, vpervouchine, hli}@i2r.a-star.edu.sg 
  
Abstract 
This paper presents two pivot strategies 
for statistical machine transliteration, 
namely system-based pivot strategy 
and model-based pivot strategy. Given 
two independent source-pivot and pi-
vot-target name pair corpora, the mod-
el-based strategy learns a direct source-
target transliteration model while the 
system-based strategy learns a source-
pivot model and a pivot-target model, 
respectively. Experimental results on 
benchmark data show that the system-
based pivot strategy is effective in re-
ducing the high resource requirement 
of training corpus for low-density lan-
guage pairs while the model-based pi-
vot strategy performs worse than the 
system-based one. 
1 Introduction 
Many technical terms and proper names, such 
as personal, location and organization names, 
are translated from one language into another 
language with approximate phonetic equiva-
lents. This phonetic translation using computer 
is referred to as machine transliteration. With 
the rapid growth of the Internet data and the 
dramatic changes in the user demographics 
especially among the non-English speaking 
parts of the world, machine transliteration play 
a crucial role in  most multilingual NLP, MT 
and CLIR applications (Hermjakob et al, 
2008; Mandl and Womser-Hacker, 2004). This 
is because proper names account for the major-
ity of OOV issues and translation lexicons 
(even derived from large parallel corpora) 
usually fail to provide good coverage over di-
verse, dynamically increasing names across 
languages.  
Much research effort has been done to ad-
dress the transliteration issue in the research 
community (Knight and Graehl, 1998; Wan 
and Verspoor, 1998; Kang and Choi, 2000; 
Meng et al, 2001; Al-Onaizan and Knight, 
2002; Gao et al, 2004; Klementiev and Roth, 
2006; Sproat, 2006; Zelenko and Aone, 2006; 
Li et al, 2004, 2009a, 2009b; Sherif and Kon-
drak, 2007; Bertoldi et al, 2008; Goldwasser 
and Roth, 2008). These previous work can be 
categorized into three classes, i.e., grapheme-
based, phoneme-based and hybrid methods. 
Grapheme-based method (Li et al, 2004) 
treats transliteration as a direct orthographic 
mapping process and only uses orthography-
related features while phoneme-based method 
(Knight and Graehl, 1998) treats transliteration 
as a phonetic mapping issue, converting source 
grapheme to source phoneme followed by a 
mapping from source phoneme to target pho-
neme/grapheme. Hybrid method in machine 
transliteration refers to the combination of sev-
eral different models or decoders via re-
ranking their outputs. The report of the first 
machine transliteration shared task (Li et al, 
2009a, 2009b) provides benchmarking data in 
diverse language pairs and systemically sum-
marizes and compares different transliteration 
methods and systems using the benchmarking 
data. 
Although promising results have been re-
ported, one of major issues is that the state-of-
the-art machine transliteration approaches rely 
heavily on significant source-target parallel 
name pair corpus to learn transliteration model. 
However, such corpora are not always availa-
1444
ble and the amounts of the current available 
corpora, even for language pairs with English 
involved, are far from enough for training, let-
ting alone many low-density language pairs. 
Indeed, transliteration corpora for most lan-
guage pairs without English involved are un-
available and usually rather expensive to ma-
nually construct. However, to our knowledge, 
almost no previous work touches this issue. 
To address the above issue, this paper 
presents two pivot language-based translitera-
tion strategies for low-density language pairs. 
The first one is system-based strategy (Khapra 
et al, 2010), which learns a source-pivot mod-
el from source-pivot data and a pivot-target 
model from pivot-target data, respectively. In 
decoding, it first transliterates a source name to 
N-best pivot names and then transliterates each 
pivot names to target names which are finally 
re-ranked using the combined two individual 
model scores. The second one is model-based 
strategy. It learns a direct source-target transli-
teration model from two independent1 source-
pivot and pivot-target name pair corpora, and 
then does direct source-target transliteration. 
We verify the proposed methods using the 
benchmarking data released by the 
NEWS20092 (Li et al, 2009a, 2009b). Expe-
riential results show that without relying on 
any source-target parallel data the system-
based pivot strategy performs quite well while 
the model-based strategy is less effective in 
capturing the phonetic equivalent information. 
The remainder of the paper is organized as 
follows. Section 2 introduces the baseline me-
thod. Section 3 discusses the two pivot lan-
guage-based transliteration strategies. Experi-
mental results are reported at section 4. Final-
ly, we conclude the paper in section 5. 
2 The Transliteration Model 
Our study is targeted to be language-
independent so that it can be applied to 
different language pairs without any adaptation 
effort. To achieve this goal, we use joint 
source-channel model (JSCM, also named as 
                                                 
1 Here ?independent? means the source-pivot and 
pivot-target data are not derived from the same 
English name source. 
2  http://www.acl-ijcnlp-2009.org/workshops/NEWS 
2009/pages/sharedtask.html 
n-gram transliteration model) (Li et la., 2004) 
under grapheme-based framework as our 
transliteration model due to its state-of-the-art 
performance by only using orthographical 
information (Li et al, 2009a). In addition, 
unlike other feature-based methods, such as 
CRFs (Lafferty et al, 2001), MaxEnt (Berger 
et al, 1996) or SVM (Vapnik, 1995), the 
JSCM model directly computes model 
probabilities using maximum likelihood 
estimation (Dempster et al, 1977). This 
property facilitates the implementation of the 
model-based strategy.  
JSCM directly models how both source and 
target names can be generated simultaneously.  
Given a source name S and a target name T, it 
estimates the joint probability of S and T as 
follows: 
 
                                 
                              
           
                         
         
                                
    
 
   
 
                                   
    
 
   
 
 
where    and    is an aligned transliteration 
unit3 pair, and n is the n-gram order.  
In implementation, we compare different 
unsupervised transliteration alignment me-
thods, including Giza++ (Och and Ney, 2003), 
the JSCM-based EM algorithm (Li et al, 
2004), the edit distance-based EM algorithm 
(Pervouchine et al, 2009) and Oh et al?s 
alignment tool (Oh et al, 2009). Based on the 
aligned transliteration corpus, we simply learn 
the transliteration model using maximum like-
lihood estimation (Dempster et al, 1977) and 
decode the transliteration result    
              using stack decoder 
(Schwartz and Chow, 1990). 
                                                 
3 Transliteration unit is language dependent. It can 
be a Chinese character, a sub-string of English 
words, a Korean Hangual or a Japanese Kanji or 
several Japanese Katakanas.  
1445
3 Pivot Transliteration Strategies 
3.1 System-based Strategy  
The system-based strategy is first proposed by  
Khapra et al (2010). They worked on system-
based strategy together with CRF and did ex-
tensively empirical studies on In-
dic/Slavic/Semetic languages and English. 
Given a source name S, a target name T and 
let Z(S, ?) be the n-best transliterations of S in 
one or more pivot language ? 4, the system-
based transliteration strategy under JSCM can 
be formalized as follows: 
 
                          
       
 
          
 
 
  
                                
 
 
 
In the above formula, we assume that there is 
only one pivot language used in the derivation 
from the first line to the second line. Under the 
pivot transliteration framework, we can further 
simplify the above formula by assuming that   
is independent of    when given  . The as-
sumption holds because the parallel name cor-
pus between S and T is not available under the 
pivot transliteration framework. The n-best 
transliterations in pivot language are expected 
to be able to carry enough information of the 
source name S for translating S to target name 
T. Then, we have: 
                     
 
 
 
                
             
    
          
 
 
Obviously we can train the two JSCMs of 
       and        using the two parallel cor-
pora of        and      , and train the lan-
guage model      using the monolingual cor-
pus of   . Following the nature of JSCM, Eq. 
                                                 
4 There can be multiple pivot languages used in the 
two strategies. However, without loss of generality, 
we only use one pivot language to facilitate our 
discussion. It is very easy to extend one pivot lan-
guage to multiple ones by considering all the pivot 
transliterations in all pivot languages.  
(1) directly models how the source name S and 
pivot name   and how the pivot name   and 
the target name   are generated simultaneous-
ly. Since   is considered twice in        and 
      , the duplicated impact of   is removed 
by dividing the model by     . 
Given the model as described at Eq. (1), the 
decoder can be formulized as: 
                 
 
       
           
 
  
             
    
 
      
 
If we consider multiple pivot languages, the 
modeling and decoding process are: 
       
    
                       
         
 
       
 
 
       
       
 
   
                       
         
       
  
 
3.2 Model-based Strategy 
Rather than combining the transitive translite-
ration results at system level, the model-based 
strategy aims to learn a direct model       by 
combining the two individual models of 
       and       , which are learned from 
the two parallel corpora of       and      , 
respectively. Now let us use bigram as an ex-
ample to illustrate how to learn the translitera-
tion model                   
 
   
         using the model-based strategy. 
 
                       
 
                  
           
        
where,  
 
                     
                           
                            
       
 
                             
       
                    
1446
The same as the system-based strategy, we 
can further simplify the above formula by as-
suming that   is independent of    when given 
 . Indeed,                            cannot 
be estimated directly from training corpus. 
Then we have:  
                   
                             
       
                    
                    
       
                    
                          
       
                    
                                        
where                   ,                    
and            can be directly learned from 
training corpus.              for Eq (3) can 
also be estimatedas follows.  
 
             
                     
      
 
 
In summary, eq. (1) formulizes the system-
based strategy and eq. (3), (4) and (5) formul-
ize the model-based strategy, where we can 
find that they share the same nature of generat-
ing source, pivot and target names simulta-
neously. The difference is that the model-based 
strategy operates at fine-grained transliteration 
unit level. 
3.3 Comparison with Previous Work  
Almost all previous work on machine translite-
ration focuses on direct transliteration or trans-
literation system combination. There is only 
one recent work (Khapra et al, 2010) touching 
this issue. They work on system-based strategy 
together with CRF. Compared with their work, 
this paper gives more formal definitions and 
derivations of system-based strategy from 
modeling and decoding viewpoints based on 
the JSCM model.  
The pivot-based strategies at both system 
and model levels have been explored in ma-
chine translation. Bertoldi et al (2008) studies 
two pivot approaches for phrase-based statis-
tical machine translation. One is at system lev-
el and one is to re-construct source-target data 
and alignments through pivot data. Cohn and 
Lapata (2007) explores how to utilize multilin-
gual parallel data (rather than pivot data) to 
improve translation performance. Wu and 
Wang (2007, 2009) extensively studies the 
model-level pivot approach and also explores 
how to leverage on rule-based translation re-
sults in pivot language to improve translation 
performance. Utiyama and Isahara (2007) 
compares different pivot approaches for 
phrase-based statistical machine translation. 
All of the previous work on machine transla-
tion works on phrase-based statistical machine 
translation. Therefore, their translation model 
is to calculate phrase-based conditional proba-
bilities at unigram level (        ) while our 
transliteration model is to calculate joint trans-
literation unit-based conditional probabilities 
at bigram level (                   ). 
4 Experimental Results 
4.1 Experimental Settings 
We use the NEWS 2009 benchmark data as 
our experimental data (Li et al, 2009). The 
NEWS 2009 data includes 8 language pairs, 
where we select English to Chinese/Japanese 
/Korean data (E-C/J/K) and based on which we 
further construct Chinese to Japanese/Korean 
and Japanese to Korean for our data.  
 
Language Pair Training Dev Test 
English-Chinese 31,961  2896 2896 
English-Japanese 23,225 1492 1489 
English-Korean 4,785 987 989 
Chinese-Japanese 12,417 75 77 
Chinese-Korean 2,148 32 31 
Japanese-Korean 6,035 65 69 
 
Table 1. Statistics on the data set 
 
Table 1 reports the statistics of all the expe-
rimental data. To have a more accurate evalua-
tion, the test sets have been cleaned up to make 
sure that there is no overlapping between any 
test set with any training set. In addition, the 
three E-C/J/K data are generated independently 
so that there is very small percentage of over-
1447
lapping between them. This can ensures the 
evaluation of the pivot study fair and accurate.  
We compare different alignment algorithms 
on the DEV set. Finally we use Pervouchine et 
al. (2009)?s alignment algorithm for Chinese-
English/Japanese/Korean and Oh et al 
(2009)?s alignment algorithm for English-
Korean and Li et al (2004)?s alignment algo-
rithm for English-Japanese and Japanese-
Korean. Given the aligned corpora, we directly 
learn each individual JSCM model (i.e., n-
gram transliteration model) using SRILM tool-
kits (Stolcke, 2002). We also use SRILM tool-
kits to do decoding. For the system-based 
strategy, we output top-20 pivot transliteration 
results.  
For the evaluation matrix, we mainly use 
top-1 accuracy (ACC) (Li et al, 2009a) to 
measure transliteration performance. For refer-
ence purpose, we also report the performance 
using all the other evaluation matrixes used in 
NEWS 2009 benchmarking (Li et al, 2009a), 
including F-score, MRR, MAP_ref, MAP_10 
and MAP_sys. It is reported that F-score has 
less correlation with other matrixes (Li et al, 
2009a). 
4.2 Experimental Results 
4.2.1 Results of Direct Transliteration 
Table 2 reports the performance of direct trans-
literation. The first three experiments (line 1-3) 
are part of the NEWS 2009 share tasks and the 
others are our additional experiments for our 
pivot studies.  
Comparison of the first three experimental 
results and the results reported at NEWS 2009 
shows that we achieve comparable perfor-
mance with their best-reported systems at the 
same conditions of using single system and 
orthographic features only. This indicates that 
our baseline represents the state-of-the-art per-
formance. In addition, we find that the back-
transliteration (line 4-6) consistently performs 
worse than its corresponding forward-
transliteration (line 1-3). This observation is 
consistent with what reported at previous work 
(Li et al, 2004; Zhang et al, 2004). The main 
reason is because English has much more 
transliteration units than foreign C/J/K lan-
guages. This makes the transliteration from 
English to C/J/K a many-to-few mapping issue 
and back-transliteration a few-to-many map-
ping issue. Therefore back-transliteration has 
more ambiguities and thus is more difficult. 
Overall, the lower six experiments (line 7-
12) shows worse performance than the upper 
six experiments which has English involved. 
This is mainly due to the less available training 
data for the language pairs without English 
involved. This observation motivates our study 
using pivot language for machine translitera-
tion. 
4.2.2 Results of System-based Strategy 
Table 3 reports three empirical studies of sys-
tem-based strategies: Japanese to Chinese 
through English, Chinese to Japanese through 
English and Chinese to Korean through Eng-
lish. Considering the fact that those language 
pairs with English involved have the most 
training data, we select English as pivot lan-
guage in the system-based study. Table 3 
clearly shows that:  
? The system-based pivot strategy is very 
effective, achieving significant perfor-
mance improvement over the direct 
transliteration by 0.09, 0.07 and 0.03 
point of ACC in the three language pairs, 
respectively; 
? Different from other pipeline methodol-
ogies, the system-based pivot strategy 
does not suffer heavily from the error 
propagation issue. Its ACC is significant-
ly better than the product of the ACCs of 
the two individual systems; 
? The combination of pivot system and di-
rect system slightly improves overall 
ACC. 
We then conduct more experiments to figure 
out the reasons. Our further statistics and anal-
ysis show the following reasons for the above 
observations: 
The pivot approach is able to use source-
pivot and pivot-target data whose amount is 
much more than that of the available direct 
source-target data.  
? The nature of transliteration is phonetic 
translation. Therefore a little bit variation 
in orthography may not hurt or even help 
to improve transliteration performance in 
some cases as long as the orthographical 
variations keep the phonetic equivalent 
1448
Language Pairs ACC F-Score MRR MAP_ref MAP_10 MAP_sys 
English  Chinese 0.678867 0.871497 0.771563 0.678867 0.252382 0.252382 
English  Japanese 0.482203 0.831983 0.594235 0.471766 0.201510 0.201510 
English  Korean 0.439838 0.722365 0.543039 0.439585 0.171621 0.171621 
Chinese  English 0.395250 0.867702 0.518292 0.372403 0.222787 0.222787 
Japanese  English 0.334839 0.838212 0.450984 0.319277 0.168032 0.168032 
Korean  English 0.088505 0.494205 0.109249 0.088759 0.034380 0.034380 
Chinese  Japanese 0.385965 0.769245 0.473851 0.348319 0.159948 0.159948 
Japanese  Chinese 0.402597 0.714193 0.491595 0.402597 0.165581 0.165581 
Chinese  Korean 0.290323 0.571587 0.341129 0.290323 0.178652 0.178652 
Korean  Chinese 0.129032 0.280645 0.156042 0.129032 0.048163 0.048163 
Japanese  Korean 0.313433 0.678240 0.422862 0.313433 0.208310 0.208310 
Korean  Japanese 0.089286 0.321617 0.143948 0.091270 0.049992 0.049992 
 
Table 2. Performance of direct transliterations 
 
 
Language Pairs ACC   F-Score MRR MAP_ref MAP_10 MAP_sys 
Jap Eng Chi (Pivot) 0.493506 0.750711 0.617440 0.493506 0.195151 0.195151 
Jap Eng Chi (Pivot)  
+ Jap  Chi (Direct) 
0.506494 0.753958 0.622851 0.506494 0.196017 0.196017 
Jap  Chi (Direct) 0.402597 0.714193 0.491595 0.402597 0.165581 0.165581 
Jap  Eng (Direct) 0.334839 0.838212 0.450984 0.319277 0.168032 0.168032 
Eng  Chi (Direct) 0.678867 0.871497 0.771563 0.678867 0.252382 0.252382 
Chi Eng Jap (Pivot) 0.456140 0.777494 0.536591 0.414961 0.183222 0.183222 
Chi Eng Jap (Pivot) 
 + Chi  Jap (Direct) 
0.491228 0.801443 0.563297 0.450049 0.191742 0.191742 
Chi  Jap (Direct) 0.385965 0.769245 0.473851 0.348319 0.159948 0.159948 
Chi  Eng (Direct) 0.395250 0.867702 0.518292 0.372403 0.222787 0.222787 
Eng  Jap (Direct) 0.482203 0.831983 0.594235 0.471766 0.201510 0.201510 
Chi Eng Kor (Pivot) 0.322581 0.628146 0.432642 0.322581 0.175822 0.175822 
Chi Eng Kor (Pivot)   
+ Chi  Kor (Direct) 
0.331631 0.632967 0.439143 0.334222 0.176543 0.176543 
Chi  Kor (Direct) 0.290323 0.571587 0.341129 0.290323 0.178652 0.178652 
Chi  Eng (Direct) 0.395250 0.867702 0.518292 0.372403 0.222787 0.222787 
Eng  Kor (Direct) 0.439838 0.722365 0.543039 0.439585 0.171621 0.171621 
 
Table 3. Performance comparison of system-based strategy on Jap (Japanese) to Chi (Chinese) and 
Chi (Chinese) to Jap (Japanese)/Kor (Korean) through Eng (English) as pivot language, 
where ??(Pivot) + ?(Direct)? means that for the same language pair we merge and re-
rank the pivot transliteration and direct  transliteration results 
 
information. Indeed, given one source 
English names, there are usually more 
than one correct transliteration references 
in Japanese/Korean. This case also hap-
pens to English to Chinese although not 
so heavy as in English to Japa-
nese/Korean. 
 
1449
Language Pairs ACC   F-Score MRR MAP_ref MAP_10 MAP_sys 
Chi Eng Jap  
(Model-based Pivot: O) 
0.087719 0.538454 0.117446 0.085770 0.040645 0.040645 
Chi Eng Jap  
(Model-based Pivot: R) 
0.210526 0.746497 0.381210 0.201267 0.156106 0.156106 
Chi Eng Jap  
(System-based Pivot) 
0.456140 0.777494 0.536591 0.414961 0.183222 0.183222 
Chi  Jap  (Direct) 0.385965 0.769245 0.473851 0.348319 0.159948 0.159948 
Jap Chi Eng  
(Model-based Pivot) 
0.148504 0.724623 0.224253 0.141791 0.088966 0.088966 
Jap Chi Eng 
(System-based Pivot) 
0.201581 0.741627 0.266507 0.191926 0.098024 0.134730 
Jap  Eng (Direct) 0.334839 0.838212 0.450984 0.319277 0.168032 0.168032 
Eng Jap Kor  
(Model-based Pivot) 
0.206269 0.547732 0.300641 0.206269 0.145882 0.145882 
Eng Jap Kor 
(System-based Pivot) 
0.315470 0.629640 0.404769 0.315723 0.167587 0.225892 
Eng  Kor (Direct) 0.439838 0.722365 0.543039 0.439585 0.171621 0.171621 
 
Table 4. Performance of Model-based Pivot Transliteration Strategy 
 
? The N-best accuracy of machine transli-
teration (of both to and from English) is 
very high5. It means that in most cases 
the correct transliteration in pivot lan-
guage can be found in the top-20 results 
and the other 19 results hold the similar 
pronunciations with the correct one, 
which can serve as alternative ?quasi-
correct? inputs to the second stage trans-
literations and thus largely improve the 
overall accuracy.  
 
The above analysis holds when using Eng-
lish as pivot language. Now let us see the case 
of using non-English as pivot language. Table 
4 reports two system-based strategies using 
Chinese and Japanese as pivot languages, 
                                                 
5  Both our studies and previous work (Li et al, 
2004; Zhang et al, 2004) shows that the top-20 
accuracy from English to J/K is more than 0.85 and 
more than 0.95 in English-Chinese case. The top-20 
accuracy is a little worse from C/J/K to English, but 
still more than 0.7. 
where we can find that the performance of two 
system-based strategies is worse than that of 
the direct transliterations. The main reason is 
because that the direct transliteration utilizes 
much more training data than the pivot ap-
proach. However, the good thing is that the 
system-based pivot strategy using non-English 
as pivot language still does not suffer from 
error propagation issue. Its ACC is significant-
ly better than the product of the ACCs of the 
two individual systems. 
4.2.3 Results of Model-based Strategy 
Table 4 reports the performance of model-
based strategy. It clearly shows that the model-
based strategy is less effective and performs 
much worse than both the system-based strate-
gy and direct transliteration.  
While the model-based strategy works well 
at phrase-based statistical machine translation 
(Wu and Wang, 2007, 2009), it does not work 
at machine transliteration. To investigate the 
reasons, we conduct many additional experi-
ments and do statistics on the model and 
1450
aligned training data6. From this in-depth anal-
ysis, we find that main reason is due to the fact 
that the model-based strategy introduces too 
many entries (ambiguities) to the final transli-
teration model. For example, in the 
Jap Chi Eng experiment, the unigram and 
bigram entries of the transliteration model ob-
tained by the model-based strategy are 45 and 
6.6 times larger than that of the transliteration 
model trained directly from parallel data.  This 
is not surprising. Given a transliteration unit in 
pivot language, it can generate     source-
to-target transliteration unit mappings (unigram 
entry of the model), where  is the number of 
the source units that can be mapped to the pi-
vot unit and   is the number of the target units 
that can be mapped from the pivot unit. 
Besides the ambiguities introduced by the 
large amount of entries in the model, another 
reason that leads to the worse performance of 
model-based strategy is the size inconsistence 
of transliteration unit of pivot language. As 
shown at Table 4, we conduct three experi-
ments. In the first experiment (Chi Eng Jap), 
we use English as pivot language. We find that 
the English transliteration unit size in 
Chi Eng model is much larger than that in 
Eng Jap model. This is because from phonetic 
viewpoint, in Chi Eng model, the English unit 
is at syllable level (corresponding one Chinese 
character) while in Eng Jap model, the English 
unit is at sub-syllable level (consonant or vowel 
or syllable, corresponding one Japanese Kata-
kana). This is the reason why we conduct two 
model-based experiments for Chi Eng Jap. 
One is based on the original alignments (Mod-
el-based Pivot: O) and one is based on the re-
constructed alignments 7  (Model-based Pivot: 
R). Experimental results clearly show that the 
reconstruction improves performance signifi-
cantly. In the second and third experiments 
(Jap Chi Eng, Eng Jap Kor), we use Chi-
nese and Japanese as pivot languages. Therefore 
we do not need to re-construct transliteration 
                                                 
6 However, due to space limitation, we are not al-
lowed to report the details of those experiments.  
7Based on the English transliteration units obtained 
from Chi Eng, we reconstruct the English transli-
teration units and alignments in Eng Jap by merg-
ing the adjacent units of both English and Japanese 
to syllable level. 
units and alignments. However, the perfor-
mance is still very poor. This is due to the first 
reason of the large amount of ambiguities. 
The above two reasons (ambiguities and 
transliteration unit inconsistence) are mixed 
together, leading to the worse performance of 
the model-based strategy. We believe that the 
fundamental reason is because the pivot transli-
teration unit is too small to be able to convey 
enough phonetic information of source lan-
guage to target language and thus generates too 
many alignments and ambiguities. 
5 Conclusions 
A big challenge to statistical-based machine 
transliteration is the lack of the training data, 
esp. to those language pairs without English 
involved. To address this issue, inspired by the 
research in the SMT research community, we 
study two pivot transliteration methods. One is 
at system level while another one is at model 
level. We conduct extensive experiments using 
NEW 2009 benchmarking data. Experimental 
results show that system-based method is very 
effective in capturing the phonetic information 
of source language. It not only avoids success-
fully the error propagation issue, but also fur-
ther boosts the transliteration performance by 
generating more alternative pivot results as the 
inputs of the second stage. In contrast, the 
model-based method in its current form fails to 
convey enough phonetic information from 
source language to target language.  
For the future work, we plan to study how to 
improve the model-based strategy by pruning 
out the so-called ?bad? transliteration unit 
pairs and re-sampling the so-called ?good? unit 
pairs for better model parameters. In addition, 
we also would like to explore other pivot-
based transliteration methods, such as con-
structing source-target training data through 
pivot languages. 
References 
Yaser Al-Onaizan and Kevin Knight. 2002. Trans-
lating named entities using monolingual and bi-
lingual resources. ACL-02 
Adam L. Berger, Stephen A. Della Pietra and 
Vincent J. Della Pietra. 1996. A Maximum En-
tropy Approach to Natural Language Processing. 
Computational Linguistics. 22(1):39?71 
1451
N. Bertoldi, M. Barbaiant, M. Federico and R. Cat-
toni. 2008. Phrase-based Statistical Machine 
Translation with Pivot Languages. IWSLT-08 
Trevor Cohn and Mirella Lapata. 2007. Machine 
Translation by Triangulation: Making Effective 
Use of Multi-Parallel Corpora. ACL-07 
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. IJCNLP-08 
Wei Gao, Kam-Fai Wong and Wai Lam. 2004. 
Phoneme-based Transliteration of Foreign 
Names for OOV Problems. IJCLNP-04  
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. EMNLP-08 
A.P. Dempster, N.M. Laird, D.B.Rubin.1977. Max-
imum likelihood from incomplete data via the 
EM algorithm, J. Roy. Stat. Soc., Ser. B. Vol. 39 
Ulf Hermjakob, K. Knight and Hal Daum e?. 2008. 
Name translation in statistical machine transla-
tion: Learning when to transliterate. ACL-08 
John Lafferty, Fernando Pereira, Andrew McCal-
lum. 2001. Conditional random fields: Probabil-
istic models for segmenting and labeling se-
quence data. ICML-01  
B.J. Kang and Key-Sun Choi. 2000. Automatic 
Transliteration and Back-transliteration by De-
cision Tree Learning. LREC-00 
Mitesh Khapra, Kumaran A and Pushpak Bhatta-
charyya. 2010. Everybody loves a rich cousin: 
An empirical study of transliteration through 
bridge languages. NAACL-HLT-10 
Alexandre Klementiev and Dan Roth. 2006. Weakly 
supervised named entity transliteration and dis-
covery from multilingual comparable corpora. 
COLING-ACL-06 
K. Knight and J. Graehl. 1998. Machine Translite-
ration, Computational Linguistics, Vol 24, No. 4 
P. Koehn, F. J. Och and D. Marcu. 2003. Statistical 
phrase-based translation. HLT-NAACL-03 
J. Lafferty, A. McCallum and F. Pereira. 2001. 
Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. 
ICML-01 
Haizhou Li, A Kumaran, Vladimir Pervouchine and 
Min Zhang. 2009a. Report of NEWS 2009 Ma-
chine Transliteration Shared Task. IJCNLP-
ACL-09 Workshop: NEWS-09 
Haizhou Li, A Kumaran, Min Zhang and Vladimir 
Pervouchine. 2009b. Whitepaper of NEWS 2009 
Machine Transliteration Shared Task. IJCNLP-
ACL-09 Workshop: NEWS-09 
Haizhou Li, Ming Zhang and Jian Su. 2004. A Joint 
Source-Channel Model for Machine Translitera-
tion. ACL-04 
Thomas Mandl and Christa Womser-Hacker. 2004. 
How do Named Entities Contribute to Retrieval 
Effectiveness? CLEF-04 
Helen M. Meng, Wai-Kit Lo, Berlin Chen and Ka-
ren Tang. 2001. Generate Phonetic Cognates to 
Handle Name Entities in English-Chinese cross-
language spoken document retrieval. ASRU-01 
Jong-Hoon Oh, Kiyotaka Uchimoto, and k. Torisa-
wa. 2009. Machine Transliteration with Target-
Language Grapheme and Phoneme: Multi-
Engine Transliteration Approach. NEWS 2009 
Franz Josef Och and Hermann Ney. 2003. A Syste-
matic Comparison of Various Statistical Align-
ment Models. Computational Linguistics 29(1) 
V. Pervouchine, H. Li and B. Lin. 2009. Translite-
ration Alignment. ACL-IJCNLP-09 
R. Schwartz and Y. L. Chow. 1990. The N-best 
algorithm: An efficient and exact procedure for 
finding the N most likely sentence hypothesis, 
ICASSP-90 
Tarek Sherif and Grzegorz Kondrak. 2007. Sub-
string-based transliteration. ACL-07 
Richard Sproat, Tao Tao and ChengXiang Zhai. 
2006. Named entity transliteration with compa-
rable corpora. COLING-ACL-06 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02 
Masao Utiyama and Hitoshi Isahara. 2007. A Com-
parison of Pivot Methods for Phrase-based Sta-
tistical Machine Translation. NAACL-HLT-07 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer 
Stephen Wan and Cornelia Maria Verspoor. 1998. 
Automatic English-Chinese name transliteration 
for development of multilingual resources. COL-
ING-ACL-98 
Hua Wu and Haifeng Wang. 2007. Pivot Language 
Approach for Phrase-based Statistical Machine 
Translation. ACL-07 
Hua Wu and Haifeng Wang. 2009. Revisiting Pivot 
Language Approach for Machine Translation. 
ACL-09 
Dmitry Zelenko and Chinatsu Aone. 2006. Discri-
minative methods for transliteration. EMNLP-06 
Min Zhang, Haizhou Li and Jian Su. 2004. Direct 
Orthographical Mapping for machine translite-
ration. COLING-04 
1452
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 440?450,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Non-isomorphic Forest Pair Translation 
 
 
Hui Zhang1, 2, 3   Min Zhang1   Haizhou Li1   Eng Siong Chng2 
 
1Institute for Infocomm Research   
2Nanyang Technological University  
3USC Information Science Institute  
huizhang.fuan@gmail.com   {mzhang, hli}@i2r.a-star.edu.sg   aseschng@ntu.edu.sg 
 
 
 
 
 
 
 
 
 
 
Abstract 
This paper studies two issues, non-isomorphic 
structure translation and target syntactic structure 
usage, for statistical machine translation in the 
context of forest-based tree to tree sequence trans-
lation. For the first issue, we propose a novel 
non-isomorphic translation framework to capture 
more non-isomorphic structure mappings than tra-
ditional tree-based and tree-sequence-based trans-
lation methods. For the second issue, we propose a 
parallel space searching method to generate hypo-
thesis using tree-to-string model and evaluate its 
syntactic goodness using tree-to-tree/tree sequence 
model. This not only reduces the search complexity 
by merging spurious-ambiguity translation paths 
and solves the data sparseness issue in training, but 
also serves as a syntax-based target language mod-
el for better grammatical generation. Experiment 
results on the benchmark data show our proposed 
two solutions are very effective, achieving signifi-
cant performance improvement over baselines 
when applying to different translation models. 
1 Introduction 
Recently syntax-based methods have achieved very 
promising results and attracted increasing interests in 
statistical machine translation (SMT) research com-
munity due to their ability to provide informative 
context structure information and convenience in 
carrying out word transformation and sub-span reor-
dering. Fundamentally, syntax-based SMT views 
translation as a structural transformation process. 
Generally speaking, from modeling viewpoint, a 
syntax-based model tries to convert the source struc-
tures into target structures iteratively and recursively 
while from decoding viewpoint a syntax-based sys-
tem segments an input tree/forest into many 
sub-fragments, translates each of them separately, 
combines the translated sub-fragments and then finds 
out the best combinations. Therefore, from bilingual 
viewpoint, we face two fundamental problems: the 
mapping between bilingual structures and the way of 
carrying out the target structures combination.  
For the first issue, a number of models have been 
proposed to model the structure mapping between 
tree and string (Galley et al, 2004; Liu et al, 2006; 
Yamada and Knight, 2001; DeNeefe and Knight, 
2009) and between tree and tree (Eisner, 2003; 
Zhang et al, 2007 & 2008; Liu et al, 2009). How-
ever, one of the major challenges is that all the cur-
rent models only allow one-to-one mapping from one 
source frontier non-terminal node (Galley et al, 2004) 
to one target frontier non-terminal node in a bilingual 
translation rule. Therefore, all those translation equi-
valents with one-to-many frontier non-terminal node 
mapping cannot be covered by the current 
state-of-the-art models. This may largely compro-
mise the modeling ability of translation rules. 
For the second problem, currently, the combina-
tion is driven by only the source side (both 
tree-to-string model and tree-to-tree model only 
check the source span compatibility when combining 
different target structures in decoding) or only the 
440
target side (string to tree model). There is no well 
study in considering both the source side information 
and the compatibility between different target syn-
tactic structures during combination. In addition, it is 
well known that the traditional tree-to-tree models 
suffer heavily from the data sparseness issue in 
training and the spurious-ambiguity translation path 
issue (the same translation with different syntactic 
structures) in decoding. 
In addition, because of the performance limitation 
of automatic syntactic parser, researchers propose 
using packed forest (Tomita, 1987; Klein and Man-
ning, 2001; Huang, 2008)1 instead of 1-best parse 
tree to carry out training (Mi and Huang, 2008) and 
decoding (Mi et al, 2008) in order to reduce the side 
effect caused by parsing errors of the one-best tree. 
However, when we apply the tree-to-tree model to 
the bilingual forest structures, both training and de-
coding become very complicated. 
In this paper, to address the first issue, we propose 
a framework to model the non-isomorphic translation 
process from source tree fragment to target tree se-
quence, allowing any one source frontier 
non-terminal node to be translated into any number 
of target frontier non-terminal nodes. For the second 
issue, we propose a technology to model the combi-
nation task by considering both sides? syntactic 
structure information. We evaluate and integrate the 
two technologies into forest-based tree to tree se-
quence translation. Experimental results on the 
NIST-2003 and NIST-2005 Chinese-English transla-
tion tasks show that our methods significantly out-
perform the forest-based tree to string and previous 
tree to tree models as well as the phrase-based model.  
The remaining of the paper is organized as fol-
lowing. Section 2 reviews the related work. In sec-
tion 3 and section 4, we discuss the proposed for-
est-based rule extraction (non-isomorphic mapping) 
and decoding algorithms (target syntax information 
usage). Finally we report the experimental results in 
section 5 and conclude the paper in section 6. 
2 Related Work 
Much effort has been done in the syntax-based trans-
lation modeling. Yamada and Knight (2001) propose 
                                                          
1 A packed forest is a compact representation of a set of trees 
with sharing substructures; formally, it is defined as a triple a 
triple? ?, ?, ? ?, where ? is non-terminal node set, ? is hy-
per-edge set and ? is leaf node set (i.e. all sentence words). 
Every node in ? covers a consecutive sequence of leaf, every 
hyper-edge in ? connect the father node to its children nodes as 
in a tree. Figure 8 is a packed forest contains two trees. 
a string to tree model. Galley et al (2004) propose 
the GHKM scheme to model the string-to-tree map-
ping. Liu et al (2006) propose a tree-to-string trans-
lation model. Liu et al (2007) propose the tree se-
quence to string model to capture rules covered by 
continuous sequence of trees. Shieber (2007), De-
Neefe and Knight (2009) and Carreras and Collins 
(2009) propose synchronous tree adjoin grammar to 
capture more tree-string mapping beyond the GHKM 
scheme. Zhang et al (2009a) propose the concept of 
virtual node to reform a tree sequence as a tree, and 
design efficient algorithms for tree sequence model 
in forest context. All these works only consider either 
the source side or the target side syntax information. 
To capture both side syntax contexts, Eisner (2003) 
studies the bilingual dependency tree-to-tree map-
ping in conceptual level. Zhang et al (2008) propose 
tree sequence-based tree-to-tree modeling. Liu et al 
(2009) propose efficient algorithms for tree-to-tree 
model in the forest-based training and decoding 
scheme. One common limitation of the above works 
is they only allow the one-to-one mapping between 
each non-terminal frontier node, and thus they suffer 
from the issue of rule coverage. On the other hand, 
due to the data sparseness issue and model coverage 
issue, previous tree-to-tree (Zhang et al, 2008; Liu et 
al., 2009) decoder has to rely solely on the span in-
formation or source side information to combine the 
target syntactic structures, without checking the 
compatibility of the merging nodes, in order not to 
fail many translation paths. Thus, this solution fails 
to effectively utilize the target structure information. 
To address this issue, tree sequence (Liu et al, 
2007; Zhang et al, 2008) and virtual node (Zhang et 
al., 2009a) are two concepts with promising results 
reported. In this paper, with the help of these two 
concepts, we propose a novel framework to solve the 
one-to-many non-isomorphic mapping issue. In addi-
tion, our proposed solution of using target syntax 
information enables our forest-based tree-to-tree se-
quence translation decoding algorithm to not only 
capture bilingual forest information but also have 
almost the same complexity as forest-based 
tree-to-string translation. This reduces the time/space 
complexity exponentially. 
3 Tree to Tree Sequence Rules 
The motivation of introducing tree to tree sequence 
rules is to add target syntax information to 
tree-to-string rules. Following, we first briefly review 
the definition of tree-to-string rules, and then de-
scribe the tree-to-tree sequence rules. 
441
3.1 Tree to String Rules 
VP
ADVP
AD
VP
VV
??
(try hard to)
??
(study)
try to studyhard  
    
Fig. 1. A word-aligned sentence pair with source tree 
 
 
   
   Fig. 2 Examples of tree to string rules 
 
Fig. 2 illustrates the examples of tree to string rules 
extracted from Fig. 1. The tree-to-string rule is very 
simple. Its source side is a sub-tree of source parse 
tree and its target side is a string with only one varia-
ble/non-terminal X. The source side and the target 
side is translation of each other with the constraint of 
word alignments. Please note that there is no any 
target syntactic or linguistic information used in the 
tree-to-string model. 
3.2 Tree to Tree Sequence Rules 
It is more challenging when extracting rules with 
target tree structure as constraint. Fig. 3 extends Fig. 
1 with target tree structure. The problem is that, giv-
en a source tree node, we are able to find its target 
string translation, but these target string may not 
form a linguistic sub-tree. For example, in Fig. 3, the 
source tree node ?ADVP? in solid eclipse is trans-
lated to ?try hard to? in the target sentence, but there 
is no corresponding sub-tree covering and only cov-
ering it in the target side.  
Given the example rules in Fig. 2, what are their 
corresponding rules with target syntax information? 
The answer is that the previous tree or tree se-
quence-based models fail to model the Rule 1 and 
Rule 2 at Fig. 2, since at frontier node level they only 
allow one-to-one node mapping but the solution is 
one-to-many non-terminal frontier node mapping. 
The concept of ?virtual node? (Zhang et al 2009a) is 
a solution to this issue. To facilitate discussion, we 
first introduce three concepts. 
 
 
 
Fig. 3. A word-aligned bi-parsed tree 
 
 
Fig. 4. A restructured tree with a virtual span root 
 
? Def. 1. The ?node sequence? is a sequence of 
nodes (either leaf or internal nodes) covering a 
consecutive span. For example, in Fig 3, ?VBP 
RB TO? and ?VBP ADVP TO? are two ?node 
sequence? covering the same span ?try hard to?. 
 
442
? Def. 2. The ?root node sequence? of a span is 
such a node sequence that any node in this se-
quence could not be a child of a node in other 
node sequence of the span. Intuitively, the ?root 
node sequence? of a span is the node sequence 
with the highest topology level. For example, 
?VBP ADVP TO? is the ?root node sequence? 
of the span of ?try hard to?. It is easy to prove 
that given any span, there exist one and only one 
?root node sequence?. 
 
? Def. 3. The ?span root? of a span is such a node 
that if the ?root node sequence? contains only 
one tree node, then the ?span root? is this tree 
node; otherwise, the ?span root? is the virtual 
father node (Zhang et al, 2009a) of the ?root 
node sequence?. Fig. 4 illustrates the reformed 
Fig. 3 by introducing the virtual node 
?VBP+ADVP+TO? as the ?span root? of the 
span of ?try hard to?. 
 
 
The ?span root? facilitates us to extract rules with 
target side structure information. Given a sub-tree of 
the source tree, we have a set of non-terminal frontier 
nodes. For each such frontier node, we can find its 
corresponding target ?span root?. If the ?span root? 
is a virtual node, then we add it into the target tree as 
a virtual segmentation joint point. After adding the 
?span root? as joint point, we are able to ensure that 
each frontier source node has only one corresponding 
target node, then we can use any traditional rule ex-
traction algorithm to extract rules, including those 
rules with one-to-many non-terminal frontier map-
pings. 
 
 
Fig. 5. Tree-to-tree sequence rules 
Fig. 5 lists the corresponding rules with target 
structure information of the tree-to-string rules in Fig 
2. All the three rules cannot be extracted by previous 
tree-to-tree mapping methods (Liu et al, 2009). The 
previous tree-sequence-based methods (Zhang et al, 
2008; Zhang et al, 2009a) can extracted rule 3 since 
they allow one-to-many mapping in root node level. 
But they cannot extract rule 1 and rule 2. Therefore, 
for any tree-to-string rule, our method can always 
find the corresponding tree-to-tree sequence rule. As 
a result, our rule coverage is the same as 
tree-to-string framework while our rules contain 
more informative target syntax information. Later we 
will show that using our decoding algorithm the 
tree-to-tree sequence search space is exponentially 
reduced to the same as tree-to-string search space. 
That is to say, we do not need to worry about the ex-
ponential search space issue of tree-to-tree sequence 
model existing in previous work. 
3.3 Rule Extraction in Tree Context 
Given a word aligned tree pair, we first extract the 
set of minimum tree to string rules (Galley et al 
2004), then for each tree-to-string rule, we can easily 
extract its corresponding tree-to-tree sequence rule 
by introducing the virtual span root node. After that, 
we generate the composite rules by iteratively com-
bining small rules.  
 
    
 
Fig. 6. Rule combination and virtual node removing 
443
  Please note that in generating composite rules, if 
the joint node is a virtual node, we have to recover 
the original link and remove this virtual node to 
avoid unnecessary ambiguity. Fig. 6 illustrates the 
combination process of rule 2 and rule 3 in Fig. 5. As 
a result, all of our extract rules do not contain any 
internal virtual nodes. 
3.4 Rule Extraction in Forest Context 
In forest pair context, we also first generate the 
minimum tree-to-string rule set as Mi et al (2008), 
and for each tree-to-string rule, we find its corres-
ponding tree-to-tree sequence rules, and then do rule 
composition. 
In tree pair context, given a tree-to-string rule, 
there is one and only one corresponding tree-to-tree 
sequence rule. But in forest pair context, given one 
such tree-to-string rule, there are many correspond-
ing tree-to-tree sequence rules. All these sub-trees 
form one or more sub-forests2 of the entire big target 
forest. If we can identify the sub-forests, i.e., all of 
the hyper-edges of the sub-forests, we can retrieve all 
the sub-trees from the sub-forests as the target sides 
of the corresponding tree-to-tree sequence rules. 
Given a source sub-tree, we can obtain the target 
root span where the target sub-forests start and the 
frontier spans where the target sub-forests stop. To 
indentify all the hyper-edges in the sub-forests, we 
start from every node covering the root span, traverse 
from top to down, mark all the hyper-edges visited 
and stop at the node if its span is a sub-span of one of 
the forest frontier spans or if it is a word node. The 
reason we stop at the node once it fell into a frontier 
span (i.e. the span of the node is a sub-span of the 
frontier span) is to guarantee that given any frontier 
span, we could stop at the ?root node sequence? of 
this span by Def. 2. 
For example, Fig. 7 is a source sub-tree of rule 2 
in Fig. 5 and the circled part in Fig. 8 is one of its 
corresponding target sub-forests. Its corresponding 
target root span is [1,4] (corresponding to source root 
?VP? ) and its corresponding target frontier span is 
{[1,3], study[4,4]}. Now given the target forest, we 
start from node VP[1,4] and traverse from top to 
down, finally stop at following nodes: VBP[1,1], 
ADVP[2,2], TO[3,3], study .  
                                                          
2 All the sub-forests cover the same span. But their roots have 
different grammar tags as the roots? names. The root may be a 
virtual span root node in the case of the one-to-many frontier 
non-terminal node mappings. 
Please note that the starting root node must be a 
single node, being either a normal forest node or a 
virtual ?span root? node. The virtual ?span root? 
node serves as the frontier node of upper rules and 
root node of the currently being extracted rules. Be-
cause we extract rules in a top-to-down manner, the 
necessary virtual ?span root? node for current 
sub-forest has already been added into the global 
forest when extracting upper level rules. 
 
 
 
Figure 7. A source sub-tree in rule 2 
 
 
 
Fig. 8. The corresponding target sub-forest for the tree of 
Figure 7. 
3.5 Fractional Count of Rule 
Following Mi and Huang (2008) and Liu et al 
(2009), we assign a fractional count to a rule to 
measure how likely it appears given the context of 
the forest pair. In following equation, ?S? means 
source sub-tree, ?T? means target sub-tree, ?SF? is 
source forest and ?TF? is the target forest. 
 
 
???, ? |??, ??? ? ???|??, ??? ? ???|??, ???
? ???|??? ? ???|??? 
 
444
The above equation means the fractional count of 
a source-target tree pair is just the product of each of 
their fractional count in corresponding forest context 
in following equation. 
 
????????| ???????? ? ??????????????????? ?????? 
? ?????? ???????? ? ? ????????????? ? ? ????????????????????????????? ??????  
 
 
where ? and ? are the outside and inside probabil-
ities. In addition, if a sub-tree root is a virtual node 
(formed by a root node sequence), then we use fol-
lowing equation to approximate the outside probabil-
ity of the virtual node. 
 
????? ???????? ???? ? ?? ????
? ? ??
# ?? ????? ?? ??
 
4 Decoding 
4.1 Traditional Forest-based Decoding 
A typical translation process of a forest-based system 
is to first convert the source packed forest into a tar-
get translation forest, and then apply search algo-
rithm to find the best translation result from this tar-
get translation forest (Mi et al, 2008).  
For the tree-to-string model, the forest conversion 
process is as following: given an input packed forest, 
we do pattern matching (Zhang et al, 2009b) with 
the source side structures in the rule set. For each 
matched rule, we establish its target side as a hy-
per-edge in the target forest.   
 
 
 
Fig. 9. A forest conversion step in a tree to string model 
 
Fig. 9 exemplifies a conversion step in the tree to 
string model. A sub-tree structure with two hy-
per-edge ?VP[2,4] => ADVP[2,2] VP[3,4]? and 
?VP[3,4] => ADVP[3,4] VP[4,4]? is converted into 
a target hyper-edge ?X-VP[2,4] => X-ADVP[3,3] 
X-ADVP[2,2]  X-VP[4,4] ?.  The node ?X-VP[4,4]? 
in the target forest means that its syntactic label in 
target forest is ?X? and it is translated from the 
source node ?VP[4,4]? in the source forest. In this 
target hyper-edge, ?X-ADVP[3,3] X-ADVP[2,2]? 
means the translation from source node ?ADVP[3,3]? 
is put before the translation from ?ADVP[2,2]?, 
representing a structure reordering. 
4.2 Toward Bilingual Syntax-aware Trans-
lation Generation 
As we could see in section 4.1, there is only one kind 
of non-terminal symbol ?X? in the target side. It is a 
big challenge to rely on such a coarse label to gener-
ate a translation with fine syntactic quality. For ex-
ample, a source node may be translated into a ?NP? 
(noun phrase) in target side. However, in this rule set 
with the only symbol ?X?, it may be merged with 
upper structure as a ?VP? (verb phrase) instead, be-
cause there is no way to favor one over another. In 
this case, the target tree does not well model the 
translation syntactically. In addition, all of the inter-
nal structure information in the target side is ignored 
by the tree-to-string rules. 
One natural solution to the above issue is to use 
the tree to tree/tree sequence model, which have 
richer target syntax structures for more discrimina-
tive probability and finer labels to guide the combi-
nation process. However, the tree to tree/tree se-
quence model may face very severe computational 
problem and so-called ?spurious ambiguities? issue.  
Theoretically, if in the tree-to-tree sequence mod-
el-based decoding, we just give a penalty to the in-
compatible-node combinations instead of pruning out 
the translation paths, then the set of sentences gener-
ated by the tree-to-tree sequence model is identical to 
that of the tree-to-string model since every 
tree-to-tree sequence rule can be projected into a 
tree-to-string rule. Motivated by this, we propose a 
solution call parallel hypothesis spaces searching to 
solve the computational and ?spurious ambiguities? 
issues mentioned above. In the meanwhile, we can 
fully utilize the target structure information to guide 
translation.  
We restructure the tree-to-tree sequence rule set by 
grouping all the rules according to their correspond-
ing tree-to-string rules. This behaves like a 
?tree-to-forest? rule. The ?forest? encodes all the tree 
sequences with same corresponding string. With the 
re-constructed rule set, during decoding, we generate 
two target translation hypothesis spaces (in the form 
of packed forests) synchronously by the tree-to-string 
445
rules and tree-to-tree sequence rules, and maintain 
the projection between them. In other words, we 
generate hypothesis (searching) from the 
tree-to-string forest and calculate the probability 
(evaluating syntax goodness) for each hypothesis by 
the hyper-edges in the tree-to-tree sequence forest.  
4.3 Parallel Hypothesis Spaces 
 
 
Fig. 10. Mapping from tree-to-tree sequence into 
tree-to-string rule 
 
In this subsection, we describe what the parallel 
search spaces are and how to construct them. As 
shown at Fig. 10, given a tree-to-tree sequence rule, 
it is easy to find its corresponding tree-to-string rule 
by simply ignoring the target inside structure and 
renaming the root and leaves non-terminal labels into 
?X?. We iterate through the tree-to-tree sequence rule 
set, find its corresponding tree-to-string rule and then 
group those rules with the same tree-to-string projec-
tion. After that, the original tree-to-tree sequence rule 
set becomes a set of smaller rule sets. Each of them is 
indexed by a unique tree-to-string rule.  
We apply the tree-to-string rules to generate an 
explicit target translation forest to represent the target 
sentences space. At the same time, whenever a 
tree-to-string rule is applied, we also retrieve its cor-
responding tree-to-tree sequence rule set and gener-
ate a set of latent hyper-edges with fine-grained syn-
tax information. In this case, we have two parallel 
forests, one with coarse explicit hyper-edges and the 
other fine and latent. Given a hyper-edge (or a node) 
in the coarse forest, there are a group of correspond-
ing latent hyper-edges (or nodes) with finer syntax 
labels in the fine forest. Accordingly, given a tree in 
the coarse forest, there is a corresponding sub-forest 
in the latent fine forest. We can view the latent fine 
forest as imbedded inside the explicit coarse forest. If 
an explicit hyper-edge is viewed as a big cable, then 
the group of its corresponding latent hyper-edges is 
the small wires inside it. 
We rely on the explicit hyper-edges to enumerate 
possible hypothesis while using the latent hy-
per-edges to measure its translation probability and 
syntax goodness. Thus, the complexity of the search 
space is reduced into the tree-to-string model level, 
while keeping the target language generation syntac-
tic aware. More importantly, we thoroughly avoid 
those spurious ambiguities introduced by the 
tree-to-tree sequence rules. 
4.4 Decoding with Parallel Hypothesis 
Spaces 
 
 
 
Fig. 11. Derivation path and derivation forest 
 
In this subsection, we show exactly how our decoder 
finds the best result from the parallel spaces. We 
generate hypothesis by traversing the coarse forest in 
the parallel spaces with cube-pruning (Huang and 
Chiang, 2007). Given a newly generated hypothesis, 
it is affiliated with a derivation path (tree) in the 
coarse forest and a group of derivation paths 
(sub-forest) in the finer forest. As shown in Fig. 11, 
the left part is the derivation path formed by a coarse 
hyper-edge, consisting the newly-generated sub-tree 
?X => X X X? connecting with three previous-
ly-generated sub paths while the right part is the de-
rivation forest formed by newly-generated finer hy-
per-edges rooted at ?VP? and ?S?, and previous-
ly-generated sub-forests.  
In this paper, we use the sum of probabilities of all 
the derivation paths in the finer forest to measure the 
quality of the candidate translation suggested by the 
hypothesis. From Fig. 11, we can see there may be 
more than one corresponding finer forests, it is easy 
to understand that the sum of all the trees? probabili-
ties in these finer forests is equal to the sum of the 
inside probability of all these root nodes of these fin-
er forests. We adopt the dynamic programming to 
compute the probability of the finer forest: whenever 
we generate a new hypothesis by concatenating a 
446
coarse hyper-edge and its sub-path, we find its cor-
responding finer hyper-edges and sub-forests, do the 
combination and accumulate probabilities from bot-
tom to up. For the coarse hyper-edge, because there 
is only one label ?X?, any sub-path could be easily 
concatenated with upper structure covering the same 
sub-span without the need of checking label compa-
tibility. While for the finer hyper-edges, we only link 
the root nodes of sub-forests to upper hyper-edges 
with the same linking node label. This is to guarantee 
syntactic goodness. In case there are some leaf nodes 
of the upper hyper-edges fail to find corresponding 
sub-forest roots with the same label (e.g. the ?NP? in 
red color in the rightmost of Fig 11), we simply link 
it into the nodes with the least inside probability 
(among these sub-forests), and at the same time give 
a penalty score to this combination. If some root 
nodes of some sub-forest still cannot find upper leaf 
nodes to concatenate (e.g. the ?CP? in red color in 
Fig. 11), we simply ignore them. After the combina-
tion process, it is straightforward to accumulate the 
inside probability dynamically from bottom up. 
5 Experiment 
5.1 Experimental Settings 
We evaluate our method on the Chinese-English 
translation task. We first carry out a series empirical 
study on a set of parallel data with 30K sentence 
pairs, and then do experiment on a larger data set to 
ensure that the effectiveness of our method is consis-
tent across data set of different size. We use the 
NIST 2002 test set as our dev set, and NIST 2003 
and NIST 2005 test sets as our test set. A 3-gram 
language model is trained on the target side of the 
training data by the SRILM Toolkits (Stolcke, 2002) 
with modified Kneser-Ney smoothing (Kneser and 
Ney, 1995). We train Charniak?s parser (Charniak, 
2000) on CTB5.0 for Chinese and ETB3.0 for Eng-
lish and modify it to output packed forest. GIZA++ 
(Och and Ney, 2003) and the heuristics 
?grow-diag-final-and? are used to generate m-to-n 
word alignments. For the MER training (Och, 2003), 
Koehn?s MER trainer (Koehn, 2007) is modified for 
our system. For significance test, we use Zhang et 
al.?s implementation (Zhang et al 2004). Our evalu-
ation metrics is case-sensitive closest BLEU-4 (Pa-
pineni et al, 2002). We use following features in our 
systems: 1) bidirectional tree-to-tree sequence proba-
bility, 2) bidirectional tree-to-string probability, 3) 
bidirectional lexical translation probability, 4) target 
language model, 5) source tree probability 6) the av-
erage number of unmatched nodes in the target forest. 
7) the length of the target translation, 8) the number 
of glue rules used. 
5.2 Empirical Study on Small Data 
We set forest pruning threshold (Mi et al, 2008) to 8 
on both source and target forests for rule extraction. 
For each source sub-tree, we set its height up to 3, 
width up to 7 and extract up to 10-best target struc-
tures. In decoding, we set the pruning threshold to 10 
for the input source forest. Table 1 compares the 
performance in NIST 2003 data set of our method 
and several state-of-the-art systems as our baseline. 
 
1) MOSES: phrase-based system (Koehn et al, 
2007) 
2) FT2S: forest-based tree-to-string system (Mi 
and Huang, 2008; Mi et al, 2008) 
3) FT2T: forest-based tree-to-tree system (Liu et 
al., 2009).  
4) FT2TS (1to1): our forest-based tree-to-tree 
sequence system, where 1to1 means only 
one-to-one frontier non-terminal node map-
ping is allowed, thus the system does not fol-
low our non-isomorphic mapping framework.  
5) FT2TS (1toN): our forest-based tree-to-tree 
sequence system that allows one-to-many 
frontier non-terminal node mapping by fol-
lowing our non-isomorphic mapping frame-
work   
 
In addition, our proposed parallel searching space 
(PSS) technology can be applied to both tree to tree 
and tree to sequence systems. Thus in table 1, for the 
tree-to-tree/tree sequence systems, we report two 
BLEU scores, one uses this technology (withPSS) 
and one does not (noPSS). 
 
Model BLEU-4 
MOSES 23.39 
FT2S 26.10 
FT2T noPSS 23.40 withPSS 24.46 
FT2TS (1to1) noPSS 25.39 withPSS 26.58 
FT2TS (1toN) noPSS 26.30 withPSS 27.70 
 
Table 1. Performance comparison of different methods 
 
 From Table 1, we can see that:  
447
1) All the syntax-based systems (except FT2T 
(noPSS) (23.40)) consistently outperform the 
phrase-based system MOSES significantly 
(? ? 0.01 ), indicating that syntactic know-
ledge is very useful to SMT. 
2) The PSS technology shows significant perfor-
mance improvement ?? ? 0.01? in all mod-
els, which clearly shows effectiveness of the 
PSS technology in utilizing target structures 
for target language generation.  
3) FT2TS (1toN) significantly outperforms 
(? ? 0.01) FT2TS (1to1) in both cases (noPSS 
and withPSS). This convincingly shows the 
effectiveness of our non-isomorphic mapping 
framework in capturing the non-isomorphic 
structure translation equivalences. 
4) Both FT2TS systems significantly outperform 
FT2T( ? ? 0.01). This verifies the effective-
ness of tree sequence rules. 
5) FT2TS shows different level of performance 
improvements over FT2S with the best case 
having 1.6 (27.70-26.10) BLEU score im-
provement over FT2S. This suggests that the 
target structure information is very useful, but 
we need to find a correct way to effectively 
utilize it. 
 
1to1 1toN ratio 
1735871 2363771 1:1.36 
 
Table 2. Statistics on node mapping in forest, where 
?1to1? means the number of nodes in source forest 
that can be translated into one node in target forest 
and ?1toN? means the number of nodes in source 
forest that have to be translated into more than one 
node in target forest, where the node refers to 
non-terminal nodes only 
 
Model # of rules T2S covered 
FT2T 295732 26.8% 
FT2TS(1to1) 631487 57.1% 
FT2TS (1toN) 1945168 100% 
 
Table 3. Statistics of rule coverage, where ?T2S 
covered? means the percentage of tree-to-string 
rules that can be covered by the model 
 
Table 2 studies the node isomorphism between bi-
lingual forest pair. We can see that the 
non-isomorphic node translation mapping (1toN) 
accounts for 57.6% (=1.36/(1+1.36)) of all the forest 
non-terminal nodes with target translation. This 
means that the one-to-many node mapping is a major 
issue in structure transformation. It also empirically 
justifies the importance of our non-isomorphic map-
ping framework.  
Table 3 shows the rule coverage of different bi-
lingual structure mapping model. FT2T only covers 
26.8% tree-to-string rules, so it performs worse than 
FT2S as shown in Table 1. FT2TS (1to1) does not 
allow one-to-many frontier node mapping, so it could 
only recover the non-isomorphic node mapping in 
the root level, while FT2TS (1toN) could make it at 
both root and leaf levels. Therefore, it is not surpris-
ing that in Table 3, FT2TS (1toN) cover many more 
rules than FT2TS (1to1) because given a source tree, 
there are many leaves, if any one of them is 
non-isomorphic, then it could not be covered by the 
FT2TS (1to1).  
 
Decoding Method BLEU-4 Speed (sec/sent)
Traditional: 
FT2TS (1toN) (noPPS) 26.30 152.6 
Ours: 
FT2TS (1toN) (withPPS) 27.70 5.22 
 
Table 4. Performance and speed comparison  
 
Table 4 clearly shows the advantage of our decod-
er over the traditional one. Ours could not only gen-
erate better translation result, but also be 
152.6/5.22>30 times faster. This mainly attributes to 
two reasons: 1) one-to-many frontier node mapping 
equipments the model with more ability to capture 
more non-isomorphic structure mappings than tradi-
tional models, and 2) ?parallel search space? enables 
the decoder to fully utilize target syntactic informa-
tion, but keeping the size of search space the same as 
that a ?tree to string? model explores. 
5.3 Results on Larger Data Set 
We also carry out experiment on a larger dataset 
consisting of the small dataset used in last section 
and the FBIS corpus. In total, there are 280K parallel 
sentence pairs with 9.3M Chinese words and 11.8M 
English words. A 3-gram language model is trained 
on the target side of the parallel corpus and the GI-
GA3 Xinhua portion. We compare our system 
(FT2TS with 1toN and withPPS) with two 
state-of-the-art baselines: the phrase-based system 
MOSES and the forest-based tree-to-string system 
448
implemented by us. Table 5 clearly shows the effec-
tiveness of our method is consistent across small and 
larger corpora, outperforming FT2S by 1.6-1.8 
BLEU and the MOSES by 3.3-4.0 BLEU statistically 
significantly (p<0.01). 
 
Model BLEU 
NIST2003 NIST2005 
MOSES 29.51 27.53 
FT2S 31.21 29.72 
FT2TS 32.88 31.50 
   
Table 5. Performance on larger data set 
6 Conclusions 
In this paper, we propose a framework to address the 
issue of bilingual non-isomorphic structure mapping 
and a novel parallel searching space scheme to effec-
tively utilize target syntactic structure information in 
the context of forest-based tree to tree sequence ma-
chine translation. Based on this framework, we de-
sign an efficient algorithm to extract tree-to-tree se-
quence translation rules from word aligned bilingual 
forest pairs. We also elaborate the parallel searching 
space-based decoding algorithm and the node label 
checking scheme, which leads to very efficient de-
coding speed as fast as the forest-based tree-to-string 
model does, at the same time is able to utilize infor-
mative target structure knowledge. We evaluate our 
methods on both small and large training data sets 
and two NIST test sets. Experimental results show 
our methods statistically significantly outperform the 
state-of-the-art models across different size of cor-
pora and different test sets. In the future, we are in-
terested in testing our algorithm at forest-based tree 
sequence to tree sequence translation. 
References  
Eugene Charniak. 2000. A maximum-entropy inspired 
parser. NAACL-00. 
Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. 
Syntax-based language models for statistical machine 
translation. MT Summit IX. 40?46. 
David Chiang. 2007. Hierarchical phrase-based transla-
tion.Computational Linguistics, 33(2). 
Steve DeNeefe, Kevin Knight. 2009. Synchronous Tree 
Adjoining Machine Translation. EMNLP-2009. 
727-736. 
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for MT. ACL-03 (companion volume). 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What?s in a translation rule? 
HLT-NAACL-04. 273-280. 
Liang Huang. 2008. Forest Reranking: Discriminative 
Parsing with Non-Local Features. ACL-HLT-08. 
586-594 
Liang Huang and David Chiang. 2005. Better k-best Pars-
ing. IWPT-05. 53-64 
Liang Huang and David Chiang. 2007. Forest rescoring: 
Faster decoding with integrated language models. 
ACL-07. 144?151 
Dan Klein and Christopher D. Manning. 2001. Parsing 
and Hypergraphs. IWPT-2001. 
Reinhard Kneser and Hermann Ney. 1995. Improved 
backing-off for M-gram language modeling. 
ICASSP-95, 181-184 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Cal-
lison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Richard 
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin 
and Evan Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. ACL-07. 177-180. 
(poster) 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-String 
Alignment Template for Statistical Machine Transla-
tion. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 2007. 
Forest-to-String Statistical Translation Rules. ACL-07. 
704-711. 
Yang Liu, Yajuan L?, Qun Liu. 2009. Improving 
Tree-to-Tree Translation with Packed Forests. ACL-09. 
558-566 
Haitao Mi, Liang Huang, and Qun Liu. 2008. For-
est-based translation. ACL-HLT-08. 192-199. 
Haitao Mi and Liang Huang. 2008. Forest-based Transla-
tion Rule Extraction. EMNLP-08. 206-214. 
Franz J. Och. 2003. Minimum error rate training in statis-
tical machine translation. ACL-03. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics. 29(1) 19-51.  
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. ACL-02. 311-318. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. ICSLP-02. 901-904. 
Masaru Tomita. 1987. An Efficient Aug-
mented-Context-Free Parsing Algorithm. Computation-
al Linguistics 13(1-2): 31-46 
449
Xavier Carreras and Michael Collins. 2009. 
Non-projective Parsing for Statistical Machine Trans-
lation. EMNLP-2009. 200-209. 
K. Yamada and K. Knight. 2001. A Syntax-Based Statis-
tical Translation Model. ACL-01. 523-530. 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew 
Lim Tan. 2009a. Forest-based Tree Sequence to String 
Translation Model. ACL-IJCNLP-09. 172-180. 
Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim Tan. 
2009b. Fast Translation Rule Matching for Syn-
tax-based Statistical Machine Translation. EMNLP-09. 
1037-1045. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew Lim 
Tan and Sheng Li. 2007. A Tree-to-Tree Align-
ment-based model for statistical Machine translation. 
MT-Summit-07. 535-542 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew 
Lim Tan, Sheng Li. 2008. A Tree Sequence Align-
ment-based Tree-to-Tree Translation Model. 
ACL-HLT-08. 559-567. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do 
we need to have a better system? LREC-04. 2051-2054. 
450
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 73?83,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
SMT Helps Bitext Dependency Parsing
Wenliang Chen??, Jun?ichi Kazama?, Min Zhang?, Yoshimasa Tsuruoka??,
Yujie Zhang??, Yiou Wang?, Kentaro Torisawa? and Haizhou Li?
?Human Language Technology, Institute for Infocomm Research, Singapore
?National Institute of Information and Communications Technology (NICT), Japan
?School of Information Science, JAIST, Japan
?Beijing Jiaotong University, China
{wechen, mzhang, hli}@i2r.a-star.edu.sg
{kazama, torisawa, yujie, wangyiou}@nict.go.jp
tsuruoka@jaist.ac.jp
Abstract
We propose a method to improve the accuracy
of parsing bilingual texts (bitexts) with the
help of statistical machine translation (SMT)
systems. Previous bitext parsing methods use
human-annotated bilingual treebanks that are
hard to obtain. Instead, our approach uses an
auto-generated bilingual treebank to produce
bilingual constraints. However, because the
auto-generated bilingual treebank contains er-
rors, the bilingual constraints are noisy. To
overcome this problem, we use large-scale
unannotated data to verify the constraints and
design a set of effective bilingual features for
parsing models based on the verified results.
The experimental results show that our new
parsers significantly outperform state-of-the-
art baselines. Moreover, our approach is still
able to provide improvement when we use a
larger monolingual treebank that results in a
much stronger baseline. Especially notable
is that our approach can be used in a purely
monolingual setting with the help of SMT.
1 Introduction
Recently there have been several studies aiming to
improve the performance of parsing bilingual texts
(bitexts) (Smith and Smith, 2004; Burkett and Klein,
2008; Huang et al, 2009; Zhao et al, 2009; Chen
et al, 2010). In bitext parsing, we can use the in-
formation based on ?bilingual constraints? (Burkett
and Klein, 2008), which do not exist in monolingual
sentences. More accurate bitext parsing results can
be effectively used in the training of syntax-based
machine translation systems (Liu and Huang, 2010).
Most previous studies rely on bilingual treebanks
to provide bilingual constraints for bitext parsing.
Burkett and Klein (2008) proposed joint models on
bitexts to improve the performance on either or both
sides. Their method uses bilingual treebanks that
have human-annotated tree structures on both sides.
Huang et al (2009) presented a method to train a
source-language parser by using the reordering in-
formation on words between the sentences on two
sides. It uses another type of bilingual treebanks
that have tree structures on the source sentences and
their human-translated sentences. Chen et al (2010)
also used bilingual treebanks and made use of tree
structures on the target side. However, the bilingual
treebanks are hard to obtain, partly because of the
high cost of human translation. Thus, in their experi-
ments, they applied their methods to a small data set,
the manually translated portion of the Chinese Tree-
bank (CTB) which contains only about 3,000 sen-
tences. On the other hand, many large-scale mono-
lingual treebanks exist, such as the Penn English
Treebank (PTB) (Marcus et al, 1993) (about 40,000
sentences in Version 3) and the latest version of CTB
(over 50,000 sentences in Version 7).
In this paper, we propose a bitext parsing ap-
proach in which we produce the bilingual constraints
on existing monolingual treebanks with the help of
SMT systems. In other words, we aim to improve
source-language parsing with the help of automatic
translations.
In our approach, we first use an SMT system
to translate the sentences of a source monolingual
treebank into the target language. Then, the target
sentences are parsed by a parser trained on a tar-
get monolingual treebank. We then obtain a bilin-
gual treebank that has human annotated trees on the
source side and auto-generated trees on the target
side. Although the sentences and parse trees on the
73
target side are not perfect, we expect that we can
improve bitext parsing performance by using this
newly auto-generated bilingual treebank. We build
word alignment links automatically using a word
alignment tool. Then we can produce a set of bilin-
gual constraints between the two sides.
Because the translation, parsing, and word align-
ment are done automatically, the constraints are not
reliable. To overcome this problem, we verify the
constraints by using large-scale unannotated mono-
lingual sentences and bilingual sentence pairs. Fi-
nally, we design a set of bilingual features based on
the verified results for parsing models.
Our approach uses existing resources including
monolingual treebanks to train monolingual parsers
on both sides, bilingual unannotated data to train
SMT systems and to extract bilingual subtrees,
and target monolingual unannotated data to extract
monolingual subtrees. In summary, we make the fol-
lowing contributions:
? We propose an approach that uses an auto-
generated bilingual treebank rather than
human-annotated bilingual treebanks used in
previous studies (Burkett and Klein, 2008;
Huang et al, 2009; Chen et al, 2010). The
auto-generated bilingual treebank is built with
the help of SMT systems.
? We verify the unreliable constraints by using
the existing large-scale unannotated data and
design a set of effective bilingual features over
the verified results. Compared to Chen et al
(2010) that also used tree structures on the tar-
get side, our approach defines the features on
the auto-translated sentences and auto-parsed
trees, while theirs generates the features by
some rules on the human-translated sentences.
? Our parser significantly outperforms state-of-
the-art baseline systems on the standard test
data of CTB containing about 3,000 sentences.
Moreover, our approach continues to achieve
improvement when we build our system us-
ing the latest version of CTB (over 50,000 sen-
tences) that results in a much stronger baseline.
? We show the possibility that we can improve
the performance even if the test set has no hu-
man translation. This means that our proposed
approach can be used in a purely monolingual
setting with the help of SMT. To our knowl-
edge, this paper is the first one that demon-
strates this widened applicability, unlike the
previous studies that assumed that the parser is
applied only on the bitexts made by humans.
Throughout this paper, we use Chinese as the
source language and English as the target language.
The rest of this paper is organized as follows. Sec-
tion 2 introduces the motivation of this work. Sec-
tion 3 briefly introduces the parsing model used in
the experiments. Section 4 describes a set of bilin-
gual features based on the bilingual constraints and
Section 5 describes how to use large-scale unanno-
tated data to verify the bilingual constraints and de-
fine another set of bilingual features based on the
verified results. Section 6 explains the experimental
results. Finally, in Section 7 we draw conclusions.
2 Motivation
Here, bitext parsing is the task of parsing source sen-
tences with the help of their corresponding transla-
tions. Figure 1-(a) shows an example of the input
of bitext parsing, where ROOT is an artificial root
token inserted at the beginning and does not depend
on any other token in the sentence, the dashed undi-
rected links are word alignment links, and the di-
rected links between words indicate that they have
a dependency relation. Given such inputs, we build
dependency trees for the source sentences. Figure
1-(b) shows the output of bitext parsing for the ex-
ample in 1-(a).
ROOT!? ?? ?? ? ? ?? ?? ? ?? ??ta gaodu pingjia le yu lipeng zongli de huitan jieguo!! !!!
ROOT H hi hl d d h l f h f i h P Li!! e! g y!commen e !t e!resu ts!!o !!!t e!con erence!!!!w t !! eng
(a)
ROOT!? ?? ?? ? ? ?? ?? ? ?? ??ta gaodu pingjia le yu lipeng zongli de huitan jieguo!! !!(b)Figure 1: Input and output of our approach
In bitext parsing, some ambiguities exist on the
source side, but they may be unambiguous on the
74
target side. These differences are expected to help
improve source-side parsing.
Suppose we have a Chinese sentence shown in
Figure 2-(a). In this sentence, there is a nomi-
nalization case (Li and Thompson, 1997) in which
the particle ??(de)/nominalizer? is placed after the
verb compound ???(peiyu)??(qilai)/cultivate?
to modify ???(jiqiao)/skill?. This nominaliza-
tion is a relative clause, but does not have a clue
about its boundary. That is, it is very hard to deter-
mine which word is the head of ???(jiqiao)/skill?.
The head may be ???(fahui)/demonstrate? or ??
?(peiyu)/cultivate?, as shown in Figure 2-(b) and
-(c), where (b) is correct.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiaoPN!!!!!!!VV!!!!!!!!!DT!!!!!!!!!!!!!!!NN!!!!!!!!!!!!!!!AD!!!!!!!!!!!!!!VV!!!!!!AD!!!!!!!VV!!!!VV DEC NN!!!!CC!!!NN(a)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao(b)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao(c)Figure 2: Example of an ambiguity on the Chinese side
In its English translation (Figure 3), word ?that? is
a clue indicating the relative clause which shows the
relation between ?skill? and ?cultivate?, as shown in
Figure 3. The figure shows that the translation can
provide useful bilingual constraints. From the de-
pendency tree on the target side, we find that the
word ?skill? corresponding to ???(jiqiao)/skill?
depends on the word ?demonstrate? corresponding
to ???(fahui)/demonstrate?, while the word ?cul-
tivate? corresponding to ???(peiyu)/cultivate? is a
grandchild of ?skill?. This is a positive evidence for
supporting ???(fahui)/demonstrate? as being the
head of ???(jiqiao)/skill?.
The above case uses the human translation on
the target side. However, there are few human-
annotated bilingual treebanks and the existing bilin-
gual treebanks are usually small. In contrast, there
are large-scale monolingual treebanks, e.g., the PTB
and the latest version of CTB. So we want to use
existing resources to generate a bilingual treebank
with the help of SMT systems. We hope to improve
source side parsing by using this newly built bilin-
gual treebank.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
He!hoped!that!all!the!athletes!would!!fully!demonstrate!the!strength!and!skill!that!they!cultivate!daily
Figure 3: Example of human translation
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 4: Example of Moses translation
Figure 4 shows an example of a translation us-
ing a Moses-based system, where the target sen-
tence is parsed by a monolingual target parser. The
translation contains some errors, but it does contain
some correct parts that can be used for disambigua-
tion. In the figure, the word ?skills? corresponding
to ???(jiqiao)/skill? is a grandchild of the word
?play? corresponding to ???(fahui)/demonstrate?.
This is a positive evidence for supporting ??
?(fahui)/demonstrate? as being the head of ??
?(jiqiao)/skill?.
From this example, although the sentences and
parse trees on the target side are not perfect, we
still can explore useful information to improve bitext
parsing. In this paper, we focus on how to design
a method to verify such unreliable bilingual con-
straints.
3 Parsing model
In this paper, we implement our approach based
on graph-based parsing models (McDonald and
Pereira, 2006; Carreras, 2007). Note that our ap-
proach can also be applied to transition-based pars-
ing models (Nivre, 2003; Yamada and Matsumoto,
2003).
The graph-based parsing model is to search for
the maximum spanning tree (MST) in a graph (Mc-
Donald and Pereira, 2006). The formulation defines
the score of a dependency tree to be the sum of edge
scores,
75
s(x, y) =
?
g?y
score(w, x, g) =
?
g?y
w ?f(x, g) (1)
where x is an input sentence, y is a dependency
tree for x, and g is a spanning subgraph of y. f(x, g)
can be based on arbitrary features of the subgraph
and the input sequence x and the feature weight
vector w are the parameters to be learned by using
MIRA (Crammer and Singer, 2003) during training.
In our approach, we use two types of features
for the parsing model. One is monolingual fea-
tures based on the source sentences. The mono-
lingual features include the first- and second- order
features presented in McDonald and Pereira (2006)
and the parent-child-grandchild features used in Car-
reras (2007). The other one is bilingual features (de-
scribed in Sections 4 and 5) that consider the bilin-
gual constraints.
We call the parser with the monolingual features
on the source side Parsers, and the parser with the
monolingual features on the target side Parsert.
4 Original bilingual features
In this paper, we generate two types of bilingual fea-
tures, original and verified bilingual features. The
original bilingual features (described in this section)
are based on the bilingual constraints without being
verified by large-scale unannotated data. And the
verified bilingual features (described in Section 5)
are based on the bilingual constraints verified by us-
ing large-scale unannotated data.
4.1 Auto-generated bilingual treebank
Assuming that we have monolingual treebanks on
the source side, an SMT system that can translate
the source sentences into the target language, and a
Parsert trained on the target monolingual treebank.
We first translate the sentences of the source
monolingual treebank into the target language using
the SMT system. Usually, SMT systems can output
the word alignment links directly. If they can not, we
perform word alignment using some publicly avail-
able tools, such as Giza++ (Och and Ney, 2003) or
Berkeley Aligner (Liang et al, 2006; DeNero and
Klein, 2007). The translated sentences are parsed by
the Parsert. Then, we have a newly auto-generated
bilingual treebank.?
4.2 Bilingual constraint functions
In this paper, we focus on the first- and second-
order graph models (McDonald and Pereira, 2006;
Carreras, 2007). Thus we produce the constraints
for bigram (a single edge) and trigram (adjacent
edges) dependencies in the graph model. For the tri-
gram dependencies, we consider the parent-sibling
and parent-child-grandchild structures described in
McDonald and Pereira (2006) and Carreras (2007).
We leave the third-order models (Koo and Collins,
2010) for a future study.
Suppose that we have a (candidate) dependency
relation rs that can be a bigram or trigram de-
pendency. We examine whether the corresponding
words of the source words of rs have a dependency
relation rt in the target trees. We also consider the
direction of the dependency relation. The corre-
sponding word of the head should also be the head
in rt. We define a binary function for this bilingual
constraint: Fbn(rsn : rtk), where n and k refers to
the types of the dependencies (2 for bigram and 3 for
trigram). For example, in rs2 : rt3, rs2 is a bigram
dependency on the source side and rt3 is a trigram
dependency on the target side.
4.2.1 Bigram constraint function: Fb2
For rs2, we consider two types of bilingual con-
straints. The first constraint function, denoted as
Fb2(rs2 : rt2), checks if the corresponding words
also have a direct dependency relation rt2. Figure
5 shows an example, where the source word ??
?(quanti)? depends on ????(yundongyuan)?
and word ?all? corresponding to ???(quanti)? de-
pends on word ?athletes? corresponding to ???
?(yundongyuan)?. In this case, Fb2(rs2 : rt2) =
+. However, when the source words are ??(ta)?
and ???(xiwang)?, this time their corresponding
words ?He? and ?hope? do not have a direct depen-
dency relation. In this case, Fb2(rs2 : rt2)=?.
The second constraint function, denoted as
Fb2(rs2 : rt3), checks if the corresponding words
form a parent-child-grandchild relation that often
occurs in translation (Koehn et al, 2003). Figure 6
shows an example. The source word ???(jiqiao)?
depends on ???(fahui)? while its corresponding
word ?skills? indirectly depends on ?play? which
corresponds to ???(fahui)? via ?to?. In this case,
Fb2(rs2 : rt3)=+.
76
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 5: Example of bilingual constraints (2to2)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 6: Example of bilingual constraints (2to3)
4.2.2 Trigram constraint function: Fb3
For a second-order relation on the source side,
we consider one type of constraint. We have three
source words that form a second-order relation and
all of them have the corresponding words. We
define function Fb3(rs3 : rt3) for this constraint.
The function checks if the corresponding words
form a trigram dependencies structure. An exam-
ple is shown in Figure 7. The source words ??
?(liliang)?, ??(he)?, and ???(jiqiao)? form a
parent-sibling structure, while their corresponding
words ?strength?, ?and?, and ?skills? also form a
parent-sibling structure on the target side. In this
case, function Fb3(rs3 : rt3)=+.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 7: Example of bilingual constraints (3to3)
4.3 Bilingual reordering function: Fro
Huang et al (2009) proposed features based on
reordering between languages for a shift-reduce
parser. They define the features based on word-
alignment information to verify whether the corre-
sponding words form a contiguous span to resolve
shift-reduce conflicts. We also implement similar
features in our system. For example, in Figure 1-
(a) the source span is [??(huitan), ??(jieguo)],
which maps onto [results, conference]. Because no
word within this target span is aligned to a source
word outside of the source span, this span is a con-
tiguous span. In this case, function Fro =+, other-
wise Fro=?.
4.4 Original bilingual features
We define original bilingual features based on the
bilingual constraint functions and the bilingual re-
ordering function.
Table 1 lists the original features, where Dir
refers to the directions1 of the source-side dependen-
cies, Fb2 can be Fb2(rs2 : rt2) and Fb2(rs2 : rt3),
and Fb3 is Fb3(rs3 : rt3). Each line of the table
defines a feature template that is a combination of
functions.
First-order features Second-order features
?Fro?
?Fb2, Dir? ?Fb3, Dir?
?Fb2, Dir, Fro? ?Fb3, Dir, Fro?
Table 1: Original bilingual features
We use an example to show how to generate the
original bilingual features in practice. In Figure 4,
we want to define the bilingual features for the bi-
gram dependency (rs2) between ???(fahui)? and
???(jiqiao)?. The corresponding words form a tri-
gram relation rt3 in the target dependency tree. The
direction of the bigram dependency is right. Then
we have feature ??Fb2(rs2 : rt3)=+, RIGHT ?? for
the second first-order feature template in Table 1.
5 Verified bilingual features
However, because the bilingual treebank is gener-
ated automatically, using the bilingual constraints
alone is not reliable. Therefore, in this section we
verify the constraints by using large-scale unanno-
tated data to overcome this problem. More specifi-
cally, rtk of the constraint is verified by checking a
list of target monolingual subtrees and rsn : rtk is
verified by checking a list of bilingual subtrees. The
subtrees are extracted from the large-scale unanno-
tated data. The basic idea is as follows: if the de-
pendency structures of a bilingual constraint can be
found in the list of the target monolingual subtrees
1For the second order features, Dir is the combination of
the directions of two dependencies.
77
or bilingual subtrees, this constraint will probably be
reliable.
We first parse the large-scale unannotated mono-
lingual and bilingual data. Subsequently, we ex-
tract the monolingual and bilingual subtrees from
the parsed data. We then verify the bilingual con-
straints using the extracted subtrees. Finally, we
generate the bilingual features based on the verified
results for the parsing models.
5.1 Verified constraint functions
5.1.1 Monolingual target subtrees
Chen et al (2009) proposed a simple method to
extract subtrees from large-scale monolingual data
and used them as features to improve monolingual
parsing. Following their method, we parse large
unannotated data with the Parsert and obtain the sub-
tree list (STt) on the target side. We extract two
types of subtrees: bigram (two words) subtree and
trigram (three words) subtree.
H b ht b h b k
ROOT!!He!!!!!bought!!!!!a!!!!book
e!!!!! oug oug t!! oo !
a book b ht b k!!!!!
(a) (b)
oug !!!a!!!!! oo !
Figure 8: Example of monolingual subtree extraction
From the dependency tree in Figure 8-(a), we ob-
tain the subtrees shown in Figure 8-(b) where the
first three are bigram subtrees and the last one is
a trigram subtree. After extraction, we obtain the
subtree list STt that includes two sets, one for bi-
gram subtrees, and the other one for trigram sub-
trees. We remove the subtrees occurring only once
in the data. For each set, we assign labels to the
extracted subtrees according to their frequencies by
using the same method as that of Chen et al (2009).
If the frequency of a subtree is in the top 10% in the
corresponding set, it is labeled HF. If the frequency
is between the top 20% and 30%, it is labeled MF.
We assign the label LF to the remaining subtrees.
We use Type(stt) to refer to the label of a subtree,
stt.
5.1.2 Verified target constraint function:
Fvt(rtk)
We use the extracted target subtrees to verify the
rtk of the bilingual constraints. In fact, rtk is a can-
didate subtree. If the rtk is included in STt, func-
tion Fvt(rtk) = Type(rtk), otherwise Fvt(rtk) =
ZERO. For example, in Figure 5 the bigram struc-
ture of ?all? and ?athletes? can form a bigram sub-
tree that is included STt and its label is HF. In this
case, Fvt(rt2)= HF .
5.1.3 Bilingual subtrees
We extract bilingual subtrees from a bilingual
corpus, which is parsed by the Parsers and Parsert
on both sides. We extract three types of bilingual
subtrees: bigram-bigram (stbi22), bigram-trigram
(stbi23), and trigram-trigram (stbi33) subtrees. For
example, stbi22 consists of a bigram subtree on the
source side and a bigram subtree on the target side.
? ? ? ?? ? ? ? ??ROOT! ?ta shi yi ming xuesheng
ROOT!!He!!!!!is!!!!!a!!!!!student He!!!!!is is!!!!!student
(a) (b)
Figure 9: Example of bilingual subtree extraction
From the dependency tree in Figure 9-(a), we
obtain the bilingual subtrees shown in Figure 9-
(b). Figure 9-(b) shows the extracted bigram-bigram
bilingual subtrees. After extraction, we obtain the
bilingual subtrees STbi. We remove the subtrees oc-
curring only once in the data.
5.1.4 Verified bilingual constraint function:
Fvb(rbink)
We use the extracted bilingual subtrees to verify
the rsn : rtk (rbink in short) of the bilingual con-
straints. rsn and rtk form a candidate bilingual sub-
tree stbink. If the stbink is included in STbi, function
Fvb(rbink)=+, otherwise Fvb(rbink)=?.
5.2 Verified bilingual features
Then, we define another set of bilingual features by
combining the verified constraint functions. We call
these bilingual features ?verified bilingual features?.
78
Table 2 lists the verified bilingual features used in
our experiments, where each line defines a feature
template that is a combination of functions.
We use an example to show how to generate the
verified bilingual features in practice. In Figure 4,
we want to define the verified features for the bi-
gram dependency (rs2) between ???(fahui)? and
???(jiqiao)?. The corresponding words form a
trigram relation rt3. The direction of the bigram
dependency is right. Suppose we can find rt3 in
STt with label MF and can not find the candidate
bilingual subtree in STbi. Then we have feature
??Fb2(rs2 : rt3) = +, Fvt(rt3) = MF,RIGHT ??
for the third first-order feature template and feature
??Fb2(rs2 : rt3)=+, Fvb(rbi23)=?, RIGHT ?? for
the fifth in Table 2.
First-order features Second-order features
?Fro?
?Fb2, Fvt(rtk)? ?Fb3, Fvt(rtk)?
?Fb2, Fvt(rtk), Dir? ?Fb3, Fvt(rtk), Dir?
?Fb2, Fvb(rbink)? ?Fb3, Fvb(rbink)?
?Fb2, Fvb(rbink), Dir? ?Fb3, Fvb(rbink), Dir?
?Fb2, Fro, Fvb(rbink)?
Table 2: Verified bilingual features
6 Experiments
We evaluated the proposed method on the translated
portion of the Chinese Treebank V2 (referred to as
CTB2tp) (Bies et al, 2007), articles 1-325 of CTB,
which have English translations with gold-standard
parse trees. The tool ?Penn2Malt?2 was used to con-
vert the data into dependency structures. Following
the studies of Burkett and Klein (2008), Huang et
al. (2009) and Chen et al (2010), we used the ex-
act same data split: 1-270 for training, 301-325 for
development, and 271-300 for testing. Note that we
did not use human translation on the English side
of this bilingual treebank to train our new parsers.
For testing, we used two settings: a test with hu-
man translation and another with auto-translation.
To process unannotated data, we trained a first-order
Parsers on the training data.
To prove that the proposed method can work on
larger monolingual treebanks, we also tested our
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
methods on the CTB7 (LDC2010T07) that includes
much more sentences than CTB2tp. We used arti-
cles 301-325 for development, 271-300 for testing,
and the other articles for training. That is, we eval-
uated the systems on the same test data as CTB2tp.
Table 3 shows the statistical information on the data
sets.
Train Dev Test
CTB2tp 2,745 273 290
CTB7 50,747 273 290
Table 3: Number of sentences of data sets used
We built Chinese-to-English SMT systems based
on Moses3. Minimum error rate training (MERT)
with respect to BLEU score was used to tune the de-
coder?s parameters. The translation model was cre-
ated from the FBIS corpus (LDC2003E14). We used
SRILM4 to train a 5-gram language model. The lan-
guage model was trained on the target side of the
FBIS corpus and the Xinhua news in English Gi-
gaword corpus (LDC2009T13). The development
and test sets were from NIST MT08 evaluation cam-
paign5. We then used the SMT systems to translate
the training data of CTB2tp and CTB7.
To directly compare with the results of Huang
et al (2009) and Chen et al (2010), we also used
the same word alignment tool, Berkeley Aligner
(Liang et al, 2006; DeNero and Klein, 2007), to
perform word alignment for CTB2tp and CTB7.
We trained a Berkeley Aligner on the FBIS corpus
(LDC2003E14). We removed notoriously bad links
in {a, an, the}?{?(de),?(le)} following the work
of Huang et al (2009).
To train an English parser, we used the PTB
(Marcus et al, 1993) in our experiments and the
tool ?Penn2Malt? to convert the data. We split the
data into a training set (sections 2-21), a develop-
ment set (section 22), and a test set (section 23).
We trained first-order and second-order Parsert on
the training data. The unlabeled attachment score
(UAS) of the second-order Parsert was 91.92, in-
dicating state-of-the-art accuracy on the test data.
We used the second-order Parsert to parse the auto-
translated/human-made target sentences in the CTB
3http://www.statmt.org/moses/
4http://www.speech.sri.com/projects/srilm/download.html
5http://www.itl.nist.gov/iad/mig//tests/mt/2008/
79
data.
To extract English subtrees, we used the BLLIP
corpus (Charniak et al, 2000) that contains about
43 million words of WSJ texts. We used the MX-
POST tagger (Ratnaparkhi, 1996) trained on train-
ing data to assign POS tags and used the first-order
Parsert to process the sentences of the BLLIP cor-
pus. To extract bilingual subtrees, we used the FBIS
corpus and an additional bilingual corpus contain-
ing 800,000 sentence pairs from the training data of
NIST MT08 evaluation campaign. On the Chinese
side, we used the morphological analyzer described
in (Kruengkrai et al, 2009) trained on the training
data of CTBtp to perform word segmentation and
POS tagging and used the first-order Parsers to parse
all the sentences in the data. On the English side, we
used the same procedure as we did for the BLLIP
corpus. Word alignment was performed using the
Berkeley Aligner.
We reported the parser quality by the UAS, i.e.,
the percentage of tokens (excluding all punctuation
tokens) with correct HEADs.
6.1 Experimental settings
For baseline systems, we used the monolingual fea-
tures mentioned in Section 3. We called these fea-
tures basic features. To compare the results of (Bur-
kett and Klein, 2008; Huang et al, 2009; Chen et
al., 2010), we used the test data with human trans-
lation in the following three experiments. The tar-
get sentences were parsed by using the second-order
Parsert. We used PAG to refer to our parsers trained
on the auto-generated bilingual treebank.
6.2 Training with CTB2tp
Order-1 Order-2
Baseline 84.35 87.20
PAGo 84.71(+0.36) 87.85(+0.65)
PAG 85.37(+1.02) 88.49(+1.29)
ORACLE 85.79(+1.44) 88.87(+1.67)
Table 4: Results of training with CTB2tp
First, we conducted the experiments on the stan-
dard data set of CTB2tp, which was also used in
other studies (Burkett and Klein, 2008; Huang et al,
2009; Chen et al, 2010). The results are given in
Table 4, where Baseline refers to the system with
the basic features, PAGo refers to that after adding
the original bilingual features of Table 1 to Baseline,
PAG refers to that after adding the verified bilingual
features of Table 2 to Baseline, and ORACLE6 refers
to using human-translation for training data with
adding the features of Table 1. We obtained an ab-
solute improvement of 1.02 points for the first-order
model and 1.29 points for the second-order model by
adding the verified bilingual features. The improve-
ments of the final systems (PAG) over the Baselines
were significant in McNemar?s Test (p < 0.001 for
the first-order model and p < 0.0001 for the second-
order model). If we used the original bilingual fea-
tures (PAGo), the system dropped 0.66 points for the
first-order and 0.64 points for the second-order com-
pared with system PAG. This indicated that the ver-
ified bilingual constraints did provide useful infor-
mation for the parsing models.
We also found that PAG was about 0.3 points
lower than ORACLE. The reason is mainly due
to the imperfect translations, although we used
the large-scale subtree lists to help verify the con-
straints. We tried adding the features of Table 2 to
the ORACLE system, but the results were worse.
These facts indicated that our approach obtained the
benefits from the verified constraints, while using
the bilingual constraints alone was enough for OR-
ACLE.
6.3 Training with CTB7
 0.83
 0.84
 0.85
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 5  10  20  30  40  50
U
A
S
Amount of training data (K)
Baseline1PAG1Baseline2PAG2
Figure 10: Results of using different sizes of training data
Here, we demonstrate that our approach is still
able to provide improvement, even if we use larger
6Note that we also used the tool to perform the word align-
ment automatically.
80
Baseline D10 D20 D50 D100 GTran
BLEU n/a 14.71 15.84 16.92 17.95 n/a
UAS 87.20 87.63 87.67 88.20 88.49 88.58
Table 5: Results of using different translations
training data that result in strong baseline systems.
We incrementally increased the training sentences
from the CTB7. Figure 10 shows the results of us-
ing different sizes of CTB7 training data, where the
numbers of the x-axis refer to the sentence numbers
of training data used, Baseline1 and Baseline2 re-
fer to the first- and second-order baseline systems,
and PAG1 and PAG2 refer to our first- and second-
order systems. The figure indicated that our sys-
tem always outperformed the baseline systems. For
small data sizes, our system performed much better
than the baselines. For example, when using 5,000
sentences, our second-order system provided a 1.26
points improvement over the second-order baseline.
Finally, when we used all of the CTB7 training
data, our system achieved 91.66 for the second-order
model, while the baseline achieved 91.10.
6.4 With different settings of SMT systems
We investigated the effects of different settings of
SMT systems. We randomly selected 10%, 20%,
and 50% of FBIS to train the Moses systems and
used them to translate CTB2tp. The results are in
Table 5, where D10, D20, D50, and D100 refer to
the system with 10%, 20%, 50%, and 100% data re-
spectively. For reference, we also used the Google-
translate online system7, indicated as GTran in the
table, to translate the CTB2tp.
From the table, we found that our system outper-
formed the Baseline even if we used only 10% of the
FBIS corpus. The BLEU and UAS scores became
higher, when we used more data of the FBIS corpus.
And the gaps among the results of D50, D100, and
GTran were small. This indicated that our approach
was very robust to the noise produced by the SMT
systems.
6.5 Testing with auto-translation
We also translated the test data into English using
the Moses system and tested the parsers on the new
7http://translate.google.com/
test data. Table 6 shows the results. The results
showed that PAG outperformed the baseline systems
for both the first- and second-order models. This
indicated that our approach can provide improve-
ment in a purely monolingual setting with the help
of SMT.
Order-1 Order-2
Baseline 84.35 87.20
PAG 84.88(+0.53) 87.89(+0.69)
Table 6: Results of testing with auto-translation (training
with CTB2tp)
6.6 Comparison results
With CTB2tp With CTB7
Type System UAS System UAS
M Baseline 87.20 Baseline 91.10
HA
Huang2009 86.3 n/a
Chen2010BI 88.56
Chen2010ALL 90.13
AG PAG 88.49 PAG 91.66PAG+STs 89.75
Table 7: Comparison of our results with other pre-
vious reported systems. Type M denotes training on
monolingual treebank. Types HA and AG denote training
on human-annotated and auto-generated bilingual tree-
banks respectively.
We compared our results with the results reported
previously for the same data. Table 7 lists the re-
sults, where Huang2009 refers to the result of Huang
et al (2009), Chen2010BI refers to the result of
using bilingual features in Chen et al (2010), and
Chen2010ALL refers to the result of using all of
the features in Chen et al (2010). The results
showed that our new parser achieved better accuracy
than Huang2009 and comparable to Chen2010BI .
To achieve higher performance, we also added the
source subtree features (Chen et al, 2009) to our
system: PAG+STs. The new result is close to
Chen2010ALL. Compared with the approaches of
81
Huang et al (2009) and Chen et al (2010), our
approach used an auto-generated bilingual treebank
while theirs used a human-annotated bilingual tree-
bank. By using all of the training data of CTB7, we
obtained a more powerful baseline that performed
much better than the previous reported results. Our
parser achieved 91.66, much higher accuracy than
the others.
7 Conclusion
We have presented a simple yet effective approach
to improve bitext parsing with the help of SMT sys-
tems. Although we trained our parser on an auto-
generated bilingual treebank, we achieved an accu-
racy comparable to the systems trained on human-
annotated bilingual treebanks on the standard test
data. Moreover, our approach continued to pro-
vide improvement over the baseline systems when
we used a much larger monolingual treebank (over
50,000 sentences) where target human translations
are not available and very hard to construct. We also
demonstrated that the proposed approach can be ef-
fective in a purely monolingual setting with the help
of SMT.
Acknowledgments
This study was started when Wenliang Chen, Yu-
jie Zhang, and Yoshimasa Tsuruoka were members
of Language Infrastructure Group, National Insti-
tute of Information and Communications Technol-
ogy (NICT), Japan. We would also thank the anony-
mous reviewers for their detailed comments, which
have helped us to improve the quality of this work.
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese Translation Treebank V 1.0,
LDC2007T02. Linguistic Data Consortium.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP 2008, pages 877?886, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957?961, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP 1987-
89 WSJ Corpus Release 1, LDC2000T43. Linguistic
Data Consortium.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of EMNLP 2009, pages 570?579, Singapore,
August.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of ACL 2010, pages
21?29, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL 2007, pages 17?24, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP 2009, pages 1222?
1231, Singapore, August. Association for Computa-
tional Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL 2003, pages 48?54. Association for Computa-
tional Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL
2010, pages 1?11, Uppsala, Sweden, July. Association
for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL-IJCNLP2009, pages
513?521, Suntec, Singapore, August. Association for
Computational Linguistics.
Charles N. Li and Sandra A. Thompson. 1997. Man-
darin Chinese - A Functional Reference Grammar.
University of California Press.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL 2006,
pages 104?111, New York City, USA, June. Associa-
tion for Computational Linguistics.
Yang Liu and Liang Huang. 2010. Tree-based and forest-
based translation. In Tutorial Abstracts of ACL 2010,
page 2, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
82
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguisticss, 19(2):313?330.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL 2006, pages 81?88.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
IWPT2003, pages 149?160.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of EMNLP 2004, pages
49?56.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing us-
ing a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55?63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
83
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1180?1191,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Joint Models for Chinese POS Tagging and Dependency Parsing
Zhenghua Li?, Min Zhang?, Wanxiang Che?, Ting Liu?, Wenliang Chen? and Haizhou Li?
?Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{lzh,car,tliu}@ir.hit.edu.cn
?Institute for Infocomm Research, Singapore
{mzhang,wechen,hli}@i2r.a-star.edu.sg
Abstract
Part-of-speech (POS) is an indispensable fea-
ture in dependency parsing. Current research
usually models POS tagging and dependency
parsing independently. This may suffer from
error propagation problem. Our experiments
show that parsing accuracy drops by about
6% when using automatic POS tags instead
of gold ones. To solve this issue, this pa-
per proposes a solution by jointly optimiz-
ing POS tagging and dependency parsing in a
unique model. We design several joint models
and their corresponding decoding algorithms
to incorporate different feature sets. We fur-
ther present an effective pruning strategy to re-
duce the search space of candidate POS tags,
leading to significant improvement of parsing
speed. Experimental results on Chinese Penn
Treebank 5 show that our joint models sig-
nificantly improve the state-of-the-art parsing
accuracy by about 1.5%. Detailed analysis
shows that the joint method is able to choose
such POS tags that are more helpful and dis-
criminative from parsing viewpoint. This is
the fundamental reason of parsing accuracy
improvement.
1 Introduction
In dependency parsing, features consisting of part-
of-speech (POS) tags are very effective, since pure
lexical features lead to severe data sparseness prob-
lem. Typically, POS tagging and dependency pars-
ing are modeled in a pipelined way. However, the
pipelined method is prone to error propagation, es-
pecially for Chinese. Due to the lack of morpholog-
ical features, Chinese POS tagging is even harder
than other languages such as English. The state-of-
the-art accuracy of Chinese POS tagging is about
93.5%, which is much lower than that of English
(about 97% (Collins, 2002)). Our experimental re-
sults show that parsing accuracy decreases by about
6% on Chinese when using automatic POS tagging
results instead of gold ones (see Table 3 in Section
5). Recent research on dependency parsing usually
overlooks this issue by simply adopting gold POS
tags for Chinese data (Duan et al, 2007; Zhang and
Clark, 2008b; Huang and Sagae, 2010). In this pa-
per, we address this issue by jointly optimizing POS
tagging and dependency parsing.
Joint modeling has been a popular and effec-
tive approach to simultaneously solve related tasks.
Recently, many successful joint models have been
proposed, such as joint tokenization and POS tag-
ging (Zhang and Clark, 2008a; Jiang et al, 2008;
Kruengkrai et al, 2009), joint lemmatization and
POS tagging (Toutanova and Cherry, 2009), joint
tokenization and parsing (Cohen and Smith, 2007;
Goldberg and Tsarfaty, 2008), joint named en-
tity recognition and parsing (Finkel and Manning,
2009), joint parsing and semantic role labeling
(SRL) (Li et al, 2010), joint word sense disambigua-
tion and SRL (Che and Liu, 2010), joint tokenization
and machine translation (MT) (Dyer, 2009; Xiao et
al., 2010) and joint parsing and MT (Liu and Liu,
2010). Note that the aforementioned ?parsing? all
refer to constituent parsing.
As far as we know, there are few successful mod-
els for jointly solving dependency parsing and other
tasks. Being facilitated by Conference on Com-
putational Natural Language Learning (CoNLL)
2008 and 2009 shared tasks, several joint models
of dependency parsing and SRL have been pro-
posed. Nevertheless, the top-ranked systems all
adopt pipelined approaches (Surdeanu et al, 2008;
1180
Hajic? et al, 2009). Theoretically, joint modeling
of POS tagging and dependency parsing should be
helpful to the two individual tasks. On the one hand,
syntactic information can help resolve some POS
ambiguities which are difficult to handle for the se-
quential POS tagging models. On the other hand,
more accurate POS tags should further improve de-
pendency parsing.
For joint POS tagging and dependency parsing,
the major issue is to design effective decoding algo-
rithms to capture rich features and efficiently search
out the optimal results from a huge hypothesis
space.1 In this paper, we propose several dynamic
programming (DP) based decoding algorithms for
our joint models by extending existing parsing algo-
rithms. We also present effective pruning techniques
to speed up our decoding algorithms. Experimen-
tal results on Chinese Penn Treebank show that our
joint models can significantly improve the state-of-
the-art parsing accuracy by about 1.5%.
The remainder of this paper is organized as fol-
lows. Section 2 describes the pipelined method, in-
cluding the POS tagging and parsing models. Sec-
tion 3 discusses the joint models and the decod-
ing algorithms, while Section 4 presents the pruning
techniques. Section 5 reports the experimental re-
sults and error analysis. We review previous work
closely related to our method in Section 6, and con-
clude this paper in Section 7.
2 The Baseline Pipelined Method
Given an input sentence x = w1...wn, we denote its
POS tag sequence by t = t1...tn, where ti ? T , 1 ?
i ? n, and T is the POS tag set. A dependency tree
is denoted by d = {(h,m) : 0 ? h ? n, 0 < m ?
n}, where (h,m) represents a dependency wh ?
wm whose head word (or father) is wh and modifier
(or child) is wm. w0 is an artificial root token which
is used to simplify the formalization of the problem.
The pipelined method treats POS tagging and de-
pendency parsing as two cascaded problems. First,
1It should be noted that it is straightforward to simultane-
ously do POS tagging and constituent parsing, as POS tags can
be regarded as non-terminals in the constituent structure (Levy
and Manning, 2003). In addition, Rush et al (2010) describes
an efficient and simple inference algorithm based on dual de-
composition and linear programming relaxation to combine a
lexicalized constituent parser and a trigram POS tagger.
an optimal POS tag sequence t? is determined.
t? = arg max
t
Scorepos(x, t)
Then, an optimal dependency tree d? is determined
based on x and t?.
d? = arg max
d
Scoresyn(x, t?,d)
2.1 POS Tagging
POS tagging is a typical sequence labeling prob-
lem. Many models have been successfully applied
to sequence labeling problems, such as maximum-
entropy (Ratnaparkhi, 1996), conditional random
fields (CRF) (Lafferty et al, 2001) and perceptron
(Collins, 2002). We use perceptron to build our POS
tagging baseline for two reasons. Firstly, as a linear
model, perceptron is simple, fast, and effective. It is
competitive to CRF in tagging accuracy but requires
much less training time (Shen et al, 2007). Sec-
ondly, perceptron has been successfully applied to
dependency parsing as well (Koo and Collins, 2010).
In this paper, perceptron is used in all models includ-
ing the POS tagging model, the dependency parsing
models and the joint models.
In a perceptron, the score of a tag sequence is
Scorepos(x, t) = wpos ? fpos(x, t)
where fpos(x, t) refers to the feature vector andwpos
is the corresponding weight vector.
For POS tagging features, we follow the work of
Zhang and Clark (2008a). Three feature sets are
considered: POS unigram, bigram and trigram fea-
tures. For brevity, we will refer to the three sets as
wi ti, ti?1 ti and ti?2 ti?1 ti.
Given wpos, we adopt the Viterbi algorithm to get
the optimal tagging sequence.
2.2 Dependency Parsing
Recently, graph-based dependency parsing has
gained more and more interest due to its state-of-
the-art accuracy. Graph-based dependency parsing
views the problem as finding the highest scoring tree
from a directed graph. Based on dynamic program-
ming decoding, it can efficiently find an optimal tree
in a huge search space. In a graph-based model, the
1181
score of a dependency tree is factored into scores of
small parts (subtrees).
Scoresyn(x, t,d) = wsyn ? fsyn(x, t,d)
=
?
p?d
Scoresyn(x, t, p)
where p is a scoring part which contains one or more
dependencies in the dependency tree d. Figure 1
shows different types of scoring parts used in current
graph-based models.
h m
dependency
h s
sibling
m g h
grandparent
m
h s
tri-sibling
mth s
grand-sibling
mg
Figure 1: Different types of scoring parts used in current
graph-based models (Koo and Collins, 2010).
Eisner (1996) proposes an O(n3) decoding al-
gorithm for dependency parsing. Based on the al-
gorithm, McDonald et al (2005) propose the first-
order model, in which the scoring parts only con-
tains dependencies. The second-order model of Mc-
Donald and Pereira (2006) incorporates sibling parts
and also needs O(n3) parsing time. The second-
order model of Carreras (2007) incorporates both
sibling and grandparent parts, and needs O(n4)
parsing time. However, the grandparent parts are
restricted to those composed of outermost grand-
children. Koo and Collins (2010) propose efficient
decoding algorithms of O(n4) for third-order mod-
els. In their paper, they implement two versions
of third-order models, Model 1 and Model 2 ac-
cording to their naming. Model 1 incorporates only
grand-sibling parts, while Model 2 incorporates both
grand-sibling and tri-sibling parts. Their experi-
ments on English and Czech show that Model 1 and
Model 2 obtain nearly the same parsing accuracy.
Therefore, we use Model 1 as our third-order model
in this paper.
We use three versions of graph-based dependency
parsing models.
? The first-order model (O1): the same with Mc-
Donald et al (2005).
? The second-order model (O2): the same with
Model 1 in Koo and Collins (2010), but without
using grand-sibling features.2
? The third-order model (O3): the same with
Model 1 in Koo and Collins (2010).
We adopt linear models to define the score of a de-
pendency tree. For the third-order model, the score
of a dependency tree is represented as:
Scoresyn(x, t,d) =
?
{(h,m)}?d
wdep ? fdep(x, t, h,m)
+
?
{(h,s)(h,m)}?d
wsib ? fsib(x, t, h, s,m)
+
?
{(g,h),(h,m)}?d
wgrd ? fgrd(x, t, g, h,m)
+
?
{(g,h),(h,s),(h,m)}?d
wgsib ? fgsib(x, t, g, h, s,m)
For the first- and second-order models, the above
formula is modified by deactivating extra parts.
For parsing features, we follow standard prac-
tice for graph-based dependency parsing (McDon-
ald, 2006; Carreras, 2007; Koo and Collins, 2010).
Since these features are highly related with our joint
decoding algorithms, we summarize the features as
follows.
? Dependency Features, fdep(x, t, h,m)
? Unigram Features: whth dir, wmtm dir
? Bigram Features: whth wmtm dir dist
? In Between Features: th tb tm dir dist
? Surrounding Features:
th?1 th th+1 tm?1 tm tm+1 dir dist
? Sibling Features, fsib(x, t, h, s,m)
wh th ws ts wm tm dir
? Grandparent Features, fgrd(x, t, g, h,m)
wg tg wh th wm tm dir gdir
? Grand-sibling Features, fgsib(x, t, g, h, s,m)
wg tg wh th ws ts wm tm dir gdir
2This second-order model incorporates grandparent features
composed of all grandchildren rather than just outermost ones,
and outperforms the one of Carreras (2007) according to the
results in Koo and Collins (2010).
1182
where b denotes an index between h and m; dir
and dist are the direction and distance of (h,m);
gdir is the direction of (g, h). We also use back-
off features by generalizing from very specific fea-
tures over word forms, POS tags, directions and dis-
tances to less sparse features over just POS tags or
considering fewer nodes. To avoid producing too
many sparse features, at most two word forms are
used at the same time in sibling, grandparent and
grand-sibling features, while POS tags are used in-
stead for other nodes; meanwhile, at most four POS
tags are considered at the same time for surrounding
features.
3 Joint Models
In the joint method, we aim to simultaneously solve
the two problems.
(t?, d?) = arg max
t,d
Scorejoint(x, t,d)
Under the linear model, the score of a tagged de-
pendency tree is:
Scorejoint(x, t,d) = Scorepos(x, t)
+ Scoresyn(x, t,d)
= wpos?syn ? fpos?syn(x, t,d)
where fpos?syn(.) means the concatenation of fpos(.)
and fsyn(.). Under the joint model, the weights of
POS and syntactic features, wpos?syn, are simulta-
neously learned. We expect that POS and syntactic
features can interact each other to determine an op-
timal joint result.
Similarly to the baseline dependency parsing
models, we define the first-, second-, and third-order
joint models according to the syntactic features con-
tained in fsyn(.).
In the following, we propose two versions of joint
models which can capture different feature sets and
have different complexity.
3.1 Joint Models of Version 1
The crucial problem for the joint method is to de-
sign effective decoding algorithms to capture rich
features and efficiently search out the optimal re-
sults from a huge hypothesis space. Eisner (2000)
describes a preliminary idea to handle polysemy by
extending parsing algorithms. Based on this idea,
we extend decoding algorithms of McDonald et al
(2005) and Koo and Collins (2010), and propose two
DP based decoding algorithms for our joint models
of version 1.
(b)
(a)
i r r j
r+1 ji ri j
i j
Figure 2: The DP structures and derivations of the first-
order decoding algorithm of joint models of version 1.
We omit symmetric right-headed versions for brevity.
Trapezoids denote incomplete spans. Triangles denote
complete spans. Solid circles denote POS tags of the cor-
responding indices.
The decoding algorithm of O1: As shown in
Figure 2, the first-order joint decoding algorithm
utilizes two types of dynamic programming struc-
tures. (1) Incomplete spans consist of a dependency
and the region between the head and modifier; (2)
Complete spans consist of a headword and its de-
scendants on one side. Each span is recursively cre-
ated by combining two smaller and adjacent spans
in a bottom-up fashion.
The pseudo codes are given in Algorithm 1.
I(i,j)(ti,tj) denotes an incomplete span from i to j
whose boundary POS tags are ti and tj . C(i,j)(ti,tj)
refers to a complete span from i to j whose bound-
ary POS tags are ti and tj . Conversely, I(j,i)(tj ,ti)
andC(j,i)(tj ,ti) represent spans of the other direction.
Note that in these notations the first argument index
always refers to the head of the span.
Line 6 corresponds to the derivation in Figure 2-
(a). Scorejoint(x, ti, tr, tr+1, tj , p = {(i, j)}) cap-
tures the joint features invented by this combina-
tion, where p = {(i, j)} means that the newly ob-
served scoring part is the dependency (i, j). The
syntactic features, denoted by fsyn(x, ti, tj , i, j), can
only incorporate syntactic unigram and bigram fea-
tures. The surrounding and in between features
are unavailable, because the context POS tags, such
as tb and ti?1, are not contained in the DP struc-
1183
Algorithm 1 The first-order joint decoding algorithm of version 1
1: ?0 ? i ? n, ti ? T C(i,i)(ti,ti) = 0 ? initialization
2: for w = 1..n do ? span width
3: for i = 0..(n? w) do ? span start index
4: j = i + w ? span end index
5: for (ti, tj) ? T 2 do
6: I(i,j)(ti,tj) = maxi?r<j max(tr,tr+1)?T 2{C(i,r)(ti,tr) + C(j,r+1)(tj ,tr+1) + Scorejoint(x, ti, tr, tr+1, tj , p = {(i, j)})}7: I(j,i)(tj ,ti) = maxi?r<j max(tr,tr+1)?T 2{C(i,r)(ti,tr) + C(j,r+1)(tj ,tr+1) + Scorejoint(x, ti, tr, tr+1, tj , p = {(j, i)})}8: C(i,j)(ti,tj) = maxi<r?j maxtr?T {I(i,r)(ti,tr) + C(r,j)(tr,tj) + Scorejoint(x, ti, tr, tj , p = ?)}
9: C(j,i)(tj ,ti) = maxi?r<j maxtr?T {C(r,i)(tr,ti) + I(j,r)(tj ,tr) + Scorejoint(x, ti, tr, tj , p = ?)}
10: end for
11: end for
12: end for
tures. Therefore, we adopt pseudo surrounding
and in between features by simply fixing the con-
text POS tags as the single most likely ones (Mc-
Donald, 2006). Taking the in between features
as an example, we use ti t?b tj dir dist instead,
where t?b is the 1-best tag determined by the base-
line POS tagger. The POS features, denoted by
fpos(x, ti, tr, tr+1, tj), can only incorporate all POS
unigram and bigram features.3 Similarly, we use
pseudo POS trigram features such as t?r?1 tr tr+1.
Line 8 corresponds to the derivation in Figure 2-
(b). Since this combination invents no scoring part
(p = ?), Scorejoint(x, ti, tr, tj , p = ?) is only com-
posed of POS features.4
Line 7 and Line 9 create spans in the opposite di-
rection, which can be analogously illustrated. The
space and time complexity of the algorithm are re-
spectively O(n2q2) and O(n3q4), where q = |T |.5
The decoding algorithm of O2 & O3: Figure
3 illustrates the second- and third-order decoding
algorithm of joint models of version 1. A new
kind of span, named the sibling span, is used to
capture sibling structures. Furthermore, each span
is augmented with a grandparent-index to capture
both grandparent and grand-sibling structures. It is
straightforward to derive the pseudo codes of the al-
3? wr tr if i ?= r; ? wr+1 tr+1 if r + 1 ?= j; ? tr tr+1
if r ?= i or r + 1 ?= j; ? ti tr if r ? 1 = i; ? tr+1 tj if
r + 2 = j. Note that wi ti, wj tj and ti tj (if i = j ? 1) are
not incorporated here to avoid double counting.
4? wr tr if r ?= j;? ti tr if i = r?1;? tr tj if r+1 = j.
Pseudo trigram features can be added accordingly.
5We can reduce the time complexity to O(n3q3) by strictly
adopting the DP structures in the parsing algorithm of Eisner
(1996). However, that may make the algorithm harder to com-
prehend.
ig j g i i ji+1
(a)
g i j g i k i k j
(b)
i k j i ik jr+1r
(c)
g i j g i r i r j
(d)
Figure 3: The DP structures and derivations of the
second- and third-order joint decoding algorithm of ver-
sion 1. For brevity, we elide the right-headed and right-
grandparented versions. Rectangles represent sibling
spans.
i j i r r j
i j i r r+1 j
(b)
(a)
Figure 4: The DP structures and derivations of the first-
order joint decoding algorithm of version 2. We omit the
right-headed version for brevity.
1184
gorithm from Figure 3. We omit them due to space
limitation. Pseudo surrounding, in between and POS
trigram features are used due to the same reason as
above. The space and time complexity of the algo-
rithm are respectively O(n3q3) and O(n4q5).
3.2 Joint Models of Version 2
To further incorporate genuine syntactic surround-
ing and POS trigram features in the DP structures,
we extend the algorithms of joint models of version
1, and propose our joint models of version 2.
The decoding algorithm of O1: Figure 4 illus-
trates the first-order joint decoding algorithm of ver-
sion 2. Compared with the structures in Figure 2,
each span is augmented with the POS tags surround-
ing the boundary indices. These context POS tags
enable Scorejoint(.) in line 6-9 of Algorithm 1 to
capture the syntactic surrounding and POS trigram
features, but also require enumeration of POS tags
over more indices. For brevity, we skip the pseudo
codes which can be easily derived from Algorithm
1. The space and time complexity of the algorithm
are respectively O(n2q6) and O(n3q10).
The decoding algorithm of O2 & O3: Using the
same idea as above, the second- and third-order joint
decoding algorithms of version 2 can be derived
based on Figure 3. Again, we omit both its DP struc-
tures and pseudo codes for the sake of brevity. Its
space and time complexity are respectively O(n3q7)
and O(n4q11).
In between features, which should be regarded as
non-local features in the joint situation, still cannot
be incorporated in our joint models of version 2.
Again, we adopt the pseudo version.
3.3 Comparison
Based on the above illustration, we can see that joint
models of version 1 are more efficient with regard
to the number of POS tags for each word, but fail to
incorporate syntactic surrounding features and POS
trigram features in the DP structures. On the con-
trary, joint models of version 2 can incorporate both
aforementioned feature sets, but have higher com-
plexity. These two versions of models will be thor-
oughly compared in the experiments.
4 Pruning Techniques
In this section, we introduce two pruning strategies
to constrain the search space of our models due to
their high complexity.
4.1 POS Tag Pruning
The time complexity of the joint decoding algorithm
is unbearably high with regard to the number of can-
didate POS tags for each word (q = |T |). We
find that it would be extremely time-consuming even
when we only use two most likely POS tags for each
word (q = 2) even for joint models of version 1.
To deal with this problem, we propose a pruning
method that can effectively reduce the POS tag space
based on a probabilistic tagging model.
We adopt a conditional log-linear model (Lafferty
et al, 2001), which defines a conditional distribution
of a POS tag sequence t given x:
P (t|x) = e
wpos?fpos(x,t)
?
t ewpos?fpos(x,t)
We use the same feature set fpos defined in Sec-
tion 2.1, and adopt the exponentiated gradient algo-
rithm to learn the weight vector wpos (Collins et al,
2008).
The marginal probability of tagging a word wi as
t is
P (ti = t|x) =
?
t:t[i]?t
P (t|x)
which can be efficiently computed using the
forward-backward algorithm.
We define pmaxi(x) to be the highest marginal
probability of tagging the word wi:
pmaxi(x) = maxt?T P (ti = t|x)
We then define the allowable candidate POS tags
of the word wi to be
Ti(x) = {t : t ? T , P (ti = t|x) ? ?t?pmaxi(x)}
where ?t is the pruning threshold. Ti(x) is used to
constrain the POS search space by replacing T in
Algorithm 1.
1185
4.2 Dependency Pruning
The parsing time grows quickly for the second- and
third-order models (both baseline and joint) when
the input sentence gets longer (O(n4)). Follow-
ing Koo and Collins (2010), we eliminate unlikely
dependencies using a form of coarse-to-fine prun-
ing (Charniak and Johnson, 2005; Petrov and Klein,
2007). On the development set, 68.87% of the de-
pendencies are pruned, while the oracle dependency
accuracy is 99.77%. We use 10-fold cross validation
to do pruning on the training set.
5 Experiments
We use the Penn Chinese Treebank 5.1 (CTB5) (Xue
et al, 2005). Following the setup of Duan et al
(2007), Zhang and Clark (2008b) and Huang and
Sagae (2010), we split CTB5 into training (secs 001-
815 and 1001-1136), development (secs 886-931
and 1148-1151), and test (secs 816-885 and 1137-
1147) sets. We use the head-finding rules of Zhang
and Clark (2008b) to turn the bracketed sentences
into dependency structures.
We use the standard tagging accuracy to evalu-
ate POS tagging. For dependency parsing, we use
word accuracy (also known as dependency accu-
racy), root accuracy and complete match rate (all
excluding punctuation) .
For the averaged training, we train each model for
15 iterations and select the parameters that perform
best on the development set.
5.1 Results of POS Tag Pruning
Figure 5 shows the distribution of words with dif-
ferent number of candidate POS tags and the k-best
oracle tagging accuracy under different ?t. To avoid
dealing with words that have many candidate POS
tags, we further apply a hard criterion that the decod-
ing algorithms only consider top k candidate POS
tags.
To find the best ?t, we train and evaluate the
second-order joint model of version 1 on the train-
ing and development sets pruned with different ?t
(top k = 5). We adopt the second-order joint model
of version 1 because of its efficiency compared with
the third-order models and its capability of captur-
ing rich features compared with the first-order mod-
els. The results are shown in Table 1. The model
0
10
20
30
40
50
60
70
80
90
100
1 2 3 4 5 >5
pro
por
tion
 of 
wor
ds (
%)
number of candidate POS tags 
0.1
0.01
0.001
93
94
95
96
97
98
99
100
1 2 3 4 5 ?
k-b
est 
ora
cle 
tagg
ing
 acc
ura
cy (
%)
k
0.1
0.01
0.001
Figure 5: Results of POS tag pruning with different prun-
ing threshold ?t on the development set.
?t word root compl. acc. speed
0.1 81.53 76.88 30.00 94.17 2.5
0.01 81.83 76.62 30.62 93.16 1.2
0.001 81.73 77.38 30.50 93.41 0.5
Table 1: Performance of the second-order joint model of
version 1 with different pruning threshold ?t (top k = 5)
on the development set. ?Acc.? means the tagging accu-
racy. ?Speed? refers to the parsing speed (the number of
sentences processed per second).
with ?t = 0.1 obtains the highest tagging accuracy,
which is much higher than that of both ?t = 0.01
and ?t = 0.001. However, its parsing accuracy
is inferior to the other two. ?t = 0.01 produces
slightly better parsing accuracy than ?t = 0.001,
and is twice faster. Finally, we choose ?t = 0.01
due to the efficiency factor and our priority over the
parsing accuracy.
Then we do experiments to find an optimal top
k. Table 2 shows the results. We decide to choose
k = 3 since it leads to best parsing accuracy.
From Table 1 and 2, we can have an interesting
finding: it seems that the harder we filter the POS
tag space, the higher tagging accuracy we get. In
other words, giving the joint model less flexibility
of choosing POS tags leads to better tagging per-
formance.
Due to time limitation, we do not tune ?t and k for
other joint models. Instead, we simply adopt ?t =
0.01 and top k = 3.
5.2 Final Results
Table 3 shows the final results on the test set. We list
a few state-of-the-art results in the bottom. Duan07
refers to the results of Duan et al (2007). They
enhance the transition-based parsing model with
1186
Syntactic Metrics Tagging Accuracy Parsing Speed
word root compl. all-word known unknown Sent/Sec
Joint Models V2
O3 80.79 75.84 29.11 92.80 93.88 76.80 0.3
O2 80.49 75.49 28.24 92.68 93.77 76.27 0.6
O1 77.37 68.64 23.09 92.96 94.05 76.64 2.0
Joint Models V1
O3 80.69 75.90 29.06 92.89 93.96 76.80 0.5
O2 80.74 75.80 28.24 93.08 94.11 77.53 1.7
O1 77.38 69.69 22.62 93.20 94.23 77.76 8.5
Auto POS
O3 79.29 74.65 27.24
93.51 94.36 80.78
2.0
O2 79.03 74.70 27.19 5.8
O1 75.68 68.06 21.10 17.4
MSTParser2 77.95 72.04 25.50 4.1
MSTParser1 75.84 68.55 21.36 5.2
MaltParser 75.24 65.92 23.19 2.6
Gold POS
O3 86.00 77.59 34.02
100.0 100.0 100.0
-
O2 86.18 78.58 34.07 -
O1 82.24 70.10 26.02 -
MSTParser2 85.24 77.41 33.19 -
MSTParser1 83.04 71.49 27.59 -
MaltParser 82.62 69.34 29.06 -
H&S10 85.20 78.32 33.72 -
Z&C08 single 84.33 76.73 32.79 -
Z&C08 hybrid 85.77 76.26 34.41 -
Duan07 83.88 73.70 32.70 -
Table 3: Final results on the test set. ?Gold POS? means that gold POS tags are used as input by the pipelined parsing
models; while ?Auto POS? means that the POS tags are generated by the baseline POS tagging model.
top k word root compl. acc. speed
2 81.46 76.12 30.50 93.51 2.7
3 82.11 76.75 29.75 93.31 1.7
4 81.75 76.62 30.38 93.25 1.4
5 81.83 76.62 30.62 93.16 1.2
Table 2: Performance of the second-order joint model of
version 1 with different top k (?t = 0.01) on the devel-
opment set.
the beam search. H&S10 refers to the results of
Huang and Sagae (2010). They greatly expand the
search space of the transition-based model by merg-
ing equivalent states with dynamic programming.
Z&C08 refers to the results of Zhang and Clark
(2008b). They use a hybrid model to combine the
advantages of both graph-based and transition-based
models. We also do experiments with two publicly
available and widely-used parsers, MSTParser6 and
MaltParser7. MSTParser1 refers to the first-order
6http://sourceforge.net/projects/mstparser/
7http://maltparser.org/
graph-based model of McDonald et al (2005), while
MSTParser2 is the second-order model of McDon-
ald and Pereira (2006). MaltParser is a transition-
based parsing system. It integrates a number of clas-
sification algorithms and transition strategies. We
adopt the support vector machine classifier and the
arc-standard strategy (Nivre, 2008).
We can see that when using gold tags, our
pipelined second- and third-order parsing models
achieve best parsing accuracy, which is even higher
than the hybrid model of Zhang and Clark (2008b).
It is a little surprising that the second-order model
slightly outperforms the third-order one. This may
be possible, since Koo and Collins (2010) shows that
the third-order model outperforms the second-order
one by only 0.32% on English and 0.07% on Czech.
In addition, we only use basic third-order features.
Both joint models of version 1 and 2 can consis-
tently and significantly improve the parsing accu-
racy by about 1.5% for all first-, second- and third-
order cases. Accidentally, the parsing accuracy of
the second-order joint model of version 2 is lower
1187
error pattern # ? error pattern # ?
DEC ? DEG 237 114 NR ? NN 184 100
NN ? VV 389 73 NN ? NR 106 91
DEG ? DEC 170 39 NN ? JJ 95 70
VV ? NN 453 27 VA ? VV 29 41
P ? VV 52 24 JJ ? NN 126 29
P ? CC 39 13 VV ? VA 67 10
Table 4: Error analysis of POS tagging. # means the
error number of the corresponding pattern made by the
baseline tagging model. ? and ? mean the error number
reduced or increased by the joint model.
than that of its counterparts by about 0.3%. More
experiments and further analysis may be needed to
find out the reason. The two versions of joint models
performs nearly the same, which indicates that using
pseudo surrounding and POS trigram features may
be sufficient for the joint method on this data set.
In summary, we can conclude that the joint frame-
work is certainly helpful for dependency parsing.
It is clearly shown in Table 3 that the joint
method surprisingly hurts the tagging accuracy,
which diverges from our discussion in Section 1.
Some insights into this issue will be given in Sec-
tion 5.3. Moreover, it seems that the more syntac-
tic features the joint method incorporates (from
O1 to O3), the more the tagging accuracy drops.
We suspect that this is because the joint models are
dominated by the syntactic features. Take the first-
order joint model as an example. The dimension of
the syntactic features fsyn is about 3.5 million, while
that of fpos is only about 0.5 million. The gap be-
comes much larger for the second- and third-order
cases.
Comparing the parsing speed, we can find that the
pruning of POS tags is very effective. The second-
order joint model of version 1 can parse 1.7 sen-
tences per second, while the pipelined second-order
parsing model can parse 5.8 sentences per second,
which is rather close considering that there is a fac-
tor of q5.
5.3 Error Analysis
To find out the impact of our joint models on the
individual tasks, we conduct detailed error analy-
sis through comparing the results of the pipelined
second-order parsing model and the second-order
joint model of version 1.
Impact on POS tagging: Table 4 shows how the
joint model changes the quantity of POS tagging er-
ror patterns compared with the pipelined model. An
error pattern ?X ? Y? means that the focus word,
whose true tag is ?X?, is assigned a tag ?Y?. We
choose these patterns with largest reduction or in-
crease in the error number, and rank them in de-
scending order of the variation.
From the left part of Table 4, we can see that
the joint method is clearly better at resolving tag-
ging ambiguities like {VV, NN} and {DEG, DEC}.8
One common characteristic of these ambiguous
pairs is that the local or even whole syntactic struc-
ture will be destructed if the wrong tag is chosen. In
other words, resolving these ambiguities is critical
and helpful from the parsing viewpoint. From an-
other perspective, the joint model is capable of pre-
ferring the right tag with the help of syntactic struc-
tures, which is impossible for the baseline sequential
labeling model.
In contrast, pairs like {NN, NR}, {VV, VA} and
{NN, JJ} only slightly influence the syntactic struc-
ture when mis-tagged. The joint method performs
worse on these ambiguous pairs, as shown in the
right part of Table 4.
Impact on parsing: Table 5 studies the change of
parsing error rates between the pipelined and joint
model on different POS tag patterns. We present the
most typical and prominent patterns in the table, and
rank them in descending order of X?s frequency of
occurrence. We also show the change of proportion
of different patterns, which is consistent with the re-
sults in Table 4.
From the table, we can see the joint model can
achieve a large error reduction (0.8?4.0%) for all
the patterns ?X ? X?. In other words, the joint
model can do better given the correct tags than
the pipelined method.
For all the patterns marked by ?, except for the
ambiguous pair {NN, JJ} (which we find is difficult
to explain even after careful result analysis), the joint
model also reduces the error rates (2.2?15.4%). As
8DEG and DEC are the two POS tags for the frequently used
auxiliary word ??? (de?, of) in Chinese. The associative ???
is tagged as DEG, such as ???/father ? ??/eyes (eyes of
the father)?; while the one in a relative clause is tagged as DEC,
such as ??/he ??/made ? ??/progress (progress that he
made)?.
1188
pattern pipelined jointprop (%) error (%) prop (%) error (%)
NN ? NN 94.6 16.8 -1.1 -1.8
? VV ? 2.9 55.5 -0.5 +15.1
? NR ? 0.8 24.5 +0.7 -2.2
? JJ ? 0.7 17.9 +0.5 +2.1
VV ? VV 89.6 34.2 -0.3 ?4.0
? NN ? 6.6 66.4 -0.4 +0.7
? VA ? 1.0 38.8 +0.1 -15.4
NR ? NR 91.7 15.4 -3.7 -0.8
? NN ? 5.9 21.7 +3.2 -3.7
P ? P 92.8 22.6 +3.4 -3.2
? VV ? 3.0 50.0 -1.4 +10.7
? CC ? 2.3 74.4 -0.7 +21.9
JJ ? JJ 80.5 11.2 -2.8 -2.0
? NN ? 9.8 18.3 +2.2 +1.8
DEG ? DEG 86.5 11.1 +2.8 -3.6
? DEC ? 13.5 61.8 -3.1 +37.4
DEC ? DEC 79.7 17.2 +12.1 -4.0
? DEG ? 20.2 56.5 -9.7 +40.2
Table 5: Comparison of parsing error rates on different
POS tag patterns between the pipelined and joint models.
Given a pattern ?X ? Y?, ?prop? means its proportion in
all occurrence of ?X? (Count(X?Y )Count(X) ), and ?error? refers
to its parsing error rate ( Count(wrongly headed X?Y )Count(X?Y ) ).
The last two columns give the absolute reduction (-) or
increase (+) in proportion and error rate made by the joint
model. ? marks the patterns appearing in the left part of
Table 4, while ? marks those in the right part of Table 4.
discussed earlier, these patterns concern ambiguous
tag pairs which usually play similar roles in syn-
tactic structures. This demonstrates that the joint
model can do better on certain tagging error pat-
terns.
For patterns marked by ?, the error rate of the
joint model usually increases by large margin. How-
ever, the proportion of these patterns is substantially
decreased, since the joint model can better resolve
these ambiguities with the help of syntactic knowl-
edge.
In summary, we can conclude that the joint model
is able to choose such POS tags that are more helpful
and discriminative from parsing viewpoint. This is
the fundamental reason of the parsing performance
improvement.
6 Related Work
Theoretically, Eisner (2000) proposes a preliminary
idea of extending the decoding algorithm for de-
pendency parsing to handle polysemy. Here, word
senses can be understood as POS-tagged words.
Koo and Collins (2010) also briefly discuss that their
third-order decoding algorithm can be modified to
handle word senses using the idea of Eisner (2000).
In his PhD thesis, McDonald (2006) extends his
second-order model with the idea of Eisner (2000)
to study the impact of POS tagging errors on pars-
ing accuracy. To make inference tractable, he uses
top 2 candidate POS tags for each word based on
a maximum entropy tagger, and adopts the single
most likely POS tags for the surrounding and in be-
tween features. He conducts primitive experiments
on English Penn Treebank, and shows that parsing
accuracy can be improved from 91.5% to 91.9%.
However, he finds that the model is unbearably time-
consuming.
7 Conclusions
In this paper, we have systematically investigated
the issue of joint POS tagging and dependency pars-
ing. We propose and compare several joint models
and their corresponding decoding algorithms which
can incorporate different feature sets. We also pro-
pose an effective POS tag pruning method which can
greatly improve the decoding efficiency. The experi-
mental results show that our joint models can signif-
icantly improve the state-of-the-art parsing accuracy
by more than 1.5%. Detailed error analysis shows
that the fundamental reason for the parsing accu-
racy improvement is that the joint method is able to
choose POS tags that are helpful and discriminative
from parsing viewpoint.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments. This work was supported by National
Natural Science Foundation of China (NSFC) via
grant 60803093, 60975055, the Natural Scientific
Research Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069) and the Fun-
damental Research Funds for the Central Universi-
ties (HIT.KLOF.2010064).
References
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
1189
EMNLP/CoNLL, pages 141?150.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL-05, pages 173?180.
Wanxiang Che and Ting Liu. 2010. Jointly modeling
wsd and srl with markov logic. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 161?169.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of EMNLP-CoNLL 2007, pages 208?217.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter Bartlett. 2008. Exponentiated
gradient algorithms for conditional random fields and
max-margin markov networks. JMLR, 9:1775?1822.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP 2002.
Xiangyu Duan, Jun Zhao, , and Bo Xu. 2007. Proba-
bilistic models for action-based Chinese dependency
parsing. In Proceedings of ECML/ECPPKDD.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 406?
414.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of COLING 1996, pages 340?345.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Advances in Probabilistic
and Other Parsing Technologies, pages 29?62.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
326?334.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of ACL-08: HLT,
pages 371?379, Columbus, Ohio, June. Association
for Computational Linguistics.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoNLL
2009.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1077?1086,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL-08: HLT, pages 897?904.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1?11, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 513?521.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML 2001, pages 282?289.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse chinese, or the chinese treebank? In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 439?446,
Sapporo, Japan, July. Association for Computational
Linguistics.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of chinese. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1108?
1117.
Yang Liu and Qun Liu. 2010. Joint parsing and trans-
lation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 707?715.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL 2006.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. In Computational Lin-
guistics, volume 34, pages 513?553.
1190
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 1?11.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
760?767, Prague, Czech Republic, June. Association
for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL-2008.
Kristina Toutanova and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech pre-
diction. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 486?494.
Xinyan Xiao, Yang Liu, YoungSook Hwang, Qun Liu,
and Shouxun Lin. 2010. Joint tokenization and trans-
lation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 1200?1208.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering, volume 11, pages 207?238.
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single perceptron.
In Proceedings of ACL-08: HLT, pages 888?896.
Yue Zhang and Stephen Clark. 2008b. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 562?571, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
1191
Linguistically Annotated Reordering:
Evaluation and Analysis
Deyi Xiong?
Institute for Infocomm Research
Min Zhang??
Institute for Infocomm Research
Aiti Aw?
Institute for Infocomm Research
Haizhou Li?
Institute for Infocomm Research
Linguistic knowledge plays an important role in phrase movement in statistical machine trans-
lation. To efficiently incorporate linguistic knowledge into phrase reordering, we propose a new
approach: Linguistically Annotated Reordering (LAR). In LAR, we build hard hierarchical skele-
tons and inject soft linguistic knowledge from source parse trees to nodes of hard skeletons during
translation. The experimental results on large-scale training data show that LAR is comparable
to boundary word-based reordering (BWR) (Xiong, Liu, and Lin 2006), which is a very compet-
itive lexicalized reordering approach. When combined with BWR, LAR provides complementary
information for phrase reordering, which collectively improves the BLEU score significantly.
To further understand the contribution of linguistic knowledge in LAR to phrase reordering,
we introduce a syntax-based analysis method to automatically detect constituent movement in
both reference and system translations, and summarize syntactic reordering patterns that are
captured by reordering models. With the proposed analysis method, we conduct a comparative
analysis that not only provides the insight into how linguistic knowledge affects phrase move-
ment but also reveals new challenges in phrase reordering.
1. Introduction
The phrase-based approach is a widely accepted formalism in statistical machine trans-
lation (SMT). It segments the source sentence into a sequence of phrases (not necessarily
syntactic phrases), then translates and reorders these phrases in the target. The reason
for the popularity of phrasal SMT is its capability of non-compositional translations and
? 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: dyxiong@i2r.a-star.edu.sg.
?? 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: mzhang@i2r.a-star.edu.sg.
? 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: aaiti@i2r.a-star.edu.sg.
? 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: hli@i2r.a-star.edu.sg.
Submission received: 24 October 2008; revised submission received: 12 March 2010; accepted for publication:
21 April 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
local word reorderings within phrases. Unfortunately, reordering at the phrase level is
still problematic for phrasal SMT. The default distortion-based reordering model simply
penalizes phrase movement according to the jump distance, without considering any
linguistic contexts (morphological, lexical, or syntactic) around phrases.
In order to utilize lexical information for phrase reordering, Tillman (2004) and
Koehn et al (2005) propose lexicalized reordering models which directly condition
phrase movement on phrases themselves. One problem with such lexicalized reordering
models is that they are restricted only to reorderings of phrases seen in training data.
To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words
of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering
evidence. Although these lexicalized reordering models significantly outperform the
distortion-based reordering model as reported, only using lexical information (e.g.,
boundary words) is not adequate to move phrases to appropriate positions.
Consider the following Chinese example with its English translation:
[VP [PP(while)(develop)(related)(legislation)] [VP [VV
(consider)] [NP [DNP [NP(this) (referendum)] [DEG(of)]] [NP
(results)]]]]1
consider the results of this referendum while developing related legislation
In this example, boundary words and are able to decide that the translation of the
PP phrase ... should be postponed until some phrase that succeeds it is translated.
But they cannot provide further information about exactly which succeeding phrase
should be translated first. If high-level linguistic knowledge, such as the syntactic
context VP?PP VP, is given, the position of the PP phrase can be easily determined
since the pre-verbal modifier PP in Chinese is frequently translated into a post-verbal
counterpart in English.
In this article, we focus on linguistically motivated phrase reordering, which in-
tegrates high-level linguistic knowledge in phrase reordering. We adopt a two-step
strategy. In the first step, we establish a hierarchical skeleton in phrasal SMT by in-
corporating Bracketing Transduction Grammar (BTG) (Wu 1997) into phrasal SMT. In
the second step, we inject soft linguistic information into nodes of the skeleton.
There are two significant advantages to using BTG in phrasal SMT. First, BTG is able
to generate hierarchical structures.2 This not only enhances phrasal SMT?s capability
for hierarchical and long-distance reordering but also establishes a platform for phrasal
SMT to incorporate knowledge from linguistic structure. Second, phrase reordering is
restricted by the ITG constraint (Wu 1997). Although it only allows two orders (straight
or inverted) of nodes in any binary branching structure, it is broadly verified that the
ITG constraint has good coverage of word reorderings on various language pairs (Wu,
Carpuat, and Shen 2006). This makes phrase reordering in phrasal SMT a more tractable
task.
After enhancing phrasal SMT with a hard hierarchical skeleton, we further inject
soft linguistic information into the nodes of the skeleton. We annotate each BTG node
1 In this article, we use Penn Chinese Treebank phrase labels (Xue et al 2000).
2 Chiang (2005) also generates hierarchical structures in phrasal SMT. One difference is that Chiang?s
hierarchical grammar is lexicon-sensitive because the model requires at least one pair of aligned words in
each rule except for the ?glue rule.? The other difference is that his grammar allows multiple
nonterminals. These two differences make Chiang?s grammar more expressive than the BTG but at the
cost of learning a larger model.
536
Xiong et al Linguistically Annotated Reordering
with syntactic and lexical elements by projecting the source parse tree onto the BTG
binary tree. The challenge, of course, is that BTG hierarchical structures are not always
aligned with the linguistic structures in the source language parse tree. To address this
issue, we propose an annotation algorithm. The algorithm is able to label any BTG
nodes during decoding with very little overhead, regardless of whether the BTG nodes
are aligned with syntactic constituent nodes in the source parse tree. The annotated
linguistic elements are then used to guide phrase reordering under the ITG constraint.
We call this two-step phrase reordering strategy linguistically annotated reorder-
ing (LAR) (Xiong et al 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reorder-
ing strategy based on BTG. However, they use boundary words as reordering features
at the second step. To distinguish this from our work, we call their approach boundary
word?based reordering (BWR). LAR and BWR can be considered as two reordering
variants for BTG-based phrasal SMT, which have similar training procedures. Further-
more, they can be combined.
We evaluate LAR vs. BWR using the automatic metric BLEU (Papineni et al 2002).
The BLEU scores show that LAR is comparable to BWR and significantly improves
phrase reordering when combined with BWR.
We want to further study what happens when we combine BWR with LAR. In
particular, we want to investigate to what extent the integrated linguistic knowledge
(from LAR) changes phrase movement in an actual SMT system, and in what direction
the change takes place. The investigations will enable us to have a better understanding
of the relationship between phrase movement and linguistic context, and therefore to
explore linguistic knowledge more effectively in phrasal SMT.
Because syntactic constituents are often moved together across languages during
translation (Fox 2002), we particularly study how linguistic knowledge affects syntactic
constituent movement. To that end, we introduce a syntax-based analysis method. We
parse source sentences, and align the parse trees with reference translations as well as
system translations. We then summarize syntactic reordering patterns using context-
free grammar (CFG) rules from the obtained tree-to-string alignments. The extracted
reordering patterns clearly show the trace of syntactic constituent movement in both
reference translations and system translations.
With the proposed analysis method, we analyze the combination of BWR and LAR
vs. BWR alone. There are essentially three issues that are addressed in this syntax-based
comparative analysis.
1. The first issue concerns syntactic constituent movement in human/
machine translations. Fox (2002) investigates syntactic constituent
movement in human translations. We study syntactic constituent
movement in both human translations and machine translations that are
generated by an actual SMT system and compare them.
2. The second issue concerns the change of phrase movement after rich
linguistic knowledge is integrated into phrase reordering. To gain a better
insight into this issue, we study phrase movement patterns for 13 specific
syntactic constituents.
3. The last issue concerns which constituents remain difficult to reorder even
though rich linguistic knowledge is employed.
The rest of the article is structured as follows. Section 2 introduces background
information about BTG-based phrasal SMT and phrase reordering under the ITG
537
Computational Linguistics Volume 36, Number 3
constraint. Section 3 describes algorithms which extract training instances for reorder-
ing models of BWR and LAR. Section 4 introduces the BWR model as our baseline
reordering model. Section 5 describes LAR and the combination of LAR with BWR.
Section 6 elaborates the syntax-based analysis method. Section 7 reports our evaluation
results on large-scale data. Section 8 demonstrates our analysis results and addresses the
various issues discussed above. Section 9 discusses related work. And finally, Section 10
summarizes our main conclusions.
2. Background
2.1 BTG-Based Phrasal SMT
We establish a unified framework for BTG-based phrasal SMT in this section. There
are two kinds of rules in BTG, lexical rules (denoted as rl) and merging rules (denoted
as rm):3
rl : Ap ? x/y
rm : Ap ? [Al, Ar]|?Al, Ar? (1)
A lexical rule translates a source phrase x into a target phrase y and generates a leaf
node A in the BTG tree. Merging rules combine left and right neighboring phrases Al
and Ar into a larger phrase Ap in an order o ? {straight, inverted}. In this article, we use
?[]? to denote a straight order and ???? an inverted order.
We define a BTG derivation D as a sequence of independent applications of lexical
and merging rules (D = ?rl1..nl , r
m
1..nm?). Given a source sentence, the decoding task of
BTG-based SMT is to find a best derivation, which yields the best translation.4
We assign a probability to each rule using a log-linear model with different features
and corresponding weights ?, then multiply them to obtain P(D). To keep in line with
the common understanding of standard phrasal SMT (Koehn, Och, and Marcu 2003),
here we re-organize these features into a translation model (PT), a reordering model
(PR), and a target language model (PL) as follows:
P(D) = PT(r
l
1..nl
) ? PR(rm1..nm )
?R ? PL(e)?L ? exp(|e|)?w (2)
where exp(|e|) is the word penalty.
The translation model is defined as
PT(r
l
1..nl
) =
nl
?
i=1
P(rli)
P(rl) = p(x|y)?1 ? p(y|x)?2 ? plex(x|y)?3 ? plex(y|x)?4 ? exp(1)?5 (3)
where p(?) represents the phrase translation probabilities in both directions, plex(?) de-
notes the lexical translation probabilities in both directions, and exp(1) is the phrase
penalty.
3 The subscripts l, r, p of A do not mean that we categorize A into three different nonterminals. We use them
to represent the left node, right node, and parent node.
4 In this article, we use c to denote a source sentence and e a target sentence.
538
Xiong et al Linguistically Annotated Reordering
Similarly, the reordering model is defined on the merging rules as follows:
PR(r
m
1..nm ) =
nm
?
i=1
P(rmi ) (4)
One of the most important and challenging tasks in building a BTG-based phrasal SMT
system is to define P(rm).
2.2 Reordering Under the ITG Constraint
Under the ITG constraint, three nodes {Al, Ar, Ap} are involved when we consider the
order o between the two children {Al, Ar} in any binary subtrees. Therefore it is natural
to define the ITG reordering P(rm) as a function as follows:
P(rm) = f (Al, Ar, Ap, o) (5)
where o ? {straight, inverted}.
Based on this function, various reordering models are built according to different
assumptions. For example, the flat reordering model in the original BTG (Wu 1996)
assigns prior probabilities for the straight and inverted order assuming the order is
highly related to the properties of language pairs. It is formulated as
P(rm) =
{
ps, o = straight
1 ? ps, o = inverted
(6)
Supposing French and English are the source and target language, respectively, the
value of ps can be set as high as 0.8 to prefer monotone orientations because the two
languages have similar word orders in most cases.
The main problem of the flat reordering model is also the problem of the standard
distortion model (Koehn, Och, and Marcu 2003): Neither model considers linguistic
contexts. To be context-dependent, the ITG reordering might directly model the condi-
tional probability P(o|Al, Ar). This probability could be calculated using the maximum
likelihood estimate (MLE) by taking counts from training data, in the manner of the
lexicalized reordering model (Tillman 2004; Koehn et al 2005):
P(o|Al, Ar) =
Count(o, Al, Ar)
Count(Al, Ar)
(7)
Unfortunately this lexicalized reordering method usually leads to a serious data sparse-
ness problem under the ITG constraint because Al and Ar become larger and larger due
to the merging rules, and are finally unseen in the training data.
To avoid the data sparseness problem yet be contextually informative, attributes
of Al and Ar, instead of nodes themselves, are used as reordering evidence in a new
perspective of the ITG reordering (Xiong, Liu, and Lin 2006). The new perspective treats
the ITG reordering as a binary-classification problem where the possible order straight
or inverted between two children nodes is the target class which the reordering model
predicts given Al, Ar, and Ap.
539
Computational Linguistics Volume 36, Number 3
3. Reordering Example Extraction
Because we consider the ITG reordering as a classification problem, we need to obtain
training instances to build a classifier. Here we refer to a training instance as a reorder-
ing example, which is formally defined as a triple of (o, bl, br) where bl and br are two
neighboring blocks and o ? {straight, inverted} is the order between them.
The block is a pair of aligned source phrase and target phrase
b = (ci2i1 , e
j2
j1
) (8)
where b must be consistent with the word alignment M
?(i, j) ? M, i1 ? i ? i2 ? j1 ? j ? j2 (9)
By this, we require that no words inside the source phrase ci2i1 are aligned to words
outside the target phrase ej2j1 and that no words outside the source phrase are aligned to
words inside the target phrase. This definition is similar to that of the bilingual phrase
except that there is no length limitation over blocks. Figure 1 shows a word alignment
matrix between a Chinese sentence and English sentence. In the matrix, each block can
be represented as a rectangle, for example, blocks (c44, e
4
4), (c
5
4, e
5
4), (c
7
4, e
9
4) in red rectangles,
and (c32, e
3
3), (c
3
1, e
3
1) in blue rectangles.
In this section, we discuss two algorithms for extracting reordering examples from
word-aligned bilingual data. The first algorithm AExtractor (described in Section 3.1)
extracts reordering examples directly from word alignments by extending the bilin-
gual phrase extraction algorithm. The second algorithm TExtractor (described in Sec-
tion 3.2) extracts reordering examples from BTG-style trees which are built from word
alignments.
Figure 1
A word alignment matrix between a Chinese sentence and English sentence. Bold dots represent
junctions which connect two neighboring blocks. Red and blue rectangles are blocks which are
connected by junction J2.
540
Xiong et al Linguistically Annotated Reordering
3.1 AExtractor: Extracting Reordering Examples from Word Alignments
Before we describe this algorithm, we introduce the concept of junction in the word
alignment matrix. We define a junction as a vertex shared by two neighboring blocks.
There are two types of junctions: a straight junction, which connects two neighboring
blocks in a straight order (e.g., black dots J1 ? J4 in Figure 1) and an inverted junction,
which connects two neighboring blocks in an inverted order (e.g., the red dot J5 in
Figure 1).
The algorithm for AExtractor is shown in Figure 2. This completes three sub-tasks
as follows.
1. Find blocks (lines 4 and 5). This is similar to the standard phrase extraction
algorithm (Och 2002) except that we find blocks with arbitrary length.
2. Detect junctions and store blocks in the arrays of detected junctions (lines 7
and 8). Junctions that are included the current block can be easily detected
by looking at the previous and next blocks. A junction can connect
multiple blocks on its left and right sides. For example, the second
junction J2 in Figure 1 connects two blocks on the left side and three blocks
on the right side. To store these blocks, we maintain two arrays (left and
right) for each junction.
3. Select block pairs from each detected junction as reordering examples
(lines 12?16). This is the most challenging task for AExtractor. Because a
junction may have n blocks on its left side and m blocks on its right side,
we will obtain nm reordering examples if we enumerate all block pairs.
This will quickly increase the number of reordering examples, especially
Figure 2
AExtractor.
541
Computational Linguistics Volume 36, Number 3
those with the straight order. To keep the number of reordering examples
tractable, we define various selection rules r to heuristically select special
block pairs as reordering examples.
We define four selection rules as follows.
1. strINV: We select the smallest blocks (in terms of the target length) for
straight junctions, and the largest blocks for inverted junctions. Take the
straight junction J2 in Figure 1 as an example, the extracted reordering
example is (straight, | five,| flights).
2. STRinv: We select the largest blocks (in terms of the target length) for
straight junctions, and the smallest blocks for inverted junctions. Still
taking the straight junction J2 as an example, this time the extracted
reordering example is (straight, | The last five,|
flights all fail due to accidents).
3. RANDOM: For any junction, we randomly select one block pair from its
arrays.
4. COMBO: For each junction, we first select two block pairs using selection
rule strINV and STRinv. If there are unselected blocks, we randomly select
one block pair from the remaining blocks.
3.2 TExtractor: Extracting Reordering Examples from BTG-Style Trees
A potential problem for AExtractor is caused by the use of heuristic selection rules:
keeping some block pairs as reordering examples while abandoning other block pairs.
The kept block pairs are not necessarily the best training instances for tuning an ITG
order predictor. To avoid this problem we can extract reordering examples from the
BTG trees of sentence pairs. Reordering examples extracted in this way are naturally
suitable for BTG order prediction.
There are various ways to build BTG trees over sentence pairs. One can use BTG to
produce bilingual parses of sentence pairs, similar to the approaches proposed by Wu
(1997) and Zhang and Gildea (2005) but using the more sophisticated reordering models
BWR or LAR. After parsing, reordering examples can be extracted from bilingual parse
trees and a better reordering model is therefore induced from the extracted reordering
examples. Using the better reordering model, the bilingual sentences are parsed again.
This procedure is run iteratively until no performance gain is obtained in terms of
translation or parsing accuracy. Formally, we can use expectation-maximization (EM)
training in this procedure. In the expectation step, we first estimate the likelihood of all
BTG trees of sentence pairs with the current BTG model. Then we extract reordering
examples and collect counts for them, weighted with the probability of the BTG tree
where they occur. In the maximization step, we can train a more accurate reordering
model with updated reordering examples. Unfortunately, this method is at high com-
putational cost.
Instead, here we adopt a less expensive alternative method to produce BTG trees
over sentence pairs. Supposing we have word alignments produced by GIZA++, we
then use the shift-reduce algorithm (SRA) introduced by Zhang, Gildea, and Chiang
(2008) to decompose word alignments into hierarchical trees. The SRA can guarantee
that each node is a bilingual phrase in the decomposition tree. If the fan-out of a node
is larger than two, we binarize it from left to right: for two neighboring child nodes, if
542
Xiong et al Linguistically Annotated Reordering
they are also neighboring on both the source and target sides, we combine them and
create a new node to dominate them. In this way, we can transform the decomposition
tree into a BTG-style tree. Note that not all multi-branching nodes can be binarized. We
extract reordering examples only from binary nodes.
Figure 3 shows the BTG-style tree which is built from the word alignment in Figure 1
according to the method mentioned here. From this tree, we can easily extract four re-
ordering examples in a straight order and one reordering example in an inverted order.
4. Boundary Word-Based Reordering
Following the binary-classification perspective of the ITG reordering, Xiong, Liu, and
Lin (2006) propose a reordering model which exploits the maximum entropy (MaxEnt)
classifier for BTG order prediction
PRb (r
m) = P?(o|Al, Ar, Ap) =
exp(
?
i ?ihi(o, Al, Ar, Ap))
?
o? exp(
?
i ?ihi(o
?, Al, Ar, Ap))
(10)
where the functions hi ? {0, 1} are reordering features and the ?i are the weights of these
features.
Xiong, Liu, and Lin (2006) define reordering features using the boundary words of
the source/target sides of both children {Al, Ar}. Supposing that we have a reordering
example (inverted, 7 15 | on July 15, | held its presidential and
parliament elections), leftmost/rightmost source words {, 15,,} and target
words {on, 15, held, elections} will be extracted as boundary words. Each boundary word
will form a reordering feature as follows
hi(o, Al, Ar, Ap) =
{
1, fn = bval, o = inverted
0, otherwise
where fn denotes the feature name, and bval is the corresponding boundary word.
There are two reasons why boundary words are used as important clues for
reordering:
1. Phrases frequently cohere across languages (Fox 2002). In cohesive phrase
movement, boundary words directly interact with the external contexts of
Figure 3
The BTG-style tree built from the word alignment in Figure 1. We use ([i, j], [p, q]) to denote a tree
node, where i, j and p, q are the beginning and ending indices in the source and target language,
respectively.
543
Computational Linguistics Volume 36, Number 3
phrases. This suggests that boundary words might contain information for
phrase reordering.
2. The quantitative analysis in Xiong, Liu, and Lin (2006, page 525) further
shows that boundary words indeed contain information for order
prediction.
To train a BWR model, we follow three steps. First, we extract reordering examples
from word-aligned bilingual data as described in the last section, then generate reorder-
ing features using boundary words from the reordering examples, and finally estimate
feature weights.
5. Linguistically Annotated Reordering
In order to employ more linguistic knowledge in the ITG reordering, we annotate each
BTG node involved in reordering using linguistic elements from the source-side parse
trees. The linguistic elements include: (1) the head word hw, (2) the part-of-speech (POS)
tag ht of the head word, and (3) its syntactic category sc. In this section, we describe the
annotation algorithm and the LAR model, as well as the combination of LAR and BWR.
5.1 Annotation Algorithm
There are two steps to annotating a BTG node using source-side parse tree information:
(1) determining the sequence on the source side which is exactly covered by the node,
then (2) annotating the sequence according to the source-side parse tree. If the sequence
is exactly covered by a single subtree in the source-side parse tree, it is called a syntactic
sequence, otherwise it is a non-syntactic sequence. One of the challenges in this an-
notation is that phrases (BTG nodes) do not always cover syntactic sequences; in other
words, they are not always aligned to constituent nodes in the source-side tree. To solve
this problem, we generate a pseudo head word and composite category which consists
of the syntactic categories of three relevant constituents for the non-syntactic sequences.
In this way, our annotation is capable of labelling both syntactic and non-syntactic
phrases and therefore providing linguistic information for any phrase reordering.
The annotation algorithm is shown in Figure 4. For a syntactic sequence, the an-
notation is trivial. Annotation elements directly come from the subtree that covers the
sequence exactly. For a non-syntactic sequence, the process is more complicated. Firstly,
we need to locate the smallest subtree c? covering the sequence (line 6). Secondly, we
try to identify the head word/tag of the sequence (lines 7?12) by using its head word
directly if it is within the sequence. Otherwise, the word within the sequence which is
nearest to hw will be assigned as the head word of the sequence. Finally, we determine
the composite category of the sequence (lines 13?15), which is formulated as L-C-R.
L/R refers to the syntactic category of the left/right boundary node of s, which is the
highest leftmost/rightmost sub-node of c? not overlapping the sequence. If there is no
such boundary node (the sequence s is exactly aligned to the left/right boundary of c?),
L/R will be set to NULL. C is the syntactic category of c?. L, R, and C together describe
the external syntactic context of s. The composite category we define for non-syntactic
phrases is similar to the CCG-style category in Zollmann, Venugopal, and Vogel (2008).
Figure 5 shows a syntactic parse tree for a Chinese sentence, with the head word
annotated for each internal node. Some sample annotations are given in Table 1.
544
Xiong et al Linguistically Annotated Reordering
Figure 4
The Annotation Algorithm.
Figure 5
A syntactic parse tree with the head word annotated for each internal node. The superscripts on
leaf nodes denote their surface positions from left to right.
5.2 Reordering Model
The linguistically annotated reordering model PRa is a MaxEnt-based classification
model, which can be formulated as
PRa (r
m) = p?(o|A
ap
p , A
al
l , A
ar
r ) =
exp(
?
i ?ihi(o, A
ap
p , A
al
l , A
ar
r ))
?
o? exp(
?
i ?ihi(o
?, A
ap
p , A
al
l , A
ar
r ))
(11)
where the feature functions hi ? {0, 1} are defined using annotated linguistic elements
of each BTG node. Here we use the superscripts al, ar, and ap to stress that the BTG nodes
are linguistically annotated.
545
Computational Linguistics Volume 36, Number 3
Table 1
Annotation samples according to the tree shown in Figure 5.
sequence hw ht sc
?1, 2?  NN NULL-NP-NN
?2, 3?  NN NP
?2, 4?  VV NP-IP-NP
?3, 4?  VV NP-IP-NP
hw/ht = the head word/tag, respectively; sc = syntactic category.
Each merging rule involves three nodes (A
ap
p , A
al
l , A
ar
r ) and each node has three
linguistic elements (hw, ht, sc). Therefore, the model has nine features in total. Taking
the left node Aall as an example, the model could use its head word w as a feature as
follows:
hi(o, A
ap
p , A
al
l , A
ar
r ) =
{
1, Aall .hw = w, o = straight
0, otherwise
Training an LAR model also takes three steps. Firstly, we extract annotated reorder-
ing examples from source-side parsed, word-aligned bilingual data using the reordering
example extraction algorithm and the annotation algorithm. We then generate features
using the linguistic elements of these examples. Finally we tune feature weights to build
the MaxEnt model.
5.3 Combining LAR and BWR
LAR and BWR can be combined at two different levels:
1. Feature level. Because both LAR and BWR are trained under the
maximum entropy principle, we can combine linguistically annotated
features from LAR and boundary word features from BWR together and
train a single MaxEnt model. We call this method All-in-One combination.
2. Model level. We can also train two reordering models separately and
integrate them into BTG-based SMT
P(D) = PT(rl1..nl ) ?PRb (r
m
1..nm )
?Rb ?
PRa (r
m
1..nm )
?Ra ?PL(e)?L ? exp(|e|)?w (12)
where PRb is the BWR reordering model and PRa is the LAR reordering
model. We call this combination BWR+LAR.
We will empirically compare these two combination methods in Section 7.4.
6. A New Syntax-Based Reordering Analysis Method
In order to understand the influence of linguistic knowledge on phrase reordering, we
propose a syntax-based method to analyze phrase reordering. In this analysis method,
546
Xiong et al Linguistically Annotated Reordering
we leverage the alignments between source-side parse trees and reference/system
translations to summarize syntactic reordering patterns and calculate syntax-based
measures of precision and recall for each syntactic constituent.
6.1 Overview
The alignment between a source parse tree and a target string is a collection of rela-
tionships between parse tree nodes and their corresponding target spans.5 A syntactic
reordering pattern (SRP) is defined as
?? ? ?1...?n ? [i1]...[in]?
The first part of an SRP is a CFG structure on the source side and the second part
[i1]...[in] indicates the order of target spans ?T1 ...?
T
n of nonterminals ?1...?n on the target
side.6
Let?s take the VP structure VP ? PP1VP2 as an example to explain how the pre-
cision and recall can be obtained. On the target side, the order of PPT1 and VP
T
2
might be [1][2] or [2][1]. Therefore we have two syntactic reordering patterns for this
structure:
?VP ? PP1VP2 ? [1][2]? and ?VP ? PP1VP2 ? [2][1]?
Suppose that the two reordering patterns occur a times in the alignments between
source parse trees and reference translations, b times in the alignments between source
parse trees and system translations, and c times in both alignments. Then the reordering
precision/recall for this structure is c/b and c/a, respectively. We can further calculate
the F1-score as 2 ? c/(a + b). These syntax-based metrics intuitively show how well the
reordering model can reorder this structure. By summarizing all reordering patterns of
all constituents, we can obtain an overall precision, recall, and F1-score for the tested
reordering model.
This new syntax-based analysis for reordering is motivated in part by recent work
which transforms the order of nodes in the source-side parse tree before translation
(Xia and McCord 2004; Collins, Koehn, and Kucerova 2005; Li et al 2007; Wang,
Collins, and Koehn 2007). Here we focus on the order transformation of syntactic con-
stituents performed by reordering models during translation. In addition to aligning
parse trees with reference translations, we also align parse trees with system transla-
tions so that we can learn the movement of syntactic constituents carried out by the
reordering models and investigate the performance of the reordering models by com-
paring both alignments.
For notational convenience, we denote syntactic reordering patterns that are ex-
tracted from the alignments between source parse trees and reference translations as
REF-SRP and those from the alignments between source parse trees and system trans-
lations as SYS-SRP. We refer to those present in both alignments under some conditions
5 We adopt the definition of span from Fox (2002): Given a node n that covers a word sequence sp...si...sq
and a word alignment matrix M, the target words aligned to n are {ti : ti ? M(si )}. We define the target
span of node n as nT = (min({ti}), max({ti})). Note that nT may contain words that are not in {ti}.
6 Please note that the order of structures may not be defined in some cases (see Section 6.3).
547
Computational Linguistics Volume 36, Number 3
that will be described in Section 6.4 as MATCH-SRP. To conduct a thorough analysis on
the reorderings, we carry out the following steps on the test corpus (source sentences +
reference translations):
1. Parse source sentences.
2. Generate word alignments between source sentences and reference
translations as well as word alignments between source sentences and
system translations.
3. According to the word alignments of Step 2, for each multi-branching
node ? ? ?1...?n in the source parse tree generated in Step 1, find the
target spans ?T1 ...?
T
n and their order [i1]...[in] in the reference and system
translations, respectively.
4. Generate REF-SRPs, SYS-SRPs, and MATCH-SRPs according to the target
orders generated in Step 3 for each multi-branching node.
5. Summarize all SRPs and calculate the precision and recall as described
above.
We further elaborate Steps 2?4 in the Sections 6.2?6.4.
6.2 Generating Word Alignments
To obtain word alignments between source sentences and multiple reference transla-
tions, we pair the source sentences with each of the reference translations and include
the created sentence pairs in our bilingual training corpus. Then we run GIZA++ on the
new corpus in both directions, and apply the ?grow-diag-final? refinement rule (Koehn
et al 2005) to produce the final word alignments.
To obtain word alignments between source sentences and system translations, we
store the word alignments within each phrase pair in our phrase table. When we output
the system translation for a source sentence, we trace back the original source phrase
for each target phrase in the system translation. This will generate a phrase alignment
between the source sentence and system translation. Given the phrase alignment and
word alignments within the phrase stored in the phrase table, we can easily obtain word
alignments between the whole source sentence and system translation.
6.3 Generating Target Spans and Orders
Given the source parse tree and the word alignment between a source sentence and
a reference/system translation, for each multi-branching node ? ? ?1...?n, we firstly
determine the target span ?Ti for each child node ?i following Fox (2002). If one child
node is aligned to NULL, we define a special target span for it. The order for this special
target span will remain the same as the child node occurring in ?1...?n.
Two target spans may overlap with each other because of inherent divergences
between two languages or noise in the word alignment. When this happens on two
neighboring nodes ?i and ?i+1, we combine these two nodes together and redefine a
target span ?Ti&i+1 for the combined node. This process will be repeated until no more
neighboring nodes can be combined. For example, the target span of nodes a and b in
548
Xiong et al Linguistically Annotated Reordering
Figure 6
An example source parse tree with the word alignment between the source sentence and the
target translation. Dotted lines show the word alignment.
Figure 6 overlap ((1, 3) vs. (2, 2)). Therefore these two nodes are to be combined into a
new node, whose target span is (1, 3).
After performing all necessary node combinations, if there are no more overlaps, we
call the multi-branching node reorderable, otherwise non-reorderable. To get a clearer
picture of the reorderable nodes, we divided them into two categories:
 fully reorderable if all target spans of child nodes don?t overlap;
 partially reorderable if some child nodes are combined due to
overlapping.
In Figure 6, both nodes a and c are fully reorderable nodes.7 Node d is a partially
reorderable node. Node g is a non-reorderable node because (1) the target spans of its
child nodes d and f overlap, and (2) child nodes d and f cannot be combined because
they are not neighbors.
Because we have multiple reference translations for each source sentence, we can
define multiple orders for {?Ti }n1. If one node is non-reorderable in all reference trans-
lations, we call it REF-non-reorderable, otherwise REF-reorderable. To specify the
reorderable attribute of a node in the system translation, we prefix ?SYS-? to {non-
reorderable, reorderable, fully-reorderable, partially-reorderable}.
6.4 Generating SRPs
After we obtain the orders of the child nodes for each multi-branching node, we gener-
ate REF-SRPs and SYS-SRPs from the fully/partially reorderable nodes. We obtain the
7 Their target translations are interrupted by the other node?s translation. We will discuss this situation in
Section 8.5.
549
Computational Linguistics Volume 36, Number 3
MATCH-SRP for each multi-branching node by comparing the obtained SYS-SRP with
the REF-SRPs for this node under the following conditions:
1. Because we have multiple reference translations, we may have different
REF-SRPs. We compare the SYS-SRP with the REF-SRP where the
reference translation for this node (the sequence within the target span of
the node defined by the REF-SRP) has the shortest Levenshtein distance
(Navarro 2001) to that of the system translation.
2. If there are combined nodes in SYS/REF-SRPs, they are treated as a unit
when comparing, without considering the order within each combined
node. If the order of the SYS-SRP and the selected REF-SRP matches, we
have one MATCH-SRP for the node.
Let?s give an example to explain these conditions. Suppose that we are processing
the structure VP ? PP1ADVP2VP3. We obtain four REF-SRPs from four different ref-
erence translations and one SYS-SRP from the system output. Here we only show the
orders:
Ref.a : [3][1][2]
Ref.b : [3][2][1]
Ref.c&d : [2][3][1]
SYS : [3][1&2]
References c and d have the same order. Therefore we have three different REF-SRPs
for this structure. In the SYS-SRP, PP1 and ADVP2 are combined and moved to the right
side of VP3. Supposing that the system translation for this structure has the shortest edit
distance to that of Reference b, we use the order of Reference b to compare the system
order. In the Reference b order, both PP1 and ADVP2 are also moved to the right side of
VP3. Therefore the two orders of Reference b and SYS match. We have one matched SRP
for this structure.
7. Evaluation
Our system is a BTG-based phrasal SMT system, developed following Section 2. We
integrate the boundary word?based reordering model and the linguistically annotated
reordering model into our system according to our reordering configuration. We car-
ried out various experiments to evaluate the reordering example extraction algorithms
of Section 3, the linguistically annotated reordering model vs. boundary word?based
reordering model, and the effects of linguistically annotated features on the Chinese-to-
English translation task of the NIST MT-05 using large scale training data.
7.1 Experimental Setup
We ran GIZA++ (Och and Ney 2000) on the parallel corpora (consisting of 101.93M
Chinese words and 112.78M English words) listed in Table 2 in both directions and
then applied the ?grow-diag-final? refinement rule (Koehn, Och, and Marcu 2003) to
550
Xiong et al Linguistically Annotated Reordering
Table 2
Corpora used.
Corpus LDC catalog Chinese words English words
United Nations LDC2004E12 68.63M 76.99M
Hong Kong News LDC2004T08 15.07M 15.89M
Sinorama Magazine LDC2005T10 10.26M 9.64M
FBIS LDC2003E14 7.09M 9.28M
Xinhua LDC2002E18 0.40M 0.43M
Chinese News Translation LDC2005T06 0.28M 0.31M
Chinese Treebank LDC2003E07 0.10M 0.13M
Multiple Translation Chinese LDC2004T07 0.10M 0.11M
Total ?? 101.93M 112.78M
obtain many-to-many word alignments. From the word-aligned corpora, we extracted
bilingual phrases.
We used all corpora listed in Table 2 except for the United Nations corpus to train
our reordering models, which consist of 33.3M Chinese words and 35.79M English
words. We ran the reordering example extractor AExtractor and TExtractor of Section 3
on the chosen word-aligned corpora. We then extracted boundary word features from
the reordering examples. To extract linguistically annotated features, we parsed the
Chinese side of the chosen parallel text using a Chinese parser (Xiong, Liu, and Lin
2005) which was trained on the Penn Chinese Treebank with an F1-score of 79.4%. We
ran the off-the-shelf MaxEnt toolkit8 to tune the reordering feature weights with the
iteration number set to 100 and Gaussian prior to 1 to avoid overfitting.
We built our 4-gram language model using the SRILM toolkit (Stolcke 2002), which
was trained on the Xinhua section of the English Gigaword corpus (181.1M words).
We selected 580 short sentences (not exceeding 50 characters per sentence) from the
NIST MT-02 evaluation test data as our development set (18 words/31 characters per
sentence). The NIST MT-05 test set includes 1,082 sentences with an average of 27.4
words/47.6 characters per sentence. The reference corpus for the NIST MT-05 test set
contains four translations per source sentence. Both the development and test sets were
also parsed using the parser mentioned above.
Our evaluation metric is the case-insensitive BLEU-4 (Papineni et al 2002) using the
shortest reference sentence length for the brevity penalty. The model feature weights are
tuned on the development set to maximize BLEU using MERT (Och 2003). Statistical
significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn
2004).
7.2 Bias in AExtractor
As described in Section 3, AExtractor selectively extracts reordering examples. This
selective extraction raises three questions:
1. Is it necessary to extract all reordering examples?
8 Available at: http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
551
Computational Linguistics Volume 36, Number 3
2. If it is not necessary, do the heuristic selection rules impose any bias on
the reordering model? For example, if we use the strINV selection rule,
meaning that we always extract the largest block pairs for inverted
reordering examples, does the reordering model prefer swappings on
larger blocks to those on smaller blocks?
3. Does the bias have a strong impact on the performance in terms of BLEU
score?
The answer to the first question is no. Firstly, it is practically undesirable to extract
all reordering examples because even a very small training set will produce millions of
reordering examples if we enumerate all block pair combinations. Secondly, extracting
all reordering examples introduces a great amount of noise into training and therefore
undermines the final reordering model. In Table 3, we show the number of reorder-
ing examples extracted using different extraction algorithms and selection rules. The
AExtractor with the COMBO selection rule extracts the largest number of reordering
examples. However, it does not obtain the highest BLEU score compared with other
selection rules which extract a smaller number of reordering examples. This empirically
suggests that there is no need to extract all reordering examples.
To answer the second question, we trace the best BTG trees produced on the test set
by our system. The BWR reordering model is trained on reordering examples which are
extracted using different selection rules. Then we calculate the average number of words
on the target side which are covered by binary nodes in a straight order. We refer to this
number as straight average length. Similarly, inverted average length is calculated on
all binary nodes in an inverted order. The third and fourth columns of Table 3 show the
two average variables. Comparing these average numbers, we clearly observe that two
selection rules indeed impose noticeable bias on the reordering model.
 The strINV selection rule, which always extracts the largest block pairs for
inverted reordering examples, has the largest inverted average length.
This indicates that the strINV rule biases the reordering model towards
larger swappings.
 On the contrary, the STRinv selection rule, which extracts the largest block
pairs for straight reordering examples and smallest pairs for inverted
reordering examples, has the largest straight average length and a
Table 3
Comparison of reordering example extraction algorithms and selection rules. We only use BWR
as the reordering model for this comparison.
Ext. Alg. Reordering Straight Inverted BLEU
(Sel. rule) Examples Avg. Len. Avg. Len.
AExtractor (strINV) 10.06M 15.8 14.5 32.37
AExtractor (STRinv) 10.06M 17.3 12.8 32.47
AExtractor (RANDOM) 10.06M 14.7 11.8 32.24
AExtractor (COMBO) 23.27M 13.8 13.5 32.10
TExtractor 14.30M 15.0 14.1 29.95
552
Xiong et al Linguistically Annotated Reordering
relatively much smaller inverted average length. This suggests that the
STRinv rule makes the reordering model prefer smaller swappings.
Note that the selection rules RANDOM and COMBO do not impose bias on the length
of extracted reordering examples compared with strINV and STRinv. The latter two se-
lection rules have special preferences on the length of reordering examples and transfer
these preferences to the reordering models as shown in Table 3.
Because the preference for reordering larger/smaller blocks is imposed by the
reordering example extraction algorithm with special selection rules, one might wonder
whether we can allow the decoder to decide its own reordering preference. We add two
new features to our translation model: reordering count penalty (rc) and reordering
length penalty (rl). We accumulate rc whenever two neighboring BTG nodes are re-
ordered. And at the same time we add the number of words which are covered by these
two neighboring nodes to rl. Their weights are tuned using MERT to maximize BLEU
score on the development set with other model feature weights. These two features
are similar to the widely used word/phrase penalty features. Tuning the weights of
the word/phrase penalty features, we can allow the decoder to favor shorter or longer
phrases. Similarly, with the two new features rc and rl, we can allow the decoder to
favor shorter or longer reorderings.
We conducted experiments using reordering examples which are extracted with the
RANDOM and COMBO selection rules because these two rules do not impose bias on
the length of reordering examples. Observing the optimized weights of rc and rl on
the development set, we find that the decoder rewards larger rc but smaller rl. This
means that the decoder prefers shorter reorderings to longer reorderings. However, the
BLEU scores on the test set are 31.89 and 32.0 for RANDOM and COMBO, respectively,
which are worse than the BLEU scores of RANDOM and COMBO without using rc
and rl in Table 3, and also worse than the performance of strINV and STRinv which
impose preferences on reordering examples. This seems to suggest that the preference
for shorter/longer reorderings imposed by the reordering example extraction algorithm
is better than that decided by the decoder itself.
Finally, for the last question, we observe from Table 3 that BLEU scores are not that
much different although we have quite the opposite bias imposed by different selection
rules. The changes in BLEU score, which happen when we shift from one selection rule
to the other, are limited to a maximum of 1.2%. Among the four selection rules, the
STRinv rule achieves the highest BLEU score. The reason might be that the bias towards
smaller swappings imposed by this rule helps the decoder to reduce incorrect long-
distance swappings (Xiong et al 2008b).
7.3 AExtractor vs. TExtractor
We further compared the two algorithms for reordering example extraction. In Table 3,
we find that TExtractor significantly underperforms in comparison to AExtractor. This
is because the transformation from decomposition trees to BTG trees is not complete.
Many crossing links due to errors and noise in word alignments generated by GIZA++
make it impossible to build BTG nodes over the corresponding words. It would be better
to use alignments induced by the ITG and EM procedure described in Section 3.2 but
this has a very high cost.
Given the comparison in Table 3, we use AExtractor with the STRinv selection rule
to extract reordering examples for both BWR and LAR in all experiments described
below.
553
Computational Linguistics Volume 36, Number 3
7.4 LAR vs. BWR
Table 4 shows the results of the different integration of BWR and LAR into our systems.
Only using LAR achieves a BLEU score of 32.17, which is comparable to that of BWR.
This suggests that LAR is promising given that:
 LAR uses many fewer features than BWR does. According to our statistics,
LAR contains only 166.1k linguistically annotated features whereas BWR
has 451.4k boundary word features.
 Syntactic divergences between the source and target languages as well as
parse errors prevent the effective use of syntactic knowledge for phrase
reordering (see the in-depth analysis in Section 8.2.2).
Although BWR marginally outperforms LAR (32.47 vs. 32.17), simple boundary
word features are not adequate to move phrases to appropriate positions because
they cannot recognize syntactic contexts which are very relevant to phrase reordering.
Therefore the best way to reorder a phrase is to combine BWR and LAR so that we
can use syntactic information on the one hand and not worry too much about syntactic
divergences on the other hand.
As described in Section 5.3, we can combine BWR and LAR at two levels: the feature
level and the model level. When we combine them at the model level, we achieve an
absolute improvement of 0.83 and 1.13 BLEU points over BWR and LAR, respectively,
which are both statistically significant (p < 0.01). This shows that LAR and BWR are
complementary to each other and in particular that using linguistic knowledge can
significantly improve a very competitive lexicalized reordering model (BWR).
The other combination method All-in-One (at the feature level) also obtains signif-
icant improvements over BWR and LAR but marginally underperforms compared to
BWR+LAR. In our later experiments we use the combination method BWR+LAR.
7.5 Varying Training Data Size
To investigate how LAR improves BWR when we vary our training data size, we carried
out experiments on three different training data sets: FBIS (7.09M Chinese words, 9.28M
English words); Large1, which includes all corpora listed in Table 2 except for the United
Nations corpus (33.3M Chinese words, 35.79M English words); and Large2, which
Table 4
BLEU scores for LAR, BWR, and their combinations.
Reordering Configuration BLEU
BWR 32.47
LAR 32.17
All-in-One 33.03**++
BWR+LAR 33.30**++
** = Significantly better than BWR (p < 0.01); ++ = significantly better than LAR (p < 0.01).
554
Xiong et al Linguistically Annotated Reordering
Table 5
BLEU scores on different training data sets. Large1 refers to the corpora listed in Table 2 except
for the United Nations corpus. Large2 includes all corpora listed in Table 2.
Training Data BWR BWR+LAR Improvement
FBIS 24.97 26.52 1.55
Large1 29.96 30.78 0.82
Large2 32.47 33.30 0.83
consists of Large1 and the United Nations corpus (101.93M Chinese words, 112.78
English words). The language model remains the same for these three data sets because
it is trained on a much larger data set (181.1M words).
Table 5 shows the results. We observe that BWR+LAR is able to achieve a larger
improvement of 1.55 BLEU points over BWR on smaller training data. When we enlarge
the training data set from FBIS to Large1, both BWR and BWR+LAR improve quite a
bit. The difference between them is narrower, 0.82 BLEU points, but still significant.
When we continue to use more training data (Large2), the improvement obtained by
integrating LAR becomes stable at the 0.8 level.
7.6 Effects of Linguistically Annotated Features
We conducted further experiments to evaluate the effects of individual linguistically an-
notated features. Using the reordering configuration of BWR+LAR, we augment LAR?s
feature pool incrementally: firstly using only syntactic categories9(sc) as features (170
features in total), then constructing composite categories (cc) for non-syntactic phrases
(sc + cc) (8.6K features), and finally introducing head words and their POS tags into
the feature pool (sc + cc + hw + ht) (166.1K features). This series of experiments demon-
strates the impact and degree of contribution made by each feature for reordering.
The experimental results are presented in Table 6, from which we have the following
observations:
1. Syntactic category alone improves the performance statistically
significantly. The baseline feature set sc with only 170 features improves
the BLEU score from 32.47 to 32.87.
2. Other linguistic information, provided by the categories of boundary
nodes (cc) and head word/tag pairs (hw + ht), also improves phrase
reordering. Producing composite categories for non-syntactic BTG nodes
and integrating head word/tag pairs into LAR as reordering features are
both effective, indicating that context information complements syntactic
category for capturing reordering patterns.
9 For a non-syntactic node, we only use the single category C, without constructing the composite category
L-C-R.
555
Computational Linguistics Volume 36, Number 3
Table 6
The effect of the linguistically annotated reordering model. (sc) is the baseline feature set,
(sc + cc) and (sc + cc + hw + ht) are extended feature sets for LAR.
Reordering Configuration BLEU
BWR 32.47
BWR + LAR (sc) 32.87*
BWR + LAR (sc + cc) 33.06**
BWR + LAR (sc + cc + hw + ht) 33.30**++
* = almost significantly better than BWR (p < 0.075); ** = significantly better than BWR (p < 0.01); ++ =
significantly better than BWR + LAR (sc) (p < 0.01).
8. Analysis
We first obtain system translations of the test corpus. We generate word alignments
between source sentences and system/reference translations as described in Section 6.2.
Then we follow the analysis steps of Section 6.1 to investigate syntactic constituent
movement in the reference translations and system translations which are generated
using two different reordering configurations: BWR+LAR vs. BWR. In LAR, we use the
best reordering feature set (sc + cc + hw + ht).
8.1 Syntactic Constituent Movement: Overview
If a syntactic constituent is fully reorderable or partially reorderable, it is considered to
be movable as a unit. To denote the proportion of syntactic constituents to be moved as
a unit, we introduce two variables REF-R-rate and SYS-R-rate, which are defined as
SYS-R-rate =
count(SYS-reorderable nodes)
count(multi-branching nodes)
(13)
REF-R-rate =
count(REF-reorderable nodes)
count(multi-branching nodes)
(14)
Table 7 shows the statistics of REF/SYS-reorderable nodes on the test corpus. From
this table, we have the following observations:
1. A large number of nodes are REF-reorderable, accounting for 79.82% of all
the multi-branching nodes. This number shows that, in reference
translations, a majority of syntactic constituent movement across
Chinese?English can be performed by directly permuting constituents in a
sub-tree.
2. The R-rates of BWR and BWR+LAR are 77.46% and 81.79%, respectively.
The R-rate of BWR+LAR is obviously higher than that of BWR, which
suggests that BWR+LAR tends towards moving more syntactic
constituents together than BWR does. We will discuss this further later.
556
Xiong et al Linguistically Annotated Reordering
Table 7
Statistics of multi-branching and REF/SYS-reorderable nodes per sentence.
BWR BWR+LAR
multi-branching node 18.68
REF-reorderable node 14.91
REF-R-rate 79.82%
SYS-fully-reorderable node 13.16 14.01
SYS-partially-reorderable node 1.31 1.26
SYS-R-rate 77.46% 81.79%
8.2 Syntactic Constituent Movement among Multiple Reference Translations
8.2.1 Differences in Movement Orientation. Because each source sentence is translated by
four different human experts, we would like to analyze the differences among reference
translations, especially on the orders of constituents being translated. Table 8 shows
the overall distribution over the number of different orders for each multi-branching
constituent among the reference translations.
In most cases (75.4%), four reference translations have completely the same order
for syntactic constituents. This makes it easier for our analysis to compare the system
order with the reference order. However, there are 22% cases where two different orders
are provided, which shows the flexibility of translation. According to our study, noun
phrases taking DNP or CP modifiers, as well as DNPs and CPs themselves, are more
likely to be translated in two different orders. Table 9 shows the percentages in which
two different orders for these constituents are observed in the reference corpus.
DNP and CP are always used as pre-modifiers of noun phrases in Chinese. They
often include the particle word  (of ) at the ending position. The difference is that
DNP constructs a phrasal modifier whereas CP constructs a relative-clause modifier.
There is no fixed reordering pattern for DNP and CP and therefore for NP which takes
DNP/CP as a pre-modifier. In the DNP ? NP DEG structure, the DEG () can be
Table 8
Distribution of number of different orders by which syntactic constituents are translated in
references.
Number of different orders 1 2 3 4
Percentage 75.40 22 2.33 0.33
Table 9
Two-order translation distribution of 4 NP-related constituents.
Constituent 2-order translation percentage
NP ? DNP NP 16.93
NP ? CP NP 9.43
CP ? IP DEC 24.79
DNP ? NP DEG 34.58
557
Computational Linguistics Volume 36, Number 3
translated into ?s or of, which are both appropriate in most cases, depending on the
translator?s preference. If the former is chosen, the order of DNP and therefore the
order for NP ? DNP NP will both be straight: [1][2]. Otherwise, the two orders will be
inverted: [2][1]. Similarly, there are also different translation patterns for CP ? IP DEC
and NP ? CP NP. CP can be translated into ?that + clause? or adjective-like phrases
in English. Figure 7 shows an example where the CP constituent is translated into an
adjective-like phrase. Although the ?that + clause? must be placed behind the noun
phrase which it modifies, the order for adjective-like phrases is flexible (see Figure 7).
For those constituents with different reference orders, we compare the order of
the system translation to that of the reference translation which has the shortest edit
distance to the system translation as described herein so that we can take into account
the potential influence of different translations on the order of syntactic constituents.
8.2.2 REF-Non-Reorderable Constituents. We also study REF-R-rates for the 13 most fre-
quent constituents listed in Table 10. We find that two constituents, VP1 ? PP VP2 and
NP1 ? CP NP2, have the lowest REF-R-rates, 58.20% and 61.77%, respectively. This
means that about 40% of them are REF-non-reorderable. In order to understand the
reasons why they are non-reorderable in reference translations, we further investigate
REF-non-reorderable cases for the constituent type VP1 ? PP VP2 and roughly classify
the reasons into three categories as follows.
1. Outside interruption. The reordering of PP and VP2 is interrupted by
other constituents outside VP1. For example, the Chinese sentence [NP
/somebody ] [VP1 [PP.../when...] [VP2 [/say NP[...] ] ] ] is translated
into when..., somebody said .... Here the translation of the first NP which is
outside VP1 is inserted between the translations of PP and VP2 and
therefore interrupts their reordering. Outside interruption accounts for
21.65% of REF-non-reorderable cases.
2. Inside interruption. The reordering of PP and VP2 is interrupted by the
combination of PP?s subnodes with VP2?s subnodes. Inside interruption
accounts for 48.45% of REF-non-reorderable cases, suggesting that it is the
major factor which decreases the reorderability of VP ? PP VP. Because
both PP and VP have their own complex sub-structures, the inside
Figure 7
An example of the translation of NP ? CP NP. This constituent can be translated in two
different orders: 1) the recently adopted statistical method (straight order); 2) the statistical
method recently adopted (inverted order).
558
Xiong et al Linguistically Annotated Reordering
Table 10
F1-scores ( BWR+LAR vs. BWR) for the 13 most frequent constituents in the test corpus.
Constituents indicated in bold have relatively lower F1 score for reordering.
Type Constituent Percent. (%) SYS-R-rate (%) F1-score (%)
BWR BWR+LAR BWR BWR+LAR
VP
VP ? VV NP 8.12 79.22 84.10 76.97 80.53
VP ? ADVP VP 4.30 63.45 65.86 70.83 73.67
VP ? PP VP 1.87 60.32 70.37 39.29 40.33
VP ? VV IP 1.82 79.35 86.14 77.16 82.26
NP
NP ? NN NN 6.88 84.68 85.18 76.17 79.10
NP ? NP NP 5.12 82.13 84.93 69.25 72.17
NP ? DNP NP 2.14 69.75 74.83 56.68 56.61
NP ? CP NP 2.12 59.67 73.43 48.75 54.48
Misc.
IP ? NP VP 6.78 71.99 79.80 63.22 65.79
PP ? P NP 3.63 80.63 85.95 82.75 84.93
CP ? IP DEC 3.51 83.94 87.89 69.91 72.24
QP ? CD CLP 2.74 66 65 67.52 68.47
DNP ? NP DEG 2.43 85.98 89.84 67.5 68.75
interruption is very complicated and includes a variety of cases, some of
which are quite unexpected. Here we show two frequent examples of
inside interruption:
a. The preposition in the PP and the verb word/phrase of VP2 are
aligned to only one target word or one continuous phrase. For
example,.../pressure,.../be confident of,.../
suffer from, and so on. This is caused by the lexical divergence
problem.
b. The PP is first combined with the verb word of VP2 in an inverted
order, then combined with the remainder of VP2 in a straight order.
For example, [PP [P] [omission1]] [VP [VV	] [omission2]]
might be translated into learned from omission1 that omission2.
3. Parse error. This accounts for 29.90% of REF-non-reorderable cases.
Although these reasons are summarized from our analysis on the constituent type
VP ? PP VP, they can be used to explain other REF-non-reorderable constituents, such
as NP ? CP NP.
8.3 Syntactic Constituent Movement in System Translations
8.3.1 Overall Reordering Precision and Recall of Syntactic Constituents. By summarizing all
syntactic reordering patterns (REF-SRP, SYS-SRP, and Match-SRP) for all constituents,
we can calculate the overall reordering precision and recall of syntactic constituents.
Table 11 shows the results for both BWR+LAR and BWR, where BWR+LAR clearly
outperforms BWR.
559
Computational Linguistics Volume 36, Number 3
Table 11
Syntactic reordering precision and recall of BWR+LAR vs. BWR on the test corpus.
Precision Recall F1
BWR 70.89 68.79 69.83
BWR+LAR 71.32 73.08 72.19
8.3.2 The Effect of Linguistic Knowledge on Phrase Movement. To understand the change
in phrase movement caused by linguistic knowledge, we further investigate how well
BWR and BWR+LAR reorder certain constituents, especially those with high distribu-
tion probability. Table 10 lists the 13 most frequent constituents, which jointly account
for 51.46% of all multi-branching constituents. Except for NP ? DNP NP, the reorder-
ing F1 score of all these constituents in BWR+LAR is better than that in BWR.
Our hypothesis for the phrase movement change in BWR+LAR is that the integrated
linguistic knowledge makes phrase movement in BWR+LAR pay more respect to syn-
tactic constituent boundaries. The overall R-rates of BWR+LAR vs. BWR described in
Section 8.1 indicate that BWR+LAR tends towards moving more syntactic constituents
together than BWR does. We want to know whether this is also true for a specific
constituent type. The fourth and fifth columns in Table 10 present the R-rate for each
individual constituent type that we have analyzed. It is obvious that the R-rate of
BWR+LAR is much higher than that of BWR for almost all constituents. This indicates
that higher R-rate is one of the reasons for the higher performance of BWR+LAR.
To gain a more concrete understanding of this change, we show two examples for
the reordering of VP ? PP VP in Figure 8. In both examples, BWR fails to move the PP
constituent to the right of the VP constituent, whereas BWR+LAR does it successfully.
By tracing the binary BTG trees generated by the decoder, we find that BWR generated
a very different BTG tree from the source parse tree whereas the BTG tree in BWR+LAR
almost matches the source parse tree. In the first example, BWR combines the VP phrase
Figure 8
Two examples for the translation of VP ? PP VP. Square brackets indicate combinations in a
straight order and angular brackets represent combinations in an inverted order.
560
Xiong et al Linguistically Annotated Reordering

 with  and then combines . The preposition word  is combined with
the NP phrase NHK, which makes the translation of NHK interrupt the reordering of
VP ? PP VP in this example. The BWR tree in the second example is even worse. The
non-syntactic phrase   in the VP phrase is first combined with  ,
which is a sub-phrase of PP preceding VP in an inverted order. The remaining part of
the VP phrase is then merged. This merging process continues regardless of the source
parse tree. The comparison of BTG trees of BWR+LAR and BWR in the two examples
suggests that reordering models should respect syntactic structures in order to capture
reorderings under these structures.
Our observation on phrase movement change resonates with the recent efforts in
phrasal SMT that allow the decoder to prefer translations which show more respect
for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto,
Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other
words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early
syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has
receded in more powerful syntax-based models (Galley et al 2004; Chiang 2005) and
non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008)
and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses
which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose
this as a hard constraint on the ITG constraint to allow reorderings which respect the
source parse tree. They all report significant improvements on different language pairs,
which indicates that syntactic cohesion is very useful for phrasal SMT. Our analysis
demonstrates that linguistically annotated reordering provides an alternative way to
incorporate syntactic cohesion into phrasal SMT.
8.4 Challenges in Phrase Reordering and Suggestions
We highlight three constituent types in Table 10 (indicated in bold) which are much
more difficult to reorder, as indicated by their relatively lower F1 scores. The lower F1
scores indicate that BWR+LAR is not fully sufficient for reordering these constituents
although it performs much better than BWR. We find two main reasons for the lower F1
scores and provide suggestions accordingly as follows.
1. Constrained decoding. We observe that in reorderable constituents which
involve long-distance reorderings, their boundaries are easily violated by
phrases outside them. To prohibit boundary violations, we propose
constrained decoding. In constrained decoding, we define special zones
in source sentences. Reorderings and translations within the zones cannot
be interrupted by fragments outside the zones. We can also define other
constrained operations on the zones. For example, we can prohibit
swappings in any zones which contain punctuation (Xiong et al 2008b).
The beginning and ending positions of a zone are automatically learned.
To be more flexible, they are not necessarily constituent boundaries.
Constrained decoding is different from both soft constraints (Cherry 2008;
Marton and Resnik 2008) and hard constraints (Yamamoto, Okuma, and
Sumita 2008). It can be considered as in between both of these because it is
harder than the former but softer than the latter.
2. Integrating special reordering rules. Some constituents are indeed
non-reorderable as we discussed in Section 8.2.2. Inside or outside
561
Computational Linguistics Volume 36, Number 3
interruptions have to be allowed to obtain fluent translations for these
constituents. However, the allowance of interruptions is sometimes
beyond the representability of BTG rules. For example, to solve the lexical
divergence problem, bilingual rules with aligned lexicons have to be
introduced. To capture reorderings of these constituents, we propose to
integrate special reordering rules with richer contextual information into
BTG to extend BTG?s ability to deal with interruptions. Completely
replacing BTG with richer formalisms, such as hierarchical phrase
(Chiang 2005) and tree-to-string (Liu, Liu, and Lin 2006) or string-to-tree
(Marcu et al 2006), introduces a huge extra cost. Instead, integrating a
small number of reordering rules into BTG to model reorderings of
non-reorderable constituents would be more desirable.
8.5 Discussion
In the definition of syntactic reordering patterns, we only consider the relative order
of individual constituents on the target side. We do not consider whether or not they
remain contiguous on the target side. It is possible that other words are inserted be-
tween spans of two contiguous constituents. We use the term gap to refer to when
this happens. The absence of a gap in the definition of syntactic reordering patterns
may produce more matched SRPs and therefore lead to higher precision and recall.
Table 12 shows the revised overall precision and recall of syntactic reordering patterns
when we also compare gaps. The revised results show that BWR+LAR still significantly
outperforms BWR. This also applies to the 13 constituents identified in Table 10. The
analysis results obtained before are still valid when we consider gaps.
9. Related Work
9.1 Linguistically Motivated Phrase Reordering
There are various approaches which are devoted to incorporating linguistic knowledge
into phrase reordering. Generally, these approaches can be roughly divided into three
categories: (1) reordering the source language in a preprocessing step before decoding
begins; (2) estimating phrase movement with reordering models; and (3) capturing
reorderings by synchronous grammars. The preprocessing approach applies manual or
automatically extracted reordering knowledge from linguistic structures to transform
the source language sentence into a word order that is closer to the target sentence.
The second reordering approach moves phrases under certain reordering constraints
and estimates the probabilities of movement with linguistic information. In the third
Table 12
Revised overall precision and recall of BWR+LAR vs. BWR on the test corpus when we consider
the gap in syntactic reordering patterns.
Precision Recall F1
BWR (gap) 46.28 44.91 45.58
BWR+LAR (gap) 48.80 50 49.39
562
Xiong et al Linguistically Annotated Reordering
approach, reordering knowledge is included in synchronous rules. The last two cate-
gories reorder the source sentence during decoding, which distinguishes them from the
first approach. Note that some researchers integrate multiple reordering approaches in
one decoder (Lin 2004; Quirk, Menezes, and Cherry 2005; Ge, Ittycheriah, and Papineni
2008).
9.1.1 The Preprocessing Approach. In early work, Brown et al (1992) describe an approach
to reordering French phrases in a preprocessing step. Xia and McCord (2004) present a
preprocessing approach which automatically learns reordering patterns based on CFG
productions. Since then, the preprocessing approach seems to have been more popular.
Collins, Koehn, and Kucerova (2005) propose reordering German clauses with six types
of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees
using fine-grained human-written rules, mostly concentrating on VP and NP structures.
Li et al (2007) improve the preprocessing approach by generating n-best reordered
source sentences with reordering knowledge automatically learned from the alignments
between source parse trees and target translations. The approach proposed in Li et al
also enhances the connection between the preprocessing and decoding by adding a
source reordering probability feature. Other approaches introduced in Nie?n and Ney
(2001), Popovic? and Ney (2006), and Zhang, Zens, and Ney (2007) use morphological,
POS, and chunk knowledge in the preprocessing approach, respectively.
9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM con-
straint (Zens and Ney 2003), the early work uses a distortion-based reordering model
to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG
constraint, the corresponding model is the flat model which assigns a prior probability
to the straight or inverted order (Wu 1996). These two models don?t respect the content
of phrases which are moved. To address this issue, lexicalized reordering models which
are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn
et al 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and
Lin (2006) introduce a more flexible reordering model under the ITG constraint using
discriminative features which are automatically learned from a training corpus. Zhang
et al (2007) propose a model for syntactic phrase reordering which uses syntactic
knowledge from source parse trees. Our reordering approach is most similar to those in
Xiong, Liu, and Lin (2006) and Zhang et al but extends them further by using syntactic
knowledge and allowing non-syntactic phrase reordering.
9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use
synchronous grammars to capture reorderings between two languages. Chiang (2005)
introduces formal synchronous grammars for phrase-based translation. In his work,
hierarchical reordering knowledge is included in synchronous rules which are automat-
ically learned from word-aligned corpus. In linguistically syntax-based models, string-
to-tree (Marcu et al 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and
Lin 2006), and tree-to-tree (Zhang et al 2008) translation rules, just to name a few, are
explored. Linguistical reordering knowledge is naturally included in these syntax-based
translation rules.
9.2 Automatic Analysis of Reordering
Although there is a variety of work on phrase reordering, automatic analysis of phrase
reordering is not widely explored in the SMT literature. Chiang et al (2005) propose
563
Computational Linguistics Volume 36, Number 3
an automatic method to compare different system outputs in a fine-grained manner
with regard to reordering. In their method, common word n-grams occurring in both
reference translations and system translations are extracted and generalized to part-of-
speech tag sequences. A recall is calculated for each certain tag sequence to indicate the
ability of reordering models to capture this tag sequence in system translations. Popovic
et al (2006) use the relative difference between WER (word error rate) and PER (position
independent word error rate) to indicate reordering errors. The larger the difference, the
more reordering errors there are.
Callison-Burch et al (2007) propose a constituent-based evaluation that is very simi-
lar to our method in Steps (1)?(3). They also parse the source sentence and automatically
align the parse tree with the reference/system translations. The difference is that they
highlight constituents from the parse tree to enable human evaluation of the translations
of these constituents, rather than automatically analyzing constituent movement. They
use this method for human evaluation in the shared translation task of the 2007 and
2008 ACL Workshop on Statistical Machine Translation.
Fox (2002) systematically studies syntactic cohesion between French and English
using human translations and alignments. Compared with her work, our analysis here
includes, but is not limited to, an investigation of syntactic cohesion in an actual MT
system.
10. Conclusion
We have presented a novel linguistically motivated phrase reordering approach:
Linguistically Annotated Reordering. The LAR approach incorporates soft linguistic
knowledge from the source parse tree into hard hierarchical skeletons generated by
BTG in phrasal SMT. To automatically learn reordering features, we have introduced
algorithms for reordering example extraction and linguistic annotation. We have also
proposed a new syntax-based analysis method to detect syntactic constituent movement
in human/machine translations.
We have conducted experiments on large-scale training data to evaluate LAR and
BWR as well as the reordering example extraction algorithms. Our evaluation results
show that:
1. Extracting reordering examples directly from word alignments is much
better than from BTG-style trees which are built from word alignments.
2. Selection rules which bias the reordering model towards smaller
swappings improve translation quality.
3. BWR+LAR significantly outperforms BWR, which suggests that the
integration of linguistic knowledge improves reordering; and tuning two
separate reordering models is better than the All-in-One combination
method.
We have further analyzed the outputs of BWR+LAR vs. BWR using the proposed
syntax-based analysis method. Our analysis results show that:
1. BWR+LAR achieves a significantly higher reordering precision and recall
than BWR does with regard to syntactic constituent movement.
564
Xiong et al Linguistically Annotated Reordering
2. For most reorderable constituents, integrating source-side linguistic
knowledge into the reordering model can significantly improve
reorderings by guiding reordering models to prefer hypotheses that pay
more respect to constituent boundaries.
3. For non-reorderable constituents or constituents involving long-distance
reorderings, integrating source-side linguistic knowledge into the
reordering model is not sufficient to avoid illegal boundary violations or to
capture reordering patterns.
To avoid illegal boundary violations in long-span constituents, we suggest con-
strained decoding, which protects special zones in the source sentence from being
interrupted by phrases outside the zones. Beginning and ending positions of the zones
are automatically learned using lexical and syntactic knowledge. To capture complex re-
orderings which cross constituent boundaries, phrasal SMT should integrate reordering
rules with richer contextual information.
Acknowledgments
We would like to thank the three anonymous
reviewers for their helpful comments and
suggestions.
References
Al-Onaizan, Yaser and Kishore Papineni.
2006. Distortion models for statistical
machine translation. In Proceedings
of the 21st International Conference on
Computational Linguistics and 44th
Annual Meeting of the Association for
Computational Linguistics, pages 529?536,
Sydney.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, John D. Lafferty,
and Robert L. Mercer. 1992. Analysis,
statistical transfer, and synthesis in
machine translation. In Proceedings
of the Fourth International Conference on
Theoretical and Methodological Issues in
Machine Translation, pages 83?100,
Montreal.
Callison-Burch, Chris, Cameron Fordyce,
Philipp Koehn, Christof Monz, and Josh
Schroeder. 2007. (Meta-) evaluation of
machine translation. In Proceedings of the
Second Workshop on Statistical Machine
Translation, pages 136?158, Prague.
Cherry, Colin. 2008. Cohesive phrase-based
decoding for statistical machine
translation. In Proceedings of ACL-08: HLT,
pages 72?80, Columbus, OH.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
translation. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics, pages 263?270,
Ann Arbor, MI.
Chiang, David, Adam Lopez, Nitin
Madnani, Christof Monz, Philip Resnik,
and Michael Subotin. 2005. The hiero
machine translation system: Extensions,
evaluation, and analysis. In Proceedings of
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 779?786,
Vancouver.
Collins, Michael, Philipp Koehn, and Ivona
Kucerova. 2005. Clause restructuring for
statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 531?540, Ann Arbor, MI.
Eisner, Jason. 2003. Learning non-isomorphic
tree mappings for machine translation. In
The Companion Volume to the Proceedings of
41st Annual Meeting of the Association for
Computational Linguistics, pages 205?208,
Sapporo.
Fox, Heidi. 2002. Phrasal cohesion and
statistical machine translation. In
Proceedings of the 2002 Conference on
Empirical Methods in Natural Language
Processing, pages 304?311,
Philadelphia, PA.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In Proceedings of the
Human Language Technology Conference
of the North American Chapter of the
Association for Computational Linguistics:
HLT-NAACL 2004, pages 273?280,
Boston, MA.
Ge, Niyu, Abe Ittycheriah, and Kishore
Papineni. 2008. Multiple reorderings in
phrase-based machine translation. In
Proceedings of the ACL-08: HLT Second
Workshop on Syntax and Structure in
565
Computational Linguistics Volume 36, Number 3
Statistical Translation (SSST-2), pages 61?68,
Columbus, OH.
Huang, Liang, Kevi Knight, and Aravind
Joshi. 2006. Statistical syntax-directed
translation with extended domain of
locality. In Proceedings of the 7th Conference
of the Association for Machine Translation
of the Americas, pages 66?73,
Cambridge, MA.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation. In
Proceedings of EMNLP 2004, pages 388?395,
Barcelona.
Koehn, Philipp, Amittai Axelrod, Alexandra
Birch Mayne, Chris Callison-Burch,
Miles Osborne, and David Talbot. 2005.
Edinburgh system description for the
2005 IWSLT speech translation
evaluation. In Proceedings of the
International Workshop on Spoken
Language Translation 2005, pages 78?85,
Pittsburgh, PA.
Koehn, Philipp, Franz Joseph Och, and
Daniel Marcu. 2003. Statistical
phrase-based translation. In Proceedings of
the 2003 Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 58?54, Edmonton.
Kumar, Shankar and William Byrne. 2005.
Local phrase reordering models for
statistical machine translation. In
Proceedings of Human Language Technology
Conference and Conference on Empirical
Methods in Natural Language Processing,
pages 161?168, Vancouver.
Li, Chi-Ho, Minghui Li, Dongdong Zhang,
Mu Li, Ming Zhou, and Yi Guan. 2007. A
probabilistic approach to syntax-based
reordering for statistical machine
translation. In Proceedings of the 45th
Annual Meeting of the Association of
Computational Linguistics, pages 720?727,
Prague.
Lin, Dekang. 2004. A path-based transfer
model for machine translation. In
Proceedings of the 20th International
Conference on Computational Linguistics
(Coling 2004), pages 625?630, Geneva.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006.
Tree-to-string alignment template for
statistical machine translation. In
Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 609?616,
Sydney.
Marcu, Daniel, Wei Wang, Abdessamad
Echihabi, and Kevin Knight. 2006. SPMT:
Statistical machine translation with
syntactified target language phrases. In
Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 44?52, Sydney.
Marton, Yuval and Philip Resnik. 2008. Soft
syntactic constraints for hierarchical
phrased-based translation. In Proceedings
of ACL-08: HLT, pages 1003?1011,
Columbus, OH.
Navarro, Gonzalo. 2001. A guided tour to
approximate string matching. ACM
Computing Surveys, 33(1):31?88.
Nie?n, Sonja and Hermann Ney. 2001.
Morpho-syntactic analysis for
reordering in statistical machine
translation. In Proceedings of MT Summit
VIII, pages 247?252, Santiago de
Compostela.
Och, Franz Josef. 2002. Statistical Machine
Translation: From Single-Word Models to
Alignment Templates. Ph.D. thesis, RWTH
Aachen University, Germany.
Och, Franz Josef. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics,
pages 160?167, Sapporo.
Och, Franz Josef and Hermann Ney. 2000.
Improved statistical alignment models. In
Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics,
pages 440?447, Hong Kong.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU: A
method for automatic evaluation of
machine translation. In Proceedings of 40th
Annual Meeting of the Association for
Computational Linguistics, pages 311?318,
Philadelphia, PA.
Popovic, Maja, Adria` de Gispert, Deepa
Gupta, Patrik Lambert, Hermann Ney,
Jose? B. Marin?o, Marcello Federico, and
Rafael Banchs. 2006. Morpho-syntactic
information for automatic error analysis
of statistical machine translation output.
In Proceedings on the Workshop on
Statistical Machine Translation, pages 1?6,
New York, NY.
Popovic?, Maja and Hermann Ney. 2006.
Pos-based word reorderings for statistical
machine translation. In Proceedings of the
Fifth International Conference on Language
Resources and Evaluation (LREC 2006),
pages 1278?1283, Genoa.
Quirk, Chris, Arul Menezes, and Colin
Cherry. 2005. Dependency treelet
translations: Syntactically informed
phrasal smt. In Proceedings of the 43rd
566
Xiong et al Linguistically Annotated Reordering
Annual Meeting of the ACL, pages 271?279,
Ann Arbor, MI.
Stolcke, Andreas. 2002. SRILM?an
extensible language modeling toolkit.
In Proceedings of the 7th International
Conference on Spoken Language
Processing (ICSLP 2002), pages 901?904,
Denver, CO.
Tillman, Christoph. 2004. A unigram
orientation model for statistical machine
translation. In Proceedings of the Human
Language Technology Conference of the North
American Chapter of the Association for
Computational Linguistics (HLT-NAACL
2004): Short Papers, pages 101?104,
Boston, MA.
Wang, Chao, Michael Collins, and Philipp
Koehn. 2007. Chinese syntactic reordering
for statistical machine translation. In
Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL),
pages 737?745, Prague.
Wu, Dekai. 1996. A polynomial-time
algorithm for statistical machine
translation. In Proceedings of the 34th
Annual Meeting of the Association for
Computational Linguistics, pages 152?158,
Santa Cruz, CA.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
Wu, Dekai, Marine Carpuat, and Yihai Shen.
2006. Inversion transduction grammar
coverage of Arabic-English word
alignment for tree-structured statistical
machine translation. In Proceeding
of the IEEE/ACL 2006 Workshop on
Spoken Language Technology (SLT 2006),
pages 234?237, Aruba.
Xia, Fei and Michael McCord. 2004.
Improving a statistical MT system
with automatically learned rewrite
patterns. In Proceedings of the 20th
International Conference on Computational
Linguistics (Coling 2004), pages 508?514,
Geneva.
Xiong, Deyi, Qun Liu, and Shouxun Lin.
2005. Parsing the penn chinese treebank
with semantic knowledge. In Proceedings of
The 2nd International Joint Conference on
Natural Language Processing (IJCNLP-05),
pages 70?81, Jeju Island.
Xiong, Deyi, Qun Liu, and Shouxun Lin.
2006. Maximum entropy based phrase
reordering model for statistical machine
translation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 521?528, Sydney.
Xiong, Deyi, Min Zhang, Aiti Aw, and
Haizhou Li. 2008a. A linguistically
annotated reordering model for
BTG-based statistical machine
translation. In Proceedings of ACL-08:
HLT, Short Papers, pages 149?152,
Columbus, OH.
Xiong, Deyi, Min Zhang, Aiti Aw, Haitao Mi,
Qun Liu, and Shouxun Lin. 2008b.
Refinements in BTG-based statistical
machine translation. In Proceedings
of the Third International Joint Conference
on Natural Language Processing,
pages 505?512, Hyderabad.
Xue, Nianwen, Fei Xia, Shizhe Huang, and
Anthony Kroch. 2000. The bracketing
guidelines for the Penn Chinese treebank
(3.0). Technical report IRCS 00-07,
University of Pennsylvania Institute for
Research in Cognitive Science,
Philadelphia.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics,
pages 523?530, Toulouse.
Yamamoto, Hirofumi, Hideo Okuma, and
Eiichiro Sumita. 2008. Imposing
constraints from the source tree on ITG
constraints for SMT. In Proceedings of the
ACL-08: HLT Second Workshop on Syntax
and Structure in Statistical Translation
(SSST-2), pages 1?9, Columbus, OH.
Zens, Richard, and Hermann Ney. 2003.
A comparative study on reordering
constraints in statistical machine
translation. In Proceedings of the 41st
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 144?151, Sapporo, Japan.
Zhang, Dongdong, Mu Li, Chi-Ho Li, and
Ming Zhou. 2007. Phrase reordering
model integrating syntactic knowledge
for SMT. In Proceedings of the 2007 Joint
Conference on Empirical Methods in
Natural Language Processing and
Computational Natural Language
Learning (EMNLP-CoNLL), pages 533?540,
Prague.
Zhang, Hao and Daniel Gildea. 2005.
Stochastic lexicalized inversion
transduction grammar for alignment. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 475?482, Ann Arbor, MI.
567
Computational Linguistics Volume 36, Number 3
Zhang, Hao, Daniel Gildea, and David
Chiang. 2008. Extracting synchronous
grammar rules from word-level
alignments in linear time. In Proceedings of
the 22nd International Conference on
Computational Linguistics (Coling 2008),
pages 1081?1088, Manchester.
Zhang, Min, Hongfei Jiang, Aiti Aw, Haizhou
Li, Chew Lim Tan, and Sheng Li. 2008. A
tree sequence alignment-based tree-to-tree
translation model. In Proceedings of ACL-08:
HLT, pages 559?567, Columbus, OH.
Zhang, Yuqi, Richard Zens, and Hermann
Ney. 2007. Chunk-level reordering of
source language sentences with
automatically learned rules for statistical
machine translation. In Proceedings
of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in
Statistical Translation, pages 1?8,
Rochester, NY.
Zollmann, Andreas, Ashish Venugopal, and
Stephan Vogel. 2008. The CMU
syntax-augmented machine translation
system: SAMT on hadoop with N-best
alignments. In Proceedings of International
Workshop on Spoken Language Translation
(IWSLT), pages 18?25, Honolulu, HI.
568
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 136?144,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning Translation Boundaries for Phrase-Based Decoding
Deyi Xiong, Min Zhang, Haizhou Li
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632.
{dyxiong, mzhang, hli}@i2r.a-star.edu.sg
Abstract
Constrained decoding is of great importance
not only for speed but also for translation qual-
ity. Previous efforts explore soft syntactic con-
straints which are based on constituent bound-
aries deduced from parse trees of the source
language. We present a new framework to es-
tablish soft constraints based on a more nat-
ural alternative: translation boundary rather
than constituent boundary. We propose sim-
ple classifiers to learn translation boundaries
for any source sentences. The classifiers are
trained directly on word-aligned corpus with-
out using any additional resources. We report
the accuracy of our translation boundary clas-
sifiers. We show that using constraints based
on translation boundaries predicted by our
classifiers achieves significant improvements
over the baseline on large-scale Chinese-to-
English translation experiments. The new
constraints also significantly outperform con-
stituent boundary based syntactic constrains.
1 Introduction
It has been known that phrase-based decoding
(phrase segmentation/translation/reordering (Chi-
ang, 2005)) should be constrained to some extent not
only for transferring the NP-hard problem (Knight,
1999) into a tractable one in practice but also for im-
proving translation quality. For example, Xiong et
al. (2008) find that translation quality can be signif-
icantly improved by either prohibiting reorderings
around punctuation or restricting reorderings within
a 15-word window.
Recently, more linguistically motivated con-
straints are introduced to improve phrase-based de-
coding. (Cherry, 2008) and (Marton and Resnik,
2008) introduce syntactic constraints into the stan-
dard phrase-based decoding (Koehn et al, 2003) and
hierarchical phrase-based decoding (Chiang, 2005)
respectively by using a counting feature which ac-
cumulates whenever hypotheses violate syntactic
boundaries of source-side parse trees. (Xiong et al,
2009) further presents a bracketing model to include
thousands of context-sensitive syntactic constraints.
All of these approaches achieve their improvements
by guiding the phrase-based decoder to prefer trans-
lations which respect source-side parse trees.
One major problem with such constituent bound-
ary based constraints is that syntactic structures of
the source language do not necessarily reflect trans-
lation structures where the source and target lan-
guage correspond to each other. In this paper,
we investigate building classifiers that directly ad-
dress the problem of translation boundary, rather
than extracting constituent boundary from source-
side parsers built for a different purpose. A trans-
lation boundary is a position in the source sequence
which begins or ends a translation zone 1 spanning
multiple source words. In a translation zone, the
source phrase is translated as a unit. Reorderings
which cross translation zones are not desirable.
Inspired by (Roark and Hollingshead, 2008)
which introduces classifiers to decide if a word can
begin/end a multi-word constituent, we build two
discriminative classifiers to tag each word in the
source sequence with a binary class label. The first
classifier decides if a word can begin a multi-source-
word translation zone; the second classifier decides
if a word can end a multi-source-word translation
1We will give a formal definition of translation zone in Sec-
tion 2.
136
zone. Given a partial translation covering source se-
quence (i, j) with start word ci and end word cj 2,
this translation can be penalized if the first classifier
decides that the start word ci can not be a beginning
translation boundary or the second classifier decides
that the end word cj can not be an ending translation
boundary. In such a way, we can guide the decoder
to boost hypotheses that respect translation bound-
aries and therefore the common translation structure
shared by the source and target language, rather than
the syntactic structure of the source language.
We report the accuracy of such classifiers by com-
paring their outputs with ?gold? translation bound-
aries obtained from reference translations on the de-
velopment set. We integrate translation boundary
based constraints into phrase-based decoding and
display that they improve translation quality signif-
icantly in large-scale experiments. Furthermore, we
confirm that they also significantly outperform con-
stituent boundary based syntactic constraints.
2 Beginning and Ending Translation Zones
To better understand the particular task that we ad-
dress in this paper, we study the distribution of
classes of translation boundaries in real-world data.
First, we introduce some notations. Given a source
sentence c1...cn, we will say that a word ci (1 < i <
n) is in the class By if there is a translation zone ?
spanning ci...cj for some j > i; and ci ? Bn oth-
erwise. Similarly, we will say that a word cj is in
the class Ey if there is a translation zone spanning
ci...cj for some j > i; and cj ? En otherwise.
Here, a translation zone ? is a pair of aligned
source phrase and target phrase
? = (cji , e
q
p)
where ? must be consistent with the word alignment
M
?(u, v) ? M, i ? u ? j ? p ? v ? q
By this, we require that no words inside the source
phrase cji are aligned to words outside the target
phrase eqp and that no words outside the source
phrase are aligned to words inside the target phrase.
2In this paper, we use c to denote the source language and e
the target language.
Item Count (M) P (%)
Sentences 3.8 ?
Words 96.9 ?
Words ? By 22.7 23.4
Words ? Ey 41.0 42.3
Words /? By and /? Ey 33.2 34.3
Table 1: Statistics on word classes from our bilingual
training data. All numbers are calculated on the source
side. P means the percentage.
This means, in other words, that the source phrase
cji is mapped as a unit onto the target phrase eqp.
When defining the By and Ey class, we also re-
quire that the source phrase cji in the translation zone
must contain multiple words (j > i). Our interest
is the question of whether a sequence of consecu-
tive source words can be translated as a unit (i.e.
whether there is a translation zone covering these
source words). For a single-word source phrase, if
it can be translated separately, it is always translated
as a unit in the context of phrase-based decoding.
Therefore this question does not exist.
Note that the first word c1 and the last word cn
are unambiguous in terms of whether they begin or
end a translation zone. The first word c1 must begin
a translation zone spanning the whole source sen-
tence. The last word cn must end a translation zone
spanning the whole source sentence. Therefore, our
classifiers only need to predict the other n?2 words
for a source sentence of length n.
Table 1 shows statistics of word classes from our
training data which contain nearly 100M words in
approximately 4M sentences. Among these words,
only 22.7M words can begin a translation zone
which covers multiple source words. 41M words
can end a translation zone spanning multiple source
words, which accounts for more than 42% in all
words. We still have more than 33M words, ac-
counting for 34.3%, which neither begin nor end
a multi-source-word translation zone. Apparently,
translations that begin/end on words ? By/? Ey are
preferable to those which begin/end on other words.
Yet another interesting study is to compare trans-
lation boundaries with constituent boundaries de-
duced from source-side parse trees. In doing so,
we can know further how well constituent boundary
137
Classification Task Avg. Accuracy (%)
By/Bn 46.9
Ey/En 52.2
Table 2: Average classification accuracy on the develop-
ment set when we treat constituent boundary deducer (ac-
cording to source-side parse trees) as a translation bound-
ary classifier.
based syntactic constraints can improve translation
quality. We pair the source sentences of our devel-
opment set with each of the reference translations
and include the created sentence pairs in our bilin-
gual training corpus. Then we obtain word align-
ments on the new corpus (see Section 5.1 for the de-
tails of learning word alignments). From the word
alignments we obtain translation boundaries (see de-
tails in the next section). We parse the source sen-
tences of our development set and obtain constituent
boundaries from parse trees.
To make a clear comparison with our transla-
tion boundary classifiers (see Section 3.3), we treat
constituent boundaries deduced from source-side
parse trees as output from beginning/ending bound-
ary classifiers: the constituent beginning boundary
corresponds to By; the constituent ending boundary
corresponds to Ey. We have four reference transla-
tions for each source sentence. Therefore we have
four translation boundary sets, each of which is pro-
duced from word alignments between source sen-
tences and one reference translation set. Each of
the four translation boundary sets will be used as a
gold standard. We calculate classification accuracy
for our constituent boundary deducer on each gold
standard and average them finally.
Table 2 shows the accuracy results. The average
accuracies on the four gold standard sets are very
low, especially for the By/Bn classification task. In
section 3.3, we will show that our translation bound-
ary classifiers achieve higher accuracy than that of
constituent boundary deducer. This suggests that
pure constituent boundary based constraints are not
the best choice to constrain phrase-based decoding.
3 Learning Translation Boundaries
In this section, we investigate building classifiers
to predict translation boundaries. First, we elabo-
rate the acquisition of training instances from word
alignments. Second, we build two classifiers with
simple features on the obtained training instances.
Finally, we evaluate our classifiers on the develop-
ment set using the ?gold? translation boundaries ob-
tained from reference translations.
3.1 Obtaining Translation Boundaries from
Word Alignments
We can easily obtain constituent boundaries from
parse trees. Similarly, if we have a tree covering
both source and target sentence, we can easily get
translation boundaries from this tree. Fortunately,
we can build such a tree directly from word align-
ments. We use (Zhang et al, 2008)?s shift-reduce al-
gorithm (SRA) to decompose word alignments into
hierarchical trees.
Given an arbitrary word-level alignment as an in-
put, SRA is able to output a tree representation of the
word alignment (a.k.a decomposition tree). Each
node of the tree is a translation zone as we defined
in the Section 2. Therefore the first word on the
source side of each multi-source-word node is a be-
ginning translation boundary (? By); the last word
on the source side of each multi-source-word node
is an ending translation boundary (? Ey).
Figure 1a shows an example of many-to-many
alignment, where the source language is Chinese
and the target language is English. Each word is
indexed with their occurring position from left to
right. Figure 1b is the tree representation of the word
alignment after hierarchical analysis using SRA. We
use ([i, j], [p, q]) to denote a tree node, where i, j
and p, q are the beginning and ending index in the
source and target language, respectively. By check-
ing nodes which cover multiple source words, we
can easily decide that the source words {??, ?,
??} are in the class By and any other words are
in the class Bn if we want to train a By/Bn classi-
fier with class labels {By, Bn}. Similarly, the source
words {?,??,?,??} are in the class Ey and
any other words are in the class En when we train a
Ey/En classifier with class labels {Ey, En}.
By using SRA on each word-aligned bilingual
sentence, as described above, we can tag each source
word with two sets of class labels: {By, Bn} and
{Ey, En}. The tagged source sentences will be used
to train our two translation boundary classifiers.
138
?? ??? ? ?? ??
The last five flights all failed due to accidents
?
1 2 3 4 5 6 7
1 2 3 4 5 6 7 8 9
([1, 7], [1, 9])
([6, 7], [6, 9])
([6, 6], [7, 9]) ([7, 7], [6, 6])
([1, 5], [1, 5])
([1, 4], [1, 4]) ([5, 5], [5, 5])
([1, 3], [1, 3]) ([4, 4], [4, 4])
([1, 1], [1, 2]) ([2, 3], [3, 3])
a) b)
Figure 1: An example of many-to-many word alignment and its tree representation produced by (Zhang et al, 2008)?s
shift-reduce algorithm.
3.2 Building Translation Boundary Classifiers
We build two discriminative classifiers based on
Maximum Entropy Markov Models (MEMM) (Mc-
Callum et al, 2000). One classifier is to predict the
word class ? ? {By, Bn} for each source word. The
other is to predict the word class ? ? {Ey, En}.
These two classifiers are separately trained using
training instances obtained from our word-aligned
training data as demonstrated in the last section.
We use features from surrounding words, includ-
ing 2 before and 2 after the current word position
(c?2, c?1, c+1, c+2). We also use class features to
train models with Markov order 1 (including class
feature ?c?1), and Markov order 2 (including class
features ?c?1 , ?c?2).
3.3 Evaluating Translation Boundary
Classifiers
How well can we perform these binary classifica-
tion tasks using the classifiers described above? Can
we obtain better translation boundary predictions
than extracting constituent boundary from source-
side parse trees? To investigate these questions, we
evaluate our MEMM based classifiers. We trained
them on our 100M-word word-aligned corpus. We
ran the two trained classifiers on the development
set separately to obtain the By/Bn words and Ey/En
words. Then we built our four gold standards using
four reference translation sets as described in Sec-
Avg. Accuracy (%)
Classification Task MEMM 1 MEMM 2
By/Bn 71.7 70.2
Ey/En 59.2 58.8
Table 3: Average classification accuracy on the develop-
ment set for our MEMM based translation boundary clas-
sifiers with various Markov orders.
tion 2. The average classification accuracy results
are shown in Table 3.
Comparing Table 3 with Table 2, we find that our
MEMM based classifiers significantly outperform
constituent boundary deducer in predicting transla-
tion boundaries, especially in the By/Bn classifi-
cation task, where our MEMM based By/Bn clas-
sifier (Markov order 1) achieves a relative increase
of 52.9% in accuracy over the constituent bound-
ary deducer. In the Ey/En classification task, our
classifiers also perform much better than constituent
boundary deducer.
Then are our MEMM based translation boundary
classifiers good enough? The accuracies are still low
although they are higher than those of constituent
boundary deducer. One reason why we have low
accuracies is that our gold standard based evalua-
tion is not established on real gold standards. In
other words, we don?t have gold standards in terms
of translation boundary since different translations
139
Classification Task Avg. Accuracy (%)
By/Bn 80.6
Ey/En 75.7
Table 4: Average classification accuracy on the develop-
ment set when treating each reference translation set as a
boundary classifier.
generate very different translation boundaries. We
can measure these differences in reference transla-
tions using the same evaluation metric (classification
accuracy). We treat each reference translation set
as a translation boundary classifier while the other
three reference translation sets as gold standards.
We calculate the classification accuracy for the cur-
rent reference translation set and finally average all
four accuracies. Table 4 presents the results.
Comparing Table 4 with Table 3, we can see that
the accuracy of our translation boundary classifica-
tion approach is not that low when considering vast
divergences of reference translations. The question
now becomes, how can classifier output be used to
constrain phrase-based decoding, and what is the
impact on the system performance of using such
constraints.
4 Integrating Translation Boundaries into
Decoding
By running the two trained classifiers on the source
sentence separately, we obtain two classified word
sets: By/Bn words, and Ey/En words. We can pro-
hibit any translations or reorderings spanning ci...cj
(j > i) where ci /? By according to the first classi-
fier or cj /? Ey according to the second classifier. In
such a way, we integrate translation boundaries into
phrase-based decoding as hard constraints, which,
however, is at the risk of producing no translation
covering the whole source sentence.
Alternatively, we introduce soft constraints based
on translation boundary that our classifiers pre-
dict, similar to constituent boundary based soft con-
straints in (Cherry, 2008) and (Marton and Resnik,
2008). We add a new feature to the decoder?s log-
linear model: translation boundary violation count-
ing feature. This counting feature accumulates
whenever hypotheses have a partial translation span-
ning ci...cj (j > i) where ci /? By or cj /? Ey. The
LDC ID Description
LDC2004E12 United Nations
LDC2004T08 Hong Kong News
LDC2005T10 Sinorama Magazine
LDC2003E14 FBIS
LDC2002E18 Xinhua News V1 beta
LDC2005T06 Chinese News Translation
LDC2003E07 Chinese Treebank
LDC2004T07 Multiple Translation Chinese
Table 5: Training corpora.
weight ?v of this feature is tuned via minimal error
rate training (MERT) (Och, 2003) with other feature
weights.
Unlike hard constraints, which simply prevent
any hypotheses from violating translation bound-
aries, soft constraints allow violations of translation
boundaries but with a penalty of exp(??vCv) where
Cv is the violation count. By using soft constraints,
we can enable the model to prefer hypotheses which
are consistent with translation boundaries.
5 Experiment
Our baseline system is a phrase-based system us-
ing BTGs (Wu, 1997), which includes a content-
dependent reordering model discriminatively trained
using reordering examples (Xiong et al, 2006). We
carried out various experiments to evaluate the im-
pact of integrating translation boundary based soft
constraints into decoding on the system performance
on the Chinese-to-English translation task of the
NIST MT-05 using large scale training data.
5.1 Experimental Setup
Our training corpora are listed in Table 5. The
whole corpora consist of 96.9M Chinese words and
109.5M English words in 3.8M sentence pairs. We
ran GIZA++ (Och and Ney, 2000) on the par-
allel corpora in both directions and then applied
the ?grow-diag-final? refinement rule (Koehn et al,
2005) to obtain many-to-many word alignments.
From the word-aligned corpora, we extracted bilin-
gual phrases and trained our translation model.
We used all corpora in Table 5 except for the
United Nations corpus to train our MaxEnt based
reordering model (Xiong et al, 2006), which con-
140
sist of 33.3M Chinese words and 35.8M English
words. We built a four-gram language model us-
ing the SRILM toolkit (Stolcke, 2002), which was
trained on Xinhua section of the English Gigaword
corpus (181.1M words).
To train our translation boundary classifiers, we
extract training instances from the whole word-
aligned corpora, from which we obtain 96.9M train-
ing instances for the By/Bn and Ey/En classifier.
We ran the off-the-shelf MaxEnt toolkit (Zhang,
2004) to tune classifier feature weights with Gaus-
sian prior set to 1 to avoid overfitting.
We used the NIST MT-03 evaluation test data as
our development set (919 sentences in total, 27.1
words per sentence). The NIST MT-05 test set in-
cludes 1082 sentences with an average of 27.4 words
per sentence. Both the reference corpus for the NIST
MT-03 set and the reference corpus for the NIST
MT-05 set contain 4 translations per source sen-
tence. To compare with constituent boundary based
constraints, we parsed source sentences of both the
development and test sets using a Chinese parser
(Xiong et al, 2005) which was trained on the Penn
Chinese Treebank with an F1-score of 79.4%.
Our evaluation metric is case-insensitive BLEU-4
(Papineni et al, 2002) using the shortest reference
sentence length for the brevity penalty. Statistical
significance in BLEU score differences was tested
by paired bootstrap re-sampling (Koehn, 2004).
5.2 Using Translation Boundaries from
Reference Translations
The most direct way to investigate the impact on the
system performance of using translation boundaries
is to integrate ?right? translation boundaries into de-
coding which are directly obtained from reference
translations. For both the development set and test
set, we have four reference translation sets, which
are named ref1, ref2, ref3 and ref4, respectively.
For the development set, we used translation bound-
aries obtained from ref1. Based on these boundaries,
we built our translation boundary violation counting
feature and tuned its feature weight with other fea-
tures using MERT. When we obtained the best fea-
ture weights ?s, we evaluated on the test set using
translation boundaries produced from ref1, ref2, ref3
and ref4 of the test set respectively.
Table 6 shows the results. We clearly see that us-
System BLEU-4 (%)
Base 33.05
Ref1 33.99*
Ref2 34.17*
Ref3 33.93*
Ref4 34.21*
Table 6: Results of using translation boundaries obtained
from reference translations. *: significantly better than
baseline (p < 0.01).
ing ?right? translation boundaries to build soft con-
straints significantly improve the performance mea-
sured by BLEU score. The best result comes from
ref4, which achieves an absolute increase of 1.16
BLEU points over the baseline. We believe that the
best result here only indicates the lower bound of
potential improvement when using right translation
boundaries. If we have consistent translation bound-
aries on the development and test set (for example,
we have the same 4 translators build reference trans-
lations for both the development and test set), the
performance improvement will be higher.
5.3 Using Automatically Learned Translation
Boundaries
The success of using translation boundaries from
reference translations inspires us to pursue trans-
lation boundaries predicted by our MEMM based
classifiers. We ran our MEMM1 (Markov order 1)
and MEMM2 (Markov order 2) By/Bn and Ey/En
classifiers on both the development and test set.
Based on translation boundaries output by MEMM1
and MEMM2 classifiers, we built our translation
boundary violation feature and tuned it on the de-
velopment set. The evaluation results on the test set
are shown in Table 7.
From Table 7 we observe that using soft con-
straints based on translation boundaries from both
our MEMM 1 and MEMM 2 significantly outper-
form the baseline. Impressively, when using outputs
from MEMM 2, we achieve an absolute improve-
ment of almost 1 BLEU point over the baseline. This
result is also very close to the best result of using
translation boundaries from reference translations.
To compare with constituent boundary based syn-
tactic constraints, we also carried out experiments
using two kinds of such constraints. One is the
141
System BLEU-4 (%)
Base 33.05
Condeducer 33.18
XP+ 33.58*
BestRef 34.21*+
MEMM 1 33.70*
MEMM 2 34.04*+
Table 7: Results of using automatically learned trans-
lation boundaries. Condeducer means using pure con-
stituent boundary based soft constraint. XP+ is another
constituent boundary based soft constraint but with dis-
tinction among special constituent types (Marton and
Resnik, 2008). BestRef is the best result using reference
translation boundaries in Table 6. MEMM 1 and MEMM
2 are our MEMM based translation boundary classifiers
with Markov order 1 and 2. *: significantly better than
baseline (p < 0.01). +: significantly better than XP+
(p < 0.01).
Condeducer which uses pure constituent bound-
ary based syntactic constraint: any partial transla-
tions which cross any constituent boundaries will
be penalized. The other is the XP+ from (Marton
and Resnik, 2008) which only penalizes hypotheses
which violate the boundaries of a constituent with
a label from {NP, VP, CP, IP, PP, ADVP, QP, LCP,
DNP}. The XP+ is the best syntactic constraint
among all constraints that Marton and Resnik (2008)
use for Chinese-to-English translation.
Still in Table 7, we find that both syntactic con-
straint Condeducer and XP+ are better than the base-
line. But only XP+ is able to obtain significant im-
provement. Both our MEMM 1 and MEMM 2 out-
perform Condeducer. MEMM 2 achieves significant
improvement over XP+ by approximately 0.5 BLEU
points. This comparison suggests that translation
boundary is a better option than constituent bound-
ary when we build constraints to restrict phrase-
based decoding.
5.4 One Classifier vs. Two Classifiers
Revisiting the classification task in this paper, we
can also consider it as a sequence labeling task
where the first source word of a translation zone
is labeled ?B?, the last source word of the trans-
lation zone is labeled ?E?, and other words are la-
beled ?O?. To complete such a sequence labeling
task, we built only one classifier which is still based
on MEMM (with Markov order 2) with the same
features as described in Section 3.2. We built soft
constraints based on the outputs of this classifier and
evaluated them on the test set. The case-insensitive
BLEU score is 33.62, which is lower than the per-
formance of using two separate classifiers (34.04).
We calculated the accuracy for class ?B? by map-
ping ?B? to By and ?E? and ?O? to Bn. The result is
67.9%. Similarly, we obtained the accuracy of class
?E?, which is as low as 48.6%. These two accura-
cies are much lower than those of using two separate
classifiers, especially the accuracy of ?E?. This sug-
gests that the By and Ey are not interrelated tightly.
It is better to learn them separately with two classi-
fiers.
Another advantage of using two separate classi-
fiers is that we can explore more constraints. A word
ck can be possibly labeled asBy by the first classifier
and Ey by the second classifier. Therefore we can
build soft constraints on span (ci, ck) (ci ? By, ck ?
Ey) and span (ck, cj) (ck ? By, cj ? Ey). This is
impossible if we use only one classifier since each
word can have only one class label. We can build
only one constraint on span (ci, ck) or span (ck, cj).
6 Related Work
Various approaches incorporate constraints into
phrase-based decoding in a soft or hard manner. Our
introduction has already briefly mentioned (Cherry,
2008) and (Marton and Resnik, 2008), which utilize
source-side parse tree boundary violation counting
feature to build soft constraints for phrase-based de-
coding, and (Xiong et al, 2009), which calculates a
score to indicate to what extent a source phrase can
be translated as a unit using a bracketing model with
richer syntactic features. More previously, (Chi-
ang, 2005) rewards hypotheses whenever they ex-
actly match constituent boundaries of parse trees on
the source side.
In addition, hard linguistic constraints are also ex-
plored. (Wu and Ng, 1995) employs syntactic brack-
eting information to constrain search in order to im-
prove speed and accuracy. (Collins et al, 2005) and
(Wang et al, 2007) use hard syntactic constraints to
perform reorderings according to source-side parse
trees. (Xiong et al, 2008) prohibit any swappings
142
which violate punctuation based constraints.
Non-linguistic constraints are also widely used
in phrase-based decoding. The IBM and ITG con-
straints (Zens et al, 2004) are used to restrict re-
orderings in practical phrase-based systems.
(Berger et al, 1996) introduces the concept of rift
into a machine translation system, which is similar
to our definition of translation boundary. They also
use a maximum entropy model to predict whether a
source position is a rift based on features only from
source sentences. Our work differs from (Berger et
al., 1996) in three major respects.
1) We distinguish a segment boundary into two
categories: beginning and ending boundary due
to their different distributions (see Table 1).
However, Berger et al ignore this difference.
2) We train two classifiers to predict beginning
and ending boundary respectively while Berger
et al build only one classifier. Our experiments
show that two separate classifiers outperform
one classifier.
3) The last difference is how segment bound-
aries are integrated into a machine transla-
tion system. Berger et al use predicted
rifts to divide a long source sentence into a
series of smaller segments, which are then
translated sequentially in order to increase de-
coding speed (Brown et al, 1992; Berger
et al, 1996). This can be considered as a
hard integration, which may undermine trans-
lation accuracy given wrongly predicted rifts.
We integrate predicted translation boundaries
into phrase-based decoding in a soft manner,
which improves translation accuracy in terms
of BLEU score.
7 Conclusion and Future Work
In this paper, we have presented a simple approach
to learn translation boundaries on source sentences.
The learned translation boundaries are used to con-
strain phrase-based decoding in a soft manner. The
whole approach has several properties.
? First, it is based on a simple classification task
that can achieve considerably high accuracy
when taking translation divergences into ac-
count using simple models and features.
? Second, the classifier output can be straightfor-
wardly used to constrain phrase-based decoder.
? Finally, we have empirically shown that, to
build soft constraints for phrase-based decod-
ing, translation boundary predicted by our clas-
sifier is a better choice than constituent bound-
ary deduced from source-side parse tree.
Future work in this direction will involve trying
different methods to define more informative trans-
lation boundaries, such as a boundary to begin/end
a swapping. We would also like to investigate new
methods to incorporate automatically learned trans-
lation boundaries more efficiently into decoding in
an attempt to further improve search in both speed
and accuracy.
References
Adam L. Berger, Stephen A. Della Pietra and Vincent J.
Della Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational Lin-
guistics, 22(1):39-71.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Robert L. Mercer, and Surya Mohanty.
1992. Dividing and Conquering Long Sentences in a
Translation System. In Proceedings of the workshop
on Speech and Natural Language, Human Language
Technology.
Colin Cherry. 2008. Cohesive Phrase-based Decoding
for Statistical Machine Translation. In Proceedings of
ACL.
David Chiang. 2005. A Hierarchical Phrase-based
Model for Statistical Machine Translation. In Pro-
ceedings of ACL, pages 263?270.
Michael Collins, Philipp Koehn and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of ACL.
Kevin Knight. 1999. Decoding Complexity in Word Re-
placement Translation Models. In Computational Lin-
guistics, 25(4):607? 615.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of HLT-NAACL.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of IWSLT.
143
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrase-Based Translation.
In Proceedings of ACL.
Andrew McCallum, Dayne Freitag and Fernando Pereira
2000. Maximum Entropy Markov Models for Infor-
mation Extraction and Segmentation. In Proceedings
of the Seventeenth International Conference on Ma-
chine Learning 2000.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL
2000.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of
ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatically
Evaluation of Machine Translation. In Proceedings of
ACL 2002.
Brian Roark and Kristy Hollingshead. 2008. Classifying
Chart Cells for Quadratic Complexity Context-Free In-
ference. In Proceedings of COLING 2008.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Chao Wang, Michael Collins and Philipp Koehn 2007.
Chinese Syntactic Reordering for Statistical Machine
Translation. In Proceedings of EMNLP.
Dekai Wu and Cindy Ng. 1995. Using Brackets to Im-
prove Search for Statistical Machine Translation In
Proceedings of PACLIC-IO, Pacific Asia Conference
on Language, Information and Computation.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
Yueliang Qian. 2005. Parsing the Penn Chinese Tree-
bank with Semantic Knowledge. In Proceedings of
IJCNLP, Jeju Island, Korea.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase ReorderingModel for Sta-
tistical Machine Translation. In Proceedings of ACL-
COLING 2006.
Deyi Xiong, Min Zhang, Ai Ti Aw, Haitao Mi, Qun Liu
and Shouxun Lin. 2008. Refinements in BTG-based
Statistical Machine Translation. In Proceedings of
IJCNLP 2008.
Deyi Xiong, Min Zhang, Ai Ti Aw, and Haizhou Li.
2009. A Syntax-Driven Bracketing Model for Phrase-
Based Translation. In Proceedings of ACL-IJCNLP
2009.
Richard Zens, Hermann Ney, TaroWatanabe and Eiichiro
Sumita 2004. Reordering Constraints for Phrase-
Based Statistical Machine Translation. In Proceedings
of COLING.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting Synchronous Grammars Rules from Word-
Level Alignments in Linear Time. In Proceeding of
COLING 2008.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
144
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 148?156,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Pseudo-word for Phrase-based Machine Translation 
 
 
Xiangyu Duan Min Zhang Haizhou Li 
Institute for Infocomm Research, A-STAR, Singapore 
{Xduan, mzhang, hli}@i2r.a-star.edu.sg 
 
  
 
Abstract 
 
The pipeline of most Phrase-Based Statistical 
Machine Translation (PB-SMT) systems starts 
from automatically word aligned parallel cor-
pus. But word appears to be too fine-grained 
in some cases such as non-compositional 
phrasal equivalences, where no clear word 
alignments exist. Using words as inputs to PB-
SMT pipeline has inborn deficiency. This pa-
per proposes pseudo-word as a new start point 
for PB-SMT pipeline. Pseudo-word is a kind 
of basic multi-word expression that character-
izes minimal sequence of consecutive words in 
sense of translation. By casting pseudo-word 
searching problem into a parsing framework, 
we search for pseudo-words in a monolingual 
way and a bilingual synchronous way. Ex-
periments show that pseudo-word significantly 
outperforms word for PB-SMT model in both 
travel translation domain and news translation 
domain. 
1 Introduction 
The pipeline of most Phrase-Based Statistical 
Machine Translation (PB-SMT) systems starts 
from automatically word aligned parallel corpus 
generated from word-based models (Brown et al, 
1993), proceeds with step of induction of phrase 
table (Koehn et al, 2003) or synchronous gram-
mar (Chiang, 2007) and with model weights tun-
ing step. Words are taken as inputs to PB-SMT at 
the very beginning of the pipeline. But there is a 
deficiency in such manner that word is too fine-
grained in some cases such as non-compositional 
phrasal equivalences, where clear word align-
ments do not exist. For example in Chinese-to-
English translation, ??? and ?would like to? 
constitute a 1-to-n phrasal equivalence, ??? 
?? and ?how much is it? constitute a m-to-n 
phrasal equivalence. No clear word alignments 
are there in such phrasal equivalences. Moreover, 
should basic translational unit be word or coarse-
grained multi-word is an open problem for opti-
mizing SMT models. 
Some researchers have explored coarse-
grained translational unit for machine translation. 
Marcu and Wong (2002) attempted to directly 
learn phrasal alignments instead of word align-
ments. But computational complexity is prohibi-
tively high for the exponentially large number of 
decompositions of a sentence pair into phrase 
pairs. Cherry and Lin (2007) and Zhang et al 
(2008) used synchronous ITG (Wu, 1997) and 
constraints to find non-compositional phrasal 
equivalences, but they suffered from intractable 
estimation problem. Blunsom et al (2008; 2009) 
induced phrasal synchronous grammar, which 
aimed at finding hierarchical phrasal equiva-
lences. 
Another direction of questioning word as basic 
translational unit is to directly question word 
segmentation on languages where word bounda-
ries are not orthographically marked. In Chinese-
to-English translation task where Chinese word 
boundaries are not marked, Xu et al (2004) used 
word aligner to build a Chinese dictionary to re-
segment Chinese sentence. Xu et al (2008) used 
a Bayesian semi-supervised method that com-
bines Chinese word segmentation model and 
Chinese-to-English translation model to derive a 
Chinese segmentation suitable for machine trans-
lation. There are also researches focusing on the 
impact of various segmentation tools on machine 
translation (Ma et al 2007; Chang et al 2008; 
Zhang et al 2008). Since there are many 1-to-n 
phrasal equivalences in Chinese-to-English trans-
lation (Ma and Way. 2009), only focusing on 
Chinese word as basic translational unit is not 
adequate to model 1-to-n translations. Ma and 
Way (2009) tackle this problem by using word 
aligner to bootstrap bilingual segmentation suit-
able for machine translation. Lambert and 
Banchs (2005) detect bilingual multi-word ex-
148
pressions by monotonically segmenting a given 
Spanish-English sentence pair into bilingual 
units, where word aligner is also used. 
IBM model 3, 4, 5 (Brown et al, 1993) and 
Deng and Byrne (2005) are another kind of re-
lated works that allow 1-to-n alignments, but 
they rarely questioned if such alignments exist in 
word units level, that is, they rarely questioned 
word as basic translational unit. Moreover, m-to-
n alignments were not modeled. 
This paper focuses on determining the basic 
translational units on both language sides without 
using word aligner before feeding them into PB-
SMT pipeline. We call such basic translational 
unit as pseudo-word to differentiate with word. 
Pseudo-word is a kind of multi-word expression 
(includes both unary word and multi-word). 
Pseudo-word searching problem is the same to 
decomposition of a given sentence into pseudo-
words. We assume that such decomposition is in 
the Gibbs distribution. We use a measurement, 
which characterizes pseudo-word as minimal 
sequence of consecutive words in sense of trans-
lation, as potential function in Gibbs distribution. 
Note that the number of decomposition of one 
sentence into pseudo-words grows exponentially 
with sentence length. By fitting decomposition 
problem into parsing framework, we can find 
optimal pseudo-word sequence in polynomial 
time. Then we feed pseudo-words into PB-SMT 
pipeline, and find that pseudo-words as basic 
translational units improve translation perform-
ance over words as basic translational units. Fur-
ther experiments of removing the power of 
higher order language model and longer max 
phrase length, which are inherent in pseudo-
words, show that pseudo-words still improve 
translational performance significantly over 
unary words. 
This paper is structured as follows: In section 
2, we define the task of searching for pseudo-
words and its solution. We present experimental 
results and analyses of using pseudo-words in 
PB-SMT model in section 3. The conclusion is 
presented at section 4. 
2 Searching for Pseudo-words 
Pseudo-word searching problem is equal to de-
composition of a given sentence into pseudo-
words. We assume that the distribution of such 
decomposition is in the form of Gibbs distribu-
tion as below: 
)exp(
1
)|( ?= ySigXYP
where X denotes the sentence, Y denotes a de-
composition of X. Sig function acts as potential 
function on each multi-word yk, and ZX acts as 
partition function. Note that the number of yk is 
not fixed given X because X can be decomposed 
into various number of multi-words. 
Given X, ZX is fixed, so searching for optimal 
decomposition is as below: 
?==
k
y
YY kK
SigARGMAXXYPARGMAXY
1
)|(?   (2) 
where Y1K denotes K multi-word units from de-
composition of X. A multi-word sequence with 
maximal sum of Sig function values is the search 
target ? pseudo-word sequence. From (2) we 
can see that Sig function is vital for pseudo-word 
searching. In this paper Sig function calculates 
sequence significance which is proposed to char-
acterize pseudo-word as minimal sequence of 
consecutive words in sense of translation. The 
detail of sequence significance is described in the 
following section. 
2.1 Sequence Significance 
Two kinds of definitions of sequence signifi-
cance are proposed. One is monolingual se-
quence significance. X and Y are monolingual 
sentence and monolingual multi-words respec-
tively in this monolingual scenario. The other is 
bilingual sequence significance. X and Y are sen-
tence pair and multi-word pairs respectively in 
this bilingual scenario. 
2.1.1 Monolingual Sequence Significance 
Given a sentence w1, ?, wn, where wi denotes 
unary word, monolingual sequence significance 
is defined as: 
1,1
,
,
+?
=
ji
ji
ji Freq
Freq
Sig   (3) 
where Freqi, j (i?j) represents frequency of word 
sequence wi, ?, wj in the corpus, Sigi, j  repre-
sents monolingual sequence significance of a 
word sequence wi, ?, wj. We also denote word 
sequence wi, ?, wj as span[i, j], whole sentence 
as span[1, n]. Each span is also a multi-word ex-
pression. 
Monolingual sequence significance of span[i, j] 
is proportional to span[i, j]?s frequency, while is 
inversely proportion to frequency of expanded 
span (span[i-1, j+1]). Such definition character-
izes minimal sequence of consecutive words 
which we are looking for. Our target is to find 
pseudo-word sequence which has maximal sum 
of spans? significances: kX kZ
  (1) 
149
k (4) ? == Kk spanspanK K SigARGMAXpw 11 1
where pw denotes pseudo-word, K is equal to or 
less than sentence?s length. spank is the kth span 
of K spans span1K. Equation (4) is the rewrite of 
equation (2) in monolingual scenario. Searching 
for pseudo-words pw1K is the same to finding 
optimal segmentation of a sentence into K seg-
ments span1K (K is a variable too). Details of 
searching algorithm are described in section 
2.2.1. 
We firstly search for monolingual pseudo-
words on source and target side individually. 
Then we apply word alignment techniques to 
build pseudo-word alignments. We argue that 
word alignment techniques will work fine if non-
existent word alignments in such as non-
compositional phrasal equivalences have been 
filtered by pseudo-words. 
2.1.2 Bilingual Sequence Significance 
Bilingual sequence significance is proposed to 
characterize pseudo-word pairs. Co-occurrence 
of sequences on both language sides is used to 
define bilingual sequence significance. Given a 
bilingual sequence pair: span-pair[is, js, it, jt] 
(source side span[is, js] and target side span[it, jt]), 
bilingual sequence significance is defined as be-
low: 
1
k
,1,1,1
,,,
,,,
+?+?
=
ttss
ttss
ttss
jiji
jiji
jiji Freq
Freq
Sig   (5) 
where Freq denotes the frequency of a span-pair. 
Bilingual sequence significance is an extension 
of monolingual sequence significance. Its value 
is proportional to frequency of span-pair[is, js, it, 
jt], while is inversely proportional to frequency 
of expanded span-pair[is-1, js+1, it-1, jt+1]. 
Pseudo-word pairs of one sentence pair are such 
pairs that maximize the sum of span-pairs? bilin-
gual sequence significances: 
? = ??= Kk pairspanpairspanK K SigARGMAXpwp 11 1  (6) 
pwp represents pseudo-word pair. Equation (6) is 
the rewrite of equation (2) in bilingual scenario. 
Searching for pseudo-word pairs pwp1K is equal 
to bilingual segmentation of a sentence pair into 
optimal span-pair1K. Details of searching algo-
rithm are presented in section 2.2.2. 
2.2 Algorithms of Searching for Pseudo-
words 
Pseudo-word searching problem is equal to de-
composition of a sentence into pseudo-words. 
But the number of possible decompositions of 
the sentence grows exponentially with the sen-
tence length in both monolingual scenario and 
bilingual scenario. By casting such decomposi-
tion problem into parsing framework, we can 
find pseudo-word sequence in polynomial time. 
According to the two scenarios, searching for 
pseudo-words can be performed in a monolin-
gual way and a synchronous way. Details of the 
two kinds of searching algorithms are described 
in the following two sections. 
2.2.1 Algorithm of Searching for Monolin-
gual Pseudo-words (SMP) 
Searching for monolingual pseudo-words is 
based on the computation of monolingual se-
quence significance. Figure 1 presents the search 
algorithm. It is performed in a way similar to 
CKY (Cocke-Kasami-Younger) parser. 
 
Initialization: Wi, i = Sigi, i; 
Wi, j = 0,  (i?j); 
1:  for d = 2 ? n do 
2:      for all i, j s.t. j-i=d-1 do 
3:          for k = i ? j ? 1 do 
4:              v = Wi, k + Wk+1, j
5:              if v > Wi, j then 
6:                  Wi, j = v; 
7:          u = Sigi, j
8:          if u > Wi, j then 
9:              Wi, j = u; 
Figure 1. Algorithm of searching for monolingual 
pseudo-words (SMP). 
 
In this algorithm, Wi, j records maximal sum of 
monolingual sequence significances of sub spans 
of span[i, j]. During initialization, Wi, i is initial-
ized as Sigi,i (note that this sequence is word wi 
only). For all spans that have more than one 
word (i?j), Wi, j is initialized as zero. 
In the main algorithm, d represents span?s 
length, ranging from 2 to n, i represents start po-
sition of a span, j represents end position of a 
span, k represents decomposition position of 
span[i,j]. For span[i, j], Wi, j is updated if higher 
sum of monolingual sequence significances is 
found. 
The algorithm is performed in a bottom-up 
way. Small span?s computation is first. After 
maximal sum of significances is found in small 
spans, big span?s computation, which uses small 
spans? maximal sum, is continued. Maximal sum 
of significances for whole sentence (W1,n, n is 
sentence?s length)  is guaranteed in this way, and 
optimal decomposition is obtained correspond-
ingly. 
150
The method of fitting the decomposition prob-
lem into CKY parsing framework is located at 
steps 7-9. After steps 3-6, all possible decompo-
sitions of span[i, j] are explored and Wi, j of op-
timal decomposition of span[i, j] is recorded. 
Then monolingual sequence significance Sigi,j of 
span[i, j] is computed at step 7, and it is com-
pared to Wi, j at step 8. Update of Wi, j is taken at 
step 9 if Sigi,j is bigger than Wi, j, which indicates 
that span[i, j] is non-decomposable. Thus 
whether span[i, j] should be non-decomposable 
or not is decided through steps 7-9. 
2.2.2 Algorithm of Synchronous Searching 
for Pseudo-words (SSP) 
Synchronous searching for pseudo-words utilizes 
bilingual sequence significance. Figure 2 pre-
sents the search algorithm. It is similar to ITG 
(Wu, 1997), except that it has no production 
rules and non-terminal nodes of a synchronous 
grammar. What it cares about is the span-pairs 
that maximize the sum of bilingual sequence sig-
nificances. 
 
Initialization:  if is = js or it = jt then 
ttssttss
ttss
jijijiji SigW ,,,,,, = ; 
                       else 
0,,, =jijiW ; 
1:  for ds = 2 ? ns, dt = 2 ? nt do 
2:      for all  is, js, it, jt s.t. js-is=ds-1 and jt-it=dt-1 do
3:             for ks = is ? js ? 1, kt = it ? jt ? 1 do 
4:                    v = max{ ,
ttssttss jkjkkiki
WW ,1,,1,,, +++
ttsst
tjiji ,,,
tj,,,
tj,,,
jiji ,,,
tss kijkjkki
WW ,,,1,1,, ++ + } 
5:                    if v > W  then 
tss
6:                           W = v; 
tss iji
7:              u =  
ttss jiji
Sig ,,,
8:              if u > W  then 
tss iji
9:                    W = u; 
ttss
Figure 2. Algorithm of Synchronous Searching for 
Pseudo-words(SSP). 
 
In the algorithm, records maximal 
sum of bilingual sequence significances of sub 
span-pairs of span-pair[i
ttss jiji
W ,,,
s, js, it, jt]. For 1-to-m 
span-pairs, Ws are initialized as bilingual se-
quence significances of such span-pairs. For 
other span-pairs, Ws are initialized as zero. 
In the main algorithm, ds/dt denotes the length 
of a span on source/target side, ranging from 2 to 
ns/nt (source/target sentence?s length). is/it is the 
start position of a span-pair on source/target side, 
js/jt is the end position of a span-pair on 
source/target side, ks/kt is the decomposition po-
sition of a span-pair[is, js, it, jt] on source/target 
side. 
Update steps in Figure 2 are similar to that of 
Figure 1, except that the update is about span-
pairs, not monolingual spans. Reversed and non-
reversed alignments inside a span-pair are com-
pared at step 4. For span-pair[is, js, it, jt], 
 is updated at step 6 if higher sum of 
bilingual sequence significances is found. 
ttss jiji
W ,,,
Fitting the bilingually searching for pseudo-
words into ITG framework is located at steps 7-9. 
Steps 3-6 have explored all possible decomposi-
tions of span-pair[is, js, it, jt] and have recorded 
maximal 
ttss
 of these decompositions. Then 
bilingual sequence significance of span-pair[i
jijiW ,,,
s, js, 
it, jt] is computed at step 7. It is compared to 
ttss
 at step 8. Update is taken at step 9 if 
bilingual sequence significance of span-pair[i
jijiW ,,,
s, js, 
it, jt] is bigger than 
ttss
, which indicates that 
span-pair[i
jijiW ,,,
s, js, it, jt] is non-decomposable. 
Whether the span-pair[is, js, it, jt] should be non-
decomposable  or not is decided through steps 7-
9. 
In addition to the initialization step, all span-
pairs? bilingual sequence significances are com-
puted. Maximal sum of bilingual sequence sig-
nificances for one sentence pair is guaranteed 
through this bottom-up way, and the optimal de-
composition of the sentence pair is obtained cor-
respondingly. 
z Algorithm of Excluded Synchronous 
Searching for Pseudo-words (ESSP) 
The algorithm of SSP in Figure 2 explores all 
span-pairs, but it neglects NULL alignments, 
where words and ?empty? word are aligned. In 
fact, SSP requires that all parts of a sentence pair 
should be aligned. This requirement is too strong 
because NULL alignments are very common in 
many language pairs. In SSP, words that should 
be aligned to ?empty? word are programmed to 
be aligned to real words. 
Unlike most word alignment methods (Och 
and Ney, 2003) that add ?empty? word to ac-
count for NULL alignment entries, we propose a 
method to naturally exclude such NULL align-
ments. We call this method as Excluded Syn-
chronous Searching for Pseudo-words (ESSP). 
The main difference between ESSP and SSP is 
in steps 3-6 in Figure 3. We illustrate Figure 3?s 
span-pair configuration in Figure 4. 
151
 
Initialization:  if is = js or it = jt then 
ttssttss jijijiji ,,,,,,
,,, jijiW
SigW = ; 
                       else 
0=
ttss
; 
1:  for ds = 2 ? ns, dt = 2 ? nt do 
2:        for all  is, js, it, jt s.t. js-is=ds-1 and jt-it=dt-1 do 
3:              for ks1=is+1 ? js, ks2=ks1-1 ? js-1 
kt1=it+1 ? jt, kt2=kt1-1 ? jt-1 do 
4:                    v = max{W ,
ttssttss jkjkkiki
W ,1,,11,,1, 2211 ++?? +
1,,,1,1, 122 ?++ + ttsstt kijkjk W
tt j,,,
tj,,,
Sig
tt ji ,,,
ttss jiji ,,,
1, 1?ss kiW }
5:                    if v > W  then 
ss iji
6:                           W = v; 
tss iji
7:               u =  
ttss jiji ,,,
8:               if u > W  then 
ss ji
9:                    W = u; 
Figure 3. Algorithm of Excluded Synchronous 
Searching for Pseudo-words (ESSP). 
 
The solid boxes in Figure 4 represent excluded 
parts of span-pair[is, js, it, jt] in ESSP. Note that, 
in SSP, there is no excluded part, that is, ks1=ks2 
and kt1=kt2. 
We can see that in Figure 4, each monolingual 
span is configured into three parts, for example: 
span[is, ks1-1], span[ks1, ks2] and span[ks2+1, js] 
on source language side. ks1 and ks2 are two new 
variables gliding between is and js, span[ks1, ks2] 
is source side excluded part of span-pair[is, js, it, 
jt]. Bilingual sequence significance is computed 
only on pairs of blank boxes, solid boxes are ex-
cluded in this computation to represent NULL 
alignment cases. 
 
 
Figure 4. Illustration of excluded configuration. 
 
Note that, in Figure 4, solid box on either lan-
guage side can be void (i.e., length is zero) if 
there is no NULL alignment on its side. If all 
solid boxes are shrunk into void, algorithm of 
ESSP is the same to SSP. 
Generally, span length of NULL alignment is 
not very long, so we can set a length threshold 
for NULL alignments, eg. ks2-ks1?EL, where EL 
denotes Excluded Length threshold. Computa-
tional complexity of the ESSP remains the same 
to SSP?s complexity O(ns3.nt3), except multiply a 
constant EL2. 
There is one kind of NULL alignments that 
ESSP can not consider. Since we limit excluded 
parts in the middle of a span-pair, the algorithm 
will end without considering boundary parts of a 
sentence pair as NULL alignments. 
3 Experiments and Results 
In our experiments, pseudo-words are fed into 
PB-SMT pipeline. The pipeline uses GIZA++ 
model 4 (Brown et al, 1993; Och and Ney, 2003) 
for pseudo-word alignment, uses Moses (Koehn 
et al, 2007) as phrase-based decoder, uses the 
SRI Language Modeling Toolkit to train lan-
guage model with modified Kneser-Ney smooth-
ing (Kneser and Ney 1995; Chen and Goodman 
1998). Note that MERT (Och, 2003) is still on 
original words of target language. In our experi-
ments, pseudo-word length is limited to no more 
than six unary words on both sides of the lan-
guage pair. 
We conduct experiments on Chinese-to-
English machine translation. Two data sets are 
adopted, one is small corpus of IWSLT-2008 
BTEC task of spoken language translation in 
travel domain (Paul, 2008), the other is large 
corpus in news domain, which consists Hong 
Kong News (LDC2004T08), Sinorama Magazine 
(LDC2005T10), FBIS (LDC2003E14), Xinhua 
(LDC2002E18), Chinese News Translation 
(LDC2005T06), Chinese Treebank 
(LDC2003E07), Multiple Translation Chinese 
(LDC2004T07). Table 1 lists statistics of the 
corpus used in these experiments. 
is ks1 ks2 js
it kt1 kt2 jt
is ks1 ks2 js
it kt1 kt2 jt
a) non-reversed 
b) reversed 
 
small large  
Ch ? En Ch ? En 
Sent. 23k 1,239k 
word 190k 213k 31.7m 35.5m
ASL 8.3 9.2 25.6 28.6 
Table 1. Statistics of corpora, ?Ch? denotes Chinese, 
?En? denotes English, ?Sent.? row is the number of 
sentence pairs, ?word? row is the number of words, 
?ASL? denotes average sentence length. 
 
152
For small corpus, we use CSTAR03 as devel-
opment set, use IWSLT08 official test set for test. 
A 5-gram language model is trained on English 
side of parallel corpus. For large corpus, we use 
NIST02 as development set, use NIST03 as test 
set. Xinhua portion of the English Gigaword3 
corpus is used together with English side of large 
corpus to train a 4-gram language model. 
Experimental results are evaluated by case-
insensitive BLEU-4 (Papineni et al, 2001). 
Closest reference sentence length is used for 
brevity penalty. Additionally, NIST score (Dod-
dington, 2002) and METEOR (Banerjee and La-
vie, 2005) are also used to check the consistency 
of experimental results. Statistical significance in 
BLEU score differences was tested by paired 
bootstrap re-sampling (Koehn, 2004). 
3.1 Baseline Performance 
Our baseline system feeds word into PB-SMT 
pipeline. We use GIZA++ model 4 for word 
alignment, use Moses for phrase-based decoding. 
The setting of language model order for each 
corpus is not changed. Baseline performances on 
test sets of small corpus and large corpus are re-
ported in table 2. 
 
 small Large 
BLEU 0.4029 0.3146 
NIST 7.0419 8.8462 
METEOR 0.5785 0.5335 
Table 2. Baseline performances on test sets of small 
corpus and large corpus. 
3.2 Pseudo-word Unpacking 
Because pseudo-word is a kind of multi-word 
expression, it has inborn advantage of higher 
language model order and longer max phrase 
length over unary word. To see if such inborn 
advantage is the main contribution to the per-
formance or not, we unpack pseudo-word into 
words after GIZA++ aligning. Aligned pseudo-
words are unpacked into m?n word alignments. 
PB-SMT pipeline is executed thereafter. The ad-
vantage of longer max phrase length is removed 
during phrase extraction, and the advantage of 
higher order of language model is also removed 
during decoding since we use language model 
trained on unary words. Performances of pseudo-
word unpacking are reported in section 3.3.1 and 
3.4.1. Ma and Way (2009) used the unpacking 
after phrase extraction, then re-estimated phrase 
translation probability and lexical reordering 
model. The advantage of longer max phrase 
length is still used in their method. 
3.3 Pseudo-word Performances on Small 
Corpus 
Table 3 presents performances of SMP, SSP, 
ESSP on small data set. pwchpwen denotes that 
pseudo-words are on both language side of train-
ing data, and they are input strings during devel-
opment and testing, and translations are also 
pseudo-words, which will be converted to words 
as final output. wchpwen/pwchwen denotes that 
pseudo-words are adopted only on Eng-
lish/Chinese side of the data set. 
We can see from table 3 that, ESSP attains the 
best performance, while SSP attains the worst 
performance. This shows that excluding NULL 
alignments in synchronous searching for pseudo-
words is effective. SSP puts overly strong align-
ment constraints on parallel corpus, which im-
pacts performance dramatically. ESSP is superior 
to SMP indicating that bilingually motivated 
searching for pseudo-words is more effective. 
Both SMP and ESSP outperform baseline consis-
tently in BLEU, NIST and METEOR. 
There is a common phenomenon among SMP, 
SSP and ESSP. wchpwen always performs better 
than the other two cases. It seems that Chinese 
word prefers to have English pseudo-word 
equivalence which has more than or equal to one 
word. pwchpwen in ESSP performs similar to the 
baseline, which reflects that our direct pseudo-
word pairs do not work very well with GIZA++ 
alignments. Such disagreement is weakened by 
using pseudo-words on only one language side 
(wchpwen or pwchwen), while the advantage of 
pseudo-words is still leveraged in the alignments. 
Best ESSP (wchpwen) is significantly better 
than baseline (p<0.01) in BLEU score, best SMP 
(wchpwen) is significantly better than baseline 
(p<0.05) in BLEU score. This indicates that 
pseudo-words, through either monolingual 
searching or synchronous searching, are more 
effective than words as to being basic transla-
tional units. 
Figure 5 illustrates examples of pseudo-words 
of one Chinese-to-English sentence pair. Gold 
standard word alignments are shown at the bot-
tom of figure 5. We can see that ?front desk? is 
recognized as one pseudo-word in ESSP. Be-
cause SMP performs monolingually, it can not 
consider ???? and ?front desk? simultaneously. 
SMP only detects frequent monolingual multi-
words as pseudo-words. SSP has a strong con-
straint that all parts of a sentence pair should be 
aligned, so source sentence and target sentence 
have same length after merging words into 
153
 Table 3. Performance of using pseudo-words on small data. 
 
pseudo-words. We can see that too many pseudo-
words are detected by SSP. 
 
 
Figure 5. Outputs of the three algorithms ESSP, 
SMP and SSP on one sentence pair and gold standard 
word alignments. Words in one pseudo-word are con-
catenated by ?_?. 
 
3.3.1 Pseudo-word Unpacking Perform-
ances on Small Corpus 
We test pseudo-word unpacking in ESSP. Table 
4 presents its performances on small corpus. 
 
unpackingESSP 
pwchpwen wchpwen pwchwen
baseline
BLEU 0.4097 0.4182 0.4031 0.4029
NIST 7.5547 7.2893 7.2670 7.0419
METEOR 0.5951 0.5874 0.5846 0.5785
Table 4. Performances of pseudo-word unpacking on 
small corpus. 
 
We can see that pseudo-word unpacking sig-
nificantly outperforms baseline. wchpwen is sig-
nificantly better than baseline (p<0.04) in BLEU 
score. Unpacked pseudo-word performs com-
paratively with pseudo-word without unpacking. 
There is no statistical difference between them. It 
shows that the improvement derives from 
pseudo-word itself as basic translational unit, 
does not rely very much on higher language 
model order or longer max phrase length setting. 
3.4 Pseudo-word Performances on Large 
Corpus 
Table 5 lists the performance of using pseudo-
words on large corpus. We apply SMP on this 
task. ESSP is not applied because of its high 
computational complexity. Table 5 shows that all 
three configurations (pwchpwen, wchpwen, pwchwen) 
of SMP outperform the baseline. If we go back to 
the definition of sequence significance, we can 
see that it is a data-driven definition that utilizes 
corpus frequencies. Corpus scale has an influ-
ence on computation of sequence significance in 
long sentences which appear frequently in news 
domain. SMP benefits from large corpus, and 
wchpwen is significantly better than baseline 
(p<0.01). Similar to performances on small cor-
pus, wchpwen always performs better than the 
other two cases, which indicates that Chinese 
word prefers to have English pseudo-word 
equivalence which has more than or equal to one 
word. 
 
SMP  
pwchpwen wchpwen pwchwen
baseline
BLEU 0.3185 0.3230 0.3166 0.3146
NIST 8.9216 9.0447 8.9210 8.8462
METEOR 0.5402 0.5489 0.5435 0.5335
Table 5. Performance of using pseudo-words on large 
corpus. 
3.4.1 Pseudo-word Unpacking Perform-
ances on Large Corpus 
Table 6 presents pseudo-word unpacking per-
formances on large corpus. All three configura-
tions improve performance over baseline after 
pseudo-word unpacking. pwchpwen attains the 
best BLEU among the three configurations, and 
is significantly better than baseline (p<0.03). 
wchpwen is also significantly better than baseline 
(p<0.04). By comparing table 6 with table 5, we 
can see that unpacked pseudo-word performs 
comparatively with pseudo-word without un-
packing. There is no statistical difference be-
SMP SSP ESSP  
pwchpwen wchpwen pwchwen pwchpwen wchpwen pwchwen pwchpwen wchpwen pwchwen
baseline
BLEU 0.3996 0.4155 0.4024 0.3184 0.3661 0.3552 0.3998 0.4229 0.4147 0.4029
NIST 7.4711 7.6452 7.6186 6.4099 6.9284 6.8012 7.1665 7.4373 7.4235 7.0419
METEOR 0.5900 0.6008 0.6000 0.5255 0.5569 0.5454 0.5739 0.5963 0.5891 0.5785
??  ?  ??  ?  ?  ??  ? 
The guy at the front desk is pretty rude . 
??  ?  ??  ?  ?  ??  ? 
The guy_at the front_desk is pretty_rude . 
??  ?  ??  ?  ?  ??  ? 
The guy at the front_desk is pretty rude . 
ESSP
??   ?    ??    ?    ?     ??    ? 
 
 
The guy at the front desk is pretty rude  .
Gold standard word alignments 
SMP
SSP 
154
tween them. It shows that the improvement de-
rives from pseudo-word itself as basic transla-
tional unit, does not rely very much on higher 
language model order or longer max phrase 
length setting. In fact, slight improvement in 
pwchpwen and pwchwen is seen after pseudo-word 
unpacking, which indicates that higher language 
model order and longer max phrase length im-
pact the performance in these two configurations. 
 
UnpackingSMP 
pwchpwen wchpwen pwchwen
Baseline
BLEU 0.3219 0.3192 0.3187 0.3146 
NIST 8.9458 8.9325 8.9801 8.8462 
METEOR 0.5429 0.5424 0.5411 0.5335 
Table 6. Performance of pseudo-word unpacking on 
large corpus. 
3.5 Comparison to English Chunking 
English chunking is experimented to compare 
with pseudo-word. We use FlexCRFs (Xuan-
Hieu Phan et al, 2005) to get English chunks. 
Since there is no standard Chinese chunking data 
and code, only English chunking is executed. 
The experimental results show that English 
chunking performs far below baseline, usually 8 
absolute BLEU points below. It shows that sim-
ple chunks are not suitable for being basic trans-
lational units. 
4 Conclusion 
We have presented pseudo-word as a novel ma-
chine translational unit for phrase-based machine 
translation. It is proposed to replace too fine-
grained word as basic translational unit. Pseudo-
word is a kind of basic multi-word expression 
that characterizes minimal sequence of consecu-
tive words in sense of translation. By casting 
pseudo-word searching problem into a parsing 
framework, we search for pseudo-words in poly-
nomial time. Experimental results of Chinese-to-
English translation task show that, in phrase-
based machine translation model, pseudo-word 
performs significantly better than word in both 
spoken language translation domain and news 
domain. Removing the power of higher order 
language model and longer max phrase length, 
which are inherent in pseudo-words, shows that 
pseudo-words still improve translational per-
formance significantly over unary words. 
References  
S. Banerjee, and A. Lavie. 2005. METEOR: An 
automatic metric for MT evaluation with im-
proved correlation with human judgments. In 
Proceedings of the ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for Machine Trans-
lation and/or Summarization (ACL?05). 65?72. 
P. Blunsom, T. Cohn, C. Dyer, M. Osborne. 2009. A 
Gibbs Sampler for Phrasal Synchronous 
Grammar Induction. In Proceedings of ACL-
IJCNLP, Singapore. 
P. Blunsom, T. Cohn, M. Osborne. 2008. Bayesian 
synchronous grammar induction. In Proceed-
ings of NIPS 21, Vancouver, Canada. 
P. Brown, S. Della Pietra, V. Della Pietra, and R. 
Mercer. 1993. The mathematics of machine 
translation: Parameter estimation. Computa-
tional Linguistics, 19:263?312. 
P.-C. Chang, M. Galley, and C. D. Manning. 2008. 
Optimizing Chinese word segmentation for 
machine translation performance. In Proceed-
ings of the 3rd Workshop on Statistical Machine 
Translation (SMT?08). 224?232. 
Chen, Stanley F. and Joshua Goodman. 1998. An 
empirical study of smoothing techniques for 
language modeling. Technical Report TR-10-98, 
Harvard University Center for Research in Com-
puting Technology. 
C. Cherry, D. Lin. 2007. Inversion transduction 
grammar for joint phrasal translation model-
ing. In Proc. of the HLTNAACL Workshop on 
Syntax and Structure in Statistical Translation 
(SSST 2007), Rochester, USA. 
D. Chiang. 2007. Hierarchical phrase-based 
translation.Computational Linguistics, 33(2):201?
228. 
Y. Deng and W. Byrne. 2005. HMM word and 
phrase alignment for statistical machine trans-
lation. In Proc. of HLT-EMNLP, pages 169?176. 
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram cooc-
currence statistics. In Proceedings of the 2nd In-
ternational Conference on Human Language Tech-
nology (HLT?02). 138?145. 
Kneser, Reinhard and Hermann Ney. 1995. Improved 
backing-off for M-gram language modeling. In 
Proceedings of the IEEE International Conference 
on Acoustics, Speech, and Signal Processing, 
pages 181?184, Detroit, MI. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan,W. Shen, C. 
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, 
E. Herbst. 2007. Moses: Open source toolkit for 
statistical machine translation. In Proc. of the 
155
45th Annual Meeting of the ACL (ACL-2007), 
Prague. 
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical 
phrasebased translation. In Proc. of the 3rd In-
ternational conference on Human Language Tech-
nology Research and 4th Annual Meeting of the 
NAACL (HLT-NAACL 2003), 81?88, Edmonton, 
Canada. 
P. Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proceed-
ings of EMNLP. 
P. Lambert and R. Banchs. 2005. Data Inferred 
Multi-word Expressions for Statistical Ma-
chine Translation. In Proceedings of MT Summit 
X. 
Y. Ma, N. Stroppa, and A. Way. 2007. Bootstrap-
ping word alignment via word packing. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL?07). 
304?311. 
Y. Ma, and A. Way. 2009. Bilingually Motivated 
Word Segmentation for Statistical Machine 
Translation. In ACM Transactions on Asian Lan-
guage Information Processing, 8(2). 
D. Marcu,W.Wong. 2002. A phrase-based, joint 
probability model for statistical machine 
translation. In Proc. of the 2002 Conference on 
Empirical Methods in Natural Language Process-
ing (EMNLP-2002), 133?139, Philadelphia. Asso-
ciation for Computational Linguistics. 
F. J. Och. 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL, 
pages 160?167. 
F. J. Och and H. Ney. 2003. A systematic compari-
son of various statistical alignment models. 
Computational Linguistics, 29(1):19?51. 
Xuan-Hieu Phan, Le-Minh Nguyen, and Cam-Tu 
Nguyen. 2005. FlexCRFs: Flexible Conditional 
Random Field Toolkit, http://flexcrfs.sourceforge. 
net 
K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu: 
a method for automatic evaluation of machine 
translation, 2001. 
M. Paul, 2008. Overview of the IWSLT 2008 
evaluation campaign. In Proc. of Internationa 
Workshop on Spoken Language Translation, 20-21 
October 2008. 
A. Stolcke. (2002). SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of 
ICSLP, Denver, Colorado. 
D. Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel 
corpora. Computational Linguistics, 23(3):377?
403. 
J. Xu, Zens., and H. Ney. 2004. Do we need Chi-
nese word segmentation for statistical ma-
chine translation? In Proceedings of the ACL 
Workshop on Chinese Language Processing 
SIGHAN?04). 122?128. 
J. Xu, J. Gao, K. Toutanova, and H. Ney. 2008. 
Bayesian semi-supervised chinese word seg-
mentation for statistical machine translation. 
In Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING?08). 
1017?1024. 
H. Zhang, C. Quirk, R. C. Moore, D. Gildea. 2008. 
Bayesian learning of non-compositional 
phrases with synchronous parsing. In Proc. of 
the 46th Annual Conference of the Association for 
Computational Linguistics: Human Language 
Technologies (ACL-08:HLT), 97?105, Columbus, 
Ohio. 
R. Zhang, K. Yasuda, and E. Sumita. 2008. Improved 
statistical machine translation by multiple 
Chinese word segmentation. In Proceedings of 
the 3rd Workshop on Statistical Machine Transla-
tion (SMT?08). 216?223. 
 
156
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 604?611,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Error Detection for Statistical Machine
Translation Using Linguistic Features
Deyi Xiong, Min Zhang, Haizhou Li
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632.
{dyxiong, mzhang, hli}@i2r.a-star.edu.sg
Abstract
Automatic error detection is desired in
the post-processing to improve machine
translation quality. The previous work is
largely based on confidence estimation us-
ing system-based features, such as word
posterior probabilities calculated from N -
best lists or word lattices. We propose to
incorporate two groups of linguistic fea-
tures, which convey information from out-
side machine translation systems, into er-
ror detection: lexical and syntactic fea-
tures. We use a maximum entropy clas-
sifier to predict translation errors by inte-
grating word posterior probability feature
and linguistic features. The experimen-
tal results show that 1) linguistic features
alone outperform word posterior probabil-
ity based confidence estimation in error
detection; and 2) linguistic features can
further provide complementary informa-
tion when combined with word confidence
scores, which collectively reduce the clas-
sification error rate by 18.52% and im-
prove the F measure by 16.37%.
1 Introduction
Translation hypotheses generated by a statistical
machine translation (SMT) system always contain
both correct parts (e.g. words, n-grams, phrases
matched with reference translations) and incor-
rect parts. Automatically distinguishing incorrect
parts from correct parts is therefore very desir-
able not only for post-editing and interactive ma-
chine translation (Ueffing and Ney, 2007) but also
for SMT itself: either by rescoring hypotheses in
the N -best list using the probability of correct-
ness calculated for each hypothesis (Zens and Ney,
2006) or by generating new hypotheses using N -
best lists from one SMT system or multiple sys-
tems (Akibay et al, 2004; Jayaraman and Lavie,
2005).
In this paper we restrict the ?parts? to words.
That is, we detect errors at the word level for SMT.
A common approach to SMT error detection at the
word level is calculating the confidence at which a
word is correct. The majority of word confidence
estimation methods follows three steps:
1) Calculate features that express the correct-
ness of words either based on SMT model
(e.g. translation/language model) or based on
SMT system output (e.g. N -best lists, word
lattices) (Blatz et al, 2003; Ueffing and Ney,
2007).
2) Combine these features together with a clas-
sification model such as multi-layer percep-
tron (Blatz et al, 2003), Naive Bayes (Blatz
et al, 2003; Sanchis et al, 2007), or log-
linear model (Ueffing and Ney, 2007).
3) Divide words into two groups (correct trans-
lations and errors) by using a classification
threshold optimized on a development set.
Sometimes the step 2) is not necessary if only one
effective feature is used (Ueffing and Ney, 2007);
and sometimes the step 2) and 3) can be merged
into a single step if we directly output predicting
results from binary classifiers instead of making
thresholding decision.
Various features from different SMT models
and system outputs are investigated (Blatz et al,
2003; Ueffing and Ney, 2007; Sanchis et al, 2007;
Raybaud et al, 2009). Experimental results show
that they are useful for error detection. However,
it is not adequate to just use these features as dis-
cussed in (Shi and Zhou, 2005) because the infor-
mation that they carry is either from the inner com-
ponents of SMT systems or from system outputs.
To some extent, it has already been considered by
SMT systems. Hence finding external information
604
sources from outside SMT systems is desired for
error detection.
Linguistic knowledge is exactly such a good
choice as an external information source. It has al-
ready been proven effective in error detection for
speech recognition (Shi and Zhou, 2005). How-
ever, it is not widely used in SMT error detection.
The reason is probably that people have yet to find
effective linguistic features that outperform non-
linguistic features such as word posterior proba-
bility features (Blatz et al, 2003; Raybaud et al,
2009). In this paper, we would like to show an
effective use of linguistic features in SMT error
detection.
We integrate two sets of linguistic features into
a maximum entropy (MaxEnt) model and develop
aMaxEnt-based binary classifier to predict the cat-
egory (correct or incorrect) for each word in a
generated target sentence. Our experimental re-
sults show that linguistic features substantially im-
prove error detection and even outperform word
posterior probability features. Further, they can
produce additional improvements when combined
with word posterior probability features.
The rest of the paper is organized as follows. In
Section 2, we review the previous work on word-
level confidence estimation which is used for error
detection. In Section 3, we introduce our linguistic
features as well as the word posterior probability
feature. In Section 4, we elaborate our MaxEnt-
based error detection model which combine lin-
guistic features and word posterior probability fea-
ture together. In Section 5, we describe the SMT
system which we use to generate translation hy-
potheses. We report our experimental results in
Section 6 and conclude in Section 7.
2 Related Work
In this section, we present an overview of confi-
dence estimation (CE) for machine translation at
the word level. As we are only interested in error
detection, we focus on work that uses confidence
estimation approaches to detect translation errors.
Of course, confidence estimation is not limited to
the application of error detection, it can also be
used in other scenarios, such as translation predic-
tion in an interactive environment (Grandrabur and
Foster, 2003) .
In a JHU workshop, Blatz et al (2003) investi-
gate using neural networks and a naive Bayes clas-
sifier to combine various confidence features for
confidence estimation at the word level as well as
at the sentence level. The features they use for
word level CE include word posterior probabil-
ities estimated from N -best lists, features based
on SMT models, semantic features extracted from
WordNet as well as simple syntactic features, i.e.
parentheses and quotation mark check. Among all
these features, the word posterior probability is the
most effective feature, which is much better than
linguistic features such as semantic features, ac-
cording to their final results.
Ueffing and Ney (2007) exhaustively explore
various word-level confidence measures to label
each word in a generated translation hypothe-
sis as correct or incorrect. All their measures
are based on word posterior probabilities, which
are estimated from 1) system output, such as
word lattices or N -best lists and 2) word or
phrase translation table. Their experimental re-
sults show that word posterior probabilities di-
rectly estimated from phrase translation table are
better than those from system output except for the
Chinese-English language pair.
Sanchis et al (2007) adopt a smoothed naive
Bayes model to combine different word posterior
probability based confidence features which are
estimated from N -best lists, similar to (Ueffing
and Ney, 2007).
Raybaud et al (2009) study several confi-
dence features based on mutual information be-
tween words and n-gram and backward n-gram
language model for word-level and sentence-level
CE. They also explore linguistic features using in-
formation from syntactic category, tense, gender
and so on. Unfortunately, such linguistic features
neither improve performance at the word level nor
at the sentence level.
Our work departs from the previous work in two
major respects.
? We exploit various linguistic features and
show that they are able to produce larger im-
provements than widely used system-related
features such as word posterior probabilities.
This is in contrast to some previous work. Yet
another advantage of using linguistic features
is that they are system-independent, which
therefore can be used across different sys-
tems.
? We treat error detection as a complete bi-
nary classification problem. Hence we di-
605
rectly output prediction results from our dis-
criminatively trained classifier without opti-
mizing a classification threshold on a distinct
development set beforehand.1 Most previous
approaches make decisions based on a pre-
tuned classification threshold ? as follows
class =
{
correct, ?(correct, ?) > ?
incorrect, otherwise
where ? is a classifier or a confidence mea-
sure and ? is the parameter set of ?. The per-
formance of these approaches is strongly de-
pendent on the classification threshold.
3 Features
We explore two sets of linguistic features for each
word in a machine generated translation hypoth-
esis. The first set of linguistic features are sim-
ple lexical features. The second set of linguistic
features are syntactic features which are extracted
from link grammar parse. To compare with the
previously widely used features, we also investi-
gate features based on word posterior probabili-
ties.
3.1 Lexical Features
We use the following lexical features.
? wd: word itself
? pos: part-of-speech tag from a tagger trained
on WSJ corpus. 2
For each word, we look at previous n
words/tags and next n words/tags. They together
form a word/tag sequence pattern. The basic idea
of using these features is that words in rare pat-
terns are more likely to be incorrect than words
in frequently occurring patterns. To some extent,
these two features have similar function to a tar-
get language model or pos-based target language
model.
3.2 Syntactic Features
High-level linguistic knowledge such as syntac-
tic information about a word is a very natural and
promising indicator to decide whether this word is
syntactically correct or not. Words occurring in an
1This does not mean we do not need a development set.
We do validate our feature selection and other experimental
settings on the development set.
2Available via http://www-tsujii.is.s.u-tokyo.ac.jp/
?tsuruoka/postagger/
ungrammatical part of a target sentence are prone
to be incorrect. The challenge of using syntac-
tic knowledge for error detection is that machine-
generated hypotheses are rarely fully grammati-
cal. They are mixed with grammatical and un-
grammatical parts, which hence are not friendly
to traditional parsers trained on grammatical sen-
tences because ungrammatical parts of a machine-
generated sentence could lead to a parsing failure.
To overcome this challenge, we select the Link
Grammar (LG) parser 3 as our syntactic parser to
generate syntactic features. The LG parser pro-
duces a set of labeled links which connect pairs of
words with a link grammar (Sleator and Temper-
ley, 1993).
The main reason why we choose the LG parser
is that it provides a robustness feature: null-link
scheme. The null-link scheme allows the parser to
parse a sentence even when the parser can not fully
interpret the entire sentence (e.g. including un-
grammatical parts). When the parser fail to parse
the entire sentence, it ignores one word each time
until it finds linkages for remaining words. After
parsing, those ignored words are not connected to
any other words. We call them null-linked words.
Our hypothesis is that null-linked words are
prone to be syntactically incorrect. We hence
straightforwardly define a syntactic feature for a
word w according to its links as follows
link(w) =
{
yes, w has links
no, otherwise
In Figure 1 we show an example of a generated
translation hypothesis with its link parse. Here
links are denoted with dotted lines which are an-
notated with link types (e.g., Jp, Op). Bracketed
words, namely ?,? and ?including?, are null-linked
words.
3.3 Word Posterior Probability Features
Our word posterior probability is calculated onN -
best list, which is first proposed by (Ueffing et al,
2003) and widely used in (Blatz et al, 2003; Ueff-
ing and Ney, 2007; Sanchis et al, 2007).
Given a source sentence f , let {en}N1 be theN -
best list generated by an SMT system, and let ein is
the i-th word in en. The major work of calculating
word posterior probabilities is to find the Leven-
shtein alignment (Levenshtein, 1966) between the
best hypothesis e1 and its competing hypothesis
3Available at http://www.link.cs.cmu.edu/link/
606
Figure 1: An example of Link Grammar parsing results.
en in the N -best list {en}N1 . We denote the align-
ment between them as ?(e1, en). The word in the
hypothesis en which ei1 is Levenshtein aligned to
is denoted as ?i(e1, en).
The word posterior probability of ei1 is then cal-
culated by summing up the probabilities over all
hypotheses containing ei1 in a position which is
Levenshtein aligned to ei1.
pwpp(ei1) =
?
en: ?i(e1,en)=ei1
p(en)
?N
1 p(en)
To use the word posterior probability in our er-
ror detection model, we need to make it discrete.
We introduce a feature for a word w based on its
word posterior probability as follows
dwpp(w) = ??log(pwpp(w))/df?
where df is the discrete factor which can be set to
1, 0.1, 0.01 and so on. ?? ?? is a rounding oper-
ator which takes the largest integer that does not
exceed ?log(pwpp(w))/df . We optimize the dis-
crete factor on our development set and find the
optimal value is 1. Therefore a feature ?dwpp =
2? represents that the logarithm of the word poste-
rior probability is between -3 and -2;
4 Error Detection with a Maximum
Entropy Model
As mentioned before, we consider error detec-
tion as a binary classification task. To formal-
ize this task, we use a feature vector ? to rep-
resent a word w in question, and a binary vari-
able c to indicate whether this word is correct or
not. In the feature vector, we look at 2 words
before and 2 words after the current word posi-
tion (w?2, w?1, w, w1, w2). We collect features
{wd, pos, link, dwpp} for each word among these
words and combine them into the feature vector
? for w. As such, we want the feature vector to
capture the contextual environment, e.g., pos se-
quence pattern, syntactic pattern, where the word
w occurs.
For classification, we employ the maximum
entropy model (Berger et al, 1996) to predict
whether a word w is correct or incorrect given its
feature vector ?.
p(c|?) = exp(
?
i ?ifi(c, ?))
?
c? exp(
?
i ?ifi(c?, ?))
where fi is a binary model feature defined on c
and the feature vector ?. ?i is the weight of fi.
Table 1 shows some examples of our binary model
features.
In order to learn the model feature weights ? for
probability estimation, we need a training set of
m samples {?i, ci}m1 . The challenge of collect-
ing training instances is that the correctness of a
word in a generated translation hypothesis is not
intuitively clear (Ueffing and Ney, 2007). We will
describe the method to determine the correctness
of a word in Section 6.1, which is broadly adopted
in previous work.
We tune our model feature weights using an
off-the-shelf MaxEnt toolkit (Zhang, 2004). To
avoid overfitting, we optimize the Gaussian prior
on the development set. During test, if the proba-
bility p(correct|?) is larger than p(incorrect|?)
according the trained MaxEnt model, the word is
labeled as correct otherwise incorrect.
5 SMT System
To obtain machine-generated translation hypothe-
ses for our error detection, we use a state-of-the-art
phrase-based machine translation system MOSES
(Koehn et al, 2003; Koehn et al, 2007). The
translation task is on the official NIST Chinese-
to-English evaluation data. The training data con-
sists of more than 4 million pairs of sentences (in-
cluding 101.93MChinese words and 112.78M En-
glish words) from LDC distributed corpora. Table
2 shows the corpora that we use for the translation
task.
We build a four-gram language model using the
SRILM toolkit (Stolcke, 2002), which is trained
607
Feature Example
wd f(c, ?) =
{
1, ?.w.wd = ?.?, c = correct
0, otherwise
pos f(c, ?) =
{
1, ?.w2.pos = ?NN?, c = incorrect
0, otherwise
link f(c, ?) =
{
1, ?.w.link = no, c = incorrect
0, otherwise
dwpp f(c, ?) =
{
1, ?.w?2.dwpp = 2, c = correct
0, otherwise
Table 1: Examples of model features.
LDC ID Description
LDC2004E12 United Nations
LDC2004T08 Hong Kong News
LDC2005T10 Sinorama Magazine
LDC2003E14 FBIS
LDC2002E18 Xinhua News V1 beta
LDC2005T06 Chinese News Translation
LDC2003E07 Chinese Treebank
LDC2004T07 Multiple Translation Chinese
Table 2: Training corpora for the translation task.
on Xinhua section of the English Gigaword cor-
pus (181.1M words). For minimum error rate tun-
ing (Och, 2003), we use NIST MT-02 as the de-
velopment set for the translation task. In order
to calculate word posterior probabilities, we gen-
erate 10,000 best lists for NIST MT-02/03/05 re-
spectively. The performance, in terms of BLEU
(Papineni et al, 2002) score, is shown in Table 4.
6 Experiments
We conducted our experiments at several levels.
Starting with MaxEnt models with single linguis-
tic feature or word posterior probability based fea-
ture, we incorporated additional features incre-
mentally by combining features together. In do-
ing so, we would like the experimental results not
only to display the effectiveness of linguistic fea-
tures for error detection but also to identify the ad-
ditional contribution of each feature to the task.
6.1 Data Corpus
For the error detection task, we use the best trans-
lation hypotheses of NIST MT-02/05/03 generated
by MOSES as our training, development, and test
corpus respectively. The statistics about these cor-
pora is shown in Table 3. Each translation hypoth-
esis has four reference translations.
Corpus Sentences Words
Training MT-02 878 24,225
Development MT-05 1082 31,321
Test MT-03 919 25,619
Table 3: Corpus statistics (number of sentences
and words) for the error detection task.
To obtain the linkage information, we run the
LG parser on all translation hypotheses. We find
that the LG parser can not fully parse 560 sen-
tences (63.8%) in the training set (MT-02), 731
sentences (67.6%) in the development set (MT-05)
and 660 sentences (71.8%) in the test set (MT-03).
For these sentences, the LG parser will use the the
null-link scheme to generate null-linked words.
To determine the true class of a word in a gen-
erated translation hypothesis, we follow (Blatz et
al., 2003) to use the word error rate (WER). We
tag a word as correct if it is aligned to itself in
the Levenshtein alignment between the hypothesis
and the nearest reference translation that has min-
imum edit distance to the hypothesis among four
reference translations. Figure 2 shows the Lev-
enshtein alignment between a machine-generated
hypothesis and its nearest reference translation.
The ?Class? row shows the label of each word ac-
cording to the alignment, where ?c? and ?i? repre-
sent correct and incorrect respectively.
There are several other metrics to tag single
words in a translation hypothesis as correct or in-
correct, such as PER where a word is tagged as
correct if it occurs in one of reference translations
with the same number of occurrences, Setwhich is
a less strict variant of PER, ignoring the number of
occurrences per word. In Figure 2, the two words
?last year? in the hypothesis will be tagged as cor-
rect if we use the PER or Set metric since they do
not consider the occurring positions of words. Our
608
China Unicom net profit rose up 38% last year
China Unicom net profit rose up 38%last yearHypothesis
Reference
China/c Unicom/c net/c profit/c rose/c up/c 38%/clast/i year/iClass
Figure 2: Tagging a word as correct/incorrect according to the Levenshtein alignment.
Corpus BLEU (%) RCW (%)
MT-02 33.24 47.76
MT-05 32.03 47.85
MT-03 32.86 47.57
Table 4: Case-insensitive BLEU score and ratio
of correct words (RCW) on the training, develop-
ment and test corpus.
metric corresponds to the m-WER used in (Ueff-
ing and Ney, 2007), which is stricter than PER and
Set. It is also stricter than normal WER metric
which compares each hypothesis to all references,
rather than the nearest reference.
Table 4 shows the case-insensitive BLEU score
and the percentage of words that are labeled as cor-
rect according to the method described above on
the training, development and test corpus.
6.2 Evaluation Metrics
To evaluate the overall performance of the error
detection, we use the commonly used metric, clas-
sification error rate (CER) to evaluate our classi-
fiers. CER is defined as the percentage of words
that are wrongly tagged as follows
CER = # of wrongly tagged words
Total # of words
The baseline CER is determined by assuming
the most frequent class for all words. Since the ra-
tio of correct words in both the development and
test set is lower than 50%, the most frequent class
is ?incorrect?. Hence the baseline CER in our ex-
periments is equal to the ratio of correct words as
these words are wrongly tagged as incorrect.
We also use precision and recall on errors to
evaluate the performance of error detection. Let
ng be the number of words of which the true class
is incorrect, nt be the number of words which are
tagged as incorrect by classifiers, and nm be the
number of words tagged as incorrect that are in-
deed translation errors. The precision Pre is the
percentage of words correctly tagged as transla-
tion errors.
Pre = nm
nt
The recall Rec is the proportion of actual transla-
tion errors that are found by classifiers.
Rec = nm
ng
F measure, the trade-off between precision and re-
call, is also used.
F = 2 ? Pre?Rec
Pre+Rec
6.3 Experimental Results
Table 5 shows the performance of our experiments
on the error detection task. To compare with pre-
vious work using word posterior probabilities for
confidence estimation, we carried out experiments
using wpp estimated from N -best lists with the
classification threshold ? , which was optimized on
our development set to minimize CER. A relative
improvement of 9.27% is achieved over the base-
line CER, which reconfirms the effectiveness of
word posterior probabilities for error detection.
We conducted three groups of experiments us-
ing the MaxEnt based error detection model with
various feature combinations.
? The first group of experiments uses single
feature, such as dwpp, pos. We find the
most effective feature is pos, which achieves
a 16.12% relative improvement over the base-
line CER and 7.55% relative improvement
over the CER of word posterior probabil-
ity thresholding. Using discrete word pos-
terior probabilities as features in the Max-
Ent based error detection model is marginally
better than word posterior probability thresh-
olding in terms of CER, but obtains a 13.79%
relative improvement in F measure. The syn-
tactic feature link also improves the error de-
tection in terms of CER and particularly re-
call.
609
Combination Features CER (%) Pre (%) Rec (%) F (%)
Baseline - 47.57 - - -
Thresholding wpp - 43.16 58.98 58.07 58.52
MaxEnt (dwpp) 44 43.07 56.12 81.86 66.59
MaxEnt (wd) 19,164 41.57 58.25 73.11 64.84
MaxEnt (pos) 199 39.90 58.88 79.23 67.55
MaxEnt (link) 19 44.31 54.72 89.72 67.98
MaxEnt (wd+ pos) 19,363 39.43 59.36 78.60 67.64
MaxEnt (wd+ pos+ link) 19,382 39.79 58.74 80.97 68.08
MaxEnt (dwpp+ wd) 19,208 41.04 57.18 83.75 67.96
MaxEnt (dwpp+ wd+ pos) 19,407 38.88 59.87 78.38 67.88
MaxEnt (dwpp+ wd+ pos+ link) 19,426 38.76 59.89 78.94 68.10
Table 5: Performance of the error detection task.
? The second group of experiments concerns
with the combination of linguistic features
without word posterior probability feature.
The combination of lexical features improves
both CER and precision over single lexical
feature (wd, pos). The addition of syntactic
feature link marginally undermines CER but
improves recall by a lot.
? The last group of experiments concerns about
the additional contribution of linguistic fea-
tures to error detection with word posterior
probability. We added linguistic features in-
crementally into the feature pool. The best
performance was achieved by using all fea-
tures, which has a relative of improvement of
18.52% over the baseline CER.
The first two groups of experiments show that
linguistic features, individually (except for link)
or by combination, are able to produce much better
performance than word posterior probability fea-
tures in both CER and F measure. The best com-
bination of linguistic features achieves a relative
improvement of 8.64% and 15.58% in CER and
F measure respectively over word posterior prob-
ability thresholding.
The Table 5 also reveals how linguistic fea-
tures improve error detection. The lexical features
(pos, wd) improve precision when they are used.
This suggests that lexical features can help the sys-
tem find errors more accurately. Syntactic features
(link), on the other hand, improve recall whenever
they are used, which indicates that they can help
the system find more errors.
We also show the number of features in each
combination in Table 5. Except for the wd feature,
0 200 400 600 800 1000
38.6
38.8
39.0
39.2
39.4
39.6
39.8
40.0
40.2
40.4
40.6
 

 
CE
R 
(%
)
Number of Training Sentences
Figure 3: CER vs. the number of training sen-
tences.
the pos has the largest number of features, 199,
which is a small set of features. This suggests that
our error detection model can be learned from a
rather small training set.
Figure 3 shows CERs for the feature combina-
tion MaxEnt (dwpp + wd + pos + link) when
the number of training sentences is enlarged incre-
mentally. CERs drop significantly when the num-
ber of training sentences is increased from 100 to
500. After 500 sentences are used, CERs change
marginally and tend to converge.
7 Conclusions and Future Work
In this paper, we have presented a maximum en-
tropy based approach to automatically detect er-
rors in translation hypotheses generated by SMT
610
systems. We incorporate two sets of linguistic
features together with word posterior probability
based features into error detection.
Our experiments validate that linguistic features
are very useful for error detection: 1) they by
themselves achieve a higher improvement in terms
of both CER and F measure than word posterior
probability features; 2) the performance is further
improved when they are combined with word pos-
terior probability features.
The extracted linguistic features are quite com-
pact, which can be learned from a small train-
ing set. Furthermore, The learned linguistic fea-
tures are system-independent. Therefore our ap-
proach can be used for other machine translation
systems, such as rule-based or example-based sys-
tem, which generally do not produce N -best lists.
Future work in this direction involve detect-
ing particular error types such as incorrect po-
sitions, inappropriate/unnecessary words (Elliott,
2006) and automatically correcting errors.
References
Yasuhiro Akibay, Eiichiro Sumitay, Hiromi Nakaiway,
Seiichi Yamamotoy, and Hiroshi G. Okunoz. 2004.
Using a Mixture of N-best Lists from Multiple MT
Systems in Rank-sum-based Confidence Measure
for MT Outputs. In Proceedings of COLING.
Adam L. Berger, Stephen A. Della Pietra andVincent
J. Della Pietra. 1996. A Maximum Entropy Ap-
proach to Natural Language Processing. Computa-
tional Linguistics, 22(1): 39-71.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, Nicola Ueffing. 2003. Confidence estima-
tion for machine translation. final report, jhu/clsp
summer workshop.
Debra Elliott. 2006 Corpus-based Machine Transla-
tion Evaluation via Automated Error Detection in
Output Texts. Phd Thesis, University of Leeds.
Simona Gandrabur and George Foster. 2003. Confi-
dence Estimation for Translation Prediction. In Pro-
ceedings of HLT-NAACL.
S. Jayaraman and A. Lavie. 2005. Multi-engine Ma-
chine Translation Guided by Explicit Word Match-
ing. In Proceedings of EAMT.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of HLT-NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constrantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL, Demonstration Session.
V. I. Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. Soviet
Physics Doklady, Feb.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: aMethod for Automatically
Evaluation of Machine Translation. In Proceedings
of ACL 2002.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
Kamel Sma??li. 2009. Word- and Sentence-level
Confidence Measures for Machine Translation. In
Proceedings of EAMT 2009.
Alberto Sanchis, Alfons Juan and Enrique Vidal. 2007.
Estimation of Confidence Measures for Machine
Translation. In Procedings of Machine Translation
Summit XI.
Daniel Sleator and Davy Temperley. 1993. Parsing En-
glish with a Link Grammar. In Proceedings of Third
International Workshop on Parsing Technologies.
Yongmei Shi and Lina Zhou. 2005. Error Detec-
tion Using Linguistic Features. In Proceedings of
HLT/EMNLP 2005.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Nicola Ueffing, Klaus Macherey, and Hermann Ney.
2003. Confidence Measures for Statistical Machine
Translation. In Proceedings. of MT Summit IX.
Nicola Ueffing and Hermann Ney. 2007. Word-
Level Confidence Estimation for Machine Transla-
tion. Computational Linguistics, 33(1):9-40.
Richard Zens and Hermann Ney. 2006. N-gram Pos-
terior Probabilities for Statistical Machine Transla-
tion. In HLT/NAACL: Proceedings of the Workshop
on Statistical Machine Translation.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
611
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 875?885,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Convolution Kernel over Packed Parse Forest 
 
 
Min Zhang     Hui Zhang    Haizhou Li 
Institute for Infocomm Research 
A-STAR, Singapore 
{mzhang,vishz,hli}@i2r.a-star.edu.sg 
 
  
 
Abstract 
This paper proposes a convolution forest ker-
nel to effectively explore rich structured fea-
tures embedded in a packed parse forest. As 
opposed to the convolution tree kernel, the 
proposed forest kernel does not have to com-
mit to a single best parse tree, is thus able to 
explore very large object spaces and much 
more structured features embedded in a forest. 
This makes the proposed kernel more robust 
against parsing errors and data sparseness is-
sues than the convolution tree kernel. The pa-
per presents the formal definition of convolu-
tion forest kernel and also illustrates the com-
puting algorithm to fast compute the proposed 
convolution forest kernel. Experimental results 
on two NLP applications, relation extraction 
and semantic role labeling, show that the pro-
posed forest kernel significantly outperforms 
the baseline of the convolution tree kernel. 
1 Introduction 
Parse tree and packed forest of parse trees are 
two widely used data structures to represent the 
syntactic structure information of sentences in 
natural language processing (NLP). The struc-
tured features embedded in a parse tree have 
been well explored together with different ma-
chine learning algorithms and proven very useful 
in many NLP applications (Collins and Duffy, 
2002; Moschitti, 2004; Zhang et al, 2007). A 
forest (Tomita, 1987) compactly encodes an ex-
ponential number of parse trees. In this paper, we 
study how to effectively explore structured fea-
tures embedded in a forest using convolution 
kernel (Haussler, 1999). 
As we know, feature-based machine learning 
methods are less effective in modeling highly 
structured objects (Vapnik, 1998), such as parse 
tree or semantic graph in NLP. This is due to the 
fact that it is usually very hard to represent struc-
tured objects using vectors of reasonable dimen-
sions without losing too much information. For 
example, it is computationally infeasible to enu-
merate all subtree features (using subtree a fea-
ture) for a parse tree into a linear feature vector. 
Kernel-based machine learning method is a good 
way to overcome this problem. Kernel methods 
employ a kernel function, that must satisfy the 
properties of being symmetric and positive, to 
measure the similarity between two objects by 
computing implicitly the dot product of certain 
features of the input objects in high (or even in-
finite) dimensional feature spaces without enu-
merating all the features (Vapnik, 1998).  
Many learning algorithms, such as SVM 
(Vapnik, 1998), the Perceptron learning algo-
rithm (Rosenblatt, 1962) and Voted Perceptron 
(Freund and Schapire, 1999), can work directly 
with kernels by replacing the dot product with a 
particular kernel function. This nice property of 
kernel methods, that implicitly calculates the dot 
product in a high-dimensional space over the 
original representations of objects, has made 
kernel methods an effective solution to modeling 
structured objects in NLP. 
In the context of parse tree, convolution tree 
kernel (Collins and Duffy, 2002) defines a fea-
ture space consisting of all subtree types of parse 
trees and counts the number of common subtrees 
as the syntactic similarity between two parse 
trees. The tree kernel has shown much success in 
many NLP applications like parsing (Collins and 
Duffy, 2002), semantic role labeling (Moschitti, 
2004; Zhang et al, 2007), relation extraction 
(Zhang et al, 2006), pronoun resolution (Yang et 
al., 2006), question classification (Zhang and 
Lee, 2003) and machine translation (Zhang and 
Li, 2009), where the tree kernel is used to com-
pute the similarity between two NLP application 
instances that are usually represented by parse 
trees. However, in those studies, the tree kernel 
only covers the features derived from single 1-
875
best parse tree. This may largely compromise the 
performance of tree kernel due to parsing errors 
and data sparseness. 
To address the above issues, this paper con-
structs a forest-based convolution kernel to mine 
structured features directly from packed forest. A 
packet forest compactly encodes exponential 
number of n-best parse trees, and thus containing 
much more rich structured features than a single 
parse tree. This advantage enables the forest ker-
nel not only to be more robust against parsing 
errors, but also to be able to learn more reliable 
feature values and help to solve the data sparse-
ness issue that exists in the traditional tree kernel. 
We evaluate the proposed kernel in two real NLP 
applications, relation extraction and semantic 
role labeling. Experimental results on the 
benchmark data show that the forest kernel sig-
nificantly outperforms the tree kernel. 
The rest of the paper is organized as follows. 
Section 2 reviews the convolution tree kernel 
while section 3 discusses the proposed forest 
kernel in details. Experimental results are re-
ported in section 4. Finally, we conclude the pa-
per in section 5. 
 
2 Convolution Kernel over Parse Tree 
Convolution kernel was proposed as a concept of 
kernels for discrete structures by Haussler (1999) 
and related but independently conceived ideas on 
string kernels first presented in (Watkins, 1999). 
The framework defines the kernel function be-
tween input objects as the convolution of ?sub-
kernels?, i.e. the kernels for the decompositions 
(parts) of the input objects.  
The parse tree kernel (Collins and Duffy, 2002) 
is an instantiation of convolution kernel over 
syntactic parse trees. Given a parse tree, its fea-
tures defined by a tree kernel are all of its subtree 
types and the value of a given feature is the 
number of the occurrences of the subtree in the 
parse tree. Fig. 1 illustrates a parse tree with all 
of its 11 subtree features covered by the convolu-
tion tree kernel. In the tree kernel, a parse tree T  
is represented by a vector of integer counts of 
each subtree type (i.e., subtree regardless of its 
ancestors, descendants and span covered):  
 
( )T? ? (# subtreetype1(T), ?, # subtreetypen(T))         
 
where # subtreetypei(T) is the occurrence number 
of the ith subtree type in T. The tree kernel counts 
the number of common subtrees as the syntactic 
similarity between two parse trees. Since the 
number of subtrees is exponential with the tree 
size, it is computationally infeasible to directly 
use the feature vector ( )T? . To solve this com-
putational issue, Collins and Duffy (2002) pro-
posed the following tree kernel to calculate the 
dot product between the above high dimensional 
vectors implicitly. 
 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
1 2
( , ) ( ), ( )
   # ( ) # ( )
   ( ) ( )
   ( , )
i i
i i
i
subtree subtree
i n N n N
n N n N
K T T T T
subtreetype T subtreetype T
I n I n
n n
? ?
? ?
? ?
?? ?
?
? ? ? ?
? ? ? ? ?
? ? ? ?
? ?
?
?
?
? ? ?
? ?
 
 
where N1 and N2 are the sets of nodes in trees T1 
and T2, respectively, and ( )isubtreeI n
 is a function 
that is 1 iff the subtreetypei occurs with root at 
node n and zero otherwise, and 
1 2( , )n n?  is the 
number of the common subtrees rooted at n1 and 
n2, i.e., 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? ? ??  
 
1 2( , )n n? can be computed by the following recur-
sive rules:  
IN
in the bank
DT NN
PP
IN
the bank
DT NN
PP
IN
in bank
DT NN
PP
IN
in the
DT NN
PP
IN
in
DT NN
PP
IN
the
DT
PP
NN IN
bank
DT NN
PP
IN DT NN
PP
IN
in
the bank
DT NN
IN
in the bank
DT NN
PP
 
Figure 1. A parse tree and its 11 subtree features covered by convolution tree kernel 
876
Rule 1: if the productions (CFG rules) at 
1n  and 
2n  are different, 1 2( , ) 0n n? ? ; 
 
Rule 2: else if both
1n  and 2n  are pre-terminals 
(POS tags), 
1 2( , ) 1n n ?? ? ? ; 
 
Rule 3: else,  
 
1( )
1 2 1 2
1
( , ) (1 ( ( , ), ( , )))
nc n
j
n n ch n j ch n j?
?
? ? ? ? ??
,  
 
where 
1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and? (0<? ?1) is the de-
cay factor in order to make the kernel value less 
variable with respect to the subtree sizes (Collins 
and Duffy, 2002). The recursive Rule 3 holds 
because given two nodes with the same children, 
one can construct common subtrees using these 
children and common subtrees of further 
offspring. The time complexity for computing 
this kernel is
1 2(| | | |)O N N? . 
As discussed in previous section, when convo-
lution tree kernel is applied to NLP applications, 
its performance is vulnerable to the errors from 
the single parse tree and data sparseness. In this 
paper, we present a convolution kernel over 
packed forest to address the above issues by ex-
ploring structured features embedded in a forest. 
3 Convolution Kernel over Forest 
In this section, we first illustrate the concept of 
packed forest and then give a detailed discussion 
on the covered feature space, fractional count, 
feature value and the forest kernel function itself. 
3.1 Packed forest of parse trees 
Informally, a packed parse forest, or (packed) 
forest in short, is a compact representation of all 
the derivations (i.e. parse trees) for a given sen-
tence under context-free grammar (Tomita, 1987; 
Billot and Lang, 1989; Klein and Manning, 
2001). It is the core data structure used in natural 
language parsing and other downstream NLP 
applications, such as syntax-based machine 
translation (Zhang et al, 2008; Zhang et al, 
2009a). In parsing, a sentence corresponds to 
exponential number of parse trees with different 
tree probabilities, where a forest can compact all 
the parse trees by sharing their common subtrees 
in a bottom-up manner. Formally, a packed for-
est ? can be described as a triple: 
 
? = < ?,?, ? > 
 
where  ?is the set of non-terminal nodes, ? is the 
set of hyper-edges and ?  is a sentence 
 
NNP[1,1] VV[2,2] NN[4,4] IN[5,5]
John saw a man
NP[3,4]
in the bank
DT[3,3] DT[6,6]
NN[7,7]
PP[5,7]
VP[2,4] NP[3,7]
VP[2,7]
IP[1,7]
NNP VV NN     IN     DT   NN
John saw a man in the bank
DT
VP
NP
VP
IP
PP
NNP VV NN     IN     DT   NN
John saw a man in the bank
DT
NP
NP
VP
IP
PP
IP[1,7]
VP[2,7]
NNP[1,1]
a) A Forest f
b) A Hyper-edge e
c) A Parse Tree T1
d) A Parse Tree T2
 
 
Figure 2. An example of a packed forest, a hyper-edge and two parse trees covered by the packed forest 
 
877
represented as an ordered word sequence. A hy-
per-edge ?  is a group of edges in a parse tree 
which connects a father node and its all child 
nodes, representing a CFG rule. A non-terminal 
node in a forest is represented as a ?label [start, 
end]?, where the ?label? is its syntax category 
and ?[start, end]? is the span of words it covers. 
As shown in Fig. 2, these two parse trees (?1 
and ?2) can be represented as a single forest by 
sharing their common subtrees (such as NP[3,4] 
and PP[5,7]) and merging common non-terminal 
nodes covering the same span (such as VP[2,7], 
where there are two hyper-edges attach to it). 
Given the definition of forest, we introduce 
the concepts of inside probability ? .   and out-
side probability ?(. )  that are widely-used in 
parsing (Baker, 1979; Lari and Young, 1990) and 
are also to be used in our kernel calculation. 
 
? ? ?,?  = ?(? ? ?[?]) 
 
? ? ?, ?  =   
 
 
 
 ? ? 
?  ??  ?  ????? ?????  
????? ???  ??  ?
?  ?(??[?? , ??])
?? ?? ,?? ??  ?  ????  
????  ??  ?  
 
 
 
? ????(?) = 1       
? ? ?, ?  =   
 
 
 
 
? ???? ?    ? ? ? 
?  ??  ?  ????? ?  
????  ???  ? 
??  ???  ???
 ????  ????
?   ?(??[?? , ??]))
?? ?? ,?? ??  ?  
????????  ????  
??  ? ??????  ?
 
 
where ? is a forest node, ?[?] is the ???  word of 
input sentence ?, ?(? ? ?[?]) is the probability 
of the CFG rule ? ? ?[?] , ????(. )  returns the 
root node of input structure, [?? , ??] is a sub-span 
of  ?, ? , being covered by ?? , and ? ? is the 
PCFG probability of ? . From these definitions, 
we can see that the inside probability is total 
probability of generating words ? ?, ?  from 
non-terminal node ? ?, ?  while the outside 
probability is the total probability of generating 
node ? ?, ?  and words outside ?[?, ?] from the 
root of forest. The inside probability can be cal-
culated using dynamic programming in a bottom-
up fashion while the outside probability can be 
calculated using dynamic programming in a top-
to-down way. 
3.2 Convolution forest kernel 
In this subsection, we first define the feature 
space covered by forest kernel, and then define 
the forest kernel function. 
3.2.1 Feature space, object space and fea-
ture value 
The forest kernel counts the number of common 
subtrees as the syntactic similarity between two 
forests. Therefore, in the same way as tree kernel, 
its feature space is also defined as all the possible 
subtree types that a CFG grammar allows. In a 
forest kernel, forest ? is represented by a vector 
of fractional counts of each subtree type (subtree 
regardless of its ancestors, descendants and span 
covered):  
 
( )F? ? (# subtreetype1(F), ?,  
              # subtreetypen(F)) 
   = (#subtreetype1(n-best parse trees), ?,   (1) 
      # subtreetypen(n-best parse trees))  
 
where # subtreetypei(F) is the occurrence number 
of the ith subtree type (subtreetypei) in forest F, 
i.e., a n-best parse tree lists with a huge n.  
Although the feature spaces of the two kernels 
are the same, their object spaces (tree vs. forest) 
and feature values (integer counts vs. fractional 
counts) differ very much. A forest encodes expo-
nential number of parse trees, and thus contain-
ing exponential times more subtrees than a single 
parse tree. This ensures forest kernel to learn 
more reliable feature values and is also able to 
help to address the data sparseness issues in a 
better way than tree kernel does. Forest kernel is 
also expected to yield more non-zero feature val-
ues than tree kernel. Furthermore, different parse 
tree in a forest represents different derivation and 
interpretation for a given sentence. Therefore, 
forest kernel should be more robust to parsing 
errors than tree kernel. 
In tree kernel, one occurrence of a subtree 
contributes 1 to the value of its corresponding 
feature (subtree type), so the feature value is an 
integer count. However, the case turns out very 
complicated in forest kernel. In a forest, each of 
its parse trees, when enumerated, has its own 
878
probability. So one subtree extracted from differ-
ent parse trees should have different fractional 
count with regard to the probabilities of different 
parse trees. Following the previous work (Char-
niak and Johnson, 2005; Huang, 2008), we de-
fine the fractional count of the occurrence of a 
subtree in a parse tree ??  as  
 
? ???????, ?? =  
0                      ?? ??????? ? ??  
? ???????, ??|?, ?   ?????????
  
 
                           =  
0                         ?? ??????? ? ??  
? ??|?, ?                    ?????????
  
 
where we have ? ???????, ??|?, ? = ? ??|?, ?  if 
??????? ? ?? . Then we define the fractional count 
of the occurrence of a subtree in a forest f as 
 
 ? ???????,? = ? ???????|?, ?  
                            =   ? ???????, ?? |?, ? ??         (2) 
                            =   ????????  ?? ? ? ??|?, ? ??   
 
where ????????  ??  is a binary function that is 1 
iif the ??????? ? ??  and zero otherwise. Ob-
viously, it needs exponential time to compute the 
above fractional counts. However, due to the 
property of forest that compactly represents all 
the parse trees, the posterior probability of a 
subtree in a forest, ? ???????|?, ? , can be easi-
ly computed in an Inside-Outside fashion as the 
product of three parts: the outside probability of 
its root node, the probabilities of parse hyper-
edges involved in the subtree, and the inside 
probabilities of its leaf nodes (Lari and Young, 
1990; Mi and Huang, 2008).  
 
? ???????,? = ? ???????|?, ?             (3) 
 
=
??(???????)
??(???? ? )
     
where 
 
?? ??????? = ? ???? ???????         (4) 
?  ? ? 
?????????
           
?  ? ? 
??????  ???????  
            
and 
 
        ?? ???? ?  = ? ???? ?  ? ? ???? ?   
       = ? ???? ?   
 
where ? .   and ?(. ) denote the outside and in-
side probabilities. They can be easily obtained 
using the equations introduced at section 3.1.  
Given a subtree, we can easily compute its 
fractional count (i.e. its feature value) directly 
using eq. (3) and (4) without the need of enume-
rating each parse trees as shown at eq. (2) 1 .  
Nonetheless, it is still computationally infeasible 
to directly use the feature vector ?(?) (see eq. 
(1)) by explicitly enumerating all subtrees  al-
though its fractional count is easily calculated. In 
the next subsection, we present the forest kernel 
that implicitly calculates the dot-product between 
two ?(?)s in a polynomial time. 
3.2.2 Convolution forest kernel 
The forest kernel counts the fractional numbers 
of common subtrees as the syntactic similarity 
between two forests. We define the forest kernel 
function ?? ?1 ,?2  in the following way. 
 
   ?? ?1 ,?2 =< ? ?1 ,? ?2 >                       (5) 
  = #????????????(?1). #????????????(?2)
?
 
  =      ???  ???????1, ???????2 
??????? 1??1
??????? 2??2
? ? ???????1,?1 
? ? ???????2,?2                 
   =   ?? ?1 ,?2  ?2??2?1??1   
 
where 
? ???  ?,?  is a binary function that is 1 iif 
the input two subtrees are identical (i.e. 
they have the same typology and node 
labels) and zero otherwise; 
? ? ?,?  is the fractional count defined at 
eq. (3); 
? ?1  and ?2  are the sets of nodes in fo-
rests ?1 and ?2; 
? ?? ?1,?2  returns the accumulated value 
of products between each two fractional 
counts of the common subtrees rooted at 
?1 and ?2, i.e.,  
 
?? ?1,?2  
=      ???  ???????1, ???????2 
????  ??????? 1 =?1
????  ??????? 2 =?2
? ? ???????1,?1     
? ? ???????2,?2                 
                                                 
1  It has been proven in parsing literatures (Baker, 
1979; Lari and Young, 1990) that eq. (3) defined by 
Inside-Outside probabilities is exactly to compute the 
sum of those parse tree probabilities that cover the 
subtree of being considered as defined at eq. (2). 
879
We next show that ?? ?1 ,?2  can be computed 
recursively in a polynomial time as illustrated at 
Algorithm 1. To facilitate discussion, we tempo-
rarily ignore all fractional counts in Algorithm 1. 
Indeed, Algorithm 1 can be viewed as a natural 
extension of convolution kernel from over tree to 
over forest. In forest2, a node can root multiple 
hyper-edges and each hyper-edge is independent 
to each other. Therefore, Algorithm 1 iterates 
each hyper-edge pairs with roots at ?1  and ?2 
(line 3-4), and sums over (eq. (7) at line 9) each 
recursively-accumulated sub-kernel scores of 
subtree pairs extended from the hyper-edge pair 
 ?1 , ?2  (eq. (6) at line 8). Eq. (7) holds because 
the hyper-edges attached to the same node are 
independent to each other. Eq. (6) is very similar 
to the Rule 3 of tree kernel (see section 2) except 
its inputs are hyper-edges and its further expan-
sion is based on forest nodes. Similar to tree ker-
nel (Collins and Duffy, 2002), eq. (6) holds be-
cause a common subtree by extending from 
(?1 , ?2) can be formed by taking the hyper-edge 
(?1 , ?2), together with a choice at each of their 
leaf nodes of simply taking the non-terminal at 
the leaf node, or any one of the common subtrees 
with root at the leaf node. Thus there are 
 1 + ?? ???? ?1 , ? , ???? ?2 , ?   possible 
choices at the jth leaf node. In total, there are 
???  ?1 , ?2  (eq. (6)) common subtrees by extend-
ing from (?1 , ?2)  and ?
? ?1,?2  (eq. (7)) com-
mon subtrees with root at  ?1 ,?2 .  
Obviously ?? ?1 ,?2  calculated by Algorithm 
1 is a proper convolution kernel since it simply 
counts the number of common subtrees under the 
root  ?1 ,?2 . Therefore, ?? ?1,?2  defined at eq. 
(5) and calculated through ?? ?1,?2  is also a 
proper convolution kernel. From eq. (5) and Al-
gorithm 1, we can see that each hyper-edge pair 
(?1 , ?2) is only visited at most one time in com-
puting the forest kernel. Thus the time complexi-
ty for computing ?? ?1,?2  is ?(|?1| ? |?2|) , 
where ?1  and ?2 are the set of hyper-edges in 
forests ?1  and ?2 , respectively. Given a forest 
and the best parse trees, the number of hyper-
edges is only several times (normally <=3 after 
pruning) than that of tree nodes in the parse tree3. 
                                                 
2 Tree can be viewed as a special case of forest with 
only one hyper-edge attached to each tree node. 
3 Suppose there are K forest nodes in a forest, each 
node has M associated hyper-edges fan out and each 
hyper-edge has N children. Then the forest is capable 
of encoding ?
??1
??1  parse trees at most (Zhang et al, 
2009b). 
Same as tree kernel, forest kernel is running 
more efficiently in practice since only two nodes 
with the same label needs to be further processed 
(line 2 of Algorithm 1). 
Now let us see how to integrate fractional 
counts into forest kernel. According to Algo-
rithm 1 (eq. (7)), we have (?1/?2  are attached to 
?1/?2, respectively) 
 
?? ?1, ?2 =  ?
??  ?1, ?2 ?1=?2   
 
Recall eq. (4), a fractional count consists of 
outside, inside and subtree probabilities. It is 
more straightforward to incorporate the outside 
and subtree probabilities since all the subtrees 
with roots at  ?1 , ?2  share the same outside 
probability and each hyper-edge pair is only vi-
sited one time. Thus we can integrate the two 
probabilities into ?? ?1,?2  as follows. 
 
     ?? ?1,?2 = ? ? ? ?1 ? ? ?2  
        ?   ? ?1 ? ? ?2 ? ?
??  ?1, ?2  ?1=?2    (8) 
 
where, following tree kernel, a decay factor 
?(0 < ? ? 1) is also introduced in order to make 
the kernel value less variable with respect to the 
subtree sizes (Collins and Duffy, 2002). It func-
tions like multiplying each feature value by 
????? ? , where ?????  is the number of hyper-edges 
in ???????? . 
Algorithm 1.  
Input:  
        ?1 ,?2: two packed forests 
        ?1 ,?2: any two nodes of ?1 and ?2 
Notation:  
    ???  ?,? : defined at eq. (5) 
   ?? ?1 : number of leaf node of ?1    ???? ?1 , ? : the jth leaf node of ?1 
Output:  ?? ?1,?2  
 
1. ?? ?1,?2 = 0 
2. if  ?1 . ????? ? ?2 . ?????  exit 
3. for each hyper-edge ?1 attached to ?1 do 
4.      for each hyper-edge ?2 attached to ?2 do 
5.           if ???  ?1, ?2 == 0 do 
6.                 goto line 3 
7.           else do 
8.                  ???  ?1 , ?2  =    1 +
??  ?1 
?=1
                        ?? ???? ?1 , ? , ???? ?2 , ?     (6) 
9.                   ?? ?1,?2  +=  ?
??  ?1, ?2            (7) 
10.            end if  
11.       end for 
12. end for 
 
880
The inside probability is only involved when a 
node does not need to be further expanded. The 
integer 1 at eq. (6) represents such case. So the 
inside probability is integrated into eq. (6) by 
replacing the integer 1 as follows. 
  
 ???  ?1, ?2 =   ? ???? ?1, ?   ? ? ???? ?2, ?  
??  ?1 
?=1
+
 ?? ???? ?1 , ? , ???? ?2, ?  
? ???? ?1 , ?  ? ? ???? ?2, ?  
  (9) 
 
where in the last expression the two outside 
probabilities ? ???? ?1 , ?   and ? ???? ?2 , ?   
are removed. This is because  ???? ?1 , ? and 
???? ?2 , ?  are not roots of the subtrees of being 
explored (only outside probabilities of the root of 
a subtree should be counted in its fractional 
count), and  ?? ???? ?1 , ? , ???? ?2 , ?   already 
contains the two outside probabilities of 
???? ?1 , ?  and ???? ?2 , ? . 
Referring to eq. (3), each fractional count 
needs to be normalized by ??(???? ? ). Since 
??(???? ? ) is independent to each individual 
fractional count, we do the normalization outside 
the recursive function ???  ?1 , ?2 . Then we can 
re-formulize eq. (5) as 
 
     ?? ?1,?2 =< ? ?1 ,? ?2 >  
=
   ?? ?1,?2  ?2??2?1??1  
?? ???? ?1  ? ?? ???? ?2  
   (10) 
 
Finally, since the size of input forests is not 
constant, the forest kernel value is normalized 
using the following equation.  
 
        ? ? ?1,?2 =
   ?? ?1,?2 
 ?? ?1,?1 ? ?? ?2,?2 
     (11) 
 
From the above discussion, we can see that the 
proposed forest kernel is defined together by eqs. 
(11), (10), (9) and (8). Thanks to the compact 
representation of trees in forest and the recursive 
nature of the kernel function, the introduction of 
fractional counts and normalization do not 
change the convolution property and the time 
complexity of the forest kernel. Therefore, the 
forest kernel ? ? ?1 ,?2  is still a proper convolu-
tion kernel with quadratic time complexity. 
3.3 Comparison with previous work 
To the best of our knowledge, this is the first 
work to address convolution kernel over packed 
parse forest. 
Convolution tree kernel is a special case of the 
proposed forest kernel. From feature exploration 
viewpoint, although theoretically they explore 
the same subtree feature spaces (defined recur-
sively by CFG parsing rules), their feature values 
are different. Forest encodes exponential number 
of trees. So the number of subtree instances ex-
tracted from a forest is exponential number of 
times greater than that from its corresponding 
parse tree. The significant difference of the 
amount of subtree instances makes the parame-
ters learned from forests more reliable and also 
can help to address the data sparseness issue. To 
some degree, forest kernel can be viewed as a 
tree kernel with very powerful back-off mechan-
ism. In addition, forest kernel is much more ro-
bust against parsing errors than tree kernel. 
Aiolli et al (2006; 2007) propose using Direct 
Acyclic Graphs (DAG) as a compact representa-
tion of tree kernel-based models. This can largely 
reduce the computational burden and storage re-
quirements by sharing the common structures 
and feature vectors in the kernel-based model. 
There are a few other previous works done by 
generalizing convolution tree kernels (Kashima 
and Koyanagi, 2003; Moschitti, 2006; Zhang et 
al., 2007). However, all of these works limit 
themselves to single tree structure from modeling 
viewpoint in nature. 
From a broad viewpoint, as suggested by one 
reviewer of the paper, we can consider the forest 
kernel as an alternative solution proposed for the 
general problem of noisy inference pipelines (eg. 
speech translation by composition of FSTs, ma-
chine translation by translating over 'lattices' of 
segmentations (Dyer  et al, 2008) or using parse 
tree info for downstream applications in our cas-
es) . Following this line, Bunescu (2008) and 
Finkel et al (2006) are two typical related works 
done in reducing cascading noisy. However, our 
works are not overlapped with each other as 
there are two totally different solutions for the 
same general problem. In addition, the main mo-
tivation of this paper is also different from theirs. 
4 Experiments 
Forest kernel has a broad application potential in 
NLP. In this section, we verify the effectiveness 
of the forest kernel on two NLP applications, 
semantic role labeling (SRL) (Gildea, 2002) and 
relation extraction (RE) (ACE, 2002-2006). 
In our experiments, SVM (Vapnik, 1998) is 
selected as our classifier and the one vs. others 
strategy is adopted to select the one with the 
881
largest margin as the final answer. In our imple-
mentation, we use the binary SVMLight (Joa-
chims, 1998) and borrow the framework of the 
Tree Kernel Tools (Moschitti, 2004) to integrate 
our forest kernel into the SVMLight. We modify 
Charniak parser (Charniak, 2001) to output a 
packed forest. Following previous forest-based 
studies (Charniak and Johnson, 2005), we use the 
marginal probabilities of hyper-edges (i.e., the 
Viterbi-style inside-outside probabilities and set 
the pruning threshold as 8) for forest pruning. 
4.1 Semantic role labeling 
Given a sentence and each predicate (either a 
target verb or a noun), SRL recognizes and maps 
all the constituents in the sentence into their cor-
responding semantic arguments (roles, e.g., A0 
for Agent, A1 for Patient ?) of the predicate or 
non-argument. We use the CoNLL-2005 shared 
task on Semantic Role Labeling (Carreras and 
Ma rquez, 2005) for the evaluation of our forest 
kernel method. To speed up the evaluation 
process, the same as Che et al (2008), we use a 
subset of the entire training corpus (WSJ sections 
02-05 of the entire sections 02-21) for training, 
section 24 for development and section 23 for 
test, where there are 35 roles including 7 Core 
(A0?A5, AA), 14 Adjunct (AM-) and 14 Refer-
ence (R-) arguments. 
The state-of-the-art SRL methods (Carreras 
and Ma rquez, 2005) use constituents as the labe-
ling units to form the labeled arguments. Due to 
the errors from automatic parsing, it is impossi-
ble for all arguments to find their matching con-
stituents in the single 1-best parse trees. Statistics 
on the training data shows that 9.78% of argu-
ments have no matching constituents using the 
Charniak parser (Charniak, 2001), and the num-
ber increases to 11.76% when using the Collins 
parser (Collins, 1999). In our method, we break 
the limitation of 1-best parse tree and regard each 
span rooted by a single forest node (i.e., a sub-
forest with one or more roots) as a candidate ar-
gument. This largely reduces the unmatched ar-
guments from 9.78% to 1.31% after forest prun-
ing. However, it also results in a very large 
amount of argument candidates that is 5.6 times 
as many as that from 1-best tree. Fortunately, 
after the pre-processing stage of argument prun-
ing (Xue and Palmer, 2004) 4 , although the 
                                                 
4  We extend (Xue and Palmer, 2004)?s argument 
pruning algorithm from tree-based to forest-based. 
The algorithm is very effective. It can prune out 
around 90% argument candidates in parse tree-based 
amount of unmatched argument increases a little 
bit to 3.1%, its generated total candidate amount 
decreases substantially to only 1.31 times of that 
from 1-best parse tree. This clearly shows the 
advantages of the forest-based method over tree-
based in SRL. 
The best-reported tree kernel method for SRL 
??????? = ? ? ???? ? + (1? ?) ? ???  (0 ? ? ?
1), proposed by Che et al (2006)5, is adopted as 
our baseline kernel. We implemented the ???????  
in tree case (????????? , using tree kernel to 
compute ???? ?  and ??? ) and in forest case 
(????????? , using tree kernel to compute ???? ?  
and ??? ).  
 
 Precision Recall  F-Score 
?????????  (Tree) 76.02 67.38  71.44 
?????????  (Forest) 79.06 69.12 73.76 
Table 1: Performance comparison of SRL (%) 
 
Table 1 shows that the forest kernel significant-
ly outperforms (?2 test with p=0.01) the tree ker-
nel with an absolute improvement of 2.32 (73.76-
71.42) percentage in F-Score, representing a rela-
tive error rate reduction of 8.19% (2.32/(100-
71.64)). This convincingly demonstrates the ad-
vantage of the forest kernel over the tree kernel. It 
suggests that the structured features represented 
by subtree are very useful to SRL. The perfor-
mance improvement is mainly due to the fact that 
forest encodes much more such structured features 
and the forest kernel is able to more effectively 
capture such structured features than the tree ker-
nel. Besides F-Score, both precision and recall 
also show significantly improvement (?2 test with 
p=0.01). The reason for recall improvement is 
mainly due to the lower rate of unmatched argu-
ment (3.1% only) with only a little bit overhead 
(1.31 times) (see the previous discussion in this 
section). The precision improvement is mainly 
attributed to fact that we use sub-forest to 
represent argument instances, rather than sub-
tree used in tree kernel, where the sub-tree is on-
ly one tree encoded in the sub-forest. 
                                                                          
SRL and thus makes the amounts of positive and neg-
ative training instances (arguments) more balanced. 
We apply the same pruning strategies to forest plus 
our heuristic rules to prune out some of the arguments 
with span overlapped with each other and those ar-
guments with very small inside probabilities, depend-
ing on the numbers of candidates in the span. 
5 Kpath and Kcs are two standard convolution tree ker-
nels to describe predicate-argument path substructures 
and argument syntactic substructures, respectively. 
882
4.2 Relation extraction  
As a subtask of information extraction, relation 
extraction is to extract various semantic relations 
between entity pairs from text. For example, the 
sentence ?Bill Gates is chairman and chief soft-
ware architect of Microsoft Corporation? con-
veys the semantic relation ?EMPLOY-
MENT.executive? between the entities ?Bill 
Gates? (person) and ?Microsoft Corporation? 
(company). We adopt the method reported in 
Zhang et al (2006) as our baseline method as it 
reports the state-of-the-art performance using 
tree kernel-based composite kernel method for 
RE. We replace their tree kernels with our forest 
kernels and use the same experimental settings as 
theirs. We carry out the same five-fold cross va-
lidation experiment on the same subset of ACE 
2004 data (LDC2005T09, ACE 2002-2004) as 
that in Zhang et al (2006). The data contain 348 
documents and 4400 relation instances.  
In SRL, constituents are used as the labeling 
units to form the labeled arguments. However, 
previous work (Zhang et al, 2006) shows that if 
we use complete constituent (MCT) as done in 
SRL to represent relation instance, there is a 
large performance drop compared with using the 
path-enclosed tree (PT)6. By simulating PT, we 
use the minimal fragment of a forest covering the 
two entities and their internal words to represent 
a relation instance by only parsing the span cov-
ering the two entities and their internal words. 
 
 
 Precision  Recall  F-Score 
Zhang et al (2006):Tree 68.6 59.3 6  63.6 
Ours: Forest  70.3 60.0   64.7 
 
Table 2: Performance Comparison of RE (%) 
over 23 subtypes on the ACE 2004 data 
  
Table 2 compares the performance of the for-
est kernel and the tree kernel on relation extrac-
tion. We can see that the forest kernel significant-
ly outperforms (?2 test with p=0.05) the tree ker-
nel by 1.1 point of F-score. This further verifies 
the effectiveness of the forest kernel method for 
                                                 
6 MCT is the minimal constituent rooted by the near-
est common ancestor of the two entities under consid-
eration while PT is the minimal portion of the parse 
tree (may not be a complete subtree) containing the 
two entities and their internal lexical words. Since in 
many cases, the two entities and their internal words 
cannot form a grammatical constituent, MCT may 
introduce too many noisy context features and thus 
lead to the performance drop. 
modeling NLP structured data. In summary, we 
further observe the high precision improvement 
that is consistent with the SRL experiments. How-
ever, the recall improvement is not as significant 
as observed in SRL. This is because unlike SRL, 
RE has no un-matching issues in generating rela-
tion instances. Moreover, we find that the perfor-
mance improvement in RE is not as good as that 
in SRL. Although we know that performance is 
task-dependent, one of the possible reasons is 
that SRL tends to be long-distance grammatical 
structure-related while RE is local and semantic-
related as observed from the two experimental 
benchmark data. 
5 Conclusions and Future Work 
Many NLP applications have benefited from the 
success of convolution kernel over parse tree. 
Since a packed parse forest contains much richer 
structured features than a parse tree, we are mo-
tivated to develop a technology to measure the 
syntactic similarity between two forests. 
To achieve this goal, in this paper, we design a 
convolution kernel over packed forest by genera-
lizing the tree kernel. We analyze the object 
space of the forest kernel, the fractional count for 
feature value computing and design a dynamic 
programming algorithm to realize the forest ker-
nel with quadratic time complexity. Compared 
with the tree kernel, the forest kernel is more ro-
bust against parsing errors and data sparseness 
issues. Among the broad potential NLP applica-
tions, the problems in SRL and RE provide two 
pointed scenarios to verify our forest kernel. Ex-
perimental results demonstrate the effectiveness 
of the proposed kernel in structured NLP data 
modeling and the advantages over tree kernel.  
In the future, we would like to verify the forest 
kernel in more NLP applications. In addition, as 
suggested by one reviewer, we may consider res-
caling the probabilities (exponentiating them by 
a constant value) that are used to compute the 
fractional counts. We can sharpen or flatten the 
distributions. This basically says "how seriously 
do we want to take the very best derivation" 
compared to the rest. However, the challenge is 
that we compute the fractional counts together 
with the forest kernel recursively by using the 
Inside-Outside probabilities. We cannot differen-
tiate the individual parse tree?s contribution to a 
fractional count on the fly. One possible solution 
is to do the probability rescaling off-line before 
kernel calculation. This would be a very interest-
ing research topic of our future work. 
883
References  
ACE (2002-2006). The Automatic Content Extraction 
Projects. http://www.ldc.upenn.edu/Projects/ACE/ 
Fabio Aiolli, Giovanni Da San Martino, Alessandro 
Sperduti and Alessandro Moschitti. 2006. Fast On-
line Kernel Learning for Trees. ICDM-2006 
Fabio Aiolli, Giovanni Da San Martino, Alessandro 
Sperduti and Alessandro Moschitti. 2007. Efficient 
Kernel-based Learning for Trees. IEEE Sympo-
sium on Computational Intelligence and Data Min-
ing (CIDM-2007) 
J. Baker. 1979. Trainable grammars for speech rec-
ognition. The 97th meeting of the Acoustical So-
ciety of America  
S. Billot and S. Lang. 1989. The structure of shared 
forest in ambiguous parsing. ACL-1989  
Razvan Bunescu. 2008. Learning with Probabilistic 
Features for Improved Pipeline Models. EMNLP-
2008 
X. Carreras and Llu?s Ma rquez. 2005. Introduction to 
the CoNLL-2005 shared task: SRL. CoNLL-2005 
E. Charniak. 2001. Immediate-head Parsing for Lan-
guage Models. ACL-2001 
E. Charniak and Mark Johnson. 2005. Corse-to-fine-
grained n-best parsing and discriminative re-
ranking. ACL-2005 
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 
2006. A hybrid convolution tree kernel for seman-
tic role labeling. COLING-ACL-2006 (poster) 
WanXiang Che, Min Zhang, Aiti Aw, Chew Lim Tan, 
Ting Liu and Sheng Li. 2008. Using a Hybrid 
Convolution Tree Kernel for Semantic Role Labe-
ling. ACM Transaction on Asian Language Infor-
mation Processing 
M. Collins. 1999. Head-driven statistical models for 
natural language parsing. Ph.D. dissertation, 
Pennsylvania University 
M. Collins and N. Duffy. 2002. Convolution Kernels 
for Natural Language. NIPS-2002 
Christopher Dyer, Smaranda Muresan and Philip Res-
nik. 2008. Generalizing Word Lattice Translation. 
ACL-HLT-2008 
Jenny Rose Finkel, Christopher D. Manning and And-
rew Y. Ng. 2006. Solving the Problem of Cascad-
ing Errors: Approximate Bayesian Inference for 
Linguistic Annotation Pipelines. EMNLP-2006 
Y. Freund and R. E. Schapire. 1999. Large margin 
classification using the perceptron algorithm. Ma-
chine Learning, 37(3):277-296 
D. Guldea. 2002. Probabilistic models of verb-
argument structure. COLING-2002 
D. Haussler. 1999. Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, 
University of California, Santa Cruz 
Liang Huang. 2008. Forest reranking: Discriminative 
parsing with non-local features. ACL-2008 
Karim Lari and Steve J. Young. 1990. The estimation 
of stochastic context-free grammars using the in-
side-outside algorithm. Computer Speech and Lan-
guage. 4(35?56) 
H. Kashima and T. Koyanagi. 2003. Kernels for Semi-
Structured Data. ICML-2003 
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. IWPT-2001 
T. Joachims. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. EMNLP-2008 
Alessandro Moschitti. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. ACL-2004 
Alessandro Moschitti. 2006. Syntactic kernels for 
natural language learning: the semantic role labe-
ling case. HLT-NAACL-2006 (short paper) 
Martha Palmer, Dan Gildea and Paul Kingsbury. 
2005. The proposition bank: An annotated corpus 
of semantic roles. Computational Linguistics. 31(1) 
F. Rosenblatt. 1962. Principles of Neurodynamics: 
Perceptrons and the theory of brain mechanisms. 
Spartan Books, Washington D.C. 
Masaru Tomita. 1987. An Efficient Augmented-
Context-Free Parsing Algorithm. Computational 
Linguistics 13(1-2): 31-46 
Vladimir N. Vapnik. 1998. Statistical Learning 
Theory. Wiley 
C. Watkins. 1999. Dynamic alignment kernels. In A. J. 
Smola, B. Sch o?lkopf, P. Bartlett, and D. Schuur-
mans (Eds.), Advances in kernel methods. MIT 
Press 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. EMNLP-2004  
Xiaofeng Yang, Jian Su and Chew Lim Tan. 2006. 
Kernel-Based Pronoun Resolution with Structured 
Syntactic Knowledge. COLING-ACL-2006 
Dell Zhang and W. Lee. 2003. Question classification 
using support vector machines. SIGIR-2003 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and 
Chew Lim Tan. 2009a. Forest-based Tree Se-
quence to String Translation Model. ACL-
IJCNLP-2009 
Hui Zhang, Min Zhang, Haizhou Li and Chew Lim 
Tan. 2009b. Fast Translation Rule Matching for 
884
Syntax-based Statistical Machine Translation. 
EMNLP-2009 
Min Zhang, Jie Zhang, Jian Su and GuoDong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Fea-
tures. COLING-ACL-2006 
Min Zhang, W. Che, A. Aw, C. Tan, G. Zhou, T. Liu 
and S. Li. 2007. A Grammar-driven Convolution 
Tree Kernel for Semantic Role Classification. 
ACL-2007 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation 
Model. ACL-2008 
Min Zhang and Haizhou Li. 2009. Tree Kernel-based 
SVM with Structured Syntactic Knowledge for 
BTG-based Phrase Reordering. EMNLP-2009 
885
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1288?1297,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Enhancing Language Models in Statistical Machine Translation
with Backward N-grams and Mutual Information Triggers
Deyi Xiong, Min Zhang, Haizhou Li
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632
{dyxiong, mzhang, hli}@i2r.a-star.edu.sg
Abstract
In this paper, with a belief that a language
model that embraces a larger context provides
better prediction ability, we present two ex-
tensions to standard n-gram language mod-
els in statistical machine translation: a back-
ward language model that augments the con-
ventional forward language model, and a mu-
tual information trigger model which captures
long-distance dependencies that go beyond
the scope of standard n-gram language mod-
els. We integrate the two proposed models
into phrase-based statistical machine transla-
tion and conduct experiments on large-scale
training data to investigate their effectiveness.
Our experimental results show that both mod-
els are able to significantly improve transla-
tion quality and collectively achieve up to 1
BLEU point over a competitive baseline.
1 Introduction
Language model is one of the most important
knowledge sources for statistical machine transla-
tion (SMT) (Brown et al, 1993). The standard
n-gram language model (Goodman, 2001) assigns
probabilities to hypotheses in the target language
conditioning on a context history of the preceding
n ? 1 words. Along with the efforts that advance
translation models from word-based paradigm to
syntax-based philosophy, in recent years we have
also witnessed increasing efforts dedicated to ex-
tend standard n-gram language models for SMT. We
roughly categorize these efforts into two directions:
data-volume-oriented and data-depth-oriented.
In the first direction, more data is better. In or-
der to benefit from monolingual corpora (LDC news
data or news data collected from web pages) that
consist of billions or even trillions of English words,
huge language models are built in a distributed man-
ner (Zhang et al, 2006; Brants et al, 2007). Such
language models yield better translation results but
at the cost of huge storage and high computation.
The second direction digs deeply into monolin-
gual data to build linguistically-informed language
models. For example, Charniak et al (2003) present
a syntax-based language model for machine transla-
tion which is trained on syntactic parse trees. Again,
Shen et al (2008) explore a dependency language
model to improve translation quality. To some ex-
tent, these syntactically-informed language models
are consistent with syntax-based translation models
in capturing long-distance dependencies.
In this paper, we pursue the second direction with-
out resorting to any linguistic resources such as a
syntactic parser. With a belief that a language model
that embraces a larger context provides better pre-
diction ability, we learn additional information from
training data to enhance conventional n-gram lan-
guage models and extend their ability to capture
richer contexts and long-distance dependencies. In
particular, we integrate backward n-grams and mu-
tual information (MI) triggers into language models
in SMT.
In conventional n-gram language models, we look
at the preceding n ? 1 words when calculating the
probability of the current word. We henceforth call
the previous n ? 1 words plus the current word
as forward n-grams and a language model built
1288
on forward n-grams as forward n-gram language
model. Similarly, backward n-grams refer to the
succeeding n ? 1 words plus the current word. We
train a backward n-gram language model on back-
ward n-grams and integrate the forward and back-
ward language models together into the decoder. In
doing so, we attempt to capture both the preceding
and succeeding contexts of the current word.
Different from the backward n-gram language
model, the MI trigger model still looks at previous
contexts, which however go beyond the scope of for-
ward n-grams. If the current word is indexed as wi,
the farthest word that the forward n-gram includes
is wi?n+1. However, the MI triggers are capable of
detecting dependencies between wi and words from
w1 to wi?n. By these triggers ({wk ? wi}, 1 ?
k ? i?n), we can capture long-distance dependen-
cies that are outside the scope of forward n-grams.
We integrate the proposed backward language
model and the MI trigger model into a state-of-
the-art phrase-based SMT system. We evaluate
the effectiveness of both models on Chinese-to-
English translation tasks with large-scale training
data. Compared with the baseline which only uses
the forward language model, our experimental re-
sults show that the additional backward language
model is able to gain about 0.5 BLEU points, while
the MI trigger model gains about 0.4 BLEU points.
When both models are integrated into the decoder,
they collectively improve the performance by up to
1 BLEU point.
The paper is structured as follows. In Section 2,
we will briefly introduce related work and show how
our models differ from previous work. Section 3 and
4 will elaborate the backward language model and
the MI trigger model respectively in more detail, de-
scribe the training procedures and explain how the
models are integrated into the phrase-based decoder.
Section 5 will empirically evaluate the effectiveness
of these two models. Section 6 will conduct an in-
depth analysis. In the end, we conclude in Section
7.
2 Related Work
Previous work devoted to improving language mod-
els in SMT mostly focus on two categories as we
mentioned before1: large language models (Zhang
et al, 2006; Emami et al, 2007; Brants et al, 2007;
Talbot and Osborne, 2007) and syntax-based lan-
guage models (Charniak et al, 2003; Shen et al,
2008; Post and Gildea, 2008). Since our philoso-
phy is fundamentally different from them in that we
build contextually-informed language models by us-
ing backward n-grams and MI triggers, we discuss
previous work that explore these two techniques
(backward n-grams and MI triggers) in this section.
Since the context ?history? in the backward lan-
guage model (BLM) is actually the future words
to be generated, BLM is normally used in a post-
processing where all words have already been gener-
ated or in a scenario where sentences are proceeded
from the ending to the beginning. Duchateau et al
(2002) use the BLM score as a confidence measure
to detect wrongly recognized words in speech recog-
nition. Finch and Sumita (2009) use the BLM in
their reverse translation decoder where source sen-
tences are proceeded from the ending to the begin-
ning. Our BLM is different from theirs in that we ac-
cess the BLM during decoding (rather than after de-
coding) where source sentences are still proceeded
from the beginning to the ending.
Rosenfeld et al (1994) introduce trigger pairs
into a maximum entropy based language model as
features. The trigger pairs are selected accord-
ing to their mutual information. Zhou (2004) also
propose an enhanced language model (MI-Ngram)
which consists of a standard forward n-gram lan-
guage model and an MI trigger model. The latter
model measures the mutual information of distance-
dependent trigger pairs. Our MI trigger model is
mostly inspired by the work of these two papers, es-
pecially by Zhou?s MI-Ngram model (2004). The
difference is that our model is distance-independent
and, of course, we are interested in an SMT problem
rather than a speech recognition one.
Raybaud et al (2009) useMI triggers in their con-
fidence measures to assess the quality of translation
results after decoding. Our method is different from
theirs in the MI calculation and trigger pair selec-
tion. Mauser et al (2009) propose bilingual triggers
where two source words trigger one target word to
1Language model adaptation is not very related to our work
so we ignore it.
1289
improve lexical choice of target words. Our analysis
(Section 6) show that our monolingual triggers can
also help in the selection of target words.
3 Backward Language Model
Given a sequence of words wm1 = (w1...wm), a
standard forward n-gram language model assigns a
probability Pf (wm1 ) to wm1 as follows.
Pf (wm1 ) =
m
?
i=1
P (wi|wi?11 ) ?
m
?
i=1
P (wi|wi?1i?n+1) (1)
where the approximation is based on the nth order
Markov assumption. In other words, when we pre-
dict the current word wi, we only consider the pre-
ceding n ? 1 words wi?n+1...wi?1 instead of the
whole context history w1...wi?1.
Different from the forward n-gram language
model, the backward n-gram language model as-
signs a probability Pb(wm1 ) to wm1 by looking at the
succeeding context according to
Pb(wm1 ) =
m
?
i=1
P (wi|wmi+1) ?
m
?
i=1
P (wi|wi+n?1i+1 ) (2)
3.1 Training
For the convenience of training, we invert the or-
der in each sentence in the training data, i.e., from
the original order (w1...wm) to the reverse order
(wm...w1). In this way, we can use the same toolkit
that we use to train a forward n-gram language
model to train a backward n-gram language model
without any other changes. To be consistent with
training, we also need to reverse the order of trans-
lation hypotheses when we access the trained back-
ward language model2. Note that the Markov con-
text history of Eq. (2) is wi+n?1...wi+1 instead of
wi+1...wi+n?1 after we invert the order. The words
are the same but the order is completely reversed.
3.2 Decoding
In this section, we will present two algorithms
to integrate the backward n-gram language model
into two kinds of phrase-based decoders respec-
tively: 1) a CKY-style decoder that adopts bracket-
ing transduction grammar (BTG) (Wu, 1997; Xiong
2This is different from the reverse decoding in (Finch and
Sumita, 2009) where source sentences are reversed in the order.
et al, 2006) and 2) a standard phrase-based decoder
(Koehn et al, 2003). Both decoders translate source
sentences from the beginning of a sentence to the
ending. Wu (1996) introduce a dynamic program-
ming algorithm to integrate a forward bigram lan-
guage model with inversion transduction grammar.
His algorithm is then adapted and extended for inte-
grating forward n-gram language models into syn-
chronous CFGs by Chiang (2007). Our algorithms
are different from theirs in two major aspects
1. The string input to the algorithms is in a reverse
order.
2. We adopt a different way to calculate language
model probabilities for partial hypotheses so
that we can utilize incomplete n-grams.
Before we introduce the integration algorithms,
we define three functions P , L, and R on strings (in
a reverse order) over the English terminal alphabet
T . The function P is defined as follows.
P(wk...w1) =P (wk)...P (wk?n+2|wk...wk?n+3)
? ?? ?
a
?
?
1?i?k?n+1
P (wi|wi+n?1...wi+1)
? ?? ?
b
(3)
This function consists of two parts:
? The first part (a) calculates incomplete n-gram
language model probabilities for word wk to
wk?n+2. That means, we calculate the uni-
gram probability for wk (P (wk)), bigram prob-
ability for wk?1 (P (wk?1|wk)) and so on un-
til we take n ? 1-gram probability for wk?n+2
(P (wk?n+2|wk...wk?n+3)). This resembles
the way in which the forward language model
probability in the future cost is computed in
the standard phrase-based SMT (Koehn et al,
2003).
? The second part (b) calculates complete n-
gram backward language model probabilities
for word wk?n+1 to w1.
The function is different from Chiang?s p func-
tion in that his function p only calculates language
model probabilities for the complete n-grams. Since
1290
we calculate backward language model probabilities
during a beginning-to-ending (left-to-right) decod-
ing process, the succeeding context for the current
word is either yet to be generated or incomplete in
terms of n-grams. The P function enables us to
utilize incomplete succeeding contexts to approxi-
mately predict words. Once the succeeding con-
texts are complete, we can quickly update language
model probabilities in an efficient way in our algo-
rithms.
The other two functions L and R are defined as
follows
L(wk...w1) =
{
wk...wk?n+2, if k ? n
wk...w1, otherwise
(4)
R(wk...w1) =
{
wn?1...w1, if k ? n
wk...w1, otherwise
(5)
The L and R function return the leftmost and right-
most n ? 1 words from a string in a reverse order
respectively.
Following Chiang (2007), we describe our algo-
rithms in a deductive system. We firstly show the
algorithm3 that integrates the backward language
model into a BTG-style decoder (Xiong et al, 2006)
in Figure 1. The item [A, i, j; l|r] indicates that a
BTG node A has been constructed spanning from i
to j on the source side with the leftmost|rightmost
n? 1 words l|r on the target side. As mentioned be-
fore, all target strings assessed by the defined func-
tions (P , L, and R) are in an inverted order (de-
noted by e). We only display the backward lan-
guage model probability for each item, ignoring all
other scores such as phrase translation probabilities.
The Eq. (8) in Figure 1 shows how we calculate
the backward language model probability for the ax-
iom which applies a BTG lexicon rule to translate
a source phrase c into a target phrase e. The Eq.
(9) and (10) show how we update the backward lan-
guage model probabilities for two inference rules
which combine two neighboring blocks in a straight
and inverted order respectively. The fundamental
theories behind this update are
P(e1e2) = P(e1)P(e2)
P(R(e2)L(e1))
P(R(e2))P(L(e1))
(6)
3It can also be easily adapted to integrate the forward n-
gram language model.
Function Value
e1 a1a2a3
e2 b1b2b3
R(e2) b2b1
L(e1) a3a2
P(R(e2)) P (b2)P (b1|b2)
P(L(e1)) P (a3)P (a2|a3)
P(e1) P (a3)P (a2|a3)P (a1|a3a2)
P(e2) P (b3)P (b2|b3)P (b1|b3b2)
P(R(e2)L(e1))
P (b2)P (b1|b2)
P (a3|b2b1)P (a2|b1a3)
P(e1e2)
P (b3)P (b2|b3)P (b1|b3b2)
P (a3|b2b1)P (a2|b1a3)P (a1|a3a2)
Table 1: Values of P , L, and R in a 3-gram example .
P(e2e1) = P(e1)P(e2)
P(R(e1)L(e2))
P(R(e1))P(L(e2))
(7)
Whenever two strings e1 and e2 are concatenated
in a straight or inverted order, we can reuse their
P values (P(e1) and P(e2)) in terms of dynamic
programming. Only the probabilities of boundary
words (e.g., R(e2)L(e1) in Eq. (6)) need to be re-
calculated since they have complete n-grams after
the concatenation. Table 1 shows values of P , L,
and R in a 3-gram example which helps to verify
Eq. (6). These two equations guarantee that our
algorithm can correctly compute the backward lan-
guage model probability of a sentence stepwise in a
dynamic programming framework.4
The theoretical time complexity of this algorithm
is O(m3|T |4(n?1)) because in the update parts in
Eq. (6) and (7) both the numerator and denomina-
tor have up to 2(n?1) terminal symbols. This is the
same as the time complexity of Chiang?s language
model integration (Chiang, 2007).
Figure 2 shows the algorithm that integrates the
backward language model into a standard phrase-
based SMT (Koehn et al, 2003). V denotes a cover-
age vector which records source words translated so
far. The Eq. (11) shows how we update the back-
ward language model probability for a partial hy-
pothesis when it is extended into a longer hypothesis
by a target phrase translating an uncovered source
4The start-of-sentence symbol ?s? and end-of-sentence sym-
bol ?/s? can be easily added to update the final language model
probability when a translation hypothesis covering the whole
source sentence is completed.
1291
A ? c/e
[A, i, j;L(e)|R(e)] : P(e)
(8)
A ? [A1, A2] [A1, i, k;L(e1)|R(e1)] : P(e1) [A2, k + 1, j;L(e2)|R(e2)] : P(e2)
[A, i, j;L(e1e2)|R(e1e2)] : P(e1)P(e2) P(R(e2)L(e1))P(R(e2))P(L(e1))
(9)
A ? ?A1, A2? [A1, i, k;L(e1)|R(e1)] : P(e1) [A2, k + 1, j;L(e2)|R(e2)] : P(e2)
[A, i, j;L(e2e1)|R(e2e1)] : P(e1)P(e2) P(R(e1)L(e2))P(R(e1))P(L(e2))
(10)
Figure 1: Integrating the backward language model into a BTG-style decoder.
[V;L(e1)] : P(e1) c/e2 : P(e2)
[V ?;L(e1e2)] : P(e1)P(e2) P(R(e2)L(e1))P(R(e2))P(L(e1))
(11)
Figure 2: Integrating the backward language model into
a standard phrase-based decoder.
segment. This extension on the target side is simi-
lar to the monotone combination of Eq. (9) in that a
newly translated phrase is concatenated to an early
translated sequence.
4 MI Trigger Model
It is well-known that long-distance dependencies be-
tween words are very important for statistical lan-
guage modeling. However, n-gram language models
can only capture short-distance dependencies within
an n-word window. In order to model long-distance
dependencies, previous work such as (Rosenfeld et
al., 1994) and (Zhou, 2004) exploit trigger pairs. A
trigger pair is defined as an ordered 2-tuple (x, y)
where word x occurs in the preceding context of
word y. It can also be denoted in a more visual man-
ner as x ? y with x being the trigger and y the
triggered word5.
We use pointwise mutual information (PMI)
(Church and Hanks, 1990) to measure the strength
of the association between x and y, which is defined
as follows
PMI(x, y) = log( P (x, y)
P (x)P (y)
) (12)
5In this paper, we require that word x and y occur in the
same sentence.
Zhou (2004) proposes a new language model en-
hanced with MI trigger pairs. In his model, the prob-
ability of a given sentence wm1 is approximated as
P (wm1 ) ?(
m
?
i=1
P (wi|wi?1i?n+1))
?
m
?
i=n+1
i?n
?
k=1
exp(PMI(wk, wi, i? k ? 1))
(13)
There are two components in his model. The first
component is still the standard n-gram language
model. The second one is the MI trigger model
which multiples all exponential PMI values for trig-
ger pairs where the current word is the triggered
word and all preceding words outside the n-gram
window of the current word are triggers. Note that
his MI trigger model is distance-dependent since
trigger pairs (wk, wi) are sensitive to their distance
i? k? 1 (zero distance for adjacent words). There-
fore the distance between word x and word y should
be taken into account when calculating their PMI.
In this paper, for simplicity, we adopt a distance-
independent MI trigger model as follows
MI(wm1 ) =
m
?
i=n+1
i?n
?
k=1
exp(PMI(wk, wi)) (14)
We integrate the MI trigger model into the log-
linear model of machine translation as an additional
knowledge source which complements the standard
n-gram language model in capturing long-distance
dependencies. By MERT (Och, 2003), we are even
able to tune the weight of the MI trigger model
against the weight of the standard n-gram language
model while Zhou (2004) sets equal weights for both
models.
1292
4.1 Training
We can use the maximum likelihood estimation
method to calculate PMI for each trigger pair by tak-
ing counts from training data. Let C(x, y) be the
co-occurrence count of the trigger pair (x, y) in the
training data. The joint probability of (x, y) is cal-
culated as
P (x, y) = C(x, y)?
x,y C(x, y)
(15)
The marginal probabilities of x and y can be de-
duced from the joint probability as follows
P (x) =
?
y
P (x, y) (16)
P (y) =
?
x
P (x, y) (17)
Since the number of distinct trigger pairs is
O(|T |2), the question is how to select valuable trig-
ger pairs. We select trigger pairs according to the
following three steps
1. The distance between x and y must not be less
than n? 1. Suppose we use a 5-gram language
model and y = wi , then x ? {w1...wi?5}.
2. C(x, y) > c. In all our experiments we set c =
10.
3. Finally, we only keep trigger pairs whose PMI
value is larger than 0. Trigger pairs whose PMI
value is less than 0 often contain stop words,
such as ?the?, ?a?. These stop words have very
large marginal probabilities due to their high
frequencies.
4.2 Decoding
The MI trigger model of Eq. (14) can be directly
integrated into the decoder. For the standard phrase-
based decoder (Koehn et al, 2003), whenever a par-
tial hypothesis is extended by a new target phrase,
we can quickly retrieve the pre-computed PMI value
for each trigger pair where the triggered word lo-
cates in the newly translated target phrase and the
trigger is outside the n-word window of the trig-
gered word. It?s a little more complicated to in-
tegrate the MI trigger model into the CKY-style
phrase-based decoder. But we still can handle it by
dynamic programming as follows
MI(e1e2) = MI(e1)MI(e2)MI(e1 ? e2) (18)
where MI(e1 ? e2) represents the PMI values in
which a word in e1 triggers a word in e2. It is defined
as follows
MI(e1 ? e2) =
?
wi?e2
?
wk?e1
i?k?n
exp(PMI(wk, wi))
(19)
5 Experiments
In this section, we conduct large-scale experiments
on NIST Chinese-to-English translation tasks to
evaluate the effectiveness of the proposed backward
language model and MI trigger model in SMT. Our
experiments focus on the following two issues:
1. How much improvements can we achieve by
separately integrating the backward language
model and the MI trigger model into our
phrase-based SMT system?
2. Can we obtain a further improvement if we
jointly apply both models?
5.1 System Overview
Without loss of generality6, we evaluate our models
in a phrase-based SMT system which adapts brack-
eting transduction grammars to phrasal translation
(Xiong et al, 2006). The log-linear model of this
system can be formulated as
w(D) =MT (rl1..nl) ?MR(r
m
1..nm)
?R
? PfL(e)?fL ? exp(|e|)?w
(20)
where D denotes a derivation, rl1..nl are the BTG
lexicon rules which translate source phrases to tar-
get phrases, and rm1..nm are the merging rules which
combine two neighboring blocks into a larger block
in a straight or inverted order. The translation
model MT consists of widely used phrase and lex-
ical translation probabilities (Koehn et al, 2003).
6We have discussed how to integrate the backward language
model and the MI trigger model into the standard phrase-based
SMT system (Koehn et al, 2003) in Section 3.2 and 4.2 respec-
tively.
1293
The reordering model MR predicts the merging or-
der (straight or inverted) by using discriminative
contextual features (Xiong et al, 2006). PfL is the
standard forward n-gram language model.
If we simultaneously integrate both the backward
language model PbL and the MI trigger model MI
into the system, the new log-linear model will be
formulated as
w(D) =MT (rl1..nl) ?MR(r
m
1..nm)
?R ? PfL(e)?fL
? PbL(e)?bL ?MI(e)?MI ? exp(|e|)?w
(21)
5.2 Experimental Setup
Our training corpora7 consist of 96.9M Chinese
words and 109.5M English words in 3.8M sentence
pairs. We used all corpora to train our translation
model and smaller corpora without the United Na-
tions corpus to build a maximum entropy based re-
ordering model (Xiong et al, 2006).
To train our language models and MI trigger
model, we used the Xinhua section of the En-
glish Gigaword corpus (306 million words). Firstly,
we built a forward 5-gram language model using
the SRILM toolkit (Stolcke, 2002) with modified
Kneser-Ney smoothing. Then we trained a back-
ward 5-gram language model on the same monolin-
gual corpus in the way described in Section 3.1. Fi-
nally, we trained our MI trigger model still on this
corpus according to the method in Section 4.1. The
trained MI trigger model consists of 2.88M trigger
pairs.
We used the NIST MT03 evaluation test data as
the development set, and the NIST MT04, MT05 as
the test sets. We adopted the case-insensitive BLEU-
4 (Papineni et al, 2002) as the evaluation metric,
which uses the shortest reference sentence length for
the brevity penalty. Statistical significance in BLEU
differences is tested by paired bootstrap re-sampling
(Koehn, 2004).
5.3 Experimental Results
The experimental results on the two NIST test sets
are shown in Table 2. When we combine the back-
ward language model with the forward language
7LDC2004E12, LDC2004T08, LDC2005T10,
LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07
and LDC2004T07.
Model MT-04 MT-05
Forward (Baseline) 35.67 34.41
Forward+Backward 36.16+ 34.97+
Forward+MI 36.00+ 34.85+
Forward+Backward+MI 36.76+ 35.12+
Table 2: BLEU-4 scores (%) on the two test sets for dif-
ferent language models and their combinations. +: better
than the baseline (p < 0.01).
model, we obtain 0.49 and 0.56 BLEU points over
the baseline on theMT-04 andMT-05 test set respec-
tively. Both improvements are statistically signifi-
cant (p < 0.01). The MI trigger model also achieves
statistically significant improvements of 0.33 and
0.44 BLEU points over the baseline on the MT-04
and MT-05 respectively.
When we integrate both the backward language
model and the MI trigger model into our system,
we obtain improvements of 1.09 and 0.71 BLEU
points over the single forward language model on
the MT-04 and MT-05 respectively. These improve-
ments are larger than those achieved by using only
one model (the backward language model or the MI
trigger model).
6 Analysis
In this section, we will study more details of the two
models by looking at the differences that they make
on translation hypotheses. These differences will
help us gain some insights into how the presented
models improve translation quality.
Table 3 shows an example from our test set. The
italic words in the hypothesis generated by using the
backward language model (F+B) exactly match the
reference. However, the italic words in the base-
line hypothesis fail to match the reference due to
the incorrect position of the word ?decree? (??).
We calculate the forward/backward language model
score (the logarithm of language model probability)
for the italic words in both the baseline and F+B hy-
pothesis according to the trained language models.
The difference in the forward language model score
is only 1.58, which may be offset by differences in
other features in the log-linear translation model. On
the other hand, the difference in the backward lan-
guage model score is 3.52. This larger difference
may guarantee that the hypothesis generated by F+B
1294
Source ??????? ,??????
?????????????
?
Baseline Beijing Youth Daily reported that
Beijing Agricultural decree recently
issued a series of control and super-
vision
F+B Beijing Youth Daily reported that
Beijing Bureau of Agriculture re-
cently issued a series of prevention
and control laws
Reference Beijing Youth Daily reported that
Beijing Bureau of Agriculture re-
cently issued a series of preventative
and monitoring ordinances
Table 3: Translation example from the MT-04 test set,
comparing the baseline with the backward language
model. F+B: forward+backward language model .
is better enough to be selected as the best hypothe-
sis by the decoder. This suggests that the backward
language model is able to provide useful and dis-
criminative information which is complementary to
that given by the forward language model.
In Table 4, we present another example to show
how the MI trigger model improves translation qual-
ity. The major difference in hypotheses of this ex-
ample is the word choice between ?is? and ?was?.
The new system enhanced with the MI trigger model
(F+M) selects the former while the baseline selects
the latter. The forward language model score for the
baseline hypothesis is -26.41, which is higher than
the score of the F+M hypothesis -26.67. This could
be the reason why the baseline selects the word
?was? instead of ?is?. As can be seen, there is an-
other ?is? in the preceding context of the word ?was?
in the baseline hypothesis. Unfortunately, this word
?is? is located just outside the scope of the preceding
5-gram context of ?was?. The forward 5-gram lan-
guage model is hence not able to take it into account
when calculating the probability of ?was?. However,
this is not a problem for the MI trigger model. Since
?is? and ?was? rarely co-occur in the same sentence,
the PMI value of the trigger pair (is, was)8 is -1.03
8Since we remove all trigger pairs whose PMI value is neg-
ative, the PMI value of this pair (is, was) is set 0 in practice in
the decoder.
Source ???????????? ,?
?????????????
?
Baseline Self-Defense Force ?s trip is remark-
able , because it was not an isolated
incident .
F+M Self-Defense Force ?s trip is remark-
able , because it is not an isolated in-
cident .
Reference The Self-Defense Forces? trip
arouses attention because it is not an
isolated incident.
Table 4: Translation example from the MT-04 test set,
comparing the baseline with the MI trigger model. Both
system outputs are not detokenized so that we can see
how language model scores are calculated. The un-
derlined words highlight the difference between the en-
hanced models and the baseline. F+M: forward language
model + MI trigger model.
while the PMI value of the trigger pair (is, is) is as
high as 0.32. Therefore our MI trigger model selects
?is? rather than ?was?.9 This example illustrates that
the MI trigger model is capable of selecting correct
words by using long-distance trigger pairs.
7 Conclusion
We have presented two models to enhance the abil-
ity of standard n-gram language models in captur-
ing richer contexts and long-distance dependencies
that go beyond the scope of forward n-gram win-
dows. The two models have been integrated into
the decoder and have shown to improve a state-of-
the-art phrase-based SMT system. The first model
is the backward language model which uses back-
ward n-grams to predict the current word. We in-
troduced algorithms that directly integrate the back-
ward language model into a CKY-style and a stan-
dard phrase-based decoder respectively. The sec-
ond model is the MI trigger model that incorporates
long-distance trigger pairs into language modeling.
Overall improvements are up to 1 BLEU point on
the NIST Chinese-to-English translation tasks with
large-scale training data. Further study of the two
9The overall MI trigger model scores (the logarithm of Eq.
(14)) of the baseline hypothesis and the F+M hypothesis are
2.09 and 2.25 respectively.
1295
models indicates that backward n-grams and long-
distance triggers provide useful information to im-
prove translation quality.
In future work, we would like to integrate the
backward language model into a syntax-based sys-
tem in a way that is similar to the proposed algo-
rithm shown in Figure 1. We are also interested in
exploring more morphologically- or syntactically-
informed triggers. For example, a verb in the past
tense triggers another verb also in the past tense
rather than the present tense.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 858?
867, Prague, Czech Republic, June. Association for
Computational Linguistics.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proceedings of MT Summit IX.
Intl. Assoc. for Machine Translation.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Jacques Duchateau, Kris Demuynck, and Patrick
Wambacq. 2002. Confidence scoring based on back-
ward language models. In Proceedings of ICASSP,
pages 221?224, Orlando, FL, April.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of ICASSP, pages 37?40, Honolulu, HI,
April.
Andrew Finch and Eiichiro Sumita. 2009. Bidirectional
phrase-based statistical machine translation. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1124?
1132, Singapore, August. Association for Computa-
tional Linguistics.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling extended version. Technical report,
Microsoft Research.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 58?54, Edmon-
ton, Canada, May-June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 210?218, Singa-
pore, August. Association for Computational Linguis-
tics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Matt Post and Daniel Gildea. 2008. Parsers as language
models for statistical machine translation. In Proceed-
ings of AMTA.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
and Kamel Sma??li. 2009. New confidence measures
for statistical machine translation. In Proceedings of
the International Conference on Agents and Artificial
Intelligence, pages 61?68, Porto, Portugal, January.
Roni Rosenfeld, Jaime Carbonell, and Alexander Rud-
nicky. 1994. Adaptive statistical language model-
ing: A maximum entropy approach. Technical report,
Carnegie Mellon University.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. Srilm?an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, Colorado, USA, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine translation.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 512?519,
1296
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proceedings of the 34th
Annual Meeting of the Association for Computational
Linguistics, pages 152?158, Santa Cruz, California,
USA, June.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 521?528, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list
re-ranking. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 216?223, Sydney, Australia, July. Association
for Computational Linguistics.
GuoDong Zhou. 2004. Modeling of long distance con-
text dependency. In Proceedings of Coling, pages 92?
98, Geneva, Switzerland, Aug 23?Aug 27. COLING.
1297
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 153?158,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
AM-FM: A Semantic Framework for Translation Quality Assessment 
 
 
Rafael E. Banchs Haizhou Li 
Human Language Technology Department Human Language Technology Department 
Institute for Infocomm Research Institute for Infocomm Research 
1 Fusionopolis Way, Singapore 138632 1 Fusionopolis Way, Singapore 138632 
rembanchs@i2r.a-star.edu.sg hli@i2r.a-star.edu.sg 
 
 
 
 
 
 
Abstract 
This work introduces AM-FM, a semantic 
framework for machine translation evalua-
tion. Based upon this framework, a new 
evaluation metric, which is able to operate 
without the need for reference translations, 
is implemented and evaluated. The metric 
is based on the concepts of adequacy and 
fluency, which are independently assessed 
by using a cross-language latent semantic 
indexing approach and an n-gram based 
language model approach, respectively. 
Comparative analyses with conventional 
evaluation metrics are conducted on two 
different evaluation tasks (overall quality 
assessment and comparative ranking) over 
a large collection of human evaluations in-
volving five European languages. Finally, 
the main pros and cons of the proposed 
framework are discussed along with future 
research directions. 
1 Introduction 
Evaluation has always been one of the major issues 
in Machine Translation research, as both human 
and automatic evaluation methods exhibit very 
important limitations. On the one hand, although 
highly reliable, in addition to being expensive and 
time consuming, human evaluation suffers from 
inconsistency problems due to inter- and intra-
annotator agreement issues. On the other hand, 
while being consistent, fast and cheap, automatic 
evaluation has the major disadvantage of requiring 
reference translations. This makes automatic eval-
uation not reliable in the sense that good transla-
tions not matching the available references are 
evaluated as poor or bad translations.  
The main objective of this work is to propose 
and evaluate AM-FM, a semantic framework for 
assessing translation quality without the need for 
reference translations. The proposed framework is 
theoretically grounded on the classical concepts of 
adequacy and fluency, and it is designed to account 
for these two components of translation quality in 
an independent manner. First, a cross-language la-
tent semantic indexing model is used for assessing 
the adequacy component by directly comparing the 
output translation with the input sentence it was 
generated from. Second, an n-gram based language 
model of the target language is used for assessing 
the fluency component.  
Both components of the metric are evaluated at 
the sentence level, providing the means for defin-
ing and implementing a sentence-based evaluation 
metric. Finally, the two components are combined 
into a single measure by implementing a weighted 
harmonic mean, for which the weighting factor can 
be adjusted for optimizing the metric performance.  
The rest of the paper is organized as follows. 
Section 2, presents some background work and the 
specific dataset that has been used in the experi-
mental work. Section 3, provides details on the 
proposed AM-FM framework and the specific met-
ric implementation. Section 4 presents the results 
of the conducted comparative evaluations. Finally, 
section 5 presents the main conclusions and rele-
vant issues to be dealt with in future research. 
153
2 Related Work and Dataset 
Although BLEU (Papineni et al, 2002) has be-
come a de facto standard for machine translation 
evaluation, other metrics such as NIST (Dodding-
ton, 2002) and, more recently, Meteor (Banerjee 
and Lavie, 2005), are commonly used too. Regard-
ing the specific idea of evaluating machine trans-
lation without using reference translations, several 
works have proposed and evaluated different ap-
proaches, including round-trip translation (Somers, 
2005; Rapp, 2009), as well as other regression- and 
classification-based approaches (Quirk, 2004; Ga-
mon et al, 2005; Albrecht and Hwa, 2007; Specia 
et al, 2009). 
As part of the recent efforts on machine transla-
tion evaluation, two workshops have been organiz-
ing shared-tasks and evaluation campaigns over the 
last four years: the NIST Metrics for Machine 
Translation Challenge 1  (MetricsMATR) and the 
Workshop on Statistical Machine Translation 2  
(WMT); which were actually held as one single 
event in their most recent edition in 2010. 
The dataset used in this work corresponds to 
WMT-07. This dataset is used, instead of a more 
recent one, because no human judgments on ade-
quacy and fluency have been conducted in WMT 
after year 2007, and human evaluation data is not 
freely available from MetricsMATR. 
In this dataset, translation outputs are available 
for fourteen tasks involving five European lan-
guages: English (EN), Spanish (ES), German (DE), 
French (FR) and Czech (CZ); and two domains: 
News Commentaries (News) and European Par-
liament Debates (EPPS). A complete description 
on WMT-07 evaluation campaign and dataset is 
available in Callison-Burch et al (2007). 
System outputs for fourteen of the fifteen sys-
tems that participated in the evaluation are availa-
ble. This accounts for 86 independent system 
outputs with a total of 172,315 individual sentence 
translations, from which only 10,754 were rated 
for both adequacy and fluency by human judges.  
The specific vote standardization procedure de-
scribed in section 5.4 of Blatz et al (2003) was 
applied to all adequacy and fluency scores for re-
moving individual voting patterns and averaging 
votes. Table 1 provides information on the corre-
sponding domain, and source and target languages 
                                                          
1 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/  
2 http://www.statmt.org/wmt10/  
for each of the fourteen translation tasks, along 
with their corresponding number of system outputs 
and the amount of sentence translations for which 
human evaluations are available. 
 
Task Domain Src. Tgt. Syst. Sent. 
T1 News CZ EN 3 727 
T2 News EN CZ 2 806 
T3 EPPS EN FR 7 577 
T4 News EN FR 8 561 
T5 EPPS EN DE 6 924 
T6 News EN DE 6 892 
T7 EPPS EN ES 6 703 
T8 News EN ES 7 832 
T9 EPPS FR EN 7 624 
T10 News FR EN 7 740 
T11 EPPS DE EN 7 949 
T12 News DE EN 5 939 
T13 EPPS ES EN 8 812 
T14 News ES EN 7 668 
 
Table 1: Domain, source language, target lan-
guage, system outputs and total amount of sentence 
translations (with both adequacy and fluency hu-
man assessments) included in the WMT-07 dataset 
3 Semantic Evaluation Framework  
The framework proposed in this work (AM-FM) 
aims at assessing translation quality without the 
need for reference translations, while maintaining 
consistency with human quality assessments. Dif-
ferent from other approaches not using reference 
translations, we rely on a cross-language version of 
latent semantic indexing (Dumais et al, 1997) for 
creating a semantic space where translation outputs 
and inputs can be directly compared.  
A two-component evaluation metric, based on 
the concepts of adequacy and fluency (White et al, 
1994) is defined. While adequacy accounts for the 
amount of source meaning being preserved by the 
translation (5:all, 4:most, 3:much, 2:little, 1:none), 
fluency accounts for the quality of the target lan-
guage in the translation (5:flawless, 4:good, 3:non-
native, 2:disfluent, 1:incomprehensible). 
3.1 Metric Definition 
For implementing the adequacy-oriented compo-
nent (AM) of the metric, the cross-language latent 
semantic indexing approach is used (Dumais et al, 
1997), in which the source sentence originating the 
translation is used as evaluation reference. Accord-
154
ing to this, the AM component can be regarded to 
be mainly adequacy-oriented as it is computed on a 
cross-language semantic space. 
For implementing the fluency-oriented compo-
nent (FM) of the proposed metric, an n-gram based 
language model approach is used (Manning and 
Schutze, 1999). This component can be regarded to 
be mainly fluency-oriented as it is computed on the 
target language side in a manner that is totally in-
dependent from the source language.  
For combining both components into a single 
metric, a weighted harmonic mean is proposed: 
 
AM-FM = AM FM / (? AM + (1-?) FM) (1) 
 
where ? is a weighting factor ranging from ?=0 
(pure AM component) to ?=1 (pure FM compo-
nent), which can be adjusted for maximizing the 
correlation between the proposed metric AM-FM 
and human evaluation scores. 
3.2 Implementation Details 
The adequacy-oriented component of the metric 
(AM) was implemented by following the proce-
dure proposed by Dumais et al (1997), where a 
bilingual collection of data is used to generate a 
cross-language projection matrix for a vector-space 
representation of texts (Salton et al, 1975) by 
using singular value decomposition: SVD (Golub 
and Kahan, 1965).  
According to this formulation, a bilingual term-
document matrix Xab of dimensions M*N, where 
M=(Ma+Mb) are vocabulary terms in languages a 
and b, and N are documents (sentences in our 
case), can be decomposed as follows:  
 
Xab = [Xa;Xb] = Uab ?ab Vab T (2) 
 
where [Xa;Xb] is the concatenation of the two 
monolingual term-document matrices Xa and Xb 
(of dimensions Ma*N and Mb*N) corresponding to 
the available parallel training collection, Uab and 
Vab are unitary matrices of dimensions M*M and 
N*N, respectively, and ? is an M*N diagonal matrix 
containing the singular values associated to the de-
composition. 
From the singular value decomposition depicted 
in (2), a low-dimensional representation for any 
sentence vector xa or xb, in language a or b, can be 
computed as follows: 
ya T  =  [xa ;0] T  UabM*L (3.a) 
 
yb T  =  [0; xb] T  UabM*L (3.b) 
 
where ya and yb represent the L-dimensional vec-
tors corresponding to the projections of the full-
dimensional sentence vectors xa and xb, respective-
ly; and UabM*L is a cross-language projection matrix 
composed of the first L column vectors of the 
unitary matrix Uab obtained in (2).  
Notice, from (3a) and (3b), how both sentence 
vectors xa and xb are padded with zeros at each 
corresponding other-language vocabulary locations 
for performing the cross-language projections. As 
similar terms in different languages would have 
similar occurrence patterns, theoretically, a close 
representation in the cross-language reduced space 
should be obtained for terms and sentences that are 
semantically related. Therefore, sentences can be 
compared across languages in the reduced space. 
The AM component of the metric is finally com-
puted in the projected space by using the cosine 
similarity between the source and target sentences:  
 
AM = [s;0]TP ([0;t]TP)T / |[s;0]TP| / |[0;t]TP| (4) 
 
where P is the projection matrix UabM*L described 
in (3a) and (3b), [s;0] and [0;t] are vector space 
representations of the source and target sentences 
being compared (with their target and source 
vocabulary elements set to zero, respectively), and 
| | is the L2-norm operator. In a final implementa-
tion stage, the range of AM is restricted to the 
interval [0,1] by truncating negative results.  
For computing the projection matrices, random 
sets of 10,000 parallel sentences3 were drawn from 
the available training datasets. The only restriction 
we imposed to the extracted sentences was that 
each should contain at least 10 words. Seven pro-
jection matrices were constructed in total, one for 
each different combination of domain and lan-
guage pair. TF-IDF weighting was applied to the 
constructed term-document matrices while main-
taining all words in the vocabularies (i.e. no stop-
words were removed). All computations related to 
SVD, sentence projections and cosine similarities 
were conducted with MATLAB. 
                                                          
3 Although this accounts for a small proportion of the datasets 
(20% of News and 1% of European Parliament), it allowed for 
maintaining computational requirements under control while 
still providing a good vocabulary coverage. 
155
The fluency-oriented component FM is imple-
mented by using an n-gram language model. In 
order to avoid possible effects derived from dif-
ferences in sentence lengths, a compensation factor 
is introduced in log-probability space. According 
to this, the FM component is computed as follows: 
 
FM  =  exp(?n=1:N log(p(wn|wn-1,?))/N) (5) 
 
where p(wn|wn-1,?) represent the target language 
n-gram probabilities and N is the total number of 
words in the target sentence being evaluated.  
By construction, the values of FM are also re-
stricted to the interval [0,1]; so, both component 
values range within the same interval.  
Fourteen language models were trained in total, 
one per task, by using the available training data-
sets. The models were computed with the SRILM 
toolbox (Stolcke, 2002). 
As seen from (4) and (5), different from con-
ventional metrics that compute matches between 
translation outputs and references, in the AM-FM 
framework, a semantic embedding is used for as-
sessing the similarities between outputs and inputs 
(4) and, independently, an n-gram model is used 
for evaluating output language quality (5). 
4 Comparative Evaluations   
In order to evaluate the AM-FM framework, two 
comparative evaluations with standard metrics 
were conducted. More specifically, BLEU, NIST 
and Meteor were considered, as they are the met-
rics most frequently used in machine translation 
evaluation campaigns.  
4.1 Correlation with Human Scores 
In this first evaluation, AM-FM is compared with 
standard evaluation metrics in terms of their corre-
lations with human-generated scores. Different 
from Callison-Burch et al (2007), where Spear-
man?s correlation coefficients were used, we use 
here Pearson?s coefficients as, instead of focusing 
on ranking; this first evaluation exercise focuses on 
evaluating the significance and noisiness of the 
association, if any, between the automatic metrics 
and human-generated scores. 
Three parameters should be adjusted for the 
AM-FM implementation described in (1): the di-
mensionality of the reduced space for AM, the or-
der of n-gram model for FM, and the harmonic 
mean weighting parameter ?. Such parameters can 
be adjusted for maximizing the correlation coeffi-
cient between the AM-FM metric and human-
generated scores. 4  After exploring the solution 
space, the following values were selected, dimen-
sionality for AM: 1,000; order of n-gram model for 
FM: 3; and, weighting parameter ?: 0.30 
In the comparative evaluation presented here, 
correlation coefficients between the automatic met-
rics and human-generated scores were computed at 
the system level (i.e. the units of analysis were sys-
tem outputs), by considering all 86 available sys-
tem outputs (see Table 1). For computing human 
scores and AM-FM at the system level, average 
values of sentence-based scores for each system 
output were considered.  
Table 2 presents the Pearson?s correlation coef-
ficients computed between the automatic metrics 
(BLEU, NIST, Meteor and our proposed AM-FM) 
and the human-generated scores (adequacy, fluen-
cy and the harmonic mean of both; i.e. 2af/(a+f)). 
All correlation coefficients presented in the table 
are statistically significant with p<0.01 (where p is 
the probability of getting the same correlation 
coefficient, with a similar number of 86 samples, 
by chance).
 
Metric Adequacy Fluency H Mean 
BLEU 0.4232 0.4670 0.4516 
NIST 0.3178 0.3490 0.3396 
Meteor 0.4048 0.3920 0.4065 
AM-FM 0.3719 0.4558 0.4170 
 
Table 2: Pearson?s correlation coefficients (com-
puted at the system level) between automatic met-
rics and human-generated scores 
 
As seen from the table, BLEU is the metric ex-
hibiting the largest correlation coefficients with 
human-generated scores, followed by Meteor and 
AM-FM, while NIST exhibits the lowest correla-
tion coefficient values. Recall that our proposed 
AM-FM metric is not using reference translations 
for assessing translation quality, while the other 
three metrics are. 
In a similar exercise, the correlation coefficients 
were also computed at the sentence level (i.e. the 
units of analysis were sentences). These results are 
summarized in Table 3. As metrics are computed 
                                                          
4 As no development dataset was available for this particular 
task, a subset of the same evaluation dataset had to be used. 
156
at the sentence level, smoothed-bleu (Lin and Och, 
2004) was used in this case. Again, all correlation 
coefficients presented in the table are statistically 
significant with p<0.01.
 
Metric Adequacy Fluency H Mean 
sBLEU 0.3089 0.3361 0.3486 
NIST 0.1208 0.0834 0.1201 
Meteor 0.3220 0.3065 0.3405 
AM-FM 0.2142 0.2256 0.2406 
 
Table 3: Pearson?s correlation coefficients (com-
puted at the sentence level) between automatic 
metrics and human-generated scores 
 
As seen from the table, in this case, BLEU and 
Meteor are the metrics exhibiting the largest 
correlation coefficients, followed by AM-FM and 
NIST.
4.2 Reproducing Rankings   
In addition to adequacy and fluency, the WMT-07 
dataset includes rankings of sentence translations. 
To evaluate the usefulness of AM-FM and its 
components in a different evaluation setting, we 
also conducted a comparative evaluation on their 
capacity for predicting human-generated rankings.   
As ranking evaluations allowed for ties among 
sentence translations, we restricted our analysis to 
evaluate whether automatic metrics were able to 
predict the best, the worst and both sentence trans-
lations for each of the 4,060 available rankings5. 
The number of items per ranking varies from 2 to 
5, with an average of 4.11 items per ranking. Table 
4 presents the results of the comparative evaluation 
on predicting rankings. 
As seen from the table, Meteor is the automatic 
metric exhibiting the largest ranking prediction 
capability, followed by BLEU and NIST, while our 
proposed AM-FM metric exhibits the lowest rank-
ing prediction capability. However, it still performs 
well above random chance predictions, which, for 
the given average of 4 items per ranking, is about 
25% for best and worst ranking predictions, and 
about 8.33% for both. Again, recall that the AM-
FM metric is not using reference translations, 
while the other three metrics are. Also, it is worth 
mentioning that human rankings were conducted 
                                                          
5 We discarded those rankings involving the translation system 
for which translation outputs were not available that, conse-
quently, only had one translation output left. 
by looking at the reference translations and not the 
source. See Callison-Burch et al (2007) for details 
on the human evaluation task. 
 
Metric Best Worst Both 
sBLEU 51.08% 54.90% 37.86% 
NIST 49.56% 54.98% 37.36% 
Meteor 52.83% 58.03% 39.85% 
AM-FM 35.25% 41.11% 25.20% 
AM 37.19% 46.92% 28.47% 
FM 34.01% 39.01% 24.11% 
 
Table 4: Percentage of cases in which each auto-
matic metric is able to predict the best, the worst, 
and both ranked sentence translations 
 
Additionally, results for the individual compo-
nents, AM and FM, are also presented in the table. 
Notice how the AM component exhibits a better 
ranking capability than the FM component. 
5 Conclusions and Future Work 
This work presented AM-FM, a semantic frame-
work for translation quality assessment. Two com-
parative evaluations with standard metrics have 
been conducted over a large collection of human-
generated scores involving different languages. 
Although the obtained performance is below stand-
ard metrics, the proposed method has the main 
advantage of not requiring reference translations. 
Notice that a monolingual version of AM-FM is 
also possible by using monolingual latent semantic 
indexing (Landauer et al, 1998) along with a set of 
reference translations. A detailed evaluation of a 
monolingual implementation of AM-FM can be 
found in Banchs and Li (2011).  
As future research, we plan to study the impact 
of different dataset sizes and vector space model 
parameters for improving the performance of the 
AM component of the metric. This will include the 
study of learning curves based on the amount of 
training data used, and the evaluation of different 
vector model construction strategies, such as re-
moving stop-words and considering bigrams and 
word categories in addition to individual words.   
Finally, we also plan to study alternative uses of 
AM-FM within the context of statistical machine 
translation as, for example, a metric for MERT 
optimization, or using the AM component alone as 
an additional feature for decoding, rescoring and/or 
confidence estimation.
157
References  
Joshua S. Albrecht and Rebeca Hwa. 2007. Regression 
for sentence-level MT evaluation with pseudo 
references. In Proceedings of the 45th Annual 
Meeting of the Association of Computational 
Linguistics, 296-303. 
Rafael E. Banchs and Haizhou Li. 2011. Monolingual 
AM-FM: a two-dimensional machine translation 
evaluation method. Submitted to the Conference on 
Empirical Methods in Natural Language Processing. 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
an automatic metric for MT evaluation with 
improved correlation with human judgments. In 
Proceedings of the ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or 
Summarization, 65-72.  
John Blatz, Erin Fitzgerald, George Foster, Simona 
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto 
Sanchis and Nicola Ueffing. 2003. Confidence 
estimation for machine translation. Final Report 
WS2003 CLSP Summer Workshop, Johns Hopkins 
University   
Chris Callison-Burch, Cameron Fordyce,Philipp Koehn, 
Christof Monz and Josh Schroeder. 2007. (Meta-) 
evaluation of machine translation. In Proceedings of 
Statistical Machine Translation Workshop, 136-158. 
George Doddington. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Human 
Language Technology Conference. 
Susan Dumais, Thomas K. Landauer and Michael L. 
Littman. 1997. Automatic cross-linguistic 
information retrieval using latent semantic indexing. 
In Proceedings of the SIGIR Workshop on Cross-
Lingual Information Retrieval, 16-23. 
Michael Gamon, Anthony Aue and Martine Smets. 
2005. Sentence-level MT evaluation without 
reference translations: beyond language modeling. In 
Proceedings of the 10th Annual Conference of the 
European Association for Machine Translation, 103-
111. 
G. H. Golub and W. Kahan. 1965. Calculating the 
singular values and pseudo-inverse of a matrix. 
Journal of the Society for Industrial and Applied 
Mathematics: Numerical Analysis, 2(2):205-224. 
 
 
Thomas K. Landauer, Peter W. Foltz and Darrell 
Laham. 1998. Introduction to Latent Semantic 
Analysis. Discourse Processes, 25:259-284. 
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a 
method for evaluating automatic evaluation metrics 
for machine translation. In Proceedings of the 20th 
international conference on Computational 
Linguistics, pp 501, Morristown, NJ. 
Christopher D. Manning and Hinrich Schutze. 1999. 
Foundations of Statistical Natural Language 
Processing (Chapter 6). Cambridge, MA: The MIT 
Press. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jung Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
the Association for Computational Linguistics, 311-
318.  
Christopher B. Quirk. 2004. Training a sentence-level 
machine translation confidence measure. In 
Proceedings of the 4th International Conference on 
Language Resources and Evaluation, 825-828. 
Reinhard Rapp. 2009. The back-translation score: 
automatic MT evaluation at the sentences level 
without reference translations. In Proceedings of the 
ACL-IJCNLP, 133-136. 
Gerard M. Salton, Andrew K. Wong and C. S. Yang. 
1975. A vector space model for automatic indexing. 
Communications of the ACM, 18(11):613-620. 
Harold Somers. 2005. Round-trip translation: what is it 
good for? In proceedings of the Australasian 
Language Technology Workshop, 127-133. 
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran 
Wang and John Shawe-Taylor. 2009. Improving the 
confidence of machine translation quality estimates. 
In Proceedings of MT Summit XII. Ottawa, Canada. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. In Proceedings of the International 
Conference on Spoken Language Processing.  
John S. White, Theresa O?Cornell and Francis O?Nava. 
1994. The ARPA MT evaluation methodologies: 
evolution, lessons and future approaches. In 
Proceedings of the Association for Machine 
Translation in the Americas, 193-205. 
 
158
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 213?222,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Utilizing Dependency Language Models for Graph-based Dependency
Parsing Models
Wenliang Chen, Min Zhang?, and Haizhou Li
Human Language Technology, Institute for Infocomm Research, Singapore
{wechen, mzhang, hli}@i2r.a-star.edu.sg
Abstract
Most previous graph-based parsing models in-
crease decoding complexity when they use
high-order features due to exact-inference de-
coding. In this paper, we present an approach
to enriching high-order feature representations
for graph-based dependency parsing models
using a dependency language model and beam
search. The dependency language model is
built on a large-amount of additional auto-
parsed data that is processed by a baseline
parser. Based on the dependency language
model, we represent a set of features for the
parsing model. Finally, the features are effi-
ciently integrated into the parsing model dur-
ing decoding using beam search. Our ap-
proach has two advantages. Firstly we utilize
rich high-order features defined over a view
of large scope and additional large raw cor-
pus. Secondly our approach does not increase
the decoding complexity. We evaluate the pro-
posed approach on English and Chinese data.
The experimental results show that our new
parser achieves the best accuracy on the Chi-
nese data and comparable accuracy with the
best known systems on the English data.
1 Introduction
In recent years, there are many data-driven mod-
els that have been proposed for dependency parsing
(McDonald and Nivre, 2007). Among them, graph-
based dependency parsing models have achieved
state-of-the-art performance for a wide range of lan-
guages as shown in recent CoNLL shared tasks
?Corresponding author
(Buchholz and Marsi, 2006; Nivre et al, 2007).
In the graph-based models, dependency parsing is
treated as a structured prediction problem in which
the graphs are usually represented as factored struc-
tures. The information of the factored structures de-
cides the features that the models can utilize. There
are several previous studies that exploit high-order
features that lead to significant improvements.
McDonald et al (2005) and Covington (2001)
develop models that represent first-order features
over a single arc in graphs. By extending the first-
order model, McDonald and Pereira (2006) and Car-
reras (2007) exploit second-order features over two
adjacent arcs in second-order models. Koo and
Collins (2010) further propose a third-order model
that uses third-order features. These models utilize
higher-order feature representations and achieve bet-
ter performance than the first-order models. But this
achievement is at the cost of the higher decoding
complexity, from O(n2) to O(n4), where n is the
length of the input sentence. Thus, it is very hard to
develop higher-order models further in this way.
How to enrich high-order feature representations
without increasing the decoding complexity for
graph-based models becomes a very challenging
problem in the dependency parsing task. In this pa-
per, we solve this issue by enriching the feature rep-
resentations for a graph-based model using a depen-
dency language model (DLM) (Shen et al, 2008).
The N-gram DLM has the ability to predict the next
child based on the N-1 immediate previous children
and their head (Shen et al, 2008). The basic idea
behind is that we use the DLM to evaluate whether a
valid dependency tree (McDonald and Nivre, 2007)
213
is well-formed from a view of large scope. The pars-
ing model searches for the final dependency trees
by considering the original scores and the scores of
DLM.
In our approach, the DLM is built on a large
amount of auto-parsed data, which is processed
by an original first-order parser (McDonald et al,
2005). We represent the features based on the DLM.
The DLM-based features can capture the N-gram in-
formation of the parent-children structures for the
parsing model. Then, they are integrated directly
in the decoding algorithms using beam-search. Our
new parsing model can utilize rich high-order fea-
ture representations but without increasing the com-
plexity.
To demonstrate the effectiveness of the proposed
approach, we conduct experiments on English and
Chinese data. The results indicate that the approach
greatly improves the accuracy. In summary, we
make the following contributions:
? We utilize the dependency language model to
enhance the graph-based parsing model. The
DLM-based features are integrated directly into
the beam-search decoder.
? The new parsing model uses the rich high-order
features defined over a view of large scope and
and additional large raw corpus, but without in-
creasing the decoding complexity.
? Our parser achieves the best accuracy on the
Chinese data and comparable accuracy with the
best known systems on the English data.
2 Dependency language model
Language models play a very important role for sta-
tistical machine translation (SMT). The standard N-
gram based language model predicts the next word
based on the N?1 immediate previous words. How-
ever, the traditional N-gram language model can
not capture long-distance word relations. To over-
come this problem, Shen et al (2008) proposed a
dependency language model (DLM) to exploit long-
distance word relations for SMT. The N-gram DLM
predicts the next child of a head based on the N ? 1
immediate previous children and the head itself. In
this paper, we define a DLM, which is similar to the
one of Shen et al (2008), to score entire dependency
trees.
An input sentence is denoted by x =
(x0, x1, ..., xi, ..., xn), where x0 = ROOT and
does not depend on any other token in x and each
token xi refers to a word. Let y be a depen-
dency tree for x and H(y) be a set that includes the
words that have at least one dependent. For each
xh ? H(y), we have a dependency structure Dh =
(xLk, ...xL1, xh, xR1...xRm), where xLk, ...xL1 are
the children on the left side from the farthest to the
nearest and xR1...xRm are the children on the right
side from the nearest to the farthest. Probability
P (Dh) is defined as follows:
P (Dh) = PL(Dh)? PR(Dh) (1)
Here PL and PR are left and right side generative
probabilities respectively. Suppose, we use a N-
gram dependency language model. PL is defined as
follows:
PL(Dh) ? PLc(xL1|xh)
?PLc(xL2|xL1, xh)
?... (2)
?PLc(xLk|xL(k?1), ..., xL(k?N+1), xh)
where the approximation is based on the nth order
Markov assumption. The right side probability is
similar. For a dependency tree, we calculate the
probability as follows:
P (y) =
?
xh?H(y)
P (Dh) (3)
In this paper, we use a linear model to calculate
the scores for the parsing models (defined in Section
3.1). Accordingly, we reform Equation 3. We define
fDLM as a high-dimensional feature representation
which is based on arbitrary features of PLc, PRc and
x. Then, the DLM score of tree y is in turn computed
as the inner product of fDLM with a corresponding
weight vector wDLM .
scoreDLM (y) = fDLM ? wDLM (4)
3 Parsing with dependency language
model
In this section, we propose a parsing model which
includes the dependency language model by extend-
ing the model of McDonald et al (2005).
214
3.1 Graph-based parsing model
The graph-based parsing model aims to search for
the maximum spanning tree (MST) in a graph (Mc-
Donald et al, 2005). We write (xi, xj) ? y
if there is a dependency in tree y from word xi
to word xj (xi is the head and xj is the depen-
dent). A graph, denoted by Gx, consists of a set
of nodes Vx = {x0, x1, ..., xi, ..., xn} and a set of
arcs (edges) Ex = {(xi, xj)|i 6= j, xi ? Vx, xj ?
(Vx ? x0)}, where the nodes in Vx are the words
in x. Let T (Gx) be the set of all the subgraphs of
Gx that are valid dependency trees (McDonald and
Nivre, 2007) for sentence x.
The formulation defines the score of a depen-
dency tree y ? T (Gx) to be the sum of the edge
scores,
s(x, y) =
?
g?y
score(w, x, g) (5)
where g is a spanning subgraph of y. g can be a
single dependency or adjacent dependencies. Then
y is represented as a set of factors. The model
scores each factor using a weight vector w that con-
tains the weights for the features to be learned dur-
ing training using the Margin Infused Relaxed Algo-
rithm (MIRA) (Crammer and Singer, 2003; McDon-
ald and Pereira, 2006). The scoring function is
score(w, x, g) = f(x, g) ? w (6)
where f(x, g) is a high-dimensional feature repre-
sentation which is based on arbitrary features of g
and x.
The parsing model finds a maximum spanning
tree (MST), which is the highest scoring tree in
T (Gx). The task of the decoding algorithm for a
given sentence x is to find y?,
y? = argmax
y?T (Gx)
s(x, y) = argmax
y?T (Gx)
?
g?y
score(w, x, g)
3.2 Add DLM scores
In our approach, we consider the scores of the DLM
when searching for the maximum spanning tree.
Then for a given sentence x, we find y?DLM ,
y?DLM = argmax
y?T (Gx)
(
?
g?y
score(w, x, g)+scoreDLM (y))
After adding the DLM scores, the new parsing
model can capture richer information. Figure 1 illus-
trates the changes. In the original first-order parsing
model, we only utilize the information of single arc
(xh, xL(k?1)) for xL(k?1) as shown in Figure 1-(a).
If we use 3-gram DLM, we can utilize the additional
information of the two previous children (nearer to
xh than xL(k?1)): xL(k?2) and xL(k?3) as shown in
Figure 1-(b).
Figure 1: Adding the DLM scores to the parsing model
3.3 DLM-based feature templates
We define DLM-based features for Dh =
(xLk, ...xL1, xh, xR1...xRm). For each child xch on
the left side, we have PLc(xch|HIS), where HIS
refers to the N ? 1 immediate previous right chil-
dren and head xh. Similarly, we have PRc(xch|HIS)
for each child on the right side. Let Pu(xch|HIS)
(Pu(ch) in short) be one of the above probabilities.
We use the map function ?(Pu(ch)) to obtain the
predefined discrete value (defined in Section 5.3).
The feature templates are outlined in Table 1, where
TYPE refers to one of the types:PL or PR, h pos
refers to the part-of-speech tag of xh, h word refers
to the lexical form of xh, ch pos refers to the part-of-
speech tag of xch, and ch word refers to the lexical
form of xch.
4 Decoding
In this section, we turn to the problem of adding the
DLM in the decoding algorithm. We propose two
ways: (1) Rescoring, in which we rescore the K-
best list with the DLM-based features; (2) Intersect,
215
< ?(Pu(ch)),TYPE >
< ?(Pu(ch)),TYPE, h pos >
< ?(Pu(ch)),TYPE, h word >
< ?(Pu(ch)),TYPE, ch pos >
< ?(Pu(ch)),TYPE, ch word >
< ?(Pu(ch)),TYPE, h pos, ch pos >
< ?(Pu(ch)),TYPE, h word, ch word >
Table 1: DLM-based feature templates
in which we add the DLM-based features in the de-
coding algorithm directly.
4.1 Rescoring
We add the DLM-based features into the decoding
procedure by using the rescoring technique used in
(Shen et al, 2008). We can use an original parser
to produce the K-best list. This method has the po-
tential to be very fast. However, because the perfor-
mance of this method is restricted to the K-best list,
we may have to set K to a high number in order to
find the best parsing tree (with DLM) or a tree ac-
ceptably close to the best (Shen et al, 2008).
4.2 Intersect
Then, we add the DLM-based features in the decod-
ing algorithm directly. The DLM-based features are
generated online during decoding.
For our parser, we use the decoding algorithm
of McDonald et al (2005). The algorithm was ex-
tensions of the parsing algorithm of (Eisner, 1996),
which was a modified version of the CKY chart
parsing algorithm. Here, we describe how to add
the DLM-based features in the first-order algorithm.
The second-order and higher-order algorithms can
be extended by the similar way.
The parsing algorithm independently parses the
left and right dependents of a word and combines
them later. There are two types of chart items (Mc-
Donald and Pereira, 2006): 1) a complete item in
which the words are unable to accept more depen-
dents in a certain direction; and 2) an incomplete
item in which the words can accept more dependents
in a certain direction. In the algorithm, we create
both types of chart items with two directions for all
the word pairs in a given sentence. The direction of
a dependency is from the head to the dependent. The
right (left) direction indicates the dependent is on the
right (left) side of the head. Larger chart items are
created from pairs of smaller ones in a bottom-up
style. In the following figures, complete items are
represented by triangles and incomplete items are
represented by trapezoids. Figure 2 illustrates the
cubic parsing actions of the algorithm (Eisner, 1996)
in the right direction, where s, r, and t refer to the
start and end indices of the chart items. In Figure
2-(a), all the items on the left side are complete and
the algorithm creates the incomplete item (trapezoid
on the right side) of s ? t. This action builds a de-
pendency relation from s to t. In Figure 2-(b), the
item of s ? r is incomplete and the item of r ? t is
complete. Then the algorithm creates the complete
item of s ? t. In this action, all the children of r are
generated. In Figure 2, the longer vertical edge in a
triangle or a trapezoid corresponds to the subroot of
the structure (spanning chart). For example, s is the
subroot of the span s ? t in Figure 2-(a). For the left
direction case, the actions are similar.
Figure 2: Cubic parsing actions of Eisner (Eisner, 1996)
Then, we add the DLM-based features into the
parsing actions. Because the parsing algorithm is
in the bottom-up style, the nearer children are gen-
erated earlier than the farther ones of the same head.
Thus, we calculate the left or right side probabil-
ity for a new child when a new dependency rela-
tion is built. For Figure 2-(a), we add the features of
PRc(xt|HIS). Figure 3 shows the structure, where
cRs refers to the current children (nearer than xt) of
xs. In the figure, HIS includes cRs and xs.
Figure 3: Add DLM-based features in cubic parsing
216
We use beam search to choose the one having the
overall best score as the final parse, where K spans
are built at each step (Zhang and Clark, 2008). At
each step, we perform the parsing actions in the cur-
rent beam and then choose the best K resulting spans
for the next step. The time complexity of the new de-
coding algorithm is O(Kn3) while the original one
is O(n3), where n is the length of the input sentence.
With the rich feature set in Table 1, the running time
of Intersect is longer than the time of Rescoring. But
Intersect considers more combination of spans with
the DLM-based features than Rescoring that is only
given a K-best list.
5 Implementation Details
5.1 Baseline parser
We implement our parsers based on the MSTParser1,
a freely available implementation of the graph-based
model proposed by (McDonald and Pereira, 2006).
We train a first-order parser on the training data (de-
scribed in Section 6.1) with the features defined in
McDonald et al (2005). We call this first-order
parser Baseline parser.
5.2 Build dependency language models
We use a large amount of unannotated data to build
the dependency language model. We first perform
word segmentation (if needed) and part-of-speech
tagging. After that, we obtain the word-segmented
sentences with the part-of-speech tags. Then the
sentences are parsed by the Baseline parser. Finally,
we obtain the auto-parsed data.
Given the dependency trees, we estimate the prob-
ability distribution by relative frequency:
Pu(xch|HIS) =
count(xch,HIS)
?
x?ch
count(x?ch,HIS)
(7)
No smoothing is performed because we use the
mapping function for the feature representations.
5.3 Mapping function
We can define different mapping functions for the
feature representations. Here, we use a simple way.
First, the probabilities are sorted in decreasing order.
Let No(Pu(ch)) be the position number of Pu(ch)
in the sorted list. The mapping function is:
1http://mstparser.sourceforge.net
?(Pu(ch)) =
{ PH if No(Pu(ch)) ? TOP10
PM if TOP10 < No(Pu(ch)) ? TOP30
PL if TOP30 < No(Pu(ch))
PO if Pu(ch)) = 0
where TOP10 and TOP 30 refer to the position num-
bers of top 10% and top 30% respectively. The num-
bers, 10% and 30%, are tuned on the development
sets in the experiments.
6 Experiments
We conducted experiments on English and Chinese
data.
6.1 Data sets
For English, we used the Penn Treebank (Marcus et
al., 1993) in our experiments. We created a stan-
dard data split: sections 2-21 for training, section
22 for development, and section 23 for testing. Tool
?Penn2Malt?2 was used to convert the data into de-
pendency structures using a standard set of head
rules (Yamada and Matsumoto, 2003). Following
the work of (Koo et al, 2008), we used the MX-
POST (Ratnaparkhi, 1996) tagger trained on training
data to provide part-of-speech tags for the develop-
ment and the test set, and used 10-way jackknifing
to generate part-of-speech tags for the training set.
For the unannotated data, we used the BLLIP corpus
(Charniak et al, 2000) that contains about 43 million
words of WSJ text.3 We used the MXPOST tagger
trained on training data to assign part-of-speech tags
and used the Baseline parser to process the sentences
of the BLLIP corpus.
For Chinese, we used the Chinese Treebank
(CTB) version 4.04 in the experiments. We also used
the ?Penn2Malt? tool to convert the data and cre-
ated a data split: files 1-270 and files 400-931 for
training, files 271-300 for testing, and files 301-325
for development. We used gold standard segmenta-
tion and part-of-speech tags in the CTB. The data
partition and part-of-speech settings were chosen to
match previous work (Chen et al, 2008; Yu et al,
2008; Chen et al, 2009). For the unannotated data,
we used the XIN CMN portion of Chinese Giga-
word5 Version 2.0 (LDC2009T14) (Huang, 2009),
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
3We ensured that the text used for extracting subtrees did not
include the sentences of the Penn Treebank.
4http://www.cis.upenn.edu/?chinese/.
5We excluded the sentences of the CTB data from the Giga-
word data
217
which has approximately 311 million words whose
segmentation and POS tags are given. We discarded
the annotations due to the differences in annotation
policy between CTB and this corpus. We used the
MMA system (Kruengkrai et al, 2009) trained on
the training data to perform word segmentation and
POS tagging and used the Baseline parser to parse
all the sentences in the data.
6.2 Features for basic and enhanced parsers
The previous studies have defined four types of
features: (FT1) the first-order features defined in
McDonald et al (2005), (FT2SB) the second-order
parent-siblings features defined in McDonald and
Pereira (2006), (FT2GC) the second-order parent-
child-grandchild features defined in Carreras (2007),
and (FT3) the third-order features defined in (Koo
and Collins, 2010).
We used the first- and second-order parsers of
the MSTParser as the basic parsers. Then we en-
hanced them with other higher-order features us-
ing beam-search. Table 2 shows the feature set-
tings of the systems, where MST1/2 refers to the ba-
sic first-/second-order parser and MSTB1/2 refers to
the enhanced first-/second-order parser. MSTB1 and
MSTB2 used the same feature setting, but used dif-
ferent order models. This resulted in the difference
of using FT2SB (beam-search in MSTB1 vs exact-
inference in MSTB2). We used these four parsers as
the Baselines in the experiments.
System Features
MST1 (FT1)
MSTB1 (FT1)+(FT2SB+FT2GC+FT3)
MST2 (FT1+FT2SB)
MSTB2 (FT1+FT2SB)+(FT2GC+FT3)
Table 2: Baseline parsers
We measured the parser quality by the unlabeled
attachment score (UAS), i.e., the percentage of to-
kens (excluding all punctuation tokens) with the cor-
rect HEAD. In the following experiments, we used
?Inter? to refer to the parser with Intersect, and
?Rescore? to refer to the parser with Rescoring.
6.3 Development experiments
Since the setting of K (for beam search) affects our
parsers, we studied its influence on the development
set for English. We added the DLM-based features
to MST1. Figure 4 shows the UAS curves on the
development set, where K is beam size for Inter-
sect and K-best for Rescoring, the X-axis represents
K, and the Y-axis represents the UAS scores. The
parsing performance generally increased as the K
increased. The parser with Intersect always outper-
formed the one with Rescoring.
 0.912
 0.914
 0.916
 0.918
 0.92
 0.922
 0.924
 0.926
 0.928
1 2 4 8 16
UA
S
K
Rescore
Inter
Figure 4: The influence of K on the development data
K 1 2 4 8 16
English 157.1 247.4 351.9 462.3 578.2
Table 3: The parsing times on the development set (sec-
onds for all the sentences)
Table 3 shows the parsing times of Intersect on
the development set for English. By comparing the
curves of Figure 4, we can see that, while using
larger K reduced the parsing speed, it improved the
performance of our parsers. In the rest of the ex-
periments, we set K=8 in order to obtain the high
accuracy with reasonable speed and used Intersect
to add the DLM-based features.
N 0 1 2 3 4
English 91.30 91.87 92.52 92.72 92.72
Chinese 87.36 87.96 89.33 89.92 90.40
Table 4: Effect of different N-gram DLMs
Then, we studied the effect of adding different N-
gram DLMs to MST1. Table 4 shows the results.
From the table, we found that the parsing perfor-
mance roughly increased as the N increased. When
N=3 and N=4, the parsers obtained the same scores
for English. For Chinese, the parser obtained the
best score when N=4. Note that the size of the Chi-
nese unannotated data was larger than that of En-
glish. In the rest of the experiments, we used 3-gram
for English and 4-gram for Chinese.
218
6.4 Main results on English data
We evaluated the systems on the testing data for En-
glish. The results are shown in Table 5, where -
DLM refers to adding the DLM-based features to the
Baselines. The parsers using the DLM-based fea-
tures consistently outperformed the Baselines. For
the basic models (MST1/2), we obtained absolute
improvements of 0.94 and 0.63 points respectively.
For the enhanced models (MSTB1/2), we found that
there were 0.63 and 0.66 points improvements re-
spectively. The improvements were significant in
McNemar?s Test (p < 10?5)(Nivre et al, 2004).
Order1 UAS Order2 UAS
MST1 90.95 MST2 91.71
MST-DLM1 91.89 MST-DLM2 92.34
MSTB1 91.92 MSTB2 92.10
MSTB-DLM1 92.55 MSTB-DLM2 92.76
Table 5: Main results for English
6.5 Main results on Chinese data
The results are shown in Table 6, where the abbrevi-
ations used are the same as those in Table 5. As in
the English experiments, the parsers using the DLM-
based features consistently outperformed the Base-
lines. For the basic models (MST1/2), we obtained
absolute improvements of 4.28 and 3.51 points re-
spectively. For the enhanced models (MSTB1/2),
we got 3.00 and 2.93 points improvements respec-
tively. We obtained large improvements on the Chi-
nese data. The reasons may be that we use the very
large amount of data and 4-gram DLM that captures
high-order information. The improvements were
significant in McNemar?s Test (p < 10?7).
Order1 UAS Order2 UAS
MST1 86.38 MST2 88.11
MST-DLM1 90.66 MST-DLM2 91.62
MSTB1 88.38 MSTB2 88.66
MSTB-DLM1 91.38 MSTB-DLM2 91.59
Table 6: Main results for Chinese
6.6 Compare with previous work on English
Table 7 shows the performance of the graph-based
systems that were compared, where McDonald06
refers to the second-order parser of McDonald
and Pereira (2006), Koo08-standard refers to the
second-order parser with the features defined in
Koo et al (2008), Koo10-model1 refers to the
third-order parser with model1 of Koo and Collins
(2010), Koo08-dep2c refers to the second-order
parser with cluster-based features of (Koo et al,
2008), Suzuki09 refers to the parser of Suzuki et
al. (2009), Chen09-ord2s refers to the second-order
parser with subtree-based features of Chen et al
(2009), and Zhou11 refers to the second-order parser
with web-derived selectional preference features of
Zhou et al (2011).
The results showed that our MSTB-DLM2 ob-
tained the comparable accuracy with the previous
state-of-the-art systems. Koo10-model1 (Koo and
Collins, 2010) used the third-order features and
achieved the best reported result among the super-
vised parsers. Suzuki2009 (Suzuki et al, 2009) re-
ported the best reported result by combining a Semi-
supervised Structured Conditional Model (Suzuki
and Isozaki, 2008) with the method of (Koo et al,
2008). However, their decoding complexities were
higher than ours and we believe that the performance
of our parser can be further enhanced by integrating
their methods with our parser.
Type System UAS Cost
G
McDonald06 91.5 O(n3)
Koo08-standard 92.02 O(n4)
Koo10-model1 93.04 O(n4)
S
Koo08-dep2c 93.16 O(n4)
Suzuki09 93.79 O(n4)
Chen09-ord2s 92.51 O(n3)
Zhou11 92.64 O(n4)
D MSTB-DLM2 92.76 O(Kn3)
Table 7: Relevant results for English. G denotes the su-
pervised graph-based parsers, S denotes the graph-based
parsers with semi-supervised methods, D denotes our
new parsers
6.7 Compare with previous work on Chinese
Table 8 shows the comparative results, where
Chen08 refers to the parser of (Chen et al, 2008),
Yu08 refers to the parser of (Yu et al, 2008), Zhao09
refers to the parser of (Zhao et al, 2009), and
Chen09-ord2s refers to the second-order parser with
subtree-based features of Chen et al (2009). The
results showed that our score for this data was the
219
best reported so far and significantly higher than the
previous scores.
System UAS
Chen08 86.52
Yu08 87.26
Zhao09 87.0
Chen09-ord2s 89.43
MSTB-DLM2 91.59
Table 8: Relevant results for Chinese
7 Analysis
Dependency parsers tend to perform worse on heads
which have many children. Here, we studied the ef-
fect of DLM-based features for this structure. We
calculated the number of children for each head and
listed the accuracy changes for different numbers.
We compared the MST-DLM1 and MST1 systems
on the English data. The accuracy is the percentage
of heads having all the correct children.
Figure 5 shows the results for English, where the
X-axis represents the number of children, the Y-
axis represents the accuracies, OURS refers to MST-
DLM1, and Baseline refers to MST1. For example,
for heads having two children, Baseline obtained
89.04% accuracy while OURS obtained 89.32%.
From the figure, we found that OURS achieved bet-
ter performance consistently in all cases and when
the larger the number of children became, the more
significant the performance improvement was.
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3  4  5  6  7  8  9  10
Ac
cu
ra
cy
Number of children
Baseline
OURS
Figure 5: Improvement relative to numbers of children
8 Related work
Several previous studies related to our work have
been conducted.
Koo et al (2008) used a clustering algorithm to
produce word clusters on a large amount of unan-
notated data and represented new features based on
the clusters for dependency parsing models. Chen
et al (2009) proposed an approach that extracted
partial tree structures from a large amount of data
and used them as the additional features to im-
prove dependency parsing. They approaches were
still restricted in a small number of arcs in the
graphs. Suzuki et al (2009) presented a semi-
supervised learning approach. They extended a
Semi-supervised Structured Conditional Model (SS-
SCM)(Suzuki and Isozaki, 2008) to the dependency
parsing problem and combined their method with
the approach of Koo et al (2008). In future work,
we may consider apply their methods on our parsers
to improve further.
Another group of methods are the co-
training/self-training techniques. McClosky et
al. (2006) presented a self-training approach for
phrase structure parsing. Sagae and Tsujii (2007)
used the co-training technique to improve perfor-
mance. First, two parsers were used to parse the
sentences in unannotated data. Then they selected
some sentences which have the same trees produced
by those two parsers. They retrained a parser on
newly parsed sentences and the original labeled
data. We are able to use the output of our systems
for co-training/self-training techniques.
9 Conclusion
We have presented an approach to utilizing the de-
pendency language model to improve graph-based
dependency parsing. We represent new features
based on the dependency language model and in-
tegrate them in the decoding algorithm directly us-
ing beam-search. Our approach enriches the feature
representations but without increasing the decoding
complexity. When tested on both English and Chi-
nese data, our parsers provided very competitive per-
formance compared with the best systems on the En-
glish data and achieved the best performance on the
Chinese data in the literature.
References
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
220
CoNLL-X. SIGNLL.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957?961, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP 1987-
89 WSJ Corpus Release 1, LDC2000T43. Linguistic
Data Consortium.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto,
Yujie Zhang, and Hitoshi Isahara. 2008. Dependency
parsing with short dependency relations in unlabeled
data. In Proceedings of IJCNLP 2008.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of EMNLP 2009, pages 570?579, Singapore,
August.
Michael A. Covington. 2001. A dundamental algorithm
for dependency parsing. In Proceedings of the 39th
Annual ACM Southeast Conference, pages 95?102.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
COLING1996, pages 340?345.
Chu-Ren Huang. 2009. Tagged Chinese Gigaword Ver-
sion 2.0, LDC2009T14. Linguistic Data Consortium.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL
2010, pages 1?11, Uppsala, Sweden, July. Association
for Computational Linguistics.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceedings
of ACL-08: HLT, Columbus, Ohio, June.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL-IJCNLP2009, pages
513?521, Suntec, Singapore, August. Association for
Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguisticss, 19(2):313?330.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of Coling-ACL, pages 337?344.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL, pages 122?131.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL 2006, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98.
Association for Computational Linguistics.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proc. of CoNLL 2004, pages
49?56.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 915?932.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser ensem-
bles. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 1044?1050.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL-08: HLT,
pages 665?673, Columbus, Ohio, June. Association
for Computational Linguistics.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proceedings of EMNLP2009, pages 551?560, Sin-
gapore, August. Association for Computational Lin-
guistics.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
K. Yu, D. Kawahara, and S. Kurohashi. 2008. Chi-
nese dependency parsing with large scale automati-
cally constructed case structures. In Proceedings of
Coling 2008, pages 1049?1056, Manchester, UK, Au-
gust.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing. In Proceedings of EMNLP
2008, pages 562?571, Honolulu, Hawaii, October.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing us-
221
ing a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55?63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting web-derived selectional preference to im-
prove statistical dependency parsing. In Proceedings
of ACL-HLT2011, pages 1556?1565, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
222
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 902?911,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Modeling the Translation of Predicate-Argument Structure for SMT
Deyi Xiong, Min Zhang?, Haizhou Li
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632
{dyxiong, mzhang, hli}@i2r.a-star.edu.sg
Abstract
Predicate-argument structure contains rich se-
mantic information of which statistical ma-
chine translation hasn?t taken full advantage.
In this paper, we propose two discriminative,
feature-based models to exploit predicate-
argument structures for statistical machine
translation: 1) a predicate translation model
and 2) an argument reordering model. The
predicate translation model explores lexical
and semantic contexts surrounding a verbal
predicate to select desirable translations for
the predicate. The argument reordering model
automatically predicts the moving direction
of an argument relative to its predicate af-
ter translation using semantic features. The
two models are integrated into a state-of-the-
art phrase-based machine translation system
and evaluated on Chinese-to-English transla-
tion tasks with large-scale training data. Ex-
perimental results demonstrate that the two
models significantly improve translation accu-
racy.
1 Introduction
Recent years have witnessed increasing efforts to-
wards integrating predicate-argument structures into
statistical machine translation (SMT) (Wu and Fung,
2009b; Liu and Gildea, 2010). In this paper, we take
a step forward by introducing a novel approach to in-
corporate such semantic structures into SMT. Given
a source side predicate-argument structure, we at-
tempt to translate each semantic frame (predicate
and its associated arguments) into an appropriate tar-
get string. We believe that the translation of predi-
cates and reordering of arguments are the two central
?Corresponding author
issues concerning the transfer of predicate-argument
structure across languages.
Predicates1 are essential elements in sentences.
Unfortunately they are usually neither correctly
translated nor translated at all in many SMT sys-
tems according to the error study by Wu and Fung
(2009a). This suggests that conventional lexical and
phrasal translation models adopted in those SMT
systems are not sufficient to correctly translate pred-
icates in source sentences. Thus we propose a
discriminative, feature-based predicate translation
model that captures not only lexical information
(i.e., surrounding words) but also high-level seman-
tic contexts to correctly translate predicates.
Arguments contain information for questions of
who, what, when, where, why, and how in sentences
(Xue, 2008). One common error in translating ar-
guments is about their reorderings: arguments are
placed at incorrect positions after translation. In or-
der to reduce such errors, we introduce a discrim-
inative argument reordering model that uses the
position of a predicate as the reference axis to es-
timate positions of its associated arguments on the
target side. In this way, the model predicts moving
directions of arguments relative to their predicates
with semantic features.
We integrate these two discriminative models into
a state-of-the-art phrase-based system. Experimen-
tal results on large-scale Chinese-to-English transla-
tion show that both models are able to obtain signif-
icant improvements over the baseline. Our analysis
on system outputs further reveals that they can in-
deed help reduce errors in predicate translations and
argument reorderings.
1We only consider verbal predicates in this paper.
902
The paper is organized as follows. In Section 2,
we will introduce related work and show the signif-
icant differences between our models and previous
work. In Section 3 and 4, we will elaborate the pro-
posed predicate translation model and argument re-
ordering model respectively, including details about
modeling, features and training procedure. Section
5 will introduce how to integrate these two models
into SMT. Section 6 will describe our experiments
and results. Section 7 will empirically discuss how
the proposed models improve translation accuracy.
Finally we will conclude with future research direc-
tions in Section 8.
2 Related Work
Predicate-argument structures (PAS) are explored
for SMT on both the source and target side in some
previous work. As PAS analysis widely employs
global and sentence-wide features, it is computa-
tionally expensive to integrate target side predicate-
argument structures into the dynamic programming
style of SMT decoding (Wu and Fung, 2009b).
Therefore they either postpone the integration of tar-
get side PASs until the whole decoding procedure is
completed (Wu and Fung, 2009b), or directly project
semantic roles from the source side to the target side
through word alignments during decoding (Liu and
Gildea, 2010).
There are other previous studies that explore only
source side predicate-argument structures. Komachi
and Matsumoto (2006) reorder arguments in source
language (Japanese) sentences using heuristic rules
defined on source side predicate-argument structures
in a pre-processing step. Wu et al (2011) automate
this procedure by automatically extracting reorder-
ing rules from predicate-argument structures and ap-
plying these rules to reorder source language sen-
tences. Aziz et al (2011) incorporate source lan-
guage semantic role labels into a tree-to-string SMT
system.
Although we also focus on source side predicate-
argument structures, our models differ from the pre-
vious work in two main aspects: 1) we propose two
separate discriminative models to exploit predicate-
argument structures for predicate translation and ar-
gument reordering respectively; 2) we consider ar-
gument reordering as an argument movement (rel-
ative to its predicate) prediction problem and use
a discriminatively trained classifier for such predic-
tions.
Our predicate translation model is also related to
previous discriminative lexicon translation models
(Berger et al, 1996; Venkatapathy and Bangalore,
2007; Mauser et al, 2009). While previous models
predict translations for all words in vocabulary, we
only focus on verbal predicates. This will tremen-
dously reduce the amount of training data required,
which usually is a problem in discriminative lexi-
con translation models (Mauser et al, 2009). Fur-
thermore, the proposed translation model also dif-
fers from previous lexicon translation models in that
we use both lexical and semantic features. Our ex-
perimental results show that semantic features are
able to further improve translation accuracy.
3 Predicate Translation Model
In this section, we present the features and the train-
ing process of the predicate translation model.
3.1 Model
Following the context-dependent word models in
(Berger et al, 1996), we propose a discriminative
predicate translation model. The essential compo-
nent of our model is a maximum entropy classifier
pt(e|C(v)) that predicts the target translation e for
a verbal predicate v given its surrounding context
C(v). The classifier can be formulated as follows.
pt(e|C(v)) =
exp(?i ?ifi(e, C(v)))
?
e? exp(
?
i ?ifi(e?, C(v)))
(1)
where fi are binary features, ?i are weights of these
features. Given a source sentence which contains
N verbal predicates {vi}N1 , our predicate translation
model Mt can be denoted as
Mt =
N
?
i=1
pt(evi |C(vi)) (2)
Note that we do not restrict the target translation
e to be a single word. We allow e to be a phrase
of length up to 4 words so as to capture multi-word
translations for a verbal predicate. For example, a
Chinese verb ?u1(issue)? can be translated as ?to
be issued? or ?have issued? with modality words.
903
This will increase the number of classes to be pre-
dicted by the maximum entropy classifier. But ac-
cording to our observation, it is still computation-
ally tractable (see Section 3.3). If a verbal predicate
is not translated, we set e = NULL so that we can
also capture null translations for verbal predicates.
3.2 Features
The apparent advantage of discriminative lexicon
translation models over generative translation mod-
els (e.g., conventional lexical translation model as
described in (Koehn et al, 2003)) is that discrim-
inative models allow us to integrate richer contexts
(lexical, syntactic or semantic) into target translation
prediction. We use two kinds of features to predict
translations for verbal predicates: 1) lexical features
and 2) semantic features. All features are in the fol-
lowing binary form.
f(e, C(v)) =
{
1, if e = ? and C(v).? = ?
0, else
(3)
where the symbol ? is a placeholder for a possible
target translation (up to 4 words), the symbol ? indi-
cates a contextual (lexical or semantic) element for
the verbal predicate v, and the symbol ? represents
the value of ?.
Lexical Features: The lexical element ? is
extracted from the surrounding words of verbal
predicate v. We use the preceding 3 words and
the succeeding 3 words to define the lexical con-
text for the verbal predicate v. Therefore ? ?
{w?3, w?2, w?1, v, w1, w2, w3}.
Semantic Features: The semantic element ? is
extracted from the surrounding arguments of ver-
bal predicate v. In particular, we define a seman-
tic window centered at the verbal predicate with
6 arguments {A?3, A?2, A?1, A1, A2, A3} where
A?3 ? A?1 are arguments on the left side of v
while A1 ? A3 are those on the right side. Differ-
ent verbal predicates have different number of argu-
ments in different linguistic scenarios. We observe
on our training data that the number of arguments for
96.5% verbal predicates on each side (left/right) is
not larger than 3. Therefore the defined 6-argument
semantic window is sufficient to describe argument
contexts for predicates.
For each argument Ai in the defined seman-
f(e, C(v)) = 1 if and only if
e = adjourn and C(v).Ah?3 =Sn?
e = adjourn and C(v).Ar?1 = ARGM-TMP
e = adjourn and C(v).Ah1 =U
e = adjourn and C(v).Ar2 = null
e = adjourn and C(v).Ah3 = null
Table 1: Semantic feature examples.
tic window, we use its semantic role (i.e., ARG0,
ARGM-TMP and so on) Ari and head word Ahi to
define semantic context elements ?. If an argument
Ai does not exist for the verbal predicate v 2, we set
the value of both Ari and Ahi to null.
Figure 1 shows a Chinese sentence with its
predicate-argument structure and English transla-
tion. The verbal predicate ?>?/adjourn? (in bold)
has 4 arguments: one in an ARG0 agent role, one
in an ARGM-ADV adverbial modifier role, one in
an ARGM-TMP temporal modifier role and the last
one in an ARG1 patient role. Table 1 shows several
semantic feature examples of this verbal predicate.
3.3 Training
In order to train the discriminative predicate transla-
tion model, we first parse source sentences and la-
beled semantic roles for all verbal predicates (see
details in Section 6.1) in our word-aligned bilingual
training data. Then we extract all training events for
verbal predicates which occur at least 10 times in
the training data. A training event for a verbal predi-
cate v consists of all contextual elements C(v) (e.g.,
w1, Ah1 ) defined in the last section and the target
translation e. Using these events, we train one max-
imum entropy classifier per verbal predicate (16,121
verbs in total) via the off-the-shelf MaxEnt toolkit3.
We perform 100 iterations of the L-BFGS algorithm
implemented in the training toolkit for each verbal
predicate with both Gaussian prior and event cutoff
set to 1 to avoid overfitting. After event cutoff, we
have an average of 140 classes (target translations)
per verbal predicate with the maximum number of
classes being 9,226. The training takes an average of
52.6 seconds per verb. In order to expedite the train-
2For example, the verb v has only two arguments on its left
side. Thus argument A?3 doest not exist.
3Available at: http://homepages.inf.ed.ac.uk/lzhang10/
maxent toolkit.html
904
The [Security Council] will adjourn for [4 days] [starting Thursday]
Sn?1 ?2 [g3 ?o4 m?5] >?6 [o7 U8]
ARG0
ARGM-ADV
ARGM-TMP
ARG1
Figure 1: An example of predicate-argument structure in Chinese and its aligned English translation. The bold word in
Chinese is the verbal predicate. The subscripts on the Chinese sentence show the indexes of words from left to right.
ing, we run the training toolkit in a parallel manner.
4 Argument Reordering Model
In this section we introduce the discriminative ar-
gument reordering model, features and the training
procedure.
4.1 Model
Since the predicate determines what arguments are
involved in its semantic frame and semantic frames
tend to be cohesive across languages (Fung et al,
2006), the movements of predicate and its arguments
across translations are like the motions of a planet
and its satellites. Therefore we consider the reorder-
ing of an argument as the motion of the argument
relative to its predicate. In particular, we use the po-
sition of the predicate as the reference axis. The mo-
tion of associated arguments relative to the reference
axis can be roughly divided into 3 categories4: 1) no
change across languages (NC); 2) moving from the
left side of its predicate to the right side of the predi-
cate after translation (L2R); and 3) moving from the
right side of its predicate to the left side of the pred-
icate after translation (R2L).
Let?s revisit Figure 1. The ARG0, ARGM-ADV
and ARG1 are located at the same side of their predi-
cate after being translated into English, therefore the
reordering category of these three arguments is as-
signed as ?NC?. The ARGM-TMP is moved from
the left side of ?>?/adjourn? to the right side of
?adjourn? after translation, thus its reordering cate-
gory is L2R.
In order to predict the reordering category for
an argument, we propose a discriminative argu-
ment reordering model that uses a maximum en-
4Here we assume that the translations of arguments are not
interrupted by their predicates, other arguments or any words
outside the arguments in question. We leave for future research
the task of determining whether arguments should be translated
as a unit or not.
tropy classifier to calculate the reordering category
m ? {NC, L2R, R2L} for an argument A as fol-
lows.
pr(m|C(A)) =
exp(?i ?ifi(m, C(A)))
?
m? exp(
?
i ?ifi(m?, C(A)))
(4)
where C(A) indicates the surrounding context of A.
The features fi will be introduced in the next sec-
tion. We assume that motions of arguments are in-
dependent on each other. Given a source sentence
with labeled arguments {Ai}N1 , our discriminative
argument reordering model Mr is formulated as
Mr =
N
?
i=1
pr(mAi |C(Ai)) (5)
4.2 Features
The features fi used in the argument reordering
model still takes the binary form as in Eq. (3). Table
2 shows the features that are used in the argument
reordering model. We extract features from both the
source and target side. On the source side, the fea-
tures include the verbal predicate, the semantic role
of the argument, the head word and the boundary
words of the argument. On the target side, the trans-
lation of the verbal predicate, the translation of the
head word of the argument, as well as the boundary
words of the translation of the argument are used as
features.
4.3 Training
To train the argument reordering model, we first ex-
tract features defined in the last section from our
bilingual training data where source sentences are
annotated with predicate-argument structures. We
also study the distribution of argument reordering
categories (i.e.,NC, L2R and R2L) in the training
data, which is shown in Table 3. Most arguments,
accounting for 82.43%, are on the same side of their
verbal predicates after translation. The remaining
905
Features of an argument A for reordering
src
its verbal predicate Ap
its semantic role Ar
its head word Ah
the leftmost word of A
the rightmost word of A
tgt
the translation of Ap
the translation of Ah
the leftmost word of the translation of A
the rightmost word of the translation of A
Table 2: Features adopted in the argument reordering
model.
Reordering Category Percent
NC 82.43%
L2R 11.19%
R2L 6.38%
Table 3: Distribution of argument reordering categories
in the training data.
arguments (17.57%) are moved either from the left
side of their predicates to the right side after transla-
tion (accounting for 11.19%) or from the right side
to the left side of their translated predicates (ac-
counting for 6.38%).
After all features are extracted, we use the maxi-
mum entropy toolkit in Section 3.3 to train the maxi-
mum entropy classifier as formulated in Eq. (4). We
perform 100 iterations of L-BFGS.
5 Integrating the Two Models into SMT
In this section, we elaborate how to integrate the two
models into phrase-based SMT. In particular, we in-
tegrate the models into a phrase-based system which
uses bracketing transduction grammars (BTG) (Wu,
1997) for phrasal translation (Xiong et al, 2006).
Since the system is based on a CKY-style decoder,
the integration algorithms introduced here can be
easily adapted to other CKY-based decoding sys-
tems such as the hierarchical phrasal system (Chi-
ang, 2007).
5.1 Integrating the Predicate Translation
Model
It is straightforward to integrate the predicate trans-
lation model into phrase-based SMT (Koehn et al,
2003; Xiong et al, 2006). We maintain word
alignments for each phrase pair in the phrase ta-
ble. Given a source sentence with its predicate-
argument structure, we detect all verbal predicates
and load trained predicate translation classifiers for
these verbs. Whenever a hypothesis covers a new
verbal predicate v, we find the target translation e
for v through word alignments and then calculate its
translation probability pt(e|C(v)) according to Eq.
(1).
The predicate translation model (as formulated in
Eq. (2)) is integrated into the whole log-linear model
just like the conventional lexical translation model
in phrase-based SMT (Koehn et al, 2003). The
two models are independently estimated but comple-
mentary to each other. While the lexical translation
model calculates the probability of a verbal predi-
cate being translated given its local lexical context,
the discriminative predicate translation model is able
to employ both lexical and semantic contexts to pre-
dict translations for verbs.
5.2 Integrating the Argument Reordering
Model
Before we introduce the integration algorithm for
the argument reordering model, we define two
functions A and N on a source sentence and its
predicate-argument structure ? as follows.
? A(i, j, ?): from the predicate-argument struc-
ture ? , the function finds all predicate-argument
pairs which are completely located within the
span from source word i to j. For example, in
Figure 1, A(3, 6, ?) = {(>?, ARGM-TMP)}
while A(2, 3, ?) = {}, A(1, 5, ?) = {} because
the verbal predicate ?>?? is located outside
the span (2,3) and (1,5).
? N (i, k, j, ?): the function finds all predicate-
argument pairs that cross the two neighboring
spans (i, k) and (k+1, j). It can be formulated
as A(i, j, ?)? (A(i, k, ?)?A(k + 1, j, ?)).
We then define another function Pr to calculate
the argument reordering model probability on all ar-
guments which are found by the previous two func-
tions A and N as follows.
Pr(B) =
?
A?B
pr(mA|C(A)) (6)
906
where B denotes either A or N .
Following (Chiang, 2007), we describe the algo-
rithm in a deductive system. It is shown in Figure
2. The algorithm integrates the argument reordering
model into a CKY-style decoder (Xiong et al, 2006).
The item [X, i, j] denotes a BTG node X spanning
from i to j on the source side. For notational con-
venience, we only show the argument reordering
model probability for each item, ignoring all other
sub-model probabilities such as the language model
probability. The Eq. (7) shows how we calculate the
argument reordering model probability when a lex-
ical rule is applied to translate a source phrase c to
a target phrase e. The Eq. (8) shows how we com-
pute the argument reordering model probability for a
span (i, j) in a dynamic programming manner when
a merging rule is applied to combine its two sub-
spans in a straight (X ? [X1, X2]) or inverted or-
der (X ? ?X1, X2?). We directly use the probabili-
ties Pr(A(i, k, ?)) and Pr(A(k + 1, j, ?)) that have
been already obtained for the two sub-spans (i, k)
and (k + 1, j). In this way, we only need to calcu-
late the probability Pr(N (i, k, j, ?)) for predicate-
argument pairs that cross the two sub-spans.
6 Experiments
In this section, we present our experiments on
Chinese-to-English translation tasks, which are
trained with large-scale data. The experiments are
aimed at measuring the effectiveness of the proposed
discriminative predicate translation model and argu-
ment reordering model.
6.1 Setup
The baseline system is the BTG-based phrasal sys-
tem (Xiong et al, 2006). Our training corpora5
consist of 3.8M sentence pairs with 96.9M Chinese
words and 109.5M English words. We ran GIZA++
on these corpora in both directions and then applied
the ?grow-diag-final? refinement rule to obtain word
alignments. We then used all these word-aligned
corpora to generate our phrase table. Our 5-gram
language model was trained on the Xinhua section
of the English Gigaword corpus (306 million words)
5The corpora include LDC2004E12, LDC2004T08,
LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06,
LDC2003E07 and LDC2004T07.
using the SRILM toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing.
To train the proposed predicate translation model
and argument reordering model, we first parsed all
source sentences using the Berkeley Chinese parser
(Petrov et al, 2006) and then ran the Chinese se-
mantic role labeler6 (Li et al, 2010) on all source
parse trees to annotate semantic roles for all verbal
predicates. After we obtained semantic roles on the
source side, we extracted features as described in
Section 3.2 and 4.2 and used these features to train
our two models as described in Section 3.3 and 4.3.
We used the NIST MT03 evaluation test data as
our development set, and the NIST MT04, MT05
as the test sets. We adopted the case-insensitive
BLEU-4 (Papineni et al, 2002) as the evaluation
metric. Statistical significance in BLEU differences
was tested by paired bootstrap re-sampling (Koehn,
2004).
6.2 Results
Our first group of experiments is to investigate
whether the predicate translation model is able to
improve translation accuracy in terms of BLEU and
whether semantic features are useful. The experi-
mental results are shown in Table 4. From the table,
we have the following two observations.
? The proposed predicate translation models
achieve an average improvement of 0.57 BLEU
points across the two NIST test sets when all
features (lex+sem) are used. Such an improve-
ment is statistically significant (p < 0.01). Ac-
cording to our statistics, there are 5.07 verbal
predicates per sentence in NIST04 and 4.76
verbs per sentence in NIST05, which account
for 18.02% and 16.88% of all words in NIST04
and 05 respectively. This shows that not only
verbal predicates are semantically important,
they also form a major part of the sentences.
Therefore, whether verbal predicates are trans-
lated correctly or not has a great impact on the
translation accuracy of the whole sentence 7.
6Available at: http://nlp.suda.edu.cn/?jhli/.
7The example in Table 6 shows that the translations of
verbs even influences reorderings and translations of neighbor-
ing words.
907
X ? c/e
[X, i, j] : Pr(A(i, j, ?))
(7)
X ? [X1, X2] or ?X1, X2? [X1, i, k] : Pr(A(i, k, ?)) [X2, k + 1, j] : Pr(A(k + 1, j, ?))
[X, i, j] : Pr(A(i, k, ?)) ? Pr(A(k + 1, j, ?)) ? Pr(N (i, k, j, ?))
(8)
Figure 2: Integrating the argument reordering model into a BTG-style decoder.
Model NIST04 NIST05
Base 35.52 33.80
Base+PTM (lex) 35.71+ 34.09+
Base+PTM (lex+sem) 36.10++** 34.35++*
Table 4: Effects of the proposed predicate translation
model (PTM). PTM (lex): predicate translation model
with lexical features; PTM (lex+sem): predicate transla-
tion model with both lexical and semantic features; +/++:
better than the baseline (p < 0.05/0.01). */**: better
than Base+PTM (lex) (p < 0.05/0.01).
Model NIST04 NIST05
Base 35.52 33.80
Base+ARM 35.82++ 34.29++
Base+ARM+PTM 36.19++ 34.72++
Table 5: Effects of the proposed argument reordering
model (ARM) and the combination of ARM and PTM.
++: better than the baseline (p < 0.01).
? When we integrate both lexical and semantic
features (lex+sem) described in Section 3.2, we
obtain an improvement of about 0.33 BLEU
points over the system where only lexical fea-
tures (lex) are used. Such a gain, which is sta-
tistically significant, confirms the effectiveness
of semantic features.
Our second group of experiments is to validate
whether the argument reordering model is capable
of improving translation quality. Table 5 shows the
results. We obtain an average improvement of 0.4
BLEU points on the two test sets over the base-
line when we incorporate the proposed argument re-
ordering model into our system. The improvements
on the two test sets are both statistically significant
(p < 0.01).
Finally, we integrate both the predicate translation
model and argument reordering model into the final
system. The two models collectively achieve an im-
provement of up to 0.92 BLEU points over the base-
line, which is shown in Table 5.
7 Analysis
In this section, we conduct some case studies to
show how the proposed models improve translation
accuracy by looking into the differences that they
make on translation hypotheses.
Table 6 displays a translation example which
shows the difference between the baseline and
the system enhanced with the predicate translation
model. There are two verbal predicates ?` /head
to? and ??\/attend? in the source sentence. In
order to get the most appropriate translations for
these two verbal predicates, we should adopt differ-
ent ways to translate them. The former should be
translated as a corresponding verb word or phrase
while the latter into a preposition word ?for?. Unfor-
tunately, the baseline incorrectly translates the two
verbs. Furthermore, such translation errors even re-
sult in undesirable reorderings of neighboring words
??|?/Bethlehem and ??g/mass?. This indi-
cates that verbal predicate translation errors may
lead to more errors, such as inappropriate reorder-
ings or lexical choices for neighboring words. On
the contrary, we can see that our predicate transla-
tion model is able to help select appropriate words
for both verbs. The correct translations of these two
verbs also avoid incorrect reorderings of neighbor-
ing words.
Table 7 shows another example to demonstrate
how the argument reordering model improve re-
orderings. The verbal predicate ??1/carry out?
has three arguments, ARG0, ARG-ADV and ARG1.
The ARG1 argument should be moved from the
right side of the predicate to its left side after trans-
lation. The ARG0 argument can either stay on the
left side or move to right side of the predicate. Ac-
908
Base
[?Z] &? ` ?|? ?\ [?S?] ?g
[thousands of] followers to Mass in Bethlehem [Christmas Eve]
Base+PTM
[?Z] &? ` ?|? ?\ [?S?] ?g
[thousands of] devotees [rushed to] Bethlehem for [Christmas Eve] mass
Ref thousands of worshippers head to Bethlehem for Christmas Midnight mass
Table 6: A translation example showing the difference between the baseline and the system with the predicate transla-
tion model (PTM). Phrase alignments in the two system outputs are shown with dashed lines. Chinese words in bold
are verbal predicates.
PAS [k'?@/J?wX?] ?? ?1 [??????]
ARG0
ARGM-ADV
ARG1
Base
[k'?] @ /J [?wX?] ?? [?1??] [????]
the more [important consultations] also set disaster [warning system]
Base+ARM
k' [?@] /J [?wX?] [???1] [??] [????]
more [important consultations] on [such a] disaster [warning system] [should be carried out]
Ref more important discussions will be held on the disaster warning system
Table 7: A translation example showing the difference between the baseline and the system with the argument re-
ordering model (ARM). The predicate-argument structure (PAS) of the source sentence is also displayed in the first
row.
cording to the phrase alignments of the baseline,
we clearly observe three serious translation errors:
1) the ARG0 argument is translated into separate
groups which are not adjacent on the target side;
2) the predicate is not translated at all; and 3) the
ARG1 argument is not moved to the left side of the
predicate after translation. All of these 3 errors are
avoided in the Base+ARM system output as a re-
sult of the argument reordering model that correctly
identifies arguments and moves them in the right di-
rections.
8 Conclusions and Future Work
We have presented two discriminative models to
incorporate source side predicate-argument struc-
tures into SMT. The two models have been inte-
grated into a phrase-based SMT system and evalu-
ated on Chinese-to-English translation tasks using
large-scale training data. The first model is the pred-
icate translation model which employs both lexical
and semantic contexts to translate verbal predicates.
The second model is the argument reordering model
which estimates the direction of argument move-
ment relative to its predicate after translation. Ex-
perimental results show that both models are able to
significantly improve translation accuracy in terms
of BLEU score.
In the future work, we will extend our predicate
translation model to translate both verbal and nom-
inal predicates. Nominal predicates also frequently
occur in Chinese sentences and thus accurate trans-
lations of them are desirable for SMT. We also want
to address another translation issue of arguments as
shown in Table 7: arguments are wrongly translated
into separate groups instead of a cohesive unit (Wu
and Fung, 2009a). We will build an argument seg-
mentation model that follows (Xiong et al, 2011) to
determine whether arguments should be translated
as a unit or not.
909
References
Wilker Aziz, Miguel Rios, and Lucia Specia. 2011. Shal-
low semantic trees for smt. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
316?322, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Pascale Fung, Wu Zhaojun, Yang Yongsheng, and Dekai
Wu. 2006. Automatic learning of chinese english se-
mantic structure mapping. In IEEE/ACL 2006 Work-
shop on Spoken Language Technology (SLT 2006),
Aruba, December.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 58?54, Edmon-
ton, Canada, May-June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Mamoru Komachi and Yuji Matsumoto. 2006. Phrase
reordering for statistical machine translation based on
predicate-argument structure. In In Proceedings of the
International Workshop on Spoken Language Trans-
lation: Evaluation Campaign on Spoken Language
Translation, pages 77?82.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of chinese. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1108?
1117, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Ding Liu and Daniel Gildea. 2010. Semantic role
features for machine translation. In Proceedings of
the 23rd International Conference on Computational
Linguistics (Coling 2010), pages 716?724, Beijing,
China, August. Coling 2010 Organizing Committee.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 210?218, Singa-
pore, August. Association for Computational Linguis-
tics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Andreas Stolcke. 2002. Srilm?an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, Colorado, USA, September.
Sriram Venkatapathy and Srinivas Bangalore. 2007.
Three models for discriminative machine translation
using global lexical selection and sentence reconstruc-
tion. In Proceedings of SSST, NAACL-HLT 2007 /
AMTA Workshop on Syntax and Structure in Statisti-
cal Translation, pages 96?102, Rochester, New York,
April. Association for Computational Linguistics.
Dekai Wu and Pascale Fung. 2009a. Can semantic
role labeling improve smt. In Proceedings of the
13th Annual Conference of the EAMT, pages 218?225,
Barcelona, May.
Dekai Wu and Pascale Fung. 2009b. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 13?16, Boulder, Colorado,
June. Association for Computational Linguistics.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting pre-
ordering rules from predicate-argument structures. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 29?37, Chiang
Mai, Thailand, November. Asian Federation of Natu-
ral Language Processing.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 521?528, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2011. A
maximum-entropy segmentation model for statistical
machine translation. IEEE Transactions on Audio,
Speech and Language Processing, 19(8):2494?2505.
910
Nianwen Xue. 2008. Labeling chinese predicates
with semantic roles. Computational Linguistics,
34(2):225?255.
911
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 37?42,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
IRIS: a Chat-oriented Dialogue System based on the Vector Space Model 
 
 
Rafael E. Banchs Haizhou Li 
Human Language Technology Human Language Technology 
Institute for Infocomm Research Institute for Infocomm Research 
Singapore 138632 Singapore 138632 
rembanchs@i2r.a-star.edu.sg hli@i2r.a-star.edu.sg 
 
 
 
Abstract 
This system demonstration paper presents 
IRIS (Informal Response Interactive Sys-
tem), a chat-oriented dialogue system based 
on the vector space model framework. The 
system belongs to the class of example-
based dialogue systems and builds its chat 
capabilities on a dual search strategy over a 
large collection of dialogue samples. Addi-
tional strategies allowing for system adap-
tation and learning implemented over the 
same vector model space framework are 
also described and discussed.    
1 Introduction 
Dialogue systems have been gaining popularity re-
cently as the demand for such kind of applications 
have increased in many different areas. Addition-
ally, recent advances in other related language 
technologies such as speech recognition, discourse 
analysis and natural language understanding have 
made possible for dialogue systems to find practi-
cal applications that are commercially exploitable 
(Pieraccini et al, 2009; Griol et al, 2010).   
From the application point of view, dialogue 
systems can be categorized into two major classes: 
task-oriented and chat-oriented. In the case of task-
oriented dialogue systems, the main objective of 
such a system is to help the user to complete a task, 
which typically includes booking transportation or 
accommodation services, requesting specific infor-
mation from a service facility, etc. (Busemann et 
al., 1997; Seneff and Polifroni, 2000; Stallard, 
2000). On the other hand, chat-oriented systems 
are not intended to help the user completing any 
specific task, but to provide a means for participa-
ting in a game, or just for chitchat or entertain-
ment. Typical examples of chat-oriented dialogue 
systems are the so called chat bots (Weizenbaum, 
1966; Ogura et al, 2003, Wallis, 2010).     
In this paper, we introduce IRIS (Informal Res-
ponse Interactive System), a chat-oriented dialogue 
system that is based on the vector space model 
framework (Salton et al, 1975; van Rijsbergen, 
2005). From the operational point of view, IRIS 
belongs to the category of example-based dialogue 
systems (Murao et al, 2003). Its dialogue strategy 
is supported by a large database of dialogues that is 
used to provide candidate responses to a given user 
input. The search for candidate responses is per-
formed by computing the cosine similarity metric 
into the vector space model representation, in 
which each utterance in the dialogue database is 
represented by a vector. 
Different from example-based question answer-
ing systems (Vicedo, 2002; Xue et al, 2008), IRIS 
uses a dual search strategy. In addition to the cur-
rent user input, which is compared with all existent 
utterances in the database, a vector representation 
of the current dialogue history is also compared 
with vector representations of full dialogues in the 
database. Such a dual search strategy allows for in-
corporating information about the dialogue context 
into the response selection process. 
The rest of the paper is structured as follows. 
Section 2 presents the architecture of IRIS as well 
as provides a general description of the dataset that 
has been used for its implementation. Section 3 
presents some illustrative examples of dialogues 
generated by IRIS, and Section 4 presents the main 
conclusions of this work. 
37
2 The IRIS Implementation  
In this section we first provide a detailed descrip-
tion of the IRIS architecture along with the most 
relevant issues behind its implementation. Then, 
we describe the specific dialogue dataset that sup-
ports the IRIS implementation.  
2.1 Architecture 
As already mentioned, IRIS architecture is heavily 
based on a vector space model framework, which 
includes a standard similarity search module from 
vector-based information retrieval systems (Salton 
and McGill, 1983). However, it also implements 
some additional modules that provide the system 
with capabilities for automatic chatting.   
Figure 1 depicts a block diagram that illustrates 
the main modules in the IRIS architecture. As seen 
from the picture, the whole system comprises se-
ven processing modules and three repositories.     
  
Figure 1: General block diagram for IRIS          
The main operation of IRIS can be described as 
follows. When a new dialogue starts, the control of 
the dialogue is passed from the dialogue manage-
ment module to the initiation/ending module. This 
module implements a two-state dialogue strategy 
which main objectives are: first, to greet the user 
and self-introduce IRIS and, second, to collect the 
name of the user. This module uses a basic parsing 
algorithm that is responsible for extracting the 
user?s name from the provided input. The name is 
the first vocabulary term learned by IRIS, which is 
stored in the vocabulary learning repository. 
Once the dialogue initiation has been concluded 
the dialogue management system gains back the 
control of the dialogue and initializes the current 
history vector. Two types of vector initializations 
are possible here. If the user is already know by 
IRIS, it will load the last stored dialogue history 
for that user; otherwise, IRIS will randomly select 
one dialogue history vector from the dialogue data-
base. After this initialization, IRIS prompts the 
user for what he desires to do. From this moment, 
the example-based chat strategy starts.  
For each new input from the user, the dialogue 
management module makes a series of actions that, 
after a decision process, can lead to different types 
of responses. In the first action, the dynamic repla-
cement module searches for possible matches bet-
ween the terms within the vocabulary learning 
repository and the input string. In a new dialogue, 
the only two terms know by IRIS are its own name 
and the user name. If any of this two terms are 
identified, they are automatically replaced by the 
placeholders <self-name> and <other-name>, res-
pectively. 
In the case of a mature dialogue, when there are 
more terms into the vocabulary learning repository, 
every term matched in the input is replaced by its 
corresponding definition stored in the vocabulary 
learning database.  
Just after the dynamic replacement is conducted, 
tokenization and vectorization of the user input is 
carried out. During tokenization, an additional 
checking is conducted by the dialogue manager. It 
looks for any adaptation command that could be 
possibly inserted at the beginning of the user input. 
More details on adaptation commands will be 
given when describing the style/manner adaptation 
module. Immediately after tokenization, unknown 
vocabulary terms (OOVs) are identified. IRIS will 
consider as OOV any term that is not contained in 
either the dialogue or vocabulary learning data-
bases. In case an OOV is identified, a set of heuris-
tics (aiming at avoiding confusing misspellings 
with OOVs) are applied to decide whether IRIS 
should ask the user for the meaning of such a term. 
38
If IRIS decides to ask for the meaning of the 
term, the control of the dialogue is passed to the 
vocabulary learning module which is responsible 
for collecting the meaning of the given term from 
the user or, alternatively, from an external source 
of information. Once the definition is collected and 
validated, it is stored along with the OOV term into 
the vocabulary learning repository. After comple-
ting a learning cycle, IRIS acknowledges the user 
about having ?understood? the meaning of the term 
and control is passed back to the dialogue manage-
ment module, which waits for a new user input.  
If IRIS decides not to ask for the meaning of the 
OOV term, or if no OOV term has been identified, 
vectorization of the user input is completed by the 
vector similarity modules and similarity scores are 
computed for retrieving best matches from the 
dialogue database. Two different similarity scores 
are actually used by IRIS. The first score is applied 
at the utterance level. It computes the cosine 
similarities between the current user input vector 
and all single utterances stored in the database. 
This score is used for retrieving a large amount of 
candidate utterances from the dialogue database, 
generally between 50 and 100, depending on the 
absolute value of the associated scores. 
The second score is computed over history 
vectors. The current dialogue history, which is 
available from the current history repository, inclu-
des all utterances interchanged by the current user 
and IRIS. In other to facilitate possible topic chan-
ges along the dialogue evolution, a damping or 
?forgetting? factor is used for giving more impor-
tance to the most recent utterances in the dialogue 
history. A single vector representation is then com-
puted for the currently updated dialogue history 
after applying the damping factor. The cosine 
similarity between this vector and the vector repre-
sentations for each full dialogue stored in the dia-
logue database are computed and used along with 
the utterance-level score for generating a final rank 
of candidate utterances. A log-linear combination 
scheme is used for combining the two scores. The 
dialogue management module randomly selects 
one of the top ranked utterances and prompts back 
to the user the corresponding reply (from the dia-
logue database) to the wining utterance. 
Just immediately before prompting back the res-
ponse to the user, the dynamic replacement module 
performs an inverse operation for replacing the two 
placeholders <self-name> and <other-name>, in 
case they occur in the response, by their actual 
values. 
The final action taken by IRIS is related to the 
style/manner adaptation module. For this action to 
take place the user has to include one of three pos-
sible adaptations commands at the beginning of 
her/his new turn. The three adaptation commands 
recognized by IRIS are: ban (*), reinforce (+), and 
discourage (?). By using any of these three charac-
ters as the first character in the new turn, the user is 
requesting IRIS to modify the vector space repre-
sentation of the previous selected response as 
follows: 
? Ban (*): IRIS will mark its last response as a 
prohibited response and will not show such 
response ever again. 
? Reinforce (+): IRIS will pull the vector space 
representation of its last selected utterance 
towards the vector space representation of the 
previous user turn, so that the probability of 
generating the same response given a similar 
user input will be increased. 
? Discourage (?): IRIS will push the vector 
space representation of its last selected utter-
ance apart from the vector space represen-
tation of the previous user turn, so that the 
probability of generating the same response 
given a similar user input will be decreased. 
2.2 Dialogue Data Collection 
For the current implementation of IRIS, a subset of 
the Movie-DiC dialogue data collection has been 
used (Banchs, 2012). Movie-DiC is a dialogue 
corpus that has been extracted from movie scripts 
which are freely available at The Internet Movie 
Script Data Collection (http://www.imsdb.com/). 
In this subsection, we present a brief description on 
the specific data subset used for the implementa-
tion of IRIS, as well as we briefly review the 
process followed for collecting the data and ex-
tracting the dialogues. 
First of all, dialogues have to be identified and 
parsed from the collected html files. Three basic 
elements are extracted from the scripts: speakers, 
utterances and context. The speaker and utterance 
elements contain information about the characters 
who speak and what they said at each dialogue 
turn. On the other hand, context elements contain 
all the additional information (explanations and 
descriptions) appearing in the scripts. 
39
The extracted dialogues are stored into a data 
structure such that the information about turn se-
quences within the dialogues and dialogue sequen-
ces within the scripts are preserved. 
Some post-processing is also necessary to filter 
out and/or repair the most common parsing errors 
occurring during the dialogue extraction phase. 
Some of these errors include: bad script formatting, 
same-speaker turn continuations, explanatory notes 
inserted within the turns, misspelling of names in 
the speaker headers, changes in the encoding for-
mat, etc. 
The final dialogue collection used in the IRIS 
implementation consists of dialogues from 153 
movie scripts, mainly belonging to the comedy, 
action and family genres. Table 1 summarizes the 
main statistics of the resulting dataset. 
 
Total number of movie scripts 153 
Total number of  dialogues 24,265 
Total number of  speaker turns 159,182 
Average amount of dialogues per movie 158.59 
Average amount of turns per dialogue 6.56 
Total number of running words 1,670,879
Overall vocabulary size 34,903 
 
Table 2: Main statistics of the movie dialogue 
dataset used in the IRIS implementation 
 
For each turn in the dialogue collection, a vector 
space model representation was constructed. For 
this, the standard bag-of-words weighting scheme 
known as TF-IDF was used (Spark, 1972; Salton 
and Buckley, 1988).  
Before performing the vectorization, word toke-
nization was conducted. In this step, all punctua-
tion marks were removed, with the exception of 
the question ??? and exclamation ?!? marks. Simi-
larly, all other non-alphanumeric characters occur-
ring in the utterances were removed as well. Also 
during the tokenization phase, all self-references to 
current speaker names in the utterances were re-
placed by the <self-name> placeholder, as well as 
all references to the names of other speakers 
participating in the same dialogue were replaced 
by the <other-name> place-holder. 
Finally, a vector space model representation was 
also computed for each full dialogue in the collec-
tion. For this bag-of-words model at the dialogue 
level, both utterance and context information were 
taken into account. Again, the TF-IDF weighting 
scheme was used.  
3 Some Dialogue Examples 
In this section we show some real examples of 
interactions between IRIS and human users. First, 
we present some interesting examples of good per-
formance, as well as illustrate some of the learning 
capabilities of IRIS. Then, we present some of the 
common failures which identify specific points of 
attention for further improvements.  
3.1 Good Performance Examples 
Our first example illustrates the beginning of a 
typical chat session between IRIS and a new user. 
This example is depicted in Table 2.   
 
# Spkr. Turn 
1 IRIS Hi, I?m IRIS? What?s your name? 
2 USER Hi Iris, I am Peter 
3 IRIS Nice name Peter... What do you want to talk 
about? 
4 USER Sports? 
5 IRIS How come in all that time I was trying to get 
you a date, you never asked me out? 
6 USER ok, do you want to go for dinner tonight? 
7 IRIS Yes. 
8 USER What kind of food do you like? 
9 IRIS Anything but guts and mushrooms. 
10 USER Ok pick you up at seven! 
11 IRIS Ok. Pick me up at seven. 
 
Table 2: Beginning of a chat session between IRIS 
and a new user 
 
For the dialogue depicted in Table 2, turn num-
bers 1, 2 and 3 are processed by the dialogue intia-
tion/ending module. The example-based dialogue 
management strategy starts from turn 4 onwards. 
Notice that as far as this is a new user, not previous 
dialogue history exists, so in this case a random 
history vector has been selected and instead of 
focusing in the sports topic suggested by the user, 
IRIS ?takes the initiative? of asking for a date.  
In our second example, which is presented in 
Table 3, we illustrate the beginning of a typical 
chat session between IRIS and a returning user. 
For this particular user, her last interaction with 
IRIS was about sports. 
Similar to the previous example, turn 1 is pro-
cessed by the dialogue intiation/ending module and 
the example-based dialogue management strategy 
starts from turn 2 onwards. In this particular case, 
IRIS is much more centered on the sports topic as 
this context information has been already provided 
40
by the stored dialogue history of the previous chat 
session with this particular user. 
 
# Spkr. Turn 
1 IRIS Hey, Helen... welcome back! How are you 
doing? 
2 USER fine and you? 
3 IRIS Is there more? 
4 USER Yes, I finally went to that tennis match I told 
you the last time. 
5 IRIS The match! 
6 USER Yes, have you seen some sports recently? 
7 IRIS I hate sports. 
 
Table 3: Beginning of a chat session between IRIS 
and a returning user 
 
In our third example, which is presented in 
Table 4, we illustrate the learning of a new vocabu-
lary word by IRIS. In this example, when the un-
known term paella is detected, the control of the 
dialogue is passed to the vocabulary learning mo-
dule, which takes care of turns 7, 8, 9 and 10. 
 
# Spkr. Turn 
1 USER do you cook anything Iris? 
2 IRIS Eggs. 
3 USER ha ha ha! 
4 IRIS Ha-ha-ha! Ha-ha-ha! 
5 USER Can you cook anything else? 
6 IRIS Do you cook? 
7 USER Paella 
8 IRIS What the hell is that???? 
9 USER It is a Spanish food. Yellow rice with some 
seafood on it. 
10 IRIS Ok, I see... thanks! 
11 USER So, do you want some paella today? 
12 IRIS Well, it beats working in a seafood restaurant...
 
Table 4: Chat segment in which IRIS learns the 
new vocabulary term paella 
 
Notice that when the user asks IRIS about 
having some paella today, IRIS is already able to 
associate it with seafood as it was stated in the 
user?s provided definition. The process actually 
occurs as follows: after tokenization, but before 
vectorization of the user input in turn 11, the 
dynamic replacement module substitutes the term 
paella by its definition, which has been previously 
stored in the vocabulary learning repository when 
turn 9 was processed. The actual user input that is 
finally vectorized in turn 11 is the following one: 
so do you want some it is a spanish food yellow 
rice with some seafood on it today ?, which is the 
utterance used by IRIS to retrieve and select the 
response it provides in turn 12. 
3.2 Common Failure Examples 
In this subsection we focus our attention in the 
most common failures exhibited by IRIS. Some of 
these failures put in evidence specific points of 
attention that should be taken into account for 
further improvements of the system.    
Our first example illustrates the problem of IRIS 
lack of consistency in issues for which consistent 
answers are required. Two specific chat segments 
in which IRIS provides inconsistent responses are 
presented in Table 5.  
 
# Spkr. Turn 
1 USER how old are you Iris? 
2 IRIS I'm thirteen, how old are you? 
3 USER I am 40 
4 IRIS Only I did better... 40 crates a day. 
5 USER how old are you Iris? 
6 IRIS Ten.
# Spkr. Turn 
1 USER Yes, have you seen some sports recently? 
2 IRIS I hate sports. 
3 USER Really, what about soccer? 
4 IRIS I'd sooner watch soccer. 
5 USER what do you prefer soccer or football? 
6 IRIS Football is my life. 
 
Table 5: IRIS incapability to being consistent 
 
The first example presented in Table 5 constitu-
tes a serious consistency problem. In this case IRIS 
has reported two different ages in the same chat 
session. The second case, although not so serious 
as the previous one, also constitutes a consistency 
failure. In this case IRIS states Football is my life 
just two turns after saying I hate sports. 
Our second example, which is presented in Ta-
ble 6, illustrates a problem derived from the noise 
that is still present in the dataset.  
       
# Spkr. Turn 
1 IRIS You watching the ballgame? 
2 USER No 
3 IRIS Bianca stares at him for a moment. 
 
Table 6: Example of noise in the dialogue dataset 
 
In the particular example illustrated in Table 6, 
as seen from turn 3, a context element has been 
41
mistakenly stored in the data collection as an 
utterance during the dataset preparation phase. 
Several problems similar to this one, which are 
related to noise in the database, have been detected 
already. To tackle this problem we need to refine 
the parsing and post-processing algorithms used 
during the dialogue dataset construction phase. 
4 Conclusions and Future Work  
In this paper, we have presented IRIS (Informal 
Response Interactive System), a chat-oriented dia-
logue system that is based on the vector space 
model framework. The system belongs to the class 
of example-based dialogue systems and builds its 
chat capabilities on a dual search strategy over a 
large collection of movie dialogues.  
Additional strategies allowing for system adap-
tation and learning have been also implemented 
over the same vector space model framework. 
More specifically, IRIS is capable of learning new 
vocabulary terms and semantically relating them to 
previous knowledge, as well as adapting its dia-
logue decisions to some stated user preferences.  
We have also described the main characteristics 
of the architecture of IRIS and the most important 
functions performed by each of its constituent 
modules. Finally, we have provided some exam-
ples of good chat performance and some examples 
of the common failures exhibited by IRIS.  
As future work, we intend to improve IRIS per-
formance by addressing some of the already identi-
fied common failures. Similarly, we intend to aug-
ment IRIS chatting capabilities by extending the 
size of the current dialogue database and integra-
ting a strategy for group chatting. 
Acknowledgments 
The authors would like to thank the Institute for 
Infocomm Research for its support and permission 
to publish this work. 
References  
Banchs R E (2012) Movie-DiC: a movie dialogue cor-
pus for research and development. In Proc. of the 50th 
Annual Meeting of the ACL. 
Busemann S, Declerck T, Diagne A, Dini L, Klein J, 
Schmeier S (1997) Natural language dialogue service 
for appointment scheduling agents. In Proc. of the 5th 
Conference on Applied NLP, pp 25-32. 
Griol D, Callejas Z, Lopez-Cozar R (2010) Statistical 
dialog management methodologies for real applica-
tions. In Proc. of SIGDIAL?10, pp 269-272. 
Murao H, Kawaguchi N, Matsubara S, Yamaguchi Y, 
Inagaki Y (2003) Example-based spoken dialogue 
system using WOZ system log. In Proc. of the 4th 
SIGDIAL, pp 140-148. 
Ogura K, Masuda T, Ishizaki M (2003) Building a new 
Internet chat system for sharing timing information. 
In Proc. of the 4th SIGDIAL, pp 97-104. 
Pieraccini R, Suendermann D, Dayanidhi K, Liscombe J 
(2009) Are we there yet? Research in commercial 
spoken dialog systems. In Proc. of TSD?09, pp 3-13. 
Salton G, Wong A, Yang C (1975) A vector space mo-
del for automatic indexing. Communications of the 
ACM 18(11):613-620. 
Salton G, McGill M (1983) Introduction to modern 
information retrieval. McGraw-Hill.  
Salton G, Buckley C (1988) Term-weighting approa-
ches in automatic text retrieval. Information Proces-
sing & Management 24(5):513-523 
Seneff S, Polifroni J (2000) Dialogue management in 
the Mercury flight reservation system. In Proc. of the 
ANLP-NAACL 2000 Workshop on Conversational 
Systems, pp 11-16. 
Spark K (1972) A statistical interpretation of term speci-
ficity and its application in retrieval. Journal of Do-
cumentation 28(1):11-21  
Stallard D (2000) Talk?n?travel: a conversational system 
for air travel planning. In Proc. of the 6th Conference 
on Applied NLP, pp 68-75. 
van Rijsbergen C (2005) A probabilistic logic for infor-
mation retrieval. In Advances in Information Retrie-
val, Lecture Notes in Computer Science 3408:1-6.   
Vicedo J (2002) SEMQA: A semantic model applied to 
question answering systems. PhD Thesis, University 
of Alicante. 
Wallis P (2010) A robot in the kitchen. In Proceedings 
of the ACL 2010 Workshop on Companionable Dia-
logue Systems, pp 25-30. 
Weizenbaum J (1966) ELIZA ? A computer program 
for the study of natural language communication be-
tween man and machine. Communications of the 
ACM 9(1):36-45.  
Xue X, Jeon J, Croft W (2008) Retrieval models for 
question and answer archives. In Proc. of the 31st 
Annual International ACM SIGIR Conference on 
R&D in Information Retrieval, pp 475-482. 
42
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190?195,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Broadcast News Story Segmentation Using Manifold Learning on Latent
Topic Distributions
Xiaoming Lu1,2, Lei Xie1?, Cheung-Chi Leung2, Bin Ma2, Haizhou Li2
1School of Computer Science, Northwestern Polytechnical University, China
2Institute for Infocomm Research, A?STAR, Singapore
luxiaomingnpu@gmail.com, lxie@nwpu.edu.cn, {ccleung,mabin,hli}@i2r.a-star.edu.sg
Abstract
We present an efficient approach for
broadcast news story segmentation using a
manifold learning algorithm on latent top-
ic distributions. The latent topic distribu-
tion estimated by Latent Dirichlet Alloca-
tion (LDA) is used to represent each text
block. We employ Laplacian Eigenmap-
s (LE) to project the latent topic distribu-
tions into low-dimensional semantic rep-
resentations while preserving the intrinsic
local geometric structure. We evaluate t-
wo approaches employing LDA and prob-
abilistic latent semantic analysis (PLSA)
distributions respectively. The effects of
different amounts of training data and dif-
ferent numbers of latent topics on the two
approaches are studied. Experimental re-
sults show that our proposed LDA-based
approach can outperform the correspond-
ing PLSA-based approach. The proposed
approach provides the best performance
with the highest F1-measure of 0.7860.
1 Introduction
Story segmentation refers to partitioning a mul-
timedia stream into homogenous segments each
embodying a main topic or coherent story (Allan,
2002). With the explosive growth of multimedia
data, it becomes difficult to retrieve the most rel-
evant components. For indexing broadcast news
programs, it is desirable to divide each of them
into a number of independent stories. Manual seg-
mentation is accurate but labor-intensive and cost-
ly. Therefore, automatic story segmentation ap-
proaches are highly demanded.
Lexical-cohesion based approaches have been
widely studied for automatic broadcast news story
segmentation (Beeferman et al, 1997; Choi, 1999;
Hearst, 1997; Rosenberg and Hirschberg, 2006;
?corresponding author
Lo et al, 2009; Malioutov and Barzilay, 2006;
Yamron et al, 1999; Tur et al, 2001). In this
kind of approaches, the audio portion of the da-
ta stream is passed to an automatic speech recog-
nition (ASR) system. Lexical cues are extracted
from the ASR transcripts. Lexical cohesion is the
phenomenon that different stories tend to employ
different sets of terms. Term repetition is one of
the most common appearances.
These rigid lexical-cohesion based approach-
es simply take term repetition into consideration,
while term association in lexical cohesion is ig-
nored. Moreover, polysemy and synonymy are not
considered. To deal with these problems, some
topic model techniques which provide conceptu-
al level matching have been introduced to text and
story segmentation task (Hearst, 1997). Proba-
bilistic latent semantic analysis (PLSA) (Hofman-
n, 1999) is a typical instance and used widely.
PLSA is the probabilistic variant of latent seman-
tic analysis (LSA) (Choi et al, 2001), and offers a
more solid statistical foundation. PLSA provides
more significant improvement than LSA for story
segmentation (Lu et al, 2011; Blei and Moreno,
2001).
Despite the success of PLSA, there are con-
cerns that the number of parameters in PLSA
grows linearly with the size of the corpus. This
makes PLSA not desirable if there is a consid-
erable amount of data available, and causes seri-
ous over-fitting problems (Blei, 2012). To deal
with this issue, Latent Dirichlet Allocation (L-
DA) (Blei et al, 2003) has been proposed. LDA
has been proved to be effective in many segmenta-
tion tasks (Arora and Ravindran, 2008; Hall et al,
2008; Sun et al, 2008; Riedl and Biemann, 2012;
Chien and Chueh, 2012).
Recent studies have shown that intrinsic di-
mensionality of natural text corpus is significant-
ly lower than its ambient Euclidean space (Belkin
and Niyogi, 2002; Xie et al, 2012). Therefore,
190
Laplacian Eigenmaps (LE) was proposed to com-
pute corresponding natural low-dimensional struc-
ture. LE is a geometrically motivated dimen-
sionality reduction method. It projects data into
a low-dimensional representation while preserv-
ing the intrinsic local geometric structure infor-
mation (Belkin and Niyogi, 2002). The locali-
ty preserving property attempts to make the low-
dimensional data representation more robust to the
noise from ASR errors (Xie et al, 2012).
To further improve the segmentation perfor-
mance, using latent topic distributions and LE in-
stead of term frequencies to represent text blocks
is studied in this paper. We study the effects of
the size of training data and the number of latent
topics on the LDA-based and the PLSA-based ap-
proaches. Another related work (Lu et al, 2013)
is to use local geometric information to regularize
the log-likelihood computation in PLSA.
2 Our Proposed Approach
In this paper, we propose to apply LE on the L-
DA topic distributions, each of which is estimat-
ed from a text block. The low-dimensional vec-
tors obtained by LE projection are used to detect
story boundaries through dynamic programming.
Moreover, as in (Xie et al, 2012), we incorporate
the temporal distances between block pairs as a
penalty factor in the weight matrix.
2.1 Latent Dirichlet Allocation
Latent Dirichlet alocation (LDA) (Blei et al,
2003) is a generative probabilistic model of a cor-
pus. It considers that documents are represented
as random mixtures over latent topics, where each
topic is characterized by a distribution over terms.
In LDA, given a corpus D = {d1, d2, . . . , dM}
and a set of terms W = (w1, w2, . . . , wV ), the
generative process can be summarized as follows:
1) For each document d, pick a multinomial dis-
tribution ? from a Dirichlet distribution parameter
?, denoted as ? ? Dir(?).
2) For each term w in document d, select a topic
z from the multinomial distribution ?, denoted as
z ? Multinomial(?).
3) Select a term w from P (w|z, ?), which is a
multinomial probability conditioned on the topic.
An LDA model is characterized by two sets of
prior parameters ? and ?. ? = (?1, ?2, . . . , ?K)
represents the Dirichlet prior distributions for each
K latent topics. ? is aK?V matrix, which defines
the latent topic distributions over terms.
2.2 Construction of weight matrix in
Laplacian Eigenmaps
Laplacian Eigenmaps (LE) is introduced to project
high-dimensional data into a low-dimensional rep-
resentation while preserving its locality property.
Given the ASR transcripts of N text blocks, we ap-
ply LDA algorithm to compute the corresponding
latent topic distributions X = [x1, x2, . . . , xN ] in
RK , where K is the number of latent topics, name-
ly the dimensionality of LDA distributions.
We use G to denote an N-node (N is number of
LDA distributions) graph which represents the re-
lationship between all the text block pairs. If dis-
tribution vectors xi and xj come from the same
story, we put an edge between nodes i and j. We
define a weight matrix S of the graph G to denote
the cohesive strength between the text block pairs.
Each element of this weight matrix is defined as:
sij = cos(xi, xj)?|i?j|, (1)
where ?|i?j| serves the penalty factor for the dis-
tance between i and j. ? is a constant lower than
1.0 that we tune from a set of development data.
It makes the cohesive strength of two text blocks
dramatically decrease when their distance is much
larger than the normal length of a story.
2.3 Data projection in Laplacian Eigenmaps
Given the weight matrix S, we define C as the di-
agonal matrix with its element:
cij =
?K
i=1
sij . (2)
Finally, we obtain the Laplacian matrix L, which
is defined as:
L = C? S. (3)
We use Y = [y1, y2, . . . , yN ] (yi is a column
vector) to indicate the low-dimensional represen-
tation of the latent topic distributions X. The pro-
jection from the latent topic distribution space to
the target space can be defined as:
f : xi ? yi. (4)
A reasonable criterion for computing an optimal
mapping is to minimize the objective as follows:
K?
i=1
K?
j=1
? yi ? yj ?2 sij . (5)
Under this constraint condition, we can preserve
the local geometrical property in LDA distribu-
tions. The objective function can be transformed
191
as:
K?
i=1
K?
j=1
(yi ? yj)sij = tr(YTLY). (6)
Meanwhile, zero matrix and matrices with it-
s rank less than K are meaningless solutions for
our task. We impose YTLY = I to prevent this
situation, where I is an identity matrix. By the
Reyleigh-Ritz theorem (Lutkepohl, 1997), the so-
lution can obtained by the Q smallest eigenvalues
of the generalized eigenmaps problem:
XLXT y = ?XCXT y. (7)
With this formula, we calculate the mapping ma-
trix Y, and its row vectors y?1, y?2, . . . , y?Q are in the
order of their eigenvalues ?1 ? ?2 ? . . . ? ?Q.
y?i is a Q-dimensional (Q<K) eigenvectors.
2.4 Story boundary detection
In story boundary detection, dynamic program-
ming (DP) approach is adopted to obtain the glob-
al optimal solution. Given the low-dimensional se-
mantic representation of the test data, an objective
function can be defined as follows:
? =
Ns?
t=1
(
?
i,j?Segt
? yi ? yj ?2), (8)
where yi and yj are the latent topic distributions of
text blocks i and j respectively, and ? yi ? yj ?2
is the Euclidean distance between them. Segt in-
dicates these text blocks assigned to a certain hy-
pothesized story. Ns is the number of hypothe-
sized stories.
The story boundaries which minimize the ob-
jective function ? in Eq.(8) form the optimal re-
sult. Compared with classical local optimal ap-
proach, DP can more effectively capture the s-
mooth story shifts, and achieve better segmenta-
tion performance.
3 Experimental setup
Our experiments were evaluated on the ASR tran-
scripts provided in TDT2 English Broadcast news
corpus1, which involved 1033 news programs. We
separated this corpus into three non-overlapping
sets: a training set of 500 programs for parameter
estimation in topic modeling and LE, a develop-
ment set of 133 programs for empirical tuning and
a test set of 400 programs for performance evalu-
ation.
In the training stage, ASR transcripts with man-
ually labeled boundary tags were provided. Text
1http://projects.ldc.upenn.edu/TDT2/
streams were broken into block units according to
the given boundary tags, with each text block be-
ing a complete story. In the segmentation stage,
we divided test data into text blocks using the time
labels of pauses in the transcripts. If the pause du-
ration between two blocks last for more than 1.0
sec, it was considered as a boundary candidate. To
avoid the segmentation being suffered from ASR
errors and the out-of-vocabulary issue, phoneme
bigram was used as the basic term unit (Xie et al,
2012). Since the ASR transcripts were at word lev-
el, we performed word-to-phoneme conversion to
obtain the phoneme bigram basic units. The fol-
lowing approaches, in which DP was used in story
boundary detection, were evaluated in the experi-
ments:
? PLSA-DP: PLSA topic distributions were
used to compute sentence cohesive strength.
? LDA-DP: LDA topic distributions were used
to compute sentence cohesive strength.
? PLSA-LE-DP: PLSA topic distributions fol-
lowed by LE projection were used to com-
pute sentence cohesive strength.
? LDA-LE-DP: LDA topic distributions fol-
lowed by LE projection were used to com-
pute sentence cohesion strength.
For LDA, we used the implementation from
David M. Blei?s webpage2. For PLSA, we used
the Lemur Toolkit3.
F1-measure was used as the evaluation crite-
rion.We followed the evaluation rule: a detected
boundary candidate is considered correct if it lies
within a 15 sec tolerant window on each side of a
reference boundary. A number of parameters were
set through empirical tuning on the developent set.
The penalty factor was set to 0.8. When evaluating
the effects of different size of the training set, the
number of latent topics in topic modeling process
was set to 64. After the number of latent topics
was fixed, the dimensionality after LE projection
was set to 32. When evaluating the effects of d-
ifferent number of latent topics in topic modeling
computation, we fixed the size of the training set
to 500 news programs and changed the number of
latent topics from 16 to 256.
4 Experimental results and analysis
4.1 Effect of the size of training dataset
We used the training set from 100 programs to 500
programs (adding 100 programs in each step) to e-
2http://www.cs.princeton.edu/ blei/lda-c/
3http://www.lemurproject.org/
192
valuate the effects of different size of training data
in both PLSA-based and LDA-based approaches.
Figure 1 shows the results on the development set
and the test set.
0.55
0.6
0.65
0.7
0.75
0.8
100 200 300 400 500
F1-
me
asu
re
PLSA-LE-DP LDA-LE-DP
LDA-DP
PLSA-DP
Development Set
0.55
0.6
0.65
0.7
0.75
0.8
100 200 300 400 500
F1-
me
asu
re
Number of programs in training data
PLSA-LE-DP
LDA-LE-DP
PLSA-DP
LDA-DP
Test Set
Figure 1: Segmentation performance with differ-
ent amounts of training data
LDA-LE-DP approach achieved the best result
(0.7927 and 0.7860) on both the development and
the test sets, when there were 500 programs in the
training set. This demonstrates that LDA model
and LE projection used in combination is excellent
for the story segmentation task. The LE projection
applied on the latent topic representations made
relatively 9.88% and 10.93% improvement over
the LDA-based approach and the PLSA-based ap-
proach, respectively on the test set. We can reveal
that employing LE on PLSA and LDA topic dis-
tributions achieves much better performance than
the corresponding approaches without using LE.
We have compared the performances between
PLSA and LDA. We found that when the train-
ing data size was small, PLSA performed better
than LDA. Both PLSA-based and LDA-based ap-
proaches got better with the increase in the size of
the training data set. All the four approaches had
similar performances on the development set and
the test set.
With the increase in the size of the training da-
ta, the LDA-based approaches were improved dra-
matically. They even outperformed the PLSA-
based approaches when the training data contained
more than 300 programs. This may be attributed
to the fact that LDA needs more training data to
estimate the parameters. When the training data is
not enough, its parameters estimated in the train-
ing stage is not stable for the development and the
test data. Moreover, compared with PLSA, the pa-
rameters in LDA do not grow linearly with the size
of the corpus.
4.2 Effect of the number of latent topics
We evaluated the F1-measure of the four ap-
proaches with different number of latent topics
prior to LE projection. Figure 2 shows the cor-
responding results.
0.6
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
16 32 48 64 80 96 128 256
F1-m
easu
re
Number of latent topics
PLSA-DP
LDA-DP
PLSA-LE-DP
LDA-LE-DP
Figure 2: Segmentation performance with differ-
ent numbers of latent topics
The best performances (0.7816-0.7847) were
achieved at the number of latent topics between
64 and 96. When the number of latent topics was
increased from 16 to 64, F1-measure increased.
When the number of latent topics was larger than
96, F1-measure decreased gradually. We found
that the best results were achieved when the num-
ber of topics was close to the real number of top-
ics. There are 80 manually labeled main topics in
the test set.
We observe that LE projection makes the topic
model more stable with different numbers of latent
topics. The best and the worst performances dif-
fered by relatively 9.12% in LDA-DP and 7.97%
in PLSA-DP. However, the relative difference of
2.79% and 2.46% were observed in LDA-LE-DP
and PLSA-LE-DP respectively.
5 Conclusions
Our proposed approach achieves the best F1-
measure of 0.7860. In the task of story segmen-
tation, we believe that LDA can avoid data overfit-
ting problem when there is a sufficient amount of
training data. This is also applicable to LDA-LE-
LP. Moreover, we find that when we apply LE pro-
jection to latent topic distributions, the segmen-
tation performances become less sensitive to the
predefined number of latent topics.
193
Acknowledgments
This work is supported by the National Natu-
ral Science Foundation of China (61175018), the
Natural Science Basic Research Plan of Shaanx-
i Province (2011JM8009) and the Fok Ying Tung
Education Foundation (131059).
References
J. Allan. 2002. Topic Detection and Tracking: Event-
Based Information Organization. Kluwer Academic
Publisher, Norwell, MA.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. A Model of Lexical Attraction and repulsion.
In Proceedings of the 8th Conference on European
Chapter of the Association for Computational Lin-
guistics (EACL), pp.373-380.
Freddy Y. Y. Choi. 2000. Advances in Domain In-
dependent Linear Text Segmentation. In Proceed-
ings of the 1st North American Chapter of the As-
sociation for Computational Linguistics Conference
(NAACL), pp.26-33.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 21st Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pp.20-57.
Mimi Lu, Cheung-Chi Leung, Lei Xie, Bin Ma,
Haizhou Li. 2011. Probabilistic Latent Seman-
tic Analysis for Broadcast New Story Segmentation.
In Proceedings of the 11th Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pp.1109-1112.
David M. Blei. 2012. Probabilistic topic models.
Communication of the ACM, vol. 55, pp.77-84.
David M. Blei, Andrew Y. Ng, Michael I. Jordan.
2003. Latent Dirichlet Allocation. the Journal of
Machine Learning Research, vol. 3, pp.993-1022.
Marti A. Hearst. 1997. TextTiling: Segmenting Text
into Multiparagraph subtopic passages. Computa-
tional Liguistic, vol. 23, pp.33-64.
Gokhan Tur, Dilek Hakkani-Tur, Andreas Stolcke,
Elizabeth Shriberg. 2001. Integrating Prosodic
and Lexicial Cues for Automatic Topic Segmenta-
tion. Computational Liguistic, vol. 27, pp.31-57.
Andrew Rosenberg and Julia Hirschberg. 2006. Story
Segmentation of Broadcast News in English, Man-
darin and Aribic. In Proceedings of the 7th North
American Chapter of the Association for Compu-
tational Linguistics Conference (NAACL), pp.125-
128.
David M. Blei and Pedro J. Moreno. 2001. Topic Seg-
mentation with An Aspect Hidden Markov Model. In
Proceedings of the 24th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrival (SIGIR), pp.343-348.
Wai-Kit Lo, Wenying Xiong, Helen Meng. 2009. Au-
tomatic Story Segmentation Using a Bayesian De-
cision Framwork for Statistical Models of Lexical
Chain Feature. In Proceedings of the 47th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pp.357-364.
Igor Malioutov and Regina Barzilay. 2006. Minimum
Cut Model for Spoken Lecture Segmenation. In Pro-
ceedings of the 44th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pp.25-32.
Freddy Y. Y. Choi, Peter Wiemer-Hastings, Juhanna
Moore. 2001. Latent Semantic Analysis for Tex-
t Segmentation. In Proceedings of the 2001 Con-
ference on Empirical Methods on Natural Language
Processing (EMNLP), pp.109-117.
Rachit Arora and Balaraman Ravindran. 2008. Latent
Dirichlet Allocation Based Multi-document Summa-
rization. In Proceedings of the 2nd Workshop on
Analytics for Noisy Unstructured Text Data (AND),
pp.91-97.
David Hall, Daniel Jurafsky, Christopher D. Manning.
2008. Latent Studying the History Ideas Using Topic
Models. In Proceedings of the 2008 Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP), pp.363-371.
Qi Sun, Runxin Li, Dingsheng Luo, Xihong Wu. 2008.
Text Segmentation with LDA-based Fisher Kernel.
In Proceedings of the 46th Annual Meeting of the As-
socation for Computational Linguistics on Human
Language Technologies (HLT-ACL), pp.269-272.
Mikhail Belkin and Partha Niyogi. 2002. Laplacian
Eigenmaps for Dimensionality Reduction and Da-
ta Representation. Neural Computation, vol. 15,
pp.1383-1396.
Lei Xie, Lilei Zheng, Zihan Liu and Yanning Zhang.
2012. Laplacian Eigenmaps for Automatic Story
Segmentation of Broadcast News. IEEE Transaction
on Audio, Speech and Language Processing, vol. 20,
pp.264-277.
Deng Cai, Qiaozhu Mei, Jiawei Han, and Chengxiang
Zhai. 2008. Modeling Hidden Topics on Document
Manifold. In Proceedings of the 17th ACM Confer-
ence on Information and Knowledge Managemen-
t (CIKM), pp.911-120.
Xiaoming Lu, Cheung-Chi Leung, Lei Xie, Bin Ma,
and Haizhou Li. 2013. Broadcast News Story Seg-
mentation Using Latent Topics on Data Manifold. In
Proceedings of the 38th International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
194
J. P. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van
Mulbregt. 1999. AHiddenMarkov Model Approach
to Text Segmenation and Event Tracking. In Pro-
ceedings of the 1999 International Conference on
Acoustics, Speech, and Signal Processing (ICASSP),
pp.333-336.
Martin Riedl and Chris Biemann. 2012. Text Segmen-
tation with Topic Models. the Journal for Language
Technology and Computational Linguistics, pp.47-
69.
P. Fragkou , V. Petridis , Ath. Kehagias. 2002. A Dy-
namic Programming algorithm for Linear Text Story
Segmentation. the Joural of Intelligent Information
Systems, vol. 23, pp.179-197.
H. Lutkepohl. 1997. Handbook of Matrices. Wiley,
Chichester, UK.
Jen-Tzung Chien and Chuang-Hua Chueh. 2012.
Topic-Based Hieraachical Segmentation. IEEE
Transaction on Audio, Speech and Language Pro-
cessing, vol. 20, pp.55-66.
195
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233?237,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modeling of term-distance and term-occurrence information for im-
proving n-gram language model performance 
 
Tze Yuang Chong1,2, Rafael E. Banchs3, Eng Siong Chng1,2, Haizhou Li1,2,3 
1Temasek Laboratory, Nanyang Technological University, Singapore 639798 
2School of Computer Engineering, Nanyang Technological University, Singapore 639798 
3Institute for Infocomm Research, Singapore 138632 
tychong@ntu.edu.sg, rembanchs@i2r.a-star.edu.sg, 
aseschng@ntu.edu.sg, hli@i2r.a-star.edu.sg 
  
 
Abstract 
In this paper, we explore the use of distance 
and co-occurrence information of word-pairs 
for language modeling. We attempt to extract 
this information from history-contexts of up to 
ten words in size, and found it complements 
well the n-gram model, which inherently suf-
fers from data scarcity in learning long histo-
ry-contexts. Evaluated on the WSJ corpus, bi-
gram and trigram model perplexity were re-
duced up to 23.5% and 14.0%, respectively. 
Compared to the distant bigram, we show that 
word-pairs can be more effectively modeled in 
terms of both distance and occurrence. 
1 Introduction 
Language models have been extensively studied 
in natural language processing. The role of a lan-
guage model is to measure how probably a (tar-
get) word would occur based on some given evi-
dence extracted from the history-context. The 
commonly used n-gram model (Bahl et al 1983) 
takes the immediately preceding history-word 
sequence, of length   1 , as the evidence for 
prediction. Although n-gram models are simple 
and effective, modeling long history-contexts 
lead to severe data scarcity problems. Hence, the 
context length is commonly limited to as short as 
three, i.e. the trigram model, and any useful in-
formation beyond this window is neglected. 
In this work, we explore the possibility of 
modeling the presence of a history-word in terms 
of: (1) the distance and (2) the co-occurrence, 
with a target-word. These two attributes will be 
exploited and modeled independently from each 
other, i.e. the distance is described regardless the 
actual frequency of the history-word, while the 
co-occurrence is described regardless the actual 
position of the history-word. We refer to these 
two attributes as the term-distance (TD) and the 
term-occurrence (TO) components, respectively. 
The rest of this paper is structured as follows. 
The following section presents the most relevant 
related works. Section 3 introduces and moti-
vates our proposed approach. Section 4 presents 
in detail the derivation of both TD and TO model 
components. Section 5 presents some perplexity 
evaluation results. Finally, section 6 presents our 
conclusions and proposed future work. 
2 Related Work 
The distant bigram model (Huang et.al 1993, 
Simon et al 1997, Brun et al 2007) disassembles 
the n-gram into (n?1) word-pairs, such that each 
pair is modeled by a distance-k bigram model, 
where 1      1 . Each distance-k bigram 
model predicts the target-word based on the oc-
currence of a history-word located k positions 
behind.  
Zhou & Lua (1998) enhanced the effective-
ness of the model by filtering out those word-
pairs exhibiting low correlation, so that only the 
well associated distant bigrams are retained. This 
approach is referred to as the distance-dependent 
trigger model, and is similar to the earlier pro-
posed trigger model (Lau et al 1993, Rosenfeld 
1996) that relies on the bigrams of arbitrary dis-
tance, i.e. distance-independent. 
Latent-semantic language model approaches 
(Bellegarda 1998, Coccaro 2005) weight word 
counts with TFIDF to highlight their semantic 
importance towards the prediction. In this type of 
approach, count statistics are accumulated from 
long contexts, typically beyond ten to twenty 
words. In order to confine the complexity intro-
duced by such long contexts, word ordering is 
ignored (i.e. bag-of-words paradigm). 
Other approaches such as the class-based lan-
guage model (Brown 1992, Kneser & Ney 1993) 
233
use POS or POS-like classes of the history-words 
for prediction. The structured language model 
(Chelba & Jelinek 2000) determines the ?heads? 
in the history-context by using a parsing tree. 
There are also works on skipping irrelevant his-
tory-words in order to reveal more informative n-
grams (Siu & Ostendorf 2000, Guthrie et al 
2006). Cache language models exploit temporal 
word frequencies in the history (Kuhn & Mori 
1990, Clarkson & Robinson 1997). 
3 Motivation of the Proposed Approach 
The attributes of distance and co-occurrence are 
exploited and modeled differently in each lan-
guage modeling approach. In the n-gram model, 
for example, these two attributes are jointly taken 
into account in the ordered word-sequence. Con-
sequently, the n-gram model can only be effec-
tively implemented within a short history-context 
(e.g. of size of three or four). 
Both, the conventional trigger model and the 
latent-semantic model capture the co-occurrence 
information while ignoring the distance informa-
tion. It is reasonable to assume that distance in-
formation at far contexts is less likely to be in-
formative and, hence, can be discarded. Howev-
er, intermediate distances beyond the n-gram 
model limits can be very useful and should not 
be discarded. 
On the other hand, distant-bigram models and 
distance-dependent trigger models make use of 
both, distance and co-occurrence, information up 
to window sizes of ten to twenty. They achieve 
this by compromising inter-dependencies among 
history-words (i.e. the context is represented as 
separated word-pairs). However, similarly to n-
gram models, distance and co-occurrence infor-
mation are implicitly tied within the word-pairs. 
In our proposed approach, we attempt to ex-
ploit the TD and TO attributes, separately, to in-
corporate distant context information into the n-
gram, as a remedy to the data scarcity problem 
when learning the far context. 
4 Language Modeling with TD and TO 
A language model estimates word probabilities 
given their history, i.e.  	 
| 	 
 , 
where  denotes the target word and  denotes its 
corresponding history. Let the word located at ith 
position, 
 , be the target-word and its preceding 
word-sequence 
 	 
?

  of 
length   1, be its history-context. Also, in or-
der to alleviate the data scarcity problem, we as-
sume the occurrences of the history-words to be 
independent from each other, conditioned to the 
occurrence of the target-word 
 , i.e.  
 
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 19?23,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
A Composite Kernel Approach for Dialog Topic Tracking with
Structured Domain Knowledge from Wikipedia
Seokhwan Kim, Rafael E. Banchs, Haizhou Li
Human Language Technology Department
Institute for Infocomm Research
Singapore 138632
{kims,rembanchs,hli}@i2r.a-star.edu.sg
Abstract
Dialog topic tracking aims at analyzing
and maintaining topic transitions in on-
going dialogs. This paper proposes a com-
posite kernel approach for dialog topic
tracking to utilize various types of do-
main knowledge obtained fromWikipedia.
Two kernels are defined based on history
sequences and context trees constructed
based on the extracted features. The ex-
perimental results show that our compos-
ite kernel approach can significantly im-
prove the performances of topic tracking
in mixed-initiative human-human dialogs.
1 Introduction
Human communications in real world situations
interlace multiple topics which are related to each
other in conversational contexts. This fact sug-
gests that a dialog system should be also capable
of conducting multi-topic conversations with users
to provide them a more natural interaction with the
system. However, the majority of previous work
on dialog interfaces has focused on dealing with
only a single target task. Although some multi-
task dialog systems have been proposed (Lin et al,
1999; Ikeda et al, 2008; Celikyilmaz et al, 2011),
they have aimed at just choosing the most proba-
ble one for each input from the sub-systems, each
of which is independently operated from others.
To analyze and maintain dialog topics from a
more systematic perspective in a given dialog flow,
some researchers (Nakata et al, 2002; Lagus and
Kuusisto, 2002; Adams and Martell, 2008) have
considered this dialog topic identification as a sep-
arate sub-problem of dialog management and at-
tempted to solve it with text categorization ap-
proaches for the recognized utterances in a given
turn. The major obstacle to the success of these
approaches results from the differences between
written texts and spoken utterances. In most text
categorization tasks, the proper category for each
textual unit can be assigned based only on its own
content. However, the dialog topic at each turn
can be determined not only by the user?s inten-
tions captured from the given utterances, but also
by the system?s decisions for dialog management
purposes. Thus, the text categorization approaches
can only be effective for the user-initiative cases
when users tend to mention the topic-related ex-
pressions explicitly in their utterances.
The other direction of dialog topic tracking ap-
proaches made use of external knowledge sources
including domain models (Roy and Subramaniam,
2006), heuristics (Young et al, 2007), and agen-
das (Bohus and Rudnicky, 2003; Lee et al, 2008).
These knowledge-based methods have an advan-
tage of dealing with system-initiative dialogs, be-
cause dialog flows can be controlled by the sys-
tem based on given resources. However, this as-
pect can limit the flexibility to handle the user?s
responses which are contradictory to the system?s
suggestions. Moreover, these approaches face cost
problems for building a sufficient amount of re-
sources to cover broad states of complex dialogs,
because these resources should be manually pre-
pared by human experts for each specific domain.
In this paper, we propose a composite kernel
to explore various types of information obtained
from Wikipedia for mixed-initiative dialog topic
tracking without significant costs for building re-
sources. Composite kernels have been success-
fully applied to improve the performances in other
NLP problems (Zhao and Grishman, 2005; Zhang
et al, 2006) by integrating multiple individual ker-
nels, which aim to overcome the errors occurring
at one level by information from other levels. Our
composite kernel consists of a history sequence
and a domain context tree kernels, both of which
are composed based on similar textual units in
Wikipedia articles to a given dialog context.
19
t Speaker Utterance Topic Transition
0 Guide How can I help you? NONE?NONE
1
Tourist Can you recommend some good places to visit
in Singapore?
NONE?ATTR
Guide Well if you like to visit an icon of Singapore,
Merlion park will be a nice place to visit.
2
Tourist Merlion is a symbol for Singapore, right?
ATTR?ATTR
Guide Yes, we use that to symbolise Singapore.
3
Tourist Okay.
ATTR?ATTR
Guide The lion head symbolised the founding of the is-
land and the fish body just symbolised the hum-
ble fishing village.
4
Tourist How can I get there from Orchard Road?
ATTR?TRSP
Guide You can take the north-south line train from Or-
chard Road and stop at Raffles Place station.
5
Tourist Is this walking distance from the station to the
destination?
TRSP?TRSP
Guide Yes, it?ll take only ten minutes on foot.
6
Tourist Alright.
TRSP?FOOD
Guide Well, you can also enjoy some seafoods at the
riverside near the place.
7
Tourist What food do you have any recommendations
to try there?
FOOD?FOOD
Guide If you like spicy foods, you must try chilli crab
which is one of our favourite dishes here in Sin-
gapore.
8 Tourist Great! I?ll try that. FOOD?FOOD
Figure 1: Examples of dialog topic tracking on
Singapore tour guide dialogs
2 Dialog Topic Tracking
Dialog topic tracking can be considered as a clas-
sification problem to detect topic transitions. The
most probable pair of topics at just before and after
each turn is predicted by the following classifier:
f(x
t
) = (y
t?1
, y
t
), where x
t
contains the input
features obtained at a turn t, y
t
? C , and C is a
closed set of topic categories. If a topic transition
occurs at t, y
t
should be different from y
t?1
. Oth-
erwise, both y
t
and y
t?1
have the same value.
Figure 1 shows an example of dialog topic
tracking in a given dialog fragment on Singapore
tour guide domain between a tourist and a guide.
This conversation is divided into three segments,
since f detects three topic transitions at t
1
, t
4
and
t
6
. Then, a topic sequence of ?Attraction?, ?Trans-
portation?, and ?Food? is obtained from the results.
3 Wikipedia-based Composite Kernel for
Dialog Topic Tracking
The classifier f can be built on the training exam-
ples annotated with topic labels using supervised
machine learning techniques. Although some fun-
damental features extracted from the utterances
mentioned at a given turn or in a certain number of
previous turns can be used for training the model,
this information obtained solely from an ongoing
dialog is not sufficient to identify not only user-
initiative, but also system-initiative topic transi-
tions.
To overcome this limitation, we propose to
leverage on Wikipedia as an external knowledge
source that can be obtained without significant
effort toward building resources for topic track-
ing. Recently, some researchers (Wilcock, 2012;
Breuing et al, 2011) have shown the feasibility
of using Wikipedia knowledge to build dialog sys-
tems. While each of these studies mainly focuses
only on a single type of information including cat-
egory relatedness or hyperlink connectedness, this
work aims at incorporating various knowledge ob-
tained from Wikipedia into the model using a com-
posite kernel method.
Our composite kernel consists of two different
kernels: a history sequence kernel and a domain
context tree kernel. Both represent the current di-
alog context at a given turn with a set of relevant
Wikipedia paragraphs which are selected based on
the cosine similarity between the term vectors of
the recently mentioned utterances and each para-
graph in the Wikipedia collection as follows:
sim (x, p
i
) =
?(x) ? ?(p
i
)
|?(x)||?(p
i
)|
,
where x is the input, p
i
is the i-th paragraph in
the Wikipedia collection, ?(p
i
) is the term vector
extracted from p
i
. The term vector for the input x,
?(x), is computed by accumulating the weights in
the previous turns as follows:
?(x) =
(
?
1
, ?
2
, ? ? ? , ?
|W |
)
? R
|W |
,
where ?
i
=
?
h
j=0
(
?
j
? tf idf(w
i
, u
(t?j)
)
)
, u
t
is
the utterance mentioned in a turn t, tf idf(w
i
, u
t
)
is the product of term frequency of a word w
i
in
u
t
and inverse document frequency of w
i
, ? is a
decay factor for giving more importance to more
recent turns, |W | is the size of word dictionary,
and h is the number of previous turns considered
as dialog history features.
After computing this relatedness between the
current dialog context and every paragraph in the
Wikipedia collection, two kernel structures are
constructed using the information obtained from
the highly-ranked paragraphs in the Wikipedia.
3.1 History Sequence Kernel
The first structure to be constructed for our com-
posite kernel is a sequence of the most similar
paragraph IDs of each turn from the beginning of
the session to the current turn. Formally, the se-
quence S at a given turn t is defined as:
S = (s
0
, ? ? ? , s
t
),
where s
j
= argmax
i
(sim (x
j
, p
i
)).
20
Since our hypothesis is that the more similar the
dialog histories of the two inputs are, the more
similar aspects of topic transtions occur for them,
we propose a sub-sequence kernel (Lodhi et al,
2002) to map the data into a new feature space de-
fined based on the similarity of each pair of history
sequences as follows:
K
s
(S
1
, S
2
) =
?
u?A
n
?
i:u=S
1
[i]
?
j:u=S
2
[j]
?
l(i)+l(j)
,
where A is a finite set of paragraph IDs, S is a fi-
nite sequence of paragraph IDs, u is a subsequence
of S, S[j] is the subsequence with the i-th charac-
ters ?i ? j, l(i) is the length of the subsequence,
and ? ? (0, 1) is a decay factor.
3.2 Domain Context Tree Kernel
The other kernel incorporates more various types
of domain knowledge obtained from Wikipedia
into the feature space. In this method, each in-
stance is encoded in a tree structure constructed
following the rules in Figure 2. The root node of
a tree has few children, each of which is a subtree
rooted at each paragraph node in:
P
t
= {p
i
|sim (x
t
, p
i
) > ?},
where ? is a threshold value to select the relevant
paragraphs. Each subtree consists of a set of fea-
tures from a given paragraph in the Wikipedia col-
lection in a hierarchical structure. Figure 3 shows
an example of a constructed tree.
Since this constructed tree structure represents
semantic, discourse, and structural information
extracted from the similar Wikipedia paragraphs
to each given instance, we can explore these more
enriched features to build the topic tracking model
using a subset tree kernel (Collins and Duffy,
2002) which computes the similarity between each
pair of trees in the feature space as follows:
K
t
(T
1
, T
2
) =
?
n
1
?N
T
1
?
n
2
?N
T
2
? (n
1
, n
2
) ,
where N
T
is the set of T ?s nodes, ? (n
1
, n
2
) =
?
i
I
i
(n
i
) ? I
i
(n
2
), and I
i
(n) is a function that is
1 iff the i-th tree fragment occurs with root at node
n and 0 otherwise.
3.3 Kernel Composition
In this work, a composite kernel is defined by com-
bining the individual kernels including history se-
quence and domain context tree kernels, as well as
<TREE>:=(ROOT <PAR>...<PAR>)
<PAR>:=(PAR_ID <PARENTS>
<PREV_PAR><NEXT_PAR><LINKS>)
<PARENTS>:=(?PARENTS? <ART><SEC>)
<ART>:=(ART_ID <ART_NAME><CAT_LIST>)
<ART_NAME>:=(?ART_NAME? ART_NAME)
<CAT_LIST>:=(?CAT? <CAT>...<CAT>)
<CAT>:=(CAT_ID
*
)
<SEC>:=(SEC_ID <SEC_NAME><PARENT_SEC>
<PREV_SEC><NEXT_SEC>)
<SEC_NAME>:=(?SEC_NAME? SEC_NAME)
<PARENT_SEC>:=(?PRN_SEC?, PRN_SEC_ID)
<PREV_SEC>:=(?PREV_SEC?, PREV_SEC_NAME)
<NEXT_SEC>:=(?NEXT_SEC?, NEXT_SEC_NAME)
<PREV_PAR>:=(?PREV_PAR?, PREV_PAR_ID)
<NEXT_PAR>:=(?NEXT_PAR?, NEXT_PAR_ID)
<LINKS>:=(?LINKS? <LINK>...<LINK>)
<LINK>:=(LINK_NAME
*
)
Figure 2: Rules for constructing a domain context
tree from Wikipedia: PAR, ART, SEC, and CAT
are acronyms for paragraph, article, section, and
category, respectively
Figure 3: An example of domain context tree
the linear kernel between the vectors representing
fundamental features extracted from the utterances
themselves and the results of linguistic preproces-
sors. The composition is performed by linear com-
bination as follows:
K(x
1
, x
2
) =? ?K
l
(V
1
, V
2
) + ? ?K
s
(S
1
, S
2
)
+ ? ?K
t
(T
1
, T
2
),
where V
i
, S
i
, and T
i
are the feature vector, his-
tory sequence, and domain context tree of x
i
, re-
spectively, K
l
is the linear kernel computed by in-
ner product of the vectors, ?, ?, and ? are coeffi-
cients for linear combination of three kernels, and
? + ? + ? = 1.
4 Evaluation
To demonstrate the effectiveness of our proposed
kernel method for dialog topic tracking, we per-
formed experiments on the Singapore tour guide
dialogs which consists of 35 dialog sessions col-
lected from real human-human mixed initiative
conversations related to Singapore between guides
21
and tourists. All the recorded dialogs with the total
length of 21 hours were manually transcribed, then
these transcribed dialogs with 19,651 utterances
were manually annotated with the following nine
topic categories: Opening, Closing, Itinerary, Ac-
commodation, Attraction, Food, Transportation,
Shopping, and Other.
Since we aim at developing the system which
acts as a guide communicating with tourist users,
an instance for both training and prediction of
topic transition was created for each turn of
tourists. The annotation of an instance is a pair of
previous and current topics, and the actual number
of labels occurred in the dataset is 65.
For each instance, the term vector was gener-
ated from the utterances in current user turn, previ-
ous system turn, and history turns within the win-
dow sizes h = 10. Then, the history sequence and
tree context structures for our composite kernel
were constructed based on 3,155 articles related
to Singapore collected from Wikipedia database
dump as of February 2013. For the linear ker-
nel baseline, we used the following features: n-
gram words, previous system actions, and current
user acts which were manually annotated. Finally,
8,318 instances were used for training the model.
We trained the SVM models using
SVM
light 1
(Joachims, 1999) with the follow-
ing five different combinations of kernels: K
l
only, K
l
withP as features, K
l
+K
s
,K
l
+K
t
, and
K
l
+K
s
+K
t
. The threshold value ? for selecting
P was 0.5, and the combinations of kernels were
performed with the same ?, ?, or ? coefficient
values for all sub-kernels. All the evaluations
were done in five-fold cross validation to the man-
ual annotations with two different metrics: one
is accuracy of the predicted topic label for every
turn, and the other is precision/recall/F-measure
for each event of topic transition occurred either
in the answer or the predicted result.
Table 1 compares the performances of the five
combinations of kernels. When just the para-
graph IDs were included as additional features,
it failed to improve the performances from the
baseline without any external features. However,
our proposed kernels using history sequences and
domain context trees achieved significant perfor-
mances improvements for both evaluation metrics.
While the history sequence kernel enhanced the
coverage of the model to detect topic transitions,
1
http://svmlight.joachims.org/
Turn-level Transition-level
Accuracy P R F
K
l
62.45 42.77 24.77 31.37
K
l
+ P 62.44 42.76 24.77 31.37
K
l
+ K
s
67.19 39.94 40.59 40.26
K
l
+ K
t
68.54 45.55 35.69 40.02
All 69.98 44.82 39.83 42.18
Table 1: Experimental Results
0
500
1000
1500
2000
2500
3000
K
l
K
l
+ P K
l
+K
s
K
l
+K
t
ALL
N
um
be
r
of
T
ra
ns
it
io
n
E
rr
or
s FP(SYS)
FN(SYS)
FP(USR)
FN(USR)
Figure 4: Error distibutions of topic transitions:
FN and FP denotes false negative and false posi-
tive respectively. USR and SYS in the parentheses
indicate the initiativity of the transitions.
the domain context tree kernel contributed to pro-
duce more precise outputs. Finally, the model
combining all the kernels outperformed the base-
line by 7.53% in turn-level accuracy and 10.81%
in transition-level F-measure.
The error distributions in Figure 4 indicate that
these performance improvements were achieved
by resolving the errors not only on user-initiative
topic transitions, but also on system-initiative
cases, which implies the effectiveness of the struc-
tured knowledge from Wikipedia to track the top-
ics in mixed-initiative dialogs.
5 Conclusions
This paper presented a composite kernel approach
for dialog topic tracking. This approach aimed to
represent various types of domain knowledge ob-
tained from Wikipedia as two structures: history
sequences and domain context trees; then incor-
porate them into the model with kernel methods.
Experimental results show that the proposed ap-
proaches helped to improve the topic tracking per-
formances in mixed-initiative human-human di-
alogs with respect to the baseline model.
22
References
P. H. Adams and C. H. Martell. 2008. Topic detection
and extraction in chat. In Proceedings of the 2008
IEEE International Conference on Semantic Com-
puting, pages 581?588.
D. Bohus and A. Rudnicky. 2003. Ravenclaw: dia-
log management using hierarchical task decomposi-
tion and an expectation agenda. In Proceedings of
the European Conference on Speech, Communica-
tion and Technology, pages 597?600.
A. Breuing, U. Waltinger, and I. Wachsmuth. 2011.
Harvesting wikipedia knowledge to identify topics
in ongoing natural language dialogs. In Proceedings
of the IEEE/WIC/ACM International Conference on
Web Intelligence and Intelligent Agent Technology
(WI-IAT), pages 445?450.
A. Celikyilmaz, D. Hakkani-Tu?r, and G. Tu?r. 2011.
Approximate inference for domain detection in
spoken language understanding. In Proceedings
of the 12th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH), pages 713?716.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th annual meeting on association
for computational linguistics, pages 263?270.
S. Ikeda, K. Komatani, T. Ogata, H. G. Okuno, and
H. G. Okuno. 2008. Extensibility verification of ro-
bust domain selection against out-of-grammar utter-
ances in multi-domain spoken dialogue system. In
Proceedings of the 9th INTERSPEECH, pages 487?
490.
T. Joachims. 1999. Making large-scale SVM learn-
ing practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning, chapter 11, pages 169?
184. MIT Press, Cambridge, MA.
K. Lagus and J. Kuusisto. 2002. Topic identification
in natural language dialogues using neural networks.
In Proceedings of the 3rd SIGdial workshop on Dis-
course and dialogue, pages 95?102.
C. Lee, S. Jung, and G. G. Lee. 2008. Robust dia-
log management with n-best hypotheses using di-
alog examples and agenda. In Proceedings of the
46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 630?637.
B. Lin, H. Wang, and L. Lee. 1999. A distributed
architecture for cooperative spoken dialogue agents
with coherent dialogue state and history. In Pro-
ceedings of the IEEE Automatic Speech Recognition
and Understanding Workshop (ASRU).
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. The Journal of
Machine Learning Research, 2:419?444.
T. Nakata, S. Ando, and A. Okumura. 2002. Topic de-
tection based on dialogue history. In Proceedings of
the 19th international conference on Computational
linguistics (COLING), pages 1?7.
S. Roy and L. V. Subramaniam. 2006. Automatic gen-
eration of domain models for call centers from noisy
transcriptions. In Proceedings of COLING/ACL,
pages 737?744.
G. Wilcock. 2012. Wikitalk: a spoken wikipedia-
based open-domain knowledge access system. In
Proceedings of the Workshop on Question Answer-
ing for Complex Domains, page 5770.
S. Young, J. Schatzmann, K. Weilhammer, and H. Ye.
2007. The hidden information state approach to di-
alog management. In Proceedings of the Interna-
tional Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 149?152.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 825?832.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 419?426.
23
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 1?11,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Report of NEWS 2010 Transliteration Generation Shared Task
Haizhou Li?, A Kumaran?, Min Zhang? and Vladimir Pervouchine?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
This report documents the Translitera-
tion Generation Shared Task conducted as
a part of the Named Entities Workshop
(NEWS 2010), an ACL 2010 workshop.
The shared task features machine translit-
eration of proper names from English to
9 languages and from 3 languages to En-
glish. In total, 12 tasks are provided. 7
teams from 5 different countries partici-
pated in the evaluations. Finally, 33 stan-
dard and 8 non-standard runs are submit-
ted, where diverse transliteration method-
ologies are explored and reported on the
evaluation data. We report the results with
4 performance metrics. We believe that the
shared task has successfully achieved its
objective by providing a common bench-
marking platform for the research commu-
nity to evaluate the state-of-the-art tech-
nologies that benefit the future research
and development.
1 Introduction
Names play a significant role in many Natural
Language Processing (NLP) and Information Re-
trieval (IR) systems. They are important in Cross
Lingual Information Retrieval (CLIR) and Ma-
chine Translation (MT) as the system performance
has been shown to positively correlate with the
correct conversion of names between the lan-
guages in several studies (Demner-Fushman and
Oard, 2002; Mandl and Womser-Hacker, 2005;
Hermjakob et al, 2008; Udupa et al, 2009). The
traditional source for name equivalence, the bilin-
gual dictionaries ? whether handcrafted or sta-
tistical ? offer only limited support because new
names always emerge.
All of the above point to the critical need for ro-
bust Machine Transliteration technology and sys-
tems. Much research effort has been made to ad-
dress the transliteration issue in the research com-
munity (Knight and Graehl, 1998; Meng et al,
2001; Li et al, 2004; Zelenko and Aone, 2006;
Sproat et al, 2006; Sherif and Kondrak, 2007;
Hermjakob et al, 2008; Al-Onaizan and Knight,
2002; Goldwasser and Roth, 2008; Goldberg and
Elhadad, 2008; Klementiev and Roth, 2006; Oh
and Choi, 2002; Virga and Khudanpur, 2003; Wan
and Verspoor, 1998; Kang and Choi, 2000; Gao
et al, 2004; Zelenko and Aone, 2006; Li et al,
2009b; Li et al, 2009a). These previous work
fall into three categories, i.e., grapheme-based,
phoneme-based and hybrid methods. Grapheme-
based method (Li et al, 2004) treats translitera-
tion as a direct orthographic mapping and only
uses orthography-related features while phoneme-
based method (Knight and Graehl, 1998) makes
use of phonetic correspondence to generate the
transliteration. Hybrid method refers to the com-
bination of several different models or knowledge
sources to support the transliteration generation.
The first machine transliteration shared task (Li
et al, 2009b; Li et al, 2009a) was held in NEWS
2009 at ACL-IJCNLP 2009. It was the first time
to provide common benchmarking data in diverse
language pairs for evaluation of state-of-the-art
techniques. NEWS 2010 is a continued effort of
NEWS 2009. It builds on the foundations estab-
lished in the first transliteration shared task and
extends the scope to include new language pairs.
The rest of the report is organised as follows.
Section 2 outlines the machine transliteration task
and the corpora used and Section 3 discusses the
metrics chosen for evaluation, along with the ratio-
nale for choosing them. Sections 4 and 5 present
the participation in the shared task and the results
with their analysis, respectively. Section 6 con-
cludes the report.
1
2 Transliteration Shared Task
In this section, we outline the definition and the
description of the shared task.
2.1 ?Transliteration?: A definition
There exists several terms that are used inter-
changeably in the contemporary research litera-
ture for the conversion of names between two
languages, such as, transliteration, transcription,
and sometimes Romanisation, especially if Latin
scripts are used for target strings (Halpern, 2007).
Our aim is not only at capturing the name con-
version process from a source to a target lan-
guage, but also at its practical utility for down-
stream applications, such as CLIR and MT. There-
fore, we adopted the same definition of translit-
eration as during the NEWS 2009 workshop (Li
et al, 2009a) to narrow down ?transliteration? to
three specific requirements for the task, as fol-
lows:?Transliteration is the conversion of a given
name in the source language (a text string in the
source writing system or orthography) to a name
in the target language (another text string in the
target writing system or orthography), such that
the target language name is: (i) phonemically
equivalent to the source name (ii) conforms to the
phonology of the target language and (iii) matches
the user intuition of the equivalent of the source
language name in the target language, consider-
ing the culture and orthographic character usage
in the target language.?
In NEWS 2010, we introduce three
back-transliteration tasks. We define back-
transliteration as a process of restoring translit-
erated words to their original languages. For
example, NEWS 2010 offers the tasks to convert
western names written in Chinese and Thai into
their original English spellings, or romanized
Japanese names into their original Kanji writings.
2.2 Shared Task Description
Following the tradition in NEWS 2009, the shared
task at NEWS 2010 is specified as development of
machine transliteration systems in one or more of
the specified language pairs. Each language pair
of the shared task consists of a source and a target
language, implicitly specifying the transliteration
direction. Training and development data in each
of the language pairs have been made available to
all registered participants for developing a translit-
eration system for that specific language pair using
any approach that they find appropriate.
At the evaluation time, a standard hand-crafted
test set consisting of between 1,000 and 3,000
source names (approximately 10% of the train-
ing data size) have been released, on which the
participants are required to produce a ranked list
of transliteration candidates in the target language
for each source name. The system output is
tested against a reference set (which may include
multiple correct transliterations for some source
names), and the performance of a system is cap-
tured in multiple metrics (defined in Section 3),
each designed to capture a specific performance
dimension.
For every language pair every participant is re-
quired to submit at least one run (designated as a
?standard? run) that uses only the data provided by
the NEWS workshop organisers in that language
pair, and no other data or linguistic resources. This
standard run ensures parity between systems and
enables meaningful comparison of performance
of various algorithmic approaches in a given lan-
guage pair. Participants are allowed to submit
more ?standard? runs, up to 4 in total. If more than
one ?standard? runs is submitted, it is required to
name one of them as a ?primary? run, which is
used to compare results across different systems.
In addition, up to 4 ?non-standard? runs could be
submitted for every language pair using either data
beyond that provided by the shared task organisers
or linguistic resources in a specific language, or
both. This essentially may enable any participant
to demonstrate the limits of performance of their
system in a given language pair.
The shared task timelines provide adequate time
for development, testing (approximately 1 month
after the release of the training data) and the final
result submission (7 days after the release of the
test data).
2.3 Shared Task Corpora
We considered two specific constraints in select-
ing languages for the shared task: language diver-
sity and data availability. To make the shared task
interesting and to attract wider participation, it is
important to ensure a reasonable variety among
the languages in terms of linguistic diversity, or-
thography and geography. Clearly, the ability of
procuring and distributing a reasonably large (ap-
proximately 10K paired names for training and
testing together) hand-crafted corpora consisting
2
primarily of paired names is critical for this pro-
cess. At the end of the planning stage and after
discussion with the data providers, we have cho-
sen the set of 12 tasks shown in Table 1 (Li et al,
2004; Kumaran and Kellner, 2007; MSRI, 2009;
CJKI, 2010).
NEWS 2010 leverages on the success of NEWS
2009 by utilizing the training and dev data of
NEWS 2009 as the training data of NEWS 2010
and the test data of NEWS 2009 as the dev data
of NEWS 2010. NEWS 2010 provides totally new
test data across all 12 tasks for evaluation. In ad-
dition to the 7 tasks inherited from NEWS 2009,
NEWS 2010 is enhanced with 5 new tasks, three
new languages (Arabic, Bangla and Thai) and two
back-transliteration (Chinese to English and Thai
to English).
The names given in the training sets for Chi-
nese, Japanese, Korean and Thai languages are
Western names and their respective translitera-
tions; the Japanese Name (in English)? Japanese
Kanji data set consists only of native Japanese
names; the Arabic data set consists only of native
Arabic names. The Indic data set (Hindi, Tamil,
Kannada, Bangla) consists of a mix of Indian and
Western names.
For all of the tasks chosen, we have been
able to procure paired names data between the
source and the target scripts and were able to
make them available to the participants. For
some language pairs, such as English-Chinese and
English-Thai, there are both transliteration and
back-transliteration tasks. Most of the task are just
one-way transliteration, although Indian data sets
contained mixture of names of both Indian and
Western origins. The language of origin of the
names for each task is indicated in the first column
of Table 1.
Finally, it should be noted here that the corpora
procured and released for NEWS 2010 represent
perhaps the most diverse and largest corpora to be
used for any common transliteration tasks today.
3 Evaluation Metrics and Rationale
The participants have been asked to submit results
of up to four standard and four non-standard runs.
One standard run must be named as the primary
submission and is used for the performance sum-
mary. Each run contains a ranked list of up to
10 candidate transliterations for each source name.
The submitted results are compared to the ground
truth (reference transliterations) using 4 evaluation
metrics capturing different aspects of translitera-
tion performance. We have dropped two MAP
metrics used in NEWS 2009 because they don?t
offer additional information to MAPref . Since a
name may have multiple correct transliterations,
all these alternatives are treated equally in the eval-
uation, that is, any of these alternatives is consid-
ered as a correct transliteration, and all candidates
matching any of the reference transliterations are
accepted as correct ones.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
3.1 Word Accuracy in Top-1 (ACC)
Also known as Word Error Rate, it measures cor-
rectness of the first transliteration candidate in the
candidate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC = 1
N
N
?
i=1
{
1 if ?ri,j : ri,j = ci,1;
0 otherwise
}
(1)
3.2 Fuzziness in Top-1 (Mean F-score)
The mean F-score measures how different, on av-
erage, the top transliteration candidate is from its
closest reference. F-score for each source word
is a function of Precision and Recall and equals 1
when the top candidate matches one of the refer-
ences, and 0 when there are no common characters
between the candidate and any of the references.
Precision and Recall are calculated based on
the length of the Longest Common Subsequence
(LCS) between a candidate and a reference:
LCS(c, r) = 1
2
(|c|+ |r| ? ED(c, r)) (2)
3
Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Test
Western English Chinese Institute for Infocomm Research 32K 6K 2K EnCh
Western Chinese English Institute for Infocomm Research 25K 5K 2K ChEn
Western English Korean Hangul CJK Institute 5K 2K 2K EnKo
Western English Japanese Katakana CJK Institute 23K 3K 3K EnJa
Japanese English Japanese Kanji CJK Institute 7K 3K 3K JnJk
Arabic Arabic English CJK Institute 25K 2.5K 2.5K ArAe
Mixed English Hindi Microsoft Research India 10K 2K 2K EnHi
Mixed English Tamil Microsoft Research India 8K 2K 2K EnTa
Mixed English Kannada Microsoft Research India 8K 2K 2K EnKa
Mixed English Bangla Microsoft Research India 10K 2K 2K EnBa
Western English Thai NECTEC 26K 2K 2K EnTh
Western Thai English NECTEC 24K 2K 2K ThEn
Table 1: Source and target languages for the shared task on transliteration.
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses etc.)
3.3 Mean Reciprocal Rank (MRR)
Measures traditional MRR for any right answer
produced by the system, from among the candi-
dates. 1/MRR tells approximately the average
rank of the correct transliteration. MRR closer to 1
implies that the correct answer is mostly produced
close to the top of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR = 1
N
N
?
i=1
RRi (8)
3.4 MAPref
Measures tightly the precision in the n-best can-
didates for i-th source name, for which reference
transliterations are available. If all of the refer-
ences are produced, then the MAP is 1. Let?s de-
note the number of correct candidates for the i-th
source word in k-best list as num(i, k). MAPref
is then given by
MAPref =
1
N
N
?
i
1
ni
( ni
?
k=1
num(i, k)
)
(9)
4 Participation in Shared Task
7 teams from 5 countries and regions (Canada,
Hong Kong, India, Japan, Thailand) submitted
their transliteration results.
Two teams have participated in all or almost all
tasks while others participated in 1 to 4 tasks. Each
language pair has attracted on average around 4
teams. The details are shown in Table 3.
Teams are required to submit at least one stan-
dard run for every task they participated in. In
total, we receive 33 standard and 8. Table 2
shows the number of standard and non-standard
runs submitted for each task. It is clear that the
most ?popular? task is transliteration from English
to Hindi attempted by 5 participants. The next
most popular are other Indic scripts (Tamil, Kan-
nada, Bangla) and Thai, attempted by 3 partici-
pants. This is somewhat different from NEWS
2009, where the two most popular tasks were En-
glish to Hindi and English to Chinese translitera-
tion.
4
English to
Chinese
Chinese to
English
English to
Thai
Thai to En-
glish
English to
Hindi
English to
Tamil
Language pair code EnCh ChEn EnTh ThEn EnHi EnTa
Standard runs 5 2 2 2 7 3
Non-standard runs 0 0 1 1 2 1
English to
Kannada
English to
Japanese
Katakana
English
to Korean
Hangul
English to
Japanese
Kanji
Arabic to
English
English to
Bengali
(Bangla)
Language pair code EnKa EnJa EnKo JnJk ArAe EnBa
Standard runs 3 2 1 1 2 3
Non-standard runs 1 0 0 0 0 2
Table 2: Number of runs submitted for each task. Number of participants coincides with the number of
standard runs submitted.
Team
ID
Organisation EnCh ChEn EnTh ThEn EnHi EnTa EnKa EnJa EnKo JnJk ArAe EnBa
1? IIT, Bombay x
2 University of Alberta x x x x x x x x x x x x
3 x
4 City University of
Hong Kong
x x
5 NICT x x x x x x x x
6 x x
7 Jadavpur University x x x x
Table 3: Participation of teams in different tasks. ?Participation without a system paper.
5 Task Results and Analysis
5.1 Standard runs
All the results are presented numerically in Ta-
bles 4?15, for all evaluation metrics. These are the
official evaluation results published for this edition
of the transliteration shared task.
Among the four submitted system papers1,
Song et al (2010) and Finch and Sumita (2010)
adopt the approach of phrase-based statistical ma-
chine transliteration (Finch and Sumita, 2008),
an approach initially developed for machine trans-
lation (Koehn et al, 2003) while Das et al
(2010) adopts the approach of Conditional Ran-
dom Fields (CRF) (Lafferty et al, 2001). Jiampo-
jamarn et al (2010) further develop DirectTL ap-
proach presented at the previous NEWS work-
shop (Jiampojamarn et al, 2009), achieving very
good performance in the NEWS 2010.
An example of a completely language-
1To maintain anonymity, papers of the teams that submit-
ted anonymous results are not cited in this report.
independent approach is (Finch and Sumita,
2010). Other participants used language-
independent approach but added language-
specific pre- or post-processing (Jiampojamarn
et al, 2010; Das et al, 2010; Song et al, 2010),
including name origin recognition for English to
Hindi task (Jiampojamarn et al, 2010).
Combination of different models via re-ranking
of their outputs has been used in most of the sys-
tems (Das et al, 2010; Song et al, 2010; Finch and
Sumita, 2010). In fact, one system (Song et al,
2010) is mostly devoted to re-ranking of the sys-
tem output to achieve significant improvement of
the ACC (accuracy in top-1) results compared to
the same system in NEWS 2009 workshop (Song,
2009).
Compared the same seven tasks among the
NEWS 2009 and the NEWS 2010 (almost same
training sets, but different test sets), we can see
that the performance in the NEWS 2010 drops ex-
cept the English to Korean task. This could be due
to the fact that NEWS 2010 introduces a entirely
5
new test set, which come from different sources
than the train and dev sets, while NEWS 2009
have all train, dev and test sets from the same
sources.
As far as back-transliteration is concerned, we
can see that English-to-Thai and Thai-to-English
have the similar performance. However, Chinese-
to-English back transliteration performs much
worse than English-to-Chinese forward transliter-
ation. This could be due to the fact that Thai
and English are alphabet languages in nature while
Chinese is not. As a result, Chinese have much
fewer transliteration units than English and Thai.
In other words, Chinese to English translitera-
tion is a one-to-many mapping while English-to-
Chinese is a many-to-one mapping. The later one
has fewer mapping ambiguities.
5.2 Non-standard runs
For the non-standard runs there exist no restric-
tions on the use of data or other linguistic re-
sources. The purpose of non-standard runs is to
see how best personal name transliteration can be,
for a given language pair. In NEWS 2010, the ap-
proaches used in non-standard runs are typical and
may be summarised as follows:
? Pronunciation dictionaries to convert words
to their phonetic transcription (Jiampojamarn
et al, 2010).
? Web search. First, transliteration candidates
are generated. A Web search is then per-
formed to re-affirm or re-rank the candi-
dacy (Das et al, 2010).
Unfortunately, these additional knowledge used
in the non-standard runs is not helpful since all
non-standard runs perform worse than their cor-
responding standard runs. This would be an inter-
esting issue to look into.
6 Conclusions and Future Plans
The Transliteration Generation Shared Task in
NEWS 2010 shows that the community has a
continued interest in this area. This report sum-
marizes the results of the shared task. Again,
we are pleased to report a comprehensive cal-
ibration and baselining of machine translitera-
tion approaches as most state-of-the-art machine
transliteration techniques are represented in the
shared task. The most popular techniques such
as Phrase-Based Machine Transliteration (Koehn
et al, 2003), system combination and re-ranking,
are inspired by recent progress in statistical ma-
chine translation. As the standard runs are lim-
ited by the use of corpus, most of the systems are
implemented under the direct orthographic map-
ping (DOM) framework (Li et al, 2004). While
the standard runs allow us to conduct meaningful
comparison across different algorithms, we recog-
nise that the non-standard runs open up more op-
portunities for exploiting larger linguistic corpora.
It is also noted that two systems have reported
significant performance improvement over their
NEWS 2009 systems.
NEWS 2010 Shared Task represents a success-
ful debut of a community effort in driving machine
transliteration techniques forward. We would like
to continue this event in the future conference to
promote the machine transliteration research and
development.
Acknowledgements
The organisers of the NEWS 2010 Shared Task
would like to thank the Institute for Infocomm
Research (Singapore), Microsoft Research India,
CJK Institute (Japan) and National Electronics and
Computer Technology Center (Thailand) for pro-
viding the corpora and technical support. Without
those, the Shared Task would not be possible. We
thank those participants who identified errors in
the data and sent us the errata. We also want to
thank the members of programme committee for
their invaluable comments that improve the qual-
ity of the shared task papers. Finally, we wish to
thank all the participants for their active participa-
tion that have made this first machine translitera-
tion shared task a comprehensive one.
6
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proc.
ACL-2002Workshop: Computational Apporaches to
Semitic Languages, Philadelphia, PA, USA.
CJKI. 2010. CJK Institute. http://www.cjk.org/.
Amitava Das, Tanik Saikh, Tapabrata Mondal, Asif Ek-
bal, and Sivaji Bandyopadhyay. 2010. English to
Indian languages machine transliteration system at
NEWS 2010. In Proc. ACL Named Entities Work-
shop Shared Task.
D. Demner-Fushman and D. W. Oard. 2002. The ef-
fect of bilingual term list size on dictionary-based
cross-language information retrieval. In Proc. 36-th
Hawaii Int?l. Conf. System Sciences, volume 4, page
108.2.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proc. 3rd Int?l.
Joint Conf NLP, volume 1, Hyderabad, India, Jan-
uary.
Andrew Finch and Eiichiro Sumita. 2010. Transliter-
ation using a phrase-based statistical machine trans-
lation system to re-score the output of a joint multi-
gram model. In Proc. ACL Named Entities Work-
shop Shared Task.
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004.
Phoneme-based transliteration of foreign names for
OOV problem. In Proc. IJCNLP, pages 374?381,
Sanya, Hainan, China.
Yoav Goldberg andMichael Elhadad. 2008. Identifica-
tion of transliterated foreign words in Hebrew script.
In Proc. CICLing, volume LNCS 4919, pages 466?
477.
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. In Proc. EMNLP,
pages 353?362.
Jack Halpern. 2007. The challenges and pitfalls
of Arabic romanization and arabization. In Proc.
Workshop on Comp. Approaches to Arabic Script-
based Lang.
Ulf Hermjakob, Kevin Knight, and Hal Daume?. 2008.
Name translation in statistical machine translation:
Learning when to transliterate. In Proc. ACL,
Columbus, OH, USA, June.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language-independent approach to translit-
eration. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Sittichai Jiampojamarn, Kenneth Dwyer, Shane
Bergsma, Aditya Bhargava, Qing Dou, Mi-Young
Kim, and Grzegorz Kondrak. 2010. Translitera-
tion generation and mining with limited training re-
sources. In Proc. ACL Named Entities Workshop
Shared Task.
Byung-Ju Kang and Key-Sun Choi. 2000.
English-Korean automatic transliteration/back-
transliteration system and character alignment. In
Proc. ACL, pages 17?18, Hong Kong.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Proc.
21st Int?l Conf Computational Linguistics and 44th
Annual Meeting of ACL, pages 817?824, Sydney,
Australia, July.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721?722.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. Int?l.
Conf. Machine Learning, pages 282?289.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159?166,
Barcelona, Spain.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009a. Report of NEWS 2009 machine
transliteration shared task. In Proc. Named Entities
Workshop at ACL 2009.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. ACL-IJCNLP 2009 Named
Entities Workshop ? Shared Task on Translitera-
tion. In Proc. Named Entities Workshop at ACL
2009.
T. Mandl and C. Womser-Hacker. 2005. The effect of
named entities on effectiveness in cross-language in-
formation retrieval evaluation. In Proc. ACM Symp.
Applied Comp., pages 1059?1064.
Helen M. Meng, Wai-Kit Lo, Berlin Chen, and Karen
Tang. 2001. Generate phonetic cognates to han-
dle name entities in English-Chinese cross-language
spoken document retrieval. In Proc. ASRU.
MSRI. 2009. Microsoft Research India.
http://research.microsoft.com/india.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation
and contextual rules. In Proc. COLING 2002,
Taipei, Taiwan.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proc. 45th Annual Meeting
of the ACL, pages 944?951, Prague, Czech Repub-
lic, June.
7
Yan Song, Chunyu Kit, and Hai Zhao. 2010. Rerank-
ing with multiple features for better transliteration.
In Proc. ACL Named Entities Workshop Shared
Task.
Yan Song. 2009. Name entities transliteration via
improved statistical translation on character-level
chunks. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable cor-
pora. In Proc. 21st Int?l Conf Computational Lin-
guistics and 44th Annual Meeting of ACL, pages 73?
80, Sydney, Australia.
Raghavendra Udupa, K. Saravanan, Anton Bakalov,
and Abhijit Bhole. 2009. ?They are out there, if
you know where to look?: Mining transliterations
of OOV query terms for cross-language informa-
tion retrieval. In LNCS: Advances in Information
Retrieval, volume 5478, pages 437?448. Springer
Berlin / Heidelberg.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. ACL MLNER, Sapporo, Japan.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proc. COL-
ING, pages 1352?1356.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In Proc. EMNLP,
pages 612?617, Sydney, Australia, July.
8
Team ID ACC F -score MRR MAPref Organisation
Primary runs
4 0.477333 0.740494 0.506209 0.455491 City University of Hong Kong
2 0.363333 0.707435 0.430168 0.347701 University of Alberta
Non-primary standard runs
2 0.362667 0.704284 0.428854 0.347500 University of Alberta
2 0.360333 0.706765 0.428990 0.345215 University of Alberta
2 0.357000 0.702902 0.419415 0.341567 University of Alberta
Table 4: Runs submitted for English to Chinese task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
4 0.226766 0.749237 0.268557 0.226090 City University of Hong Kong
2 0.137209 0.740364 0.197665 0.136702 University of Alberta
Table 5: Runs submitted for Chinese to English back-transliteration task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.391000 0.872526 0.505264 0.391000 NICT
2 0.377500 0.866254 0.467328 0.377500 University of Alberta
Non-standard runs
6 0.247000 0.842063 0.366959 0.247000
Table 6: Runs submitted for English to Thai task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.396690 0.872642 0.524511 0.396690 NICT
2 0.352056 0.861207 0.450472 0.352056 University of Alberta
Non-standard runs
6 0.092778 0.706995 0.131779 0.092778
Table 7: Runs submitted for Thai to English back-transliteration task.
9
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.456456 0.884199 0.559212 0.456456 University of Alberta
5 0.445445 0.883841 0.574195 0.445445 NICT
3 0.381381 0.860320 0.403172 0.381381
1 0.158158 0.810309 0.231594 0.158158 IIT, Bombay
7 0.150150 0.714490 0.307674 0.150150 Jadavpur University
Non-primary standard runs
2 0.456456 0.885122 0.558203 0.456456 University of Alberta
1 0.142142 0.799092 0.205945 0.142142 IIT, Bombay
Non-standard runs
7 0.254254 0.751766 0.369072 0.254254 Jadavpur University
7 0.170170 0.738777 0.314335 0.170170 Jadavpur University
Table 8: Runs submitted for English to Hindi task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.390000 0.890692 0.515298 0.390000 University of Alberta
5 0.390000 0.886560 0.522088 0.390000 NICT
7 0.013000 0.562917 0.121233 0.013000 Jadavpur University
Non-standard runs
7 0.082000 0.759856 0.142317 0.082000 Jadavpur University
Table 9: Runs submitted for English to Tamil task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.371000 0.871131 0.506010 0.371000 NICT
2 0.341000 0.867133 0.460189 0.341000 University of Alberta
7 0.056000 0.663196 0.111500 0.056000 Jadavpur University
Non-standard runs
7 0.055000 0.662106 0.168750 0.055000 Jadavpur University
Table 10: Runs submitted for English to Kannada task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.397933 0.791233 0.507828 0.398062 University of Alberta
5 0.378295 0.782682 0.510096 0.377778 NICT
Table 11: Runs submitted for English to Japanese Katakana task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.553604 0.770168 0.672665 0.553835 University of Alberta
Table 12: Runs submitted for English to Korean task.
10
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.125937 0.426349 0.201497 0.127339 University of Alberta
Table 13: Runs submitted for English to Japanese Kanji back-transliteration task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.463679 0.923826 0.535097 0.265379 University of Alberta
5 0.403014 0.891443 0.512337 0.327418 NICT
Table 14: Runs submitted for Arabic to English task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.411705 0.882858 0.549913 0.411705 NICT
2 0.394551 0.876947 0.511876 0.394551 University of Alberta
7 0.232089 0.818470 0.325345 0.232089 Jadavpur University
Non-standard runs
7 0.429869 0.875349 0.526152 0.429869 Jadavpur University
7 0.369324 0.845273 0.450589 0.369324 Jadavpur University
Table 15: Runs submitted for English to Bengali (Bangla) task.
11
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 12?20,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Whitepaper of NEWS 2010 Shared Task on Transliteration Generation?
Haizhou Li?, A Kumaran?, Min Zhang? and Vladimir Pervouchine?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
Transliteration is defined as phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is necessary in many applications, such
as machine translation, corpus alignment,
cross-language IR, information extraction
and automatic lexicon acquisition. All
such systems call for high-performance
transliteration, which is the focus of
shared task in the NEWS 2010 workshop.
The objective of the shared task is to pro-
mote machine transliteration research by
providing a common benchmarking plat-
form for the community to evaluate the
state-of-the-art technologies.
1 Task Description
The task is to develop machine transliteration sys-
tem in one or more of the specified language pairs
being considered for the task. Each language pair
consists of a source and a target language. The
training and development data sets released for
each language pair are to be used for developing
a transliteration system in whatever way that the
participants find appropriate. At the evaluation
time, a test set of source names only would be
released, on which the participants are expected
to produce a ranked list of transliteration candi-
dates in another language (i.e. n-best translitera-
tions), and this will be evaluated using common
metrics. For every language pair the participants
must submit at least one run that uses only the
data provided by the NEWS workshop organisers
in a given language pair (designated as ?standard?
run, primary submission). Users may submit more
?stanrard? runs. They may also submit several
?non-standard? runs for each language pair that
?http://translit.i2r.a-star.edu.sg/news2010/
use other data than those provided by the NEWS
2010 workshop; such runs would be evaluated and
reported separately.
2 Important Dates
Research paper submission deadline 1 May 2010
Shared task
Registration opens 1 Feb 2010
Registration closes 13 Mar 2010
Training/Development data release 19 Feb 2010
Test data release 13 Mar 2010
Results Submission Due 20 Mar 2010
Results Announcement 27 Mar 2010
Task (short) Papers Due 5 Apr 2010
For all submissions
Acceptance Notification 6 May 2010
Workshop Date 16 Jul 2010
3 Participation
1. Registration (1 Feb 2010)
(a) NEWS Shared Task opens for registra-
tion.
(b) Prospective participants are to register to
the NEWS Workshop homepage.
2. Training & Development Data (19 Feb 2010)
(a) Registered participants are to obtain
training and development data from the
Shared Task organiser and/or the desig-
nated copyright owners of databases.
(b) All registered participants are required
to participate in the evaluation of at least
one language pair, submit the results and
a short paper and attend the workshop at
ACL 2010.
3. Evaluation Script (19 Feb 2010)
12
(a) A sample test set and expected user out-
put format are to be released.
(b) An evaluation script, which runs on the
above two, is to be released.
(c) The participants must make sure that
their output is produced in a way that
the evaluation script may run and pro-
duce the expected output.
(d) The same script (with held out test data
and the user outputs) would be used for
final evaluation.
4. Test data (13 Mar 2010)
(a) The test data would be released on 13
March 2010, and the participants have a
maximum of 7 days to submit their re-
sults in the expected format.
(b) One ?standard? run must be submit-
ted from every group on a given lan-
guage pair. Additional ?standard? runs
may be submitted, up to 4 ?standard?
runs in total. However, the partici-
pants must indicate one of the submit-
ted ?standard? runs as the ?primary sub-
mission?. The primary submission will
be used for the performance summary.
In addition to the ?standard? runs, more
?non-standard? runs may be submitted.
In total, maximum 8 runs (up to 4 ?stan-
dard? runs plus up to 4 ?non-standard?
runs) can be submitted from each group
on a registered language pair. The defi-
nition of ?standard? and ?non-standard?
runs is in Section 5.
(c) Any runs that are ?non-standard? must
be tagged as such.
(d) The test set is a list of names in source
language only. Every group will pro-
duce and submit a ranked list of translit-
eration candidates in another language
for each given name in the test set.
Please note that this shared task is a
?transliteration generation? task, i.e.,
given a name in a source language one
is supposed to generate one or more
transliterations in a target language. It
is not the task of ?transliteration discov-
ery?, i.e., given a name in the source lan-
guage and a set of names in the target
language evaluate how to find the ap-
propriate names from the target set that
are transliterations of the given source
name.
5. Results (27 Mar 2010)
(a) On 27 March 2010, the evaluation re-
sults would be announced and will be
made available on the Workshop web-
site.
(b) Note that only the scores (in respective
metrics) of the participating systems on
each language pairs would be published,
and no explicit ranking of the participat-
ing systems would be published.
(c) Note that this is a shared evaluation task
and not a competition; the results are
meant to be used to evaluate systems on
common data set with common metrics,
and not to rank the participating sys-
tems. While the participants can cite the
performance of their systems (scores on
metrics) from the workshop report, they
should not use any ranking information
in their publications.
(d) Furthermore, all participants should
agree not to reveal identities of other
participants in any of their publications
unless you get permission from the other
respective participants. By default, all
participants remain anonymous in pub-
lished results, unless they indicate oth-
erwise at the time of uploading their re-
sults. Note that the results of all systems
will be published, but the identities of
those participants that choose not to dis-
close their identity to other participants
will be masked. As a result, in this case,
your organisation name will still appear
in the web site as one of participants, but
it will not be linked explicitly to your re-
sults.
6. Short Papers on Task (5 Apr 2010)
(a) Each submitting site is required to sub-
mit a 4-page system paper (short paper)
for its submissions, including their ap-
proach, data used and the results on ei-
ther test set or development set or by n-
fold cross validation on training set.
(b) The review of the system papers will be
done to improve paper quality and read-
ability and make sure the authors? ideas
13
and methods can be understood by the
workshop participants. We are aiming
at accepting all system papers, and se-
lected ones will be presented orally in
the NEWS 2010 workshop.
(c) All registered participants are required
to register and attend the workshop to
introduce your work.
(d) All paper submission and review will be
managed electronically through https://
www.softconf.com/acl2010/NEWS.
4 Language Pairs
The tasks are to transliterate personal names or
place names from a source to a target language as
summarised in Table 1. NEWS 2010 Shared Task
offers 12 evaluation subtasks, among them ChEn
and ThEn are the back-transliteration of EnCh and
EnTh tasks respectively. NEWS 2010 releases
training, development and testing data for each of
the language pairs. NEWS 2010 continues some
language pairs that were evaluated in NEWS 2009.
In such cases, the training and development data in
the release of NEWS 2010 may overlap with those
in NEWS 2009. However, the test data in NEWS
2010 are entirely new.
The names given in the training sets for Chi-
nese, Japanese, Korean and Thai languages are
Western names and their respective translitera-
tions; the Japanese Name (in English)? Japanese
Kanji data set consists only of native Japanese
names; the Arabic data set consists only of native
Arabic names. The Indic data set (Hindi, Tamil,
Kannada, Bangla) consists of a mix of Indian and
Western names.
Examples of transliteration:
English? Chinese
Timothy????
English? Japanese Katakana
Harrington??????
English? Korean Hangul
Bennett ? ??
Japanese name in English? Japanese Kanji
Akihiro???
English? Hindi
English? Tamil
English? Kannada
Arabic? Arabic name in English
? 
Khalid
????
5 Standard Databases
Training Data (Parallel)
Paired names between source and target lan-
guages; size 5K ? 32K.
Training Data is used for training a basic
transliteration system.
Development Data (Parallel)
Paired names between source and target lan-
guages; size 2K ? 6K.
Development Data is in addition to the Train-
ing data, which is used for system fine-tuning
of parameters in case of need. Participants
are allowed to use it as part of training data.
Testing Data
Source names only; size 2K ? 3K.
This is a held-out set, which would be used
for evaluating the quality of the translitera-
tions.
1. Participants will need to obtain licenses from
the respective copyright owners and/or agree
to the terms and conditions of use that are
given on the downloading website (Li et al,
2004; MSRI, 2010; CJKI, 2010). NEWS
2010 will provide the contact details of each
individual database. The data would be pro-
vided in Unicode UTF-8 encoding, in XML
format; the results are expected to be sub-
mitted in UTF-8 encoding in XML format.
The XML formats details are available in Ap-
pendix A.
2. The data are provided in 3 sets as described
above.
3. Name pairs are distributed as-is, as provided
by the respective creators.
(a) While the databases are mostly man-
ually checked, there may be still in-
consistency (that is, non-standard usage,
region-specific usage, errors, etc.) or in-
completeness (that is, not all right varia-
tions may be covered).
(b) The participants may use any method to
further clean up the data provided.
14
Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Test
Western English Chinese Institute for Infocomm Research 32K 6K 2K EnCh
Western Chinese English Institute for Infocomm Research 25K 5K 2K ChEn
Western English Korean Hangul CJK Institute 5K 2K 2K EnKo
Western English Japanese Katakana CJK Institute 23K 3K 3K EnJa
Japanese English Japanese Kanji CJK Institute 7K 3K 3K JnJk
Arabic Arabic English CJK Institute 25K 2.5K 2.5K ArAe
Mixed English Hindi Microsoft Research India 10K 2K 2K EnHi
Mixed English Tamil Microsoft Research India 8K 2K 2K EnTa
Mixed English Kannada Microsoft Research India 8K 2K 2K EnKa
Mixed English Bangla Microsoft Research India 10K 2K 2K EnBa
Western English Thai NECTEC 26K 2K 2K EnTh
Western Thai English NECTEC 24K 2K 2K ThEn
Table 1: Source and target languages for the shared task on transliteration.
i. If they are cleaned up manually, we
appeal that such data be provided
back to the organisers for redistri-
bution to all the participating groups
in that language pair; such sharing
benefits all participants, and further
ensures that the evaluation provides
normalisation with respect to data
quality.
ii. If automatic cleanup were used,
such cleanup would be considered a
part of the system fielded, and hence
not required to be shared with all
participants.
4. Standard Runs We expect that the partici-
pants to use only the data (parallel names)
provided by the Shared Task for translitera-
tion task for a ?standard? run to ensure a fair
evaluation. One such run (using only the data
provided by the shared task) is mandatory for
all participants for a given language pair that
they participate in.
5. Non-standard Runs If more data (either par-
allel names data or monolingual data) were
used, then all such runs using extra data must
be marked as ?non-standard?. For such ?non-
standard? runs, it is required to disclose the
size and characteristics of the data used in the
system paper.
6. A participant may submit a maximum of 8
runs for a given language pair (including the
mandatory 1 ?standard? run marked as ?pri-
mary submission?).
6 Paper Format
Paper submissions to NEWS 2010 should follow
the ACL 2010 paper submission policy, includ-
ing paper format, blind review policy and title and
author format convention. Full papers (research
paper) are in two-column format without exceed-
ing eight (8) pages of content plus one extra page
for references and short papers (task paper) are
also in two-column format without exceeding four
(4) pages, including references. Submission must
conform to the official ACL 2010 style guidelines.
For details, please refer to the ACL 2010 website2.
7 Evaluation Metrics
We plan to measure the quality of the translitera-
tion task using the following 4 metrics. We accept
up to 10 output candidates in a ranked list for each
input entry.
Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
of these alternatives are considered as a correct
transliteration, and the first correct transliteration
in the ranked list is accepted as a correct hit.
The following notation is further assumed:
2http://acl2010.org/authors.html
15
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
1. Word Accuracy in Top-1 (ACC) Also
known as Word Error Rate, it measures correct-
ness of the first transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ? ri,j : ri,j = ci,1;
0 otherwise
}
(1)
2. Fuzziness in Top-1 (Mean F-score) The
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.
Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses? etc.)
3. Mean Reciprocal Rank (MRR) Measures
traditional MRR for any right answer produced by
the system, from among the candidates. 1/MRR
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available. If all of
the references are produced, then the MAP is 1.
Let?s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
8 Contact Us
If you have any questions about this share task and
the database, please email to
Dr. Haizhou Li
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
hli@i2r.a-star.edu.sg
16
Dr. A. Kumaran
Microsoft Research India
Scientia, 196/36, Sadashivnagar 2nd Main
Road
Bangalore 560080 INDIA
a.kumaran@microsoft.com
Mr. Jack Halpern
CEO, The CJK Dictionary Institute, Inc.
Komine Building (3rd & 4th floors)
34-14, 2-chome, Tohoku, Niiza-shi
Saitama 352-0001 JAPAN
jack@cjki.org
References
[CJKI2010] CJKI. 2010. CJK Institute.
http://www.cjk.org/.
[Li et al2004] Haizhou Li, Min Zhang, and Jian Su.
2004. A joint source-channel model for machine
transliteration. In Proc. 42nd ACL Annual Meeting,
pages 159?166, Barcelona, Spain.
[MSRI2010] MSRI. 2010. Microsoft Research India.
http://research.microsoft.com/india.
17
A Training/Development Data
? File Naming Conventions:
NEWS10 train XXYY nnnn.xml
NEWS10 dev XXYY nnnn.xml
NEWS10 test XXYY nnnn.xml
? XX: Source Language
? YY: Target Language
? nnnn: size of parallel/monolingual
names (?25K?, ?10000?, etc)
? File formats:
All data will be made available in XML for-
mats (Figure 1).
? Data Encoding Formats:
The data will be in Unicode UTF-8 encod-
ing files without byte-order mark, and in the
XML format specified.
B Submission of Results
? File Naming Conventions:
You can give your files any name you like.
During submission online you will need to
indicate whether this submission belongs to
a ?standard? or ?non-standard? run, and if it
is a ?standard? run, whether it is the primary
submission.
? File formats:
All data will be made available in XML for-
mats (Figure 2).
? Data Encoding Formats:
The results are expected to be submitted in
UTF-8 encoded files without byte-order mark
only, and in the XML format specified.
18
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationCorpus
CorpusID = "NEWS2010-Train-EnHi-25K"
SourceLang = "English"
TargetLang = "Hindi"
CorpusType = "Train|Dev"
CorpusSize = "25000"
CorpusFormat = "UTF8">
<Name ID=?1?>
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh1_1</TargetName>
<TargetName ID="2">hhhhhh1_2</TargetName>
...
<TargetName ID="n">hhhhhh1_n</TargetName>
</Name>
<Name ID=?2?>
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh2_1</TargetName>
<TargetName ID="2">hhhhhh2_2</TargetName>
...
<TargetName ID="m">hhhhhh2_m</TargetName>
</Name>
...
<!-- rest of the names to follow -->
...
</TransliterationCorpus>
Figure 1: File: NEWS2010 Train EnHi 25K.xml
19
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationTaskResults
SourceLang = "English"
TargetLang = "Hindi"
GroupID = "Trans University"
RunID = "1"
RunType = "Standard"
Comments = "HMM Run with params: alpha=0.8 beta=1.25">
<Name ID="1">
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh11</TargetName>
<TargetName ID="2">hhhhhh12</TargetName>
<TargetName ID="3">hhhhhh13</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
<Name ID="2">
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh21</TargetName>
<TargetName ID="2">hhhhhh22</TargetName>
<TargetName ID="3">hhhhhh23</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
...
<!-- All names in test corpus to follow -->
...
</TransliterationTaskResults>
Figure 2: Example file: NEWS2010 EnHi TUniv 01 StdRunHMMBased.xml
20
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 21?28,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Report of NEWS 2010 Transliteration Mining Shared Task 
 
A Kumaran Mitesh M. Khapra Haizhou Li 
Microsoft Research India 
Bangalore, India 
Indian Institute of Technology Bombay 
Mumbai, India 
Institute for Infocomm  
Research, Singapore 
 
Abstract 
This report documents the details of the Trans-
literation Mining Shared Task that was run as 
a part of the Named Entities Workshop 
(NEWS 2010), an ACL 2010 workshop.  The 
shared task featured mining of name translite-
rations from the paired Wikipedia titles in 5 
different language pairs, specifically, between 
English and one of Arabic, Chinese, Hindi 
Russian and Tamil.  Totally 5 groups took part 
in this shared task, participating in multiple 
mining tasks in different languages pairs.  The 
methodology and the data sets used in this 
shared task are published in the Shared Task 
White Paper [Kumaran et al 2010]. We meas-
ure and report 3 metrics on the submitted re-
sults to calibrate the performance of individual 
systems on a commonly available Wikipedia 
dataset.  We believe that the significant contri-
bution of this shared task is in (i) assembling a 
diverse set of participants working in the area 
of transliteration mining, (ii) creating a base-
line performance of transliteration mining sys-
tems in a set of diverse languages using com-
monly available Wikipedia data, and (iii) pro-
viding a basis for meaningful comparison and 
analysis of trade-offs between various algo-
rithmic approaches used in mining.  We be-
lieve that this shared task would complement 
the NEWS 2010 transliteration generation 
shared task, in enabling development of prac-
tical systems with a small amount of seed data 
in a given pair of languages. 
1 Introduction  
Proper names play a significant role in Machine 
Translation (MT) and Information Retrieval (IR) 
systems.  When the systems involve multiple 
languages, The MT and IR system rely on Ma-
chine Transliteration systems, as the proper 
names are not usually available in standard trans-
lation lexicons. The quality of the Machine 
Transliteration systems plays a significant part in 
determining the overall quality of the system, 
and hence, they are critical for most multilingual 
application systems.  The importance of Machine 
Transliteration systems has been well understood 
by the community, as evidenced by significant 
publication in this important area. 
While research over the last two decades has 
shown that reasonably good quality Machine 
Transliteration systems may be developed easily, 
they critically rely on parallel names corpora for 
their development.  The Machine Transliteration 
Shared Task of the NEWS 2009 workshop 
(NEWS 2009) has shown that many interesting 
approaches exist for Machine Transliteration, 
and about 10-25K parallel names is sufficient for 
most state of the art systems to provide a practic-
al solution for the critical need.  The traditional 
source for crosslingual parallel data ? the bilin-
gual dictionaries ? offer only limited support as 
they do not include proper names (other than 
ones of historical importance).  The statistical 
dictionaries, though they contain parallel names, 
do not have sufficient coverage, as they depend 
on some threshold statistical evidence 1 . New 
names and many variations of them are intro-
duced to the vocabulary of a language every day 
that need to be captured for any good quality 
end-to-end system such as MT or CLIR.   So 
there is a perennial need for harvesting parallel 
names data, to support end-user applications and 
systems well and accurately. 
This is the specific focus of the Transliteration 
Mining Shared Task in NEWS 2010 workshop 
(an ACL 2010 Workshop): To mine accurately 
parallel names from a popular, ubiquitous source, 
the Wikipedia.  Wikipedia exists in more than 
250 languages, and every Wikipedia article has a 
link to an equivalent article in other languages2.  
We focused on this specific resource ? the Wiki-
pedia titles in multiple languages and the inter-
linking between them ? as the source of parallel 
names.  Any successful mining of parallel names 
from title would signal copious availability of 
parallel names data, enabling transliteration gen-
eration systems in many languages of the world. 
                                                 
1 In our experiments with Indian Express news corpo-
ra over 2 years shows that 80% of the names occur 
less than 5 times in the entire corpora. 
2 Note that the titles contain concepts, events, dates, 
etc., in addition to names.  Even when the titles are 
names, parts of them may not be transliterations. 
21
2 Transliteration Mining Shared Task 
In this section, we provide details of the shared 
task, and the datasets used for the task and results 
evaluation.  
2.1 Shared Task: Task Details 
The task featured in this shared task was to de-
velop a mining system for identifying single 
word transliteration pairs from the standard inter-
linked Wikipedia topics (aka, Wikipedia Inter-
Language Links, or WIL3) in one or more of the 
specified language pairs. The WIL?s link articles 
on the same topic in multiple languages, and are 
traditionally used as a parallel language resource 
for many natural language processing applica-
tions, such as Machine Translation, Crosslingual 
Search, etc.  Specific WIL?s of interest for our 
task were those that contained proper names ? 
either wholly or partly ? which can yield rich 
transliteration data.   
The task involved transliteration mining in the 
language pairs summarized in Table 1.  
 
Source 
Language 
Target Lan-
guage 
Track ID 
English  Chinese  WM-EnCn 
English  Hindi  WM-EnHi 
English  Tamil WM-EnTa 
English  Russian  WM-EnRu 
English Arabic WM-EnAr 
Table 1: Language Pairs in the shared task 
 
Each WIL consisted of a topic in the source 
and target language pair, and the task was to 
identify parts of the topic (in the respective lan-
guage titles) that are transliterations of each oth-
er. A seed data set (of about 1K transliteration 
pairs) was provided for each language pair, and 
was the only resource to be used for developing a 
mining system.  The participants were expected 
to produce a paired list of source-target single 
word named entities, for every WIL provided. At 
the evaluation time, a random subset of WIL?s 
(about 1K WIL?s) in each language pair were 
hand labeled, and used to test the results pro-
duced by the participants.  
Participants were allowed to use only the 1K 
seed data provided by the organizers to produce 
?standard? results; this restriction is imposed to 
provide a meaningful way of comparing the ef-
                                                 
3 Wikipedia?s Interlanguage Links: 
http://en.wikipedia.org/wiki/Help:Interlanguage_links
.  
fective methods and approaches.  However, 
?non-standard? runs were permitted where par-
ticipants were allowed to use more seed data or 
any language-specific resource available to them. 
2.2 Data Sets for the Task  
The following datasets were used for each lan-
guage pair, for this task.   
 
Training Data  Size Remarks 
Seed Data  
(Parallel 
names) 
~1K Paired names be-
tween source and 
target languages. 
To-be-mined 
Wikipedia In-
ter-Wiki-Link 
Data (Noisy) 
Vari-
able 
Paired named entities 
between source and 
target languages ob-
tained directly from 
Wikipedia 
Test Data 
 
~1K This was a subset of 
Wikipedia Inter-
Wiki-Link data, 
which was hand la-
beled for evaluation. 
Table 2: Datasets created for the shared task 
 
The first two sets were provided by the orga-
nizers to the participants, and the third was used 
for evaluation. 
 
Seed transliteration data:  In addition we pro-
vided approximately 1K parallel names in each 
language pair as seed data to develop any metho-
dology to identify transliterations.  For standard 
run results, only this seed data was to be used, 
though for non-standard runs, more data or other 
linguistics resources were allowed. 
 
English Names Hindi Names 
village ????? 
linden ?????? 
market ??????? 
mysore ????? 
Table 3: Sample English-Hindi seed data 
 
English Names Russian Names 
gregory ???????? 
hudson ?????? 
victor ?????? 
baranowski ??????????? 
Table 4: Sample English-Russian seed data 
 
To-Mine-Data WIL data:  All WIL?s were ex-
tracted from the Wikipedia around January 2010, 
22
and provided to the participants.  The extracted 
names were provided as-is, with no hand verifi-
cation about their correctness, completeness or 
consistency.  As sample of the WIL data for Eng-
lish-Hindi and English-Russian is shown in 
Tables 5 and 6 respectively.  Note that there are 
0, 1 or more single-word transliterations from 
each WIL. 
 
# English Wikipedia  
Title 
Hindi Wikipedia 
Title 
1 Indian National Congress ?????? ????????? ????????? 
2 University of Oxford ????????? ????????????? 
3 Indian Institute of Science 
?????? ??????? 
???????? 
4 Jawaharlal Nehru University 
???????? ????? 
?????????????  
Table 5: English-Hindi Wikipedia title pairs 
 
# English Wikipedia  
Title 
Russian Wikipedia 
Title 
1 Mikhail Gorbachev 
????????, ?????? 
????????? 
2 George Washington ?????????, ??????  
3 Treaty of Versailles ??????????? ??????? 
4 French Republic ??????? 
Table 6: English-Russian Wikipedia title pairs 
Test set:  We randomly selected ~1000 wikipe-
dia links (from the large noisy Inter-wiki-links) 
as test-set, and manually extracted the single 
word transliteration pairs associated with each of 
these WILs.  Please note that a given WIL can 
provide 0, 1 or more single-word transliteration 
pairs.  To keep the task simple, it was specified 
that only those transliterations would be consi-
dered correct that were clear transliterations 
word-per-word (morphological variations one or 
both sides are not considered transliterations) 
These 1K test set was be a subset of Wikipedia 
data provided to the user.  The gold dataset is as 
shown in Tables 7 and 8. 
 
WIL# English Names Hindi Names 
1 Congress ????????? 
2 Oxford ????????? 
3 <Null> <Null> 
4 Jawaharlal ???????? 
4 Nehru ????? 
  Table 7: Sample English-Hindi transliteration 
pairs mined from Wikipedia title pairs 
WIL# English Names Russian Names 
1 Mikhail ?????? 
1 Gorbachev ???????? 
2 George ?????? 
2 Washington ????????? 
3 Versailles ??????????? 
4 <Null> <Null> 
  Table 8: Sample English-Russian translitera-
tion pairs mined from Wikipedia title pairs 
2.3 Evaluation: 
The participants were expected to mine such sin-
gle-word transliteration data for every specific 
WIL, though the evaluation was done only 
against the randomly selected, hand-labeled test 
set.  A participant may submit a maximum of 10 
runs for a given language pair (including a min-
imum of one mandatory ?standard? run).  There 
could be more standard runs, without exceeding 
10 (including the non-standard runs). 
At evaluation time, the task organizers 
checked every WIL in test set from among the 
user-provided results, to evaluate the quality of 
the submission on the 3 metrics described later.  
3 Evaluation Metrics 
We measured the quality of the mining task us-
ing the following measures:  
1. PrecisionCorrectTransliterations(PTrans) 
2. RecallCorrectTransliteration  (RTrans) 
3. F-ScoreCorrectTransliteration (FTrans).   
Please refer to the following figures for the ex-
planations: 
 
A = True Positives (TP) = Pairs that were identi-
fied as "Correct Transliterations" by the partici-
pant and were indeed "Correct Transliterations" 
as per the gold standard 
B = False Positives (FP) = Pairs that were identi-
fied as "Correct Transliterations" by the partici-
pant but they were "Incorrect Transliterations" as 
per the gold standard. 
C = False Negatives (FN) = Pairs that were iden-
tified as "Incorrect Transliterations" by the par-
ticipant but were actually "Correct Translitera-
tions" as per the gold standard. 
D = True Negatives (TN) = Pairs that were iden-
tified as "Incorrect Transliterations" by the par-
ticipant and were indeed "Incorrect Translitera-
tions" as per the gold standard.  
 
23
Figure 1: Overview of the mining task and evaluation 
 
1. RecallCorrectTransliteration  (RTrans) 
The recall was computed using the sample as 
follows: 
?????? =
??
?? + ??
=  
?
? + ?
=  
?
?
 
 
2. PrecisionCorrectTransliteration  (PTrans) 
The precision was computed using the sample as 
follows: 
?????? =
??
?? + ??
=  
?
? + ?
 
 
3. F-Score (F) 
? =
2 ? ?????? ? ??????
?????? + ??????
 
4 Participants & Approaches 
The following 5 teams participated in the Trans-
literation Mining Task?: 
 
# Team Organization 
1   Alberta University of Alberta, Canada 
2   CMIC Cairo Microsoft Innovation  
Centre, Egypt 
3   Groningen University of Groningen,  
Netherlands 
4   IBM Egypt IBM Egypt, Cairo, Egypt 
5   MINT? Microsoft Research India, India 
                                                 
? Non-participating system, included for reference.  
  Table 9: Participants in the Shared Task  
The approaches used by the 4 participating 
groups can be broadly classified as discrimina-
tive and generation based approaches. Discri-
minative approaches treat the mining task as a 
binary classification problem where the goal is to 
build a classifier that identifies whether a given 
pair is a valid transliteration pair or not. Genera-
tion based approaches on the other hand generate 
transliterations for each word in the source title 
and measure their similarity with the candidate 
words in the target title. Below, we give a sum-
mary of the various participating systems. 
The CMIC team (Darwish et. al., 2010) used a 
generative transliteration model (HMM) to trans-
literate each word in the source title and com-
pared the transliterations with the words appear-
ing in the target title. For example, for a given 
word Ei in the source title if the model generates 
a transliteration Fj which appears in the target 
title then (Ei, Fj) are considered as transliteration 
pairs. The results are further improved by using 
phonetic conflation (PC) and iteratively training 
(IterT) the generative model using the mined 
transliteration pairs. For phonetic conflation a 
modified SOUNDEX scheme is used wherein 
vowels are discarded and phonetically similar 
characters are conflated. Both, phonetic confla-
tion and iterative training, led to an increase in 
24
recall which was better than the corresponding 
decline in precision. 
The Alberta team (Jiampojamarn et. al., 2010) 
fielded 5 different systems in the shared task. 
The first system uses a simple edit distance based 
method where a pair of strings is classified as a 
transliteration pair if the Normalized Edit Dis-
tance (NED) between them is above a certain 
threshold. To calculate the NED, the target lan-
guage string is first Romanized by replacing each 
target grapheme by the source grapheme having 
the highest conditional probability. These condi-
tional probabilities are obtained by aligning the 
seed set of transliteration pairs using an M2M-
aligner approach (Jiampojamarn et. al., 2007). 
The second system uses a SVM based discrimin-
ative classifier trained using an improved feature 
representation (BK 2007) (Bergsma and Kon-
drak, 2007). These features include all substring 
pairs up to a maximum length of three as ex-
tracted from the aligned word pairs. The transli-
teration pairs in the seed data provided for the 
shared task were used as positive examples. The 
negative examples were obtained by generating 
all possible source-target pairs in the seed data 
and taking those pairs which are not translitera-
tions but have a longest common subsequence 
ratio above a certain threshold. One drawback of 
this system is that longer substrings cannot be 
used due to the combinatorial explosion in the 
number of unique features as the substring length 
increases. To overcome this problem they pro-
pose a third system which uses a standard n-gram 
string kernel (StringKernel) that implicitly em-
beds a string in a feature space that has one co-
ordinate for each unique n-gram (Shawe-Taylor 
and Cristianini, 2004). The above 3 systems are 
essentially discriminative systems. In addition, 
they propose a generation based approach (DI-
RECTL+) which determines whether the gener-
ated transliteration pairs of a source word and 
target word are similar to a given candidate pair. 
They use a state-of-the-art online discriminative 
sequence prediction model based on many-to-
many alignments, further augmented by the in-
corporation of joint n-gram features (Jiampoja-
marn et. al., 2010). Apart from the four systems 
described above, they propose an additional sys-
tem for English Chinese, wherein they formulate 
the mining task as a matching problem (Match-
ing) and greedily extract the pairs with highest 
similarity. The similarity is calculated using the 
alignments obtained by training a generation 
model (Jiampojamarn et. al., 2007) using the 
seed data. 
The IBM Cairo team (Noemans et. al., 2010) 
proposed a generation based approach which 
takes inspiration from Phrase Based Statistical 
Machine Translation (PBSMT) and learns a cha-
racter-to-character alignment model between the 
source and target language using GIZA++. This 
alignment table is then represented using a finite 
state automaton (FSA) where the input is the 
source character and the output is the target cha-
racter. For a given word in the source title, can-
didate transliterations are generated using this 
FST and are compared with the words in the tar-
get title. In addition they also submitted a base-
line run which used phonetic edit distance. 
The Groningen (Nabende et. al., 2010) team 
used a generation based approach that uses pair 
HMMs (P-HMM) to find the similarity between 
a given pair of source and target strings. The 
proposed variant of pair HMM uses transition 
parameters that are distinct between each of the 
edit states and emission parameters that are also 
distinct. The three edits states are substitution 
state, deletion state and insertion state. The pa-
rameters of the pair HMM are estimated using 
the Baum-Welch Expectation Maximization al-
gorithm (Baum et. al. 1970).  
Finally, as a reference, results of a previously 
published system ? MINT (Udupa et. al., 2009) ? 
were also included in this report as a reference.  
MINT is a large scalable mining system for min-
ing transliterations from comparable corpora, 
essentially multilingual news articles in the same 
timeline.  While MINT takes a two step approach 
? first aligning documents based on content simi-
larity, and subsequently mining transliterations 
based on a name similarity model ? for this task, 
only the transliteration mining step is employed. 
For mining transliterations a logistic function 
based similarity model (LFS) trained discrimina-
tively with the seed parallel names data was em-
ployed.  It should be noted here that the MINT 
algorithm was used as-is for mining translitera-
tions from Wikipedia paired titles, with no fine-
tuning.  While the standard runs used only the 
data provided by the organizers, the non-standard 
runs used about 15K (Seed+) parallel names be-
tween the languages. 
5 Results & Analysis 
The results for EnAr, EnCh, EnHi, EnRu and 
EnTa are summarized in Tables 10, 11, 12, 13 
and 14 respectively. The results clearly indicate 
that there is no single approach which performs 
well across all languages. In fact, there is even 
25
no single genre (discriminative v/s generation 
based) which performs well across all languages. 
We, therefore, do a case by case analysis of the 
results and highlight some important observa-
tions. 
? The discriminative classifier using string 
kernels proposed by Jiampojamarn et. al. 
(2010) consistently performed well in all the 
4 languages that it was tested on. Specifical-
ly, it gave the best performance for EnHi and 
EnTa. 
? The simple discriminative approach based on 
Normalized Edit Distance (NED) gave the 
best result for EnRu. Further, the authors re-
port that the results of StringKernel and BK-
2007 were not significantly better than NED. 
? The use of phonetic conflation consistently 
performed better than the case when phonet-
ic conflation was not used.  
? The results for EnCh are significantly lower 
when compared to the results for other lana-
guge pairs. This shows that mining translite-
ration pairs between alphabetic languages 
(EnRu, EnAr, EnHi, EnTa) is relatively easi-
er as compared to the case when one of the 
languages is non-alphabetic (EnCh) 
6 Plans for the Future Editions 
This shared task was designed as a comple-
mentary shared task to the popular NEWS 
Shared Tasks on Transliteration Generation; suc-
cessful mining of transliteration pairs demon-
strated in this shared task would be a viable 
source for generating data for developing a state 
of the art transliteration generation system.    
We intend to extend the scope of the mining in 
3 different ways: (i) extend mining to more lan-
guage pairs, (ii) allow identification of near 
transliterations where there may be changes do to 
the morphology of the target (or the source) lan-
guages, and, (iii) demonstrate an end-to-end 
transliteration system that may be developed 
starting with a small seed corpora of, say, 1000 
paired names. 
 
 
References  
Baum, L., Petrie, T., Soules, G. and Weiss, N. 1970. A 
Maximization Technique Occurring in the Statis-
tical Analysis of Probabilistic Functions of Markov 
Chains. In The Annals of Mathematical Statistics, 
41 (1): 164-171. 
Bergsma, S. and Kondrak, G. 2007. Alignment Based 
Discriminative String Similarity. In Proceedings of 
the 45th Annual Meeting of the ACL, 2007.  
Darwish, K. 2010. Transliteration Mining with Pho-
netic Conflation and Iterative Training. Proceed-
ings of the 2010 Named Entities Workshop: Shared 
Task on Transliteration Mining, 2010.  
Jiampojamarn, S., Dwyer, K., Bergsma, S., Bhargava, 
A., Dou, Q., Kim, M. Y. and Kondrak, G. 2010. 
Transliteration generation and mining with limited 
training resources. Proceedings of the 2010 
Named Entities Workshop: Shared Task on Trans-
literation Mining, 2010. 
Shawe-Taylor, J and Cristianini, N. 2004. Kernel Me-
thods for Pattern Analysis. Cambridge University 
Press. 
Klementiev, A. and Roth, D. 2006. Weakly supervised 
named entity transliteration and discovery from 
multilingual comparable corpora. Proceedings of 
the 44th Annual Meeting of the ACL, 2006.  
Knight, K. and Graehl, J. 1998. Machine Translitera-
tion. Computational Linguistics.  
Kumaran, A., Khapra, M. and Li, Haizhou. 2010. 
Whitepaper on NEWS 2010 Shared Task on Trans-
literation Mining. Proceedings of the 2010 Named 
Entities Workshop: Shared Task on Transliteration 
Mining, 2010. 
Nabende, P. 2010. Mining Transliterations from Wi-
kipedia using Pair HMMs. Proceedings of the 2010 
Named Entities Workshop: Shared Task on Trans-
literation Mining, 2010. 
Noeman, S. and Madkour, A. 2010. Language inde-
pendent Transliteration mining system using Finite 
State Automata framework. Proceedings of the 
2010 Named Entities Workshop: Shared Task on 
Transliteration Mining, 2010.  
Udupa, R., Saravanan, K., Kumaran, A. and Jagarla-
mudi, J. 2009. MINT: A Method for Effective and 
Scalable Mining of Named Entity Transliterations 
from Large Comparable Corpora. Proceedings of 
the 12th Conference of the European Chapter of 
Association for Computational Linguistics, 2009.  
  
26
Participant Run Type Description Precision Recall F-Score 
IBM Egypt 
 
Standard 
FST, edit distance 2 with nor-
malized characters 0.887 0.945 0.915 
IBM Egypt 
 
Standard 
FST, edit distance 1 with nor-
malized characters 0.859 0.952 0.903 
IBM Egypt 
 
Standard 
Phonetic distance, with norma-
lized characters 0.923 0.830 0.874 
CMIC Standard HMM + IterT 0.886 0.817 0.850 
CMIC Standard HMM + PC 0.900 0.796 0.845 
CMIC Standard (HMM + ItertT) + PC 0.818 0.827 0.822 
Alberta Non- Standard  0.850 0.780 0.820 
Alberta Standard BK-2007 0.834 0.798 0.816 
Alberta Standard NED+ 0.818 0.783 0.800 
CMIC Standard (HMM + PC + ItertT) + PC 0.895 0.678 0.771 
Alberta Standard DirecTL+ 0.861 0.652 0.742 
CMIC Standard HMM 0.966 0.587 0.730 
CMIC Standard HMM + PC + IterT 0.952 0.588 0.727 
IBM Egypt 
 
Standard 
FST, edit distance 2 without 
normalized characters 0.701 0.747 0.723 
IBM Egypt 
 
Standard 
FST, edit distance 1 without 
normalized characters 0.681 0.755 0.716 
IBM Egypt 
 
Standard 
Phonetic distance, without 
normalized characters 0.741 0.666 0.702 
Table 10: Results of the English Arabic task 
 
Participant Run Type Description Precision Recall F-Score 
Alberta Standard Matching 0.698 0.427 0.530 
Alberta Non-Standard  0.700 0.430 0.530 
CMIC Standard (HMM + IterT) + PC 1 0.030 0.059 
CMIC Standard HMM + IterT 1 0.026 0.05 
CMIC Standard HMM + PC 1 0.024 0.047 
CMIC Standard (HMM + PC + IterT) + PC 1 0.022 0.044 
CMIC Standard HMM 1 0.016 0.032 
CMIC Standard HMM + PC + IterT 1 0.016 0.032 
Alberta Standard DirecTL+ 0.045 0.005 0.009 
Table 11: Results of the English Chinese task 
 
Participant Run Type Description Precision Recall F-Score 
MINT? Non-Standard LFS + Seed+ 0.967 0.923 0.944 
Alberta  Standard StringKernel 0.954 0.895 0.924 
Alberta Standard NED+ 0.875 0.941 0.907 
Alberta Standard DirecTL+ 0.945 0.866 0.904 
CMIC Standard (HMM + PC + IterT) + PC 0.953 0.855 0.902 
Alberta Standard BK-2007 0.883 0.880 0.882 
CMIC Standard (HMM + IterT) + PC  0.951 0.812 0.876 
CMIC Standard HMM + PC 0.959 0.786 0.864 
Alberta Non-Standard  0.890 0.820 0.860 
MINT? Standard LFS 0.943 0.780 0.854 
MINT? Standard LFS 0.946 0.773 0.851 
                                                 
? Non-participating system 
27
CMIC Standard HMM + PC + IterT 0.981 0.687 0.808 
CMIC Standard HMM + IterT 0.984 0.569 0.721 
CMIC Standard HMM 0.987 0.559 0.714 
Table 10: Results of the English Hindi task 
 
Participant Run Type Description Precision Recall F-Score 
Alberta Standard NED+ 0.880 0.869 0.875 
CMIC Standard HMM + PC 0.813 0.839 0.826 
MINT? Non-Standard LFS + Seed+ 0.797 0.853 0.824 
Groningen? Standard P-HMM 0.780 0.834 0.806 
Alberta Standard StringKernel 0.746 0.889 0.811 
CMIC Standard HMM 0.868 0.748 0.804 
CMIC Standard HMM + PC + IterT 0.843 0.747 0.792 
Alberta Non-Standard  0.730 0.870 0.790 
Alberta Standard DirecTL+ 0.778 0.795 0.786 
CMIC Standard HMM + IterT 0.716 0.868 0.785 
MINT? Standard LFS 0.822 0.752 0.785 
CMIC Standard (HMM + PC + IterT) + PC 0.771 0.794 0.782 
Alberta Standard BK-2007 0.684 0.902 0.778 
CMIC Standard (HMM + IterT) + PC 0.673 0.881 0.763 
Groningen Standard P-HMM 0.658 0.334 0.444 
Table 11: Results of the English Russian task 
 
Participant Run Type Description Precision Recall F-Score 
Alberta Standard StringKernel 0.923 0.906 0.914 
MINT? Non-Standard LFS + Seed+ 0.910 0.897 0.904 
MINT? Standard LFS 0.899 0.814 0.855 
MINT? Standard LFS 0.913 0.790 0.847 
Alberta Standard BK-2007 0.808 0.852 0.829 
CMIC Standard (HMM + IterT) + PC 0.939 0.741 0.828 
Alberta Non-Standard  0.820 0.820 0.820 
Alberta Standard DirectL+ 0.919 0.710 0.801 
Alberta Standard NED+ 0.916 0.696 0.791 
CMIC Standard HMM + IterT 0.952 0.668 0.785 
CMIC Standard HMM + PC 0.963 0.604 0.743 
CMIC Standard (HMM + PC + IterT) + PC 0.968 0.567 0.715 
CMIC Standard HMM + PC + IterT 0.975 0.446 0.612 
CMIC Standard HMM 0.976 0.407 0.575 
Table 12: Results of the English Tamil task 
 
                                                 
? Non-participating system 
? Post-deadline submission of the participating system 
28
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 29?38,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Whitepaper of NEWS 2010 Shared Task on  
Transliteration Mining 
A Kumaran Mitesh M. Khapra Haizhou Li 
Microsoft Research India 
Bangalore, India 
Indian Institute of Technology-Bombay 
Mumbai, India 
Institute for Infocomm  
Research, Singapore 
 
 
 
Abstract 
Transliteration is generally defined as phonetic 
translation of names across languages. Ma-
chine Transliteration is a critical technology in 
many domains, such as machine translation, 
cross-language information retriev-
al/extraction, etc. Recent research has shown 
that high quality machine transliteration sys-
tems may be developed in a language-neutral 
manner, using a reasonably sized good quality 
corpus (~15-25K parallel names) between a 
given pair of languages.  In this shared task, 
we focus on acquisition of such good quality 
names corpora in many languages, thus com-
plementing the machine transliteration shared 
task that is concurrently conducted in the same 
NEWS 2010 workshop.  Specifically, this task 
focuses on mining the Wikipedia paired enti-
ties data (aka, inter-wiki-links) to produce 
high-quality transliteration data that may be 
used for transliteration tasks. 
1 Task Description  
The task is to develop a system for mining single 
word transliteration pairs from the standard Wi-
kipedia paired topics (aka, Wikipedia Inter-
Language Links, or WIL1) in one or more of the 
specified language pairs. The WIL?s link articles 
on the same topic in multiple languages, and are 
traditionally used as a parallel language resource 
for many NLP applications, such as Machine 
Translation, Crosslingual Search, etc.  Specific 
WIL?s of interest for our task are those that con-
tain proper names ? either wholly or partly ? 
which can yield rich transliteration data.   
Each WIL consists of a topic in the source and 
the language pair, and the task is to identify parts 
of the topic (in the respective language titles) that 
are transliterations of each other. A seed data set 
(of about 1K transliteration pairs) would be pro-
vided for each language pair, and are the only 
resource to be used for developing a mining sys-
tem.  The participants are expected to produce a 
                                                 
1
 Wikipedia?s Interlanguage Links:  
http://en.wikipedia.org/wiki/Help:Interlanguage_links.  
paired list of source-target single word named 
entities, for every WIL provided. At the evalua-
tion time, a random subset of WIL?s (about 1K 
WIL?s) in each language pair that are hand la-
beled would be used to test the results produced 
by the participants.  
Participants may use only the 1K seed data 
provided by the organizers to produce ?standard? 
results; this restriction is imposed to provide a 
meaningful way of comparing the effective me-
thods and approaches.  However, ?non-standard? 
runs would be permitted where participants may 
use more seed data or any language-specific re-
source available to them. 
2 Important Dates  
SHARED TASK SCHEDULES 
Registration Opens  1-Feb-2010 
Registration Closes   13-Mar-2010 
Training Data Release  26 -Feb-2010 
Test Data Release  13-Mar-2010 
Results Submission Due  20-Mar-2010 
Evaluation Results An-
nouncement 27-Mar-2010 
Short Papers Due  5-Apr-2010 
Workshop Paper Sub-
mission Closes  5-Apr-2010 
Workshop & Task Pa-
pers Acceptance  6-May-2010 
CRC Due  15-May-2010 
Workshop Date   16-Jul-2010 
3 Participation 
1. Registration (1 Feb 2010) 
a. Prospective participants are to register to 
the NEWS-2010 Workshop homepage, for 
this specific task. 
2. Training Data Release (26 Feb 2010) 
a. Registered participants are to obtain seed 
and Wikipedia data from the Shared Task 
organizers. 
 
29
3. Evaluation Script (1 March 2010) 
a. A sample submission and an evaluation 
script will be released in due course. 
b. The participants must make sure that their 
output is produced in a way that the evalua-
tion script may run and produce the ex-
pected output. 
c. The same script (with held out test data and 
the user outputs) would be used for final 
evaluation. 
 
4. Testing data (13 March 2010) 
a. The test data would be a held out data of 
approximately 1K ?gold-standard? mined 
data. 
b. The submissions (up to 10) would be tested 
against the test data, and the results pub-
lished. 
 
5. Results (27 March 2010) 
a. On the results announcement date, the 
evaluation results would be published on 
the Workshop website. 
b. Note that only the scores (in respective me-
trics) of the participating systems on each 
language pairs would be published, but no 
explicit ranking of the participating sys-
tems.   
c. Note that this is a shared evaluation task 
and not a competition; the results are meant 
to be used to evaluate systems on common 
data set with common metrics, and not to 
rank the participating systems.  While the 
participants can cite the performance of 
their systems (scores on metrics) from the 
workshop report, they should not use any 
ranking information in their publications. 
d. Further, all participants should agree not to 
reveal identities of other participants in any 
of their publications unless you get permis-
sion from the other respective participants. 
If the participants want to remain anonym-
ous in published results, they should inform 
the organizers at the time of registration.  
Note that the results of their systems would 
still be published, but with the participant 
identities masked. As a result, in this case, 
your organization name will still appear in 
the web site as one of participants, but it is 
not linked explicitly with your results. 
 
6. Short Papers on Task (5 April 2010) 
a. Each submitting site is required to submit a 
4-page system paper (short paper) for its 
submissions, including their approach, data 
used and the results. 
b. All system short papers will be included in 
the proceedings. Selected short papers will 
be presented in the NEWS 2010 workshop.  
Acceptance of the system short-papers 
would be announced together with that of 
other papers. 
4 Languages Involved  
The task involves transliteration mining in the 
language pairs summarized in the following ta-
ble.   
   
Source Lan-
guage 
Target Lan-
guage 
Track ID 
English  Chinese  WM-EnCn 
English  Hindi  WM-EnHi 
English  Tamil WM-EnTa 
English  Russian  WM-EnRu 
English Arabic WM-EnAr 
Table 1: Language Pairs in the shared task 
5 Data Sets for the Task  
The following datasets are used for each lan-
guage pair, for this task.   
 
Training Data  Size Remarks 
Seed Data (Pa-
rallel) 
~1K Paired names be-
tween source and 
target languages. 
To-be-mined 
Wikipedia Inter-
Wiki-Link Data 
(Noisy) 
Vari-
able 
Paired named entities 
between source and 
target languages ob-
tained directly from 
Wikipedia 
Test Data 
 
~1K This is a subset of 
Wikipedia Inter-
Wiki-Link data, 
which will be hand 
labeled. 
Table 2: Datasets for the shared task 
The first two sets would be provided by the or-
ganizers to the participants, and the third will be 
used for evaluation. 
 
To-Mine-Data WIL data:  All WIL?s from an 
appropriate download from Wikipedia would be 
provided.  The WIL data might look like the 
samples shown in Tables 3 and 4, with the sin-
30
gle-word transliterations highlighted.  Note that 
there could be 0, 1 or more single-word translite-
rations from each WIL. 
 
# English Wikipedia  
Title 
Hindi Wikipedia 
Title 
1 Indian National Congress ?????? ????????? ????????? 
2 University of Oxford ????????? 
????????????? 
3 Indian Institute of Science ?????? ??????? 
???????? 
4 Jawaharlal Nehru Univer-
sity 
???????? ????? 
?????????????  
Table 3: Sample English-Hindi Wikipedia title 
pairs 
 
# English Wikipedia  
Title 
Russian Wikipedia 
Title 
1 Mikhail Gorbachev ????????, ?????? 
????????? 
2 George Washington ?????????, ??????  
3 Treaty of Versailles ??????????? ??????? 
4 French Republic ??????? 
Table 4: Sample English-Russian Wikipedia title 
pairs 
Seed transliteration data:  In addition we pro-
vide approximately 1K parallel names in each 
language pair as seed data to develop any metho-
dology to identify transliterations.  For standard 
run results, only this seed data could be used, 
though for non-standard runs, more data or other 
linguistics resources may be used. 
English Names Hindi Names 
Village ????? 
Linden ??????? 
Market ????? 
Mysore ????? 
Table 5: Sample English-Hindi seed data 
 
English Names Russian Names 
Gregory ???????? 
Hudson ?????? 
Victor ?????? 
baranowski ??????????? 
Table 6: Sample English-Russian seed data 
 
Test set:  We plan to randomly select ~1000 wi-
kipedia links (from the large noisy Inter-wiki-
links) as test-set, and manually extract the single 
word transliteration pairs associated with each of 
these WILs.  Please note that a given WIL can 
provide 0, 1 or more single-word transliteration 
pairs.  To keep the task simple, we consider as 
correct transliterations only those that are clear 
transliterations word-per-word (morphological 
variations one or both sides are not considered 
transliterations) These 1K test set will be a subset 
of Wikipedia data provided to the user.  The gold 
dataset might look like the following (assuming 
the items 1, 2, 3 and 4 in Tables 3 and 4 were 
among the randomly selected WIL?s from To-
Mine-Data).   
 
WIL# English Names Hindi Names 
1 Congress ????????? 
2 Oxford ????????? 
3 <Null> <Null> 
4 Jawaharlal ???????? 
4 Nehru ????? 
  Table 7: Sample English-Hindi transliteration 
pairs mined from Wikipedia title pairs 
 
WIL# English Names Russian Names 
1 Mikhail ?????? 
1 Gorbachev ???????? 
2 George ?????? 
2 Washington ????????? 
3 Versailles ??????????? 
4 <Null> <Null> 
  Table 8: Sample English-Russian translitera-
tion pairs mined from Wikipedia title pairs 
 
Evaluation: The participants are expected to 
mine such single-word transliteration data for 
every specific WIL, though the evaluation would 
be done only against the randomly selected, 
hand-labeled test set.  At evaluation time, the 
task organizers check every WIL in test set from 
among the user-provided results, to evaluate the 
quality of the submission on the 3 metrics de-
scribed later.  
Additional information on data use: 
1. Seed data may have ownership and appropri-
ate licenses may need to be procured for use.  
2. To-be-mined Wikipedia data is extracted 
from Wikipedia (in Jan/Feb 2010), and dis-
tributed as-is.  No assurances that they are 
correct, complete or consistent. 
 
31
Figure 1: Overview of the mining task and evaluation 
 
3. The hand-labeled test set is created by 
NEWS shared task organizers, and will be 
used for computing the metrics for a given 
submission. 
4. We expect that the participants to use only 
the seed data (parallel names) provided by 
the Shared Task for a standard run to ensure 
a fair evaluation and a meaningful compari-
son between the effectiveness of approaches 
taken by various systems.  At least one such 
run (using only the data provided by the 
shared task) is mandatory for all participants 
for a given task that they participate in.   
5. If more data (either parallel names data or 
monolingual data), or any language-specific 
modules were used, then all such runs using 
extra data or resources must be marked as 
?Non-standard?.  For such non-standard 
runs, it is required to disclose the size and 
characteristics of the data or the nature of 
languages resources used, in their paper. 
6. A participant may submit a maximum of 10 
runs for a given language pair (including one 
or more ?standard? run).  There could be 
more standard runs, without exceeding 10 
(including the non-standard runs). 
6 Paper Format 
All paper submissions to NEWS 2010 should 
follow the ACL 2010 paper submission policy 
(http://acl2010.org/papers.html), including paper 
format, blind review policy and title and author 
format convention. Shared task system short pa-
pers are also in two-column format without ex-
ceeding four (4) pages plus any extra page for 
references. However, there is no need for double-
blind requirements, as the users may refer to 
their runs and metrics in the published results.   
7 Evaluation Metrics 
We plan to measure the quality of the mining 
task using the following measures:  
 
1. PrecisionCorrectTransliterations (PTrans) 
2. RecallCorrectTransliteration (RTrans) 
3. F-ScoreCorrectTransliteration (FTrans).   
 
Please refer to the following figures for the ex-
planations: 
 
A = True Positives (TP) = Pairs that were identi-
fied as "Correct Transliterations" by the partici-
pant and were indeed "Correct Transliterations" 
as per the gold standard 
B = False Positives (FP) = Pairs that were identi-
fied as "Correct Transliterations" by the partici-
pant but they were "Incorrect Transliterations" as 
per the gold standard. 
C = False Negatives (FN) = Pairs that were iden-
tified as "Incorrect Transliterations" by the par-
ticipant but were actually "Correct Translitera-
tions" as per the gold standard. 
32
D = True Negatives (TN) = Pairs that were iden-
tified as "Incorrect Transliterations" by the par-
ticipant and were indeed "Incorrect Translitera-
tions" as per the gold standard. 
 
1. RecallCorrectTransliteration  (RTrans) 
The recall is going to be computed using the 
sample as follows: 
?????? =
??
?? + ??
=  
?
? + ?
=  
?
?
 
 
2. PrecisionCorrectTransliteration  (PTrans) 
The precision is going to be computed using the 
sample as follows: 
?????? =
??
?? + ??
=  
?
? + ?
 
3. F-Score (F) 
? =
2 ? ?????? ? ??????
?????? + ??????
 
8 Contact Us 
If you have any questions about this share task 
and the database, please contact one of the orga-
nizers below: 
 
Dr. A. Kumaran 
 Microsoft Research India 
Bangalore 560080 INDIA 
a.kumaran@microsoft.com 
 
Mitesh Khapra  
 Indian Institute of Technology-Bombay 
 Mumbai, INDIA 
MKhapra@cse.iitb.ac.in.  
 
Dr Haizhou Li 
 Institute for Infocomm Research 
 Singapore, SINGAPORE 138632 
hli@i2r.a-star.edu.sg.  
  
33
Appendix A: Seed Parallel Names Data  
 
? File Naming Conventions: 
o NEWS09_Seed_XXYY_1K.xml,  
? XX: Source Language 
? YY: Target Language 
? 1K: number of parallel names  
 
? File Formats:  
o All data would be made available in XML formats (Appendix A). 
 
? Data Encoding Formats:  
o The data would be in Unicode, in UTF-8 encoding.  The results are expected to be 
submitted in UTF-8 format only, and in the XML format specified. 
 
File: NEWS2009_Seed_EnHi_1000.xml 
 
<?xml version="1.0" encoding="UTF-8"?> 
 <SeedCorpus 
      CorpusID = "NEWS2009-Seed-EnHi-1K" 
     SourceLang = "English" 
     TargetLang = "Hindi" 
     CorpusType = "Seed" 
     CorpusSize = "1000" 
     CorpusFormat = "UTF8"> 
  <Name ID=?1?> 
   <SourceName>eeeeee1</SourceName> 
   <TargetName ID="1">hhhhhh1_1</TargetName> 
   <TargetName ID="2">hhhhhh1_2</TargetName> 
   ... 
   <TargetName ID="n">hhhhhh1_n</TargetName> 
  </Name> 
  <Name ID=?2?> 
   <SourceName>eeeeee2</SourceName> 
   <TargetName ID="1">hhhhhh2_1</TargetName> 
   <TargetName ID="2">hhhhhh2_2</TargetName> 
   ... 
   <TargetName ID="m">hhhhhh2_m</TargetName> 
  </Name> 
... 
  <!-- rest of the names to follow --> 
  ... 
 </SeedCorpus> 
 
 
Appendix B: Wikipedia InterwikiLinks Data  
 
? File Naming Conventions: 
o NEWS09_Wiki_XXYY_nnnn.xml,  
? XX: Source Language 
? YY: Target Language 
? nnnn: size of paired entities culled from Wikipedia (?25K?, ?10000?, etc.) 
? File Formats:  
o All data would be made available in XML formats (Appendix A). 
? Data Encoding Formats:  
o The data would be in Unicode, in UTF-8 encoding.  The results are expected to be 
submitted in UTF-8 format only, and in the XML format specified. 
 
 
34
File: NEWS2009_Wiki_EnHi_10K.xml 
<?xml version="1.0" encoding="UTF-8"?> 
 <WikipediaCorpus 
      CorpusID = "NEWS2009-Wiki-EnHi-10K" 
     SourceLang = "English" 
     TargetLang = "Hindi" 
     CorpusType = "Wiki" 
     CorpusSize = "10000" 
     CorpusFormat = "UTF8"> 
  <Title ID=?1?> 
   <SourceEntity>e1 e2 ? en</SourceEntity> 
   <TargetEntity>h1 h2 ? hm</TargetEntity> 
  </Title> 
  <Title ID=?2?> 
   <SourceEntity>e1 e2 ? ei</SourceEntity> 
   <TargetEntity>h1 h2 ? hj</TargetEntity> 
  </Title> 
... 
  <!-- rest of the titles to follow --> 
  ... 
 </ WikipediaCorpus> 
 
 
Appendix C: Results Submission - Format 
 
? File Naming Conventions: 
o NEWS09_Result_XXYY_gggg_nn_description.xml 
? XX: Source 
? YY: Target 
? gggg: Group ID 
? nn: run ID.  
? description: Description of the run 
? File Formats:  
o All results would be submitted in XML formats (Appendix B). 
? Data Encoding Formats:  
o The data would be in Unicode, in UTF-8 encoding.  The results are expected to be 
submitted in UTF-8 format only. 
Example: NEWS2009_EnHi_TUniv_01_HMMBased.xml 
 
<?xml version="1.0" encoding="UTF-8"?> 
 <WikipediaMiningTaskResults 
      SourceLang = "English" 
     TargetLang = "Hindi" 
     GroupID = "Trans University" 
     RunID = "1" 
     RunType = "Standard" 
    Comments = "SVD Run with params: alpha=xxx beta=yyy"> 
  <Title ID="1"> 
   <MinedPair ID="1"> 
<SourceName>e1</SourceName> 
    <TargetName>h1</TargetName> 
</MinedPair> 
   <MinedPair ID="2"> 
<SourceName>e2</SourceName> 
    <TargetName>h2</TargetName> 
</MinedPair> 
    <!?followed by other pairs mined from this title--> 
  </Title> 
  <Title ID="2"> 
   <MinedPair ID="1"> 
<SourceName>e1</SourceName> 
    <TargetName>h1</TargetName> 
</MinedPair> 
35
   <MinedPair ID="2"> 
<SourceName>e2</SourceName> 
    <TargetName>h2</TargetName> 
</MinedPair> 
   <!?followed by other pairs mined from this title--> 
  </Title> 
... 
  <!-- All titles in the culled data to follow --> 
  ... 
 </WikipediaMiningTaskResults> 
 
 
Appendix D: Sample Eng-Hindi Interwikilink Data 
 
<?xml version="1.0" encoding="UTF-8"?>  
<WikipediaCorpus CorpusID = "NEWS2009-Wiki-EnHi-Sample"  
SourceLang = "English"  
TargetLang = "Hindi"  
CorpusType = "Wiki" CorpusSize = "3" 
 CorpusFormat = "UTF8"> 
  <Title ID="1"> 
  <SourceEntity>Indian National Congress</SourceEntity> 
  <TargetEntity>?????? ????????? ?????????</TargetEntity> 
 </Title> 
<!-- {Congress, ?????????} should be identified by the paricipants--> 
 <Title ID="2"> 
  <SourceEntity>University of Oxford</SourceEntity> 
  <TargetEntity>????????? ?????????????</TargetEntity> 
 </Title> 
<!-- {Oxford, ?????????} should be identified by the paricipants--> 
 <Title ID="3"> 
  <SourceEntity>Jawaharlal Nehru University</SourceEntity> 
  <TargetEntity>???????? ????? ?????????????</TargetEntity> 
 </Title> 
<!-- {Jawaharlal, ????????} and {Nehru, ?????} should be  
identified by the paricipants--> 
 <Title ID="4"> 
  <SourceEntity>Indian Institute Of Science</SourceEntity> 
  <TargetEntity>?????? ??????? ????????</TargetEntity> 
 </Title> 
<!--There are no transliteration pairs here --> 
</WikipediaCorpus> 
 
 
Appendix E: Eng-Hindi Gold Mined Data (wrt the above WIL Data) 
 
<?xml version="1.0" encoding="UTF-8"?> 
<WikipediaMiningTaskResults 
 SourceLang = "English" 
 TargetLang = "Hindi" 
 GroupID = "Gold-Standard" 
 RunID = "" 
 RunType = "" 
Comments = ""> 
 <Title ID="1"> 
  <MinedPair ID="1"> 
   <SourceName>Congress</SourceName> 
   <TargetName> ?????????</TargetName> 
  </MinedPair> 
 </Title> 
 <Title ID="2"> 
  <MinedPair ID="1"> 
36
   <SourceName>Oxford</SourceName> 
   <TargetName> ?????????</TargetName> 
  </MinedPair> 
 </Title> 
 <Title ID="3"> 
  <MinedPair ID="1"> 
   <SourceName>Jawaharlal</SourceName> 
   <TargetName> ????????</TargetName> 
  </MinedPair> 
  <MinedPair ID="2"> 
   <SourceName>Nehru</SourceName> 
   <TargetName> ?????</TargetName> 
  </MinedPair> 
 </Title> 
 <Title ID="4"> 
 </Title> 
</WikipediaMiningTaskResults> 
 
 
Appendix F: English-Hindi Sample Submission and Evaluation 
 
<?xml version="1.0" encoding="UTF-8"?> 
<WikipediaMiningTaskResults 
 SourceLang = "English" 
 TargetLang = "Hindi" 
 GroupID = "Gold-Standard" 
 RunID = "" 
 RunType = "" 
 <Title ID="1"> 
  <MinedPair ID="1"> 
   <SourceName>Congress</SourceName> 
   <TargetName> ?????????</TargetName> 
  </MinedPair> 
The participant mined all correct transliteration pairs  
 </Title> 
 <Title ID="2"> 
  <MinedPair ID="1"> 
   <SourceName>Oxford</SourceName> 
   <TargetName> ?????????</TargetName> 
  </MinedPair> 
  <MinedPair ID="1"> 
   <SourceName>University</SourceName> 
   <TargetName>?????????????</TargetName> 
  </MinedPair> 
The participant mined an incorrect transliteration pair {University,?????????????} 
 </Title> 
 <Title ID="3"> 
  <MinedPair ID="1"> 
   <SourceName>Jawaharlal</SourceName> 
   <TargetName> ????????</TargetName> 
  </MinedPair> 
The participant missed the correct transliteration pair {Nehru, ?????} 
 </Title> 
 <Title ID="4"> 
  <MinedPair ID="1"> 
   <SourceName>Indian</SourceName> 
   <TargetName>??????</TargetName> 
  </MinedPair> 
The participant mined an incorrect transliteration pair {Indian, ??????} 
 </Title> 
</WikipediaMiningTaskResults> 
 
37
Sample Evaluation 
T = |{(Congress, ?????????), (Oxford, ?????????), (Jawaharlal, ????????),(Nehru, ?????)} | = 4 
A = TP = | {(Congress, ?????????), (Oxford, ?????????), (Jawaharlal, ????????)}| = 3 
B = FP = |{(Indian, ??????), (University, ?????????????) }| = 2 
C = FN = |{(Nehru, ?????)}| = 1 
 
?????? =
??
?? + ??
=  
?
? + ?
=  
?
?
=  
3
4
= 0.75 
 
?????? =
??
??+??
=  
?
?+?
=  
3
5
= 0.60 
 
? =
2 ? ?????? ? ??????
?????? + ??????
=  
2 ? 0.6 ? 0.75
0.6 + 0.75
=  0.67 
 
38
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?9,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Whitepaper of NEWS 2012 Shared Task on Machine Transliteration?
Min Zhang?, Haizhou Li?, Ming Liu?, A Kumaran?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{mzhang,hli,mliu}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
Transliteration is defined as phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is necessary in many applications, such
as machine translation, corpus alignment,
cross-language IR, information extraction
and automatic lexicon acquisition. All
such systems call for high-performance
transliteration, which is the focus of
shared task in the NEWS 2012 workshop.
The objective of the shared task is to pro-
mote machine transliteration research by
providing a common benchmarking plat-
form for the community to evaluate the
state-of-the-art technologies.
1 Task Description
The task is to develop machine transliteration sys-
tem in one or more of the specified language pairs
being considered for the task. Each language pair
consists of a source and a target language. The
training and development data sets released for
each language pair are to be used for developing
a transliteration system in whatever way that the
participants find appropriate. At the evaluation
time, a test set of source names only would be
released, on which the participants are expected
to produce a ranked list of transliteration candi-
dates in another language (i.e. n-best translitera-
tions), and this will be evaluated using common
metrics. For every language pair the participants
must submit at least one run that uses only the
data provided by the NEWS workshop organisers
in a given language pair (designated as ?standard?
run, primary submission). Users may submit more
?stanrard? runs. They may also submit several
?non-standard? runs for each language pair that
?http://translit.i2r.a-star.edu.sg/news2012/
use other data than those provided by the NEWS
2012 workshop; such runs would be evaluated and
reported separately.
2 Important Dates
Research paper submission deadline 25 March 2012
Shared task
Registration opens 18 Jan 2012
Registration closes 11 Mar 2012
Training/Development data release 20 Jan 2012
Test data release 12 Mar 2012
Results Submission Due 16 Mar 2012
Results Announcement 20 Mar 2012
Task (short) Papers Due 25 Mar 2012
For all submissions
Acceptance Notification 20 April 2012
Camera-Ready Copy Deadline 30 April 2012
Workshop Date 12/13/14 July 2012
3 Participation
1. Registration (18 Jan 2012)
(a) NEWS Shared Task opens for registra-
tion.
(b) Prospective participants are to register to
the NEWS Workshop homepage.
2. Training & Development Data (20 Jan 2012)
(a) Registered participants are to obtain
training and development data from the
Shared Task organiser and/or the desig-
nated copyright owners of databases.
(b) All registered participants are required
to participate in the evaluation of at least
one language pair, submit the results and
a short paper and attend the workshop at
ACL 2012.
3. Test data (12 March 2012)1
(a) The test data would be released on 12
March 2012, and the participants have a
maximum of 5 days to submit their re-
sults in the expected format.
(b) One ?standard? run must be submit-
ted from every group on a given lan-
guage pair. Additional ?standard? runs
may be submitted, up to 4 ?standard?
runs in total. However, the partici-
pants must indicate one of the submit-
ted ?standard? runs as the ?primary sub-
mission?. The primary submission will
be used for the performance summary.
In addition to the ?standard? runs, more
?non-standard? runs may be submitted.
In total, maximum 8 runs (up to 4 ?stan-
dard? runs plus up to 4 ?non-standard?
runs) can be submitted from each group
on a registered language pair. The defi-
nition of ?standard? and ?non-standard?
runs is in Section 5.
(c) Any runs that are ?non-standard? must
be tagged as such.
(d) The test set is a list of names in source
language only. Every group will pro-
duce and submit a ranked list of translit-
eration candidates in another language
for each given name in the test set.
Please note that this shared task is a
?transliteration generation? task, i.e.,
given a name in a source language one
is supposed to generate one or more
transliterations in a target language. It
is not the task of ?transliteration discov-
ery?, i.e., given a name in the source lan-
guage and a set of names in the target
language evaluate how to find the ap-
propriate names from the target set that
are transliterations of the given source
name.
4. Results (20 March 2012)
(a) On 20 March 2012, the evaluation re-
sults would be announced and will be
made available on the Workshop web-
site.
(b) Note that only the scores (in respective
metrics) of the participating systems on
each language pairs would be published,
and no explicit ranking of the participat-
ing systems would be published.
(c) Note that this is a shared evaluation task
and not a competition; the results are
meant to be used to evaluate systems on
common data set with common metrics,
and not to rank the participating sys-
tems. While the participants can cite the
performance of their systems (scores on
metrics) from the workshop report, they
should not use any ranking information
in their publications.
(d) Furthermore, all participants should
agree not to reveal identities of other
participants in any of their publications
unless you get permission from the other
respective participants. By default, all
participants remain anonymous in pub-
lished results, unless they indicate oth-
erwise at the time of uploading their re-
sults. Note that the results of all systems
will be published, but the identities of
those participants that choose not to dis-
close their identity to other participants
will be masked. As a result, in this case,
your organisation name will still appear
in the web site as one of participants, but
it will not be linked explicitly to your re-
sults.
5. Short Papers on Task (25 March 2012)
(a) Each submitting site is required to sub-
mit a 4-page system paper (short paper)
for its submissions, including their ap-
proach, data used and the results on ei-
ther test set or development set or by n-
fold cross validation on training set.
(b) The review of the system papers will be
done to improve paper quality and read-
ability and make sure the authors? ideas
and methods can be understood by the
workshop participants. We are aiming
at accepting all system papers, and se-
lected ones will be presented orally in
the NEWS 2012 workshop.
(c) All registered participants are required
to register and attend the workshop to
introduce your work.
(d) All paper submission and review will be
managed electronically through https://
www.softconf.com/acl2012/news2012/.2
4 Language Pairs
The tasks are to transliterate personal names or
place names from a source to a target language as
summarised in Table 1. NEWS 2012 Shared Task
offers 14 evaluation subtasks, among them ChEn
and ThEn are the back-transliteration of EnCh and
EnTh tasks respectively. NEWS 2012 releases
training, development and testing data for each of
the language pairs. NEWS 2012 continues all lan-
guage pairs that were evaluated in NEWS 2011. In
such cases, the training and development data in
the release of NEWS 2012 are the same as those
in NEWS 2011. However, the test data in NEWS
2012 are entirely new.
Please note that in order to have an accurate
study of the research progress of machine transla-
tion technology, different from previous practice,
the test/reference sets of NEWS 2011 are not re-
leased to the research community. Instead, we
use the test sets of NEWS 2011 as progress test
sets in NEWS 2012. NEWS 2012 participants are
requested to submit results on the NEWS 2012
progress test sets (i.e., NEWS 2011 test sets). By
doing so, we would like to do comparison studies
by comparing the NEWS 2012 and NEWS 2011
results on the progress test sets. We hope that we
can have some insightful research findings in the
progress studies.
The names given in the training sets for Chi-
nese, Japanese, Korean, Thai and Persian lan-
guages are Western names and their respective
transliterations; the Japanese Name (in English)
? Japanese Kanji data set consists only of native
Japanese names; the Arabic data set consists only
of native Arabic names. The Indic data set (Hindi,
Tamil, Kannada, Bangla) consists of a mix of In-
dian and Western names.
Examples of transliteration:
English ? Chinese
Timothy ???
English ? Japanese Katakana
Harrington ??????
English ? Korean Hangul
Bennett ? ??
Japanese name in English ? Japanese Kanji
Akihiro ???
English ? Hindi
San Francisco ? ????????????????
English ? Tamil
London ? ??????
English ? Kannada
Tokyo ? ??????
Arabic ? Arabic name in English
? 
Khalid
????
5 Standard Databases
Training Data (Parallel)
Paired names between source and target lan-
guages; size 7K ? 37K.
Training Data is used for training a basic
transliteration system.
Development Data (Parallel)
Paired names between source and target lan-
guages; size 1K ? 2.8K.
Development Data is in addition to the Train-
ing data, which is used for system fine-tuning
of parameters in case of need. Participants
are allowed to use it as part of training data.
Testing Data
Source names only; size 1K ? 2K.
This is a held-out set, which would be used
for evaluating the quality of the translitera-
tions.
Progress Testing Data
Source names only; size 0.6K ? 2.6K.
This is the NEWS 2011 test set, it is held-out
for progress study.
1. Participants will need to obtain licenses from
the respective copyright owners and/or agree
to the terms and conditions of use that are
given on the downloading website (Li et al,
2004; MSRI, 2010; CJKI, 2010). NEWS
2011 will provide the contact details of each
individual database. The data would be pro-
vided in Unicode UTF-8 encoding, in XML
format; the results are expected to be sub-
mitted in UTF-8 encoding in XML format.
The XML formats details are available in Ap-
pendix A.
2. The data are provided in 3 sets as described
above.
3. Name pairs are distributed as-is, as provided
by the respective creators.3
Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Progress Test 2012 Test
Western English Chinese Institute for Infocomm Research 37K 2.8K 2K 1K EnCh
Western Chinese English Institute for Infocomm Research 28K 2.7K 2.2K 1K ChEn
Western English Korean Hangul CJK Institute 7K 1K 609 1K EnKo
Western English Japanese Katakana CJK Institute 26K 2K 1.8K 1K EnJa
Japanese English Japanese Kanji CJK Institute 10K 2K 571 1K JnJk
Arabic Arabic English CJK Institute 27K 2.5K 2.6K 1K ArEn
Mixed English Hindi Microsoft Research India 12K 1K 1K 1K EnHi
Mixed English Tamil Microsoft Research India 10K 1K 1K 1K EnTa
Mixed English Kannada Microsoft Research India 10K 1K 1K 1K EnKa
Mixed English Bangla Microsoft Research India 13K 1K 1K 1K EnBa
Western English Thai NECTEC 27K 2K 2K 1K EnTh
Western Thai English NECTEC 25K 2K 1.9K 1K ThEn
Western English Persian Sarvnaz Karimi / RMIT 10K 2K 2K 1K EnPe
Western English Hebrew Microsoft Research India 9.5K 1K 1K 1K EnHe
Table 1: Source and target languages for the shared task on transliteration.
(a) While the databases are mostly man-
ually checked, there may be still in-
consistency (that is, non-standard usage,
region-specific usage, errors, etc.) or in-
completeness (that is, not all right varia-
tions may be covered).
(b) The participants may use any method to
further clean up the data provided.
i. If they are cleaned up manually, we
appeal that such data be provided
back to the organisers for redistri-
bution to all the participating groups
in that language pair; such sharing
benefits all participants, and further
ensures that the evaluation provides
normalisation with respect to data
quality.
ii. If automatic cleanup were used,
such cleanup would be considered a
part of the system fielded, and hence
not required to be shared with all
participants.
4. Standard Runs We expect that the partici-
pants to use only the data (parallel names)
provided by the Shared Task for translitera-
tion task for a ?standard? run to ensure a fair
evaluation. One such run (using only the data
provided by the shared task) is mandatory for
all participants for a given language pair that
they participate in.
5. Non-standard Runs If more data (either par-
allel names data or monolingual data) were
used, then all such runs using extra data must
be marked as ?non-standard?. For such ?non-
standard? runs, it is required to disclose the
size and characteristics of the data used in the
system paper.
6. A participant may submit a maximum of 8
runs for a given language pair (including the
mandatory 1 ?standard? run marked as ?pri-
mary submission?).
6 Paper Format
Paper submissions to NEWS 2012 should follow
the ACL 2012 paper submission policy, includ-
ing paper format, blind review policy and title and
author format convention. Full papers (research
paper) are in two-column format without exceed-
ing eight (8) pages of content plus two (2) extra
page for references and short papers (task paper)
are also in two-column format without exceeding
four (4) pages content plus two (2) extra page for
references. Submission must conform to the offi-
cial ACL 2012 style guidelines. For details, please
refer to the ACL 2012 website2.
7 Evaluation Metrics
We plan to measure the quality of the translitera-
tion task using the following 4 metrics. We accept
up to 10 output candidates in a ranked list for each
input entry.
Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
of these alternatives are considered as a correct
transliteration, and the first correct transliteration
in the ranked list is accepted as a correct hit.
2http://www.ACL2012.org/4
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
1. Word Accuracy in Top-1 (ACC) Also
known as Word Error Rate, it measures correct-
ness of the first transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ? ri,j : ri,j = ci,1;
0 otherwise
}
(1)
2. Fuzziness in Top-1 (Mean F-score) The
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.
Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses etc.)
3. Mean Reciprocal Rank (MRR) Measures
traditional MRR for any right answer produced by
the system, from among the candidates. 1/MRR
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available. If all of
the references are produced, then the MAP is 1.
Let?s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
8 Contact Us
If you have any questions about this share task and
the database, please email to
Mr. Ming Liu
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
mliu@i2r.a-star.edu.sg5
Dr. Min Zhang
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
mzhang@i2r.a-star.edu.sg
References
[CJKI2010] CJKI. 2010. CJK Institute.
http://www.cjk.org/.
[Li et al2004] Haizhou Li, Min Zhang, and Jian Su.
2004. A joint source-channel model for machine
transliteration. In Proc. 42nd ACL Annual Meeting,
pages 159?166, Barcelona, Spain.
[MSRI2010] MSRI. 2010. Microsoft Research India.
http://research.microsoft.com/india.
6
A Training/Development Data
? File Naming Conventions:
NEWS12 train XXYY nnnn.xml
NEWS12 dev XXYY nnnn.xml
NEWS12 test XXYY nnnn.xml
NEWS11 test XXYY nnnn.xml
(progress test sets)
? XX: Source Language
? YY: Target Language
? nnnn: size of parallel/monolingual
names (?25K?, ?10000?, etc)
? File formats:
All data will be made available in XML for-
mats (Figure 1).
? Data Encoding Formats:
The data will be in Unicode UTF-8 encod-
ing files without byte-order mark, and in the
XML format specified.
B Submission of Results
? File Naming Conventions:
You can give your files any name you like.
During submission online you will need to
indicate whether this submission belongs to
a ?standard? or ?non-standard? run, and if it
is a ?standard? run, whether it is the primary
submission.
? File formats:
All data will be made available in XML for-
mats (Figure 2).
? Data Encoding Formats:
The results are expected to be submitted in
UTF-8 encoded files without byte-order mark
only, and in the XML format specified.
7
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationCorpus
CorpusID = "NEWS2012-Train-EnHi-25K"
SourceLang = "English"
TargetLang = "Hindi"
CorpusType = "Train|Dev"
CorpusSize = "25000"
CorpusFormat = "UTF8">
<Name ID=fl1fl>
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh1_1</TargetName>
<TargetName ID="2">hhhhhh1_2</TargetName>
...
<TargetName ID="n">hhhhhh1_n</TargetName>
</Name>
<Name ID=fl2fl>
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh2_1</TargetName>
<TargetName ID="2">hhhhhh2_2</TargetName>
...
<TargetName ID="m">hhhhhh2_m</TargetName>
</Name>
...
<!-- rest of the names to follow -->
...
</TransliterationCorpus>
Figure 1: File: NEWS2012 Train EnHi 25K.xml
8
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationTaskResults
SourceLang = "English"
TargetLang = "Hindi"
GroupID = "Trans University"
RunID = "1"
RunType = "Standard"
Comments = "HMM Run with params: alpha=0.8 beta=1.25">
<Name ID="1">
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh11</TargetName>
<TargetName ID="2">hhhhhh12</TargetName>
<TargetName ID="3">hhhhhh13</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
<Name ID="2">
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh21</TargetName>
<TargetName ID="2">hhhhhh22</TargetName>
<TargetName ID="3">hhhhhh23</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
...
<!-- All names in test corpus to follow -->
...
</TransliterationTaskResults>
Figure 2: Example file: NEWS2012 EnHi TUniv 01 StdRunHMMBased.xml
9
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 10?20,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Report of NEWS 2012 Machine Transliteration Shared Task
Min Zhang?, Haizhou Li?, A Kumaran? and Ming Liu ?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{mzhang,hli,mliu}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
This report documents the Machine
Transliteration Shared Task conducted as
a part of the Named Entities Workshop
(NEWS 2012), an ACL 2012 workshop.
The shared task features machine translit-
eration of proper names from English to
11 languages and from 3 languages to
English. In total, 14 tasks are provided.
7 teams participated in the evaluations.
Finally, 57 standard and 1 non-standard
runs are submitted, where diverse translit-
eration methodologies are explored and
reported on the evaluation data. We report
the results with 4 performance metrics.
We believe that the shared task has
successfully achieved its objective by pro-
viding a common benchmarking platform
for the research community to evaluate the
state-of-the-art technologies that benefit
the future research and development.
1 Introduction
Names play a significant role in many Natural
Language Processing (NLP) and Information Re-
trieval (IR) systems. They are important in Cross
Lingual Information Retrieval (CLIR) and Ma-
chine Translation (MT) as the system performance
has been shown to positively correlate with the
correct conversion of names between the lan-
guages in several studies (Demner-Fushman and
Oard, 2002; Mandl and Womser-Hacker, 2005;
Hermjakob et al, 2008; Udupa et al, 2009). The
traditional source for name equivalence, the bilin-
gual dictionaries ? whether handcrafted or sta-
tistical ? offer only limited support because new
names always emerge.
All of the above point to the critical need for ro-
bust Machine Transliteration technology and sys-
tems. Much research effort has been made to ad-
dress the transliteration issue in the research com-
munity (Knight and Graehl, 1998; Meng et al,
2001; Li et al, 2004; Zelenko and Aone, 2006;
Sproat et al, 2006; Sherif and Kondrak, 2007;
Hermjakob et al, 2008; Al-Onaizan and Knight,
2002; Goldwasser and Roth, 2008; Goldberg and
Elhadad, 2008; Klementiev and Roth, 2006; Oh
and Choi, 2002; Virga and Khudanpur, 2003; Wan
and Verspoor, 1998; Kang and Choi, 2000; Gao
et al, 2004; Zelenko and Aone, 2006; Li et al,
2009b; Li et al, 2009a). These previous work
fall into three categories, i.e., grapheme-based,
phoneme-based and hybrid methods. Grapheme-
based method (Li et al, 2004) treats translitera-
tion as a direct orthographic mapping and only
uses orthography-related features while phoneme-
based method (Knight and Graehl, 1998) makes
use of phonetic correspondence to generate the
transliteration. Hybrid method refers to the com-
bination of several different models or knowledge
sources to support the transliteration generation.
The first machine transliteration shared task (Li
et al, 2009b; Li et al, 2009a) was held in NEWS
2009 at ACL-IJCNLP 2009. It was the first time
to provide common benchmarking data in diverse
language pairs for evaluation of state-of-the-art
techniques. While the focus of the 2009 shared
task was on establishing the quality metrics and
on baselining the transliteration quality based on
those metrics, the 2010 shared task (Li et al,
2010a; Li et al, 2010b) expanded the scope of
the transliteration generation task to about a dozen
languages, and explored the quality depending on
the direction of transliteration, between the lan-
guages. In NEWS 2011 (Zhang et al, 2011a;
Zhang et al, 2011b), we significantly increased
the hand-crafted parallel named entities corpora to
include 14 different language pairs from 11 lan-
guage families, and made them available as the
common dataset for the shared task. NEWS 2012
was a continued effort of NEWS 2011, NEWS10
2010 and NEWS 2009.
The rest of the report is organised as follows.
Section 2 outlines the machine transliteration task
and the corpora used and Section 3 discusses the
metrics chosen for evaluation, along with the ratio-
nale for choosing them. Sections 4 and 5 present
the participation in the shared task and the results
with their analysis, respectively. Section 6 con-
cludes the report.
2 Transliteration Shared Task
In this section, we outline the definition and the
description of the shared task.
2.1 ?Transliteration?: A definition
There exists several terms that are used inter-
changeably in the contemporary research litera-
ture for the conversion of names between two
languages, such as, transliteration, transcription,
and sometimes Romanisation, especially if Latin
scripts are used for target strings (Halpern, 2007).
Our aim is not only at capturing the name con-
version process from a source to a target lan-
guage, but also at its practical utility for down-
stream applications, such as CLIR and MT. There-
fore, we adopted the same definition of translit-
eration as during the NEWS 2009 workshop (Li
et al, 2009a) to narrow down ?transliteration? to
three specific requirements for the task, as fol-
lows:?Transliteration is the conversion of a given
name in the source language (a text string in the
source writing system or orthography) to a name
in the target language (another text string in the
target writing system or orthography), such that
the target language name is: (i) phonemically
equivalent to the source name (ii) conforms to the
phonology of the target language and (iii) matches
the user intuition of the equivalent of the source
language name in the target language, consider-
ing the culture and orthographic character usage
in the target language.?
Following NEWS 2011, in NEWS 2012, we
still keep the three back-transliteration tasks. We
define back-transliteration as a process of restor-
ing transliterated words to their original lan-
guages. For example, NEWS 2012 offers the tasks
to convert western names written in Chinese and
Thai into their original English spellings, and ro-
manized Japanese names into their original Kanji
writings.
2.2 Shared Task Description
Following the tradition of NEWS workshop se-
ries, the shared task at NEWS 2012 is specified
as development of machine transliteration systems
in one or more of the specified language pairs.
Each language pair of the shared task consists of a
source and a target language, implicitly specifying
the transliteration direction. Training and develop-
ment data in each of the language pairs have been
made available to all registered participants for de-
veloping a transliteration system for that specific
language pair using any approach that they find
appropriate.
At the evaluation time, a standard hand-crafted
test set consisting of between 500 and 3,000
source names (approximately 5-10% of the train-
ing data size) have been released, on which the
participants are required to produce a ranked list
of transliteration candidates in the target language
for each source name. The system output is
tested against a reference set (which may include
multiple correct transliterations for some source
names), and the performance of a system is cap-
tured in multiple metrics (defined in Section 3),
each designed to capture a specific performance
dimension.
For every language pair each participant is re-
quired to submit at least one run (designated as a
?standard? run) that uses only the data provided by
the NEWS workshop organisers in that language
pair, and no other data or linguistic resources. This
standard run ensures parity between systems and
enables meaningful comparison of performance
of various algorithmic approaches in a given lan-
guage pair. Participants are allowed to submit
more ?standard? runs, up to 4 in total. If more than
one ?standard? runs is submitted, it is required to
name one of them as a ?primary? run, which is
used to compare results across different systems.
In addition, up to 4 ?non-standard? runs could be
submitted for every language pair using either data
beyond that provided by the shared task organisers
or linguistic resources in a specific language, or
both. This essentially may enable any participant
to demonstrate the limits of performance of their
system in a given language pair.
The shared task timelines provide adequate time
for development, testing (more than 1 month after
the release of the training data) and the final re-
sult submission (4 days after the release of the test
data).11
2.3 Shared Task Corpora
We considered two specific constraints in select-
ing languages for the shared task: language diver-
sity and data availability. To make the shared task
interesting and to attract wider participation, it is
important to ensure a reasonable variety among
the languages in terms of linguistic diversity, or-
thography and geography. Clearly, the ability of
procuring and distributing a reasonably large (ap-
proximately 10K paired names for training and
testing together) hand-crafted corpora consisting
primarily of paired names is critical for this pro-
cess. At the end of the planning stage and after
discussion with the data providers, we have cho-
sen the set of 14 tasks shown in Table 1 (Li et al,
2004; Kumaran and Kellner, 2007; MSRI, 2009;
CJKI, 2010).
NEWS 2012 leverages on the success of NEWS
2011 by utilizing the training set of NEWS 2011 as
the training data of NEWS 2012 and the dev data
of NEWS 2011 as the dev data of NEWS 2012.
NEWS 2012 provides entirely new test data across
all 14 tasks for evaluation.
The names given in the training sets for Chi-
nese, Japanese, Korean, Thai, Persian and Hebrew
languages are Western names and their respective
transliterations; the Japanese Name (in English)
? Japanese Kanji data set consists only of native
Japanese names; the Arabic data set consists only
of native Arabic names. The Indic data set (Hindi,
Tamil, Kannada, Bangla) consists of a mix of In-
dian and Western names.
For all of the tasks chosen, we have been
able to procure paired names data between the
source and the target scripts and were able to
make them available to the participants. For
some language pairs, such as English-Chinese and
English-Thai, there are both transliteration and
back-transliteration tasks. Most of the task are just
one-way transliteration, although Indian data sets
contained mixture of names of both Indian and
Western origins. The language of origin of the
names for each task is indicated in the first column
of Table 1.
Finally, it should be noted here that the corpora
procured and released for NEWS 2012 represent
perhaps the most diverse and largest corpora to be
used for any common transliteration tasks today.
3 Evaluation Metrics and Rationale
The participants have been asked to submit results
of up to four standard and four non-standard runs.
One standard run must be named as the primary
submission and is used for the performance sum-
mary. Each run contains a ranked list of up to
10 candidate transliterations for each source name.
The submitted results are compared to the ground
truth (reference transliterations) using 4 evalua-
tion metrics capturing different aspects of translit-
eration performance. The same as the NEWS
2011, we have dropped two MAP metrics used
in NEWS 2009 because they don?t offer additional
information to MAPref . Since a name may have
multiple correct transliterations, all these alterna-
tives are treated equally in the evaluation, that is,
any of these alternatives is considered as a correct
transliteration, and all candidates matching any of
the reference transliterations are accepted as cor-
rect ones.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
3.1 Word Accuracy in Top-1 (ACC)
Also known as Word Error Rate, it measures cor-
rectness of the first transliteration candidate in the
candidate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ?ri,j : ri,j = ci,1;
0 otherwise
}
(1)
3.2 Fuzziness in Top-1 (Mean F-score)
The mean F-score measures how different, on av-
erage, the top transliteration candidate is from its
closest reference. F-score for each source word12
Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Test
Western English Chinese Institute for Infocomm Research 37K 2.8K 2K 1K EnCh
Western Chinese English Institute for Infocomm Research 28K 2.7K 2.2K 1K ChEn
Western English Korean Hangul CJK Institute 7K 1K 609 1K EnKo
Western English Japanese Katakana CJK Institute 26K 2K 1.8K 1K EnJa
Japanese English Japanese Kanji CJK Institute 10K 2K 571 1K JnJk
Arabic Arabic English CJK Institute 27K 2.5K 2.6K 1K ArEn
Mixed English Hindi Microsoft Research India 12K 1K 1K 1K EnHi
Mixed English Tamil Microsoft Research India 10K 1K 1K 1K EnTa
Mixed English Kannada Microsoft Research India 10K 1K 1K 1K EnKa
Mixed English Bangla Microsoft Research India 13K 1K 1K 1K EnBa
Western English Thai NECTEC 27K 2K 2K 1K EnTh
Western Thai English NECTEC 25K 2K 1.9K 1K ThEn
Western English Persian Sarvnaz Karimi / RMIT 10K 2K 2K 1K EnPe
Western English Hebrew Microsoft Research India 9.5K 1K 1K 1K EnHe
Table 1: Source and target languages for the shared task on transliteration.
is a function of Precision and Recall and equals 1
when the top candidate matches one of the refer-
ences, and 0 when there are no common characters
between the candidate and any of the references.
Precision and Recall are calculated based on
the length of the Longest Common Subsequence
(LCS) between a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses etc.)
3.3 Mean Reciprocal Rank (MRR)
Measures traditional MRR for any right answer
produced by the system, from among the candi-
dates. 1/MRR tells approximately the average
rank of the correct transliteration. MRR closer to 1
implies that the correct answer is mostly produced
close to the top of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
3.4 MAPref
Measures tightly the precision in the n-best can-
didates for i-th source name, for which reference
transliterations are available. If all of the refer-
ences are produced, then the MAP is 1. Let?s de-
note the number of correct candidates for the i-th
source word in k-best list as num(i, k). MAPref
is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
4 Participation in Shared Task
7 teams submitted their transliteration results. Ta-
ble 3 shows the details of registration tasks. Teams
are required to submit at least one standard run for
every task they participated in. In total, we re-
ceive 57 standard and 1 non-standard runs. Table 2
shows the number of standard and non-standard
runs submitted for each task. It is clear that the
most ?popular? task is the transliteration from En-
glish to Chinese being attempted by 7 participants.13
English to
Chinese
Chinese to
English
English to
Thai
Thai to En-
glish
English to
Hindi
English to
Tamil
English to
Kannada
Language pair code EnCh ChEn EnTh ThEn EnHi EnTa EnKa
Standard runs 14 5 2 2 2 2 2
Non-standard runs 0 0 0 0 0 0 0
English to
Japanese
Katakana
English
to Korean
Hangul
English to
Japanese
Kanji
Arabic to
English
English to
Bengali
(Bangla)
English to
Persian
English to
Hebrew
Language pair code EnJa EnKo JnJk ArEn EnBa EnPe EnHe
Standard runs 3 4 4 5 4 4 4
Non-standard runs 0 1 0 0 0 0 0
Table 2: Number of runs submitted for each task. Number of participants coincides with the number of
standard runs submitted.
Team
ID
Organisation EnCh ChEn EnTh ThEn EnHi EnTa EnKa EnJa EnKo JnJk ArEn EnBa EnPe EnHe
1 University of Alberta x
2 NICT x x x x x x x x x x x x x x
3 MIT@Lab of HIT x
4 IASL, Academia
Sinica
x
5 Yahoo Japan Corpora-
tion
x x x x x x x x
6 Yuan Ze University x
7 CMU x x x x x x x x x x x x x x
Table 3: Participation of teams in different tasks.
5 Task Results and Analysis
5.1 Standard runs
All the results are presented numerically in Ta-
bles 4?17, for all evaluation metrics. These are the
official evaluation results published for this edition
of the transliteration shared task.
The methodologies used in the ten submitted
system papers are summarized as follows. Similar
to their NEWS 2011 system, Finch et al (2012)
employ non-Parametric Bayesian method to co-
segment bilingual named entities for model train-
ing and report very good performance. This sys-
tem is based on phrase-based statistical machine
transliteration (SMT) (Finch and Sumita, 2008),
an approach initially developed for machine trans-
lation (Koehn et al, 2003), where the SMT sys-
tem?s log-linear model is augmented with a set of
features specifically suited to the task of translit-
eration. In particular, the model utilizes a fea-
ture based on a joint source-channel model, and
a feature based on a maximum entropy model that
predicts target grapheme sequences using the local
context of graphemes and grapheme sequences in
both source and target languages. Different from
their NEWS 2011 system, in order to solve the
data sparseness issue, they use two RNN-based
LM to project the grapheme set onto a smaller hid-
den representation: one for the target grapheme se-
quence and the other for the sequence of grapheme
sequence pair used to generate the target.
Zhang et al (2012) also use the statistical
phrase-based SMT framework. They propose the
fine-grained English segmentation algorithm and
other new features and achieve very good perfor-
mance. Wu et al (2012) uses m2m-aligner and
DirecTL-p decoder and two re-ranking methods:
co-occurrence at web corpus and JLIS-Reranking
method based on the features from alignment re-
sults. They report very good performance at
English-Korean tasks. Okuno (2012) studies the
mpaligner (an improvement of m2m-aligner) and14
shows that mpaligner is more effective than m2m-
aligner. They also find that de-romanization is cru-
cial to JnJk task and mora is the best alignment
unit for EnJa task. Ammar et al (2012) use CRF
as the basic model but with two innovations: a
training objective that optimizes toward any of a
set of possible correct labels (i.e., multiple refer-
ences) and a k-best reranking with non-local fea-
tures. Their results on ArEn show that the two
features are very effective in accuracy improve-
ment. Kondrak et al (2012) study the language-
specific adaptations in the context of two language
pairs: English to Chinese (Pinyin representation)
and Arabic to English (letter mapping). They con-
clude that Pinyin representation is useful while let-
ter mapping is less effective. Kuo et al (2012) ex-
plore two-stage CRF for Enligsh-to-Chinese task
and show that the two-stage CRF outperform tra-
ditional one-stage CRF.
5.2 Non-standard runs
For the non-standard runs, we pose no restrictions
on the use of data or other linguistic resources.
The purpose of non-standard runs is to see how
best personal name transliteration can be, for a
given language pair. In NEWS 2012, only one
non-standard run (Wu et al, 2012) was submitted.
Their reported web-based re-validation method is
very effective.
6 Conclusions and Future Plans
The Machine Transliteration Shared Task in
NEWS 2012 shows that the community has a con-
tinued interest in this area. This report summa-
rizes the results of the shared task. Again, we
are pleased to report a comprehensive calibra-
tion and baselining of machine transliteration ap-
proaches as most state-of-the-art machine translit-
eration techniques are represented in the shared
task.
In addition to the most popular techniques such
as Phrase-Based Machine Transliteration (Koehn
et al, 2003), CRF, re-ranking, DirecTL-p de-
coder, Non-Parametric Bayesian Co-segmentation
(Finch et al, 2011), and Multi-to-Multi Joint
Source Channel Model (Chen et al, 2011) in the
NEWS 2011, we are delighted to see that sev-
eral new techniques have been proposed and ex-
plored with promising results reported, including
RNN-based LM (Finch et al, 2012), English Seg-
mentation algorithm (Zhang et al, 2012), JLIS-
reranking method (Wu et al, 2012), improved
m2m-aligner (Okuno, 2012), multiple reference-
optimized CRF (Ammar et al, 2012), language
dependent adaptation (Kondrak et al, 2012) and
two-stage CRF (Kuo et al, 2012). As the stan-
dard runs are limited by the use of corpus, most of
the systems are implemented under the direct or-
thographic mapping (DOM) framework (Li et al,
2004). While the standard runs allow us to con-
duct meaningful comparison across different al-
gorithms, we recognise that the non-standard runs
open up more opportunities for exploiting a vari-
ety of additional linguistic corpora.
Encouraged by the success of the NEWS work-
shop series, we would like to continue this event
in the future conference to promote the machine
transliteration research and development.
Acknowledgements
The organisers of the NEWS 2012 Shared Task
would like to thank the Institute for Infocomm
Research (Singapore), Microsoft Research In-
dia, CJK Institute (Japan), National Electronics
and Computer Technology Center (Thailand) and
Sarvnaz Karim / RMIT for providing the corpora
and technical support. Without those, the Shared
Task would not be possible. We thank those par-
ticipants who identified errors in the data and sent
us the errata. We also want to thank the members
of programme committee for their invaluable com-
ments that improve the quality of the shared task
papers. Finally, we wish to thank all the partici-
pants for their active participation that have made
this first machine transliteration shared task a com-
prehensive one.
15
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proc.
ACL-2002Workshop: Computational Apporaches to
Semitic Languages, Philadelphia, PA, USA.
Waleed Ammar, Chris Dyer, and Noah Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In Proc. Named Entities
Workshop at ACL 2012.
Yu Chen, Rui Wang, and Yi Zhang. 2011. Statisti-
cal machine transliteration with multi-to-multi joint
source channel model. In Proc. Named Entities
Workshop at IJCNLP 2011.
CJKI. 2010. CJK Institute. http://www.cjk.org/.
D. Demner-Fushman and D. W. Oard. 2002. The ef-
fect of bilingual term list size on dictionary-based
cross-language information retrieval. In Proc. 36-th
Hawaii Int?l. Conf. System Sciences, volume 4, page
108.2.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proc. 3rd Int?l.
Joint Conf NLP, volume 1, Hyderabad, India, Jan-
uary.
Andrew Finch, Paul Dixon, and Eiichiro Sumita. 2011.
Integrating models derived from non-parametric
bayesian co-segmentation into a statistical machine
transliteration system. In Proc. Named Entities
Workshop at IJCNLP 2011.
Andrew Finch, Paul Dixon, and Eiichiro Sumita. 2012.
Rescoring a phrase-based machine transliteration
systemwith recurrent neural network language mod-
els. In Proc. Named Entities Workshop at ACL 2012.
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004.
Phoneme-based transliteration of foreign names for
OOV problem. In Proc. IJCNLP, pages 374?381,
Sanya, Hainan, China.
Yoav Goldberg andMichael Elhadad. 2008. Identifica-
tion of transliterated foreign words in Hebrew script.
In Proc. CICLing, volume LNCS 4919, pages 466?
477.
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. In Proc. EMNLP,
pages 353?362.
Jack Halpern. 2007. The challenges and pitfalls
of Arabic romanization and arabization. In Proc.
Workshop on Comp. Approaches to Arabic Script-
based Lang.
Ulf Hermjakob, Kevin Knight, and Hal Daume?. 2008.
Name translation in statistical machine translation:
Learning when to transliterate. In Proc. ACL,
Columbus, OH, USA, June.
Byung-Ju Kang and Key-Sun Choi. 2000.
English-Korean automatic transliteration/back-
transliteration system and character alignment. In
Proc. ACL, pages 17?18, Hong Kong.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Proc.
21st Int?l Conf Computational Linguistics and 44th
Annual Meeting of ACL, pages 817?824, Sydney,
Australia, July.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL.
Grzegorz Kondrak, Xingkai Li, and Mohammad
Salameh. 2012. Transliteration experiments on chi-
nese and arabic. In Proc. Named Entities Workshop
at ACL 2012.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721?722.
Chan-Hung Kuo, Shih-Hung Liu, Mike Tian-Jian
Jiang, Cheng-Wei Lee, and Wen-Lian Hsu. 2012.
Cost-benefit analysis of two-stage conditional
random fields based english-to-chinese machine
transliteration. In Proc. Named Entities Workshop
at ACL 2012.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159?166,
Barcelona, Spain.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009a. Report of NEWS 2009 machine
transliteration shared task. In Proc. Named Entities
Workshop at ACL 2009.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. ACL-IJCNLP 2009 Named
Entities Workshop ? Shared Task on Translitera-
tion. In Proc. Named Entities Workshop at ACL
2009.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2010a. Report of news 2010 translit-
eration generation shared task. In Proc. Named En-
tities Workshop at ACL 2010.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2010b. Whitepaper of news 2010
shared task on transliteration generation. In Proc.
Named Entities Workshop at ACL 2010.
T. Mandl and C. Womser-Hacker. 2005. The effect of
named entities on effectiveness in cross-language in-
formation retrieval evaluation. In Proc. ACM Symp.
Applied Comp., pages 1059?1064.16
Helen M. Meng, Wai-Kit Lo, Berlin Chen, and Karen
Tang. 2001. Generate phonetic cognates to han-
dle name entities in English-Chinese cross-language
spoken document retrieval. In Proc. ASRU.
MSRI. 2009. Microsoft Research India.
http://research.microsoft.com/india.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation
and contextual rules. In Proc. COLING 2002,
Taipei, Taiwan.
Yoh Okuno. 2012. Applying mpaligner to machine
transliteration with japanese-specific heuristics. In
Proc. Named Entities Workshop at ACL 2012.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proc. 45th Annual Meeting
of the ACL, pages 944?951, Prague, Czech Repub-
lic, June.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable cor-
pora. In Proc. 21st Int?l Conf Computational Lin-
guistics and 44th Annual Meeting of ACL, pages 73?
80, Sydney, Australia.
Raghavendra Udupa, K. Saravanan, Anton Bakalov,
and Abhijit Bhole. 2009. ?They are out there, if
you know where to look?: Mining transliterations
of OOV query terms for cross-language informa-
tion retrieval. In LNCS: Advances in Information
Retrieval, volume 5478, pages 437?448. Springer
Berlin / Heidelberg.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. ACL MLNER, Sapporo, Japan.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proc. COL-
ING, pages 1352?1356.
Chun-Kai Wu, Yu-Chun Wang, and Richard Tzong-
Han Tsai. 2012. English-korean named entity
transliteration using substring alignment and re-
ranking methods. In Proc. Named Entities Work-
shop at ACL 2012.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In Proc. EMNLP,
pages 612?617, Sydney, Australia, July.
Min Zhang, A Kumaran, and Haizhou Li. 2011a.
Whitepaper of news 2011 shared task on machine
transliteration. In Proc. Named Entities Workshop
at IJCNLP 2011.
Min Zhang, Haizhou Li, A Kumaran, and Ming Liu.
2011b. Report of news 2011 machine transliteration
shared task. In Proc. Named Entities Workshop at
IJCNLP 2011.
Chunyue Zhang, Tingting Li, and Tiejun Zhao. 2012.
Syllable-based machine transliteration with extra
phrase features. In Proc. Named Entities Workshop
at ACL 2012.
17
Team ID ACC F -score MRR MAPref Organisation
Primary runs
3 0.330357 0.66898 0.413062 0.320285 MIT@Lab of HIT
1 0.325397 0.67228 0.418079 0.316296 University of Alberta
2 0.310516 0.66585 0.44664 0.307788 NICT
4 0.310516 0.662467 0.37696 0.299266 IASL, Academia Sinica
5 0.300595 0.655091 0.376025 0.292252 Yahoo Japan Corporation
7 0.031746 0.430698 0.055574 0.030265 CMU
Non-primary standard runs
3 0.330357 0.676232 0.407755 0.3191 MIT@Lab of HIT
1 0.325397 0.673053 0.409452 0.316055 University of Alberta
1 0.324405 0.668165 0.424517 0.316248 University of Alberta
3 0.31746 0.666551 0.399476 0.308187 MIT@Lab of HIT
4 0.298611 0.658836 0.362263 0.288725 IASL, Academia Sinica
5 0.298611 0.656974 0.357481 0.289373 Yahoo Japan Corporation
4 0.294643 0.651988 0.357495 0.284274 IASL, Academia Sinica
4 0.290675 0.653565 0.370733 0.282545 IASL, Academia Sinica
Table 4: Runs submitted for English to Chinese task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.20314 0.736058 0.308801 0.199569 NICT
3 0.176644 0.701791 0.257324 0.172991 MIT@Lab of HIT
7 0.030422 0.489705 0.048211 0.03004 CMU
5 0.012758 0.258962 0.017354 0.012758 Yahoo Japan Corporation
Non-primary standard runs
5 0.007851 0.258013 0.012163 0.007851 Yahoo Japan Corporation
Table 5: Runs submitted for Chinese to English back-transliteration task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.122168 0.746824 0.183318 0.122168 NICT
7 0.000809 0.288585 0.001883 0.000809 CMU
Table 6: Runs submitted for English to Thai task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.139968 0.765534 0.21551 0.139968 NICT
7 0 0.417451 0.000566 0 CMU
Table 7: Runs submitted for Thai to English back-transliteration task.
18
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.668 0.923347 0.73795 0.661278 NICT
7 0.048 0.645666 0.087842 0.048528 CMU
Table 8: Runs submitted for English to Hindi task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.592 0.908444 0.67881 0.5915 NICT
7 0.052 0.638029 0.083728 0.052 CMU
Table 9: Runs submitted for English to Tamil task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.546 0.900557 0.640534 0.545361 NICT
7 0.116 0.737857 0.180234 0.11625 CMU
Table 10: Runs submitted for English to Kannada task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.400774 0.810109 0.522758 0.397386 NICT
5 0.362052 0.802701 0.468973 0.35939 Yahoo Japan Corporation
7 0 0.147441 0.00038 0 CMU
Table 11: Runs submitted for English to Japanese Katakana task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
6 0.398095 0.731212 0.398095 0.396905 Yuan Ze University
2 0.38381 0.721247 0.464553 0.383095 NICT
5 0.334286 0.687794 0.411264 0.334048 Yahoo Japan Corporation
7 0 0 0.00019 0 CMU
Non-standard runs
6 0.458095 0.756755 0.484048 0.458095 Yuan Ze University
Table 12: Runs submitted for English to Korean task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.513242 0.693184 0.598304 0.418708 NICT
5 0.512329 0.693029 0.581803 0.400505 Yahoo Japan Corporation
7 0 0 0 0 CMU
Non-primary standard runs
5 0.511416 0.691131 0.580485 0.402127 Yahoo Japan Corporation
Table 13: Runs submitted for English to Japanese Kanji back-transliteration task.19
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.588235 0.929787 0.709003 0.506991 NICT
7 0.58391 0.925292 0.694338 0.367162 CMU
1 0.583045 0.932959 0.670457 0.42041 University of Alberta
Non-primary standard runs
7 0.57699 0.93025 0.678898 0.330353 CMU
7 0.573529 0.925306 0.675125 0.328782 CMU
Table 14: Runs submitted for Arabic to English task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.46 0.891476 0.582944 0.458417 NICT
5 0.404 0.882395 0.514541 0.402917 Yahoo Japan Corporation
7 0.178 0.783893 0.248674 0.177139 CMU
Non-primary standard runs
5 0.398 0.880286 0.510148 0.396528 Yahoo Japan Corporation
Table 15: Runs submitted for English to Bengali (Bangla) task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.658349 0.940642 0.761223 0.639873 Yahoo Japan Corporation
2 0.65547 0.941044 0.773843 0.642663 NICT
7 0.18618 0.803002 0.311881 0.184961 CMU
Non-primary standard runs
5 0.054702 0.627335 0.082754 0.054367 Yahoo Japan Corporation
Table 16: Runs submitted for English to Persian task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.190909 0.808491 0.253575 0.19 Yahoo Japan Corporation
2 0.153636 0.787254 0.228649 0.152727 NICT
7 0.097273 0.759444 0.130955 0.096818 CMU
Non-primary standard runs
5 0.165455 0.803019 0.241948 0.164545 Yahoo Japan Corporation
Table 17: Runs submitted for English to Hebrew task.
20
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 1?9,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Meaning Unit Segmentation in English and Chinese: a New Approach toDiscourse PhenomenaJennifer Williams ?1,2, Rafael Banchs2, and Haizhou Li2
1Department of Linguistics, Georgetown University, Washington, D.C., USA
2Institute for Infocomm Research, 1 Fusionpolis Way, Singaporejaw97@georgetown.edu {rembanchs,hli}@i2r.a-star.edu.sgAbstract
We present a new approach to dialogue
processing in terms of ?meaning units?. In
our annotation task, we asked speakers of
English and Chinese to mark boundaries
where they could construct the maximal
concept using minimal words. We com-
pared English data across genres (news,
literature, and policy). We analyzed the
agreement for annotators using a state-of-
the-art segmentation similarity algorithm
and compared annotations with a random
baseline. We found that annotators are
able to identify meaning units systemati-
cally, even though they may disagree on
the quantity and position of units. Our
analysis includes an examination of phrase
structure for annotated units using con-
stituency parses.1 Introduction
When humans translate and interpret speech in
real-time, they naturally segment speech in ?min-
imal sense units? (Ole?ron & Nanpon, 1965;
Ben??tez & Bajo, 1998) in order to convey the
same information from one language to another as
though there were a 1-to-1 mapping of concepts
between both languages. Further, it is known that
people can hold up to 7+/- 2 ?chunks? of informa-
tion in memory at a time by creating and applying
meaningful organization schemes to input (Miller,
1956). However, there is no definitive linguistic
description for the kind of ?meaning units? that
human translators create (Signorelli et al, 2011;
Hamon et al, 2009; Mima et al, 1998).
The ability to chunk text according to units of
meaning is key to developing more sophisticated
machine translation (MT) systems that operate in
? Now affiliated with Massachusetts Institute of Tech-
nology Lincoln Laboratory.
real-time, as well as informing discourse process-
ing and natural language understanding (NLU)
(Kola?r?, 2008). We present an approach to dis-
course phenomena to address Keller?s (2010) call
to find a way to incorporate ?cognitive plausibil-
ity? into natural language processing (NLP) sys-
tems. As it has been observed that human trans-
lators and interpreters naturally identify a certain
kind of ?meaning unit? when translating speech
in real-time (Ole?ron & Nanpon, 1965; Ben??tez &
Bajo, 1998), we want to uncover the features of
those units in order to automatically identify them
in discourse.
This paper presents an experimental approach
to annotating meaning units using human anno-
tators from Mechanical Turk. Our goal was to
use the results of human judgments to inform
us if there are salient features of meaning units
in English and Chinese text. We predicted that
human-annotated meaning units should systemat-
ically correspond to some other linguistic features
or combinations of those features (i.e. syntax,
phrase boundaries, segments between stop words,
etc.). We are interested in the following research
questions:
? At what level of granularity do English and
Chinese speakers construct meaning units in
text?
? Do English and Chinese speakers organize
meaning units systematically such that mean-
ing unit segmentations are not random?
? How well do English and Chinese speakers
agree on meaning unit boundaries?
? Are there salient syntactic features of mean-
ing units in English and Chinese?
? Can we automatically identify a 1-to-1 map-
ping of concepts for parallel text, even if there
is paraphrasing in one or both languages?
1
While we have not built a chunker or classifier
for meaning unit detection, it is our aim that this
work will inform how to parse language system-
atically in a way that is human-understandable. It
remains to be seen that automatic tools can be de-
veloped to detect meaning units in discourse. Still,
we must be informed as to what kinds of chunks
are appropriate for humans to allow them to under-
stand information transmitted during translation
(Kola?r?, 2008). Knowledge about meaning units
could be important for real-time speech process-
ing, where it is not always obvious where an ut-
terance begins and ends, due to any combination
of natural pauses, disfluencies and fillers such as
?like, um..?. We believe this work is a step towards
creating ultra-fast human-understandable simulta-
neous translation systems that can be used for con-
versations in different languages.
This paper is organized as follows: Section 2
discusses related work, Section 3 describes the
segmentation similarity metric that we used for
measuring annotator agreement, Section 4 de-
scribes our experiment design, Section 5 shows
experiment results, Section 6 provides analysis,
and Section 7 discusses future work.2 Related Work
At the current state of the art, automatic simultane-
ous interpretation systems for speech function too
slowly to allow people to conduct normal-paced
conversations in different languages. This prob-
lem is compounded by the difficulty of identifying
meaningful endpoints of utterances before trans-
mitting a translation. For example, there is a per-
ceived lag time for speakers when trying to book
flights or order products over the phone. This lag
time diminishes conversation quality since it takes
too long for each speaker to receive a translation
at either end of the system (Paulik et al, 2009). If
we can develop a method to automatically identify
segments of meaning as they are spoken, then we
could significantly reduce the perceived lag time
in real-time speech-to-speech translation systems
and improve conversation quality (Baobao et al,
2002; Hamon et al, 2009).
The problem of absence of correspondence
arises when there is a lexical unit (single words
or groups of words) that occurs in L1 but not
in L2 (Lambert et al, 2005). It happens when
words belonging to a concept do not correspond to
phrases that can be aligned in both languages. This
problem is most seen when translating speech-to-
speech in real-time. One way to solve this prob-
lem is to identify units for translation that cor-
respond to concepts. A kind of meaning unit
had been previously proposed as information units
(IU), which would need to be richer than seman-
tic roles and also be able to adjust when a mis-
take or assumption is realized (Mima et al, 1998).
These units could be used to reduce the explosion
of unresolved structural ambiguity which happens
when ambiguity is inherited by a higher level syn-
tactic structure, similar to the use of constituent
boundaries for transfer-driven machine translation
(TDMT) (Furuse et al, 1996).
The human ability to construct concepts in-
volves both bottom-up and top-down strategies in
the brain. These two kinds of processes inter-
act and form the basis of comprehension (Kintsch,
2005). The construction-integration model (CI-2)
describes how meaning is constructed from both
long-term memory and short-term memory. One
of the challenges of modeling meaning is that it
requires a kind of world-knowledge or situationalknowledge, in addition to knowing the meanings
of individual words and knowing how words can
be combined. Meaning is therefore constructed
from long-term memory ? as can be modeled by
latent semantic analysis (LSA) ? but also from
short-term memory which people use in the mo-ment (Kintsch & Mangalath, 2011). In our work,
we are asking annotators to construct meaning
from well-formed text and annotate where units of
meaning begin and end.3 Similarity Agreement
We implemented segmentation similarity (S) from
Fournier and Inkpen (2012). Segmentation sim-
ilarity was formulated to address some gaps of
the WindowDiff (WD) metric, including unequal
penalty for errors as well as the need to add
padding to the ends of each segmentation (Pevzner
& Hearst, 2002). There are 3 types of segmenta-
tion errors for (S), listed below:
1. s1 contains a boundary that is off by n poten-
tial boundaries in s2
2. s1 contains a boundary that s2 does not, or
3. s2 contains a boundary that s1 does not
These three types of errors are understood astranspositions in the case of error type 1, and as
2
substitutions in the case of error types 2 and 3.
Note that there is no distinction between insertions
and deletions because neither of the segmentations
are considered reference or hypothesis. We show
the specification of (S) in (1):
S(si1,si2) =
t ? mass(i)  t  d(si1,si2,T )
t ? mass(i)  t
(1)
such that S scales the cardinality of the set of
boundary types t because the edit distance func-
tion d(si1,si2,T ) will return a value for potential
boundaries of [0, t ? mass(i)] normalized by the
number of potential boundaries per boundary type.
The value of mass(i) depends on task, in our
work we treat mass units as number of words, for
English, and number of characters for Chinese.
Since our annotators were marking only units of
meaning, there was only one boundary type, and
(t = 1). The distance function d(si1,si2,T ) is the
edit distance between segments calculated as the
number of boundaries involved in transposition
operations subtracted from the number of substi-
tution operations that could occur. A score of 1.0
indicates full agreement whereas a score of 0 indi-
cates no agreement.
In their analysis and comparison of this new
metric, Fournier and Inkpen (2012) demonstrated
the advantages of using (S) over using (WD)
for different kinds of segmentation cases such
as maximal/minimal segmentation, full misses,
near misses, and segmentation mass scale effects.
They found that in each of these cases (S) was
more stable than (WD) over a range of segment
sizes. That is, when considering different kinds
of misses (false-positive, false-negative, and both),
the metric (S) is less variable to internal segment
size. These are all indications that (S) is a more
reliable metric than (WD).
Further, (S) properly takes into account chance
agreement - called coder bias - which arises in
segmentation tasks when human annotators either
decide not to place a boundary at all, or are un-
sure if a boundary should be placed. Fournier and
Inkpen (2012) showed that metrics that follow (S)
specification reflect most accurately on coder bias,
when compared to mean pairwise 1   WD met-
rics. Therefore we have decided to use segmenta-
tion similarity as a metric for annotator agreement.
4 Experiment Design
This section describes how we administered our
experiment as an annotation task. We surveyed
participants using Mechanical Turk and presented
participants with either English or Chinese text.
While the ultimate goal of this research direc-
tion is to obtain meaning unit annotations for
speech, or transcribed speech, we have used well-
structured text in our experiment in order to find
out more about the potential features of meaning
units in the simplest case.4.1 Sample Text PreparationGenre: Our text data was selected from three dif-
ferent genres for English (news, literature, and
policy) and one genre for Chinese (policy). We
used 10 articles from the Universal Declaration of
Human Rights (UDHR) in parallel for English and
Chinese. The English news data (NEWS) con-
sisted of 10 paragraphs that were selected online
from www.cnn.com and reflected current events
from within the United States. The English liter-
ature data (LIT) consisted of 10 paragraphs from
the novel Tom Sawyer by Mark Twain. The En-
glish and Chinese UDHR data consisted of 12 par-
allel paragraphs from the Universal Declaration of
Human Rights. The number of words and number
of sentences by language and genre is presented
below in Table 1.Preprocessing: To prepare the text samples for
annotation, we did some preprocessing. We re-
moved periods and commas in both languages,
since these markings can give structure and mean-
ing to the text which could influence annotator de-
cisions about meaning unit boundaries. For the
English data, we did not fold to lowercase and we
acknowledge that this was a design oversight. The
Chinese text was automatically segmented into
words before the task using ICTCLAS (Zhang et
al., 2003). This was done in order to encourage
Chinese speakers to look beyond the character-
level and word-level, since word segmentation is
a well-known NLP task for the Chinese language.
The Chinese UDHR data consisted of 856 charac-
ters. We placed checkboxes between each word in
the text.4.2 Mechanical Turk Annotation
We employed annotators using Amazon Mechan-
ical Turk Human Intelligence Tasks (HITs). All
instructions for the task were presented in En-
3
Language and Genre # words # Sentences
Chinese UDHR 485 20
English NEWS 580 20
English LIT 542 27
English UDHR 586 20
Table 1: Number of words and sentences by lan-
guage and genre.
glish. Each participant was presented with a set of
numbered paragraphs with a check-box between
each word where a boundary could possibly ex-
ist. In the instructions, participants were asked
to check the boxes between words correspond-
ing to the boundaries of meaning units. They
were instructed to create units of meaning larger
than words but that are also the ?maximal concept
that you can construct that has the minimal set of
words that can be related to each individual con-
cept?1. We did not provide marked examples to
the annotators so as to avoid influencing their an-
notation decisions.
Participants were given a maximum of 40 min-
utes to complete the survey and were paid USD
$1.00 for their participation. As per Amazon
Mechanical Turk policy, each of the participants
were at least 18 years of age. The annotation
task was restricted to one task per participant, in
other words if a participant completed the English
NEWS annotation task then they could not partic-
ipate in the Chinese UDHR task, etc. We did not
test any of the annotators for language aptitude
or ability, and we did not survey language back-
ground. It is possible that for some annotators,
English and Chinese were not a native language.5 Results
We omitted survey responses for which partici-
pants marked less than 30 boundaries total, as well
as participants who completed the task in less than
5 minutes. We did this in an effort to eliminate
annotator responses that might have involved ran-
dom marking of the checkboxes, as well as those
who marked only one or two checkboxes. We de-
cided it would be implausible that less than 30
boundaries could be constructed, or that the task
1The definition of ?meaning units? we provide is very am-
biguous and can justify for different people understanding the
task differently. However, this is part of what we wanted to
measure, as giving a more precise and operational definition
would bias people to some specific segmentation criteria.
could be completed in less than 5 minutes, con-
sidering that there were several paragraphs and
sentences for each dataset. After we removed
those responses, we had solicited 47 participants
for English NEWS, 40 participants for English
LIT, 59 participants for English UDHR, and 10
participants for Chinese UDHR. The authors ac-
knowledge that the limited sample size for Chi-
nese UDHR data does not allow a direct compar-
ison across the two languages, however we have
included it in results and analysis as supplemental
findings and encourage future work on this task
across multiple languages. We are unsure as to
why there was a low number of Chinese annota-
tors in this task, except perhaps the task was not as
accessible to native Chinese speakers because the
task instructions were presented in English.5.1 Distributions by Genre
We show distributions of number of annotators
and number of units identified for each language
and genre in Figures 1 ? 4. For each of the
language/genres, we removed one annotator be-
cause the number of units that they found was
greater than 250, which we considered to be
an outlier in our data. We used the Shapiro-
Wilk Test for normality to determine which, if
any, of these distributions were normally dis-
tributed. We failed to reject the null hypothesis for
Chinese UDHR (p = 0.373) and English NEWS
(p = 0.118), and we rejected the null hypothe-
sis for English LIT (p = 1.8X10 04) and English
UDHR (p = 1.39X10 05).Dataset N Avg AvgUnits Words/Unit
Chinese UDHR 9 70.1 ?
English NEWS 46 84.9 6.8
English LIT 39 85.4 6.3
English LIT G1 26 66.9 8.1
English LIT G2 13 129.0 4.2
English UDHR 58 90.1 6.5
English UDHR G1 17 52.2 11.2
English UDHR G2 19 77.3 7.6
English UDHR G3 22 132.2 4.4
Table 2: Number of annotators (N), average num-
ber of units identified, average number of words
per unit identified, by language and genre.
Since the number of units were not normally
distributed for English LIT and English UDHR,
4
Figure 1: Distribution of total number of annota-
tions per annotator for Chinese UDHR.
Figure 2: Distribution of total number of annota-
tions per annotator for English UDHR.
Figure 3: Distribution of total number of annota-
tions per annotator for English NEWS.
we used 2-sample Kolmogorov-Smirnov (KS)
Tests to identify separate distributions for each of
these genres. We found 3 distinct groups in En-
glish UDHR (G1?G3) and 2 distinct groups in En-
glish LIT (G1 and G2). Table 2 provides more
Figure 4: Distribution of total number of annota-
tions per annotator for English LIT.
detailed information about distributions for num-
ber of annotations, as well as the average number
of units found, and average words per unit. This
information informs us as to how large or small
on average the meaning units are. Note that in Ta-
ble 2 we include information for overall English
UDHR and overall English LIT distributions for
reference. The authors found it interesting that,
from Table 2, the number of words per meaning
unit generally followed the 7 +/- 2 ?chunks? phe-
nomenon, where chunks are words.5.2 Annotator Agreement
Even though some of the annotators agreed about
the number of units, that does not imply that
they agreed on where the boundaries were placed.
We used segmentation similarity (S) as a metric
for annotator agreement. The algorithm requires
specifying a unit of measurement between bound-
aries ? in our case we used word-level units for
English data and character-level units for Chinese
data. We calculated average similarity agreement
for segment boundaries pair-wise within-group
for annotators from each of the 9 language/genre
datasets, as presented in Table 3.
While the segmentation similarity agreements
seem to indicate high annotator agreement, we
wanted to find out if that agreement was bet-
ter than what we could generate at random, so
we compared annotator agreement with random
baselines. To generate the baselines, we used
the average number of segments per paragraph in
each language/genre dataset and inserted bound-
aries at random. For each of the 9 language/genre
datasets, we generated 30 baseline samples. We
calculated the baseline segmentation similarity
5
Dataset (S) (SBL)
Chinese UDHR 0.930 0.848
English NEWS 0.891 0.796
English LIT 0.875 0.790
English LIT G1 0.929 0.824
English LIT G2 0.799 0.727
English UDHR 0.870 0.802
English UDHR G1 0.929 0.848
English UDHR G2 0.910 0.836
English UDHR G3 0.826 0.742
Table 3: Within-group segmentation similarity
agreement (S) and segmentation similarity agree-
ment for random baseline (SBL).
(SBL) in the same way using average pair-wise
agreement within-group for all of the baseline
datasets, shown in Table 3.
For English UDHR, we also calculated average
pair-wise agreement across groups, shown in Ta-
ble 4. For example, we compared English UDHR
G1 with English UDHR G2, etc. Human annota-
tors consistently outperformed the baseline across
groups for English UDHR.Dataset (S) (SBL)
English UDHR G1?G2 0.916 0.847
English UDHR G1?G3 0.853 0.782
English UDHR G2?G3 0.857 0.778
Table 4: English UDHR across-group segmenta-
tion similarity agreement (S) and random baseline
(SBL).6 Analysis
Constructing concepts in this task is systematic
as was shown from the segmentation similarity
scores. Since we know that the annotators agreed
on some things, it is important to find out what
they have agreed on. In our analysis, we exam-
ined unit boundary locations across genres in addi-
tion to phrase structure using constituency parses.
In this section, we begin to address another of
our original research questions regarding how well
speakers agree on meaning unit boundary posi-
tions across genres and which syntactic features
are the most salient for meaning units.
6.1 Unit Boundary Positions for Genres
Boundary positions are interesting because they
can potentially indicate if there are salient parts
of the texts which stand out to annotators across
genres. We have focused this analysis across gen-
res for the overall data for each of the 4 lan-
guage/genre pairs. Therefore, we have omitted the
subgroups ? English UDHR groups (G1,G2, G3)
and English LIT groups (G1, G2). Although seg-
mentation similarity is greater within-group from
Table 3, this was not enough to inform us of which
boundaries annotators fully agree on. For each of
the datasets, we counted the number of annotators
who agreed on a given boundary location and plot-
ted histograms. In these plots we show the number
of annotators of each potential boundary between
words. We show the resulting distributions in Fig-
ures 5 ? 8.
Figure 5: Annotated boundary positions Chinese
UDHR.
Figure 6: Annotated boundary positions English
UDHR.
While there were not many annotators for the
Chinese UDHR data, we can see from Figure 5
6
Figure 7: Annotated boundary positions English
NEWS.
Figure 8: Annotated boundary positions English
LIT.
that at most 4 annotators agreed on boundary po-
sitions. We can see from Figures 6 ? 8 that there
is high frequency of agreement in the text which
corresponds to paragraph boundaries for the En-
glish data, however paragraph boundaries were ar-
tificially introduced into the experiment because
each paragraph was numbered.
Since we had removed all punctuation mark-
ings, including periods and commas for both lan-
guages, it is interesting to note there was not full
agreement about sentence boundaries. While we
did not ask annotators to mark sentence bound-
aries, we hoped that these would be picked up by
the annotators when they were constructing mean-
ing units in the text. Only 3 sentence boundaries
were identified by at most 2 Chinese UDHR an-
notators. On the other hand, all of the sentence
boundaries were idenfied for English UDHR and
English NEWS, and one sentence boundary was
unmarked for English LIT. However, there were
no sentence boundaries in the English data that
were marked by all annotators - in fact the sin-
gle most heavily annotated sentence boundary was
for English NEWS, where 30% of the annota-
tors marked it. The lack for identifying sentence
boundaries could be due to an oversight by anno-
tators, or it could also be indicative of the difficulty
and ambiguity of the task.6.2 Phrase Structure
To answer our question of whether or not there are
salient syntactic features for meaning units, we did
some analysis with constituency phrase structure
and looked at the maximal projections of meaning
units. For each of the 3 English genres (UDHR,
NEWS, and LIT) we identified boundaries where
at least 50% of the annotators agreed. For the Chi-
nese UDHR data, we identified boundaries where
at least 30% of annotators agreed. We used the
Stanford PCFG Parser on the original English and
Chinese text to obtain constituency parses (Klein
& Manning, 2003), then aligned the agreeable
segment boundaries with the constituency parses.
We found the maximal projection corresponding
to each annotated unit and we calculated the fre-
quency of each of the maximal projections. The
frequencies of part-of-speech for maximal projec-
tions are shown in Tables 5 - 8. Note that the part-
of-speech tags reflected here come from the Stan-
ford PCFG Parser.Max. Projection Description Freq.S, SBAR, SINV Clause 28PP Prepositional Phrase 14VP Verb Phrase 11NP Noun Phrase 5ADJP Adjective Phrase 3ADVP Adverb Phrase 1
Table 5: Frequency of maximal projections for En-
glish UDHR, for 62 boundaries.Max. Projection Description Freq.S, SBAR, SINV Clause 30VP Verb Phrase 23NP Noun Phrase 11PP Prepositional Phrase 3ADVP Adverb Phrase 2
Table 6: Frequency of maximal projections for En-
glish NEWS, for 69 boundaries.
7
Max. Projection Description Freq.S, SBAR Clause 32VP Verb Phrase 10NP Noun Phrase 3PP Prepositional Phrase 2ADVP Adverb Phrase 2
Table 7: Frequency of maximal projections for En-
glish LIT, for 49 boundaries.Max. Projection Description Freq.NN, NR Noun 22VP Verb Phrase 8NP Noun Phrase 8CD Determiner 3ADVP Adverb Phrase 1AD Adverb 1VV Verb 1JJ Other noun mod. 1DP Determiner Phrase 1
Table 8: Frequency of maximal projections for
Chinese UDHR, for 46 boundaries.
Clauses were by far the most salient bound-
aries for annotators of English. On the other hand,
nouns, noun phrases, and verb phrases were the
most frequent for annotators of Chinese. There
is some variation across genres for English. This
analysis begins to address whether or not it is
possible to identify syntactic features of meaning
units, however it leaves open another question as
to if it is possible to automatically identify a 1-to-1
mapping of concepts across languages.7 Discussion and Future Work
We have presented an experimental framework
for examining how English and Chinese speakers
make meaning out of text by asking them to la-
bel places that they could construct concepts with
as few words as possible. Our results show that
there is not a unique ?meaning unit? segmentation
criteria among annotators. However, there seems
to be some preferential trends on how to perform
this task, which suggest that any random segmen-
tation is not acceptable. As we have simplified the
task of meaning unit identification by using well-
structured text from the Universal Declaration of
Human Rights, news, and literature, future work
should examine identifying meaning units in tran-
scribed speech.
Annotators for the English UDHR and English
LIT datasets could be characterized by their dif-
ferent granularities of annotation in terms of num-
ber of units identified. These observations are in-
sightful to our first question: what granularity do
people use to construct meaning units? For some,
meaning units consist of just a few words, whereas
for others they consist of longer phrases or possi-
bly clauses. As we did not have enough responses
for the Chinese UDHR data, we are unable to com-
ment if identification of meaning units in Chinese
fit a similar distribution as with English and we
leave in-depth cross-language analysis to future
work.
A particularly interesting finding was that hu-
man annotators share agreement even across
groups, as seen from Table 4. This means that al-
though annotators may not agree on the number of
meaning units found, they do share some agree-
ment regarding where in the text they are creating
the meaning units. These findings seem to indicate
that annotators are creating meaning units system-
atically regardless of granularity.
Our findings suggest that different people orga-
nize and process information differently. This is a
very important conclusion for discourse analysis,
machine translation and many other applications
as this suggests that there is no optimal solution
to the segmentation problems considered in these
tasks. Future research should focus on better un-
derstanding the trends we identified and the ob-
served differences among different genres. While
we did not solicit feedback from annotators in this
experiment, we believe that it will be important
to do so in future work to improve the annota-
tion task. We know that the perceived lag time in
speech-to-speech translation cannot be completely
eliminated but we are interested in systems that are
?fast? enough for humans to have quality conver-
sations in different languages.Acknowledgments
This work was partly supported by Singapore
Agency for Science, Technology and Research
(A-STAR) and the Singapore International Pre-
Graduate Award (SIPGA) and was partly sup-
ported by the National Science Foundation (NSF)
award IIS-1225629. Any opinions expressed in
this material are those of the authors and do not
necessarily reflect the views of A-STAR and NSF.
8
References
Chang Baobao, Pernilla Danielsson, and Wolfgang
Teubert. 2002. Extraction of translation units from
Chinese-English parallel corpora. In Proceedingsof the first SIGHAN workshop on Chinese languageprocessing - Volume 18 (SIGHAN ?02), 1?5.
Presentacio?n Padilla Ben??tez and Teresa Bajo. 1998.
Hacia un modelo de memoria y atencio?n en inter-
pretacio?n simulta?nea. Quaderns. Revista de tra-duccio?, 2:107?117.
Chris Fournier and Diana Inkpen. 2012. Segmenta-
tion and similarity agreement. In Proceedings ofthe 2012 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies (NAACL HLT ?12),
Montreal, Canada, 152?161.
Osamu Furuse and Hitashi Iida. 1996. Incremental
translation utilizing constituent boundary patterns.
In Proceedings of the 16th conference on Computa-tional linguistics (COLING ?96), Copenhagen, Den-
mark, 412?417.
Olivier Hamon, Christian Fgen, Djamel Mostefa, Vic-
toria Arranz1, Munstin Kolss, Alex Waibel, and
Khalid Choukri. 2009. End-to-End Evaluation in
Simultaneous Translation. In Proceedings of the12th Conference of the European Chapter of theAssociation for Computational Linguistics, (EACL?09), Athens, Greece, 345?353.
Daniel Jurafsky. 1988. Issues in relating syntax and
semantics. In Proceedings of the 12th Internationalconference on Computational Linguistics (COLING?88), Budapest, Hungary, 278?284.
Frank Keller. 2010. Cognitively plausible models of
human language processing. In Proceedings of theACL 2010 Conference Short Papers, Uppsala, Swe-
den, 60?67.
Walter Kintsch. 2005. An Overview of Top-down and
Bottom?up Effects in Comprehension: The CI Per-
spective. Discourse Processes, 39(2&3):125?128.
Walter Kintsch and Praful Mangalath. 2011. The Con-
struction of Meaning. Topics in Cognitive Science,
3:346?370.
Dan Klein and Christopher D. Manning 2003. Ac-
curate Unlexicalized Parsing. In Proceedings of the41st Meeting of the Association for ComputationalLinguistics, 423?430.
Ja?chym Kola?r?. 2008. Automatic Segmentation ofSpeech into Sentence-like Units. Ph.D. thesis, Uni-
versity of West Bohemia, Pilsen, Czech Republic.
Patrik Lambert, Adria`. De Gispert, Rafael Banchs, and
Jose? B. Marin?o. 2005. Guidelines for Word Align-
ment Evaluation and Manual Alignment. LanguageResources and Evaluation (LREC), 39:267?285.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic MT evaluation. InProceedings of the Seventh Workshop on StatisticalMachine Translation (WMT ?12), Montreal, Canada
243?252.
George A. Miller. 1956. The Magical Number Seven,
Plus or Minus Two: Some Limits on Our Capacity of
Processing Information. The Psychological Review,
Vol 63:81?97.
Hideki Mima, Hitoshi Iida, and Osamu Furuse. 1998.
Simultaneous interpretation utilizing example-based
incremental transfer. In Proceedings of the 17th In-ternational Conference on Computational Linguis-tics (COLING ?98) Montreal, Quebec, Canada, 855?
861.
Pierre Ole?ron and Hubert Nanpon. 1965. Recherches
sur la traduction simultane?e. Journal de PsychologieNormale et Pathologique, 62(1):73?94.
Mathais Paulik and Alex Waibel. 2009. Automatic
Translation from Parallel Speech: Simultaneous In-
terpretation as MT Training Data. IEEE Workshopon Automatic Speech Recognition and Understand-ing, Merano, Italy, 496?501.
Lev Pevzner and Marti A. Hearst 2002. A critique and
improvement of an evaluation metric for text seg-
mentation. Computational Linguistics, 28(1):1936.
MIT Press, Cambridge, MA, USA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
H. Mar- tin, and Dan Jurafsky. 2004. Shallow Se-
mantic Parsing Using Support Vector Machines. InProceedings of the 2004 Conference on Human Lan-guage Technology and the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL-04).
Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar.
2010. Incremental Decoding for Phrase-based Sta-
tistical Machine Translation. In Proceedings of theJoint 5th Workshop on Statistical Machine Transla-tion and Metrics (MATR), Uppsala, Sweden, 222?
229.
Teresa M. Signorelli, Henk J. Haarmann, and Loraine
K. Obler. 2011. Working memory in simultaneous
interpreters: Effects of task and age. InternationalJournal of Billingualism, 16(2): 192?212.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, Qun
Liu. 2003. HHMM-based Chinese Lexical An-
alyzer ICTCLAS. In Proceedings of the SecondSIGHAN Workshop on Chinese Language Process-ing (SIGHAN ?03) - Volume 17, Sapporo, Japan,
184-187.
9
