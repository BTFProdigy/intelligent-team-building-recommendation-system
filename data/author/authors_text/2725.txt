Proceedings of NAACL HLT 2009: Short Papers, pages 241?244,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Sentence Boundary Detection and the Problem with the U.S.
Dan Gillick
Computer Science Division
University of California, Berkeley
dgillick@cs.berkeley.edu
Abstract
Sentence Boundary Detection is widely used
but often with outdated tools. We discuss what
makes it difficult, which features are relevant,
and present a fully statistical system, now pub-
licly available, that gives the best known er-
ror rate on a standard news corpus: Of some
27,000 examples, our system makes 67 errors,
23 involving the word ?U.S.?
1 Introduction
Many natural language processing tasks begin by
identifying sentences, but due to the semantic am-
biguity of the period, the sentence boundary detec-
tion (SBD) problem is non-trivial. While reported
error rates are low, significant improvement is pos-
sible and potentially valuable. For example, since
a single error can ruin an automatically generated
summary, reducing the error rate from 1% to 0.25%
reduces the rate of damaged 10-sentence summaries
from 1 in 10 to 1 in 40. Better SBD may improve
language models and sentence alignment as well.
SBD has been addressed only a few times in the
literature, and each result points to the importance of
developing lists of common abbreviations and sen-
tence starters. Further, most practical implementa-
tions are not readily available (with one notable ex-
ception). Here, we present a fully statistical system
that we argue benefits from avoiding manually con-
structed or tuned lists. We provide a detailed anal-
ysis of features, training variations, and errors, all
of which are under-explicated in the literature, and
discuss the possibility of a more structured classifi-
cation approach. Our implementation gives the best
performance, to our knowledge, reported on a stan-
dard Wall Street Journal task; it is open-source and
available to the public.
2 Previous Work
We briefly outline the most important existing meth-
ods and cite error rates on a standard English data
set, sections 03-06 of the Wall Street Journal (WSJ)
corpus (Marcus et al, 1993), containing nearly
27,000 examples. Error rates are computed as
(number incorrect/total ambiguous periods). Am-
biguous periods are assumed to be those followed
by white space or punctuation. Guessing the major-
ity class gives a 26% baseline error rate.
A variety of systems use lists of hand-crafted reg-
ular expressions and abbreviations, notably Alem-
bic (Aberdeen et al, 1995), which gives a 0.9% er-
ror rate. Such systems are highly specialized to lan-
guage and genre.
The Satz system (Palmer and Hearst, 1997)
achieves a 1.0% error rate using part-of-speech
(POS) features as input to a neural net classifier (a
decision tree gives similar results), trained on held-
out WSJ data. Features were generated using a
5000-word lexicon and a list of 206 abbreviations.
Another statistical system, mxTerminator (Reynar
and Ratnaparkhi, 1997) employs simpler lexical fea-
tures of the words to the left and right of the can-
didate period. Using a maximum entropy classifier
trained on nearly 1 million words of additional WSJ
data, they report a 1.2% error rate with an automati-
cally generated abbreviation list and special corpus-
specific abbreviation features.
There are two notable unsupervised systems.
Punkt (Kiss and Strunk, 2006) uses a set of log-
likelihood-based heuristics to infer abbreviations
and common sentence starters from a large text
corpus. Deriving these lists from the WSJ test
data gives an error rate of 1.65%. Punkt is eas-
ily adaptable but requires a large (unlabeled) in-
domain corpus for assembling statistics. An imple-
mentation is bundled with NLTK (Loper and Bird,
2002). (Mikheev, 2002) describes a ?document-
241
centered? approach to SBD, using a set of heuris-
tics to guess which words correspond to abbrevia-
tions and names. Adding carefully tuned lists from
an extra news corpus gives an error rate of 0.45%,
though this increases to 1.41% without the abbrevi-
ation list. Combining with a supervised POS-based
system gives the best reported error rate on this task:
0.31%.
Our system is closest in spirit to mxTerminator,
and we use the same training and test data in our
experiments to aid comparison.
3 Our Approach
Each example takes the general form ?L. R?, where
L is the context on the left side of the period in
question, and R is the context on the right (we use
only one word token of context on each side). We
are interested in the probability of the binary sen-
tence boundary class s, conditional on its context:
P (s|?L. R?). We take a supervised learning ap-
proach, extracting features from ?L. R?.
Table 1 lists our features and their performance,
using a Support Vector Machine (SVM) with a lin-
ear kernel1. Feature 1 by itself, the token ending
with the candidate period, gives surprisingly good
performance, and the combination of 1 and 2 out-
performs nearly all documented systems. While no
published result uses an SVM, we note that a simple
Naive Bayes classifier gives an error rate of 1.05%
(also considerably better than mxTerminator), sug-
gesting that the choice of classifier alone does not
explain the performance gap.
There are a few possible explanations. First,
proper tokenization is key. While there is not room
to catalog our tokenizer rules, we note that both un-
tokenized text and mismatched train-test tokeniza-
tion can increase the error rate by a factor of 2.
Second, poor feature choices can hurt classifica-
tion. In particular, adding a feature that matches a
list of abbreviations can increase the error rate; us-
ing the list (?Mr.?, ?Co.?) increases the number of
errors by up to 25% in our experiments. This is be-
cause some abbreviations end sentences often, and
others do not. In the test data, 0 of 1866 instances
of ?Mr.? end a sentence, compared to 24 of 86 in-
stances of ?Calif.? (see Table 2). While there may
1We use SVM Light, with c = 1 (Joachims, 1999). Non-
linear kernels did not improve performance in our experiments.
# Feature Description Error
1 L = wi 1.88%
2 R = wj 9.36%
3 len(L) = l 9.12%
4 is cap(R) 12.56%
5 int(log(count(L; no period))) = ci 12.14%
6 int(log(count(R; is lower)) = cj 18.79%
7 (L = wi, R = wj) 10.01%
8 (L = wi, is cap(R)) 7.54%
1+2 0.77%
1+2+3+4 0.36%
1+2+3+4+5+6 0.32%
1+2+3+4+5+6+7+8 0.25%
Table 1: All features are binary. SVM classification re-
sults shown; Naive Bayes gives 0.35% error rate with all
features.
be meaningful abbreviation subclasses, a feature in-
dicating mere presence is too coarse.
Abbr. Ends Sentence Total Ratio
Inc. 109 683 0.16
Co. 80 566 0.14
Corp. 67 699 0.10
U.S. 45 800 0.06
Calif. 24 86 0.28
Ltd. 23 112 0.21
Table 2: The abbreviations appearing most often as sen-
tence boundaries. These top 6 account for 80% of
sentence-ending abbreviations in the test set, though only
5% of all abbreviations.
Adding features 3 and 4 better than cuts the re-
maining errors in half. These can be seen as a kind
of smoothing for sparser token features 1 and 2. Fea-
ture 3, the length of the left token, is a reasonable
proxy for the abbreviation class (mean abbreviation
length is 2.6, compared to 6.1 for non-abbreviation
sentence enders). The capitalization of the right to-
ken, feature 4, is a proxy for a sentence starter. Ev-
ery new sentence that starts with a word (as opposed
to a number or punctuation) is capitalized, but 70%
of words following abbreviations are also, so this
feature is mostly valuable in combination.
While we train on nearly 1 million words, most of
these are ignored because our features are extracted
only near possible sentence boundaries. Consider
the fragment ?... the U.S. Apparently some ...?,
242
which our system fails to split after ?U.S.? The word
?Apparently? starts only 8 sentences in the train-
ing data, but since it usually appears lowercased (89
times in training), its capitalization here is meaning-
ful. Feature 6 encodes this idea, indicating the log
count of lowercased appearances of the word right
of the candidate period. Similarly, feature 5 gives
the log count of occurrences of the token left of the
candidate appearing without a final period.
Another way to incorporate all of the training
data is to build a model of P (s|?L R?), as is of-
ten used in sentence segmentation for speech recog-
nition. Without a period in the conditional, many
more negative examples are included. The resulting
SVM model is very good at placing periods given
input text without them (0.31% error rate), but when
limiting the input to examples with ambiguous peri-
ods, the error rate is not competitive with our origi-
nal model (1.45%).
Features 7 and 8 are added to model the nuances
of abbreviations at sentence boundaries, helping to
reduce errors involving the examples in Table 2.
4 Two Classes or Three?
SBD has always been treated as a binary classifica-
tion problem, but there are really three classes: sen-
tence boundary only (S); abbreviation only (A); ab-
breviation at sentence boundary (A + S). The label
space of the test data, which has all periods anno-
tated, is shown in Figure 1.
  
Sentence Boundaries (S)
Abbreviations (A)
(A+S)
(A)
(A+S)All Data
Errors
Figure 1: The overlapping label space of the test data:
sentence boundaries 74%; abbreviations 26%; intersec-
tion 2%. The distribution of errors given by our classifier
is shown as well (not to scale with all data).
Relative to the size of the classes, A + S exam-
ples are responsible for a disproportionate number
of errors, pointing towards the problem with a bi-
nary classifier: In the absence of A + S examples,
the left context L and the right context R both help
distinguish S from A. But A + S cases have L re-
sembling the A class and R resembling the S class.
One possibility is to add a third class, but this does
not improve results, probably because we have so
few A + S examples. We also tried taking a more
structured approach, depicted in Figure 2, but this
too fails to improve performance, mostly because
the first step, identifying abbreviations without the
right context, is too hard. Certainly, the A+S cases
are more difficult to identify, but perhaps some bet-
ter structured approach could reduce the error rate
further.
? ?
P(A?|??L.?)?>?0.5
P(S?|??R?)?>?0.5
S
A+S
A
no
yes
no
yes
Figure 2: A structured classification approach. The left
context is used to separate S examples first, then those
remaining are classified as either A or A + S using the
right context.
5 Training Data
One common objection to supervised SBD systems
is an observation in (Reynar and Ratnaparkhi, 1997),
that training data and test data must be a good match,
limiting the applicability of a model trained from a
specific genre. Table 3 shows respectable error rates
for two quite different test sets: The Brown corpus
includes 500 documents, distributed across 15 gen-
res roughly representative of all published English;
The Complete Works of Edgar Allen Poe includes
an introduction, prose, and poetry.
A second issue is a lack of labeled data, espe-
cially in languages besides English. Table 4 shows
that results can be quite good without extensive la-
beled resources, and they are likely to continue to
improve if additional resources were available. At
the least, (Kiss and Strunk, 2006) have labeled over
243
Corpus Examples in S SVM Err NB Err
WSJ 26977 74% 0.25% 0.35%
Brown 53688 91% 0.36% 0.45%
Poe 11249 95% 0.52% 0.44%
Table 3: SVM and Naive Bayes classification error rates
on different corpora using a model trained from a disjoint
WSJ data set.
10000 sentences in each of 11 languages, though we
have not experimented with this data.
Corpus 5 50 500 5000 42317
WSJ 7.26% 3.57% 1.36% 0.52% 0.25%
Brown 5.65% 4.46% 1.65% 0.74% 0.36%
Poe 4.01% 2.68% 2.22% 0.98% 0.52%
Table 4: SVM error rates on the test corpora, using mod-
els built from different numbers of training sentences.
We also tried to improve results using a standard
bootstrapping method. Our WSJ-trained model was
used to annotate 100 million words of New York
Times data from the AQUAINT corpus, and we in-
cluded high-confidence examples in a new training
set. This did not degrade test error, nor did it im-
prove it.
6 Errors
Our system makes 67 errors out of 26977 examples
on the WSJ test set; a representative few are shown
in Table 5. 34% of the errors involve the word ?U.S.?
which distinguishes itself as the most difficult of to-
kens to classify: Not only does it appear frequently
as a sentence boundary, but even when it does not,
the next word is often capitalized (?U.S. Govern-
ment?; ?U.S. Commission?), further confusing the
classifier. In fact, abbreviations for places, includ-
ing ?U.K.?, ?N.Y.?, ?Pa.? constitute 46% of all er-
rors for the same reason. Most of the remaining er-
rors involve abbreviations like those in Table 2, and
all are quite difficult for a human to resolve without
more context. Designing features to exploit addi-
tional context might help, but could require parsing.
7 Conclusion
We have described a simple yet powerful method for
SBD. While we have not tested models in languages
other than English, we are providing the code and
our models, complete with tokenization, available
Context Label P (S)
... the U.S. Amoco already ... A + S 0.45
... the U.K. Panel on ... A 0.57
... the U.S. Prudential Insurance ... A + S 0.44
... Telephone Corp. President Haruo ... A 0.73
... Wright Jr. Room ... A 0.67
... 6 p.m. Travelers who ... A + S 0.44
Table 5: Sample errors with the probability of being in
the S class assigned by the SVM.
at http://code.google.com/p/splitta. Future work in-
cludes further experiments with structured classifi-
cation to treat the three classes appropriately.
Acknowledgments
Thanks to Benoit Favre, Dilek Hakkani-Tu?r, Kofi
Boakye, Marcel Paret, James Jacobus, and Larry
Gillick for helpful discussions.
References
J. Aberdeen, J. Burger, D. Day, L. Hirschman, P. Robin-
son, and M. Vilain. 1995. MITRE: description of the
Alembic system used for MUC-6. In Proceedings of
the 6th conference on Message understanding, pages
141?155. Association for Computational Linguistics
Morristown, NJ, USA.
T. Joachims. 1999. Making large-scale support vector
machine learning practical, Advances in kernel meth-
ods: support vector learning.
T. Kiss and J. Strunk. 2006. Unsupervised Multilingual
Sentence Boundary Detection. Computational Lin-
guistics, 32(4):485?525.
E. Loper and S. Bird. 2002. NLTK: The Natural Lan-
guage Toolkit. In Proceedings of the ACL Workshop
on Effective Tools and Methodologies for Teaching
Natural Language Processing and Computational Lin-
guistics, pages 62?69.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: the penn treebank. Computational Linguistics,
19(2):313?330.
A. Mikheev. 2002. Periods, Capitalized Words, etc.
Computational Linguistics, 28(3):289?318.
D.D. Palmer and M.A. Hearst. 1997. Adaptive Multilin-
gual Sentence Boundary Disambiguation. Computa-
tional Linguistics, 23(2):241?267.
J.C. Reynar and A. Ratnaparkhi. 1997. A maximum en-
tropy approach to identifying sentence boundaries. In
Proceedings of the Fifth Conference on Applied Natu-
ral Language Processing, pages 16?19.
244
Proceedings of the Workshop on Statistical Machine Translation, pages 31?38,
New York City, June 2006. c?2006 Association for Computational Linguistics
Why Generative Phrase Models Underperform Surface Heuristics
John DeNero, Dan Gillick, James Zhang, Dan Klein
Department of Electrical Engineering and Computer Science
University of California, Berkeley
Berkeley, CA 94705
{denero, dgillick, jyzhang, klein}@eecs.berkeley.edu
Abstract
We investigate why weights from generative mod-
els underperform heuristic estimates in phrase-
based machine translation. We first propose a sim-
ple generative, phrase-based model and verify that
its estimates are inferior to those given by surface
statistics. The performance gap stems primarily
from the addition of a hidden segmentation vari-
able, which increases the capacity for overfitting
during maximum likelihood training with EM. In
particular, while word level models benefit greatly
from re-estimation, phrase-level models do not: the
crucial difference is that distinct word alignments
cannot all be correct, while distinct segmentations
can. Alternate segmentations rather than alternate
alignments compete, resulting in increased deter-
minization of the phrase table, decreased general-
ization, and decreased final BLEU score. We also
show that interpolation of the two methods can re-
sult in a modest increase in BLEU score.
1 Introduction
At the core of a phrase-based statistical machine
translation system is a phrase table containing
pairs of source and target language phrases, each
weighted by a conditional translation probability.
Koehn et al (2003a) showed that translation qual-
ity is very sensitive to how this table is extracted
from the training data. One particularly surprising
result is that a simple heuristic extraction algorithm
based on surface statistics of a word-aligned training
set outperformed the phrase-based generative model
proposed by Marcu and Wong (2002).
This result is surprising in light of the reverse sit-
uation for word-based statistical translation. Specif-
ically, in the task of word alignment, heuristic ap-
proaches such as the Dice coefficient consistently
underperform their re-estimated counterparts, such
as the IBM word alignment models (Brown et al,
1993). This well-known result is unsurprising: re-
estimation introduces an element of competition into
the learning process. The key virtue of competition
in word alignment is that, to a first approximation,
only one source word should generate each target
word. If a good alignment for a word token is found,
other plausible alignments are explained away and
should be discounted as incorrect for that token.
As we show in this paper, this effect does not pre-
vail for phrase-level alignments. The central differ-
ence is that phrase-based models, such as the ones
presented in section 2 or Marcu and Wong (2002),
contain an element of segmentation. That is, they do
not merely learn correspondences between phrases,
but also segmentations of the source and target sen-
tences. However, while it is reasonable to sup-
pose that if one alignment is right, others must be
wrong, the situation is more complex for segmenta-
tions. For example, if one segmentation subsumes
another, they are not necessarily incompatible: both
may be equally valid. While in some cases, such
as idiomatic vs. literal translations, two segmenta-
tions may be in true competition, we show that the
most common result is for different segmentations
to be recruited for different examples, overfitting the
training data and overly determinizing the phrase
translation estimates.
In this work, we first define a novel (but not rad-
ical) generative phrase-based model analogous to
IBM Model 3. While its exact training is intractable,
we describe a training regime which uses word-
level alignments to constrain the space of feasible
segmentations down to a manageable number. We
demonstrate that the phrase analogue of the Dice co-
efficient is superior to our generative model (a re-
sult also echoing previous work). In the primary
contribution of the paper, we present a series of ex-
periments designed to elucidate what re-estimation
learns in this context. We show that estimates are
overly determinized because segmentations are used
31
in unintuitive ways for the sake of data likelihood.
We comment on both the beneficial instances of seg-
ment competition (idioms) as well as the harmful
ones (most everything else). Finally, we demon-
strate that interpolation of the two estimates can
provide a modest increase in BLEU score over the
heuristic baseline.
2 Approach and Evaluation Methodology
The generative model defined below is evaluated
based on the BLEU score it produces in an end-
to-end machine translation system from English to
French. The top-performing diag-and extraction
heuristic (Zens et al, 2002) serves as the baseline for
evaluation.1 Each approach ? the generative model
and heuristic baseline ? produces an estimated con-
ditional distribution of English phrases given French
phrases. We will refer to the distribution derived
from the baseline heuristic as ?H . The distribution
learned via the generative model, denoted ?EM , is
described in detail below.
2.1 A Generative Phrase Model
While our model for computing ?EM is novel, it
is meant to exemplify a class of models that are
not only clear extensions to generative word align-
ment models, but also compatible with the statistical
framework assumed during phrase-based decoding.
The generative process we modeled produces a
phrase-aligned English sentence from a French sen-
tence where the former is a translation of the lat-
ter. Note that this generative process is opposite to
the translation direction of the larger system because
of the standard noisy-channel decomposition. The
learned parameters from this model will be used to
translate sentences from English to French. The gen-
erative process modeled has four steps:2
1. Begin with a French sentence f.
1This well-known heuristic extracts phrases from a sentence
pair by computing a word-level alignment for the sentence and
then enumerating all phrases compatible with that alignment.
The word alignment is computed by first intersecting the direc-
tional alignments produced by a generative IBM model (e.g.,
model 4 with minor enhancements) in each translation direc-
tion, then adding certain alignments from the union of the di-
rectional alignments based on local growth rules.
2Our notation matches the literature for phrase-based trans-
lation: e is an English word, e? is an English phrase, and e?I1 is a
sequence of I English phrases, and e is an English sentence.
2. Segment f into a sequence of I multi-word
phrases that span the sentence, f? I1 .
3. For each phrase f?i ? f? I1 , choose a correspond-
ing position j in the English sentence and es-
tablish the alignment aj = i, then generate ex-
actly one English phrase e?j from f?i.
4. The sequence e?j ordered by a describes an En-
glish sentence e.
The corresponding probabilistic model for this gen-
erative process is:
P (e|f) =
?
f?I1 ,e?
I
1,a
P (e, f? I1 , e?
I
1, a|f)
=
?
f?I1 ,e?
I
1,a
?(f? I1 |f)
?
f?i?f?I1
?(e?j |f?i)d(aj = i|f)
where P (e, f? I1 , e?
I
1, a|f) factors into a segmentation
model ?, a translation model ? and a distortion
model d. The parameters for each component of this
model are estimated differently:
? The segmentation model ?(f? I1 |f) is assumed to
be uniform over all possible segmentations for
a sentence.3
? The phrase translation model ?(e?j |f?i) is pa-
rameterized by a large table of phrase transla-
tion probabilities.
? The distortion model d(aj = i|f) is a discount-
ing function based on absolute sentence posi-
tion akin to the one used in IBM model 3.
While similar to the joint model in Marcu and Wong
(2002), our model takes a conditional form com-
patible with the statistical assumptions used by the
Pharaoh decoder. Thus, after training, the param-
eters of the phrase translation model ?EM can be
used directly for decoding.
2.2 Training
Significant approximation and pruning is required
to train a generative phrase model and table ? such
as ?EM ? with hidden segmentation and alignment
variables using the expectation maximization algo-
rithm (EM). Computing the likelihood of the data
3This segmentation model is deficient given a maximum
phrase length: many segmentations are disallowed in practice.
32
for a set of parameters (the e-step) involves summing
over exponentially many possible segmentations for
each training sentence. Unlike previous attempts to
train a similar model (Marcu and Wong, 2002), we
allow information from a word-alignment model to
inform our approximation. This approach allowed
us to directly estimate translation probabilities even
for rare phrase pairs, which were estimated heuristi-
cally in previous work.
In each iteration of EM, we re-estimate each
phrase translation probability by summing fractional
phrase counts (soft counts) from the data given the
current model parameters.
?new(e?j |f?i) =
c(f?i, e?j)
c(f?i)
=
?
(f,e)
?
f?I1 :f?i?f?
I
1
?
e?I1:e?j?e?
I
1
?
a:aj=i P (e, f?
I
1 , e?
I
1, a|f)
?
f?I1 :f?i?f?
I
1
?
e?I1
?
a P (e, f?
I
1 , e?
I
1, a|f)
This training loop necessitates approximation be-
cause summing over all possible segmentations and
alignments for each sentence is intractable, requiring
time exponential in the length of the sentences. Ad-
ditionally, the set of possible phrase pairs grows too
large to fit in memory. Using word alignments, we
can address both problems.4 In particular, we can
determine for any aligned segmentation (f? I1 , e?
I
1, a)
whether it is compatible with the word-level align-
ment for the sentence pair. We define a phrase pair
to be compatible with a word-alignment if no word
in either phrase is aligned with a word outside the
other phrase (Zens et al, 2002). Then, (f? I1 , e?
I
1, a)
is compatible with the word-alignment if each of its
aligned phrases is a compatible phrase pair.
The training process is then constrained such that,
when evaluating the above sum, only compatible
aligned segmentations are considered. That is, we
allow P (e, f? I1 , e?
I
1, a|f) > 0 only for aligned seg-
mentations (f? I1 , e?
I
1, a) such that a provides a one-
to-one mapping from f? I1 to e?
I
1 where all phrase pairs
(f?aj , e?j) are compatible with the word alignment.
This constraint has two important effects. First,
we force P (e?j |f?i) = 0 for all phrase pairs not com-
patible with the word-level alignment for some sen-
tence pair. This restriction successfully reduced the
4The word alignments used in approximating the e-step
were the same as those used to create the heuristic diag-and
baseline.
total legal phrase pair types from approximately 250
million to 17 million for 100,000 training sentences.
However, some desirable phrases were eliminated
because of errors in the word alignments.
Second, the time to compute the e-step is reduced.
While in principle it is still intractable, in practice
we can compute most sentence pairs? contributions
in under a second each. However, some spurious
word alignments can disallow all segmentations for
a sentence pair, rendering it unusable for training.
Several factors including errors in the word-level
alignments, sparse word alignments and non-literal
translations cause our constraint to rule out approx-
imately 54% of the training set. Thus, the reduced
size of the usable training set accounts for some of
the degraded performance of ?EM relative to ?H .
However, the results in figure 1 of the following sec-
tion show that ?EM trained on twice as much data
as ?H still underperforms the heuristic, indicating a
larger issue than decreased training set size.
2.3 Experimental Design
To test the relative performance of ?EM and ?H ,
we evaluated each using an end-to-end translation
system from English to French. We chose this non-
standard translation direction so that the examples
in this paper would be more accessible to a primar-
ily English-speaking audience. All training and test
data were drawn from the French/English section of
the Europarl sentence-aligned corpus. We tested on
the first 1,000 unique sentences of length 5 to 15 in
the corpus and trained on sentences of length 1 to 60
starting after the first 10,000.
The system follows the structure proposed in
the documentation for the Pharaoh decoder and
uses many publicly available components (Koehn,
2003b). The language model was generated from
the Europarl corpus using the SRI Language Model-
ing Toolkit (Stolcke, 2002). Pharaoh performed de-
coding using a set of default parameters for weight-
ing the relative influence of the language, translation
and distortion models (Koehn, 2003b). A maximum
phrase length of three was used for all experiments.
To properly compare ?EM to ?H , all aspects of
the translation pipeline were held constant except for
the parameters of the phrase translation table. In par-
ticular, we did not tune the decoding hyperparame-
ters for the different phrase tables.
33
Source 25k 50k 100kHeuristic 0.3853 0.3883 0.3897Iteration 1 0.3724 0.3775 0.3743Iteration 2 0.3735 0.3851 0.3814iteration 3 0.3705 0.384 0.3827Iteration 4 0.3695 0.285 0.3801iteration 5 0.3705 0.284 0.3774interpSource 25k 50k 100kHeuristic 0.3853 0.3883 0.3897Iteration 1 0.3724 0.3775 0.3743iteration 3 0.3705 0.384 0.3827iteration 3 0.3705 0.384 0.3827
0.360.37
0.380.39
0.40
25k 50k 100kTraining sentences
BLEU
HeuristicIteration 1iteration 3
0%20%
40%60%
80%100%
0 10 20 30 40 50 60Sentence Length
Senten
ces Sk
ipped
Figure 1: Statistical re-estimation using a generative
phrase model degrades BLEU score relative to its
heuristic initialization.
3 Results
Having generated ?H heuristically and ?EM with
EM, we now compare their performance. While the
model and training regimen for ?EM differ from the
model from Marcu and Wong (2002), we achieved
results similar to Koehn et al (2003a): ?EM slightly
underperformed ?H . Figure 1 compares the BLEU
scores using each estimate. Note that the expecta-
tion maximization algorithm for training ?EM was
initialized with the heuristic parameters ?H , so the
heuristic curve can be equivalently labeled as itera-
tion 0.
Thus, the first iteration of EM increases the ob-
served likelihood of the training sentences while si-
multaneously degrading translation performance on
the test set. As training proceeds, performance on
the test set levels off after three iterations of EM. The
system never achieves the performance of its initial-
ization parameters. The pruning of our training regi-
men accounts for part of this degradation, but not all;
augmenting ?EM by adding back in all phrase pairs
that were dropped during training does not close the
performance gap between ?EM and ?H .
3.1 Analysis
Learning ?EM degrades translation quality in large
part because EM learns overly determinized seg-
mentations and translation parameters, overfitting
the training data and failing to generalize. The pri-
mary increase in richness from generative word-
level models to generative phrase-level models is
due to the additional latent segmentation variable.
Although we impose a uniform distribution over
segmentations, it nonetheless plays a crucial role
during training. We will characterize this phe-
nomenon through aggregate statistics and transla-
tion examples shortly, but begin by demonstrating
the model?s capacity to overfit the training data.
Let us first return to the motivation behind in-
troducing and learning phrases in machine transla-
tion. For any language pair, there are contiguous
strings of words whose collocational translation is
non-compositional; that is, they translate together
differently than they would in isolation. For in-
stance, chat in French generally translates to cat in
English, but appeler un chat un chat is an idiom
which translates to call a spade a spade. Introduc-
ing phrases allows us to translate chat un chat atom-
ically to spade a spade and vice versa.
While introducing phrases and parameterizing
their translation probabilities with a surface heuris-
tic allows for this possibility, statistical re-estimation
would be required to learn that chat should never be
translated to spade in isolation. Hence, translating I
have a spade with ?H could yield an error.
But enforcing competition among segmentations
introduces a new problem: true translation ambigu-
ity can also be spuriously explained by the segmen-
tation. Consider the french fragment carte sur la
table, which could translate to map on the table or
notice on the chart. Using these two sentence pairs
as training, one would hope to capture the ambiguity
in the parameter table as:
French English ?(e|f)
carte map 0.5
carte notice 0.5
carte sur map on 0.5
carte sur notice on 0.5
sur on 1.0
... ... ...
table table 0.5
table chart 0.5
Assuming we only allow non-degenerate seg-
mentations and disallow non-monotonic alignments,
this parameter table yields a marginal likelihood
P (f|e) = 0.25 for both sentence pairs ? the intu-
itive result given two independent lexical ambigu-
34
ities. However, the following table yields a likeli-
hood of 0.28 for both sentences:5
French English ?(e|f)
carte map 1.0
carte sur notice on 1.0
carte sur la notice on the 1.0
sur on 1.0
sur la table on the table 1.0
la the 1.0
la table the table 1.0
table chart 1.0
Hence, a higher likelihood can be achieved by al-
locating some phrases to certain translations while
reserving overlapping phrases for others, thereby
failing to model the real ambiguity that exists across
the language pair. Also, notice that the phrase sur
la can take on an arbitrary distribution over any en-
glish phrases without affecting the likelihood of ei-
ther sentence pair. Not only does this counterintu-
itive parameterization give a high data likelihood,
but it is also a fixed point of the EM algorithm.
The phenomenon demonstrated above poses a
problem for generative phrase models in general.
The ambiguous process of translation can be mod-
eled either by the latent segmentation variable or the
phrase translation probabilities. In some cases, opti-
mizing the likelihood of the training corpus adjusts
for the former when we would prefer the latter. We
next investigate how this problem manifests in ?EM
and its effect on translation quality.
3.2 Learned parameters
The parameters of ?EM differ from the heuristically
extracted parameters ?H in that the conditional dis-
tributions over English translations for some French
words are sharply peaked for ?EM compared to flat-
ter distributions generated by ?H . This determinism
? predicted by the previous section?s example ? is
not atypical of EM training for other tasks.
To quantify the notion of peaked distributions
over phrase translations, we compute the entropy of
the distribution for each French phrase according to
5For example, summing over the first translation ex-
pands to 17 (?(map | carte)?(on the table | sur la table)
+?(map | carte)?(on | sur)?(the table | la table)).
it 2.76E-08 as there are 0.073952202code 2.29E-08 the 0.002670946to 1.98E-12 less helpful 6.22E-05it be 1.11E-14 please stop messing 1.12E-05
0 10 20 30 400 - .01
.01 - .5.5 - 1
1 - 1.51.5 - 2
> 2
Entrop
y
% Phrase TranslationsLearnedHeuristic
1E-04 1E-02 1E+00 1E+02',de.lall 'leetlesMost
 Comm
on Fre
nch Ph
rases
EntropyLearned Heuristic
Figure 2: Many more French phrases have very low
entropy under the learned parameterization.
the standard definition.
H(?(e?|f?)) =
?
e?
?(e?|f?) log2 ?(e?|f?)
The average entropy, weighted by frequency, for the
most common 10,000 phrases in the learned table
was 1.55, comparable to 3.76 for the heuristic table.
The difference between the tables becomes much
more striking when we consider the histogram of
entropies for phrases in figure 2. In particular, the
learned table has many more phrases with entropy
near zero. The most pronounced entropy differences
often appear for common phrases. Ten of the most
common phrases in the French corpus are shown in
figure 3.
As more probability mass is reserved for fewer
translations, many of the alternative translations un-
der ?H are assigned prohibitively small probabili-
ties. In translating 1,000 test sentences, for example,
no phrase translation with ?(e?|f?) less than 10?5 was
used by the decoder. Given this empirical threshold,
nearly 60% of entries in ?EM are unusable, com-
pared with 1% in ?H .
3.3 Effects on Translation
While this determinism of ?EM may be desirable
in some circumstances, we found that the ambi-
guity in ?H is often preferable at decoding time.
35
it 2.76E-08 as there are 0.073952202code 2.29E-08 the 0.002670946to 1.98E-12 less helpful 6.22E-05it be 1.11E-14 please stop messing 1.12E-05
01020
3040
0 - .01 .01 - .5 .5 - 1 1 - 1.5 1.5 - 2 > 2Entropy
% Phr
ase 
Transl
ations HeuristicLearned
1E-04 1E-02 1E+00 1E+02 ',.ll 'n 'quequiplusl ' unionC
ommo
n Fren
ch Phr
ases
EntropyLearned Heuristic
Figure 3: Entropy of 10 common French phrases.
Several learned distributions have very low entropy.
In particular, the pattern of translation-ambiguous
phrases receiving spuriously peaked distributions (as
described in section 3.1) introduces new tra slation
errors relative to the baseline. We now investigate
both positive and negative effects of the learning
process.
The issue that motivated training a generative
model is sometimes resolved correctly: for a word
that translates differently alone than in the context
of an idiom, the translation probabilities can more
accurately reflect this. Returning to the previous ex-
ample, the phrase table for chat has been corrected
through the learning process. The heuristic process
gives the incorrect translation spade with 61% prob-
ability, while the statistical learning approach gives
cat with 95% probability.
While such examples of improvement are en-
couraging, the trend of spurious determinism over-
whelms this benefit by introducing errors in four re-
lated ways, each of which will be explored in turn.
1. Useful phrase pairs can be assigned very low
probabilities and therefore become unusable.
2. A proper translation for a phrase can be over-
ridden by another translation with spuriously
high probability.
3. Error-prone, common, ambiguous phrases be-
come active during decoding.
4. The language model cannot distinguish be-
tween different translation options as effec-
tively due to deterministic translation model
distributions.
The first effect follows from our observation in
section 3.2 that many phrase pairs are unusable due
to vanishingly small probabilities. Some of the en-
tries that are made unusable by re-estimation are
helpful at decoding time, evidenced by the fact
that pruning the set of ?EM ?s low-scoring learned
phrases from the original heuristic table reduces
BLEU score by 0.02 for 25k training sentences (be-
low the score for ?EM ).
The second effect is more subtle. Consider the
sentence in figure 4, which to a first approxima-
tion can be translated as a series of cognates, as
demonstrated by the decoding that follows from the
heuristic parameterization ?H .6 Notice also that the
translation probabilities from heuristic extraction are
non-deterministic. On the other hand, the translation
system makes a significant lexical error on this sim-
ple sentence when parameterized by ?EM : the use
of caracte?rise in this context is incorrect. This error
arises from a sharply peaked distribution over En-
glish phrases for caracte?rise.
This example illustrates a recurring problem: er-
rors do not necessarily arise because a correct trans-
lation is not available. Notice that a preferable trans-
lation of degree as degre? is available under both pa-
rameterizations. Degre? is not used, however, be-
cause of the peaked distribution of a competing
translation candidate. In this way, very high prob-
ability translations can effectively block the use of
more appropriate translations at decoding time.
What is furthermore surprising and noteworthy in
this example is that the learned, near-deterministic
translation for caracte?rise is not a common trans-
lation for the word. Not only does the statistical
learning process yield low-entropy translation dis-
tributions, but occasionally the translation with un-
desirably high conditional probability does not have
a strong surface correlation with the source phrase.
This example is not unique; during different initial-
izations of the EM algorithm, we noticed such pat-
6While there is some agreement error and awkwardness, the
heuristic translation is comprehensible to native speakers. The
learned translation incorrectly translates degree, degrading the
translation quality.
36
the situation varies to an
la situation varie d ' une
Heuristically Extracted Phrase Table
Learned Phrase Table
enormous
immense
degree
degr?
situation varies to
la varie d '
an enormous
une immense
degree
caract?rise
the
situation
caracte?rise
English ?(e|f)
degree 0.998
characterises 0.001
characterised 0.001
caracte?rise
English ?(e|f)
characterises 0.49
characterised 0.21
permeate 0.05
features 0.05
typifies 0.05
degr e?
English ?(e|f)
degree 0.49
level 0.38
extent 0.02
amount 0.02
how 0.01
degre?
English ?(e|f)
degree 0.64
level 0.26
extent 0.10
Figure 4: Spurious determinism in the learned phrase parameters degrades translation quality.
terns even for common French phrases such as de
and ne.
The third source of errors is closely related: com-
mon phrases that translate in many ways depending
on the context can introduce errors if they have a
spuriously peaked distribution. For instance, con-
sider the lone apostrophe, which is treated as a sin-
gle token in our data set (figure 5). The shape of
the heuristic translation distribution for the phrase is
intuitively appealing, showing a relatively flat dis-
tribution among many possible translations. Such
a distribution has very high entropy. On the other
hand, the learned table translates the apostrophe to
the with probability very near 1.
Heuristic
English ?H(e|f)
our 0.10
that 0.09
is 0.06
we 0.05
next 0.05
Learned
English ?EM (e|f)
the 0.99
, 4.1 ? 10?3
is 6.5 ? 10?4
to 6.3 ? 10?4
in 5.3 ? 10?4
Figure 5: Translation probabilities for an apostro-
phe, the most common french phrase. The learned
table contains a highly peaked distribution.
Such common phrases whose translation depends
highly on the context are ripe for producing transla-
tion errors. The flatness of the distribution of ?H en-
sures that the single apostrophe will rarely be used
during decoding because no one phrase table entry
has high enough probability to promote its use. On
the other hand, using the peaked entry ?EM (the|?)
incurs virtually no cost to the score of a translation.
The final kind of errors stems from interactions
between the language and translation models. The
selection among translation choices via a language
model ? a key virtue of the noisy channel frame-
work ? is hindered by the determinism of the transla-
tion model. This effect appears to be less significant
than the previous three. We should note, however,
that adjusting the language and translation model
weights during decoding does not close the perfor-
mance gap between ?H and ?EM .
3.4 Improvements
In light of the low entropy of ?EM , we could hope to
improve translations by retaining entropy. There are
several strategies we have considered to achieve this.
Broadly, we have tried two approaches: combin-
ing ?EM and ?H via heuristic interpolation methods
and modifying the training loop to limit determin-
ism.
The simplest strategy to increase entropy is to
interpolate the heuristic and learned phrase tables.
Varying the weight of interpolation showed an im-
provement over the heuristic of up to 0.01 for 100k
sentences. A more modest improvement of 0.003 for
25k training sentences appears in table 1.
In another experiment, we interpolated the out-
put of each iteration of EM with its input, thereby
maintaining some entropy from the initialization pa-
rameters. BLEU score increased to a maximum of
0.394 using this technique with 100k training sen-
tences, outperforming the heuristic by a slim margin
of 0.005.
We might address the determinization in ?EM
without resorting to interpolation by modifying the
37
training procedure to retain entropy. By imposing a
non-uniform segmentation model that favors shorter
phrases over longer ones, we hope to prevent the
error-causing effects of EM training outlined above.
In principle, this change will encourage EM to ex-
plain training sentences with shorter sentences. In
practice, however, this approach has not led to an
improvement in BLEU.
Another approach to maintaining entropy during
the training process is to smooth the probabilities
generated by EM. In particular, we can use the fol-
lowing smoothed update equation during the train-
ing loop, which reserves a portion of probability
mass for unseen translations.
?new(e?j |f?i) =
c(f?i, e?j)
c(f?i) + kl?1
In the equation above, l is the length of the French
phrase and k is a tuning parameter. This formula-
tion not only serves to reduce very spiked probabili-
ties in ?EM , but also boosts the probability of short
phrases to encourage their use. With k = 2.5, this
smoothing approach improves BLEU by .007 using
25k training sentences, nearly equaling the heuristic
(table 1).
4 Conclusion
Re-estimating phrase translation probabilities using
a generative model holds the promise of improving
upon heuristic techniques. However, the combina-
torial properties of a phrase-based generative model
have unfortunate side effects. In cases of true ambi-
guity in the language pair to be translated, parameter
estimates that explain the ambiguity using segmen-
tation variables can in some cases yield higher data
likelihoods by determinizing phrase translation esti-
mates. However, this behavior in turn leads to errors
at decoding time.
We have also shown that some modest benefit can
be obtained from re-estimation through the blunt in-
strument of interpolation. A remaining challenge is
to design more appropriate statistical models which
tie segmentations together unless sufficient evidence
of true non-compositionality is present; perhaps
such models could properly combine the benefits of
both current approaches.
Estimate BLEU
?H 0.385
?H phrase pairs that also appear in ?EM 0.365
?EM 0.374
?EM with a non-uniform segmentation model 0.374
?EM with smoothing 0.381
?EM with gaps filled in by ?H 0.374
?EM interpolated with ?H 0.388
Table 1: BLEU results for 25k training sentences.
5 Acknowledgments
We would like to thank the anonymous reviewers for
their valuable feedback on this paper.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2), 1993.
Philipp Koehn. Europarl: A Multilingual Corpus for
Evaluation of Machine Translation. USC Information
Sciences Institute, 2002.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. Sta-
tistical phrase-based translation. HLT-NAACL, 2003.
Philipp Koehn. Pharaoh: A Beam Search Decoder for
Phrase-Based Statisical Machine Translation Models.
USC Information Sciences Institute, 2003.
Daniel Marcu and William Wong. A phrase-based, joint
probability model for statistical machine translation.
Conference on Empirical Methods in Natual Language
Processing, 2002.
Franz Josef Och and Hermann Ney. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51, 2003.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
Improved alignment models for statistical machine
translation. ACL Workshops, 1999.
Andreas Stolcke. Srilm ? an extensible language model-
ing toolkit. Proceedings of the International Confer-
ence on Statistical Language Processing, 2002.
Richard Zens, Franz Josef Och and Hermann Ney.
Phrase-Based Statistical Machine Translation. Annual
German Conference on AI, 2002.
38
Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Scalable Global Model for Summarization
Dan Gillick1,2, Benoit Favre2
1 Computer Science Division, University of California Berkeley, USA
2 International Computer Science Institute, Berkeley, USA
{dgillick,favre}@icsi.berkeley.edu
Abstract
We present an Integer Linear Program for
exact inference under a maximum coverage
model for automatic summarization. We com-
pare our model, which operates at the sub-
sentence or ?concept?-level, to a sentence-
level model, previously solved with an ILP.
Our model scales more efficiently to larger
problems because it does not require a
quadratic number of variables to address re-
dundancy in pairs of selected sentences. We
also show how to include sentence compres-
sion in the ILP formulation, which has the
desirable property of performing compression
and sentence selection simultaneously. The
resulting system performs at least as well as
the best systems participating in the recent
Text Analysis Conference, as judged by a va-
riety of automatic and manual content-based
metrics.
1 Introduction
Automatic summarization systems are typically ex-
tractive or abstractive. Since abstraction is quite
hard, the most successful systems tested at the Text
Analysis Conference (TAC) and Document Under-
standing Conference (DUC)1, for example, are ex-
tractive. In particular, sentence selection represents
a reasonable trade-off between linguistic quality,
guaranteed by longer textual units, and summary
content, often improved with shorter units.
Whereas the majority of approaches employ a
greedy search to find a set of sentences that is
1TAC is a continuation of DUC, which ran from 2001-2007.
both relevant and non-redundant (Goldstein et al,
2000; Nenkova and Vanderwende, 2005), some re-
cent work focuses on improved search (McDonald,
2007; Yih et al, 2007). Among them, McDonald is
the first to consider a non-approximated maximiza-
tion of an objective function through Integer Linear
Programming (ILP), which improves on a greedy
search by 4-12%. His formulation assumes that the
quality of a summary is proportional to the sum of
the relevance scores of the selected sentences, penal-
ized by the sum of the redundancy scores of all pairs
of selected sentences. Under a maximum summary
length constraint, this problem can be expressed as
a quadratic knapsack (Gallo et al, 1980) and many
methods are available to solve it (Pisinger et al,
2005). However, McDonald reports that the method
is not scalable above 100 input sentences and dis-
cusses more practical approximations. Still, an ILP
formulation is appealing because it gives exact so-
lutions and lends itself well to extensions through
additional constraints.
Methods like McDonald?s, including the well-
known Maximal Marginal Relevance (MMR) algo-
rithm (Goldstein et al, 2000), are subject to an-
other problem: Summary-level redundancy is not
always well modeled by pairwise sentence-level re-
dundancy. Figure 1 shows an example where the
combination of sentences (1) and (2) overlaps com-
pletely with sentence (3), a fact not captured by pair-
wise redundancy measures. Redundancy, like con-
tent selection, is a global problem.
Here, we discuss a model for sentence selection
with a globally optimal solution that also addresses
redundancy globally. We choose to represent infor-
10
(1) The cat is in the kitchen.
(2) The cat drinks the milk.
(3) The cat drinks the milk in the kitchen.
Figure 1: Example of sentences redundant as a group.
Their redundancy is only partially captured by sentence-
level pairwise measurement.
mation at a finer granularity than sentences, with
concepts, and assume that the value of a summary is
the sum of the values of the unique concepts it con-
tains. While the concepts we use in experiments are
word n-grams, we use the generic term to emphasize
that this is just one possible definition. Only credit-
ing each concept once serves as an implicit global
constraint on redundancy. We show how the result-
ing optimization problem can be mapped to an ILP
that can be solved efficiently with standard software.
We begin by comparing our model to McDonald?s
(section 2) and detail the differences between the re-
sulting ILP formulations (section 3), showing that
ours can give competitive results (section 4) and of-
fer better scalability2 (section 5). Next we demon-
strate how our ILP formulation can be extended to
include efficient parse-tree-based sentence compres-
sion (section 6). We review related work (section 7)
and conclude with a discussion of potential improve-
ments to the model (section 8).
2 Models
The model proposed by McDonald (2007) considers
information and redundancy at the sentence level.
The score of a summary is defined as the sum of
the relevance scores of the sentences it contains mi-
nus the sum of the redundancy scores of each pair of
these sentences. If si is an indicator for the presence
of sentence i in the summary, Reli is its relevance,
and Redij is its redundancy with sentence j, then a
summary is scored according to:
?
i
Relisi ?
?
ij
Redijsisj
Generating a summary under this model involves
maximizing this objective function, subject to a
2Strictly speaking, exact inference for the models discussed
in this paper is NP-hard. Thus we use the term ?scalable? in a
purely practical sense.
length constraint. A variety of choices for Reli and
Redij are possible, from simple word overlap met-
rics to the output of feature-based classifiers trained
to perform information retrieval and textual entail-
ment.
As an alternative, we consider information and re-
dundancy at a sub-sentence, ?concept? level, model-
ing the value of a summary as a function of the con-
cepts it covers. While McDonald uses an explicit
redundancy term, we model redundancy implicitly:
a summary only benefits from including each con-
cept once. With ci an indicator for the presence of
concept i in the summary, and its weight wi, the ob-
jective function is:
?
i
wici
We generate a summary by choosing a set of sen-
tences that maximizes this objective function, sub-
ject to the usual length constraint.
In summing over concept weights, we assume that
the value of including a concept is not effected by the
presence of any other concept in the summary. That
is, concepts are assumed to be independent. Choos-
ing a suitable definition for concepts, and a map-
ping from the input documents to concept weights,
is both important and difficult. Concepts could be
words, named entities, syntactic subtrees or seman-
tic relations, for example. While deeper semantics
make more appealing concepts, their extraction and
weighting are much more error-prone. Any error in
concept extraction can result in a biased objective
function, leading to poor sentence selection.
3 Inference by ILP
Each model presented above can be formalized as an
Integer Linear Program, with a solution represent-
ing an optimal selection of sentences under the ob-
jective function, subject to a length constraint. Mc-
Donald observes that the redundancy term makes for
a quadratic objective function, which he coerces to
a linear function by introducing additional variables
sij that represent the presence of both sentence i and
sentence j in the summary. Additional constraints
ensure the consistency between the sentence vari-
ables (si, sj) and the quadratic term (sij). With li
the length of sentence i and L the length limit for
11
the whole summary, the resulting ILP is:
Maximize:
?
i
Relisi ?
?
ij
Redijsij
Subject to:
?
j
ljsj ? L
sij ? si sij ? sj ?i, j
si + sj ? sij ? 1 ?i, j
si ? {0, 1} ?i
sij ? {0, 1} ?i, j
To express our concept-based model as an ILP, we
maintain our notation from section 2, with ci an in-
dicator for the presence of concept i in the summary
and sj an indicator for the presence of sentence j in
the summary. We add Occij to indicate the occur-
rence of concept i in sentence j, resulting in a new
ILP:
Maximize:
?
i
wici
Subject to:
?
j
ljsj ? L
sjOccij ? ci, ?i, j (1)?
j
sjOccij ? ci ?i (2)
ci ? {0, 1} ?i
sj ? {0, 1} ?j
Note that Occ, like Rel and Red, is a constant pa-
rameter. The constraints formalized in equations (1)
and (2) ensure the logical consistency of the solu-
tion: selecting a sentence necessitates selecting all
the concepts it contains and selecting a concept is
only possible if it is present in at least one selected
sentence. Constraint (1) also prevents the inclusion
of concept-less sentences.
4 Performance
Here we compare both models on a common sum-
marization task. The data is part of the Text Analy-
sis Conference (TAC) multi-document summariza-
tion evaluation and involves generating 100-word
summaries from 10 newswire documents, each on
a given topic. While the 2008 edition of TAC also
includes an update task?additional summaries as-
suming some prior knowledge?we focus only on
the standard task. This includes 48 topics, averag-
ing 235 input sentences (ranging from 47 to 652).
Since the mean sentence length is around 25 words,
a typical summary consists of 4 sentences.
In order to facilitate comparison, we generate
summaries from both models using a common
pipeline:
1. Clean input documents. A simple set of rules
removes headers and formatting markup.
2. Split text into sentences. We use the unsuper-
vised Punkt system (Kiss and Strunk, 2006).
3. Prune sentences shorter than 5 words.
4. Compute parameters needed by the models.
5. Map to ILP format and solve. We use an open
source solver3.
6. Order sentences picked by the ILP for inclusion
in the summary.
The specifics of step 4 are described in detail in
(McDonald, 2007) and (Gillick et al, 2008). Mc-
Donald?s sentence relevance combines word-level
cosine similarity with the source document and the
inverse of its position (early sentences tend to be
more important). Redundancy between a pair of sen-
tences is their cosine similarity. For sentence i in
document D,
Reli = cosine(i,D) + 1/pos(i,D)
Redij = cosine(i, j)
In our concept-based model, we use word bi-
grams, weighted by the number of input documents
in which they appear. While word bigrams stretch
the notion of a concept a bit thin, they are eas-
ily extracted and matched (we use stemming to al-
low slightly more robust matching). Table 1 pro-
vides some justification for document frequency as a
weighting function. Note that bigrams gave consis-
tently better performance than unigrams or trigrams
for a variety of ROUGE measures. Normalizing
by document frequency measured over a generic set
(TFIDF weighting) degraded ROUGE performance.
3gnu.org/software/glpk
12
Bigrams consisting of two stopwords are pruned, as
are those appearing in fewer than three documents.
We largely ignore the sentence ordering problem,
sorting the resulting sentences first by source docu-
ment date, and then by position, so that the order of
two originally adjacent sentences is preserved, for
example.
Doc. Freq. (D) 1 2 3 4 5 6
In Gold Set 156 48 25 15 10 7
Not in Gold Set 5270 448 114 42 21 11
Relevant (P ) 0.03 0.10 0.18 0.26 0.33 0.39
Table 1: There is a strong relationship between the docu-
ment frequency of input bigrams and the fraction of those
bigrams that appear in the human generated ?gold? set:
Let di be document frequency i and pi be the percent of
input bigrams with di that are actually in the gold set.
Then the correlation ?(D,P ) = 0.95 for DUC 2007 and
0.97 for DUC 2006. Data here averaged over all prob-
lems in DUC 2007.
The summaries produced by the two systems
have been evaluated automatically with ROUGE and
manually with the Pyramid metric. In particular,
ROUGE-2 is the recall in bigrams with a set of
human-written abstractive summaries (Lin, 2004).
The Pyramid score arises from a manual alignment
of basic facts from the reference summaries, called
Summary Content Units (SCUs), in a hypothesis
summary (Nenkova and Passonneau, 2004). We
used the SCUs provided by the TAC evaluation.
Table 2 compares these results, alongside a base-
line that uses the first 100 words of the most re-
cent document. All the scores are significantly
different, showing that according to both human
and automatic content evaluation, the concept-
based model outperforms McDonald?s sentence-
based model, which in turn outperforms the base-
line. Of course, the relevance and redundancy func-
tions used for McDonald?s formulation in this exper-
iment are rather primitive, and results would likely
improve with better relevance features as used in
many TAC systems. Nonetheless, our system based
on word bigram concepts, similarly primitive, per-
formed at least as well as any in the TAC evaluation,
according to two-tailed t-tests comparing ROUGE,
Pyramid, and manually evaluated ?content respon-
siveness? (Dang and Owczarzak, 2008) of our sys-
tem and the highest scoring system in each category.
System ROUGE-2 Pyramid
Baseline 0.058 0.186
McDonald 0.072 0.295
Concepts 0.110 0.345
Table 2: Scores for both systems and a baseline on TAC
2008 data (Set A) for ROUGE-2 and Pyramid evalua-
tions.
5 Scalability
McDonald?s sentence-level formulation corresponds
to a quadratic knapsack, and he shows his particu-
lar variant is NP-hard by reduction to 3-D matching.
The concept-level formulation is similar in spirit to
the classical maximum coverage problem: Given a
set of items X , a set of subsets S of X , and an in-
teger k, the goal is to pick at most k subsets from
S that maximizes the size of their union. Maximum
coverage is known to be NP-hard by reduction to the
set cover problem (Hochbaum, 1996).
Perhaps the simplest way to show that our formu-
lation is NP-hard is by reduction to the knapsack
problem (Karp, 1972). Consider the special case
where sentences do not share any overlapping con-
cepts. Then, the value of each sentence to the sum-
mary is independent of every other sentence. This is
a knapsack problem: trying to maximize the value
in a container of limited size. Given a solver for our
problem, we could solve all knapsack problem in-
stances, so our problem must also be NP-hard.
With n input sentences and m concepts, both
formulations generate a quadratic number of con-
straints. However, McDonald?s has O(n2) variables
while ours has O(n + m). In practice, scalability
is largely determined by the sparsity of the redun-
dancy matrix Red and the sentence-concept matrix
Occ. Efficient solutions thus depend heavily on the
choice of redundancy measure in McDonald?s for-
mulation and the choice of concepts in ours. Prun-
ing to reduce complexity involves removing low-
relevance sentences or ignoring low redundancy val-
ues in the former, and corresponds to removing low-
weight concepts in the latter. Note that pruning con-
cepts may be more desirable: Pruned sentences are
irretrievable, but pruned concepts may well appear
in the selected sentences through co-occurrence.
Figure 2 compares ILP run-times for the two
13
formulations, using a set of 25 topics from DUC
2007, each of which have at least 500 input sen-
tences. These are very similar to the TAC 2008
topics, but more input documents are provided for
each topic, which allowed us to extend the analysis
to larger problems. While the ILP solver finds opti-
mal solutions efficiently for our concept-based for-
mulation, run-time for McDonald?s approach grows
very rapidly. The plot includes timing results for
250-word summaries as well, showing that our ap-
proach is fast even for much more complex prob-
lems: A rough estimate for the number of possible
summaries has
(500
4
) = 2.6?109 for 100-word sum-
maries and
(500
10
) = 2.5 ? 1020 for 250 words sum-
maries.
While exact solutions are theoretically appealing,
they are only useful in practice if fast approxima-
tions are inferior. A greedy approximation of our
objective function gives 10% lower ROUGE scores
than the exact solution, a gap that separates the high-
est scoring systems from the middle of the pack in
the TAC evaluation. The greedy solution (linear in
the number of sentences, assuming a constant sum-
mary length) marks an upper bound on speed and
a lower bound on performance; The ILP solution
marks an upper bound on performance but is subject
to the perils of exponential scaling. While we have
not experimented with much larger documents, ap-
proximate methods will likely be valuable in bridg-
ing the performance gap for complex problems. Pre-
liminary experiments with local search methods are
promising in this regard.
6 Extensions
Here we describe how our ILP formulation can
be extended with additional constraints to incor-
porate sentence compression. In particular, we
are interested in creating compressed alternatives
for the original sentence by manipulating its parse
tree (Knight and Marcu, 2000). This idea has been
applied with some success to summarization (Turner
and Charniak, 2005; Hovy et al, 2005; Nenkova,
2008) with the goal of removing irrelevant or redun-
dant details, thus freeing space for more relevant in-
formation. One way to achieve this end is to gen-
erate compressed candidates for each sentence, cre-
ating an expanded pool of input sentences, and em-
50 100 150 200 250 300 350 400 450 5000
1
2
3
4
5
6
7
8
Number of Sentences
Av
era
ge 
Tim
e p
er P
rob
lem
 (se
con
ds)
 
 
100 word summaries
250 word summaries
100 word summaries (McDonald)
Figure 2: A comparison of ILP run-times (on an AMD
1.8Ghz desktop machine) of McDonald?s sentence-based
formulation and our concept-based formulation with an
increasing number of input sentences.
ploy some redundancy removal on the final selec-
tion (Madnani et al, 2007).
We adapt this approach to fit the ILP formulations
so that the optimization procedure decides which
compressed alternatives to pick. Formally, each
compression candidate belongs to a group gk corre-
sponding to its original sentence. We can then craft
a constraint to ensure that at most one sentence can
be selected from group gk, which also includes the
original:
?
i?gk
si ? 1,?gk
Assuming that all the compressed candidates are
themselves well-formed, meaningful sentences, we
would expect this approach to generate higher qual-
ity summaries. In general, however, compression
algorithms can generate an exponential number of
candidates. Within McDonald?s framework, this
can increase the number of variables and constraints
tremendously. Thus, we seek a compact representa-
tion for compression in our concept framework.
Specifically, we assume that compression in-
volves some combination of three basic operations
on sentences: extraction, removal, and substitution.
In extraction, a sub-sentence (perhaps the content of
a quotation) may be used independently, and the rest
of the sentence is dropped. In removal, a substring
14
is dropped (a temporal clause, for example) that pre-
serves the grammaticality of the sentence. In sub-
stitution, one substring is replaced by another (US
replaces United States, for example).
Arbitrary combinations of these operations are
too general to be represented efficiently in an ILP.
In particular, we need to compute the length of a
sentence and the concepts it covers for all compres-
sion candidates. Thus, we insist that the operations
can only affect non-overlapping spans of text, and
end up with a tree representation of each sentence:
Nodes correspond to compression operations and
leaves map to the words. Each node holds the length
it contributes to the sentence recursively, as the sum
of the lengths of its children. Similarly, the concepts
covered by a node are the union of the concepts cov-
ered by its children. When a node is activated in the
ILP, we consider that the text attached to it is present
in the summary and update the length constraint and
concept selection accordingly. Figure 3 gives an ex-
ample of this tree representation for a sentence from
the TAC data, showing the derivations of some com-
pressed candidates.
For a given sentence j, let Nj be the set of nodes
in its compression tree, Ej ? Nj be the set of
nodes that can be extracted (used as independent
sentences), Rj ? Nj be the set of nodes that can
be removed, and Sj ? Nj be the set of substitu-
tion group nodes. Let x and y be nodes from Nj ; we
create binary variables nx and ny to represent the in-
clusion of x or y in the summary. Let x ? y denote
the fact that x ? Nj is a direct parent of y ? Nj .
The constraints corresponding to the compression
tree are:
?
x?Ej
nx ? 1 ?j (3)
?
x?y
ny = nx ?x ? Sj ?j (4)
nx ? ny ?(y ? x ? x /? {Rj ? Sj}) ?j (5)
nx ? ny ?(y ? x ? x /? {Ej ? Sj}) ?j (6)
Eq. (3) enforces that only one sub-sentence is ex-
tracted from the original sentence; eq. (4) enforces
that one child of a substitution group is selected if
and only if the substitution node is selected; eq. (5)
ensures that a child node is selected when its parent
is selected unless the child is removable (or a substi-
tution group); eq. (6) ensures that if a child node is
selected, its parent is also selected unless the child is
an extraction node (that can be used as a root).
Each node is associated with the words and the
concepts it contains directly (which are not con-
tained by a child node) in order to compute the new
length constraints and activate concepts in the ob-
jective function. We set Occix to represent the oc-
currence of concept i in node x as a direct child.
Let lx be the length contributed to node x as direct
children. The resulting ILP for performing sentence
compression jointly with sentence selection is:
Maximize:
?
i
wici
Subject to:
?
j
lxnx ? L
nxOccix ? ci, ?i, x?
x
nxOccix ? ci ?i
idem constraints (3) to (6)
ci ? {0, 1} ?i
nx ? {0, 1} ?x
While this framework can be used to imple-
ment a wide range of compression techniques, we
choose to derive the compression tree from the
sentence?s parse tree, extracted with the Berkeley
parser (Petrov and Klein, 2007), and use a set of
rules to label parse tree nodes with compression op-
erations. For example, declarative clauses contain-
ing a subject and a verb are labeled with the extract
(E) operation; adverbial clauses and non-mandatory
prepositional clauses are labeled with the remove
(R) operation; Acronyms can be replaced by their
full form by using substitution (S) operations and a
primitive form of co-reference resolution is used to
allow the substitution of noun phrases by their refer-
ent.
System R-2 Pyr. LQ
No comp. 0.110 0.345 2.479
Comp. 0.111 0.323 2.021
Table 3: Scores of the system with and without sentence
compression included in the ILP (TAC?08 Set A data).
When implemented in the system presented in
section 4, this approach gives a slight improvement
15
countries are    planning to hold the euroas part of their    reserves
the magazine quoted    chief Wilm Disenbergas sayingalready (ECB | European Central Bank)foreign currency
(1):E(2):E
(6):R (7):R(8):R
(4):R(3):S
A number of(5):R
Node Len. Concepts
(1):E 6 {the magazine, magazine quoted, chief Wilm, Wilm Disenberg}
(2):E 7 {countries are, planning to, to hold, hold the, the euro}
(3):S 0 {}
(3a) 1 {ECB}
(3b) 3 {European Central, Central Bank}
(4):R 2 {as saying}
(5):R 3 {a number, number of}
(6):R 1 {}
(7):R 5 {as part, part of, reserves}
(8):R 2 {foreign currency}
? Original: A number of Countries
are already planning to hold the
euro as part of their foreign cur-
rency reserves, the magazine quoted
European Central Bank chief Wim
Duisenberg as saying.
? [1,2,5,3a]: A number of countries are
planning to hold the euro, the maga-
zine quoted ECB chief Wim Duisen-
berg.
? [2,5,6,7,8]: A number of countries
are already planning to hold the euro
as part of their foreign currency re-
serves.
? [2,7,8]: Countries are planning to
hold the euro as part of their foreign
currency reserves.
? [2]: Countries are planning to hold
the euro.
Figure 3: A compression tree for an example sentence. E-nodes (diamonds) can be extracted and used as an indepen-
dent sentences, R-nodes (circles) can be removed, and S-nodes (squares) contain substitution alternatives. The table
shows the word bigram concepts covered by each node and the length it contributes to the summary. Examples of
resulting compression candidates are given on the right side, with the list of nodes activated in their derivations.
in ROUGE-2 score (see Table 3), but a reduction in
Pyramid score. An analysis of the resulting sum-
maries showed that the rules used for implementing
sentence compression fail to ensure that all com-
pression candidates are valid sentences, and about
60% of the summaries contain ungrammatical sen-
tences. This is confirmed by the linguistic qual-
ity4 score drop for this system. The poor quality
of the compressed sentences explains the reduction
in Pyramid scores: Human judges tend to not give
credit to ungrammatical sentences because they ob-
scure the SCUs.
We have shown in this section how sentence com-
pression can be implemented in a more scalable way
under the concept-based model, but it remains to be
shown that such a technique can improve summary
quality.
7 Related work
In addition to proposing an ILP for the sentence-
level model, McDonald (2007) discusses a kind of
summary-level model: The score of a summary is
4As measured according to the TAC?08 guidelines.
determined by its cosine similarity to the collection
of input documents. Though this idea is only imple-
mented with approximate methods, it is similar in
spirit to our concept-based model since it relies on
weights for individual summary words rather than
sentences.
Using a maximum coverage model for summa-
rization is not new. Filatova (2004) formalizes the
idea, discussing its similarity to the classical NP-
hard problem, but in the end uses a greedy approxi-
mation to generate summaries. More recently, Yih et
al. (2007) employ a similar model and uses a stack
decoder to improve on a greedy search. Globally
optimal summaries are also discussed by Liu (2006)
and Jaoua Kallel (2004) who apply genetic algo-
rithms for finding selections of sentences that maxi-
mize summary-level metrics. Hassel (2006) uses hill
climbing to build summaries that maximize a global
information criterion based on random indexing.
The general idea of concept-level scoring for
summarization is employed in the SumBasic sys-
tem (Nenkova and Vanderwende, 2005), which
chooses sentences greedily according to the sum
of their word values (values are derived from fre-
16
quency). Conroy (2006) describes a bag-of-words
model, with the goal of approximating the distribu-
tion of words from the input documents in the sum-
mary. Others, like (Yih et al, 2007) train a model to
learn the value of each word from a set of features
including frequency and position. Filatova?s model
is most theoretically similar to ours, though the con-
cepts she chooses are ?events?.
8 Conclusion and Future Work
We have synthesized a number of ideas from
the field of automatic summarization, including
concept-level weighting, a maximum coverage
model to minimize redundancy globally, and sen-
tence compression derived from parse trees. While
an ILP formulation for summarization is not novel,
ours provides reasonably scalable, efficient solutions
for practical problems, including those in recent
TAC and DUC evaluations. We have also shown
how it can be extended to perform sentence com-
pression and sentence selection jointly.
In ROUGE and Pyramid evaluation, our system
significantly outperformed McDonald?s ILP sys-
tem. However, we would note that better design of
sentence-level scoring would likely yield better re-
sults as suggested by the success of greedy sentence-
based methods at the DUC and TAC conferences
(see for instance (Toutanova et al, 2007)). Still, the
performance of our system, on par with the current
state-of-the-art, is encouraging.
There are three principal directions for future
work. First, word bigram concepts are convenient,
but semantically unappealing. We plan to explore
concepts derived from parse trees, where weights
may be a function of frequency as well as hierar-
chical relationships.
Second, our current approach relies entirely on
word frequency, a reasonable proxy for relevance,
but likely inferior to learning weights from train-
ing data. A number of systems have shown im-
provements by learning word values, though prelim-
inary attempts to improve on our frequency heuristic
by learning bigram values have not produced sig-
nificant gains. Better features may be necessary.
However, since the ILP gives optimal solutions so
quickly, we are more interested in discriminative
training where we learn weights for features that
push the resulting summaries in the right direction,
as opposed to the individual concept values.
Third, our rule-based sentence compression is
more of a proof of concept, showing that joint com-
pression and optimal selection is feasible. Better
statistical methods have been developed for produc-
ing high quality compression candidates (McDon-
ald, 2006), that maintain linguistic quality, some re-
cent work even uses ILPs for exact inference (Clarke
and Lapata, 2008). The addition of compressed sen-
tences tends to yield less coherent summaries, mak-
ing sentence ordering more important. We would
like to add constraints on sentence ordering to the
ILP formulation to address this issue.
Acknowledgments
This work is supported by the Defense Advanced
Research Projects Agency (DARPA) GALE project,
under Contract No. HR0011-06-C-0023. Any opin-
ions, findings, conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
References
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?Leary. 2006. Topic-focused multi-document sum-
marization using an approximate oracle score. In Pro-
ceedings of COLING/ACL.
Hoa Trang Dang and Karolina Owczarzak. 2008.
Overview of the TAC 2008 Update Summarization
Task. In Proceedings of Text Analysis Conference.
E. Filatova and V. Hatzivassiloglou. 2004. Event-based
extractive summarization. In Proceedings of ACL
Workshop on Summarization, volume 111.
G. Gallo, PL Hammer, and B. Simeone. 1980. Quadratic
knapsack problems. Mathematical Programming
Study, 12:132?149.
D. Gillick, B. Favre, and D. Hakkani-Tur. 2008. The
ICSI Summarization System at TAC 2008. In Pro-
ceedings of the Text Understanding Conference.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization by
sentence extraction. Proceedings of the ANLP/NAACL
Workshop on Automatic Summarization, pages 40?48.
17
Martin Hassel and Jonas Sjo?bergh. 2006. Towards
holistic summarization: Selecting summaries, not sen-
tences. In Proceedings of Language Resources and
Evaluation.
D.S. Hochbaum. 1996. Approximating covering and
packing problems: set cover, vertex cover, indepen-
dent set, and related problems. PWS Publishing Co.
Boston, MA, USA, pages 94?143.
E. Hovy, C.Y. Lin, and L. Zhou. 2005. A BE-based
multi-document summarizer with sentence compres-
sion. In Proceedings of Multilingual Summarization
Evaluation.
Fatma Jaoua Kallel, Maher Jaoua, Lamia Bel-
guith Hadrich, and Abdelmajid Ben Hamadou. 2004.
Summarization at LARIS Laboratory. In Proceedings
of the Document Understanding Conference.
Richard Manning Karp. 1972. Reducibility among com-
binatorial problems. Complexity of Computer Compu-
tations, 43:85?103.
Tibor Kiss and Jan Strunk. 2006. Unsupervised multi-
lingual sentence boundary detection. Computational
Linguistics, 32.
K. Knight and D. Marcu. 2000. Statistics-Based
Summarization-Step One: Sentence Compression. In
Proceedings of the National Conference on Artificial
Intelligence, pages 703?710. Menlo Park, CA; Cam-
bridge, MA; London; AAAI Press; MIT Press; 1999.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of the
Workshop on Text Summarization Branches Out (WAS
2004), pages 25?26.
D. Liu, Y. Wang, C. Liu, and Z. Wang. 2006. Multiple
Documents Summarization Based on Genetic Algo-
rithm. Lecture Notes in Computer Science, 4223:355.
N. Madnani, D. Zajic, B. Dorr, N.F. Ayan, and J. Lin.
2007. Multiple Alternative Sentence Compressions
for Automatic Text Summarization. In Proceed-
ings of the Document Understanding Conference at
NLT/NAACL.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
the 11th EACL, pages 297?304.
R. McDonald. 2007. A Study of Global Inference Al-
gorithms in Multi-document Summarization. Lecture
Notes in Computer Science, 4425:557.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings of HLT-NAACL.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Technical Report MSR-
TR-2005-101, Microsoft Research, Redmond, Wash-
ington.
A. Nenkova. 2008. Entity-driven rewrite for multidocu-
ment summarization. Proceedings of IJCNLP.
Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In AAAI 2007
(Nectar Track).
D. Pisinger, A.B. Rasmussen, and R. Sandvik. 2005.
Solution of large-sized quadratic knapsack problems
through aggressive reduction. INFORMS Journal on
Computing.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The Pythy
Summarization System: Microsoft Research at DUC
2007. In Proceedings of the Document Understanding
Conference.
J. Turner and E. Charniak. 2005. Supervised and Unsu-
pervised Learning for Sentence Compression. In Pro-
ceedings of ACL.
W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki.
2007. Multi-document summarization by maximiz-
ing informative content-words. In International Joint
Conference on Artificial Intelligence.
18
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 205?209,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
A New Entity Salience Task with Millions of Training Examples
Jesse Dunietz
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA
jdunietz@cs.cmu.edu
Dan Gillick
Google Research
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
dgillick@google.com
Abstract
Although many NLP systems are moving
toward entity-based processing, most still
identify important phrases using classi-
cal keyword-based approaches. To bridge
this gap, we introduce the task of entity
salience: assigning a relevance score to
each entity in a document. We demon-
strate how a labeled corpus for the task
can be automatically generated from a cor-
pus of documents and accompanying ab-
stracts. We then show how a classifier
with features derived from a standard NLP
pipeline outperforms a strong baseline by
34%. Finally, we outline initial experi-
ments on further improving accuracy by
leveraging background knowledge about
the relationships between entities.
1 Introduction
Information retrieval, summarization, and online
advertising rely on identifying the most important
words and phrases in web documents. While tradi-
tional techniques treat documents as collections of
keywords, many NLP systems are shifting toward
understanding documents in terms of entities. Ac-
cordingly, we need new algorithms to determine
the prominence ? the salience ? of each entity in
the document.
Toward this end, we describe three primary con-
tributions. First, we show how a labeled cor-
pus for this task can be automatically constructed
from a corpus of documents with accompanying
abstracts. We also demonstrate the validity of
the corpus with a manual annotation study. Sec-
ond, we train an entity salience model using fea-
tures derived from a coreference resolution sys-
tem. This model significantly outperforms a base-
line model based on sentence position. Third, we
suggest how our model can be improved by lever-
aging background information about the entities
and their relationships ? information not specifi-
cally provided in the document in question.
Our notion of salience is similar to that of Bogu-
raev and Kenney (1997): ?discourse objects with
high salience are the focus of attention?, inspired
by earlier work on Centering Theory (Walker et
al., 1998). Here we take a more empirical ap-
proach: salient entities are those that human read-
ers deem most relevant to the document.
The entity salience task in particular is briefly
alluded to by Cornolti et al. (2013), and addressed
in the context of Twitter messages by Meij et. al
(2012). It is also similar in spirit to the much more
common keyword extraction task (Tomokiyo and
Hurst, 2003; Hulth, 2003).
2 Generating an entity salience corpus
Rather than manually annotating a corpus, we au-
tomatically generate salience labels for an existing
corpus of document/abstract pairs. We derive the
labels using the assumption that the salient entities
will be mentioned in the abstract, so we identify
and align the entities in each text.
Given a document and abstract, we run a stan-
dard NLP pipeline on both. This includes a POS
tagger and dependency parser, comparable in ac-
curacy to the current Stanford dependency parser
(Klein and Manning, 2003); an NP extractor that
uses POS tags and dependency edges to identify
a set of entity mentions; a coreference resolver,
comparable to that of Haghighi and Klein, (2009)
for clustering mentions; and an entity resolver that
links entities to Freebase profiles. The entity re-
solver is described in detail by Lao, et al. (2012).
We then apply a simple heuristic to align the
entities in the abstract and document: Let M
E
be
the set of mentions of an entity E that are proper
names. An entityE
A
from the abstract aligns to an
entity E
D
from the document if the syntactic head
token of some mention in M
E
A
matches the head
token of some mention in M
E
D
. If E
A
aligns with
more than one document entity, we align it with
the document entity that appears earliest.
In general, aligning an abstract to its source doc-
ument is difficult (Daum?e III and Marcu, 2005).
205
We avoid most of this complexity by aligning only
entities with at least one proper-name mention, for
which there is little ambiguity. Generic mentions
like CEO or state are often more ambiguous, so re-
solving them would be closer to the difficult prob-
lem of word sense disambiguation.
Once we have entity alignments, we assume
that a document entity is salient only if it has
been aligned to some abstract entity. Ideally, we
would like to induce a salience ranking over enti-
ties. Given the limitations of short abstracts, how-
ever, we settle for binary classification, which still
captures enough salience information to be useful.
2.1 The New York Times corpus
Our corpus of document/abstract pairs is the anno-
tated New York Times corpus (Sandhaus, 2008).
It includes 1.8 million articles published between
January 1987 and June 2007; some 650,000 in-
clude a summary written by one of the newspa-
per?s library scientists. We selected a subset of the
summarized articles from 2003-2007 by filtering
out articles and summaries that were very short or
very long, as well as several special article types
(e.g., corrections and letters to the editor).
Our full labeled dataset includes 110,639 docu-
ments with 2,229,728 labeled entities; about 14%
are marked as salient. For comparison, the average
summary is about 6% of the length (in tokens) of
the associated article. We use the 9,719 documents
from 2007 as test data and the rest as training.
2.2 Validating salience via manual evaluation
To validate our alignment method for inferring en-
tity salience, we conducted a manual evaluation.
Two expert linguists discussed the task and gen-
erated a rubric, giving them a chance to calibrate
their scores. They then independently annotated
all detected entities in 50 random documents from
our corpus (a total of 744 entities), without read-
ing the accompanying abstracts. Each entity was
assigned a salience score in {1, 2, 3, 4}, where 1 is
most salient. We then thresholded the annotators?
scores as salient/non-salient for comparison to the
binary NYT labels.
Table 1 summarizes the agreement results, mea-
sured by Cohen?s kappa. The experts? agreement
is probably best described as moderate,
1
indicat-
ing that this is a difficult, subjective task, though
deciding on the most salient entities (with score 1)
is easier. Even without calibrating to the induced
1
For comparison, word sense disambiguation tasks have
reported agreement as low as ? = 0.3 (Yong and Foo, 1999).
NYT salience scores, the expert vs. NYT agree-
ment is close enough to the inter-expert agreement
to convince us that our induced labels are a rea-
sonable if somewhat noisy proxy for the experts?
definition of salience.
Comparison ?{1,2} ?{1}
A1 vs. A2 0.56 0.69
A1 vs. NYT 0.36 0.48
A2 vs. NYT 0.39 0.35
A1 & A2 vs. NYT 0.43 0.38
Table 1: Annotator agreement for entity salience
as a binary classification. A1 and A2 are expert an-
notators; NYT represents the induced labels. The
first ? column assumes annotator scores {1, 2} are
salient and {3, 4} are non-salient, while the second
? column assumes only scores of 1 are salient.
3 Salience classification
We built a regularized binary logistic regression
model to predict the probability that an entity is
salient. To simplify feature selection and to add
some further regularization, we used feature hash-
ing (Ganchev and Dredze, 2008) to randomly map
each feature string to an integer in [1, 100000];
larger alphabet sizes yielded no improvement. The
model was trained with L-BGFS.
3.1 Positional baseline
For news documents, it is well known that sen-
tence position is a very strong indicator for rele-
vance. Thus, our baseline is a system that identi-
fies an entity as salient if it is mentioned in the first
sentence of the document. (Including the next few
sentences did not significantly change the score.)
3.2 Model features
Table 2 describes our feature classes; each indi-
vidual feature in the model is a binary indicator.
Count features are bucketed by applying the func-
tion f(x) = round(log(k(x + 1))), where k can
be used to control the number of buckets. We sim-
ply set k = 10 in all cases.
3.3 Experimental results
Table 3 shows experimental results on our test set.
Each experiment uses a classification threshold of
0.3 to determine salience, which in each case is
very close to the threshold that maximizes F
1
. For
comparison, a classifier that always predicts the
majority class, non-salient, has F
1
= 23.9 (for the
salient class).
206
Feature name Description
1st-loc Index of the sentence in
which the first mention of the
entity appears.
head-count Number of times the head
word of the entity?s first men-
tion appears.
mentions Conjuction of the numbers
of named (Barack Obama),
nominal (president), pronom-
inal (he), and total mentions
of the entity.
headline POS tag of each word that ap-
pears in at least one mention
and also in the headline.
head-lex Lowercased head word of the
first mention.
Table 2: The feature classes used by the classifier.
Lines 2 and 3 serve as a comparison between
traditional keyword counts and the mention counts
derived from our coreference resolution system.
Named, nominal, and pronominal mention counts
clearly add significant information despite coref-
erence errors. Lines 4-8 show results when our
model features are incrementally added. Each fea-
ture raises accuracy, and together our simple set of
features improves on the baseline by 34%.
4 Entity centrality
All the features described above use only infor-
mation available within the document. But arti-
cles are written with the assumption that the reader
knows something about at least some of the enti-
ties involved. Inspired by results using Wikipedia
to improve keyword extraction tasks (Mihalcea
and Csomai, 2007; Xu et al., 2010), we experi-
mented with a simple method for including back-
ground knowledge about each entity: an adapta-
tion of PageRank (Page et al., 1999) to a graph
of connected entities, in the spirit of Erkan and
Radev?s work (2004) on summarization.
Consider, for example, an article about a recent
congressional budget debate. Although House
Speaker John Boehner may be mentioned just
once, we know he is likely salient because he is
closely related to other entities in the article, such
as Congress, the Republican Party, and Barack
Obama. On the other hand, the Federal Emer-
gency Management Agency may be mentioned re-
peatedly because it happened to host a major pres-
idential speech, but it is less related to the story?s
# Description P R F
1
1 Positional baseline 59.5 37.8 46.2
2 head-count 37.3 54.7 44.4
3 mentions 57.2 51.3 54.1
4 1st-loc 46.1 60.2 52.2
5 + head-count 52.6 63.4 57.5
6 + mentions 59.3 61.3 60.3
7 + headline 59.1 61.9 60.5
8 + head-lex 59.7 63.6 61.6
9 + centrality 60.5 63.5 62.0
Table 3: Test set (P)recision, (R)ecall, and (F)
measure of the salient class for some com-
binations of features listed in Table 2. The
centrality feature is discussed in Section 4.
key figures and less central to the article?s point.
Our intuition about these relationships, mostly
not explicit in the document, can be formalized in
a local PageRank computation on the entity graph.
4.1 PageRank for computing centrality
In the weighted version of the PageRank algorithm
(Xing and Ghorbani, 2004), a web link is con-
sidered a weighted vote by the containing page
for the landing page ? a directed edge in a graph
where each node is a webpage. In place of the web
graph, we consider the graph of Freebase entities
that appear in the document. The nodes are the
entities, and a directed edge from E
1
to E
2
repre-
sents P (E
2
|E
1
), the probability of observing E
2
in a document given that we have observed E
1
.
We estimate P (E
2
|E
1
) by counting the number of
training documents in which E
1
and E
2
co-occur
and normalizing by the number of training docu-
ments in which E
1
occurs.
The nodes? initial PageRank values act as a
prior, where the uniform distribution, used in the
classic PageRank algorithm, indicates a lack of
prior knowledge. Since we have some prior sig-
nal about salience, we initialize the node values to
the normalized mention counts of the entities in
the document. We use a damping factor d, allow-
ing random jumps between nodes with probability
1? d, with the standard value d = 0.85.
We implemented the iterative version of
weighted PageRank, which tends to converge in
under 10 iterations. The centrality features
in Table 3 are indicators for the rank orders of the
converged entity scores. The improvement from
adding centrality features is small but statistically
significant at p ? 0.001.
207
ObamaObama
Boehner
FEMA
Boehner
FEMA
Republican
Party
Republican
Party
Figure 1: A graphical representation of the centrality computation on a toy example. Circle size and
arrow thickness represent node value and edge weight, respectively. The initial node values, based on
mention count, are shown on the left. The final node values are on the right; dotted circles show the
initial sizes for comparison. Edge weights remain constant.
4.2 Discussion
We experimented with a number of variations on
this algorithm, but none gave much meaningful
improvement. In particular, we tried to include
the neighbors of all entities to increase the size
of the graph, with the values of neighbor enti-
ties not in the document initialized to some small
value k. We set a minimum co-occurrence count
for an edge to be included, varying it from 1
to 100 (where 1 results in very large graphs).
We also tried using Freebase relations between
entities (rather than raw co-occurrence counts)
to determine the set of neighbors. Finally, we
experimented with undirected graphs using un-
normalized co-occurrence counts.
While the ranked centrality scores look reason-
able for most documents, the addition of these fea-
tures does not produce a substantial improvement.
One potential problem is our reliance on the entity
resolver. Because the PageRank computation links
all of a document?s entities, a single resolver error
can significantly alter all the centrality scores. Per-
haps more importantly, the resolver is incomplete:
many tail entities are not included in Freebase.
Still, it seems likely that even with perfect reso-
lution, entity centrality would not significantly im-
prove the accuracy of our model. The mentions
features are sufficiently powerful that entity cen-
trality seems to add little information to the model
beyond what these features already provide.
5 Conclusions
We have demonstrated how a simple alignment
of entities in documents with entities in their ac-
companying abstracts provides salience labels that
roughly agree with manual salience annotations.
This allows us to create a large corpus ? over
100,000 labeled documents with over 2 million la-
beled entities ? that we use to train a classifier for
predicting entity salience.
Our experiments show that features derived
from a coreference system are more robust than
simple word count features typical of a keyword
extraction system. These features combine nicely
with positional features (and a few others) to give
a large improvement over a first-sentence baseline.
There is likely significant room for improve-
ment, especially by leveraging background infor-
mation about the entities, and we have presented
some initial experiments in that direction. Perhaps
features more directly linked to Wikipedia, as in
related work on keyword extraction, can provide
more focused background information.
We believe entity salience is an important task
with many applications. To facilitate further re-
search, our automatically generated salience an-
notations, along with resolved entity ids, for the
subset of the NYT corpus discussed in this paper
are available here:
https://code.google.com/p/nyt-salience/
208
References
Branimir Boguraev and Christopher Kennedy. 1997.
Salience-based content characterisation of text doc-
uments. In Proceedings of the ACL, volume 97,
pages 2?9.
Marco Cornolti, Paolo Ferragina, and Massimiliano
Ciaramita. 2013. A framework for benchmarking
entity-annotation systems. In Proceedings of the
22nd international conference on World Wide Web,
pages 249?260.
Hal Daum?e III and Daniel Marcu. 2005. Induction
of word and phrase alignments for automatic doc-
ument summarization. Computational Linguistics,
31(4):505?530.
G?unes Erkan and Dragomir R Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research (JAIR), 22(1):457?479.
Kuzman Ganchev and Mark Dredze. 2008. Small sta-
tistical models by random feature mixing. In Pro-
ceedings of the ACL08 HLT Workshop on Mobile
Language Processing, pages 19?20.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3, pages 1152?1161. Asso-
ciation for Computational Linguistics.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, pages 216?223.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1017?1026. Association for Computational Linguis-
tics.
Edgar Meij, Wouter Weerkamp, and Maarten de Ri-
jke. 2012. Adding semantics to microblog posts.
In Proceedings of the fifth ACM international con-
ference on Web search and data mining, pages 563?
572. ACM.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
linking documents to encyclopedic knowledge. In
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, pages 233?242.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The PageRank citation
ranking: Bringing order to the web. Technical Re-
port 1999-66, Stanford InfoLab.
Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium, Philadelphia,
6(12):e26752.
Takashi Tomokiyo and Matthew Hurst. 2003. A
language model approach to keyphrase extraction.
In Proceedings of the ACL 2003 workshop on
Multiword expressions: analysis, acquisition and
treatment-Volume 18, pages 33?40.
Marilyn A Walker, Aravind Krishna Joshi, and
Ellen Friedman Prince. 1998. Centering theory in
discourse. Oxford University Press.
Wenpu Xing and Ali Ghorbani. 2004. Weighted page-
rank algorithm. In Communication Networks and
Services Research, pages 305?314. IEEE.
Songhua Xu, Shaohui Yang, and Francis Chi-Moon
Lau. 2010. Keyword extraction and headline gener-
ation using novel word features. In AAAI.
Chung Yong and Shou King Foo. 1999. A case study
on inter-annotator agreement for word sense disam-
biguation.
209
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 481?490,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Jointly Learning to Extract and Compress
Taylor Berg-Kirkpatrick Dan Gillick Dan Klein
Computer Science Division
University of California at Berkeley
{tberg, dgillick, klein}@cs.berkeley.edu
Abstract
We learn a joint model of sentence extraction
and compression for multi-document summa-
rization. Our model scores candidate sum-
maries according to a combined linear model
whose features factor over (1) the n-gram
types in the summary and (2) the compres-
sions used. We train the model using a margin-
based objective whose loss captures end sum-
mary quality. Because of the exponentially
large set of candidate summaries, we use a
cutting-plane algorithm to incrementally de-
tect and add active constraints efficiently. In-
ference in our model can be cast as an ILP
and thereby solved in reasonable time; we also
present a fast approximation scheme which
achieves similar performance. Our jointly
extracted and compressed summaries outper-
form both unlearned baselines and our learned
extraction-only system on both ROUGE and
Pyramid, without a drop in judged linguis-
tic quality. We achieve the highest published
ROUGE results to date on the TAC 2008 data
set.
1 Introduction
Applications of machine learning to automatic sum-
marization have met with limited success, and, as a
result, many top-performing systems remain largely
ad-hoc. One reason learning may have provided lim-
ited gains is that typical models do not learn to opti-
mize end summary quality directly, but rather learn
intermediate quantities in isolation. For example,
many models learn to score each input sentence in-
dependently (Teufel and Moens, 1997; Shen et al,
2007; Schilder and Kondadadi, 2008), and then as-
semble extractive summaries from the top-ranked
sentences in a way not incorporated into the learn-
ing process. This extraction is often done in the
presence of a heuristic that limits redundancy. As
another example, Yih et al (2007) learn predictors
of individual words? appearance in the references,
but in isolation from the sentence selection proce-
dure. Exceptions are Li et al (2009) who take a
max-margin approach to learning sentence values
jointly, but still have ad hoc constraints to handle
redundancy. One main contribution of the current
paper is the direct optimization of summary quality
in a single model; we find that our learned systems
substantially outperform unlearned counterparts on
both automatic and manual metrics.
While pure extraction is certainly simple and does
guarantee some minimal readability, Lin (2003)
showed that sentence compression (Knight and
Marcu, 2001; McDonald, 2006; Clarke and Lap-
ata, 2008) has the potential to improve the resulting
summaries. However, attempts to incorporate com-
pression into a summarization system have largely
failed to realize large gains. For example, Zajic et
al (2006) use a pipeline approach, pre-processing
to yield additional candidates for extraction by ap-
plying heuristic sentence compressions, but their
system does not outperform state-of-the-art purely
extractive systems. Similarly, Gillick and Favre
(2009), though not learning weights, do a limited
form of compression jointly with extraction. They
report a marginal increase in the automatic word-
overlap metric ROUGE (Lin, 2004), but a decline in
manual Pyramid (Nenkova and Passonneau, 2004).
A second contribution of the current work is to
show a system for jointly learning to jointly com-
press and extract that exhibits gains in both ROUGE
and content metrics over purely extractive systems.
Both Martins and Smith (2009) and Woodsend and
Lapata (2010) build models that jointly extract and
compress, but learn scores for sentences (or phrases)
using independent classifiers. Daume? III (2006)
481
learns parameters for compression and extraction
jointly using an approximate training procedure, but
his results are not competitive with state-of-the-art
extractive systems, and he does not report improve-
ments on manual content or quality metrics.
In our approach, we define a linear model that
scores candidate summaries according to features
that factor over the n-gram types that appear in the
summary and the structural compressions used to
create the sentences in the summary. We train these
parameters jointly using a margin-based objective
whose loss captures end summary quality through
the ROUGE metric. Because of the exponentially
large set of candidate summaries, we use a cutting
plane algorithm to incrementally detect and add ac-
tive constraints efficiently. To make joint learning
possible we introduce a new, manually-annotated
data set of extracted, compressed sentences. Infer-
ence in our model can be cast as an integer linear
program (ILP) and solved in reasonable time using
a generic ILP solver; we also introduce a fast ap-
proximation scheme which achieves similar perfor-
mance. Our jointly extracted and compressed sum-
maries outperform both unlearned baselines and our
learned extraction-only system on both ROUGE and
Pyramid, without a drop in judged linguistic quality.
We achieve the highest published comparable results
(ROUGE) to date on our test set.
2 Joint Model
We focus on the task of multi-document summariza-
tion. The input is a collection of documents, each
consisting of multiple sentences. The output is a
summary of length no greater than Lmax. Let x be
the input document set, and let y be a representation
of a summary as a vector. For an extractive sum-
mary, y is as a vector of indicators y = (ys : s ? x),
one indicator ys for each sentence s in x. A sentence
s is present in the summary if and only if its indica-
tor ys = 1 (see Figure 1a). Let Y (x) be the set of
valid summaries of document set x with length no
greater than Lmax.
While past extractive methods have assigned
value to individual sentences and then explicitly rep-
resented the notion of redundancy (Carbonell and
Goldstein, 1998), recent methods show greater suc-
cess by using a simpler notion of coverage: bigrams
Figure 1: Diagram of (a) extractive and (b) joint extrac-
tive and compressive summarization models. Variables
ys indicate the presence of sentences in the summary.
Variables yn indicate the presence of parse tree nodes.
Note that there is intentionally a bigram missing from (a).
contribute content, and redundancy is implicitly en-
coded in the fact that redundant sentences cover
fewer bigrams (Nenkova and Vanderwende, 2005;
Gillick and Favre, 2009). This later approach is as-
sociated with the following objective function:
max
y?Y (x)
?
b?B(y)
vb (1)
Here, vb is the value of bigram b, andB(y) is the set
of bigrams present in the summary encoded by y.
Gillick and Favre (2009) produced a state-of-the-art
system1 by directly optimizing this objective. They
let the value vb of each bigram be given by the num-
ber of input documents the bigram appears in. Our
implementation of their system will serve as a base-
line, referred to as EXTRACTIVE BASELINE.
We extend objective 1 so that it assigns value not
just to the bigrams that appear in the summary, but
also to the choices made in the creation of the sum-
mary. In our complete model, which jointly extracts
and compresses sentences, we choose whether or not
to cut individual subtrees in the constituency parses
1See Text Analysis Conference results in 2008 and 2009.
482
of each sentence. This is in contrast to the extractive
case where choices are made on full sentences.
max
y?Y (x)
?
b?B(y)
vb +
?
c?C(y)
vc (2)
C(y) is the set of cut choices made in y, and vc
assigns value to each.
Next, we present details of our representation of
compressive summaries. Assume a constituency
parse ts for every sentence s. We represent a com-
pressive summary as a vector y = (yn : n ? ts, s ?
x) of indicators, one for each non-terminal node in
each parse tree of the sentences in the document set
x. A word is present in the output summary if and
only if its parent parse tree node n has yn = 1 (see
Figure 1b). In addition to the length constraint on
the members of Y (x), we require that each node
n may have yn = 1 only if its parent pi(n) has
ypi(n) = 1. This ensures that only subtrees may
be deleted. While we use constituency parses rather
than dependency parses, this model has similarities
with the vine-growth model of Daume? III (2006).
For the compressive model we define the set of
cut choices C(y) for a summary y to be the set of
edges in each parse that are broken in order to delete
a subtree (see Figure 1b). We require that each sub-
tree has a non-terminal node for a root, and say that
an edge (n, pi(n)) between a node and its parent is
broken if the parent has ypi(n) = 1 but the child has
yn = 0. Notice that breaking a single edge deletes
an entire subtree.
2.1 Parameterization
Before learning weights in Section 3, we parameter-
ize objectives 1 and 2 using features. This entails to
parameterizing each bigram score vb and each sub-
tree deletion score vc. For weights w ? Rd and
feature functions g(b, x) ? Rd and h(c, x) ? Rd we
let:
vb = w
Tg(b, x)
vc = wTh(c, x)
For example, g(b, x) might include a feature the
counts the number of documents in x that b appears
in, and h(c, x) might include a feature that indicates
whether the deleted subtree is an SBAR modifying
a noun.
This parameterization allows us to cast summa-
rization as structured prediction. We can define a
feature function f(y, x) ? Rd which factors over
summaries y through B(y) and C(y):
f(y, x) =
?
b?B(y)
g(b, x) +
?
c?C(y)
h(c, x)
Using this characterization of summaries as feature
vectors we can define a linear predictor for summa-
rization:
d(x;w) = argmax
y?Y (x)
wTf(y, x) (3)
= argmax
y?Y (x)
?
b?B(y)
vb +
?
c?C(y)
vc
The arg max in Equation 3 optimizes Objective 2.
Learning weights for Objective 1 where Y (x) is
the set of extractive summaries gives our LEARNED
EXTRACTIVE system. Learning weights for Objec-
tive 2 where Y (x) is the set of compressive sum-
maries, and C(y) the set of broken edges that pro-
duce subtree deletions, gives our LEARNED COM-
PRESSIVE system, which is our joint model of ex-
traction and compression.
3 Structured Learning
Discriminative training attempts to minimize the
loss incurred during prediction by optimizing an ob-
jective on the training set. We will perform discrim-
inative training using a loss function that directly
measures end-to-end summarization quality.
In Section 4 we show that finding summaries that
optimize Objective 2, Viterbi prediction, is efficient.
Online learning algorithms like perceptron or the
margin-infused relaxed algorithm (MIRA) (Cram-
mer and Singer, 2003) are frequently used for struc-
tured problems where Viterbi inference is available.
However, we find that such methods are unstable on
our problem. We instead turn to an approach that
optimizes a batch objective which is sensitive to all
constraints on all instances, but is efficient by adding
these constraints incrementally.
3.1 Max-margin objective
For our problem the data set consists of pairs of doc-
ument sets and label summaries, D = {(xi,y?i ) :
i ? 1, . . . , N}. Note that the label summaries
483
can be expressed as vectors y? because our training
summaries are variously extractive or extractive and
compressive (see Section 5). We use a soft-margin
support vector machine (SVM) (Vapnik, 1998) ob-
jective over the full structured output space (Taskar
et al, 2003; Tsochantaridis et al, 2004) of extractive
and compressive summaries:
min
w
1
2
?w?2 +
C
N
N?
i=1
?i (4)
s.t. ?i,?y ? Y (xi) (5)
wT
(
f(y?i , xi)? f(y, xi)
)
? `(y,y?i )? ?i
The constraints in Equation 5 require that the differ-
ence in model score between each possible summary
y and the gold summary y?i be no smaller than the
loss `(y,y?i ), padded by a per-instance slack of ?i.
We use bigram recall as our loss function (see Sec-
tion 3.3). C is the regularization constant. When the
output space Y (xi) is small these constraints can be
explicitly enumerated. In this case it is standard to
solve the dual, which is a quadratic program. Un-
fortunately, the size of the output space of extractive
summaries is exponential in the number of sentences
in the input document set.
3.2 Cutting-plane algorithm
The cutting-plane algorithm deals with the expo-
nential number of constraints in Equation 5 by per-
forming constraint induction (Tsochantaridis et al,
2004). It alternates between solving Objective 4
with a reduced set of currently active constraints,
and adding newly active constraints to the set. In
our application, this approach efficiently solves the
structured SVM training problem up to some speci-
fied tolerance .
Suppose w? and ?? optimize Objective 4 under the
currently active constraints on a given iteration. No-
tice that the y?i satisfying
y?i = argmax
y?Y (xi)
[
w?Tf(y, xi) + `(y,y?i )
]
(6)
corresponds to the constraint in the fully constrained
problem, for training instance (xi,y?i ), most vio-
lated by w? and ??. On each round of constraint induc-
tion the cutting-plane algorithm computes the arg
max in Equation 6 for a training instance, which is
referred to as loss-augmented prediction, and adds
the corresponding constraint to the active set.
The constraints from Equation (5) are equivalent
to: ?i wTf(y?i , xi) ? maxy?Y (xi)
[
wTf(y, xi) +
`(y,y?i )
]
? ?i. Thus, if loss-augmented prediction
turns up no new constraints on a given iteration, the
current solution to the reduced problem, w? and ??,
is the solution to the full SVM training problem. In
practice, constraints are only added if the right hand
side of Equation (5) exceeds the left hand side by at
least . Tsochantaridis et al (2004) prove that only
O(N ) constraints are added before constraint induc-
tion finds a C-optimal solution.
Loss-augmented prediction is not always
tractable. Luckily, our choice of loss function,
bigram recall, factors over bigrams. Thus, we can
easily perform loss-augmented prediction using
the same procedure we use to perform Viterbi
prediction (described in Section 4). We simply
modify each bigram value vb to include bigram
b?s contribution to the total loss. We solve the
intermediate partially-constrained max-margin
problems using the factored sequential minimal
optimization (SMO) algorithm (Platt, 1999; Taskar
et al, 2004). In practice, for  = 10?4, the
cutting-plane algorithm converges after only three
passes through the training set when applied to our
summarization task.
3.3 Loss function
In the simplest case, 0-1 loss, the system only re-
ceives credit for exactly identifying the label sum-
mary. Since there are many reasonable summaries
we are less interested in exactly matching any spe-
cific training instance, and more interested in the de-
gree to which a predicted summary deviates from a
label.
The standard method for automatically evaluating
a summary against a reference is ROUGE, which we
simplify slightly to bigram recall. With an extractive
reference denoted by y?, our loss function is:
`(y,y?) =
|B(y)
?
B(y?)|
|B(y?)|
We verified that bigram recall correlates well with
ROUGE and with manual metrics.
484
4 Efficient Prediction
We show how to perform prediction with the extrac-
tive and compressive models by solving ILPs. For
many instances, a generic ILP solver can find exact
solutions to the prediction problems in a matter of
seconds. For difficult instances, we present a fast
approximate algorithm.
4.1 ILP for extraction
Gillick and Favre (2009) express the optimization of
Objective 1 for extractive summarization as an ILP.
We begin here with their algorithm. Let each input
sentence s have length ls. Let the presence of each
bigram b inB(y) be indicated by the binary variable
zb. LetQsb be an indicator of the presence of bigram
b in sentence s. They specify the following ILP over
binary variables y and z:
max
y,z
?
b
vbzb
s.t.
?
s
lsys ? Lmax
?b
?
s
Qsb ? zb (7)
?s, b ysQsb ? zb (8)
Constraints 7 and 8 ensure consistency between sen-
tences and bigrams. Notice that the Constraint 7 re-
quires that selecting a sentence entails selecting all
its bigrams, and Constraint 8 requires that selecting
a bigram entails selecting at least one sentence that
contains it. Solving the ILP is fast in practice. Us-
ing the GNU Linear Programming Kit (GLPK) on
a 3.2GHz Intel machine, decoding took less than a
second on most instances.
4.2 ILP for joint compression and extraction
We can extend the ILP formulation of extraction
to solve the compressive problem. Let ln be the
number of words node n has as children. With
this notation we can write the length restriction as
?
n lnyn ? Lmax. Let the presence of each cut c in
C(y) be indicated by the binary variable zc, which
is active if and only if yn = 0 but ypi(n) = 1, where
node pi(n) is the parent of node n. The constraints
on zc are diagrammed in Figure 2.
While it is possible to let B(y) contain all bi-
grams present in the compressive summary, the re-
Figure 2: Diagram of ILP for joint extraction and com-
pression. Variables zb indicate the presence of bigrams
in the summary. Variables zc indicate edges in the parse
tree that have been cut in order to remove subtrees. The
figure suppresses bigram variables zstopped,in and zfrance,he
to reduce clutter. Note that the edit shown is intentionally
bad. It demonstrates a loss of bigram coverage.
duction of B(y) makes the ILP formulation effi-
cient. We omit fromB(y) bigrams that are the result
of deleted intermediate words. As a result the re-
quired number of variables zb is linear in the length
of a sentence. The constraints on zb are given in
Figure 2. They can be expressed in terms of the vari-
ables yn.
By solving the following ILP we can compute the
arg max required for prediction in the joint model:
max
y,z
?
b
vbzb +
?
c
vczc
s.t.
?
n
lnyn ? Lmax
?n yn ? ypi(n) (9)
?b zb = 1
[
b ? B(y)
]
(10)
?c zc = 1
[
c ? C(y)
]
(11)
485
Constraint 9 encodes the requirement that only full
subtrees may be deleted. For simplicity, we have
written Constraints 10 and 11 in implicit form.
These constraints can be encoded explicitly using
O(N) linear constraints, where N is the number
of words in the document set x. The reduction of
B(y) to include only bigrams not resulting from
deleted intermediate words avoids O(N2) required
constraints.
In practice, solving this ILP for joint extraction
and compression is, on average, an order of magni-
tude slower than solving the ILP for pure extraction,
and for certain instances finding the exact solution is
prohibitively slow.
4.3 Fast approximate prediction
One common way to quickly approximate an ILP
is to solve its LP relaxation (and round the results).
We found that, while very fast, the LP relaxation of
the joint ILP gave poor results, finding unacceptably
suboptimal solutions. This appears possibly to have
been problematic for Martins and Smith (2009) as
well. We developed an alternative fast approximate
joint extractive and compressive solver that gives
better results in terms of both objective value and
bigram recall of resulting solutions.
The approximate joint solver first extracts a sub-
set of the sentences in the document set that total no
more than M words. In a second step, we apply the
exact joint extractive and compressive summarizer
(see Section 4.2) to the resulting extraction. The ob-
jective we maximize in performing the initial extrac-
tion is different from the one used in extractive sum-
marization. Specifically, we pick an extraction that
maximizes
?
s?y
?
b?s vb. This objective rewards
redundant bigrams, and thus is likely to give the joint
solver multiple options for including the same piece
of relevant content.
M is a parameter that trades-off between approx-
imation quality and problem difficulty. When M
is the size of the document set x, the approximate
solver solves the exact joint problem. In Figure 3
we plot the trade-off between approximation quality
and computation time, comparing to the exact joint
solver, an exact solver that is limited to extractive
solutions, and the LP relaxation solver. The results
show that the approximate joint solver yields sub-
stantial improvements over the LP relaxation, and
Figure 3: Plot of objective value, bigram recall, and
elapsed time for the approximate joint extractive and
compressive solver against size of intermediate extraction
set. Also shown are values for an LP relaxation approx-
imate solver, a solver that is restricted to extractive so-
lutions, and finally the exact compressive solver. These
solvers do not use an intermediate extraction. Results are
for 44 document sets, averaging about 5000 words per
document set.
can achieve results comparable to those produced by
the exact solver with a 5-fold reduction in compu-
tation time. On particularly difficult instances the
parameter M can be decreased, ensuring that all in-
stances are solved in a reasonable time period.
5 Data
We use the data from the Text Analysis Conference
(TAC) evaluations from 2008 and 2009, a total of
92 multi-document summarization problems. Each
problem asks for a 100-word-limited summary of
10 related input documents and provides a set of
four abstracts written by experts. These are the non-
update portions of the TAC 2008 and 2009 tasks.
To train the extractive system described in Sec-
tion 2, we use as our labels y? the extractions with
the largest bigram recall values relative to the sets
of references. While these extractions are inferior
to the abstracts, they are attainable by our model, a
quality found to be advantageous in discriminative
training for machine translation (Liang et al, 2006;
486
COUNT: 1(docCount(b) = ?) where docCount(b) is the
number of documents containing b.
STOP: 1(isStop(b1) = ?, isStop(b2) = ?) where
isStop(w) indicates a stop word.
POSITION: 1(docPosition(b) = ?) where docPosition(b) is
the earliest position in a document of any sen-
tence containing b, buckets earliest positions? 4.
CONJ: All two- and three-way conjunctions of COUNT,
STOP, and POSITION features.
BIAS: Bias feature, active on all bigrams.
Table 1: Bigram features: component feature functions
in g(b, x) that we use to characterize the bigram b in both
the extractive and compressive models.
Chiang et al, 2008).
Previous work has referred to the lack of ex-
tracted, compressed data sets as an obstacle to joint
learning for summarizaiton (Daume? III, 2006; Mar-
tins and Smith, 2009). We collected joint data via
a Mechanical Turk task. To make the joint anno-
tation task more feasible, we adopted an approx-
imate approach that closely matches our fast ap-
proximate prediction procedure. Annotators were
shown a 150-word maximum bigram recall extrac-
tions from the full document set and instructed to
form a compressed summary by deleting words un-
til 100 or fewer words remained. Each task was per-
formed by two annotators. We chose the summary
we judged to be of highest quality from each pair
to add to our corpus. This gave one gold compres-
sive summary y? for each of the 44 problems in the
TAC 2009 set. We used these labels to train our joint
extractive and compressive system described in Sec-
tion 2. Of the 288 total sentences presented to anno-
tators, 38 were unedited, 45 were deleted, and 205
were compressed by an average of 7.5 words.
6 Features
Here we describe the features used to parameterize
our model. Relative to some NLP tasks, our fea-
ture sets are small: roughly two hundred features
on bigrams and thirteen features on subtree dele-
tions. This is because our data set is small; with
only 48 training documents we do not have the sta-
tistical support to learn weights for more features.
For larger training sets one could imagine lexical-
ized versions of the features we describe.
COORD: Indicates phrase involved in coordination. Four
versions of this feature: NP, VP, S, SBAR.
S-ADJUNCT: Indicates a child of an S, adjunct to and left of
the matrix verb. Four version of this feature:
CC, PP, ADVP, SBAR.
REL-C: Indicates a relative clause, SBAR modifying a
noun.
ATTR-C: Indicates a sentence-final attribution clause,
e.g. ?the senator announced Friday.?
ATTR-PP: Indicates a PP attribution, e.g. ?according to the
senator.?
TEMP-PP: Indicates a temporal PP, e.g. ?on Friday.?
TEMP-NP: Indicates a temporal NP, e.g. ?Friday.?
BIAS: Bias feature, active on all subtree deletions.
Table 2: Subtree deletion features: component feature
functions in h(c, x) that we use to characterize the sub-
tree deleted by cutting edge c = (n, pi(n)) in the joint
extractive and compressive model.
6.1 Bigram features
Our bigram features include document counts, the
earliest position in a document of a sentence that
contains the bigram, and membership of each word
in a standard set of stopwords. We also include all
possible two- and three-way conjuctions of these
features. Table 1 describes the features in detail.
We use stemmed bigrams and prune bigrams that
appear in fewer than three input documents.
6.2 Subtree deletion features
Table 2 gives a description of our subtree tree dele-
tion features. Of course, by training to optimize a
metric like ROUGE, the system benefits from re-
strictions on the syntactic variety of edits; the learn-
ing is therefore more about deciding when an edit
is worth the coverage trade-offs rather than fine-
grained decisions about grammaticality.
We constrain the model to only allow subtree
deletions where one of the features in Table 2 (aside
from BIAS) is active. The root, and thus the entire
sentence, may always be cut. We choose this par-
ticular set of allowed deletions by looking at human
annotated data and taking note of the most common
types of edits. Edits which are made rarely by hu-
mans should be avoided in most scenarios, and we
simply don?t have enough data to learn when to do
them safely.
487
System BR R-2 R-SU4 Pyr LQ
LAST DOCUMENT 4.00 5.85 9.39 23.5 7.2
EXT. BASELINE 6.85 10.05 13.00 35.0 6.2
LEARNED EXT. 7.43 11.05 13.86 38.4 6.6
LEARNED COMP. 7.75 11.70 14.38 41.3 6.5
Table 3: Bigram Recall (BR), ROUGE (R-2 and R-SU4)
and Pyramid (Pyr) scores are multiplied by 100; Linguis-
tic Quality (LQ) is scored on a 1 (very poor) to 10 (very
good) scale.
7 Experiments
7.1 Experimental setup
We set aside the TAC 2008 data set (48 problems)
for testing and use the TAC 2009 data set (44 prob-
lems) for training, with hyper-parameters set to max-
imize six-fold cross-validation bigram recall on the
training set. We run the factored SMO algorithm
until convergence, and run the cutting-plane algo-
rithm until convergence for  = 10?4. We used
GLPK to solve all ILPs. We solved extractive ILPs
exactly, and joint extractive and compressive ILPs
approximately using an intermediate extraction size
of 1000. Constituency parses were produced using
the Berkeley parser (Petrov and Klein, 2007). We
show results for three systems, EXTRACTIVE BASE-
LINE, LEARNED EXTRACTIVE, LEARNED COM-
PRESSIVE, and the standard baseline that extracts
the first 100 words in the the most recent document,
LAST DOCUMENT.
7.2 Results
Our evaluation results are shown in Table 3.
ROUGE-2 (based on bigrams) and ROUGE-SU4
(based on both unigrams and skip-bigrams, sepa-
rated by up to four words) are given by the offi-
cial ROUGE toolkit with the standard options (Lin,
2004).
Pyramid (Nenkova and Passonneau, 2004) is a
manually evaluated measure of recall on facts or
Semantic Content Units appearing in the reference
summaries. It is designed to help annotators dis-
tinguish information content from linguistic qual-
ity. Two annotators performed the entire evaluation
without overlap by splitting the set of problems in
half.
To evaluate linguistic quality, we sent all the sum-
maries to Mechanical Turk (with two times redun-
System Sents Words/Sent Word Types
LAST DOCUMENT 4.0 25.0 36.5
EXT. BASELINE 5.0 20.8 36.3
LEARNED EXT. 4.8 21.8 37.1
LEARNED COMP. 4.5 22.9 38.8
Table 4: Summary statistics for the summaries gener-
ated by each system: Average number of sentences per
summary, average number of words per summary sen-
tence, and average number of non-stopword word types
per summary.
dancy), using the template and instructions designed
by Gillick and Liu (2010). They report that Turk-
ers can faithfully reproduce experts? rankings of av-
erage system linguistic quality (though their judge-
ments of content are poorer). The table shows aver-
age linguistic quality.
All the content-based metrics show substantial
improvement for learned systems over unlearned
ones, and we see an extremely large improvement
for the learned joint extractive and compressive sys-
tem over the previous state-of-the-art EXTRACTIVE
BASELINE. The ROUGE scores for the learned
joint system, LEARNED COMPRESSIVE, are, to our
knowledge, the highest reported on this task. We
cannot compare Pyramid scores to other reported
scores because of annotator difference. As expected,
the LAST DOCUMENT baseline outperforms other
systems in terms of linguistic quality. But, impor-
tantly, the gains achieved by the joint extractive and
compressive system in content-based metrics do not
come at the cost of linguistic quality when compared
to purely extractive systems.
Table 4 shows statistics on the outputs of the sys-
tems we evaluated. The joint extractive and com-
pressive system fits more word types into a sum-
mary than the extractive systems, but also produces
longer sentences on average. Reading the output
summaries more carefully suggests that by learning
to extract and compress jointly, our joint system has
the flexibility to use or create reasonable, medium-
length sentences, whereas the extractive systems are
stuck with a few valuable long sentences, but several
less productive shorter sentences. Example sum-
maries produced by the joint system are given in Fig-
ure 4 along with reference summaries produced by
humans.
488
LEARNED COMPRESSIVE: The country?s work safety authority will
release the list of the first batch of coal mines to be closed down said
Wang Xianzheng, deputy director of the National Bureau of Produc-
tion Safety Supervision and Administration. With its coal mining
safety a hot issue, attracting wide attention from both home and over-
seas, China is seeking solutions from the world to improve its coal
mining safety system. Despite government promises to stem the car-
nage the death toll in China?s disaster-plagued coal mine industry is
rising according to the latest statistics released by the government Fri-
day. Fatal coal mine accidents in China rose 8.5 percent in the first
eight months of this year with thousands dying despite stepped-up ef-
forts to make the industry safer state media said Wednesday.
REFERENCE: China?s accident-plagued coal mines cause thousands
of deaths and injuries annually. 2004 saw over 6,000 mine deaths.
January through August 2005, deaths rose 8.5% over the same period
in 2004. Most accidents are gas explosions, but fires, floods, and cave-
ins also occur. Ignored safety procedures, outdated equipment, and
corrupted officials exacerbate the problem. Official responses include
shutting down thousands of ill-managed and illegally-run mines, pun-
ishing errant owners, issuing new safety regulations and measures,
and outlawing local officials from investing in mines. China also
sought solutions at the Conference on South African Coal Mining
Safety Technology and Equipment held in Beijing.
LEARNED COMPRESSIVE: Karl Rove the White House deputy chief
of staff told President George W. Bush and others that he never en-
gaged in an effort to disclose a CIA operative?s identity to discredit
her husband?s criticism of the administration?s Iraq policy according
to people with knowledge of Rove?s account in the investigation. In a
potentially damaging sign for the Bush administration special counsel
Patrick Fitzgerald said that although his investigation is nearly com-
plete it?s not over. Lewis Scooter Libby Vice President Dick Cheney?s
chief of staff and a key architect of the Iraq war was indicted Friday on
felony charges of perjury making false statements to FBI agents and
obstruction of justice for impeding the federal grand jury investigating
the CIA leak case.
REFERENCE: Special Prosecutor Patrick Fitzgerald is investigating
who leaked to the press that Valerie Plame, wife of former Ambas-
sador Joseph Wilson, was an undercover CIA agent. Wilson was a
critic of the Bush administration. Administration staffers Karl Rove
and I. Lewis Libby are the focus of the investigation. NY Times cor-
respondent Judith Miller was jailed for 85 days for refusing to testify
about Libby. Libby was eventually indicted on five counts: 2 false
statements, 1 obstruction of justice, 2 perjury. Libby resigned imme-
diately. He faces 30 years in prison and a fine of $1.25 million if
convicted. Libby pleaded not guilty.
Figure 4: Example summaries produced by our learned
joint model of extraction and compression. These are
each 100-word-limited summaries of a collection of ten
documents from the TAC 2008 data set. Constituents that
have been removed via subtree deletion are grayed out.
References summaries produced by humans are provided
for comparison.
8 Conclusion
Jointly learning to extract and compress within a
unified model outperforms learning pure extraction,
which in turn outperforms a state-of-the-art extrac-
tive baseline. Our system gives substantial increases
in both automatic and manual content metrics, while
maintaining high linguistic quality scores.
Acknowledgements
We thank the anonymous reviewers for their com-
ments. This project is supported by DARPA under
grant N10AP20007.
References
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. of SIGIR.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.
J. Clarke and M. Lapata. 2008. Global Inference for Sen-
tence Compression: An Integer Linear Programming
Approach. Journal of Artificial Intelligence Research,
31:399?429.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research, 3:951?991.
H.C. Daume? III. 2006. Practical structured learning
techniques for natural language processing. Ph.D.
thesis, University of Southern California.
D. Gillick and B. Favre. 2009. A scalable global model
for summarization. In Proc. of ACL Workshop on In-
teger Linear Programming for Natural Language Pro-
cessing.
D. Gillick and Y. Liu. 2010. Non-Expert Evaluation of
Summarization Systems is Risky. In Proc. of NAACL
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
K. Knight and D. Marcu. 2001. Statistics-based
summarization-step one: Sentence compression. In
Proc. of AAAI.
L. Li, K. Zhou, G.R. Xue, H. Zha, and Y. Yu. 2009.
Enhancing diversity, coverage and balance for summa-
rization through structure learning. In Proc. of the 18th
International Conference on World Wide Web.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of the ACL.
489
C.Y. Lin. 2003. Improving summarization performance
by sentence compression: a pilot study. In Proc. of
ACL Workshop on Information Retrieval with Asian
Languages.
C.Y. Lin. 2004. Rouge: A package for automatic evalua-
tion of summaries. In Proc. of ACL Workshop on Text
Summarization Branches Out.
A.F.T. Martins and N.A. Smith. 2009. Summarization
with a joint model for sentence extraction and com-
pression. In Proc. of NAACL Workshop on Integer Lin-
ear Programming for Natural Language Processing.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proc. of EACL.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: The pyramid method.
In Proc. of NAACL.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Technical report, MSR-
TR-2005-101. Redmond, Washington: Microsoft Re-
search.
S. Petrov and D. Klein. 2007. Learning and inference for
hierarchically split PCFGs. In AAAI.
J.C. Platt. 1999. Fast training of support vector machines
using sequential minimal optimization. In Advances in
Kernel Methods. MIT press.
F. Schilder and R. Kondadadi. 2008. Fastsum: Fast and
accurate query-based multi-document summarization.
In Proc. of ACL.
D. Shen, J.T. Sun, H. Li, Q. Yang, and Z. Chen. 2007.
Document summarization using conditional random
fields. In Proc. of IJCAI.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. of NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. of EMNLP.
S. Teufel and M. Moens. 1997. Sentence extraction as
a classification task. In Proc. of ACL Workshop on
Intelligent and Scalable Text Summarization.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proc. of ICML.
V.N. Vapnik. 1998. Statistical learning theory. John
Wiley and Sons, New York.
K. Woodsend and M. Lapata. 2010. Automatic genera-
tion of story highlights. In Proc. of ACL.
W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki.
2007. Multi-document summarization by maximizing
informative content-words. In Proc. of IJCAI.
D.M. Zajic, B.J. Dorr, R. Schwartz, and J. Lin. 2006.
Sentence compression as a component of a multi-
document summarization system. In Proc. of the 2006
Document Understanding Workshop.
490
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 148?151,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Non-Expert Evaluation of Summarization Systems is Risky
Dan Gillick
University of California, Berkeley
Computer Science Division
dgillick@cs.berkeley.edu
Yang Liu
University of Texas, Dallas
Department of Computer Science
yangl@hlt.utdallas.edu
Abstract
We provide evidence that intrinsic evalua-
tion of summaries using Amazon?s Mechan-
ical Turk is quite difficult. Experiments mir-
roring evaluation at the Text Analysis Con-
ference?s summarization track show that non-
expert judges are not able to recover system
rankings derived from experts.
1 Introduction
Automatic summarization is a particularly difficult
task to evaluate. What makes a good summary?
What information is relevant? Is it possible to sepa-
rate information content from linguistic quality?
Besides subjectivity issues, evaluation is time-
consuming. Ideally, a judge would read the original
set of documents before deciding how well the im-
portant aspects are conveyed by a summary. A typ-
ical 10-document problem could reasonably involve
25 minutes of reading or skimming and 5 more min-
utes for assessing a 100-word summary. Since sum-
mary output can be quite variable, at least 30 top-
ics should be evaluated to get a robust estimate of
performance. Assuming a single judge evaluates all
summaries for a topic (more redundancy would be
better), we get a rough time estimate: 17.5 hours to
evaluate two systems.
Thus it is of great interest to find ways of speeding
up evaluation while minimizing subjectivity. Ama-
zon?s Mechanical Turk (MTurk) system has been
used for a variety of labeling and annotation tasks
(Snow et al, 2008), but such crowd-sourcing has not
been tested for summarization.
We describe an experiment to test whether MTurk
is able to reproduce system-level rankings that
match expert opinion. Unlike the results of other
crowd-sourcing annotations for natural language
tasks, we find that non-expert judges are unable to
provide expert-like scores and tend to disagree sig-
nificantly with each other.
This paper is organized as follows: Section 2 in-
troduces the particular summarization task and data
we use in our experiments; Section 3 describes the
design of our Human Intelligence Task (HIT). Sec-
tion 4 shows experimental results and gives some
analysis. Section 5 reviews our main findings and
provides suggestions for researchers wishing to con-
duct their own crowd-sourcing evaluations.
2 TAC Summarization Task
Topic: Peter Jennings
Description: Describe Peter Jennings? lung cancer and its
effects.
Reference: Peter Jennings?s announcement April 5, 2005,
that he had lung cancer left his colleagues at ABC News sad-
dened and dismayed. He had been ?World News Tonight?
anchorman since 1983. By the end of the week, ABC had re-
ceived 3,400 e-mails offering him prayers and good wishes.
A former heavy smoker, Jennings had not been well for some
time and was unable to travel abroad to cover foreign events.
However, his diagnosis came as a surprise to him. ABC an-
nounced that Jennings would continue to anchor the news
during chemotherapy treatment, but he was unable to do so.
Table 1: An example topic and reference summary from
the TAC 2009 summarization task.
Our data comes from the submissions to the Text
Analysis Conference (TAC) summarization track in
2009 (Dang, 2009). The main task involved 44
query-focused topics, each requiring a system to
produce a 100-word summary of 10 related news
documents. Experts provided four reference sum-
maries for each topic. Table 1 shows an example.
148
Score Difference
0 1 2 3 mean
OQ 119 92 15 0 0.54
LQ 117 82 20 7 0.63
Table 2: Identical summaries often were given different
scores by the same expert human judge at TAC 2009.
Counts of absolute score differences are shown for Over-
all Quality (OQ) and Linguistic Quality (LQ).
2.1 Agreement and consistency
In the official TAC evaluation, each summary was
judged by one of eight experts for ?Overall Quality?
and ?Linguistic Quality? on a 1 (?very poor?) to 10
(?very good?) scale. Unfortunately, the lack of re-
dundant judgments means we cannot estimate inter-
annotator agreement. However, we note that out of
all 4576 submitted summaries, there are 226 pairs
that are identical, which allows us to estimate anno-
tator consistency. Table 2 shows that an expert an-
notator will give the same summary the same score
just over half the time.
2.2 Evaluation without source documents
One way to dramatically speed up evaluation is to
use the experts? reference summaries as a gold stan-
dard, leaving the source documents out entirely.
This is the idea behind automatic evaluation with
ROUGE (Lin, 2004), which measures ngram over-
lap with the references, and assisted evaluation with
Pyramid (Nenkova and Passonneau, 2004), which
measures overlap of facts or ?Semantic Content
Units? with the references. The same idea has also
been employed in various manual evaluations, for
example by Haghighi and Vanderwende (2009), to
directly compare the summaries of two different sys-
tems. The potential bias introduced by such abbre-
viated evaluation has not been explored.
3 HIT design
The overall structure of the HIT we designed for
summary evaluation is as follows: The worker is
asked to read the topic and description, and then
two reference summaries (there is no mention of the
source documents). The candidate summary appears
next, followed by instructions to provide scores be-
tween 1 (very poor) and 10 (very good) in each cat-
egory1. Mouse-over on the category names provides
1Besides Overall Quality and Linguistic Quality, we include
Information Content, to encourage judges to distinguish be-
extra details, copied with slight modifications from
Dang (2007).
Our initial HIT design asked workers to perform
a head-to-head comparison of two candidate sum-
maries, but we found this unsatisfactory for a num-
ber of reasons. First, many of the resulting scores
did not obey the transitive property: given sum-
maries x, y, and z, a single worker showed a pref-
erence for y > x and z > y, but also x > z.
Second, while this kind of head-to-head evalua-
tion may be useful for system development, we are
specifically interested here in comparing non-expert
MTurk evaluation with expert TAC evaluation.
We went through a few rounds of revisions to the
language in the HIT after observing worker feed-
back. Specifically, we found it was important to em-
phasize that a good summary not only responds to
the topic and description, but also conveys the infor-
mation in the references.
3.1 Quality control
Only workers with at least a 96% HIT approval rat-
ing2 were allowed access to this task. We moni-
tored results manually and blocked workers (reject-
ing their work) if they completed a HIT in under 25
seconds. Such suspect work typically showed uni-
form scores (usually all 10s). Nearly 30% of HITs
were rejected for this reason.
To encourage careful work, we included this note
in our HITs: ?High annotator consistency is impor-
tant. If the scores you provide deviate from the av-
erage scores of other annotators on the same HIT,
your work will be rejected. We will award bonuses
for particularly good work.? We gave a few small
bonuses ($0.50) to workers who left thoughtful com-
ments.
3.2 Compensation
We experimented with a few different compensation
levels and observed a somewhat counter-intuitive re-
sult. Higher compensation ($.10 per HIT) yielded
lower quality work than lower compensation ($.07
per HIT), judging by the number of HITs we re-
jected. It seems that lower compensation attracts
workers who are less interested in making money,
and thus willing to spend more time and effort.
There is a trade-off, though, as there are fewer work-
ers willing to do the task for less money.
tween content and readability.
2MTurk approval ratings calculated as the fraction of HITs
approved by requesters.
149
Sys TAC MTurk
OQ LQ OQ LQ C
A 5.16 5.64 7.03 7.27 7.27
B 4.84 5.27 6.78 6.97 6.78
C 4.50 4.93 6.51 6.85 6.49
D 4.20 4.09 6.15 6.59 6.50
E 3.91 4.70 6.19 6.54 6.58
F 3.64 6.70 7.06 7.78 6.56
G 3.57 3.43 5.82 6.33 6.28
H 3.20 5.23 5.75 6.06 5.62
Table 3: Comparison of Overall Quality (OQ) and Lin-
guistic Quality (LQ) scores between the TAC and MTurk
evaluations. Content (C) is evaluated by MTurk workers
as well. Note that system F is the lead baseline.
4 Experiments and Analysis
To assess how well MTurk workers are able to em-
ulate the work of expert judges employed by TAC,
we chose a subset of systems and analyze the results
of the two evaluations. The systems were chosen to
represent the entire range of average Overall Qual-
ity scores. System F is a simple lead baseline, which
generates a summary by selecting the first sentences
up to 100 words of the most recent document. The
rest of the systems were submitted by various track
participants. The MTurk evaluation included two-
times redundancy. That is, each summary was eval-
uated by two different people. The cost for the full
evaluation, including 44 topics, 8 systems, and 2x
redundancy, at $.07 per HIT, plus 10% commission
for Amazon, was $55.
Table 3 shows average scores for the two evalu-
ations. The data suggest that the MTurk judges are
better at evaluating Linguistic Quality than Content
or Overall Quality. In particular, the MTurk judges
appear to have difficulty distinguishing Linguistic
Quality from Content. We will defend these claims
with more analysis, below.
4.1 Worker variability
The first important question to address involves the
consistency of the workers. We cannot compare
agreement between TAC and MTurk evaluations, but
the MTurk agreement statistics suggest considerable
variability. In Overall Quality, the mean score differ-
ence between two workers for the same HIT is 2.4
(the standard deviation is 2.0). The mean is 2.2 for
Linguistic Quality (the standard deviation is 1.5).
In addition, the TAC judges show more similarity
with each other?as if they are roughly in agreement
about what makes a good summary. We compute
each judge?s average score and look at the standard
deviation of these averages for the two groups. The
TAC standard deviation is 1.0 (ranging from 3.0 to
6.1), whereas the MTurk standard deviation is 2.3
(ranging from 1.0 to 9.5). Note that the average
number of HITs performed by each MTurk worker
was just over 5.
Finally, we can use regression analysis to show
what fraction of the total score variance is captured
by judges, topics, and systems. We fit linear models
in R using binary indicators for each judge, topic,
and system. Redundant evaluations in the MTurk
set are removed for unbiased comparison with the
TAC set. Table 4 shows that the differences between
the TAC and MTurk evaluations are quite striking:
Taking the TAC data alone, the topics are the major
source of variance, whereas the judges are the major
source of variance in the MTurk data. The systems
account for only a small fraction of the variance in
the MTurk evaluation, which makes system ranking
more difficult.
Eval Judges Topics Systems
TAC 0.28 0.40 0.13
MTurk 0.44 0.13 0.05
Table 4: Linear regression is used to model Overall Qual-
ity scores as a function of judges, topics, and systems, re-
spectively, for each data set. The R2 values, which give
the fraction of variance explained by each of the six mod-
els, are shown.
4.2 Ranking comparisons
The TAC evaluation, while lacking redundant judg-
ments, was a balanced experiment. That is, each
judge scored every system for a single topic. The
same is not true for the MTurk evaluation, and as
a result, the average per-system scores shown in
Table 3 may be biased. As a result, and because
we need to test multiple system-level differences si-
multaneously, a simple t-test is not quite sufficient.
We use Tukey?s Honestly Significant Differences
(HSD), explained in detail by Yandell (1997), to as-
sess statistical significance.
Tukey?s HSD test computes significance intervals
based on the range of the sample means rather than
individual differences, and includes an adjustment to
correct for imbalanced experimental designs. The R
implementation takes as input a linear model, so we
150
Eval Ranking
TAC (OQ) A B C DA EB FC GC HD
MTurk (OQ) F A B C EF GF DB HB
TAC (LQ) F AF BF HF CF EA DB GE
MTurk (LQ) F A BF CF DF EF HC GC
MTurk (C) A B E F D C GA HD
Table 5: Systems are shown in rank order from highest
(left) to lowest (right) for each scoring metric: Over-
all Quality (OQ), Linguistic Quality (LQ), and Content
(C). The superscripts indicate the rightmost system that
is significantly different (at 95% confidence) according
to Tukey?s HSD test.
model scores using binary indicators for (J)udges,
(T)opics, and (S)ystems (see equation 1), and mea-
sure significance in the differences between system
coefficients (?k).
score = ?+
?
i
?iJi +
?
j
?jTj +
?
k
?kSk (1)
Table 5 shows system rankings for the two evalu-
ations. The most obvious discrepancy between the
TAC and MTurk rankings is system F, the base-
line. Both TAC and MTurk judges gave F the high-
est scores for Linguistic Quality, a reasonable result
given its construction, whereas the other summaries
tend to pull sentences out of context. But the MTurk
judges also gave F the highest scores in Overall
Quality, suggesting that readability is more impor-
tant to amateur judges than experts, or at least easier
to identify. Content appears the most difficult cate-
gory for the MTurk judges, as few significant score
differences emerge. Even with more redundancy, it
seems unlikely that MTurk judges could produce a
ranking resembling the TAC Overall Quality rank-
ing using this evaluation framework.
5 Discussion
Through parallel evaluations by experts at TAC and
non-experts on MTurk, we have shown two main
results. First, as expected, MTurk workers pro-
duce considerably noisier work than experts. That
is, more redundancy is required to achieve statisti-
cal significance on par with expert judgments. This
finding matches prior work with MTurk. Second,
MTurk workers are unlikely to produce a score rank-
ing that matches expert rankings for Overall Quality.
This seems to be the result of some confusion in sep-
arating content from readability.
What does this mean for future evaluations? If
we want to assess overall summary quality?that is,
balancing content and linguistic quality like expert
judges do?we will need to redesign the task for
non-experts. Perhaps MTurk workers will be bet-
ter able to understand Nenkova?s Pyramid evaluation
(2004), which is designed to isolate content. Extrin-
sic evaluation, where judges use the summary to an-
swer questions derived from the source documents
or the references, as done by Callison-Burch for
evaluation of Machine Translation systems (2009),
is another possibility.
Finally, our results suggest that anyone conduct-
ing an evaluation of summarization systems using
non-experts should calibrate their results by asking
their judges to score summaries that have already
been evaluated by experts.
Acknowledgments
Thanks to Benoit Favre for discussing the evaluation
format and to the anonymous reviewers for helpful,
detailed feedback.
References
C. Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazons Me-
chanical Turk. Proceedings of EMNLP.
H.T. Dang. 2007. Overview of DUC 2007. In Proceed-
ings of the Document Understanding Conference.
H.T. Dang. 2009. Overview of the TAC 2009 opinion
question answering and summarization tasks. In Pro-
ceedings of Text Analysis Conference (TAC 2009).
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of HLT-NAACL.
C.Y. Lin. 2004. Rouge: A package for automatic evalu-
ation of summaries. In Proceedings of the Workshop:
Text Summarization Branches Out.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: The pyramid method.
In Proceedings of HLT-NAACL.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: Evaluating non-
expert annotations for natural language tasks. In Pro-
ceedings of EMNLP.
B.S. Yandell. 1997. Practical data analysis for designed
experiments. Chapman & Hall/CRC.
151
