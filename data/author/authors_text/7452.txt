Proceedings of NAACL HLT 2007, pages 372?379,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Applying Many-to-Many Alignments and Hidden Markov Models to
Letter-to-Phoneme Conversion
Sittichai Jiampojamarn, Grzegorz Kondrak and Tarek Sherif
Department of Computing Science,
University of Alberta,
Edmonton, AB, T6G 2E8, Canada
{sj,kondrak,tarek}@cs.ualberta.ca
Abstract
Letter-to-phoneme conversion generally
requires aligned training data of letters
and phonemes. Typically, the align-
ments are limited to one-to-one align-
ments. We present a novel technique of
training with many-to-many alignments.
A letter chunking bigram prediction man-
ages double letters and double phonemes
automatically as opposed to preprocess-
ing with fixed lists. We also apply
an HMM method in conjunction with
a local classification model to predict a
global phoneme sequence given a word.
The many-to-many alignments result in
significant improvements over the tradi-
tional one-to-one approach. Our system
achieves state-of-the-art performance on
several languages and data sets.
1 Introduction
Letter-to-phoneme (L2P) conversion requires a sys-
tem to produce phonemes that correspond to a given
written word. Phonemes are abstract representa-
tions of how words should be pronounced in natural
speech, while letters or graphemes are representa-
tions of words in written language. For example, the
phonemes for the word phoenix are [ f i n I k s ].
The L2P task is a crucial part of speech synthesis
systems, as converting input text (graphemes) into
phonemes is the first step in representing sounds.
L2P conversion can also help improve performance
in spelling correction (Toutanova and Moore, 2001).
Unfortunately, proper nouns and unseen words pre-
vent a table look-up approach. It is infeasible to con-
struct a lexical database that includes every word in
the written language. Likewise, orthographic com-
plexity of many languages prevents us from using
hand-designed conversion rules. There are always
exceptional rules that need to be added to cover a
large vocabulary set. Thus, an automatic L2P sys-
tem is desirable.
Many data-driven techniques have been proposed
for letter-to-phoneme conversion systems, including
pronunciation by analogy (Marchand and Damper,
2000), constraint satisfaction (Van Den Bosch and
Canisius, 2006), Hidden Markov Model (Taylor,
2005), decision trees (Black et al, 1998), and
neural networks (Sejnowski and Rosenberg, 1987).
The training data usually consists of written words
and their corresponding phonemes, which are not
aligned; there is no explicit information indicating
individual letter and phoneme relationships. These
relationships must be postulated before a prediction
model can be trained.
Previous work has generally assumed one-to-one
alignment for simplicity (Daelemans and Bosch,
1997; Black et al, 1998; Damper et al, 2005).
An expectation maximization (EM) based algo-
rithm (Dempster et al, 1977) is applied to train the
aligners. However, there are several problems with
this approach. Letter strings and phoneme strings
are not typically the same length, so null phonemes
and null letters must be introduced to make one-
to-one-alignments possible, Furthermore, two letters
frequently combine to produce a single phoneme
372
(double letters), and a single letter can sometimes
produce two phonemes (double phonemes).
To help address these problems, we propose an
automatic many-to-many aligner and incorporate it
into a generic classification predictor for letter-to-
phoneme conversion. Our many-to-many aligner
automatically discovers double phonemes and dou-
ble letters, as opposed to manually preprocessing
data by merging phonemes using fixed lists. To our
knowledge, applying many-to-many alignments to
letter-to-phoneme conversion is novel.
Once we have our many-to-many alignments, we
use that data to train a prediction model. Many
phoneme prediction systems are based on local pre-
diction methods, which focus on predicting an indi-
vidual phoneme given each letter in a word. Con-
versely, a method like pronunciation by analogy
(PbA) (Marchand and Damper, 2000) is considered
a global prediction method: predicted phoneme se-
quences are considered as a whole. Recently, Van
Den Bosch and Canisius (2006) proposed trigram
class prediction, which incorporates a constraint sat-
isfaction method to produce a global prediction for
letter-to-phoneme conversion. Both PbA and tri-
gram class prediction show improvement over pre-
dicting individual phonemes, confirming that L2P
systems can benefit from incorporating the relation-
ship between phonemes in a sequence.
In order to capitalize on the information found
in phoneme sequences, we propose to apply an
HMM method after a local phoneme prediction pro-
cess. Given a candidate list of two or more possible
phonemes, as produced by the local predictor, the
HMM will find the best phoneme sequence. Using
this approach, our system demonstrates an improve-
ment on several language data sets.
The rest of the paper is structured as follows.
We describe the letter-phoneme alignment methods
including a standard one-to-one alignment method
and our many-to-many approach in Section 2. The
alignment methods are used to align graphemes
and phonemes before the phoneme prediction mod-
els can be trained from the training examples. In
Section 3, we present a letter chunk prediction
method that automatically discovers double letters
in grapheme sequences. It incorporates our many-
to-many alignments with prediction models. In
Section 4, we present our application of an HMM
method to the local prediction results. The results
of experiments on several language data sets are dis-
cussed in Section 5. We conclude and propose future
work in Section 6.
2 Letter-phoneme alignment
2.1 One-to-one alignment
There are two main problems with one-to-one align-
ments:
1. Double letters: two letters map to one phoneme
(e.g. sh - [ S ], ph - [ f ]).
2. Double phonemes: one letter maps to two
phonemes (e.g. x - [ k s ], u - [ j u ]).
First, consider the double letter problem. In most
cases when the grapheme sequence is longer than
the phoneme sequence, it is because some letters are
silent. For example, in the word abode, pronounced
[ @ b o d ], the letter e produces a null phoneme (?).
This is well captured by one-to-one aligners. How-
ever, the longer grapheme sequence can also be gen-
erated by double letters; for example, in the word
king, pronounced [ k I N ], the letters ng together
produce the phoneme [ N ]. In this case, one-to-one
aligners using null phonemes will produce an in-
correct alignment. This can cause problems for the
phoneme prediction model by training it to produce
a null phoneme from either of the letters n or g.
In the double phoneme case, a new phoneme is
introduced to represent a combination of two (or
more) phonemes. For example, in the word fume
with phoneme sequence [ f j u m ], the letter u pro-
duces both the [ j ] and [ u ] phonemes. There
are two possible solutions for constructing a one-
to-one alignment in this case. The first is to cre-
ate a new phoneme by merging the phonemes [ j ]
and [ u ]. This requires constructing a fixed list of
new phonemes before beginning the alignment pro-
cess. The second solution is to add a null letter in
the grapheme sequence. However, the null letter not
only confuses the phoneme prediction model, but
also complicates the the phoneme generation phase.
For comparison with our many-to-many ap-
proach, we implement a one-to-one aligner based on
the epsilon scattering method (Black et al, 1998).
The method applies the EM algorithm to estimate
373
Algorithm 1: Pseudocode for a many-to-many
expectation-maximization algorithm.
Algorithm:EM-many2many
Input: xT , yV ,maxX,maxY
Output: ?
forall mapping operations z do
?(z) := 0
foreach sequence pair (xT , yV ) do
Expectation-many2many(xT , yV ,maxX,maxY, ?)
Maximization-Step(?)
the probability of mapping a letter l to a phoneme
p, P (l, p). The initial probability table starts by
mapping all possible alignments between letters and
phonemes for each word in the training data, in-
troducing all possible null phoneme positions. For
example, the word/phoneme-sequence pair abode
[ @ b o d ] has five possible positions where a null
phoneme can be added to make an alignment.
The training process uses the initial probability ta-
ble P (l, p) to find the best possible alignments for
each word using the Dynamic Time Warping (DTW)
algorithm (Sankoff and Kruskal, 1999). At each it-
eration, the probability table P (l, p) is re-calculated
based on the best alignments found in that iteration.
Finding the best alignments and re-calculating the
probability table continues iteratively until there is
no change in the probability table. The final proba-
bility table P (l, p) is used to find one-to-one align-
ments given graphemes and phonemes.
2.2 Many-to-Many alignment
We present a many-to-many alignment algorithm
that overcomes the limitations of one-to-one align-
ers. The training of the many-to-many aligner is
an extension of the forward-backward training of a
one-to-one stochastic transducer presented in (Ris-
tad and Yianilos, 1998). Partial counts are counts of
all possible mappings from letters to phonemes that
are collected in the ? table, while mapping probabil-
ities (initially uniform) are maintained in the ? table.
For each grapheme-/phoneme-sequence pair (x, y),
the EM-many2many function (Algorithm 1) calls the
Expectation-many2many function (Algorithm 2) to
collect partial counts. T and V are the lengths of x
and y respectively. The maxX and maxY variables
are the maximum lengths of subsequences used in
a single mapping operation for x and y. (For the
Algorithm 2: Pseudocode for a many-to-many
expectation algorithm.
Algorithm:Expectation-many2many
Input: xT , yV ,maxX,maxY, ?
Output: ?
? := Forward-many2many (xT , yV ,maxX,maxY )
? := Backward-many2many (xT , yV ,maxX,maxY )
if (?T,V = 0) then
return
for t = 0...T do
for v = 0...V do
if (t > 0 ?DELX) then
for i = 1...maxX st t? i ? 0 do
?(xtt?i+1, ?)+ =
?t?i,v?(x
t
t?i+1,?)?t,v
?T,V
if (v > 0 ?DELY ) then
for j = 1...maxY st v ? j ? 0 do
?(?, yvv?j+1)+ =
?t,v?j?(?,y
v
v?j+1)?t,v
?T,V
if (v > 0 ? t > 0) then
for i = 1...maxX st t? i ? 0 do
for j = 1...maxY st v ? j ? 0 do
?(xtt?i+1, y
v
v?j+1)+ =
?t?i,v?j?(x
t
t?i+1,y
v
v?j+1)?t,v
?T,V
task at hand, we set both maxX and maxY to 2.)
The Maximization-step function simply normalizes
the partial counts to create a probability distribution.
Normalization can be done over the whole table to
create a joint distribution or per grapheme to create
a conditional distribution.
The Forward-many2many function (Algorithm 3)
fills in the table ?, with each entry ?(t, v) being the
sum of all paths through the transducer that gen-
erate the sequence pair (xt1, yv1). Analogously, the
Backward-many2many function fills in ?, with each
entry ?(t, v) being the sum of all paths through the
transducer that generate the sequence pair (xTt , yVv ).
The constants DELX and DELY indicate whether
or not deletions are allowed on either side. In our
system, we allow letter deletions (i.e. mapping of
letters to null phoneme), but not phoneme deletions.
Expectation-many2many first calls the two func-
tions to fill the ? and ? tables, and then uses the
probabilities to calculate partial counts for every
possible mapping in the sequence pair. The par-
tial count collected at positions t and v in the se-
quence pair is the sum of all paths that generate the
sequence pair and go through (t, v), divided by the
sum of all paths that generate the entire sequence
pair (?(T, V )).
Once the probabilities are learned, the Viterbi
374
Algorithm 3: Pseudocode for a many-to-many
forward algorithm.
Algorithm:Forward-many2many
Input: (xT , yV ,maxX,maxY )
Output: ?
?0,0 := 1
for t = 0...T do
for v = 0...V do
if (t > 0 ? v > 0) then
?t,v = 0
if (t > 0 ?DELX) then
for i = 1...maxX st t? i ? 0 do
?t,v+ = ?(xtt?i+1, ?)?t?i,v
if (v > 0 ?DELY ) then
for j = 1...maxY st v ? j ? 0 do
?t,v+ = ?(?, yvv?j+1)?t,v?j
if (v > 0 ? t > 0) then
for i = 1...maxX st t? i ? 0 do
for j = 1...maxY st v ? j ? 0 do
?t,v+ = ?(xtt?i+1, y
v
v?j+1)?t?i,v?j
algorithm can be used to produce the most likely
alignment as in the following equations. Back point-
ers to maximizing arguments are kept at each step so
the alignment can be reconstructed.
?(0, 0) = 1 (1)
?(t, v) = max
1?i?maxX,
1?j?maxY
8
<
:
?(xtt?i+1, ?)?t?i,v
?(?, yvv?j+1)?t,v?j
?(xtt?i+1, y
v
v?j+1)?t?i,v?j
(2)
Given a set of words and their phonemes, align-
ments are made across graphemes and phonemes.
For example, the word phoenix, with phonemes [ f i
n I k s ], is aligned as:
ph oe n i x
| | | | |
f i n I ks
The letters ph are an example of the double let-
ter problem (mapping to the single phoneme [ f ]),
while the letter x is an example of the double
phoneme problem (mapping to both [ k ] and [ s ]
in the phoneme sequence). These alignments pro-
vide more accurate grapheme-to-phoneme relation-
ships for a phoneme prediction model.
3 Letter chunking
Our new alignment scheme provides more accu-
rate alignments, but it is also more complex ?
sometimes a prediction model should predict two
phonemes for a single letter, while at other times
the prediction model should make a prediction based
on a pair of letters. In order to distinguish between
these two cases, we propose a method called ?letter
chunking?.
Once many-to-many alignments are built across
graphemes and phonemes, each word contains a set
of letter chunks, each consisting of one or two let-
ters aligned with phonemes. Each letter chunk can
be considered as a grapheme unit that contains either
one or two letters. In the same way, each phoneme
chunk can be considered as a phoneme unit consist-
ing of one or two phonemes. Note that the double
letters and double phonemes are implicitly discov-
ered by the alignments of graphemes and phonemes.
They are not necessarily consistent over the train-
ing data but based on the alignments found in each
word.
In the phoneme generation phase, the system has
only graphemes available to predict phonemes, so
there is no information about letter chunk bound-
aries. We cannot simply merge any two letters that
have appeared as a letter chunk in the training data.
For example, although the letter pair sh is usually
pronounced as a single phoneme in English (e.g.
gash [ g ae S ]), this is not true universally (e.g.
gasholder [ g ae s h o l d @ r ]). Therefore, we im-
plement a letter chunk prediction model to provide
chunk boundaries given only graphemes.
In our system, a bigram letter chunking predic-
tion automatically discovers double letters based on
instance-based learning (Aha et al, 1991). Since the
many-to-many alignments are drawn from 1-0, 1-1,
1-2, 2-0, and 2-1 relationships, each letter in a word
can form a chunk with its neighbor or stand alone
as a chunk itself. We treat the chunk prediction as
a binary classification problem. We generate all the
bigrams in a word and determine whether each bi-
gram should be a chunk based on its context. Table 1
shows an example of how chunking prediction pro-
ceeds for the word longs. Letters li?2, li?1, li+1, and
li+2 are the context of the bigram li; chunk = 1 if
the letter bigram li is a chunk. Otherwise, the chunk
simply consists of an individual letter. In the exam-
ple, the word is decomposed as l|o|ng|s, which can
be aligned with its pronunciation [ l | 6 | N | z ]. If
the model happens to predict consecutive overlap-
ping chunks, only the first of the two is accepted.
375
li?2 li?1 li li+1 li+2 chunk
lo n g 0
l on g s 0
l o ng s 1
o n gs 0
Table 1: An example of letter chunking prediction.
4 Phoneme prediction
Most of the previously proposed techniques for
phoneme prediction require training data to be
aligned in one-to-one alignments. Those models
approach the phoneme prediction task as a classi-
fication problem: a phoneme is predicted for each
letter independently without using other predictions
from the same word. These local predictions assume
independence of predictions, even though there are
clearly interdependencies between predictions. Pre-
dicting each phoneme in a word without considering
other assignments may not satisfy the main goal of
finding a set of phonemes that work together to form
a word.
A trigram phoneme prediction with constraint sat-
isfaction inference (Van Den Bosch and Canisius,
2006) was proposed to improve on local predictions.
From each letter unit, it predicts a trigram class that
has the target phoneme in the middle surrounded by
its neighboring phonemes. The phoneme sequence
is generated in such a way that it satisfies the tri-
gram, bigram and unigram constraints. The over-
lapping predictions improve letter-to-phoneme per-
formance mainly by repairing imperfect one-to-one
alignments.
However, the trigram class prediction tends to be
more complex as it increases the number of tar-
get classes. For English, there are only 58 uni-
gram phoneme classes but 13,005 tri-gram phoneme
classes. The phoneme combinations in the tri-gram
classes are potentially confusing to the prediction
model because the model has more target classes in
its search space while it has access to the same num-
ber of local features in the grapheme side.
We propose to apply a supervised HMM method
embedded with local classification to find the most
likely sequence of phonemes given a word. An
HMM is a statistical model that combines the obser-
vation likelihood (probability of phonemes given let-
ters) and transition likelihood (probability of current
phoneme given previous phonemes) to predict each
phoneme. Our approach differs from a basic Hidden
Markov Model for letter-to-phoneme system (Tay-
lor, 2005) that formulates grapheme sequences as
observation states and phonemes as hidden states.
The basic HMM system for L2P does not provide
good performance on the task because it lacks con-
text information on the grapheme side. In fact, a
pronunciation depends more on graphemes than on
the neighboring phonemes; therefore, the transition
probability (language model) should affect the pre-
diction decisions only when there is more than one
possible phoneme that can be assigned to a letter.
Our approach is to use an instance-based learn-
ing technique as a local predictor to generate a set
of phoneme candidates for each letter chunk, given
its context in a word. The local predictor produces
confidence values for Each candidate phoneme. We
normalize the confidence values into values between
0 and 1, and treat them as the emission probabilities,
while the transition probabilities are derived directly
from the phoneme sequences in the training data.
The pronunciation is generated by considering
both phoneme prediction values and transition prob-
abilities. The optimal phoneme sequence is found
with the Viterbi search algorithm. We limit the size
of the context to n = 3 in order to avoid over-
fitting and minimize the complexity of the model.
Since the candidate set is from the classifier, the
search space is limited to a small number of can-
didate phonemes (1 to 5 phonemes in most cases).
The HMM postprocessing is independent of local
predictions from the classifier. Instead, it selects the
best phoneme sequence from a set of possible lo-
cal predictions by taking advantage of the phoneme
language model, which is trained on the phoneme
sequences in the training data.
5 Evaluation
We evaluated our approaches on CMUDict, Brulex,
and German, Dutch and English Celex cor-
pora (Baayen et al, 1996). The corpora (except
English Celex) are available as part of the Letter-
to-Phoneme Conversion PRONALSYL Challenge1.
1The PRONALSYL Challenge: http://www.
pascal-network.org/Challenges/PRONALSYL/.
376
Language Data set Number of words
English CMUDict 112,102
English Celex 65,936
Dutch Celex 116,252
German Celex 49,421
French Brulex 27,473
Table 2: Number of words in each data set.
For the English Celex data, we removed duplicate
words as well as words shorter than four letters. Ta-
ble 2 shows the number of words and the language
of each corpus.
For all of our experiments, our local classifier
for predicting phonemes is the instance-based learn-
ing IB1 algorithm (Aha et al, 1991) implemented
in the TiMBL package (Daelemans et al, 2004).
The HMM technique is applied as post process-
ing to the instance-based learning to provide a se-
quence prediction. In addition to comparing one-to-
one and many-to-many alignments, we also compare
our method to the constraint satisfaction inference
method as described in Section 4. The results are
reported in word accuracy rate based on the 10-fold
cross validation, with the mean and standard devia-
tion values.
Table 3 shows word accuracy performance across
a variety of methods. We show results comparing
the one-to-one aligner described in Section 2.1 and
the one-to-one aligner provided by the PRONAL-
SYL challenge. The PRONALSYS one-to-one
alignments are taken directly from the PRONAL-
SYL challenge, whose method is based on an EM
algorithm. For both alignments, we use instance-
based learning as the prediction model.
Overall, our one-to-one alignments outperform
the alignments provided by the data sets for all cor-
pora. The main difference between the PRONAL-
SYS one-to-one alignment and our one-to-one align-
ment is that our aligner does not allow a null letter
on the grapheme side. Consider the word abomina-
tion [ @ b 6 m I n e S @ n ]: the first six letters and
phonemes are aligned the same way by both align-
ers (abomin- [ @ b 6 m I n ]). However, the two
aligners produce radically different alignments for
the last five letters. The alignment provided by the
PRONALSYS one-to-one alignments is:
a t i o n
| | | | | | |
e S @ n
while our one-to-one alignment is:
a t i o n
| | | | |
e S @ n
Clearly, the latter alignment provides more informa-
tion on how the graphemes map to the phonemes.
Table 3 also shows that impressive improvements
for all evaluated corpora are achieved by using
many-to-many alignments rather than one-to-one
alignments (1-1 align vs. M-M align). The signif-
icant improvements, ranging from 2.7% to 7.6% in
word accuracy, illustrate the importance of having
more precise alignments. For example, we can now
obtain the correct alignment for the second part of
the word abomination:
a ti o n
| | | |
e S @ n
Instead of adding a null phoneme in the phoneme
sequence, the many-to-many aligner maps the letter
chunk ti to a single phoneme.
The HMM approach is based on the same hy-
pothesis as the constraint satisfaction inference
(CSInf) (Van Den Bosch and Canisius, 2006). The
results in Table 3 (1-1+CSInf vs. 1-1+HMM) show
that the HMM approach consistently improves per-
formance over the baseline system (1-1 align), while
the CSInf degrades performance on the Brulex data
set. For the CSInf method, most errors are caused
by trigram confusion in the prediction phase.
The results of our best system, which combines
the HMM method with the many-to-many align-
ments (M-M+HMM), are better than the results re-
ported in (Black et al, 1998) on both the CMU-
Dict and German Celex data sets. This is true even
though Black et al (1998) use explicit lists of letter-
phoneme mappings during the alignment process,
while our approach is a fully automatic system that
does not require any handcrafted list.
6 Conclusion and future work
We presented a novel technique of applying many-
to-many alignments to the letter-to-phoneme conver-
sion problem. The many-to-many alignments relax
377
Language Data set PRONALSYS 1-1 align 1-1+CsInf 1-1+HMM M-M align M-M+HMM
English CMUDict 58.3? 0.49 60.3? 0.53 62.9? 0.45 62.1? 0.53 65.1? 0.60 65.6? 0.72
English Celex ? 74.6? 0.80 77.8? 0.72 78.5? 0.76 82.2? 0.63 83.6? 0.63
Dutch Celex 84.3? 0.34 86.6? 0.36 87.5? 0.32 87.6? 0.34 91.1? 0.27 91.4? 0.24
German Celex 86.0? 0.40 86.6? 0.54 87.6? 0.47 87.6? 0.59 89.3? 0.53 89.8? 0.59
French Brulex 86.3? 0.67 87.0? 0.38 86.5? 0.68 88.2? 0.39 90.6? 0.57 90.9? 0.45
Table 3: Word accuracies achieved on data sets based on the 10-fold cross validation. PRONALSYS: one-
to-one alignments provided by the PRONALSYL challenge. 1-1 align: our one-to-one alignment method
described in Section 2.1. CsInf: Constraint satisfaction inference (Van Den Bosch and Canisius, 2006).
M-M align: our many-to-many alignment method. HMM: our HMM embedded with a local prediction.
the constraint assumptions of the traditional one-to-
one alignments. Letter chunking bigram prediction
incorporates many-to-many alignments into the con-
ventional phoneme prediction models. Finally, the
HMM technique yields global phoneme predictions
based on language models.
Impressive word accuracy improvements are
achieved when the many-to-many alignments are ap-
plied over the baseline system. On several languages
and data sets, using the many-to-many alignments,
word accuracy improvements ranged from 2.7% to
7.6%, as compared to one-to-one alignments. The
HMM cooperating with the local predictions shows
slight improvements when it is applied to the many-
to-many alignments. We illustrated that the HMM
technique improves the word accuracy more con-
sistently than the constraint-based approach. More-
over, the HMM can be easily incorporated into the
many-to-many alignment approach.
We are investigating the possibility of integrat-
ing syllabification information into our system. It
has been reported that syllabification can poten-
tially improve pronunciation performance in En-
glish (Marchand and Damper, 2005). We plan
to explore other sequence prediction approaches,
such as discriminative training methods (Collins,
2004), and sequence tagging with Support Vector
Machines (SVM-HMM) (Altun et al, 2003) to in-
corporate more features (context information) into
the phoneme generation model. We are also inter-
ested in applying our approach to other related areas
such as morphology and transliteration.
Acknowledgements
We would like to thank Susan Bartlett, Colin Cherry,
and other members of the Natural Language Pro-
cessing research group at University of Alberta for
their helpful comments and suggestions. This re-
search was supported by the Natural Sciences and
Engineering Research Council of Canada.
References
David W. Aha, Dennis Kibler, and Marc K. Albert. 1991.
Instance-based learning algorithms. Machine Learn-
ing, 6(1):37?66.
Yasemin Altun, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden Markov Support Vector Ma-
chines. In Proceedings of the 20th International Con-
ference on Machine Learning (ICML-2003).
Harald Baayen, Richard Piepenbrock, and Leon Gulikers.
1996. The CELEX2 lexical database. LDC96L14.
Alan W. Black, Kevin Lenzo, and Vincent Pagel. 1998.
Issues in building general letter to sound rules. In The
Third ESCA Workshop in Speech Synthesis, pages 77?
80.
Michael Collins. 2004. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
gauge Processing (EMNLP).
Walter Daelemans and Antal Van Den Bosch. 1997.
Language-independent data-oriented grapheme-to-
phoneme conversion. In Progress in Speech Synthesis,
pages 77?89. Springer, New York.
Walter Daelemans, Jakub Zavrel, Ko Van Der Sloot, and
Antal Van Den Bosch. 2004. TiMBL: Tilburg Mem-
ory Based Learner, version 5.1, reference guide. In
ILK Technical Report Series 04-02.
Robert I. Damper, Yannick Marchand, John DS.
Marsters, and Alexander I. Bazin. 2005. Aligning
text and phonemes for speech technology applications
using an EM-like algorithm. International Journal of
Speech Technology, 8(2):147?160, June.
378
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. In Journal of the Royal Statistical Society,
pages B:1?38.
Yannick Marchand and Robert I. Damper. 2000. A
multistrategy approach to improving pronunciation by
analogy. Computational Linguistics, 26(2):195?219,
June.
Yannick Marchand and Robert I. Damper. 2005. Can
syllabification improve pronunciation by analogy of
English? In Natural Language Engineering, pages
(1):1?25.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 20(5):522?532.
David Sankoff and Joseph Kruskal, 1999. Time Warps,
String Edits, and Macromolecules, chapter 2, pages
55?91. CSLI Publications.
Terrence J. Sejnowski and Charles R. Rosenberg. 1987.
Parallel networks that learn to pronounce English text.
In Complex Systems, pages 1:145?168.
Paul Taylor. 2005. Hidden Markov Models for grapheme
to phoneme conversion. In Proceedings of the 9th
European Conference on Speech Communication and
Technology 2005.
Kristina Toutanova and Robert C. Moore. 2001. Pro-
nunciation modeling for improved spelling correction.
In ACL ?02: Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
144?151, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Antal Van Den Bosch and Sander Canisius. 2006.
Improved morpho-phonological sequence processing
with constraint satisfaction inference. Proceedings of
the Eighth Meeting of the ACL Special Interest Group
in Computational Phonology, SIGPHON ?06, pages
41?49, June.
379
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 864?871,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Bootstrapping a Stochastic Transducer
for Arabic-English Transliteration Extraction
Tarek Sherif and Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada T6G 2E8
{tarek,kondrak}@cs.ualberta.ca
Abstract
We propose a bootstrapping approach to
training a memoriless stochastic transducer
for the task of extracting transliterations
from an English-Arabic bitext. The trans-
ducer learns its similarity metric from the
data in the bitext, and thus can func-
tion directly on strings written in different
writing scripts without any additional lan-
guage knowledge. We show that this boot-
strapped transducer performs as well or bet-
ter than a model designed specifically to de-
tect Arabic-English transliterations.
1 Introduction
Transliterations are words that are converted from
one writing script to another on the basis of their pro-
nunciation, rather than being translated on the basis
of their meaning. Transliterations include named en-
tities (e.g. 	??? 	?


g
.
/Jane Austen) and lexical loans
(e.g. 	??K


	
Q



	
??

K/television).
An algorithm to detect transliterations automati-
cally in a bitext can be an effective tool for many
tasks. Models of machine transliteration such as
those presented in (Al-Onaizan and Knight, 2002) or
(AbdulJaleel and Larkey, 2003) require a large set of
sample transliterations to use for training. If such a
training set is unavailable for a particular language
pair, a detection algorithm would lead to a signif-
icant gain in time over attempting to build the set
manually. Algorithms for cross-language informa-
tion retrieval often encounter the problem of out-of-
vocabulary words, or words not present in the algo-
rithm?s lexicon. Often, a significant proportion of
these words are named entities and thus are candi-
dates for transliteration. A transliteration detection
algorithm could be used to map named entities in a
query to potential transliterations in the target lan-
guage text.
The main challenge in transliteration detection
lies in the fact that transliteration is a lossy process.
In other words, information can be lost about the
original word when it is transliterated. This can oc-
cur because of phonetic gaps in one language or the
other. For example, the English [p] sound does not
exist in Arabic, and the Arabic [Q] sound (made by
the letter ?) does not exist in English. Thus, Paul is
transliterated as ??K
.
[bul], and
?


?
? [Qali] is translit-
erated as Ali. Another form of loss occurs when the
relationship between the orthographic and phonetic
representations of a word are unclear. For example,
the [k] sound will always be written with the letter ?
in Arabic, but in English it can be written as c, k ch,
ck, cc or kk (not to mention being one of the sounds
produced by x). Finally, letters may be deleted in
one language or the other. In Arabic, short vowels
will often be omitted (e.g. 	???K


/Yousef ), while in
English the Arabic Z and ? are often deleted (e.g.
?J


?A?
?
?
/Ismael).
We explore the use of word similarity metrics on
the task of Arabic-English transliteration detection
and extraction. One of our primary goals in explor-
ing these metrics is to assess whether it is possible
maintain high performance without making the al-
gorithms language-specific. Many word-similarity
metrics require that the strings being compared be
864
written in the same script. Levenshtein edit distance,
for example, does not produce a meaningful score in
the absence of character identities. Thus, if these
metrics are to be used for transliteration extraction,
modifications must be made to allow them to com-
pare different scripts.
Freeman et al (2006) take the approach of man-
ually encoding a great deal of language knowl-
edge directly into their Arabic-English fuzzy match-
ing algorithm. They define equivalence classes be-
tween letters in the two scripts and perform several
rule-based transformations to make word pairs more
comparable. This approach is unattractive for two
reasons. Firstly, predicting all possible relationships
between letters in English and Arabic is difficult.
For example, allowances have to be made for un-
usual pronunciations in foreign words such as the ch
in cliche? or the c in Milosevic. Secondly, the algo-
rithm becomes completely language-specific, which
means that it cannot be used for any other language
pair.
We propose a method to learn letter relation-
ships directly from the bitext containing the translit-
erations. Our model is based on the memoriless
stochastic transducer proposed by Ristad and Yian-
ilos (1998), which derives a probabilistic word-
similarity function from a set of examples. The
transducer is able to learn edit distance costs be-
tween disjoint sets of characters representing dif-
ferent writing scripts without any language-specific
knowledge. The transducer approach, however, re-
quires a large set of training examples, which is a
limitation not present in the fuzzy matching algo-
rithm. Thus, we propose a bootstrapping approach
(Yarowsky, 1995) to train the stochastic transducer
iteratively as it extracts transliterations from a bi-
text. The bootstrapped stochastic transducer is com-
pletely language-independent, and we show that it is
able to perform at least as well as the Arabic-English
specific fuzzy matching algorithm.
The remainder of this paper is organized as fol-
lows. Section 2 presents our bootstrapping method
to train a stochastic transducer. Section 3 outlines
the Arabic-English fuzzy matching algorithm. Sec-
tion 4 discusses other word-similarity models used
for comparison. Section 5 describes the results of
two experiments performed to test the models. Sec-
tion 6 briefly discusses previous approaches to de-
tecting transliterations. Section 7 presents our con-
clusions and possibilities for future work.
2 Bootstrapping with a Stochastic
Transducer
Ristad and Yianilos (1998) propose a probabilistic
framework for word similarity, in which the simi-
larity of a pair of words is defined as the sum of
the probabilities of all paths through a memoriless
stochastic transducer that generate the pair of words.
This is referred to as the forward score of the pair of
words. They outline a forward-backward algorithm
to train the model and show that it outperforms Lev-
enshtein edit distance on the task of pronunciation
classification.
The training algorithm begins by calling the for-
ward (Equation 1) and backward (Equation 2) func-
tions to fill in the F and B tables for training pair s
and t with respective lengths I and J .
F (0, 0) = 1
F (i, j) = P (si, ?)F (i ? 1, j)
+P (?, tj)F (i, j ? 1)
+P (si, tj)F (i ? 1, j ? 1)
(1)
B(I, J) = 1
B(i, j) = P (si+1, ?)B(i + 1, j)
+P (?, tj+1)B(i, j + 1)
+P (si+1, tj+1)B(i + 1, j + 1)
(2)
The forward value at each position (i, j) in the F
matrix signifies the sum of the probabilities of all
paths through the transducer that produce the prefix
pair (si1, tj1), while B(i, j) contains the sum of the
probabilities of all paths through the transducer that
generate the suffix pair (sIi+1, tJj+1). These tables
can then be used to collect partial counts to update
the probabilities. For example, the mapping (si, tj)
would contribute a count according to Equation 3.
These counts are then normalized to produce the up-
dated probability distribution.
C(si, tj)+ =
F (i? 1, j ? 1)P (si, tj)B(i, j)
F (I, J) (3)
The major issue in porting the memoriless trans-
ducer over to our task of transliteration extraction
865
is that its training is supervised. In other words, it
would require a relatively large set of known translit-
erations for training, and this is exactly what we
would like the model to acquire. In order to over-
come this problem, we look to the bootstrapping
method outlined in (Yarowsky, 1995). Yarowsky
trains a rule-based classifier for word sense disam-
biguation by starting with a small set of seed ex-
amples for which the sense is known. The trained
classifier is then used to label examples for which
the sense is unknown, and these newly labeled ex-
amples are then used to retrain the classifier. The
process is repeated until convergence.
Our method uses a similar approach to train the
stochastic transducer. The algorithm proceeds as
follows:
1. Initialize the training set with the seed pairs.
2. Train the transducer using the forward-
backward algorithm on the current training set.
3. Calculate the forward score for all word pairs
under consideration.
4. If the forward score for a pair of words is above
a predetermined acceptance threshold, add the
pair to the training set.
5. Repeat steps 2-4 until the training set ceases to
grow.
Once training stops, the transducer can be used
to score pairs of words not in the training set. For
our experiments, the acceptance threshold was op-
timized on a separate development set. Forward
scores were normalized by the average of the lengths
of the two words.
3 Arabic-English Fuzzy String Matching
In this section, we outline the fuzzy string matching
algorithm proposed by Freeman et al (2006). The
algorithm is based on the standard Levenshtein dis-
tance approach, but encodes a great deal of knowl-
edge about the relationships between English and
Arabic letters.
Initially, the candidate word pair is modified in
two ways. The first transformation is a rule-based
letter normalization of both words. Some examples
of normalization include:
? English double letter collapse: e.g.
Miller?Miler.
,

,
?
,?? a,e,i,o,u H
.
? b,p,v

H,?,

H? t h
.
? j,g
	
?? d,z ?,Z ? ?,c,a,e,i,o,u

? ? q,g,k ? ? k,c,s
?


? y,i,e,j ? ? a,e
Table 1: A sample of the letter equivalence classes
for fuzzy string matching.
Algorithm VowelNorm (Estring,Astring)
for each i := 0 to min(|Estring|, |Astring|)
for each j := 0 to min(|Estring|, |Astring|)
if Astringi = Estringj
Outstring. = Estringj; i + +; j + +;
if vowel(Astringi) ? vowel(Estringj)
Outstring. = Estringj; i + +; j + +;
if ?vowel(Astringi) ? vowel(Estringj)
j + +;
if j < |Estringj |
Outstring. = Estringj ; i + +; j + +;
else
Outstring. = Estringj; i + +; j + +;
while j < |Estring|
if ?vowel(Estringj)
Outstring. = Estringj ;
j + +;
return Outstring;
Figure 1: Pseudocode for the vowel transformation
procedure.
? Arabic hamza collapse: e.g. 	?Q??

? 	?Q?? .
? Individual letter normalizations: e.g. Hen-
drix?Hendriks or 	?K


Q?

?? 	?K


Q?D?.
The second transformation is an iteration through
both words to remove any vowels in the English
word for which there is no similarly positioned
vowel in the Arabic word. The pseudocode for our
implementation of this vowel transformation is pre-
sented in Figure 1.
After letter and vowel transformations, the Leven-
shtein distance is computed using the letter equiva-
lences as matches instead of identities. Some equiv-
alence classes between English and Arabic letters
are shown in Table 1. The Arabic and English letters
within a class are treated as identities. For example,
the Arabic 	? can match both f and v in English with
no cost. The resulting Levenshtein distance is nor-
malized by the sum of the lengths of both words.
866
Levenshtein ALINE Fuzzy Match Bootstrap
Lang.-specific No No Yes No
Preprocessing Romanization Phon. Conversion None None
Data-driven No No No Yes
Table 2: Comparison of the word-similarity models.
Several other modifications, such as light stem-
ming and multiple passes to discover more diffi-
cult mappings, were also proposed, but they were
found to influence performance minimally. Thus,
the equivalence classes and transformations are the
only modifications we reproduce for our experi-
ments here.
4 Other Models of Word Similarity
In this section, we present two models of word simi-
larity used for purposes of comparison. Levenshtein
distance and ALINE are not language-specific per
se, but require that the words being compared be
written in a common script. Thus, they require some
language knowledge in order to convert one or both
of the words into the common script. A comparison
of all the models presented is given in Table 2.
4.1 Levenshtein Edit Distance
As a baseline for our experiments, we used Leven-
shtein edit distance. The algorithm simply counts
the minimum number of insertions, deletions and
substitutions required to convert one string into an-
other. Levenshtein distance depends on finding iden-
tical letters, so both words must use the same al-
phabet. Prior to comparison, we convert the Ara-
bic words into the Latin alphabet using the intuitive
mappings for each letter shown in Table 3. The
distances are also normalized by the length of the
longer of the two words to avoid excessively penal-
izing longer words.
4.2 ALINE
Unlike other algorithms presented here, the ALINE
algorithm (Kondrak, 2000) operates in the phonetic,
rather than the orthographic, domain. It was orig-
inally designed to identify cognates in related lan-
guages, but it can be used to compute similarity be-
tween any pair of words, provided that they are ex-
pressed in a standard phonetic notation. Individual
,

,
?
,Z ? a H
.
? b H,?? t

? ? a H ? th h
.
? j
h, ? ? h p ? kh X, 	? ? d
	
X,
	
?? th P ? r 	P ? z
?,? ? s ? ? sh ? ? ?
	
? ? g 	? ? f ? ? q
? ? k ? ? l  ? m
	
? ? n ? ? w ?


? y
Table 3: Arabic Romanization for Levenshtein dis-
tance.
phonemes input to the algorithm are decomposed
into a dozen phonetic features, such as Place, Man-
ner and Voice. A substitution score between a pair
of phonemes is based on the similarity as assessed
by a comparison of the individual features. After
an optimal alignment of the two words is computed
with a dynamic programming algorithm, the overall
similarity score is set to the sum of the scores of all
links in the alignment normalized by the length of
the longer of the two words.
In our experiments, the Arabic and English words
were converted into phonetic transcriptions using a
deterministic rule-based transformation. The tran-
scriptions were only approximate, especially for En-
glish vowels. Arabic emphatic consonants were de-
pharyngealized.
5 Evaluation
The word-similarity metrics were evaluated on two
separate tasks. In experiment 1 (Section 5.1) the
task was to extract transliterations from a sentence
aligned bitext. Experiment 2 (Section 5.2) provides
the algorithms with named entities from an English
document and requires them to extract the transliter-
ations from the document?s Arabic translation.
The two bitexts used in the experiments were the
867
Figure 2: Precision per number of words extracted for the various algorithms from a sentence-aligned bitext.
Arabic Treebank Part 1-10k word English Transla-
tion corpus and the Arabic English Parallel News
Part 1 corpus (approx. 2.5M words). Both bi-
texts contain Arabic news articles and their English
translations aligned at the sentence level, and both
are available from the Linguistic Date Consortium.
The Treebank data was used as a development set
to optimize the acceptance threshold used by the
bootstrapped transducer. Testing for the sentence-
aligned extraction task was done on the first 20k
sentences (approx. 50k words) of the parallel news
data, while the named entity extraction task was per-
formed on the first 1000 documents of the paral-
lel news data. The seed set for bootstrapping the
stochastic transducer was manually constructed and
consisted of 14 names and their transliterations.
5.1 Experiment 1: Sentence-Aligned Data
The first task used to test the models was to compare
and score the words remaining in each bitext sen-
tence pair after preprocessing the bitext in the fol-
lowing way:
? The English corpus is tokenized using a modi-
fied1 version of Word Splitter2.
? All uncapitalized English words are removed.
? Stop words (mainly prepositions and auxiliary
1The way the program handles apostrophes(?) had to be
modified since they are sometimes used to represent glottal
stops in transliterations of Arabic words, e.g. qala?a.
2Available at http://l2r.cs.uiuc.edu/?cogcomp/tools.php.
verbs) are removed from both sides of the bi-
text.
? Any English words of length less than 4 and
Arabic words of length less than 3 are removed.
Each algorithm finds the top match for each En-
glish word and the top match for each Arabic word.
If two words mark each other as their top scorers,
then the pair is marked as a transliteration pair. This
one-to-one constraint is meant to boost precision,
though it will also lower recall. This is because for
many of the tasks in which transliteration extraction
would be useful (such as building a lexicon), preci-
sion is deemed more important. Transliteration pairs
are sorted according to their scores, and the top 500
hundred scoring pairs are returned.
The results for the sentence-aligned extraction
task are presented in Figure 2. Since the number
of actual transliterations in the data was unknown,
there was no way to compute recall. The measure
used here is the precision for each 100 words ex-
tracted up to 500. The bootstrapping method is equal
to or outperforms the other methods at all levels, in-
cluding the Arabic-English specific fuzzy match al-
gorithm. Fuzzy matching does well for the first few
hundred words extracted, but eventually falls below
the level of the baseline Levenshtein.
Interestingly, the bootstrapped transducer does
not seem to have trouble with digraphs, despite the
one-to-one nature of the character operations. Word
pairs with two-to-one mappings such as sh/ ? or
868
Metric Arabic Romanized English
1 Bootstrap 	?K


Q



	
gB alakhyryn Algerian
2 Bootstrap ???? wslm Islam
3 Fuzzy M. ??? lkl Alkella
4 Fuzzy M. 	?A?? ?mAn common
5 ALINE Q?? skr sugar
6 Leven. H
.
A?

 asab Arab
7 All ?PA? mark Marks
8 All 	??J


??P rwsywn Russian
9 All ?J


j
.




K
Q

? istratyjya strategic
10 All ?	KQ 	? frnk French
Table 4: A sample of the errors made by the word-
similarity metrics.
x/?? tend to score lower than their counterparts
composed of only one-to-one mappings, but never-
theless score highly.
A sample of the errors made by each word-
similarity metric is presented in Table 4. Errors 1-
6 are indicative of the weaknesses of each individ-
ual algorithm. The bootstrapping method encoun-
ters problems when erroneous pairs become part of
the training data, thereby reinforcing the errors. The
only problematic mapping in Error 1 is the p/g map-
ping, and thus the pair has little trouble getting into
the training data. Once the pair is part of training
data, the algorithm learns that the mapping is ac-
ceptable and uses it to acquire other training pairs
that contain the same erroneous mapping. The prob-
lem with the fuzzy matching algorithm seems to be
that it creates too large a class of equivalent words.
The pairs in errors 3 and 4 are given a total edit cost
of 0. This is possible because of the overly gen-
eral letter and vowel transformations, as well as un-
usual choices made for letter equivalences (e.g. ?/c
in error 4). ALINE?s errors tend to occur when it
links two letters, based on phonetic similarity, that
are never mapped to each other in transliteration be-
cause they each have a more direct equivalent in the
other language (error 5). Although the Arabic ? [k]
is phonetically similar to the English g, they would
never be mapped to each other since English has sev-
eral ways of representing an actual [k] sound. Errors
made by Levenshtein distance (error 6) are simply
due to the fact that it considers all non-identity map-
pings to be equivalent.
Errors 7-10 are examples of general errors made
by all the algorithms. The most common error was
related to inflection (error 7). The words are essen-
tially transliterations of each other, but one or the
other of the two words takes a plural or some other
inflectional ending that corrupts the phonetic match.
Error 8 represents the common problem of inciden-
tal letter similarity. The English -ian ending used for
nationalities is very similar to the Arabic 	??J


[ijun]
and 	?


J


[ijin] endings which are used for the same
purpose. They are similar phonetically and, since
they are functionally similar, will tend to co-occur.
Since neither can be said to be derived from the
other, however, they cannot be considered translit-
erations. Error 9 is a case of two words of common
origin taking on language-specific derivational end-
ings that corrupt the phonetic match. Finally, error
10 shows a mapping (?/c) that is often correct in
transliteration, but is inappropriate in this particular
case.
5.2 Experiment 2: Document-Aligned Named
Entity Recognition
The second experiment provides a more challenging
task for the evaluation of the models. It is struc-
tured as a cross-language named entity recognition
task similar to those outlined in (Lee and Chang,
2003) and (Klementiev and Roth, 2006). Essen-
tially, the goal is to use a language for which named
entity recognition software is readily available as a
reference for tagging named entities in a language
for which such software is not available. For this
task, the sentence alignment of the bitext is ignored.
For each named entity in an English document, the
models must select a transliteration from within the
document?s entire Arabic translation. This is meant
to be a loose approximation of the ?comparable?
corpora used in (Klementiev and Roth, 2006). The
comparable corpora are related documents in differ-
ent languages that are not translations (e.g. news ar-
ticles describing the same event), and thus sentence
alignment is not possible.
The first 1000 documents in the parallel news data
were used for testing. The English side of the bi-
text was tagged with Named Entity Tagger3, which
labels named entities as person, location, organiza-
3Available at http://l2r.cs.uiuc.edu/?cogcomp/tools.php.
869
Method Accuracy
Levenshtein 69.3
ALINE 71.9
Fuzzy Match 74.6
Bootstrapping 74.6
Table 5: Precision of the various algorithms on the
NER detection task.
Metric Arabic Romanized English
1 Both YJ
.
? ?bd Abdallah
2 Bootstrap YK


Y?? al?dyd Alhadidi
3 Fuzzy Match 	?? ? thmn Othman
Table 6: A sample of errors made on the NER detec-
tion task.
tion or miscellaneous. The words labeled as per-
son were extracted. Person names are almost always
transliterated, while for the other categories this is
far less certain. The list was then hand-checked to
ensure that all names were candidates for transliter-
ation, leaving 822 names. The restrictions on word
length and stop words were the same as before, but
in this task each of the English person names from
a given document were compared to all valid words
in the corresponding Arabic document, and the top
scorer for each English name was returned.
The results for the NER detection task are pre-
sented in Table 5. It seems the bootstrapped trans-
ducer?s advantage is relative to the proportion of
correct transliteration pairs to the total number of
candidates. As this proportion becomes smaller the
transducer is given more opportunities to corrupt its
training data and performance is affected accord-
ingly. Nevertheless, the transducer is able to per-
form as well as the language-specific fuzzy match-
ing algorithm on this task, despite the greater chal-
lenge posed by selecting candidates from entire doc-
uments.
A sample of errors made by the bootstrapped
transducer and fuzzy matching algorithms is shown
in Table 6. Error 1 was due to the fact that names are
sometimes split differently in Arabic and English.
The Arabic ???  YJ
.
? (2 words) is generally written
as Abdallah in English, leading to partial matches
with part of the Arabic name. Error 2 shows an issue
with the one-to-one nature of the transducer. The
deleted h can be learned in mappings such as sh/ ?
or ph/ 	?, but it is generally inappropriate to delete
an h on its own. Error 3 again shows that the fuzzy
matching algorithm?s letter transformations are too
general. The vowel removals lead to a 0 cost match
in this case.
6 Related Work
Several other methods for detecting transliterations
between various language pairs have been proposed.
These methods differ in their complexity as well as
in their applicability to language pairs other than the
pair for which they were originally designed.
Collier et al (1997) present a method for identi-
fying transliterations in an English-Japanese bitext.
Their model first transcribes the Japanese word ex-
pressed in the katakana syllabic script as the con-
catenation of all possible transliterations of the in-
dividual symbols. A depth-first search is then ap-
plied to compute the number of matches between
this transcription and a candidate English transliter-
ation. The method requires a manual enumeration of
the possible transliterations for each katakana sym-
bol, which is unfeasible for many language pairs.
In the method developed by Tsuji (2002),
katakana strings are first split into their mora units,
and then the transliterations of the units are assessed
manually from a set of training pairs. For each
katakana string in a bitext, all possible translitera-
tions are produced based on the transliteration units
determined from the training set. The translitera-
tion candidates are then compared to the English
words according to the Dice score. The manual enu-
meration of possible mappings makes this approach
unattractive for many language pairs, and the gen-
eration of all possible transliteration candidates is
problematic in terms of computational complexity.
Lee and Chang (2003) detect transliterations with
a generative noisy channel transliteration model
similar to the transducer presented in (Knight and
Graehl, 1998). The English side of the corpus is
tagged with a named entity tagger, and the model
is used to isolate the transliterations in the Chinese
translation. This model, like the transducer pro-
posed by Ristad and Yianilos (1998), must be trained
on a large number of sample transliterations, mean-
ing it cannot be used if such a resource is not avail-
870
able.
Klementiev and Roth (2006) bootstrap with a per-
ceptron and use temporal analysis to detect translit-
erations in comparable Russian-English news cor-
pora. The English side is first tagged by a named
entity tagger, and the perceptron proposes transliter-
ations for the named entities. The candidate translit-
eration pairs are then reranked according the similar-
ity of their distributions across dates, as calculated
by a discrete Fourier transform.
7 Conclusion and Future Work
We presented a bootstrapping approach to training
a stochastic transducer, which learns scoring param-
eters automatically from a bitext. The approach is
completely language-independent, and was shown
to perform as well or better than an Arabic-English
specific similarity metric on the task of Arabic-
English transliteration extraction.
Although the bootstrapped transducer is
language-independent, it learns only one-to-one
letter relationships, which is a potential drawback in
terms of porting it to other languages. Our model is
able to capture English digraphs and trigraphs, but,
as of yet, we cannot guarantee the model?s success
on languages with more complex letter relationships
(e.g. a logographic writing system such as Chinese).
More research is necessary to evaluate the model?s
performance on other languages.
Another area open to future research is the use
of more complex transducers for word comparison.
For example, Linden (2006) presents a model which
learns probabilities for edit operations by taking into
account the context in which the characters appear.
It remains to be seen how such a model could be
adapted to a bootstrapping setting.
Acknowledgments
We would like to thank the members of the NLP re-
search group at the University of Alberta for their
helpful comments and suggestions. This research
was supported by the Natural Sciences and Engi-
neering Research Council of Canada.
References
N. AbdulJaleel and L. S. Larkey. 2003. Statistical
transliteration for English-Arabic cross language in-
formation retrieval. In CIKM, pages 139?146.
Y. Al-Onaizan and K. Knight. 2002. Machine translit-
eration of names in Arabic text. In ACL Workshop on
Comp. Approaches to Semitic Languages.
N. Collier, A. Kumano, and H. Hirakawa. 1997. Acqui-
sition of English-Japanese proper nouns from noisy-
parallel newswire articles using Katakana matching.
In Natural Language Pacific Rim Symposium (NL-
PRS?97), Phuket, Thailand, pages 309?314, Decem-
ber.
A. Freeman, S. Condon, and C. Ackerman. 2006.
Cross linguistic name matching in English and Ara-
bic. In Human Language Technology Conference of
the NAACL, pages 471?478, New York City, USA,
June. Association for Computational Linguistics.
A. Klementiev and D. Roth. 2006. Named entity translit-
eration and discovery from multilingual comparable
corpora. In Human Language Technology Conference
of the NAACL, pages 82?88, New York City, USA,
June. Association for Computational Linguistics.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, 24(4):599?612.
G. Kondrak. 2000. A new algorithm for the alignment of
phonetic sequences. In NAACL 2000, pages 288?295.
C. Lee and J. S. Chang. 2003. Acquisition of English-
Chinese transliterated word pairs from parallel-aligned
texts using a statistical machine transliteration model.
In HLT-NAACL 2003 Workshop on Building and using
parallel texts, pages 96?103, Morristown, NJ, USA.
Association for Computational Linguistics.
K. Linden. 2006. Multilingual modeling of cross-lingual
spelling variants. Information Retrieval, 9(3):295?
310, June.
E. S. Ristad and P. N. Yianilos. 1998. Learning string-
edit distance. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 20(5):522?532.
K. Tsuji. 2002. Automatic extraction of translational
Japanese-katakana and English word pairs. Interna-
tional Journal of Computer Processing of Oriental
Languages, 15(3):261?279.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Meeting of
the Association for Computational Linguistics, pages
189?196.
871
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 944?951,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Substring-Based Transliteration
Tarek Sherif and Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada T6G 2E8
{tarek,kondrak}@cs.ualberta.ca
Abstract
Transliteration is the task of converting a
word from one alphabetic script to another.
We present a novel, substring-based ap-
proach to transliteration, inspired by phrase-
based models of machine translation. We in-
vestigate two implementations of substring-
based transliteration: a dynamic program-
ming algorithm, and a finite-state transducer.
We show that our substring-based transducer
not only outperforms a state-of-the-art letter-
based approach by a significant margin, but
is also orders of magnitude faster.
1 Introduction
A significant proportion of out-of-vocabulary words
in machine translation models or cross language in-
formation retrieval systems are named entities. If
the languages are written in different scripts, these
names must be transliterated. Transliteration is the
task of converting a word from one writing script to
another, usually based on the phonetics of the orig-
inal word. If the target language contains all the
phonemes used in the source language, the translit-
eration is straightforward. For example, the Arabic
transliteration of Amanda is Y 	K A?

, which is essen-
tially pronounced in the same way. However, if
some of the sounds are missing in the target lan-
guage, they are generally mapped to the most pho-
netically similar letter. For example, the sound [p]
in the name Paul, does not exist in Arabic, and the
phonotactic constraints of Arabic disallow the sound
[A] in this context, so the word is transliterated as
??K
.
, pronounced [bul].
The information loss inherent in the process of
transliteration makes back-transliteration, which is
the restoration of a previously transliterated word,
a particularly difficult task. Any phonetically rea-
sonable forward transliteration is essentially correct,
although occasionally there is a standard translitera-
tion (e.g. Omar Sharif ). In the original script, how-
ever, there is usually only a single correct form. For
example, both Naguib Mahfouz and Najib Mahfuz
are reasonable transliterations of 	?? 	? m? I
.
J


m
.
Proceedings of the Workshop on Linguistic Distances, pages 43?50,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Evaluation of Several Phonetic Similarity Algorithms
on the Task of Cognate Identification
Grzegorz Kondrak and Tarek Sherif
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada T6G 2E8
{kondrak,tarek}@cs.ualberta.ca
Abstract
We investigate the problem of measuring
phonetic similarity, focusing on the iden-
tification of cognates, words of the same
origin in different languages. We com-
pare representatives of two principal ap-
proaches to computing phonetic similar-
ity: manually-designed metrics, and learn-
ing algorithms. In particular, we consider
a stochastic transducer, a Pair HMM, sev-
eral DBN models, and two constructed
schemes. We test those approaches on
the task of identifying cognates among
Indoeuropean languages, both in the su-
pervised and unsupervised context. Our
results suggest that the averaged context
DBN model and the Pair HMM achieve
the highest accuracy given a large training
set of positive examples.
1 Introduction
The problem of measuring phonetic similarity be-
tween words arises in various contexts, including
speech processing, spelling correction, commercial
trademarks, dialectometry, and cross-language in-
formation retrieval (Kessler, 2005). A number of
different schemes for computing word similarity
have been proposed. Most of those methods are de-
rived from the notion of edit distance. In its simplest
form, edit distance is the minimum number of edit
operations required to transform one word into the
other. The set of edit operations typically includes
substitutions, insertions, and deletions, and may in-
corporate more complex transformations.
By assigning variable weights to various edit op-
erations depending on the characters involved in
the operations, one can design similarity schemes
that are more sensitive to a given task. Such vari-
able weight schemes can be divided into two main
groups. One approach is to manually design edit op-
eration weights on the basis of linguistic intuition
and/or physical measurements. Another approach
is to use machine learning techniques to derive the
weights automatically from training data composed
of a set of word pairs that are considered similar.
The manually-designed schemes tend to be some-
what arbitrary, but can be readily applied to diverse
tasks. The learning approaches are also easily adapt-
able to various tasks, but they crucially require train-
ing data sets of reasonable size. In general, the more
complex the underlying model, the larger the data
sets needed for parameter estimation.
In this paper, we focus on a few representatives
of both approaches, and compare their performance
on the specific task of cognate identification. Cog-
nate identification is a problem of finding, in distinct
languages, words that can be traced back to a com-
mon word in a proto-language. Beyond historical
linguistics, cognate identification has applications
in other areas of computational linguistics (Mackay
and Kondrak, 2005). Because the likelihood that
two words across different languages are cognates is
highly correlated with their phonetic similarity, cog-
nate identification provides an objective test of the
quality of phonetic similarity schemes.
The remainder of this paper is organized as fol-
43
lows. Section 2 discusses the two manually designed
schemes: the ALINE algorithm and a linguistically-
motivated metric. Section 3 discusses various learn-
ing approaches. In Section 4, we describe Dynamic
Bayesian Nets. Finally, in Section 5, we discuss the
results of our experiments.
2 Two manually constructed schemes
In this section, we first describe two different con-
structed schemes and then compare their properties.
2.1 ALINE
The ALINE algorithm (Kondrak, 2000) assigns a
similarity score to pairs of phonetically-transcribed
words on the basis of the decomposition of phone-
mes into elementary phonetic features. The algo-
rithm was originally designed to identify and align
cognates in vocabularies of related languages. Nev-
ertheless, thanks to its grounding in universal pho-
netic principles, the algorithm can be used for esti-
mating the similarity of any pair of words.
The principal component of ALINE is a function
that calculates the similarity of two phonemes that
are expressed in terms of about a dozen multi-valued
phonetic features (Place, Manner, Voice, etc.). The
phonetic features are assigned salience weights that
express their relative importance. Feature values
are encoded as floating-point numbers in the range
[0,1]. For example, the feature Manner can take any
of the following seven values: stop = 1.0, affricate
= 0.9, fricative = 0.8, approximant = 0.6, high vowel
= 0.4, mid vowel = 0.2, and low vowel = 0.0. The
numerical values reflect the distances between vocal
organs during speech production.
The overall similarity score is the sum of individ-
ual similarity scores between pairs of phonemes in
an optimal alignment of two words, which is com-
puted by a dynamic programming algorithm (Wag-
ner and Fischer, 1974). A constant insertion/deletion
penalty is applied for each unaligned phoneme.
Another constant penalty is set to reduce relative
importance of vowel?as opposed to consonant?
phoneme matches. The similarity value is normal-
ized by the length of the longer word.
ALINE?s behavior is controlled by a number of
parameters: the maximum phonemic score, the in-
sertion/deletion penalty, the vowel penalty, and the
feature salience weights. The parameters have de-
fault settings for the cognate matching task, but
these settings can be optimized (tuned) on a devel-
opment set that includes both positive and negative
examples of similar words.
2.2 A linguistically-motivated metric
Phonetically natural classes such as /p b m/ are much
more common among world?s languages than unnat-
ural classes such as /o z g/. In order to show that the
bias towards phonetically natural patterns of phono-
logical classes can be modeled without stipulating
phonological features, Mielke (2005) developed a
phonetic distance metric based on acoustic and ar-
ticulatory measures. Mielke?s metric encompasses
63 phonetic segments that are found in the invento-
ries of multiple languages. Each phonetic segment
is represented by a 7-dimensional vector that con-
tains three acoustic dimensions and four articulatory
dimensions (perceptual dimensions were left out be-
cause of the difficulties involved in comparing al-
most two thousand different sound pairs). The pho-
netic distance between any two phonetic segments
were then computed as the Euclidean distance be-
tween the corresponding vectors.
For determining the acoustic vectors, the record-
ings of 63 sounds were first transformed into wave-
form matrices. Next, distances between pairs of
matrices were calculated using the Dynamic Time
Warping technique. These acoustic distances were
subsequently mapped to three acoustic dimensions
using multidimensional scaling. The three dimen-
sions can be interpreted roughly as (a) sonorous vs.
sibilant, (b) grave vs. acute, and (c) low vs. high
formant density.
The articulatory dimensions were based on ultra-
sound images of the tongue and palate, video im-
ages of the face, and oral and nasal airflow measure-
ments. The four articulatory dimensions were: oral
constriction location, oral constriction size, lip con-
striction size, and nasal/oral airflow ratio.
2.3 Comparison
When ALINE was initially designed, there did not
exist any concrete linguistically-motivated similarity
scheme to which it could be compared. Therefore, it
is interesting to perform such a comparison with the
recently proposed metric.
44
The principal difficulty in employing the metric
for computing word similarity is the limited size
of the phonetic segment set, which was dictated by
practical considerations. The underlying database
of phonological inventories representing 610 lan-
guages contains more than 900 distinct phonetic seg-
ments, of which almost half occur in only one lan-
guage. However, because a number of complex
measurements have to be performed for each sound,
only 63 phonetic segments were analyzed, which is
a set large enough to cover only about 20% of lan-
guages in the database. The set does not include
such common phones as dental fricatives (which oc-
cur in English and Spanish), and front rounded vow-
els (which occur in French and German). It is not
at all clear how one to derive pairwise distances in-
volving sounds that are not in the set.
In contrast, ALINE produces a similarity score for
any two phonetic segment so long as they can be ex-
pressed using the program?s set of phonetic features.
The feature set can in turn be easily extended to in-
clude additional phonetic features required for ex-
pressing unusual sounds. In practice, any IPA sym-
bol can be encoded as a vector of universal phonetic
features.
Another criticism that could be raised against
Mielke?s metric is that it has no obvious reference
point. The choice of the particular suite of acous-
tic and articulatory measurements that underlie the
metric is not explicitly justified. It is not obvious
how one would decide between different metrics for
modeling phonetic generalizations if more than one
were available.
On the other hand, ALINE was designed with a
specific reference in mind, namely cognate identi-
fication. The ?goodness? of alternative similarity
schemes can be objectively measured on a test set
containing both cognates and unrelated pairs from
various languages.
A perusal of individual distances in Mielke?s met-
ric reveals that some of them seem quite unintuitive.
For example, [t] is closer to [j] than it is to [   ], [  ]
is closer to [n] than to [i], [  ] is closer to [e] than
to [g]. etc. This may be caused either by the omis-
sion of perceptual features from the underlying set
of features, or by the assignment of uniform weights
to different features (Mielke, personal communica-
tion).
It is difficult to objectively measure which pho-
netic similarity scheme produces more ?intuitive?
values. In order to approximate a human evalua-
tion, we performed a comparison with the perceptual
judgments of Laver (1994), who assigned numerical
values to pairwise comparisons of 22 English conso-
nantal phonemes on the basis of ?subjective auditory
impressions?. We counted the number of perceptual
conflicts with respect to Laver?s judgments for both
Mielke?s metric and ALINE?s similarity values. For
example, the triple ([ ], [j], [k]) is an example of a
conflict because [ ] is considered closer to [j] than to
[k] in Mielke?s matrix but the order is the opposite
in Laver?s matrix. The program identified 1246 con-
flicts with Mielke?s metric, compared to 1058 con-
flicts with ALINE?s scheme, out of 4620 triples. We
conclude that in spite of the fact that ALINE is de-
signed for identifying cognates, rather than directly
for phonetic similarity, it is more in agreement with
human perceptual judgments than Mielke?s metric
which was explicitly designed for quantifying pho-
netic similarity.
3 Learning algorithms
In this section, we briefly describe several ma-
chine learning algorithms that automatically derive
weights or probabilities for different edit operations.
3.1 Stochastic transducer
Ristad and Yianilos (1998) attempt to model edit
distance more robustly by using Expectation Max-
imization to learn probabilities for each of the pos-
sible edit operations. These probabilities are then
used to create a stochastic transducer, which scores
a pair of words based on either the most probable
sequence of operations that could produce the two
words (Viterbi scoring), or the sum of the scores of
all possible paths that could have produced the two
words (stochastic scoring). The score of an individ-
ual path here is simply the product of the probabili-
ties of the edit operations in the path. The algorithm
was evaluated on the task of matching surface pro-
nunciations in the Switchboard data to their canoni-
cal pronunciations in a lexicon, yielding a significant
improvement in accuracy over Levenshtein distance.
45
3.2 Levenshtein with learned weights
Mann and Yarowsky (2001) applied the stochastic
transducer of Ristad and Yianilos (1998) for induc-
ing translation lexicons between two languages, but
found that in some cases it offered no improvement
over Levenshtein distance. In order to remedy this
problem, they they proposed to filter the probabili-
ties learned by EM into a few discrete cost classes,
which are then used in the standard edit distance
algorithm. The LLW approach yielded improve-
ment over both regular Levenshtein and the stochas-
tic transducer.
3.3 CORDI
CORDI (Kondrak, 2002) is a program for detect-
ing recurrent sound correspondences in bilingual
wordlists. The idea is to relate recurrent sound cor-
respondences in wordlists to translational equiva-
lences in bitexts. A translation model is induced be-
tween phonemes in two wordlists by combining the
maximum similarity alignment with the competitive
linking algorithm of Melamed (2000). Melamed?s
approach is based on the one-to-one assumption,
which implies that every word in the bitext is aligned
with at most one word on the other side of the bitext.
In the context of the bilingual wordlists, the cor-
respondences determined under the one-to-one as-
sumption are restricted to link single phonemes to
single phonemes. Nevertheless, the method is pow-
erful enough to determine valid correspondences in
wordlists in which the fraction of cognate pairs is
well below 50%.
The discovered phoneme correspondences can be
used to compute a correspondence-based similar-
ity score between two words. Each valid corre-
spondence is counted as a link and contributes a
constant positive score (no crossing links are al-
lowed). Each unlinked segment, with the exception
of the segments beyond the rightmost link, is as-
signed a smaller negative score. The alignment with
the highest score is found using dynamic program-
ming (Wagner and Fischer, 1974). If more than one
best alignment exists, links are assigned the weight
averaged over the entire set of best alignments. Fi-
nally, the score is normalized by dividing it by the
average of the lengths of the two words.
3.4 Pair HMM
Mackay and Kondrak (2005) propose to computing
similarity between pairs of words with a technique
adapted from the field of bioinformatics. A Pair Hid-
den Markov Model differs form a standard HMM by
producing two output streams in parallel, each corre-
sponding to a word that is being aligned. The model
has three states that correspond to the basic edit op-
erations: substitution, insertion, and deletion. The
parameters of the model are automatically learned
from training data that consists of word pairs that
are known to be similar. The model is trained using
the Baum-Welch algorithm (Baum et al, 1970).
4 Dynamic Bayesian Nets
A Bayesian Net is a directed acyclic graph in which
each of the nodes represents a random variable.
The random variable can be either deterministic, in
which case the node can only take on one value for a
given configuration of its parents, or stochastic, in
which case the configuration of the parents deter-
mines the probability distribution of the node. Arcs
in the net represent dependency relationships.
Filali and Bilmes (2005) proposed to use Dy-
namic Bayesian Nets (DBNs) for computing word
similarity. A DBN is a Bayesian Net where a set
of arcs and nodes are maintained for each point in
time in a dynamic process. This involves set of pro-
logue frames denoting the beginning of the process,
chunk frames which are repeated for the middle of
the process, and epilogue frames to end the process.
The conditional probability relationships are time-
independent. DBNs can encode quite complex in-
terdependencies between states.
We tested four different DBN models on the task
of cognate identification. In the following descrip-
tion of the models, Z denotes the current edit opera-
tion, which can be either a substitution, an insertion,
or a deletion.
MCI The memoriless context-independent model
(Figure 1) is the most basic model, which is
meant to be equivalent to the stochastic trans-
ducer of Ristad and Yianilos (1998). Its lack
of memory signifies that the probability of Z
taking on a given value does not depend in any
way on what previous values of Z have been.
The context-independence refers to the fact that
46
end
sc
Z
tc
t
b
a
s
(P) (C) (E)
sc
Z
tc
t
b
a
s
end
sc
Z
tc
t
b
a
s
tend
send
Figure 1: The MCI model.
the probability of Z taking on a certain value
does not depend on the letters of the source or
target word. The a and b nodes in Figure 1 rep-
resent the current position in the source and tar-
get words, respectively. The s and t nodes rep-
resent the current letter in the source and target
words. The end node is a switching parent of Z
and is triggered when the values of the a and b
nodes move past the end of both the source and
target words. The sc and tc nodes are consis-
tency nodes which ensure that the current edit
operation is consistent with the current letters
in the source and target words. Consistency
here means that the source side of the edit oper-
ation must either match the current source letter
or be ?, and that the same be true for the target
side. Finally, the send and tend nodes appear
only in the last frame of the model, and are only
given a positive probability if both words have
already been completely processed, or if the
final edit operation will conclude both words.
The following models all use the MCI model
as a basic framework, while adding new depen-
dencies to Z.
MEM In the memory model, the probability of the
current operation being performed depends on
what the previous operation was.
CON In the context-dependent model, the probabil-
ity that Z takes on certain values is dependent
on letters in the source word or target word.
The model that we test in Section 5, takes into
account the context of two letters in the source
word: the current one and the immediately
preceding one. We experimented with several
other variations of context sets, but they either
performed poorly on the development set, or re-
quired inordinate amounts of memory.
LEN The length model learns the probability dis-
tribution of the number of edit operations to
be performed, which is the incorporated into
the similarity score. This model represents an
attempt to counterbalance the effect of longer
words being assigned lower probabilities.
The models were implemented with the GMTK
toolkit (Bilmes and Zweig, 2002). A more detailed
description of the models can be found in (Filali and
Bilmes, 2005).
5 Experiments
5.1 Setup
We evaluated various methods for computing word
similarity on the task of the identification of cog-
nates. The input consists of pairs of words that
have the same meaning in distinct languages. For
each pair, the system produces a score represent-
ing the likelihood that the words are cognate. Ide-
ally, the scores for true cognate pairs should always
be higher than scores assigned to unrelated pairs.
For binary classification, a specific score thresh-
old could be applied, but we defer the decision on
the precision-recall trade-off to downstream applica-
tions. Instead, we order the candidate pairs by their
scores, and evaluate the ranking using 11-point in-
terpolated average precision (Manning and Schutze,
2001). Scores are normalized by the length of the
longer word in the pair.
Word similarity is not always a perfect indicator
of cognation because it can also result from lexical
borrowing and random chance. It is also possible
that two words are cognates and yet exhibit little sur-
face similarity. Therefore, the upper bound for aver-
age precision is likely to be substantially lower than
100%.
47
Languages Proportion Method
of cognates EDIT MIEL ALINE R&Y LLW PHMM DBN
English German 0.590 0.906 0.909 0.912 0.894 0.918 0.930 0.927
French Latin 0.560 0.828 0.819 0.862 0.889 0.922 0.934 0.923
English Latin 0.290 0.619 0.664 0.732 0.728 0.725 0.803 0.822
German Latin 0.290 0.558 0.623 0.705 0.642 0.645 0.730 0.772
English French 0.275 0.624 0.623 0.623 0.684 0.720 0.812 0.802
French German 0.245 0.501 0.510 0.534 0.475 0.569 0.734 0.645
Albanian Latin 0.195 0.597 0.617 0.630 0.568 0.602 0.680 0.676
Albanian French 0.165 0.643 0.575 0.610 0.446 0.545 0.653 0.658
Albanian German 0.125 0.298 0.340 0.369 0.376 0.345 0.379 0.420
Albanian English 0.100 0.184 0.287 0.302 0.312 0.378 0.382 0.446
AVERAGE 0.2835 0.576 0.597 0.628 0.601 0.637 0.704 0.709
Table 1: 11-point average cognate identification precision for various methods.
5.2 Data
The training data for our cognate identification ex-
periments comes from the Comparative Indoeuro-
pean Data Corpus (Dyen et al, 1992). The data con-
tains word lists of 200 basic meanings representing
95 speech varieties from the Indoeuropean family
of languages. Each word is represented in an or-
thographic form without diacritics using the 26 let-
ters of the Roman alphabet. Approximately 180,000
cognate pairs were extracted from the corpus.
The development set was composed of three lan-
guage pairs: Italian-Croatian, Spanish-Romanian,
and Polish-Russian. We chose these three language
pairs because they represent very different levels of
relatedness: 25.3%, 58.5%, and 73.5% of the word
pairs are cognates, respectively. The percentage of
cognates within the data is important, as it provides
a simple baseline from which to compare the success
of our algorithms. If our cognate identification pro-
cess were random, we would expect to get roughly
these percentages for our recognition precision (on
average).
The test set consisted of five 200-word lists repre-
senting English, German, French, Latin, and Alba-
nian, compiled by Kessler (2001). The lists for these
languages were removed from the training data (ex-
cept Latin, which was not part of the training set), in
order to keep the testing and training data as sepa-
rate as possible. For the supervised experiments, we
converted the test data to have the same orthographic
representation as the training data.
The training process for the DBN models con-
sisted of three iterations of Expectation Maximiza-
tion, which was determined to be optimal on the de-
velopment data. Each pair was used twice, once in
each source-target direction, to enforce the symme-
try of the scoring, One of the models, the context-
dependent model, remained asymmetrical despite to
two-way training. In order to remove the undesir-
able asymmetry, we averaged the scores in both di-
rections for each word pair.
5.3 Results
Table 1 shows the average cognate identification
precision on the test set for a number of meth-
ods. EDIT is a baseline edit distance with uniform
costs. MIEL refers to edit distance with weights
computed using the approach outlined in (Mielke,
2005). ALINE denotes the algorithm for aligning
phonetic sequences (Kondrak, 2000) described in
Section 2.1. R&Y is the stochastic transducer of
Ristad and Yianilos (1998). LLW stands for Lev-
enshtein with learned weights, which is a modifi-
cation of R&Y proposed by Mann and Yarowsky
(2001). The PHMM column provides the results
reported in (Mackay and Kondrak, 2005) for the
best Pair HMM model, which uses log odds scor-
ing. Finally, DBN stands for our best results ob-
tained with a DBN model, in this case the averaged
context model.
Table 2 show the aggregate results for various
DBN models. Two different results are given for
each model: the raw score, and the score normal-
48
Model Raw Score Normalized
MCI 0.515 0.601
MEM 0.563 0.595
LEN 0.516 0.587
CON-FOR 0.582 0.599
CON-REV 0.624 0.619
CON-AVE 0.629 0.709
Table 2: Average cognate identification precision for
various DBN models.
ized by the length of the longer word. The mod-
els are the memoriless context-independent model
(MCI), memory model (MEM), length model (LEN)
and context model (CON). The context model re-
sults are split as follows: results in the original di-
rection (FOR), results with all word pairs reversed
(REV), and the results of averaging the scores for
each word pair in the forward and reverse directions
(AVE).
Table 3 shows the aggregate results for the un-
supervised approaches. In the unsupervised tests,
the training set was not used, as the models were
trained directly on the testing data without access
to the cognation information. For the unsupervised
tests, the original, the test set was in its original pho-
netic form. The table compares the results obtained
with various DBN models and with the CORDI al-
gorithm described in Section 3.3.
5.4 Discussion
The results in Table 1 strongly suggest that the
learning approaches are more effective than the
manually-designed schemes for cognate identifica-
tion. However, it has to be remembered that the
learning process was conducted on a relatively large
set of Indoeuropean cognates. Even though there
was no overlap between the training and the test
set, the latter also contained cognate pairs from the
same language family. For each of the removed lan-
guages, there are other closely related languages that
are retained in the training set, which may exhibit
similar or even identical regular correspondences.
The manually-designed schemes have the advan-
tage of not requiring any training sets after they
have been developed. Nevertheless, Mielke?s met-
ric appears to produce only small improvement over
Model Raw Score Normalized
MCI 0.462 0.430
MEM 0.351 0.308
LEN 0.464 0.395
CON-AVE 0.433 0.414
CORDI ? 0.629
Table 3: Phonetic test results.
simple edit distance. ALINE outperforms Mielke?s
metric, which is not surprising considering that
ALINE was developed specifically for identifying
cognates, and Mielke?s substitution matrix lacks
several phonemes that occur in the test set.
Among the DBN models, the average context
model performs the best. The averaged context
model is clearly better than either of the unidirec-
tional models on which it is based. It is likely that
the averaging allows the scoring to take contextual
information from both words into account, instead
of just one or the other. The averaged context DBN
model performs about as well as on average as the
Pair HMM approach, but substantially better than
the R&Y approach and its modification, LLW.
In the unsupervised context, all DBN models fail
to perform meaningfully, regardless of whether the
scores are normalized or not. In view of this, it is re-
markable that CORDI achieves a respectable perfor-
mance just by utilizing discovered correspondences,
having no knowledge of phonetics nor identity of
phonemes. The precision of CORDI is at the same
level as the phonetically-based ALINE. In fact, a
method that combines ALINE and CORDI achieves
the average precision of 0.681 on the same test set
(Kondrak, in preparation).
In comparison with the results of Filali and
Bilmes (2005), certain differences are apparent. The
memory and length models, which performed better
than the memoriless context-independent model on
the pronunciation task, perform worse overall here.
This is especially notable in the case of the length
model which was the best overall performer on their
task. The context-dependent model, however, per-
formed well on both tasks.
As mentioned in (Mann and Yarowsky, 2001),
it appears that there are significant differences be-
tween the pronunciation task and the cognate iden-
49
tification task. They offer some hypotheses as to
why this may be the case, such as noise in the data
and the size of the training sets, but these issues are
not apparent in the task presented here. The train-
ing set was quite large and consisted only of known
cognates. The two tasks are inherently different, in
that scoring in the pronunciation task involves find-
ing the best match of a surface pronunciation with
pronunciations in a lexicon, while the cognate task
involves the ordering of scores relative to each other.
Certain issues, such as length of words, may become
more prominent in this setup. We countered this by
normalizing all scores, which was not done in (Filali
and Bilmes, 2005). As can be seen in Table 2, the
normalization by length appears to improve the re-
sults on average. It notable that normalization even
helps the length model on this task, despite the fact
that it was designed to take word length into account.
6 Conclusion
We have compared the effectiveness of a number of
different methods, including the DBN models, on
the task of cognate identification. The results sug-
gest that some of the learning methods, namely the
Pair HMMs and the averaged context DBN model,
outperform the manually designed methods, pro-
vided that large training sets are available.
In the future, we would like to apply DBNs
to other tasks involving computing word similarity
and/or alignment. An interesting next step would be
to use them for tasks involving generation, for ex-
ample the task of machine transliteration.
Acknowledgments
We would like to thank Karim Filali for the DBN
scripts, and for advice about how to use them.
Thanks to Jeff Mielke for making his phoneme sim-
ilarity matrix available for our experiments, and for
commenting on the results. This research was sup-
ported by the Natural Sciences and Engineering Re-
search Council of Canada.
References
Leonard E. Baum, Ted Petrie, George Soules, and Nor-man Weiss. 1970. A maximization technique occur-ring in the statistical analysis of probabilistic function
of Markov chains. The Annals of Mathematical Statis-
tics, 41(1):164?171.
Jeff Bilmes and Geoffrey Zweig. 2002. The graphical
models toolkit: An open source software system forspeech and time-series processing. In Proc. IEEE Intl.
Conf. on Acoustics, Speech, and Signal Processing.
Isidore Dyen, Joseph B. Kruskal, and Paul Black. 1992.An Indoeuropean classification: A lexicostatistical ex-periment. Transactions of the American Philosophical
Society, 82(5).
Karim Filali and Jeff Bilmes. 2005. A dynamic Bayesianframework to model context and memory in edit dis-
tance learning: An application to pronunciation classi-fication. In Proceedings of ACL 2005, pages 338?345.
Brett Kessler. 2001. The Significance of Word Lists.Stanford: CSLI Publications, Stanford, California.
Brett Kessler. 2005. Phonetic comparison algorithms.
Transactions of the Philological Society, 103(2):243?260.
Grzegorz Kondrak. 2000. A new algorithm for the
alignment of phonetic sequences. In Proceedings of
NAACL 2000, pages 288?295.
Grzegorz Kondrak. 2002. Determining recurrent sound
correspondences by inducing translation models. In
Proceedings of COLING 2002, pages 488?494.
John Laver. 1994. Principles of Phonetics. CambridgeUniversity Press.
Wesley Mackay and Grzegorz Kondrak. 2005. Com-
puting word similarity and identifying cognates withPair Hidden Markov Models. In Proceedings of the
9th Conference on Computational Natural Language
Learning (CoNLL), pages 40?47.
Gideon S. Mann and David Yarowsky. 2001. Multipathtranslation lexicon induction via bridge languages. In
Proceedings of NAACL 2001, pages 151?158.
Christopher D. Manning and Hinrich Schutze. 2001.
Foundations of Statistical Natural Language Process-
ing. The MIT Press.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,26(2):221?249.
Jeff Mielke. 2005. Modeling distinctive feature emer-
gence. In Proceedings of the 24th West Cost Confer-
ence on Formal Linguistics, pages 281?289.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 20(5):522?532.
Robert A. Wagner and Michael J. Fischer. 1974. Thestring-to-string correction problem. Journal of the
ACM, 21(1):168?173.
50
