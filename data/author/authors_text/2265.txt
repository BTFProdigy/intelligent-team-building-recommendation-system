Proceedings of the Second Workshop on Statistical Machine Translation, pages 88?95,
Prague, June 2007. c?2007 Association for Computational Linguistics
Efficient Handling of N -gram Language Models
for Statistical Machine Translation
Marcello Federico
Fondazione Bruno Kessler - IRST
I-38050 Trento, Italy
federico@itc.it
Mauro Cettolo
Fondazione Bruno Kessler - IRST
I-38050 Trento, Italy
cettolo@itc.it
Abstract
Statistical machine translation, as well as
other areas of human language processing,
have recently pushed toward the use of large
scale n-gram language models. This paper
presents efficient algorithmic and architec-
tural solutions which have been tested within
the Moses decoder, an open source toolkit
for statistical machine translation. Exper-
iments are reported with a high perform-
ing baseline, trained on the Chinese-English
NIST 2006 Evaluation task and running on
a standard Linux 64-bit PC architecture.
Comparative tests show that our representa-
tion halves the memory required by SRI LM
Toolkit, at the cost of 44% slower translation
speed. However, as it can take advantage
of memory mapping on disk, the proposed
implementation seems to scale-up much bet-
ter to very large language models: decoding
with a 289-million 5-gram language model
runs in 2.1Gb of RAM.
1 Introduction
In recent years, we have seen an increasing interest
toward the application of n-gram Language Mod-
els (LMs) in several areas of computational lin-
guistics (Lapata and Keller, 2006), such as ma-
chine translation, word sense disambiguation, text
tagging, named entity recognition, etc. The origi-
nal framework of n-gram LMs was principally au-
tomatic speech recognition, under which most of
the standard LM estimation techniques (Chen and
Goodman, 1999) were developed. Nowadays, the
availability of larger and larger text corpora is stress-
ing the need for efficient data structures and algo-
rithms to estimate, store and access LMs. Unfortu-
nately, the rate of progress in computer technology
seems for the moment below the space requirements
of such huge LMs, at least by considering standard
lab equipment.
Statistical machine translation (SMT) is today
one of the research areas that, together with speech
recognition, is pushing mostly toward the use of
huge n-gram LMs. In the 2006 NIST Machine
Translation Workshop (NIST, 2006), best perform-
ing systems employed 5-grams LMs estimated on at
least 1.3 billion-word texts. In particular, Google
Inc. presented SMT results with LMs trained on
8 trillion-word texts, and announced the availabil-
ity of n-gram statistics extracted from one trillion
of words. The n-gram Google collection is now
publicly available through LDC, but their effective
use requires either to significantly expand computer
memory, in order to use existing tools (Stolcke,
2002), or to develop new ones.
This work presents novel algorithms and data
structures suitable to estimate, store, and access
very large LMs. The software has been integrated
into a popular open source SMT decoder called
Moses.1 Experimental results are reported on the
Chinese-English NIST task, starting from a quite
well-performing baseline, that exploits a large 5-
gram LM.
This paper is organized as follows. Section 2
presents techniques for the estimation and represen-
1http://www.statmt.org/moses/
88
tation in memory of n-gram LMs that try to optimize
space requirements. Section 3 describes methods
implemented in order to efficiently access the LM
at run time, namely by the Moses SMT decoder.
Section 4 presents a list of experiments addressing
specific questions on the presented implementation.
2 Language Model Estimation
LM estimation starts with the collection of n-grams
and their frequency counters. Then, smoothing pa-
rameters are estimated (Chen and Goodman, 1999)
for each n-gram level; infrequent n-grams are possi-
bly pruned and, finally, a LM file is created contain-
ing n-grams with probabilities and back-off weights.
2.1 N -gram Collection
Clearly, a first bottleneck of the process might occur
if all n-grams have to be loaded in memory. This
problem is overcome by splitting the collection of n-
grams statistics into independent steps and by mak-
ing use of an efficient data-structure to collect and
store n-grams. Hence, first the dictionary of the cor-
pus is extracted and split into K word lists, balanced
with respect to the frequency of the words. Then,
for each list, only n-grams whose first word belongs
to the list are extracted from the corpus. The value
of K is determined empirically and should be suffi-
ciently large to permit to fit the partial n-grams into
memory. The collection of each subset of n-grams
exploits a dynamic prefix-tree data structure shown
in Figure 1. It features a table with all collected 1-
grams, each of which points to its 2-gram succes-
sors, namely the 2-grams sharing the same 1-gram
prefix. All 2-gram entries point to all their 3-gram
successors, and so on. Successor lists are stored
in memory blocks allocated on demand through a
memory pool. Blocks might contain different num-
ber of entries and use 1 to 6 bytes to encode fre-
quencies. In this way, a minimal encoding is used
in order to represent the highest frequency entry of
each block. This strategy permits to cope well with
the high sparseness of n-grams and with the pres-
ence of relatively few highly-frequent n-grams, that
require counters encoded with 6 bytes.
The proposed data structure differs from other im-
plementations mainly in the use of dynamic alloca-
tion of memory required to store frequencies of n-
3  
w | fr | succ | ptr | flags
6 3  8  1  
3  
w | fr
1 
1-gr   
2-gr   
3-gr   
Figure 1: Dynamic data structure for storing n-
grams. Blocks of successors are allocated on de-
mand and might vary in the number of entries
(depth) and bytes used to store counters (width).
Size in bytes is shown to encode words (w), frequen-
cies (fr), and number of (succ), pointer to (ptr) and
table type of (flags) successors.
grams. In the structure proposed by (Wessel et al,
1997) counters of n-grams occurring more than once
are stored into 4-byte integers, while singleton n-
grams are stored in a special table with no counters.
This solution permits to save memory at the cost of
computational overhead during the collection of n-
grams. Moreover, for historical reasons, this work
ignores the issue with huge counts. In the SRILM
toolkit (Stolcke, 2002), n-gram counts are accessed
through a special class type. Counts are all repre-
sented as 4-byte integers by applying the following
trick: counts below a given threshold are represented
as unsigned integers, while those above the thresh-
old, which are typically very sparse, correspond in-
deed to indexes of a table storing their actual value.
To our opinion, this solution is ingenious but less
general than ours, which does not make any assump-
tion about the number of different high order counts.
2.2 LM Smoothing
For the estimation of the LM, a standard interpo-
lation scheme (Chen and Goodman, 1999) is ap-
plied in combination with a well-established and
simple smoothing technique, namely the Witten-
Bell linear discounting method (Witten and Bell,
1991). Smoothing of probabilities up from 2-grams
is performed separately on each subset of n-grams.
89
For example, smoothing statistics for a 5-gram
(v, w, x, y, z) are computed by means of statistics
that are local to the subset of n-grams starting with
v. Namely, they are the counters N(v, w, x, y, z),
N(v, w, x, y), and the number D(v, w, x, y) of dif-
ferent words observed in context (v, w, x, y).
Finally, K LM files are created, by just read-
ing through the n-gram files, which are indeed not
loaded in memory. During this phase pruning of in-
frequent n-grams is also permitted. Finally, all LM
files are joined, global 1-gram probabilities are com-
puted and added, and a single large LM file, in the
standard ARPA format (Stolcke, 2002), is generated.
We are well aware that the implemented smooth-
ing method is below the state-of-the-art. However,
from one side, experience tells that the gap in per-
formance between simple and sophisticated smooth-
ing techniques shrinks when very large corpora are
used; from the other, the chosen smoothing method
is very suited to the kind of decomposition we are
applying to the n-gram statistics. In the future, we
will nevertheless address the impact of more sophis-
ticated LM smoothing on translation performance.
2.3 LM Compilation
The final textual LM can be compiled into a binary
format to be efficiently loaded and accessed at run-
time. Our implementation follows the one adopted
by the CMU-Cambridge LM Toolkit (Clarkson and
Rosenfeld, 1997) and well analyzed in (Whittaker
and Raj, 2001). Briefly, n-grams are stored in
a data structure which privileges memory saving
rather than access time. In particular, single com-
ponents of each n-gram are searched, via binary
search, into blocks of successors stored contigu-
ously (Figure 2). Further improvements in mem-
ory savings are obtained by quantizing both back-off
weights and probabilities.
2.4 LM Quantization
Quantization provides an effective way of reducing
the number of bits needed to store floating point
variables. (Federico and Bertoldi, 2006) showed that
best results were achieved with the so-called binning
method. This method partitions data points into uni-
formly populated intervals or bins. Bins are filled in
in a greedy manner, starting from the lowest value.
The center of each bin corresponds to the mean value
1-gr 2-gr 3-gr
3  
w   | bo | pr | idx
1 1  4  
w  | pr
3  1  
Figure 2: Static data structure for LMs. Number of
bytes are shown used to encode single words (w),
quantized back-off weights (bo) and probabilities
(pr), and start index of successors (idx).
of all its points. Quantization is applied separately
at each n-gram level and distinctly to probabilities
or back-off weights. The chosen level of quantiza-
tion is 8 bits (1 byte), that experimentally showed to
introduce negligible loss in translation performance.
The quantization algorithm can be applied to any
LM represented with the ARPA format. Quantized
LMs can also be converted into a binary format that
can be efficiently uploaded at decoding time.
3 Language Model Access
One motivation of this work is the assumption that
efficiency, both in time and space, can be gained by
exploiting peculiarities of the way the LM is used
by the hosting program, i.e. the SMT decoder. An
analysis of the interaction between the decoder and
the LM was carried out, that revealed some impor-
tant properties. The main result is shown in Figure 3,
which plots all calls to a 3-gram LM by Moses dur-
ing the translation from German to English of the
following text, taken from the Europarl task:
ich bin kein christdemokrat und
glaube daher nicht an wunder .
doch ich mo?chte dem europa?ischen
parlament , so wie es gegenwu?rtig
beschaffen ist , fu?r seinen
grossen beitrag zu diesen arbeiten
danken.
Translation of the above text requires about 1.7 mil-
lion calls of LM probabilities, that however involve
only 120,000 different 3-grams. The plot shows typ-
ical locality phenomena, that is the decoder tends to
90
Figure 3: LM calls during translation of a German
text: each point corresponds to a specific 3-gram.
access the LM n-grams in nonuniform, highly local-
ized patterns. Locality is mainly temporal, namely
the first call of an n-gram is easily followed by
other calls of the same n-gram. This property sug-
gests that gains in access speed can be achieved by
exploiting a cache memory in which to store al-
ready called n-grams. Moreover, the relatively small
amount of involved n-grams makes viable the access
of the LM from disk on demand. Both techniques
are briefly described.
3.1 Caching of probabilities
In order to speed-up access time of LM probabilities
different cache memories have been implemented
through the use of hash tables. Cache memories are
used to store all final n-gram probabilities requested
by the decoder, LM states used to recombine theo-
ries, as well as all partial n-gram statistics computed
by accessing the LM structure. In this way, the need
of performing binary searches, at every level of the
LM tables, is reduced at a minimum.
All cache memories are reset before decoding
each single input set.
3.2 Memory Mapping
Since a limited collection of all n-grams is needed
to decode an input sentence, the LM is loaded on
demand from disk. The data structure shown in Fig-
ure 2 permits indeed to efficiently exploit the so-
called memory mapped file access.2 Memory map-
ping basically permits to include a file in the address
2POSIX-compliant operating systems and Windows support
some form of memory-mapped file access.
Memory
1-gr 2-gr 3-gr
Disk file
Figure 4: Memory mapping of the LM on disk.
Only the memory pages (grey blocks) of the LM that
are accessed while decoding the input sentence are
loaded in memory.
space of a process, whose access is managed as vir-
tual memory (see Figure 4).
During decoding of a sentence, only those n-
grams, or better memory pages, of the LM that are
actually accessed are loaded into memory, which re-
sults in a significant reduction of the resident mem-
ory space required by the process. Once the decod-
ing of the input sentence is completed, all loaded
pages are released, so that resident memory is avail-
able for the n-gram probabilities of the following
sentence. A remarkable feature is that memory-
mapping also permits to share the same address
space among multiple processes, so that the same
LM can be accessed by several decoding processes
(running on the same machine).
4 Experiments
In order to assess the quality of our implementa-
tion, henceforth named IRSTLM, we have designed
a suite of experiments with a twofold goal: from
one side the comparison of IRSTLM against a pop-
ular LM library, namely the SRILM toolkit (Stol-
cke, 2002); from the other, to measure the actual
impact of the implementation solution discussed in
previous sections. Experiments were performed on a
common statistical MT platform, namely Moses, in
which both the IRSTLM and SRILM toolkits have
been integrated.
The following subsection lists the questions
91
set type |W|
source target
large parallel 83.1M 87.6M
giga monolingual - 1.76G
NIST 02 dev 23.7K 26.4K
NIST 03 test 25.6K 28.5K
NIST 04 test 51.0K 58.9K
NIST 05 test 31.2K 34.6K
NIST 06 nw test 18.5K 22.8K
NIST 06 ng test 9.4K 11.1K
NIST 06 bn test 12.0K 13.3K
Table 1: Statistics of training, dev. and test sets.
Evaluation sets of NIST campaigns include 4 ref-
erences: in table, average lenghts are provided.
which our experiments aim to answer.
Assessing Questions
1. Is LM estimation feasible for large amounts of
data?
2. How does IRSTLM compare with SRILM
w.r.t.:
(a) decoding speed?
(b) memory requirements?
(c) translation performance?
3. How does LM quantization impact in terms of
(a) memory consumption?
(b) decoding speed?
(c) translation performance?
(d) tuning of decoding parameters?
4. What is the impact of caching on decoding
speed?
5. What are the advantages of memory mapping?
Task and Experimental Setup
The task chosen for our experiments is the transla-
tion of news from Chinese to English, as proposed
by the NIST MT Evaluation Workshop of 2006.3
A translation system was trained according to the
large-data condition. In particular, all the allowed
bilingual corpora have been used for estimating the
phrase-table. The target side of these texts was also
employed for the estimation of three 5-gram LMs,
henceforth named large. In particular, two LMs
3www.nist.gov/speech/tests/mt/
were estimated with the SRILM toolkit by prun-
ing singletons events and by employing the Witten-
Bell and the absolute discounting (Kneser and Ney,
1995) smoothing methods; the shorthand for these
two LMs will be ?lrg-sri-wb? and ?lrg-sri-kn?, re-
spectively. Another large LM was estimated with the
IRSTLM toolkit, by employing the only smoothing
method available in the package (Witten-Bell) and
by pruning singletons n-grams; its shorthand will be
?lrg?. An additional, much larger, 5-gram LM was
instead trained with the IRSTLM toolkit on the so-
called English Gigaword corpus, one of the allowed
monolingual resources for this task.
Automatic translation was performed by means of
Moses which, among other things, permits the con-
temporary use of more LMs, feature we exploited in
our experiments as specified later.
Optimal interpolation weights for the log-linear
model were estimated by running a minimum error
training algorithm, available in the Moses toolkit,
on the evaluation set of the NIST 2002 campaign.
Tests were performed on the evaluation sets of the
successive campaigns (2003 to 2006). Concern-
ing the NIST 2006 evaluation set, results are given
separately for three different types of texts, namely
newswire (nw) and newsgroup (ng) texts, and broad-
cast news transcripts (bn).
Table 1 gives figures about training, development
and test corpora, while Table 2 provides main statis-
tics of the estimated LMs.
LM millions of
1-gr 2-gr 3-gr 4-gr 5-gr
lrg-sri-kn 0.3 5.2 5.9 7.1 6.8
lrg-sri-wb 0.3 5.2 6.4 7.8 6.8
lrg 0.3 5.3 6.6 8.4 8.0
giga 4.5 64.4 127.5 228.8 288.6
Table 2: Statistics of LMs.
MT performance are provided in terms of case-
insensitive BLEU and NIST scores, as computed
with the NIST scoring tool. For time reasons,
the decoder run with monotone search; prelimi-
nary experiments showed that this choice does not
affect comparison of LMs. Reported decoding
speed is the elapsed real time measured with the
Linux/UNIX time command divided by the num-
ber of source words to be translated. dual Intel/Xeon
92
CPU 3.20GHz with 8Gb RAM. Experiments run on
dual Intel/Xeon CPUs 3.20GHz/8Gb RAM.
4.1 LM estimation
First of all, let us answer the question (number 1)
on the feasibility of the procedure for the estima-
tion of huge LMs. Given the amount of training data
employed, it is worth to provide some details about
the estimation process of the ?giga? LM. According
to the steps listed in Section 2.1, the whole dictio-
nary was split into K = 14 frequency balanced lists.
Then, 5-grams beginning with words from each list
were extracted and stored. Table 3 shows some fig-
ures about these dictionaries and 5-gram collections.
Note that the dictionary size increases with the list
index: this means only that more frequent words
were used first. This stage run in few hours with
1-2Gb parallel processes.
list dictionary number of 5-grams:
index size observed distinct non-singletons
0 4 217M 44.9M 16.2M
1 11 164M 65.4M 20.7M
2 8 208M 85.1M 27.0M
3 44 191M 83.0M 26.0M
4 64 143M 56.6M 17.8M
5 137 142M 62.3M 19.1M
6 190 142M 64.0M 19.5M
7 548 142M 66.0M 20.1M
8 783 142M 63.3M 19.2M
9 1.3K 141M 67.4M 20.2M
10 2.5K 141M 69.7M 20.5M
11 6.1K 141M 71.8M 20.8M
12 25.4K 141M 74.5M 20.9M
13 4.51M 141M 77.4M 20.6M
total 4.55M 2.2G 951M 289M
Table 3: Estimation of the ?giga? LM: dictionary
and 5-gram statistics (K = 14).
The actual estimation of the LM was performed
with the scheme presented in Section 2.2. For each
collection of non-singletons 5-grams, a sub-LM was
built by computing smoothed n-gram (n = 1 ? ? ? 5)
probabilities and interpolation parameters. Again,
by exploiting parallel processing, this phase took
only few hours on standard HW resources. Finally,
sub-LMs were joined in a single LM, which can be
stored in two formats: (i) the standard textual ARPA
LM format quantization file size
lrg-sri-kn textual n 893Mb
lrg-sri-wb textual n 952Mb
lrg textual n 1088Mb
y 789Mb
binary n 368Mb
y 220Mb
giga textual n 28.0Gb
y 21.0Gb
binary n 8.5Gb
y 5.1Gb
Table 4: Figures of LM files.
format, and (ii) the binary format of Section 2.3. In
addition, LM probabilities can be quantized accord-
ing to the procedure of Section 2.4.
The estimation of the ?lrg-sri? LMs, performed
by means of the SRILM toolkit, took about 15 min-
utes requiring 5Gb of memory. The ?lrg? LM was
estimated as the ?giga? LM in about half an hour
demanding only few hundreds of Mb of memory.
Table 4 lists the size of files storing various ver-
sions of the ?large? and ?giga? LMs which differ in
format and/or type.
4.2 LM run-time usage
Tables 5 and 6 shows BLEU and NIST scores, re-
spectively, measured on test sets for each specific
LM configuration. The first two rows of the two ta-
bles regards runs of Moses with the SRILM, that
uses ?lrg-sri? LMs. The other rows refer to runs of
Moses with IRSTLM, either using LM ?lrg? only,
or both LMs, ?lrg? and ?giga?. LM quantization is
marked by a ?q?.
Finally, in Table 7 figures about the decoding pro-
cesses are recorded. For each LM configuration, the
process size, both virtual and resident, is provided
together with the average time required for translat-
ing a source word with/without the activation of the
caching mechanism described in Section 3.1. It is
to worth noticing that the ?giga? LM (both original
and quantized) is loaded through the memory map-
ping service presented in Section 3.2.
Table 7 includes most of the answers to question
number 2:
2.a Under the same conditions, Moses running
with SRILM permits almost double faster
93
LM NIST test set
03 04 05 06 06 06
nw ng bn
lrg-sri-kn 28.74 30.52 26.99 29.28 23.47 27.27
lrg-sri-wb 28.05 29.86 26.52 28.37 23.13 26.37
lrg 28.49 29.84 26.97 28.69 23.28 26.70
q-lrg 28.05 29.66 26.48 28.58 22.64 26.05
lrg+giga 30.77 31.93 29.09 29.74 24.39 28.50
q-lrg+q-giga 30.42 31.47 28.62 29.76 24.28 28.23
Table 5: BLEU scores on NIST evaluation sets for
different LM configurations.
LM NIST test set
03 04 05 06 06 06
nw ng bn
lrg-sri-kn 8.73 9.29 8.47 8.98 7.81 8.52
lrg-sri-wb 8.52 9.14 8.27 8.96 7.90 8.34
lrg 8.73 9.21 8.45 8.95 7.82 8.47
q-lrg 8.60 9.11 8.32 8.88 7.73 8.31
lrg+giga 9.08 9.49 8.80 8.92 7.86 8.66
q-lrg+q-giga 8.93 9.38 8.65 9.05 7.99 8.60
Table 6: NIST scores on NIST evaluation sets for
different LM configurations.
translation than IRSTLM (13.33 vs. 6.80
words/s). Anyway, IRSTLM can be sped-up to
7.52 words/s by applying caching.
2.b IRSTLM requires about half memory than
SRILM for storing an equivalent LM during
decoding. If the LM is quantized, the gain is
even larger. Concerning file sizes (Table 4), the
size of IRSTLM binary files is about 30% of
the corresponding textual versions. Quantiza-
tion further reduces the size to 20% of the orig-
inal textual format.
2.c Performance of IRSTLM and SRILM on the
large LMs smoothed with the same method are
comparable, as expected (see entries ?lrg-sri-
wb? and ?lrg? of Tables 5 and 6). The small
differences are due to different probability val-
ues assigned by the two libraries to out-of-
vocabulary words.
Concerning quantization, gains in terms of memory
space (question 3.a) have already been highlighted
(see answer 2.b). For the remaining points:
3.b comparing ?lrg? vs. ?q-lrg? rows and
LM process size caching dec. speed
virtual resident (src w/s)
lrg-sri-kn/wb 1.2Gb 1.2Gb - 13.33
lrg 750Mb 690Mb n 6.80
y 7.42
q-lrg 600Mb 540Mb n 6.99
y 7.52
lrg+giga 9.9Gb 2.1Gb n 3.52
y 4.28
q-lrg+q-giga 6.8Gb 2.1Gb n 3.64
y 4.35
Table 7: Process size and decoding speed with/wo
caching for different LM configurations.
?lrg+giga? vs. ?q-lrg+q-giga? rows of Ta-
ble 7, it results that quantization allows only a
marginal decoding time reduction (1-3%)
3.c comparing the same rows of Tables 5 and 6, it
can be claimed that quantization doesn?t affect
translation performance in a significant way
3.d no specific training of decoder weights is re-
quired since the original LM and its quan-
tized version are equivalent. For example,
by translating the NIST 05 test set with the
weights estimated on the ?lrg+giga? configu-
ration, the following BLEU/NIST scores are
got: 28.99/8.79 with the ?q-lrg+q-giga? LMs,
29.09/8.80 with the ?lrg+giga? LMs (the latter
scores are also given in Tables 5 and 6). Em-
ploying weights estimated on ?q-lrg+q-giga?
scores are: 28.58/8.66 with ?lrg+giga? LMs,
28.62/8.65 with ?q-lrg+q-giga? LMs (again
also in Tables 5 and 6). Also on other test sets
differences are negligible.
Table 7 answers the question number 4 on
caching, by reporting the decoding speed-up due to
this mechanism: a gain of 8-9% is observed on ?lrg?
and ?q-lrg? configurations, of 20-21% in case also
?giga/q-giga? LMs are employed.
The answer to the last question is that thanks to
the memory mapping mechanism it is possible run
Moses with huge LMs, which is expected to im-
prove performance. Tables 5 and 6 provide quan-
titative support to the statement. In fact, a gain of
1-2 absolute BLEU was measured on different test
sets when ?giga? LM was employed in addition to
94
NIST test set
03 04 05 06 06 06
nw ng bn
BLEU
ci 33.62 35.04 31.92 32.74 26.18 32.43
cs 31.44 32.99 29.95 30.49 24.35 31.10
NIST
ci 9.27 9.75 9.00 9.24 8.00 8.97
cs 8.88 9.40 8.64 8.82 7.69 8.77
Table 8: Case insensitive (ci) and sensitive (cs)
scores of the best performing system.
?lrg? LM. The SRILM-based decoder would require
a process of about 30Gb to load the ?giga? LM; on
the contrary, the virtual size of the IRSTLM-based
decoder is 6.8Gb, while the actual resident memory
is only 2.1Gb.
4.3 Best Performing System
Experimental results discussed so far are not the best
we are able to get. In fact, the adopted setup fixed
the monotone search and the use of no reordering
model. Then, in order to allow a fair comparison
of the IRSTLM-based Moses system with the ones
participating to the NIST MT evaluation campaigns,
we have (i) set the maximum reordering distance to
6 and (ii) estimated a lexicalized reordering model
on the large parallel data by means of the training
option ?orientation-bidirectional-fe?.
Table 8 shows BLEU/NIST scores measured on
test sets by employing the IRSTLM-based Moses
with this setting and employing ?q-lrg+q-giga?
LMs. It ranks at the top 5 systems (out of 24) with
respect to the results of the NIST 06 evaluation cam-
paign.
5 Conclusions
We have presented a method for efficiently estimat-
ing and handling large scale n-gram LMs for the
sake of statistical machine translation. LM estima-
tion is performed by splitting the task with respect
to the initial word of the n-grams, and by merging
the resulting sub-LMs. Estimated LMs can be quan-
tized and compiled in a compact data structure. Dur-
ing the search, LM probabilities are cached and only
the portion of effectively used LM n-grams is loaded
in memory from disk. This method permits indeed
to exploit locality phenomena shown by the search
algorithm when accessing LM probabilities. Results
show an halving of memory requirements, at the cost
of 44% slower decoding speed. In addition, loading
the LM on demand permits to keep the size of mem-
ory allocated to the decoder nicely under control.
Future work will investigate the way for includ-
ing more sophisticated LM smoothing methods in
our scheme and will compare IRSTLM and SRILM
toolkits on increasing size training corpora.
6 Acknowledgments
This work has been funded by the European Union
under the integrated project TC-STAR - Technol-
ogy and Corpora for Speech-to-Speech Translation
- (IST-2002-FP6-506738, http://www.tc-star.org).
References
S.F. Chen and J. Goodman. 1999. An empirical study of
smoothing techniques for language modeling. Computer
Speech and Language, 4(13):359?393.
P. Clarkson and R. Rosenfeld. 1997. Statistical language mod-
eling using the CMU?cambridge toolkit. In Proc. of Eu-
rospeech, pages 2707?2710, Rhodes, Greece.
M. Federico and N. Bertoldi. 2006. How many bits are needed
to store probabilities for phrase-based translation? In Proc.
of the Workshop on Statistical Machine Translation, pages
94?101, New York City, June. Association for Computa-
tional Linguistics.
R. Kneser and H. Ney. 1995. Improved backing-off for m-gram
language modeling. In Proc. of ICASSP, volume 1, pages
181?184, Detroit, MI.
M. Lapata and F. Keller. 2006. Web-based models for natu-
ral language processing. ACM Transactions on Speech and
Language Processing, 1(2):1?31.
NIST. 2006. Proc. of the NIST MT Workshop. Washington,
DC. NIST.
A. Stolcke. 2002. SRILM - an extensible language modeling
toolkit. In Proc. of ICSLP, Denver, Colorado.
F. Wessel, S. Ortmanns, and H. Ney. 1997. Implementation
of word based statistical language models. In Proc. SQEL
Workshop on Multi-Lingual Information Retrieval Dialogs,
pages 55?59, Pilsen, Czech Republic.
E. W. D. Whittaker and B. Raj. 2001. Quantization-based Lan-
guage Model Compression. In Proc. of Eurospeech, pages
33?36, Aalborg.
I. H. Witten and T. C. Bell. 1991. The zero-frequency problem:
Estimating the probabilities of novel events in adaptive text
compression. IEEE Trans. Inform. Theory, IT-37(4):1085?
1094.
95
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 182?189,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Domain Adaptation for Statistical Machine Translation
with Monolingual Resources
Nicola Bertoldi Marcello Federico
FBK-irst - Ricerca Scientifica e Tecnologica
Via Sommarive 18, Povo (TN), Italy
{bertoldi, federico}@fbk.eu
Abstract
Domain adaptation has recently gained
interest in statistical machine translation
to cope with the performance drop ob-
served when testing conditions deviate
from training conditions. The basic idea
is that in-domain training data can be ex-
ploited to adapt all components of an al-
ready developed system. Previous work
showed small performance gains by adapt-
ing from limited in-domain bilingual data.
Here, we aim instead at significant per-
formance gains by exploiting large but
cheap monolingual in-domain data, either
in the source or in the target language.
We propose to synthesize a bilingual cor-
pus by translating the monolingual adap-
tation data into the counterpart language.
Investigations were conducted on a state-
of-the-art phrase-based system trained on
the Spanish?English part of the UN cor-
pus, and adapted on the corresponding
Europarl data. Translation, re-ordering,
and language models were estimated after
translating in-domain texts with the base-
line. By optimizing the interpolation of
these models on a development set the
BLEU score was improved from 22.60%
to 28.10% on a test set.
1 Introduction
A well-known problem of Statistical Machine
Translation (SMT) is that performance quickly de-
grades as soon as testing conditions deviate from
training conditions. The very simple reason is that
the underlying statistical models always tend to
closely approximate the empirical distributions of
the training data, which typically consist of bilin-
gual texts and monolingual target-language texts.
The former provide a means to learn likely trans-
lations pairs, the latter to form correct sentences
with translated words. Besides the general diffi-
culties of language translation, which we do not
consider here, there are two aspects that make
machine learning of this task particularly hard.
First, human language has intrinsically very sparse
statistics at the surface level, hence gaining com-
plete knowledge on translation phrase pairs or tar-
get language n-grams is almost impractical. Sec-
ond, language is highly variable with respect to
several dimensions, style, genre, domain, topics,
etc. Even apparently small differences in domain
might result in significant deviations in the un-
derlying statistical models. While data sparseness
corroborates the need of large language samples in
SMT, linguistic variability would indeed suggest
to consider many alternative data sources as well.
By rephrasing a famous saying we could say that
?no data is better than more and assorted data?.
The availability of language resources for SMT
has dramatically increased over the last decade,
at least for a subset of relevant languages and es-
pecially for what concerns monolingual corpora.
Unfortunately, the increase in quantity has not
gone in parallel with an increase in assortment, es-
pecially for what concerns the most valuable re-
source, that is bilingual corpora. Large parallel
data available to the research community are for
the moment limited to texts produced by interna-
tional organizations (European Parliament, United
Nations, Canadian Hansard), press agencies, and
technical manuals.
The limited availability of parallel data poses
challenging questions regarding the portability of
SMT across different application domains and lan-
guage pairs, and its adaptability with respect to
language variability within the same application
domain.
This work focused on the second issue, namely
the adaptation of a Spanish-to-English phrase-
based SMT system across two apparently close
domains: the United Nation corpus and the Euro-
182
pean Parliament corpus. Cross-domain adaptation
is faced under the assumption that only monolin-
gual texts are available, either in the source lan-
guage or in the target language.
The paper is organized as follows. Section 2
presents previous work on the problem of adap-
tation in SMT; Section 3 introduces the exemplar
task and research questions we addressed; Sec-
tion 4 describes the SMT system and the adapta-
tion techniques that were investigated; Section 5
presents and discusses experimental results; and
Section 6 provides conclusions.
2 Previous Work
Domain adaptation in SMT has been investigated
only recently. In (Eck et al, 2004) adaptation is
limited to the target language model (LM). The
background LM is combined with one estimated
on documents retrieved from the WEB by using
the input sentence as query and applying cross-
language information retrieval techniques. Refine-
ments of this approach are described in (Zhao et
al., 2004).
In (Hildebrand et al, 2005) information retrieval
techniques are applied to retrieve sentence pairs
from the training corpus that are relevant to the test
sentences. Both the language and the translation
models are retrained on the extracted data.
In (Foster and Kuhn, 2007) two basic settings are
investigated: cross-domain adaptation, in which
a small sample of parallel in-domain text is as-
sumed, and dynamic adaptation, in which only
the current input source text is considered. Adap-
tation relies on mixture models estimated on the
training data through some unsupervised cluster-
ing method. Given available adaptation data, mix-
ture weights are re-estimated ad-hoc. A varia-
tion of this approach was also recently proposed
in (Finch and Sumita, 2008). In (Civera and Juan,
2007) mixture models are instead employed to
adapt a word alignment model to in-domain par-
allel data.
In (Koehn and Schroeder, 2007) cross-domain
adaptation techniques were applied on a phrase-
based SMT trained on the Europarl task, in or-
der to translate news commentaries, from French
to English. In particular, a small portion of in-
domain bilingual data was exploited to adapt the
Europarl language model and translation models
by means of linear interpolation techniques. Ueff-
ing et al (2007) proposed several elaborate adap-
tation methods relying on additional bilingual data
synthesized from the development or test set.
Our work is mostly related to (Koehn and
Schroeder, 2007) but explores different assump-
tions about available adaptation data: i.e. only
monolingual in-domain texts are available. The
adaptation of the translation and re-ordering mod-
els is performed by generating synthetic bilingual
data from monolingual texts, similarly to what
proposed in (Schwenk, 2008). Interpolation of
multiple phrase tables is applied in a more prin-
cipled way than in (Koehn and Schroeder, 2007):
all entries are merged into one single table, cor-
responding feature functions are concatenated and
smoothing is applied when observations are miss-
ing. The approach proposed in this paper has
many similarities with the simplest technique in
(Ueffing et al, 2007), but it is applied to a much
larger monolingual corpus.
Finally, with respect to previous work we also
investigate the behavior of the minimum error
training procedure to optimize the combination of
feature functions on a small in-domain bilingual
sample.
3 Task description
This paper addresses the issue of adapting an al-
ready developed phrase-based translation system
in order to work properly on a different domain,
for which almost no parallel data are available but
only monolingual texts.1
The main components of the SMT system are
the translation model, which aims at porting the
content from the source to the target language, and
the language model, which aims at building fluent
sentences in the target language. While the former
is trained with bilingual data, the latter just needs
monolingual target texts. In this work, a lexical-
ized re-ordering model is also exploited to control
re-ordering of target words. This model is also
learnable from parallel data.
Assuming some large monolingual in-domain
texts are available, two basic adaptation ap-
proaches are pursued here: (i) generating syn-
thetic bilingual data with an available SMT sys-
tem and use this data to adapt its translation and
re-ordering models; (ii) using synthetic or pro-
vided target texts to also, or only, adapt its lan-
guage model. The following research questions
1We assume only availability of a development set and an
evaluation set.
183
summarize our basic interest in this work:
? Is automatic generation of bilingual data ef-
fective to tackle the lack of parallel data?
? Is it more effective to use source language
adaptation data or target language adaptation
data?
? Is it convenient to combine models learned
from adaptation data with models learned
from training data?
? How can interpolation of models be effec-
tively learned from small amounts of in-
domain parallel data?
4 System description
The investigation presented in this paper was car-
ried out with the Moses toolkit (Koehn et al,
2007), a state-of-the-art open-source phrase-based
SMT system. We trained Moses in a standard con-
figuration, including a 4-feature translation model,
a 7-feature lexicalized re-ordering model, one LM,
word and phrase penalties.
The translation and the re-ordering model re-
lied on ?grow-diag-final? symmetrized word-to-
word alignments built using GIZA++ (Och and
Ney, 2003) and the training script of Moses. A
5-gram language model was trained on the tar-
get side of the training parallel corpus using the
IRSTLM toolkit (Federico et al, 2008), exploiting
Modified Kneser-Ney smoothing, and quantizing
both probabilities and backoff weights. Decoding
was performed applying cube-pruning with a pop-
limit of 6000 hypotheses.
Log-linear interpolations of feature functions
were estimated with the parallel version of mini-
mum error rate training procedure distributed with
Moses.
4.1 Fast Training from Synthetic Data
The standard procedure of Moses for the estima-
tion of the translation and re-ordering models from
a bilingual corpus consists in three main steps:
1. A word-to-word alignment is generated with
GIZA++.
2. Phrase pairs are extracted from the word-to-
word alignment using the method proposed
by (Och and Ney, 2003); countings and re-
ordering statistics of all pairs are stored. A
word-to-word lexicon is built as well.
3. Frequency-based and lexicon-based direct
and inverted probabilities, and re-ordering
probabilities are computed using statistics
from step 2.
Recently, we enhanced Moses decoder to also
output the word-to-word alignment between the
input sentence and its translation, given that they
have been added to the phrase table at training
time. Notice that the additional information intro-
duces an overhead in disk usage of about 70%, but
practically no overhead at decoding time. How-
ever, when training translation and re-ordering
models from synthetic data generated by the de-
coder, this feature allows to completely skip the
time-expensive step 1.2
We tested the efficiency of this solution for
training a translation model on a synthesized cor-
pus of about 300K Spanish sentences and 8.8M
running words, extracted from the EuroParl cor-
pus. With respect to the standard procedure, the
total training time was reduced by almost 50%,
phrase extraction produced 10% more phrase
pairs, and the final translation system showed a
loss in translation performance (BLEU score) be-
low 1% relative. Given this outcome we decided
to apply the faster procedure in all experiments.
4.2 Model combination
Once monolingual adaptation data is automati-
cally translated, we can use the synthetic parallel
corpus to estimate new language, translation, and
re-ordering models. Such models can either re-
place or be combined with the original models of
the SMT system. There is another simple option
which is to concatenate the synthetic parallel data
with the original training data and re-build the sys-
tem. We did not investigate this approach because
it does not allow to properly balance the contribu-
tion of different data sources, and also showed to
underperform in preliminary work.
Concerning the combination of models, in the
following we explain how Moses was extended
to manage multiple translation models (TMs) and
multiple re-ordering models (RMs).
4.3 Using multiple models in Moses
In Moses, a TM is provided as a phrase table,
which is a set S = {(f? , e?)} of phrase pairs as-
sociated with a given number of features values
2Authors are aware of an enhanced version of GIZA++,
which allows parallel computation, but it was not taken into
account in this work.
184
h(f? , e?;S). In our configuration, 5 features for the
TM (the phrase penalty is included) are taken into
account.
In the first phase of the decoding process, Moses
generates translation options for all possible in-
put phrases f? through a lookup into S; it simply
extracts alternative phrase pairs (f? , e?) for a spe-
cific f? and optionally applies pruning (based on
the feature values and weights) to limit the num-
ber of such pairs. In the second phase of decod-
ing, it creates translation hypotheses of the full
input sentence by combining in all possible ways
(satisfying given re-ordering constraints) the pre-
fetched translation options. In this phase the hy-
potheses are scored, according to all features func-
tions, ranked, and possibly pruned.
When more TMs Sj are available, Moses can
behave in two different ways in pre-fetching the
translation options. It searches a given f? in all sets
and keeps a phrase pair (f? , e?) if it belongs to either
i) their intersection or ii) their union. The former
method corresponds to building one new TM SI ,
whose set is the intersection of all given sets:
SI = {(f? , e?) | ?j (f? , e?) ? Sj}
The set of features of the new TM is the union of
the features of all single TMs. Straightforwardly,
all feature values are well-defined.
The second method corresponds to building one
new TM SU , whose set is the union of all given
sets:
SU = {(f? , e?) | ?j (f? , e?) ? Sj}
Again, the set of features of the new TM is the
union of the features of all single TMs; but for a
phrase pair (f? , e?) belonging to SU \Sj , the feature
values h(f? , e?;Sj) are undefined. In these unde-
fined situations, Moses provides a default value of
0, which is the highest available score, as the fea-
ture values come from probabilistic distributions
and are expressed as logarithms. Henceforth, a
phrase pair belonging to all original sets is penal-
ized with respect to phrase pairs belonging to few
of them only.
To address this drawback, we proposed a
new method3 to compute a more reliable and
smoothed score in the undefined case, based on
the IBM model 1 (Brown et al, 1993). If (f? =
f1, . . . , fl, e? = e1, . . . , el) ? SU \ Sj for any j the
3Authors are not aware of any work addressing this issue.
phrase-based and lexical-based direct features are
defined as follows:
h(f? , e?;Sj) =

(l + 1)m
m?
k=1
l?
h=0
?(ek | fh)
Here, ?(ek | fh) is the probability of ek given fh
provided by the word-to-word lexicon computed
on Sj . The inverted features are defined simi-
larly. The phrase penalty is trivially set to 1. The
same approach has been applied to build the union
of re-ordering models. In this case, however, the
smoothing value is constant and set to 0.001.
As concerns as the use of multiple LMs, Moses
has a very easy policy, consisting of querying each
of them to get the likelihood of a translation hy-
potheses, and uses all these scores as features.
It is worth noting that the exploitation of mul-
tiple models increases the number of features of
the whole system, because each model adds its
set of features. Furthermore, the first approach of
Moses for model combination shrinks the size of
the phrase table, while the second one enlarges it.
5 Evaluation
5.1 Data Description
In this work, the background domain is given by
the Spanish-English portion of the UN parallel
corpus,4 composed by documents coming from
the Office of Conference Services at the UN in
New York spanning the period between 1988 and
1993. The adaptation data come from the Eu-
ropean Parliament corpus (Koehn, 2002) (EP) as
provided for the shared translation task of the
2008 Workshop on Statistical Machine Transla-
tion.5 Development and test sets for this task,
namely dev2006 and test2008, are supplied as
well, and belong to the European Parliament do-
main.
We use the symbol S? (E?) to denote synthetic
Spanish (English) data. Spanish-to-English and
English-to-Spanish systems trained on UN data
were exploited to generate English and Spanish
synthetic portions of the original EP corpus, re-
spectively. In this way, we created two synthetic
versions of the EP corpus, named SE?-EP and S?E-
EP, respectively. All presented translation systems
were optimized on the dev2006 set with respect to
4Distributed by the Linguistic Data Consortium, cata-
logue # LDC94T4A.
5http://www.statmt.org/wmt08
185
the BLEU score (Papineni et al, 2002), and tested
on test2008. (Notice that one reference translation
is available for both sets.) Table 1 reports statistics
of original and synthetic parallel corpora, as well
of the employed development and evaluation data
sets. All the texts were just tokenized and mixed
case was kept. Hence, all systems were developed
to produce case-sensitive translations.
corpus sent Spanish English
word dict word dict
UN 2.5M 50.5M 253K 45.2M 224K
EP 1.3M 36.4M 164K 35.0M 109K
SE?-EP 1.3M 36.4M 164K 35.4M 133K
S?E-EP 1.3M 36.2M 120K 35.0M 109K
dev 2,000 60,438 8,173 58,653 6,548
test 2,000 61,756 8,331 60,058 6,497
Table 1: Statistics of bilingual training corpora,
development and test data (after tokenization).
5.2 Baseline systems
Three Spanish-to-English baseline systems were
trained by exploiting different parallel or mono-
lingual corpora summarized in the first three lines
in Table 2. For each system, the table reports the
perplexity and out-of-vocabulary (OOV) percent-
age of their LM, and its translation performance
achieved on the test set in terms of BLEU score,
NIST score, WER (word error rate) and PER (po-
sition independent error rate).
The distance in style, genre, jargon, etc. be-
tween the UN and the EP corpora is made evident
by the gap in perplexity (Federico and De Mori,
1998) and OOV percentage between their English
LMs: 286 vs 74 and 1.12% vs 0.15%, respectively.
Performance of the system trained on the EP
corpus (third row) can be taken as an upper bound
for any adaptation strategy trying to exploit parts
of the EP corpus, while those of the first line
clearly provide the corresponding lower-bound.
The system in the second row can instead be con-
sider as the lower bound when only monolingual
English adaptation data are assumed.
The synthesis of the SE?-EP corpus was per-
formed with the system trained just on the UN
training data (first row of Table 2), because we had
assumed that the in-domain data were only mono-
lingual Spanish and thus not useful for neither the
TM, RM nor target LM estimation.
Similarly, the system in the last row of Table 2
was developed on the UN corpus to translate the
English part of the EP data to generate the syn-
thetic S?E-EP corpus. Again, any in-domain data
were exploited to train this sytem. Of course, this
system cannot be compared with any other be-
cause of the different translation direction.
In order to compare reported performance with
the state-of-the-art, Table 2 also reports results
of the best system published in the EuroMatrix
project website6 and of the Google online trans-
lation engine.7
5.3 Analysis of the tuning process
It is well-known that tuning the SMT system is
fundamental to achieve good performance. The
standard tuning procedure consists of a minimum
error rate training (mert) (Och and Ney, 2003)
which relies on the availability of a development
data set. On the other hand, the most important
assumption we make is that almost no parallel in-
domain data are available.
conf sent n-best time (min) BLEU (?)
? ? ? ? 22.28
a 2000 1000 2034 23.68 (1.40)
b 2000 200 391 23.67 (1.39)
c 200 1000 866 23.13 (0.85)
d 200 200 551 23.54 (1.26)
Table 3: Global time, not including decoding, of
the tuning process and BLEU score achieved on
the test set by the uniform interpolation weights
(first row), and by the optimal weights with differ-
ent configurations of the tuning parameters.
In a preliminary phase, we investigated different
settings of the tuning process in order to under-
stand how much development data is required to
perform a reliable weight optimization. Our mod-
els were trained on the SE?-EP parallel corpus and
by using uniform interpolation weights the system
achieved a BLEU score of 22.28% on the test set
(see Table 3).
We assumed to dispose of either a regular
in-domain development set of 2,000 sentences
(dev2006), or a small portion of it of just 200 sen-
6http://www.euromatrix.net. Translations of the best sys-
tem were downloaded on November 7th, 2008. Published
results differ because we performed a case-sensitive evalua-
tion.
7Google was queried on November 3rd, 2008.
186
language pair training data PP OOV (%) BLEU NIST WER PER
TM/RM LM
Spanish-English UN UN 286 1.12 22.60 6.51 64.60 48.52
? UN EP 74 0.15 27.83 7.12 60.93 45.19
? EP EP ? ? 32.80 7.84 56.47 41.15
? UN SE?-EP 89 0.21 23.52 6.64 63.86 47.68
? SE?-EP SE?-EP ? ? 23.68 6.65 63.64 47.56
? S?E-EP EP 74 0.15 28.10 7.18 60.86 44.85
? Google na na 28.60 7.55 57.38 57.38
? Euromatrix na na 32.99 7.86 56.36 41.12
English-Spanish UN UN 281 1.39 23.24 6.44 65.81 49.61
Table 2: Description and performance on the test set of compared systems in terms of perplexity, out-of-
vocabulary percentage of their language model, and four translation scores: BLEU, NIST, word-error-
rate, and position-independent error rate. Systems were optimized on the dev2006 development set.
 0
 500
 1000
 1500
 2000
 2500
 5  10  15  20  25  30  35
tim
e (
min
ute
s)
iteration
a) large dev, 1000 bestb) large dev, 200 bestc) small dev, 1000 bestd) small dev, 200 best
 19
 20
 21
 22
 23
 24
 5  10  15  20  25  30  35
BL
EU
 (%
)
iteration
a) large dev, 1000 bestb) large dev, 200 bestc) small dev, 1000 bestd) small dev, 200 best
Figure 1: Incremental time of the tuning process (not including decoding phase) (left) and BLEU score on
the test set using weights produced at each iteration of the tuning process. Four different configurations
of the tuning parameters are considered.
tences. Moreover, we tried to employ either 1,000-
best or 200-best translation candidates during the
mert process.
From a theoretical point of view, computational
effort of the tuning process is proportional to the
square of the number of translation alternatives
generated at each iteration times the number of it-
erations until convergence.
Figure 1 reports incremental tuning time and
translation performance on the test set at each it-
eration. Notice that the four tuning configurations
are ranked in order of complexity. Table 3 sum-
maries the final performance of each tuning pro-
cess, after convergence was reached.
Notice that decoding time is not included in this
plot, as Moses allows to perform this step in par-
allel on a computer cluster. Hence, to our view
the real bottleneck of the tuning process is actu-
ally related to the strictly serial part of the mert
implementation of Moses.
As already observed in previous literature
(Macherey et al, 2008), first iterations of the tun-
ing process produces very bad weights (even close
to 0); this exceptional performance drop is at-
tributed to an over-fitting on the candidate reposi-
tory.
Configurations exploiting the small develop-
ment set (c,d) show a slower and more unstable
convergence; however, their final performance in
Table 3 result only slightly lower than that ob-
tained with the standard dev sets (a, b). Due to the
larger number of iterations they needed, both con-
figurations are indeed more time consuming than
the intermediate configuration (b), which seems
the best one. In conclusion, we found that the size
of the n-best list has essentially no effect on the
quality of the final weights, but it impacts signif-
icantly on the computational time. Moreover, us-
ing the regular development set with few transla-
tion alternatives ends up to be the most efficient
187
configuration in terms of computational effort, ro-
bustness, and performance.
Our analysis suggests that it is important to dis-
pose of a sufficiently large development set al
though reasonably good weights can be obtained
even if such data are very few.
5.4 LM adaptation
A set of experiments was devoted to the adapta-
tion of the LM only. We trained three different
LMs on increasing portions of the EP and we em-
ployed them either alone or in combination with
the background LM trained on the UN corpus.
2 LMs (+UN)1 LM
  20
  22
  24
  26
  30
10050250
B
L
E
U
 
(
%
)
Percentage of monolingual English adaptation data
  28
Figure 2: BLEU scores achieved by systems ex-
ploiting one or two LMs trained on increasing per-
centages of English in-domain data.
Figure 2 reports BLEU score achieved by these
systems. The absolute gain with respect to the
baseline is fairly high, even with the smallest
amount of adaptation data (+4.02). The benefit
of using the background data together with in-
domain data is very small, and rapidly vanishes
as the amount of such data increases.
If English synthetic texts are employed to adapt
the LM component, the increase in performance
is significantly lower but still remarkable (see Ta-
ble 2). By employing all the available data, the
gain in BLEU% score was of 4% relative, that is
from 22.60 to 23.52.
5.5 TM and RM adaptation
Another set of experiments relates to the adapta-
tion of the TM and the RM. In-domain TMs and
RMs were estimated on three different versions of
the full parallel EP corpus, namely EP, SE?-EP, and
S?E-EP. In-domain LMs were trained on the cor-
responding English side. All in-domain models
were either used alone or combined with the base-
line models according to multiple-model paradigm
explained in Section 4.3. Tuning of the interpola-
tion weights was performed on the standard devel-
opment set as usual. Results of these experiments
are reported in Figure 3.
Results suggest that regardless of the used bilin-
gual corpora the in-domain TMs and RMs work
better alone than combined with the original mod-
els. We think that this behavior can be explained
by a limited disciminative power of the result-
ing combined model. The background translation
model could contain phrases which either do or
do not fit the adaptation domain. As the weights
are optimized to balance the contribution of all
phrases, the system is not able to well separate the
positive examples from the negative ones. In ad-
dition to it, system tuning is much more complex
because the number of features increases from 14
to 26.
Finally, TMs and RMs estimated from synthetic
data show to provide smaller, but consistent, con-
tributions than the corresponding LMs. When En-
glish in-domain data is provided, BLEU% score
increases from 22.60 to 28.10; TM and RM con-
tribute by about 5% relative, by covering the gap
from 27.83 to 28.10. When Spanish in-domain
data is provided BLEU% score increases from
22.60 to 23.68; TM and RM contribute by about
15% relative, by covering the gap from 23.52 to
23.68 .
Summarizing, the most important role in the do-
main adaptation is played by the LM; nevertheless
the adaptation of the TM and RM gives a small
further improvement..
2 TMs, RMs, LMs (+UN)1 TM, RM, LM
  20
  22
  24
  26
  28
  32
  34
bilingualEnglishSpanishnothing
B
L
E
U
 
(
%
)
Type of adaptation data
  30
Figure 3: BLEU scores achieved by system ex-
ploiting both TM, RM and LM trained on different
corpora.
6 Conclusion
This paper investigated cross-domain adaptation
of a state-of-the-art SMT system (Moses), by ex-
ploiting large but cheap monolingual data. We
proposed to generate synthetic parallel data by
188
translating monolingual adaptation data with a
background system and to train statistical models
from the synthetic corpus.
We found that the largest gain (25% relative) is
achieved when in-domain data are available for the
target language. A smaller performance improve-
ment is still observed (5% relative) if source adap-
tation data are available. We also observed that the
most important role is played by the LM adapta-
tion, while the adaptation of the TM and RM gives
consistent but small improvement.
We also showed that a very tiny development set
of only 200 parallel sentences is adequate enough
to get comparable performance as a 2000-sentence
set.
Finally, we described how to reduce the time
for training models from a synthetic corpus gen-
erated through Moses by 50% at least, by exploit-
ing word-alignment information provided during
decoding.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?312.
Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 177?180,
Prague, Czech Republic.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language model adaptation for statistical machine
translation based on information retrieval. In Pro-
ceedings of the International Conference on Lan-
guage Resources and Evaluation (LREC), pages
327?330, Lisbon, Portugal.
Marcello Federico and Renato De Mori. 1998. Lan-
guage modelling. In Renato DeMori, editor, Spoken
Dialogues with Computers, chapter 7, pages 199?
230. Academy Press, London, UK.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. Irstlm: an open source toolkit for han-
dling large scale language models. In Proceedings
of Interspeech, pages 1618?1621, Melbourne, Aus-
tralia.
Andrew Finch and Eiichiro Sumita. 2008. Dy-
namic model interpolation for statistical machine
translation. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 208?215,
Columbus, Ohio.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 128?135, Prague, Czech Republic.
Almut Silja Hildebrand, Matthias Eck, Stephan Vo-
gel, and Alex Waibel. 2005. Adaptation of the
translation model for statistical machine translation
based on information retrieval. In Proceedings of
the 10th Conference of the European Association for
Machine Translation (EAMT), pages 133?142, Bu-
dapest.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation. Unpublished,
http://www.isi.edu/?koehn/europarl/.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
725?734, Honolulu, Hawaii.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association of
Computational Linguistics (ACL), pages 311?318,
Philadelphia, PA.
Holger Schwenk. 2008. Investigations on Large-Scale
Lightly-Supervised Training for Statistical Machine
Translation. In Proc. of the International Workshop
on Spoken Language Translation, pages 182?189,
Hawaii, USA.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Semi-supervised model adaptation
for statistical machine translation. Machine Trans-
lation, 21(2):77?94.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation via structured query models. In Pro-
ceedings of Coling 2004, pages 411?417, Geneva,
Switzerland.
189
A Web-based Demonstrator of a Multi-lingual Phrase-based
Translation System
Roldano Cattoni, Nicola Bertoldi, Mauro Cettolo, Boxing Chen and Marcello Federico
ITC-irst - Centro per la Ricerca Scientifica e Tecnologica
38050 Povo - Trento, Italy
{surname}@itc.it
Abstract
This paper describes a multi-lingual
phrase-based Statistical Machine Transla-
tion system accessible by means of a Web
page. The user can issue translation re-
quests from Arabic, Chinese or Spanish
into English. The same phrase-based sta-
tistical technology is employed to realize
the three supported language-pairs. New
language-pairs can be easily added to the
demonstrator. The Web-based interface al-
lows the use of the translation system to
any computer connected to the Internet.
1 Introduction
At this time, Statistical Machine Translation
(SMT) has empirically proven to be the most
competitive approach in international competi-
tions like the NIST Evaluation Campaigns1 and
the International Workshops on Spoken Language
Translation (IWSLT-20042 and IWSLT-20053).
In this paper we describe our multi-lingual
phrase-based Statistical Machine Translation sys-
tem which can be accessed by means of a Web
page. Section 2 presents the general log-linear
framework to SMT and gives an overview of
our phrase-based SMT system. In section 3
the software architecture of the demo is out-
lined. Section 4 focuses on the currently supported
language-pairs: Arabic-to-English, Chinese-to-
English and Spanish-to-English. In section 5 the
Web-based interface of the demo is described.
1http://www.nist.gov/speech/tests/mt/
2http://www.slt.atr.jp/IWSLT2004/
3http://www.is.cs.cmu.edu/iwslt2005/
2 SMT System Description
2.1 Log-Linear Model
Given a string f in the source language, the goal of
the statistical machine translation is to select the
string e in the target language which maximizes
the posterior distribution Pr(e | f). By introduc-
ing the hidden word alignment variable a, the fol-
lowing approximate optimization criterion can be
applied for that purpose:
e? = arg max
e
Pr(e | f)
= arg max
e
?
a
Pr(e,a | f)
? arg max
e,a
Pr(e,a | f)
Exploiting the maximum entropy (Berger et
al., 1996) framework, the conditional distribu-
tion Pr(e,a | f) can be determined through
suitable real valued functions (called features)
hr(e, f ,a), r = 1 . . . R, and takes the parametric
form:
p?(e,a | f) ? exp{
R
?
r=1
?rhr(e, f ,a)}
The ITC-irst system (Chen et al, 2005) is
based on a log-linear model which extends the
original IBM Model 4 (Brown et al, 1993)
to phrases (Koehn et al, 2003; Federico and
Bertoldi, 2005). In particular, target strings e are
built from sequences of phrases e?1 . . . e?l. For each
target phrase e? the corresponding source phrase
within the source string is identified through three
random quantities: the fertility ?, which estab-
lishes its length; the permutation pii, which sets
its first position; the tablet f? , which tells its word
string. Notice that target phrases might have fer-
tility equal to zero, hence they do not translate any
91
source word. Moreover, uncovered source posi-
tions are associated to a special target word (null)
according to specific fertility and permutation ran-
dom variables.
The resulting log-linear model applies eight fea-
ture functions whose parameters are either esti-
mated from data (e.g. target language models,
phrase-based lexicon models) or empirically fixed
(e.g. permutation models). While feature func-
tions exploit statistics extracted from monolingual
or word-aligned texts from the training data, the
scaling factors ? of the log-linear model are esti-
mated on the development data by applying a min-
imum error training procedure (Och, 2004).
2.2 Decoding Strategy
The translation of an input string is performed by
the SMT system in two steps. In the first pass a
beam search algorithm (decoder) computes a word
graph of translation hypotheses. Hence, either
the best translation hypothesis is directly extracted
from the word graph and output, or an N-best list
of translations is computed (Tran et al, 1996). The
N-best translations are then re-ranked by applying
additional features and the top ranking translation
is finally output.
The decoder exploits dynamic programming,
that is the optimal solution is computed by expand-
ing and recombining previously computed partial
theories. A theory is described by its state which is
the only information needed for its expansion. Ex-
panded theories sharing the same state are recom-
bined, that is only the best scoring one is stored
for further expansions. In order to output a word
graph of translations, backpointers to all expanded
theories are mantained, too.
To cope with the large number of generated the-
ories some approximations are introduced during
the search: less promising theories are pruned off
(beam search) and a new source position is se-
lected by limiting the number of vacant positions
on the left-hand and the distance from the left most
vacant position (re-ordering constraints).
2.3 Phrase extraction and model training
Training of the phrase-based translation model
requires a parallel corpus provided with word-
alignments in both directions, i.e. from source
to target positions, and viceversa. This pre-
processing step can be accomplished by applying
the GIZA++ toolkit (Och and Ney, 2003) that pro-
vides Viterbi alignments based on IBM Model-4.
Starting from the parallel training corpus, pro-
vided with direct and inverted alignments, the so-
called union alignment (Och and Ney, 2003) is
computed.
Phrase-pairs are extracted from each sentence pair
which correspond to sub-intervals of the source
and target positions, J and I , such that the union
alignment links all positions of J into I and all
positions of I into J . In general, phrases are ex-
tracted with maximum length in the source and tar-
get defined by the parameters Jmax and Imax. All
such phrase-pairs are efficiently computed by an
algorithm with complexity O(lImaxJ2max) (Cet-
tolo et al, 2005).
Given all phrase-pairs extracted from the train-
ing corpus, lexicon probabilities and fertility prob-
abilities are estimated.
Target language models (LMs) used by the de-
coder and rescoring modules are, respectively,
estimated from 3-gram and 4-gram statistics
by applying the modified Kneser-Ney smoothing
method (Goodman and Chen, 1998). LMs are es-
timated with an in-house software toolkit which
also provides a compact binary representation of
the LM which is used by the decoder.
3 Demo Architecture
Figure 1 shows the two-layer architecture of the
demo. At the bottom lie the programs that provide
the actual translation services: for each language-
pair a wrapper coordinates the activity of a special-
ized pre-processing tool and a MT decoder. The
translation programs run on a grid-based cluster
of high-end PCs to optimize the processing speed.
All the wrappers communicate with the MT front-
end whose main task is to forward translation re-
quests to the appropriate language-pair wrapper
and to report an error in case of wrong requests
(e.g. unsupported language-pair). It is worth
noticing here that a new language-pair can be eas-
ily added to the system with a minimal interven-
tion on the code of the MT front-end.
At the top of the architecture are the programs
that provide the interface with the user. This layer
is separated from the translation layer (hosted by
internal machines only) by means of a firewall.
The user interface is implemented as a Web page
in which a translation request (a source sentence
and a language-pair) is input by means of an
HTML form. The cgi script invocated by the form
manages the interaction with the MT front-end.
92
Web Page
(form)
script
CGI
lang 1
wrapper
prepro?
cessing
MT
decoder
prepro?
cessing
MT
decoder
wrapper
lang 2
prepro?
cessing
MT
decoder
wrapper
lang N
...
MT
front?end
firewall
external host
internal hosts
fast machines
Figure 1: Architecture of the demo. For each
language-pair a set of programs (in particular the
MT decoder) provides the translation service. The
request issued by the user on the Web page is
sent by the cgi script to the MT front-end. The
translation is then performed on the appropriate
language-pair service and the output sent back to
the Web browser.
When a user issues a translation request after
filling the form fields, the cgi script sends the re-
quest to the MT front-end and waits for its reply.
The input sentence is then forwarded to the wrap-
per of the appropriate language-pair. After a pre-
processing step, the actual translation is performed
by the specific MT decoder. The output in the tar-
get language is then sent back to the user?s Web
browser through the chain in the reverse order.
From a technical point of view, the inter-process
communication is realized by means of standard
TCP-IP sockets. As far as the encoding of texts is
concerned, all the languages are encoded in UTF-
8: this allows to manage the processing phase in
an uniform way and to render graphically different
character sets.
4 The supported language-pairs
Although there is no theoretical limit to the num-
ber of supported language-pairs, the current ver-
sion of the demo provides translations to English
from three source languages: Arabic, Chinese and
Spanish. For demonstration purpose, three differ-
ent application domains are covered too.
Arabic-to-English (Tourism)
The Arabic-to-English system has been trained
with the data provided by the International Work-
shop on Spoken Language Translation 2005 The
context is that of the Basic Traveling Expres-
sion Corpus (BTEC) task (Takezawa et al, 2002).
BTEC is a multilingual speech corpus which con-
tains sentences coming from phrase books for
tourists. Training set includes 20k sentences con-
taining 159K Arabic and 182K English running
words; vocabulary size is 18K for Arabic, 7K for
English.
Chinese-to-English (Newswire)
The Chinese-to-English system has been trained
with the data provided by the NIST MT Evaluation
Campaign 2005 , large-data condition. In this case
parallel data are mainly news-wires provided by
news agencies. Training set includes 71M Chinese
and 77M English running words; vocabulary size
is 157K for Chinese, 214K for English.
Spanish-to-English (European Parliament)
The Spanish-to-English system has been trained
with the data provided by the Evaluation Cam-
paign 2005 of the European integrated project TC-
STAR4. The context is that of the speeches of
the European Parliament Plenary sessions (EPPS)
from April 1996 to October 2004. Training set for
the Final Text Edition transcriptions includes 31M
Spanish and 30M English running words; vocabu-
lary size is 140K for Spanish, 94K for English.
5 The Web-based Interface
Figure 2 shows a snapshot of the Web-based in-
terface of the demo ? the URL has been removed
to make this submission anonymous. In the upper
part of the page the user provides the two informa-
tion required for the translation: the source sen-
tence can be input in a 80x5 textarea html struc-
ture, while the language-pair can be selected by
means of a set a radio-buttons. The user can reset
the input area or send the translation request by
means of standard reset and submit buttons. Some
examples of bilingual sentences are provided in
the lower part of the page.
4http://www.tc-star.org
93
Figure 2: A snapshot of the Web-based interface.
The user provides the sentence to be translated
in the desired language-pair. Some examples of
bilingual sentences are also available to the user.
The output of a translation request is simple: the
requested source sentence, the translation in the
target language and the selected language-pair are
presented to the user. Figure 3 shows an example
of an Arabic sentence translated into English.
We plan to extend the interface with the pos-
sibility for the user to ask additional information
about the translation ? e.g. the number of explored
theories or the score of the first-best translation.
6 Acknowledgements
This work has been funded by the European Union
under the integrated project TC-STAR - Technol-
ogy and Corpora for Speech to Speech Translation
- (IST-2002-FP6-506738, http://www.tc-star.org).
References
A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra.
1996. A Maximum Entropy Approach to Natural
Language Processing. Computational Linguistics,
22(1):39?71.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2):263?313.
Figure 3: Example of an Arabic sentence trans-
lated into English.
Mauro Cettolo, Marcello Federico, Nicola Bertoldi,
Roldano Cattoni, and Boxing Chen. 2005. A look
inside the itc-irst smt system. In Proceedings of the
10th Machine Translation Summit, pages 451?457,
Phuket, Thailand, September.
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo, and
M. Federico. 2005. The ITC-irst SMT System for
IWSLT-2005. In Proceedings of the IWSLT 2005,
Pittsburgh, USA.
M. Federico and N. Bertoldi. 2005. A Word-to-Phrase
Statistical Translation Model. ACM Transactions on
Speech and Language Processing. to appear.
J. Goodman and S. Chen. 1998. An empirical study of
smoothing techniques for language modeling. Tech-
nical Report TR-10-98, Harvard University, August.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of HLT-
NAACl 2003, pages 127?133, Edmonton, Canada.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F.J. Och. 2004. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, Sapporo, Japan.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a Broad-Coverage
Bilingual Corpus for Speech Translation of Travel
Conversations in the Real World. In Proceedings of
3rd LREC, pages 147?152, Las Palmas, Spain.
B. H. Tran, F. Seide, and V. Steinbiss. 1996. A Word
Graph based N-Best Search in Continuous Speech
Recognition. In Proceedings of ICLSP, Philadel-
phia, PA, USA.
94
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177?180,
Prague, June 2007. c?2007 Association for Computational Linguistics 
Moses: Open Source Toolkit for Statistical Machine Translation 
Philipp Koehn 
Hieu Hoang  
Alexandra Birch 
Chris Callison-Burch 
University of Edin-
burgh1 
Marcello Federico 
Nicola Bertoldi 
ITC-irst2 
Brooke Cowan 
Wade Shen 
Christine Moran 
MIT3 
Richard Zens 
RWTH Aachen4 
Chris Dyer 
University of Maryland5 
 
Ond?ej Bojar 
Charles University6 
Alexandra Constantin 
Williams College7 
Evan Herbst 
Cornell8 
1 pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk. 
2{federico, bertoldi}@itc.it. 3 brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu. 4 
zens@i6.informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 
07aec_2@williams.edu. 8 evh4@cornell.edu 
 
Abstract 
We describe an open-source toolkit for sta-
tistical machine translation whose novel 
contributions are (a) support for linguisti-
cally motivated factors, (b) confusion net-
work decoding, and (c) efficient data for-
mats for translation models and language 
models. In addition to the SMT decoder, 
the toolkit also includes a wide variety of 
tools for training, tuning and applying the 
system to many translation tasks.  
1 Motivation 
Phrase-based statistical machine translation 
(Koehn et al 2003) has emerged as the dominant 
paradigm in machine translation research. How-
ever, until now, most work in this field has been 
carried out on proprietary and in-house research 
systems. This lack of openness has created a high 
barrier to entry for researchers as many of the 
components required have had to be duplicated. 
This has also hindered effective comparisons of the 
different elements of the systems. 
By providing a free and complete toolkit, we 
hope that this will stimulate the development of the 
field. For this system to be adopted by the commu-
nity, it must demonstrate performance that is com-
parable to the best available systems. Moses has 
shown that it achieves results comparable to the 
most competitive and widely used statistical ma-
chine translation systems in translation quality and 
run-time (Shen et al 2006). It features all the ca-
pabilities of the closed sourced Pharaoh decoder 
(Koehn 2004). 
Apart from providing an open-source toolkit 
for SMT, a further motivation for Moses is to ex-
tend phrase-based translation with factors and con-
fusion network decoding. 
The current phrase-based approach to statisti-
cal machine translation is limited to the mapping of 
small text chunks without any explicit use of lin-
guistic information, be it morphological, syntactic, 
or semantic. These additional sources of informa-
tion have been shown to be valuable when inte-
grated into pre-processing or post-processing steps. 
Moses also integrates confusion network de-
coding, which allows the translation of ambiguous 
input. This enables, for instance, the tighter inte-
gration of speech recognition and machine transla-
tion. Instead of passing along the one-best output 
of the recognizer, a network of different word 
choices may be examined by the machine transla-
tion system. 
Efficient data structures in Moses for the 
memory-intensive translation model and language 
model allow the exploitation of much larger data 
resources with limited hardware. 
177
 2 Toolkit 
The toolkit is a complete out-of-the-box trans-
lation system for academic research. It consists of 
all the components needed to preprocess data, train 
the language models and the translation models. It 
also contains tools for tuning these models using 
minimum error rate training (Och 2003) and evalu-
ating the resulting translations using the BLEU 
score (Papineni et al 2002).  
Moses uses standard external tools for some of 
the tasks to avoid duplication, such as GIZA++ 
(Och and Ney 2003) for word alignments and 
SRILM for language modeling.  Also, since these 
tasks are often CPU intensive, the toolkit has been 
designed to work with Sun Grid Engine parallel 
environment to increase throughput.  
In order to unify the experimental stages, a 
utility has been developed to run repeatable ex-
periments. This uses the tools contained in Moses 
and requires minimal changes to set up and cus-
tomize. 
The toolkit has been hosted and developed un-
der sourceforge.net since inception. Moses has an 
active research community and has reached over 
1000 downloads as of 1st March 2007.  
The main online presence is at  
http://www.statmt.org/moses/ 
where many sources of information about the 
project can be found. Moses was the subject of this 
year?s Johns Hopkins University Workshop on 
Machine Translation (Koehn et al 2006). 
The decoder is the core component of Moses. 
To minimize the learning curve for many research-
ers, the decoder was developed as a drop-in re-
placement for Pharaoh, the popular phrase-based 
decoder. 
In order for the toolkit to be adopted by the 
community, and to make it easy for others to con-
tribute to the project, we kept to the following 
principles when developing the decoder: 
? Accessibility 
? Easy to Maintain 
? Flexibility 
? Easy for distributed team development 
? Portability 
It was developed in C++ for efficiency and fol-
lowed modular, object-oriented design. 
3 Factored Translation Model 
Non-factored SMT typically deals only with 
the surface form of words and has one phrase table, 
as shown in Figure 1. 
i am buying you a green cat
using phrase dictionary:
i
 am buying
you
a
green
cat
je
ach?te
vous
un
vert
chat
a une
je vous ach?te un chat vert
Translate:
 
In factored translation models, the surface 
forms may be augmented with different factors, 
such as POS tags or lemma. This creates a factored 
representation of each word, Figure 2.  
1 1 1 / sing /
                                 
je vous achet un chat
PRO PRO VB ART NN
je vous acheter un chat
st st st present masc masc
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?
1 1 / 1 sing sing
i buy you a cat
PRO VB PRO ART NN
i tobuy you a cat
st st present st
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?  
 
Mapping of source phrases to target phrases 
may be decomposed into several steps. Decompo-
sition of the decoding process into various steps 
means that different factors can be modeled sepa-
rately. Modeling factors in isolation allows for 
flexibility in their application. It can also increase 
accuracy and reduce sparsity by minimizing the 
number dependencies for each step. 
For example, we can decompose translating 
from surface forms to surface forms and lemma, as 
shown in Figure 3. 
Figure 2. Factored translation 
Figure 1. Non-factored translation 
178
  
Figure 3. Example of graph of decoding steps 
By allowing the graph to be user definable, we 
can experiment to find the optimum configuration 
for a given language pair and available data.  
The factors on the source sentence are consid-
ered fixed, therefore, there is no decoding step 
which create source factors from other source fac-
tors. However, Moses can have ambiguous input in 
the form of confusion networks. This input type 
has been used successfully for speech to text 
translation (Shen et al 2006). 
Every factor on the target language can have its 
own language model. Since many factors, like 
lemmas and POS tags, are less sparse than surface 
forms, it is possible to create a higher order lan-
guage models for these factors. This may encour-
age more syntactically correct output. In Figure 3 
we apply two language models, indicated by the 
shaded arrows, one over the words and another 
over the lemmas. Moses is also able to integrate 
factored language models, such as those described 
in (Bilmes and Kirchhoff 2003) and (Axelrod 
2006). 
4 Confusion Network Decoding 
Machine translation input currently takes the 
form of simple sequences of words. However, 
there are increasing demands to integrate machine 
translation technology into larger information 
processing systems with upstream NLP/speech 
processing tools (such as named entity recognizers, 
speech recognizers, morphological analyzers, etc.). 
These upstream processes tend to generate multiple, 
erroneous hypotheses with varying confidence. 
Current MT systems are designed to process only 
one input hypothesis, making them vulnerable to 
errors in the input.  
In experiments with confusion networks, we 
have focused so far on the speech translation case, 
where the input is generated by a speech recog-
nizer. Namely, our goal is to improve performance 
of spoken language translation by better integrating 
speech recognition and machine translation models. 
Translation from speech input is considered more 
difficult than translation from text for several rea-
sons. Spoken language has many styles and genres, 
such as, formal read speech, unplanned speeches, 
interviews, spontaneous conversations; it produces 
less controlled language, presenting more relaxed 
syntax and spontaneous speech phenomena. Fi-
nally, translation of spoken language is prone to 
speech recognition errors, which can possibly cor-
rupt the syntax and the meaning of the input. 
There is also empirical evidence that better 
translations can be obtained from transcriptions of 
the speech recognizer which resulted in lower 
scores. This suggests that improvements can be 
achieved by applying machine translation on a 
large set of transcription hypotheses generated by 
the speech recognizers and by combining scores of 
acoustic models, language models, and translation 
models. 
Recently, approaches have been proposed for 
improving translation quality through the process-
ing of multiple input hypotheses. We have imple-
mented in Moses confusion network decoding as 
discussed in (Bertoldi and Federico 2005), and de-
veloped a simpler translation model and a more 
efficient implementation of the search algorithm. 
Remarkably, the confusion network decoder re-
sulted in an extension of the standard text decoder. 
5 Efficient Data Structures for Transla-
tion Model and Language Models 
With the availability of ever-increasing 
amounts of training data, it has become a challenge 
for machine translation systems to cope with the 
resulting strain on computational resources. Instead 
of simply buying larger machines with, say, 12 GB 
of main memory, the implementation of more effi-
cient data structures in Moses makes it possible to 
exploit larger data resources with limited hardware 
infrastructure. 
A phrase translation table easily takes up giga-
bytes of disk space, but for the translation of a sin-
gle sentence only a tiny fraction of this table is 
needed. Moses implements an efficient representa-
tion of the phrase translation table. Its key proper-
ties are a prefix tree structure for source words and 
on demand loading, i.e. only the fraction of the 
phrase table that is needed to translate a sentence is 
loaded into the working memory of the decoder. 
179
 For the Chinese-English NIST  task, the mem-
ory requirement of the phrase table is reduced from 
1.7 gigabytes to less than 20 mega bytes, with no 
loss in translation quality and speed (Zens and Ney 
2007). 
The other large data resource for statistical ma-
chine translation is the language model. Almost 
unlimited text resources can be collected from the 
Internet and used as training data for language 
modeling. This results in language models that are 
too large to easily fit into memory. 
The Moses system implements a data structure 
for language models that is more efficient than the 
canonical SRILM (Stolcke 2002) implementation 
used in most systems. The language model on disk 
is also converted into this binary format, resulting 
in a minimal loading time during start-up of the 
decoder.  
An even more compact representation of the 
language model is the result of the quantization of 
the word prediction and back-off probabilities of 
the language model. Instead of representing these 
probabilities with 4 byte or 8 byte floats, they are 
sorted into bins, resulting in (typically) 256 bins 
which can be referenced with a single 1 byte index. 
This quantized language model, albeit being less 
accurate, has only minimal impact on translation 
performance (Federico and Bertoldi 2006). 
6 Conclusion and Future Work 
This paper has presented a suite of open-source 
tools which we believe will be of value to the MT 
research community. 
We have also described a new SMT decoder 
which can incorporate some linguistic features in a 
consistent and flexible framework. This new direc-
tion in research opens up many possibilities and 
issues that require further research and experimen-
tation. Initial results show the potential benefit of 
factors for statistical machine translation, (Koehn 
et al 2006) and (Koehn and Hoang 2007). 
References 
Axelrod, Amittai. "Factored Language Model for Sta-
tistical Machine Translation." MRes Thesis. 
Edinburgh University, 2006. 
Bertoldi, Nicola, and Marcello Federico. "A New De-
coder for Spoken Language Translation Based 
on Confusion Networks." Automatic Speech 
Recognition and Understanding Workshop 
(ASRU), 2005. 
Bilmes, Jeff A, and Katrin Kirchhoff. "Factored Lan-
guage Models and Generalized Parallel Back-
off." HLT/NACCL, 2003. 
Koehn, Philipp. "Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation 
Models." AMTA, 2004. 
Koehn, Philipp, Marcello Federico, Wade Shen, Nicola 
Bertoldi, Ondrej Bojar, Chris Callison-Burch, 
Brooke Cowan, Chris Dyer, Hieu Hoang, 
Richard Zens, Alexandra Constantin, Christine 
Corbett Moran, and Evan Herbst. "Open 
Source Toolkit for Statistical Machine Transla-
tion". Report of the 2006 Summer Workshop at 
Johns Hopkins University, 2006. 
Koehn, Philipp, and Hieu Hoang. "Factored Translation 
Models." EMNLP, 2007. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
"Statistical Phrase-Based Translation." 
HLT/NAACL, 2003. 
Och, Franz Josef. "Minimum Error Rate Training for 
Statistical Machine Translation." ACL, 2003. 
Och, Franz Josef, and Hermann Ney. "A Systematic 
Comparison of Various Statistical Alignment 
Models." Computational Linguistics 29.1 
(2003): 19-51. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. "BLEU: A Method for Automatic 
Evaluation of Machine Translation." ACL, 
2002. 
Shen, Wade, Richard Zens, Nicola Bertoldi, and 
Marcello Federico. "The JHU Workshop 2006 
Iwslt System." International Workshop on Spo-
ken Language Translation, 2006. 
Stolcke, Andreas. "SRILM an Extensible Language 
Modeling Toolkit." Intl. Conf. on Spoken Lan-
guage Processing, 2002. 
Zens, Richard, and Hermann Ney. "Efficient Phrase-
Table Representation for Machine Translation 
with Applications to Online MT and Speech 
Recognition." HLT/NAACL, 2007. 
180
 	

Maximum Entropy Tagging with Binary and Real-Valued Features
Vanessa Sandrini Marcello Federico Mauro Cettolo
ITC-irst - Centro per la Ricerca Scientifica e Tecnologica
38050 Povo (Trento) - ITALY
{surname}@itc.it
Abstract
Recent literature on text-tagging reported
successful results by applying Maximum
Entropy (ME) models. In general, ME
taggers rely on carefully selected binary
features, which try to capture discrimi-
nant information from the training data.
This paper introduces a standard setting
of binary features, inspired by the litera-
ture on named-entity recognition and text
chunking, and derives corresponding real-
valued features based on smoothed log-
probabilities. The resulting ME models
have orders of magnitude fewer parame-
ters. Effective use of training data to esti-
mate features and parameters is achieved
by integrating a leaving-one-out method
into the standard ME training algorithm.
Experimental results on two tagging tasks
show statistically significant performance
gains after augmenting standard binary-
feature models with real-valued features.
1 Introduction
The Maximum Entropy (ME) statistical frame-
work (Darroch and Ratcliff, 1972; Berger et al,
1996) has been successfully deployed in several
NLP tasks. In recent evaluation campaigns, e.g.
DARPA IE and CoNLL 2000-2003, ME models
reached state-of-the-art performance on a range of
text-tagging tasks.
With few exceptions, best ME taggers rely on
carefully designed sets of features. Features cor-
respond to binary functions, which model events,
observed in the (annotated) training data and sup-
posed to be meaningful or discriminative for the
task at hand. Hence, ME models result in a log-
linear combination of a large set of features, whose
weights can be estimated by the well known Gen-
eralized Iterative Scaling (GIS) algorithm by Dar-
roch and Ratcliff (1972).
Despite ME theory and its related training algo-
rithm (Darroch and Ratcliff, 1972) do not set re-
strictions on the range of feature functions1 , pop-
ular NLP text books (Manning and Schutze, 1999)
and research papers (Berger et al, 1996) seem
to limit them to binary features. In fact, only
recently, log-probability features have been de-
ployed in ME models for statistical machine trans-
lation (Och and Ney, 2002).
This paper focuses on ME models for two text-
tagging tasks: Named Entity Recognition (NER)
and Text Chuncking (TC). By taking inspiration
from the literature (Bender et al, 2003; Borth-
wick, 1999; Koeling, 2000), a set of standard bi-
nary features is introduced. Hence, for each fea-
ture type, a corresponding real-valued feature is
developed in terms of smoothed probability distri-
butions estimated on the training data. A direct
comparison of ME models based on binary, real-
valued, and mixed features is presented. Besides,
performance on the tagging tasks, complexity and
training time by each model are reported. ME es-
timation with real-valued features is accomplished
by combining GIS with the leave-one-out method
(Manning and Schutze, 1999).
Experiments were conducted on two publicly
available benchmarks for which performance lev-
els of many systems are published on theWeb. Re-
sults show that better ME models for NER and TC
can be developed by integrating binary and real-
valued features.
1Darroch and Ratcliff (1972) show how any set of real-
valued feature functions can be properly handled.
1
2 ME Models for Text Tagging
Given a sequence of words wT1 = w1, . . . , wT and
a set of tags C, the goal of text-tagging is to find
a sequence of tags cT1 = c1, . . . , cT which maxi-
mizes the posterior probability, i.e.:
c?T1 = argmaxcT1
p(cT1 | wT1 ). (1)
By assuming a discriminative model, Eq. (1) can
be rewritten as follows:
c?T1 = argmaxcT1
T
?
t=1
p(ct | ct?11 , wT1 ), (2)
where p(ct|ct?11 , wT1 ) is the target conditional
probability of tag ct given the context (ct?11 , wT1 ),
i.e. the entire sequence of words and the full se-
quence of previous tags. Typically, independence
assumptions are introduced in order to reduce the
context size. While this introduces some approxi-
mations in the probability distribution, it consid-
erably reduces data sparseness in the sampling
space. For this reason, the context is limited here
to the two previous tags (ct?1t?2) and to four words
around the current word (wt+2t?2). Moreover, limit-
ing the context to the two previous tags permits to
apply dynamic programming (Bender et al, 2003)
to efficiently solve the maximization (2).
Let y = ct denote the class to be guessed (y ? Y)
at time t and x = ct?1t?2, wt+2t?2 its context (x ? X ).
The generic ME model results:
p?(y | x) =
exp(
?n
i=1 ?ifi(x, y))
?
y? exp(
?n
i=1 ?ifi(x, y?))
. (3)
The n feature functions fi(x, y) represent any kind
of information about the event (x, y) which can be
useful for the classification task. Typically, binary
features are employed which model the verifica-
tion of simple events within the target class and
the context.
InMikheev (1998), binary features for text tagging
are classified into two broad classes: atomic and
complex. Atomic features tell information about
the current tag and one single item (word or tag) of
the context. Complex features result as a combina-
tion of two or more atomic features. In this way, if
the grouped events are not independent, complex
features should capture higher correlations or de-
pendencies, possibly useful to discriminate.
In the following, a standard set of binary fea-
tures is presented, which is generally employed
for text-tagging tasks. The reader familiar with the
topic can directly check this set in Table 1.
3 Standard Binary Features
Binary features are indicator functions of specified
events of the sample space X ? Y . Hence, they
take value 1 if the event occurs or 0 otherwise. For
the sake of notation, the feature name denotes the
type of event, while the index specifies its param-
eters. For example:
Orthperson,Cap,?1(x, y)
corresponds to an Orthographic feature which is
active if and only if the class at time t is person
and the word at time t?1 in the context starts with
capitalized letter.
3.1 Atomic Features
Lexical features These features model co-
occurrences of classes and single words of the con-
text. Lexical features are defined on a window
of ?2 positions around the current word. Lexical
features are denoted by the name Lex and indexed
with the triple c, w, d which fixes the current class,
i.e. ct = c, the identity and offset of the word in
the context, i.e. wt+d = w. Formally, the feature
is computed by:
Lex c,w,d(x, y) =? ?(ct = c) ? ?(wt+d = w).
For example, the lexical feature for word
Verona, at position t with tag loc (location) is:
Lexloc,Verona,0(x, y) = ?(ct = loc) ?
??(wt = Verona).
Lexical features might introduce data sparseness
in the model, given that in real texts an impor-
tant fraction of words occur only once. In other
words, many words in the test set will have no
corresponding features-parameter pairs estimated
on the training data. To cope with this problem,
all words observed only once in the training data
were mapped into the special symbol oov.
Syntactic features They model co-occurrences
of the current class with part-of-speech or chunk
tags of a specific position in the context. Syntactic
features are denoted by the name Syn and indexed
with a 4-tuple (c, Pos, p, d) or (c, Chnk, p, d),
2
Name Index Definition
Lex c, w, d ?(ct = c) ? ?(wt+d = w), d ? Z
Syn c, T, p, d ?(ct = c) ? ?(T(wt+d) = p) , T ? {Pos, Chnk}, d ? Z
Orth c, F, d ?(ct = c) ? F(wt+d) , F ? {IsCap, IsCAP}, d ? Z
Dict c, L, d ?(ct = c) ? InList(L,wt+d), d ? Z
Tran c, c?, d ?(ct = c) ? ?(ct?d = c?) d ? N+
Lex+ c, s, k, ws+k?1s
?s+k?1
d=s Lexc,wd,d(x, y), k ? N+, s ? Z
Syn+ c, T, s, k, ps+k?1s
?s+k?1
d=s Sync,T,pd,d(x, y), k ? N+, s ? Z
Orth+ c, F, k, b+k?k ?(ct = c) ?
?k
d=?k ?(Orthc,F,d(x, y) = bd) , bd ? {0, 1}, k ? N+
Dict+ c, L, k, b+k?k ?(ct = c) ?
?k
d=?k ?(Dictc,L,d(x, y) = bd) , bd ? {0, 1}, k ? N+
Tran+ c, k, ck1
?k
d=1? Tranc,cd,d(x, y) k ? N+
Table 1: Standard set of binary features for text tagging.
which fixes the class ct, the considered syntactic
information, and the tag and offset within the con-
text. Formally, these features are computed by:
Sync,Pos,p,d(x, y)=??(ct = c) ? ?(Pos(wt+d) = p)
Sync,Chnk,p,d(x, y)=??(ct = c)??(Chnk(wt+d) = p).
Orthographic features These features model
co-occurrences of the current class with surface
characteristics of words of the context, e.g. check
if a specific word in the context starts with cap-
italized letter (IsCap) or is fully capitalized
(IsCAP). In this framework, only capitalization
information is considered. Analogously to syntac-
tic features, orthographic features are defined as
follows:
Orthc,IsCap,d(x, y)=??(ct = c) ? IsCap(wt+d)
Orthc,IsCAP,d(x, y)=??(ct = c) ? IsCAP(wt+d).
Dictionary features These features check if
specific positions in the context contain words oc-
curring in some prepared list. This type of feature
results relevant for tasks such as NER, in which
gazetteers of proper names can be used to improve
coverage of the training data. Atomic dictionary
features are defined as follows:
Dictc,L,d(x, y)=??(ct = c) ? InList(L,wt+d)
where L is a specific pre-compiled list, and
InList is a function which returns 1 if the spec-
ified word matches one of the multi-word entries
of list L, and 0 otherwise.
Transition features Transition features model
Markov dependencies between the current tag and
a previous tag. They are defined as follows:
Tranc,c?,d(x, y)=??(ct = c) ? ?(ct?d = c?).
3.2 Complex Features
More complex events are defined by combining
two or more atomic features in one of two ways.
Product features take the intersection of the cor-
responding atomic events. V ector features con-
sider all possible outcomes of the component fea-
tures.
For instance, the product of 3 atomic Lexical
features, with class c, offsets ?2,?1, 0, and words
v?2, v?1, v0, is:
Lex+c,?2,3,v?2,v?1,v0(x, y)=?
0
?
d=?2
Lexc,vd,d(x, y).
Vector features obtained from three Dictionary
features with the same class c, list L, and offsets,
respectively, -1,0,+1, are indexed over all possible
binary outcomes b?1, b0, b1 of the single atomic
features, i.e.:
Dict+c,L,1,b?1,b0,b+1(x, y)=??(ct = c)?
1
?
d=?1
?(Dictc,L,d(x, y) = bd).
Complex features used in the experiments are de-
scribed in Table 1.
The use of complex features significantly in-
creases the model complexity. Assuming that
there are 10, 000 words occurring more than once
in the training corpus, the above lexical feature po-
tentially adds O(|C|1012) parameters!
As complex binary features might result pro-
hibitive from a computational point of view, real-
valued features should be considered as an alter-
native.
3
Feature Index Probability Distribution
Lex d p(ct | wt+d)
Syn T, d p(ct | T(wt+d))
Orth F, d p(ct | F(wt+d))
Dict List, d p(ct | IsIn(List, wt+d))
Tran d p(ct | ct?d)
Lex+ s, k p(ct | wt+s, .., wt+s+k?1
Syn+ T, s, k p(ct | T(wt+s, . . . , wt+s+k?1))
Orth+ k, F p(ct | F(wt?k), . . . , F(wt+k))
Dict+ k,L p(ct | InList(L, wt?k), . . . , InList(L, wt+k))
Tran+ k p(ct | ct?k, . . . , ct+k))
Table 2: Corresponding standard set of real-values features.
4 Real-valued Features
A binary feature can be seen as a probability mea-
sure with support set made of a single event. Ac-
cording to this point of view, we might easily ex-
tend binary features to probability measures de-
fined over larger event spaces. In fact, it results
convenient to introduce features which are log-
arithms of conditional probabilities. It can be
shown that in this way linear constraints of the
MEmodel can be interpreted in terms of Kullback-
Leibler distances between the target model and the
conditional distributions (Klakow, 1998).
Let p1(y|x), p2(y|x), . . . , pn(y|x) be n different
conditional probability distributions estimated on
the training corpus. In our framework, each con-
ditional probability pi is associated to a feature fi
which is defined over a subspace [X ]i ? Y of the
sample space X ? Y . Hence, pi(y|x) should be
read as a shorthand of p(y | [x]i).
The corresponding real-valued feature is:
fi(x, y) = log pi(y | x). (4)
In this way, the ME in Eq. (3) can be rewritten as:
p?(y|x) =
?n
i pi(y|x)?i
?
y?
?
i pi(y
? |x)?i . (5)
According to the formalism adopted in Eq. (4),
real-valued features assume the following form:
fi(ct, ct?1t?2, wt+2t?2) = log pi(ct | ct?1t?2, wt+2t?2). (6)
For each so far presented type of binary feature,
a corresponding real-valued type can be easily de-
fined. The complete list is shown in Table 2. In
general, the context subspace was defined on the
basis of the offset parameters of each binary fea-
ture. For instance, all lexical features selecting
two words at distances -1 and 0 from the current
position t are modeled by the conditional distri-
bution p(ct | wt?1, wt). While distributions of
lexical, syntactic and transition features are con-
ditioned on words or tags, dictionary and ortho-
graphic features are conditioned on binary vari-
ables.
An additional real-valued feature that was em-
ployed is the so called prior feature, i.e. the prob-
ability of a tag to occur:
Prior(x, y) = log p(ct)
A major effect of using real-valued features is
the drastic reduction of model parameters. For
example, each complex lexical features discussed
before introduce just one parameter. Hence, the
small number of parameters eliminates the need
of smoothing the ME estimates.
Real-valued features present some drawbacks.
Their level of granularity, or discrimination, might
result much lower than their binary variants. For
many features, it might result difficult to compute
reliable probability values due to data sparseness.
For the last issue, smoothing techniques devel-
oped for statistical language models can be applied
(Manning and Schutze, 1999).
5 Mixed Feature Models
This work, beyond investigating the use of real-
valued features, addresses the behavior of models
combining binary and real-valued features. The
reason is twofold: on one hand, real-valued fea-
tures allow to capture complex information with
fewer parameters; on the other hand, binary fea-
tures permit to keep a good level of granularity
over salient characteristics. Hence, finding a com-
promise between binary and real-valued features
4
might help to develop ME models which better
trade-off complexity vs. granularity of informa-
tion.
6 Parameter Estimation
From the duality of ME and maximum likeli-
hood (Berger et al, 1996), optimal parameters
?? for model (3) can be found by maximizing
the log-likelihood function over a training sample
{(xt, yt) : t = 1, . . . ,N}, i.e.:
?? = argmax
?
N
?
t=1
log p?(yt|xt). (7)
Now, whereas binary features take only two values
and do not need any estimation phase, conditional
probability features have to be estimated on some
data sample. The question arises about how to ef-
ficiently use the available training data in order to
estimate the parameters and the feature distribu-
tions of the model, by avoiding over-fitting.
Two alternative techniques, borrowed from sta-
tistical language modeling, have been consid-
ered: the Held-out and the Leave-one-out methods
(Manning and Schutze, 1999).
Held-out method. The training sample S is split
into two parts used, respectively, to estimate the
feature distributions and the ME parameters.
Leave-one-out. ME parameters and feature dis-
tributions are estimated over the same sample S.
The idea is that for each addend in eq. (7), the cor-
responding sample point (xt, yt) is removed from
the training data used to estimate the feature distri-
butions of the model. In this way, it can be shown
that occurrences of novel observations are simu-
lated during the estimation of the ME parameters
(Federico and Bertoldi, 2004).
In our experiments, language modeling smooth-
ing techniques (Manning and Schutze, 1999) were
applied to estimate feature distributions pi(y|x).
In particular, smoothing was based on the dis-
counting method in Ney et al (1994) combined to
interpolation with distributions using less context.
Given the small number of smoothing parameters
involved, leave-one-out probabilities were approx-
imated by just modifying count statistics on the
fly (Federico and Bertoldi, 2004). The rationale is
that smoothing parameters do not change signifi-
cantly after removing just one sample point.
For parameter estimation, the GIS algorithm
by Darroch and Ratcliff (1972) was applied. It
is known that the GIS algorithm requires feature
functions fi(x, y) to be non-negative. Hence, fea-
tures were re-scaled as follows:
fi(x, y) = log pi(y|x) + log
1 + 
min pi
, (8)
where  is a small positive constant and the de-
nominator is a constant term defined by:
min pi = min
(x,y)?S
pi(y|x). (9)
The factor (1 + ) was introduced to ensure that
real-valued features are always positive. This con-
dition is important to let features reflect the same
behavior of the conditional distributions, which
assign a positive probability to each event.
It is easy to verify that this scaling operation
does not affect the original model but only impacts
on the GIS calculations. Finally, a slack feature
was introduced by the algorithm to satisfy the con-
straint that all features sum up to a constant value
(Darroch and Ratcliff, 1972).
7 Experiments
This section presents results of MEmodels applied
to two text-tagging tasks, Named Entity Recogni-
tion (NER) and Text Chunking (TC).
After a short introduction to the experimen-
tal framework, the detailed feature setting is pre-
sented. Then, experimental results are presented
for the following contrastive conditions: binary
versus real-valued features, training via held-out
versus leave-one-out, atomic versus complex fea-
tures.
7.1 Experimental Set-up
Named Entity Recognition English NER ex-
periments were carried out on the CoNLL-2003
shared task2. This benchmark is based on texts
from the Reuters Corpus which were manually
annotated with parts-of-speech, chunk tags, and
named entity categories. Four types of categories
are defined: person, organization, location and
miscellaneous, to include e.g. nations, artifacts,
etc. A filler class is used for the remaining words.
After including tags denoting the start of multi-
word entities, a total of 9 tags results. Data are
partitioned into training (200K words), develop-
ment (50K words), and test (46K words) samples.
2Data and results in http://cnts.uia.ac.be/conll2003/ner.
5
Text Chunking English TC experiments were
conducted on the CoNLL-2000 shared task3.
Texts originate from the Wall Street Journal and
are annotated with part-of-speech tags and chunks.
The chunk set consists of 11 syntactic classes. The
set of tags which also includes start-markers con-
sists of 23 classes. Data is split into training (210K
words) and test (47K words) samples.
Evaluation Tagging performance of both tasks
is expressed in terms of F-score, namely the har-
monic mean of precision and recall. Differences in
performance have been statistically assessed with
respect to precision and recall, separately, by ap-
plying a standard test on proportions, with signif-
icance levels ? = 0.05 and ? = 0.1. Henceforth,
claimed differences in precision or recall will have
their corresponding significance level shown in
parenthesis.
7.2 Settings and Baseline Models
Feature selection and setting for ME models is an
art. In these experiments we tried to use the same
set of features with minor modifications across
both tasks. In particular, used features and their
settings are shown in Table 3.
Training of models with GIS and estimation
of feature distributions used in-house developed
toolkits. Performance of binary feature models
was improved by smoothing features with Gaus-
sian priors (Chen and Rosenfeld, 1999) with mean
zero and standard deviation ? = 4. In general,
tuning of models was carried out on a development
set.
Most of the comparative experiments were per-
formed on the NER task. Three baseline models
using atomic features Lex, Syn, and Tran were
investigated first: model BaseBin, with all binary
features; model BaseReal, with all real-valued fea-
tures plus the prior feature; model BaseMix, with
real-valued Lex and binary Tran and Syn. Mod-
els BaseReal and BaseMix were trained with the
held-out method. In particular, feature distribu-
tions were estimated on the training data while ME
parameters on the development set.
7.3 Binary vs. Real-valued Features
The first experiment compares performance of the
baseline models on the NER task. Experimental
results are summarized in Table 4. Models Base-
Bin, BaseReal, and BaseMix achieved F-scores of
3Data and results in http://cnts.uia.ac.be/conll2000/chunking.
Model ID Num P% R% F-score
BaseBin 580K 78.82 75.62 77.22
BaseReal 10 79.74 74.15 76.84
BaseMix 753 78.90 75.85 77.34
Table 4: Performance of baseline models on the
NER task. Number of parameters, precision, re-
call, and F-score are reported for each model.
Model Methods P% R% F-score
BaseMix Held-Out 78.90 75.85 77.34
BaseMix L-O-O 80.64 76.40 78.46
Table 5: Performance of mixed feature models
with two different training methods.
77.22, 76.84, and 77.34. Statistically meaning-
ful differences were in terms of recall, between
BaseBin and BaseReal (? = 0.1), and between
BaseMix and BaseReal (? = 0.05).
Despite models BaseMix and BaseBin perform
comparably, the former has many fewer parame-
ters, i.e. 753 against 580,000. In fact, BaseMix re-
quires storing and estimating feature distributions,
which is however performed at a marginal compu-
tational cost and off-line with respect to GIS train-
ing.
7.4 Training with Mixed Features
An experiment was conducted with the BaseMix
model to compare the held-out and leave-one-out
training methods. Results in terms of F-score are
reported in Table 5. By applying the leave-one-
out method F-score grows from 77.34 to 78.46,
with a meaningful improvement in recall (? =
0.05). With respect to models BaseBin and Base-
Real, leave-one-out estimation significantly im-
proved precision (? = 0.05).
In terms of training time, ME models with real-
valued features took significantly more GIS iter-
ations to converge. Figures of cost per iteration
and number of iterations are reported in Table 6.
(Computation times are measured on a single CPU
Pentium-4 2.8GHz.) Memory size of the training
process is instead proportional to the number n of
parameters.
7.5 Complex Features
A final set of experiments aims at comparing the
baseline MEmodels augmented with complex fea-
tures, again either binary only (model FinBin),
6
Feature Index NE Task Chunking Task
Lex c, w, d N(w) > 1,?2 ? d ? +2 ?2 ? d ? +2
Syn c, T, p, d T ? {Pos, Chnk}, d = 0 T = Pos,?2 ? d ? +2
Tran c, c?, d d = ?2,?1 d = ?2,?1
Lex+ c, s, k, ws+k?1s s = ?1, 0, k = 1 s = ?1, 0 k = 1
Syn+ c, T, s, k, ps+k?1s not used s = ?1, 0 k = 1
Orth+ c, k, F, b+k?k F = {Cap, CAP}, k = 2 F = Cap, k = 1
Dict+ c, k, L, b+k?k k = 3L = {LOC, PER, ORG, MISC} not used
Tran+ c, k, ck1 k = 2 k = 2
Table 3: Setting used for binary and real-valued features in the reported experiments.
Model Single Iteration Iterations Total
BaseBin 54 sec 750 ? 11 h
BaseReal 9.6 sec 35,000 ? 93 h
BaseMix 42 sec 4,000 ? 46 h
Table 6: Computational cost of parameter estima-
tion by different baseline models.
real-valued only (FinReal), or mixed (FinMix).
Results are provided both for NER and TC.
This time, compared models use different fea-
ture settings. In fact, while previous experiments
aimed at comparing the same features, in either
real or binary form, these experiments explore al-
ternatives to a full-fledged binary model. In par-
ticular, real-valued features are employed whose
binary versions would introduce a prohibitively
large number of parameters. Parameter estima-
tion of models including real-valued features al-
ways applies the leave-one-out method.
For the NER task, model FinBin adds Orth+
and Dict+; FinReal adds Lex+, Orth+ and
Dict+; and, FinMix adds real-valued Lex+ and
binary-valued Orth+ and Dict+.
In the TC task, feature configurations are as fol-
lows: FinBin uses Lex, Syn, Tran, and Orth+;
FinReal uses Lex, Syn, Tran, Prior, Orth+,
Lex+, Syn+, Tran+; and, finally, FinMix uses
binary Syn, Tran, Orth+ and real-valued Lex,
Lex+, Syn+.
Performance of the models on the two tasks are
reported in Table 7 and Table 8, respectively.
In the NER task, all final models outperform the
baseline model. Improvements in precision and
recall are all significant (? = 0.05). Model Fin-
Mix improves precision with respect to model Fin-
Bin (? = 0.05) and requires two order of magni-
tude fewer parameters.
Model Num P% R% F-score
FinBin 673K 81.92 80.36 81.13
FinReal 19 83.58 74.03 78.07
FinMix 3K 84.34 80.38 82.31
Table 7: Results with complex features on the
NER task.
Model Num P% R% F-score
FinBin 2M 91.04 91.48 91.26
FinReal 19 88.73 90.58 89.65
FinMix 6K 91.93 92.24 92.08
Table 8: Results with complex features on the TC
task.
In the TC task, the same trend is observed.
Again, best performance is achieved by the model
combining binary and real-valued features. In par-
ticular, all observable differences in terms of pre-
cision and recall are significant (? = 0.05).
8 Discussion
In summary, this paper addressed improvements to
ME models for text tagging applications. In par-
ticular, we showed how standard binary features
from the literature can be mapped into correspond-
ing log-probability distributions. ME training with
the so-obtained real-valued features can be accom-
plished by combining the GIS algorithm with the
leave-one-out or held-out methods.
With respect to the best performing systems at
the CoNLL shared tasks, our models exploit a rel-
atively smaller set of features and perform signifi-
cantly worse. Nevertheless, performance achieved
by our system are comparable with those reported
by other ME-based systems taking part in the eval-
uations.
Extensive experiments on named-entity recog-
7
nition and text chunking have provided support to
the following claims:
? The introduction of real-valued features dras-
tically reduces the number of parameters of
the ME model with a small loss in perfor-
mance.
? The leave-one-out method is significantly
more effective than the held-out method for
training ME models including real-valued
features.
? The combination of binary and real-valued
features can lead to better MEmodels. In par-
ticular, state-of-the-art ME models with bi-
nary features are significantly improved by
adding complex real-valued features which
model long-span lexical dependencies.
Finally, the GIS training algorithm does not
seem to be the optimal choice for ME models in-
cluding real-valued features. Future work will in-
vestigate variants of and alternatives to the GIS
algorithm. Preliminary experiments on the Base-
Real model showed that training with the Simplex
algorithm (Press et al, 1988) converges to simi-
lar parameter settings 50 times faster than the GIS
algorithm.
9 Acknowledgments
This work was partially financed by the Euro-
pean Commission under the project FAME (IST-
2000-29323), and by the Autonomous Province of
Trento under the the FU-PAT project WebFaq.
References
O. Bender, F. J. Och, and H. Ney. 2003. Maximum
entropy models for named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 148?151. Edmon-
ton, Canada.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A Maximum Entropy Approach to Natural
Language Processing. Computational Linguistics,
22(1):39?72.
A. Borthwick. 1999. A Maximum Entropy approach
to Named Entity Recognition. Ph.D. thesis, Com-
puter Science Department - New York University,
New York, USA.
S. Chen and R. Rosenfeld. 1999. A Gaussian prior
for smoothing maximum entropy models. Techni-
cal Report CMUCS-99-108, Carnegie Mellon Uni-
versity.
J.N. Darroch and D. Ratcliff. 1972. Generalized Itera-
tive Scaling for Log-Liner models. Annals of Math-
ematical Statistics, 43:1470?1480.
M. Federico and N. Bertoldi. 2004. Broadcast news
lm adaptation over time. Computer Speech and Lan-
guage, 18(4):417?435, October.
D. Klakow. 1998. Log-linear interpolation of language
models. In Proceedings of the International Confer-
ence of Spoken Language P rocessing (ICSLP), Sid-
ney, Australia.
R. Koeling. 2000. Chunking with maximum entropy
models. In Proceedings of CoNLL-2000, pages
139?141, Lisbon, Portugal.
C. D. Manning and H. Schutze. 1999. Foundations
of Statistical Natural Language Processing. MIT
Press.
A. Mikheev. 1998. Feature lattices for maximum en-
tropy modelling. In COLING-ACL, pages 848?854.
H. Ney, U. Essen, and R. Kneser. 1994. On structur-
ing probabilistic dependences in stochastic language
modeling. Computer Speech and Language, 8(1):1?
38.
F.J. Och and H. Ney. 2002. Discriminative training and
maximum entropy models for statistical machin e
translation. In ACL02: Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 295?302, PA, Philadelphia.
W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T.
Vetterling. 1988. Numerical Recipes in C. Cam-
bridge University Press, New York, NY.
8
Proceedings of the Workshop on Statistical Machine Translation, pages 1?6,
New York City, June 2006. c?2006 Association for Computational Linguistics
Morpho-syntactic Information for Automatic Error Analysis of Statistical
Machine Translation Output
Maja Popovic??
Hermann Ney?
Adria` de Gispert?
Jose? B. Marin?o?
Deepa Gupta?
Marcello Federico?
Patrik Lambert?
Rafael Banchs?
? Lehrstuhl fu?r Informatik VI - Computer Science Department, RWTH Aachen University, Aachen, Germany
? TALP Research Center, Universitat Polite`cnica de Catalunya (UPC), Barcelona, Spain
? ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy
{popovic,ney}@informatik.rwth-aachen.de {agispert,canton}@gps.tsc.upc.es
{gupta,federico}@itc.it {lambert,banchs}@gps.tsc.upc.es
Abstract
Evaluation of machine translation output
is an important but difficult task. Over the
last years, a variety of automatic evalua-
tion measures have been studied, some of
them like Word Error Rate (WER), Posi-
tion Independent Word Error Rate (PER)
and BLEU and NIST scores have become
widely used tools for comparing different
systems as well as for evaluating improve-
ments within one system. However, these
measures do not give any details about
the nature of translation errors. Therefore
some analysis of the generated output is
needed in order to identify the main prob-
lems and to focus the research efforts. On
the other hand, human evaluation is a time
consuming and expensive task. In this
paper, we investigate methods for using
of morpho-syntactic information for auto-
matic evaluation: standard error measures
WER and PER are calculated on distinct
word classes and forms in order to get a
better idea about the nature of translation
errors and possibilities for improvements.
1 Introduction
The evaluation of the generated output is an impor-
tant issue for all natural language processing (NLP)
tasks, especially for machine translation (MT). Au-
tomatic evaluation is preferred because human eval-
uation is a time consuming and expensive task.
A variety of automatic evaluation measures have
been proposed and studied over the last years, some
of them are shown to be a very useful tool for com-
paring different systems as well as for evaluating
improvements within one system. The most widely
used are Word Error Rate (WER), Position Indepen-
dent Word Error Rate (PER), the BLEU score (Pap-
ineni et al, 2002) and the NIST score (Doddington,
2002). However, none of these measures give any
details about the nature of translation errors. A rela-
tionship between these error measures and the actual
errors in the translation outputs is not easy to find.
Therefore some analysis of the translation errors is
necessary in order to define the main problems and
to focus the research efforts. A framework for hu-
man error analysis and error classification has been
proposed in (Vilar et al, 2006), but like human eval-
uation, this is also a time consuming task.
The goal of this work is to present a framework
for automatic error analysis of machine translation
output based on morpho-syntactic information.
2 Related Work
There is a number of publications dealing with
various automatic evaluation measures for machine
translation output, some of them proposing new
measures, some proposing improvements and exten-
sions of the existing ones (Doddington, 2002; Pap-
ineni et al, 2002; Babych and Hartley, 2004; Ma-
tusov et al, 2005). Semi-automatic evaluation mea-
sures have been also investigated, for example in
(Nie?en et al, 2000). An automatic metric which
uses base forms and synonyms of the words in or-
der to correlate better to human judgements has been
1
proposed in (Banerjee and Lavie, 2005). However,
error analysis is still a rather unexplored area. A
framework for human error analysis and error clas-
sification has been proposed in (Vilar et al, 2006)
and a detailed analysis of the obtained results has
been carried out. Automatic methods for error anal-
ysis to our knowledge have not been studied yet.
Many publications propose the use of morpho-
syntactic information for improving the perfor-
mance of a statistical machine translation system.
Various methods for treating morphological and
syntactical differences between German and English
are investigated in (Nie?en and Ney, 2000; Nie?en
and Ney, 2001a; Nie?en and Ney, 2001b). Mor-
phological analysis has been used for improving
Arabic-English translation (Lee, 2004), for Serbian-
English translation (Popovic? et al, 2005) as well as
for Czech-English translation (Goldwater and Mc-
Closky, 2005). Inflectional morphology of Spanish
verbs is dealt with in (Popovic? and Ney, 2004; de
Gispert et al, 2005). To the best of our knowledge,
the use of morpho-syntactic information for error
analysis of translation output has not been investi-
gated so far.
3 Morpho-syntactic Information and
Automatic Evaluation
We propose the use of morpho-syntactic informa-
tion in combination with the automatic evaluation
measures WER and PER in order to get more details
about the translation errors.
We investigate two types of potential problems for
the translation with the Spanish-English language
pair:
? syntactic differences between the two lan-
guages considering nouns and adjectives
? inflections in the Spanish language considering
mainly verbs, adjectives and nouns
As any other automatic evaluation measures,
these novel measures will be far from perfect. Pos-
sible POS-tagging errors may introduce additional
noise. However, we expect this noise to be suffi-
ciently small and the new measures to be able to give
sufficiently clear ideas about particular errors.
3.1 Syntactic differences
Adjectives in the Spanish language are usually
placed after the corresponding noun, whereas in En-
glish is the other way round. Although in most cases
the phrase based translation system is able to han-
dle these local permutations correctly, some errors
are still present, especially for unseen or rarely seen
noun-adjective groups. In order to investigate this
type of errors, we extract the nouns and adjectives
from both the reference translations and the sys-
tem output and then calculate WER and PER. If the
difference between the obtained WER and PER is
large, this indicates reordering errors: a number of
nouns and adjectives is translated correctly but in the
wrong order.
3.2 Spanish inflections
Spanish has a rich inflectional morphology, espe-
cially for verbs. Person and tense are expressed
by the suffix so that many different full forms of
one verb exist. Spanish adjectives, in contrast to
English, have four possible inflectional forms de-
pending on gender and number. Therefore the er-
ror rates for those word classes are expected to be
higher for Spanish than for English. Also, the er-
ror rates for the Spanish base forms are expected to
be lower than for the full forms. In order to investi-
gate potential inflection errors, we compare the PER
for verbs, adjectives and nouns for both languages.
For the Spanish language, we also investigate differ-
ences between full form PER and base form PER:
the larger these differences, more inflection errors
are present.
4 Experimental Settings
4.1 Task and Corpus
The corpus analysed in this work is built in the
framework of the TC-Star project. It contains more
than one million sentences and about 35 million run-
ning words of the Spanish and English European
Parliament Plenary Sessions (EPPS). A description
of the EPPS data can be found in (Vilar et al, 2005).
In order to analyse effects of data sparseness, we
have randomly extracted a small subset referred to
as 13k containing about thirteen thousand sentences
and 370k running words (about 1% of the original
2
Training corpus: Spanish English
full Sentences 1281427
Running Words 36578514 34918192
Vocabulary 153124 106496
Singletons [%] 35.2 36.2
13k Sentences 13360
Running Words 385198 366055
Vocabulary 22425 16326
Singletons [%] 47.6 43.7
Dev: Sentences 1008
Running Words 25778 26070
Distinct Words 3895 3173
OOVs (full) [%] 0.15 0.09
OOVs (13k) [%] 2.7 1.7
Test: Sentences 840 1094
Running Words 22774 26917
Distinct Words 4081 3958
OOVs (full) [%] 0.14 0.25
OOVs (13k) [%] 2.8 2.6
Table 1: Corpus statistics for the Spanish-English
EPPS task (running words include punctuation
marks)
corpus). The statistics of the corpora can be seen in
Table 1.
4.2 Translation System
The statistical machine translation system used in
this work is based on a log-linear combination of
seven different models. The most important ones are
phrase based models in both directions, additionally
IBM1 models at the phrase level in both directions
as well as phrase and length penalty are used. A
more detailed description of the system can be found
in (Vilar et al, 2005; Zens et al, 2005).
4.3 Experiments
The translation experiments have been done in both
translation directions on both sizes of the corpus. In
order to examine improvements of the baseline sys-
tem, a new system with POS-based word reorderings
of nouns and adjectives as proposed in (Popovic? and
Ney, 2006) is also analysed. Adjectives in the Span-
ish language are usually placed after the correspond-
ing noun, whereas for English it is the other way
round. Therefore, local reorderings of nouns and ad-
Spanish?English WER PER BLEU
full baseline 34.5 25.5 54.7
reorder 33.5 25.2 56.4
13k baseline 41.8 30.7 43.2
reorder 38.9 29.5 48.5
English?Spanish WER PER BLEU
full baseline 39.7 30.6 47.8
reorder 39.6 30.5 48.3
13k baseline 49.6 37.4 36.2
reorder 48.1 36.5 37.7
Table 2: Translation Results [%]
jective groups in the source language have been ap-
plied. If the source language is Spanish, each noun is
moved behind the corresponding adjective group. If
the source language is English, each adjective group
is moved behind the corresponding noun. An adverb
followed by an adjective (e.g. ?more important?) or
two adjectives with a coordinate conjunction in be-
tween (e.g. ?economic and political?) are treated as
an adjective group. Standard translation results are
presented in Table 2.
5 Error Analysis
5.1 Syntactic errors
As explained in Section 3.1, reordering errors due
to syntactic differences between two languages have
been measured by the relative difference between
WER and PER calculated on nouns and adjectives.
Corresponding relative differences are calculated
also for verbs as well as adjectives and nouns sep-
arately.
Table 3 presents the relative differences for the
English and Spanish output. It can be seen that
the PER/WER difference for nouns and adjectives
is relatively high for both language pairs (more than
20%), and for the English output is higher than for
the Spanish one. This corresponds to the fact that
the Spanish language has a rather free word order:
although the adjective usually is placed behind the
noun, this is not always the case. On the other hand,
adjectives in English are always placed before the
corresponding noun. It can also be seen that the
difference is higher for the reduced corpus for both
outputs indicating that the local reordering problem
3
English output 1? PERWER
full nouns+adjectives 24.7
+reordering 20.8
verbs 4.1
adjectives 10.2
nouns 20.1
13k nouns+adjectives 25.7
+reordering 20.1
verbs 4.6
adjectives 8.4
nouns 19.1
Spanish output 1? PERWER
full nouns+adjectives 21.5
+reordering 20.3
verbs 3.3
adjectives 5.6
nouns 16.9
13k nouns+adjectives 22.9
+reordering 19.8
verbs 3.9
adjectives 5.4
nouns 19.3
Table 3: Relative difference between PER and
WER [%] for different word classes
is more important when only small amount of train-
ing data is available. As mentioned in Section 3.1,
the phrase based translation system is able to gen-
erate frequent noun-adjective groups in the correct
word order, but unseen or rarely seen groups intro-
duce difficulties.
Furthermore, the results show that the POS-based
reordering of adjectives and nouns leads to a de-
crease of the PER/WER difference for both out-
puts and for both corpora. Relative decrease of the
PER/WER difference is larger for the small corpus
than for the full corpus. It can also be noted that the
relative decrease for both corpora is larger for the
English output than for the Spanish one due to free
word order - since the Spanish adjective group is not
always placed behind the noun, some reorderings in
English are not really needed.
For the verbs, PER/WER difference is less than
5% for both outputs and both training corpora, in-
dicating that the word order of verbs is not an im-
English output PER
full verbs 44.8
adjectives 27.3
nouns 23.0
13k verbs 56.1
adjectives 38.1
nouns 31.7
Spanish output PER
full verbs 61.4
adjectives 41.8
nouns 28.5
13k verbs 73.0
adjectives 50.9
nouns 37.0
Table 4: PER [%] for different word classes
portant issue for the Spanish-English language pair.
PER/WER difference for adjectives and nouns is
higher than for verbs, for the nouns being signifi-
cantly higher than for adjectives. The reason for this
is probably the fact that word order differences in-
volving only the nouns are also present, for example
?export control = control de exportacio?n?.
5.2 Inflectional errors
Table 4 presents the PER for different word classes
for the English and Spanish output respectively. It
can be seen that all PERs are higher for the Spanish
output than for the English one due to the rich in-
flectional morphology of the Spanish language. It
can be also seen that the Spanish verbs are espe-
cially problematic (as stated in (Vilar et al, 2006))
reaching 60% of PER for the full corpus and more
than 70% for the reduced corpus. Spanish adjectives
also have a significantly higher PER than the English
ones, whereas for the nouns this difference is not so
high.
Results of the further analysis of inflectional er-
rors are presented in Table 5. Relative difference
between full form PER and base form PER is sig-
nificantly lower for adjectives and nouns than for
verbs, thus showing that the verb inflections are the
main source of translation errors into the Spanish
language.
Furthermore, it can be seen that for the small cor-
4
Spanish output 1? PERbPERf
full verbs 26.9
adjectives 9.3
nouns 8.4
13k verbs 23.7
adjectives 15.1
nouns 6.5
Table 5: Relative difference between PER of base
forms and PER of full forms [%] for the Spanish
output
pus base/full PER difference for verbs and nouns is
basically the same as for the full corpus. Since nouns
in Spanish only have singular and plural form as in
English, the number of unseen forms is not partic-
ularly enlarged by the reduction of the training cor-
pus. On the other hand, base/full PER difference of
adjectives is significantly higher for the small corpus
due to an increased number of unseen adjective full
forms.
As for verbs, intuitively it might be expected that
the number of inflectional errors for this word class
also increases by reducing the training corpus, even
more than for adjectives. However, the base/full
PER difference is not larger for the small corpus,
but even smaller. This is indicating that the problem
of choosing the right inflection of a Spanish verb ap-
parently is not related to the number of unseen full
forms since the number of inflectional errors is very
high even when the translation system is trained on
a very large corpus.
6 Conclusion
In this work, we presented a framework for auto-
matic analysis of translation errors based on the use
of morpho-syntactic information. We carried out a
detailed analysis which has shown that the results
obtained by our method correspond to those ob-
tained by human error analysis in (Vilar et al, 2006).
Additionally, it has been shown that the improve-
ments of the baseline system can be adequately mea-
sured as well.
This work is just a first step towards the devel-
opment of linguistically-informed evaluation mea-
sures which provide partial and more specific infor-
mation of certain translation problems. Such mea-
sures are very important to understand what are the
weaknesses of a statistical machine translation sys-
tem, and what are the best ways and methods for
improvements.
For our future work, we plan to extend the pro-
posed measures in order to carry out a more de-
tailed error analysis, for example examinating dif-
ferent types of inflection errors for Spanish verbs.
We also plan to investigate other types of translation
errors and other language pairs.
Acknowledgements
This work was partly supported by the TC-STAR
project by the European Community (FP6-506738)
and partly by the Generalitat de Catalunya and the
European Social Fund.
References
Bogdan Babych and Anthony Hartley. 2004. Extending
bleu mt evaluation method with frequency weighting.
In Proc. of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Barcelona,
Spain, July.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for mt evaluation with improved
correlation with human judgements. In 43rd Annual
Meeting of the Assoc. for Computational Linguistics:
Proc. Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, pages 65?72,
Ann Arbor, MI, June.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego.
2005. Improving statistical machine translation by
classifying and generalizing inflected verb forms. In
Proc. of the 9th European Conf. on Speech Commu-
nication and Technology (Interspeech), pages 3185?
3188, Lisbon, Portugal, September.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology, pages 128?132, San Diego.
Sharon Goldwater and David McClosky. 2005. Improv-
ing stastistical machine translation through morpho-
logical analysis. In Proc. of the Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
Vancouver, Canada, October.
Young-suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In Proc. 2004 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), Boston, MA, May.
5
Evgeny Matusov, Gregor Leusch, Oliver Bender, and
Hermann Ney. 2005. Evaluating machine transla-
tion output with automatic sentence segmentation. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 148?154, Pitts-
burgh, PA, October.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In COLING
?00: The 18th Int. Conf. on Computational Linguistics,
pages 1081?1085, Saarbru?cken, Germany, July.
Sonja Nie?en and Hermann Ney. 2001a. Morpho-
syntactic analysis for reordering in statistical machine
translation. In Proc. MT Summit VIII, pages 247?252,
Santiago de Compostela, Galicia, Spain, September.
Sonja Nie?en and Hermann Ney. 2001b. Toward hier-
archical models for statistical machine translation of
inflected languages. In Data-Driven Machine Trans-
lation Workshop, pages 47?54, Toulouse, France, July.
Sonja Nie?en, Franz J. Och, Gregor Leusch, and Her-
mann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research. In
Proc. Second Int. Conf. on Language Resources and
Evaluation (LREC), pages 39?45, Athens, Greece,
May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Maja Popovic? and Hermann Ney. 2004. Towards the use
of word stems & suffixes for statistical machine trans-
lation. In Proc. 4th Int. Conf. on Language Resources
and Evaluation (LREC), pages 1585?1588, Lissabon,
Portugal, May.
Maja Popovic? and Hermann Ney. 2006. POS-based
word reorderings for statistical machine translation. In
Proc. of the Fifth Int. Conf. on Language Resources
and Evaluation (LREC), Genova, Italy, May.
Maja Popovic?, David Vilar, Hermann Ney, Slobodan
Jovic?ic?, and Zoran ?Saric?. 2005. Augmenting a small
parallel text with morpho-syntactic language resources
for Serbian?English statistical machine translation. In
43rd Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Building and Using
Parallel Texts: Data-Driven Machine Translation and
Beyond, pages 41?48, Ann Arbor, MI, June.
David Vilar, Evgeny Matusov, Sas?a Hasan, Richard Zens,
and Hermann Ney. 2005. Statistical machine transla-
tion of european parliamentary speeches. In Proc. MT
Summit X, pages 259?266, Phuket, Thailand, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error analysis of statistical machine
translation output. In Proc. of the Fifth Int. Conf. on
Language Resources and Evaluation (LREC), page to
appear, Genova, Italy, May.
Richard Zens, Oliver Bender, Sas?a Hasan, Shahram
Khadivi, Evgeny Matusov, Jia Xu, Yuqi Zhang, and
Hermann Ney. 2005. The RWTH phrase-based statis-
tical machine translation system. In Proceedings of the
International Workshop on Spoken Language Transla-
tion (IWSLT), pages 155?162, Pittsburgh, PA, October.
6
Proceedings of the Workshop on Statistical Machine Translation, pages 94?101,
New York City, June 2006. c?2006 Association for Computational Linguistics
How Many Bits Are Needed To Store Probabilities
for Phrase-Based Translation?
Marcello Federico and Nicola Bertoldi
ITC-irst - Centro per la Ricerca Scientica e Tecnologica
38050 Povo - Trento, Italy
{federico,bertoldi}@itc.it
Abstract
State of the art in statistical machine trans-
lation is currently represented by phrase-
based models, which typically incorpo-
rate a large number of probabilities of
phrase-pairs and word n-grams. In this
work, we investigate data compression
methods for efficiently encoding n-gram
and phrase-pair probabilities, that are usu-
ally encoded in 32-bit floating point num-
bers. We measured the impact of com-
pression on translation quality through a
phrase-based decoder trained on two dis-
tinct tasks: the translation of European
Parliament speeches from Spanish to En-
glish, and the translation of news agencies
from Chinese to English. We show that
with a very simple quantization scheme all
probabilities can be encoded in just 4 bits
with a relative loss in BLEU score on the
two tasks by 1.0% and 1.6%, respectively.
1 Introduction
In several natural language processing tasks, such as
automatic speech recognition and machine transla-
tion, state-of-the-art systems rely on the statistical
approach.
Statistical machine translation (SMT) is based
on parametric models incorporating a large num-
ber of observations and probabilities estimated from
monolingual and parallel texts. The current state of
the art is represented by the so-called phrase-based
translation approach (Och and Ney, 2004; Koehn et
al., 2003). Its core components are a translation
model that contains probabilities of phrase-pairs,
and a language model that incorporates probabilities
of word n-grams.
Due to the intrinsic data-sparseness of language
corpora, the set of observations increases almost lin-
early with the size of the training data. Hence, to
efficiently store observations and probabilities in a
computer memory the following approaches can be
tackled: designing compact data-structures, pruning
rare or unreliable observations, and applying data
compression.
In this paper we only focus on the last approach.
We investigate two different quantization methods
to encode probabilities and analyze their impact on
translation performance. In particular, we address
the following questions:
? How does probability quantization impact on
the components of the translation system,
namely the language model and the translation
model?
? Which is the optimal trade-off between data
compression and translation performance?
? How do quantized models perform under dif-
ferent data-sparseness conditions?
? Is the impact of quantization consistent across
different translation tasks?
Experiments were performed with our phrase-
based SMT system (Federico and Bertoldi, 2005) on
two large-vocabulary tasks: the translation of Euro-
pean Parliament Plenary Sessions from Spanish to
94
English, and the translation of news agencies from
Chinese to English, according to the set up defined
by the 2005 NIST MT Evaluation Workshop.
The paper is organized as follows. Section 2 re-
views previous work addressing efficiency in speech
recognition and information retrieval. Section 3 in-
troduces the two quantization methods considered
in this paper, namely the Lloyd?s algorithm and the
Binning method. Section 4 briefly describes our
phrase-based SMT system. Sections 5 reports and
discusses experimental results addressing the ques-
tions in the introduction. Finally, Section 6 draws
some conclusions.
2 Previous work
Most related work can be found in the area of speech
recognition, where n-gram language models have
been used for a while.
Efforts targeting efficiency have been mainly fo-
cused on pruning techniques (Seymore and Rosen-
feld, 1996; Gao and Zhang, 2002), which permit
to significantly reduce the amount of n-grams to be
stored at a negligible cost in performance. More-
over, very compact data-structures for storing back-
off n-gram models have been recently proposed by
Raj and Whittaker (2003).
Whittaker and Raj (2001) discuss probability en-
coding as a means to reduce memory requirements
of an n-gram language model. Quantization of a
3-gram back-off model was performed by applying
the k-means Lloyd-Max algorithm at each n-gram
level. Experiments were performed on several large-
vocabulary speech recognition tasks by considering
different levels of compression. By encoded proba-
bilities in 4 bits, the increase in word-error-rate was
only around 2% relative with respect to a baseline
using 32-bit floating point probabilities.
Similar work was carried out in the field of in-
formation retrieval, where memory efficiency is in-
stead related to the indexing data structure, which
contains information about frequencies of terms in
all the individual documents. Franz and McCarley
(2002) investigated quantization of term frequencies
by applying a binning method. The impact on re-
trieval performance was analyzed against different
quantization levels. Results showed that 2 bits are
sufficient to encode term frequencies at the cost of a
negligible loss in performance.
In our work, we investigate both data compres-
sion methods, namely the Lloyd?s algorithm and the
binning method, in a SMT framework.
3 Quantization
Quantization provides an effective way of reducing
the number of bits needed to store floating point
variables. The quantization process consists in par-
titioning the real space into a finite set of k quantiza-
tion levels and identifying a center ci for each level,
i = 1, . . . , k. A function q(x) maps any real-valued
point x onto its unique center ci. Cost of quantiza-tion is the approximation error between x and ci.
If k = 2h, h bits are enough to represent a floating
point variable; as a floating point is usually encoded
in 32 bits (4 byte), the compression ratio is equal
to 32/h1 . Hence, the compression ratio also gives
an upper bound for the relative reduction of mem-
ory use, because it assumes an optimal implemen-
tation of data structures without any memory waste.
Notice that memory consumption for storing the k-
entry codebook is negligible (k ? 32 bits).
As we will apply quantization on probabilistic
distribution, we can restrict the range of real val-
ues between 0 and 1. Most quantization algorithms
require a fixed (although huge) amount of points
in order to define the quantization levels and their
centers. Probabilistic models used in SMT satisfy
this requirement because the set of parameters larger
than 0 is always limited.
Quantization algorithms differ in the way parti-
tion of data points is computed and centers are iden-
tified. In this paper we investigate two different
quantization algorithms.
Lloyd?s Algorithm
Quantization of a finite set of real-valued data points
can be seen as a clustering problem. A large fam-
ily of clustering algorithms, called k-means algo-
rithms (Kanungo et al, 2002), look for optimal cen-
ters ci which minimize the mean squared distancefrom each data point to its nearest center. The map
between points and centers is trivially derived.
1In the computation of the compression ratio we take into
account only the memory needed to store the probabilities of the
observations, and not the memory needed to store the observa-
tions themselves which depends on the adopted data structures.
95
As no efficient exact solution to this problem
is known, either polynomial-time approximation or
heuristic algorithms have been proposed to tackle
the problem. In particular, Lloyd?s algorithm starts
from a feasible set of centers and iteratively moves
them until some convergence criterion is satisfied.
Finally, the algorithm finds a local optimal solution.
In this work we applied the version of the algorithm
available in the K-MEANS package2 .
Binning Method
The binning method partitions data points into uni-
formly populated intervals or bins. The center of
each bin corresponds to the mean value of all points
falling into it. If Ni is the number of points of the
i-th bin, and xi the smallest point in the i-th bin, apartition [xi, xi+1] results such that Ni is constantfor each i = 0, . . . , k ? 1, where xk = 1 by default.The following map is thus defined:
q(x) = ci if xi <= x < xi+1.
Our implementation uses the following greedy
strategy: bins are build by uniformly partition all
different points of the data set.
4 Phrase-based Translation System
Given a string f in the source language, our SMT
system (Federico and Bertoldi, 2005; Cettolo et al,
2005), looks for the target string e maximizing the
posterior probability Pr(e,a | f) over all possible
word alignments a. The conditional distribution is
computed with the log-linear model:
p?(e,a | f) ? exp
{ R
?
r=1
?rhr(e, f ,a)
}
,
where hr(e, f ,a), r = 1 . . . R are real valued featurefunctions.
The log-linear model is used to score translation
hypotheses (e,a) built in terms of strings of phrases,
which are simple sequences of words. The transla-
tion process works as follows. At each step, a target
phrase is added to the translation whose correspond-
ing source phrase within f is identified through three
random quantities: the fertility which establishes its
length; the permutation which sets its first position;
2www.cs.umd.edu/?mount/Projects/KMeans.
the tablet which tells its word string. Notice that tar-
get phrases might have fertility equal to zero, hence
they do not translate any source word. Moreover,
untranslated words in f are also modeled through
some random variables.
The choice of permutation and tablets can be
constrained in order to limit the search space un-
til performing a monotone phrase-based translation.
In any case, local word reordering is permitted by
phrases.
The above process is performed by a beam-search
decoder and is modeled with twelve feature func-
tions (Cettolo et al, 2005) which are either esti-
mated from data, e.g. the target n-gram language
models and the phrase-based translation model, or
empirically fixed, e.g. the permutation models.
While feature functions exploit statistics extracted
from monolingual or word-aligned texts from the
training data, the scaling factors ? of the log-linear
model are empirically estimated on development
data.
The two most memory consuming feature func-
tions are the phrase-based Translation Model (TM)
and the n-gram Language Model (LM).
Translation Model
The TM contains phrase-pairs statistics computed
on a parallel corpus provided with word-alignments
in both directions. Phrase-pairs up to length 8 are
extracted and singleton observations are pruned off.
For each extracted phrase-pair (f? , e?), four transla-
tion probabilities are estimated:
? a smoothed frequency of f? given e?
? a smoothed frequency of e? given f?
? an IBM model 1 based probability of e? given f?
? an IBM model 1 based probability of f? given e?
Hence, the number of parameters of the transla-
tion models corresponds to 4 times the number of
extracted phrase-pairs. From the point of view of
quantization, the four types of probabilities are con-
sidered separately and a specific codebook is gener-
ated for each type.
Language Model
The LM is a 4-gram back-off model estimated with
the modied Kneser-Ney smoothing method (Chen
and Goodman, 1998). Singleton pruning is applied
on 3-gram and 4-gram statistics. In terms of num-
96
task parallel resources mono resources LM TM
src trg words 1-gram 2-gram 3-gram 4-gram phrase pairs
NIST 82,168 88,159 463,855 1,408 20,475 29,182 46,326 10,410
EPPS 34,460 32,951 3,2951 110 2,252 2,191 2,677 3,877
EPPS-800 23,611 22,520 22,520 90 1,778 1,586 1,834 2,499
EPPS-400 11,816 11,181 11,181 65 1,143 859 897 1,326
EPPS-200 5,954 5,639 5,639 47 738 464 439 712
EPPS-100 2,994 2,845 2,845 35 469 246 213 387
Table 1: Figures (in thousand) regarding the training data of each translation task.
ber of parameters, each n-gram, with n < 4, has
two probabilities associated with: the probability of
the n-gram itself, and the back-off probability of the
corresponding n + 1-gram extensions. Finally, 4-
grams have only one probability associated with.
For the sake of quantization, two separate code-
books are generated for each of the first three lev-
els, and one codebook is generated for the last level.
Hence, a total of 7 codebooks are generated. In all
discussed quantized LMs, unigram probabilities are
always encoded with 8 bits. The reason is that uni-
gram probabilities have indeed the largest variability
and do not contribute significantly to the total num-
ber of parameters.
5 Experiments
Data and Experimental Framework
We performed experiments on two large vocabulary
translation tasks: the translation of European Parlia-
mentary Plenary Sessions (EPPS) (Vilar et al, 2005)
from Spanish to English, and the translation of doc-
uments from Chinese to English as proposed by the
NIST MT Evaluation Workshops3 .
Translation of EPPS is performed on the so-called
final text editions, which are prepared by the trans-
lation office of the European Parliament. Both the
training and testing data were collected by the TC-
STAR4 project and were made freely available to
participants in the 2006 TC-STAR Evaluation Cam-
paign. In order to perform experiments under differ-
ent data sparseness conditions, four subsamples of
the training data with different sizes were generated,
too.
Training and test data used for the NIST task are
3www.nist.gov/speech/tests/mt/.
4www.tc-star.org
task sentences src words ref words
EPPS 840 22725 23066
NIST 919 25586 29155
Table 2: Statistics of test data for each task.
available through the Linguistic Data Consortium5.
Employed training data meet the requirements set
for the Chinese-English large-data track of the 2005
NIST MT Evaluation Workshop. For testing we
used instead the NIST 2003 test set.
Table 1 reports statistics about the training data of
each task and the models estimated on them. That
is, the number of running words of source and target
languages, the number of n-grams in the language
model and the number phrase-pairs in the transla-
tion model. Table 2 reports instead statistics about
the test sets, namely, the number of source sentences
and running words in the source part and in the gold
reference translations.
Translation performance was measured in terms
of BLEU score, NIST score, word-error rate (WER),
and position independent error rate (PER). Score
computation relied on two and four reference trans-
lations per sentence, respectively, for the EPPS
and NIST tasks. Scores were computed in case-
insensitive modality with punctuation. In general,
none of the above measures is alone sufficiently in-
formative about translation quality, however, in the
community there seems to be a preference toward
reporting results with BLEU. Here, to be on the safe
side and to better support our findings we will report
results with all measures, but will limit discussion
on performance to the BLEU score.
In order to just focus on the effect of quantiza-
5www.ldc.upenn.edu
97
LM-h
32 8 6 5 4 3 2
32 54.78 54.75 54.73 54.65 54.49 54.24 53.82
8 54.78 54.69 54.69 54.79 54.55 54.18 53.65
6 54.57 54.49 54.76 54.57 54.63 54.26 53.60
TM-h 5 54.68 54.68 54.56 54.61 54.60 54.10 53.39
4 54.37 54.36 54.47 54.44 54.23 54.06 53.26
3 54.28 54.03 54.22 53.96 53.75 53.69 53.03
2 53.58 53.51 53.47 53.35 53.39 53.41 52.41
Table 3: BLEU scores in the EPPS task with different quantization levels of the LM and TM.
tion, all reported experiments were performed with
a plain configuration of the ITC-irst SMT system.
That is, we used a single decoding step, no phrase
re-ordering, and task-dependent weights of the log-
linear model.
Henceforth, LMs and TM quantized with h bits
are denoted with LM-h and TM-h, respectively.
Non quantized models are indicated with LM-32
and TM-32.
Impact of Quantization on LM and TM
A first set of experiments was performed on the
EPPS task by applying probability quantization ei-
ther on the LM or on the TMs. Figures 1 and 2
compare the two proposed quantization algorithms
(LLOYD and BINNING) against different levels of
quantization, namely 2, 3, 4, 5, 6, and 8 bits.
The scores achieved by the non quantized models
(LM-32 and TM-32) are reported as reference.
The following considerations can be drawn from
these results. The Binning method works slightly,
but not significantly, better than the Lloyd?s algo-
rithm, especially with the highest compression ra-
tios.
In general, the LM seems less affected by data
compression than the TM. By comparing quantiza-
tion with the binning method against no quantiza-
tion, the BLEU score with LM-4 is only 0.42% rel-
ative worse (54.78 vs 54.55). Degradation of BLEU
score by TM-4 is 0.77% (54.78 vs 54.36). For all the
models, encoding with 8 bits does not affect transla-
tion quality at all.
In following experiments, binning quantization
was applied to both LM and TM. Figure 3 plots
all scores against different levels of quantization.
As references, the curves corresponding to only
LM-h TM-h BLEU NIST WER PER
32 32 28.82 8.769 62.41 42.30
8 8 28.87 8.772 62.39 42.19
4 4 28.36 8.742 62.94 42.45
2 2 25.95 8.491 65.87 44.04
Table 4: Translation scores on the NIST task with
different quantization levels of the LM and TM.
LM quantization (LM-h) and only TM quantization
(TM-h) are shown. Independent levels of quantiza-
tion of the LM and TM were also considered. BLEU
scores related to several combinations are reported
in Table 3.
Results show that the joint impact of LM and TM
quantization is almost additive. Degradation with
4 bits quantization is only about 1% relative (from
54.78 to 54.23). Quantization with 2 bits is sur-
prisingly robust: the BLEU score just decreases by
4.33% relative (from 54.78 to 52.41).
Quantization vs. Data Sparseness
Quantization of LM and TM was evaluated with re-
spect to data-sparseness. Quantized and not quan-
tized models were trained on four subset of the EPPS
corpus with decreasing size. Statistics about these
sub-corpora are reported in Table 1. Quantization
was performed with the binning method using 2,
4, and 8 bit encodings. Results in terms of BLEU
score are plotted in Figure 4. It is evident that the
gap in BLEU score between the quantized and not
quantized models is almost constant under different
training conditions. This result suggests that perfor-
mance of quantized models is not affected by data
sparseness.
98
Consistency Across Different Tasks
A subset of quantization settings tested with the
EPPS tasks was also evaluated on the NIST task.
Results are reported in Table 4.
Quantization with 8 bits does not affect perfor-
mance, and gives even slightly better scores. Also
quantization with 4 bits produces scores very close
to those of non quantized models, with a loss in
BLEU score of only 1.60% relative. However, push-
ing quantization to 2 bits significantly deteriorates
performance, with a drop in BLEU score of 9.96%
relative.
In comparison to the EPPS task, performance
degradation due to quantization seems to be twice as
large. In conclusion, consistent behavior is observed
among different degrees of compression. Absolute
loss in performance, though quite different from the
EPPS task, remains nevertheless very reasonable.
Performance vs. Compression
From the results of single versus combined com-
pression, we can reasonably assume that perfor-
mance degradation due to quantization of LM and
TM probabilities is additive. Hence, as memory sav-
ings on the two models are also independent we can
look at the optimal trade-off between performance
and compression separately. Experiments on the
NIST and EPPS tasks seem to show that encoding
of LM and TM probabilities with 4 bits provides the
best trade-off, that is a compression ratio of 8 with a
relative loss in BLEU score of 1% and 1.6%. It can
be seen that score degradation below 4 bits grows
generally faster than the corresponding memory sav-
ings.
6 Conclusion
In this paper we investigated the application of data
compression methods to the probabilities stored by
a phrase-based translation model. In particular,
probability quantization was applied on the n-gram
language model and on the phrase-pair translation
model. Experimental results confirm previous find-
ings in speech recognition: language model proba-
bilities can be encoded in just 4 bits at the cost of
a very little loss in performance. The same resolu-
tion level seems to be a good compromise even for
the translation model. Remarkably, the impact of
quantization on the language model and translation
model seems to be additive with respect to perfor-
mance. Finally, quantization does not seems to be
affected by data sparseness and behaves similarly on
different translation tasks.
References
M Cettolo, M. Federico, N. Bertoldi, R. Cattoni, and B.Chen. 2005. A Look Inside the ITC-irst SMT Sys-
tem. In Proc. of MT Summit X, pp. 451?457, Pukhet,Thailand.
S. F. Chen and J. Goodman. 1998. An Empirical
Study of Smoothing Techniques for Language Mod-eling. Technical Report TR-10-98, Computer ScienceGroup, Harvard University, Cambridge, MA, USA.
M. Federico and N. Bertoldi. 2005. A Word-to-PhraseStatistical Translation Model. ACM Transaction onSpeech Language Processing, 2(2):1?24.
M. Franz and J. S. McCarley. 2002. How Many Bits areNeeded to Store Term Frequencies. In Proc. of ACMSIGIR, pp. 377?378, Tampere, Finland.
J. Gao and M. Zhang. 2002. Improving Language ModelSize Reduction using Better Pruning Criteria. In Proc.of ACL, pp. 176?182, Philadelphia, PA.
T. Kanungo, D. M. Mount, N. Netanyahu, C. Piatko,
R. Silverman, , and A. Y. Wu. 2002. An EfficientK-Means Clustering Algorithm: Analysis and Imple-mentation. IEEE Transaction on Pattern Analysis andMachine Intelligence, 24(7):881?892.
P. Koehn, F. J. Och, and D. Marcu. 2003. StatisticalPhrase-Based Translation. In Proc. of HLT/NAACL2003, pp. 127?133, Edmonton, Canada.
F. J. Och and H. Ney. 2004. The Alignment Template
Approach to Statistical Machine Translation. Compu-tational Linguistics, 30(4):417?449.
B. Raj and E. W. D. Whittaker. 2003. Lossless Compres-
sion of Language Model Structure and Word Identi-fiers. In Proc. of ICASSP, pp. 388?391, Honk Kong.
K. Seymore and R. Rosenfeld. 1996. Scalable Backoff
Language Models. In Proc. of ICSLP, vol. 1, pp. 232?235, Philadelphia, PA.
D. Vilar, E. Matusov, S. Hasan, R . Zens, , and H. Ney.2005. Statistical Machine Translation of EuropeanParliamentary Speeches. In Proc. of MT Summit X,
pp. 259?266, Pukhet, Thailand.
E. W. D. Whittaker and B. Raj. 2001. Quantization-based Language Model Compression. In Proc. of Eu-rospeech, pp. 33?36, Aalborg, Denmark.
99
 51
 51.5
 52
 52.5
 53
 53.5
 54
 54.5
 55
32865432
B
L
E
U
 
S
C
O
R
E
BITS
BINNING
LLOYD
 10.3
 10.4
 10.5
 10.6
 10.7
 10.8
32865432
N
I
S
T
 
S
C
O
R
E
BITS
BINNING
LLOYD
 34.5
 35
 35.5
 36
 36.5
32865432
W
E
R
BITS
BINNING
LLOYD
 25
 25.5
 26
 26.5
 27
 27.5
 28
32865432
P
E
R
BITS
BINNING
LLOYD
Figure 1: EPPS task: translation scores vs. quantization level of LM. TM is not quantized.
 51
 51.5
 52
 52.5
 53
 53.5
 54
 54.5
 55
32865432
B
L
E
U
 
S
C
O
R
E
BITS
BINNING
LLOYD
 10.3
 10.4
 10.5
 10.6
 10.7
 10.8
32865432
N
I
S
T
 
S
C
O
R
E
BITS
BINNING
LLOYD
 34.5
 35
 35.5
 36
 36.5
32865432
W
E
R
BITS
BINNING
LLOYD
 25
 25.5
 26
 26.5
 27
 27.5
 28
32865432
P
E
R
BITS
BINNING
LLOYD
Figure 2: EPPS task: translation scores vs. quantization level of TM. LM is not quantized.
100
 52
 52.5
 53
 53.5
 54
 54.5
 55
32865432
B
L
E
U
 
S
C
O
R
E
BITS
LM-h+TM-h
LM-h
TM-h
 10.4
 10.45
 10.5
 10.55
 10.6
 10.65
 10.7
 10.75
 10.8
32865432
N
I
S
T
 
S
C
O
R
E
BITS
LM-h+TM-h
LM-h
TM-h
 34.6
 34.8
 35
 35.2
 35.4
 35.6
 35.8
 36
32865432
W
E
R
BITS
LM-h+TM-h
LM-h
TM-h
 25
 25.5
 26
 26.5
 27
32865432
P
E
R
BITS
LM-h+TM-h
LM-h
TM-h
Figure 3: EPPS task: translation scores vs. quantization level of LM and TM. Quantization was performed
with the Binning algorithm.
 44
 46
 48
 50
 52
 54
EPPSEPPS-800EPPS-400EPPS-200EPPS-100
B
L
E
U
 
S
C
O
R
E
LM-32+TM32
LM-8+TM-8
LM-4+TM-4
LM-2+TM-2
 9.6
 9.8
 10
 10.2
 10.4
 10.6
 10.8
EPPSEPPS-800EPPS-400EPPS-200EPPS-100
N
I
S
T
 
S
C
O
R
E
LM-32+TM-32
LM-8+TM-8
LM-4+TM-4
LM-2+TM-2
 34
 35
 36
 37
 38
 39
 40
 41
EPPSEPPS-800EPPS-400EPPS-200EPPS-100
W
E
R
 
S
C
O
R
E
LM-32+TM-32
LM-8+TM-8
LM-4+TM-4
LM-2+TM-2
 25
 26
 27
 28
 29
 30
EPPSEPPS-800EPPS-400EPPS-200EPPS-100
P
E
R
 
S
C
O
R
E
LM-32+TM-32
LM-8+TM-8
LM-4+TM-4
LM-2+TM-2
Figure 4: EPPS task: translation scores vs. amount of training data. Different levels of quantization were
generated with the Binning algorithm.
101
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 120?123, Dublin, Ireland, August 23-29 2014.
MT-EQuAl: a Toolkit for Human Assessment of Machine Translation
Output
Christian Girardi
(1)
Luisa Bentivogli
(1)
Mohammad Amin Farajian
(1,2)
Marcello Federico
(1)
(1)
FBK - Fondazione Bruno Kessler, Trento, Italy
(2)
University of Trento, Italy
{cgirardi,bentivo,farajian,federico}@fbk.eu
Abstract
MT-EQuAl (Machine Translation Errors, Quality, Alignment) is a toolkit for human assessment
of Machine Translation (MT) output. MT-EQuAl implements three different tasks in an inte-
grated environment: annotation of translation errors, translation quality rating (e.g. adequacy
and fluency, relative ranking of alternative translations), and word alignment. The toolkit is web-
based and multi-user, allowing large scale and remotely managed manual annotation projects. It
incorporates a number of project management functions and sophisticated progress monitoring
capabilities. The implemented evaluation tasks are configurable and can be adapted to several
specific annotation needs. The toolkit is open source and released under Apache 2.0 license.
1 Introduction
It is widely recognized within the MT field that human evaluation can play a crucial role in improv-
ing MT technology. Despite the well-known difficulties in collecting human annotations (the process is
time-consuming, costly and often subjective), state of the art MT research is now moving towards inte-
grating as much as possible human quality assessment into the MT workflow. The most commonly used
human evaluation methodologies are based on absolute adequacy and fluency scores, relative ranking of
alternative MT outputs, and, more recently, human post-editing. Although very useful, these methods do
not provide information about the specific problems of MT systems. To address this limitation, new ap-
proaches based on human error analysis have emerged, where annotators identify and classify translation
errors thus giving precise indications about specific deficiencies of the evaluated MT systems. Given the
outlined trend, it is of the utmost importance to make available to the MT community tools (i) able to
support large-scale annotation projects involving a great variety of languages, (ii) addressing the most
required MT assessment tasks, and (iii) designed in a way to reduce as much as possible the problems re-
lated to manual annotation. MT-EQuAl is a toolkit for the manual assessment of MT output, created with
the aim of addressing the above requirements. The main characteristics of MT-EQuAl are the following:
? Web-based and multi-user: allows large-scale and remotely managed annotation projects. It incor-
porates project management functions and sophisticated progress monitoring capabilities.
? Three different MT assessment tasks in an integrated environment: annotation of translation errors,
translation quality rating (e.g. adequacy, fluency, relative ranking), and word alignment. An inte-
grated environment offering different tasks can address the needs of a higher number of potential
users within the MT field.
? Highly configurable tasks: possibility to evaluate a single MT output as well as two or more au-
tomatic translations in parallel, which is useful if the purpose of the annotation is to compare MT
systems. Furthermore, all tasks can be adapted to specific annotation needs (see Section 2).
? Fast and well-designed annotation interfaces: particular attention was paid to the usability of the
interfaces, especially for the error annotation task where a lot of annotations, often overlapping and
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
120
covering long sequences of words, have to be made. A fast and easy-to-use interface can reduce the
problems related to manual evaluation and ensure annotation speed and data quality.
? Open source: released under Apache 2.0 license at http://github.com/hltfbk/mt-equal .
We think that these features give to MT-EQuAl an added value with respect to other existing annotation
tools which only partially fulfill the requirements illustrated above.
Over the years, various annotation tools with different characteristics have been made available for
the assessment tasks offered by our toolkit. However, none of them incorporates all the features of MT-
EQuAl: either the integration in a multi-task platform, or a web-based interface, or the implementation
of the error annotation task which is the most needed to support the upcoming research. The most com-
parable tools to MT-EQuAl are PET (Aziz et al., 2012), COSTA (Chatzitheodorou and Chatzistamatis,
2013), TAUS DQF framework,
1
translate5,
2
Blast (Stymne, 2011), and Appraise (Federmann, 2012),
since they all implement translation error annotation. These tools were created for different purposes and
differ in various ways among each other and with respect to MT-EQuAl. All of them except Appraise
do not support multiple MT outputs, and PET, COSTA, and Blast are stand-alone tools. From the error
analysis point of view, their interfaces show different levels of flexibility. PET and COSTA permit only
sentence-level annotation, which is not the suitable granularity for that kind of information. Appraise
offers word-level annotation but displays the MT output word by word, which does not facilitate the
annotator in getting a global view of the sentence and of the errors. Finally, the translate5 and Blast in-
terfaces show the whole MT output and allow the annotator to mark the specific portion(s) of text where
an error occurs. This type of annotation is the same implemented in MT-EQuAl. However, with respect
to these tools, MT-EQuAl represents a step further as one of our main design goals was usability. The
MT-EQuAl error analysis interface is simple and very intuitive, and offers visualization functions aimed
at reducing annotators? cognitive load, so to enable them to focus on the task itself (see Section 2.2).
2 System Overview
MT-EQuAl is a web-based application implemented using PHP and JavaScript. It takes as input several
UTF-8 encoded csv files: the source text, the reference translation (optional), and one file for each of the
MT outputs to be evaluated. This allows the evaluation of single systems as well as the comparison of
multiple systems. Each row in the input csv files contains one evaluation item, typically one sentence.
In order to annotate translation errors, the sentences must be tokenized. To this purpose, the tool ac-
cepts input files already tokenized by the user or - if needed - it applies a simple tokenization based on
spaces, punctuation, and other language-dependent rules (e.g. a character-based tokenization is applied
to Chinese texts). The source and target languages must be declared in the csv, so that the tool can ap-
ply the most suitable text tokenization and visualisation (e.g. the text can be displayed left to right and
viceversa). The annotated data can be exported both in csv and XML format.
As regards data storage, all recorded information is permanently stored in a MySQL database. An
interesting feature is that immediate persistence of data is achieved without an explicit action by the
user to save the data, since every annotation is immediately sent to the server and stored in the database.
Finally - being a web-based application - if the server encounters some problems, annotation is blocked
and the user is notified with a warning message.
The MT-EQuAl front-end is composed of a project management interface and three annotation inter-
faces, one for each evaluation task.
2.1 Project Management Interface
The various project management functions implemented in the tool are accessible to the project manager
through an interface which is composed of four tabs:
? Task. In this tab the project manager creates the task and sets its specific features. For the error
analysis task, a default error typology - based on (Vilar et al., 2006) - is available, but any alternative
1
https://evaluation.taus.net/tools
2
http://www.translate5.net
121
tagset can be adopted. In the rating task it is possible to decide the number of points in the rating
scale, while in the alignment task the number of alignment types (e.g. sure, possible) can be set.
? Data. In this tab the project manager can import the input files and apply the tokenization module
if desired. Moreover, a table summarizing the data stored in the database for each task is displayed.
? Users. In this tab the project manager can create accounts for users and assign them to different
tasks. Each user will see only the task(s) s/he has been assigned to. Users do not see other users?
annotations unless they are working in ?revision mode?, where an existing annotation is presented
for revision.
? Annotation. This tab contains the progress monitoring and export functions. As regards progress
monitoring, a report containing real-time information about the progress of the annotation is dis-
played both at the task level and the user level. Moreover, the project manager can monitor user
activity through the visualization of the remote client interface in read-only mode. This feature is
particularly useful as it addresses the typical problems related to training and supervision of remote
annotators. Regarding annotation export, data can be exported (i) for all the tasks, (ii) for each
single task, and (iii) for each user. Furthermore, the annotations carried out by a user can be directly
copied into another user account for revision.
2.2 Error Annotation Interface
The error annotation interface requires the annotator to identify the type of errors present in the MT out-
put, according to the adopted error typology, and to mark their position in the text. As shown in Figure
1, the annotator is presented with the source sentence, a reference translation (optional) and the MT out-
put(s) to be analyzed. Two buttons allow the annotator to mark the MT output as containing ?no errors?
or ?too many errors?. In order to annotate the errors, the annotator selects with the mouse the word(s)
to be annotated. The selected word(s) are highlighted and, by right-clicking, the error typology menu is
displayed and the suitable error type can be chosen. It is possible to annotate single words (including
punctuation), spaces (e.g. to indicate the correct place for missing words in the candidate translation),
and sequences of words (very useful especially for reordering problems which can involve entire portions
of the sentence). The annotated errors are listed at the right of the corresponding sentences, subdivided
by error type. If the mouse hovers over a given error instance, the corresponding word(s) appear under-
lined in the text. It is possible to delete single error instances (by clicking on the bin icon) or all the errors
of a give type (by clicking on the ?reset? button).
Figure 1: Error annotation interface configured for two MT outputs and with the default error typology.
2.3 Translation Quality Rating Interface
As shown in Figure 2(a), the quality rating interface displays the source sentence, a reference trans-
lation (optional) and the MT output(s) to be rated. When the assessor clicks on a point in the scale,
the annotation is automatically saved and the point is highlighted in red. By clicking on the button
?Done?, the assessor confirms that the evaluation item has been completed. This layout is suitable for
adequacy/fluency evaluation, ranking outputs relatively to each other, and in general all those assessment
tasks that require rating MT outputs.
122
(a) Quality Rating (b) Word Alignment
Figure 2: (a) Quality Rating interface with 3 systems and a 5-point scale (b) Word Alignment interface.
2.4 Word Alignment Annotation Interface
The word alignment interface displays a traditional alignment matrix, where the rows correspond to the
words of the sentence in one language and the columns to the words of its translation. Word alignments
can be edited by clicking the respective matrix cells to add or remove links between words. The interface
is designed to allow the alignment of discontinuous text segments. Figure 2(b) shows an alignment
example where light grey, dark grey, and black cells respectively represent unlinked words, possible and
sure alignments.
3 Applications of MT-EQuAl and Forthcoming Extensions
MT-EQuAl is currently being used by professional translators on English to Italian data to assess the
performance of the alignment models and annotate translation errors of the MT systems developed within
the MateCat project.
3
MT-EQuAl was also extensively used within an industrial project for the evaluation
of commercial MT systems. To this purpose, professional translators performed error annotation and
quality rating on data for three different language pairs (English to Arabic/Chinese/Russian). MT-EQuAl
is being actively developed on the basis of the feedback and requirements collected from its users. We
are also currently implementing the automatic computation of Inter-Annotator Agreement scores, as an
additional feature to further improve the toolkit.
Acknowledgments
This work has been partially supported by the EC-funded project MateCat (ICT-2011.4.2-287688).
References
Wilker Aziz, Sheila Castilho Monteiro de Sousa, and Lucia Specia. 2012. PET: a tool for post-editing and
assessing machine translation. In Proceedings of the Eight International Conference on Language Resources
and Evaluation (LREC?12), Istanbul, Turkey, May. European Language Resources Association (ELRA).
Konstantinos Chatzitheodorou and Stamatis Chatzistamatis. 2013. COSTA MT Evaluation Tool: An Open Toolkit
for Human Machine Translation Evaluation. Prague Bulletin of Mathematical Linguistics, pages 83?89.
Christian Federmann. 2012. Appraise: an open-source toolkit for manual evaluation of mt output. Prague Bulletin
of Mathematical Linguistics, pages 25?35.
Sara Stymne. 2011. Blast: A tool for error analysis of machine translation output. In Proceedings of the ACL-HLT
2011 System Demonstrations, pages 56?61, Portland, Oregon, June. Association for Computational Linguistics.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Hermann Ney. 2006. Error Analysis of Machine Translation Out-
put. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC?06),
pages 697?702, Genoa, Italy, May. European Language Resources Association (ELRA).
3
www.matecat.com
123
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 129?132, Dublin, Ireland, August 23-29 2014.
THE MATECAT TOOL
M. Federico and N. Bertoldi and M. Cettolo and M. Negri and M. Turchi
Fondazione Bruno Kessler, Trento (Italy)
M. Trombetti and A. Cattelan and A. Farina and
D. Lupinetti and A. Martines and A. Massidda
Translated Srl, Roma (Italy)
H. Schwenk and L. Barrault and F. Blain
Universit?e du Maine, Le Mans (France)
P. Koehn and C. Buck and U. Germann
The University of Edinburgh (United Kingdom)
www.matecat.com
Abstract
We present a new web-based CAT tool providing translators with a professional work environ-
ment, integrating translation memories, terminology bases, concordancers, and machine transla-
tion. The tool is completely developed as open source software and has been already successfully
deployed for business, research and education. The MateCat Tool represents today probably the
best available open source platform for investigating, integrating, and evaluating under realistic
conditions the impact of new machine translation technology on human post-editing.
1 Introduction
The objective of MateCat
1
is to improve the integration of machine translation (MT) and human transla-
tion within the so-called computer aided translation (CAT) framework. CAT tools represent nowadays the
dominant technology in the translation industry. They provide translators with text editors that can man-
age several document formats and suitably arrange their content into text segments ready to be translated.
Most importantly, CAT tools provide access to translation memories (TMs), terminology databases, con-
cordance tools and, more recently, to machine translation (MT) engines. A TM is basically a repository
of translated segments. During translation, the CAT tool queries the TM to search for exact or fuzzy
matches of the current source segment. These matches are proposed to the user as translation sugges-
tions. Once a segment is translated, its source and target texts are added to the TM for future queries. The
integration of suggestions from an MT engine as a complement to TM matches is motivated by recent
studies (Federico et al., 2012; Green et al., 2013; L?aubli et al., 2013), which have shown that post-editing
MT suggestions can substantially improve the productivity of professional translators. MateCat lever-
ages the growing interest and expectations in statistical MT by advancing the state-of-the-art along three
directions:
? Self-tuning MT, i.e. methods to train statistical MT for specific domains or translation projects;
? User adaptive MT, i.e. methods to quickly adapt statistical MT from user corrections and feedback;
? Informative MT, i.e. supply more information to enhance users? productivity and work experience.
Research along these three directions has converged into a new generation CAT software, which is
both an enterprise level translation workbench (currently used by several hundreds of professional trans-
lators) as well as an advanced research platform for integrating new MT functions, running post-editing
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/.
1
MateCat, acronym of Machine Translation Enhanced Computer Assisted Translation, is a 3-year research project (11/2011-
10/2014) funded by the European Commission under FP7 (grant agreement no 287688). The project consortium is led by FBK
(Trento, Italy) and includes the University of Edinburgh (United Kingdom), Universit?e du Maine (Le Mans, France), and
Translated Srl (Rome, Italy).
129
Figure 1: The MateCat Tool editing page.
experiments and measuring user productivity. The MateCat Tool, which is distributed under the LGPL
open source license, combines features of the most advanced systems (either commercial, like the pop-
ular SDL Trados Workbench,
2
or free like OmegaT
3
) with new functionalities. These include: i) an
advanced API for the Moses Toolkit,
4
customizable to languages and domains, ii) ease of use through a
clean and intuitive web interface that enables the collaboration of multiple users on the same project, iii)
concordancers, terminology databases and support for customizable quality estimation components and
iv) advanced logging functionalities.
2 The MateCat Tool in a Nutshell
Overview. The MateCat Tool runs as a web-server accessible through Chrome, Firefox and Safari. The
CAT web-server connects with other services via open APIs: the TM server MyMemory
5
, the commer-
cial Google Translate (GT) MT server, and a list of Moses-based servers specified in a configuration file.
While MyMemory?s and GT?s servers are always running and available, customized Moses servers have
to be first installed and set-up. Communication with the Moses servers extends the GT API in order to
support self-tuning, user-adaptive and informative MT functions. The natively supported document for-
mat of MateCat Tool is XLIFF,
6
although its configuration file makes it possible to specify external file
converters. The tool supports Unicode (UTF-8) encoding, including non latin alphabets and right-to-left
languages, and handles texts embedding mark-up tags.
How it works. The tool is intended both for individual translators or managers of translation projects
involving one or more translators. A translation project starts by uploading one or more documents and
specifying the desired translation direction. Then the user can optionally select a MT engine from an
available list and/or a new or existing private TM in MyMemory, by specifying its private key. Notice
that the public MyMemory TM and the GT MT services are assumed by default. The following step is
the volume analysis of the document, which reports statistics about the words to be actually translated
based on the coverage provided by the TM. At this stage, long documents can be also split into smaller
portions to be for instance assigned to different translators or translated at different times. The following
step starts the actual translation process by opening the editing window. All source segments of the
2
http://www.translationzone.com/
3
http://www.omegat.org/
4
http://www.statmt.org/moses/
5
http://mymemory.translated.net
6
http://docs.oasis-open.org/xliff/v1.2/os/xliff-core.html
130
document and their corresponding target segments are arranged side-by-side on the screen. By selecting
one segment, an editing pane opens (Figure 1) including an editable field that is initialized with the best
available suggestion or with the last post-edit. Translation hints are shown right below together with
their origin (MT or TM). Their ranking is based on the TM match score or the MT confidence score. MT
hints with no confidence score are assigned a default score. Tag consistency is automatically checked
during translation and warnings are possibly shown in the editing window. An interesting feature of the
MateCat Tool is that each translation project is uniquely identified by its URL page which also includes
the currently edited segment. This permits for instance more users to simultaneously access and work on
the same project. Moreover, to support simultaneous team work on the same project, translators can mark
the status (draft, translated, approved, rejected) of each segment with a corresponding color (see Figure
1, right blue bar). The user interface is enriched with search and replace functions, a progress report at
the bottom of the page, and several shortcut commands for the skilled users. Finally, the tool embeds a
concordance tool to search for terms in the TM, and a glossary where each user can upload, query and
update her terminology base. Users with a Google account can access a project management page which
permits then to manage all their projects, including storage, deletion, and access to the editing page.
MT support. The tool supports Moses-based servers able to provide an enhanced CAT-MT commu-
nication. In particular, the GT API is augmented with feedback information provided to the MT engine
every time a segment is post-edited as well as enriched MT output, including confidence scores, word
lattices, etc. The developed MT server supports multi-threading to serve multiple translators, properly
handles text segments including tags, and instantly adapts from the post-edits performed by each user
(Bertoldi et al., 2013).
Edit Log. During post-editing the tool collects timing information for each segment, which is updated
every time the segment is opened and closed. Moreover, for each segment, information is collected about
the generated suggestions and the one that has actually been post-edited. This information is accessible at
any time through a link in the Editing Page, named Editing Log. The Editing Log page (Figure 2) shows
a summary of the overall editing performed so far on the project, such as the average translation speed
and post-editing effort and the percentage of top suggestions coming from MT or the TM. Moreover,
for each segment, sorted from the slowest to the fastest in terms of translation speed, detailed statistics
about the performed edit operations are reported. This information, with even more details, can be also
downloaded as a CSV file to perform a more detailed post-editing analysis. While the information shown
in the Edit Log page is very useful to monitor progress of a translation project in real time, the CSV file
is a fundamental source of information for detailed productivity analyses once the project is ended.
3 Applications.
The MateCat Tool has been exploited by the MateCat project to investigate new MT functions (Bertoldi
et al., 2013; Cettolo et al., 2013; Turchi et al., 2013; Turchi et al., 2014) and to evaluate them in a real
professional setting, in which translators have at disposal all the sources of information they are used
to work with. Moreover, taking advantage of its flexibility and ease of use, the tool has been recently
exploited for data collection and education purposes (a course on CAT technology for students in trans-
lation studies). An initial version of the tool has also been leveraged by the Casmacat project
7
to create
a workbench (Alabau et al., 2013), particularly suitable for investigating advanced interaction modalities
such as interactive MT, eye tracking, and handwritten input. Currently the tool is employed by Trans-
lated for their internal translation projects and is being tested by several international companies, both
language service providers and IT companies. This has made possible to collect continuous feedback
from hundreds of translators, which besides helping us to improve the robustness of the tool is also
influencing the way new MT functions will be integrated to supply the best help to the final user.
7
http://www.casmacat.eu
131
Figure 2: The MateCat Tool edit log page.
References
Vicent Alabau, Ragnar Bonk, Christian Buck, Michael Carl, Francisco Casacuberta, Mercedes Garca-Mart??nez,
Jes?us Gonz?alez, Philipp Koehn, Luis Leiva, Bartolom?e Mesa-Lao, Daniel Oriz, Herv?e Saint-Amand, Germ?an
Sanchis, and Chara Tsiukala. 2013. Advanced computer aided translation with a web-based workbench. In
Proceedings of Workshop on Post-editing Technology and Practice, pages 55?62.
Nicola Bertoldi, Mauro Cettolo, and Marcello Federico. 2013. Cache-based Online Adaptation for Machine
Translation Enhanced Computer Assisted Translation. In Proceedings of the MT Summit XIV, pages 35?42,
Nice, France, September.
Mauro Cettolo, Christophe Servan, Nicola Bertoldi, Marcello Federico, Lo??c Barrault, and Holger Schwenk. 2013.
Issues in Incremental Adaptation of Statistical MT from Human Post-edits. In Proceedings of the MT Summit
XIV Workshop on Post-editing Technology and Practice (WPTP-2), pages 111?118, Nice, France, September.
Marcello Federico, Alessandro Cattelan, and Marco Trombetti. 2012. Measuring user productivity in machine
translation enhanced computer assisted translation. In Proceedings of the Tenth Conference of the Association
for Machine Translation in the Americas (AMTA).
Spence Green, Jeffrey Heer, and Christopher D Manning. 2013. The efficacy of human post-editing for language
translation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 439?
448. ACM.
Samuel L?aubli, Mark Fishel, Gary Massey, Maureen Ehrensberger-Dow, and Martin Volk. 2013. Assessing Post-
Editing Efficiency in a Realistic Translation Environment. In Michel Simard Sharon O?Brien and Lucia Specia
(eds.), editors, Proceedings of MT Summit XIVWorkshop on Post-editing Technology and Practice, pages 83?91,
Nice, France.
Marco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the subjectivity of human judgements
in MT quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages
240?251, Sofia, Bulgaria, August. Association for Computational Linguistics.
Marco Turchi, Antonios Anastasopoulos, Jos?e G.C. de Souza, and Matteo Negri. 2014. Adaptive Quality Estima-
tion for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics (ACL ?14). Association for Computational Linguistics.
132
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1643?1653,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Assessing the Impact of Translation Errors
on Machine Translation Quality with Mixed-effects Models
Marcello Federico, Matteo Negri, Luisa Bentivogli, Marco Turchi
FBK - Fondazione Bruno Kessler
Via Sommarive 18, 38123 Trento, Italy
{federico,negri,bentivogli,turchi}@fbk.eu
Abstract
Learning from errors is a crucial aspect of
improving expertise. Based on this no-
tion, we discuss a robust statistical frame-
work for analysing the impact of different
error types on machine translation (MT)
output quality. Our approach is based on
linear mixed-effects models, which allow
the analysis of error-annotated MT out-
put taking into account the variability in-
herent to the specific experimental setting
from which the empirical observations are
drawn. Our experiments are carried out
on different language pairs involving Chi-
nese, Arabic and Russian as target lan-
guages. Interesting findings are reported,
concerning the impact of different error
types both at the level of human perception
of quality and with respect to performance
results measured with automatic metrics.
1 Introduction
The dominant statistical approach to machine
translation (MT) is based on learning from large
amounts of parallel data and tuning the result-
ing models on reference-based metrics that can
be computed automatically, such as BLEU (Pap-
ineni et al., 2001), METEOR (Banerjee and Lavie,
2005), TER (Snover et al., 2006), GTM (Turian
et al., 2003). Despite the steady progress in the
last two decades, especially for few well resourced
translation directions having English as target lan-
guage, this way to approach the problem is quickly
reaching a performance plateau. One reason is
that parallel data are a source of reliable informa-
tion but, alone, limit systems knowledge to ob-
served positive examples (i.e. how a sentence
should be translated) without explicitly modelling
any notion of error (i.e. how a sentence should
not be translated). Another reason is that, as a
development and evaluation criterion, automatic
metrics provide a holistic view of systems? be-
haviour without identifying the specific issues of a
translation. Indeed, the global scores returned by
MT evaluation metrics depend on comparisons be-
tween translation hypotheses and reference trans-
lations, where the causes and the nature of the dif-
ferences between them are not identified.
To cope with these issues and define system
improvement priorities, the focus of MT evalua-
tion research is gradually shifting towards profil-
ing systems? behaviour with respect to various ty-
pologies of errors (Vilar et al., 2006; Popovi?c and
Ney, 2011; Farr?us et al., 2012, inter alia). This
shift has enriched the traditional MT evaluation
framework with a new element, that is the actual
errors done by a system. Until now, most of the
research has focused on the relationship (i.e. the
correlation) between two elements of the frame-
work: humans and automatic evaluation metrics.
As a new element of the framework, which be-
comes a sort of ?evaluation triangle?, the analy-
sis of error annotations opens interesting research
problems related to the relationships between: i)
error types and human perception of MT quality
and ii) error types and the sensitivity of automatic
metrics.
Besides motivating further investigation on met-
rics featuring high correlation with human judge-
ments (a well-established MT research sub-field,
which is out of the scope of this paper), connecting
the vertices of this triangle raises new challenging
questions such as:
(1) Which types of MT errors have the high-
est impact on human perception of translation
quality? Surprisingly, little prior work focused
on this side of the triangle. Error annotations
have been considered to highlight strengths and
weaknesses of MT engines or to investigate the
influence of different error types on post-editors?
work. However, the direct connection between er-
1643
rors and users? preferences has been only partially
understood, mainly from a descriptive standpoint
and through rudimentary techniques unsuitable to
draw clear-cut conclusions or reliable inferences.
(2) To which types of errors are different MT
evaluation metrics more sensitive? This side of
the triangle has been even less explored. For in-
stance, little has been done to understand which
automatic metric is more suitable to assess sys-
tem improvements with respect to a specific issue
(e.g. word order or morphology) or to shed light
on the joint impact of different error types on per-
formance results calculated with different metrics.
To answer these questions, we propose a ro-
bust statistical framework to analyse the im-
pact of different error types, alone and in com-
bination, both on human perception of quality and
on MT evaluation metrics? results. Our analysis
is carried out by employing linear mixed-effects
models, a generalization of linear regression mod-
els suited to model responses with fixed and ran-
dom effects. Experiments are performed on data
covering three translation directions (English to
Chinese, Arabic and Russian). For each direc-
tion, two automatic translations were collected for
around 400 sentences and were manually evalu-
ated by expert translators through absolute quality
judgements and error annotation.
Building on the advantages offered by linear
mixed-effects models, our main contributions in-
clude:
? A rigorous method, novel to MT error anal-
ysis research, to relate MT issues to human
preferences and MT metrics? results;
? The application of such method to three
translation directions having English as
source and different languages as target;
? A number of findings, specific to each lan-
guage direction, which are out of the reach of
the few simpler methods proposed so far.
Overall, our study has clear practical implica-
tions for MT systems? development and evalu-
ation. Indeed, the proposed statistical analysis
framework represents an ideal instrument to: i)
identify translation issues having the highest im-
pact on human perception of quality and ii) choose
the most appropriate evaluation metric to measure
progress towards their solution.
2 Related Work
Error analysis, as a way to identify systems? weak-
nesses and define priorities for their improvement,
is gaining increasing interest in the MT com-
munity (Popovi?c and Ney, 2011; Popovic et al.,
2013). Along this direction, the initial efforts to
develop error taxonomies covering different levels
of granularity (Flanagan, 1994; Vilar et al., 2006;
Farr?us Cabeceran et al., 2010; Stymne and Ahren-
berg, 2012; Lommel et al., 2014) have been re-
cently complemented by investigations on how to
exploit error annotations for diagnostic purposes.
Error annotations of sentences produced by differ-
ent MT systems, in different target languages and
domains, have been used to determine the qual-
ity of translations according to the amount of er-
rors encountered (Popovic et al., 2013), to design
new automatic metrics that take into considera-
tion human annotations (Popovic, 2012; Bojar et
al., 2013), and to train classifiers that can auto-
matic identify fine-grained errors in the MT output
(Popovi?c and Ney, 2011). The impact of edit op-
erations on post-editors? productivity, which im-
plicitly connects the severity of different errors to
human activity, has also been studied (Temnikova,
2010; O?Brien, 2011; Blain et al., 2011), but
few attempts have been made to explicitly model
how fine-grained errors impact on human quality
judgements and automatic metrics.
Recently, the relation between different error
types, their frequency, and human quality judge-
ments has been investigated from a descriptive
standpoint in (Lommel et al., 2014; Popovi?c et al.,
2014). In both works, however, the underlying as-
sumption that the most frequent error has also the
largest impact on quality perception is not verified
(in general and, least of all, across language pairs,
domains, MT systems and post-editors). Another
limitation of the proposed (univariate) analysis lies
in the fact that it exclusively focuses on error types
taken in isolation. This simplification excludes the
possibility that humans, when assigning a global
quality score to a translation, may be influenced
not only by the error types but also by their inter-
action. The implications of such possibility call
for a multivariate analysis capable to model also
error interactions.
In (Kirchhoff et al., 2013), a statistically-
grounded approach based on conjoint analysis has
been used to investigate users? reactions to dif-
ferent types of translation errors. According to
1644
their results, word order is the most dispreferred
error type, and the count of the errors in a sen-
tence is not a good predictor of users? prefer-
ences. Though more sophisticated than methods
based on rough error counts, the conjoint model
is bound to several constraints that limit its us-
ability. In particular, the application of conjoint
analysis in this context requires to: i) operate with
semi-automatically created (hence artificial) data
instead of real MT output, ii) manually define dif-
ferent levels of severity for each error type (e.g.
high/medium/low), and iii) limit the number of er-
ror types considered to avoid the explosion of all
possible combinations. Finally, the conjoint anal-
ysis framework is not able to explicitly model vari-
ance in the translated sentences, the human anno-
tators, and the SMT systems used to translate the
source sentences. Our claim is that avoiding any
possible bias introduced by these factors should be
a priority in the analysis of empirical observations
in a given experimental setting.
So far, the relation between errors and auto-
matic metrics has been analysed by measuring the
correlation between single or total error frequen-
cies and automatic scores (Popovi?c and Ney, 2011;
Farr?us et al., 2012). Using two different error tax-
onomies, both works show that the sum of the er-
rors has a high correlation with BLEU and TER
scores. Similar to the aforementioned works ad-
dressing the impact of MT errors on human per-
ception, these studies disregard error interactions,
and their possible impact on automatic scores.
To overcome these issues, we propose a ro-
bust statistic analysis framework based on mixed-
effects models, which have been successfully ap-
plied to several NLP problems such as sentiment
analysis (Greene and Resnik, 2009), automatic
speech recognition (Goldwater et al., 2010), and
spoken language translation (Ruiz and Federico,
2014). Despite their effectiveness, the use of
mixed-effects models in the MT field is rather re-
cent and limited to the analysis of human post-
editions (Green et al., 2013; L?aubli et al., 2013).
In both studies, the goal was to evaluate the im-
pact of post-editing on the quality and productivity
of human translation assuming an ANOVA mixed
model for a between-subject design, in which hu-
man translators either post-edited or translated the
same texts. Our scenario is rather different as we
employ mixed models to measure the influence of
different MT error types - expressed as continu-
ous fixed effects - on quality judgements and auto-
matic quality metrics. Mixed models, having the
capability to absorb random variability due to the
specific experimental set-up, provide a robust mul-
tivariate method to efficiently analyse the impor-
tance of error types.
Finally, differently from all previous works, our
analysis is run on language pairs having English
as source and languages distant from English (in
term of morphology and word-order) as target.
3 Mixed-effects Models
Mixed-effects models - or simply mixed models
- like any regression model, express the relation-
ship between a response variable and some co-
variates and/or contrast factors. They enhance
conventional models by complementing fixed ef-
fects with so-called random effects. Random ef-
fects are introduced to absorb random variability
inherent to the specific experimental setting from
which the observations are drawn. In general, ran-
dom effects correspond to covariates that are not -
or cannot be - exhaustively observed in an experi-
ment, e.g. the human annotators and the evaluated
systems. Hence, mixed models permit to elegantly
cope with experimental design aspects that hinder
the applicability of conventional regression mod-
els. These are, in particular, the use of repeated
and/or clustered observations that introduce corre-
lations in the response variable that clearly violate
the independence and homoscedasticity assump-
tions of conventional linear, ANOVA, and logis-
tic regression models. Significance testing with
mixed models is in general more powerful, i.e. less
prone to Type II Errors, and also permits to reduce
the chance of Type I Errors in within-subject de-
signs, which are prone to the ?fallacy of language-
as-a-fixed-effect? (Clark, 1973).
Random effects can be directly associated to
the regression model parameters, as random in-
tercepts and random slopes, and have the same
form of the generic error component of the model,
i.e. normally distributed with zero mean and un-
known variance. As random effects introduce hid-
den variables, mixed models are trained with Ex-
pectation Maximization, while significance testing
is performed via likelihood-ratio (LR) tests.
In this work we employ mixed linear models to
measure the influence of different MT error types,
expressed as continuous fixed effects, on quality
1645
judgements or on automatic quality metrics.
1
We illustrate mixed linear models (Baayen et
al., 2008) by referring to our analysis, which ad-
dresses the relationships between a quality metric
(y) and different types of errors (e.g. A, B, and
C)
2
observed at the sentence level. For the sake of
simplicity, we assume to have balanced repeated
observations for one single crossed effect. That is,
we have i ? {1, . . . , I} MT systems (our groups)
each of which translated the same j ? {1, . . . , J}
test sentences. Our response variable y
ij
- a nu-
meric quality score - is computed on each (sen-
tence, system) pair, and we aim to investigate its
relationship with error statistics available for each
MT output, namely A
ij
, B
ij
and C
ij
. A (possible)
linear mixed model for our study would be:
y
ij
= ?
0
+ ?
1
A
ij
+ ?
2
B
ij
+ ?
3
C
ij
+ (1)
b
0,i
+ b
1,i
A
ij
+ b
2,i
B
ij
+ b
3,i
C
i
+ 
ij
The model is split into two lines on purpose. The
first line shows the fixed effect component, that is
intercept (?
0
) and slopes (?
1
, ?
2
, ?
3
) for each error
type. The second line specifies the random struc-
ture of the model, which includes random inter-
cept and slopes for each MT system and the resid-
ual error. Borrowing the notation from (Green
et al., 2013), we conveniently rewrite (1) in the
group-wise arranged matrix notation:
y
i
= x
T
i
? + z
T
i
b
i
+ 
i
(2)
where y
i
is the J ? 1 vector of responses, x
i
is the
J?p design matrix of covariates (including the in-
tercept) with fixed coefficients ? ? R
p?1
, z is the
random structure matrix defined by J ? q covari-
ates with random coefficients b
i
? R
q?1
, and 
i
is
the vector of residuals (in our example, p = 4 and
q = 4). By packing together vectors and matrices
indexed over groups i, we can rewrite the model
in a general form (Baayen et al., 2008), which can
represent any possible crossed-effects and random
structures defined over them allowing, at the same
time, for a compact model specification:
y = X
T
? + Z
T
b+  (3)
 ? N (0, ?
2
I), b ? N (0, ?
2
?), b ? 
1
Although mixed ordinal models (Tutz and Hennevogl,
1996) are in principle more appropriate to target quality
judgements, in our preliminary investigations mixed linear
models showed a significantly higher predictive power.
2
Here, A, B and C represent three generic error classes.
Their actual number in a given experimental setting will de-
pend on the granularity of the reference error taxonomy.
where ? is the relative variance-covariance q ? q
matrix of the random effects (now q = 4I), ?
2
is the variance of the per-observation term , the
symbol ? denotes independence of random vari-
ables, andN indicates the multivariate normal dis-
tribution. While b, ?, and ? are estimated via max-
imum likelihood, the single random intercept and
slope values for each group are calculated subse-
quently. They are referred to as Best Linear Un-
biased Predictors (BLUPS) and, formally, are not
parameters of the model.
The significance of the contribution of each sin-
gle parameter (e.g. single entries of ?) to the
goodness of fit can be tested via likelihood ratio.
In this way, both the fixed and random effect struc-
ture of the model can be investigated with respect
to its actual necessity to the model.
4 Dataset
For our analysis we used a dataset that covers
three translation directions, corresponding to En-
glish to Chinese, Arabic, and Russian. An inter-
national organization provided us a set of English
sentences together with their translation produced
by two anonymous MT systems. For each evalu-
ation item (source sentence and two MT outputs)
three experts were asked to assign quality scores to
the MT outputs, and a fourth expert was asked to
annotate translation errors. The four experts, who
were all professional translators native in the ex-
amined target languages, were carefully trained to
get acquainted with the evaluation guidelines and
the annotation tool specifically developed for these
evaluation tasks (Girardi et al., 2014). The anno-
tation process was carried out in parallel by all an-
notators over one week, resulting in a final dataset
composed of 312 evaluation items for the ENZH
direction, 393 for ENAR, and 437 for ENRU.
4.1 Quality Judgements
Quality judgements were collected by asking the
three experts to rate each automatic translation
according to a 1-5 Likert scale, where 1 means
?incomprehensible translation? and 5 means ?per-
fect translation?. The distribution of the collected
annotations with respect to each quality score is
shown in Figure 1. As we can see, this distri-
bution reflects different levels of perceived qual-
ity across languages. ENZH, for instance, has the
highest number of low quality scores (1 and 2),
while ENRU has the highest number of high qual-
1646
0%	 ?
20%	 ?
40%	 ?
60%	 ?
80%	 ?
100%	 ?
ENZH	 ? ENAR	 ? ENRU	 ?
5	 ?
4	 ?
3	 ?
2	 ?
1	 ?
Figure 1: Distribution of quality scores.
ity scores (4 and 5).
Table 1 shows the average of all the qual-
ity scores assigned by each annototator as well
as the average score obtained for each MT sys-
tem. These values demonstrate the variability
of annotators and systems. A particularly high
variability among human judges is observed for
the ENAR language direction (also reflected by
the inter-annotator agreement scores discussed be-
low), while ENZH shows the highest variability
between systems. As we will see in ?5.1, we suc-
cessfully cope with this variability by considering
systems and annotators as random effects, which
allow the regression models to abstract from these
differences.
Ann1 Ann2 Ann3 Sys1 Sys2
ENZH 2.38 2.69 2.21 2.29 2.56
ENAR 2.76 2.77 1.84 2.39 2.53
ENRU 2.82 2.72 2.96 2.87 2.79
Table 1: Average quality scores per annotator and
per system.
Inter-annotator agreement was computed using
the Fleiss? kappa coefficient (Fleiss, 1971), and re-
sulted in 22.70% for ENZH, 5.24% for ENAR, and
21.80% for ENRU. While for ENZH and ENRU
the results fall in the range of ?fair? agreement
(Landis and Koch, 1977), for ENAR only ?slight?
agreement is reached, reflecting the higher anno-
tators? variability evidenced in Table 1.
A more fine-grained agreement analysis is pre-
sented in Figure 2, where the kappa values are
given for each score class. In general we no-
tice a lower agreement on the intermediate quality
scores, while annotators tend to agree on very bad
and, even more, on good translations. In partic-
ular, we see that the agreement for ENAR is sys-
tematically lower than the values measured for the
other languages on all the score classes.
-??0.1	 ?
0	 ?
0.1	 ?
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
FLE
ISS
'	 ?KA
PPA
	 ?
QUALITY	 ?SCORES	 ?
ENZH	 ?
ENAR	 ?
ENRU	 ?
Figure 2: Class specific inter-annotator agreement.
4.2 Error Annotation
This evaluation task was carried out by one ex-
pert for each language direction, who was asked to
identify the type of errors present in the MT output
and to mark their position in the text. Since the fo-
cus of our work is the analysis method rather than
the definition of an ideal error taxonomy, for the
difficult language directions addressed we opted
for the following general error classes, partially
overlapping with (Vilar et al., 2006): i) reordering
errors, ii) lexicon errors (including wrong lexical
choices and extra words), iii) missing words, iv)
morphology errors.
Figure 3 shows the distribution of the errors in
terms of affected tokens (words) for each error
type. Since token counts for Chinese are not word-
based but character-based, for readability purposes
the number of errors counted for Chinese trans-
lations have been divided by 2.5. Note also that
morphological errors annotated for ENZH involve
only 13 characters and thus are not visible in the
plot. The total number of errors amounts to 16,320
characters for ENZH, 4,926 words for ENAR, and
5,965 words for ENRU.
This distribution highlights some differences
between languages directions. For example, trans-
lations into Arabic and Russian present several
morphology errors, while word reordering is the
most frequent issue for translations into Chinese.
As we will see in ?5.1, error frequency does not
give a direct indication of their impact on trasla-
tion quality judgements.
4.3 Automatic Metrics
In our investigation we consider three popular au-
tomatic metrics: sentence-level BLEU (Lin and
Och, 2004), TER (Snover et al., 2006), and GTM
(Turian et al., 2003). We compute all automatic
scores by relying on a single reference and by
1647
0	 ?
500	 ?
1000	 ?
1500	 ?
2000	 ?
2500	 ?
3000	 ?
3500	 ?
4000	 ?
ENZH	 ? ENAR	 ? ENRU	 ?
LEX	 ?
MISS	 ?
MORPH	 ?
REO	 ?
Figure 3: Distribution of error types.
means of standard packages. In particular, auto-
matic scores on Chinese are computed at the char-
acter level. Moreover, as we use metrics as re-
sponse variables for our regression models, we
compute all metrics at the sentence level. The
overall mean scores for all systems and languages
are reported in Table 2. Differences in systems?
performance can be observed for all language
pairs; as we will observe in ?5.2 such variability
explains the effectiveness of considering the MT
systems as a random effect.
BLEU TER GTM
Sys1 Sys2 Sys1 Sys2 Sy1 Sys2
ENZH 27.95 44.11 64.52 48.13 62.15 72.30
ENAR 19.63 25.25 68.83 63.99 47.20 52.33
ENRU 27.10 31.07 60.89 54.41 53.74 56.41
Table 2: Overall automatic scores per system.
5 Experiments
To assess the impact of translation errors on MT
quality we perform two sets of experiments. The
first set (?5.1) addresses the relation between er-
rors and human quality judgements. The sec-
ond set (?5.2) focuses on the relation between er-
rors and automatic metrics. In both cases, be-
fore measuring the impact of different errors on
the response variable (respectively quality judge-
ments and metrics), we validate the effectiveness
of mixed linear models by comparing their predic-
tion capability with other methods.
In all experiments, error counts of each category
were normalized into percentages with respect to
the sentence length and mapped in a logarithmic
scale. In this way, we basically assume that the
impact of errors tends to saturate above a given
threshold, hypothesis that also results in better fits
by our models.
3
Notice that while the chosen log-
3
In other words, we assume that human sensitivity to er-
10 base is easy to interpret, linear models can im-
plicitly adjust it. Our analysis makes use of mixed
linear models incorporating, as fixed effects, the
four types of errors (lex, miss, morph and reo) and
their pairwise interactions (the product of the sin-
gle error log counts), while their random struc-
ture depends on each specific experiment. For
the experiments we rely on the R language (R
Core Team, 2013) implementation of linear mixed
model in the lme4 library (Bates et al., 2014).
We assess the quality of our mixed linear mod-
els (MLM) by comparing their prediction capabil-
ity with a sequence of simpler linear models in-
cluding only fixed effects. In particular, we built
five univariate models and two multivariate mod-
els. The univariate models use as covariates, re-
spectively, the sum of all error types (baseline),
and each of the four types of errors (lex, miss,
morph and reo). The two multivariate models in-
clude all the four error types, considering them
without interactions (FLM w/o Interact.) and with
interactions (FLM).
Prediction performance is computed in terms of
Mean Absolute Error (MAE),
4
which we estimate
by averaging over 1,000 random splits of the data
in 90% training and 10% test. In particular, for the
human quality classes we pick the integer between
1-5 that is closest to the predicted value.
5.1 Errors vs. Quality Judgements
The response variable we target in this experiment
is the quality score produced by human annotators.
Our measurements follow a typical within-subject
design in which all the 3 annotators are exposed
to the same conditions (levels of the independent
variables), corresponding in our case to perfectly
balanced observations from 2 MT systems and N
sentences. This setting results in repeated or clus-
tered observations (thus violating independence)
corresponding to groups which naturally identify
possible random effects,
5
namely the annotators
(3 levels with 2xN observations each), the systems
(2 levels and 3xN observations each), and the sen-
rors follows a log-scale law: e.g. more sensitive to variations
in the interval [1-10] that in the interval [30-40].
4
MAE is calculated as the average of the absolute errors
|f
i
? y
i
|, where f
i
is the prediction of the model and y
i
the
true value for the i
th
instance. As it is a measure of error,
lower MAE scores indicate that our predictions are closer to
the true values of each test instance.
5
In all our experiments, random effects are limited to ran-
dom shifts since preliminary experiments also including ran-
dom slopes did not provide consistent results.
1648
Model ENZH ENAR ENRU
baseline 0.58 0.73 0.67
lex 0.67 0.78 0.72
miss 0.72 0.89 0.74
morph 0.72 0.89 0.74
reo 0.70 0.82 0.76
FLM w/o Interact. 0.59 0.77 0.65
FLM 0.57 0.72 0.63
MLM 0.53 0.61 0.61
Table 3: Prediction capability of human judge-
ments (MAE).
tences (N levels with 6 observations each). In prin-
ciple, such random effects permit to remove sys-
tematic biases of individual annotators, single sys-
tems and even single sentences, which are mod-
elled as random variables sampled from distinct
populations.
Table 3 shows a comparison of the prediction
capability of the mixed model
6
with simpler ap-
proaches. While the good performance achieved
by our strong baseline cannot be outperformed
by separately counting the number of errors of a
single type, lower MAE results are obtained by
methods based on multivariate analysis. Among
them, FLM brings the first consistent improve-
ments over the baseline by considering error in-
teractions, while MLM leads to the lowest MAE
due to the addition of random effects. The impor-
tance of random effects is particularly evidenced
by ENAR (12 points below the baseline). Indeed,
as discussed in ?4.1, for this language combina-
tion human annotators show the lowest agreement
score. This variability, which hides the smaller
differences in systems? behaviour, demonstrates
the importance of accounting for the erratic fac-
tors that might influence empirical observations in
a given setting. The good performance achieved
by MLM, combined with their high descriptive
power,
7
motivates their adoption in our study.
Concerning the analysis of error impact, Ta-
ble 4 shows the statistically significant coefficients
for the full-fledged MLM models for each trans-
lation direction. By default, all reported coeffi-
cients have p-values ? 10
?4
, while those marked
with ? and ? have respectively p-values ? 10
?3
and ? 10
?2
. Slope coefficients basically show
6
Note that the mixed model used in prediction does not in-
clude the random effect on sentences since the training sam-
ples do not guarantee sufficient observations for each test sen-
tence.
7
Note that the strong baseline used for comparison is not
capable to describe the contribution of the different error
types.
Error ENZH ENAR ENRU
Intercept 4.29 3.79
?
4.21
lex -1.27 -0.96 -1.12
miss -1.76 -0.90 -1.30
morph -0.48
?
-0.83 -0.51
reo -1.01 -0.75 -0.18
lex:miss 1.00 0.39 0.68
lex:morph - 0.29 0.32
lex:reo 0.50 0.21 -
miss:morph - 0.35 -
miss:reo 0.54 0.33 -
morph:reo - 0.37 -
Table 4: Effect of translation errors on MT qual-
ity perception on all judged sentences. Reported
coefficients (?) are all statistically significant with
p ? 10
?4
, except those marked with
?
(p ? 10
?3
),
and
?
(p ? 10
?2
).
the impact of different error types (alone and in
combination) on human quality scores. Those that
are not statistically significant are omitted as they
do not increase the fitting capability of our model.
As can be seen from the table, such impact varies
across the different language combinations. While
for ENZH and ENRU miss is the error having
the highest impact (highest decrement with respect
to the intercept), the most problematic error for
ENAR is lex. It is interesting to observe that pos-
itive values for error combinations indicate that
their combined impact is lower that the sum of the
impact of the single errors. For instance, while for
ENZH a one-step increment in lex and miss errors
would respectively cause a reduction in the human
judgement of 1.27 and 1.76, their occurrence in
the same sentence would be discounted by 1.00.
This would result in a global judgement of 2.26
(4.29 -1.27 -1.76 +1.00) instead of 1.26. While
for ENAR this phenomenon can be observed for
all error combinations, such discount effects are
not always significant for the other two language
pairs. The existence of discount effects of various
magnitude associated to the different error com-
binations is a novel finding made possible by the
adoption of mixed-effect models.
Another interesting observation is that, in con-
trast with the common belief that the most fre-
quent errors have the highest impact on human
quality judgements, our experiments do not re-
veal such strict correlation (at least for the exam-
ined language pairs). For instance, for ENZH and
ENRU the impact of miss errors is higher than the
impact of other more frequent issues.
1649
BLEU score TER GTM
Model ENZH ENAR ENRU ENZH ENAR ENRU ENZH ENAR ENRU
baseline 12.4 9.8 12.2 15.7 13.4 14.4 9.8 10.6 11.5
lex 12.9 10.4 13.0 16.3 13.8 14.9 9.7 10.9 12.1
miss 13.8 10.5 14.1 17.3 14.2 16.4 10.5 11.1 13.2
morph 13.9 10.3 13.6 17.5 13.8 16.3 10.5 10.9 13.1
reo 13.7 10.5 14.0 17.4 14.1 16.3 10.4 11.1 13.1
FLM w/o Interact. 12.9 9.9 12.2 16.3 13.5 14.4 9.7 10.7 11.7
FLM 12.3 9.7 12.1 15.6 13.4 14.3 9.4 10.6 11.6
MLM 10.8 9.5 12.0 14.7 13.0 14.2 8.9 10.5 11.6
Table 5: Prediction capability of BLEU score, TER and GTM (MAE).
5.2 Errors vs. Automatic Metrics
In this experiment, the response variable is an au-
tomatic metric which is computed on a sample of
MT outputs (which are again perfectly balanced
over systems and sentences) and a set of reference
translations. As no subjects are involved in the ex-
periment, random variability is assumed to come
from the involved systems, the tested sentences,
and the unknown missing link between the covari-
ates (error types) and the response variable which
is modelled by the residual noise. Notice that,
in this case, the random effect on the sentences
also incorporates in some sense the randomness
of the corresponding reference translations, which
are themselves representatives of larger samples.
The prediction capability of the mixed model,
in comparison with the simpler ones, is reported
in Table 5. Also in this case, the low MAE
achieved by the baseline is out of the reach of uni-
variate methods. Again, small improvements are
brought by FLM when considering error interac-
tions, whereas the most visible gains are achieved
by MLM due to their control of random effects.
This is more evident for some language combina-
tions and can be explained by the differences in
systems? performance, a variability factor easily
absorbed by random effects. Indeed, the largest
MAE decrements over the baseline are always ob-
served for ENZH (for which the overall mean re-
sults reported in Table 2 show the largest dif-
ferences) and the smallest decrements relate to
language/metric combinations where systems? be-
haviour is more similar (e.g. ENRU/GTM).
Concerning the analysis of error impact, Table
6 shows how different error types (alone and in
combination) influence performance results mea-
sured with automatic metrics. To ease interpre-
tation of the reported figures we also show Pear-
son and Spearman correlations of each set of coef-
ficients (excluding intercept estimates) with their
corresponding coefficients reported in Table 4. In
fact, our primary interest in this experiment is to
see which metrics show a sensitivity to specific er-
ror types similar to human perception. As we can
see, the coefficients for each metric significantly
vary depending on the language, for the simple
reason that also the distribution and co-occurrence
of errors vary significantly across the different lan-
guages and MT systems. Remarkably, for some
translation directions, some of the metrics show
a sensitivity to errors that is very similar to that
of human judges. In particular, BLEU for ENZH
and ENAR, and GTM for ENZH show a very high
correlation with the human sensitivity to transla-
tion errors, with Pearson correlation coefficient ?
0.97. For ENRU, the best Pearson correlation is
instead achieved by TER (-0.78).
Besides these general observations, a closer
look at the reported scores brings additional find-
ings. In three cases (BLEU for ENZH, GTM for
ENZH and ENAR) the analysed metrics are most
sensitive to the same error type that has the high-
est influence on human judgements (according to
Table 4, these are miss for ENZH and ENRU, lex
for ENAR). On the contrary, in one case (TER for
ENZH) the analysed metric is insensitive to the er-
ror type (miss) which has the highest impact on hu-
man quality scores. From a practical point of view,
these remarks provide useful indications about the
appropriateness of each metric to highlight the de-
ficiencies of a specific system and to measure im-
provements targeting specific issues. As a rule of
thumb, for instance, to measure improvements of
an ENZH system with respect to missing words,
it would be more advisable to use BLEU or GTM
instead of TER.
8
8
Note that this conclusion holds for our data sample, in
which different types of errors co-occur and only one refer-
ence translation is available. In such conditions, our regres-
sion model shows that TER is not influenced by miss errors in
a statistically significant way. This does not mean that TER
is insensitive to missing words when occurring in isolation,
1650
BLEU score TER GTM
Error ENZH ENAR ENRU ENZH ENAR ENRU ENZH ENAR ENRU
Intercept 60.55
2
38.45
?
51.73 32.41
2
52.25
?
33.4
?
83.57
?
60.11
?
75.38
lex -18.78 -9.25 -16.57 16.87 9.66 18.45 -13.63 -7.60 -16.13
miss -23.20 -10.41 -6.75 - - 8.24 -14.87 - -5.98
morph - -9.97 -12.65 - 8.90 11.41 - -6.60 -10.42
reo -13.27 -7.62 -10.57 14.44 9.81 6.39 -7.29 -5.50 -7.03
lex:miss 14.37 4.97
?
- - - - 8.24
?
- -
lex:morph - - 5.27
?
- - -5.22
?
- - 4.92
lex:reo 8.57 3.57
?
5.40
?
-7.24
?
-4.35
?
- 5.46 3.22
?
3.65
2
miss:morph - 4.44
?
- - - - - - -
miss:reo 6.74
?
- 4.30 - - -6.38
?
5.07
?
- 4.71
?
morph:reo - 3.81
?
- - -4.97
?
- - 2.57
?
-
Pearson 0.98 0.97 0.70 -0.58 -0.78 -0.78 0.98 0.78 0.74
Spearman 0.97 0.91 0.73 -0.57 -0.59 -0.80 0.97 0.59 0.76
Table 6: Effect of translation errors on BLEU score, TER and GTM on all judged sentences and correla-
tion with their corresponding effects on human quality scores (from Table 4). Reported coefficients (?)
are statistically significant with p ? 10
?4
, except those marked with
?
(p ? 10
?3
),
?
(p ? 10
?2
) and
2
(p ? 10
?1
).
Similar considerations also apply to the analysis
of the impact of error combinations. The same dis-
count effects that we noticed when analysing the
impact of errors? co-occurrence on human percep-
tion (?5.1) are evidenced, with different degrees of
sensitivity, by the automatic metrics. While some
of them substantially reflect human response (e.g.
BLEU and GTM for ENZH), in some cases we
observe either the insensitivity to specific combi-
nations (mostly for ENAR), or a higher sensitivity
compared to the values measured for human as-
sessors (mostly for ENRU, where the impact of
miss:reo combinations is discounted - hence un-
derestimated - by all the metrics).
Despite such small differences, the coherence of
our results with previous findings (?5.1) suggests
the reliability of the applied method. Complet-
ing the picture along the side of the MT evalua-
tion triangle which connects error annotations and
automatic metrics, our findings contribute to shed
light on the existing relationships between transla-
tion errors, their interaction, and the sensitivity of
widely used automatic metrics.
6 Conclusion
We investigated the MT evaluation triangle (hav-
ing as corners automatic metrics, human quality
judgements and error annotations) along the two
less explored sides, namely: i) the relation be-
tween MT errors and human quality judgements
but that TER becomes less sensitive to such errors when they
co-occur with other types of errors. Overall, our experiments
show that when MT outputs contain more than one error type,
automatic metrics show different levels of sensitivity to each
specific error type.
and ii) the relation between MT errors and auto-
matic metrics. To this aim we employed a ro-
bust statistical analysis framework based on lin-
ear mixed-effects models (the first contribution of
the paper), which have a higher descriptive power
than simpler methods based on the raw count of
translation errors and are less artificial compared
to previous statistically-grounded approaches.
Working on three translation directions having
Chinese, Arabic and Russian as target (our second
contribution), we analysed error-annotated trans-
lations considering the impact of specific errors
(alone and in combination) and accounting for the
variability of the experimental set-up that origi-
nated our empirical observations. This led us to
interesting findings specific to each language pair
(third contribution). Concerning the relation be-
tween MT errors and quality judgements, we have
shown that: i) the frequency of errors of a given
type does not correlate with human preferences,
ii) errors having the highest impact can be pre-
cisely isolated and iii) the impact of error inter-
actions is often subject to measurable and previ-
ously unknown ?discount? effects. Concerning the
relation between MT errors and automatic met-
rics (BLEU, TER and GTM), our analysis evi-
denced significant differences in the sensitivity of
each metric to different error types. Such differ-
ences provide useful indications about the most
appropriate metric to assess system improvements
with respect to specific weaknesses. If learning
from errors is a crucial aspect of improving exper-
tise, our method and the resulting empirical find-
ings represent a significant contribution towards a
1651
more informed approach to system development,
improvement and evaluation.
Acknowledgements
This work has been partially supported by the EC-
funded project MateCat (ICT-2011.4.2-287688).
References
Harald R. Baayen, Douglas J. Davidson, and Dou-
glas M. Bates. 2008. Mixed-effects modeling with
crossed random effects for subjects and items. Jour-
nal of memory and language, 59(4):390?412.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Douglas Bates, Martin Maechler, Ben Bolker, and
Steven Walker, 2014. lme4: Linear mixed-effects
models using Eigen and S4. R package version 1.1-
6.
Fr?ed?eric Blain, Jean Senellart, Holger Schwenk, Mirko
Plitt, and Johann Roturier. 2011. Qualitative analy-
sis of post-editing for high quality machine transla-
tion. In Asia-Pacific Association for Machine Trans-
lation (AAMT), editor, Machine Translation Summit
XIII, Xiamen (China), 19-23 sept.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Herbert H. Clark. 1973. The language-as-fixed-effect
fallacy: A critique of language statistics in psycho-
logical research. Journal of verbal learning and ver-
bal behavior, 12(4):335?359.
Mireia Farr?us, Marta R. Costa-juss`a, and Maja
Popovi?c. 2012. Study and correlation analysis of
linguistic, perceptual, and automatic machine trans-
lation evaluations. J. Am. Soc. Inf. Sci. Technol.,
63(1):174?184, January.
Mireia Farr?us Cabeceran, Marta Ruiz Costa-Juss`a,
Jos?e Bernardo Mari?no Acebal, Jos?e Adri?an
Rodr??guez Fonollosa, et al. 2010. Linguistic-based
evaluation criteria to identify statistical machine
translation errors. In Proceedings of the 14th
Annual Conference of the European Association for
Machine Translation (EAMT).
Mary Flanagan. 1994. Error classification for mt eval-
uation. In Technology Partnerships for Crossing the
Language Barrier: Proceedings of the First Confer-
ence of the Association for Machine Translation in
the Americas, pages 65?72.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5).
Christian Girardi, Luisa Bentivogli, Mohammad Amin
Farajian, and Marcello Federico. 2014. Mt-equal:
a toolkit for human assessment of machine trans-
lation output. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: System Demonstrations, pages 120?
123, Dublin, Ireland, August. Dublin City Univer-
sity and Association for Computational Linguistics.
Sharon Goldwater, Daniel Jurafsky, and Christopher D.
Manning. 2010. Which words are hard to rec-
ognize? prosodic, lexical, and disfluency factors
that increase speech recognition error rates. Speech
Communication, 52(3):181?200.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, pages 439?448. ACM.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 503?511, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Katrin Kirchhoff, Daniel Capurro, and Anne M. Turner.
2013. A conjoint analysis framework for evaluating
user preferences in machine translation. Machine
Translation, pages 1?17.
Richard J. Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33 (1):159?174.
Samuel L?aubli, Mark Fishel, Gary Massey, Maureen
Ehrensberger-Dow, and Martin Volk. 2013. Assess-
ing Post-Editing Efficiency in a Realistic Translation
Environment. In Michel Simard Sharon O?Brien
and Lucia Specia (eds.), editors, Proceedings of MT
Summit XIV Workshop on Post-editing Technology
and Practice, pages 83?91, Nice, France.
Chin-Yew Lin and Franz Josef Och. 2004. Orange:
a method for evaluating automatic evaluation met-
rics for machine translation. In Proceedings of Col-
ing 2004, pages 501?507, Geneva, Switzerland, Aug
23?Aug 27. COLING.
Arle Lommel, Aljoscha Burchardt, Maja Popovi?c, Kim
Harris, Eleftherios Avramidis, and Hans Uszkoreit.
1652
2014. Using a new analytic measure for the anno-
tation and analysis of mt errors on real data. In
Proceedings of the 17th Conference of the Euro-
pean Association for Machine Translation (EAMT),
Dubrovnik, Croatia, June.
Sharon O?Brien. 2011. Cognitive Explorations of
Translation. Bloomsbury Studies in Translation.
Bloomsbury Academic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Research Report
RC22176, IBM Research Division, Thomas J. Wat-
son Research Center.
Maja Popovi?c and Hermann Ney. 2011. Towards au-
tomatic error analysis of machine translation output.
Comput. Linguist., 37(4):657?688, December.
Maja Popovic, Eleftherios Avramidis, Aljoscha Bur-
chardt, Sabine Hunsicker, Sven Schmeier, Cindy
Tscherwinka, David Vilar, and Hans Uszkoreit.
2013. Learning from human judgments of machine
translation output. In Proceedings of the MT Summit
XIV. Proceedings of MT Summit XIV.
Maja Popovi?c, Arle Lommel, Aljoscha Burchardt,
Eleftherios Avramidis, and Hans Uszkoreit. 2014.
Relations between different types of post-editing op-
erations, cognitive effort and temporal effort. In
Proceedings of the 17th Conference of the Euro-
pean Association for Machine Translation (EAMT),
Dubrovnik, Croatia, June.
Maja Popovic. 2012. Class error rates for evaluation
of machine translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 71?75, Montr?eal, Canada, June. Associ-
ation for Computational Linguistics.
R Core Team, 2013. R: A Language and Environment
for Statistical Computing. R Foundation for Statis-
tical Computing, Vienna, Austria.
Nick Ruiz and Marcello Federico. 2014. Assessing the
Impact of Speech Recognition Errors on Machine
Translation Quality. In 11th Conference of the As-
sociation for Machine Translation in the Americas
(AMTA), Vancouver, BC, Canada.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In 5th Conference of the Association for Machine
Translation in the Americas (AMTA), Boston, Mas-
sachusetts, August.
Sara Stymne and Lars Ahrenberg. 2012. On
the practice of error analysis for machine trans-
lation evaluation. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Uur Doan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, may. European Language Resources
Association (ELRA).
Irina Temnikova. 2010. Cognitive evaluation approach
for a controlled language post-editing experiment.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC?10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Joseph P. Turian, I. Dan Melamed, and Luke Shen.
2003. Evaluation of machine translation and its
evaluation. In Proceedings of the MT Summit IX.
Gerhard Tutz and Wolfgang Hennevogl. 1996. Ran-
dom effects in ordinal regression models. Computa-
tional Statistics & Data Analysis, 22(5):537?557.
David Vilar, Jia Xu, Luis Fernando dHaro, and Her-
mann Ney. 2006. Error analysis of statistical ma-
chine translation output. In Proceedings of the Fifth
International Conference on Language Resources
and Evaluation (LREC?06), pages 697?702.
1653
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 439?448,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Cutting the Long Tail: Hybrid Language Models
for Translation Style Adaptation
Arianna Bisazza and Marcello Federico
Fondazione Bruno Kessler
Trento, Italy
{bisazza,federico}@fbk.eu
Abstract
In this paper, we address statistical ma-
chine translation of public conference talks.
Modeling the style of this genre can be very
challenging given the shortage of available
in-domain training data. We investigate the
use of a hybrid LM, where infrequent words
are mapped into classes. Hybrid LMs are
used to complement word-based LMs with
statistics about the language style of the
talks. Extensive experiments comparing
different settings of the hybrid LM are re-
ported on publicly available benchmarks
based on TED talks, from Arabic to English
and from English to French. The proposed
models show to better exploit in-domain
data than conventional word-based LMs for
the target language modeling component of
a phrase-based statistical machine transla-
tion system.
1 Introduction
The translation of TED conference talks1 is an
emerging task in the statistical machine transla-
tion (SMT) community (Federico et al 2011).
The variety of topics covered by the speeches, as
well as their specific language style, make this a
very challenging problem.
Fixed expressions, colloquial terms, figures of
speech and other phenomena recurrent in the talks
should be properly modeled to produce transla-
tions that are not only fluent but that also em-
ploy the right register. In this paper, we propose
a language modeling technique that leverages in-
domain training data for style adaptation.
1http://www.ted.com/talks
Hybrid class-based LMs are trained on text
where only infrequent words are mapped to Part-
of-Speech (POS) classes. In this way, topic-
specific words are discarded and the model fo-
cuses on generic words that we assume more use-
ful to characterize the language style. The factor-
ization of similar expressions made possible by
this mixed text representation yields a better n-
gram coverage, but with a much higher discrimi-
native power than POS-level LMs.
Hybrid LM also differs from POS-level LM in
that it uses a word-to-class mapping to determine
POS tags. Consequently, it doesn?t require the de-
coding overload of factored models nor the tag-
ging of all parallel data used to build phrase ta-
bles. A hybrid LM trained on in-domain data can
thus be easily added to an existing baseline sys-
tem trained on large amounts of background data.
The proposed models are used in addition to
standard word-based LMs, in the framework of
log-linear phrase-based SMT.
The remainder of this paper is organized as fol-
lows. After discussing the language style adapta-
tion problem, we will give an overview of relevant
work. In the following sections we will describe
in detail hybrid LM and its possible variants. Fi-
nally, we will present an empirical analysis of the
proposed technique, including intrinsic evaluation
and SMT experiments.
2 Background
Our working scenario is the translation of TED
talks transcripts as proposed by the IWSLT Eval-
uation Campaign2. This genre covers a variety
of topics ranging from business to psychology.
The available training material ? both parallel and
2http://www.iwslt2011.org
439
Beginning of Sentence: [s] End of Sentence: [/s]
TED NEWS TED NEWS
1st [s] Thank you . [/s] 1st [s] ( AP ) - 1st [s] Thank you . [/s] 1st ? he said . [/s]
2 [s] Thank you very much 2 [s] WASHINGTON ( ... 2 you very much . [/s] 2 ? she said . [/s]
3 [s] I ?m going to 3 [s] NEW YORK ( AP 3 in the world . [/s] 3 , he said . [/s]
4 [s] And I said , 4 [s] ( CNN ) ? 4 and so on . [/s] 4 ? he said . [/s]
5 [s] I don ?t know 5 [s] NEW YORK ( R... 5 , you know . [/s] 5 in a statement . [/s]
6 [s] He said , ? 6 [s] He said : ? 6 of the world . [/s] 6 the United States . [/s]
7 [s] I said , ? 7 [s] ? I don ?t 7 around the world . [/s] 7 to this report . [/s]
8 [s] And of course , 8 [s] It was last updated 8 . Thank you . [/s] 8 ? he added . [/s]
9 [s] And one of the 9 [s] At the same time 9 the United States . [/s] 9 , police said . [/s]
10 [s] And I want to ... 10 all the time . [/s] 10 , officials said . [/s]
11 [s] And that ?s what 69 [s] I don ?t know 11 to do it . [/s] ...
12 [s] We ?re going to 612 [s] I ?m going to 12 and so forth . [/s] 13 in the world . [/s]
13 [s] And I think that 2434 [s] ? I said , 13 don ?t know . [/s] 17 around the world . [/s]
14 [s] And you can see 7034 [s] He said , ? 14 to do that . [/s] 46 of the world . [/s]
15 [s] And this is a 8199 [s] And I said , 15 in the future . [/s] 129 all the time . [/s]
16 [s] And this is the 8233 [s] Thank you very much 16 the same time . [/s] 157 and so on . [/s]
17 [s] And he said , ... 17 , you know ? [/s] 1652 , you know . [/s]
18 [s] So this is a ? [s] Thank you . [/s] 18 to do this . [/s] 5509 you very much . [/s]
Table 1: Common sentence-initial and sentence-final 5-grams, as ranked by frequency, in the TED and NEWS
corpora. Numbers denote the frequency rank.
monolingual ? consists of a rather small collection
of TED talks plus a variety of large out-of-domain
corpora, such as news stories and UN proceed-
ings.
Given the diversity of topics, the in-domain
data alone cannot ensure sufficient coverage to an
SMT system. The addition of background data
can certainly improve the n-gram coverage and
thus the fluency of our translations, but it may also
move our system towards an unsuitable language
style, such as that of written news.
In our study, we focus on the subproblem of
target language modeling and consider two En-
glish text collections, namely the in-domain TED
and the out-of-domain NEWS3, summarized in
Table 2. Because of its larger size ? two orders
of magnitude ? the NEWS corpus can provide a
better LM coverage than the TED on the test data.
This is reflected both on perplexity and on the av-
erage length of the context (or history h) actually
3http://www.statmt.org/wmt11/translation-task.html
LM Data |S| |W | |V | PP h5g
TED-En 124K 2.4M 51K 112 1.7
NEWS-En 30.7M 782M 2.2M 104 2.5
Table 2: Training data and coverage statistics of two
5-gram LMs used for the TED task: number of sen-
tences and tokens, vocabulary size; perplexity and av-
erage word history.
used by these two LMs to score the test?s refer-
ence translations. Note that the latter measure is
bounded at the LM order minus one, and is in-
versely proportional to the number of back-offs
performed by the model. Hence, we use this value
to estimate how well an n-gram LM fits the test
data. Indeed, despite the genre mismatch, the per-
plexity of a NEWS 5-gram LM on the TED-2010
test reference translations is 104 versus 112 for
the in-domain LM, and the average history size is
2.5 versus 1.7 words.
TED NEWS
1st , 1st the
... ...
9 I 40 I
12 you 64 you
90 actually 965 actually
268 stuff 2479 guy
370 guy 2861 stuff
436 amazing 4706 amazing
Table 3: Excerpts from TED and NEWS training vo-
cabularies, as ranked by frequency. Numbers denote
the frequency rank.
Yet we observe that the style of public speeches
is much better represented in the in-domain cor-
pus than in the out-of-domain one. For instance,
let us consider the vocabulary distribution4 of the
4Hesitations and filler words, typical of spoken language,
are not covered in our study because they are generally not
reported in the TED talk transcripts.
440
two corpora (Table 3). The very first forms, as
ranked by frequency, are quite similar in the two
corpora. However, there are important excep-
tions: the pronouns I and you are among the top
20 frequent forms in the TED, while in the NEWS
they are ranked only 40th and 64th respectively.
Other interesting cases are the words actually,
stuff, guy and amazing, all ranked about 10 times
higher in the TED than in the NEWS corpus.
We can also analyze the most typical ways
to start and end a sentence in the two text col-
lections. As shown in Table 1, the frequency
ranking of sentence-initial and sentence-final 5-
grams in the in-domain corpus is notably different
from the out-of-domain one. TED?s most frequent
sentence-initial 5-gram ?[s] Thank you . [/s] ? is
not at all attested in the NEWS corpus. As for
the 4th most common sentence start ?[s] And I
said ,? is only ranked 8199th in the NEWS, and
so on. Notably, the top ranked NEWS 5-grams in-
clude names of cities (Washington, New York) and
of news agency (AP, Reuters). As regards sen-
tence endings, we observe similar contrasts: for
instance, the word sequence ?and so on . [/s] ?
is ranked 4th in the TED and 157th in the NEWS
while ?, you know . [/s] ? is 5th in the TED and
only 1652th in the NEWS.
These figures confirm that the talks have a spe-
cific language style, remarkably different from
that of the written news genre. In summary, talks
are characterized by a massive use of first and sec-
ond persons, by shorter sentences, and by more
colloquial lexical and syntactic constructions.
3 Related Work
The brittleness of n-gram LMs in case of mis-
match between training and task data is a well
known issue (Rosenfeld, 2000). So called do-
main adaptation methods (Bellegarda, 2004) can
improve the situation, once a limited amount
of task specific data become available. Ideally,
domain-adaptive LMs aim to improve model ro-
bustness under changing conditions, involving
possible variations in vocabulary, syntax, content,
and style. Most of the known LM adaption tech-
niques (Bellegarda, 2004), however, address all
these variations in a holistic way. A possible rea-
son for this is that LM adaptation methods were
originally developed under the automatic speech
recognition framework, which typically assumes
the presence of one single LM. The progressive
adoption of the log-linear modeling framework in
many NLP tasks has recently introduced the use
of multiple LM components (features), which per-
mit to naturally factor out and integrate different
aspects of language into one model. In SMT, the
factored model (Koehn and Hoang, 2007), for in-
stance, permits to better tailor the LM to the task
syntax, by complementing word-based n-grams
with a part-of-speech (POS) LM , that can be es-
timated even on a limited amount of task-specific
data. Besides many works addressing holistic LM
domain adaptation for SMT, e.g. Foster and Kuhn
(2007), recently methods were also proposed to
explicitly adapt the LM to the discourse topic of a
talk (Ruiz and Federico, 2011). Our work makes
another step in this direction by investigating hy-
brid LMs that try to explicitly represent the speak-
ing style of the talk genre. As a difference from
standard class-based LMs (Brown et al 1992) or
the more recent local LMs (Monz, 2011), which
are used to predict sequences of classes or word-
class pairs, our hybrid LM is devised to pre-
dict sequences of classes interleaved by words.
While we do not claim any technical novelty in
the model itself, to our knowledge a deep investi-
gation of hybrid LMs for the sake of style adap-
tation is definitely new. Finally, the term hybrid
LM was inspired by Yazgan and Sarac?lar (2004),
which called with this name a LM predicting se-
quences of words and sub-words units, devised to
let a speech recognizer detect out-of-vocabulary-
words.
4 Hybrid Language Model
Hybrid LMs are n-gram models trained on a
mixed text representation where each word is ei-
ther mapped to a class or left as is. This choice
is made according to a measure of word common-
ness and is univocal for each word type.
The rationale is to discard topic-specific words,
while preserving those words that best character-
ize the language style (note that word frequency
is computed on the in-domain corpus only). Map-
ping non-frequent terms to classes naturally leads
to a shorter tail in the frequency distribution, as
visualized by Figure 1. A model trained on such
data has a better n-gram coverage of the test set
and may take advantage of a larger context when
scoring translation hypotheses.
As classes, we use deterministically assigned
POS tags, obtained by first tagging the data with
441
??
??
???
???
????
????
? ??? ??? ??? ??? ??? ???
????
????
Figure 1: Type frequency distribution in the English
TED corpus before and after POS-mapping of words
with less than 500 occurrences (25% of tokens). The
rank in the frequency list (x-axis) is plotted against the
respective frequency in logarithmic scale. Types with
less than 20 occurrences are omitted from the graph.
Tree Tagger (Schmid, 1994) and then choosing
the most likely tag for each word type. In this
way, we avoid the overload of searching for the
best tagging decisions at run-time at the cost of
a slightly higher imprecision (see Section 5.1).
The hybridly mapped data is used to train a high-
order n-gram LM that is plugged into an SMT de-
coder as an additional feature on target word se-
quences. During the translation process, words
are mapped to their class just before querying the
hybrid LM, therefore translation models can be
trained on plain un-tagged data.
As exemplified in Table 4, hybrid LMs can
draw useful statistics on the context of common
words even from a small corpus such as the TED.
To have an idea of data sparseness, consider that
in the unprocessed TED corpus the most frequent
5-gram containing the common word guy occurs
only 3 times. After the mapping of words with
frequency <500, the highest 5-gram frequency
grows to 17, the second one to 9, and so on.
guy 598 actually 3978
a guy VBN NP NP 17 [s] This is actually a 20
guy VBN NP NP , 9 [s] It ?s actually a 17
guy , NP NP , 8 , you can actually VB 13
a guy called NP NP 8 is actually a JJ NN 13
this guy , NP NP 6 This is actually a NN 12
guy VBN NP NP . 6 [s] And this is actually 12
by a guy VBN NP 5 [s] And that ?s actually 10
a JJ guy . [/s] 5 , but it ?s actually 10
I was VBG this guy 4 NN , it ?s actually 9
guy VBN NP . [/s] 4 we?re actually going to 8
Table 4: Most common hybrid 5-grams containing the
words guy and actually, along with absolute frequency.
4.1 Word commonness criteria
The most intuitive way to measure word common-
ness is by absolute term frequency (F ). We will
use this criterion in most of our experiments. A
finer solution would be to also consider the com-
monness of a word across different talks. At this
end, we propose to use the fdf statistics, that is the
product of relative term f requency and document
f requency5:
fdfw =
c(w)
?
w? c(w
?)
?
c(dw)
c(d)
where dw are the documents (talks) containing at
least one occurrence of the word w.
If available, real talk boundaries can be used
to define the documents. Alternatively, we can
simply split the corpus into chunks of fixed size.
In this work we use this approximation.
Another issue is how to set the threshold. In-
dependently from the chosen commonness mea-
sure, we can reason in terms of the ratio of tokens
that are mapped to POS classes (WP ). For in-
stance, in our experiments with English, we can
set the threshold to F=500 and observe that WP
corresponds to 25% of the tokens (and 99% of the
types). In the same corpus, a similar ratio is ob-
tained with fdf=0.012.
In our study, we consider three ratios WP ={.25,
.50, .75} that correspond to different levels of lan-
guage modeling: from a domain-generic word-
level LM to a lexically anchored POS-level LM.
4.2 Handling morphology
Token frequency-based measures may not be suit-
able for languages other than English. When
translating into French, for instance, we have to
deal with a much richer morphology.
As a solution we can use lemmas, univocally
assigned to word types in the same manner as
POS tags. Lemmas can be employed in two ways:
only for word selection, as a frequency measure,
or also for word representation, as a mapping for
common words. In the former, we preserve in-
flected variants that may be useful to model the
language style, but we also risk to see n-gram cov-
erage decrease due to the presence of rare types.
In the latter, only canonical forms and POS tags
5This differs from the tf-idf widely used in information
retrieval, which is used to measure the relevance of a term in
a document. Instead, we measure commonness of a term in
the whole corpus.
442
appear in the processed text, thus introducing a
further level of abstraction from the original text.
Here follows a TED sentence in its original
version (first line) and after three different hy-
brid mappings ? namely WP =.25, WP =.25 with
lemma forms, and WP =.50:
Now you laugh, but that quote has kind of a sting to it, right.
Now you VB , but that NN has kind of a NN to it, right.
Now you VB , but that NN have kind of a NN to it, right.
RB you VB , CC that NN VBZ NN of a NN to it, RB .
5 Evaluation
In this section we perform an intrinsic evaluation
of the proposed LM technique, then we measure
its impact on translation quality when integrated
into a state-of-the-art phrase-based SMT system.
5.1 Intrinsic evaluation
We analyze here a set of hybrid LMs trained on
the English TED corpus by varying the ratio of
POS-mapped words and the word representation
technique (word vs lemma). All models were
trained with the IRSTLM toolkit (Federico et al
2008), using a very high n-gram order (10) and
Witten-Bell smoothing.
First, we estimate an upper bound of the POS
tagging errors introduced by deterministic tag-
ging. At this end, the hybridly mapped data is
compared with the actual output of Tree Tagger on
the TED training corpus (see Table 5). Naturally,
the impact of tagging errors correlates with the ra-
tio of POS-mapped tokens, as no error is counted
on non-mapped tokens. For instance, we note that
the POS error rate is only 1.9% in our primary set-
ting, WP =.25 and word representation, whereas
on a fully POS-mapped text it is 6.6%. Note that
the English tag set used by Tree Tagger includes
43 classes.
Now we focus on the main goal of hybrid text
representation, namely increasing the coverage of
the in-domain LM on the test data. Here too, we
measure coverage by the average length of word
history h used to score the test reference transla-
tions (see Section 2). We do not provide perplex-
ity figures, since these are not directly compara-
ble across models with different vocabularies. As
shown by Table 5, n-gram coverage increases with
the ratio of POS-mapped tokens, ranging from 1.7
on an all-words LM to 4.4 on an all-POS LM. Of
Hybrid 10g LM |V | POS-Err h10g
all words 51299 0.0% 1.7
all lemmas 38486 0.0% 1.9
.25 POS/words 475 1.9% 2.7
.50 POS/words 93 4.1% 3.5
.75 POS/words 50 5.7% 4.1
allPOS 43 6.6% 4.4
.25 POS/lemmas 302 1.8% 2.8
.25 POS/words(fdf) 301 1.9% 2.7
Table 5: Comparison of LMs obtained from different
hybrid mappings of the English TED corpus: vocabu-
lary size, POS error rate, and average word history on
IWSLT?tst2010?s reference translations.
course, the more words are mapped, the less dis-
criminative our model will be. Thus, choosing the
best hybrid mapping means finding the best trade-
off between coverage and informativeness.
We also applied hybrid LM to the French lan-
guage, again using Tree Tagger to create the POS
mapping. The tag set in this case comprises 34
classes and the POS error rate with WP =.25 is
1.2% (compare with 1.9% in English). As previ-
ously discussed, morphology has a notable effect
on the modeling of French. In fact, the vocabu-
lary reduction obtained by mapping all the words
to their most probable lemma is -45% (57959 to
31908 types in the TED corpus), while in English
it is only -25%.
5.2 SMT baseline
Our SMT experiments address the translation of
TED talks from Arabic to English and from En-
glish to French. The training and test datasets
were provided by the organizers of the IWSLT11
evaluation, and are summarized in Table 6.
Marked in bold are the corpora used for hybrid
LM training. Dev and test sets have a single ref-
erence translation.
For both language pairs, we set up com-
petitive phrase-based systems6 using the Moses
toolkit (Koehn et al 2007). The decoder fea-
tures a statistical log-linear model including a
phrase translation model and a phrase reordering
model (Tillmann, 2004; Koehn et al 2005), two
word-based language models, distortion, word
and phrase penalties. The translation and re-
ordering models are obtained by combining mod-
els independently trained on the available paral-
6The SMT systems used in this paper are thoroughly de-
scribed in (Ruiz et al 2011).
443
Corpus |S| |W | `
AR-EN
TED 90K 1.7M 18.9
UN 7.9M 220M 27.8
EN
TED 124K 2.4M 19.5
NEWS 30.7M 782M 25.4
AR test
dev2010 934 19K 20.0
tst2010 1664 30K 18.1
EN-FR
TED 105K 2.0M 19.5
UN 11M 291M 26.5
NEWS 111K 3.1M 27.6
FR
TED 107K 2.2M 20.6
NEWS 11.6M 291M 25.2
EN test
dev2010 934 20K 21.5
tst2010 1664 32K 19.1
Table 6: IWSLT11 training and test data statistics:
number of sentences |S|, number of tokens |W | and
average sentence length `. Token numbers are com-
puted on the target language, except for the test sets.
lel corpora: namely TED and NEWS for Arabic-
English; TED, NEWS and UN for English-
French. To this end we applied the fill-up method
(Nakov, 2008; Bisazza et al 2011) in which out-
of-domain phrase tables are merged with the in-
domain table by adding only new phrase pairs.
Out-of-domain phrases are marked with a binary
feature whose weight is tuned together with the
SMT system weights.
For each target language, two standard 5-gram
LMs are trained separately on the monolingual
TED and NEWS datasets, and log-linearly com-
bined at decoding time. In the Arabic-English
task, we use a hierarchical reordering model (Gal-
ley and Manning, 2008; Hardmeier et al 2011),
while in the English-French task we use a default
word-based bidirectional model. The distortion
limit is set to the default value of 6. Note that
the use of large n-gram LMs and of lexicalized
reordering models was shown to wipe out the im-
provement achievable by POS-level LM (Kirch-
hoff and Yang, 2005; Birch et al 2007).
Concerning data preprocessing we apply stan-
dard tokenization to the English and French text,
while for Arabic we use an in-house tokenizer that
removes diacritics and normalizes special charac-
ters and digits. Arabic text is then segmented with
AMIRA (Diab et al 2004) according to the ATB
scheme7. The Arabic-English system uses cased
7The Arabic Treebank tokenization scheme isolates con-
junctions w+ and f+, prepositions l+, k+, b+, future marker
s+, pronominal suffixes, but not the article Al+.
translation models, while the English-French sys-
tem uses lowercased models and a standard re-
casing post-process.
Feature weights are tuned on dev2010 by
means of a minimum error training procedure
(MERT) (Och, 2003). Following suggestions by
Clark et al(2011) and Cettolo et al(2011) on
controlling optimizer instability, we run MERT
four times on the same configuration and use the
average of the resulting weights to evaluate trans-
lation performance.
5.3 Hybrid LM integration
As previously stated, hybrid LMs are trained only
on in-domain data and are added to the log-linear
decoder as an additional target LM. To this end,
we use the class-based LM implementation pro-
vided in Moses and IRSTLM, which applies the
word-to-class mapping to translation hypotheses
before LM querying8. The order of the additional
LM is set to 10 in the Arabic-English evaluation
and 7 in the English-French, as these appeared to
be the best settings in preliminary tests.
Translation quality is measured by BLEU (Pa-
pineni et al 2002), METEOR (Banerjee and
Lavie, 2005) and TER (Snover et al 2006)9. To
test whether differences among systems are statis-
tically significant we use approximate randomiza-
tion as done in (Riezler and Maxwell, 2005)10.
Model variants. The effect on MT quality of
various hybrid LM variants is shown in Table 7.
Note that allPOS and allLemmas refer to deter-
ministically assigned POS tags and lemmas, re-
spectively. Concerning the ratio of POS-mapped
tokens, the best performing values are WP =.25 in
Arabic-English and WP =.50 in English-French.
These hybrid mappings outperform all the uni-
form representations (words, lemmas and POS)
with statistically significant BLEU and METEOR
improvements.
The fdf experiment involves the use of doc-
ument frequency for the selection of common
words. Its performance is very close to that of hy-
8Detailed instructions on how to build and use hybrid
LMs can be found at http://hlt.fbk.eu/people/bisazza.
9We use case-sensitive BLEU and TER, but case-
insensitive METEOR to enable the use of paraphrase tables
distributed with the tool (version 1.3).
10Translation scores and significance tests were com-
puted with the Multeval toolkit (Clark et al 2011):
https://github.com/jhclark/multeval.
444
(a) Arabic to English, IWSLT?tst2010
Added InDomain 10gLM BLEU?MET ? TER ?
.00 POS/words (all words)? 26.1 30.5 55.4
.00 POS/lemmas (all lem.) 26.0 30.5 55.4
1.0 POS/words (all POS)? 25.9 30.6 55.3
.25 POS/words? 26.5 30.6 54.7
.50 POS/words 26.5 30.6 54.9
.75 POS/words 26.3 30.7 55.0
.25 POS/words(fdf) 26.5 30.7 54.7
.25 POS/lemmaF 26.4 30.6 54.8
.25 POS/lemmas 26.5 30.8 54.6
(b) English to French, IWSLT?tst2010
Added InDomain 7gLM BLEU?MET ? TER ?
.00 POS/words (all words) 31.1 52.5 49.9
.00 POS/lemmas (all lem.)? 31.2 52.6 49.7
1.0 POS/words (all POS)? 31.4 52.8 49.8
.25 POS/lemmas? 31.5 52.9 49.7
.50 POS/lemmas 31.9 53.3 49.5
.75 POS/lemmas 31.7 53.2 49.6
.50 POS/lemmas(fdf) 31.9 53.3 49.5
.50 POS/lemmaF 31.6 53.0 49.6
.50 POS/words 31.7 53.1 49.5
Table 7: Comparison of various hybrid LM variants. Translation quality is measured with BLEU, METEOR and
TER (all in percentage form). The settings used for weight tuning are marked with ?. Best models according to
all metrics are highlighted in bold.
brid LMs simply based on term frequency; only
METEOR gains 0.1 points in Arabic-English. A
possible reason for this is that document fre-
quency was computed on fixed-size text chunks
rather than on real document boundaries (see Sec-
tion 4.1). The lemmaF experiment refers to the
use of canonical forms for frequency measuring:
this technique does not seem to help in either lan-
guage pair. Finally, we compare the use of lem-
mas versus surface forms to represent common
words. As expected, lemmas appear to be help-
ful for French language modeling. Interestingly
this is also the case for English, even if by a small
margin (+0.2 METEOR, -0.1 TER).
Summing up, hybrid mapping appears as a
winning strategy compared to uniform map-
ping. Although differences among LM variants
are small, the best model in Arabic-English is
.25-POS/lemmas, which can be thought of as
a domain-generic lemma-level LM. In English-
French, instead, the highest scores are achieved
by .50-POS/lemmas or .50-POS/lemmas(fdf), that
is POS-level LM with few frequently occurring
lexical anchors (vocabulary size 59). An inter-
pretation of this result is that, for French, mod-
eling the syntax is more helpful than modeling
the style. We also suspect that the French TED
corpus is more irregular and diverse with respect
to the style, than its English counterpart. In fact,
while the English corpus include transcripts of
talks given by English speakers, the French one is
mostly a collection of (human) translations. Typi-
cal features of the speech style may have been lost
in this process.
Comparison with baseline. In Table 8 the
best performing hybrid LM is compared against
the baseline that only includes the standard LMs
described in Section 5.2. To complete our eval-
uation, we also report the effect of an in-domain
LM trained on 50 word classes induced from the
corpus by maximum-likelihood based clustering
(Och, 1999).
In the two language pairs, both types of LM
result in consistent improvements over the base-
line. However, the gains achieved by the hybrid
approach are larger and all statistically signifi-
cant. The hybrid approach is significantly bet-
ter than the unsupervised one by TER in Arabic-
English and by BLEU and METEOR in English-
French (these siginificances are not reported in
(a) Arabic to English, IWSLT?tst2010
Added InDomain
BLEU? MET ? TER ?
10g LM
none (baseline) 26.0 30.4 55.6
unsup. classes 26.4? 30.8? 55.1?
hybrid 26.5?(+.5) 30.8?(+.4) 54.6?(-1.0)
(b) English to French, IWSLT?tst2010
Added InDomain
BLEU? MET ? TER ?
7g LM
none (baseline) 31.2 52.7 49.8
unsup. classes 31.5 52.9 49.6
hybrid 31.9?(+.7) 53.3?(+.6) 49.5?(-.3)
Table 8: Final MT results: baseline vs unsupervised
word classes-based LM and best hybrid LM. Statis-
tically significant improvements over the baseline are
marked with ? at the p < .01 and ? at the p < .05 level.
445
the table for clarity). The proposed method ap-
pears to better leverage the available in-domain
data, achieving improvements according to all
metrics: +0.5/+0.4/-1.0 BLEU/METEOR/TER
in Arabic-English and +0.7/-0.6/-0.3 in English-
French, without requiring any bitext annotation or
decoder modification.
Talk-level analysis. To conclude the study,
we analyze the effect of our best hybrid LM
on Arabic-English translation quality, at the sin-
gle talk level. The test used in the experiments
(tst2010) consists of 11 transcripts with an av-
erage length of 151?73 sentences. For each
talk, we compare the baseline BLEU score with
that obtained by adding a .25-POS/lemmas hybrid
LM. Results are presented in Figure 2. The dark
and light columns denote baseline and hybrid-LM
BLEU scores, respectively, and refer to the left y-
axis. Additional data points, plotted on the right
y-axis in reverse order, represent talk-level per-
plexities (PP) of a standard 5-gram LM trained
on TED (?) and those of the .25-POS/lemmas
10-gram hybrid LM (M), computed on reference
translations.
What emerges first is a dramatic variation of
performance among the speeches, with baseline
BLEU scores ranging from 33.95 on talk ?00? to
only 12.42 on talk ?02?. The latter talk appears as
a corner case also according to perplexities (397
by word LM and 111 by hybrid LM). Notably, the
perplexities of the two LMs correlate well with
each other, but the hybrid?s PP is much more sta-
ble across talks: its standard deviation is only 14
?
??
??
??
??
??
??
??
??
?????
???
???
???
???
???
???
???
???
???
???
?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ??
?????? ?????? ??????? ?????
Figure 2: Talk-level evaluation on Arabic-English
(IWSLT-tst2010). Left y-axis: BLEU impact of a .25-
POS/lemma hybrid LM. Right y-axis: perplexities by
word LM and by hybrid LM.
points, while that of the word-based PP is 79. The
BLEU improvement given by hybrid LM, how-
ever modest, is consistent across the talks, with
only two outliers: a drop of -0.2 on talk ?00?, and
a drop of -0.7 on talk ?02?. The largest gain (+1.1)
is observed on talk ?10?, from 16.8 to 17.9 BLEU.
6 Conclusions
We have proposed a language modeling technique
that leverages the in-domain data for SMT style
adaptation. Trained to predict mixed sequences
of POS classes and frequent words, hybrid LMs
are devised to capture typical lexical and syntactic
constructions that characterize the style of speech
transcripts.
Compared to standard language models, hy-
brid LMs generalize better to the test data and
partially compensate for the disproportion be-
tween in-domain and out-of-domain training data.
At the same time, hybrid LMs show more dis-
criminative power than merely POS-level LMs.
The integration of hybrid LMs into a competi-
tive phrase-based SMT system is straightforward
and leads to consistent improvements on the TED
task, according to three different translation qual-
ity metrics.
Target language modeling is only one aspect
of the statistical translation problem. Now that
the usability of the proposed method has been as-
sessed for language modeling, future work will
address the extension of the idea to the modeling
of phrase translation and reordering.
Acknowledgments
This work was supported by the T4ME network
of excellence (IST-249119), funded by the DG
INFSO of the European Commission through the
7th Framework Programme. We thank the anony-
mous reviewers for their valuable suggestions.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
446
Jerome R. Bellegarda. 2004. Statistical language
model adaptation: review and perspectives. Speech
Communication, 42(1):93 ? 108.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical ma-
chine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
9?16, Prague, Czech Republic, June. Association
for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Fed-
erico. 2011. Fill-up versus Interpolation Meth-
ods for Phrase-based SMT Adaptation. In Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467?479.
Mauro Cettolo, Nicola Bertoldi, and Marcello Fed-
erico. 2011. Methods for smoothing the optimizer
instability in SMT. In MT Summit XIII: the Thir-
teenth Machine Translation Summit, pages 32?39,
Xiamen, China.
Jonathan Clark, Chris Dyer, Alon Lavie, and
Noah Smith. 2011. Better hypothesis testing
for statistical machine translation: Controlling
for optimizer instability. In Proceedings of
the Association for Computational Lingustics,
ACL 2011, Portland, Oregon, USA. Associa-
tion for Computational Linguistics. available at
http://www.cs.cmu.edu/ jhclark/pubs/significance.pdf.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004. Automatic Tagging of Arabic Text: From
Raw Text to Base Phrase Chunks. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-
NAACL 2004: Short Papers, pages 149?152,
Boston, Massachusetts, USA, May 2 - May 7. As-
sociation for Computational Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an Open Source Toolkit for
Handling Large Scale Language Models. In Pro-
ceedings of Interspeech, pages 1618?1621, Mel-
bourne, Australia.
Marcello Federico, Luisa Bentivogli, Michael Paul,
and Sebastian Stu?ker. 2011. Overview of the
IWSLT 2011 Evaluation Campaign. In Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 128?135, Prague, Czech Republic, June.
Association for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP ?08: Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 848?856, Morristown, NJ, USA.
Association for Computational Linguistics.
Christian Hardmeier, Jo?rg Tiedemann, Markus Saers,
Marcello Federico, and Mathur Prashant. 2011.
The Uppsala-FBK systems at WMT 2011. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 372?378, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Katrin Kirchhoff and Mei Yang. 2005. Improved lan-
guage modeling for statistical machine translation.
In Proceedings of the ACL Workshop on Building
and Using Parallel Texts, pages 125?128, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Philipp Koehn and Hieu Hoang. 2007. Factored
translation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 868?
876, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proc. of the International Workshop on Spoken
Language Translation, October.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic.
Christof Monz. 2011. Statistical Machine Translation
with Local Language Models. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 869?879, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Preslav Nakov. 2008. Improving English-Spanish
Statistical Machine Translation: Experiments in
Domain Adaptation, Sentence Paraphrasing, Tok-
enization, and Recasing. . In Workshop on Statis-
tical Machine Translation, Association for Compu-
tational Linguistics.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
the 9th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 71?76.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Erhard
Hinrichs and Dan Roth, editors, Proceedings of the
447
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL), pages
311?318, Philadelphia, PA.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance
testing for MT. In Proceedings of the ACL Work-
shop on Intrinsic and Extrinsic Evaluation Mea-
sures for Machine Translation and/or Summariza-
tion, pages 57?64, Ann Arbor, Michigan, June. As-
sociation for Computational Linguistics.
R. Rosenfeld. 2000. Two decades of statistical lan-
guage modeling: where do we go from here? Pro-
ceedings of the IEEE, 88(8):1270 ?1278.
Nick Ruiz and Marcello Federico. 2011. Topic adap-
tation for lecture translation through bilingual la-
tent semantic models. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
294?302, Edinburgh, Scotland, July. Association
for Computational Linguistics.
Nick Ruiz, Arianna Bisazza, Fabio Brugnara, Daniele
Falavigna, Diego Giuliani, Suhel Jaber, Roberto
Gretter, and Marcello Federico. 2011. FBK @
IWSLT 2011. In International Workshop on Spo-
ken Language Translation (IWSLT), San Francisco,
CA.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In 5th Conference of the Association for Machine
Translation in the Americas (AMTA), Boston, Mas-
sachusetts, August.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the
North American Chapter of the Association of Com-
putational Linguistics (HLT-NAACL).
A. Yazgan and M. Sarac?lar. 2004. Hybrid language
models for out of vocabulary word detection in large
vocabulary conversational speech recognition. In
Proceedings of ICASSP, volume 1, pages I ? 745?8
vol.1, may.
448
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 321?324,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Towards Cross-Lingual Textual Entailment
Yashar Mehdad1,2, Matteo Negri1, Marcello Federico1
FBK-Irst1, University of Trento2
Trento, Italy
{mehdad,negri,federico}@fbk.eu
Abstract
This paper investigates cross-lingual textual
entailment as a semantic relation between two
text portions in different languages, and pro-
poses a prospective research direction. We
argue that cross-lingual textual entailment
(CLTE) can be a core technology for sev-
eral cross-lingual NLP applications and tasks.
Through preliminary experiments, we aim at
proving the feasibility of the task, and provid-
ing a reliable baseline. We also introduce new
applications for CLTE that will be explored in
future work.
1 Introduction
Textual Entailment (TE) (Dagan and Glickman,
2004) has been proposed as a generic framework for
modeling language variability. Given two texts T
and H, the task consists in deciding if the meaning
of H can be inferred from the meaning of T. So far,
TE has been only applied in a monolingual setting,
where both texts are assumed to be written in the
same language. In this work, we propose and inves-
tigate a cross-lingual extension of TE, where we as-
sume that T and H are written in different languages.
The great potential of integrating (monolingual)
TE recognition components into NLP architectures
has been reported in several works, such as ques-
tion answering (Harabagiu and Hickl, 2006), infor-
mation retrieval (Clinchant et al, 2006), informa-
tion extraction (Romano et al, 2006), and document
summarization (Lloret et al, 2008).
To the best of our knowledge, mainly due to
the absence of cross-lingual TE (CLTE) recognition
components, similar improvements have not been
achieved yet in any cross-lingual application. As
a matter of fact, despite the great deal of attention
that TE has received in recent years (also witnessed
by five editions of the Recognizing Textual Entail-
ment Challenge1), interest for cross-lingual exten-
sions has not been in the mainstream of TE research,
which until now has been mainly focused on the En-
glish language.
Nevertheless, the strong interest towards cross-
lingual NLP applications (both from the market and
research perspectives, as demonstrated by success-
ful evaluation campaigns such as CLEF2) is, to our
view, a good reason to start investigating CLTE, as
well. Along such direction, research can now ben-
efit from recent advances in other fields, especially
machine translation (MT), and the availability of: i)
large amounts of parallel and comparable corpora in
many languages, ii) open source software to com-
pute word-alignments from parallel corpora, and iii)
open source software to set-up strong MT baseline
systems. We strongly believe that all these resources
can potentially help in developing inference mecha-
nisms on multilingual data.
Building on these considerations, this paper aims
to put the basis for future research on the cross-
lingual Textual Entailment task, in order to allow
for semantic inference across languages in different
NLP tasks. Among these, as a long-term goal, we
plan to adopt CLTE to support the alignment of text
portions that express the same meaning in different
languages. As a possible application scenario, CLTE
1http://pascallin.ecs.soton.ac.uk/Challenges/RTE/
2www.clef-campaign.org/
321
can be used to address content merging tasks in tidy
multilingual environments, such as commercial Web
sites, digital libraries, or user generated content col-
lections. Within such framework, as it will be dis-
cussed in the last section of this paper, CLTE com-
ponents can be used for automatic content synchro-
nization in a concurrent, collaborative, and multilin-
gual editing setting, e.g. Wikipedia.
2 Cross Lingual Textual Entailment
Adapting the definition of TE we define CLTE as
a relation between two natural language portions in
different languages, namely a text T (e.g. in En-
glish), and a hypothesis H (e.g. in French), that
holds if a human after reading T would infer that H
is most likely true, or otherwise stated, the meaning
of H can be entailed (inferred) from T .
We can see two main orthogonal directions for ap-
proaching CLTE: i) simply bring CLTE back to the
monolingual case by translating H into the language
of T, or vice-versa; ii) try to embed cross-lingual
processing techniques inside the TE recognition pro-
cess. In the following, we briefly overview and mo-
tivate each approach.
Basic approaches. The simplest approach is to
add a MT component to the front-end of an existing
TE engine. For instance, let the French hypothesis
H be translated into English and then run the TE en-
gine on T and the translation of H. There are sev-
eral good reasons to follow this divide-and-conquer
approach, as well as some drawbacks. Decoupling
the cross-lingual and the entailment components re-
sults in a simple and modular architecture that, ac-
cording to well known software engineering princi-
ples, results easier to develop, debug, and maintain.
Moreover, a decoupled CLTE architecture would al-
low for easy extensions to other languages as it just
requires extra MT systems. Along the same idea of
pivoting through English, in fact, the same TE sys-
tem can be employed to perform CLTE between any
language pair, once MT is available from each lan-
guage into English. A drawback of the decoupled
approach is that as MT is still far from being perfect,
translation errors are propagated to the TE engine
and might likely affect performance. To cope with
this issue, we explored the alternative approach of
applying TE on a list of n-best translations provided
by the MT engine, and take a final decision based on
some system combination criterion. This latter ap-
proach potentially reduces the impact of translation
errors, but might significantly increase the computa-
tional requirements of CLTE.
Advanced approaches. The idea is to move to-
wards a cross-lingual TE approach that takes advan-
tage of a tighter integration of MT and TE algo-
rithms and techniques. This could result in methods
for recognizing TE across languages without trans-
lating the texts and, in principle, with a lower com-
plexity. When dealing with phrase-based statistical
MT (Koehn et al, 2007), a possible approach is to
extract information from the phrase-table to enrich
the inference and entailment rules which could be
used in a distance based entailment system. As an
example the entailment relations between the French
phrase ?ordinateur portable? and the English phrase
?laptop?, or between the German phrase ?europaeis-
chen union? and the English word ?Europe? could
be captured from parallel corpora through statistical
phrase-based MT approaches.
There are several implications that make this ap-
proach interesting. First of all, we believe that re-
search on CLTE can employ inference mechanisms
and semantic knowledge sources to augment exist-
ing MT methods, leading to improvements in the
translation quality (e.g. (Pado? et al, 2009)). In
addition, the acquired rules could as well enrich
the available multilingual resources and dictionaries
such as MultiWordNet3.
3 Feasibility studies
The main purpose of our preliminary experiments is
to verify the feasibility of CLTE, as well as setting
baseline results to be further improved over time. To
this aim, we started by adopting the basic approach
previously discussed. In particular, starting from an
English/French corpus of T-H pairs, we automati-
cally translated each H fragment from French into
English.
Our decisions build on several motivations. First
of all, the reason for setting English and French
as a first language pair for experiments is to rely
on higher quality translation models, and larger
amounts of parallel data for future improvements.
3http://multiwordnet.fbk.eu/
322
Second, the reason for translating the hypotheses is
that, according to the notion of TE, they are usually
shorter, less detailed, and barely complex in terms of
syntax and concepts with respect to the texts. This
makes them easier to translate preserving the origi-
nal meaning. Finally, from an application-oriented
perspective, working with English Ts seems more
promising due the richness of English data available
(e.g. in terms of language variability, and more de-
tailed elaboration of concepts). This increases the
probability to discover entailment relations with Hs
in other languages.
In order to create a realistic and standard setting,
we took advantage of the available RTE data, select-
ing the RTE3 development set and manually trans-
lating the hypotheses into French. Since the man-
ual translation requires trained translators, and due
to time and logistics constraints, we obtained 520
translated hypotheses (randomly selected from the
entire RTE3 development set) which built our bi-
lingual entailment corpus for evaluation.
In the initial step, following our basic approach,
we translated the French hypotheses to English us-
ing Google4 and Moses5. We trained a phrase-
base translation model using Europarl6 and News
Commentary parallel corpora in Moses, applying a
6-gram language model trained on the New York
Times portion of the English Gigaword corpus7.
As a TE engine , we used the EDITS8 package
(Edit Distance Textual Entailment Suite). This sys-
tem is an open source software package based on
edit distance algorithms, which computes the T-H
distance as the cost of the edit operations (i.e. in-
sertion, deletion and substitution) that are necessary
to transform T into H. By defining the edit distance
algorithm and a cost scheme (i.e. which defines the
costs of each edit operation), this package is able to
learn a distance model over a set of training pairs,
which is used to decide if an entailment relation
holds over each test pair.
In order to obtain a monolingual TE model, we
trained and tuned (Mehdad, 2009) our model on the
RTE3 test set, to reduce the overfitting bias, since
4http://translate.google.com
5http://www.statmt.org/moses/
6http://www.statmt.org/europarl/
7http://www.ldc.upenn.edu
8http://edits.fbk.eu/
our original data was created over the RTE3 devel-
opment set. Moreover, we used a set of lexical en-
tailment rules extracted from Wikipedia and Word-
Net, as described in (Mehdad et al, 2009). To be-
gin with, we used this model to classify the cre-
ated cross-lingual entailment corpus in three differ-
ent settings: 1) hypotheses translated by Google, 2)
hypotheses translated by Moses (1st best), and 3) the
original RTE3 monolingual English pairs.
Results reported in Table 1 show that using
Google as a translator, in comparison with the orig-
inal manually-created data, does not cause any drop
in performance. This confirms that merely trans-
lating the hypothesis using a very good translation
model (Google) is a feasible and promising direc-
tion for CLTE. Knowing that Google has one of the
best French-English translation models, the down-
trend of results using Moses translator, in contrast
with Google, is not out of our expectation. Trying
to bridge this gap brings us to the next round of
experiments, where we extracted the n-best trans-
Orig. Google Moses Moses Moses
1st best 30 best > 0.4
Acc. 63.48 63.48 61.37 62.90 62.90
Table 1: Results comparison over 520 test pairs.
lations produced by Moses, to have a richer lexical
variability, beneficial for improving the TE recogni-
tion. The graph in Figure 1 shows an incremental
improvement when the n-best translated hypotheses
are used. Besides that, trying to reach a more mono-
tonic distribution of the results, we normalized the
ranking score (from 0 to 1) given by Moses, and in
each step we chose the first n results over a normal-
ized score. In this way, having the hypotheses with
the score of above 0.4, we achieved the highest accu-
racy of 62.9%. This is exactly equal to adopting the
30-best hypotheses translated by Moses. Using this
method, we could improve the performance up to
1.5% above the 1st best results, achieving almost the
same level of performance obtained with Google.
4 A possible application scenario
Among the many possible applications, the task of
managing textual information in multiple languages
represents an ideal application scenario for CLTE.
Along such direction, our long-term goal is to use
323
Figure 1: Accuracy gained by n-best Moses translations.
CLTE components in the task of synchronizing the
content of documents about the same topic (e.g.
Wikipedia articles), written in different languages.
Currently, multilingual Wikis rely on users to manu-
ally translate different Wiki pages on the same sub-
ject. This is not only a time-consuming procedure
but also the source of many inconsistencies, as users
update the different language versions separately,
and every update would require translators to com-
pare the different language versions and synchronize
the updates. Our goal is to automate this process
by integrating MT and CLTE in a two-step process
where: i) CLTE is used to identify text portions that
should ?migrate? from one page to the other, and ii)
MT is used to actually translate these portions in the
appropriate target language.
The adoption of entailment-based techniques to
address the multilingual content synchronization
task looks promising, as several issues inherent to
such task can be formalized as TE-related problems.
Given two pages (P1 and P2), these issues include
identifying (and then properly managing):
1. Text portions in P1 and P2 that express exactly
the same meaning (bi-directional entailment, or se-
mantic equivalence) and which should not migrate
across pages;
2. Text portions in P1 that are more specific than
portions of P2 (unidirectional entailment between
P2 and P1 or vice-versa) and should replace them;
3. Text portions in P1 describing facts that are not
present in P2, and which should be added in P2 or
vice-versa (the ?unknown? cases in RTE parlance);
4. Meaning discrepancies between text portions
in P1 and text portions in P2 (?contradictions? in
RTE parlance).
5 Conclusion
This paper presented a preliminary investigation to-
wards cross-lingual Textual Entailment, focusing on
possible research directions and alternative method-
ologies. Baseline results have been provided to
demonstrate the potentialities of a simple approach
that integrates MT and monolingual TE compo-
nents. Overall, our work sets a novel framework
for further studies and experiments to improve cross-
lingual NLP tasks. In particular, CLTE can be scaled
to more complex problems, such as cross-lingual
content merging and synchronization.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853)
References
S. Clinchant, C. Goutte, and E. Gaussier. 2006. Lex-
ical entailment for information retrieval. In Proc.
ECIR?06.
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. Proc. of the PASCAL Workshop of Learn-
ing Methods for Text Understanding and Mining.
S. Harabagiu and A. Hickl. 2006. Methods for using tex-
tual entailment in open-domain question answering.
In Proc. COLING/ACL 2006.
P. Koehn et al 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. ACL07 Demo
and Poster Sessions.
E. Lloret, O?. Ferra?ndez, R. Mun?oz, and M. Palomar.
2008. A text summarization approach under the in-
fluence of textual entailment. In Proc. NLPCS 2008.
Y. Mehdad, M. Negri, E. Cabrio, M. Kouylekov, and
B. Magnini. 2009. Edits: An open source framework
for recognizing textual entailment. In Proc. TAC 2009.
To appear.
Yashar Mehdad. 2009. Automatic cost estimation for
tree edit distance using particle swarm optimization.
In Proc. ACL ?09.
S. Pado?, M. Galley, D. Jurafsky, and C. D. Manning.
2009. Textual entailment features for machine trans-
lation evaluation. In Proc. StatMT ?09.
L. Romano, M. Kouylekov, I. Szpektor, I. Dagan, and
A. Lavelli. 2006. Investigating a generic paraphrase-
based approach for relation extraction. In Proc. EACL
2006.
324
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 412?419,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Statistical Machine Translation of Texts with Misspelled Words
Nicola Bertoldi Mauro Cettolo Marcello Federico
FBK - Fondazione Bruno Kessler
via Sommarive 18 - 38123 Povo, Trento, Italy
{bertoldi,cettolo,federico}@fbk.eu
Abstract
This paper investigates the impact of mis-
spelled words in statistical machine transla-
tion and proposes an extension of the transla-
tion engine for handling misspellings. The en-
hanced system decodes a word-based confu-
sion network representing spelling variations
of the input text.
We present extensive experimental results on
two translation tasks of increasing complex-
ity which show how misspellings of different
types do affect performance of a statistical ma-
chine translation decoder and to what extent
our enhanced system is able to recover from
such errors.
1 Introduction
With the widespread adoption of the Internet, of
modern communication, multimedia and mobile de-
vice technologies, the amount of multilingual in-
formation distributed and available to anyone, any-
where, has exploded. So called social media have
rapidly reshaped information exchange among Inter-
net users, providing new means of communication
(blogs, tweets, etc.), collaboration (e.g. wikis), and
sharing of multimedia content, and entertainment.
In particular, social media have today become also
an important market for advertisement as well as a
global forum for consumer opinions (Kushal et al,
2003).
The growing spread of user-generated content is
scaling-up the potential demand for on-line machine
translation (MT) but also setting new challenges to
the field of natural language processing (NLP) in
general. The language written and spoken in the
social media presents an impressive variety of con-
tent and styles (Schler et al, 2006), and writing con-
ventions that rapidly evolve over time. Moreover,
much of the content is expressed in informal style,
that more or less violates the standard grammar, con-
tains many abbreviations and acronyms, and finally
many misspelled words. From the point of view of
MT, language of social media is hence very different
from the one represented in the text corpora nowa-
days available to train statistical MT systems.
Facing all these challenges, we pragmatically
scaled down our ambition and decided to investigate
a basic, somehow preliminary, well defined prob-
lem: the impact of misspelled words in statistical
MT. Unintentional typing errors are indeed remark-
ably frequent in online chats, blogs, wikis, reviews,
and hence constitute a major source of noise (Subra-
maniam et al, 2009).
In this paper we aim at studying performance
degradation of statistical MT under different levels
and kinds of noise, and at analyzing to what extent
statistical MT is able to recover from errors by en-
riching its input with spelling variations.
After a brief overview of NLP literature related
to noisy texts, in Section 3 we consider different
types of misspellings and derive simple but realistic
models that are able to reproduce them. Such mod-
els are then used to generate errors in texts passed
to a phrase-based statistical MT system. Next, in
Section 4 we introduce an extension of a statistical
MT system able to handle misspellings by exploiting
confusion network decoding (Bertoldi et al, 2008).
Experiments are reported in Section 5 that in-
412
vestigate the trade-off between complexity of the
extended MT decoder versus translation accuracy.
Moreover, as the proposed model for handling mis-
spellings embeds specific assumptions on how er-
rors are generated, we also measure the robustness
of the enhanced MT decoder with respect to differ-
ent noise sources. Experiments are reported on two
tasks of different complexity, the translation of Eu-
roparl texts and weather bulletins, involving English
and Italian languages.
2 Previous Work
Most contributions addressing NLP of noisy user-
generated content are from the text mining commu-
nity. A survey about the different types of noise that
might affect text mining is in (Subramaniam et al,
2009), while an analysis of how noise phenomena,
commonly occurring in blogs, affect an opinion min-
ing application is in (Dey and Haque, 2009).
Concerning spelling correction literature, many
works apply the noisy channel model which con-
sists of two components: a source model (prior
of word probabilities) and a channel (error) model,
that accounts for spelling transformations on let-
ter sequences. Several approaches have been
proposed under this framework, that mainly dif-
fer in the employed error model; see for exam-
ple: (Church and Gale, 1991), (Brill and Moore,
2000) and (Toutanova and Moore, 2002).
Comprehensive surveys on methods to model and
recover spelling errors can be found in (Kukich,
1992) and (Pedler, 2007); in particular, the latter
work is specifically centered on methods for cor-
recting so-called real-word errors (cf. Section 3).
The detection of errors and the suggestion of cor-
rections typically rely on the availability of text cor-
pora or human-made lexical resources. Search for
correct alternatives can be based on word similarity
measures, such as the edit distance (Mitton, 1995),
anagram hashing (Reynaert, 2006), and semantic
distance based on WordNet (Hirst and Budanitsky,
2005). More sophisticated approaches have been
proposed by (Fossati and Di Eugenio, 2008), that
mixes surface and Part-Of-Speech Information, and
(Schaback and Li, 2007), which combines similarity
measures at the character, phonetic, word, syntax,
and semantic levels into one global feature-based
framework.
a) *W *w had just come in from Australia [Australia]
b) good service we *staid one week. [Tahiti]
c) The room was *exellent but the hallway was *filty .
[NJ]
d) is a good place to stay, if you are looking for a hotel
*arround LAX airport. [Tahiti]
e) The staff was *freindly ... I was *conerned about
the noise [CT]
Table 1: Examples of misspellings found in on-line re-
views of an hotel close to Los Angeles Int?l Airport. Cor-
responding corrections are: a) We, , b) stayed, c) excel-
lent, filthy, d) around, e) friendly, concerned.
Concerning the literature of statistical MT, inter-
est in noisy data has been so far considering is-
sues different from misspelled words. For instance,
(Davis et al, 1995) and (Vogel, 2003) address train-
ing methods coping with noisy parallel data, in the
sense that translations do not perfectly match. Work
on speech translation (Casacuberta et al, 2008) fo-
cused instead on efficient methods to couple speech
recognition and MT in order to avoid error propaga-
tion. Very recently, (Carrera et al, 2009) conducted
a qualitative study on the impact of noisy social me-
dia content on statistical and rule-based MT. Unfor-
tunately, this work does not report any quantitative
result, it is only based on a small selection of exam-
ples that are manually evaluated, and finally it does
not address the problem of integrating error correc-
tion with MT.
3 Types of Misspellings
In general, a misspelled word is a sequence of let-
ters that corresponds to no correctly spelled word of
the same language (non-word error), or to a correct
spelling of another word (real-word error). In the
examples shown in Table 1, all marked errors are
non-word errors, but the one in sentence b), which
indeed is likely a misspelling of the word stayed.
Causes of a misspelling may be an unintentional
typing error (e.g. *freindly for friendly), or lack of
knowledge about the proper spelling. Typing errors
can originate from six different typing operations
(Kukich, 1992): substitution, insertion, deletion,
transposition, run-on, and split.1 Lack of knowledge
could be the cause of the misspelled *exellent in sen-
tence c).
1 Run-on and split are the special cases of deleting and in-
serting blank spaces, respectively.
413
1. your - you?re
2. then - than
3. its - it?s
4. to - too - two
5. were - where - we?re
6. there - their - they?re
7. a - an - and
8. off - of
9. here - hear
10. lose - loose
Table 2: List of frequent real-word errors found in blogs.
Source: http://www.theprobabilist.com.
An interesting combination of cause and effect is
when lack of linguistic competence results in con-
fusing the spelling of a word with the spelling of
another word that sounds similarly (Hirst and Bu-
danitsky, 2005). This could be likely the case of the
Polynesian tourist that authored sentence b).
A short list of words frequently confused in blogs
is reported in Table 2 while a longer list can be found
in the Wikipedia.2 Real-word errors typically fool
spell checkers because their identification requires
analyzing the context in which they occur.
In this paper, we automatically corrupt clean text
with three types of noise described below. This pro-
cedure permits us to analyze the MT performance
against different sources and levels of noise and to
systematically evaluate our error-recovering strat-
egy.
Non-word Noise We randomly replace words in
the text according to a list of 4,100 frequently non-
word errors provided in the Wikipedia. A qualitative
analysis of these errors reveals that all of them origi-
nate by one or two keyboard typing errors of the kind
described beforehand. Practically, non-word noise is
introduced by defining a desired level of corruption
of the source text.
Real-word Noise Similarly to the previous case,
real-word errors are automatically introduced by
another list of frequently misused words in the
Wikipedia. This list contains about 300 pairs of con-
fusable words to which we also added the 10 fre-
quent real-word errors occurring in blogs reported
in Table 2.
2See Wikipedia?s ?list of frequently misused English
words?.
Random Noise Finally, we may corrupt the origi-
nal text by randomly replacing, inserting, and delet-
ing characters in it up to a desired percentage.
4 Error-recovering Statistical MT
An enhancement of a statistical MT system is pro-
posed with the goal of improving robustness to mis-
spellings in the input. Rrror recovery is realized
by performing a sequence of actions before the ac-
tual translation, which create reliable spelling alter-
natives of the input and store them into a compact
word-based Confusion Network (CN).
Starting from the possibly noisy input text,
spelling variations are generated by assuming that
each character is a potential typing error, indepen-
dent from other characters.
The variants are represented as a character-based
CN that models possible substitutions, insertion,
deletions of each character, with an empirically de-
termined weight assigned to each alternative. The
network is then searched by a non-monotonic search
process that scores possible character sequences
through a character n-gram language model, and
outputs a set of multiple spelling variants that is fi-
nally converted into a word-based CN. The result-
ing word-based network is finally passed to the MT
engine. In the following, more details are provided
on the augmented MT system with the help of Fig-
ure 1, which shows how the system acts on the cor-
rupted example ?all off ame?, supposed to be ?hall
of fame?.
Step 1 The input text (a) is split into a sequence
of characters (b) including punctuation marks and
blank spaces ( ), which are here considered as stan-
dard characters. Moreover, single characters inter-
leaved with the conventional empty character .
Step 2 A CN (c) is built by adding all alternative
characters of the keyboard to each input character,
including the space character and the empty char-
acter. When the string character is , the only ad-
mitted alternative is . Possible alternative spellings
of the original string correspond to paths in the CN.
Notice that each CN column beginning with a stan-
dard character permits to manage insertion, substi-
tution and split errors, while each column beginning
with the empty character permits to handle deletion
and run-on errors.
414
...
d
e
?
a
c
b
j
?
e
g
...
d
c
?
b
...
a
e
y
m
?
...
rb
c
d
?
e
a
...
...
s
z
?
w
a
f
...
...
b
?
a
e
?
_
e
d
a
...
c
?
b
f
d
...
c
?
b
?
d
c
b
...
a
e
?
...
r
c
f
d
c
a
...
?
e
b
dk
p
i
?
o
...
e
c
d
...
a
b
?
_
?
a
c
...
b
?
e
d
k
p
l
...
?
o
?
...
b
e
d
a
c
?
o
p
...
l
k
...
i
h ...
z
...
s
a
w
?
d
e
b
?
c
a
...
a
g
c
?
b
........
ah emaf
_
fo
_
ll
la em
_
fo
_
l
a lh emaf
_
ol
uh em
_
ffo
_
ll
ela maf
_
fo
_
l
ema_ffo_lla
arca della gloria
...
hull
...
...
?
hallo
ofhall me
fameoffall
(a)
(b) ?
e
?
m?
a
?
_
?f?
f
?
o
?
_
?
l
?
la
??
(c)
(d)
(e)
(f)
p(w|a) ? 0.91
1
2
3
5
4
Figure 1: The whole process to translate the mistaken
input ?all off ame [hall of fame]? into ?arca della gloria?.
A probability distribution of confusable
keystrokes is generated based on the distance
between the keys on a standard QWERTY key-
board. This distribution is intended to model how a
spelling error is actually produced. Hence, character
alternatives in the CN are associated to a probability
given by:
p (x|y) ? ?
1
k ? d(x, y) + 1
(1)
where d(x, y) is the physical distance between the
key of x and the key of y on the keyboard layout;
for example, the character a has a distance of 3 from
the character c on the considered keyboard layout.
The free parameter k tunes the discriminative power
of the model between correct and wrong typing. In
this paper, k was empirically set to 0.1. The  and
characters are assigned a default distance of 9 and
999 from any other character, respectively.
For the sake of clarity, the probability p(w|a) of
just one entry is reported in Figure 1.
Step 3 The generation of spelling variations (d) is
operated by means of the same decoder employed
for translation (see below), but in a much simplified
configuration which does not exploit any translation
model. It is designed to search the input character-
based CN for the n-best character sequences which
better ?correct? the mistaken input. In Figure 1 the
best sequence is marked by bold boxes (c), and the
empty character  is removed for the sake of clarity
(d). This process relies only on the character-based
6-gram language model trained on monolingual data
in the source language. It is worth noticing that the
generated spelling alternatives may in principle still
contain non-words, just because they are selected by
a character-based language model, which does not
explicitly embed the notion of word.
Transposition errors are modeled both (i) indi-
rectly through consecutive substitutions with appro-
priate characters and (ii) directly by permitting some
re-orderings of adjacent characters. Moreover, pre-
liminary experiments revealed that the explicit han-
dling of deletion and run-on errors by interleaving
input characters with the empty character  (Step 1)
is crucial to achieve good performance. Although
the size of the character-based CN doubles, its de-
coding time increases only by a small factor.
Step 4 The n-best character sequences (d) are
transformed into a word-based CN (e) (Mangu et
al., 2000). First, each character-based sequence is
transformed into a unifilar word-based lattice, whose
edges correspond to words and timestamps to the
character positions. Then, the unifilar lattices are put
in parallel to create one lattice with all spelling vari-
ations of the input text (a). Finally, a word-based CN
is generated by means of the lattice-tool available in
the SRILM Toolkit (Stolcke, 2002).
Step 5 Translation of the CN (e) is performed
with the Moses decoder (Koehn et al, 2007), that
has been successfully applied mainly to text trans-
lation, but also to process multiple input hypothe-
ses (Bertoldi et al, 2008), representing, for exam-
ple, speech transcriptions, word segmentations, texts
with possible punctuation marks, etc. In general,
415
set #sent. English Italian
#wrd dict. #wrd dict.
EP train 1.2M 36M 106K 35M 146K
test 2K 60K 6.5K 60K 8.3K
WF train 42K 996K 2641 994K 2843
test 328 8789 606 8704 697
Table 3: Statistics of train/test data of the Europarl (EP)
and the Weather Forecast (WF) tasks.
Moses looks for the best translation exploring the
search space defined by a set of feature functions
(models), which are log-linearly interpolated with
weights estimated during a tuning stage.
The rationale of storing the spelling alternatives
into a word-based CN instead of n-best list is two-
fold: (i) the CN contains a significantly larger num-
ber of variations, and (ii) the translation system is
much more efficient to translate CNs instead of n-
best lists.
5 Experiments
Extensive experiments have been conducted on the
Europarl shared task, from English to Italian, as
specified by the Workshop on Statistical Machine
Translation of the ACL 2008.3 Additional experi-
ments were conducted on a smaller task, namely the
translation of weather forecast bulletins between the
same language pair. Statistics on texts employed in
experiments are reported in Table 3.
For both tasks, we created evaluation data by ar-
tificially corrupting input text with the noise sources
described in Section 3. The module for generating
spelling variations (Step 3) was trained on additional
4M and 16M running words in English and Italian,
respectively.
We empirically investigated the following issues:
(a) performance of the standard MT engine versus
nature and level of the input noise; (b) performance
of the error-recovering MT engine versus number of
provided spelling variations; (c) portability of the
approach to another task and translation direction;
(d) computational requirements of the approach.
5.1 Impact of Noise
The first set of experiments involved the translation
of corrupted versions of the Europarl test set. Fig-
3http://www.statmt.org/wmt08/
 10
 15
 20
 25
20105210.50
 10
 15
 20
 25
B
L
E
U
Noise Level (%)
baselinerandom, no-recoverynon-word, no-recoveryreal-word, no-recovery
Figure 2: Translation performance as function of the
noise level (in log-scale) for different types of noise.
ure 2 plots three curves of BLEU(%) scores, corre-
sponding to different noise sources and noise ratios,
given in terms of percentage of word error rate. It
also shows the BLEU score on the original clean
text. Notice that this baseline performance (25.16)
represents the state-of-the-art4 for this task.
The major outcome of these experiments is that
the different types of errors seem to affect MT per-
formance in a very similar manner. Quantitatively,
performance degradation begins even for low noise
levels ? about 0.5 absolute BLEU loss at 1% of
noise level ? and reaches 50% when text corruption
reaches the level of 30%. The similar impact of non-
word and random errors is somehow expected. The
plain reason is that both types of errors very likely5
generate Out-Of-Vocabulary (OOV) words.
We find instead less predictable that the impact of
real-word errors is indistinguishable from that of the
other two noise sources. Notice also that most of the
real-word errors produce indeed words known to the
MT system. Hence, the question regards the behav-
ior of the MT system when the sentence includes on
OOV word or an out-of-context known word. Em-
pirically it seems that in both cases the decoder pro-
duces translations with the same amount of errors.
In some sense, the good news is that real-word er-
rors do not induce more translation errors than OOV
words do.
4http://matrix.statmt.org/matrix
5Modulo noise in the parallel data and the chance that a ran-
dom error generates a true word.
416
 15
 20
 25
5020105210.50
 15
 20
 25
B
L
E
U
Noise Level (%)
baselineno-recoverysinglemultiple, 200  20
 25
105210.50
 20
 25
B
L
E
U
Noise Level (%)
baselineno-recoverysinglemultiple, 200
Figure 3: Performance of error-recovering method with random (left) and real-word (right) noise.
5.2 Impact of Multiple Corrections
Experiments presented here address evaluation of
our enhanced MT system. In addition to nature and
level of noise, translation performance is also an-
alyzed with respect to the number (1 and 200) of
spelling alternatives generated at Step 3. Figure 3
plots BLEU scores for random (left plot) and real-
word (right plot) noises. For comparison purposes,
the curves with no error recovery are also shown.
Results with non-word noise are not provided since
they are pretty similar to those with random noise.
It is worth noticing that real-word errors are re-
covered in a different way than random errors; in
fact, for the latter a single spelling alternative seems
sufficient to guarantee a substantial error recovery,
whereas for real-word errors this is not the case.
Concerning the use of spelling variations, it is
worth remarking that our system is able to fully re-
cover from both random and non-word errors up to
noise levels of 10%, which remains high even for
noise levels up to 20%, where the BLEU degrada-
tion is limited to around 5% relative.
Real-word errors are optimally recovered in the
case of multiple spelling variations until they do not
exceed 2% of the words in the input text; after that,
the decrement of the MT quality becomes signif-
icant but still limited to about 5% BLEU relative
for a noise level of 10%. So the question arises
about what could be a realistic real-word noise level.
Clearly this question is not easy to address. How-
ever, to get a rough idea we can look at the exam-
ples reported in Table 1. These five sentences were
extracted from a text of about 100 words (of which
Table 1 only shows the sentences containing errors)
that contain in total 8 errors: 7 of which are non-
words and 1 is a real-word. Although from these
figures reliable statistics cannot be estimated, a rea-
sonable assumption could be that a global noise level
of 10%6 might contain a 1/10 ratio for real-word vs.
non-word errors. Thus, looking at the real-word er-
ror curve of Figure 3, the inability to recover errors
for noise levels greater than 2-5% should actually be
acceptable given this empirical observation.
Another relevant remark from Figure 3 is that
for low noise levels (less than 1%) the use of the
error-recovering module is counterproductive, since
it introduces more errors than those actually affect-
ing the original input text, causing a slight degra-
dation of the translation performance. If the com-
putational cost to generate variants, which will be
analyzed in the next paragraph, is also taken into ac-
count, it results evident the importance of design-
ing a good strategy for enabling or disabling on de-
mand the error-recovering stage. A starting point for
defining an effective activation strategy is the esti-
mation of the noise rate. For doing this, non-words
can be counted by exploiting proper dictionaries or
spell checkers; concerning real-word noise, its rate
can be inferred either from the non-word rate, or by
means of the perplexity, which is expected to be-
come higher as the real-word error rate increases
(Subramaniam et al, 2009). Once the noise level
of the input text is known, the decision of activat-
ing the correction module can be easily taken on a
6By the way, at this noise rate, an error-recovering strategy
would be highly recommended.
417
 0 10 20
 30 40 50
 60
501010.10  0 10
 20 30 40
 50 60
B
L
E
U
Noise Level (%)
English-Italian
baselineno-recoverymultiple, 200
 0 10 20
 30 40 50
501010.10  0 10
 20 30 40
 50
Noise Level (%)
Italian-English
baselineno-recoverymultiple, 200
Figure 4: Effects of random noise and noise correction
on translation performance for the WF task.
threshold basis. Alternatively, the proper working
point, in terms of precision and recall, of the correc-
tion model could be dynamically chosen as a func-
tion of the actual noise level.
5.3 Computational Costs
Although our investigation does not address explic-
itly computational aspects of translating noisy in-
put, nevertheless some general considerations can be
drawn.
The effectiveness of our recovering approach re-
lies on the compact representation of many spelling
alternatives in a word-based CN. The CN decod-
ing has been shown to be efficient, just minimally
larger than the single string decoding (Bertoldi et
al., 2008). On the contrary, in the current enhanced
MT setting, the sequence of Steps 1 to 4 for build-
ing the CN from the noisy input text is quite costly.
Rather than to an intrinsic complexity, this is due to
our choice of creating a rich character-based CN in
Step 3 for the sake of flexibility and to a naive im-
plementation of Step 4.
5.4 Portability
So far we have analyzed in detail our approach
on the medium-large sized Europarl task, for the
English-to-Italian translation direction. For assess-
ing portability, we also considered a simpler task
?the translation of weather forecast bulletins? where
the translation quality is definitely higher, for the
same language pair but in both translation directions.
The choice of the weather forecast task is not by
chance. In fact, as the automatically translated bul-
letins are published on the Web, a very high transla-
tion quality is required, and then the presence of any
typing error in the original text could be a concern.
(By the way, for this task the presence of real-word
errors is very marginal.)
Figure 4 plots curves of MT performance under
random noise conditions against multiple spelling
variations, for two translation directions. It can
be noticed that the error-recovering system behaves
qualitatively as for the Europarl task but even better
from a quantitative viewpoint. Again, the recovering
model introduces spurious errors which affect trans-
lation quality for low levels of noisy input, but in
this case the break-even point is less than 0.1% noise
level. On the other side, errors corrupting the input
text are fully recovered up to 30-40% of noise lev-
els, for which the BLEU score would be more than
halved for non-corrected texts.
6 Future Work
There are a number of important issues that this
work has still left open. First of all, we focused
on a specific way of generating spelling varia-
tions, based on single characters, but other possible
choices should be investigated and compared to our
approach, like the use of n-grams of words.
An important open question regards efficiency of
the proposed recovering strategy, since the problem
has been only sketched in Section 5.3. It is our in-
tention to analyze the intrinsic complexity of our
model, possibly discover its bottlenecks and imple-
ment a more efficient solution.
Another topic, mentioned in Section 5.2, is the ac-
tivation strategy of the misspelling recovery. Some
further investigation is required on how its working
point can be effectively selected; in fact, since the
enhanced system necessarily introduces spurious er-
rors, it would be desirable to increase its precision
for low-corrupted input texts.
7 Conclusions
This paper addressed the issue of automatically
translating written texts that are corrupted by mis-
spelling errors. An enhancement of a state-of-the-art
statistical MT system is proposed which efficiently
performs the translation of multiple spelling variants
of noisy input. These alternatives are generated by a
character-based error recovery system under the as-
sumption that misspellings are due to typing errors.
The enhanced MT system has been tested on texts
corrupted with increasing noise levels of three dif-
ferent sources: random, non-word, and real-word er-
rors.
418
Analysis of experimental results has led us to
draw the following conclusions:
? The impact of misspelling errors on MT perfor-
mance depends on the noise rate, but not on the
noise source.
? The capability of the enhanced MT system to
recover from errors differs according to the
noise source: real-word noise is significantly
harder to remove than random and non-word
noise, which behave substantially the same.
? The exploitation of several spelling alternatives
permits to almost fully recover from errors if
the noise rate does not exceed 10% for non-
word noise and 2% for real-word noise, which
are likely above the corruption level observed
in many social media.
? Finally, performance slightly decreases when
input text is correct or just mistaken at a negli-
gible level, because the error recovery module
rewards recall rather than precision and hence
tends to overgenerate correction alternatives,
even if not needed.
Acknowledgments
This work was supported by the EuroMatrixPlus
project (IST-231720), which is funded by the EC un-
der the 7th Framework Programme for Research and
Technological Development.
References
N. Bertoldi, et al 2008. Efficient speech translation
through confusion network decoding. IEEE Trans-
actions on Audio, Speech, and Language Processing,
16(8):1696?1705.
E. Brill and R. C. Moore. 2000. An improved error
model for noisy channel spelling correction. In Pro-
ceedings of ACL. Hong Kong.
J. Carrera, et al 2009. Machine trans-
lation for cross-language social media.
http://www.promt.com/company/technology/pdf/mach
ine translation for cross language social media.pdf.
F. Casacuberta, et al 2008. Recent efforts in spoken lan-
guage processing. IEEE Signal Processing Magazine,
25(3):80?88.
K. W. Church and W. A. Gale. 1991. Probability scor-
ing for spelling correction. Statistics and Computing,
1(2):93?103.
M. W. Davis, et al 1995. Text alignment in the real
world: Improving alignments of noisy translations us-
ing common lexical features, string matching strate-
gies and n-gram comparisons. In Proceedings of
EACL, Dublin, Ireland.
L. Dey and S. M. Haque. 2009. Studying the effects of
noisy text on text mining applications. In Proceedings
of AND, pages 107?114, Barcelona, Spain.
D. Fossati and B. Di Eugenio. 2008. I saw tree trees in
the park: How to correct real-word spelling mistakes.
In Proceedings of LREC, Marrakech, Morocco.
G. Hirst and A. Budanitsky. 2005. Correcting real-word
spelling errors by restoring lexical cohesion. Natural
Language Engineering, 11(01):87?111.
P. Koehn, et al 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL
- Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
K. Kukich. 1992. Spelling correction for the telecom-
munications network for the deaf. Communications of
the ACM, 35(5):80?90.
D. Kushal, et al 2003. Mining the peanut gallery:
opinion extraction and semantic classification of prod-
uct reviews. In Proceedings of the WWW conference,
pages 519?528, Budapest, Hungary.
L. Mangu, et al 2000. Finding consensus in speech
recognition: Word error minimization and other appli-
cations of confusion networks. Computer, Speech and
Language, 14(4):373?400.
R. Mitton. 1995. English Spelling and the Computer
(Studies in Language and Linguistics). Addison Wes-
ley Publishing Company.
J. Pedler. 2007. Computer correction of real-word
spelling errors in dyslexic text. Ph.D. thesis, Univer-
sity of London.
M. Reynaert. 2006. Corpus-induced corpus cleanup. In
Proceedings of LREC, Genoa, Italy.
J. Schaback and F. Li. 2007. Multi-level feature extrac-
tion for spelling correction. In IJCAI - Workshop on
Analytics for Noisy Unstructured Text Data, pages 79?
86, Hyderabad, India.
J. Schler, et al 2006. Effects of age and gender on blog-
ging. In Proceedings of AAAI-CAAW, Palo Alto, CA.
A. Stolcke. 2002. Srilm - an extensible language model-
ing toolkit. In Proceedings of ICSLP, Denver, CO.
L. V. Subramaniam, et al 2009. A survey of types of text
noise and techniques to handle noisy text. In Proceed-
ings of AND, pages 115?122, Barcelona, Spain.
K. Toutanova and R. C. Moore. 2002. Pronunciation
modeling for improved spelling correction. In Pro-
ceedings of ACL, pages 144?151, Philadelphia, PA
S. Vogel. 2003. Using noisy biligual data for statisti-
cal machine translation. In Proceedings of EACL, Bu-
dapest, Hungary.
419
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1336?1345,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Bilingual Parallel Corpora
for Cross-Lingual Textual Entailment
Yashar Mehdad
FBK - irst and Uni. of Trento
Povo (Trento), Italy
mehdad@fbk.eu
Matteo Negri
FBK - irst
Povo (Trento), Italy
negri@fbk.eu
Marcello Federico
FBK - irst
Povo (Trento), Italy
federico@fbk.eu
Abstract
This paper explores the use of bilingual par-
allel corpora as a source of lexical knowl-
edge for cross-lingual textual entailment. We
claim that, in spite of the inherent difficul-
ties of the task, phrase tables extracted from
parallel data allow to capture both lexical re-
lations between single words, and contextual
information useful for inference. We experi-
ment with a phrasal matching method in or-
der to: i) build a system portable across lan-
guages, and ii) evaluate the contribution of
lexical knowledge in isolation, without inter-
action with other inference mechanisms. Re-
sults achieved on an English-Spanish corpus
obtained from the RTE3 dataset support our
claim, with an overall accuracy above average
scores reported by RTE participants on mono-
lingual data. Finally, we show that using par-
allel corpora to extract paraphrase tables re-
veals their potential also in the monolingual
setting, improving the results achieved with
other sources of lexical knowledge.
1 Introduction
Cross-lingual Textual Entailment (CLTE) has been
proposed by (Mehdad et al, 2010) as an extension
of Textual Entailment (Dagan and Glickman, 2004)
that consists in deciding, given two texts T and H in
different languages, if the meaning of H can be in-
ferred from the meaning of T. The task is inherently
difficult, as it adds issues related to the multilingual
dimension to the complexity of semantic inference
at the textual level. For instance, the reliance of cur-
rent monolingual TE systems on lexical resources
(e.g. WordNet, VerbOcean, FrameNet) and deep
processing components (e.g. syntactic and semantic
parsers, co-reference resolution tools, temporal ex-
pressions recognizers and normalizers) has to con-
front, at the cross-lingual level, with the limited
availability of lexical/semantic resources covering
multiple languages, the limited coverage of the ex-
isting ones, and the burden of integrating language-
specific components into the same cross-lingual ar-
chitecture.
As a first step to overcome these problems,
(Mehdad et al, 2010) proposes a ?basic solution?,
that brings CLTE back to the monolingual scenario
by translating H into the language of T. Despite the
advantages in terms of modularity and portability of
the architecture, and the promising experimental re-
sults, this approach suffers from one main limitation
which motivates the investigation on alternative so-
lutions. Decoupling machine translation (MT) and
TE, in fact, ties CLTE performance to the availabil-
ity of MT components, and to the quality of the
translations. As a consequence, on one side trans-
lation errors propagate to the TE engine hampering
the entailment decision process. On the other side
such unpredictable errors reduce the possibility to
control the behaviour of the engine, and devise ad-
hoc solutions to specific entailment problems.
This paper investigates the idea, still unexplored,
of a tighter integration of MT and TE algorithms and
techniques. Our aim is to embed cross-lingual pro-
cessing techniques inside the TE recognition pro-
cess in order to avoid any dependency on external
MT components, and eventually gain full control of
the system?s behaviour. Along this direction, we
1336
start from the acquisition and use of lexical knowl-
edge, which represents the basic building block of
any TE system. Using the basic solution proposed
by (Mehdad et al, 2010) as a term of comparison,
we experiment with different sources of multilingual
lexical knowledge to address the following ques-
tions:
(1) What is the potential of the existing mul-
tilingual lexical resources to approach CLTE?
To answer this question we experiment with lex-
ical knowledge extracted from bilingual dictionar-
ies, and from a multilingual lexical database. Such
experiments show two main limitations of these re-
sources, namely: i) their limited coverage, and ii)
the difficulty to capture contextual information when
only associations between single words (or at most
named entities and multiword expressions) are used
to support inference.
(2) Does MT provide useful resources or tech-
niques to overcome the limitations of existing re-
sources? We envisage several directions in which
inputs from MT research may enable or improve
CLTE. As regards the resources, phrase and para-
phrase tables extracted from bilingual parallel cor-
pora can be exploited as an effective way to cap-
ture both lexical relations between single words, and
contextual information useful for inference. As re-
gards the algorithms, statistical models based on co-
occurrence observations, similar to those used inMT
to estimate translation probabilities, may contribute
to estimate entailment probabilities in CLTE. Focus-
ing on the resources direction, the main contribu-
tion of this paper is to show that the lexical knowl-
edge extracted from parallel corpora allows to sig-
nificantly improve the results achieved with other
multilingual resources.
(3) In the cross-lingual scenario, can we achieve
results comparable to those obtained in mono-
lingual TE? Our experiments show that, although
CLTE seems intrinsically more difficult, the results
obtained using phrase and paraphrase tables are bet-
ter than those achieved by average systems on mono-
lingual datasets. We argue that this is due to the
fact that parallel corpora are a rich source of cross-
lingual paraphrases with no equivalents in monolin-
gual TE.
(4) Can parallel corpora be useful also for mono-
lingual TE? To answer this question, we experiment
on monolingual RTE datasets using paraphrase ta-
bles extracted from bilingual parallel corpora. Our
results improve those achieved with the most widely
used resources in monolingual TE, namely Word-
Net, Verbocean, and Wikipedia.
The remainder of this paper is structured as fol-
lows. Section 2 shortly overviews the role of lexical
knowledge in textual entailment, highlighting a gap
between TE and CLTE in terms of available knowl-
edge sources. Sections 3 and 4 address the first three
questions, giving motivations for the use of bilingual
parallel corpora in CLTE, and showing the results of
our experiments. Section 5 addresses the last ques-
tion, reporting on our experiments with paraphrase
tables extracted from phrase tables on the monolin-
gual RTE datasets. Section 6 concludes the paper,
and outlines the directions of our future research.
2 Lexical resources for TE and CLTE
All current approaches to monolingual TE, ei-
ther syntactically oriented (Rus et al, 2005), or
applying logical inference (Tatu and Moldovan,
2005), or adopting transformation-based techniques
(Kouleykov and Magnini, 2005; Bar-Haim et al,
2008), incorporate different types of lexical knowl-
edge to support textual inference. Such information
ranges from i) lexical paraphrases (textual equiva-
lences between terms) to ii) lexical relations pre-
serving entailment between words, and iii) word-
level similarity/relatedness scores. WordNet, the
most widely used resource in TE, provides all the
three types of information. Synonymy relations
can be used to extract lexical paraphrases indicat-
ing that words from the text and the hypothesis en-
tail each other, thus being interchangeable. Hy-
pernymy/hyponymy chains can provide entailment-
preserving relations between concepts, indicating
that a word in the hypothesis can be replaced
by a word from the text. Paths between con-
cepts and glosses can be used to calculate simi-
larity/relatedness scores between single words, that
contribute to the computation of the overall similar-
ity between the text and the hypothesis.
Besides WordNet, the RTE literature documents
the use of a variety of lexical information sources
(Bentivogli et al, 2010; Dagan et al, 2009).
These include, just to mention the most popular
1337
ones, DIRT (Lin and Pantel, 2001), VerbOcean
(Chklovski and Pantel, 2004), FrameNet (Baker et
al., 1998), and Wikipedia (Mehdad et al, 2010;
Kouylekov et al, 2009). DIRT is a collection of sta-
tistically learned inference rules, that is often inte-
grated as a source of lexical paraphrases and entail-
ment rules. VerbOcean is a graph of fine-grained
semantic relations between verbs, which are fre-
quently used as a source of precise entailment rules
between predicates. FrameNet is a knowledge-base
of frames describing prototypical situations, and the
role of the participants they involve. It can be
used as an alternative source of entailment rules,
or to determine the semantic overlap between texts
and hypotheses. Wikipedia is often used to extract
probabilistic entailment rules based word similar-
ity/relatedness scores.
Despite the consensus on the usefulness of lexi-
cal knowledge for textual inference, determining the
actual impact of these resources is not straightfor-
ward, as they always represent one component in
complex architectures that may use them in differ-
ent ways. As emerges from the ablation tests re-
ported in (Bentivogli et al, 2010), even the most
common resources proved to have a positive impact
on some systems and a negative impact on others.
Some previous works (Bannard and Callison-Burch,
2005; Zhao et al, 2009; Kouylekov et al, 2009)
indicate, as main limitations of the mentioned re-
sources, their limited coverage, their low precision,
and the fact that they are mostly suitable to capture
relations mainly between single words.
Addressing CLTE we have to face additional and
more problematic issues related to: i) the stronger
need of lexical knowledge, and ii) the limited avail-
ability of multilingual lexical resources. As regards
the first issue, it?s worth noting that in the monolin-
gual scenario simple ?bag of words? (or ?bag of n-
grams?) approaches are per se sufficient to achieve
results above baseline. In contrast, their applica-
tion in the cross-lingual setting is not a viable so-
lution due to the impossibility to perform direct lex-
ical matches between texts and hypotheses in differ-
ent languages. This situation makes the availability
of multilingual lexical knowledge a necessary con-
dition to bridge the language gap. However, with
the only exceptions represented by WordNet and
Wikipedia, most of the aforementioned resources
are available only for English. Multilingual lexi-
cal databases aligned with the EnglishWordNet (e.g.
MultiWordNet (Pianta et al, 2002)) have been cre-
ated for several languages, with different degrees of
coverage. As an example, the 57,424 synsets of the
Spanish section of MultiWordNet algned to English
cover just around 50% of the WordNet?s synsets,
thus making the coverage issue even more problem-
atic than for TE. As regards Wikipedia, the cross-
lingual links between pages in different languages
offer a possibility to extract lexical knowledge use-
ful for CLTE. However, due to their relatively small
number (especially for some languages), bilingual
lexicons extracted from Wikipedia are still inade-
quate to provide acceptable coverage. In addition,
featuring a bias towards named entities, the infor-
mation acquired through cross-lingual links can at
most complement the lexical knowledge extracted
from more generic multilingual resources (e.g bilin-
gual dictionaries).
3 Using Parallel Corpora for CLTE
Bilingual parallel corpora represent a possible solu-
tion to overcome the inadequacy of the existing re-
sources, and to implement a portable approach for
CLTE. To this aim, we exploit parallel data to: i)
learn alignment criteria between phrasal elements
in different languages, ii) use them to automatically
extract lexical knowledge in the form of phrase ta-
bles, and iii) use the obtained phrase tables to create
monolingual paraphrase tables.
Given a cross-lingual T/H pair (with the text in
l1 and the hypothesis in l2), our approach leverages
the vast amount of lexical knowledge provided by
phrase and paraphrase tables to map H into T. We
perform such mapping with two different methods.
The first method uses a single phrase table to di-
rectly map phrases extracted from the hypothesis to
phrases in the text. In order to improve our system?s
generalization capabilities and increase the cover-
age, the second method combines the phrase table
with two monolingual paraphrase tables (one in l1,
and one in l2). This allows to:
1. use the paraphrase table in l2 to find para-
phrases of phrases extracted from H;
2. map them to entries in the phrase table, and ex-
tract their equivalents in l1;
1338
3. use the paraphrase table in l1 to find para-
phrases of the extracted fragments in l1;
4. map such paraphrases to phrases in T.
With the second method, phrasal matches between
the text and the hypothesis are indirectly performed
through paraphrases of the phrase table entries.
The final entailment decision for a T/H pair is as-
signed considering a model learned from the similar-
ity scores based on the identified phrasal matches.
In particular, ?YES? and ?NO? judgements are as-
signed considering the proportion of words in the
hypothesis that are found also in the text. This way
to approximate entailment reflects the intuition that,
as a directional relation between the text and the hy-
pothesis, the full content of H has to be found in T.
3.1 Extracting Phrase and Paraphrase Tables
Phrase tables (PHT) contain pairs of correspond-
ing phrases in two languages, together with associa-
tion probabilities. They are widely used in MT as a
way to figure out how to translate input in one lan-
guage into output in another language (Koehn et al,
2003). There are several methods to build phrase ta-
bles. The one adopted in this work consists in learn-
ing phrase alignments from a word-aligned bilingual
corpus. In order to build English-Spanish phrase ta-
bles for our experiments, we used the freely avail-
able Europarl V.4, News Commentary and United
Nations Spanish-English parallel corpora released
for the WMT101. We run TreeTagger (Schmid,
1994) for tokenization, and used the Giza++ (Och
and Ney, 2003) to align the tokenized corpora at
the word level. Subsequently, we extracted the bi-
lingual phrase table from the aligned corpora using
the Moses toolkit (Koehn et al, 2007). Since the re-
sulting phrase table was very large, we eliminated
all the entries with identical content in the two lan-
guages, and the ones containing phrases longer than
5 words in one of the two sides. In addition, in or-
der to experiment with different phrase tables pro-
viding different degrees of coverage and precision,
we extracted 7 phrase tables by pruning the initial
one on the direct phrase translation probabilities of
0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting
1http://www.statmt.org/wmt10/
phrase tables range from 76 to 48 million entries,
with an average of 3.9 words per phrase.
Paraphrase tables (PPHT) contain pairs of corre-
sponding phrases in the same language, possibly as-
sociated with probabilities. They proved to be use-
ful in a number of NLP applications such as natural
language generation (Iordanskaja et al, 1991), mul-
tidocument summarization (McKeown et al, 2002),
automatic evaluation of MT (Denkowski and Lavie,
2010), and TE (Dinu and Wang, 2009).
One of the proposed methods to extract para-
phrases relies on a pivot-based approach using
phrase alignments in a bilingual parallel corpus
(Bannard and Callison-Burch, 2005). With this
method, all the different phrases in one language that
are aligned with the same phrase in the other lan-
guage are extracted as paraphrases. After the extrac-
tion, pruning techniques (Snover et al, 2009) can
be applied to increase the precision of the extracted
paraphrases.
In our work we used available2 paraphrase
databases for English and Spanish which have been
extracted using the method previously outlined.
Moreover, in order to experiment with different
paraphrase sets providing different degrees of cov-
erage and precision, we pruned the main paraphrase
table based on the probabilities, associated to its en-
tries, of 0.1, 0.2 and 0.3. The number of phrase pairs
extracted varies from 6 million to about 80000, with
an average of 3.2 words per phrase.
3.2 Phrasal Matching Method
In order to maximize the usage of lexical knowledge,
our entailment decision criterion is based on similar-
ity scores calculated with a phrase-to-phrase match-
ing process.
A phrase in our approach is an n-gram composed
of up to 5 consecutive words, excluding punctua-
tion. Entailment decisions are estimated by com-
bining phrasal matching scores (Scoren) calculated
for each level of n-grams , which is the number
of 1-grams, 2-grams,..., 5-grams extracted from H
that match with n-grams in T. Phrasal matches are
performed either at the level of tokens, lemmas, or
stems, can be of two types:
2http://www.cs.cmu.edu/ alavie/METEOR
1339
1. Exact: in the case that two phrases are identical
at one of the three levels (token, lemma, stem);
2. Lexical: in the case that two different phrases
can be mapped through entries of the resources
used to bridge T and H (i.e. phrase tables, para-
phrases tables, dictionaries or any other source
of lexical knowledge).
For each phrase in H, we first search for exact
matches at the level of token with phrases in T. If
no match is found at a token level, the other levels
(lemma and stem) are attempted. Then, in case of
failure with exact matching, lexical matching is per-
formed at the same three levels. To reduce redun-
dant matches, the lexical matches between pairs of
phrases which have already been identified as exact
matches are not considered.
Once matching for each n-gram level has been
concluded, the number of matches (Mn) and the
number of phrases in the hypothesis (Nn) are used
to estimate the portion of phrases in H that are
matched at each level (n). The phrasal matching
score for each n-gram level is calculated as follows:
Scoren =
Mn
Nn
To combine the phrasal matching scores obtained
at each n-gram level, and optimize their relative
weights, we trained a Support Vector Machine clas-
sifier, SVMlight (Joachims, 1999), using each score
as a feature.
4 Experiments on CLTE
To address the first two questions outlined in Sec-
tion 1, we experimented with the phrase matching
method previously described, contrasting the effec-
tiveness of lexical information extracted from par-
allel corpora with the knowledge provided by other
resources used in the same way.
4.1 Dataset
The dataset used for our experiments is an English-
Spanish entailment corpus obtained from the orig-
inal RTE3 dataset by translating the English hy-
pothesis into Spanish. It consists of 1600 pairs
derived from the RTE3 development and test sets
(800+800). Translations have been generated by
the CrowdFlower3 channel to Amazon Mechanical
Turk4 (MTurk), adopting the methodology proposed
by (Negri and Mehdad, 2010). The method relies
on translation-validation cycles, defined as separate
jobs routed to MTurk?s workforce. Translation jobs
return one Spanish version for each hypothesis. Val-
idation jobs ask multiple workers to check the cor-
rectness of each translation using the original En-
glish sentence as reference. At each cycle, the trans-
lated hypothesis accepted by the majority of trust-
ful validators5 are stored in the CLTE corpus, while
wrong translations are sent back to workers in a
new translation job. Although the quality of the re-
sults is enhanced by the possibility to automatically
weed out untrusted workers using gold units, we per-
formed a manual quality check on a subset of the ac-
quired CLTE corpus. The validation, carried out by
a Spanish native speaker on 100 randomly selected
pairs after two translation-validation cycles, showed
the good quality of the collected material, with only
3 minor ?errors? consisting in controversial but sub-
stantially acceptable translations reflecting regional
Spanish variations.
The T-H pairs in the collected English-Spanish
entailment corpus were annotated using TreeTagger
(Schmid, 1994) and the Snowball stemmer6 with to-
ken, lemma, and stem information.
4.2 Knowledge sources
For comparison with the extracted phrase and para-
phrase tables, we use a large bilingual dictionary
and MultiWordNet as alternative sources of lexical
knowledge.
Bilingual dictionaries (DIC) allow for precise
mappings between words in H and T. To create
a large bilingual English-Spanish dictionary we
processed and combined the following dictionaries
and bilingual resources:
- XDXF Dictionaries7: 22,486 entries.
3http://crowdflower.com/
4https://www.mturk.com/mturk/
5Workers? trustworthiness can be automatically determined
by means of hidden gold units randomly inserted into jobs.
6http://snowball.tartarus.org/
7http://xdxf.revdanica.com/
1340
Figure 1: Accuracy on CLTE by pruning the phrase table
with different thresholds.
- Universal dictionary database8: 9,944 entries.
- Wiktionary database9: 5,866 entries.
- Omegawiki database10: 8,237 entries.
- Wikipedia interlanguage links11: 7,425 entries.
The resulting dictionary features 53,958 entries,
with an average length of 1.2 words.
MultiWordNet (MWN) allows to extract mappings
between English and Spanish words connected by
entailment-preserving semantic relations. The ex-
traction process is dataset-dependent, as it checks
for synonymy and hyponymy relations only between
terms found in the dataset. The resulting collection
of cross-lingual words associations contains 36,794
pairs of lemmas.
4.3 Results and Discussion
Our results are calculated over 800 test pairs of our
CLTE corpus, after training the SVM classifier over
800 development pairs. This section reports the
percentage of correct entailment assignments (accu-
racy), comparing the use of different sources of lex-
ical knowledge.
Initially, in order to find a reasonable trade-off be-
tween precision and coverage, we used the 7 phrase
tables extracted with different pruning thresholds
8http://www.dicts.info/
9http://en.wiktionary.org/
10http://www.omegawiki.org/
11http://www.wikipedia.org/
MWN DIC PHT PPHT Acc. ?
x 55.00 0.00
x 59.88 +4.88
x 62.62 +7.62
x x 62.88 +7.88
Table 1: Accuracy results on CLTE using different lexical
resources.
(see Section 3.1). Figure 1 shows that with the prun-
ing threshold set to 0.05, we obtain the highest re-
sult of 62.62% on the test set. The curve demon-
strates that, although with higher pruning thresholds
we retain more reliable phrase pairs, their smaller
number provides limited coverage leading to lower
results. In contrast, the large coverage obtained with
the pruning threshold set to 0.01 leads to a slight
performance decrease due to probably less precise
phrase pairs.
Once the threshold has been set, in order to
prove the effectiveness of information extracted
from bilingual corpora, we conducted a series of ex-
periments using the different resources mentioned in
Section 4.2.
As it can be observed in Table 1, the highest
results are achieved using the phrase table, both
alone and in combination with paraphrase tables
(62.62% and 62.88% respectively). These results
suggest that, with appropriate pruning thresholds,
the large number and the longer entries contained
in the phrase and paraphrase tables represent an ef-
fective way to: i) obtain high coverage, and ii) cap-
ture cross-lingual associations between multiple lex-
ical elements. This allows to overcome the bias to-
wards single words featured by dictionaries and lex-
ical databases.
As regards the other resources used for compari-
son, the results show that dictionaries substantially
outperform MWN. This can be explained by the
low coverage of MWN, whose entries also repre-
sent weaker semantic relations (preserving entail-
ment, but with a lower probability to be applied)
than the direct translations between terms contained
in the dictionary.
Overall, our results suggest that the lexical knowl-
edge extracted from parallel data can be successfully
used to approach the CLTE task.
1341
Dataset WN VO WIKI PPHT PPHT 0.1 PPHT 0.2 PPHT 0.3 AVG
RTE3 61.88 62.00 61.75 62.88 63.38 63.50 63.00 62.37
RTE5 62.17 61.67 60.00 61.33 62.50 62.67 62.33 61.41
RTE3-G 62.62 61.5 60.5 62.88 63.50 62.00 61.5 -
Table 2: Accuracy results on monolingual RTE using different lexical resources.
5 Using parallel corpora for TE
This section addresses the third and the fourth re-
search questions outlined in Section 1. Building
on the positive results achieved on the cross-lingual
scenario, we investigate the possibility to exploit
bilingual parallel corpora in the traditional monolin-
gual scenario. Using the same approach discussed
in Section 4, we compare the results achieved with
English paraphrase tables with those obtained with
other widely used monolingual knowledge resources
over two RTE datasets.
For the sake of completeness, we report in this
section also the results obtained adopting the ?basic
solution? proposed by (Mehdad et al, 2010). Al-
though it was presented as an approach to CLTE,
the proposed method brings the problem back to the
monolingual case by translating H into the language
of T. The comparison with this method aims at ver-
ifying the real potential of parallel corpora against
the use of a competitive MT system (Google Trans-
late) in the same scenario.
5.1 Dataset
We experiment with the original RTE3 and RTE5
datasets, annotated with token, lemma, and stem in-
formation using the TreeTagger and the Snowball
stemmer.
In addition to confront our method with the solu-
tion proposed by (Mehdad et al, 2010) we translated
the Spanish hypotheses of our CLTE dataset into En-
glish using Google Translate. The resulting dataset
was annotated in the same way.
5.2 Knowledge sources
We compared the results achieved with paraphrase
tables (extracted with different pruning thresh-
olds12) with those obtained using the three most
12We pruned the paraphrase table (PPHT), with probabilities
set to 0.1 (PPHT 0.1), 0.2 (PPHT 0.2), and 0.3 (PPHT 0.3)
widely used English resources for Textual Entail-
ment (Bentivogli et al, 2010), namely:
WordNet (WN). WordNet 3.0 has been used
to extract a set of 5396 pairs of words connected by
the hyponymy and synonymy relations.
VerbOcean (VO). VerbOcean has been used
to extract 18232 pairs of verbs connected by the
?stronger-than? relation (e.g. ?kill? stronger-than
?injure?).
Wikipedia (WIKI). We performed Latent Se-
mantic Analysis (LSA) over Wikipedia using the
jLSI tool (Giuliano, 2007) to measure the relat-
edness between words in the dataset. Then, we
filtered all the pairs with similarity lower than 0.7 as
proposed by (Kouylekov et al, 2009). In this way
we obtained 13760 word pairs.
5.3 Results and Discussion
Table 2 shows the accuracy results calculated over
the original RTE3 and RTE5 test sets, training our
classifier over the corresponding development sets.
The first two rows of the table show that pruned
paraphrase tables always outperform the other lexi-
cal resources used for comparison, with an accuracy
increase up to 3%. In particular, we observe that us-
ing 0.2 as a pruning threshold provides a good trade-
off between coverage and precision, leading to our
best results on both datasets (63.50% for RTE3, and
62.67% for RTE5). It?s worth noting that these re-
sults, compared with the average scores reported by
participants in the two editions of the RTE Challenge
(AVG column), represent an accuracy improvement
of more than 1%. Overall, these results confirm our
claim that increasing the coverage using context sen-
sitive phrase pairs obtained from large parallel cor-
pora, results in better performance not only in CLTE,
1342
but also in the monolingual scenario.
The comparison with the results achieved on
monolingual data obtained by automatically trans-
lating the Spanish hypotheses (RTE3-G row in Ta-
ble 2) leads to four main observations. First, we no-
tice that dealing with MT-derived inputs, the optimal
pruning threshold changes from 0.2 to 0.1, leading
to the highest accuracy of 63.50%. This suggests
that the noise introduced by incorrect translations
can be tackled by increasing the coverage of the
paraphrase table. Second, in line with the findings
of (Mehdad et al, 2010), the results obtained over
the MT-derived corpus are equal to those we achieve
over the original RTE3 dataset (i.e. 63.50%). Third,
the accuracy obtained over the CLTE corpus using
combined phrase and paraphrase tables (62.88%, as
reported in Table 1) is comparable to the best re-
sult gained over the automatically translated dataset
(63.50%). In all the other cases, the use of phrase
and paraphrase tables on CLTE data outperforms
the results achieved on the same data after transla-
tion. Finally, it?s worth remarking that applying our
phrase matching method on the translated dataset
without any additional source of knowledge would
result in an overall accuracy of 62.12%, which is
lower than the result obtained using only phrase ta-
bles on cross-lingual data (62.62%). This demon-
strates that phrase tables can successfully replace
MT systems in the CLTE task.
In light of this, we suggest that extracting lexi-
cal knowledge from parallel corpora is a preferable
solution to approach CLTE. One of the main rea-
sons is that placing a black-box MT system at the
front-end of the entailment process reduces the pos-
sibility to cope with wrong translations. Further-
more, the access to MT components is not easy (e.g.
Google Translate limits the number and the size of
queries, while open source MT tools cover few lan-
guage pairs). Moreover, the task of developing a
full-fledged MT system often requires the availabil-
ity of parallel corpora, and is much more complex
than extracting lexical knowledge from them.
6 Conclusion and Future Work
In this paper we approached the cross-lingual Tex-
tual Entailment task focusing on the role of lexi-
cal knowledge extracted from bilingual parallel cor-
pora. One of the main difficulties in CLTE raises
from the lack of adequate knowledge resources to
bridge the lexical gap between texts and hypothe-
ses in different languages. Our approach builds on
the intuition that the vast amount of knowledge that
can be extracted from parallel data (in the form of
phrase and paraphrase tables) offers a possible so-
lution to the problem. To check the validity of our
assumptions we carried out several experiments on
an English-Spanish corpus derived from the RTE3
dataset, using phrasal matches as a criterion to ap-
proximate entailment. Our results show that phrase
and paraphrase tables allow to: i) outperform the re-
sults achieved with the few multilingual lexical re-
sources available, and ii) reach performance levels
above the average scores obtained by participants in
the monolingual RTE3 challenge. These improve-
ments can be explained by the fact that the lexi-
cal knowledge extracted from parallel data provides
good coverage both at the level of single words, and
at the level of phrases.
As a further contribution, we explored the appli-
cation of paraphrase tables extracted from parallel
data in the traditional monolingual scenario. Con-
trasting results with those obtained with the most
widely used resources in TE, we demonstrated the
effectiveness of paraphrase tables as a mean to over-
come the bias towards single words featured by the
existing resources.
Our future work will address both the extraction
of lexical information from bilingual parallel cor-
pora, and its use for TE and CLTE. On one side,
we plan to explore alternative ways to build phrase
and paraphrase tables. One possible direction is to
consider linguistically motivated approaches, such
as the extraction of syntactic phrase tables as pro-
posed by (Yamada and Knight, 2001). Another in-
teresting direction is to investigate the potential of
paraphrase patterns (i.e. patterns including part-
of-speech slots), extracted from bilingual parallel
corpora with the method proposed by (Zhao et al,
2009). On the other side we will investigate more
sophisticated methods to exploit the acquired lexi-
cal knowledge. As a first step, the probability scores
assigned to phrasal entries will be considered to per-
form weighted phrase matching as an improved cri-
terion to approximate entailment.
1343
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853).
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. Proceedings
of COLING-ACL.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL 2005).
Roy Bar-haim , Jonathan Berant , Ido Dagan , Iddo
Greental , Shachar Mirkin , Eyal Shnarch , and Idan
Szpektor. 2008. Efficient semantic deduction and ap-
proximate matching over compact parse forests. Pro-
ceedings of the TAC 2008 Workshop on Textual Entail-
ment.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The Sixth
PASCAL Recognizing Textual Entailment Challenge.
Proceedings of the the Text Analysis Conference (TAC
2010).
Timothy Chklovski and Patrick Pantel. 2004. Verbocean:
Mining the web for fine-grained semantic verb rela-
tions. Proceedings of Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-04).
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. Proceedings of the PASCAL Workshop of
Learning Methods for Text Understanding and Min-
ing.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Journal of Natural Language
Engineering , Volume 15, Special Issue 04, pp i-xvii.
Michael Denkowski and Alon Lavie. 2010. Extending
the METEOR Machine Translation Evaluation Metric
to the Phrase Level. Proceedings of Human Language
Technologies (HLT-NAACL 2010).
Georgiana Dinu and Rui Wang. 2009. Inference Rules
and their Application to Recognizing Textual Entail-
ment. Proceedings of the 12th Conference of the Eu-
ropean Chapter of the ACL (EACL 2009).
Claudio Giuliano. 2007. jLSI a tool for la-
tent semantic indexing. Software avail-
able at http://tcc.itc.it/research/textec/tools-
resources/jLSI.html.
Lidija Iordanskaja, Richard Kittredge, and Alain Polg re..
1991. Lexical selection and paraphrase in a meaning
text generation model. Natural Language Generation
in Articial Intelligence and Computational Linguistics.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical.
Philipp Koehn, Franz Josef Och, and Daniel Marcu 2003.
Statistical Phrase-Based Translation. Proceedings of
HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
Milen Kouleykov and Bernardo Magnini. 2005. Tree
edit distance for textual entailment. Proceedings of
RALNP-2005, International Conference on Recent Ad-
vances in Natural Language Processing.
Milen Kouylekov, Yashar Mehdad, and Matteo Negri.
2010. Mining Wikipedia for Large-Scale Repositories
of Context-Sensitive Entailment Rules. Proceedings
of the Language Resources and Evaluation Conference
(LREC 2010).
Yashar Mehdad, Alessandro Moschitti and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic structures
for textual entailment recognition. Proceedings of the
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2010).
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text.. Proceedings of ACM
Conference on Knowledge Discovery and Data Mining
(KDD-01).
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with Columbias Newsblaster. Proceed-
ings of the Human Language Technology Conference..
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment.
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. Proceedings of COL-
ING.
Matteo Negri and Yashar Mehdad. 2010. Creating a Bi-
lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush. Proceed-
ings of the NAACL 2010 Workshop on Creating Speech
and Language Data With Amazons Mechanical Turk .
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):1951.
1344
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: Developing and Aligned
Multilingual Database. Proceedings of the First Inter-
national Conference on Global WordNet.
Vasile Rus, Art Graesser, and Kirtan Desai 2005.
Lexico-Syntactic Subsumption for Textual Entailment.
Proceedings of RANLP 2005.
Helmut Schmid 2005. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. Proceedings of the In-
ternational Conference on New Methods in Language
Processing.
Marta Tatu andDan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP 2005).
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with
a Tunable MT Metric. Proceedings of WMT09.
Rui Wang and Yi Zhang,. 2009. Recognizing Tex-
tual Relatedness with Predicate-Argument Structures.
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP 2009).
Kenji Yamada and Kevin Knight 2001. A Syntax-Based
Statistical Translation Model. Proceedings of the Con-
ference of the Association for Computational Linguis-
tics (ACL).
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2009. Extracting Paraphrase Patterns from Bilingual
Parallel Corpora. Journal of Natural Language Engi-
neering , Volume 15, Special Issue 04, pp 503-526.
1345
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 478?487,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Modified Distortion Matrices
for Phrase-Based Statistical Machine Translation
Arianna Bisazza and Marcello Federico
Fondazione Bruno Kessler
Trento, Italy
{bisazza,federico}@fbk.eu
Abstract
This paper presents a novel method to suggest
long word reorderings to a phrase-based SMT
decoder. We address language pairs where
long reordering concentrates on few patterns,
and use fuzzy chunk-based rules to predict
likely reorderings for these phenomena. Then
we use reordered n-gram LMs to rank the re-
sulting permutations and select the n-best for
translation. Finally we encode these reorder-
ings by modifying selected entries of the dis-
tortion cost matrix, on a per-sentence basis.
In this way, we expand the search space by a
much finer degree than if we simply raised the
distortion limit. The proposed techniques are
tested on Arabic-English and German-English
using well-known SMT benchmarks.
1 Introduction
Despite the large research effort devoted to the mod-
eling of word reordering, this remains one of the
main obstacles to the development of accurate SMT
systems for many language pairs. On one hand, the
phrase-based approach (PSMT) (Och, 2002; Zens et
al., 2002; Koehn et al, 2003), with its shallow and
loose modeling of linguistic equivalences, appears
as the most competitive choice for closely related
language pairs with similar clause structures, both
in terms of quality and of efficiency. On the other,
tree-based approaches (Wu, 1997; Yamada, 2002;
Chiang, 2005) gain advantage, at the cost of higher
complexity and isomorphism assumptions, on lan-
guage pairs with radically different word orders.
Lying between these two extremes are language
pairs where most of the reordering happens locally,
and where long reorderings can be isolated and de-
scribed by a handful of linguistic rules. Notable
examples are the family-unrelated Arabic-English
and the related German-English language pairs. In-
terestingly, on these pairs, PSMT generally pre-
vails over tree-based SMT1, producing overall high-
quality outputs and isolated but critical reordering
errors that undermine the global sentence meaning.
Previous works on this type of language pairs have
mostly focused on source reordering prior to trans-
lation (Xia and McCord, 2004; Collins et al, 2005),
or on sophisticated reordering models integrated into
decoding (Koehn et al, 2005; Al-Onaizan and Pap-
ineni, 2006), achieving mixed results. To merge the
best of both approaches ? namely, access to rich con-
text in the former and natural coupling of reorder-
ing and translation decisions in the latter ? we intro-
ducemodified distortion matrices: a novel method to
seamlessly provide to the decoder a set of likely long
reorderings pre-computed for a given input sentence.
Added to the usual space of local permutations de-
fined by a low distortion limit (DL), this results in a
linguistically informed definition of the search space
that simplifies the task of the in-decoder reordering
model, besides decreasing its complexity.
The paper is organized as follows. After review-
ing a selection of relevant works, we analyze salient
reordering patterns in Arabic-English and German-
English, and describe the corresponding chunk-
based reordering rule sets. In the following sections
we present a reordering selection technique based on
1A good comparison of phrase-based and tree-based ap-
proaches across language pairs with different reordering levels
can be found in (Zollmann et al, 2008).
478
reordered n-gram LMs and, finally, explain the no-
tion of modified distortion matrices. In the last part
of the paper, we evaluate the proposed techniques on
two popular MT tasks.
2 Previous work
Pre-processing approaches to word reordering aim
at permuting input words in a way that minimizes
the reordering needed for translation: determinis-
tic reordering aims at finding a single optimal re-
ordering for each input sentence, which is then
translated monotonically (Xia and McCord, 2004)
or with a low DL (Collins et al, 2005; Habash,
2007); non-deterministic reordering encodes mul-
tiple alternative reorderings into a word lattice and
lets a monotonic decoder find the best path accord-
ing to its models (Zhang et al, 2007; Crego and
Habash, 2008; Elming and Habash, 2009; Niehues
and Kolss, 2009). The latter approaches are ideally
conceived as alternative to in-decoding reordering,
and therefore require an exhaustive reordering rule
set. Two recent works (Bisazza and Federico, 2010;
Andreas et al, 2011) opt instead for a hybrid way:
rules are used to generate multiple likely reorder-
ings, but only for a specific phenomenon ? namely
verb-initial clauses in Arabic. This yields sparse re-
ordering lattices that can be translated with a regular
decoder performing additional reordering.
Reordering rules for pre-processing are either
manually written (Collins et al, 2005) or automat-
ically learned from syntactic parses (Xia and Mc-
Cord, 2004; Habash, 2007; Elming and Habash,
2009), shallow syntax chunks (Zhang et al, 2007;
Crego and Habash, 2008) or part-of-speech labels
(Niehues and Kolss, 2009). Similarly to hybrid ap-
proaches, in this work we use few linguistically in-
formed rules to generate multiple reorderings for se-
lected phenomena but, as a difference, we do not
employ lattices to represent them. We also include a
competitive in-decoding reordering model in all the
systems used to evaluate our methods.
Another large body of work is devoted to the mod-
eling of reordering decisions inside decoding, based
on a decomposition of the problem into a sequence
of basic reordering steps. Existing approaches range
from basic linear distortion to more complex models
that are conditioned on the words being translated.
The linear distortion model (Koehn et al, 2003)
encourages monotonic translations by penalizing
source position jumps proportionally to their length.
If used alone, this model is inadequate for language
pairs with different word orders. Green et al (2010)
tried to improve it with a future distortion cost es-
timate. Thus they were able to preserve baseline
performance at a very high DL, but not to improve
it. Lexicalized phrase orientation models (Tillmann,
2004; Koehn et al, 2005; Zens and Ney, 2006; Gal-
ley and Manning, 2008) predict the orientation of a
phrase with respect to the last translated one. These
models are known to well handle local reordering
and are widely adopted by the PSMT community.
However, they are unsuitable to model long reorder-
ing as they classify as ?discontinuous? every phrase
that does not immediately follow or precede the last
translated one. Lexicalized distortion models pre-
dict the jump from the last translated word to the
next one, with a class for each possible jump length
(Al-Onaizan and Papineni, 2006), or bin of lengths
(Green et al, 2010). These models are conceived to
deal with long reordering, but can easily suffer from
data sparseness, especially for longer jumps occur-
ring less frequently.
Following a typical sequence modeling approach,
Feng et al (2010) train n-gram language models on
source data previously reordered in accordance to
the target language translation. This method does
not directly model reordering decisions, but rather
word sequences produced by them. Despite their
high perplexities, reordered LMs yield some im-
provements when integrated to a PSMT baseline that
already includes a discriminative phrase orientation
model (Zens and Ney, 2006). In this work we use
similar models to rank sets of chunk permutations.
Attempting to improve the reordering space def-
inition, Yahyaei and Monz (2010) train a classifier
to guess the most likely jump length at each source
position, then use its predictions to dynamically set
the DL. Translation improvements are obtained on a
simple task with mostly short sentences (BTEC).
Modifying the distortion function, as proposed in
this paper, makes it possible to expand the pemuta-
tion search space by a much finer degree than vary-
ing the DL does.
479
3 Long reordering patterns
Our study focuses on Arabic-English and German-
English: two language pairs characterized by uneven
distributions of word-reordering phenomena, with
long-range movements concentrating on few pat-
terns. In Arabic-English, the internal order of most
noun phrases needs to be reversed during translation,
which is generally well handled by phrase-internal
reordering or local distortion. At the constituent
level, instead, Arabic admits both SV(O) and VS(O)
orders, the latter causing problematic long reorder-
ings. Common errors due to this issue are the ab-
sence of main verb in the English translation, or the
placement of the main verb before its own subject.
In both cases, adequacy is seriously compromised.
In German-English, the noun phrase structure is
similar between source and target languages. How-
ever, at the constituent level, the verb-second order
of German main clauses conflicts with the rigid SVO
structure of English, as does the clause-final verb
position of German subordinate clauses. As a fur-
ther complication, German compound verbs are split
apart so that the non-finite element (main verb) can
appear long after the inflected auxiliary or modal.
Thanks to sophisticated reordering models, state-
of-the-art PSMT systems are generally good at han-
dling local reordering phenomena that are not cap-
tured by phrase-internal reordering. However, they
typically fail to predict long reorderings. We believe
this is mainly not the fault of the reordering mod-
els, but rather of a too coarse definition of the search
space. To have a concrete idea, consider that a small
change of the DL from 5 to 6 words, in a sentence
of 8, makes the number of explorable permutations
increase from about 9,000 to 22,000. Existing mod-
els cannot be powerful enough to deal with such a
rapidly growing search space.
As a result, decoding at very high DLs is not
a good solution for these language pairs. Indeed,
decent performances are obtained within a low or
mediumDL, but this obviously comes at the expense
of long reorderings, which are often crucial to pre-
serve the general meaning of a translated sentence.
For instance, taking English as the target language,
it is precisely the relative positioning of predicate ar-
guments that determines their role, in the absence of
case markers. Thus, a wrongly reordered verb with
minor impact on automatic scores, can be judged
very badly by a human evaluator.
We will now describe two rule sets aimed at cap-
turing these reordering phenomena.
4 Shallow syntax reordering rules
To compute the source reorderings, we use chunk-
based rules following Bisazza and Federico (2010).
Shallow syntax chunking is indeed a lighter and
simpler task compared to full parsing, and it can
be used to constrain the number of reorderings in
a softer way. While rules based on full parses
are generally deterministic, chunk-based rules are
non-deterministic or fuzzy, as they generate sev-
eral permutations for each matching sequence2. Be-
sides defining a unique segmentation of the sen-
tence, chunk annotation provides other useful infor-
mation that can be used by the rules ? namely chunk
type and POS tags3.
For Arabic-English we apply the rules proposed
by Bisazza and Federico (2010) aimed at transform-
ing VS(O) sentences into SV(O). Reorderings are
generated by moving each verb chunk (VC), alone
or with its following chunk, by 1 to 6 chunks to the
right. The maximum movement of each VC is lim-
ited to the position of the next VC, so that neigh-
boring verb-reordering sequences may not overlap.
This rule set was shown to cover most (99.5%) of
the verb reorderings observed in a parallel news cor-
pus, including those where the verb must be moved
along with an adverbial or a complement.
For German-English we propose a set of three
rules4 aimed at arranging the German constituents
in SVO order:
? infinitive: move each infinitive VC right after a
preceding punctuation;
? subordinate: if a VC is immediately followed
by a punctuation, place it after a preceding sub-
ordinating conjunction (KOUS) or substitutive
relative pronoun (PRELS);
2Chunk annotation does not identify subject and comple-
ment boundaries, nor the relations among constituents that are
needed to deterministically rearrange a sentence in SVO order.
3We use AMIRA (Diab et al, 2004) to annotate Arabic and
Tree Tagger (Schmid, 1994) to annotate German.
4A similar rule set was previously used to produce chunk
reordering lattices in (Hardmeier et al, 2010).
480
????????????????? ??????????????????????????????????
??????????????????????????????????????????????????????
?????????????????????????????????????????????????
?????????????????????????????????? ????????????????????
? ?????????????????
?????????????????????????????????????????????????????
???? ?????????????????????????????????
????????????????????????????????????????????
?????? ?????? ??????? ????????????????????????
? ??????????????????????????????????????????? ?
?? ? ??? ? ? ? ? ? ?? ? ?? ?
?? ? ?? ?? ? ? ? ?? ?
?? ? ? ?? ?
?? ?? ? ? ?
?? ? ? ?? ? ? ?
?? ?? ? ? ? ? ??? ?
?? ?
?? ?
?? ? ?? ?
?? ?
?? ??? ?
?? ??? ??? ?
(a) Arabic VS(O) clause: five permutations
???? ? ??? ?
???? ?
??? ????? ?? ? ? ? ? ?
? ? ? ? ? ?
? ? ? ? ? ? ??? ?
??? ???? ?
??? ?
? ? ???? ? ? ? ? ? ??? ? ??? ?
??????????????????????????????????????????
?????????????????????????????????????????????
????????????????????????????????????????????????????????
??????????????????????????????
??????????????????? ?? ??? ?? ???????????????????? ?????????? ??????
????????????????????????????????????????????????????????????
(b) German broken verb chunk: three permutations
Figure 1: Examples of chunk permutations generated by shallow syntax reordering rules. Chunk types: CC conjunc-
tion, VC verb (auxiliary/past participle), PC preposition, NC noun, Pct punctuation.
? broken verb chunk: join each finite VC (auxil-
iary or modal) with the nearest following non-
finite VC (infinitive or participle). Place the re-
sulting block in any position between the orig-
inal position of the finite verb and that of the
non-finite verb5.
The application of chunk reordering rules is illus-
trated by Fig. 1: in the Arabic sentence (a), the sub-
ject ?dozens of militants? is preceded by the main
verb ?took part? and its argument ?to the march?. The
rules generate 5 permutations for one matching se-
quence (chunks 2 to 5), out of which the 5th is the
best for translation. The German sentence (b) con-
tains a broken VC with the inflected auxiliary ?has?
separated from the past participle ?initiated?. Here,
the rules generate 3 permutations for the chunk se-
quence 2 to 5, corresponding to likely locations of
the merged verb phrase, the 1st being optimal.
By construction, both rule sets generate a limited
number of permutations per matching sequence: in
5To bound the number of reorderings, we use the follow-
ing heuristics. In ?infinitive? at most 3 punctuations preceding
the VC are considered. In ?subordinate? 1 to 3 chunks are left
between the conjunction (or pronoun) and the moved VC to ac-
count for the subject. In ?broken VC? if the distance between the
finite and non-finite verb is more than 10 chunks, only the first
5 and last 5 positions of the verb-to-verb span are considered.
Arabic at most 12 for each VC; in German at most 3
for each infinitive VC and for each VC-punctuation
sequence, at most 10 for each broken VC. Empiri-
cally, this yields on average 22 reorderings per sen-
tence in the NIST-MT Arabic benchmark dev06-NW
and 3 on the WMT German benchmark test08. Ara-
bic rules are indeed more noisy, which is not surpris-
ing as reordering is triggered by any verb chunk.
5 Reordering selection
The number of chunk-based reorderings per sen-
tence varies according to the rule set, to the size of
chunks and to the context. A high degree of fuzzi-
ness can complicate the decoding process, leaving
too much work to the in-decoding reordering model.
A solution to this problem is using an external model
to score the rule-generated reorderings and discard
the less probable ones. In such a way, a further part
of reordering complexity is taken out of decoding.
At this end, instead of using a Support Vector Ma-
chine classifier as was done in (Bisazza et al, 2011),
we apply reordered n-gram models that are lighter-
weight and more suitable for a ranking task.
Differently from Feng et al (2010), we train our
models on partially reordered data and at the level of
chunks. Chunks can be represented simply by their
481
type label (such as VC or NC), but also by a com-
bination of the type and head word, to obtain finer
lexicalized distributions. LMs trained on different
chunk representations can also be applied jointly, by
log-linear combination.
We perform reordering selection as follows:
1. Chunk-based reordering rules are applied de-
terministically to the source side of the parallel
training data, using word alignment to choose
the optimal permutation (?oracle reordering?)6.
2. One or several chunk-level 5-gram LMs are
trained on such reordered data, using different
chunk representation modes.
3. Reordering rules are applied to the test sen-
tences and the resulting sets of rule-matching
sequence permutations are scored by the LMs.
The n-best reorderings of each rule-matching
sequence are selected for translation.
In experiments not reported here, we obtained
accurate rankings by scoring source permutations
with a uniformly weighted combination of two LMs
trained on chunk types and on chunk-type+head-
word, respectively. In particular, 3-best reorderings
of each rule-matching sequence yield reordering re-
calls of 77.2% in Arabic and 89.3% in German.
6 Modified distortion matrices
We present here a novel technique to encode likely
long reorderings of an input sentence, which can be
seamlessly integrated into the PSMT framework.
During decoding, the distance between source po-
sitions is used for two main purposes: (i) generating
a distortion penalty for the current hypothesis and
(ii) determining the set of source positions that can
be covered at the next hypothesis expansion. We can
then tackle the coarseness of both distortion penalty
and reordering constraints, by replacing the distance
function with a function defined ad hoc for each in-
put sentence.
Distortion can be thought of as a matrix assigning
a positive integer to any ordered pair of source posi-
tions (sx, sy). In the linear distortion model this is
6Following Bisazza and Federico (2010), the optimal re-
ordering for a source sentence is the one that minimizes dis-
tortion in the word alignment to a target translation, measured
by number of swaps and sum of distortion costs.
defined as:
DL(sx, sy) = |sy ? sx ? 1|
so that moving to the right by 1 position costs 0 and
by 2 positions costs 1. Moving to the left by 1 posi-
tion costs 2 and by 2 positions costs 3, and so on. At
the level of phrases, distortion is computed between
the last word of the last translated phrase and the
first word of the next phrase. We retain this equa-
tion as the core distortion function for our model.
Then, we modify entries in the matrix such that the
distortion cost is minimized for the decoding paths
pre-computed with the reordering rules.
Given a source sentence and its set of rule-
generated permutations, the linear distortion matrix
is modified as follows:
1. non-monotonic jumps (i.e. ordered pairs
(si, si+1) such that si+1? si "= 1) are extracted
from the permutations;
2. then, for each extracted pair, the corresponding
point in the matrix is assigned the lowest possi-
ble distortion cost, that is 0 if si < si+1 and 2
if si > si+1. We call these points shortcuts.
Although this technique is approximate and can
overgenerate minimal-distortion decoding paths7, it
practically works when the number of encoded per-
mutations per sequence is limited. This makes mod-
ifed distortion matrices particularly suitable to en-
code just those reorderings that are typically missed
by phrase-based decoders (see Sect. 3).
Since in this work we use chunk-based rules, we
also have to convert chunk-to-chunk jumps into
word-to-word shortcuts. We propose two ways to
do this, given an ordered pair of chunks (cx,cy):
mode A?A : create a shortcut from each word of
cx to each word of cy;
mode L?F : create only one shortcut from the last
word of cx to the first of cy.
The former solution admits more chunk-internal per-
mutations with the same minimal distortion cost,
whereas the latter implies that the first word of a re-
ordered chunk is covered first and the last is covered
last.
7In fact, any decoding path that includes a jump marked as
shortcut benefits from the same distortion discount in that point.
482
? ? ? ? ? ? ? ? ? ? ??
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
?? ? ? ? ? ? ? ? ? ?
?? ?? ? ? ? ? ? ? ? ?
??
???????
???
???????
??
???
???????
???
?????
????????
? ?
??
? ?
?? ? ?
?? ?
? ?
?? ? ?
??? ??
?? ??????? ??? ??
????? ??? ??? ??????? ??? ????? ???????
?
??
?
?
???
?
?
??
? ? ?
??
?
?
???
??
????? ?? ??? ???? ??? ?? ??? ???? ?
????? ?? ??? ????? ??? ? ??? ??? ?? ??
Figure 2: Modified distortion matrix (mode A?A) of the
German sentence given in Fig. 1. The chunk reordering
shown on top generates three shortcuts corresponding to
the 0?s and 2?s highlighted in the matrix.
Fig. 2 shows the distortion matrix of the German
sentence of Fig. 1, with starting positions as columns
and landing positions as rows. Suppose we want to
encode the reordering shown on top of Fig. 2, cor-
responding to the merging of the broken VC ?hat ...
eingeleitet?. This permutation contains three jumps:
(2,5), (5,3) and (4,6). Converted to word-level in
A?A mode, these yield five word shortcuts8: one
for the onward jump (2,5) assigned 0 distortion; two
for the backward jump (5,3), assigned 2; and two for
the onward jump (4,6), also assigned 0. The desired
reordering is now attainable within a DL of 2 words
instead of 5. The same process is then applied to
other permutations of the sentence.
If compared to the word reordering lattices used
by Bisazza and Federico (2010) and Andreas et al
(2011), modified distortion matrices provide a more
compact, implicit way to encode likely reorderings
in a sentence-specific fashion. Matrix representation
does not require multiplication of nodes for the same
8In L?F mode, instead, each chunk-to-chunk jump would
yield exactly one word shortcut, for a total of three.
source word and is naturally compatible with the
PSMT decoder?s standard reordering mechanisms.
7 Evaluation
In this section we evaluate the impact of modified
distortion matrices on two news translation tasks.
Matrices were integrated into the Moses
toolkit (Koehn et al, 2007) using a sentence-
level XML markup. The list of word shortcuts
for each sentence is provided as an XML tag that
is parsed by the decoder to modify the distortion
matrix just before starting the search. As usual, the
distortion matrix is queried by the distortion penalty
generator and by the hypothesis expander9.
7.1 Experimental setup
For Arabic-English, we use the union of all in-
domain parallel corpora provided for the NIST-MT09
evaluation10 for a total of 986K sentences, 31M En-
glish words. The target LM is trained on the English
side of all available NIST-MT09 parallel data, UN in-
cluded (147M words). For development and test, we
use the newswire sections of the NIST benchmarks,
hereby called dev06-NW, eval08-NW and eval09-
NW: 1033, 813 and 586 sentences, respectively, each
provided with four reference translations.
The German-English system is instead trained
on WMT10 data: namely Europarl (v.5) plus News-
commentary-2010 for a total of 1.6M parallel sen-
tences, 43M English words. The target LM is trained
on the monolingual news data provided for the con-
strained track (1133M words). For development and
test, we use the WMT10 news benchmarks test08,
test09 and test10: 2051, 2525 and 2489 sentences,
respectively, with one reference translation.
Concerning pre-processing, we apply standard to-
kenization to the English data, while for Arabic we
use our in-house tokenizer that removes diacritics
and normalizes special characters. Arabic text is
then segmented with AMIRA (Diab et al, 2004) ac-
cording to the ATB scheme11. German tokenization
9Note that lexicalized reordering models use real word dis-
tances to compute the orientation class of a new hypothesis, thus
they are not affected by changes in the matrix.
10That is everything except the small GALE corpus and the
UN corpus. As reported by Green et al (2010) the removal of
UN data does not affect baseline performances on news test.
11The Arabic Treebank tokenization scheme isolates con-
483
and compound splitting is performed with Tree Tag-
ger (Schmid, 1994) and the Gertwol morphological
analyser (Koskenniemi and Haapalainen, 1994)12.
Using Moses we build competitive baselines on
the training data described above. Word alignment
is produced by the Berkeley Aligner (Liang et al,
2006). The decoder is based on the log-linear com-
bination of a phrase translation model, a lexicalized
reordering model, a 6-gram target language model,
distortion cost, word and phrase penalties. The re-
ordering model is a hierarchical phrase orientation
model (Tillmann, 2004; Koehn et al, 2005; Galley
and Manning, 2008) trained on all the available par-
allel data. We choose the hierarchical variant, as it
was shown by its authors to outperform the default
word-based on an Arabic-English task. Finally, for
German, we enable the Moses option monotone-at-
punctuation which forbids reordering across punc-
tuation marks. The DL is initially set to 5 words
for Arabic-English and to 10 for German-English.
According to our experience, these are the optimal
settings for the evaluated tasks. Feature weights are
optimized by minimum error training (Och, 2003)
on the development sets (dev06-NW and test08).
7.2 Translation quality and efficiency results
We evaluate translations with BLEU (Papineni et al,
2002) and METEOR (Banerjee and Lavie, 2005).
As these scores are only indirectly sensitive to word
order, we also compute KRS or Kendall Reorder-
ing Score (Birch et al, 2010; Bisazza et al, 2011)
which is a positive score based on the Kendall?s
Tau distance between the source-output and source-
reference permutations. To isolate the impact of our
techniques on problematic reordering, we extract
from each test set the sentences that got permuted
by ?oracle reordering? (see Sect. 5). These consti-
tute about a half of the Arabic sentences, and about
a third of the German. We refer to the KRS com-
puted on these test subsets as KRS(R). Statistically
significant differences are assessed by approximate
randomization as in (Riezler and Maxwell, 2005)13.
Tab. 1 reports results obtained by varying the DL
junctions w+ and f+, prepositions l+, k+, b+, future marker
s+, pronominal suffixes, but not the article Al+.
12http://www2.lingsoft.fi/cgi-bin/gertwol
13Translation scores and significance tests are computed with
the tools multeval (Clark et al, 2011) and sigf (Pado?, 2006).
and modifying the distortion function. To evalu-
ate the reordering selection technique, we also com-
pare the encoding of all rule-generated reorderings
against only the 3 best per rule-matching sequence,
as ranked by our best performing reordered LM (see
end of Sect. 5). We mark the DL with a ?+? to denote
that some longer jumps are being allowed by modi-
fied distortion. Run times refer to the translation of
the first 100 sentences of eval08-NW and test09 by
a 4-core processor.
Arabic-English. As anticipated, raising the DL
does not improve, but rather worsen performances.
The decrease in BLEU and METEOR reported with
DL=8 is not significant, but the decrease in KRS is
both significant and large. Efficiency is heavily af-
fected, with a 42% increase of the run time.
Results in the row ?allReo? are obtained by encod-
ing all the rule-generated reorderings inL?F chunk-
to-word conversion mode. Except for some gains in
KRS reported on eval08-NW, most of the scores are
lower or equal to the baseline. Such inconsistent be-
haviour is probably due to the low precision of the
Arabic rule set, pointed out in Sect. 4.
Finally, we arrive to the performance of 3-best re-
orderings per sequence. With L?F we obtain sev-
eral improvements, but it?s with A?A that we are
able to beat the baseline according to all metrics.
BLEU andMETEOR improvements are rather small
but significant and consistent across test sets, the
best gain being reported on eval09-NW (+.9 BLEU).
Most importantly, substantial word order improve-
ments are achieved on both full test sets (+.7/+.6
KRS) and selected subsets (+.7/+.6 KRS(R)). Ac-
cording to these figures, word order is affected only
in the sentences that contain problematic reordering.
This is good evidence, suggesting that the decoder
does not get ?confused? by spurious shortcuts.
Looking at run times, we can say that modified
distortion matrices are a very efficient way to ad-
dress long reordering. Even when all the generated
reorderings are encoded, translation time increases
only by 4%. Reordering selection naturally helps to
further reduce decoding overload. As for conversion
modes, A?A yields slightly higher run times than
L?F because it generates more shortcuts.
German-English. In this task we manage to im-
prove translation quality with a setting that is almost
484
(a) Arabic to English
eval08-nw eval09-nw runtime
Distortion Function DL bleu met krs krs(R) bleu met krs krs(R) (s)
? plain [baseline] 5 44.5 34.9 81.6 82.9 49.9 38.0 84.1 84.4 1038
plain 8 44.2? 34.8 80.7? 82.2? 49.8 37.9 83.3? 83.5? 1470
? modified: allReo, L?F 5+ 44.4 34.9 82.2? 83.7? 49.9 37.8? 84.3 84.4 1078
modified: 3bestReo, L?F 5+ 44.5 35.1? 82.3? 83.5? 50.7? 38.1 84.8? 85.0? 1052
? modified: 3bestReo, A?A 5+ 44.8? 35.1? 82.3? 83.6? 50.8? 38.2? 84.7? 85.0? 1072
(b) German to English
test09 test10 runtime
Distortion Function DL bleu met krs krs(R) bleu met krs krs(R) (s)
? plain [baseline] 10 18.8 27.5 65.8 66.7 20.1 29.4 68.7 68.9 629
plain 20 18.4? 27.4? 63.6? 65.2 ? 19.8? 29.3? 66.3? 66.6? 792
plain 4 18.4? 27.4? 67.3? 66.9 19.6? 29.1? 70.2? 69.6? 345
? modified: allReo, L?F 4+ 19.1? 27.6? 67.6? 68.1? 20.4? 29.4 70.6? 70.7? 352
modified: 3bestReo, L?F 4+ 19.2? 27.7? 67.4? 68.1? 20.4? 29.4 70.4? 70.6? 351
? modified: 3bestReo, A?A 4+ 19.2? 27.7? 67.4? 68.4? 20.6? 29.5? 70.4? 70.7? 357
Table 1: Impact of modified distortion matrices on translation quality, measured with BLEU, METEOR and KRS
(all in percentage form, higher scores mean higher quality). The settings used for weight tuning are marked with ?.
Statistically significant differences wrt the baseline are marked with ? at the p ? .05 level and ? at the p ? .10 level.
twice as fast as the baseline. As shown by the first
part of the table, the best baseline results are ob-
tained with a rather high DL, that is 10 (only KRS
improves with a lower DL). However, with modified
distortion, the best results according to all metrics
are obtained with a DL of 4.
Looking at the rest of the table, we see that re-
ordering selection is not as crucial as in Arabic-
English. This is in line with the properties of the
more precise German reordering rule set (two rules
out of three generate at most 3 reorderings per se-
quence). Considering all scores, the last setting
(3-best reordering and A?A) appears as the best
one, achieving the following gains over the base-
line: +.4/+.5 BLEU, +.2/+.1 METEOR, +1.6/+1.7
KRS and +1.7/+1.8 KRS(R). The agreement ob-
served among such diverse metrics makes us con-
fident about the goodness of the approach.
8 Conclusions
In Arabic-English and German-English, long re-
ordering concentrates on specific patterns describ-
able by a small number of linguistic rules. By
means of non-deterministic chunk reordering rules,
we have generated likely permutations of the test
sentences and ranked them with n-gram LMs trained
on pre-reordered data. We have then introduced the
notion of modified distortion matrices to naturally
encode a set of likely reorderings in the decoder
input. Modified distortion allows for a finer and
more linguistically informed definition of the search
space, which is reflected in better translation outputs
and more efficient decoding.
We expect that further improvements may be
achieved by refining the Arabic reordering rules with
specific POS tags and lexical cues. We also plan
to evaluate modified distortion matrices in conjunc-
tion with a different type of in-decoding reorder-
ing model such as the one proposed by Green et
al. (2010). Finally, we may try to exploit not only
the ranking, but also the scores produced by the re-
ordered LMs, as an additional decoding feature.
Acknowledgments
This work was supported by the T4ME network of
excellence (IST-249119) funded by the European
Commission DG INFSO through the 7th Framework
Programme. We thank Christian Hardmeier for
helping us define the German reordering rules, and
the anonymous reviewers for valuable suggestions.
485
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 529?
536, Sydney, Australia, July. Association for Computa-
tional Linguistics.
Jacob Andreas, Nizar Habash, and Owen Rambow. 2011.
Fuzzy syntactic reordering for phrase-based statistical
machine translation. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 227?
236, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating reorder-
ing. Machine Translation, 24(1):15?26.
Arianna Bisazza and Marcello Federico. 2010. Chunk-
based verb reordering in VSO sentences for Arabic-
English statistical machine translation. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and Metrics MATR, pages 241?249, Upp-
sala, Sweden, July. Association for Computational Lin-
guistics.
Arianna Bisazza, Daniele Pighin, and Marcello Federico.
2011. Chunk-lattices for verb reordering in Arabic-
English statistical machine translation. Machine Trans-
lation, Published Online.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah
Smith. 2011. Better hypothesis testing for sta-
tistical machine translation: Controlling for opti-
mizer instability. In Proceedings of the Asso-
ciation for Computational Lingustics, ACL 2011,
Portland, Oregon, USA. Association for Com-
putational Linguistics. accepted; available at
http://www.cs.cmu.edu/ jhclark/pubs/significance.pdf.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 531?540, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Josep M. Crego and Nizar Habash. 2008. Using shal-
low syntax information to improve word alignment and
reordering for smt. In StatMT ?08: Proceedings of
the Third Workshop on Statistical Machine Translation,
pages 53?61, Morristown, NJ, USA. Association for
Computational Linguistics.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic Tagging of Arabic Text: From Raw Text to
Base Phrase Chunks. In Daniel Marcu Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004: Short
Papers, pages 149?152, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
Jakob Elming and Nizar Habash. 2009. Syntactic
reordering for English-Arabic phrase-based machine
translation. In Proceedings of the EACL 2009 Work-
shop on Computational Approaches to Semitic Lan-
guages, pages 69?77, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A source-side decoding sequence model for statistical
machine translation. In Conference of the Association
for Machine Translation in the Americas (AMTA), Den-
ver, Colorado, USA.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reordering
model. In EMNLP ?08: Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 848?856, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for sta-
tistical machine translation. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL), pages 867?875, Los An-
geles, California. Association for Computational Lin-
guistics.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Bente Maegaard, ed-
itor, Proceedings of the Machine Translation Summit
XI, pages 215?222, Copenhagen, Denmark.
Christian Hardmeier, Arianna Bisazza, and Marcello
Federico. 2010. FBK at WMT 2010: Word lattices
for morphological reduction and chunk-based reorder-
ing. In Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and Metrics MATR, pages
88?92, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
486
ings of HLT-NAACL 2003, pages 127?133, Edmonton,
Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proc. of the International Workshop on Spoken Lan-
guage Translation, October.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics Companion Volume Proceedings of
the Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Kimmo Koskenniemi and Mariikka Haapalainen, 1994.
GERTWOL ? Lingsoft Oy, chapter 11, pages 121?140.
Roland Hausser, Niemeyer, Tu?bingen.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June. Association for Computational Linguistics.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 206?214, Athens, Greece, March.
Association for Computational Linguistics.
Franz Josef Och. 2002. Statistical Machine Trans-
lation: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen, Germany.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Erhard Hinrichs
and Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 160?167.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association of Computa-
tional Linguistics (ACL), pages 311?318, Philadelphia,
PA.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL Workshop on In-
trinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 57?64, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Proceed-
ings of the Joint Conference on Human Language Tech-
nologies and the Annual Meeting of the North Ameri-
can Chapter of the Association of Computational Lin-
guistics (HLT-NAACL).
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In Proceedings of Coling 2004, pages 508?
514, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Sirvan Yahyaei and Christof Monz. 2010. Dynamic dis-
tortion in a discriminative reordering model for statisti-
cal machine translation. In International Workshop on
Spoken Language Translation (IWSLT), Paris, France.
Kenji Yamada. 2002. A syntax-based translation model.
Ph.D. thesis, Department of Computer Science, Uni-
versity of Southern California, Los Angeles.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Machine
Translation, pages 55?63, New York City, June. Asso-
ciation for Computational Linguistics.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In 25th German Confer-
ence on Artificial Intelligence (KI2002), pages 18?32,
Aachen, Germany. Springer Verlag.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-level reordering of source language sentences
with automatically learned rules for statistical machine
translation. In Proceedings of SSST, NAACL-HLT 2007
/ AMTAWorkshop on Syntax and Structure in Statistical
Translation, pages 1?8, Rochester, New York, April.
Association for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 1145?1152, Manchester, UK, August. Coling
2008 Organizing Committee.
487
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 120?124,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Detecting Semantic Equivalence and Information Disparity
in Cross-lingual Documents
Yashar Mehdad Matteo Negri Marcello Federico
Fondazione Bruno Kessler, FBK-irst
Trento , Italy
{mehdad|negri|federico}@fbk.eu
Abstract
We address a core aspect of the multilingual
content synchronization task: the identifica-
tion of novel, more informative or semanti-
cally equivalent pieces of information in two
documents about the same topic. This can be
seen as an application-oriented variant of tex-
tual entailment recognition where: i) T and
H are in different languages, and ii) entail-
ment relations between T and H have to be
checked in both directions. Using a combi-
nation of lexical, syntactic, and semantic fea-
tures to train a cross-lingual textual entailment
system, we report promising results on differ-
ent datasets.
1 Introduction
Given two documents about the same topic writ-
ten in different languages (e.g. Wiki pages), con-
tent synchronization deals with the problem of au-
tomatically detecting and resolving differences in
the information they provide, in order to produce
aligned, mutually enriched versions. A roadmap to-
wards the solution of this problem has to take into
account, among the many sub-tasks, the identifica-
tion of information in one page that is semantically
equivalent, novel, or more informative with respect
to the content of the other page. In this paper we
set such problem as an application-oriented, cross-
lingual variant of the Textual Entailment (TE) recog-
nition task (Dagan and Glickman, 2004). Along this
direction, we make two main contributions:
(a) Experiments with multi-directional cross-
lingual textual entailment. So far, cross-lingual
textual entailment (CLTE) has been only applied
to: i) available TE datasets (uni-directional rela-
tions between monolingual pairs) transformed into
their cross-lingual counterpart by translating the hy-
potheses into other languages (Negri and Mehdad,
2010), and ii) machine translation (MT) evaluation
datasets (Mehdad et al, 2012). Instead, we ex-
periment with the only corpus representative of the
multilingual content synchronization scenario, and
the richer inventory of phenomena arising from it
(multi-directional entailment relations).
(b) Improvement of current CLTE methods. The
CLTE methods proposed so far adopt either a ?piv-
oting approach? based on the translation of the two
input texts into the same language (Mehdad et al,
2010), or an ?integrated solution? that exploits bilin-
gual phrase tables to capture lexical relations and
contextual information (Mehdad et al, 2011). The
promising results achieved with the integrated ap-
proach, however, still rely on phrasal matching tech-
niques that disregard relevant semantic aspects of
the problem. By filling this gap integrating linguis-
tically motivated features, we propose a novel ap-
proach that improves the state-of-the-art in CLTE.
2 CLTE-based content synchronization
CLTE has been proposed by (Mehdad et al, 2010) as
an extension of textual entailment which consists of
deciding, given a text T and an hypothesis H in dif-
ferent languages, if the meaning of H can be inferred
from the meaning of T. The adoption of entailment-
based techniques to address content synchronization
looks promising, as several issues inherent to such
task can be formalized as entailment-related prob-
120
lems. Given two pages (P1 and P2), these issues
include identifying, and properly managing:
(1) Text portions in P1 and P2 that express the same
meaning (bi-directional entailment). In such cases
no information has to migrate across P1 and P2, and
the two text portions will remain the same;
(2) Text portions in P1 that are more informa-
tive than portions in P2 (forward entailment). In
such cases, the entailing (more informative) portions
from P1 have to be translated and migrated to P2 in
order to replace or complement the entailed (less in-
formative) fragments;
(3) Text portions in P2 that are more informa-
tive than portions in P1 (backward entailment), and
should be translated to replace or complement them;
(4) Text portions in P1 describing facts that are not
present in P2, and vice-versa (the ?unknown? cases
in RTE parlance). In such cases, the novel infor-
mation from both sides has to be translated and mi-
grated in order to mutually enrich the two pages;
(5) Meaning discrepancies between text portions in
the two pages (?contradictions? in RTE parlance).
CLTE has been previously modeled as a phrase
matching problem that exploits dictionaries and
phrase tables extracted from bilingual parallel cor-
pora to determine the number of word sequences in
H that can be mapped to word sequences in T. In
this way a semantic judgement about entailment is
made exclusively on the basis of lexical evidence.
When only unidirectional entailment relations from
T to H have to be determined (RTE-like setting), the
full mapping of the hypothesis into the text usually
provides enough evidence for a positive entailment
judgement. Unfortunately, when dealing with multi-
directional entailment, the correlation between the
proportion of matching terms and the correct entail-
ment decisions is less strong. In such framework, for
instance, the full mapping of the hypothesis into the
text is per se not sufficient to discriminate between
forward entailment and semantic equivalence. To
cope with these issues, we explore the contribution
of syntactic and semantic features as a complement
to lexical ones in a supervised learning framework.
3 Beyond lexical CLTE
In order to enrich the feature space beyond pure lex-
ical match through phrase table entries, our model
builds on two additional feature sets, derived from i)
semantic phrase tables, and ii) dependency relations.
Semantic Phrase Table (SPT) matching repre-
sents a novel way to leverage the integration of se-
mantics and MT-derived techniques. SPT matching
extends CLTE methods based on pure lexical match
by means of ?generalized? phrase tables annotated
with shallow semantic labels. SPTs, with entries in
the form ?[LABEL] word1...wordn [LABEL]?, are
used as a recall-oriented complement to the phrase
tables used in MT. A motivation for this augmenta-
tion is that semantic tags allow to match tokens that
do not occur in the original bilingual parallel cor-
pora used for phrase table extraction. Our hypothe-
sis is that the increase in recall obtained from relaxed
matches through semantic tags in place of ?out of
vocabulary? terms (e.g. unseen person names) is an
effective way to improve CLTE performance, even
at the cost of some loss in precision.
Like lexical phrase tables, SPTs are extracted
from parallel corpora. As a first step we annotate
the parallel corpora with named-entity taggers for
the source and target languages, replacing named
entities with general semantic labels chosen from
a coarse-grained taxonomy (person, location, orga-
nization, date and numeric expression). Then, we
combine the sequences of unique labels into one sin-
gle token of the same label, and we run Giza++ (Och
and Ney, 2000) to align the resulting semantically
augmented corpora. Finally, we extract the seman-
tic phrase table from the augmented aligned corpora
using the Moses toolkit (Koehn et al, 2007). For
the matching phase, we first annotate T and H in the
same way we labeled our parallel corpora. Then, for
each n-gram order (n=1 to 5) we use the SPT to cal-
culate a matching score as the number of n-grams in
H that match with phrases in T divided by the num-
ber of n-grams in H.1
Dependency Relation (DR) matching targets the
increase of CLTE precision. Adding syntactic con-
straints to the matching process, DR features aim to
reduce the amount of wrong matches often occur-
ring with bag-of-words methods (both at the lexi-
cal level and with recall-oriented SPTs). For in-
stance, the contradiction between ?Yahoo acquired
1When checking for entailment from H to T, the normaliza-
tion is carried out dividing by the number of n-grams in T.
121
Overture? and ?Overture compro? Yahoo?, which is
evident when syntax is taken into account, can not
be caught by shallow methods. We define a de-
pendency relation as a triple that connects pairs of
words through a grammatical relation. DR matching
captures similarities between dependency relations,
combining the syntactic and lexical level. In a valid
match, while the relation has to be the same, the con-
nected words can be either the same, or semantically
equivalent terms in the two languages (e.g. accord-
ing to a bilingual dictionary). Given the dependency
tree representations of T and H, for each grammati-
cal relation (r) we calculate a DR matching score as
the number of matching occurrences of r in T and
H, divided by the number of occurrences of r in H.
Separate DR matching scores are calculated for each
relation r appearing both in T and H.
4 Experiments and results
4.1 Content synchronization scenario
In our first experiment we used the English-German
portion of the CLTE corpus described in (Negri et
al., 2011), consisting of 500 multi-directional entail-
ment pairs which we equally divided into training
and test sets. Each pair in the dataset is annotated
with ?Bidirectional?, ?Forward?, or ?Backward? en-
tailment judgements. Although highly relevant for
the content synchronization task, ?Contradiction?
and ?Unknown? cases (i.e. ?NO? entailment in both
directions) are not present in the annotation. How-
ever, this is the only available dataset suitable to
gather insights about the viability of our approach to
multi-directional CLTE recognition.2 We chose the
ENG-GER portion of the dataset since for such lan-
guage pair MT systems performance is often lower,
making the adoption of simpler solutions based on
pivoting more vulnerable.
To build the English-German phrase tables we
combined the Europarl, News Commentary and ?de-
news?3 parallel corpora. After tokenization, Giza++
and Moses were respectively used to align the cor-
pora and extract a lexical phrase table (PT). Simi-
larly, the semantic phrase table (SPT) has been ex-
2Recently, a new dataset including ?Unknown? pairs has
been used in the ?Cross-Lingual Textual Entailment for Content
Synchronization? task at SemEval-2012 (Negri et al, 2012).
3http://homepages.inf.ed.ac.uk/pkoehn/
tracted from the same corpora annotated with the
Stanford NE tagger (Faruqui and Pado?, 2010; Finkel
et al, 2005). Dependency relations (DR) have been
extracted running the Stanford parser (Rafferty and
Manning, 2008; De Marneffe et al, 2006). The dic-
tionary created during the alignment of the parallel
corpora provided the lexical knowledge to perform
matches when the connected words are different, but
semantically equivalent in the two languages. To
combine and weight features at different levels we
used SVMlight (Joachims, 1999) with default pa-
rameters.
In order to experiment under testing conditions
of increasing complexity, we set the CLTE problem
both as a two-way and as a three-way classification
task. Two-way classification casts multi-directional
entailment as a unidirectional problem, where each
pair is analyzed checking for entailment both from
left to right and from right to left. In this condi-
tion, each original test example is correctly clas-
sified if both pairs originated from it are correctly
judged (?YES-YES? for bidirectional, ?YES-NO?
for forward, and ?NO-YES? for backward entail-
ment). Two-way classification represents an intu-
itive solution to capture multidirectional entailment
relations but, at the same time, a suboptimal ap-
proach in terms of efficiency since two checks are
performed for each pair. Three-way classification is
more efficient, but at the same time more challeng-
ing due to the higher difficulty of multiclass learn-
ing, especially with small datasets.
Results are compared with two pivoting ap-
proaches, checking for entailment between the orig-
inal English texts and the translated German hy-
potheses.4 The first (Pivot-EDITS), uses an op-
timized distance-based model implemented in the
open source RTE system EDITS (Kouylekov and
Negri, 2010; Kouylekov et al, 2011). The second
(Pivot-PPT) exploits paraphrase tables for phrase
matching, and represents the best monolingual
model presented in (Mehdad et al, 2011). Table
1 demonstrates the success of our results in prov-
ing the two main claims of this paper. (a) In both
settings all the feature sets used outperform the ap-
proaches taken as terms of comparison. The 61.6%
accuracy achieved in the most challenging setting
4Using Google Translate.
122
PT PT+DR PT+SPT PT+SPT+DR Pivot-EDITS Pivot-PPT
Cont. Synch. (2-way) 57.8 58.6 62.4 63.3 27.4 57.0
Cont. Synch. (3-way) 57.4 57.8 58.7 61.6 25.3 56.1
RTE-3 AVG Pivot PPT
RTE3-derived 62.6 63.6 63.5 64.5 62.4 63.5
Table 1: CLTE accuracy results over content synchronization and RTE3-derived datasets.
(3-way) demonstrates the effectiveness of our ap-
proach to capture meaning equivalence and informa-
tion disparity in cross-lingual texts.
(b) In both settings the combination of lexical, syn-
tactic and semantic features (PT+SPT+DR) signif-
icantly improves5 the state-of-the-art CLTE model
(PT). Such improvement is motivated by the joint
contribution of SPTs (matching more and longer n-
grams, with a consequent recall improvement), and
DR matching (adding constraints, with a consequent
gain in precision). However, the performance in-
crease brought by DR features over PT is mini-
mal. This might be due to the fact that both PT and
DR features are precision-oriented, and their effec-
tiveness becomes evident only in combination with
recall-oriented features (SPT).
Cross-lingual models also significantly outper-
form pivoting methods. This suggests that the noise
introduced by incorrect translations makes the pivot-
ing approach less attractive in comparison with the
more robust cross-lingual models.
4.2 RTE-like CLTE scenario
Our second experiment aims at verifying the effec-
tiveness of the improved model over RTE-derived
CLTE data. To this aim, we compare the results ob-
tained by the new CLTE model with those reported
in (Mehdad et al, 2011), calculated over an English-
Spanish entailment corpus derived from the RTE-3
dataset (Negri and Mehdad, 2010).
In order to build the English-Spanish lexical
phrase table (PT), we used the Europarl, News Com-
mentary and United Nations parallel corpora. The
semantic phrase table (SPT) was extracted from the
same corpora annotated with FreeLing (Carreras et
al., 2004). Dependency relations (DR) have been ex-
tracted parsing English texts and Spanish hypotheses
with DepPattern (Gamallo and Gonzalez, 2011).
5p < 0.05, calculated using the approximate randomization
test implemented in (Pado?, 2006).
Accuracy results have been calculated over 800
test pairs of the CLTE corpus, after training the SVM
binary classifier over the 800 development pairs.
Our new features have been compared with: i) the
state-of-the-art CLTE model (PT), ii) the best mono-
lingual model (Pivot-PPT) presented in (Mehdad et
al., 2011), and iii) the average result achieved by
participants in the monolingual English RTE-3 eval-
uation campaign (RTE-3 AVG). As shown in Ta-
ble 1, the combined feature set (PT+SPT+DR) sig-
nificantly5 outperforms the lexical model (64.5%
vs 62.6%), while SPT and DR features separately
added to PT (PT+SPT, and PT+DR) lead to marginal
improvements over the results achieved by the PT
model alone (about 1%). This confirms the con-
clusions drawn from the previous experiment, that
precision-oriented and recall-oriented features lead
to a larger improvement when they are used in com-
bination.
5 Conclusion
We addressed the identification of semantic equiv-
alence and information disparity in two documents
about the same topic, written in different languages.
This is a core aspect of the multilingual content syn-
chronization task, which represents a challenging
application scenario for a variety of NLP technolo-
gies, and a shared research framework for the inte-
gration of semantics and MT technology. Casting
the problem as a CLTE task, we extended previous
lexical models with syntactic and semantic features.
Our results in different cross-lingual settings prove
the feasibility of the approach, with significant state-
of-the-art improvements also on RTE-derived data.
Acknowledgments
This work has been partially supported by the EU-
funded project CoSyne (FP7-ICT-4-248531).
123
References
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
FreeLing: An Open-Source Suite of Language Ana-
lyzers. In Proceedings of the 4th Language Resources
and Evaluation Conference (LREC 2004), volume 4.
I. Dagan and O. Glickman. 2004. Probabilistic Textual
Entailment: Generic Applied Modeling of Language
Variability. In Proceedings of the PASCAL Workshop
of Learning Methods for Text Understanding and Min-
ing.
M.C. De Marneffe, B. MacCartney, and C.D. Man-
ning. 2006. Generating Typed Dependency Parses
from Phrase Structure Parses. In Proceedings of the
5th Language Resources and Evaluation Conference
(LREC 2006), volume 6, pages 449?454.
M. Faruqui and S. Pado?. 2010. Training and Evaluat-
ing a German Named Entity Recognizer with Seman-
tic Generalization. In Proceedings of the 10th Con-
ference on Natural Language Processing (KONVENS
2010), Saarbru?cken, Germany.
J.R. Finkel, T. Grenager, and C. Manning. 2005. Incor-
porating Non-local Information into Information Ex-
traction Systems by Gibbs Sampling. In Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics (ACL 2005).
P. Gamallo and I. Gonzalez. 2011. A grammatical for-
malism based on patterns of part of speech tags. Inter-
national Journal of Corpus Linguistics, 16(1):45?71.
T. Joachims. 1999. Advances in kernel methods. chap-
ter Making large-scale support vector machine learn-
ing practical, pages 169?184. MIT Press, Cambridge,
MA, USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings of
the 45th Annual Meeting on Association for Computa-
tional Linguistics, Demonstration Session (ACL 2007).
M. Kouylekov and M. Negri. 2010. An Open-Source
Package for Recognizing Textual Entailment. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, system demonstrations
(ACL 2010).
M. Kouylekov, Y. Mehdad, and M. Negri. 2011. Is it
Worth Submitting this Run? Assess your RTE Sys-
tem with a Good Sparring Partner. Proceedings of the
EMNLP TextInfer 2011 Workshop on Textual Entail-
ment.
Y. Mehdad, M. Negri, and M. Federico. 2010. Towards
Cross-Lingual Textual Entailment. In Proceedings of
the 11th Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL HLT 2010).
Y. Mehdad, M. Negri, and M. Federico. 2011. Using
Bilingual Parallel Corpora for Cross-Lingual Textual
Entailment. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies (ACL HLT 2011).
Y. Mehdad, M. Negri, and M. Federico. 2012. Match
without a Referee: Evaluating MT Adequacy without
Reference Translations. In Proceedings of the Ma-
chine Translation Workshop (WMT2012).
M. Negri and Y. Mehdad. 2010. Creating a Bi-lingual
Entailment Corpus through Translations with Mechan-
ical Turk: $100 for a 10-day Rush. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazons? Mechanical Turk.
M. Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, and
A. Marchetti. 2011. Divide and Conquer: Crowd-
sourcing the Creation of Cross-Lingual Textual Entail-
ment Corpora. Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011).
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and
D. Giampiccolo. 2012. Semeval-2012 Task 8: Cross-
lingual Textual Entailment for Content Synchroniza-
tion. In Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012).
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2000).
S. Pado?, 2006. User?s guide to sigf: Significance test-
ing by approximate randomisation.
A.N. Rafferty and C.D. Manning. 2008. Parsing Three
German Treebanks: Lexicalized and Unlexicalized
Baselines. In In Proceedings of the ACL 2008 Work-
shop on Parsing German.
124
Dynamically Shaping the Reordering Search Space
of Phrase-Based Statistical Machine Translation
Arianna Bisazza and Marcello Federico
Fondazione Bruno Kessler
Trento, Italy
{bisazza,federico}@fbk.eu
Abstract
Defining the reordering search space is a cru-
cial issue in phrase-based SMT between dis-
tant languages. In fact, the optimal trade-
off between accuracy and complexity of de-
coding is nowadays reached by harshly lim-
iting the input permutation space. We pro-
pose a method to dynamically shape such
space and, thus, capture long-range word
movements without hurting translation qual-
ity nor decoding time. The space defined
by loose reordering constraints is dynamically
pruned through a binary classifier that predicts
whether a given input word should be trans-
lated right after another. The integration of
this model into a phrase-based decoder im-
proves a strong Arabic-English baseline al-
ready including state-of-the-art early distor-
tion cost (Moore and Quirk, 2007) and hierar-
chical phrase orientation models (Galley and
Manning, 2008). Significant improvements in
the reordering of verbs are achieved by a sys-
tem that is notably faster than the baseline,
while BLEU and METEOR remain stable, or
even increase, at a very high distortion limit.
1 Introduction
Word order differences are among the most impor-
tant factors determining the performance of statisti-
cal machine translation (SMT) on a given language
pair (Birch et al, 2009). This is particularly true in
the framework of phrase-based SMT (PSMT) (Zens
et al, 2002; Koehn et al, 2003; Och and Ney, 2002),
an approach that remains highly competitive despite
the recent advances of the tree-based approaches.
During the PSMT decoding process, the output
sentence is built from left to right, while the input
sentence positions can be covered in different or-
ders. Thus, reordering in PSMT can be viewed as
the problem of choosing the input permutation that
leads to the highest-scoring output sentence. Due to
efficiency reasons, however, the input permutation
space cannot be fully explored, and is therefore lim-
ited with hard reordering constraints.
Although many solutions have been proposed to
explicitly model word reordering during decoding,
PSMT still largely fails to handle long-range word
movements in language pairs with different syntac-
tic structures1. We believe this is mostly not due to
deficiencies of the existing reordering models, but
rather to a very coarse definition of the reorder-
ing search space. Indeed, the existing reordering
constraints are rather simple and typically based on
word-to-word distances. Moreover, they are uni-
form throughout the input sentence and insensitive
to the actual words being translated. Relaxing this
kind of constraints means dramatically increasing
the size of the search space and making the reorder-
ing model?s task extremely complex. As a result,
even in language pairs where long reordering is reg-
ularly observed, PSMT quality degrades when long
word movements are allowed to the decoder.
We address this problem by training a binary
classifier to predict whether a given input position
should be translated right after another, given the
words at those positions and their contexts. When
this model is integrated into the decoder, its predic-
1For empirical evidence, see for instance (Birch et al, 2009;
Galley and Manning, 2008; Bisazza and Federico, 2012).
327
Transactions of the Association for Computational Linguistics, 1 (2013) 327?340. Action Editor: Philipp Koehn.
Submitted 1/2013; Revised 5/2013; Published 7/2013. c?2013 Association for Computational Linguistics.
tions can be used not only as an additional feature
function, but also as an early indication of whether
or not a given reordering path should be further ex-
plored. More specifically, at each hypothesis ex-
pansion, we consider the set of input positions that
are reachable according to the usual reordering con-
straints, and prune it based only on the reorder-
ing model score. Then, the hypothesis can be ex-
panded normally by covering the non-pruned posi-
tions. This technique makes it possible to dynami-
cally shape the search space while decoding with a
very high distortion limit, which can improve trans-
lation quality and efficiency at the same time.
The remainder of the paper is organized as fol-
lows. After an overview of the relevant literature,
we describe in detail our word reordering model. In
the following section, we introduce early pruning of
reordering steps as a way to dynamically shape the
input permutation space. Finally, we present an em-
pirical analysis of our approach, including intrinsic
evaluation of the model and SMT experiments on a
well-known Arabic-English news translation task.
2 Previous Work
In this paper, we focus on methods that guide the
reordering search during the phrase-based decoding
process. See for instance (Costa-jussa` and Fonol-
losa, 2009) for a review of pre- and post-reordering
approaches that are not treated here.
Assuming a one-to-one correspondence between
source and target phrases, reordering in PSMT can
be viewed as the problem of searching through a set
of permutations of the input sentence. Thus, two
sub-problems arise: defining the set of allowed per-
mutations (reordering constraints) and scoring the
allowed permutations according to some likelihood
criterion (reordering model). We begin with the lat-
ter, returning to the constraints later in this section.
2.1 Reordering modeling
In its original formulation, the PSMT approach
includes a basic reordering model, called distor-
tion cost, that exponentially penalizes longer jumps
among consecutively translated phrases (f?i?1, f?i):
d(f?i?1, f?i) = e?|start(f?i)? end(f?i?1)? 1|
A number of more sophisticated solutions have
been proposed to explicitly model word reorder-
ing during decoding. These can mostly be grouped
into three families: phrase orientation models, jump
models and source decoding sequence models.
Phrase orientation models (Tillmann, 2004;
Koehn et al, 2005; Zens and Ney, 2006; Galley and
Manning, 2008), also known as lexicalized reorder-
ing models, predict the orientation of a phrase with
respect to the last translated one, by classifying it
as monotone, swap or discontinuous. These mod-
els have proven very useful for short and medium-
range reordering and are among the most widely
used in PSMT. However, their coarse classification
of reordering steps makes them unsuitable to predict
long-range reorderings.
Jump models (Al-Onaizan and Papineni, 2006;
Green et al, 2010; Yahyaei and Monz, 2010) predict
the direction and length of a jump to perform after
a given input word2. Both these works achieve their
best Arabic-English results within a rather small DL:
namely, 8 in (Al-Onaizan and Papineni, 2006) and
5 in (Green et al, 2010), thus failing to capture the
rare but crucial long reorderings that were their main
motivation. A drawback of this approach is that
long jumps are typically penalized because of their
low frequency compared to short jumps. This strong
bias is undesirable, given that we are especially in-
terested in detecting probable long reorderings.
Source decoding sequence models predict which
input word is likely to be translated at a given state
of decoding. For instance, reordered source lan-
guage models (Feng et al, 2010) are smoothed n-
gram models trained on a corpus of source sentences
reordered to match the target word order. When inte-
grated into the SMT system, they assign a probabil-
ity to each newly translated word given the n-1 pre-
viously translated words. Finally, source word pair
reordering models (Visweswariah et al, 2011) esti-
mate, for each pair of input words i and j, the cost
of translating j right after i given various features of
i, j and their respective contexts. Differently from
reordered source LMs, these models are discrimina-
tive and can profit from richer feature sets. At the
same time, they do not employ decoding history-
based features, which allows for more effective hy-
2In this paper, input (or source) word denotes the word at a
given position of the input sentence, rather than a word type.
328
pothesis recombination. The model we are going to
present belongs to this last sub-group, which we find
especially suitable to predict long reorderings.
2.2 Reordering constraints
The reordering constraint originally included in the
PSMT framework and implemented in our reference
toolkit, Moses (Koehn et al, 2007), is called dis-
tortion limit (DL). This consists in allowing the de-
coder to skip, or jump, at most k words from the last
translated phrase to the next one. More precisely, the
limit is imposed on the distortion D between consec-
utively translated phrases (f?i?1, f?i):
D(f?i?1, f?i) =
???start(f?i)? end(f?i?1)? 1
??? ? DL
Limiting the input permutation space is necessary
for beam-search PSMT decoders to function in lin-
ear time. Reordering constraints are also important
for translation quality because the existing models
are typically not discriminative enough to guide the
search over very large sets of reordering hypotheses.
Despite their crucial effects on the complexity of
reordering modeling, though, reordering constraints
have drawn less attention in the literature. The ex-
isting reordering constraints are typically based on
word-to-word distances ? IBM (Berger et al, 1996)
and DL (Koehn et al, 2007) ? or on permutation pat-
terns ? ITG (Wu, 1997). Both kinds of constraints
are uniform throughout the input sentence, and in-
sensitive to the word being translated and to its con-
text. This results in a very coarse definition of the
reordering search space, which is problematic in lan-
guage pairs with different syntactic structures.
To address this problem, Yahyaei and Monz
(2010) present a technique to dynamically set the
DL: they train a classifier to predict the most prob-
able jump length after each input word, and use the
predicted value as the DL after that position. Un-
fortunately, this method can generate inconsistent
constraints leading to decoding dead-ends. As a so-
lution, the dynamic DL is relaxed when needed to
reach the first uncovered position. Translation im-
provements are reported only on a small-scale task
with short sentences (BTEC), over a baseline that in-
cludes a very simple reordering model. In our work
we develop this idea further and use a reordering
model to predict which specific input words, rather
than input intervals, are likely be translated next.
Moreover, our solution is not affected by the con-
straint inconsistency problem (see Sect. 4).
In another related work, Bisazza and Federico
(2012) generate likely reorderings of the input sen-
tence by means of language-specific fuzzy rules
based on shallow syntax. Long jumps are then sug-
gested to the PSMT decoder by reducing the distor-
tion cost for specific pairs of input words. In com-
parison to the dynamic DL, that is a much finer way
to define the reordering space, leading to consistent
improvements of both translation quality and effi-
ciency over a strong baseline. However, the need of
specific reordering rules makes the method harder to
apply to new language pairs.
3 The WaW reordering model
We model reordering as the problem of deciding
whether a given input word should be translated
after another (Word-after-Word). This formulation
is particularly suitable to help the decoder decide
whether a reordering path is promising enough to
be further explored. Moreover, when translating a
sentence, choosing the next source word to translate
appears as a more natural problem than guessing
how much to the left or to the right we should
move from the current source position. The WaW
reordering model addresses a binary decision task
through the following maximum-entropy classifier:
P (Ri,j=Y |fJ1 , i, j) =
exp[
?
m ?mhm(fJ1 , i, j, Ri,j=Y )]?
Y ? exp[
?
m ?mhm(fJ1 , i, j, Ri,j=Y ?)]
where fJ1 is a source sentence of J words, hm are
feature functions and ?m the corresponding feature
weights. The outcome Y can be either 1 or 0, with
Ri,j=1 meaning that the word at position j is trans-
lated right after the word at position i.
Our WaW reordering model is strongly related to
that of Visweswariah et al (2011) ? hereby called
Travelling Salesman Problem (TSP) model ? with
few important differences: (i) we do not include
in the features any explicit indication of the jump
length, in order to avoid the bias on short jumps;
(ii) they train a linear model with MIRA (Cram-
mer and Singer, 2003) by minimizing the number
329
of input words that get placed after the wrong po-
sition, while we use a maximum-entropy classifier
trained by maximum-likelihood; (iii) they use an
off-the shelf TSP solver to find the best source sen-
tence permutation and apply it as pre-processing to
training and test data. By contrast, we integrate the
maximum-entropy classifier directly into the SMT
decoder and let al its other models (phrase orien-
tation, translation, target LM etc.) contribute to the
final reordering decision.
3.1 Features
Like the TSP model (Visweswariah et al, 2011),
the WaW model builds on binary features similar
to those typically employed for dependency parsing
(McDonald et al, 2005): namely, combinations of
surface forms or POS tags of the words i and j and
their context. Our feature templates are presented in
Table 1. The main novelties with respect to the TSP
model are the mixed word-POS templates (rows 16-
17) and the shallow syntax features. In particular, we
use the chunk types of i, j and their context (18-19),
as well as the chunk head words of i and j (20). Fi-
nally we add a feature to indicate whether the words
i and j belong to the same chunk (21). The jump
orientation ? forward/backward ? is included in the
features that represent the words comprised between
i and j (rows 6, 7, 14, 15). No explicit indication of
the jump length is included in any feature.
3.2 Training data
To generate training data for the classifier, we first
extract reference reorderings from a word-aligned
parallel corpus. Given a parallel sentence, differ-
ent heuristics may be used to convert arbitrary word
alignments to a source permutation (Birch et al,
2010; Feng et al, 2010; Visweswariah et al, 2011).
Similarly to this last work, we compute for each
source word fi the mean ai of the target positions
aligned to fi, then sort the source words according
to this value.3 As a difference, though, we do not
discard unaligned words but assign them the mean
3Using the mean of the aligned indices makes the gener-
ation of reference permutations more robust to alignment er-
rors. Admittedly, this heuristic does not handle well the case of
source words that are correctly aligned to non-consecutive tar-
get words. However, this phenomenon is also not captured by
standard PSMT models, who only learn continuous phrases.
i?2 i?1 i i+1 b j?1 j j+1
1 w w
2 w w w
3 w w w w
4 w w w w
5 w w w w
6 w w w
7 wall w w
8 p p
9 p p p
10 p p p p
11 p p p p
12 p p p p
13 p p p p p p
14 p p p
15 pall p p
16 w p
17 p w
18 c c
19 c c c c c c
20 h h
21 belong to same chunk(i, j)?
w: word identity, p: POS tag, c: chunk type, h: chunk head
Table 1: Feature templates used to learn whether a source
position j is to be translated right after i. Positions com-
prised between i and j are denoted by b and generate two
feature templates: one for each position (6 and 14) and
one for the concatentation of them all (7 and 15).
of their neighbouring words? alignment means, so
that a complete permutation of the source sentence
(?) is obtained. Table 2(a) illustrates this procedure.
Given the reference permutation, we then gener-
ate positive and negative training samples by simu-
lating the decoding process. We traverse the source
positions in the order defined by ?, keeping track of
the positions that have already been covered and, for
each t : 1 ? t ? J , generate:
? one positive sample (R?t,?t+1=1) for the
source position that comes right after it,
? a negative sample (R?t,u=0) for each source
position in {u : ?t??+1 < u < ?t+?+1 ?
u $= ?t+1} that has not yet been translated.
Here, the sampling window ? serves to control the
size of the training data and the proportion between
positive and negative samples. Its value naturally
correlates with the DL used in decoding. The gener-
ation of training samples is illustrated by Table 2(b).
330
(a) Converting word alignments to a permutation:
source words are sorted by their target algnments
mean a. The unaligned word ?D? is assigned the
mean of its neighbouring words? a values (2 +
5)/2 = 3.5 :
(b) Generating binary samples by simulating the
decoding process: shaded rounds represent cov-
ered positions, while dashed arrows represent
negative samples:
Table 2: The classifier?s training data generation process.
3.3 Integration into phrase-based decoding
Rather than using the new reordering model for
data pre-processing as done by (Visweswariah et al,
2011), we directly integrate it into the PSMT de-
coder Moses (Koehn et al, 2007).
Two main computation phases are required by the
WaWmodel: (i) at system initialization time, all fea-
ture weights are loaded into memory, and (ii) before
translating each new sentence, features are extracted
from it and model probabilities are pre-computed
for each pair of source positions (i, j) such that
|j ? i ? 1| ? DL. Note that this efficient solution
is possible because our model does not employ de-
coding history-based features, like the word that was
translated before the last one, or like the previous
jump legth. This is an important difference with re-
spect to the reordered source LM proposed by Feng
et al (2010), which requires inclusion of the last n
translated words in the decoder state.
Fig. 1 illustrates the scoring process: when a par-
tial translation hypothesis H is expanded by cover-
ing a new source phrase f? , the model returns the
log-probability of translating the words of f? in that
particular order, just after the last translated word of
H. In details, this is done by converting the phrase-
internal word alignment4 to a source permutation, in
just the same way it was done to produce the model?s
training examples. Thus, the global score is inde-
pendent from phrase segmentation, and normalized
across outputs of different lengths: that is, the proba-
bility of any complete hypothesis decomposes into J
factors, where J is the length of the input sentence.
The WaW reordering model is fully compatible
with, and complementary to the lexicalized reorder-
ing (phrase orientation) models included in Moses.
Figure 1: Integrating the binary word reordering model
into a phrase-based decoder: when a new phrase is
covered (dashed boxes), the model returns the log-
probability of translating its words in the order defined
by the phrase-internal word alignment.
4 Early pruning of reordering steps
We now explain how the WaW reordering model can
be used to dynamically refine the input permutation
space. This method is not dependent on the particu-
lar classifier described in this paper, but can in prin-
ciple work with any device estimating the probabil-
ity of translating a given input word after another.
The method consists of querying the reordering
model at the time of hypothesis expansion, and fil-
tering out hypotheses solely based on their reorder-
ing score. The rationale is to avoid costly hypoth-
esis expansions for those source positions that the
reordering model considers very unlikely to be cov-
ered at a given point of decoding. In practice, this
works as follows:
? at each hypothesis expansion, we first enumer-
ate the set of uncovered input positions that
are reachable within a fixed DL, and query the
WaW reordering model for each of them5;
4Phrase-internal alignments are provided in the phrase table.
5The score used to prune a new word range f? is the log prob-
ability of translating the first aligned word of f? right after the
last translated word of the current hypothesis. See also Sect. 3.3.
331
? only based on the WaW score, we apply his-
togram and threshold pruning to this set and
proceed to expand the non-pruned positions.
Furthermore, it is possible to ensure that local re-
orderings are always allowed, by setting a so-called
non-prunable-zone of width ? around the last cov-
ered input position.6
According to how the DL, pruning parameters,
and ? are set, we can actually aim at different tar-
gets: with a low DL, loose pruning parameters, and
?=0 we can try to speed up search without sacrific-
ing much translation quality. With a high DL, strict
pruning parameters, and a medium ?, we ensure that
the standard medium-range reordering space is ex-
plored, as well as those few long jumps that are
promising according to the reordering model. In our
experiments, we explore this second option with the
setting DL=18 and ?=5.
The underlying idea is similar to that of early
pruning proposed by Moore and Quirk (2007),
which consisted in discarding possible extensions of
a partial hypothesis based on their estimated score
before computing the exact language model score.
Our technique too has the effect of introducing ad-
ditional points at which the search space is pruned.
However, while theirs was mainly an optimization
technique meant to avoid useless LM queries, we in-
stead aim at refining the search space by exploiting
the fact that some SMT models are more important
than others at different stages of the translation pro-
cess. Our approach actually involves a continuous
alternation of two processes: during hypothesis ex-
pansion the reordering score is combined with all
other scores, while during early pruning some re-
ordering decisions are taken only based on the re-
ordering score. In this way, we try to combine the
benefits of fully integrated reordering models with
those of monolingual pre-ordering methods.
5 Evaluation
We test our approach on an Arabic-English news
translation task where sentences are typically long
and complex. In this language pair, long reorder-
ing errors mostly concern verbs, as all of Subject-
Verb-Object (SVO), VSO and, more rarerly, VOS
6See Bisazza (2013) for technical details on the integration
of word-level pruning with phrase-level hypothesis expansion.
constructions are attested in modern written Ara-
bic. This issue is well known in the SMT field and
was addressed by several recent works, with deep
or shallow parsing-based techniques (Green et al,
2009; Carpuat et al, 2012; Andreas et al, 2011;
Bisazza et al, 2012). We question whether our ap-
proach ? which is not conceived to solve this spe-
cific problem, nor requires manual rules to predict
verb reordering ? will succeed in improving long re-
ordering in a fully data-driven way.
As SMT training data, we use all the in-domain
parallel data provided for the NIST-MT09 evalua-
tion for a total of 986K sentence pairs (31M English
words).7 The target LM used to run the main se-
ries of experiments is trained on the English side of
all available NIST-MT09 parallel data, UN included
(147M words). In the large-scale experiments, the
LM training data also include the sections of the En-
glish Gigaword that best fit to the development data
in terms of perplexity: namely, the Agence France-
Presse, Xinhua News Agency and Associated Press
Worldstream sections (2130M words in total).
For development and test, we use the newswire
sections of the NIST benchmarks: dev06-nw, eval08-
nw, eval09-nw consisting of 1033, 813, 586 sen-
tences respectively. Each set includes 4 reference
translations and the average sentence length is 33
words. To focus the evaluation on problematic re-
ordering, we also consider a subset of eval09-nw
containing only sentences where the Arabic main
verb is placed before the subject (vs-09: 299 sent.).8
As pre-processing, we apply standard tokeniza-
tion to the English data, while the Arabic data is
segmented with AMIRA (Diab et al, 2004) accord-
ing to the ATB scheme9. The same tool also pro-
duces POS tagging and shallow syntax annotation.
7The in-domain parallel data includes all the provided cor-
pora except the UN proceedings, and the non-newswire parts of
the small GALE-Y1-Q4 corpus (that is 9K sentences of audio
transcripts and web data). As reported by Green et al (2010)
the removal of UN data does not affect baseline performances
on the news benchmarks.
8Automatically detected by means of shallow syntax rules.
9The Arabic Treebank tokenization scheme isolates con-
junctions w+ and f+, prepositions l+, k+, b+, future marker
s+, pronominal suffixes, but not the article Al+.
332
5.1 Reordering model intrinsic evaluation
Before proceeding to the SMT experiments, we
evaluate the performance of the WaW reorder-
ing model in isolation. All the tested configura-
tions are trained with the freely available MegaM
Toolkit10, implementing the conjugate gradient
method (Hestenes and Stiefel, 1952), in maximum
100 iterations. Training samples are generated
within a sampling window of width ?=10, from a
subset (30K sentences) of the parallel data described
above, resulting in 8M training word pairs11. Test
samples are generated from TIDES-MT04 (1324 sen-
tences, 370K samples with ?=10), one of the corpora
included in our SMT training data. Features with
less than 20 occurrences are ignored.
Classification accuracy. Table 3 presents preci-
sion, recall, and F-score achieved by different fea-
ture subsets, where W stands for word-based, P for
POS-based and C for chunk-based feature templates.
We can see that all feature types contribute to im-
prove the classifier?s performance. The word-based
model achieves the highest precision but a very low
recall, while the POS-based has much more bal-
anced scores. A better performance overall is ob-
tained by combining word-, POS- and mixed word-
POS-based features (62.6% F-score). Finally, the
addition of chunk-based features yields a further im-
provement of about 1 point, reaching 63.8% F-score.
Given these results, we decide to use the W,P,C
model for the rest of the evaluation.
Features (templates) P R F
W [1-7] 73.1 16.4 26.8
P [8-15] 69.5 54.8 61.3
W,P [1-17] 70.2 56.5 62.6
W,P,C [1-21] 70.6 58.1 63.8
Table 3: Classification accuracy of the WaW reordering
model on TIDES-MT04, using different feature subsets.
The template numbers refer to the rows of Table 1.
Ranking accuracy. A more important aspect to
evaluate for our application is how well our model?s
scores can rank a typical set of reordering options.
In fact, the WaW model is not meant to be used as
10http://www.cs.utah.edu/?hal/megam/ (Daume? III, 2004).
11This is the maximum number of samples manageable by
MegaM. However, even scaling from 4M to 8M was only
slightly helpful in our experiments. In the future we plan to test
other learning approaches that scale better to large data sets.
a stand-alone classifier, but as one of several SMT
feature functions. Moreover, for early reordering
pruning to be effective, it is especially important that
the correct reordering option be ranked in the top n
among those available at the time of a given hypoth-
esis expansion. In order to measure this, we simulate
the decoding process by traversing the source words
in target order and, for each of them, we examine
the ranking of all words that may be translated next
(i. e. the uncovered positions within a given DL).
We check how often the correct jump was ranked
first (Top-1) or at most third (Top-3). We also com-
pute the latter score on long reorderings only (Top-
3-long): i. e. backward jumps with distortion D>7
and forward jumps with D>6. In Table 4, results
are compared with the ranking produced by standard
distortion, which always favors shorter jumps. Two
conditions are considered: DL=10 corresponding to
the sampling window ? used to produce the training
data, and DL=18 that is the maximum distortion of
jumps that will be considered in our early-pruning
SMT experiment.
Model DL DL-err Top-1 Top-3 Top-3-longback forw.
Distortion 10 2.4 61.8 79.6 50.7 66.018 0.8 62.0 80.0 18.9 52.3
WaW 10 2.4 71.2 91.2 76.4 69.318 0.8 71.2 91.8 68.0 51.8
Table 4: Word-to-word jump ranking accuracy (%) of
standard distortion and WaW reordering model, in dif-
ferent DL conditions. DL-err is the percentage of correct
jumps beyond DL. The test set consists of 40K reordering
decisions: one for each source word in TIDES-MT04.
We can see that, in terms of overall accuracies, the
WaW reordering model outperforms standard distor-
tion by a large margin (about 10% absolute). This
is an important result, considering that the jump
length, strongly correlating with the jump likeli-
hood, is not directly known to our model. As re-
gards the DL, the higher limit naturally results in a
lower DL-error rate (percentage of correct jumps be-
yond DL): namely 0.8% instead of 2.4%. However,
jump prediction becomes much harder: Top-3 accu-
racy of long jumps by distortion drops from 50.7%
to 18.9% (backward) and from 66.0% to 52.3% (for-
ward). Our model is remarkably robust to this effect
on backward jumps, where it achieves 68.0% accu-
333
racy. Due to the syntactic characteristics of Arabic
and English, the typical long reordering pattern con-
sists in (i) skipping a clause-initial Arabic verb, (ii)
covering a long subject, then finally (iii) jumping
back to translate the verb and (iv) jumping forward
to continue translating the rest of the sentence (see
Fig. 3 for an example).12 Deciding when to jump
back to cover the verb (iii) is the hardest part of
this process, and that is precisely where our model
seems more helpful, while distortion always prefers
to proceed monotonically achieving a very low ac-
curacy of 18.9%. In the case of long forward jumps
(iv), instead, distortion is advantaged as the correct
choice typically corresponds to translating the first
uncovered position, that is the shortest jump avail-
able from the last translated word. Even here, our
model achieves an accuracy of 51.8%, only slightly
lower than that of distortion (52.3%).
In summary, the WaW reordering model signifi-
cantly outperforms distortion in the ranking of long
jumps. In the large majority of cases, it is able to
rank a correct long jump in the top 3 reordering op-
tions, which suggests that it can be effectively used
for early reordering pruning.
5.2 SMT experimental setup
Our SMT systems are built with the Moses toolkit,
while word alignment is produced by the Berke-
ley Aligner (Liang et al, 2006). The baseline de-
coder includes a phrase translation model, a lexi-
calized reordering model, a 6-gram target language
model, distortion cost, word and phrase penalties.
More specifically, the baseline reordering model is a
hierarchical phrase orientation model (Tillmann,
2004; Koehn et al, 2005; Galley and Manning,
2008) trained on all the available parallel data. This
variant was shown to outperform the default word-
based on an Arabic-English task. To make our base-
line even more competitive, we apply early distor-
tion cost, as proposed by Moore and Quirk (2007).
This function has the same value as the standard one
over a complete translation hypothesis, but it antic-
ipates the gradual accumulation of the cost, mak-
ing hypotheses of the same length more compara-
ble to one another. Note that this option has no ef-
12Clearly, we would expect different figures from testing the
model on another language pair like German-English, where the
verb is often postponed in the source with respect to the target.
fect on the distortion limit, but only on the distor-
tion cost feature function. As proposed by Johnson
et al (2007), statistically improbable phrase pairs
are removed from the translation model. The lan-
guage models are estimated by the IRSTLM toolkit
(Federico et al, 2008) with modified Kneser-Ney
smoothing (Chen and Goodman, 1999).
Feature weights are optimized by minimum
BLEU-error training (Och, 2003) on dev06-nw. To
reduce the effects of the optimizer instability, we
tune each configuration four times and use the av-
erage of the resulting weight vectors to translate the
test sets, as suggested by Cettolo et al (2011).
Finally, eval08-nw is used to select the early prun-
ing parameters for the last experiment, while eval09-
nw is always reserved as blind test.
5.3 Evaluation metrics
We evaluate global translation quality with BLEU
(Papineni et al, 2002) and METEOR (Banerjee and
Lavie, 2005). These metrics, though, are only in-
directly sensitive to word order, and especially un-
likely to capture improvements at the level of long-
range reordering. For this reason, we also com-
pute the Kendall Reordering Score or KRS (Birch
et al, 2010) which is a positive score based on the
Kendall?s Tau distance between the source-output
permutation pi and the source-reference permuta-
tions ?:
KRS(pi,?) = (1?
?
K(pi,?)) ? BP
K(pi,?) =
?n
i=1
?n
j=1 d(i, j)
1
2n(n? 1)
d(i, j) =
{
1 if pii < pij and ?i > ?j
0 otherwise
where BP is a sentence-level brevity penalty, similar
to that of BLEU. The KRS is robust to lexical choice
because it performs no comparison between output
and reference words, but only between the positions
of their translations. Besides, it was shown to corre-
late strongly with human judgements of fluency.
Our work specifically addresses long-range re-
ordering phenomena in language pairs where these
are quite rare, although crucial for preserving the
source text meaning. Hence, an improvement at this
level may not be detected by the general-purpose
metrics. We then develop a KRS variant that is only
334
sensitive to the positioning of specific input words.
Assuming that each input word fi is assigned a
weight ?i, the formula above is modified as follows:
d?(i, j) =
{
?i+?j if pii < pij and ?i > ?j
0 otherwise
A similar element-weighted version of Kendall Tau
was proposed by Kumar and Vassilvitskii (2010) to
evaluate document rankings in information retrieval.
Because long reordering errors in Arabic-English
mostly affect verbs, we set the weights to 1 for verbs
and 0 for all other words to only capture verb re-
ordering errors, and call the resulting metric KRS-V.
The source-reference word alignments needed to
compute the reordering scores are generated by the
Berkeley Aligner previously trained on the training
data. Source-output word alignments are instead ob-
tained from the decoder?s trace.
5.4 Results and discussion
To motivate the choice of our baseline setup (early
distortion cost and DL=8), we first compare the per-
formance of standard and early distortion costs un-
der various DL conditions.
???
????
???
???
???
???
??????
???
???
???
???
???
???
???
???
??? ??? ??? ??? ???
??
???
???
?????
Figure 2: Standard vs early distortion cost results on
eval08-nw under different distortion limits (DL), using
the medium-size LM. Best scores are on top-right corner.
As shown in Fig. 2, most results are close to each
other in terms of BLEU and KRS, but early distor-
tion consistently outperforms the standard one (sta-
tistically significant). The most striking difference
appears at a very high distortion limit (18), where
standard distortion scores drop by more than 1 BLEU
point and almost 7 KRS points! Early distortion is
much more robust (only -1 KRS when going from
DL=8 to DL=18), which makes our baseline system
especially strong at the level of reordering.
Table 5 presents the results obtained by integrat-
ing the WaW reordering model as an additional
feature function, and by applying early reordering
pruning. The upper part of the table refers to the
medium-scale evaluation, while the lower part refers
to the large-scale evaluation. In each part, statis-
tical significance is computed against the baseline
[B] by approximate randomization as in (Riezler and
Maxwell, 2005). Run times are obtained by an Intel
Xeon X5650 processor on the first 500 sentences of
eval08-nw, and exclude loading time of all models.
Medium-scale evaluation. Integrating the WaW
model as an additional feature function results in
small but consistent improvements in all DL condi-
tions, which shows that this type of model conveys
information that is missing from the state-of-the-art
reordering models. As regards efficiency, the new
model makes decoding time increase by 8%.
Among the DL settings considered, DL=8 is con-
firmed as the optimal one ? with or without WaW
model. Raising the DL to 18 with no special prun-
ing has a negative impact on both translation quality
and efficiency. The effect is especially visible on the
reordering scores: that is, from 84.7 to 83.9 KRS and
from 86.2 to 85.8 KRS-V on eval09-nw. Run times
are almost doubled: from 87 to 164 and from 94 to
178 ms/word, that is a 89% increase.
We then proceed to the last experiment where the
reordering space is dynamically pruned based on
the WaW model score. As explained in Sect. 4, a
non-prunable-zone of width ?=5 is set around the
last covered position. To set the early pruning pa-
rameters, we perform a grid search over the values
(1, 2, 3, 4, 5) for histogram and (0.5, 0.25, 0.1) for
relative threshold, and select the values that achieve
the best BLEU and KRS on eval08-nw, namely 3 (his-
togram) and 0.1 (threshold). The resulting configu-
ration is then re-optimized by MERT on dev06-nw.
This setting implies that, at a given point of decod-
ing where i is the last covered position, a new word
can be translated only if:
? it lies within a DL of 5 from i, or
? it lies within a DL of 18 from i and its WaW
reordering score is among the top 3 and at least
equal to 1/10 of the best score (in linear space).
As shown in Table 5, early pruning achieves the
best results overall: despite the high DL, we report
335
DL Reo.models eval08-nw eval09-nw vs-09 ms/bleu met krs krs-V bleu met krs krs-V krs-V word
Using the medium-size LM (147M English tokens):
5 hier.lexreo, early disto 44.7 35.1
! 83.0! 84.7! 50.3" 38.1 84.6 85.9 84.7 59
+ WaW model 44.8 35.1 83.7 85.4 51.0# 38.3# 85.1# 86.6$ 85.5# 64
8 hier.lexreo, early disto[B] 44.8 35.2 83.4 85.6 50.6 38.1 84.7 86.2 84.8 87+ WaW model 45.0 35.2 83.7$ 85.9 51.1# 38.3# 85.1# 86.8# 85.8# 94
18
hier.lexreo, early disto 44.7 34.9! 82.4! 84.9! 50.3 38.0" 83.9! 85.8" 84.3" 164
+ WaW model 44.8 35.2 82.7! 85.5 51.0$ 38.3# 84.2" 86.2 85.2 178
+ early reo.pruning(?=5) 45.0 35.3 83.7$ 86.3# 50.9 38.3# 84.9 87.0# 86.2# 68
Using the large interpolated LM (2130M English tokens) and double beam-size:
8 hier.lexreo, early disto[B] 46.3 35.0 83.2 85.0 51.6 38.3 84.5 85.8 84.5 2579
18 hier.lexreo, early disto 45.9
" 34.9! 81.7! 84.1! 51.4 38.1! 83.0! 84.6! 83.1! 5462
+WaW+reo.pruning(?=5) 46.3 35.2 83.4 85.7# 52.8# 38.6# 84.6 86.6# 85.5# 1588
Table 5: Effects of WaW reordering modeling and early reordering pruning on translation quality, measured with
% BLEU, METEOR, and Kendall Reordering Score: regular (KRS) and verb-specific (KRS-V). Statistically significant
differences with respect to the baseline [B] are marked with #! at the p ? .05 level and $" at the p ? .10 level.
Decoding time is measured in milliseconds per input word.
no loss in BLEU, METEOR and KRS, but we actually
see several improvements. In particular, the gains on
the blind test eval09-nw are +0.3 BLEU, +0.2 ME-
TEOR and +0.2 KRS (only METEOR is significant).
While these gains are admittedly small, we recall
that our techniques affect rare and isolated events
which can hardly emerge from the general-purpose
evaluation metrics. Moreover, to our knowledge,
this is the first time that a PSMT system is shown to
maintain a good performance on this language pair
while admitting very long-range reorderings.
Finally and more importantly, the reordering of
verbs improves significantly on both generic tests
and on the VS- sentence subset (vs-09): namely, in
the latter, we achieve a notable gain of 1.4 KRS-V.
Efficiency is also largely improved by our early
reordering pruning technique: decoding time is re-
duced to 68 ms/word, corresponding to a 22%
speed-up over the baseline.
Large-scale evaluation. We also investigate
whether our methods can be useful in a scenario
where efficiency is less important and more data
is available for training. To this end, we build a
very large LM by interpolating the main LM with
three other LMs trained on different Gigaword sec-
tions (see Sect. 5). Moreover, we relax the decoder?s
beam size from the default value of 200 to 400 hy-
potheses, to reduce the risk of search errors and ob-
tain the best possible baseline performance.
By comparing the large-scale with the medium-
scale baseline in Table 5, we note that the addition
of LM data is especially beneficial for BLEU (+1.5
on eval08-nw and +1.0 on eval09-nw), but not as
much for the other metrics, which challenges the
commonly held idea that more data always improves
translation quality.
Here too, relaxing the DL without special pruning
hurts not only efficiency but also translation qual-
ity: all the scores decrease considerably, showing
that even the stronger LM is not sufficient to guide
search through a very large reordering search space.
As for our enhanced system, it achieves simi-
lar gains as in the medium-scale scenario: that is,
BLEU and METEOR are preserved or slightly im-
proved despite the very high DL, while all the re-
ordering scores increase. In particular, we report sta-
tistically significant improvements in the reordering
of verbs, which is where the impact of our method is
expected to concentrate (+0.7, +0.8 and +1.0 KRS-V
on eval08-nw, eval09-nw and vs-09, respectively).
These results confirm the usefulness of our
method not only as an optimization technique, but
also as a way to improve translation quality on top
of a very strong baseline.
336
SRC
! ! ! " #$!%&' ( )*
+, -. /0$%&' &-1! 2 +1 +03* +045(67!8 +9# +77! 5 :65 &-3*;24<5( &-73! 045( &-=>?@A ( 0B* +CD EF(23*
verb subj. obj. compl.
ywASl sfyr Almmlkp AlErbyp AlsEwdyp ldY lbnAn EbdAlEzyz xwjp tHrk -h fy AtjAh ...
continues ambassador Kingdom Arabian Saudi to Lebanon Abdulaziz Khawja move his in direction
REF The Kingdom of Saudi Arabia ?s ambassador to Lebanon Abdulaziz Khawja continues his moves towards ...
BASE continue to Saudi Arabian ambassador to Lebanon , Abdulaziz Khwja its move in the direction of ...
NEW The Kingdom of Saudi Arabia ?s ambassador to Lebanon , Abdulaziz Khwja continue its move in the direction of ...
SRC
;#7*$G'( H( +0&B5 ( )I( E4 J<K 65# +L M#NL &-O01 P5 )*QR#7*<5( S!
&7=@A ( TU*V3W XY. #8; #?7* +,
adv. verb obj. subj. compl.
fymA dEA -hm r}ys Almktb AlsyAsy l- Hrkp HmAs xAld m$El AlY AltzAm AlHyAd
meanwhile called them head bureau political of movement Hamas Khaled Mashal to necessity neutrality
REF Meanwhile, the Head of the Political Bureau of the Hamas movement, Khaled Mashal, called upon them to remain neutral
BASE The called them, head of Hamas? political bureau, Khalid Mashal, to remain neutral
NEW The head of Hamas? political bureau, Khalid Mashal, called on them to remain neutral
Figure 3: Long reordering examples showing improvements over the baseline system (BASE) when the DL is raised to
18 and early pruning based on WaW reordering scores is enabled (NEW).
Long jumps statistics and examples. To better
understand the behavior of the early-pruning system,
we extract phrase-to-phrase jump statistics from the
decoder log file. We find that 132 jumps beyond the
non-prunable zone (D>5) were performed to trans-
late the 586 sentences of eval09-nw; 38 out of these
were longer than 8 and mostly concentrated on the
VS- sentence subset (27 jumps D>8 performed in
vs-09).13 This and the higher reordering scores sug-
gest that long jumps are mainly carried out to cor-
rectly reorder clause-inital verbs over long subjects.
Fig. 3 shows two Arabic sentences taken from
eval09-nw, that were erroneuously reordered by the
baseline system. The system including the WaW
model and early reordering pruning, instead, pro-
duced the correct translation. The first sentence is
a typical example of VSO order with a long subject:
while the baseline system left the verb in its Ara-
bic position, producing an incomprehensible trans-
lation, the new system placed it rightly between the
English subject and object. This reordering involved
two long jumps: one with D=9 backward and one
with D=8 forward.
The second sentence displays another, less com-
mon, Arabic construction: namely VOS, with a per-
sonal pronoun object. In this case, a backward jump
with D=10 and a forward jump with D=8 were nec-
essary to achieve the correct reordering.
13Statistics computed on the medium-LM system.
6 Conclusions
We have trained a discriminative model to predict
likely reordering steps in a way that is complemen-
tary to state-of-the-art PSMT reordering models. We
have effectively integrated it into a PSMT decoder as
additional feature, ensuring that its total score over a
complete translation hypothesis is consistent across
different phrase segmentations. Lastly, we have pro-
posed early reordering pruning as a novel method
to dynamically shape the input reordering space and
capture long-range reordering phenomena that are
often critical when translating between languages
with different syntactic structures.
Evaluated on a popular Arabic-English news
translation task against a strong baseline, our ap-
proach leads to similar or even higher BLEU, ME-
TEOR and KRS scores at a very high distortion limit
(18), which is by itself an important achievement.
At the same time, the reordering of verbs, measured
with a novel version of the KRS, is consistently im-
proved, while decoding gets significantly faster. The
improvements are also confirmed when a very large
LM is used and the decoder?s beam size is dou-
bled, which shows that our method reduces not only
search errors but also model errors even when base-
line models are very strong.
Word reordering is probably the most difficult as-
pect of SMT and an important factor of both its qual-
ity and efficiency. Given its strong interaction with
the other aspects of SMT, it appears natural to solve
337
word reordering during decoding, rather than before
or after it. To date, however, this objective was only
partially achieved. We believe there is a promising
way to go between fully-integrated reordering mod-
els and monolingual pre-ordering methods. This
work has started to explore it.
Aknowledgments
This work was partially funded by the European
Union FP7 grant agreement 287658 (EU-BRIDGE).
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
529?536, Sydney, Australia, July.
Jacob Andreas, Nizar Habash, and Owen Rambow. 2011.
Fuzzy syntactic reordering for phrase-based statistical
machine translation. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 227?
236, Edinburgh, Scotland, July.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, Michigan, June.
A. L. Berger, P. F. Brown, S. A. Della Pietra, V. J. Della
Pietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer.
1996. Language translation apparatus and method of
using context-based translation models. United States
Patent, No. 5510981, Apr.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2009. A quantitative analysis of reordering phenom-
ena. In StatMT ?09: Proceedings of the Fourth Work-
shop on Statistical Machine Translation, pages 197?
205, Morristown, NJ, USA.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating reorder-
ing. Machine Translation, 24(1):15?26.
Arianna Bisazza and Marcello Federico. 2012. Modi-
fied distortion matrices for phrase-based statistical ma-
chine translation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 478?487, Jeju Is-
land, Korea, July.
Arianna Bisazza, Daniele Pighin, and Marcello Fed-
erico. 2012. Chunk-lattices for verb reordering in
Arabic-English statistical machine translation. Ma-
chine Translation, Special Issue on MT for Arabic,
26(1-2):85?103.
Arianna Bisazza. 2013. Linguistically Motivated Re-
ordering Modeling for Phrase-Based Statistical Ma-
chine Translation. Ph.D. thesis, University of Trento.
http://eprints-phd.biblio.unitn.it/1019/.
Marine Carpuat, Yuval Marton, and Nizar Habash. 2012.
Improved Arabic-to-English statistical machine trans-
lation by reordering post-verbal subjects for word
alignment. Machine Translation, Special Issue on MT
for Arabic, 26(1-2):105?120.
Mauro Cettolo, Nicola Bertoldi, and Marcello Federico.
2011. Methods for smoothing the optimizer instability
in SMT. In MT Summit XIII: the Thirteenth Machine
Translation Summit, pages 32?39, Xiamen, China.
Stanley F. Chen and Joshua Goodman. 1999. An empiri-
cal study of smoothing techniques for language model-
ing. Computer Speech and Language, 4(13):359?393.
Marta R. Costa-jussa` and Jose? A. R. Fonollosa. 2009.
State-of-the-art word reordering approaches in statisti-
cal machine translation: A survey. IEICE TRANSAC-
TIONS on Information and Systems, E92-D(11):2179?
2185.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
Hal Daume? III. 2004. Notes on CG and LM-BFGS op-
timization of logistic regression. Paper available at
http://pub.hal3.name, implementation avail-
able at http://hal3.name/megam.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic Tagging of Arabic Text: From Raw Text to
Base Phrase Chunks. In Daniel Marcu Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004: Short
Papers, pages 149?152, Boston, Massachusetts, USA.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an Open Source Toolkit for Handling
Large Scale Language Models. In Proceedings of In-
terspeech, pages 1618?1621, Melbourne, Australia.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A source-side decoding sequence model for statisti-
cal machine translation. In Conference of the Associa-
tion for Machine Translation in the Americas (AMTA),
Denver, Colorado, USA.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP ?08: Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 848?856, Morristown, NJ, USA.
Spence Green, Conal Sathi, and Christopher D. Man-
ning. 2009. NP subject detection in verb-initial Ara-
bic clauses. In Proceedings of the Third Workshop
338
on Computational Approaches to Arabic Script-based
Languages (CAASL3), Ottawa, Canada.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL), pages 867?875, Los
Angeles, California.
Magnus R. Hestenes and Eduard Stiefel. 1952. Meth-
ods of conjugate gradients for solving linear systems.
Journal of Research of the National Bureau of Stan-
dards, 49(6):409?436.
H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007. Im-
proving translation quality by discarding most of the
phrasetable. In In Proceedings of EMNLP-CoNLL 07,
pages 967?975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 127?133, Edmonton,
Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proc. of the International Workshop on Spoken Lan-
guage Translation, October.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic.
Ravi Kumar and Sergei Vassilvitskii. 2010. General-
ized distances between rankings. In Proceedings of
the 19th international conference on World Wide Web,
pages 571?580, New York, NY, USA. ACM.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Proceedings of the
conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, HLT
?05, pages 523?530, Stroudsburg, PA, USA.
Robert C. Moore and Chris Quirk. 2007. Faster beam-
search decoding for phrasal statistical machine transla-
tion. In In Proceedings of MT Summit XI, pages 321?
327, Copenhagen, Denmark.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 295?302, Philadelhpia, PA.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Erhard Hinrichs
and Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association of Compu-
tational Linguistics (ACL), pages 311?318, Philadel-
phia, PA.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57?
64, Ann Arbor, Michigan, June.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of the Joint Conference on Human Language
Technologies and the Annual Meeting of the North
American Chapter of the Association of Computa-
tional Linguistics (HLT-NAACL).
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for im-
proved machine translation. In Proceedings of the
2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 486?496, Edinburgh,
Scotland, UK., July.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Sirvan Yahyaei and Christof Monz. 2010. Dynamic dis-
tortion in a discriminative reordering model for sta-
tistical machine translation. In International Work-
shop on Spoken Language Translation (IWSLT), Paris,
France.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In 25th German Confer-
ence on Artificial Intelligence (KI2002), pages 18?32,
Aachen, Germany. Springer Verlag.
339
340
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 88?92,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
FBK at WMT 2010: Word Lattices for
Morphological Reduction and Chunk-based Reordering
Christian Hardmeier, Arianna Bisazza and Marcello Federico
Fondazione Bruno Kessler
Human Language Technologies
Trento, Italy
{hardmeier,bisazza,federico}@fbk.eu
Abstract
FBK participated in the WMT 2010
Machine Translation shared task with
phrase-based Statistical Machine Transla-
tion systems based on the Moses decoder
for English-German and German-English
translation. Our work concentrates on ex-
ploiting the available language modelling
resources by using linear mixtures of large
6-gram language models and on address-
ing linguistic differences between English
and German with methods based on word
lattices. In particular, we use lattices to in-
tegrate a morphological analyser for Ger-
man into our system, and we present some
initial work on rule-based word reorder-
ing.
1 System overview
The Human Language Technologies group at Fon-
dazione Bruno Kessler (FBK) participated in the
WMT 2010 Machine Translation (MT) evaluation
with systems for English-German and German-
English translation. While the English-German
system we submitted was relatively simple, we
put some more effort into the inverse translation
direction to make better use of the abundance
of language modelling data available for English
and to address the richness of German morphol-
ogy, which makes it hard for a Statistical Machine
Translation (SMT) system to achieve good vocab-
ulary coverage. In the remainder of this section,
an overview of the common features of our sys-
tems will be given. The next two sections provide
a more detailed description of our approaches to
language modelling, morphological preprocessing
and word reordering.
Both of our systems were based on the Moses
decoder (Koehn et al, 2007). They were simi-
lar to the WMT 2010 Moses baseline system. In-
stead of lowercasing the training data and adding
a recasing step, we retained the data in document
case throughout our system, except for the mor-
phologically normalised word forms described in
section 3. Our phrase tables were trained with the
standard Moses training script, then filtered based
on statistical significance according to the method
described by Johnson et al (2007). Finally, we
used Minimum Bayes Risk decoding (Kumar and
Byrne, 2004) based on the BLEU score (Papineni
et al, 2002).
2 Language modelling
At the 2009 NIST MT evaluation, our system ob-
tained good results using a mixture of linearly in-
terpolated language models (LMs) combining data
from different sources. As the training data pro-
vided for the present evaluation campaign again
included a large set of language modelling corpora
from different sources, especially for English as
a target language, we decided to adopt the same
strategy. The partial corpora for English and their
sizes can be found in table 1. Our base mod-
els of the English Gigaword texts were trained
on version 3 of the corpus (LDC2007T07). We
trained separate language models for the new data
from the years 2007 and 2008 included in ver-
sion 4 (LDC2009T13). Apart from the mono-
lingual English data, we also included language
models trained on the English part of the addi-
tional parallel datasets supplied for the French-
English and Czech-English tasks. All the mod-
els were estimated as 6-gram models with Kneser-
Ney smoothing using the IRSTLM language mod-
elling toolkit (Federico et al, 2008).
For technical reasons, we were unable to use all
the language models during decoding. We there-
fore selected a subset of the models with the fol-
lowing data selection procedure:
1. For a linear mixture of the complete set of
24 language models, we estimated a set of
88
Corpus n-grams
Europarl v5 115,702,157
News 1,437,562,740
News commentary 10 10,381,511
Gigaword v3: 6 models 7,990,828,834
Gigaword 2007/08: 6 models 1,418,281,597
109 fr-en 1,190,593,051
UNDOC fr-en 333,120,732
CzEng: 7 models 153,355,518
Total: 24 models 12,649,826,140
Table 1: Language modelling corpora for English
LMs Perplexity
DEV EVAL
2 188.57 181.38
5 163.68 158.99
10 156.43 151.73
15 154.71 144.98
20 154.39 144.91
24 154.42 144.92
Table 2: Perplexities of LM mixtures
optimal interpolation weights to minimise
the perplexity of the mixture model on the
news-test2008 development set.
2. By sorting the mixture coefficients in de-
scending order, we obtained an ordering of
the language models by their importance with
respect to the development set. We created
partial mixtures by selecting the top n mod-
els according to this order and retraining the
mixture weights with the same algorithm.
Computing the perplexities of these partial
mixtures on the news-test2008 (DEV) and
newstest2009 (EVAL) corpora shows that signif-
icant improvements can be obtained up to a mix-
tures size of about 15 elements. As this size still
turned out to be too large to be managed by our
systems, we used a 5-element mixture in our final
submission (see table 3 for details about the mix-
ture and table 4 for the evaluation results of the
submitted systems).
For the English-German system, the only cor-
pora available for the target language were Eu-
roparl v5, News commentary v10 and the mono-
lingual News corpus. Similar experiments showed
that the News corpus was by far the most impor-
tant for the text genre to be translated and that
including language models trained on the other
Weight Language model
0.368023 News
0.188156 109 fr-en
0.174802 Gigaword v3: NYT
0.144465 Gigaword v3: AFP
0.124553 Gigaword v3: APW
Table 3: 5-element LM mixture used for decoding
BLEU-cased BLEU
en-de
primary 15.5 15.8
secondary 15.3 15.6
primary: only News language model
secondary: linear mixture of 3 LMs
de-en
primary 20.9 21.9
secondary 20.3 21.3
primary: morph. reduction, linear mixture of 5 LMs
secondary: reordering, only News LM
Table 4: Evaluation results of submitted systems
corpora could even degrade system performance.
We therefore decided not to use Europarl or News
commentary for language modelling in our pri-
mary submission. However, we submitted a sec-
ondary system using a mixture of language models
based on all three corpora.
3 Morphological reduction and
decompounding of German
Compounding is a highly productive part of Ger-
man noun morphology. Unlike in English, Ger-
man compound nouns are usually spelt as sin-
gle words, which greatly increases the vocabulary.
For a Machine Translation system, this property
of the language causes a high number of out-of-
vocabulary (OOV) words. It is likely that many
compounds in an input text have not been seen in
the training corpus. We addressed this problem by
splitting compounds in the German source text.
Compound splitting was done using the Gert-
wol morphological analyser (Koskenniemi and
Haapalainen, 1996), a linguistically informed sys-
tem based on two-level finite state morphology.
Since Gertwol outputs all possible analyses of a
word form without taking into account the context,
the output has to be disambiguated. For this pur-
pose, we used part-of-speech (POS) tags obtained
from the TreeTagger (Schmid, 1994) along with
a set of POS-based heuristic disambiguation rules
89
provided to us by the Institute of Computational
Linguistics of the University of Zurich.
As a side effect, Gertwol outputs the base forms
of all words that it processes: Nominative singu-
lar of nouns, infinitive of verbs etc. We decided to
combine the tokens analysed by Gertwol, whether
or not they had been decompounded and lower-
cased, in a further attempt to reduce data sparse-
ness, with their original form in a word lattice
(see fig. 1) and to let the decoder make the choice
between the two according to the translations the
phrase table can provide for each.
Our word lattices are similar to those used by
Dyer et al (2008) for handling word segmentation
in Chinese and Arabic. For each word that was
segmented by Gertwol, we provide exactly one al-
ternative edge labelled with the component words
and base forms as identified by Gertwol, after re-
moving linking morphemes. The edge transition
probabilities are used to identify the source of an
edge: their values are e?1 = 0.36788 for edges de-
riving from Gertwol analysis and e0 = 1 for edges
carrying unprocessed words. Tokens whose de-
compounded base form according to Gertwol is
identical to the surface form in the input are rep-
resented by a single edge with transition proba-
bility e?0.5 = 0.606531. These transition proba-
bilities translate into a binary feature with values
?1, ?0.5 and 0 after taking logarithms in the de-
coder. The feature weight is determined by Min-
imum Error-Rate Training (Och, 2003), together
with the weights of the other feature functions
used in the decoder. During system training, the
processed version of the training corpus was con-
catenated with the unprocessed text.
Experiments show that decompounding and
morphological analysis have a significant impact
on the performance of the MT system. After
these steps, the OOV rate of the newstest2009
test set decreases from 5.88% to 3.21%. Us-
ing only the News language model, the BLEU
score of our development system (measured on
the newstest2009 corpus) increases from 18.77
to 19.31. There is an interesting interaction with
the language models. While using a linear mixture
of 15 language models instead of just the News
LM does not improve the performance of the base-
line system (BLEU score 18.78 instead of 18.77),
the BLEU score of the 15-LM system increases to
20.08 when adding morphological reduction. In
the baseline system, the additional language mod-
els did not have a noticeable effect on translation
quality; however, their impact was realised in the
decompounding system.
4 Word reordering
Current SMT systems are based on the assump-
tion that the word order of the source and the tar-
get languages are fundamentally similar. While
the models permit some local reordering, system-
atic differences in word order involving move-
ments of more than a few words pose major prob-
lems. In particular, Statistical Machine Transla-
tion between German and English is notoriously
impacted by the different fundamental word order
in subordinate clauses, where German Subject?
Object?Verb (SOV) order contrasts with English
Subject?Verb?Object (SVO) order.
In our English-German system, we made the
observation that the verb in an SVO subordi-
nate clause following a punctuation mark fre-
quently gets moved before the preceding punctu-
ation. This movement is triggered by the Ger-
man language model, which prefers verbs pre-
ceding punctuation as consistent with SOV or-
der, and it is facilitated by the fact that the dis-
tance from the verb to the end of the preceding
clause is often smaller than the distance to the end
of the current phrase, so moving the verb back-
wards results in a better score from the distance-
based reordering model. This tendency can be
counteracted effectively by enabling the Moses
decoder?s monotone-at-punctuation feature,
which makes sure that words are not reordered
across punctuation marks. The result is a mod-
est gain from 14.28 to 14.38 BLEU points
(newstest2009).
In the German-English system, we applied a
chunk-based technique to produce lattices repre-
senting multiple permutations of the test sentences
in order to enable long-range reorderings of verb
phrases. This approach is similar to the reorder-
ing technique based on part-of-speech tags pre-
sented by Niehues and Kolss (2009), which re-
sults in the addition of a large number of reorder-
ing paths to the lattices. By contrast, we assume
that verb reorderings only occur between shallow
syntax chunks, and not within them. This makes it
possible to limit the number of long-range reorder-
ing options in an effective way.
We used the TreeTagger to perform shallow
syntax chunking of the German text. By man-
90
Figure 1: Word lattice for morphological reduction
Sonst [drohe]VC , dass auch [weitere L?nder]NC [vom Einbruch]PC [betroffen sein w?rden]VC .
Figure 2: Chunk reordering lattice
BLEU
test-09 test-10
Baseline 18.77 20.1
+ chunk-based reordering 18.94 20.3
Morphological reduction 19.31 20.6
+ chunk-based reordering 19.79 21.1
note: only News LM, case-sensitive evaluation
Table 5: Results with morphological reduction and
chunk reordering on newstest 2009/2010
ual inspection of a data sample, we then identi-
fied a few recurrent patterns of long reorderings
involving the verbs. In particular, we focused on
clause-final verbs in German SOV clauses, which
we move to the left in order to approximate the En-
glish SVO word order. For each sentence a chunk-
based lattice is created, which is then expanded
into a word lattice like the one shown in fig. 2. The
lattice representation provides the decoder with up
to three possible reorderings for a particular verb
chunk. It always retains the original word order as
an alternative input.
For technical reasons, we were unable to pre-
pare a system with reordering, morphological re-
duction and all language models in time for the
shared task. Our secondary submission with re-
ordering is therefore not comparable with our best
system, which includes more language models
and morphological reduction. In subsequent ex-
periments, we combined morphological reduction
with chunk-based reordering (table 5). When mor-
phological reduction is used, the reordering ap-
proach yields an improvement of about 0.5 BLEU
percentage points.
5 Conclusions
There are three important features specific to the
FBK systems at WMT 2010: mixtures of large
language models, German morphological reduc-
tion and decompounding and word reordering.
Our approach to using large language models
proved successful at the 2009 NIST MT evalua-
tion. In the present evaluation, its effectiveness
was reduced by a number of technical problems,
which were mostly due to the limitations of disk
access throughput in our parallel computing en-
vironment. We are working on methods to re-
duce and distribute disk accesses to large lan-
guage models, which will be implemented in the
IRSTLM language modelling toolkit (Federico et
al., 2008). By doing so, we hope to overcome the
current limitations and exploit the power of lan-
guage model mixtures more fully.
The Gertwol-based morphological reduction
and decompounding component we used is a
working solution that results in a significant im-
provement in translation quality. It is an alterna-
tive to the popular statistical compound splitting
methods, such as the one by Koehn and Knight
(2003), incorporating a greater amount of linguis-
tic knowledge and offering morphological reduc-
tion even of simplex words to their base form in
addition. It would be interesting to compare the
relative performance of the two approaches sys-
tematically.
Word reordering between German and English
is a complex problem. Encouraged by the success
of chunk-based verb reordering lattices on Arabic-
English (Bisazza and Federico, 2010), we tried to
adapt the same approach to the German-English
language pair. It turned out that there is a larger
variety of long reordering patterns in this case.
Nevertheless, some experiments performed after
91
the official evaluation showed promising results.
We plan to pursue this work in several directions:
Defining a lattice weighting scheme that distin-
guishes between original word order and reorder-
ing paths could help the decoder select the more
promising path through the lattice. Applying sim-
ilar reordering rules to the training corpus would
reduce the mismatch between the training data and
the reordered input sentences. Finally, it would be
useful to explore the impact of different distortion
limits on the decoding of reordering lattices in or-
der to find an optimal trade-off between decoder-
driven short-range and lattice-driven long-range
reordering.
Acknowledgements
This work was supported by the EuroMatrixPlus
project (IST-231720), which is funded by the Eu-
ropean Commission under the Seventh Framework
Programme for Research and Technological De-
velopment.
References
Arianna Bisazza andMarcello Federico. 2010. Chunk-
based verb reordering in VSO sentences for Arabic-
English statistical machine translation. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and Metrics MATR, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Inter-
speech 2008, pages 1618?1621. ISCA.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967?975, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
EACL, pages 187?193.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for statistical ma-
chine translation. In Annual meeting of the Associa-
tion for Computational Linguistics: Demonstration
session, pages 177?180, Prague.
Kimmo Koskenniemi and Mariikka Haapalainen.
1996. GERTWOL ? Lingsoft Oy. In Roland
Hausser, editor, Linguistische Verifikation. Doku-
mentation zur Ersten Morpholympics 1994, chap-
ter 11, pages 121?140. Niemeyer, T?bingen.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 169?176, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece,
March. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st annual meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo (Japan).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia.
ACL.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
92
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 235?243,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Chunk-based Verb Reordering in VSO Sentences for
Arabic-English Statistical Machine Translation
Arianna Bisazza and Marcello Federico
Fondazione Bruno Kessler
Human Language Technologies
Trento, Italy
{bisazza,federico}@fbk.eu
Abstract
In Arabic-to-English phrase-based statis-
tical machine translation, a large number
of syntactic disfluencies are due to wrong
long-range reordering of the verb in VSO
sentences, where the verb is anticipated
with respect to the English word order.
In this paper, we propose a chunk-based
reordering technique to automatically de-
tect and displace clause-initial verbs in the
Arabic side of a word-aligned parallel cor-
pus. This method is applied to preprocess
the training data, and to collect statistics
about verb movements. From this anal-
ysis, specific verb reordering lattices are
then built on the test sentences before de-
coding them. The application of our re-
ordering methods on the training and test
sets results in consistent BLEU score im-
provements on the NIST-MT 2009 Arabic-
English benchmark.
1 Introduction
Shortcomings of phrase-based statistical machine
translation (PSMT) with respect to word reorder-
ing have been recently shown on the Arabic-
English pair by Birch et al (2009). An empiri-
cal investigation of the output of a strong baseline
we developed with the Moses toolkit (Koehn et
al., 2007) for the NIST 2009 evaluation, revealed
that an evident cause of syntactic disfluency is the
anticipation of the verb in Arabic Verb-Subject-
Object (VSO) sentences ? a class that is highly
represented in the news genre1.
Fig. 1 shows two examples where the Arabic
main verb phrase comes before the subject. In
such sentences, the subject can be followed by
adjectives, adverbs, coordinations, or appositions
that further increase the distance between the verb
1In fact, Arabic syntax admits both SVO and VSO orders.
and its object. When translating into English ? a
primarily SVO language ? the resulting long verb
reorderings are often missed by the PSMT decoder
either because of pure modeling errors or because
of search errors (Germann et al, 2001): i.e. their
span is longer than the maximum allowed distor-
tion distance, or the correct reordering hypothesis
does not emerge from the explored search space
because of a low score. In the two examples, the
missed verb reorderings result in different transla-
tion errors by the decoder, respectively, the intro-
duction of a subject pronoun before the verb and,
even worse, a verbless sentence.
In Arabic-English machine translation, other
kinds of reordering are of course very frequent: for
instance, adjectival modifiers following their noun
and head-initial genitive constructions (Idafa).
These, however, appear to be mostly local, there-
fore more likely to be modeled through phrase in-
ternal alignments, or to be captured by the reorder-
ing capabilities of the decoder. In general there is a
quite uneven distribution of word-reordering phe-
nomena in Arabic-English, and long-range move-
ments concentrate on few patterns.
Reordering in PSMT is typically performed
by (i) constraining the maximum allowed word
movement and exponentially penalizing long re-
orderings (distortion limit and penalty), and (ii)
through so-called lexicalized orientation models
(Och et al, 2004; Koehn et al, 2007; Galley
and Manning, 2008). While the former is mainly
aimed at reducing the computational complexity
of the decoding algorithm, the latter assigns at
each decoding step a score to the next source
phrase to cover, according to its orientation with
respect to the last translated phrase. In fact, neither
method discriminates among different reordering
distances for a specific word or syntactic class. To
our view, this could be a reason for their inade-
quacy to properly deal with the reordering pecu-
liarities of the Arabic-English language pair. In
235
src: w AstdEt kl mn AlsEwdyp w lybyA w swryASubj sfrA? hAObj fy AldnmArk .
ref: Each of Saudi Arabia , Libya and SyriaSubj recalled their ambassadorsObj from Denmark .
MT: He recalled all from Saudi Arabia , Libya and Syria ambassadors in Denmark .
src: jdd AlEAhl Almgrby Almlk mHmd AlsAdsSubj dEm hObj l m$rwE Alr}ys Alfrnsy
ref: The Moroccan monarch King Mohamed VISubj renewed his supportObj to the project of French President
MT: The Moroccan monarch King Mohamed VI his support to the French President
Figure 1: Examples of problematic SMT outputs due to verb anticipation in the Arabic source.
this work, we introduce a reordering technique
that addresses this limitation.
The remainder of the paper is organized as fol-
lows. In Sect. 2 we describe our verb reordering
technique and in Sect. 3 we present statistics about
verb movement collected through this technique.
We then discuss the results of preliminary MT ex-
periments involving verb reordering of the training
based on these findings (Sect. 4). Afterwards, we
explain our lattice approach to verb reordering in
the test and provide evaluation on a well-known
MT benchmark (Sect. 5). In the last two sections
we review some related work and draw the final
conclusions.
2 Chunk-based Verb Reordering
The goal of our work is to displace Arabic verbs
from their clause-initial position to a position that
minimizes the amount of word reordering needed
to produce a correct translation. In order to re-
strict the set of possible movements of a verb and
to abstract from the usual token-based movement
length measure, we decided to use shallow syn-
tax chunking of the source language. Full syntac-
tic parsing is another option which we have not
tried so far mainly because popular parsers that are
available for Arabic do not mark grammatical re-
lations such as the ones we are interested in.
We assume that Arabic verb reordering only
occurs between shallow syntax chunks, and not
within them. For this purpose we annotated our
Arabic data with the AMIRA chunker by Diab et
al. (2004)2. The resulting chunks are generally
short (1.6 words on average). We then consider
a specific type of reordering by defining a produc-
tion rule of the kind: ?move a chunk of type T
along with its L left neighbours and R right neigh-
bours by a shift of S chunks?. A basic set of rules
2This tool implies morphological segmentation of the
Arabic text. All word statistics in this paper refer to AMIRA-
segmented text.
that displaces the verbal chunk to the right by at
most 10 positions corresponds to the setting:
T=?VP?, L=0, R=0, S=1..10
In order to address cases where the verb is moved
along with its adverbial, we also add a set of rules
that include a one-chunk right context in the move-
ment:
T=?VP?, L=0, R=1, S=1..10
To prevent verb reordering from overlapping
with the scope of the following clause, we always
limit the maximum movement to the position of
the next verb. Thus, for each verb occurrence, the
number of allowed movements for our setting is at
most 2? 10 = 20.
Assuming that a word-aligned translation of the
sentence is available, the best movement, if any,
will be the one that reduces the amount of distor-
tion in the alignment, that is: (i) it reduces the
number of swaps by 1 or more, and (ii) it mini-
mizes the sum of distances between source posi-
tions aligned to consecutive target positions, i.e.
?
i |ai ? (ai?1 + 1)| where ai is the index of the
foreign word aligned to the ith English word. In
case several movements are optimal according to
these two criteria, e.g. because of missing word-
alignment links, only the shortest good movement
is retained.
The proposed reordering method has been ap-
plied to various parallel data sets in order to per-
form a quantitative analysis of verb anticipation,
and to train a PSMT system on more monotonic
alignments.
3 Analysis of Verb Reordering
We applied the above technique to two parallel
corpora3 provided by the organizers of the NIST-
MT09 Evaluation. The first corpus (Gale-NW)
contains human-made alignments. As these re-
fer to non-segmented text, they were adjusted to
3Newswire sections of LDC2006E93 and LDC2009E08,
respectively 4337 and 777 sentence pairs.
236
Figure 2: Percentage of verb reorderings by maxi-
mum shift (0 stands for no movement).
agree with AMIRA-style segmentation. For the
second corpus (Eval08-NW), we filtered out sen-
tences longer than 80 tokens in order to make
word alignment feasible with GIZA++ (Och and
Ney, 2003). We then used the Intersection of
the direct and inverse alignments, as computed by
Moses. The choice of such a high-precision, low-
recall alignment set is supported by the findings of
Habash (2007) on syntactic rule extraction from
parallel corpora.
3.1 The Verb?s Dance
There are 1,955 verb phrases in Gale-NW and
11,833 in Eval08-NW. Respectively 86% and 84%
of these do not need to be moved according to the
alignments. The remaining 14% and 16% are dis-
tributed by movement length as shown in Fig. 2:
most verb reorderings consist in a 1-chunk long
jump to the right (8.3% in Gale-NW and 11.6% in
Eval08-NW). The rest of the distribution is simi-
lar in the two corpora, which indicates a good cor-
respondence between verb reordering observed in
automatic and manual alignments. By increasing
the maximum movement length from 1 to 2, we
can cover an additional 3% of verb reorderings,
and around 1% when passing from 2 to 3. We
recall that the length measured in chunks doesn?t
necessarily correspond to the number of jumped
tokens. These figures are useful to determine an
optimal set of reordering rules. From now on we
will focus on verb movements of at most 6 chunks,
as these account for about 99.5% of the verb oc-
currences.
Figure 3: Distortion reduction in the GALE-NW
corpus: jump occurrences grouped by length range
(in nb. of words).
3.2 Impact on Corpus Global Distortion
We tried to measure the impact of chunk-based
verb reordering on the total word distortion found
in parallel data. For the sake of reliability, this
investigation was carried out on the manually
aligned corpus (Gale-NW) only. Fig. 3 shows the
positive effect of verb reordering on the total dis-
tortion, which is measured as the number of words
that have to be jumped on the source side in or-
der to cover the sentence in the target order (that
is |ai ? (ai?1 + 1)|). Jumps have been grouped
by length and the relative decrease of jumps per
length is shown on top of each double column.
These figures do not prove as we hoped that
verb reordering resolves most of the long range re-
orderings. Thus we manually inspected a sample
of verb-reordered sentences that still contain long
jumps, and found out that many of these were due
to what we could call ?unnecessary? reordering. In
fact, human translations that are free to some ex-
tent, often display a global sentence restructuring
that makes distortion dramatically increase. We
believe this phenomenon introduces noise in our
analysis since these are not reorderings that an MT
system needs to capture to produce an accurate
and fluent translation.
Nevertheless, we can see from the relative de-
crease percentages shown in the plot, that although
short jumps are by far the most frequent, verb
reordering affects especially medium and long
range distortion. More precisely, our selective
reordering technique solves 21.8% of the 5-to-6-
words jumps, 25.9% of the 7-to-9-words jumps
and 24.2% of the 10-to-14-words jumps, against
237
only 9.5% of the 2-words jumps, for example.
Since our primary goal is to improve the handling
of long reorderings, this makes us think that we
are advancing in a promising direction.
4 Preliminary Experiments
In this section we investigate how verb reordering
on the source language can affect translation qual-
ity. We apply verb reordering both on the training
and the test data. However, while the parallel cor-
pus used for training can be reordered by exploit-
ing word alignments, for the test corpus we need
a verb reordering ?prediction model?. For these
preliminary experiments, we assumed that optimal
verb-reordering of the test data is provided by an
oracle that has access to the word alignments with
the reference translations.
4.1 Setup
We trained a Moses-based system on a subset of
the NIST-MT09 Evaluation data4 for a total of
981K sentences, 30M words. We first aligned the
data with GIZA++ and use the resulting Intersec-
tion set to apply the technique explained in Sect. 2.
We then retrained the whole system ? from word
alignment to phrase scoring ? on the reordered
data and evaluated it on two different versions of
Eval08-NW: plain and oracle verb-reordered, ob-
tained by exploiting word alignments with the first
of the four available English references. The first
experiment is meant to measure the impact of the
verb reordering procedure on training only. The
latter will provide an estimate of the maximum im-
provement we can expect from the application to
the test of an optimal verb reordering prediction
technique. Given our experimental setting, one
could argue that our BLEU score is biased because
one of the references was also used to generate the
verb reordering. However, in a series of exper-
iments not reported here, we evaluated the same
systems using only the remaining three references
and observed similar trends as when all four refer-
ences are used.
Feature weights were optimized through MERT
(Och, 2003) on the newswire section of the NIST-
MT06 evaluation set (Dev06-NW), in the origi-
nal version for the baseline system, in the verb-
reordered version for the reordered system.
4LDC2007T08, 2003T07, 2004E72, 2004T17, 2004T18,
2005E46, 2006E25, 2006E44 and LDC2006E39 ? the two
last with first reference only.
Figure 4: BLEU scores of baseline and reordered
system on plain and oracle reordered Eval08-NW.
Fig. 4 shows the results in terms of BLEU score
for (i) the baseline system, (ii) the reordered sys-
tem on a plain version of Eval08-NW and (iii) the
reordered system on the reordered test. The scores
are plotted against the distortion limit (DL) used
in decoding. Because high DL values (8-10) im-
ply a larger search space and because we want to
give Moses the best possible conditions to prop-
erly handle long reordering, we relaxed for these
conditions the default pruning parameter to the
point that led the highest BLEU score5.
4.2 Discussion
The first observation is that the reordered system
always performs better (0.5?0.6 points) than the
baseline on the plain test, despite the mismatch
between training and test ordering. This may be
due to the fact that automatic word alignments
are more accurate when less reordering is present
in the data, although previous work (Lopez and
Resnik, 2006) showed that even large gains in
alignment accuracy seldom lead to better trans-
lation performances. Moreover phrase extraction
may benefit from a distortion reduction, since its
heuristics rely on word order in order to expand
the context of alignment links.
The results on the oracle reordered test are also
interesting: a gain of at least 1.2 point absolute
over the baseline is reported in all tested DL condi-
tions. These improvements are remarkable, keep-
ing in mind that only 31% of the train and 33% of
the test sentences get modified by verb reordering.
5That is, the histogram pruning maximum stack size was
set to 1000 instead of the default 200.
238
Figure 5: Reordering lattices for Arabic VSO sentences: word-based and chunk-based.
Concerning distortion, although long verb
movements are often observed in parallel corpora,
relaxing the DL to high values does not bene-
fit the translation, even with our ?generous? set-
ting (wider beam search). This is probably due to
the fact that, with weakly constrained distortion,
the risk of search errors increases as the reorder-
ing model fails to properly rank an exponentially
growing set of permutations. Therefore many cor-
rect reordering hypotheses receive low scores and
get lost in pruning or recombination.
5 Verb Reordering Lattices
Having assessed the negative impact of VSO sen-
tences on Arabic-English translation performance,
we now propose a method to improve the handling
of this phenomenon at decoding time. As in real
working conditions word alignments of the input
text are not available, we explore a reordering lat-
tice approach.
5.1 Lattice Construction
Firstly conceived to optimally encode multiple
transcription hypothesis produced by a speech rec-
ognizer, word lattices have later been used to rep-
resent various forms of input ambiguity, mainly at
the level of token boundaries (e.g. word segmenta-
tion, morphological decomposition, word decom-
pounding (Dyer et al, 2008)).
A main problem when dealing with permuta-
tions is that the lattice size can grow very quickly
when medium to long reorderings are represented.
We are particularly concerned with this issue be-
cause our decoding will perform additional re-
ordering on the lattice input. Thanks to the re-
strictions we set on our verb movement reordering
rules described in Sect. 2 ? i.e. only reordering be-
tween chunks and no overlap between consecutive
verb chunks movement ? we are able to produce
quite compact word lattices.
Fig. 5 illustrates how a chunk-based reordering
lattice is generated. Suppose we want to translate
the Arabic sentence ?w >kdt mSAdr rsmyp wjwd
rAbT byn AlAEtdA?At?, whose English meaning is
?Official sources confirmed that there was a link
between the attacks?. The Arabic main verb >kdt
(confirmed) is in pre-subject position. If we ap-
plied word-based rather than chunk-based rules to
move the verb to the right, we would produce the
first lattice of the figure, containing 7 paths (the
original plus 6 verb movements). With the chunk-
based rules, we treat instead chunks as units and
get the second lattice. Then, by expanding each
chunk, we obtain the final, less dense lattice, that
compared to the first does not contain 3 (unlikely)
reordering edges.
To be consistent with the reordering applied to
the training data, we use a set of rules that move
each verb phrase alone or with its following chunk
by 1 to 6 chunks to the right. With this settings,
239
Figure 6: Structure of a chunk-based reordering lattice for verb reordering, before word expansion. Edges
in boldface represent the verbal chunk.
our lattice generation algorithm computes a com-
pact lattice (Fig. 6) that introduces at most 5??S
chunk edges for each verb chunk, where ?S is the
permitted movement range (6 in this case).
Before translation, each edge has to be associ-
ated with a weight that the decoder will use as ad-
ditional feature. To differentiate between the orig-
inal word order and verb reordering we assign a
fixed weight of 1 to the edges of the plain path, and
0.25 to the other edges. As future work we will de-
vise more discriminative weighting schemes.
5.2 Evaluation
For the experiments, we relied on the existing
Moses-implementation of non-monotonic decod-
ing for word lattices (Dyer et al, 2008) with
some fixes concerning the computation of reorder-
ing distance. The translation system is the same
as the one presented in Sect. 4, to which we
added an additional feature function evaluating
the lattice weights (weight-i). Instead of rerun-
ning MERT, we directly estimated the additional
feature-function weight over a suitable interval
(0.002 to 0.5), by running the decoder several
times on the development set. The resulting op-
timal weight was 0.05.
Table 1 presents results on three test sets:
Eval08-NW which was used to calibrate the re-
ordering rules, Reo08-NW a specific test set con-
sisting of the 33% of Eval08-NW sentences that
actually require verb reordering, and Eval09-NW
a yet unseen dataset (newswire section of the
NIST-MT09 evaluation set, 586 sentences). Best
results with lattice decoding were obtained with a
distortion limit (DL) of 4, while best performance
of text decoding was obtained with a DL of 6.
As we hoped, translating a verb reordering lat-
tice yields an additional improvement to the re-
ordering of the training corpus: from 43.67%
to 44.04% on Eval08-NW and from 48.53% to
48.96% on Eval09-NW. The gap between the
baseline and the score obtainable by oracle verb
reordering, as estimated in the preliminary exper-
iments on Eval08-NW (44.36%), has been largely
filled.
On the specific test set ? Reo08-NW ? we ob-
serve a performance drop when reordered models
are applied to non-reordered (plain) input: from
46.90% to 46.64%. Hence it seems that the mis-
match between training and test data is signifi-
cantly impacting on the reordering capabilities of
the system with respect to verbs. We speculate
that such negative effect is diluted in the full test
set (Eval08-NW) and compensated by the positive
influence of verb reordering on phrase extraction.
Indeed, when the lattice technique is applied we
get an improvement of about 0.6 point over the
baseline, which is still a fair result, but not as good
as the one obtained on the general test sets.
Finally, our approach led to an overall gain of
0.8 point BLEU over the baseline, on Eval09-NW.
We believe this is a satisfactory result, given the
fairly good starting performance, and given that
the BLEU metric is known not to be very sensi-
tive to word order variations (Callison-Burch et
al., 2006). For the future, we plan to also use spe-
cific evaluation metrics that will allow us to isolate
the impact of our approach on reordering, like the
ones by Birch et al (2010).
System DL eval08nw reo08nw eval09nw
baseline 6 43.10 46.90 48.13
reord. training +
plain input 6 43.67 46.64 48.53
lattice 4 44.04 47.51 48.96
oracle reord. 4 44.36 48.25 na
Table 1: BLEU scores of baseline and reordered
system on plain test and on reordering lattices.
240
6 Related Work
Linguistically motivated word reordering for
Arabic-English has been proposed in several re-
cent works. Habash (2007) extracts syntactic re-
ordering rules from a word-aligned parallel cor-
pus whose Arabic side has been fully parsed. The
rules involve reordering of syntactic constituents
and are applied in a deterministic way (always
the most probable) as preprocessing of training
and test data. The technique achieves consistent
improvements only in very restrictive conditions:
maximum phrase size of 1 and monotonic decod-
ing, thus failing to enhance the existing reorder-
ing capabilities of PSMT. In (Crego and Habash,
2008; Elming and Habash, 2009) possible in-
put permutations are represented through a word
graph, which is then processed by a monotonic
phrase- or n-gram-based decoder. Thus, these ap-
proaches are conceived as alternatives, rather than
integrations, to PSMT reordering. On the contrary,
we focused on a single type of significant long re-
orderings, in order to integrate class-specific re-
ordering methods into a standard PSMT system.
To our knowledge, the work by Niehues and
Kolss (2009) on German-English is the only ex-
ample of a lattice-based reordering approach be-
ing coupled with reordering at decoding time. In
their paper, discontinuous non-deterministic POS-
based rules learned from a word-aligned corpus
are applied to German sentences in the form of
weighted edges in a word lattice. Their phrase-
based decoder admits local reordering within a
fixed window of 2 words, while, in our work, we
performed experiments up to a distortion limit of
10. Another major difference is that we used shal-
low syntax annotation to effectively reduce the
number of possible permutations. A first attempt
to adapt our technique to the German language is
described in Hardmeier et al (2010).
Our work is also tightly related to the prob-
lem of noun-phrase subject detection, recently ad-
dressed by Green et al (2009). In fact, detect-
ing the extension of the subject can be a suffi-
cient condition to guess the optimal reordering of
the verb. In their paper, a discriminative classi-
fier was trained on a rich variety of linguistic fea-
tures to detect the full scope of Arabic NP subjects
in verb-initial clauses. The authors reported an F-
score of 61.3%, showing that the problem is hard
to solve even when more linguistic information is
available. In order to integrate the output of the
classifier into a PSMT decoder, a specific trans-
lation feature was designed to reward hypotheses
in which the subject is translated as a contiguous
block. Unfortunately, no improvement in transla-
tion quality was obtained.
7 Conclusions
Word reordering remains one of the hardest prob-
lems in statistical machine translation. Based on
the intuition that few reordering patterns would
suffice to handle the most significant cases of long
reorderings in Arabic-English, we decided to fo-
cus on the problem of VSO sentences.
Thanks to simple linguistic assumptions on verb
movement, we developed an efficient, low-cost
technique to reorder the training data, on the one
hand, and to better handle verb reordering at de-
coding time, on the other. In particular, translation
is performed on a compact word lattice that repre-
sents likely verb movements. The resulting system
outperforms a strong baseline in terms of BLEU,
and produces globally more readable translations.
However, the problem is not totally solved because
many verb reorderings are still missed, despite
the suggestions provided by the lattice. Different
factors can explain these errors: poor interaction
between lattice and distortion/orientation models
used by the decoder; poor discriminative power of
the target language model with respect to different
reorderings of the source.
As a first step to improvement, we will devise
a discriminative weighting scheme based on the
length of the reorderings represented in the lat-
tice. For the longer term we are working towards
bringing linguistically informed reordering con-
straints inside decoding, as an alternative to the
lattice solution. In addition, we plan to couple
our reordering technique with more informative
language models, including for instance syntac-
tic analysis of the hypothesis under construction.
Finally we would like to compare the proposed
chunk-based technique with one that exploits full
syntactic parsing of the Arabic sentence to further
decrease the reordering possibilities of the verb.
Acknowledgments
This work was supported by the EuroMatrixPlus
project (IST-231720) which is funded by the Eu-
ropean Commission under the Seventh Framework
Programme for Research and Technological De-
velopment.
241
src: w A$Ar AlsnAtwr AlY dEm h m$rwEA ErD ElY mjls Al$ywx
ref: The Senator referred to his support for a project proposed to the Senate
base MT: The Senator to support projects presented to the Senate
new MT: Senator noted his support projects presented to the Senate
src: mn jAnb h hdd >bw mSEb EbdAlwdwd Amyr AlqAEdp b blAd Almgrb AlAslAmy fy nfs Al$ryT b AlqyAm
b slslp AEtdA?At w >EmAl <rhAbyp Dd AlmSAlH w Alm&ssAt AljzA}ryp fy AlEdyd mn AlmnATq
AljzA}ryp
ref: For his part , Abu Musab Abd al-Wadud , the commander of al-Qaeda in the Islamic Maghreb Countries ,
threatened in the same tape to carry out a series of attacks and terrorist actions against Algerian interests and
organizations in many parts of Algeria
base MT: For his part threatened Abu Musab EbdAlwdwd Amir al-Qaeda Islamic Morocco country in the same tape to
carry out a series of attacks and terrorist acts against the interests and the Algerian institutions in many areas of
Algiers
new MT: For his part , Abu Musab EbdAlwdwd Amir al Qaida threatened to Morocco Islamic country in the same tape
to carry out a series of attacks and terrorist acts against the interests of the Algerian and institutions in many
areas of Algiers
src: w ymtd Alm$rwE 500 km mtr w yrbT Almdyntyn Almqdstyn b mdynp jdp ElY sAHl AlbHr Al>Hmr .
ref: The project is 500 kilometers long and connects the two holy cities with the city of Jeddah on the Red Sea coast.
base MT: It extends the project 500 km and linking the two holy cities in the city of Jeddah on the Red Sea coast .
new MT: The project extends 500 km , linking the two holy cities in the city of Jeddah on the Red Sea coast .
Figure 7: Examples showing MT improvements coming from chunk-based verb-reordering.
References
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2009. A quantitative analysis of reordering phe-
nomena. In StatMT ?09: Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
197?205, Morristown, NJ, USA. Association for
Computational Linguistics.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating re-
ordering. Machine Translation, Published online.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluation the role of BLEU
in machine translation research. In Proceedings of
the 11th Conference of the European Chapter of the
Association for Computational Linguistics, Trento,
Italy, April.
Josep M. Crego and Nizar Habash. 2008. Using shal-
low syntax information to improve word alignment
and reordering for smt. In StatMT ?08: Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 53?61, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004. Automatic Tagging of Arabic Text: From
Raw Text to Base Phrase Chunks. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-
NAACL 2004: Short Papers, pages 149?152,
Boston, Massachusetts, USA, May 2 - May 7. As-
sociation for Computational Linguistics.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jakob Elming and Nizar Habash. 2009. Syntactic re-
ordering for English-Arabic phrase-based machine
translation. In Proceedings of the EACL 2009 Work-
shop on Computational Approaches to Semitic Lan-
guages, pages 69?77, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP ?08: Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 848?856, Morristown, NJ, USA.
Association for Computational Linguistics.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding
and optimal decoding for machine translation. In
Proceedings of the 39th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
228?335, Toulouse, France.
Spence Green, Conal Sathi, and Christopher D. Man-
ning. 2009. NP subject detection in verb-initial Ara-
bic clauses. In Proceedings of the Third Workshop
on Computational Approaches to Arabic Script-
based Languages (CAASL3), Ottawa, Canada.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Bente Maegaard, ed-
itor, Proceedings of the Machine Translation Summit
XI, pages 215?222, Copenhagen, Denmark.
Christian Hardmeier, Arianna Bisazza, and Marcello
Federico. 2010. FBK at WMT 2010: Word lat-
tices for morphological reduction and chunk-based
242
reordering. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
rics MATR, Uppsala, Sweden, July. Association for
Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic.
Adam Lopez and Philip Resnik. 2006. Word-based
alignment, phrase-based translation: What?s the
link? In 5th Conference of the Association for Ma-
chine Translation in the Americas (AMTA), Boston,
Massachusetts, August.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece,
March. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. In Proceedings of the Joint Conference on Hu-
man Language Technologies and the Annual Meet-
ing of the North American Chapter of the Associ-
ation of Computational Linguistics (HLT-NAACL),
Boston, MA.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Erhard Hinrichs
and Dan Roth, editors, Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 160?167.
243
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 294?302,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Topic Adaptation for Lecture Translation through
Bilingual Latent Semantic Models
Nick Ruiz?
Free University of Bozen-Bolzano
Bolzano, Italy
nicruiz@fbk.eu
Marcello Federico
FBK-irst
Fondazione Bruno Kessler
Trento, Italy
federico@fbk.eu
Abstract
This work presents a simplified approach to
bilingual topic modeling for language model
adaptation by combining text in the source
and target language into very short documents
and performing Probabilistic Latent Semantic
Analysis (PLSA) during model training. Dur-
ing inference, documents containing only the
source language can be used to infer a full
topic-word distribution on all words in the tar-
get language?s vocabulary, from which we per-
form Minimum Discrimination Information
(MDI) adaptation on a background language
model (LM). We apply our approach on the
English-French IWSLT 2010 TED Talk exer-
cise, and report a 15% reduction in perplexity
and relative BLEU and NIST improvements
of 3% and 2.4%, respectively over a baseline
only using a 5-gram background LM over the
entire translation task. Our topic modeling ap-
proach is simpler to construct than its counter-
parts.
1 Introduction
Adaptation is usually applied to reduce the per-
formance drop of Statistical Machine Translation
(SMT) systems when translating documents that de-
viate from training and tuning conditions. In this
paper, we focus primarily on language model (LM)
adaptation. In SMT, LMs are used to promote fluent
translations. As probabilistic models of sequences
of words, language models guide the selection and
ordering of phrases in translation. With respect to
?This work was carried out during an internship period at
Fondazione Bruno Kessler.
LM training, LM adaptation for SMT tries to im-
prove an existing LM by using smaller amounts of
texts. When adaptation data represents the trans-
lation task domain one generally refers to domain
adaptation, while when they just represent the con-
tent of the single document to be translated one typ-
ically refers to topic adaptation.
We propose a cross-language topic adaptation
method, enabling the adaptation of a LM based on
the topic distribution of the source document dur-
ing translation. We train a latent semantic topic
model on a collection of bilingual documents, in
which each document contains both the source and
target language. During inference, a latent topic dis-
tribution of words across both the source and tar-
get languages is inferred from a source document
to be translated. After inference, we remove all
source language words from the topic-word distribu-
tions and construct a unigram language model which
is used to adapt our background LM via Minimum
Discrimination Information (MDI) estimation (Fed-
erico, 1999, 2002; Kneser et al, 1997).
We organize the paper as follows: In Section 2,
we discuss relevant previous work. In Section 3, we
review topic modeling. In Section 4, we review MDI
adaptation. In Section 5, we describe our new bilin-
gual topic modeling based adaptation technique. In
Section 6, we report adaptation experiments, fol-
lowed by conclusions and future work in Section 7.
2 Previous work
Zhao et al (2004) construct a baseline SMT system
using a large background language model and use it
to retrieve relevant documents from large monolin-
294
gual corpora and subsequently interpolate the result-
ing small domain-specific language model with the
background language model. In Sethy et al (2006),
domain-specific language models are obtained by
including only the sentences that are similar to the
ones in the target domain via a relative entropy based
criterion.
Researchers such as Foster and Kuhn (2007) and
Koehn and Schroeder (2007) have investigated mix-
ture model approaches to adaptation. Foster and
Kuhn (2007) use a mixture model approach that in-
volves splitting a training corpus into different com-
ponents, training separate models on each compo-
nent, and applying mixture weights as a function of
the distances of each component to the source text.
Koehn and Schroeder (2007) learn mixture weights
for language models trained with in-domain and out-
of-domain data respectively by minimizing the per-
plexity of a tuning (development) set and interpolat-
ing the models. Although the application of mixture
models yields significant results, the number of mix-
ture weights to learn grows linearly with the number
of independent language models applied.
Most works focus on monolingual language
model adaptation in the context of automatic speech
recognition. Federico (2002) combines Probabilis-
tic Latent Semantic Analysis (PLSA) (Hofmann,
1999) for topic modeling with the minimum dis-
crimination information (MDI) estimation criterion
for speech recognition and notes an improvement
in terms of perplexity and word error rate (WER).
Latent Dirichlet Allocation (LDA) techniques have
been proposed as an alternative to PLSA to construct
purely generative models. LDA techniques include
variational Bayes (Blei et al, 2003) and HMM-LDA
(Hsu and Glass, 2006).
Recently, bilingual approaches to topic model-
ing have also been proposed. A Hidden Markov
Bilingual Topic AdMixture (HM-BiTAM) model is
proposed by Zhao and Xing (2008), which con-
structs a generative model in which words from a
target language are sampled from a mixture of topics
drawn from a Dirichlet distribution. Foreign words
are sampled via alignment links from a first-order
Markov process and a topic specific translation lexi-
con. While HM-BiTAM has been used for bilingual
topic extraction and topic-specific lexicon mapping
in the context of SMT, Zhao and Xing (2008) note
that HM-BiTAM can generate unigram language
models for both the source and target language and
thus can be used for language model adaptation
through MDI in a similar manner as outlined in Fed-
erico (2002). Another bilingual LSA approach is
proposed by Tam et al (2007), which consists of
two hierarchical LDA models, constructed from par-
allel document corpora. A one-to-one correspon-
dence between LDA models is enforced by learn-
ing the hyperparameters of the variational Dirichlet
posteriors in one LDA model and bootstrapping the
second model by fixing the hyperparameters. The
technique is based on the assumption that the topic
distributions of the source and target documents are
identical. It is shown by Tam et al (2007) that the
bilingual LSA framework is also capable of adapt-
ing the translation model. Their work is extended
in Tam and Schultz (2009) by constructing paral-
lel document clusters formed by monolingual doc-
uments using M parallel seed documents.
Additionally, Gong et al (2010) propose transla-
tion model adaptation via a monolingual LDA train-
ing. A monolingual LDA model is trained from ei-
ther the source or target side of the training corpus
and each phrase pair is assigned a phrase-topic dis-
tribution based on:
M? ji =
wjk ?M
j
i
?m
k=1w
j
k
, (1)
where M j is the topic distribution of document j
and wk is the number of occurrences of phrase pair
Xk in document j.
Mimno et al (2009) extend the original con-
cept of LDA to support polylingual topic models
(PLTM), both on parallel (such as EuroParl) and
partly comparable documents (such as Wikipedia ar-
ticles). Documents are grouped into tuples w =
(w1, ...,wL) for each language l = 1, ..., L. Each
document wl in tuple w is assumed to have the
same topic distribution, drawn from an asymmetric
Dirichlet prior. Tuple-specific topic distributions are
learned using LDA with distinct topic-word concen-
tration parameters ?l. Mimno et al (2009) show that
PLTM sufficiently aligns topics in parallel corpora.
295
3 Topic Modeling
3.1 PLSA
The original idea of LSA is to map documents to
a latent semantic space, which reduces the dimen-
sionality by means of singular value decomposition
(Deerwester et al, 1990). A word-document matrix
A is decomposed by the formulaA = U?V t, where
U and V are orthogonal matrices with unit-length
columns and ? is a diagonal matrix containing the
singular values of A. LSA approximates ? by cast-
ing all but the largest k singular values in ? to zero.
PLSA is a statistical model based on the likeli-
hood principle that incorporates mixing proportions
of latent class variables (or topics) for each obser-
vation. In the context of topic modeling, the latent
class variables z ? Z = {z1, ..., zk} correspond to
topics, from which we can derive probabilistic distri-
butions of words w ?W = {w1, ..., wm} in a docu-
ment d ? D = {d1, ..., dn} with k << n. Thus, the
goal is to learn P (z | d) and P (w|z) by maximizing
the log-likelihood function:
L(W,D) =
?
d?D
?
w?W
n(w, d) logP (w | d), (2)
where n(w, d) is the term frequency of w in d.
Using Bayes? formula, the conditional probability
P (w | d) is defined as:
P (w | d) =
?
z?Z
P (w | z)P (z | d). (3)
Using the Expectation Maximization (EM) algo-
rithm (Dempster et al, 1977), we estimate the pa-
rameters P (z|d) and P (w|z) via an iterative pro-
cess that alternates two steps: (i) an expectation
step (E) in which posterior probabilities are com-
puted for each latent topic z; and (ii) a maximiza-
tion (M) step, in which the parameters are updated
for the posterior probabilities computed in the previ-
ous E-step. Details of how to efficiently implement
the re-estimation formulas can be found in Federico
(2002).
Iterating the E- and M-steps will lead to a con-
vergence that approximates the maximum likelihood
equation in (2).
A document-topic distribution ?? can be inferred
on a new document d? by maximizing the following
equation:
?? = arg max
?
?
w
n(w, d?) log
?
z
P (w | z)?z,d? ,
(4)
where ?z,d? = P (z | d?). (4) can be maximized by
performing Expectation Maximization on document
d? by keeping fixed the word-topic distributions al-
ready estimated on the training data. Consequently,
a word-document distribution can be inferred by ap-
plying the mixture model (3) (see Federico, 2002 for
details).
4 MDI Adaptation
An n-gram language model approximates the prob-
ability of a sequence of words in a text W T1 =
w1, ..., wT drawn from a vocabulary V by the fol-
lowing equation:
P (W T1 ) =
T?
i=1
P (wi|hi), (5)
where hi = wi?n+1, ..., wi?1 is the history of n ?
1 words preceding wi. Given a training corpus B,
we can compute the probability of a n-gram from a
smoothed model via interpolation as:
PB(w|h) = f
?
B(w|h) + ?B(h)PB(w|h
?), (6)
where f?B(w|h) is the discounted frequency of se-
quence hw, h? is the lower order history, where
|h|?1 = |h?|, and ?B(h) is the zero-frequency prob-
ability of h, defined as:
?B(h) = 1.0?
?
w?V
f?B(w|h).
Federico (1999) has shown that MDI Adaptation
is useful to adapt a background language model
with a small adaptation text sample A, by assum-
ing to have only sufficient statistics on unigrams.
Thus, we can reliably estimate P?A(w) constraints
on the marginal distribution of an adapted language
model PA(h,w) which minimizes the Kullback-
Leibler distance from B, i.e.:
PA(?) = arg min
Q(?)
?
hw?V n
Q(h,w) log
Q(h,w)
PB(h,w)
.
(7)
296
The joint distribution in (7) can be computed us-
ing Generalized Iterative Scaling (Darroch and Rat-
cliff, 1972). Under the unigram constraints, the GIS
algorithm reduces to the closed form:
PA(h,w) = PB(h,w)?(w), (8)
where
?(w) =
P?A(w)
PB(w)
. (9)
In order to estimate the conditional distribution of
the adapted LM, we rewrite (8) and simplify the
equation to:
PA(w|h) =
PB(w|h)?(w)
?
w??V PB(w?|h)?(w?)
. (10)
The adaptation model can be improved by
smoothing the scaling factor in (9) by an exponential
term ? (Kneser et al, 1997):
?(w) =
(
P?A(w)
PB(w)
)?
, (11)
where 0 < ? ? 1. Empirically, ? values less than
one decrease the effect of the adaptation ratio to re-
duce the bias.
As outlined in Federico (2002), the adapted lan-
guage model can also be written in an interpolation
form:
f?A(w|h) =
f?B(w|h)?(w)
z(h)
, (12)
?A(h) =
?B(h)z(h?)
z(h)
, (13)
z(h) = (
?
w:NB(h,w)>0
f?B(w|h)?(w)) + ?B(h)z(h
?),
(14)
which permits to efficiently compute the normaliza-
tion term for high order n-grams recursively and by
just summing over observed n-grams. The recursion
ends with the following initial values for the empty
history :
z() =
?
w
PB(w)?(w), (15)
PA(w|) = PB(w)?(w)z()
?1. (16)
MDI adaptation is one of the adaptation methods
provided by the IRSTLM toolkit and was applied as
explained in the following section.
5 Bilingual Latent Semantic Models
Similar to the treatment of documents in HM-
BiTAM (Zhao and Xing, 2008), we combine parallel
texts into a document-pair (E,F) containing n par-
allel sentence pairs (ei, fi), 1 < i ? n, correspond-
ing to the source and target languages, respectively.
Based on the assumption that the topics in a parallel
text share the same semantic meanings across lan-
guages, the topics are sampled from the same topic-
document distribution. We make the additional as-
sumption that stop-words and punctuation, although
having high word frequencies in documents, will
generally have a uniform topic distribution across
documents; therefore, it is not necessary to remove
them prior to model training, as they will not ad-
versely affect the overall topic distribution in each
document. In order to ensure the uniqueness be-
tween word tokens between languages, we annotate
E with special characters. We perform PLSA train-
ing, as described in Section 3.1 and receive word-
topic distributions P (w|z), w ? VE ? VF
Given an untranslated text E?, we split E? into
a sequence of documents D. For each document
di ? D, we infer a full word-document distribu-
tion by learning ?? via (4). Via (3), we can generate
the full word-document distribution P (w | d) for
w ? VF .
We then convert the word-document probabilities
into pseudo-counts via a scaling function:
n(w | d) =
P (w | d)
maxw? P (w? | d)
??, (17)
where ? is a scaling factor to raise the probabil-
ity ratios above 1. Since our goal is to generate a
unigram language model on the target language for
adaptation, we remove the source words generated
in (17) prior to building the language model.
From our newly generated unigram language
model, we perform MDI adaptation on the back-
ground LM to yield an adapted LM for translating
the source document used for the PLSA inference
step.
6 Experiments
Our experiments were done using the TED Talks
collection, used in the IWSLT 2010 evaluation task1.
1http://iwslt2010.fbk.eu/
297
In IWSLT 2010, the challenge was to translate talks
from the TED website2 from English to French. The
talks include a variety of topics, including photog-
raphy and pyschology and thus do not adhere to
a single genre. All talks were given in English
and were manually transcribed and translated into
French. The TED training data consists of 329 par-
allel talk transcripts with approximately 84k sen-
tences. The TED test data consists of transcriptions
created via 1-best ASR outputs from the KIT Quaero
Evaluation System. It consists of 758 sentences and
27,432 and 27,307 English and French words, re-
spectively. The TED talk data is segmented at the
clause level, rather than at the level of sentences.
Our SMT systems are built upon the Moses open-
source SMT toolkit (Koehn et al, 2007)3. The trans-
lation and lexicalized reordering models have been
trained on parallel data. One 5-gram background
LM was constructed from the French side of the
TED training data (740k words), smoothed with the
improved Kneser-Ney technique (Chen and Good-
man, 1999) and computed with the IRSTLM toolkit
(Federico et al, 2008). The weights of the log-linear
interpolation model were optimized via minimum
error rate training (MERT) (Och, 2003) on the TED
development set, using 200 best translations at each
tuning iteration.
This paper investigates the effects of language
model adaptation via bilingual latent semantic mod-
eling on the TED background LM against a baseline
model that uses only the TED LM.
6.1 Bilingual Latent Semantic Model
Using the technique outlined in Section 5, we con-
struct bilingual documents by splitting the parallel
TED training corpus into 41,847 documents of 5
lines each. While each individual TED lecture could
be used as a document, our experimental goal is
to simulate near-time translation of speeches; thus,
we prefer to construct small documents to simulate
topic modeling on a spoken language scenario in
which the length of a talk is not known a priori.
We annotate the English source text for removal af-
ter inference. Figure 1 contains a sample document
constructed for PLSA training. (In fact, we distin-
2http://www.ted.com/talks/
3http://www.statmt.org/moses/
robert lang is a pioneer of the newest kind of origami ? us-
ing math and engineering principles to fold mind-blowingly
intricate designs that are beautiful and , sometimes , very
useful . my talk is ? flapping birds and space telescopes .
? and you would think that should have nothing to do with
one another , but i hope by the end of these 18 minutes
, you ?ll see a little bit of a relation . robert lang est un
pionnier des nouvelles techniques d? origami - base?es sur
des principes mathe?matiques et d? inge?nierie permettant de
cre?er des mode`les complexes et e?poustouflants , qui sont
beaux et parfois , tre`s utiles . ma confe?rence s? intitule ?
oiseaux en papier et te?lescopes spatiaux ? . et vous pensez
probablement que les uns et les autres n? ont rien en com-
mun , mais j? espe`re qu? a` l? issue de ces 18 minutes , vous
comprendrez ce qui les relie .
Figure 1: A sample bilingual document used for PLSA
training.
guish English words from French words by attach-
ing to the former a special suffix.) By using our in-
house implementation, training of the PLSA model
on the bilingual collection converged after 20 EM
iterations.
Using our PLSA model, we run inference on each
of the 476 test documents from the TED lectures,
constructed by splitting the test set into 5-line docu-
ments. Since our goal is to translate and evaluate the
test set, we construct monolingual (English) docu-
ments. Figure 2 provides an example of a document
to be inferred. We collect the bilingual unigram
pseudocounts after 10 iterations of inference and re-
move the English words. The TED lecture data is
transcribed by clauses, rather than full sentences, so
we do not add sentence splitting tags before training
our unigram language models.
As a result of PLSA inference, the probabilities
of target words increase with respect to the back-
ground language model. Table 1 demonstrates this
phenomenon by outlining several of the top ranked
words that have similar semantic meaning to non-
stop words on the source side. In every case, the
probabilityPA(w) increases fairly substantially with
respect to the PB(w). As a result, we expect that the
adapted language model will favor both fluent and
semantically correct translations as the adaptation is
suggesting better lexical choices of words.
298
we didn ?t have money , so we had a cheap , little ad , but we
wanted college students for a study of prison life . 75 peo-
ple volunteered , took personality tests . we did interviews .
picked two dozen : the most normal , the most healthy .
Figure 2: A sample English-only document (#230) used
for PLSA inference. A full unigram word distribution
will be inferred for both English and French.
Rank Word PA(w) PB(w) PA(w)/PB(w)
20 gens 8.41E-03 4.55E-05 184.84
22 vie 8.30E-03 1.09E-04 76.15
51 prix 2.59E-03 8.70E-05 29.77
80 e?cole 1.70E-03 6.13E-05 27.73
83 argent 1.60E-03 3.96E-05 40.04
86 personnes 1.52E-03 2.75E-04 5.23
94 aide 1.27E-03 7.71E-05 16.47
98 e?tudiants 1.20E-03 7.12E-05 16.85
119 marche? 9.22E-04 9.10E-05 10.13
133 e?tude 7.63E-04 4.55E-05 16.77
173 e?ducation 5.04E-04 2.97E-05 16.97
315 prison 2.65E-04 1.98E-05 13.38
323 universite? 2.60E-04 2.97E-05 8.75
Table 1: Sample unigram probabilities of the adaptation
model for document #230, compared to the baseline un-
igram probabilities. The French words selected are se-
mantically related to the English words in the adapted
document. The PLSA adaptation infers higher unigram
probabilities for words with latent topics related to the
source document.
6.2 MDI Adaptation
We perform MDI adaptation with each of the un-
igram language models to update the background
TED language model. We configure the adaptation
rate parameter ? to 0.3, as recommended in Fed-
erico (2002). The baseline LM is replaced with each
adapted LM, corresponding to the document to be
translated. We then calculate the mean perplexity of
the adapted LMs and the baseline, respectively. The
perplexity scores are shown in Table 2. We observe a
15.3% relative improvement in perplexity score over
the baseline.
6.3 Results
We perform MT experiments on the IWSLT 2010
evaluation set to compare the baseline and adapted
LMs. In the evaluation, we notice a 0.85 improve-
ment in BLEU (%), yielding a 3% improvement over
the baseline. The same performance trend in NIST
is observed with a 2.4% relative improvement com-
pared to the unadapted baseline. Our PLSA and
MDI-based adaptation method not only improves
fluency but also improves adequacy: the topic-
based adaptation approach is attempting to suggest
more appropriate words based on increased unigram
probabilities than that of the baseline LM. Table 3
demonstrates a large improvement in unigram se-
lection for the adapted TED model in terms of the
individual contribution to the NIST score, with di-
minishing effects on larger n-grams. The majority
of the overall improvements are on individual word
selection.
Examples of improved fluency and adequacy are
shown in Figure 3. Line 285 shows an example of a
translation that doesn?t provide much of an n-gram
improvement, but demonstrates more fluent output,
due to the deletion of the first comma and the move-
ment of the second comma to the end of the clause.
While ?installation? remains an inadequate noun in
this clause, the adapted model reorders the root
words ?rehab? and ?installation? (in comparison
with the baseline) and improves the grammaticality
of the sentence; however, the number does not match
between the determiner and the noun phrase. Line
597 demonstrates a perfect phrase translation with
respect to the reference translation using semantic
paraphrasing. The baseline phrase ?d?origine? is
transformed and attributed to the noun. Instead of
translating ?original? as a phrase for ?home?, the
adapted model captures the original meaning of the
word in the translation. Line 752 demonstrates an
improvement in adequacy through the replacement
of the word ?quelque? with ?autre.? Additionally,
extra words are removed.
These lexical changes result in the improvement
in translation quality due to topic-based adaptation
via PLSA.
LM Perplexity BLEU (%) NIST
Adapt TED 162.44 28.49 6.5956
Base TED 191.76 27.64 6.4405
Table 2: Perplexity, BLEU, and NIST scores for the base-
line and adapted models. The perplexity scores are aver-
aged across each document-specific LM adaptation.
299
NIST 1-gram 2-gram 3-gram
Adapt TED 4.8077 1.3925 0.3229
Base TED 4.6980 1.3527 0.3173
Difference 0.1097 0.0398 0.0056
Table 3: Individual unigram NIST scores for n-grams 1-3
of the baseline and adapted models. The improvement of
the adapted model over the baseline is listed below.
(Line 285)
, j? ai eu la chance de travailler dans les installations , rehab
j? ai eu la chance de travailler dans les rehab installation ,
j? ai la chance de travailler dans un centre de de?sintoxication
,
(Line 597)
d? origine , les ide?es qui ont de la valeur ?
d? avoir des ide?es originales qui ont de la valeur ?
d? avoir des ide?es originales qui ont de la valeur ?
(Line 752)
un nom qui appartient a` quelque chose d? autre , le soleil .
un nom qui appartient a` autre chose , le soleil .
le nom d? une autre chose , le soleil .
Figure 3: Three examples of improvement in MT results:
the first sentence in each collection corresponds to the
baseline, the second utilizes the adapted TED LMs, and
the third is the reference translation.
7 Conclusions
An alternative approach to bilingual topic modeling
has been presented that integrates the PLSA frame-
work with MDI adaptation that can effectively adapt
a background language model when given a docu-
ment in the source language. Rather than training
two topic models and enforcing a one-to-one cor-
respondence for translation, we use the assumption
that parallel texts refer to the same topics and have
a very similar topic distribution. Preliminary exper-
iments show a reduction in perplexity and an overall
improvement in BLEU and NIST scores on speech
translation. We also note that, unlike previous works
involving topic modeling, we did not remove stop
words and punctuation, but rather assumed that these
features would have a relatively uniform topic distri-
bution.
One downside to the MDI adaptation approach
is that the computation of the normalization term
z(h) is expensive and potentially prohibitive during
continuous speech translation tasks. Further investi-
gation is needed to determine if there is a suitable
approximation that avoids computing probabilities
across all n-grams.
Acknowledgments
This work was supported by the T4ME network of
excellence (IST-249119), funded by the DG INFSO
of the European Commission through the Seventh
Framework Programme. The first author received a
grant under the Erasmus Mundus Language & Com-
munication Technologies programme.
References
David M. Blei, Andrew Ng, and Michael Jordan.
Latent Dirichlet Allocation. JMLR, 3:993?1022,
2003.
Stanley F. Chen and Joshua Goodman. An empirical
study of smoothing techniques for language mod-
eling. Computer Speech and Language, 4(13):
359?393, 1999.
J. N. Darroch and D. Ratcliff. Generalized itera-
tive scaling for log-linear models. The Annals of
Mathematical Statistics, 43(5):1470?1480, 1972.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harsh-
man. Indexing by Latent Semantic Analysis.
Journal of the American Society for Information
Science, 41:391?407, 1990.
A. P. Dempster, N. M. Laird, and D. B. Rubin.
Maximum-likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statis-
tical Society, B, 39:1?38, 1977.
Marcello Federico. Efficient language model adap-
tation through MDI estimation. In Proceedings of
the 6th European Conference on Speech Commu-
nication and Technology, volume 4, pages 1583?
1586, Budapest, Hungary, 1999.
Marcello Federico. Language Model Adaptation
through Topic Decomposition and MDI Estima-
tion. In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, volume I, pages 703?706, Orlando, FL,
2002.
300
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. IRSTLM: an Open Source Toolkit for Han-
dling Large Scale Language Models. In Pro-
ceedings of Interspeech, pages 1618?1621, Mel-
bourne, Australia, 2008.
George Foster and Roland Kuhn. Mixture-model
adaptation for SMT. In Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, pages 128?135, Prague, Czech Republic,
June 2007. Association for Computational Lin-
guistics. URL http://www.aclweb.org/
anthology/W/W07/W07-0217.
Zhengxian Gong, Yu Zhang, and Guodong Zhou.
Statistical Machine Translation based on LDA.
In Universal Communication Symposium (IUCS),
2010 4th International, pages 286 ?290, oct.
2010. doi: 10.1109/IUCS.2010.5666182.
Thomas Hofmann. Probabilistic Latent Semantic
Analysis. In Proceedings of the 15th Conference
on Uncertainty in AI, pages 289?296, Stockholm,
Sweden, 1999.
Bo-June (Paul) Hsu and James Glass. Style & topic
language model adaptation using HMM-LDA. In
in Proc. ACL Conf. on Empirical Methods in Nat-
ural Language Processing ? EMNLP, pages 373?
381, 2006.
Reinhard Kneser, Jochen Peters, and Dietrich
Klakow. Language Model Adaptation Using Dy-
namic Marginals. In Proceedings of the 5th Euro-
pean Conference on Speech Communication and
Technology, pages 1971?1974, Rhodes, Greece,
1997.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. Moses: Open Source
Toolkit for Statistical Machine Translation. In
Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech
Republic, 2007. URL http://aclweb.org/
anthology-new/P/P07/P07-2045.pdf.
Philipp Koehn and Josh Schroeder. Experi-
ments in Domain Adaptation for Statistical Ma-
chine Translation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, pages 224?227, Prague, Czech Republic,
June 2007. Association for Computational Lin-
guistics. URL http://www.aclweb.org/
anthology/W/W07/W07-0233.
David Mimno, Hanna M. Wallach, Jason Narad-
owsky, David A. Smith, and Andrew McCallum.
Polylingual Topic Models. In Proceedings of
the 2009 Conference on Empirical Methods in
Natural Language Processing. Association for
Computational Linguistics, August 2009. URL
http://www.cs.umass.edu/?mimno/
papers/mimno2009polylingual.pdf.
Franz Josef Och. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Er-
hard Hinrichs and Dan Roth, editors, Proceed-
ings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics, pages
160?167, 2003. URL http://www.aclweb.
org/anthology/P03-1021.pdf.
Abhinav Sethy, Panayiotis Georgiou, and Shrikanth
Narayanan. Selecting relevant text subsets
from web-data for building topic specific lan-
guage models. In Proceedings of the Hu-
man Language Technology Conference of the
NAACL, Companion Volume: Short Papers,
pages 145?148, New York City, USA, June
2006. Association for Computational Linguis-
tics. URL http://www.aclweb.org/
anthology/N/N06/N06-2037.
Yik-Cheung Tam and Tanja Schultz. Incorporating
monolingual corpora into bilingual latent seman-
tic analysis for crosslingual lm adaptation. In
Acoustics, Speech and Signal Processing, 2009.
ICASSP 2009. IEEE International Conference on,
pages 4821 ?4824, april 2009. doi: 10.1109/
ICASSP.2009.4960710.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz.
Bilingual LSA-based adaptation for statistical
machine translation. Machine Translation,
21:187?207, December 2007. ISSN 0922-
6567. doi: 10.1007/s10590-008-9045-2. URL
http://portal.acm.org/citation.
cfm?id=1466799.1466803.
Bing Zhao and Eric P. Xing. HM-BiTAM: Bilin-
gual topic exploration, word alignment, and trans-
301
lation. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Informa-
tion Processing Systems 20, pages 1689?1696.
MIT Press, Cambridge, MA, 2008.
Bing Zhao, Matthias Eck, and Stephan Vogel. Lan-
guage Model Adaptation for Statistical Machine
Translation via Structured Query Models. In Pro-
ceedings of Coling 2004, pages 411?417, Geneva,
Switzerland, Aug 23?Aug 27 2004. COLING.
302
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 372?378,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The Uppsala-FBK systems at WMT 2011
Christian Hardmeier
Jo?rg Tiedemann
Uppsala universitet
Inst. fo?r lingvistik och filologi
Uppsala, Sweden
first.last@lingfil.uu.se
Markus Saers
Human Language
Technology Center
Hong Kong Univ. of
Science & Technology
masaers@cs.ust.hk
Marcello Federico
Mathur Prashant
Fondazione Bruno Kessler
Human Language Technologies
Trento, Italy
lastname@fbk.eu
Abstract
This paper presents our submissions to the
shared translation task at WMT 2011. We
created two largely independent systems
for English-to-French and Haitian Creole-to-
English translation to evaluate different fea-
tures and components from our ongoing re-
search on these language pairs. Key features
of our systems include anaphora resolution,
hierarchical lexical reordering, data selection
for language modelling, linear transduction
grammars for word alignment and syntax-
based decoding with monolingual dependency
information.
1 English to French
Our submission to the English-French task was a
phrase-based Statistical Machine Translation based
on the Moses decoder (Koehn et al, 2007). Phrase
tables were separately trained on Europarl, news
commentary and UN data and then linearly inter-
polated with uniform weights. For language mod-
elling, we used 5-gram models trained with the
IRSTLM toolkit (Federico et al, 2008) on the mono-
lingual News corpus and parts of the English-French
109 corpus. More unusual features of our system
included a special component to handle pronomi-
nal anaphora and the hierarchical lexical reordering
model by Galley and Manning (2008). Selected fea-
tures of our system will be discussed in depth in the
following sections.
1.1 Handling pronominal anaphora
Pronominal anaphora is the use of pronominal ex-
pressions to refer to ?something previously men-
tioned in the discourse? (Strube, 2006). It is a very
common phenomenon found in almost all kinds of
texts. Anaphora can be local to a sentence, or it can
cross sentence boundaries. Standard SMT methods
do not handle this phenomenon in a satisfactory way
at present: For sentence-internal anaphora, they de-
pend on the n-gram language model with its lim-
ited history, while cross-sentence anaphora is left
to chance. We therefore added a word-dependency
model (Hardmeier and Federico, 2010) to our sys-
tem to handle anaphora explicitly.
Our processing of anaphoric pronouns follows
the procedure outlined by Hardmeier and Federico
(2010). We use the open-source coreference resolu-
tion system BART (Broscheit et al, 2010) to link
pronouns to their antecedents in the text. Coref-
erence links are handled differently depending on
whether or not they cross sentence boundaries. If
a coreference link points to a previous sentence, we
process the sentence containing the antecedent with
the SMT system and look up the translation of the
antecedent in the translated output. If the corefer-
ence link is sentence-internal, the translation lookup
is done dynamically by the decoder during search.
In either case, the word-dependency model adds a
feature function to the decoder score representing
the probability of a particular pronoun choice given
the translation of the antecedent.
In our English-French system, this model was
only applied to the inanimate pronouns it and they,
which seemed to be the most promising candidates
for improvement since their French equivalents re-
quire gender marking. It was trained on data au-
tomatically annotated for anaphora taken from the
news-commentary corpus, and the vocabulary of the
predicted pronouns was limited to words recognised
as pronouns by the POS tagger.
372
1.2 Hierarchical lexical reordering
The basic word order model of SMT penalises any
divergence between the order of the words in the in-
put sentence and the order of their translation equiv-
alents in the MT output. All reordering must thus be
driven by the language model when no other reorder-
ing model is present. Lexical reordering models
making certain word order choices in the MT out-
put conditional on the identity of the words involved
have been a standard component in SMT for some
years. The lexical reordering model usually em-
ployed in the Moses decoder was implemented by
Koehn et al (2005). Adopting the perspective of the
SMT decoder, which produces the target sentence
from left to right while covering source phrases in
free order, the model distinguishes between three or-
dering classes, monotone, swap and discontinuous,
depending on whether the source phrases giving rise
to the two last target phrases emitted were adjacent
in the same order, adjacent in swapped order or sep-
arated by other source words. Probabilities for each
ordering class given source and target phrase are
estimated from a word-aligned training corpus and
integrated into MT decoding as extra feature func-
tions.
In our submission, we used the hierarchical lexi-
cal reordering model proposed by Galley and Man-
ning (2008) and recently implemented in the Moses
decoder.1 This model uses the same approach of
classifying movements as monotone, swap or dis-
continuous, but unlike the phrase-based model, it
does not require the source language phrases to be
strictly adjacent in order to be counted as monotone
or swap. Instead, a phrase can be recognised as ad-
jacent to, or swapped with, a contiguous block of
source words that has been segmented into multi-
ple phrases. Contiguous phrase blocks are recog-
nised by the decoder with a shift-reduce parsing al-
gorithm. As a result, fewer jumps are labelled with
the uninformative discontinuous class.
1.3 Data selection from the WMT Giga corpus
One of the supplied language resources for this eval-
uation is the French-English WMT Giga corpus,
1The hierarchical lexical reordering model was imple-
mented in Moses during MT Marathon 2010 by Christian Hard-
meier, Gabriele Musillo, Nadi Tomeh, Ankit Srivastava, Sara
Stymne and Marcello Federico.
 60 80 100 120 140 160 180 200 220 240 260 280
 100  150  200  250  300  350  400 60 80 100 120 140 160 180 200 220 240 260 280LM Perplexity LM size (million 5-grams)Data Selection ThresholdThreshold vs PerplexityThreshold vs LM Size
Figure 1: Perplexity and size of language models trained
on data of the WMT Giga corpus that were selected using
different perplexity thresholds.
aka 109 corpus, a large collection of parallel sen-
tences crawled from Canadian and European Union
sources. While this corpus was too large to be used
for model training with the means at our disposal,
we exploited it as a source of parallel data for trans-
lation model training as well as monolingual French
data for the language model by filtering it down to a
manageable size. In order to extract sentences close
to the news translation task, we applied a simple
data selection procedure based on perplexity. Sen-
tence pairs were selected from the WMT Giga cor-
pus if the perplexity of their French part with respect
to a language model (LM) trained on French news
data was below a given threshold. The rationale is
that text sentences which are better predictable by
the LM should be closer to the news domain. The
threshold was set in a way to capture enough novel
n-grams, from one side, but also to avoid adding too
many irrelevant n-grams. It was tuned by training
a 5-gram LM on the selected data and checking its
size and its perplexity on a development set. In fig-
ure 1 we plot perplexity and size of the WMT Giga
LM for different values of the data-selection thresh-
old. Perplexities are computed on the newstest2009
set. As a good perplexity-size trade-off, the thresh-
old 250 was chosen to estimate an additional 5-gram
LM (WMT Giga 250) that was interpolated with
the original News LM. The resulting improvement
in perplexity is reported in table 1. For translation
model data, a perplexity threshold of 159 was ap-
plied.
373
LM Perplexity OOV rate
News 146.84 0.82
News + WMT Giga 250 130.23 0.71
Table 1: Perplexity reduction after interpolating the News
LM with data selected from the 109 corpus.
newstest
2009 2010 2011
Primary submission 0.246 0.286 0.284
w/o Anaphora handling 0.246 0.286 0.284
WMT Giga data
w/o LM 0.244 0.289 0.280
w/o TM 0.247 0.286 0.282
w/o LM and TM 0.247 0.289 0.278
Lexical reordering
phrase-based reo 0.239 0.281 0.275
no lexical reo 0.239 0.281 0.275
with LDC data 0.254 0.293 0.291
Table 2: Ablation test results (case-sensitive BLEU)
1.4 Results and Ablation tests
Owing to time constraints, we were not able to run
thorough tests on our system before submitting it to
the evaluation campaign. We therefore evaluated the
various components included in a post hoc fashion
by running ablation tests. In each test, we left out
one of the system components to identify its effect
on the overall performance. The results of these tests
are reported in table 2.
Performance-wise, the most important particular-
ity of our SMT system was the hierarchical lexical
reordering model, which led to a sizeable improve-
ment of 0.7, 0.5 and 0.9 BLEU points for the 2009,
2010 and 2011 test sets, respectively. We had previ-
ously seen negative results when trying to apply the
same model to English-German SMT, so its perfor-
mance seems to be strongly dependent on the lan-
guage pair it is used with.
Compared to the scores obtained using the full
system, the anaphora handling system did not have
any effect on the BLEU scores. This result is
similar to our result for English-German transla-
tion (Hardmeier and Federico, 2010). Unfortu-
nately, for English-French, the negative results ex-
tends to the pronoun translation scores (not reported
here), where slightly higher recall with the word-
dependency model was overcompensated by de-
graded precision, so the outcome of the experiments
clearly suggests that the anaphora handling proce-
dure is in need of improvement.
The effect of the WMT Giga language model dif-
fers among the test sets. For the 2009 and 2011
test sets, it results in an improvement of 0.2 and 0.4
BLEU points, respectively, while the 2010 test set
fares better without this additional language model.
However, it should be noted that there may be a
problem with the 2010 test set and the News lan-
guage model, which was used as a component in all
our systems. In particular, upgrading the News LM
data from last year?s to this year?s release led to an
improvement of 4 BLEU points on the 2010 test set
and an unrealistically low perplexity of 73 as com-
pared to 130 for the 2009 test set, which makes us
suspect that the latest News LM data may be tainted
with data from the 2010 test corpus. If this is the
case, the 2010 test set should be considered unreli-
able for LM evaluation. The benefit of adding WMT
Giga data to the translation model is less clear. For
the 2009 and 2010 test sets, this leads to a slight
degradation, but for the 2011 corpus, we obtained
a small improvement.
Our shared task submission did not use the French
Gigaword corpus from the Linguistic Data Consor-
tium (LDC2009T28), which is not freely available
to sites without LDC membership. After the sub-
mission, we ran a contrastive experiment including
a 5-gram model trained on this corpus, which led
to a sizeable improvement of 0.7?0.8 BLEU points
across all test sets.
2 Haitian Creole to English
Our experiments with the Haitian Creole-English
data are independent of the system presented for the
English to French task above. We experimented with
both phrase-based SMT and syntax-based SMT. The
main questions we investigated were i) whether we
can improve word alignment and phrase extraction
for phrase-based SMT and ii) whether we can in-
tegrate dependency parsing into a syntax-based ap-
proach. All our experiments were conducted on the
clean data set using Moses for training and decod-
ing. In the following we will first describe the exper-
iments with phrase-based models and linear trans-
374
duction grammars for word alignment and, there-
after, our findings from integrating English depen-
dency parses into a syntax-based approach.
2.1 Phrase-based SMT
The phrase-based system that we used in this series
of experiments uses a rather traditional setup. For
the translations into English we used the news data
provided for the other translations tasks in WMT
2011 to build a large scale-background language
model. The English data from the Haitian Creole
task were used as a separate domain-specific lan-
guage model. For the other translation direction we
only used the in-domain data provided. We used
standard 5-gram models with Witten-Bell discount-
ing and backoff interpolation for all language mod-
els. For the translation model we applied standard
techniques and settings for phrase extraction and
score estimations. However, we applied two differ-
ent systems for word alignment: One is the standard
GIZA++ toolbox implementing the IBM alignment
models (Och and Ney, 2003) and extensions and the
other is based on transduction grammars which will
briefly be introduced in the next section.
2.1.1 Alignment with PLITGs
By making the assumption that the parallel cor-
pus constitutes a linear transduction (Saers, 2011)2
we can induce a grammar that is the most likely to
have generated the observed corpus. The grammar
induced will generate a parse forest for each sen-
tence pair in the corpus, and each parse tree in that
forest will correspond to an alignment between the
two sentences. Following Saers et al (2010), the
alignment corresponding to the best parse can be ex-
tracted and used instead of other word alignment ap-
proaches such as GIZA++. There are several gram-
mar types that generate linear transductions, and in
this work, stochastic bracketing preterminalized lin-
ear inversion transduction grammars (PLITG) were
used (Saers and Wu, 2011). Since we were mainly
interested in the word alignments, we did not induce
phrasal grammars.
Although alignments from PLITGs may not reach
the same level of translation quality as GIZA++,
they make different mistakes, so both complement
2A transduction is a set of pairs of strings, and thus repre-
sents a relation between two languages.
each other. By duplicating the training corpus and
aligning each copy of the corpus with a different
alignment tool, the phrase extractor seems to be able
to pick the best of both worlds, producing a phrase
table that is superior to one produced with either of
the alignments tools used in isolation.
2.1.2 Results
In the following we present our results on the pro-
vided test set3 for translating into both languages
with phrase-based systems trained on different word
alignments. Table 3 summarises the BLEU scores
obtained.
English-Haitian BLEU phrase-table
GIZA++ 0.2567 3,060,486
PLITG 0.2407 5,007,254
GIZA++ & PLITG 0.2572 7,521,754
Haitian-English BLEU phrase-table
GIZA++ 0.3045 3,060,486
PLITG 0.2922 5,049,280
GIZA++ & PLITG 0.3105 7,561,043
Table 3: Phrase-based SMT (pbsmt) on the Haitian
Creole-English test set with different word alignments.
From the table we can see that phrase-based sys-
tems trained on PLITG alignments performs slightly
worse than the ones trained on GIZA++. However
combining both alignments with the simple data du-
plication technique mentioned earlier produces the
overall best scores in both translation directions.
The fact that both alignments lead to complemen-
tary information can be seen in the size of the phrase
tables extracted (see table 3).
2.2 Syntax-based SMT
We used Moses and its syntax-mode for our exper-
iments with hierarchical phrase-based and syntax-
augmented models. Our main interest was to in-
vestigate the influence of monolingual parsing on
the translation performance. In particular, we tried
to integrate English dependency parses created by
MaltParser (Nivre et al, 2007) trained on the Wall
Street Journal section of the Penn Treebank (Mar-
cus et al, 1993) extended with about 4000 questions
3We actually swapped the development set and the test set
by mistake. But, of course, we never mixed development and
test data in any result reported.
375
from the Question Bank (Judge et al, 2006). The
conversion to dependency trees was done using the
Stanford Parser (de Marneffe et al, 2006). Again,
we ran both translation directions to test our settings
in more than just one task. Interesting here is also
the question whether there are significant differences
when integrating monolingual parses on the source
or on the target side.
The motivation for applying dependency parsing
in our experiments is to use the specific information
carried by dependency relations. Dependency struc-
tures encode functional relations between words that
can be seen as an interface to the semantics of a
sentence. This information is usually not avail-
able in phrase-structure representations. We believe
that this type of information can be beneficial for
machine translation. For example, knowing that a
noun acts as the subject of a sentence is more in-
formative than just marking it as part of a noun
phrase. Whether or not this information can be ex-
plored by current syntax-based machine translation
approaches that are optimised for phrase-structure
representations is a question that we liked to inves-
tigate. For comparison we also trained hierarchical
phrase-based models without any additional annota-
tion.
2.2.1 Converting projective dependency trees
First we needed to convert dependency parses to
a tree representation in order to use our data in
the standard models of syntax-based models imple-
mented in Moses. In our experiments, we used
a parser model that creates projective dependency
graphs that can be converted into tree structures of
nested segments. We used the yield of each word
(referring to that word and its transitive dependents)
to define spans of phrases and their dependency rela-
tions are used as span labels. Furthermore, we also
defined pre-terminal nodes that encode the part-of-
speech information of each word. These tags were
obtained using the HunPos tagger (Hala?csy et al,
2007) trained on the Wall Street Journal section of
the Penn Treebank. Figure 2 illustrates the conver-
sion process. Tagging and parsing is done for all En-
glish data without any manual corrections or optimi-
sation of parameters. After the conversion, we were
able to use the standard training procedures imple-
mented in Moses.
-ROOT- andCC howWRB oldJJ isVBZ yourPRP$ nephewNN ?.
advmoddep possnsubjcc
punctnull
<tree label="null">
<tree label="cc">
<tree label="CC">and</tree>
</tree>
<tree label="dep">
<tree label="advmod">
<tree label="WRB">how</tree>
</tree>
<tree label="JJ">old</tree>
</tree>
<tree label="VBZ">is</tree>
<tree label="nsubj">
<tree label="poss">
<tree label="PRP$">your</tree>
</tree>
<tree label="NN">nephew</tree>
</tree>
<tree label="punct">
<tree label=".">?</tree>
</tree>
</tree>
Figure 2: A dependency graph from the training corpus
and its conversion to a nested tree structure. The yield of
each word in the sentence defines a span with the label
taken from the relation of that word to its head. Part-of-
speech tags are used as additional pre-terminal nodes.
2.2.2 Experimental Results
We ran several experiments with slightly differ-
ent settings. We used the same basic setup for
all of them including the same language models
and GIZA++ word alignments that we have used
for the phrase-based models already. Further, we
used Moses for extracting rules of the syntax-based
translation model. We use standard settings for
the baseline system (=hiero) that does not employ
any linguistic markup. For the models that include
dependency-based trees we changed the maximum
span threshold to a high value of 999 (default: 15)
in order to extract as many rules as possible. This
large degree of freedom is possible due to the oth-
erwise strong constraints on rule flexibility imposed
by the monolingual syntactic markup. Rule tables
are dramatically smaller than for the unrestricted hi-
erarchical models (see table 4).
However, rule restriction by linguistic constraints
usually hurts performance due to the decreased cov-
erage of the rule set. One common way of improving
376
reference Are you going to let us die on Ile a` Vaches which is located close the city of Les Cayes. I am ...
pbsmt Do you are letting us die in Ilavach island?s on in Les Cayes. I am ...
hiero do you will let us die in the island Ilavach on the in Les Cayes . I am ...
samt2 Are you going to let us die in the island Ilavach the which is on the Les. My name is ...
reference I?m begging you please help me my situation is very critical.
pbsmt Please help me please. Because my critical situation very much.
hiero please , please help me because my critical situation very much .
samt2 Please help me because my situation very critical.
reference I don?t have money to go and give blood in Port au Prince from La Gonave.
pbsmt I don?t have money, so that I go to give blood Port-au-Prince since lagonave.
hiero I don ?t have any money , for me to go to give blood Port-au-Prince since lagonave .
samt2 I don?t have any money, to be able to go to give blood Port-au-Prince since Gona?ve Island.
Figure 3: Example translations for various models.
English-Haitian BLEU number of rules
hiero 0.2549 34,118,622
malt (source) 0.2180 1,628,496
- binarised 0.2327 9,063,933
- samt1 0.2311 11,691,279
- samt2 0.2366 29,783,694
Haitian-English BLEU number of rules
hiero 0.3034 33,231,535
malt (target) 0.2739 1,922,688
- binarised 0.2857 8,922,343
- samt1 0.2952 11,073,764
- samt2 0.2954 24,554,317
Table 4: Syntax-based SMT on the Haitian Creole-
English test set with (=malt) or without (=hiero) English
parse trees and various parse relaxation strategies. The
final system submitted to WMT11 is malt(target)-samt2.
rule extraction is based on tree manipulation and re-
laxed extraction algorithms. Moses implements sev-
eral algorithms that have been proposed in the lit-
erature. Tree binarisation is one of them. This can
be done in a left-branching and in a right-branching
mode. We used a combination of both in the set-
tings denoted as binarised. The other relaxation al-
gorithms are based on methods proposed for syntax-
augmented machine translation (Zollmann et al,
2008). We used two of them: samt1 combines pairs
of neighbouring children nodes into combined com-
plex nodes and creates additional complex nodes of
all children nodes except the first child and similar
complex nodes for all but the last child. samt2 com-
bines any pair of neighbouring nodes even if they are
not children of the same parent. All of these relax-
ation algorithms lead to increased rule sets (table 4).
In terms of translation performance there seems to
be a strong correlation between rule table size and
translation quality as measured by BLEU. None of
the dependency-based models beats the unrestricted
hierarchical model. Both translation directions be-
have similar with slightly worse performances of
the dependency-based models (relative to the base-
line) when syntax is used on the source language
side. Note also that all syntax-based models (includ-
ing hiero) are below the corresponding phrase-based
SMT systems. Of course, automatic evaluation has
its limits and interesting qualitative differences may
be more visible in manual assessments. The use of
linguistic information certainly has an impact on the
translation hypotheses produced as we can see in the
examples in figure 3. In the future, we plan to inves-
tigate the effect of dependency information on gram-
maticality of translated sentences in more detail.
3 Conclusions
In our English-French and Haitian Creole-English
shared task submissions, we investigated the use
of anaphora resolution, hierarchical lexical reorder-
ing and data selection for language modelling
(English-French) as well as LTG word alignment
and syntax-based decoding with dependency infor-
mation (Haitian Creole-English). While the re-
sults for the systems with anaphora handling were
somewhat disappointing and the effect of data fil-
tering was inconsistent, hierarchical lexical reorder-
ing brought substantial improvements. We also ob-
tained consistent gains by combining information
from different word aligners, and we presented a
simple way of including dependency parses in stan-
dard tree-based decoding.
377
Acknowledgements
Most of the features used in our English-French sys-
tem were originally developed while Christian Hard-
meier was at FBK. Activities at FBK were supported
by the EuroMatrixPlus project (IST-231720) and the
T4ME network of excellence (IST-249119), both
funded by the DG INFSO of the European Commis-
sion through the Seventh Framework Programme.
References
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodriguez, Lorenza Romano,
Olga Uryupina, Yannick Versley, and Roberto Zanoli.
2010. BART: A multilingual anaphora resolution sys-
tem. In Proceedings of the 5th International Workshop
on Semantic Evaluations (SemEval-2010), Uppsala.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Interspeech 2008,
pages 1618?1621. ISCA.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 847?855, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz. 2007.
Hunpos: an open source trigram tagger. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 209?
212.
Christian Hardmeier and Marcello Federico. 2010. Mod-
elling Pronominal Anaphora in Statistical Machine
Translation. In Marcello Federico, Ian Lane, Michael
Paul, and Franc?ois Yvon, editors, Proceedings of the
seventh International Workshop on Spoken Language
Translation (IWSLT), pages 283?289.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: creating a corpus of parse-annotated
questions. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 497?504.
Philipp Koehn, Amittai Axelrod, Alexandra
Birch Mayne, et al 2005. Edinburgh system
description for the 2005 iwslt speech translation
evaluation. In International workshop on spoken
language translation, Pittsburgh.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for Statistical Ma-
chine Translation. In Annual meeting of the Associ-
ation for Computational Linguistics: Demonstration
session, pages 177?180, Prague.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: The Penn Treebank. Computational
Linguistics, 19:313?330, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational linguistics, 29:19?51.
Markus Saers and Dekai Wu. 2011. Principled induction
of phrasal bilexica. In Proceedings of the 15th Annual
Conference of the European Association for Machine
Translation, Leuven, Belgium, May.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010. Word
alignment with stochastic bracketing linear inversion
transduction grammar. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 341?344, Los Angeles, California,
June.
Markus Saers. 2011. Translation as Linear Transduc-
tion: Models and Algorithms for Efficient Learning in
Statistical Machine Translation. Ph.D. thesis, Uppsala
University, Department of Linguistics and Philology.
M. Strube. 2006. Anaphora and coreference resolution,
Statistical. In Encyclopedia of language and linguis-
tics, pages 216?222. Elsevier.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
mt. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics - Volume 1, pages
1145?1152.
378
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 171?180,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Match without a Referee:
Evaluating MT Adequacy without Reference Translations
Yashar Mehdad Matteo Negri Marcello Federico
Fondazione Bruno Kessler, FBK-irst
Trento , Italy
{mehdad|negri|federico}@fbk.eu
Abstract
We address two challenges for automatic ma-
chine translation evaluation: a) avoiding the
use of reference translations, and b) focusing
on adequacy estimation. From an economic
perspective, getting rid of costly hand-crafted
reference translations (a) permits to alleviate
the main bottleneck in MT evaluation. From
a system evaluation perspective, pushing se-
mantics into MT (b) is a necessity in order
to complement the shallow methods currently
used overcoming their limitations. Casting
the problem as a cross-lingual textual entail-
ment application, we experiment with differ-
ent benchmarks and evaluation settings. Our
method shows high correlation with human
judgements and good results on all datasets
without relying on reference translations.
1 Introduction
While syntactically informed modelling for statis-
tical MT is an active field of research that has re-
cently gained major attention from the MT commu-
nity, work on integrating semantic models of ade-
quacy into MT is still at preliminary stages. This sit-
uation holds not only for system development (most
current methods disregard semantic information, in
favour of statistical models of words distribution),
but also for system evaluation. To realize its full po-
tential, however, MT is now in the need of semantic-
aware techniques, capable of complementing fre-
quency counts with meaning representations.
In order to integrate semantics more deeply into
MT technology, in this paper we focus on the eval-
uation dimension. Restricting our investigation to
some of the more pressing issues emerging from this
area of research, we provide two main contributions.
1. An automatic evaluation method that avoids
the use of reference translations. Most current
metrics are based on comparisons between auto-
matic translations and human references, and reward
lexical similarity at the n-gram level (e.g. BLEU
(Papineni et al, 2002), NIST (Doddington, 2002),
METEOR (Banerjee and Lavie, 2005), TER (Snover
et al, 2006)). Due to the variability of natural lan-
guages in terms of possible ways to express the same
meaning, reliable lexical similarity metrics depend
on the availability of multiple hand-crafted (costly)
realizations of the same source sentence in the tar-
get language. Our approach aims to avoid this bot-
tleneck by adapting cross-lingual semantic inference
capabilities and judging a translation only given the
source sentence.
2. A method for evaluating translation adequacy.
Most current solutions do not consistently reward
translation adequacy (semantic equivalence between
source sentence and target translation). The scarce
integration of semantic information in MT, specif-
ically at the multilingual level, led to MT systems
that are ?illiterate? in terms of semantics and mean-
ing. Moreover, current metrics are often difficult to
interpret. In contrast, our method targets the ade-
quacy dimension, producing easily interpretable re-
sults (e.g. judgements in a 4-point scale).
Our approach builds on recent advances in
cross-lingual textual entailment (CLTE) recognition,
which provides a natural framework to address MT
adequacy evaluation. In particular, we approach
the problem as an application of CLTE where bi-
171
directional entailment between source and target is
considered as evidence of translation adequacy. Be-
sides avoiding the use of references, the proposed
solution differs from most previous methods which
typically rely on surface-level features, often ex-
tracted from the source or the target sentence taken
in isolation. Although some of these features might
correlate well with adequacy, they capture seman-
tic equivalence only indirectly, and at the level of
a probabilistic prediction. Focusing on a combina-
tion of surface, syntactic and semantic features, ex-
tracted from both source and target (e.g. ?source-
target length ratio?, ?dependency relations in com-
mon?), our approach leads to informed adequacy
judgements derived from the actual observation of
a translation given the source sentence.
2 Background
Some recent works proposed metrics able to approx-
imately assess meaning equivalence between can-
didate and reference translations. Among these,
(Gime?nez and Ma`rquez, 2007) proposed a hetero-
geneous set comprising overlapping and matching
metrics, compiled from a rich set of variants at five
different linguistic levels: lexical, shallow-syntactic,
syntactic, shallow-semantic and semantic. More
similar to our approach, (Pado? et al, 2009) proposed
semantic adequacy metrics that exploit feature rep-
resentations motivated by Textual Entailment (TE).
Both metrics, however, highly depend on the avail-
ability of multiple reference translations.
Early attempts to avoid reference translations ad-
dressed quality estimation (QE) by means of large
numbers of source, target, and system-dependent
features to discriminate between ?good? and ?bad?
translations (Blatz et al, 2004; Quirk, 2004). More
recently (Specia et al, 2010b; Specia and Farzindar,
2010; Specia, 2011) conducted a series of experi-
ments using features designed to estimate translation
post-editing effort (in terms of volume and time) as
an indicator of MT output quality. Good results in
QE have been achieved by adding linguistic infor-
mation such as shallow parsing, POS tags (Xiong
et al, 2010), or dependency relations (Bach et al,
2011; Avramidis et al, 2011) as features. However,
in general these approaches do not distinguish be-
tween fluency (i.e. syntactic correctness of the out-
put translation) and adequacy, and mostly rely on
fluency-oriented features (e.g. ?number of punctu-
ation marks?). As a result, a simple surface form
variation is given the same importance of a content
word variation that changes the meaning of the sen-
tence. To the best of our knowledge, only (Specia et
al., 2011) proposed an approach to frame MT evalu-
ation as an adequacy estimation problem. However,
their method still includes many features which are
not focused on adequacy, and often look either at the
source or at the target in isolation (see for instance
?source complexity? and ?target fluency? features).
Moreover, the actual contribution of the adequacy
features used is not always evident and, for some
testing conditions, marginal.
Our approach to adequacy evaluation builds on
and extends the above mentioned works. Similarly
to (Pado? et al, 2009) we rely on the notion of textual
entailment, but we cast it as a cross-lingual problem
in order to bypass the need of reference translations.
Similarly to (Blatz et al, 2004; Quirk, 2004), we try
to discriminate between ?good? and ?bad? transla-
tions, but we focus on adequacy. To this aim, like
(Xiong et al, 2010; Bach et al, 2011; Avramidis et
al., 2011; Specia et al, 2010b; Specia et al, 2011)
we rely on a large number of features, but focusing
on source-target dependent ones, aiming at informed
adequacy evaluation of a translation given the source
instead of a more generic quality assessment based
on surface features.
3 CLTE for adequacy evaluation
We address adequacy evaluation by adapting cross-
lingual textual entailment recognition as a way to
measure to what extent a source sentence and its au-
tomatic translation are semantically similar. CLTE
has been proposed by (Mehdad et al, 2010) as an ex-
tension of textual entailment (Dagan and Glickman,
2004) that consists in deciding, given a text T and a
hypothesis H in different languages, if the meaning
of H can be inferred from the meaning of T.
The main motivation in approaching adequacy
evaluation using CLTE is that an adequate trans-
lation and the source text should convey the same
meaning. In terms of entailment, this means that an
adequate MT output and the source sentence should
entail each other (bi-directional entailment). Los-
172
ing or altering part of the meaning conveyed by the
source sentence (i.e. having more, or different infor-
mation in one of the two sides) will change the en-
tailment direction and, consequently, the adequacy
judgement. Framed in this way, CLTE-based ade-
quacy evaluation methods can be designed to dis-
tinguish meaning-preserving variations from true di-
vergence, regardless of reference translations.
Similarly to many monolingual TE approaches,
CLTE solutions proposed so far adopt supervised
learning methods, with features that measure to what
extent the hypotheses can be mapped into the texts.
The underlying assumption is that the probability of
entailment is proportional to the number of words in
H that can be mapped to words in T (Mehdad et al,
2011). Such mapping can be carried out at differ-
ent word representation levels (e.g. tokens, lemmas,
stems), possibly with the support of lexical knowl-
edge in order to cross the language barrier between
T and H (e.g. dictionaries, phrase tables).
Under the same assumption, since in the adequacy
evaluation framework the entailment relation should
hold in both directions, the mapping is performed
both from the source to the target and vice-versa,
building on features extracted from both sentences.
Moreover, to improve over previous CLTE methods
and boost MT adequacy evaluation performance, we
explore the joint contribution of a number of lexi-
cal, syntactic and semantic features (Mehdad et al,
2012).
Concerning the features used, it?s worth observ-
ing that the cost of implementing our approach (in
terms of required resources and linguistic proces-
sors), and the need of reference translations are in-
trinsically different bottlenecks for MT. While the
limited availability of processing tools for some lan-
guage pairs is a ?temporary? bottleneck, the acqui-
sition of multiple references is a ?permanent? one.
The former cost is reducing over time due to the
progress in NLP research; the latter represents a
fixed cost that has to be eliminated. Similar consid-
erations hold regarding the need of annotated data to
develop our supervised learning approach. Concern-
ing this, the cost of labelling source-target pairs with
adequacy judgments is significantly lower compared
to the creation of multiple references.
3.1 Features
In order to learn models for classification and regres-
sion we used the Support Vector Machine (SVM)
algorithms implemented in the LIBSVM package
(Chang and Lin, 2011) with a linear kernel and de-
fault parameters setting. Aiming at objective ade-
quacy evaluation, our method limits the recourse to
MT system-dependent features to reduce the bias
of evaluating MT technology with its own core
methods. The experiments described in the follow-
ing sections are carried out on publicly available
English-Spanish datasets, exploring the potential of
a combination of surface, syntactic and semantic
features. Language-dependent ones are extracted
by exploiting processing tools for the two lan-
guages (part-of-speech taggers, dependency parsers
and named entity recognizers), most of which are
available for many languages.
Our feature set can be described as follows:
Surface Form (F) features consider the num-
ber of words, punctuation marks and non-word
markers (e.g. quotations and brackets) in source
and target, as well as their ratios (source/target and
target/source), and the number of out of vocabulary
terms encountered.
Shallow Syntactic (SSyn) features consider
the number and ratios of common part-of-speech
(POS) tags in source and target. Since the list of
valid POS tags varies for different languages, we
mapped English and Spanish tags into a common
list using the FreeLing tagger (Carreras et al, 2004).
Syntactic (Syn) features consider the number
and ratios of dependency roles common to source
and target. To create a unique list of roles, we used
the DepPattern (Otero and Lopez, 2011) package,
which provides English and Spanish dependency
parsers.
Phrase Table (PT) matching features are cal-
culated as in (Mehdad et al, 2011), with a phrasal
matching algorithm that takes advantage of a lexical
phrase table extracted from a bilingual parallel
corpus. The algorithm determines the number of
phrases in the source (1 to 5-grams, at the level of
173
tokens, lemmas and stems) that can be mapped into
target word sequences, and vice-versa. To build our
English-Spanish phrase table, we used the Europarl,
News Commentary and United Nations Spanish-
English parallel corpora. After tokenization, the
Giza++ (Och and Ney, 2000) and the Moses toolkit
(Koehn et al, 2007) were respectively used to
align the corpora and extract the phrase table.
Although the phrase table was generated using MT
technology, its use to compute our features is still
compatible with a system-independent approach
since the extraction is carried out without tuning the
process towards any particular task. Moreover, our
phrase matching algorithm integrates matches from
overlapping n-grams of different size and nature
(tokens, lemmas and stems) which current MT
decoding algorithms cannot explore for complexity
reasons.
Dependency Relation (DR) matching fea-
tures target the increase of CLTE precision by
adding syntactic constraints to the matching pro-
cess. These features capture similarities between
dependency relations, combining syntactic and
lexical levels. We define a dependency relation
as a triple that connects pairs of words through a
grammatical relation. In a valid match, while the
relation has to be the same, the connected words
can be either the same, or semantically equivalent
terms in the two languages. For example, ?nsubj
(loves, John)? can match ?nsubj (ama, John)?
and ?nsubj (quiere, John)? but not ?dobj (quiere,
John)?. Term matching is carried out by means
of a bilingual dictionary extracted from parallel
corpora during PT creation. Given the dependency
tree representations of source and target produced
with DepPattern, for each grammatical relation r we
calculate two DR matching scores as the number
of matching occurrences of r in both source and
target, respectively normalized by: i) the number of
occurrences of r in the source, and ii) the number of
occurrences of r in the target.
Semantic Phrase Table (SPT) matching features
represent a novel way to leverage the integration of
semantics and MT-derived techniques. Semantically
enhanced phrase tables are used as a recall-oriented
complement to the lexical PT matching features.
SPTs are extracted from the same parallel corpora
used to build lexical PTs, augmented with shallow
semantic labels. To this aim, we first annotate the
corpora with the FreeLing named-entity tagger,
replacing named entities with general semantic
labels chosen from a coarse-grained taxonomy
(person, location, organization, date and numeric
expression). Then, we combine the sequences of
unique labels into one single token of the same
label. Finally, we extract the semantic phrase
table from the augmented corpora in the same way
mentioned above. The resulting SPTs are used to
map phrases between NE-annotated source-target
pairs, similar to PT matching. SPTs offer three
main advantages: i) semantic tags allow to match
tokens that do not occur in the original parallel
corpora used to extract the phrase table, ii) SPT
entries are often short generalizations of longer
original phrases, so the matching process can
benefit from the increased probability of mapping
higher order n-grams (i.e. those providing more
contextual information), and iii) their smaller size
has positive impact on system?s efficiency, due to
the considerable search space reduction.
4 Experiments and results
4.1 Datasets
Datasets with manual evaluation of MT output have
been made available through a number of shared
evaluation tasks. However, most of these datasets
are not specifically annotated for adequacy measure-
ment purposes, and the available adequacy judge-
ments are limited to few hundred sentences for some
language pairs. Moreover, most datasets are created
by comparing reference translations with MT sys-
tems? output, disregarding the input sentences. Such
judgements are hence biased towards the reference.
Furthermore, the inter-annotator agreement is often
low (Callison-Burch et al, 2007). In light of these
limitations, most of the available datasets are per se
not fully suitable for adequacy evaluation methods
based on supervised learning, nor to provide sta-
ble and meaningful results. To partially cope with
these problems, our experiments have been carried
out over two different datasets:
? 16K: 16.000 English-Spanish pairs, with
Spanish translations produced by multiple MT
174
systems, annotated by professional translators
with quality scores in a 4-point scale (Specia et
al., 2010a).
? WMT07: 703 English-Spanish pairs derived
from MT systems? output, with explicit ade-
quacy judgements on a 5-point scale.
The two datasets present complementary advan-
tages and disadvantages. On the one hand, al-
though it is not annotated to explicitly capture
meaning-related aspects of MT output, the quality
oriented dataset has the main advantage of being
large enough for supervised approaches. Moreover,
it should allow to check the effectiveness of our fea-
ture set in estimating adequacy as a latent aspect of
the more general notion of MT output quality. On
the other hand, the smaller dataset is less suitable
for supervised learning, but represents an appropri-
ate benchmark for MT adequacy evaluation.
4.2 Adequacy and quality prediction
To experiment with our CLTE-based evaluation
method minimizing overfitting, we randomized each
dataset 5 times (D1 to D5), and split them into 80%
for training and 20% for testing. Using different
feature sets, we then trained and tested various re-
gression models over each of the five splits, and
computed correlation coefficients between the CLTE
model predictions and the human gold standard an-
notations ([1-4] for quality, and [1-5] for adequacy).
16K quality-based dataset
In Table 1 we compare the Pearson?s correlation
coefficient of our SVM regression models against
the results reported in (Specia et al, 2010b), calcu-
lated with the same three common MT evaluation
metrics with a single reference: BLEU, TER and
Meteor. For the sake of comparison, we also re-
port the average quality correlation (QE) obtained
by (Specia et al, 2010b) over the same dataset.1
The results show that the integration of syntac-
tic and semantic information allows our adequacy-
oriented model to achieve a correlation with hu-
man quality judgements that is always significantly
1We only show the average results reported in (Specia et al,
2010b), since the distributions of the 16K dataset is different
from our randomized distribution.
higher2 than the correlation obtained by the MT
evaluation metrics used for comparison. As ex-
pected a considerable improvement over surface fea-
tures is achieved by the integration of syntactic in-
formation. A further increase, however, is brought
by the complementary contribution of SPT (recall-
oriented, due to the higher coverage of semantics-
aware phrase tables with respect to lexical PTs), and
DR matching features (precision-oriented, due to
the syntactic constraints posed to matching text por-
tions). Although they are meant to capture meaning-
related aspects of MT output, our features allow
to outperform the results obtained by the generic
quality-oriented features used by (Specia et al,
2010b), which do not discriminate between ade-
quacy and fluency.3 When dependency relations and
phrase tables (both lexical and semantics-aware) are
used in combination, our scores also outperform the
average QE score. Finally, looking at the different
random splits of the same dataset (D1 to D5), our
correlation scores remain substantially stable, prov-
ing the robustness of our approach not only for ade-
quacy, but also for quality estimation.
WMT07 adequacy-based dataset
In Table 2 we compare our regression model,
obtained in the same way previously described,
against three commonly used MT evaluation metrics
(Callison-Burch et al, 2007). In this case, the re-
ported results do not show the same consistency over
the 5 randomized datasets (D1 to D5). However, it is
worth pointing out that: i) the small dataset is partic-
ularly challenging to train models with higher corre-
lation with humans, ii) our aim is checking how far
we get using only adequacy-oriented features rather
than outperforming BLEU/TER/Meteor at any cost,
and iii) our results are not far from those achieved
by metrics that rely on reference translations. Com-
pared with Meteor, the correlation is even higher
proving the effectiveness of the proposed method.
2p < 0.05, calculated using the approximate randomization
test implemented in (Pado?, 2006).
3As reported in (Specia et al, 2010b), more than 50% (39
out of 74) of the features used is translation-independent (only
source-derived features).
175
Features D1 D2 D3 D4 D5 AVG
F 0.2506 0.2578 0.2436 0.2527 0.2443 0.25
SSyn+Syn 0.4387 0.4114 0.3994 0.4114 0.3793 0.41
F+SSyn+Syn 0.4215 0.4398 0.4059 0.4464 0.4255 0.428
F+SSyn+Syn+DR 0.4668 0.4602 0.4386 0.4437 0.4454 0.451
F+SSyn+Syn+DR+PT 0.4724 0.4715 0.4852 0.5028 0.4653 0.48
F+SSyn+Syn+DR+PT+SPT 0.4967 0.4802 0.4688 0.4894 0.4887 0.485
BLEU 0.2268
TER 0.1938
METEOR 0.2713
QE (Specia et al, 2010b) 0.4792
Table 1: Pearson?s correlation between SVM regression and human quality annotation over 16K dataset.
Features D1 D2 D3 D4 D5 AVG
F 0.10 0.03 0.04 0.10 0.14 0.083
SSyn+Syn 0.299 0.351 0.1834 0.2962 0.2417 0.274
F+SSyn+Syn 0.2648 0.2870 0.4061 0.3601 0.1327 0.29
F+SSyn+Syn+DR 0.3196 0.4568 0.2860 0.5057 0.4066 0.395
F+SSyn+Syn+DR+PT 0.3254 0.4710 0.3921 0.4599 0.3501 0.40
F+SSyn+Syn+DR+PT+SPT 0.3487 0.4032 0.4803 0.4380 0.3929 0.413
BLEU 0.466
TER 0.437
METEOR 0.357
Table 2: Pearson?s correlation between SVM regression and human adequacy annotation over WMT07.
4.3 Multi-class classification
To further explore the potential of our CLTE-based
MT evaluation method, we trained an SVM multi-
class classifier to predict the exact adequacy and
quality scores assigned by human judges. The eval-
uation was carried out measuring the accuracy of our
models with 10-fold cross validation to minimize
overfitting. As a baseline, we calculated the per-
formance of the Majority Class (MjC) classifier pro-
posed in (Specia et al, 2011), which labels all exam-
ples with the most frequent class among all classes.
The performance improvement over the result ob-
tained by the MjC baseline (?) has been calculated
to assess the contribution of different feature sets.
16K quality-based dataset
The accuracy results reported in Table 3a show
that also in this testing condition, syntactic and se-
mantic features improve over surface form ones. Be-
sides that, we observe a steady improvement over
the MjC baseline (from 5% to 12%). This demon-
strates the effectiveness of our adequacy-based fea-
tures to predict exact quality scores in a 4-point
scale, although this is a more challenging and dif-
ficult task than regression and binary classification.
Such improvement is even more interesting consid-
ering that (Specia et al, 2010b) reported discour-
aging results with multi-class classification to pre-
dict quality scores. Moreover, while they claimed
that removing target-independent features (i.e. those
only looking at the source text) significantly de-
grades their QE performance, we achieved good re-
sults without using any of these features.
WMT07 adequacy-based dataset
As we can observe in Table 3b, all variations
of adequacy estimation models significantly outper-
form the MjC baseline, with improvements rang-
176
Features 10-fold acc. ?
F 42.16% 5.16
Syn+SSyn 46.61% 9.61
F+Syn+SSyn 47.10% 10.10
F+Syn+SSyn+DR 47.26% 10.26
F+Syn+SSyn+DR+PT 48.15% 11.15
F+Syn+SSyn+DR+PT+SPT 48.74% 11.74
MjC 37% -
(a) 16K dataset.
Features 10-fold acc. ?
F 50.07% 14.07
Syn+SSyn 54.19% 18.19
F+Syn+SSyn 54.34% 18.34
F+Syn+SSyn+DR 56.47% 20.47
F+Syn+SSyn+DR+PT 56.61% 20.61
F+Syn+SSyn+DR+PT+SPT 56.75% 20.75
MjC 36% -
(b) WMT07 dataset
Table 3: Multi-class classification accuracy of the quality/adequacy scores.
Features 10-fold acc. ?
F 65.85% 11.85
Syn+SSyn 69.59% 15.59
F+Syn+SSyn 70.89% 16.89
F+Syn+SSyn+DR 71.39% 17.39
F+Syn+SSyn+DR+PT 71.92% 17.92
F+Syn+SSyn+DR+PT+SPT 72.21% 18.21
MjC 54% -
(a) 16k dataset.
Features 10-fold acc. ?
F 83.24% 12.84
Syn+SSyn 83.67% 13.27
F+Syn+SSyn 84.31% 13.91
F+Syn+SSyn+DR 84.86% 14.46
F+Syn+SSyn+DR+PT 84.96% 14.56
F+Syn+SSyn+DR+PT+SPT 85.20% 14.80
MjC 70.4% -
(b) WMT07 dataset.
Table 4: Accuracy of the binary classification into ?good? or ?adequate?, and ?bad? or ?inadequate?.
ing from 14% to 20%. Interestingly, although the
dataset is small and the number of classes is higher
(5-point scale), the improvement and overall results
are better than those obtained on the 16K dataset.
Such result confirms our hypothesis that adequacy-
based features extracted from both source and target
perform better on a dataset explicitly annotated with
adequacy judgements. In addition, the improvement
over the MjC baseline (?) of our best model is much
higher (20%) than the one reported in (Specia et al,
2011) on adequacy estimation (6%). We are aware
that their results are calculated over a dataset for a
different language pair (i.e. English-Arabic) which
brings up more challenges. However, our smaller
dataset (700 vs 2580 pairs) and the higher number
of classes (5 vs 4) compensate to some extent the
difficulty of dealing with English-Arabic pairs.
4.4 Recognizing ?good? vs ?bad? translations
Last but not least, we considered the traditional sce-
nario for quality and confidence estimation, which
is a binary classification of translations into ?good?
and ?bad? or, from the meaning point of view, ?ade-
quate? and ?inadequate?. Adequacy-oriented binary
classification has many potential applications in the
translation industry, ranging from the design of con-
fidence estimation methods that reward meaning-
preserving translations, to the optimization of the
translation workflow. For instance, an ?adequate?
translation can be just post-edited in terms of fluency
by a target language native speaker, without having
any knowledge of the source language. On the other
hand, an ?inadequate? translation should be sent to a
human translator or to another MT system, in order
to reach acceptable adequacy. Effective automatic
binary classification has an evident positive impact
on such workflow.
16K quality-based dataset
We grouped the quality scores in the 4-point scale
into two classes, where scores {1,2} are considered
as ?bad? or ?inadequate?, while {3,4} are taken as
?good? or ?adequate?. We carried out learning and
177
classification using different sets of features with 10-
fold cross validation. We also compared our accu-
racy with the MjC baseline, and calculated the im-
provement of each model (?) against it.
The results reported in Table 4a demonstrate that
the accuracy of our models is always significantly
superior to the MjC baseline. Moreover, also in this
case there is a steady improvement using syntactic
and semantic features over the results obtained by
surface form features. Additionally, it is worth men-
tioning that the best model improvement over the
baseline (?) is much higher (about 18%) than the
improvement reported in (Specia et al, 2010b) over
the same dataset (about 8%), considering the aver-
age score obtained with their data distribution. This
confirms the effectiveness of our CLTE approach
also in classifying ?good? and ?bad? translations.
WMT07 adequacy-based dataset
We mapped the 5-point scale adequacy scores into
two classes, with {1,2,3} judgements assigned to the
?inadequate? class, and {4,5} judgements assigned
to the ?adequate? class. The main motivation for this
distribution was to separate the examples in a way
that adequate translations are substantially accept-
able, while inadequate translations present evident
meaning discrepancies with the source.
The results reported in Table 4b show that the
accuracy of the binary classifiers to distinguish be-
tween ?adequate? and ?inadequate? classes was sig-
nificantly superior (up to about 15%) to the MjC
baseline. We also notice that surface form fea-
tures have a significant contribution to deal with the
adequacy-oriented dataset, while the gain obtained
using syntactic and semantic features (2%) is lower
than the improvement observed in the 16K dataset.
This might be due to the more unbalanced distribu-
tion of the classes which: i) leads to a high baseline,
and ii) together with the small size of the WMT07
dataset, makes supervised learning more challeng-
ing. Finally, the improvement of all models (?) over
the MjC baseline is much higher than the gain re-
ported in (Specia et al, 2011) over their adequacy-
oriented dataset (around 2%).
5 Conclusions
In the effort of integrating semantics into MT tech-
nology, we focused on automatic MT evaluation, in-
vestigating the potential of applying cross-lingual
textual entailment techniques for adequacy assess-
ment. The underlying assumption is that MT output
adequacy can be determined by verifying that an en-
tailment relation holds from the source to the target,
and vice-versa. Within such framework, this paper
makes two main contributions.
First, in contrast with most current metrics based
on the comparison between automatic translations
and multiple references, we avoid the bottleneck
represented by the manual creation of such refer-
ences.
Second, beyond current approaches biased to-
wards fluency or general quality judgements, we
tried to isolate the adequacy dimension of the prob-
lem, exploring the potential of adequacy-oriented
features extracted from the observation of source
and target.
To achieve our objectives, we successfully ex-
tended previous CLTE methods with a variety of lin-
guistically motivated features. Altogether, such fea-
tures led to reliable judgements that show high cor-
relation with human evaluation. Coherent results on
different datasets and classification schemes demon-
strate the effectiveness of the approach and its poten-
tial for different applications.
Future works will address both the improvement
of our adequacy evaluation method and its integra-
tion in SMT for optimization purposes. On one
hand, we plan to explore new features capturing
other semantic dimensions. A possible direction is
to consider topic modelling techniques to measure
the relatedness of source and target. Another inter-
esting direction is to investigate the use of Wikipedia
entity linking tools to support the mapping between
source and target terms. On the other hand, we plan
to explore the integration of our model as an error
criterion in SMT system training.
Acknowledgments
This work has been partially supported by the
CoSyne project (FP7-ICT-4-24853) and T4ME net-
work of excellence (FP7-IST-249119), funded by
the European Commission under the 7th Frame-
work Programme. The authors would like to thank
Hanna Bechara, Antonio Valerio Miceli Barone and
Daniele Pighin for their contributions during the MT
Marathon 2011.
178
References
E. Avramidis, M. Popovic, V. Vilar Torres, and A. Bur-
chardt. 2011. Evaluate with Confidence Estimation:
Machine Ranking of Translation Outputs using Gram-
matical Features. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation (WMT ?11).
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Good-
ness: A Method for Measuring Machine Translation
Confidence. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2011).
S. Banerjee and A. Lavie. 2005. METEOR: An Auto-
matic Metric for MT Evaluation with Improved Corre-
lation with Human Judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Con-
fidence Estimation for Machine Translation. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics (COLING ?04). Association for
Computational Linguistics.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) Evaluation of Machine
Translation. In Proceedings of the Second Workshop
on Statistical Machine Translation (WMT ?07).
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
FreeLing: An Open-Source Suite of Language An-
alyzers. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC?04).
C.C. Chang and C.J. Lin. 2011. LIBSVM: A Library
for Support Vector Machines. ACM Transactions on
Intelligent Systems and Technology (TIST), 2(3).
I. Dagan and O. Glickman. 2004. Probabilistic Textual
Entailment: Generic Applied Modeling of Language
Variability. In Proceedings of the PASCAL Workshop
of Learning Methods for Text Understanding and Min-
ing.
G. Doddington. 2002. Automatic Evaluation of Machine
Translation Quality Using N-gram Co-Occurrence
Statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, HLT ?02.
J. Gime?nez and L. Ma`rquez. 2007. Linguistic Features
for Automatic Evaluation of Heterogenous MT Sys-
tems. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation (StatMT ?07).
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions (ACL 2007).
Y. Mehdad, M. Negri, and M. Federico. 2010. Towards
Cross-Lingual Textual Entailment. In Proceedings of
the 11th Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL HLT 2010).
Y. Mehdad, M. Negri, and M. Federico. 2011. Using
Bilingual Parallel Corpora for Cross-Lingual Textual
Entailment. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies (ACL HLT 2011).
Y. Mehdad, M. Negri, and M. Federico. 2012. Detect-
ing Semantic Equivalence and Information Disparity
in Cross-lingual Documents. In Proceedings of the
ACL?12.
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2000).
P.G. Otero and I.G. Lopez. 2011. A Grammatical For-
malism Based on Patterns of Part-of-Speech Tags. In-
ternational journal of corpus linguistics, 16(1).
S. Pado?, M. Galley, D. Jurafsky, and C. D. Manning.
2009. Textual Entailment Features for Machine Trans-
lation Evaluation. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation (StatMT ?09).
S. Pado?, 2006. User?s guide to sigf: Significance test-
ing by approximate randomisation.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation (ACL 2002. In Proceedings of the
40th annual meeting on association for computational
linguistics.
C.B. Quirk. 2004. Training a Sentence-Level Machine
Translation Confidence Measure. In Proceedings of
LREC 2004.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings of
Association for Machine Translation in the Americas
(AMTA 2006).
L. Specia and A. Farzindar. 2010. Estimating Machine
Translation Post-Editing Effort with HTER. In Pro-
ceedings of the AMTA-2010 Workshop, Bringing MT
to the User: MT Research and the Translation Indus-
try.
L. Specia, N. Cancedda, and M. Dymetman. 2010a.
A Dataset for Assessing Machine Translation Eval-
uation Metrics. In Proceedings of the 7th interna-
tional conference on Language Resources and Eval-
uation (LREC10).
179
L. Specia, D. Raj, and M. Turchi. 2010b. Machine Trans-
lation Evaluation Versus Quality Estimation. Machine
translation, 24(1).
L. Specia, N. Hajlaoui, C. Hallett, and W. Aziz. 2011.
Predicting Machine Translation Adequacy. In Pro-
ceedings of the 13th Machine Translation Summit (MT-
Summit 2011).
L. Specia. 2011. Exploiting Objective Annotations for
Minimising Translation Post-editing Effort. In Pro-
ceedings of the 15th Conference of the European As-
sociation for Machine Translation (EAMT 2011).
D. Xiong, M. Zhang, and H. Li. 2010. Error Detection
for Statistical Machine Translation Using Linguistic
Features. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics (ACL
2010). Association for Computational Linguistics.
180
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 433?441,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Evaluating the Learning Curve of Domain Adaptive
Statistical Machine Translation Systems
Nicola Bertoldi Mauro Cettolo Marcello Federico
Fondazione Bruno Kessler
via Sommarive 18
38123 Trento, Italy
<surename>@fbk.eu
Christian Buck
University of Edinburgh
10 Crichton Street
EH8 9AB Edinburgh, UK
christian.buck@ed.ac.uk
Abstract
The new frontier of computer assisted transla-
tion technology is the effective integration of
statistical MT within the translation workflow.
In this respect, the SMT ability of incremen-
tally learning from the translations produced
by users plays a central role. A still open
problem is the evaluation of SMT systems that
evolve over time. In this paper, we propose
a new metric for assessing the quality of an
adaptive MT component that is derived from
the theory of learning curves: the percentage
slope.
1 Introduction
Translation memories and computer assisted trans-
lation (CAT) tools are currently the dominant tech-
nologies in the translation and localization market,
but recent achievements in statistical MT have raised
new expectations in the translation industry. So far,
statistical MT has focused on providing ready-to-use
translations, rather than outputs that minimize the
effort of a human translator. The MateCAT project1
aims at pushing what can be considered the new
frontier of CAT technology: how to effectively inte-
grate statistical MT within the translation workflow.
One pursued research direction is developing do-
main adaptive SMT models, i.e. models that dynam-
ically adapt to the translations that are continuously
added to the translation memory by the user dur-
ing her/his work. The ideal goal is to progressively
reduce the mismatch between training and testing
1http://www.matecat.com/
data, in such a way that the adapted SMT engine will
be able to provide the user with useful suggestions
? i.e. perfect or worth being post-edited ? when the
translation memory fails to retrieve perfect or almost
perfect matches. Among the well known machine
learning paradigms that fit with this scenario are on-
line learning and incremental learning, which basi-
cally differ in the amount of data that is employed
to dynamically adapt the system: a single piece of
data in the first case and a batch of data in the lat-
ter. Notice that in both cases one assumes that do-
main adaptation is performed efficiently, i.e. by only
processing the newly received data. Moreover, al-
though the quantity of acquired in-domain data is
generally limited, their high quality and relevance to
the translation task justify their exploitation by all
means possible.
Domain adaptive SMT embeds two challenges:
(1) the design of effective adaptation algorithms, and
(2) the evaluation of MT systems evolving over time.
Since the ultimate goal of our efforts is to increase
the productivity of human translators, the most ac-
curate assessment methodology would be of course
to run a field test. This way, we could compare pro-
ductivity of human translators receiving suggestions
from an MT engine featuring dynamic domain adap-
tation against the productivity of human translators
working with a static MT engine. As this evaluation
is infeasible during daily MT development, we can
resort to the several automatic MT metrics, which
however, as we will see later, are unsuitable to track
the dynamic behaviors we are interested to inves-
tigate. Metrics for measuring performance in the
case of interactive MT, see for example (Khadivi,
433
2008), like Key-Stroke Ratio (KSR), Mouse-Action
Ratio (MAR), Key-Stroke and Mouse-Action Ratio
(KSMR) are known to correlate well with the pro-
ductivity of human translators, but their computation
requires the actual use of an interactive MT system,
i.e. a field test.
In the SMART project,2 the evaluation of adap-
tive interactive MT is explored (Cesa-Bianchi et al,
2008). While no specific metric is proposed, the
analysis is based on a plot of cumulative differences
of BLEU scores between a baseline and an adaptive
system. These differences are computed sentence by
sentence and present an interesting view of the dy-
namic change of the MT system. We are going to
further elaborate on this idea.
Other metrics like Character Error Rate (CER)
and Translation Edit Rate (TER) would accurately
predict the translators? productivity if references
were generated by using the CAT system; on the
contrary, references are usually, as in this paper, gen-
erated from scratch based only on the source text
and can thus be quite far from CAT-based transla-
tions, both lexically and syntactically. The Human-
targeted variant of TER, HTER (Snover et al, 2006),
needs human intervention and is therefore unfit to
meet our requirements.
The main goal of this paper is to design an objec-
tive automatic evaluation methodology for an MT
system adapting over time. We propose to use the
percentage slope from the theory on learning curves
to measure the learning ability of adaptive MT sys-
tems.
To assess the proposed metric, we have imple-
mented a simple but effective adaptation strategy
suitable for an MT system integrated in a CAT tool.
We show that the percentage slope is able to expose
different dynamic behaviors, such as learning, no
learning, and forgetting.
2 Dynamic Adaptation Framework
In the MateCAT project scenario, the MT system,
which is embedded in the CAT tool to increase the
translators? productivity, adapts over time by ex-
ploiting translations generated by the user. The
adapted system is then used to provide the user
with translation suggestions for the next sentences.
2http://www.smart-project.eu
We refer to this process as dynamic (or incremen-
tal) adaptation to emphasize that adaptation hap-
pens continuously based on a stream of data.
2.1 Abstract View of the Adaptation Process
From an abstract point of view, the framework of in-
cremental adaptation can be summarized as follows:
i) before the process starts, an initial system is
built on available data including a parallel cor-
pus;
ii) a stream of parallel data becomes available that
is split into blocks of (not necessarily) similar
size;
iii) the first/next block is considered, but only the
source is available yet;
iv) the latest instance of the adapting system trans-
lates the source text of the current block;
v) the target part of the current block becomes
available for use;3
vi) the system is adapted using the current parallel
block and possibly all the previous ones;
vii) the loop continues from step iii) until all blocks
are processed.
In each adaptation step, all of the data available
so far can be used, but no look ahead is possible.
Note that, in principle, each block is translated with
a different instance of the adapting system; hence,
the same text occurring in two different blocks can
be translated differently.
2.2 Evaluation Goals and Requirements
Although dynamic adaptation is closely related to
static domain adaptation (Foster and Kuhn, 2007),
in this scenario we are not interested in the quality
of the final model. In fact, this model is only avail-
able once the stream is depleted and therefore is not
used anymore.
What we are interested in, and what we want to
compare among different approaches, is the systems
evolvement over time.
Consider a translator who uses such an incremen-
tally adapting system and performs post-editing on
its suggested translations. The highest productivity
3In the CAT framework, the target part of a block is the
translation post-edited by the user.
434
gain is achieved when the adaptation is quick and
persistent.
Even though in this paper we are concerned with
an automatic metric, it is important to keep the use
case of CAT in mind, in particular the presence of
a human translator. The TransType2 project4 has
found that repeated correction of the same error is
strongly disliked by editors (Macklovitch, 2006) and
may lead to rejection of the entire system. Similarly,
segments that were translated correctly by previous,
less adapted systems, should not be negatively af-
fected by updates. We will refer to these particular
aspects of adaptation as backward reliability.
Automatic measures, which are aimed at static
MT modules, can not take the evolution of the sys-
tem into account and are therefore unable to pinpoint
such problems. Thus, they are not suitable for the
dynamic adaptation scenario.
A new evaluation methodology should satisfy the
following requirements:
? ability to compare different strategies
? show behavior over time and reward early im-
provements and consistent adaptation
? expose possible overfitting, i.e. check whether
generalization is lost due to overly aggressive
adaptation
? strong correlation to human productivity
? estimate benefit over a static baseline model
without adaptation
? check backward reliability.
2.3 Evaluation Protocol
The performance of adaptive systems as sketched
in Section 2.1 is evaluated on different parts of the
stream as opposed to the global evaluation used for
static systems. We distinguish between two proto-
cols which differ in their use of historic data.
For block-wise evaluation only the translations of
the most recent block are evaluated with respect to
the correct translations once these become available.
Any static automatic MT score, e.g. TER (Snover
et al, 2006), BLEU (Papineni et al, 2001), can be
used, provided that it is reliable on a block of usually
relatively small size.
In contrast, in incremental evaluation the scores
are computed on all blocks available so far. The
4http://tt2.atosorigin.es
translations of previous blocks are kept fixed, i.e.
blocks are not translated again once a newly adapted
system becomes available as this new system has al-
ready seen this data.
Both the block-wise and incremental protocols
yield a sequence of scores that reflects the adaptation
behavior over time. The former is useful to expose
potential weaknesses as discussed above: we expect
to see improvement at first and after a while, when
enough adaptation data is available, a level curve. If
this is not the case, this indicates a problem:
i) should the scores deteriorate over time we
might be facing overfitting, possibly due to un-
expected heterogeneity in our corpus;
ii) if the scores continue to improve, then the adap-
tation method is not aggressive enough and the
system underfits.
The incremental evaluation on the other hand allows
for easy comparison of different adaptation strate-
gies. While the performance on the most recent
block becomes less important over time, the perfor-
mance on all the blocks processed so far nicely re-
flects the utility of the system in the application set-
ting.
The metric we are going to propose in the next
section processes such sequences of partial scores.
It accumulates the trend into a single number and
offers an interpretation that relates adaptive behavior
to productivity gains.
3 The Percentage Slope
Learning curves (see (Stump P.E., 2002) for a de-
tailed introduction) are mathematical models used
to estimate the efficiency gain when an activity is
repeated. The learning effect was noted in indus-
trial environment: the underlying notion is that when
people repeat an activity, there tends to be a gain in
efficiency. That is exactly the expected behavior of
our dynamically adapting MT system: it should im-
prove its performance on texts including terms and
expressions whose proper translation has been pre-
viously provided. Thus we decided to exploit ele-
ments from learning theory to measure the evolution
of translation capability.
Several learning curve models have been pro-
posed, but only two are in widespread use, the unit
435
(U) model due to Crawford and the cumulative av-
erage (CA) model due to Wright. Both models are
based on a common mathematical form:
y = axb (1)
where:
a represents the theoretical labor hours required
to build the first unit produced (a positive num-
ber)
b represents the rate of learning (negative value,
except for ?forgetting?)
x represents the number of an item in the produc-
tion sequence (unit #1,#2,#3, . . .)
The models differ in the interpretation of y:
U: y is the labor hours required to build unit #x
CA: y is the average labor hours per unit required
to build the first x units
Since b is a mathematically appropriate but
counter-intuitive number for describing the slope,
the percentage slope S is typically used:
S = 10b log10(2)+2 (2)
S provides the rate of learning on a scale of 0 to 100,
as a percentage. A 100% slope represents no learn-
ing at all, zero percentage reflects a theoretically in-
finite rate of learning. In practice, human operations
hardly ever achieve a rate of learning faster than 70%
as measured on this scale.
The correspondence between our block-wise eval-
uation (Section 2.3) with the U model, and the incre-
mental evaluation with the CA model is straightfor-
ward. In the first case, y is the number of errors
done in the translation of the block #x; in the sec-
ond case, y is the average number of errors (that is
the TER score or the 100-BLEU score) made on the
first x blocks.
From a practical point of view, the sequence of
scores can be provided while the adapting system is
being used; the learning curve which best matches
the sequence is then found5 and eventually the per-
centage slope S is computed.
5Notice that the best fitting learning curve can be estimated
in the log scale with a simple linear regression analysis.
set #sent. #src words #tgt words
train 1.2M 18.9M 19.4M
test 3.4k 57.0k 61.4k
Table 1: Overall statistics on parallel data of the IT
domain used for training and testing the SMT system.
Counts of (English) source words and (Italian) target
words refer to tokenized texts.
4 Experiments
In order to test-drive the evaluation metric intro-
duced in Section 3, several SMT systems showing
effective, weak, poor or absent adaptation capabil-
ity have been developed. Moreover, a preliminary
investigation on backward reliability has been car-
ried out. The next paragraphs detail and discuss the
experiments performed.
4.1 Data
The task considered in this work involves the trans-
lation from English into Italian of documents in the
Information Technology (IT) domain.
The training set consists of a large Translation
Memory in the IT domain and several OPUS6 sub-
corpora, namely KDE4, KDEdoc and PHP. The test
set includes the human generated translation of 6
documents, disjoint from the training set. Although
in the same domain, the test set is quite different
from the training data as shown by comparing val-
ues of perplexity (650 vs. 40) and OOV rate (2.4%
vs. 0.4%) computed on the source side.7 Further-
more, the 6 documents significantly differ among
each other: perplexity and OOV rate range from 465
to 880 and from 0.8 to 3.3, respectively. Table 1 col-
lects overall statistics on training and test sets.
To simulate the stream of fresh data, the IT test
set has been split into blocks of about a thousand8
words each. Before splitting, sentences have been
scrambled, with the rationale of generating a large
number of homogeneous blocks, simulating a test
set consisting of a single document.
6http://opus.lingfil.uu.se
7Figures for the training data were measured through a
cross-validation technique.
8Different sizes have been also considered (three and five
thousands) to test different adaptation rates, but results were
qualitatively similar to those on shorter blocks and then are not
reported.
436
4.2 Baseline System
The SMT baseline system is built upon the open-
source MT toolkit Moses9 (Koehn et al, 2007).
The translation and the lexicalized reordering mod-
els are estimated on parallel training data with the
default setting; a 5-gram LM smoothed through the
improved Kneser-Ney technique (Chen and Good-
man, 1999) is estimated on monolingual texts via
the IRSTLM toolkit (Federico et al, 2008). Here-
inafter, these models are referred to as background
(BG) models. The log-linear interpolation weights
are optimized by means of the standard MERT pro-
cedure provided within the Moses toolkit.
4.3 Adaptive System
The adapting SMT system is built on Moses as well.
Besides the BG models of the baseline system, trans-
lation, reordering and language models estimated on
the stream of fresh data are employed as additional
features. Hereinafter, these models are referred to
as foreground (FG) models. Unless differently spec-
ified, the FG models employed to translate a given
block are trained on all preceding blocks. Note that
the first instance of the adapting system (i.e. that
translating the first block) is exactly the baseline sys-
tem, because no adaptation data is available to train
FG models yet. FG translation and reordering mod-
els are trained in the same way as the BG models.
Due to the limited amount of adaptation data, the FG
LM is a 3-gram LM smoothed through the more ro-
bust Witten-Bell technique (Witten and Bell, 1991).
The interpolation weights are inherited from a
companion system trained and tuned on a different
domain ? official documents of the European Union
organization ? and are kept fixed.
4.4 Experiments on Adaptive SMT
First of all, the baseline and adapting systems were
run on the scrambled test set and compared at both
block-wise and incremental mode (see Section 2.3).
Figure 1 plots block-wise TER and BLEU scores
of the baseline and adapting systems as functions of
the amount (number of words) of adaptation data.
On one hand, it can be guessed that the adapting
system performs gradually better and better than the
baseline; on the other hand, it is evident that such
9http://www.statmt.org/moses
plots are not the most effective way to show the evo-
lution of the adapting system. In fact, the transla-
tion difficulty of contiguous blocks can differ a lot.
Hence, scores computed on them are not comparable
and the corresponding curves are jagged.
The block-wise differences of TER and BLEU
scores between the adapting and the baseline sys-
tems are plotted in Figure 2: the plots are now
cleaner and more readable and vaguely suggest a
positive trend, but still remain too jagged and do not
provide any information about the absolute perfor-
mance of the systems.
Figure 3 plots the incremental TER and BLEU
scores of the baseline and adapting systems as func-
tions of the amount of adaptation data. First of all,
it is worth noting that the right-most values are the
scores computed on the whole test set. In standard
evaluation, those would be the only scores provided
to show how the adapting system outperforms the
baseline system; in particular, the relative improve-
ment is larger for TER (9.3%) than for BLEU (3.9%)
supposedly because tuning was performed to opti-
mize BLEU score which thus is harder to improve.
However, the overall scores obscure the way they
are reached, that is the evolution over time of the
systems, which is especially important for adaptive
systems.
Secondly, the incremental evaluation yields much
smoother plots clearly showing that after initial fluc-
tuations: (i) performance of the baseline stabilizes
around an average which does not change over time;
(ii) scores of the adapting system tend to get increas-
ingly better as more adaptation data is available for
updating FG models.
The evaluation metric we are proposing, the per-
centage slope introduced in Section 3, is indeed able
to spot such kind of paradigmatic behaviors as we
will see in the next section. But before going on
with the assessment of the metric, some further com-
ments on Figure 3:
? in early stages, the adaptation is not effective,
likely because of the scarcity of data. This
raises two issues: design of more effective
adaptation strategies and, in the CAT frame-
work, identifying the appropriate time to re-
place the baseline with the adapting system;
? the adaptive system outperforms the baseline in
437
 40
 45
 50
 55
 60
 65
 0  10000  20000  30000  40000  50000  60000
T
E
R
 
(
%
)
# Number of Words
adabsln
 16
 18
 20
 22
 24
 26
 28
 30
 32
 34
 36
 0  10000  20000  30000  40000  50000  60000
B
L
E
U
 
(
%
)
# Number of Words
adabsln
Figure 1: Block-wise TER (on the left) and BLEU (right) scores of the baseline and the dynamically adapting systems.
-12
-10
-8
-6
-4
-2
 0
 2
 4
 0  10000  20000  30000  40000  50000  60000
T
E
R
 
(
%
)
# Number of Words
ada -- bsln
-8
-6
-4
-2
 0
 2
 4
 6
 8
 0  10000  20000  30000  40000  50000  60000
B
L
E
U
 
(
%
)
# Number of Words
ada -- bsln
Figure 2: Block-wise TER (left) and BLEU (right) differences between the baseline and the dynamically adapting
systems.
terms of TER very soon, while the overtaking
with regard to BLEU is observed much later.
This is because the baseline SMT system was
tuned with respect to the BLEU score on in-
domain data, differently to the adapting system.
Both these issues are out of the scope of this paper
and will be subject of future investigations.
4.5 Assessment of the Percentage Slope
To assess its effectiveness, the percentage slope has
been computed on errors committed by the baseline
system, the adapting system and an adapting system
featuring only FG models (that is without BG mod-
els). The FG-only system was used to translate each
block either fairly and unfairly: the former mode fits
the adaptation process sketched in Section 2.1; in the
latter mode, the FG model is adapted on the block
before its translation starts.
Figure 4 shows the TER and BLEU scores of such
systems in the incremental evaluation. The four dif-
ferent behaviors are expected to correspond to dif-
ferent percentage slopes. In fact, the S values col-
lected in Table 2 confirm the expectations:
? the baseline, completely unable to learn, has in
fact an S of 100%
? the adapting system, that learns through a dy-
namic adaptation of FG models and generalizes
thanks to BG models, has an S of 96-98%
? the FG-only adapting system tested in unfair
mode worsens its performance as the models
become larger, i.e. less focused on the block to
be translated: this is evidenced by an S greater
than 100%
438
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 0  10000  20000  30000  40000  50000  60000
T
E
R
 
(
%
)
# Number of Words
adabsln
 21
 21.5
 22
 22.5
 23
 23.5
 24
 24.5
 25
 25.5
 0  10000  20000  30000  40000  50000  60000
B
L
E
U
 
(
%
)
# Number of Words
adabsln
Figure 3: Incremental TER (left) and BLEU (right) scores of the baseline and the dynamically adapting systems.
model
system
baseline adapting
FG-only adapting
fair unfair
U 100.4 96.9 96.2 107.2
CA 100.3 97.7 96.5 107.4
Table 2: S values of 4 SMT systems (see text) for
the block-wise TER evaluation, corresponding to the U
model, and the incremental evaluation, corresponding to
the CA model.
? the FG-only adapting system tested in fair
mode increases its performance as the models
become larger, i.e. more general, as evidenced
by an S similar to that of our original adapting
system (96%).
Therefore, we can state that S exposes common
behaviors of evolving SMT systems; however, stan-
dard metrics like TER and BLEU are still in charge
of providing absolute performance measures.
In order to give a hint for properly interpret-
ing the values reported, we summarize the discus-
sion in (Stump P.E., 2002) about ?typical learning
slopes?. Operations that are fully automated tend
to have slopes of 100%, 70% if entirely manual, an
intermediate value if mixed. In real industrial envi-
ronments, the average slope depends on the type of
manufacturing activity: for example, in aircraft in-
dustry it is about 85%, it ranges in 90-95% in elec-
tronics and in machining. Hence, a 96-98% slope
as we measured in our experiments must be con-
sidered a significant learning ability of a fully au-
tomated system.
4.6 Experiments on Backward Reliability
A proper assessment of the backward reliability of
an evolving system as defined in Section 2.2 would
require the identification of patterns translated dif-
ferently by the system during its life. We will inves-
tigate this issue in the future. For the moment, we
try to attack the problem from a global point of view:
we simply check that the adaptive system does ?re-
member? its previous translation capabilities ?on av-
erage?, while it learns to better translate novel texts.
To this end, a cross-validation policy was fol-
lowed: the first two thirds of each test set document
are used for dynamically training the FG models,
while the remaining portions are used as held-out
test sets.
Figure 5 reports the TER and BLEU scores on
the 6 test sets of three systems: the baseline sys-
tem (bsln), the adapting system (ada) fed by in-
crementally merging the available reduced adapta-
tion sets, and the system adapted on all adaptation
data sets (final).
The final system achieves performance close
to ada system on each held-out set; this reveals that
our adaptation process is effective both in learning
and in remembering.
We think that the monitoring of the backward re-
liability of adapting systems is a good practice. A
cross validation scheme like ours allows not only to
reveal the backward reliability as shown before, but
also to discover the forgetting trend of, for example,
an MT system featuring an overly aggressive learn-
439
 20
 25
 30
 35
 40
 45
 50
 55
 60
 65
 70
 0  10000  20000  30000  40000  50000  60000
T
E
R
 
(
%
)
# Number of Words
bslnadaFGonly fair adaFGonly unfair ada
 10
 20
 30
 40
 50
 60
 70
 0  10000  20000  30000  40000  50000  60000
B
L
E
U
 
(
%
)
# Number of Words
bslnadaFGonly fair adaFGonly unfair ada
Figure 4: Incremental TER (left) and BLEU (right) of 4 systems showing different learning slopes.
ing method. On the other hand, it only provides cues
about the average behavior and it is not as quickly
informative as a single score could be. Hence, the
design of a proper metric for measuring the back-
ward reliability of MT systems is a challenging task
that should be faced by the research community.
5 Summary and Future Work
The evaluation of a dynamically adapting system is
an open issue. Metrics used in interactive MT such
as HTER or field tests, are infeasible in the daily de-
velopment as they involve human translators/judges.
On the other hand, standard MT evaluation met-
rics either do not expose changes over time (BLEU,
TER) or cannot be applied (CER).
The main contribution of this paper is to propose
the use of the percentage slope for the evaluation of
adapting MT systems, a metric borrowed from the
theory on learning curves. For assessing its effec-
tiveness, we have developed a simple but effective
adapting SMT system suitable to work in the context
of a CAT tool supported by MT. We have compared
several ways to plot the change in error rate over
time for different systems and identified the most
suitable for computing the percentage slope. Finally,
we have shown that the percentage slope well ex-
poses the paradigmatic behaviors of evolving SMT
systems.
The MateCAT project has scheduled field tests
for the near future which will allow for inclusion
of human productivity in the assessment of the per-
centage slope. Moreover, efforts will be devoted to
the design of adaptation techniques which are more
sophisticated than the simple approach used in this
work.
We have also identified the issue of backward re-
liability of an adapting system, that is the ability to
learn without forgetting the past, and the importance
of monitoring it. A best practice based on a cross
validation scheme has been proposed. Future inves-
tigations will concern finding an effective metric to
measure backward reliability.
Acknowledgments
This work was supported by the MateCAT project,
which is funded by the EC under the 7th Framework
Programme.
References
N. Cesa-Bianchi, G. Reverberi, and S. Szedmak. 2008.
Online learning algorithms for computer-assisted
translation. Deliverable 4.2, SMART project (FP6).
http://www.smart-project.eu/files/D4
2.pdf.
S. F. Chen and J. Goodman. 1999. An empirical study of
smoothing techniques for language modeling. Com-
puter Speech and Language, 4(13):359?393.
M. Federico, N. Bertoldi, and M. Cettolo. 2008.
IRSTLM: an Open Source Toolkit for Handling Large
Scale Language Models. In Proc. of Interspeech, pp.
1618?1621, Melbourne, Australia.
G. Foster and R. Kuhn. 2007. Mixture-Model Adapta-
tion for SMT. In Proc. of WMT, pp. 128?135, Prague,
Czech Republic.
S. Khadivi. 2008. Statistical Computer-Assisted Trans-
lation. Ph.D. thesis, RWTH Aachen University,
440
 45
 50
 55
 60
 65
 70
 75
 1  2  3  4  5  6
T
E
R
 
(
%
)
Document
bslnadafinal
 18
 20
 22
 24
 26
 28
 30
 32
 34
 36
 1  2  3  4  5  6
B
L
E
U
 
(
%
)
Document
bslnadafinal
Figure 5: TER (left) and BLEU (right) scores of the baseline system, the evolving system and the final adapted system
on the document-specific held-out test sets.
Aachen, Germany. Advisors: Hermann Ney and En-
rique Vidal.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc. of
ACL: Demo and Poster Sessions, pp. 177?180, Prague,
Czech Republic.
E. Macklovitch. 2006. Transtype2: The last word. In
Proc. of LREC 2006, Genoa, Italy.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Research Report RC22176, IBM Research
Division, Thomas J. Watson Research Center.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J.
Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. of AMTA, Boston,
US-MA.
E. Stump P.E. 2002. All about learning curves. In Proc.
of SCEA. http://www.galorath.com/im
ages/uploads/LearningCurves1.pdf.
I. H. Witten and T. C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Trans. Inform.
Theory, IT-37(4):1085?1094.
441
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 240?251,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Coping with the Subjectivity of Human Judgements
in MT Quality Estimation
Marco Turchi Matteo Negri Marcello Federico
Fondazione Bruno Kessler, FBK-irst
Trento , Italy
{turchi|negri|federico}@fbk.eu
Abstract
Supervised approaches to NLP tasks rely
on high-quality data annotations, which
typically result from expensive manual la-
belling procedures. For some tasks, how-
ever, the subjectivity of human judgements
might reduce the usefulness of the an-
notation for real-world applications. In
Machine Translation (MT) Quality Esti-
mation (QE), for instance, using human-
annotated data to train a binary classifier
that discriminates between good (useful
for a post-editor) and bad translations is
not trivial. Focusing on this binary task,
we show that subjective human judge-
ments can be effectively replaced with an
automatic annotation procedure. To this
aim, we compare binary classifiers trained
on different data: the human-annotated
dataset from the 7th Workshop on Statis-
tical Machine Translation (WMT-12), and
an automatically labelled version of the
same corpus. Our results show that human
labels are less suitable for the task.
1 Introduction
With the steady progress in the field of Statistical
Machine Translation (SMT), the translation indus-
try is now faced with the possibility of significant
productivity increases (i.e. amount of publishable
output per unit of time). One way to achieve this
goal, in Computer Assisted Translation (CAT) en-
vironments, is the integration of (precise, but of-
ten partial) suggestions obtained through ?fuzzy
matches? from a Translation Memory (TM), with
(complete, but potentially less precise) translations
produced by an MT system. Such integration can
loosely consist in presenting translators with un-
ranked suggestions obtained from the MT and the
TM, or rely on tighter combination strategies. For
instance, MT and TM translations can be automat-
ically ranked to ease the selection of the most suit-
able one for post-editing (He et al, 2010), or the
TM can be used to constrain and improve MT sug-
gestions (Ma et al, 2011). In all cases, the ef-
fectiveness of the integration is conditioned by:
i) the quality of MT, and ii) the accuracy in au-
tomatically predicting such quality. Higher pro-
ductivity increases depend on the capability of the
MT system to output useful material that is close
to be publishable ?as is? (Denkowski and Lavie,
2012), and the capability to automatically identify
and present to human translators only such sug-
gestions.
Recognizing good translations falls in the scope
of research on automatic MT Quality Estimation
(QE), which addresses the problem of estimating
the quality of a translated sentence at run-time,
without access to reference translations (Specia et
al., 2009; Soricut and Echihabi, 2010; Bach et al,
2011; Specia, 2011; Mehdad et al, 2012b). In
recent years QE gained increasing interest in the
MT community, resulting in several datasets avail-
able for training and evaluation (Callison-Burch et
al., 2012), the definition of features showing good
correlation with human judgements (Soricut et al,
2012), and the release of open-source software.1
The proposed solutions to the QE problem rely
on supervised methods that strongly depend on the
availability of labelled data. While early works
(Blatz et al, 2003) exploited annotations obtained
with automatic MT evaluation metrics like BLEU
(Papineni et al, 2002), the current trend is to
rely on human annotations, which seem to lead
to more accurate models (Quirk, 2004; Specia et
al., 2009). Along this direction, the QE task con-
sists in predicting scores that reflect human quality
judgements, by learning from manually annotated
datasets (e.g. collections of source-target pairs la-
1http://www.quest.dcs.shef.ac.uk/
240
belled according to an n-point Likert scale or with
real numbers in a given interval). Within this dom-
inant supervised framework, we explore different
ways to obtain labelled data for training a bi-
nary QE classifier suitable for integration in a
CAT tool. Since, to the best of our knowledge,
labelled data with binary judgements are currently
not available, we consider two alternative options.
The first option is to adapt an existing dataset,
checking whether it can be partitioned in a way
that reflects the distinction between good (use-
ful for the translator, suitable for post editing)
and bad translations (that need complete rewrit-
ing).2 To this aim we experiment with the QE
data released within the 7th Workshop on Ma-
chine Translation (WMT-12). The corpus con-
sists of source-target pairs annotated with manual
QE labels (1-5 scores) indicating the post-editing
needed to correct the translations. Besides explicit
human judgements, the availability of post-edited
translations makes also possible to calculate the
actual HTER values (Snover et al, 2009), indicat-
ing the minimum edit distance between the ma-
chine translation and its manually post-edited ver-
sion in the [0,1] interval.
The second option is to automatically re-
annotate the same dataset, trying to produce labels
that reflect an objective and more reliable binary
distinction based on empirical observations.
Our analysis aims to answer the following ques-
tions:
1. Are human labels reliable and coherent
enough to train accurate binary models?
2. Are arbitrarily-set thresholds useful to parti-
tion QE data for this task?
3. Is it possible to obtain reliable binary annota-
tions from an automatic procedure?
Negative answers to the first two questions would
respectively call into question: i) the intuitive idea
that human labels are the most reliable for a super-
vised approach to binary QE, and ii) the possibility
that thresholds on a single metric (e.g. the HTER)
can be set to capture the subtle differences separat-
ing useful from useless translations. A positive an-
swer to the third question would open to the possi-
bility to create training datasets in a more coherent
2In the remainder of the paper we will consider as ?good?
translations those for which post-editing requires a smaller
effort than translation from scratch. Conversely, we will label
as ?bad? the translations that need complete rewriting.
and replicable way compared to current data anno-
tation methods. By answering these questions, this
paper provides the following main contributions:
? We show that training a binary classifier on
arbitrary partitions of an existing dataset is
difficult. Our experiments with the WMT-
12 corpus demonstrate that neither following
standard indications (e.g. ?if more than 70%
of the MT output needs to be edited, a trans-
lation from scratch is necessary?)3, nor con-
sidering arbitrary HTER thresholds, it is pos-
sible to obtain accurate binary classifiers suit-
able for integration in a CAT environment;
? We propose a replicable automatic (hence
non subjective) method to re-annotate an ex-
isting dataset in a way that the resulting bi-
nary classifier outperforms those trained with
human labels.
? We show that, with our method, a smaller
amount of training data is sufficient to ob-
tain similar or better performance compared
to that of the human-annotated dataset used
for comparison.
2 Binary QE for CAT environments
QE has been mainly addressed as a classification
or regression task, where a quality score (respec-
tively an integer or a real value) has to be automat-
ically assigned to MT output sentences given their
source (Specia et al, 2010). Casting the problem
in this way, the integration of a QE component
in a CAT environment makes possible to present
translators with estimates of the expected quality
of each MT suggestion. Such intuitive solution,
however, disregards the fact that even precise QE
scores would not alleviate translators from the ef-
fort of reading useless MT output (or at least the
associated score).
A more effective alternative is to use the esti-
mated QE scores to filter out poor MT suggestions,
presenting only those worth for post-editing. Bi-
nary classification, however, has to confront with
the problem of setting reasonable cut-off criteria.
The arbitrary thresholds, used in several previous
works (Quirk, 2004; Specia et al, 2010; Specia
et al, 2011) are in fact hard to justify, and even
harder to learn from human-labelled training data.
3This was a guideline for the professional trans-
lators involved in the annotation of a previous ver-
sion of the dataset used for the WMT-12 evalua-
tion (see http://www.statmt.org/wmt12/
quality-estimation-task.html).
241
On one side, for instance, there is no evi-
dence that the 70% HTER threshold used in some
datasets yields the optimal separation between ac-
ceptable and totally useless suggestions. Such ar-
bitrary criterion, based on the raw count of post-
editing operations, is likely to reflect a partial view
on a complex problem, disregarding important as-
pects such as the distribution of the corrections in
the MT output. However, in some cases, having
the first 30% of words correctly translated might
take less post-editing effort than having 50% of
correctly translated terms scattered throughout the
whole sentence. In these cases, a 70% HTER
threshold would wrongly consider useless trans-
lations as positive instances and vice-versa.
On the other side, when arbitrary thresholds are
used as annotation guidelines (Callison-Burch et
al., 2012), the moderate agreement between hu-
man judges might make manual labels ill-suited to
learn accurate models.
Under the constraints posed by a CAT envi-
ronment, where only useful suggestions can lead
to a significant productivity increase, the ideal
model should maximize the number of true posi-
tives (useful translations recognized as good) min-
imizing, at the same time, the number of false pos-
itives (useless translations recognized as good). To
this aim, the more the training data are partitioned
according to objective criteria, the higher the ex-
pected reliability of the corresponding cut-off and,
in turn, the higher the expected performance of the
binary classifier.
Focusing on these issues, the following sections
discuss various methods to obtain training data for
binary QE geared to the integration in a CAT en-
vironment. Partitions based on human judgements
from the WMT-12 dataset will be compared with
an automatic method to re-annotate the same cor-
pus. The suitability of the resulting training sets
for binary classification will be assessed by mea-
suring the performance of classifiers built from
each training set. Metrics sensitive to the number
of false positives will be used for this purpose.
3 Partitioning the WMT-12 dataset
Due to the lack of datasets annotated with ex-
plicit binary (good, bad) judgements about transla-
tion quality, the most intuitive way to obtain train-
ing data for our QE classifier is to adapt exist-
ing manually-labelled data. The reasonable size
of the WMT-12 dataset makes it a good candidate
for our purposes. The corpus consists of 2,254
English-Spanish news sentences (1,832 for train-
ing, 422 for test) produced by the Moses phrase-
based SMT system (Koehn et al, 2007) trained
on Europarl (Koehn, 2005) and News Commen-
taries corpora,4 along with their source sentences,
reference translations and post-edited translations.
Training and test instances have been annotated by
professional translators with scores (1 to 5) indi-
cating the estimated post-editing effort (percent-
age of MT output that has to be corrected). Ac-
cording to the proposed scheme, the highest score
indicates lowest effort (MT output requires little or
no editing), while the lowest score indicates that
the MT output needs to be translated from scratch.
To cope with systematic biases among the anno-
tators,5 the judgements were combined in a final
score obtained from their weighted average, re-
sulting in a labelled dataset with real numbers in
the [1, 5] interval as effort scores.
In order to obtain suitable data for binary QE,
the WMT-12 training set (1,832 instances) has
been partitioned in different ways, leaving the test
set for evaluation (see Section 5). The goal, for
each partition strategy, was to label as bad (the as-
signed label is -1) only the translations that need
complete rewriting, keeping all the other transla-
tions as good instances (labelled with +1). Consid-
ering the averaged effort scores, the actual human
judgements, and the HTER values calculated be-
tween the translations and the corresponding post-
edited version, we experimented with the follow-
ing three partition criteria.
Average effort scores (AES). Three partitions
have been generated based on the effort scores
of 2, 2.5, and 3, labelling the WMT-12 train-
ing instances with scores below or equal to each
threshold as negative examples (-1), and the in-
stances with scores above the threshold as posi-
tive examples (+1). Partitions with thresholds be-
low 2 were also considered, including the most
intuitive partition with cut-off set to 1. However,
the resulting number of negative instances, if any,
was too scarce, and the overall dataset too unbal-
anced, to make standard supervised learning meth-
ods effective The creation of highly unbalanced
data is a recurring issue for all the partition meth-
4http://www.statmt.org/wmt11/
translation-task.html#download
5Such biases support the idea that labelling translations
with quality scores is per se a highly subjective task.
242
ods we applied to the WMT-12 corpus. Together
with the low homogeneity of human labels (even
for very poor translations the three judges do not
agree in assigning the lowest score), in most of
the cases the small number of low-quality transla-
tions in the dataset makes the negative class con-
siderably smaller than the positive one. This can
be observed in Table 1, which provides the to-
tal number of positive and negative instances for
each partition method. For instance, with our low-
est AES threshold (2) the total number of nega-
tive instances is 113, while the positive ones are
1,719. Although considering different cut-off cri-
teria aims to make our investigation more com-
plete, it?s also worth remarking that the higher the
threshold, the higher the distance of the result-
ing experimental setting from our target scenario.
While 2, as an effort score threshold, is likely
to reflect a reasonable separation between useless
and post-editable translations, higher values are in
principle more appropriate for ?soft? separations
into worse versus better translations.
Human scores (HS). Five partitions have been
generated using the actual labels assigned by the
three annotators to each translation instead of the
average effort scores. In particular, we considered
the following score combinations (?X? stands for
any integer between 1 and 5): 1-X-X, 2-2-2, 2-
2-X, 2-3-3, 3-3-3. Also in this case, as shown
in Table 1, partitions based on lower scores lead
to highly unbalanced datasets of limited usability,
while those based on higher scores are increas-
ingly more distant to our application scenario.6
HTER scores (HTER). Seven partitions have
been generated considering the following HTER
thresholds: 0.75, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45.
In this case, being the HTER an error measure,
training instances with scores above or equal to
the threshold were labelled as negative examples
(-1), while instances with lower scores were la-
belled as positive examples (+1). Similar to the
other partition criteria, some of our threshold val-
ues reflect our task more closely than others, but
result in more unbalanced datasets. In particular,
thresholds around 0.7 substantially adhere to the
WMT-12 annotation guidelines (as far as transla-
tions that need complete rewriting are concerned)
6The partition most closely related to our task (i.e. 1-1-1)
was impossible to produce since none of the examples was
labelled with 1 by all the annotators. Even for 1-1-X, the
negative class contains only one example.
and produce training data with fewer negative in-
stances. Other thresholds, which is still worth ex-
ploring since we do not know the optimal cut-off
value, are in principle less suitable to our task but
produce more balanced training data.
Training instances
Average effort scores (AES) Positive Negative
2 1,719 113
2.5 1,475 357
3 1,194 638
Human scores (HS) Positive Negative
1-X-X 1,736 96
2-2-2 1,719 113
2-2-X 1,612 220
2-3-3 1,457 375
3-3-3 1,360 472
HTER scores (HTER) Positive Negative
0.75 1,798 34
0.7 1,786 46
0.65 1,756 76
0.6 1,708 124
0.55 1,653 179
0.5 1,531 301
0.45 1,420 412
Table 1: Number of positive/negative instances for
each partition of the WMT-12 training set.
4 Re-annotating the WMT-12 dataset
As an alternative to partitioning methods, we in-
vestigated the possibility to re-annotate the WMT-
12 training set with an automatic procedure.
4.1 Approach
Our approach, which does not involve subjec-
tive human judgements, is based on the observa-
tion of similarities and dissimilarities between an
automatic translation (TGT), its post-edited ver-
sion (PE) and the corresponding reference trans-
lation (RT). Such comparisons provide useful in-
dications about the behaviour of a post-editor
when correcting automatic translations and, in
turn, about MT output quality.
Typically, the PE version of a good-quality TGT
preserves some characteristics (e.g. lexical, struc-
tural) that indicate a moderate correction activity
by the post editor. Conversely, in the PE ver-
sion of a low-quality TGT, such characteristics
are more difficult to observe, indicating an in-
tense correction activity. At the two extremes, the
PE of a perfect TGT preserves all its characteris-
tics, while the PE of a useless TGT looses most
of them. In the first case TGT and PE are iden-
243
tical, and their similarity is the highest possible
(i.e. sim(TGT, PE) = 1). In the second case,
TGT and PE show a degree of similarity close to
that of TGT and a completely rewritten transla-
tion featuring different lexical choices and struc-
ture. This is where reference translations come
into play: considering RT as a good example of
rewritten sentence,7 for low-quality TGT we will
have sim(TGT, PE) ? sim(TGT,RT ).
In light of these considerations, we hypothe-
size that the automatic re-annotation of WMT-12
training data can take advantage of a classifier that
learns a similarity threshold T such that:
? a PE sentence with sim(TGT, PE) ? T
will be considered as a rewritten translation
(hence TGT is useless, and the correspond-
ing source-TGT pair a negative example to
be labelled as ?-1?);
? a PE sentence with sim(TGT, PE) > T
will be considered as a real post-edition
(hence TGT is useful for the post-editor, and
the corresponding source-TGT pair a positive
example to be labelled as ?+1?).
Based on this hypothesis, to perform our au-
tomatic re-annotation procedure we: 1) create a
training set Z of positive and negative examples
(i.e. [TGT, correct translation] pairs, where cor-
rect translation is either a post-editing or a rewrit-
ten translation); 2) design a feature set capable
to capture different aspects of the similarity be-
tween TGT and correct translation; 3) build a bi-
nary classifier using Z; 4) use the classifier to label
the [TGT, PE] pairs as instances of post-editings
or rewritings; 5) assess the quality of the resulting
annotation.
4.2 Building the classifier
Training corpus. To build a classifier capable
of labelling PE sentences as rewritten/post-edited
material, we first created a set of positive and neg-
ative instances from the WMT-12 training set. For
each tuple [source, TGT, PE, RT] of the dataset,
one positive and one negative instance have been
respectively obtained as the combination of [TGT,
PE] and [TGT, RT]. Figure 1, which plots the dis-
tribution of positive and negative instances against
HTER, shows a fairly good separation between the
7Such assumption is supported by the fact that reference
sentences are, by definition, free translations manually pro-
duced without any influence from the target.
0 500 1000 1500 2000 2500 3000 3500 40000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Sentences
HTE
R
 
 TGT?PE sentencesTGT?RT sentences
Figure 1: Distribution of [TGT, PE] and [TGT,
RT] pairs plotted against the HTER.
two classes. This indicates that our use of the
references as examples of rewritten translations
builds on a reasonable assumption.
Features. Crucial to our classification task, a
number of features can be used to estimate sen-
tence similarity. Differently from the binary QE
task, where the possibility to catch common char-
acteristics between two sentences is limited by
language barriers, in our re-annotation task all the
features are extracted by comparing two monolin-
gual sentences (i.e. TGT and a correct translation,
either a PE or a RT). Although the problem of
measuring sentence similarity can be addressed
in many ways, the solutions should not overlook
the specificities of the task. In our case, for in-
stance, the scarce importance of the semantic as-
pect (TGT, PE and RT typically show a high se-
mantic similarity) makes features used for other
tasks (e.g. based on distributional similarity) less
effective than shallow features looking at the sur-
face form of the input sentences. Our problem
presents some similarities with the plagiarism de-
tection task, where subtle lexical and structural
similarities have to be identified to spot suspicious
plagiarized texts (Potthast et al, 2010). For this
reason, part of our features (e.g. ROUGE scores)
are inspired by research in such field (Chen et al,
2010), while others have been designed ad-hoc,
based on the specific requirements of our task. The
resulting feature set aims to capture text similar-
ity by measuring word/n-gram matches, as well as
the level of sparsity and density of the common
words as a shallow indicator of structural similar-
ity. In total, from each [TGT, correct translation]
244
pair, the following 22 features are extracted:
? Human-targeted Translation Error Rate ?
HTER. The editing operations considered
are: shift, insertion, substitution and deletion.
? Number of words in common.
? Number of words in common, normalized by
TGT length and correct translation length (2
features).
? Number of words in TGT and in the cor-
rect translation (2 features).
? Size of the longest common subsequence.
? Size of the longest common subsequence,
normalized by TGT length.
? Aligned word density: total number of
aligned words,8 divided by the number of
aligned blocks (more than 1 aligned word).
? Unaligned word density: total number of un-
aligned words, divided by the number of un-
aligned blocks (more than 1 unaligned word).
? Normalized number of aligned blocks: total
number of aligned blocks, divided by TGT
length.
? Normalized number of unaligned blocks: to-
tal number of unaligned blocks, divided by
TGT length.
? Normalized density difference: difference
between aligned word density and unaligned
word density, divided by TGT length.
? Modified Lesk score (Lesk, 1986): sum of
the squares of the length of n-gram matches,
normalized by the product of the sentence
lengths.
? ROUGE-1/2/3/4: n-gram recall with n=1,...,4
(4 features).9
? ROUGE-L: size of longest common
subsequence, normalized by the cor-
rect translation length.
? ROUGE-W: the ROUGE-L using different
weights for consecutive matches of length L
(default weight = 1.2).
? ROUGE-S: the ROUGE-L allowing for the
presence of skip-bigrams (pairs of words,
even not adjacent, in their sentence order).
? ROUGE-SU: the extension of ROUGE-S
adding unigrams as counting unit.
8Monolingual stem-to-stem exact matches between TGT
and correct translation are inferred by computing the HTER,
as in (Blain et al, 2012).
9All ROUGE scores, described in (Lin, 2004), have been
calculated using the software available at http://www.
berouge.com.
To increase the capability of identifying simi-
lar sentences, all sentences are tokenized, lower-
cased and stemmed using the Snowball algorithm
(Porter, 2001).
Classifier. On the resulting corpus, an SVM
classifier has been trained using the LIBSVM tool-
box (Chang and Lin, 2011). The selection of the
kernel (linear) and the optimization of the param-
eters (C=0.8) were carried out through grid search
in 5-fold cross-validation.
Labelling the dataset. Using the best parameter
setting obtained, [TGT, PE] and [TGT, RT] pairs
have been re-labelled as post-editings or rewrit-
ings through 5 rounds of cross-validation. The fi-
nal label of each instance was set to the mode of
the predictions produced by each cross-validation
round. Since we assume that the quality of the tar-
get sentence can be inferred from the amount of
correction activity done by the post-editor, the la-
bels assigned to the [TGT, PE] pairs represent the
result of our re-annotation of the corpus into posi-
tive and negative instances.
At the end of the process, of the 1,832 [TGT,
PE] pairs of the WMT 2012 training set, 1.394 are
labelled as examples of post-editing (TGT is use-
ful), and 438 as examples of complete rewriting
(TGT is useless). Compared to the distribution
of positive and negative instances obtained with
most of the partition methods described in Section
3, our automatic annotation produces a fairly bal-
anced dataset. The resulting proportion of nega-
tive examples (?1:3) is similar to what could be
reached only by partitions reflecting a ?soft? sep-
aration into worse versus better translations rather
than a strict separation into useless versus useful
translations.10 In Figure 2, the labelling results
plotted against the HTER show that there is a quite
clear separation between [TGT, PE] pairs marked
as post-editings (lower HTER values) and pairs
marked as rewritings (higher HTER values). Such
separation corresponds to an HTER value around
0.4, which is significantly lower than the thresh-
old of 0.7 proposed by the WMT-12 guidelines as
a criterion to label sentences for which ?a trans-
lation from scratch is necessary?. This confirms
that our separation differs from those produced by
partition methods based on human annotations or
arbitrary HTER thresholds. Furthermore, our au-
10Such partitions are: average effort scores = 3, human
scores = 3-3-3, HTER score = 0.45.
245
0 200 400 600 800 1000 1200 1400 1600 1800 20000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Sentences
HTE
R
 
 TGT?PE labelled as PETGT?PE labelled as RT
Figure 2: TGT-PE classification in post-editings
and rewritings.
tomatic annotation procedure relies on the contri-
bution of features designed to capture different as-
pects of the similarity between the TGT and a cor-
rect translation, while some of the partition meth-
ods discussed in Section 3 rely on thresholds set on
a single score (e.g. HTER). Considering the many
facets of the binary QE problem, we expect that
our features are more effective to deal with latent
aspects disregarded by such thresholds.
5 Experiments and results
At this point, the question is: are the automatically
labelled data more suitable than partitions based
on human labels to train a binary QE classifier?
To answer this question, all the proposed separa-
tions of the WMT-12 training set have been eval-
uated on different test sets. For each separation
we trained a binary classifier able to assign a label
(good or bad) to unseen source-target pairs. Since
the classifiers use the same algorithm and feature
set, differences in performance will mainly depend
on the quality of the training data on which they
are built. Using task-oriented metrics sensitive to
the number of false positives, results highlighting
such differences will indicate the best separation.
5.1 Experimental Setting
Binary QE classifier. Each separation of the
WMT-12 training data was used to train a binary
SVM classifier. Different kernels and parameters
were optimized through a grid search in 5-fold
cross-validation on each training set. Being the
number of positive and negative training instances
highly unbalanced, the best models were selected
optimizing a metric that takes into account the
number of true and false positives (see below).
Seventeen features proposed in (Specia et al,
2009) were extracted from each source-target pair.
This feature set, fully described in (Callison-
Burch et al, 2012), mainly takes into account the
complexity of the source sentence (e.g. number
of tokens, number of translations per source word)
and the fluency of the target translation (e.g. lan-
guage model probabilities). Results of the WMT
2012 QE task shown that these ?baseline? features
are particularly competitive in the regression task,
with only few systems able to beat them. All the
features are extracted using the Quest software11
and the model files released by the organizers of
the WMT 2013 workshop.
Test sets. To obtain different separations be-
tween good and bad translations, artificial test sets
have been created using arbitrary thresholds on
the HTER (the same used to partition the train-
ing set on a HTER basis) and the post-editing time
(PET).12 Two different datasets were split: i) the
WMT-12 test (422 source, target, post-edited and
reference sentences); ii) the WMT-13 training set
for Task 1.3 (800 source, target and post-edited
sentences labelled with PET). The first dataset, the
most similar to the WMT-12 training set, should
better reflect (and reward) the HTER-based parti-
tions proposed in Section 3. The WMT-13 dataset
contains sentences translated with a different con-
figuration (data and parameters) of the SMT en-
gine. This can result in different HTER-based par-
titions in good and bad, useful to test the portabil-
ity of our automatic re-annotation method across
different datasets. Finally, testing on data parti-
tions based on PET allows us to check the stability
of the automatic re-annotation method when eval-
uated on a test set divided according to a different
concept of translation quality. In the end, the com-
bination of different partition methods, thresholds
and datasets results in 21 different test sets (see
Table 2).
Evaluation metrics. F-score and accuracy are
the classic evaluation metrics used in classifica-
tion. In our evaluation, however, they would al-
ways result in high uninformative values due to
the unbalanced nature of the test sets (positive in-
stances  negative instances). In order to bet-
11http://www.quest.dcs.shef.ac.uk/
12PET is the time spent by a post-editor to transform the
target into a publishable sentence.
246
Test instances
WMT-12 HTER Positive Negative
0.45 289 133
0.5 319 103
0.55 352 70
0.6 371 51
0.65 386 36
0.70 398 24
0.75 406 16
WMT-13 Task 1.3 HTER Positive Negative
0.45 582 218
0.5 622 178
0.55 695 105
0.6 724 76
0.65 748 52
0.70 763 37
0.75 773 27
WMT-13 Task 1.3 PET Positive Negative
4 499 301
4.16? 517 283
4.50 554 246
5 594 206
6 659 141
7 698 102
8 727 73
Table 2: Number of positive and negative in-
stances for each partition of the WMT-12 test set
and WMT-13 training set. ?*?: Average PET com-
puted on all the instances in the WMT-13 dataset.
ter understand the real quality of the classifica-
tion, we hence opted for two task-oriented evalua-
tion metrics sensitive to the number of false posi-
tives (the main issue in a CAT environment, where
false positives and true positives should be re-
spectively minimized and maximized). These are:
i) the weighted combination of the false positive
rate (FPR) and false discovery rate (FDR) (Ben-
jamini and Hochberg, 1995), and ii) the weighed
average of sensitivity and specificity (also called
balanced/weighted accuracy). FPR measures the
level of false positives, but does not provide infor-
mation about the number of true positives. For this
reason, we combined it with FDR (1-precision),
which indirectly controls the level of true posi-
tives. FPR and FDR were equally weighted in
the average; lower values indicate good perfor-
mance. Furthermore, in our scenario it is desir-
able to have a classifier with high prediction ac-
curacy over the minority class (specificity), while
maintaining reasonable accuracy for the majority
class (sensitivity). Weighted accuracy is useful in
such situations. To better asses the performance on
the minority (negative) class, we hence gave more
importance to specificity (0.7 vs 0.3). As regards
weighted accuracy higher values in indicate bet-
ter performance. Penalizing majority voting clas-
sifiers, both metrics are particularly appropriate in
our framework. Besides evaluation, the weighted
average of FPR and FDR was also used to tune the
parameters of the SVM classifier.
5.2 Results
Table 3 presents the results achieved by classifiers
trained on different datasets, on the 21 splits pro-
duced from the test sets used for evaluation.
Although the total number of classifiers tested
is 16 (15 resulting from partitions based on human
labels, and 1 obtained with our automatic annota-
tion method), most of them are not present in the
table since they predict the majority class for all
the test points. These are, in general, trained on
highly unbalanced training sets where the number
of negative samples is really small. However, it
is interesting to note that increasing the number
of instances in the negative class does not always
result in a better classifier. For instance, the classi-
fier built on an HTER separation with threshold at
0.55 performs majority voting even if it is built on
a more balanced (but probably more noisy) train-
ing set than the classifier obtained with threshold
at 0.6. This suggests that the quality of the sep-
aration is as important as the actual proportion of
positive and negative instances.
On all test sets, and for both the evaluation met-
rics used, the results achieved by the classifier built
from the automatically annotated training set (AA)
produces lower error rates (Weighted FPR-FDR)
and higher accuracy (Weighted Accuracy), outper-
forming all the other classifiers. The effective-
ness of the automatic annotation is confirmed by
the fact that classifiers 3 (based on the average
of effort scores - AES) and 3-3-3 (based on the
actual human scores - HS), which are trained on
more balanced training sets, achieve worse perfor-
mances than the AA classifier.13
Results on the WMT-13 PET test set are not as
good as in the other two test sets. This shows that
test data labelled in terms of time are more dif-
ficult to be correctly classified compared to those
based on the HTER. This can be explained consid-
ering the intrinsic differences between the HTER
and the PET as approximations of the post-editing
13The distribution of positive/negative instances in the
training sets is: 1194/638 for classifier 3, 1360/472 for clas-
sifier 3-3-3, 1394/438 for classifier AA.
247
Weighted Training: WMT-12 Separations
FPR-FDR 3 2-2-X 2-3-3 3-3-3 0.5 0.6 AA
AES HS HS HS HTER HTER
Tes
t:W
MT
-12
HT
ER 0.45 0.61 0.66 0.66 0.66 0.66 0.66 0.550.5 0.57 0.62 0.62 0.62 0.62 0.62 0.49
0.55 0.52 0.58 0.58 0.58 0.58 0.58 0.42
0.6 0.5 0.56 0.56 0.56 0.56 0.56 0.4
0.65 0.5 0.54 0.54 0.54 0.54 0.54 0.39
0.7 0.49 0.53 0.53 0.53 0.53 0.53 0.39
0.75 0.49 0.52 0.52 0.52 0.52 0.52 0.35
Tes
t:W
MT
-13
HT
ER 0.45 0.59 0.63 0.63 0.64 0.64 0.63 0.540.5 0.57 0.6 0.6 0.61 0.61 0.6 0.5
0.55 0.51 0.56 0.56 0.57 0.57 0.56 0.41
0.6 0.49 0.54 0.54 0.55 0.55 0.54 0.37
0.65 0.47 0.53 0.53 0.53 0.53 0.53 0.33
0.7 0.44 0.52 0.52 0.52 0.52 0.52 0.29
0.75 0.44 0.52 0.52 0.52 0.52 0.52 0.28
Tes
t:W
MT
-13
PET
4 0.61 0.68 0.68 0.69 0.69 0.68 0.58
4.16 0.61 0.67 0.67 0.67 0.67 0.67 0.56
4.5 0.58 0.65 0.64 0.65 0.65 0.65 0.54
5 0.55 0.63 0.62 0.63 0.63 0.62 0.51
6 0.49 0.58 0.58 0.58 0.58 0.58 0.45
7 0.45 0.55 0.55 0.56 0.56 0.55 0.43
8 0.45 0.54 0.54 0.54 0.54 0.54 0.41
Weighted Training: WMT-12 Separations
Accuracy 3 2-2-X 2-3-3 3-3-3 0.5 0.6 AA
AES HS HS HS HTER HTER
Tes
t:W
MT
-12
HT
ER 0.45 0.35 0.3 0.3 0.3 0.3 0.3 0.410.5 0.35 0.3 0.3 0.3 0.3 0.3 0.44
0.55 0.37 0.3 0.3 0.3 0.3 0.3 0.48
0.6 0.37 0.3 0.3 0.3 0.3 0.3 0.49
0.65 0.35 0.3 0.3 0.3 0.3 0.3 0.47
0.7 0.35 0.3 0.3 0.3 0.3 0.3 0.45
0.75 0.33 0.3 0.3 0.3 0.3 0.3 0.49
Tes
t:W
MT
-13
HT
ER 0.45 0.33 0.31 0.31 0.3 0.3 0.31 0.40.5 0.34 0.31 0.31 0.3 0.3 0.31 0.42
0.55 0.35 0.31 0.31 0.3 0.3 0.31 0.48
0.6 0.35 0.31 0.31 0.3 0.3 0.31 0.51
0.65 0.36 0.3 0.3 0.3 0.3 0.3 0.54
0.7 0.39 0.3 0.3 0.3 0.3 0.3 0.56
0.75 0.38 0.3 0.3 0.3 0.3 0.3 0.59
Tes
t:W
MT
-13
PET
4 0.37 0.3 0.31 0.3 0.3 0.3 0.4
4.16 0.37 0.3 0.31 0.3 0.3 0.3 0.4
4.5 0.37 0.3 0.31 0.3 0.3 0.3 0.4
5 0.38 0.31 0.31 0.3 0.3 0.31 0.41
6 0.41 0.31 0.31 0.3 0.3 0.31 0.43
7 0.42 0.31 0.31 0.3 0.3 0.31 0.44
8 0.4 0.31 0.31 0.3 0.3 0.31 0.43
Table 3: Weighted FPR-FDR (left table) and weighted Accuracy (right table) obtained by the binary QE
classifiers trained on different separations of the WMT-12 training set. Several arbitrary partitions of the
WMT-12 Test set and WMT-13 Training set are considered.
effort, as pointed out by several recent works (Spe-
cia, 2011; Koponen, 2012).
Comparing the results calculated with the two
metrics, we note that weighted accuracy seems to
be less sensible to small variations in terms of true
and false negatives returned by the classifier, even
if the specificity (accuracy on our minority class)
is weighted more than sensitivity (accuracy on our
majority class). This often results in scores very
close (differences ? 10?3) to the accuracy ob-
tained by majority voting classification (0.3).
Overall, our experiments demonstrate that the
proposed automatic separation method is more ef-
fective than arbitrary partitions of datasets anno-
tated with subjective human judgements.
5.3 Learning Curve
Our automatic re-annotation approach requires
post-edited and reference sentences. Although all
the datasets annotated for QE include post-edited
sentences, this is not always true for the refer-
ences. The cost of having both resources is in
fact not negligible. For this reason, we investi-
gated the minimal number of training data needed
to re-annotate the WMT-12 training set without
altering performance on binary classification. To
this aim, we selected two of the test sets on which
our re-annotation method produces classifiers with
high performance results (WMT-13 HTER 0.6 and
0.75), and measured score variations with increas-
ing amounts of data.
Nine subsets of the WMT-12 training set cor-
pus were created (with 10%, 20%,..., 100% of the
dataset) by sub-sampling sentences from a uni-
form distribution. The process was iterated 10
times. Then, for each subset, a new re-annotation
process was run, the resulting training set was used
to build the relative binary QE classifier, which
was eventually evaluated on the test set in terms of
weighted FPR-FDR. Figures 3 and 4 show the ob-
tained learning curves. Each point is the average
result of the 10 runs; the error bars show ?1std.
As can be seen from both curves, performance
results with 60% of the training data are already
comparable with those obtained using the whole
training data. Similar trends have been observed
for several learning curves created with different
test sets. This shows that, besides avoiding the
use of human labelled data, our approach allows
to drastically reduce the amount of training in-
stances. Considering the high costs of collecting
post-editions, and the fact that reference transla-
tions can be taken from parallel corpora, our solu-
tion represents a viable way to overcome the lack
of training data for binary QE geared towards in-
tegration in a CAT environment.
248
0 0.2 0.4 0.6 0.8 10.36
0.38
0.4
0.42
0.44
0.46
0.48
0.5
Training Set Size
Wei
ghte
d FP
R?F
DR
Figure 3: Learning curve for WMT-13 HTER 0.60.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.32
0.37
0.42
0.47
0.52
Training Set Size
Wei
ghte
d FP
R?F
DR
Figure 4: Learning curve for WMT-13 HTER 0.75.
6 Conclusion
We presented a task-oriented analysis of the use-
fulness of human-labelled data for binary qual-
ity estimation. Our target scenario is computer-
assisted translation, which calls for solutions to
present human translators with useful MT sugges-
tions (i.e. easier to correct than to rewrite from
scratch). Within this framework, the integration
of binary classifiers capable to distinguish ?good?
(useful) from ?bad? (useless) suggestions would
make possible to significantly increase translators?
productivity. Such binary classifiers, however,
need labelled training data (possibly of good qual-
ity) that are currently not available.
An intuitive solution to fill this gap is to take
advantage of an existing dataset, adapting its man-
ual annotations to our task. Exploring this solu-
tion (the first contribution of this paper) has to
face problems related to the subjectivity of human
judgements about translation quality, and the re-
sulting variability in the annotation. In particular,
our experiments with the WMT-12 dataset show
that any adaptation (either based on human judge-
ments or arbitrarily-set HTER thresholds) collides
with the problem of setting reasonable partition
criteria. Our results suggest that the subtle dif-
ferences between useful and useless translations
make subjective human judgements inadequate to
learn effective models.
Instead of relying on manually-assigned qual-
ity labels, an alternative solution to the problem
is to re-annotate an existing dataset. Proposing
an automatic way to do that (the second contri-
bution of this paper), we argue that reliable data
separations into positive and negative examples
can be obtained by measuring the similarities be-
tween: i) automatic translations and post-editings,
and ii) automatic translations and their references.
Our results demonstrate that binary classifiers built
from training data produced with our supervised
method are less prone to the misclassification of
bad suggestions.
As in any supervised learning framework, the
amount of data needed to obtain good results is of
crucial importance. By analysing the demand of
our automatic annotation method in terms of train-
ing data (the third contribution of this paper), we
show that competitive results can be obtained with
a fraction of the data needed by methods based on
human labels. Our results indicate that a good-
quality training set for binary classification can
be obtained with 40% less instances of [training,
post edited sentence, reference sentence], totally
avoiding manually-assigned quality judgements.
Our future works will address the improvement
of the automatic annotation procedure using super-
vised methods suitable to learn from unbalanced
training sets (e.g. one-class SVM, weighted ran-
dom forests), and the integration of new features
(e.g. GTM, meteor) to refine our classification of a
correct sentence into rewritten/post-edited. Then,
to boost binary QE results on the resulting corpora,
the ?baseline? features used for experiments in this
paper will be extended with new features explored
in recent works (Mehdad et al, 2012a; de Souza
et al, 2013; Turchi and Negri, 2013).
Acknowledgments
This work has been partially supported by the EC-
funded project MateCat (ICT-2011.4.2-287688).
249
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: a Method for Measuring Ma-
chine Translation Confidence. In The 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, Proceed-
ings of the Conference, 19-24 June, 2011, Portland,
Oregon, USA, pages 211?219. The Association for
Computer Linguistics.
Yoav Benjamini and Yosef Hochberg. 1995. Control-
ling the False Discovery Rate: a Practical and Pow-
erful Approach to Multiple Testing. Journal of the
Royal Statistical Society. Series B (Methodological),
pages 289?300.
Fre?de?ric Blain, Holger Schwenk, and Jean Senellart.
2012. Incremental Adaptation Using Translation In-
formation and Post-Editing Analysis. In Interna-
tional Workshop on Spoken Language Translation,
pages 234?241, Hong-Kong (China).
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence Es-
timation for Machine Translation. Summer work-
shop final report, JHU/CLSP.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT?12), pages 10?51, Montre?al, Canada.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: a Library for Support Vector Machines. ACM
Trans. Intell. Syst. Technol., 2(3):27:1?27:27, May.
Chien-Ying Chen, Jen-Yuan Yeh, and Hao-Ren Ke.
2010. Plagiarism Detection using ROUGE and
WordNet. Journal of Computing, 2(3).
Jose? G. C. de Souza, Miquel Espla`-Gomis, Marco
Turchi, and Matteo Negri. 2013. Exploiting Quali-
tative Information from Automatic Word Alignment
for Cross-lingual NLP Tasks. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013).
Michael Denkowski and Alon Lavie. 2012. Chal-
lenges in Predicting Machine Translation Utility
for Human Post-Editors. In Proceedings of AMTA
2012.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging SMT and TM with Translation
Recommendation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philip Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit X, pages 79?86,
Phuket, Thailand.
Maarit Koponen. 2012. Comparing Human Percep-
tions of Post-editing Effort with Post-editing Oper-
ations. In Proceedings of the Seventh Workshop on
Statistical Machine Translation, pages 181?190. As-
sociation for Computational Linguistics.
Michael Lesk. 1986. Automated Sense Disambigua-
tion Using Machine-readable Dictionaries: How to
Tell a Pine Cone from an Ice Cream Cone. In Pro-
ceedings of the 5th annual international conference
on Systems documentation (SIGDOC86).
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of
the ACL workshop on Text Summarization Branches
Out., pages 74?81, Barcelona, Spain.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent Translation using Discrim-
inative Learning: a Translation Memory-inspired
Approach. Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1239?
1248.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012a. Detecting semantic equivalence and infor-
mation disparity in cross?lingual documents. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?12),
pages 120?124, Jeju Island, Korea.
Yashar Mehdad, Matteo Negri, and Marcello Fed-
erico. 2012b. Match without a Referee: Evaluat-
ing MT Adequacy without Reference Translations.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, WMT ?12, pages 171?
180, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
M. Porter. 2001. Snowball: A language for stemming
algorithms.
Martin Potthast, Alberto Barro?n-Ceden?o, Andreas
Eiselt, Benno Stein, and Paolo Rosso. 2010.
Overview of the 2nd International Competition on
Plagiarism Detection. Notebook Papers of CLEF,
10.
250
Christopher B. Quirk. 2004. Training a Sentence-
Level Machine Translation Confidence Measure. In
In Proceedings of LREC.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy,
or HTER?: Exploring Different Human Judgments
with a Tunable MT Metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ?09, pages 259?268, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Radu Soricut and Abdessamad Echihabi. 2010.
TrustRank: Inducing Trust in Automatic Transla-
tions via Ranking. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 612?621, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL language weaver systems in the WMT12
quality estimation shared task. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion (WMT?12), pages 145?151, Montre?al, Canada.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Es-
timating the Sentence-Level Quality of Machine
Translation Systems. In Proceedings of the 13th
Annual Conference of the European Association
for Machine Translation (EAMT?09), pages 28?35,
Barcelona, Spain.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation versus Quality Es-
timation. Machine translation, 24(1):39?50.
Lucia Specia, Najeh Hajlaoui, Catalina Hallett, and
Wilker Aziz. 2011. Predicting machine transla-
tion adequacy. In Proceedings of the 13th Ma-
chine Translation Summit, pages 513?520, Xiamen,
China, September.
Lucia Specia. 2011. Exploiting Objective Annota-
tions for Measuring Translation Post-editing Effort.
pages 73?80.
Marco Turchi and Matteo Negri. 2013. ALTN: Word
Alignment Features for Cross-Lingual Textual En-
tailment. Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013).
251
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 301?308,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Online Learning Approaches in Computer Assisted Translation
Prashant Mathur??, Mauro Cettolo?, Marcello Federico?
? University of Trento
? FBK - Fondazione Bruno Kessler
Trento, Italy
{prashant, cettolo, federico}@fbk.eu
Abstract
We present a novel online learning ap-
proach for statistical machine translation
tailored to the computer assisted transla-
tion scenario. With the introduction of
a simple online feature, we are able to
adapt the translation model on the fly
to the corrections made by the transla-
tors. Additionally, we do online adaption
of the feature weights with a large mar-
gin algorithm. Our results show that our
online adaptation technique outperforms
the static phrase based statistical machine
translation system by 6 BLEU points abso-
lute, and a standard incremental adaptation
approach by 2 BLEU points absolute.
1 Introduction
The growing needs of the localization and trans-
lation industry have recently boosted research
around computer assisted translation (CAT) tech-
nology. The purpose of CAT is to increase the pro-
ductivity of a human translator. A CAT tool comes
as a package of a Translation Memory (TM), built-
in spell checkers, a dictionary, a terminology list
etc. which help the translator while translating
a sentence. Recent research has led to the in-
tegration of CAT tools with statistical machine
translation (SMT) engines. SMT makes use of a
large available parallel corpus to generate statisti-
cal models for translation. Due to their generaliza-
tion capability, SMT systems are a good fit in this
scenario and a seamless integration of SMT en-
gines in CAT have shown to increase translator?s
productivity (Federico et al, 2012).
Although automatic systems generate reliable
translations they are not accurate enough to be
used directly and need postedition by human trans-
lators. In state-of-the-art CAT tools, the SMT sys-
tems are static in nature and so they cannot adapt
to these corrections. When a SMT system keeps
repeating the same error, productivity of transla-
tors as well as their trust in SMT technology are
negatively affected. As an example, technical doc-
umentation typically contains a lot of repetitions
due to the employed writing style and pervasive
use of terminology. Hence, in order to provide
useful hints, SMT systems are expected to behave
consistently regarding the translation of domain-
specific terms. However, if the user edits the trans-
lation of a technical term in the target text, most
current SMT systems are incapable to learn from
those corrections.
Online learning is a machine learning task
where a predictor iteratively: (1) receives an input
and outputs a label, (2) receives the correct label
from a human and if the two labels do not match, it
learns from the mistake. The task of learning from
user corrections at the sentence level fits well the
online learning scenario, and its expected useful-
ness is clearly related to the amount of repetitions
occurring in the text. The higher the number of
repetititions in a document the more the SMT sys-
tem has chances to translate consistently through
the use of online learning.
In this paper, we implemented two online learn-
ing methods through which a phrase-based SMT
system evolves over time, sentence after sentence,
by taking advantage of the post-edition or transla-
tion of the previous sentence by the user.1
In the first approach, we focus on the translation
model aspect of SMT which is represented by five
conventional features, namely lexical and phrase
translation probabilities in both directed and in-
verted directions, plus a phrase penalty score.
Translation, language and reordering models are
combined in a linear fashion to obtain a score for
1Moses code is available in the github reposi-
tory. https://github.com/mtresearcher/
mosesdecoder/tree/moses_onlinelearning
301
the translation hypothesis as shown in Equation 1.
score(e?, f) = ?i?ihi(e?, f) (1)
where hi(?) are the feature functions representing
the models and ?i are the linear weights. The
highest scored translation is the best hypothesis
e? output by the system. We extend the transla-
tion model with a new feature which provides ex-
tra phrase-pair scores changing according to the
user feedback. The scores of the new feature are
adapted in a discriminative fashion, by reward-
ing phrase-pairs observed in the search space and
in the reference, and penalizing phrase-pairs ob-
served in the search space but not in the reference.
In the second approach, we also adapt the model
weights of the linear combination after each test
sentence by using a margin infused relaxed algo-
rithm (MIRA).
For assessing the robustness of our methods, we
performed experiments on two datasets from dif-
ferent domains and language pairs (?6). More-
over, our online learning approaches are compared
against a static baseline system and against the in-
cremental adaptation approach proposed by Lev-
enberg et. al. (2010) (?5).
2 Related Works
Several online adaptation strategies have been pro-
posed in the past, only a few deal with adaptation
of post-edited/evaluation data while most works
are on adaptation over development data during
tuning of parameters (Och and Ney, 2003).
2.1 Online Adaptation during Tuning
Liang et. al. (2006) improved SMT perfor-
mance by online adaptation of scaling factors (? in
(1)) using averaged perceptron algorithm (Collins,
2002). They presented different strategies to up-
date the SMT models towards reference or oracle
translation: (1) aggressively updating towards ref-
erence, bold update; (2) update towards the ora-
cle translation in N-Best list, local update; (3) a
hybrid approach in which a bold update is per-
formed when the reference is reachable, other-
wise a local update is performed. Liang and Klein
(2009) compared two online EM algorithms, step-
wise online EM (Sato and Ishii, 2000; Cappe? and
Moulines, 2007) and incremental EM (Neal and
Hinton, 1998) which they use to update the align-
ment models (the generative component of SMT)
on the fly. However, stepwise EM is prone to fail-
ure if mini-batch size and stepsize parameters are
not chosen correctly, while incremental EM re-
quires substantial storage costs because it has to
store sufficient statistics for each sample. Other
works on online minimum error rate training in
SMT (Och and Ney, 2003) that deserve mention-
ing are (Hopkins and May, 2011; Hasler et al,
2011).
2.2 Online Adaptation during Decoding
Cesa-Bianchi et. al. (2008) proposed an online
learning approach during decoding. They con-
struct a layer of online weights over the regu-
lar feature weights and update these weights at
sentence level using margin infused relaxed algo-
rithm (Crammer and Singer, 2003); to our knowl-
edge, this is the first work on online adaptation
during decoding. Mart??nez-Go?mez et. al. (2011;
2012) presented a comparison of online adapta-
tion techniques in post editing scenario. They
compared different adaptation strategies on scal-
ing factors and feature functions (respectively, ?
and h(?) in (1)). However, they modified the fea-
ture values during adaptation without any normal-
ization, which disregards the initial assumption of
the feature values being probabilities.
In our approach, the value of the additional on-
line feature can be modified during decoding with-
out changing other feature values (probabilities)
and thus preserving their probability distribution.
3 Feature Adaptation
In the CAT scenario, the user receives a translation
suggestion for each source segment, post-edits it
and finally approves it. From the SMT point of
view, for each source segment the decoder ex-
plores a search space of possible translations and
finally returns the best scoring one (bestHyp) to
the user. The user possibly corrects this suggestion
thus generating the final translation (postedit).
Our online learning procedure is based on the
following idea. For each N-best translation (candi-
date) in the search space, we compute a similarity
score against the postedit using the sentence-level
BLEU metric (Lin and Och, 2004), a smoothed
variant of the popular BLEU metric (Papineni
et al, 2001). We hence compare the similar-
ity score of each candidate against the similar-
ity score achieved by the bestHyp, that was also
computed against the postedit. If the candidate
302
scores better than the bestHyp, then we promote
the building blocks, i.e. phrase-pairs, of candi-
date that were not used in bestHyp and demote the
phrase-pairs used in bestHyp that were not used
for candidate. On the contrary, if the candidate
scores worse than the bestHyp, we promote the
building blocks of bestHyp that are not in candi-
date and demote those of candidate that are not in
bestHyp.
Our promotion/demotion mechanism could be
implemented by updating the features values of
the phrase pairs used in the candidate and bestHyp
translations. However, features in the translation
models are conditional probabilities and perturb-
ing a subset of them by also preserving their nor-
malization constraints can be computationally ex-
pensive. Instead, we propose to introduce an addi-
tional online feature which represents a goodness
score of each phrase-pair in the test set.
We call the set of phrase pairs used to generate
a candidate as candidatePP and the set of phrase
pairs used to generate the bestHyp as bestPP . The
online feature value of each phrase-pair is initial-
ized to a constant and is updated according to the
perceptron update (Rosenblatt, 1958) method. In
particular, the amount by which a current feature
value is rewarded or penalized depends on a learn-
ing rate ? and on the difference between the model
scores (i.e. h ?w) of candidate and bestHyp as cal-
culated by the MT system. A sketch of our online
learning procedure is shown in Algorithm 1.
Algorithm 1: Online Learning
foreach sourceSeg do
bestHyp = Translate(sourceSeg);
postedit = Human(bestHyp);
for i = 1 ? iterations do
N-best=Nbest(source);
foreach candidate ? N-best do
sign = sgn |sBLEU(candidate) -
sBLEU(bestHyp)| ;
foreach phrasePair ? candidatePP do
if phrasePair /? bestPP then
f i = f i?1 + (? ? (?h ? w) ?
sign);
end
end
foreach phrasePair ? bestPP do
if phrasePair /? candidatePP then
f i = f i?1 - (? ? (?h ? w) ?
sign);
end
end
end
end
end
In Algorithm 1, ?h ? w is the above mentioned
score difference as computed by the decoder; mul-
tiplied by ?, it is the margin, that is the value with
which the online feature score (f ) of the phrase
pair under processing is modified. We can observe
that the feature scores are unbounded and could
lead to instability of the algorithm; therefore, we
normalise the scores through the sigmoid function:
f(x) = 21 + exp(x) ? 1 (2)
4 Weight Adaptation
In addition to adapting the online feature values,
we can also apply online adaptation on the fea-
ture weights of the linear combination (eq. 1). In
particular, after translating each sentence we can
adapt the parameters depending on how good the
last translation was. A commonly used algorithm
in this online paradigm for tuning of parameters is
the Margin Infused Relaxed Algorithm (MIRA).
MIRA is an online large margin algorithm that
updates the parameter w? of a given model accord-
ing to the loss that is occurred due to incorrect
classification. In the case of SMT this margin
can be coupled with the loss function, which in
this case is the complement of the sentence level
BLEU(sBLEU). Thus, the loss function can be
formulated as:
l(y?) = sBLEU(y?)? sBLEU(y?) (3)
where y? is the oracle (closest translation to the
reference) and y? is the candidate being processed.
Ideally, this loss should correspond to the differ-
ence between the model scores:
?h ? w? = score(y?)? score(y?) (4)
MIRA is an ultraconservative algorithm, meaning
that the update of the current weight vector is the
smallest possible value satisfying the constraint
that the variation incurred by the objective func-
tion must not be larger than the variation incurred
by the model (plus a non-negative slack variable
?). Formally, weight update at ith iteration is de-
fined as:
wi = argminw
1
2? ||w ? wi?1||
2
? ?? ?
conservative
+ C????
aggressive
?
j
?j
subject to
lj ? ?hj ? w + ?j ?j ? J ? {1 . . . N}
(5)
303
where j ranges over all candidates in the N-
best list, lj is the loss between oracle and the
candidate j, and ?hj ? w is the corresponding
difference in the model scores. C is an aggressive
parameter which controls the size of the update, ?
is the learning rate of the algorithm and ? is usu-
ally a very small value (in our experiments we kept
it as 0.0001). After partial differentiation and lin-
earizing the loss, equation 5 can be rewritten as:
wi = wi?1 + ? ?
?
j
?j ??hj
where
?j = min
{
C, lj ? ?hj ? w||?hj ||2
}
(6)
We solve equation 5, by computing ? with
the optimizer integrated in the Moses toolkit by
(Hasler et al, 2011). Algorithm 2 gives an
overview of the online margin infused relaxed al-
gorithm we implemented in Moses.
Algorithm 2: Online Margin Infused Relaxed
foreach sourceSeg do
bestHyp = Translate(sourceSeg);
postedit = Human(bestHyp);
w0 = w;
for i = 1 ? iterations do
N-best=Nbest(sourceSeg,wi?1);
foreach candidatej ? N-best do
if ?hj ? w + ?j ? lj then
?j = Optimize(lj , hj , w, C);
wi = wi?1 + ? ??j ?j?hj ;
end
end
end
end
In the following section we overview a stream
based adaptation method with which we exper-
imentally compared our two online learning ap-
proaches as it well fits the framework we are work-
ing in.
5 Stream based adaptation
Continuously updating an SMT system to an in-
coming stream of parallel data comes under stream
based adaptation. Levenberg et. al. (2010) pro-
posed an incremental adaptation technique for the
core generative component of the SMT system,
word alignments and language models (Leven-
berg and Osborne, 2009). To get the word align-
ments on the new data they use a Stepwise online
EM algorithm, where old counts (from previous
alignment models) are interpolated with the new
counts.
Since we work at the sentence level, on-the-
fly computation of probabilities of translation and
reordering models is expensive in terms of both
computational and memory requirements. To save
these costs, we prefer using dynamic suffix ar-
ray approach described in (Levenberg et al, 2010;
Callison-Burch et al, 2005; Lopez, 2008). They
are used to efficiently store the source and the tar-
get corpus and alignments in efficient data struc-
ture, namely the suffix array. When a phrase
translation is asked by the decoder, the corpus is
searched, the counts are collected and its probabil-
ities are computed on the fly. However, the current
implementation in Moses of the stream based MT
relying on the suffix arrays is severely limited as
it allows the computation of only three translation
features, namely the two direct translation proba-
bilities and the phrase penalty. This results in a
significant degradation of performance.
6 Experiments
6.1 Datasets
We compared our online learning approaches
(Sections 3 and 4) and the stream based adapta-
tion method (Section 5) on two datasets from dif-
ferent domains, namely Information Technology
(IT) and TED talks, and two different language
pairs. The IT domain dataset is proprietary, it in-
volves the translation of technical documents from
English to Italian and has been used in the field
test carried out under the MateCat project2. Ex-
periments are also conducted on English to French
TED talks dataset (Cettolo et al, 2012) to assess
the robustness of the proposed approaches in a dif-
ferent scenario and to provide results on a publicly
available dataset for the sake of reproducibility.
The training, development (dev2010) and evalu-
ation (tst20103) sets are the same as used in the
last IWSLT last evaluation campaigns. In experi-
ments on TED data, we considered the human ref-
erence translations as post edits, even if they were
2www.matecat.com
3As the size of evaluation set in TED data is too large with
respect to the current implementation of our algorithms, we
performed evaluation on the first 200 sentences only.
304
actually generated from scratch.
In our experiments, the extent of usefulness of
online learning highly depends on the amount of
repetition of text. A reasonable way to measure the
quantity of repetition in each document is through
the repetition rate (Bertoldi et al, 2013). It com-
putes the rate of non-singleton n-grams, n=1...4,
averaging the values over sub-samples S of thou-
sand words from the text, and then combining the
rate of each n-gram to a single score by using the
geometric mean. Equation 7 shows the formula
for calculating the repetition rate of a document,
where dict(n) represents the total number of
different n-grams and nr is the number of different
n-grams occurring exactly r times:
RR =
( 4?
n=1
?
S dict(n)? n1?
S dict(n)
)1/4
(7)
Statistics of the parallel sets and their repetition
rate on both sides are reported in Table 1.
Domain Set #srcTok srcRR #tgtTok tgtRR
ITen?it
Train 57M na 60M na
Dev 3.3k 12.03 3.5k 11.87
Test 3.3k 15.00 3.3k 14.57
TEDen?fr
Train 2.6M na 2.8M na
Dev 20k 3.43 20k 5.27
Test 32k 4.08 34k 3.57
Table 1: Statistics of the parallel data along with
the corresponding repetition rate (RR).
It can be noted that the repetition rates of IT
and TED sets are significantly different, partic-
ularly high in IT documents, much lower in the
TED talks.
6.2 Systems
The SMT systems were built using the Moses
toolkit (Koehn et al, 2007). Training data in each
domain was used to create translation and lexical
reordering models. We created a 5-gram LM for
TED talks and a 6-gram LM for the IT domain
using IRSTLM (Federico et al, 2008) with im-
proved Kneser-Ney smoothing (Chen and Good-
man, 1996) on the target side of the training paral-
lel corpora. The log linear weights for the baseline
systems are optimized using MERT (Och, 2003)
provided in the Moses toolkit. To counter the in-
stability of MERT, we averaged the weights of
three MERT runs in each case. Performance is
measured in terms of BLEU and TER (Snover
et al, 2006) computed using the MultEval script
(Clark et al, 2011). Since the implementations of
standard Giza and of incremental Giza combined
with dynamic suffix arrays are not comparable,
we constructed two baselines, a standard phrase
based SMT system and an incremental Giza base-
line (?5). Details on experimental SMT systems
we built follow.
Baseline This system was built on the parallel
training data for each domain. We run 5 iterations
of model 1, 5 of HMM (Vogel et al, 1996), 3 of
model 3, 3 of model 4 (Brown et al, 1993) us-
ing MGiza (Gao and Vogel, 2008) toolkit to align
the parallel corpus at word level. Translation and
reordering models were built using Moses, while
log-linear weights were optimized with MERT on
the corresponding development sets. The same IT
baseline system was used in the field test of Mate-
Cat and the references in the IT data are actual
postedits of its translation.
IncGiza Baseline We trained alignment models
with incGiza++4 with 5 iterations of model 1 and
10 iterations of the HMM model. To build in-
cremental Giza baselines, we used dynamic suf-
fix arrays as implemented in Moses which allow
the addition of new parallel data during decod-
ing. In the incremental Giza baseline, once a sen-
tence of the test set is translated, the sentence pair
(source and target post-edit/reference) along with
the alignment provided by incGiza are added to
the models.
Online learning systems We developed several
online systems on top of the two aforementioned
baseline systems: (1) +O employ the additional
online feature (Section 3) updated with Algorithm
1; (2) +O+NS as (1) but with the online fea-
ture normalized with the sigmoid function; (3)
+W weights updated (Section 4) with Algorithm
2; (4) +O+W combination of online feature and
weight update; (5) +O+NS+W as system (4) with
normalized online feature score.
In the online learning system we have three ad-
ditional parameters: a weight for the online fea-
ture, a learning rate for features (used in the per-
ceptron update), and a learning rate for feature
weights used by MIRA. These additional param-
eters were optimized by maximizing the BLEU
4http://code.google.com/p/inc-giza-pp/
305
score on the devset and on top of already opti-
mized feature weights. For practical reasons, opti-
mization of the parameters was run with the Sim-
plex algorithm (Nelder and Mead, 1965).
7 Results and Discussion
Tables 2 and 3 collect results by the systems de-
scribed in Section 6.2 on the IT and TED transla-
tion tasks, respectively.
In Table 2, the online system (1st block
?+O+NS+W? system with 10 iterations of online
learning) shows significant improvements, over 6
BLEU points absolute above the baseline. In this
case the online feature can clearly take advantage
of the high repetition rates observed in the IT dev
and test sets (Table 1). Similarly, in the second
block, the online system (2nd block ?+O+NS+W?
with 10 iterations of online learning) outperforms
IncGiza baseline, too. It is interesting to note that
by continuously updating the baseline system af-
ter each translation step, even the plain translation
models are capable to learn from the correction in
the post-edited text.
Figure 1 depicts learning curve of Baseline sys-
tem, ?+O+NS? (referred as +online feature) and
?+O+NS+W? (referred as +MIRA). We plotted in-
cremental BLEU scores after translation of each
sentence, thereby the last point on the plot shows
the corpus level BLEU on the whole test set.
In Table 3, from the first block we can observe
that online learning systems perform only slightly
better than the baseline systems, the main reason
being the low repetition rate observed in the eval-
uation set (as shown in Table 1). The positive re-
sults observed in the second block (?+O+W? with
10 iterations) are probably due to the larger room
for improvement available for translation models
implemented with dynamic suffix arrays, as they
only incorporate 3 features instead of 5. Some-
times, online learning systems show worse results
with higher numbers of iterations, which seems
due to overfitting. It is also interesting to notice
that after optimization the weight value of the on-
line feature was 0.509 for the IT task and 0.072 for
the TED talk task. This confirms the different use
and potential assigned to the online feature by the
SMT systems in the two tasks.
8 Conclusion
We have shown a new way to update the transla-
tion model on the fly without changing the original
probability distribution. We empirically proved
that this method is robust and works for differ-
ent domain datasets be it Information Technology
or TED talks. In addition, if the repetition rate is
high in the text, online learning works much bet-
ter than if the rate is low. We tested both with an
unbounded and a bounded range on the online fea-
ture and found out that bounded values produce
more stable and consistent results. From previous
works, it has been proven that MIRA works well
with sparse features too, so, as for the future plan
we would like to treat each phrase pair as a sparse
feature and tune the sparse weights using MIRA.
From the results, it is evident that we have not used
any sort of stopping criterion for online learning; a
random of 1, 5 and 10 iterations were chosen in a
naive way. Our future plan will extend to working
on finding a stopping criterion for online learning
process.
Acknowledgements
This work was supported by the MateCat project,
which is funded by the EC under the 7th Frame-
work Programme.
References
N. Bertoldi, M. Cettolo, and M. Federico. 2013.
Cache-based online adaptation for machine trans-
lation enhanced computer assisted translation. In
Proc. of MT Summit, Nice, France.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?312.
C. Callison-Burch, C. Bannard, and J. Schroeder.
2005. Scaling phrase-based statistical machine
translation to larger corpora and longer phrases. In
Proc. of ACL, pages 255?262, Ann Arbor, US-MI.
O. Cappe? and E. Moulines. 2009. Online EM algo-
rithm for latent data models. Journal of the Royal
Statistical Society Series B (Statistical Methodol-
ogy), 71(3):593?613.
N. Cesa-Bianchi, G. Reverberi, and S. Szedmak. 2008.
Online learning algorithms for computer-assisted
translation. Technical report, SMART project
(www.smart-project.eu).
M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3:
web inventory of transcribed and translated talks. In
Proc. of EAMT, Trento, Italy.
S. F. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
Proc. of ACL, pages 310?318, Santa Cruz, US-CA.
306
System Bleu (?) TER (?)
1 Iter 5 Iter 10 Iter 1 Iter 5 Iter 10 Iter
Baseline 38.46(1.79) - - 39.98(1.35) - -
+O 39.88(1.77) 41.22(1.80) 41.16(1.74) 38.69(1.30) 37.78(1.32) 38.37(1.30)
+O+NS 39.91(1.80) 40.54(1.79) 40.71(1.76) 38.67(1.31) 38.21(1.29) 38.17(1.31)
+W 39.76(1.76) 38.16(1.77) 37.57(1.82) 38.58(1.27) 39.53(1.30) 39.93(1.30)
+O+W 41.23(1.66) 40.29(1.54) 29.36(1.45) 37.53(1.26) 38.03(1.24) 49.08(1.25)
+O+NS+W 41.19(1.86) 43.07(1.87) 45.13(1.74) 37.60(1.35) 36.43(1.43) 34.53(1.36)
IncGiza Baseline 28.48(1.50) - - 49.23(1.43) - -
+O 29.34(1.51) 27.80(1.49) 27.52(1.38) 47.86(1.41) 48.20(1.30) 51.01(1.53)
+O+NS 28.69(1.53) 29.68(1.45) 29.36(1.49) 48.21(1.45) 47.51(1.45) 47.92(1.45)
+W 28.25(1.56) 27.68(1.53) 27.57(1.50) 49.05(1.43) 48.74(1.36) 48.10(1.23)
+O+W 29.36(1.61) 29.94(1.64) 25.95(1.25) 47.15(1.41) 46.56(1.31) 50.31(1.15)
+O+NS+W 29.76(1.49) 30.28(1.54) 30.83(1.60) 46.62(1.39) 45.60(1.28) 46.54(1.31)
Table 2: Result on the IT domain task (EN>IT). Baseline is a standard phrase based SMT system, +O
has the online feature, +NS adds normalization of online feature, +W has online weight adaptation.
 20
 25
 30
 35
 40
 45
 50
 0  20  40  60  80  100  120  140  160  180
B
LE
U
 S
co
re
Sentence Number
baseline
+online feature
+MIRA
Figure 1: Incremental BLEU vs. evaluation test size on the information-technology task. Three systems
are tracked: Baseline, +online feature, +MIRA
System Bleu (?) TER (?)
1 Iter 5 Iter 10 Iter 1 Iter 5 Iter 10 Iter
Baseline 22.18(1.23) - - 58.70(1.38) - -
+O 22.17(1.19) 21.85(1.25) 21.51(1.23) 58.75(1.35) 59.22(1.36) 60.48(1.35)
+O+NS 21.97(1.20) 22.37(1.20) 22.24(1.22) 58.86(1.37) 58.75(1.37) 59.09(1.40)
+W 22.39(1.23) 21.44(1.20) 21.00(1.13) 58.96(1.40) 58.73(1.34) 58.71(1.28)
+O+W 22.33(1.21) 22.11(1.22) 21.54(1.20) 58.63(1.37) 58.31(1.38) 58.70(1.36)
+O+NS+W 22.34(1.23) 22.09(1.21) 21.62(1.18) 58.60(1.37) 58.48(1.36) 58.40(1.33)
IncGiza Baseline 15.04(1.08) - - 72.64(1.34) - -
+O 15.30(1.08) 15.47(1.10) 15.86(1.11) 72.33(1.35) 71.68(1.37) 71.09(1.36)
+O+NS 15.21(1.09) 15.48(1.12) 15.48(1.11) 72.19(1.33) 72.06(1.36) 71.65(1.33)
+W 14.81(1.08) 14.61(1.07) 14.73(1.08) 73.03(1.37) 74.69(1.48) 74.28(1.46)
+O+W 15.08(1.08) 15.59(1.09) 16.42(1.11) 72.55(1.33) 70.98(1.32) 70.07(1.27)
+O+NS+W 15.09(1.08) 15.64(1.08) 16.15(1.10) 72.57(1.34) 71.13(1.31) 70.61(1.33)
Table 3: Result on the TED talk task (EN>FR). Baseline is a standard phrase based SMT system, +O
has the online feature, +NS adds normalization of online feature, +W includes online weight adaptation.
307
J. Clark, C. Dyer, A. Lavie, and N. Smith. 2011. Bet-
ter hypothesis testing for statistical machine transla-
tion: Controlling for optimizer instability. In Proc.
of ACL, Portland, US-OR.
M. Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP,
Philadelphia, US-PA.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
M. Federico, N. Bertoldi, and M. Cettolo. 2008.
IRSTLM: an open source toolkit for handling large
scale language models. In Proc. of Interspeech,
pages 1618?1621, Brisbane, Australia.
M. Federico, A. Cattelan, and M. Trombetti. 2012.
Measuring user productivity in machine translation
enhanced computer assisted translation. In Proc. of
AMTA, Bellevue, US-WA.
Q. Gao and S. Vogel. 2008. Parallel implementations
of word alignment tool. In Proc. of SETQA-NLP,
pages 49?57, Columbus, US-OH.
E. Hasler, B. Haddow, and P. Koehn. 2011. Margin
infused relaxed algorithm for Moses. The Prague
Bulletin of Mathematical Linguistics, 96:69?78.
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proc. of EMNLP, pages 1352?1362, Edinburgh, UK.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proc.
of ACL Companion Volume of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic.
A. Levenberg and M. Osborne. 2009. Stream-based
randomised language models for SMT. In Proc. of
EMNLP, pages 756?764, Singapore.
A. Levenberg, C. Callison-Burch, and M. Osborne.
2010. Stream-based translation models for statisti-
cal machine translation. In Proc. of HLT-NAACL,
Los Angeles, US-CA.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL, pages 611?619,
Boulder, US-CO.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of ACL, pages 761?768,
Sydney, Australia.
C.-Y. Lin and F. J. Och. 2004. Orange: a method for
evaluating automatic evaluation metrics for machine
translation. In Proc. of COLING, pages 501?507,
Geneva, Switzerland.
A. Lopez. 2008. Tera-scale translation models via pat-
tern matching. In Proc. of COLING, pages 505?512,
Manchester, UK.
P. Mart??nez-Go?mez, G. Sanchis-Trilles, and F. Casacu-
berta. 2011. Online learning via dynamic reranking
for computer assisted translation. In Proc. of CI-
CLing, pages 93?105, Tokyo, Japan.
P. Mart??nez-Go?mez, G. Sanchis-Trilles, and F. Casacu-
berta. 2012. Online adaptation strategies for statis-
tical machine translation in post-editing scenarios.
Pattern Recogn., 45(9):3193?3203.
R. Neal and G. E. Hinton. 1998. A view of the EM al-
gorithm that justifies incremental, sparse, and other
variants. In Learning in Graphical Models, pages
355?368. Kluwer Academic Publishers.
J. A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7(4):308?313.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Research Report RC22176, IBM Re-
search Division, Thomas J. Watson Research Center.
F. Rosenblatt. 1958. The Perceptron: a probabilistic
model for information storage and organization in
the brain. Psychological Review, 65:386?408.
M.-A. Sato and S. Ishii. 2000. On-line EM algorithm
for the normalized Gaussian network. Neural Com-
put., 12(2):407?432.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J.
Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proc. of AMTA,
Boston, US-MA.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. of
COLING, pages 836?841, Copenhagen, Denmark.
308
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 440?451,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Efficient solutions for word reordering in German-English
phrase-based statistical machine translation
Arianna Bisazza and Marcello Federico
Fondazione Bruno Kessler
Trento, Italy
{bisazza,federico}@fbk.eu
Abstract
Despite being closely related languages,
German and English are characterized by
important word order differences. Long-
range reordering of verbs, in particular,
represents a real challenge for state-of-the-
art SMT systems and is one of the main
reasons why translation quality is often so
poor in this language pair. In this work,
we review several solutions to improve
the accuracy of German-English word re-
ordering while preserving the efficiency of
phrase-based decoding. Among these, we
consider a novel technique to dynamically
shape the reordering search space and
effectively capture long-range reordering
phenomena. Through an extensive eval-
uation including diverse translation qual-
ity metrics, we show that these solutions
can significantly narrow the gap between
phrase-based and hierarchical SMT.
1 Introduction
Modeling the German-English language pair is
known to be a challenging task for state-of-the-
art statistical machine translation (SMT) methods.
A major factor of difficulty is given by word or-
der differences that yield important long-range re-
ordering phenomena.
Thanks to specific reordering modeling compo-
nents, phrase-based SMT (PSMT) systems (Zens
et al, 2002; Koehn et al, 2003; Och and Ney,
2002) are generally good at handling local re-
ordering phenomena that are not captured inside
phrases. However, they typically fail to predict
long reorderings. On the other hand, hierarchi-
cal SMT (HSMT) systems (Chiang, 2005) can
learn reordering patterns by means of discontinu-
ous translation rules, and are therefore considered
a better choice for language pairs characterized by
massive and hierarchical reordering.
Looking at the results of the Workshop of
Machine Translation?s last edition (WMT12)
(Callison-Burch et al, 2012), no particular SMT
approach appears to be clearly dominating. In
both language directions (official results excluding
the online systems) the rule-based systems outper-
formed all SMT approaches, and among the best
SMT systems we find a variety of approaches:
pure phrase-based, phrase-based and hierarchical
systems combination, n-gram based, a rich syntax-
based approach, and a phrase-based system cou-
pled with POS-based pre-ordering. This gives an
idea of how challenging this language pair is for
SMT and raises the question of which SMT ap-
proach is best suited to model it.
In this work, we aim at answering this ques-
tion by focussing on the word reordering problem,
which is known to be an important factor of SMT
performance (Birch et al, 2008). We hypothe-
size that PSMT can be as successful for German-
English as the more computationally costly HSMT
approach, provided that the reordering-related pa-
rameters are carefully chosen and the best avail-
able reordering models are used. More specifi-
cally, our study covers the following topics: dis-
tortion functions and limits, and dynamic shaping
of the reordering search space based on a discrim-
inative reordering model.
We first review these topics, and then evaluate
them systematically on the WMT task using both
generic and reordering-specific metrics, with the
aim of providing a reference for future system de-
velopers? choices.
2 Background
Word order differences between German and En-
glish are mainly found at the clause (global) level,
as opposed to the phrase (local) level. We refer to
Collins et al (2005) and Gojun and Fraser (2012)
for a detailed description of the German clause
structure. To briefly summarize, we can say that
440
the verb-second order of German main clauses
contrasts with the rigid SVO structure of English,
as does the clause-final verb position of German
subordinate clauses. A further difficulty is given
by the German discontinuous verb phrases, where
the main verb is separated from the inflected auxil-
iary or modal. The distance between the two parts
of a verb phrase can be arbitrarily long as shown
in the following example:
[DE] Jedoch konnten sie Kinder in Teilen von Helmand
und Kandahar im Su?den aus Sicherheitsgrund nicht er-
reichen.
[EN] But they could not reach children in parts of Hel-
mand and Kandahar in the south for security reasons.
Translating this sentence with a PSMT engine
implies performing two very long jumps that are
not even considered by typical systems employing
a distortion limit of 6 or 8 words. At the same
time, increasing the distortion limit to very high
values is known to have a negative impact on both
efficiency and translation quality (cf. results pre-
sented later in this paper).
Because reordering patterns of this kind are
very common between German and English, this
paper focuses on techniques that enable the PSMT
decoder to explore long jumps and thus improve
reordering accuracy without hurting efficiency nor
general translation quality.
2.1 Alternative approaches
German-English reordering in SMT has been
widely studied and is still an open topic. In this
work, we only consider efficient solutions that are
fully integrated into the decoding process, and that
do not require syntactic parsers or manual reorder-
ing rules. Still, it has to be mentioned that sev-
eral alternative solutions were proposed in the lit-
erature. A well-known strategy consists of pre-
ordering the German sentence in an English-like
order by applying a set of manually written rules
to its syntactic parse tree (Collins et al, 2005).1
Other approaches learn the pre-ordering rules au-
tomatically, from syntactic parses (Xia and Mc-
Cord, 2004; Genzel, 2010) or from part-of-speech
labels (Niehues and Kolss, 2009). In the former
case, pre-ordering decisions are typically taken de-
terministically (i. e. one permuation per sentence),
whereas in the latter, multiple alternatives are rep-
resented as word lattices, and the optimal path is
1A similar solution for the opposite translation direction
(English-German) was proposed by Gojun and Fraser (2012).
selected by the decoder at translation time. In
(Tromble and Eisner, 2009), pre-ordering is cast
as a permutation problem and solved by a model
that estimates the probability of reversing the rel-
ative order of any two input words.
In the field of tree-based SMT, positive results
in German-English were achieved by combining
syntactic translation rules with unlabeled hierar-
chical SMT rules (Hoang and Koehn, 2010). More
recently, Braune et al (2012) proposed to improve
the long-range reordering capability of an HSMT
system by integrating constraints based on clausal
boundaries and by manually selecting the rule pat-
terns applicable to long word spans. The paper
did not analyse the impact of the technique on ef-
ficiency.
2.2 Evaluation methods
A large number of previous works on word re-
ordering measured their success with general-
purpose metrics such as BLEU (Papineni et al,
2001) or METEOR (Banerjee and Lavie, 2005).
These metrics, however, are only indirectly sensi-
tive to word order and do not sufficiently penalize
long-range reordering errors, as demonstrated for
instance by Birch et al (2010). While BLEU re-
mains a standard choice for many evaluation cam-
paigns, we believe it is extremely important to
complement it with metrics that are specifically
designed to capture word order differences. In this
work, we adopt two reordering-specific metrics in
addition to BLEU and METEOR:
Kendall Reordering Score (KRS). As pro-
posed by Birch et al (2010), the KRS measures
the similarity between the input-output reordering
and the input-reference reordering. This is done by
converting word alignments to permutations and
computing a permutation distance among them.
When interpolated with BLEU, this score is called
LRscore.2
Verb-specific KRS (KRS-V). The ideal way
to automatically evaluate our systems would be
to use syntax- or semantics-based metrics, as the
impact of long reordering errors is particularly
important at these levels. As a light-weight al-
ternative, we instead concentrate the evaluation
on those word classes that are typically crucial
to guess the general structure of a sentence. To
this end, we adopt a word-weighted version of the
2Thus, our KRS results correspond exactly to the
LRscore(?=1) presented in other papers.
441
KRS and set the weights to 1 for verbs and 0 for
all other words, so that only verb reordering errors
are captured. We call the resulting metric KRS-V.
The KRS-V rates a translation hypothesis as per-
fect (100%) when the translations of all source
verbs are located in their correct position, regard-
less of the other words? ordering.
3 Early distortion cost
In its original formulation, the PSMT approach in-
cludes a basic reordering model, called distortion
cost, that exponentially penalizes longer jumps
among consecutively translated phrases simply
based on their distance. Thus, a completely mono-
tonic translation has a total distortion cost of zero.
A weakness of this model is that it penalizes
long jumps only when they are performed, rather
than accumulating their cost gradually. As an ef-
fect, hypotheses with gaps (i. e. uncovered input
positions) can proliferate and cause the pruning
of more monotonic hypotheses that could lead to
overall better translations.
To solve this problem, Moore and Quirk (2007)
proposed an improved version of the distortion
cost function that anticipates the gradual accumu-
lation of the total distortion cost, making hypothe-
ses with the same number of covered words more
comparable with one another. Early distortion
cost (as called in Moses, or ?distortion penalty es-
timation? in the original paper) is computed by a
simple algorithm that keeps track of the uncovered
input positions. Note that this option affects the
distortion feature function, but not the distortion
limit, which always corresponds to the maximum
distance allowed between consecutively translated
phrases.
Early distortion cost was shown by its authors to
yield similar BLEU scores as the standard one but
with stricter pruning parameters, i. e. faster decod-
ing. Experiments were performed on an English-
French task, with a fixed distortion limit of 5 and
without lexicalized reordering models. Our study
deals with a language pair that is arguably more
difficult at the level of reordering. Moreover, we
start from a stronger baseline and measure the im-
pact of early distortion cost in various distortion
limit settings, using also reordering-specific met-
rics. Results are presented in Section 6.2.
4 Word-after-word reordering
modeling and pruning
Phrase orientation (lexicalized reordering) mod-
els (Tillmann, 2004; Koehn et al, 2005; Galley
and Manning, 2008) have proven very useful for
short and medium-range reordering and are prob-
ably the most widely used in PSMT nowadays.
However, their coarse classification of reordering
steps makes them unsuitable to capture long-range
reordering phenomena, such as those attested in
German-English. Indeed, Galley and Manning
(2008) reported a decrease of translation qual-
ity when the distortion limit was set beyond 6 in
Chinese-English and beyond 4 in Arabic-English.
To address this problem, we have developed a
different reordering model that predicts what in-
put word should be translated at a given decod-
ing state (Bisazza, 2013; Bisazza and Federico,
2013). The model is similar to the one proposed
by Visweswariah et al (2011), however we use
it differently: that is, not simply for data pre-
processing but as an additional feature function
fully integrated in the phrase-based decoder. More
importantly, we propose to use the same model
to dynamically shape the space of reorderings ex-
plored during decoding (cf. Section 4.2), which
was never done before.
Another related work is the source-side decod-
ing sequence model by Feng et al (2010), that is
a generative n-gram model trained on a corpus of
pre-ordered source sentences. Although reminis-
cent of a source-side bigram model, our model has
two important differences: (i) the discriminative
modeling framework enables us to design a much
richer feature set including, for instance, the con-
text of the next word to pick; (ii) all our features
are independent from the decoding history, which
allows for an efficient decoder-integration with no
effect on hypothesis recombination.
Finally, we have to mention the models by Al-
Onaizan and Papineni (2006) and Green et al
(2010), who predict the direction and (binned)
length of a jump to perform after a given input
word. Those models too were only used as ad-
ditional feature functions, and were not shown to
maintain translation quality and efficiency at very
high distortion limits.
4.1 The model
The Word-after-word (WaW) reordering model is
trained to predict whether a given input position
442
should be translated right after another, given the
words at those positions and their contexts. It is
based on the following maximum-entropy binary
classifier:
P (Ri,j=Y |fJ1 , i, j) =
exp[
?
m ?mhm(fJ1 , i, j, Ri,j=Y )]?
Y ? exp[
?
m ?mhm(fJ1 , i, j, Ri,j=Y ?)]
where fJ1 is a source sentence of J words, hm are
feature functions and ?m the corresponding fea-
ture weights. The outcome Y can be either 1 or 0,
with Ri,j=1 meaning that the word at position j is
translated right after the word at position i.
Training examples are extracted from a corpus
of reference reorderings, obtained by converting
the word-aligned parallel data into a set of source
sentence permutations. A heuristic similar to the
one proposed by Visweswariah et al (2011) is
used to this end. For each input word, we gen-
erate: (i) one positive example for the word that
should be translated right after it; (ii) negative ex-
amples for all the uncovered words that lie within a
given sampling window or ?. The latter parameter
serves to control the proportion between positive
and negative examples.
The WaW model builds on binary features that
are extracted from the local context of positions
i and j, and from the words occurring between
them. In addition to the actual words, the features
may include POS tags and shallow syntax labels
(i. e. chunk types and boundaries). For instance,
one feature may indicate that the last translated
word (wi) is an adjective while the currently trans-
lated one (wj) is a noun:
POS(wi)=adj ? POS(wj)=noun
Other features indicate that a given word or punc-
tuation is found between wi and wj :
wb=?jedoch? ... wb=?.?
or that wi and wj belong to the same shallow syn-
tax chunk.
The WaW reordering model can be seamlessy
integrated into a standard phrase-based decoder
that already includes phrase orientation models.
When a partial hypothesis is expanded with a
given phrase pair, the model returns the log-
probability of translating its words in the order
defined by the phrase-internal word alignment.
Moreover, the global WaW score is independent
from phrase segmentation, and normalized across
outputs of different lengths.
The complete list of features, training data gen-
eration algorithm and other implementation details
are presented in (Bisazza, 2013) and (Bisazza and
Federico, 2013).
4.2 Early reordering pruning
Besides providing an additional feature function
for the log-linear PSMT framework, the WaW
model?s predictions can be used as an early indi-
cation of whether or not a given reordering path
should be further explored. In fact, we have men-
tioned that the existing reordering models are not
capable of guiding the search through very large
reordering search spaces. As a solution, we pro-
pose to decode with loose reordering constraints
(i. e. high distortion limit) but only explore those
long reorderings that are promising according to
the WaW model.
More specifically, at each hypothesis expansion,
we consider the set of input positions that are
reachable within the fixed distortion limit. Only
based on the WaW score, we apply histogram and
threshold pruning to this set and then proceed to
expand only the non-pruned positions.3 Further-
more, it is possible to ensure that local reorderings
are always allowed, by setting a so-called non-
prunable-zone of width ? around the last covered
input position.4 In this way, we can ensure that the
usual space of short to medium-range reordering is
exhaustively explored in addition to few promising
long-range reorderings.
The rationale of this approach is two-fold: First,
to avoid costly hypothesis expansions for very un-
likely reordering steps and thus speed up decod-
ing under loose reordering constraints. Second, to
decrease the risk of model errors by exploiting the
fact that some components of the PSMT log-linear
model are more important than others at different
stages of the translation process.
The WaW model is not the only scoring func-
tion that can be used for early reordering prun-
ing. In principle, even phrase orientation model
scores could be used, but we expect them to per-
form poorly due to the coarse classification of re-
ordering steps (all phrases that are not adjacent to
the current one are treated as discontinuous steps).
3The idea is reminiscent of early pruning by Moore and
Quirk (2007): an optimization technique that consists of dis-
carding hypothesis extensions based on their estimated score
before computing the exact language model score.
4See (Bisazza, 2013) for technical details on the integra-
tion of word-level pruning with phrase-level hypothesis ex-
pansion.
443
5 Reordering in hierarchical SMT
To allow for a fair evaluation of our systems,
we also perform a contrastive experiment using a
tree-based SMT approach: namely, hierarchical
phrase-based SMT (HSMT) (Chiang, 2005).
Reordering in HSMT is not modeled separately
but is embedded in the translation model itself,
which contains lexicalized, non syntactically mo-
tivated rules that are directly learnt from word-
aligned parallel text. The major strength of HSMT
compared to PSMT, is the ability to learn discon-
tinous phrases and long-range lexicalized reorder-
ing rules. However, this modeling power has a cost
in terms of model size and decoding complexity.
To have a concrete idea, consider that the
phrase-table trained on our SMT training data (cf.
Section 6.1) with a maximum phrase length of 7
contains 127 million entries (before phrase table
pruning). The hierarchical rule table trained on the
same data with a comparable span constraint (10)
contains instead 1.2 billion entries ? one order of
magnitude larger.
Furthermore, the HSMT decoder is based on a
chart parsing algorithm, whose complexity is cu-
bic in the input length, and even higher when tak-
ing into account the target language model. This
issue can be partially addressed by different strate-
gies such as cube pruning (Chiang, 2007), which
reduces the LM complexity to a constant, or rule
application constraints. One of such constraints is
the maximum number of source words that may
be covered by non-terminal symbols (span con-
straint). Setting a span constraint ? which is essen-
tial to obtain reasonable decoding times ? means
preventing long-range reordering similarly to set-
ting a distortion limit in PSMT. In our experi-
ments, we consider two settings for this parameter:
10 to capture short to medium-range reorderings,
and 20 to also capture long-range reorderings.
6 Experiments
In this section we evaluate the impact on transla-
tion quality and efficiency of the techniques pre-
sented above. Our main objective is to empiri-
cally verify the hypothesis that better reordering
modeling and better reordering space definition
can significantly improve the accuracy of PSMT in
German-English without sacrificing its efficiency.
6.1 Experimental setup
We choose the WMT German-English news trans-
lation task as our case study. More specifically
we use the WMT10 training data: Europarl (v.5)
plus News-commentary-2010 for a total of 1.6M
parallel sentences, 44M German tokens. The tar-
get LM is trained on the monolingual news data
provided for the constrained track of WMT10
(1133M English tokens). For development we use
theWMT08 news benchmark, while for testing we
use the following data sets:
tests(09-11): the concatentation of three previous
years? benchmarks from 2009 to 2011 (8017
sentences, 21K German tokens).
test12: the latest released benchmark (3003 sen-
tences, 8K German tokens).
Each data set includes one reference translation.
Note that our goal is not to reach the performance
of the best systems participating at the last WMT
edition, but rather to assess the usefulness of our
techniques on a larger and therefore more reliable
test set, while starting from a reasonable baseline.5
For German tokenization and compound split-
ting we use Tree Tagger (Schmid, 1994) and the
Gertwol morphological analyser (Koskenniemi
and Haapalainen, 1994).6
All our SMT systems are built with the Moses
toolkit (Koehn et al, 2007; Hoang et al, 2009),
and word alignments are generated by the Berke-
ley Aligner (Liang et al, 2006). The target lan-
guage model is estimated by the IRSTLM toolkit
(Federico et al, 2008) with modified Kneser-Ney
smoothing (Chen and Goodman, 1999).
The phrase-based baseline decoder includes a
phrase translation model (two phrasal and two lex-
ical probability features), a lexicalized reorder-
ing model (six features), a 6-gram target language
model, distortion cost, word and phrase penalties.
As lexicalized reordering model, we use a hierar-
chical phrase orientation model (Galley and Man-
ning, 2008) trained on all the parallel data using
three orientation classes ? monotone, swap or dis-
continuous ? in bidirectional mode. Statistically
5Our results on test12 are not directly comparable to the
WMT12 submissions due to the different training data: that
is, the WMT12 parallel data includes 50M German tokens
of Europarl data and 4M of news-commentary, as opposed
to the 41M and 2.5M released for WMT10 and used in our
experiments.
6http://www2.lingsoft.fi/cgi-bin/gertwol
444
improbable phrase pairs are pruned from the trans-
lation model as proposed by Johnson et al (2007).
The hierarchical system is trained and tested
using the standard Moses configuration which in-
cludes: a rule table (two phrasal and two lexi-
cal probability features), a 6-gram target language
model, word and rule penalties. We set the span
constraint (cf. Section 5) to the default value of
10 words for rule extraction, while for decoding
we consider two different settings: the default 10
words and a large value of 20 to enable very long-
range reorderings.
Feature weights for all systems are optimized
by minimum BLEU-error training (Och, 2003) on
test08. To reduce the effects of the optimizer insta-
bility, we tune each configuration four times and
use the average of the resulting weight vectors for
testing, as suggested by Cettolo et al (2011).
The source-to-reference word alignments that
are needed to compute the reordering scores are
generated by the Berkeley Aligner previously
trained on the training data. Source-to-output
alignments are obtained from the decoder?s trace.
6.2 Distortion function and limit
We start by measuring the difference between
standard and early distortion cost.7 Figure 1
shows the results in terms of BLEU and KRS, plot-
ted against the distortion limit (DL).
Indeed, early distortion cost (Moore and Quirk,
2007) outperforms the standard one in all the
tested configurations and according to both met-
rics. We can see that the quality of both systems
deteriorates as the distortion limit increases, how-
ever the system with early distortion cost is more
robust to this effect. In particular, when passing
from DL=12 to DL=18, the baseline system loses
1.2 BLEU and no less than 6.8 KRS, whereas the
system with early distortion cost loses 0.8 BLEU
and 4.9 KRS. Given these results, we decide to use
early distortion cost in all the remaining experi-
ments.
6.3 WaW reordering pruning
We have seen that early distortion cost can effec-
tively reduce the loss of translation quality, but
cannot totally prevent it. Moreover, increasing
the distortion limit means exploring many more
7For this first series of experiments, feature weights are
tuned in the DL=8 setting and the two resulting weight vec-
tors (one for standard, one for early distortion) are re-used in
the higher-DL experiments.
Figure 1: Standard vs early distortion cost perfor-
mance measured in terms of BLEU and KRS on
tests(09-11) under different distortion limits.
hypotheses and, consequently, slowing down the
decoding process. With our WaW model-based
reordering pruning technique, we aim at solving
both issues.
We generate the WaW training data from the
first 30K sentences of the News-commentary-
2010 parallel corpus, using a sampling window of
width ?=10. This results in 8 million training sam-
ples, which are fed to the binary classifier imple-
mentation of the MegaM Toolkit8. Features with
less than 20 occurrences are ignored and the max-
imum number of training iterations is set to 100.
Evaluated intrinsically on test08, the model
achieves the following classification accuracy:
67.0% precision, 50.0% recall, 57.2% F-score.
While these figures are rather low, we recall that
the WaW model is not meant to be used as a stand-
alone classifier, but rather as one of several SMT
feature functions and as a way to detect very un-
likely reordering steps. Hence, we also evaluate its
ability to rank a typical set of reordering options
during decoding: that is, we traverse the source
words in target order and, for each of them, we ex-
8http://www.cs.utah.edu/?hal/megam/ (Daume? III, 2004).
445
tests(09-11) test12 ms/
System DL bleu met krs krs-V bleu met krs krs-V word
Allowing only short to medium-range reordering:
PSMT, early disto 8 19.2 28.1 67.4 65.4 19.0 28.1 67.8 66.1
! 202
+WaW (feature only) 19.4! 28.2! 67.6! 65.5! 19.5! 28.3! 67.8 66.2 212
HSMT, max.span=10 20.1! 28.5! 68.4! 66.7! 19.7" 28.4" 68.6! 67.3! 406
Allowing also long-range reordering:
PSMT, early disto
18
18.2 28.0 62.9 62.0 18.2 28.1 63.4 62.5 408
+WaW (feature only) 18.4! 28.0 61.8# 61.3# 18.1 28.1 62.2# 61.7# 428
+WaW reo.pruning (?=5) 19.5! 28.3! 67.9! 66.3! 19.3! 28.4! 67.8! 66.3! 142
HSMT, max.span=20 20.0! 28.5! 68.1! 66.7! 19.7! 28.4 68.2! 67.1! 706
Table 1: Effects of WaW reordering model and early reordering pruning on PSMT translation quality
and efficiency, compared against a hierarchical SMT baseline. Translation quality is measured with
% BLEU, METEOR, and Kendall Reordering Score: regular (KRS) and verb-specific (KRS-V). Statistically
significant differences with respect to the previous row are marked with !# at the p ? .05 level and "$
at the p ? .10 level. Decoding time is measured in milliseconds per input word.
amine the ranking of all words that may be trans-
lated next (i. e. the uncovered positions within
a given DL). We find that, even when the DL is
very high (18), the correct jump is ranked among
the top 3 reachable jumps in the large majority of
cases (81.4%). If we only consider long jumps ?
i. e. spanning more than 6 words ? the Top-3 accu-
racy is 56.4% while that of a baseline that simply
favors shorter jumps (as the distortion cost does)
is only 26.5%.
For the early reordering pruning experiment, we
set the pruning parameters to 2 for histogram and
0.25 for relative threshold.9 A non-prunable-zone
of width ?=5 is set around the last covered posi-
tion. The resulting configuration is re-optimized
by MERT on test08 for the final experiment.
Table 1 shows the effects of integrating the
WaW reordering model into a PSMT decoder
that already includes a state-of-the-art hierarchi-
cal phrase orientation model. The same table also
presents the results of the HSMT constrastive ex-
periments. Two scenarios are considered: in the
first block, the PSMT distortion limit is set to a
medium value (8) and the HSMT maximum span
constraint is set to 10. Although not directly com-
parable, these settings have the same effect of dis-
allowing long-range reorderings. In the second
block, long-range reorderings are instead allowed
9Pruning parameters were optimized for BLEU with a
grid search over the values (1, 2, 3, 4, 5) for histogram and
(0.5, 0.25, 0.1) for threshold.
with a DL of 18 and a HSMT span constraint of
20.
Feature weights are optimized for each exper-
iment using the procedure described above (four
averaged MERT runs). Statistical significance is
computed for each experiment against the pre-
vious one (i. e. previous row), using approxi-
mate randomization as in (Riezler and Maxwell,
2005). Run times are obtained by an Intel Xeon
X5650 processor on the first 500 sentences of
tests(09-11), excluding loading time of all models.
Medium reordering space. Integrating the
WaW model as an additional feature function
yields small but consistent improvements (second
row of Table 1). Concerning the run time, we no-
tice just a small overload of about 5%: that is, from
202 to 212 ms/word.
In comparison, the tree-based system (third
row) has almost double decoding time but
achieves statistically significant higher translation
quality, especially at the level of reordering.
Large reordering space. As expected, raising
the DL to 18 with no special pruning (fourth row)
results in much slower decoding (from 202 to 408
ms/word) but also in very poor translation qual-
ity. This loss is especially visible on the reordering
scores: e. g. from 67.4 to 62.9 KRS on tests(09-
11). Unfortunately, adding the WaW model as a
feature function (fifth row) does not appear to be
helpful under the high DL condition.
On the other hand, when using the WaW model
446
adv. verbmod subj. obj. compl.
Jedoch konnten sie Kinder in Teilen von Helmand und Kandahar im Su?den aus Sicherheit? grund
SRC however could they children in parts of Helmand and Kandahar in South for security reasons
(de) neg verbinf
nicht erreichen .
not reach
REF But they could not reach children in parts of Helm. and Kand. in the south for security reasons.
BASE-8 However, they were children in parts of Helm. and Kand. in the south, for security reasons.
HIER-10 However, they were children in parts of Helm. and Kand. in the south not reach for security reasons.
BASE-18 However, they were children in parts of Helm. and Kand. in the south do not reach for security reasons.
WAWP-18 However, they could not reach children in parts of Helm. and Kand. in the south for security reasons.
HIER-20 However, they were children in parts of Helm. and Kand. in the south not reach for security reasons.
Table 2: Long-range reordering example showing the behavior of different systems: [BASE-*] are phrase-
based systems with a DL of 8 and 18 respectively; [WAWP-18] refers to the WaW-pruning PSMT system;
[HIER-*] are hierarchical SMT systems with a span constraint of 10 and 20 words respectively.
also for reordering pruning (sixth row) we are able
to recover the performance of the medium-DL
baseline performance and even to slightly improve
it. It is interesting to note that the largest improve-
ment concerns the accuracy of verb reordering on
tests(09-11): from 65.4 to 66.3 KRS-V. Although
the other gains are rather small, we emphasize the
fact that our solutions mostly affect rare and iso-
lated events, which have a limited impact on the
general purpose evaluation metrics but are are es-
sential to produce readable translations. WaW re-
ordering pruning has also a remarkable effect on
efficiency, making decoding time decrease from
428 ms/word to 142 ms/word, that is even faster
than a baseline that does not explore any long-
range reordering at all (202 ms/word).
Finally, we can see from the last row of Ta-
ble 1 that the gap between PSMT and HSMT has
been narrowed significantly. While more work is
needed to reach and outperform the quality of the
HSMT system, we were able to closely approach
it with five times lower decoding time (142 versus
706 ms/word) and about ten times smaller mod-
els (cf. Section 5). Comparing our best system
with the best HSMT system (i. e. span constraint
10), we see that the gap in translation accuracy
is slightly larger and that the decoding speed-up
is smaller (142 versus 406 ms/word). However,
the better performance and efficiency of HSMT-10
comes at the expense of all long-range reorderings.
Thus, our enhanced PSMT appears as an opti-
mal choice in terms of trade-off between transla-
tion quality and efficiency.
Table 3 reports two kinds of decoding statistics
that allow us to explain the very different decod-
ing times observed, and to verify that the WaW-
pruning system actually performs long-range re-
orderings: #hyp/sent is the average number of
partial translation hypotheses created10 per test
sentence; (#jumps/sent)?100 is the average
number of phrase-to-phrase jumps included in
the 1-best translation of every 100 test sentences.
Only medium and long jumps are shown (distor-
tion D?6), divided into three distortion buckets.
System DL #hyp/sent (#jumps/sent)?100D: [6..8] [9..12] [13..18]
baseline 8 600K 90 ? ?
baseline 18 1278K 88 61 48
+WaW r.prun. 18 364K 52 29 17
Table 3: Decoding statistics of three PSMT sys-
tems exploring different reordering search spaces
for the translation of test12.
We can see that the early-pruning system in-
deed performed several long jumps but it explored
a much smaller search space compared to the high-
distortion baseline (364K versus 1278K partial hy-
potheses). As for the lower number of long jumps
(e. g. 29 versus 61 with D in [9..12] and 17 versus
48 in [13..18]) it suggests that the early-pruning
system is more precise, while the high-distortion
baseline is over-reordering.
The output of different systems for our exam-
ple sentence is shown in Table 2. In this sentence,
a jump forward with D=12 and a jump backward
with D=14 were necessary to achieve the correct
reordering of the verb and its negation. Although
10That is, the hypotheses that were scored by all the PSMT
model components and added to a hypothesis stack.
447
Figure 2: Effects of beam size on translation quality measured by BLEU, KRS and KRS-V, in two base-
line PSMT systems (DL=8 and DL=18) and in the WaW early-pruning system (test12). For comparison,
the hierarchical system performance (span constraint 20) is provided as a dotted line.
these jumps were reachable for both the [PSMT-
18] and the [HSMT-20] systems, only the WaW-
pruning PSMT system actually performed them.
6.4 Interaction with beam-search pruning
During the beam-search decoding process, early
reordering pruning interacts with regular hypoth-
esis pruning based on the weighted sum of all
model scores. In particular, all the PSMT systems
presented so far apply a default histogram thresh-
old of 200 to each hypothesis stack. To examine
this interaction, we increase the histogram thresh-
old (beam size) from the default value of 200 up to
800, while keeping all other parameters and fea-
ture weights fixed. The results on test12 are plot-
ted against the beam size and reported in Figure 2.
The dotted line in each plot represents the perfor-
mance of the hierarchical system presented in the
last row of Table 1 (span constraint 20).
We can see that increasing the beam size is more
beneficial for the high-DL baseline (baseDL18)
than for the medium one (baseDL8). This is not
surprising as the risk of search error is higher when
a larger search space is explored with equal mod-
els and pruning parameters. Nevertheless, bas-
eDL18 remains by far the worst performing sys-
tem, even in our largest beam setting (800) corre-
sponding to four times longer decoding time (1582
ms/word). What is remarkable, instead, is that
the larger beam size also results in better perfor-
mances by the WaW-pruning system, which is the
PSMT system that explores by far the smallest
search space (cf. Table 3). The superiority of the
WaW-pruning system over the PSMT baselines is
maintained in all tested settings and according to
all metrics, which confirms the usefulness of our
methods not only as optimization techniques, but
also for reducing model errors of a baseline that
already includes strong reordering models.
With a very large beam size (800) our en-
hanced PSMT system can closely approach the
performance of HSMT-20 in terms of BLEU and
KRS-V, and even surpass it in terms of KRS (sta-
tistically significant) while still remaining faster:
that is, 554 versus 706 ms/word.
Overall HSMT-10 remains the best system, with
slightly higher KRS and KRS-V and lower de-
coding time than our best enhanced PSMT sys-
tem (406 versus 554 ms/word). However, we note
once more that this performance comes at the ex-
pense of all long-range reorderings. For a com-
pletely fair comparison, the HSMT system should
also be enhanced with similar reordering-pruning
techniques ? a research path that we plan to ex-
plore in the future, possibly inspiring from the ap-
proach of Braune et al (2012).
7 Conclusions
We have presented a few techniques that can im-
prove the accuracy of the word reordering per-
formed by a German-English phrase-based SMT
system. In particular, we have shown how long-
range reorderings can be captured without worsen-
ing the general quality of translation and without
renouncing to efficiency. Our best PSMT system
is actually faster than a system that does not even
attempt to perform long-range reordering, and it
448
obtains significantly higher evaluation scores.
In comparison to a more computationally costly
tree-based approach (hierarchical SMT), our en-
hanced PSMT system produces slightly lower
translation quality but in five times lower decod-
ing time when long-range reordering is allowed.
Moreover, when a larger beam size is explored,
the performance of our system can equal that of
the long-reordering hierarchical system, but still
with faster decoding.
In summary, we have shown that an appropri-
ate modeling of the word reordering problem can
lead to narrow or even fill the gap between phrase-
based and hierarchical SMT in this difficult lan-
guage pair. We have also disproved the common
belief that sacrificing long-range reorderings by
setting a low distortion limit is the only way to
obtain well-performing PSMT systems.
Acknowledgments
This work was partially funded by the European
Union under FP7 grant agreement EU-BRIDGE,
Project Number 287658.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 529?536, Sydney, Australia, July.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting success in machine translation. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 745?
754, Stroudsburg, PA, USA.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating re-
ordering. Machine Translation, 24(1):15?26.
Arianna Bisazza and Marcello Federico. 2013. Dy-
namically shaping the reordering search space of
phrase-based statistical machine translation. To ap-
pear in Transactions of the ACL.
Arianna Bisazza. 2013. Linguistically Motivated
Reordering Modeling for Phrase-Based Statistical
Machine Translation. Ph.D. thesis, University of
Trento. http://eprints-phd.biblio.unitn.it/1019/.
Fabienne Braune, Anita Gojun, and Alexander Fraser.
2012. Long-distance reordering during search for
hierarchical phrase-based SMT. In Proceedings of
the Annual Conference of the European Associa-
tion for Machine Translation (EAMT), pages 28?30,
Trento, Italy.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June.
Mauro Cettolo, Nicola Bertoldi, and Marcello Fed-
erico. 2011. Methods for smoothing the optimizer
instability in SMT. In MT Summit XIII: the Thir-
teenth Machine Translation Summit, pages 32?39,
Xiamen, China.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
4(13):359?393.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
263?270, Ann Arbor, Michigan, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 531?540, Ann Arbor,
Michigan, June.
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper avail-
able at http://pub.hal3.name, implementa-
tion available at http://hal3.name/megam.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an Open Source Toolkit
for Handling Large Scale Language Models. In
Proceedings of Interspeech, pages 1618?1621, Bris-
bane, Australia.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A source-side decoding sequence model for statis-
tical machine translation. In Conference of the As-
sociation for Machine Translation in the Americas
(AMTA), Denver, Colorado, USA.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP ?08: Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 848?856, Morristown, NJ, USA.
449
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of the 23rd International
Conference on Computational Linguistics, COLING
?10, pages 376?384, Stroudsburg, PA, USA.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English-to-
German SMT. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association
for Computational Linguistics, pages 726?735, Avi-
gnon, France, April.
Spence Green, Michel Galley, and Christopher D.Man-
ning. 2010. Improved models of distortion cost
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL), pages 867?
875, Los Angeles, California.
Hieu Hoang and Philipp Koehn. 2010. Improved trans-
lation with source syntax labels. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 409?417, Up-
psala, Sweden, July.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
International Workshop on Spoken Language Trans-
lation (IWSLT), pages 152?159, Tokyo, Japan.
H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007.
Improving translation quality by discarding most
of the phrasetable. In In Proceedings of EMNLP-
CoNLL 07, pages 967?975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127?133, Ed-
monton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proc. of the International Workshop on Spoken
Language Translation, October.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages
177?180, Prague, Czech Republic.
Kimmo Koskenniemi and Mariikka Haapalainen,
1994. GERTWOL ? Lingsoft Oy, chapter 11, pages
121?140. Roland Hausser, Niemeyer, Tu?bingen.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104?111, New York City,
USA, June.
Robert C. Moore and Chris Quirk. 2007. Faster
beam-search decoding for phrasal statistical ma-
chine translation. In In Proceedings of MT Summit
XI, pages 321?327, Copenhagen, Denmark.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 295?302, Philadelhpia, PA.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Erhard
Hinrichs and Dan Roth, editors, Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Research Report
RC22176, IBM Research Division, Thomas J. Wat-
son Research Center.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57?64, Ann Arbor, Michigan, June.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the
North American Chapter of the Association of Com-
putational Linguistics (HLT-NAACL).
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1007?1016,
Singapore, August.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for im-
proved machine translation. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 486?496, Edinburgh,
Scotland, UK., July.
450
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of Coling 2004,
pages 508?514, Geneva, Switzerland, Aug 23?Aug
27. COLING.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In 25th German Con-
ference on Artificial Intelligence (KI2002), pages
18?32, Aachen, Germany. Springer Verlag.
451
Workshop on Humans and Computer-assisted Translation, pages 84?92,
Gothenburg, Sweden, 26 April 2014.
c?2014 Association for Computational Linguistics
Online Word Alignment for Online Adaptive Machine Translation
M. Amin Farajian
FBK-irst,
University of Trento
Trento, Italy
farajian@fbk.eu
Nicola Bertoldi
FBK-irst
Trento, Italy
bertoldi@fbk.eu
Marcello Federico
FBK-irst
Trento, Italy
federico@fbk.eu
Abstract
A hot task in the Computer Assisted
Translation scenario is the integration of
Machine Translation (MT) systems that
adapt sentence after sentence to the post-
edits made by the translators. A main
role in the MT online adaptation process is
played by the information extracted from
source and post-edited sentences, which
in turn depends on the quality of the
word alignment between them. In fact,
this step is particularly crucial when the
user corrects the MT output with words
for which the system has no prior infor-
mation. In this paper, we first discuss
the application of popular state-of-the-art
word aligners to this scenario and reveal
their poor performance in aligning un-
known words. Then, we propose a fast
procedure to refine their outputs and to
get more reliable and accurate alignments
for unknown words. We evaluate our
enhanced word-aligner on three language
pairs, namely English-Italian, English-
French, and English-Spanish, showing a
consistent improvement in aligning un-
known words up to 10% absolute F-
measure.
1 Introduction
In the adaptive MT the goal is to let the MT system
take as soon and as much as possible advantage of
user feedback, in order to learn from corrections
and to hence avoid repeating the same mistakes in
future sentences.
A typical application scenario is the usage by
a professional translator of a Computer Assisted
Translation (CAT) tool enhanced with a SMT sys-
tem. For each input sentence, first the translator
receives one or more translation suggestions from
either a Translation Memory or a SMT system,
then (s)he chooses which suggestion is more use-
ful, and finally (s)he creates an approved transla-
tion by post-editing. The pair of input sentence
and post-edit is a valuable feedback to improve the
quality of next suggestions. While the sentence
pair is trivially added to the Translation Memory,
how to exploit it for improving the SMT system is
far to be a solved problem, but rather is a hot and
quite recent topic in the MT community.
In online MT adaptation specific issues have to
be addressed, which distinguish it from the more
standard and investigated task of domain adapta-
tion. First of all, the SMT system should adapt
very quickly, because the time between two con-
secutive requests are usually short, and very pre-
cisely, because the translator is annoyed by cor-
recting the same error several time. Then, a crucial
point is which and how information is extracted
from the feedback, and how it is exploited to up-
date the SMT system. Finally, model updating re-
lies on a little feedback consisting of just one sen-
tence pair.
In this work we focus on the word alignment
task which is the first and most important step in
extracting information from the given source and
its corresponding post-edit. In particular, we are
interested in the cases where the given sentence
pairs contain new words, for which no prior infor-
mation is available. This is an important and chal-
lenging problem in the online scenario, in which
the user interacts with the system and expects that
it learns from the previous corrections and does
not repeat the same errors again and again.
Unfortunately, state-of-the-art word-aligners
show poor generalization capability and are prone
to errors when infrequent or new words occur in
the sentence pair. Word alignment errors at this
stage could cause the extraction of wrong phrase
pairs, i.e. wrong translation alternatives, which
can lead in producing wrong translations for those
84
words, if they appear in the following sentences.
Our investigation focuses on how to quickly
build a highly precise word alignment from a
source sentence and its translation. Moreover, we
are interested in improving the word alignment of
unknown terms, i.e. not present in the training
data, because they are one of the most important
source of errors in model updating.
Although we are working in the online MT
adaptation framework, our proposal is worthwhile
per se; indeed, having an improved and fast word
aligner can be useful for other interesting tasks,
like for instance terminology extraction, transla-
tion error detection, and pivot translation.
In Section 2 we report on some recent ap-
proaches aiming at improving word alignment. In
Section 3, we describe three widely used toolk-
its, highlight their pros and cons in the online
MT adaptation scenario, and compare their per-
formance in aligning unknown terms. In Section 4
we propose a standalone module which refines the
word alignment of unknown words; moreover, we
present an enhanced faster implementation of the
best performing word aligner, to make it usable in
the online scenario. In Section 5 we show exper-
imental results of this module on three different
languages. Finally, we draw some final comments
in Section 6.
2 Related works
Hardt et al. (2010) presented an incremental re-
training method which simulates the procedure
of learning from post-edited MT outputs (refer-
ences), in a real time fashion. By dividing the
learning task into word alignment and phrase ex-
traction tasks, and replacing the standard word-
alignment module, which is a variation of EM
algorithm (Och and Ney, 2003), with a greedy
search algorithm, they attempt to find a quick ap-
proximation of the word alignments of the newly
translated sentence. They also use some heuris-
tics to improve the obtained alignments, without
supporting it with some proofs or even providing
some experimental results. Furthermore, the run-
ning time of this approach is not discussed, and it
is not clear how effective this approach is in online
scenarios.
Blain et al. (2012) have recently studied the
problem of incremental learning from post-editing
data, with minimum computational complexity
and acceptable quality. They use the MT out-
put (hypothesis) as a pivot to find the word align-
ments between the source sentence and its corre-
sponding reference. Similarly to (Hardt and Elm-
ing, 2010), once the word alignment between the
source and post-edit sentence pair is generated,
they use the standard phrase extraction method
to extract the parallel phrase pairs. This work
is based on an implicit assumption that MT out-
put is reliable enough to make a bridge between
source and reference. However, in the real world
this is not always true. The post-editor sometimes
makes a lot of changes in the MT output, or even
translates the entire sentence from scratch, which
makes the post-edit very different from the auto-
matic translation. Moreover, in the presence of
new words in the source sentence, the MT system
either does not produce any translation for the new
word, or directly copies it in the output. Due to
the above two reasons, there will be missing align-
ments between the automatic translation and post-
edit, which ultimately results in incomplete paths
from source to post-edit. But, the goal here is to
accurately align the known words, as well as learn-
ing the alignments of the new words, which is not
feasible by this approach.
In order to improve the quality of the word
alignments McCarley et al. (2011) proposed a
trainable correction model which given a sentence
pair and their corresponding automatically pro-
duced word alignment, it tries to fix the wrong
alignment links. Similar to the hill-climbing ap-
proach used in IBM models 3-5 (Brown et al.,
1993), this approach iteratively performs small
modifications in each step, based on the changes
of the previous step. However, the use of addi-
tional sources of knowledge, such as POS tags of
the words and their neighbours, helps the system
to take more accurate decisions. But, requiring
manual word alignments for learning the align-
ment moves makes this approach only applicable
for a limited number of language pairs for which
manual aligned gold references are available.
Tomeh et al. (2010) introduced a supervised
discriminative word alignment model for produc-
ing higher quality word alignments, which is
trained on a manually aligned training corpus. To
reduce the search space of the word aligner, they
propose to provide the system with a set of au-
tomatic word alignments and consider the union
of these alignments as the possible search space.
This transforms the word alignment process into
85
the alignment refinement task in which given a set
of automatic word alignments, the system tries to
find the best word alignment points. Similar to
(McCarley et al., 2011), this approach relies on the
manually annotated training corpora which is not
available for most of the language pairs.
3 Word Alignment
Word alignment is the task of finding the corre-
spondence among the words of a sentence pair
(Figure 1). From a mathematical point of view,
it is a relation among the words, because any word
in a sentence can be mapped into zero, one or
more words of the other, and vice-versa; in other
words, any kind of link is allowed, namely one-to-
one, many-to-one, many-to-many, as well as leav-
ing words unaligned. So called IBM models 1-5
(Brown et al., 1993) as well as the HMM-based
alignment models (Vogel et al., 1996), and their
variations are extensively studied and widely used
for this task. They are directional alignment mod-
els, because permit only many-to-one links; but
often the alignments in the two opposite directions
are combined in a so-called symmetrized align-
ment, which is obtained by intersection, union or
other smart combination.
Nowadays, word-aligners are mostly employed
in an intermediate step of the training procedure
of a SMT system; In this step, the training cor-
pus is word aligned as a side effect of the es-
timation of the alignment models by means of
the Expectation-Maximization algorithm. For this
task, they perform sufficiently well, because the
training data are often very large, and the limited
amount of alignment errors do not have strong im-
pact on the estimation of the translation model.
Instead, the already trained word-aligners are
rarely applied for aligning new sentence pairs. In
this task their performance are often not satisfac-
tory, due to their poor generalization capability;
they are especially prone to errors when infrequent
or new words occur in the sentence pair.
This is the actual task to be accomplished in the
online adaptive scenario: as soon as a new source
and post-edited sentence pair is available, it has
to be word aligned quickly and precisely. In this
scenario, the sentence pair likely does not belong
to the training corpus, hence might contain infre-
quent or new words, for which the aligner has little
or no prior information.
3.1 Evaluation Measures
A word aligner is usually evaluated in terms of
Precision, Recall, and F-measure (or shortly F ),
which are defined as follows (Fraser and Marcu,
2007):
Precision =
|A
?
P |
|A|
, Recall =
|A
?
S|
|S|
F ?measure =
1
?
Precision
+
1??
Recall
where A is the set of automatically computed
alignments, and S and P refer to the sure (un-
ambiguous) and possible (ambiguous) manual
alignments; note that S ? P . In this paper, ? is
set to 0.5 for all the experiments, in order to have
a balance between Precision and Recall.
In this paper we are mainly interested how the
word-aligner performs on the unknown words;
hence, we define a version of Precision, Recall,
and F metrics focused on the oov-alignment only,
i.e. the alignments for which either the source or
the target word is not included in the training cor-
pus. The subscript all identifies the standard met-
rics; the subscript oov identifies their oov-based
versions.
In Figure 1 we show manual and automatic
word alignments between an English-Italian sen-
tence pair. A sure alignment, like are-sono, is rep-
resented by a solid line, and a possible alignment,
like than-ai, by a dash line. An oov-alignment,
like that linking the unknown English word de-
ployable to the Italian word attivabili, is identi-
fied by a dotted line. According to this example,
Precision and Recall will be about 0.85 (=11/13)
and 0.91 (=10/11), respectively, and the corre-
sponding F is hence about 0.88. Focusing on the
oov-alignment only, Precision
oov
is 1.00 (=1/1),
Recall
oov
is 0.50 (=1/2), and F
oov
is 0.67.
3.2 Evaluation Benchmark
In this paper, we compare word-alignment perfor-
mance of three word-aligners introduced in Sec-
tion 3.3 on three distinct tasks, namely English-
Italian, English-French, and English-Spanish; the
training corpora, common to all word-aligners, are
subset of the JRC-legal corpus
1
(Steinberger et
al., ), of the Europarl corpus V7.0 (Koehn, 2005),
and of the Hansard parallel corpus
2
, respectively.
1
langtech.jrc.it/JRC-Acquis.html
2
www.isi.edu/natural-language/
download/hansard/index.html
86
financial assistance mechanisms are less rapidly deployable than conventional budgetary mechanisms
i meccanismi diassistenza finanziaria sono attivabili meno rapidamente rispetto ai meccanismi bilancio convenzionalidi
financial assistance mechanisms are less rapidly deployable than conventional budgetary mechanisms
i meccanismi diassistenza finanziaria sono attivabili meno rapidamente rispetto ai meccanismi bilancio convenzionalidi
Figure 1: Example of manual (above) and automatic (below) word alignments between an English-Italian
sentence pair. Sure and possible alignments are identified by solid and dash lines, respectively, and the
oov-alignments by a dotted line. The OOV words, like deployable (English) and finanziaria (Italian), are
printed in italics.
Statistics of the three training corpora are reported
in Table 1.
En-It En-Fr En-Es
Segments 940K 1.1M 713K
Tokens
src
19.8M 19.8M 19.8M
Tokens
trg
20.3M 23.3M 20.4M
Table 1: Statistics of the training corpora
for English-Italian, English-French, and English-
Spanish tasks.
Three evaluation data sets are also available,
which belong to the same domains of the cor-
responding training corpora. The English-Italian
test set was built by two professional translators
by correcting an automatically produced word-
alignment. The English-French test set is the man-
ually aligned parallel corpus introduced in (Och
and Ney, 2000)
3
. The English-Spanish test set was
provided by (Lambert et al., 2005)
4
. Statistics of
the three test sets are reported in Table 2.
To have a better understanding of the behavior
of the word aligners on the unknown words, we
created new test sets with an increasing ratio of the
unknown words (oov-rate), for each task. Starting
from each of the original test set, we replaced an
increasing portion of randomly chosen words by
strings which do not exist in the training corpus;
the oov-noise artificially introduced ranges from
3
www.cse.unt.edu/
?
rada/wpt/data/
English-French.test.tar.gz
4
www.computing.dcu.ie/
?
plambert/data/
epps-alignref.html
En-It En-Fr En-Es
Segments 200 484 500
Tokens
src
6,773 7,681 14,652
Tokens
trg
7,430 8,482 15,516
oov-rate
src
0.90 0.27 0.35
oov-rate
trg
0.84 0.34 0.32
#alignment 7,380 19,220 21,442
Table 2: Staticts of the test corpora for English-
Italian, English-French, and English-Spanish
tasks. oov-rate
src
and oov-rate
trg
are the ratio of
the new words in the source and target side of the
test corpus, respectively.
1% to 50%. For each value of the artificial oov-
noise (m = 1, ..., 50), we randomly selected m%
words in both the source and target side indepen-
dently, and replaced them by artificially created
strings. For selecting the words to be replaced
by artificially created strings, we do not differenti-
ate between the known and unknown words; hence
the actual oov-rate in the test corpus, used in the
plots, might be slightly larger.
To further make sure that the random selection
of the words does not affect the systems, for each
oov-noise we created 10 different test corpora and
reported the averaged results. One might think of
other approaches for introducing oov-noise, such
as replacing singletons or low-frequency words
which have more potential to be unknown, instead
of randomly selection of the words. But in this pa-
per we decided to follow the random selection of
the words.
87
3.3 State-of-the-art Word Aligners
We consider three widely-used word aligners,
namely berkeley, fast-align, and mgiza++. We
analyze their performance in aligning an held-out
test corpora; in particular, we compare their capa-
bility in handling the unknown words. For a fair
comparison, all aligners are trained on the same
training corpora described in Section 3.2.
berkeley aligner (Liang et al., 2006) applies the
co-training approach for training the IBM model
1 and HMM. We trained berkeley aligner using
5 iterations of model 1 followed by 5 iterations
of HMM. When applied to new sentence pairs,
the system produces bi-directional symmetrized
alignment.
fast-align is a recently developed unsuper-
vised word aligner that uses a log-linear re-
parametrization of IBM model 2 for training the
word alignment models (Dyer et al., 2013). We
exploited the default configuration with 5 itera-
tions for training. As the system is directional, we
trained two systems (source-to-target and target-
to-source). When applied to new sentence pairs,
we first produced the two directional alignments,
and then combined them into a symmetrized align-
ment by using the grow-diag-final-and heuristic
(Och and Ney, 2003).
mgiza++ (Gao and Vogel, 2008) and its an-
cestors, i.e. giza, and giza++, implement all the
IBM models and HMM based alignment models.
mgiza++ is a multithreaded version of giza++,
which enables an efficient use of multi-core plat-
forms. We trained the system using the follow-
ing configuration for model iterations: 1
5
h
5
3
3
4
3
.
mgiza++ also produces directional alignment;
hence, we followed the same protocol to create a
symmetrize alignment of sentence pairs as we did
for fast-align.
Differently from berkeley and fast-align,
mgiza++ somehow adapts its models when
applied to new sentence pairs. According to
the so-called ?forced alignment?, it essentially
proceeds with the training procedure on these
new data starting from pre-trained and pre-loaded
models, and produces the alignment as a by-
product. In preliminary experiments, we observed
that performing 3 iterations of model 4 is the
best configuration for mgiza++ to align the new
sentence pairs.
These word aligners are designed to work in of-
fline mode; they load the models and align the
whole set of available input data in one shot. How-
ever, in the online scenario where a single sen-
tence pair is provided at a time, they need to reload
the models every time which is very expensive in
terms of I/O operations. In this paper we first
were interested in measuring the quality of the
word aligners to select the best one. Therefore,
we mimic the online modality by forcing them to
align one sentence pair at a time.
Precision Recall F-measure
all oov all oov all oov
English-Italian
fast-align 82.6 33.3 82.8 19.6 82.7 24.7
berkeley 91.9 ? 81.0 ? 86.1 ?
mgiza++ 86.2 84.6 89.4 30.8 87.8 45.2
English-French
fast-align 81.5 47.2 91.8 19.5 86.3 27.6
berkeley 87.9 ? 92.9 ? 90.3 ?
mgiza++ 89.0 88.2 96.0 17.2 92.4 28.8
English-Spanish
fast-align 81.5 31.3 71.8 12.7 76.3 18.1
berkeley 88.7 ? 71.2 ? 79.0 ?
mgiza++ 89.2 95.5 80.6 35.6 84.7 51.9
Table 3: Comparison of different widely-used
word aligners in terms of precision, recall, and F-
measure on English-Italian, English-French, and
English-Spanish language pairs. Columns all re-
port the evaluation performed on all alignments,
while columns oov the evaluation performed on
the oov-alignments.
The three word aligners were evaluated on the
three tasks introduced in Section 3.2. Table 3
shows their performance on the full set of align-
ments (all) and on the subset of oov-alignments
(oov) in terms of Precision, Recall, and F-measure.
The figures show that all aligners perform well on
the whole test corpus. mgiza++ is definitely su-
perior to fast-align; it also outperforms berkeley
in terms of F-measure, but they are comparable in
terms of Precision.
Unfortunately, the quality of the word align-
ments produced for the new words is quite poor for
all systems. mgiza++ outperforms the other align-
ers in all the language pairs on oov-alignments,
and in particular it achieves a very high preci-
sion. On the contrary, berkeley aligner always fails
to detect out-of-vocabulary words; its precision is
hence undefined, and consequently its F-measure.
To our knowledge of the system, this behavior is
expected because of the joint alignment approach
used in berkeley which produces an alignment be-
tween two terms if both the directional models
88
 30 40
 50 60
 70 80
 90
 1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
OOV rate
English-Italian
mgizaberkeleyfast-align  30 40
 50 60
 70 80
 90
 0.5  1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
OOV rate
English-French
mgizaberkeleyfast-align  30 40
 50 60
 70 80
 90
 0.5  1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
OOV rate
English-Spanish
mgizaberkeleyfast-align
 10 15 20
 25 30 35
 40 45 50
 1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
-
o
o
v
OOV rate
English-Italianmgizafast-align
 10 15 20
 25 30 35
 40 45 50
 0.5  1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
-
o
o
v
OOV rate
English-Frenchmgizafast-align
 10 15 20
 25 30 35
 40 45 50
 0.5  1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
-
o
o
v
OOV rate
English-Spanishmgizafast-align
Figure 2: Performance in terms of standard F-measure (above) and oov-based F-measure (below) of the
word aligners on test sets with increasing oov-rate, for all language pairs. The oov-based F-measure for
berkeley is not reported because it is undefined.
agree, and this hardly occurs for unknown words.
To further investigate the behavior of the word
aligners on the unknown words, we evaluated their
performance on the artificially created test sets,
described in Section 3.2. The performance of the
word aligners in terms of standard and oov-based
F-measure is shown in Figure 2. As expected, the
overall F-measure decreases by introducing un-
known words. mgiza++ is more accurate than the
other aligners up to oov-rate of 16%.
We observe that mgiza++ outperforms the oth-
ers in terms of the oov-based F-measure on
the English-Italian and English-Spanish language
pairs up to oov-noise of 32% and 16%, respec-
tively. fast-align instead performs better in the
English-French task. fast-align always show a
better quality when the oov-rate is very high.
oov-based F-measure is not reported for berke-
ley because this aligner is not able to detect oov-
alignments as explained above.
4 Enhancement to Word Alignment
4.1 Refinement of oov-alignments
To address the problem of unaligned new words,
we present a novel approach, in which the word
alignments of the source and target segment pair
are induced in two-steps. First, a standard word
aligner is applied; most of the words in the source
and target sentence pair will be aligned, but most
of the unknown words will not. It is worth men-
tioning that aligning unknown words in this step
depends on the quality of the employed word
aligner. Once the alignments are computed and
symmetrized (if required), phrase extraction pro-
cedure is applied to extract all valid phrase-pairs.
Note that un-aligned words are included in the ex-
tracted phrase pairs, if their surrounding words are
aligned.
It has been shown that inclusion of un-aligned
words in the phrase-pairs, generally, has neg-
ative effects on the translation quality and can
produce errors in the translation output (Zhang
et al., 2009). Nevertheless, the overlap among
phrase-pairs, which contain un-aligned unknown
words, can be considered as a valuable source
of knowledge for inducing the correct alignment
of these words. To get their alignments from
the extracted phrase-pairs we follow an approach
similar to (Espl?a-Gomis et al., 2012) in which
the word alignment probabilities are determined
by the alignment strength measure. Given the
source and target segments (S = {s
1
, . . . , s
l
}
and T = {t
1
, . . . , s
m
}), and the set of extracted
parallel phrase-pairs (?), the alignment strength
A
i,j
(S, T,?) of the s
i
and t
j
can be calculated as
follows:
A
i,j
(S, T,?) =
?
(?,?)??
cover(i, j, ?, ?)
|?|.|? |
cover(i, j, ?, ?) =
{
1 if s
i
? ? and t
j
? ?
0 otherwise
where |?| and |? | are the source and target
lengths (in words) of the phrase pair (?, ?).
89
cover(i, j, ?, ?) simply spots whether the word-
pair (s
i
, t
j
) is covered by the phrase pair (?, ?).
The alignment strengths are then used to pro-
duce the a directional source-to-target word align-
ments; s
i
is aligned to t
j
if A
i,j
> 0 and A
i,j
?
A
i,k
, ?k ? [1, |T |]. One-to-many alignment is
allowed in cases that multiple target words have
equal probabilities to be aligned to i-th source
word (A
i,j
= A
i,k
). The directional word align-
ments are then symmetrized.
The new set of symmetrized alignments can be
used in different ways: (i) as a replacement of the
initial word alignments as in (Espl?a-Gomis et al.,
2012), or (ii) as additional alignment points to be
added to the initial set. According to a prelim-
inary investigation, we choose the latter option:
only a subset of the new word alignments is used
for updating the initial alignments. More specifi-
cally, we add only the alignments of the new words
which are not already aligned.
Moreover, our approach differs from that pro-
posed by Espl?a-Gomis et al. (2012) in the proce-
dure to collect the original set of phrase pairs from
the source and target sentence pair. They rely on
the external sources of information such as online
machine translation systems (e.g. Google Trans-
late, and Microsoft Translator). Communicating
with external MT systems imposes some delays
to the pipeline, which is not desired for the on-
line scenario. Furthermore, the words that are not
known by the machine translation systems are not
covered by any phrase-pair, hence the refinement
module is not able to align them.
We instead employ the phrase-extract software
5
provided by the Moses toolkit, which relies on the
alignment information of the given sentence pair,
and allows the inclusion of un-aligned unknown
words in the extracted phrase pairs; hence, the re-
finement module has the potential to find the cor-
rect alignment for those words.
Note that there is no constraint on the word
alignment and phrase extraction modules used in
the first step, hence, any word aligner and phrase
extractor can be used for computing the initial
alignments and extracting the parallel phrase pairs
from the given sentence pairs. But, since the out-
puts of the first aligner make the ground for obtain-
ing the alignments of the second level, they need
to be highly accurate and precise.
5
The ?grow-diag-final-and? heuristic was set for the sym-
metrization.
4.2 onlineMgiza++
The experiments to compare state-of-the-art word
aligners, reported and discussed in Section 3, are
carried out offline. This is because the aforemen-
tioned word aligners are not designed to work on-
line, and need to load the models every time re-
ceives a new sentence pair. Loading the models is
very time consuming, and depending on the size
of the models might take several minutes, which
is not desired for the online scenario.
To overcome this problem, we decided to im-
plement an online version of mgiza++ which
provides the best performance as shown in Sec-
tion 3.3. This new version, called onlineM-
giza++, works in client-server mode. It con-
sists of two main modules mgizaServer and mgiza-
Client. mgizaServer is responsible for computing
the alignment of the given sentence pairs. To avoid
unnecessary I/O operations, mgizaServer loads all
the required models once at the beginning of the
alignment session, and releases them at the end.
mgizaClient communicates with the client appli-
cations through the standard I/O channel.
In our final experiments we observed some
unexpected differences between the results of
mgiza++ and onlineMgiza++. Therefore, we do
not present the results of onlineMgiza++ in this
paper. However, we expect the two systems pro-
duce the same results.
5 Experimental Results
In this section we evaluate the effectiveness of
the proposed refinement module. Each consid-
ered word aligner was equipped by our refinement
module, and compared to its corresponding base-
line. Figure 3 shows the oov-based F-measure
achieved by the baseline and enhanced word align-
ers on all test sets and all tasks. We observe that
the refinement module consistently improves the
F-measure of all aligners on all language pairs;
The improvement for mgiza++ are big (up to
10%) for very low oov-rates and decreases when
the oov-rate increases; the same but smaller be-
havior is observed for fast-align. This is due to the
fact that by inserting more oov words into the test
sets the systems are able to produce less accurate
alignment points, which leads in lower contextual
information (i.e. smaller number of overlapping
phrase-pairs) for aligning the unknown words. In-
terestingly, the refinement module applied to the
berkeley output permits the correct detection of
90
 0 10 20
 30 40 50
 60 70 80
 1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
-
o
o
v
OOV rate
English-Italianmgiza + enhberkeley + enhfast-align + enhmgizafast-align
 0 10 20
 30 40 50
 60 70 80
 0.5  1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
-
o
o
v
OOV rate
English-Frenchmgiza + enhberkeley + enhfast-align + enhmgizafast-align
 0 10 20
 30 40 50
 60 70 80
 0.5  1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
-
o
o
v
OOV rate
English-Spanishmgiza + enhberkeley + enhfast-align + enhmgizafast-align
Figure 3: Performance in terms of oov-based F-measure of the baseline and enhanced word aligners on
test sets with increasing oov rate, for all language pairs. The oov-based F-measure for berkeley is not
reported because it is undefined.
 0 0.5
 1 1.5
 2
 1  2  4  8  16  32
D
e
l
t
a
 
F
-
m
e
a
s
u
r
e
OOV rate
English-Italianmgiza + enhberkeley + enhfast-align + enh
 0 0.5
 1 1.5
 2
 0.5  1  2  4  8  16  32
D
e
l
t
a
 
F
-
m
e
a
s
u
r
e
OOV rate
English-Frenchmgiza + enhberkeley + enhfast-align + enh
 0 0.5
 1 1.5
 2
 0.5  1  2  4  8  16  32
D
e
l
t
a
 
F
-
m
e
a
s
u
r
e
OOV rate
English-Spanishmgiza + enhberkeley + enhfast-align + enh
Figure 4: Difference of performance in terms of standard F-measure of the enhanced word aligners from
their corresponding baselines on test sets with increasing OOV rate, for all language pairs.
many oov-alignments, which the baseline system
can not find most of them.
Furthermore, Figure 4 reports the F-measure
differences achieved by the enhanced word-
aligners from their corresponding baselines on the
full data sets. The refinement module slightly
but consistently improves the overall F-measure as
well, especially for high oov-rates. The highest
improvement is achieved by the enhanced berke-
ley aligner, mainly because its baseline performs
worse in this condition.
6 Conclusion
In this paper we discussed the need of having a fast
and reliable online word aligner in the online adap-
tive MT scenario that is able to accurately align
the new words. The quality of three state-of-the-
art word aligners, namely berkeley, mgiza++, and
fast-align, were evaluated on this task in terms of
Precision, Recall, and F-measure. For this purpose
we created a benchmark in which an increasing
amount of the words of the test corpus are ran-
domly replaced by new words in order to augment
the oov-rate. The results show that the quality of
the aligners on new words is quite low, and sug-
gest that new models are required to effectively ad-
dress this task. As a first step, we proposed a fast
and language independent procedure for aligning
the unknown words which refines any given au-
tomatic word alignment. The results show that
the proposed approach significantly increases the
word alignment quality of the new words.
In future we plan to evaluate our approach in an
end-to-end evaluation to measure its effect on the
final translation. We also plan to investigate the
exploitation of additional features such as linguis-
tic and syntactic information in order to further
improve the quality of the word alignment mod-
els as well as the proposed refinement procedure.
However, this requires other policies of introduc-
ing new words, rather than just randomly selecting
the words and replacing them by artificial strings.
Acknowledgments
This work was supported by the MateCat project,
which is funded by the EC under the 7
th
Frame-
work Programme.
References
F. Blain, H. Schwenk, and J. Senellart. 2012. In-
cremental adaptation using translation information
and post-editing analysis. In International Work-
shop on Spoken Language Translation, pages 234?
241, Hong-Kong (China).
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
91
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?312.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 644?648, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Miquel Espl?a-Gomis, Felipe S?anchez-Mart??nez, and
Mikel L. Forcada. 2012. A simple approach to use
bilingual information sources for word alignment.
Procesamiento del Lenguaje Natural, (49):93?100.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Comput. Linguist., 33(3):293?303.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?
57, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Daniel Hardt and Jakob Elming. 2010. Incremental
re-training for post-editing smt. In 9th Conference
of the Association for Machine Translation in the
Americas (AMTA), Denver, United States.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
Tenth Machine Translation Summit (MT Summit X),
pages 79?86, Phuket, Thailand.
Patrik Lambert, Adri`a de Gispert, Rafael E. Banchs,
and Jos?e B. Mari?no. 2005. Guidelines for word
alignment evaluation and manual alignment. Lan-
guage Resources and Evaluation, 39(4):267?285.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreent. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June. Association for Computational Linguistics.
J. Scott McCarley, Abraham Ittycheriah, Salim
Roukos, Bing Xiang, and Jian-ming Xu. 2011. A
correction model for word alignments. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?11, pages 889?
898, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association of Compu-
tational Linguistics (ACL).
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma?z Erjavec, Dan Tufis?, and
D?aniel Varga. The jrc-acquis: A multilingual
aligned parallel corpus with 20+ languages. In In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC), pages
2142?2147, Genoa, Italy.
Nadi Tomeh, Alexandre Allauzen, Guillaume Wis-
niewski, and Franois Yvon. 2010. Refining word
alignment with discriminative training. In Proceed-
ings of the ninth Conference of the Association for
Machine Translation in the America (AMTA).
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Pro-
ceedings of COLING, pages 836?841, Copenhagen,
Denmark.
Yuqi Zhang, Evgeny Matusov, and Hermann Ney.
2009. Are unaligned words important for machine
translation? In Conference of the European As-
sociation for Machine Translation, pages 226?233,
Barcelona, Spain.
92
