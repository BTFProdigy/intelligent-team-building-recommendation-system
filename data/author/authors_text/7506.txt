Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 157?160,
New York, June 2006. c?2006 Association for Computational Linguistics
Sentence Planning for Realtime Navigational Instructions
Laura Stoia and Donna K. Byron and
Darla Magdalene Shockley and Eric Fosler-Lussier
The Ohio State University
Computer Science and Engineering
2015 Neil Ave., Columbus, Ohio 43210
stoia|dbyron|shockley|fosler@cse.ohio-state.edu
Abstract
In the current work, we focus on systems that
provide incremental directions and monitor
the progress of mobile users following those
directions. Such directions are based on dy-
namic quantities like the visibility of reference
points and their distance from the user. An
intelligent navigation assistant might take ad-
vantage of the user?s mobility within the set-
ting to achieve communicative goals, for ex-
ample, by repositioning him to a point from
which a description of the target is easier to
produce. Calculating spatial variables over a
corpus of human-human data developed for
this study, we trained a classifier to detect con-
texts in which a target object can be felici-
tously described. Our algorithm matched the
human subjects with 86% precision.
1 Introduction and Related Work
Dialog agents have been developed for a variety of
navigation domains such as in-car driving directions
(Dale et al, 2003), tourist information portals (John-
ston et al, 2002) and pedestrian navigation (Muller,
2002). In all these applications, the human partner
receives navigation instructions from a system. For
these domains, contextual features of the physical
setting must be taken into account for the agent to
communicate successfully.
In dialog systems, one misunderstanding can of-
ten lead to additional errors (Moratz and Tenbrink,
2003), so the system must strategically choose in-
structions and referring expressions that can be
clearly understood by the user. Human cognition
studies have found that the in front of/behind axis
is easier to perceive than other relations (Bryant et
al., 1992). In navigation tasks, this suggests that de-
scribing an object when it is in front of the follower
is preferable to using other spatial relations. Studies
on direction-giving language have found that speak-
ers interleave repositioning commands (e.g. ?Turn
right 90 degrees?) designating objects of interest
(e.g. ?See that chair??) and action commands (e.g.
?Keep going?)(Tversky and Lee, 1999). The con-
tent planner of a spoken dialog system must decide
which of these dialog moves to produce at each turn.
A route plan is a linked list of arcs between nodes
representing locations and decision-points in the
world. A direction-giving agent must perform sev-
eral content-planning and surface realization steps,
one of which is to decide how much of the route
to describe to the user at once (Dale et al, 2003).
Thus, the system selects the next target destination
and must describe it to the user. In an interactive
system, the generation agent must not only decide
what to say to the user but also when to say it.
2 Dialog Collection Procedure
Our task setup employs a virtual-reality (VR) world
in which one partner, the direction-follower (DF),
moves about in the world to perform a series of
tasks, such as pushing buttons to re-arrange ob-
jects in the room, picking up items, etc. The part-
ners communicated through headset microphones.
The simulated world was presented from first-person
perspective on a desk-top computer monitor. The
DF has no knowledge of the world map or tasks.
His partner, the direction-giver (DG), has a paper
2D map of the world and a list of tasks to complete.
During the task, the DG has instant feedback about
157
video frame: 00:13:16
00:13:16 ?keep going forward?
video frame: 00:15:12
00:14:05 ?ok, stop?
00:15:20 ?turn right?
video frame: 00:17:07
00:17:19: ?and go through that door
[D6]?Figure 1: An example sequence with repositioning
DG: ok, yeah, go through that door [D9, locate]
turn to your right
?mkay, and there?s a door [D11, vague]
in there um, go through the one
straight in front of you [D11, locate]
ok, stop... and then turn around and look at
the buttons [B18,B20,B21]
ok, you wanna push the button that?s there
on the left by the door [B18]
ok, and then go through the door [D10]
look to your left
there, in that cabinet there [C6, locate]
Figure 2: Sample dialog fragment
the DF?s location in the VR world, via mirroring of
his partner?s screen on his own computer monitor.
The DF can change his position or orientation within
the virtual world independently of the DG?s direc-
tions, but since the DG knows the task, their collab-
oration is necessary. In this study, we are most inter-
ested in the behavior of the DG, since the algorithm
we develop emulates this role. Our paid participants
were recruited in pairs, and were self-identified na-
tive speakers of North American English.
The video output of DF?s computer was captured
to a camera, along with the audio stream from both
microphones. A logfile created by the VR engine
recorded the DF?s coordinates, gaze angle, and the
position of objects in the world. All 3 data sources
were synchronized using calibration markers. A
technical report is available (Byron, 2005) that de-
scribes the recording equipment and software used.
Figure 2 is a dialog fragment in which the DG
steers his partner to a cabinet, using both a sequence
of target objects and three additional repositioning
commands (in bold) to adjust his partner?s spatial
relationship with the target.
2.1 Developing the Training Corpus
We recorded fifteen dialogs containing a total of
221 minutes of speech. The corpus was transcribed
and word-aligned. The dialogs were further anno-
tated using the Anvil tool (Kipp, 2004) to create a
set of target referring expressions. Because we are
interested in the spatial properties of the referents
of these target referring expressions, the items in-
cluded in this experiment were restricted to objects
with a defined spatial position (buttons, doors and
cabinets). We excluded plural referring expressions,
since their spatial properties are more complex, and
also expressions annotated as vague or abandoned.
Overall, the corpus contains 1736 markable items,
of which 87 were annotated as vague, 84 abandoned
and 228 sets.
We annotated each referring expression with a
boolean feature called Locate that indicates whether
the expression is the first one that allowed the fol-
lower to identify the object in the world, in other
words, the point at which joint spatial reference was
achieved. The kappa (Carletta, 1996) obtained on
this feature was 0.93. There were 466 referring ex-
pressions in the 15-dialog corpus that were anno-
tated TRUE for this feature.
The dataset used in the experiments is a consensus
version on which both annotators agreed on the set
of markables. Due to the constraints introduced by
the task, referent annotation achieved almost perfect
agreement. Annotators were allowed to look ahead
in the dialog to assign the referent. The data used in
the current study is only the DG?s language.
3 Algorithm Development
The generation module receives as input a route plan
produced by a planning module, composed of a list
of graph nodes that represent the route. As each sub-
sequent target on the list is selected, content plan-
ning considers the tuple of variables   ID, LOC 
where ID is an identifier for the target and LOC is
the DF?s location (his Cartesian coordinates and ori-
entation angle). Target ID?s are always object id?s
to be visited in performing the task, such as a door
158
 = Visible area(  )
 = Angle to target
	 = distance to target
In this scene:
Distractors = 5

 B1, B2, B3, C1, D1 
VisDistracts = 3 
 B2, B3, C1 
VisSemDistracts = 2 
 B2, B3 
Figure 3: An example configuration with spatial context fea-
tures. The target obje ct is B4 and [B1, B2, B3, B4, C1, D1] are
perceptually accessible.
that the DF must pass through. The VR world up-
dates the value of LOC at a rate of 10 frames/sec.
Using these variables, the content planner must de-
cide whether the DF?s current location is appropriate
for producing a referring expression to describe the
object.
The following features are calculated from this in-
formation: absolute Angle between target and fol-
lower?s view direction, which implicitly gives the in
front relation, Distance from target, visible distrac-
tors (VisDistracts), visible distractors of the same
semantic category (VisSemDistracts), whether the
target is visible (boolean Visible), and the target?s
semantic category (Cat: button/door/cabinet). Fig-
ure 3 is an example spatial configuration with these
features identified.
3.1 Decision Tree Training
Training examples from the annotation data are tu-
ples containing the ID of the annotated description,
the LOC of the DF at that moment (from the VR en-
gine log), and a class label: either Positive or Nega-
tive. Because we expect some latency between when
the DG judges that a felicity condition is met and
when he begins to speak, rather than using spatial
context features that co-occur with the onset of each
description, we averaged the values over a 0.3 sec-
ond window centered at the onset of the expression.
Negative contexts are difficult to identify since
they often do not manifest linguistically: the DG
may say nothing and allow the user to continue mov-
ing along his current vector, or he may issue a move-
ment command. A minimal criterion for producing
an expression that can achieve joint spatial reference
is that the addressee must have perceptual accessi-
bility to the item. Therefore, negative training exam-
ples for this experiment were selected from the time-
periods that elapsed between the follower achiev-
ing perceptual access to the object (coming into the
same room with it but not necessarily looking at it),
but before the Locating description was spoken. In
these negative examples, we consider the basic felic-
ity conditions for producing a descriptive reference
to the object to be met, yet the DG did not produce
a description. The dataset of 932 training examples
was balanced to contain 50% positive and 50% neg-
ative examples.
3.2 Decision Tree Performance
This evaluation is based on our algorithm?s ability
to reproduce the linguistic behavior of our human
subjects, which may not be ideal behavior.
The Weka1 toolkit was used to build a decision
tree classifier (Witten and Frank, 2005). Figure 4
shows the resulting tree. 20% of the examples were
held out as test items, and 80% were used for train-
ing with 10 fold cross validation. Based on training
results, the tree was pruned to a minimum of 30 in-
stances per leaf. The final tree correctly classified
 of the test data.
The number of positive and negative examples
was balanced, so the first baseline is 50%. To incor-
porate a more elaborate baseline, we consider that a
description will be made only if the referent is visi-
ble to the DF. Marking all cases where the referent
was visible as describe-id and all the other examples
as delay gives a higher baseline of 70%, still 16%
lower than the result of our tree.2
Previous findings in spatial cognition consider an-
gle, distance and shape as the key factors establish-
ing spatial relationships (Gapp, 1995), the angle de-
viation being the most important feature for projec-
tive spatial relationship. Our algorithm also selects
Angle and Distance as informative features. Vis-
Distracts is selected as the most important feature
by the tree, suggesting that having a large number
of objects to contrast makes the description harder,
which is in sync with human intuition. We note that
Visible is not selected, but that might be due to the
fact that it reduces to Angle  . In terms of the
referring expression generation algorithm described
by (Reiter and Dale, 1992), in which the description
which eliminates the most distractors is selected, our
1http://www.cs.waikato.ac.nz/ml/weka/
2not all positive examples were visible
159
results suggest that the human subjects chose to re-
duce the size of the distractor set before producing a
description, presumably in order to reduce the com-
putational load required to calculate the optimal de-
scription.
VisDistracts <= 3
| Angle <= 33
| | Distance <=154: describe-id (308/27)
| | Distance > 154: delay (60/20)
| Angle > 33
| | Distance <= 90
| | | Angle <=83:describe-id(79/20)
| | | Angle > 83: delay (53/9)
| | Distance >90: delay(158/16)
VisDistracts > 3: delay (114/1)
Figure 4: The decision tree obtained.
Class Precision Recall F-measure
describe-id 0.822 0.925 0.871
delay 0.914 0.8 0.853
Table 1: Detailed Performance
The exact values of features shown in our deci-
sion tree are specific to our environment. However,
the features themselves are domain-independent and
are relevant for any spatial direction-giving task, and
their relative influence over the final decision may
transfer to a new domain. To incorporate our find-
ings in a system, we will monitor the user?s context
and plan a description only when our tree predicts it.
4 Conclusions and Future Work
We describe an experiment in content planning for
spoken dialog agents that provide navigation in-
structions. Navigation requires the system and the
user to achieve joint reference to objects in the envi-
ronment. To accomplish this goal human direction-
givers judge whether their partner is in an appropri-
ate spatial configuration to comprehend a reference
spoken to an object in the scene. If not, one strategy
for accomplishing the communicative goal is to steer
their partner into a position from which the object is
easier to describe.
The algorithm we developed in this study, which
takes into account spatial context features replicates
our human subject?s decision to produce a descrip-
tion with 86%, compared to a 70% baseline based
on the visibility of the object. Although the spatial
details will vary for other spoken dialog domains,
the process developed in this study for producing de-
scription dialog moves only at the appropriate times
should be relevant for spoken dialog agents operat-
ing in other navigation domains.
Building dialog agents for situated tasks provides
a wealth of opportunity to study the interaction be-
tween context and linguistic behavior. In the future,
the generation procedure for our interactive agent
will be further developed in areas such as spatial de-
scriptions and surface realization. We also plan to
investigate whether different object types in the do-
main require differential processing, as prior work
on spatial semantics would suggest.
5 Acknowledgements
We would like to thank the OSU CSE department for funding
this work, our participants in the study and to M. White and
our reviewers for useful comments on the paper. We also thank
Brad Mellen for building the virtual world.
References
D. J. Bryant, B. Tversky, and N. Franklin. 1992. Internal and
external spatial frameworks representing described scenes.
Journal of Memory and Language, 31:74?98.
D. K. Byron. 2005. The OSU Quake 2004 corpus of two-
party situated problem-solving dialogs. Technical Report
OSU-CISRC-805-TR57, The Ohio State University Com-
puter Science and Engineering Department, Sept., 2005.
J. Carletta. 1996. Assessing agreement on classification tasks:
The kappa statistic. Computational Linguistics, 22(2):249?
254.
R. Dale, S. Geldof, and J. Prost. 2003. CORAL: Using natural
language generation for navigational assistance. In M. Oud-
shoorn, editor, Proceedings of the 26th Australasian Com-
puter Science Conference, Adelaide, Australia.
K. Gapp. 1995. Angle, distance, shape, and their relationship
to projective relations. Technical Report 115, Universitat des
Saarlandes.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen,
M. Walker, S. Whittaker, and P. Maloor. 2002. MATCH:
An architecture for multimodal dialogue systems. In Pro-
ceedings of the 40  Annual Meeting of the Association for
Computational Linguistics (ACL ?02), pages 376?383.
M. Kipp. 2004. Gesture Generation by Imitation - From Hu-
man Behavior to Computer Character Animation. Disserta-
tion.com.
R. Moratz and T. Tenbrink. 2003. Instruction modes for
joint spatial reference between naive users and a mobile
robot. In Proc. RISSP 2003 IEEE International Conference
on Robotics, Intelligent Systems and Signal Processing, Spe-
cial Session on New Methods in Human Robot Interaction.
C. Muller. 2002. Multimodal dialog in a pedestrian navi-
gation system. In Proceedings of ISCA Tutorial and Re-
search Workshop on Multi-Modal Dialogue in Mobile En-
vironments.
E. Reiter and R. Dale. 1992. A fast algorithm for the generation
of referring expressions. COLING.
B. Tversky and P. U. Lee. 1999. Pictorial and verbal tools for
conveying routes. Stade, Germany.
I. Witten and E. Frank. 2005. Data Mining: Practical machine
learning tools and techniques, 2nd Edition. Morgan Kauf-
mann, San Francisco.
160
Proceedings of the Fourth International Natural Language Generation Conference, pages 81?88,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Noun Phrase Generation for Situated Dialogs
Laura Stoia and Darla Magdalene Shockley and Donna K. Byron and Eric Fosler-Lussier
The Ohio State University
Computer Science and Engineering
2015 Neil Ave., Columbus, Ohio 43210
stoia|shockley|dbyron|fosler@cse.ohio-state.edu
Abstract
We report on a study examining the gener-
ation of noun phrases within a spoken di-
alog agent for a navigation domain. The
task is to provide real-time instructions
that direct the user to move between a se-
ries of destinations within a large interior
space. A subtask within sentence plan-
ning is determining what form to choose
for noun phrases. This choice is driven by
both the discourse history and spatial con-
text features derived from the direction-
follower?s position, e.g. his view angle,
distance from the target referent and the
number of similar items in view. The al-
gorithm was developed as a decision tree
and its output was evaluated by a group of
human judges who rated 62.6% of the ex-
pressions generated by the system to be as
good as or better than the language origi-
nally produced by human dialog partners.
1 Introduction
In today?s world of mobile, context-aware com-
puting, intelligent software agents are being de-
ployed in a wide variety of domains to aid hu-
mans in performing navigation tasks. Exam-
ples include hand-held tourist information por-
tals (Johnston et al, 2002) campus tour guides
(Yang et al, 1999; Long et al, 1996; Striegnitz
et al, 2005), direction-giving avatars for visitors
to a building (Cassell et al, 2002; Chou et al,
2005), in-car driving direction systems (Dale et al,
2003; Wahlster et al, 2001), and pedestrian navi-
gation systems (Muller, 2002). These applications
present an exciting and challenging new frontier
for dialog agents, since attributes of the real-world
setting must be combined with other contextual
factors for the agent to communicate successfully.
In the current work, we focus on a scenario
in which the system provides incremental direc-
tions to a mobile user who is following the instruc-
tions as they are produced. Unlike the rigid di-
rections produced by applications like Mapquest,1
which describes the entire route from start to fin-
ish, this task requires realtime instructions issued
while monitoring the user?s progress. Instructions
are based on dynamic local context variables such
as the visibility of and distance to reference points.
In referring to items in the setting, human speak-
ers produce a wide variety of noun phrase forms,
including descriptions that are headed by a com-
mon noun and that employ a definite, indefinite, or
demonstrative determiner, one anaphors, and pro-
nouns such as it, this and that. Our goal in the
current work is to model that entire space of varia-
tion, which makes the task more difficult than the
noun phrase generation task defined in many pre-
vious studies that simplify the alternatives down to
description or pronoun.
In order to study this process, we developed a
task domain in which a human partner is directed
through an interior space (a graphically-presented
3D virtual world) to perform a sequence of ma-
nipulation tasks. In the first stages of the work, we
collected and annotated a corpus of human-human
dialogs from this domain. Then, using this data,
we trained a decision-tree classifier to utilize con-
text variables such as distance, target object visi-
bility, discourse history, etc., to determine lexical
properties of referring expressions to be produced
by the generation component of our dialog system.
2 Generation for Situated Tasks
Many previous projects, such as (Lauria et al,
2001; Moratz and Tenbrink, 2003; Skubic et al,
2002), inter alia, study interpretation of situated
language, e.g. for giving directions to a robot. The
focus of our work is rather on generating naviga-
tion instructions for a human partner to follow.
Linguistic studies have shown that speakers se-
lect noun phrase forms to refer to entities based on
a variety of factors. Some of the factors are intrin-
sic to the object being described, while others are
features of the context in which the expression is
spoken. The entity?s status within the discourse,
1www.mapquest.com
81
spatial position, and the presence of similar items
from which the target referent must be distin-
guished, have all been found to cause changes to
the lexical properties chosen for a particular refer-
ring expression (i.e. (Gundel et al, 1993; Prince,
1981; Grosz et al, 1995)). This variation is ex-
pressed in terms of the determiner chosen (e.g.
that/a), the head noun (e.g. that/door/one), and
the presence of additional modifiers such as pre-
nominal adjectives or prepositional phrases.
In natural language generation, the process of
generating referring expressions occurs in stages
(Reiter and Dale, 1992). The process we explore
in this paper is the sentence planning stage, which
determines whether the context supports generat-
ing a particular referring expression as a pronoun,
description, one-anaphor, etc.
There has been extensive research in both au-
tomatic route description and on general noun
phrase (NP) generation, but few projects consider
extra-linguistic information as part of the context
that influences dialog behavior. (Poesio et al,
1999) applies statistical techniques for the prob-
lem of NP generation. However, even though the
corpus used in that study contains descriptions of
museum items visually accessible to the user, the
features used in generation were mostly linguis-
tic, and included little information about the vi-
sual or spatial properties of the referent. Another
related study in statistical NP generation (Cheng et
al., 2001) focuses on choosing the modifiers to be
included. Again, no features derived from the sit-
uated world were used in that study. (Maass et al,
1995) use features from the world, including ob-
jects? color, height, width, and visibility, as well as
the user?s direction of travel and distance from ob-
jects, for generating instructions in a situated task.
However, their focus is on selecting landmarks and
descriptions under time pressure, rather than se-
lecting the linguistic form to be produced.
3 Data Collection
Our task setup is designed to elicit natural, spon-
taneous situated language examples from human
partners. The experimental platform employs a
virtual-reality (VR) world in which one partner,
the direction-follower (DF), moves about to per-
form a series of tasks, such as pushing buttons to
re-arrange objects in the room, finding and picking
up treasures, etc. The simulated world was pre-
sented from first-person perspective on a desk-top
computer monitor. The DF had no knowledge of
the world map or tasks.
DG: you can currently see three buttons... there?s
actually a fourth button that?s kind of hidden
DF: yeah
DG: by this cabinet on the right
DF: I know, yeah
DG: ok, um, so what you wanna do is you want to
go in and you?re gonna press one of the buttons
that?s on the right hand wall, so you wanna go
all the way straight into the room and then face
the wall
DF: mhm
DG: there with the two buttons
DF: yep
DG: um and you wanna push the one that?s on the left
Figure 1: Sample dialog fragment and accompanying video
frame
His partner, the direction-giver (DG), had a pa-
per 2D map of the world and a list of tasks to
complete. As they performed the task, the DG
had instant feedback about the DF?s location in
the VR world, via mirroring of the DF?s computer
screen on the DG?s computer monitor. The part-
ners communicated through headset microphones.
Our paid participants were self-identified native
speakers of North American English. Figure 1
shows an example view of the world and the ac-
companying dialog fragment.
The video output of DF?s computer was cap-
tured to a camera, along with the audio from both
microphones. A logfile created by the VR soft-
ware recorded the DF?s coordinates, gaze angle,
and the position of objects in the world 10 times
per second. These data sources were synchronized
using calibration markers. A technical report is
available that describes the recording equipment
and software used (Byron, 2005).
3.1 Corpus and Annotation Scheme
Using the above-described setup, we created a cor-
pus consisting of 15 dialogs containing a total of
221 minutes of speech. It was transcribed and
word-aligned using Praat 2 and SONIC.3 The di-
alogs were further annotated using the Anvil soft-
ware (Kipp, 2004) to identify a set of target refer-
ring expressions in the corpus. Because we are in-
2http://www.praat.org
3http://cslr.colorado.edu/beginweb/speech recognition/sonic.html/
82
terested in the spatial properties of the referents of
these target referring expressions, the items of in-
terest in this experiment were restricted to objects
with a defined spatial position.
Each object in the virtual world was assigned a
symbolic id, and the id of each target referring ex-
pression was added to the annotation. Referring
expressions with plural referents were marked as
Set, and were labeled with a list of the members
in the set. Expressions were also annotated as ei-
ther vague when the referent was not clear at the
time of utterance or abandoned in case the utter-
ance was cut short. Items that did not contain a
surface realization of the head of the NP (e.g., on
the left), were marked with the tag empty.
The corpus contains 1736 target expressions, of
which 221 were Vague, 45 were Empty, and 228
were Sets. The remaining 1242 form the set of test
items in the experiment described below. Vague
items were excluded since we do not wish for the
algorithm we develop to reproduce this behavior.
Set items were excluded in order to avoid the more
complex calculation of spatial properties associ-
ated with plural entities.
The data used in the experiments is a consensus
version on which both annotators, two of the au-
thors, agreed on the set of target expressions and
their properties. Due to the constraints introduced
by the task, referent annotation achieved almost
perfect agreement. The data used in this study is
only the DG?s language.
4 Algorithm Development
Our ultimate goal is to provide input to a surface
realization component for NP generation, given
the ID of a target referent and a vector of context
features. It is desirable for these context features
to be automatically derived, to limit the reliance
on human annotation, so we restricted out study to
features that either were derived automatically, or
required minimal human annotation.
One impact of this decision is that even though
the linguistic literature predicts that syntactic fea-
tures such as grammatical role are important in
selecting NP forms, these features were difficult
to obtain. Our corpus contains spontaneous spo-
ken discourse, which has no sentence boundaries
and relaxed structural constraints. Thus, automatic
parsing was problematic. With improved parsing
techniques, we may include syntactic information
in the decision process for NP generation in future,
but this was not included in the current study.
Following (Poesio et al, 1999), we consider the
det a, the, that, none
head it, that, one, noun, none
mod +, -
The possible values of each NP frame slot
[
det : none
head : it
mod : ?
] [
det : that
head : noun
mod : +
]
it that button on the right
NP frames for it and that button on the right
Figure 2: NP frame slot values and examples
information conveyed by an NP to be divided into
four slots which must be filled to be able to gen-
erate the NP form: a determiner/quantifier, a pre
or post-modifier and a head noun slot. There were
very few examples of premodifiers in the corpus,
so we collapsed the modifier feature. Therefore,
the output from our algorithm is an NP frame spec-
ifying values for the three slots for each target ex-
pression. Figure 2 shows the possible values in
each slot and example slot values for two NPs. The
number of occurrences in the entire corpus for the
NP frame slot values are shown in Table 2.4
In the experimental VR world developed for this
study, all items from the same category were de-
signed to look identical. This was intended to en-
courage the subjects to use referring expressions
that rely on spatial attributes or deictic reference
such as that one. The spatial properties of target
referents and distractors are used as inputs to the
content planning algorithm. Their values in this
study were calculated automatically based on ge-
ometric properties of the virtual world.
To form the training dataset, we processed each
target expression with a syntactic chunker.5 The
partial parse it produced was further processed
with a regular-expression matcher to isolate the
values corresponding to the three slots. Parser er-
rors caused some low-count NP frame values, so
we retained only items that occurred at least 10
times in the entire corpus. Any parser errors that
remained in the data were not hand corrected, in
order to minimize human intervention.
4.1 Context Features
Given the restrictions that we impose over what
is accessible to the learning algorithm, we devel-
oped a set of features for each referring expression
that characterize both the referent and the context
in which the expression was spoken. The context
4The two possible tags for Mod occurred in almost equal
proportion (49%/51%)
5http://www.ltg.ed.ac.uk/software/chunk/index.html
83
Dialog history features
1. Count and chainCount the mention counts for the referent over the dialog and inside a reference chaina
2. DeltaTime and DeltaTimeChain the time elapsed since it was last mentioned in the dialog overall or in a chain
3. PrevSpeaker the previous speaker that mentioned the ID (either DG or DF)
4. Mod
i?1
, Det
i?1
, Head
i?1
the values of the slots of the NP frame of the prior mention of the same referent
5. Mod
i?2
, Det
i?2
, Head
i?2
the previous-1 values of the slots
6. WordDistance and the number of words spoken by both speakers since the last mention of the ID
chainWordDistance overall or in the chain
7. Type
i?1
indicates if the previous mention was in a Set, was Vague, or was a test item
Spatial/Visual featuresb
8. Distance the distance between the referent and the DF?s VR coordinates
9. Angle the angle between the center of the DF?s view angle and the center of the referent
10. Visible a boolean value which indicates if the object is visible
Relation to other objects in the world
11. Visible Distractors the number of other objects besides the target referent in the field of view
12. SameCatVisDistractors the number of visible distractors of the same type as the referent
Object category and its information status
13. Cat the semantic category of the referent: door/cabinet/button
14. First Locate indicates if this is the first expression that allowed the DF to identify the object
in the world. The point where joint spatial reference is accomplished.
Table 1: The Context Features Used by the Algorithm
amention counts are not considered over vague or ambiguous tags, or over sets.
bnote that an Angle value smaller than 500 ensures the object is visible
Det Head
Value Count Percent Value Count Percent
the 364 39% noun 558 60%
that/this 264 29% one 166 18%
none 253 27% it 116 13%
a 46 5% that 57 6%
none 30 3%
Table 2: Distribution of Det and Head values in the corpus
v = Visible area(100o)
? = Angle to target
d = distance to target
In this scene:
VisDistr =3 {B2, B3, C1}
VisSemDistr =2 {B2, B3}
Figure 3: An example configuration with spatial context
features. The target object is B4.
features are not only linguistic but also derived
from the extralinguistic situation, including spatial
relations between the referent and the DF?s posi-
tion and orientation at that instant. The context
feature for each target expression includes these
automatically-calculated attributes as well as fea-
tures from the annotation described above. Table 1
describes the full set of context features, and Fig-
ure 3 shows a schematic of the spatial context fea-
tures.
The mention history of any target referent is im-
portant for determining the form to use in a subse-
quent referring expression. Ideally, the discourse
history feature should indicate whether a refer-
ent has already been discussed, and the distance
between a new mention and its antecedent. But
determining the discourse status of items in this
world was complicated by two factors. All ob-
jects in the world of the same semantic category
had identical visual features, and the VR world
in which the task is conducted is a maze, which
required the subjects to perform tasks, move to a
different portion of the maze, and possibly return
to a previously visited room. Due to the visual
and spatial confusion possible in this setting, there
is no guarantee that our subjects could accurately
calculate whether they were discussing the same
object they had encountered before, or remember
whether that object had been discussed. While
the subjects were focused on a task in a particular
room, however, it is reasonable to expect that they
could remember which items had been discussed.
Therefore, the discourse histories of target objects
were calculated using a re-initialization process.
Each time the subjects left a VR room to pursue a
different task, if more than 25s elapsed before the
next mention of objects in that room, those sub-
sequent expressions were considered to be in new
coreference chains. This time constant was estab-
lished by examining pronominal referring expres-
sions in the training dialogs.
These features were used as input to develop a
classifier to determine NP frames for unseen tar-
get referents in context. We chose decision trees
due to their ease of interpretation, but we plan to
test other machine learning techniques in the fu-
ture. 5 dialogs were held out as unseen data and
the remaining 10 were used to train and adjust the
parameters of the decision process. The first pro-
cedure was to test whether the three slot values
are interdependent. In contrast to previous work,
84
which focused on predicting the values for one of
the slots at a time, we hold that due to their inter-
dependence, these decisions should not be made
separately. For example, a noun form that has the
pronoun it as the head will never have a modifier
or a determiner. If the three slots are independent,
training three separate classifiers and then com-
bining their decisions will yield better results. On
the other hand, if they are dependent, better results
will be obtained through training a single classifier
on the combined label. Unfortunately, combining
the labels is problematic due to data sparsity. To
test these dependencies, we trained several deci-
sion trees, varying the independence assumptions:
Independent - a decision tree was trained for each
slot and their outputs combined at the end.
Joint - a decision tree was trained for the com-
bined label for all three slots
Conditional - three decision trees were trained
in sequence, each having access to the output of
the previous tree. For example, Mod-Det-Head
means that first the Mod tree was trained, then a
tree to classify Det, using the output from Mod,
and finally a tree for Head , using both the Det
and Mod values.
All possible orderings between Mod, Head and
Det were tested. The best result obtained was from
the ordering Mod-Det-Head, but the differences
between the orderings were not significant. The
10 fold cross-validation results are shown in Ta-
ble 3. There were 632 items in the data set. The
Conditional trees outperformed the Independent
trees by 9%, which is significant at the level of
(p < .0002).
As our training data suggests, we test the Mod-
Det-Head trees against our held out data. We
decided to use a leave one out method of train-
ing/testing due to the sparsity of data.
Independent Joint Mod-Det-Head
22.0 % 28.8 % 31.0 %
Table 3: Testing independence of the slot values
Decision tree classifiers offer the opportunity to
examine the relevance of particular features in the
final decision. Algorithm 1 and 2 show example
trees derived for the Mod and Det features (the
Head tree is not shown due to space limitations).
We found that there are significant dependencies
between the slots in the NP form. Each time one of
the slots? values was available to the decision pro-
cess, it was selected as most informative feature in
the next tree. The spatial features were selected as
informative in all the trees, most prevelantly in the
Algorithm 1 An example decision tree for Mod
if FirstLocate = True then
if V isibleDistractors = 0 then
if Distance ? 116 then
return Mod: -
else
return Mod:+
else
if SameCatV isDistractors = 0 then
if V isibleDistractors ? 2 then
if Angle ? 38 then
return Mod: -
else
return Mod: +
else
return Mod: +
else
return Mod: +
else
if chainWordDistance = 0 then
if prevMention 6= Set/AllV ague then
if firstMention = True then
return Mod: +
else
if Angle ? 27 then
return Mod: -
else
return Mod: +
else
if noprevMention then
return Mod: +
else {prevMention = Set/AllV ague}
return Mod: -
else
return Mod: -
Algorithm 2 An example decision tree for Det
if Mod : ? then
if FirstLocate = True then
return Det:that
else
if prevMention 6= Set/AllV ague then
if notV isible then
if Cat = Button/Cabinet then
return Det:none
else {Cat = Door}
return Det:that
else {isV isible}
if Head
i?1
= it then
return Det:none
else if Head
i?1
= noun then
if DeltaT ime ? 6.3 then
if Cat = Button/Cabinet then
return Det:none
else {Cat = Door}
return Det:that
else
return Det:the
else if Head
i?1
= one/none/low then
return Det:that
else {Head
i?1
= that}
return Det:none
else if noprevMention then
return Det:that
else {prevMention = Set/AllV ague}
return Det:none
else {target has modifier}
return Det:the
85
decision tree for Mod, suggesting that the decision
of including extra information is driven largely by
the spatial configuration. The information status
features and discourse history, such as First Lo-
cate, Type, and attributes of the prior mention,
were selected as good predictors for the Det slot.
5 Evaluation
We report several methods of evaluating the NP
frames produced using the process given by the
decision trees. First, we report the results of a
strict evaluation in which the system?s output must
exactly match expressions produced by the hu-
man subjects. We also compare this result with
a hand-crafted Centering-style generation algo-
rithm. Requiring the algorithm to exactly match
human performance is an overly-strict criterion,
since in many contexts several possible referring
expression forms could be equally felicitous in a
given context, so we also conducted a human judg-
ment study. The 5 test dialogs contain 295 target
expressions.
5.1 Exact Match Evaluation
The output of the decision tree classifier was com-
pared to the expressions observed in the test dia-
log. Table 4 reports the results of this evaluation.
The accuracy obtained was 31.2%. The most fre-
quent tag gives a 20.0% baseline performance us-
ing this strict match criterion.
Exact match results
Predicted All three features det mod head
Correct 31% 48% 72% 56%
Exact match: head feature per value
Predicted noun it none one that
Correct 65% 64% 0% 30% 38%
Exact match: det feature per value
Predicted a none that the
Correct 0% 49% 36% 66%
Table 4: Classifier results using Exact-match criterion
5.2 Comparison to Centering
For purposes of comparing the performance of our
generation algorithm to existing work on genera-
tion of NPs, we performed a manual evaluation of
the centering-style generation algorithm described
in (Kibble and Power, 2000) against our dialog
corpus. Algorithms developed according to the
centering framework use discourse coherence to
make decisions about pronominalization (Grosz et
al., 1995), where coherence is measured in terms
of topical continuity from one sentence to the next.
Centering designates the backward-looking cen-
ter (Cb) as the item in the current sentence that
was most topical in the previous sentence. There-
fore, to perform a centering-style evaluation, the
dialogs must be broken into sentence-like units,
and a ranking procedure must be devised for the
items mentioned in each unit.
The current evaluation corpus, being a spo-
ken dialog, has not been parsed to automatically
determine the syntactic or dependency structure,
but rather was manually segmented into utterance
units, where each unit contained a main predicate
and its satellites. The items mentioned in each unit
were ranked according to thematic roles, using the
ranking {AGENT > PATIENT > COMP > AD-
JUNCT}, and excluding references to the speakers
themselves, which often appear in AGENT posi-
tion (Byron and Stent, 1998). The Cb in each unit,
if there is one, is the highest-ranked item from the
prior unit?s list that is repeated in the current unit?s
list. Following a procedure similar to that reported
by Kibble and Power, our decision procedure rec-
ommends pronominalizing an item if it is the Cb
of its unit and if it is in Subject position, otherwise
a description is generated. Based on this rule, all
items that are being mentioned for the first time
in the discourse are predicted to require a descrip-
tion.
Although most prior studies take the recom-
mendation to pronominalize to mean that a per-
sonal pronoun (e.g. it) should be generated, due
to the demonstrative nature of our domain, the de-
cision to produce a pronoun can result in either a
demonstrative or a personal pronoun. Therefore,
we considered the algorithm?s output to match hu-
man production when the target expression in the
human corpus was either a personal or demonstra-
tive pronoun, and the algorithm generated either
category of pronoun. Table 5 shows the compari-
son of our system?s output and the output from the
centering algorithm on anaphoric mentions. The 5
dialogs used for testing in this study contained 145
such items. Both algorithms obtained a similar ac-
curacy (64.8% our system vs. 64.1% centering)
and over-generated pronoouns. Although our al-
gorithm does not outperform centering, it assumes
less structural analysis of the input text.
5.3 Human Judgment Evaluation
Evaluating generation studies by calculating their
similarity to human spontaneous speech may not
be the ideal performance metric, since several dif-
ferent realizations may be equally felicitous in a
86
Pron Desc Total
Human Production 28 117 145
Predicted by Our Algorithm 55 90
Predicted by Centering 64 81
Table 5: Comparison to Coherence-based Generation
Figure 4: The Anvil software tool used for judging
given context. Therefore, we also performed a
human judgement evaluation. In this evaluation,
judges compared the NPs generated by our algo-
rithm to the NPs produced by human subjects, and
to NPs with randomly generated feature assign-
ments. Judges viewed test NPs in the context of
the original test corpus.
To re-create the context in which the original
expression was produced, the video, audio, and
dialog transcript were played for the judges us-
ing the Anvil annotation tool (Kipp, 2004). The
judges could play or pause the video as they
wished. Using the word-alignments established
during the data annotation phase, the audio of the
test NPs was replaced by silence, and the words
were removed in the transcript shown in the time-
line viewer. For each test item, the judges were
presented with a selection box showing two pos-
sible referring expressions that they were asked to
compare using a qualitative ranking (option 1 is
better, option 2 is better, or they are equal), given
a particular target ID and the context. Figure 4
shows a screen-shot of the judges? annotation tool.
The judges did not know the source of the expres-
sions they evaluated (system, human production,
or random). The 10 judges were volunteers from
the university community who were self-identified
native speakers of English. They were not com-
pensated for their time.
The decision tree selected NP-frame slot val-
ues which were converted into realized NPs. The
Det and Head choices were directly translated into
surface forms (for Head=noun we chose a consis-
tent common noun for each semantic class: but-
All Items
System compared to Human Trials: 577
equal 45.9%
system preferred 16.6%
(system equal or preferred to human) (62.6%)
human preferred 37.4%
System compared to Random Trials: 289
equal 24.2%
system preferred 53.3%
(system equal or preferred to random) (77.5%)
random preferred 22.5%
Random compared to Human Trials: 292
equal 23.3%
random preferred 13.0%
(random equal or preferred to human) (36.3%)
human preferred 65.7%
Items with two judges & judges agreed
System against Human Trials: 197
equal 37.3%
system preferred 19.8%
(system equal or preferred to human) (57.1%)
human preferred 36.6%
Table 6: Results of Human Judging
ton, door or cabinet. If the system?s selection of
Mod feature matched the value from the corpus,
we used the expression produced by the original
speaker. If the original expression did not include
a modifier, but the system selected Mod:+, we lex-
icalized this feature to a simple but correct spatial
description like on the right, on the left or in front.
Table 6 shows the results of human judging.
The system?s output was either equal or preferred
to the original spontaneous language in 62.6%
of cases where these two choices were compared
directly. Interestingly, the randomly-generated
choice was preferred over the original spontaneous
language in 13.0% of trials, and was preferred over
the system?s output in 22.5% of trials. Aggregat-
ing over all judges, the system?s performance was
judged to be much better than random, but not as
good as the original human language.
Trials were balanced among judges so that each
target item was seen by four judges: with two
comparing the system?s response to the original
human language, one comparing the system to
random, and one comparing the human to random.
There were 282 trials for which 2 judges saw the
identical pair of choices. Out of these, the two
judges? responses agreed in 197 cases, producing
an inter-annotator reliability (kappa score) of 0.51,
with raw agreement of 69% and expected agree-
ment of 37%. Although this is a relatively low
kappa value, we believe that the aggregate judg-
ments of all of the judges over all of the test items
are still informative, since the scores of items for
which we have two judgements follow a very sim-
87
ilar pattern to the overal distribution of responses.
The low inter-annotator agreement may be due to
the substitutability of the expressions.
6 Conclusions and Future Work
In this paper we describe a generation study for
situated dialog and a novel evaluation setup of the
system?s output. The algorithm decides upon the
determiner, head and modifier values to be pro-
duced in a noun phrase describing an object in
a particular moment in the dialog. The decision
is influenced by dialog history, spatial and visual
relations and information status of the ID to be
described. Our algorithm achieved 31.2% exact
match with human language, but human evalua-
tors judged the output as good as or better than the
original human language 62.6% of the time.
For our future work, we intend to develop the
generation module of a dialog system that per-
forms the direction giver?s role. We plan to incor-
porate the results of this study in an extension of
(Reiter and Dale, 1992) algorithm that would take
into account other types of properties of the ob-
jects like visual salience, temporal attributes (for
example time elapsed between mentions), if it par-
ticipated in an action (like the case of a door open-
ing, or a button being pushed) or its importance to
the overall task completion.
Acknowledgments
The authors would like to thank our undergradu-
ate RA, Bradley Mellen, for building the virtual
world, the 11 judges who rated the system output,
and the anonymous reviewers.
References
D. Byron and A. Stent. 1998. A preliminary model of center-
ing in dialog. In Proceedings of ACL ?98, pp. 1475?1477.
D. Byron. 2005. The OSU Quake 2004 corpus of two-party
situated problem-solving dialogs. Technical Report OSU-
CISRC-805-TR57, The Ohio State University Computer
Science and Engineering Department, September.
J. Cassell, T. Stocky, T. Bickmore, Y. Gao, Y. Nakano,
K. Ryokai, D. Tversky, C. Vaucelle, and H. Vilhjalmsson.
2002. MACK: Media lab Autonomous Conversational
Kiosk. In Proceedings of IMAGINA?02, Monte Carlo, Jan-
uary.
H. Cheng, M. Poesio, R. Henschel, and C. Mellish. 2001.
Corpus-based NP modifier generation. In NAACL ?01,
pp. 1?8, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
S. Chou, W. Hsieh, F. Gandon, and N. Sadeh. 2005. Se-
mantic web technologies for context-aware museum tour
guide applications. In Proceedings of the 2005 Interna-
tional Workshop on Web and Mobile Information Systems.
R. Dale, S. Geldof, and J. Prost. 2003. CORAL: Using
natural language generation for navigational assistance.
In M. Oudshoorn, editor, Proceedings of the 26th Aus-
tralasian Computer Science Conference, Adelaide, Aus-
tralia.
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering: A
framework for modeling the local coherence of discourse.
Computational Linguistics, 21(2):203?226.
J. Gundel, N. Hedberg, and R. Zacharski. 1993. Cognitive
status and the form of referring expressions in discourse.
Language, 69(2):274?307.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen,
M. Walker, S. Whittaker, and P. Maloor. 2002. MATCH:
An architecture for multimodal dialogue systems. In Pro-
ceedings of ACL ?02, pp. 376?383.
R. Kibble and R. Power. 2000. An integrated framework for
text planning and pronominalisation. In Proceedings of
INLG?2000, pp. 77?84.
M. Kipp. 2004. Gesture Generation by Imitation - From
Human Behavior to Computer Character Animation. Dis-
sertation.com.
S. Lauria, G. Bugmann, T. Kyriacou, J. Bos, and E. Klein.
2001. Training personal robots using natural langauge in-
struction. IEEE Intelligent Systems, 16(5):2?9.
S. Long, R. Kooper, G. Abowd, and C. Atkesonet. 1996.
Rapid prototyping of mobile context-aware applications:
The cyberguide case study. In 2nd ACM International
Conference on Mobile Computing and Networking (Mo-
biCom?96), November 10-12.
W. Maass, J. Baus, and J. Paul. 1995. Visual grounding of
route descriptions in dynamic environments.
R. Moratz and T. Tenbrink. 2003. Instruction modes for joint
spatial reference between naive users and a mobile robot.
In Proc. RISSP 2003 IEEE International Conference on
Robotics, Intelligent Systems and Signal Processing, Spe-
cial Session on NewMethods in Human Robot Interaction.
C. Muller. 2002. Multimodal dialog in a pedestrian navi-
gation system. In Proceedings of ISCA Tutorial and Re-
search Workshop on Multi-Modal Dialogue in Mobile En-
vironments.
M. Poesio, R. Henschel, J. Hitzeman, and R. Kibble. 1999.
Statistical NP generation: A first report. Utrecht, August.
E. Prince. 1981. On the inferencing of indefinite this NPs.
In Aravind K. Joshi, Bonnie Lynn Webber, and Ivan Sag,
editors, Elements of Discourse Understanding, pp. 231?
250. Cambridge University Press.
E. Reiter and R. Dale. 1992. A fast algorithm for the gen-
erations referring expressions. In Proceedings of COL-
ING ?92, pp. 232?238.
M. Skubic, D. Perzanowski, A. Schultz, and W. Adams.
2002. Using spatial language in a human-robot dialog.
In 2002 IEEE International Conference on Robotics and
Automation, Washington, D.C.
K. Striegnitz, P. Tepper, A. Lovett, and J. Cassell. 2005.
Knowledge representation for generating locating gestures
in route directions. In Proceedings of Workshop on Spa-
tial Language and Dialogue (5th Workshop on Language
and Space), Delmenhorst, Germany, October.
W. Wahlster, N. Reithinger, and A. Blocher. 2001.
Smartkom: Towards multimodal dialogues with anthropo-
morphic interface agents. In International Status Confer-
ence: Lead Projects HumanComputer -Interaction, Saar-
bruecken, Germany.
J. Yang, W. Yang, M. Denecke, and A. Waibel. 1999. Smart
sight: a tourist assistant system. In Proceedings of the
3rd International Symposium on Wearable Computers, pp.
73?78, San Francisco, California, 18-19 October.
88
