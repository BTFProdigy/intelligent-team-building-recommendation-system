2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 284?294,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Correcting Comma Errors in Learner Essays, and Restoring Commas in
Newswire Text
Ross Israel
Indiana University
Memorial Hall 322
Bloomington, IN 47405, USA
raisrael@indiana.edu
Joel Tetreault
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
jtetreault@ets.org
Martin Chodorow
Hunter College of CUNY
695 Park Avenue
New York, NY 10065, USA
mchodoro@hunter.cuny.edu
Abstract
While the field of grammatical error detection
has progressed over the past few years, one
area of particular difficulty for both native and
non-native learners of English, comma place-
ment, has been largely ignored. We present a
system for comma error correction in English
that achieves an average of 89% precision and
25% recall on two corpora of unedited student
essays. This system also achieves state-of-the-
art performance in the sister task of restor-
ing commas in well-formed text. For both
tasks, we show that the use of novel features
which encode long-distance information im-
proves upon the more lexically-driven features
used in prior work.
1 Introduction
Automatically detecting and correcting grammati-
cal errors in learner language is a growing sub-field
of Natural Language Processing. As the field has
progressed, we have seen research focusing on a
range of grammatical phenomena including English
articles and prepositions (c.f. Tetreault et al, 2010;
De Felice and Pulman, 2008), particles in Korean
and Japanese (c.f. Dickinson et al, 2011; Oyama,
2010), and broad approaches that aim to find mul-
tiple error types (c.f Rozovskaya et al, 2011; Ga-
mon, 2011). However, to the best of our knowledge,
there has not been any research published specifi-
cally on correcting erroneous comma usage in En-
glish (though there have been efforts such as the MS
Word grammar checker, and products like Gram-
marly and White Smoke that include comma check-
ing).
There are a variety of reasons that motivate our
interest in attempting to correct comma errors. First
of all, a review of error typologies in Leacock et al
(2010) reveals that comma usage errors are the
fourth most common error type among non-native
writers in the Cambridge Learner Corpus (Nicholls,
1999), which is composed of millions of words of
text from essays written by learners of English. The
problem of comma usage is not limited to non-
native writers; six of the top twenty error types for
native writers involve misuse of commas (Connors
and Lunsford, 1988). Given these apparent deficits
among both non-native and native speakers, devel-
oping a sound methodology for automatically iden-
tifying comma errors will prove useful in both learn-
ing and automatic assessment environments.
A quick examination of English learner essays re-
veals a variety of errors, with writers both overusing
and underusing commas in certain contexts. Con-
sider examples (1) and (2):
(1) erroneous: If you want to be a master you
should know your subject well.
corrected: If you want to be a master , you
should know your subject well.
(2) erroneous: I suppose , that it is better to spe-
cialize in one specific subject.
corrected: I suppose that it is better to special-
ize in one specific subject.
In example (1), an introductory conditional phrase
begins the sentence, but the learner has not used the
appropriate comma to separate the dependent clause
from the independent clause. The comma in this
case helps the reader to see where one clause ends
284
and another begins. In example (2), the comma after
suppose is unnecessary in American English, and al-
though this error is related more to style than to read-
ability, most native writers would omit the comma
in this context, so it should be avoided by learners as
well.
Another motivating factor for this work is the fact
that sentence internal punctuation contributes to the
overall readability of a sentence (Hill and Murray,
1998). Proper comma placement can lead to faster
reading times and reduce the need to re-read en-
tire sentences. Commas also help remove or reduce
problems arising from difficult ambiguities; the gar-
den path effect can be greatly reduced if commas are
correctly inserted after introductory phrases and re-
duced relative clauses.
This paper makes the following contributions:
? We present the first published comma error cor-
rection system for English, evaluated on essays
written by both native and non-native speakers
of English.
? The same system also achieves state-of-the-art
performance in the task of restoring commas in
well-edited text.
? We describe a novel annotation scheme that al-
lows for robust mark up of comma errors and
use it to annotate two corpora of student essays.
? We show that distance and combination fea-
tures can improve performance for both the er-
ror correction and restoration tasks.
The rest of this paper is organized as follows.
In section 2, we review prior work. Section 3 de-
tails our typology of comma usage. We discuss our
choice of classifier and selection of features in sec-
tion 4. In section 5, we apply our system to the task
of comma restoration. We describe our annotation
scheme and error correction system and evaluation
in sections 6 and 7. Finally, we summarize and out-
line plans for future research in section 8.
2 Previous Work
The only reported research that we are aware
of which specifically deals with comma errors in
learner writing is reported in Hardt (2001) and Ale-
gria et al (2006), two studies that deal with Dan-
ish and Basque, respectively. Hardt (2001) employs
an error driven approach featuring the Brill tagger
(Brill, 1993). The Brill tagger works as it would
for the part-of-speech tagging task for which it was
designed, i.e. it learns rules based on templates by
iterating over a large corpus. This work is also eval-
uated on native text where all existing commas are
considered correct, and additional ?erroneous? com-
mas are added randomly to a sub-corpus, so that the
tagger can learn from the errors. The system is tested
on a distinct subset for the task of correcting exist-
ing comma errors and achieves 91.4% precision and
76.9% recall.
Alegria et al (2006) compare implementations
of Naive Bayes, decision-tree, and support vector
machine (SVM) classifiers and utilize a feature set
based on word-forms, categories, and syntactic in-
formation about each decision point. While the sys-
tem is designed as a possible means for correcting
errors, it is only evaluated on the task of restor-
ing commas in well-formed text produced by native
writers. The system obtains good precision (96%)
and recall (98.3%) for correctly not inserting com-
mas, but performs less well at actually inserting
commas (69.6% precision, 48.6% recall).
It is important to note that the results in both of the
projects are based on constructed errors in an other-
wise native corpus which is free of any other con-
textual errors that might be present in actual learner
data. Moreover, as we will show in section 6, er-
rors of omission (failing to use needed commas) are
much more common than errors of commission (in-
serting commas inappropriately) in the English as
a Foreign Language (EFL) data that we use. Cru-
cially, our error correction efforts described in sec-
tion 7 must be able to account for noise and be able
to insert new commas as well as remove erroneous
ones, as we do evaluate on a set of English learner
essays.
Although we have not found any work published
specifically on correcting comma errors in English,
for language learners or otherwise, there is a fairly
large amount of work that focuses on the task of
comma restoration. Comma restoration refers to
placing commas in a sentence which is presented
with no sentence internal punctuation. This task is
285
mostly attempted in the larger context of Automatic
Speech Recognition (ASR), since there are no ab-
solute cues of where commas should be placed in a
stream of speech. Many of these systems use feature
sets that include prosodic elements that are clearly
not available for text based work (see e.g., Favre
et al, 2009; Huang and Zweig, 2002; Moniz et al,
2009).
There are, however, a few punctuation restora-
tion projects that have used well-formed text-only
data. Shieber and Tao (2003) explore restoring com-
mas to the Wall Street Journal (WSJ) section of
the Penn Treebank (PTB). The authors augment a
HMM trigram-based system with constituency parse
information at each insertion point. Using fully
correct parses directly from the PTB, the authors
achieve an F-score of 74.8% and sentence accuracy
of 57.9%1. However, a shortcoming of this method-
ology is that it dictates that all commas are missing,
but these parses were generated with comma infor-
mation present in the sentence and moreover hand-
corrected by human annotators. Using parses auto-
matically generated with commas removed from the
data, they achieve an F-score of 70.1% and sentence
accuracy of 54.9%.
More recently, Gravano et al (2009), who work
with newswire text, including WSJ, pursue the task
of inserting all punctuation and correcting capital-
ization in a string of text in a single pass, rather
than just comma restoration, but do provide results
based solely on comma insertion. The authors em-
ploy an n-gram language model and experiment with
n-grams from size n = 3 to n = 6, and with different
training data sizes. The result relevant to our work is
their comma F-score on WSJ test data, which is just
over 60% when using 5-grams and 55 billion train-
ing tokens. Baldwin and Joseph (2009) also restore
punctuation and capitalization to newswire texts, us-
ing machine based learning with retagging. Their
results are difficult to compare with our work be-
cause they use a different data set and do not focus
on commas in their evaluation.
Lu and Ng (2010) take an approach that inserts all
1Sentence accuracy is a measure used by some in the field
that counts sentences with 100% correct comma decisions as
correct, and any sentence where a comma is missing or mis-
takenly placed as incorrect. It is motivated by the idea that all
commas are essential to understanding a sentence.
punctuation symbols into text. They use transcribed
English and Chinese speech data and do not provide
specific evaluation for commas, however one im-
portant contribution of their research to our current
task is the finding that Conditional Random Fields
(CRFs) perform better at this task than Hidden Event
Language Models, another algorithm that has been
used for restoration. One reason for this could be
CRFs? better handling of long range dependencies
because they model the entire sequence, rather than
making a singular decision based on information at
each point in the sequence (Liu et al, 2005). CRFs
also do not suffer from the label bias problem that
affects Maximum Entropy classifiers (Lafferty et al,
2001).
3 Comma Usage
One of the challenges present in this research is the
ambiguity as to what constitutes ?correct? comma
usage in American English. For one thing, not
all commas contribute to grammaticality; some are
more tied to stylistic rules and preferences. While
there are certainly rule-based decision points for
comma insertion (Doran, 1998), particularly in the
case of commas that set off significant chunks or
phrases within sentences, there are also some com-
mas that appear to be more prescriptive, as they have
less of an effect on sentence processing (such as in
example (2) in the introduction), and opposing us-
age rules for the same contexts are attested in differ-
ent style manuals. A common example of opposing
rules is the notorious serial or Oxford comma that
refers to the final comma found in a series, which
is required by the Chicago Manual of Style (Univer-
sity of Chicago, 1993), but is considered incorrect
by the New York Times Manual of Style (Siegal and
Connolly, 1999).
As a starting point, we needed to know what kinds
of commas are taught by English language teachers,
as well as what style manuals recommend and/or re-
quire. However, creating a list of comma uses was
a non-trivial part of the process. After consulting
style manuals (University of Chicago, 1993; Siegal
and Connolly, 1999; Strunk and White, 1999) and
popular ESL websites, we compiled a list of over 30
rules for use of commas in English. We took the
most commonly mentioned rules and created a final
286
Rule Example
Elements in a List Paul put the kettle on, Don fetched the teapot, and I made tea.
Initial Word/Phrase Hopefully, this car will last for a while.
Dependent Clause After I brushed the cat, I lint-rollered my clothes.
Independent Clause I have finished painting, but he is still sanding the doors.
Parentheticals My father, a jaded and bitter man, ate the muffin.
Quotations ?Why,? I asked, ?do you always forget to do it??
Adjectives She is a strong, healthy woman.
Conjunctive Adverbs I would be happy, however, to volunteer for the Red Cross.
Contrasting Elements He was merely ignorant, not stupid.
Numbers 345,280,000
Dates She met her husband on December 5, 2003.
Geographical Names I lived in San Francisco, California, for 20 years.
Titles Al Mooney, M.D., is a good doctor
Introducing Words You may be required to bring many items, e.g., spoons, pans, and flashlights.
Other Catch-all rule for any other comma use
Table 1: Common Comma Uses
list of 15 usage rules (the 14 most common plus one
miscellaneous category) for our annotation scheme,
which is discussed in section 6. These rules are
given in Table 1. The 16 rules that were removed
from the list occurred in only one source or were
similar enough to other rules to be conflated. It is
worth noting here that while many of the comma
uses in this table might be best served by some sta-
tistical methodology like the one we describe in sec-
tion 4, one can envision fairly simple heuristic rules
to insert commas and find errors in numbers, dates,
geographical names, titles, and introducing words.
4 Classifier and Features
We use CRFs2 as the basis for our system and treat
the task of comma insertion as a sequence label-
ing task; each space between words is considered
by the classifier, and a comma is either inserted or
not. The feature set incorporates features that have
proven useful in comma restoration and other error
correction tasks, as well as a handful of new features
devised for this specific task (combination and dis-
tance features). The full set of features used in our
final system is given in Figure 1 along with exam-
ples of each feature for the sentence If the teacher
easily gets mad , then the child will always fear go-
ing to school and class. The target insertion point is
after the word mad.
2http://crfpp.sourceforge.net/
Feature Example(s)
Lexical and Syntactic Features
unigram easily, gets, mad, then, the
bigram easily gets, gets mad, mad then, ...
trigram easily gets mad, gets mad then, ...
pos uni RB, VBZ, JJ, RB, DT
pos bi RB VBZ, VBZ JJ, JJ RB, ...
pos tri RB VBZ JJ, VBZ JJ RB, ...
combo easily+RB, gets+VBZ,mad+JJ, ...
first combo If+RB
Distance Features
bos dist 5
eos dist 10
prevCC dist -
nextCC dist 9
Figure 1: CRF Features with examples for:
If the teacher easily gets mad , then the child will always
fear going to school and class.
4.1 Lexical and Syntactic Features
The first six features in Figure 1 refer to simple uni-
grams, bigrams, and trigrams of the words and POS
tags in a sliding 5 word window (target word, +/- 2
words). The lexical items help to encode any id-
iosyncratic relationships between words and com-
mas that might not be exploited through the exami-
nation of more in-depth linguistic features. For ex-
ample, then is a special case of an adverb (RB) that
is often preceded by a comma, even if other adverbs
are not, so POS tags might not capture this relation-
287
ship. The lexical items also provide an approxima-
tion of a language model or hidden event language
model approach, which has proven to be useful in
comma restoration tasks (see e.g. Lu and Ng, 2010).
The POS features abstract away from the words
and avoid the problem of data sparseness by allow-
ing the classifier to focus on the categories of the
words, rather than the lexical items themselves. The
combination (combo) feature is a unigram of the
word+pos for every word in the sliding window. It
reinforces the relationship between the lexical items
and their POS tags, further strengthening the evi-
dence of entries like then RB. All of these features
have been used in previous grammatical error detec-
tion tasks which target particle, article, and prepo-
sition errors (c.f., Dickinson et al, 2011; Gamon,
2010; Tetreault and Chodorow, 2008).
The first combo feature keeps track of the first
combination feature of the sentence so that it can
be referred to by the classifier throughout process-
ing the entire sentence. This feature is helpful when
an introductory phrase is longer than the classifier?s
five word window. Figure 1 provides a good exam-
ple of the utility of this feature, as If the teacher eas-
ily gets mad is so long that by the time the window
has moved to the target position of the space follow-
ing mad, the first word and POS, If RB, which can
often indicate an introductory phrase, is beyond the
scope of the sliding window.
4.2 Distance Features
Next, we encode four distance features. We keep
track of the following distances: from the beginning
of the sentence (bos dist), to the end of the sentence
(eos dist), from the previous coordinating conjunc-
tion (prevCC dist), and to the next coordinating con-
junction (nextCC dist). All of these distance fea-
tures help the classifier by encoding measures for
components of the sentence that can affect the deci-
sion to insert a comma. These features are especially
helpful over long range dependencies, when the in-
formation encoded by the feature is far outside the
scope of the 5-word window the CRF uses. The dis-
tance to the beginning of the sentence helps to en-
code introductory words and phrases, which make
up the bulk of the commas used in essays by learners
of English. The distance to the end of the sentence
is less obviously useful, but it can let the classifier
know the likelihood of a phrase beginning or ending
at a certain point in the sentence. The distances to
and from the nearest CC are useful because many
commas are collocated with coordinating conjunc-
tions. The distance features, as well as first combo,
were designed specifically for the task of comma er-
ror correction, and have not, as far as we know, been
utilized in previous research.
5 Comma Restoration
Before applying our system to the task of error cor-
rection, we tested its utility in restoring commas in
newswire texts. Specifically, we evaluate on section
23 of the WSJ, training on sections 02-22. Here,
the task is straightforward: we remove all commas
from the test data and performance is measured on
the system?s ability to put the commas back in the
right places. After stripping all commas from our
test data, the text is tokenized and POS tagged using
a maximum entropy tagger (Ratnaparkhi, 1996) and
every token is considered by the classifier as either
requiring a following comma or not. Out of 53,640
tokens, 3062 should be followed by a comma. We
provide accuracy, precision, recall, F1-score, and
sentence accuracy (S Acc.) for these tests, along
with results from Gravano et al (2009) and Shieber
and Tao (2003) in Table 2. The first system (LexSyn)
includes only the lexical and syntactic features from
Figure 1; the second (LexSyn+Dist) includes all of
the features.
System Acc. P R F S Acc.
LexSyn 97.4 85.8 64.9 73.9 60.5
LexSyn+Dist 97.5 85.8 66.3 74.8 61.4
Shieber & Tao 97.0 79.7 62.6 70.1 54.9
Gravano et al N.A. 57 67 ?61 N.A.
Table 2: Comma Restoration System Results (%)
As can be seen in Table 2, the full system
(LexSyn+Dist) performs significantly better than
WSJ LexSyn (p < .02, two-tailed), achieving an
F-score of 74.8 on WSJ. This F-score outperforms
Shieber and Tao?s system, which was also tested on
section 23 of the WSJ, by about 4% and our sentence
accuracy of 61.5% is about 7% higher than theirs.
Our F-score is also about 13% higher than that of
Gravano et al (2009), however, they evaluate on the
288
entire WSJ section of the Penn Treebank, so it is not
totally fair to compare results.
6 Annotation
For the comma restoration task, we needed only to
obtain well-formed text and remove the commas to
produce a test set. However, this is not so in the case
of error correction. In order to test a system that
corrects errors in learner essays, we need an anno-
tated test corpus that tells us where the errors are.
Although there are a handful of corpora that include
punctuation errors in their annotation scheme, such
as NUCLE (Dahlmeier and Ng, 2011) and HOO
(Dale and Kilgarriff, 2010), there are none to our
knowledge that focus specifically on commas. Thus,
we designed and implemented our own annotation
scheme on a set of essays to allow us the freedom to
identify the most important aspects of comma usage
for our work.
Our annotation scheme allows the mark-up of a
number of aspects of comma usage. First, each
comma in a text is marked as rejected or accepted
by the annotator. Additionally, any space between
words can be treated as an insertion point for a miss-
ing comma. The annotators also marked all accepted
and inserted commas as either required or optional.
Finally, the annotation also includes the appropriate
usage rule from the set in Table 1.3 In contrast, the
NUCLE and HOO data sets do not have this gran-
ularity of information (the annotation only indicates
whether a comma should be inserted or removed)
and are not exhaustively annotated.
After a one-hour training session on comma us-
age rules, three native English speakers were given a
set of ten learner essays comprising 3,665 tokens to
annotate for comma errors. To assess the difficulty
of the annotation task, we calculated agreement and
kappa. Agreement is a simple measure of how often
the annotators agree, and kappa provides a more ro-
bust measure of agreement since it takes chance into
account (Cohen, 1960). Table 3 provides the results
of these measurements. As can be seen in the table,
the agreement is quite high at either 97 or 98%, and
kappa is a bit lower, ranging from 72 to 81%. The
3The full annotation manual is available at
http://www.cs.rochester.edu/?tetreaul/
comma-manual.pdf
agreement is likely so high due to the great number
of decision points where it is obvious to any native
writer that no comma is needed. To account for this
imbalance, we also provide an adjusted agreement
in the final column of the table that excludes all de-
cisions where both annotators agree that no comma
is necessary.
Annotators Agreement Kappa Adj. agr.
1 & 2 97 74 61
1 & 3 98 72 61
2 & 3 98 81 76
Table 3: Agreement over Annotation Training Set (%)
After completing the training phase, we assigned
one annotator the task of annotating our develop-
ment and test data from two different corpora: es-
says written by English as a foreign language learn-
ers (EFL) and essays written by native speakers of
English (Native). For both data sets we selected 60
essays for development and 60 essays for test. The
annotation was carried out using an annotation tool
developed in-house that gives the annotator an easy
to use interface and outputs standoff annotations in
xml format. (3) is an example of an annotated sen-
tence from an EFL essay, where ? ?? marks a span
for annotation.
(3) The new millenium , 1 the 21st century 2
has dawned upon us 3 and this new century
has brought many positive advancements in our
daily lives .
1) Accept, required, parenthetical
2) Insert, required, parenthetical
3) Insert, required, independent clause
Table 4 provides the comma usage information for
the essays in both sets used in development and test-
ing. The table shows the total number of sentences,
commas in the original text that were accepted by
the annotator, and errors (rejected and missing com-
mas) for the 60 essays in each set.
As can be seen in Table 4, the majority of exist-
ing commas (columns Accept plus Rej) in the texts
were accepted by the annotator; about 84% in the
EFL development set, 87% in the EFL test set, 85%
in the Native development set, and 88% in the Na-
tive test set. The important fact uncovered by these
numbers is that most of the commas that learners do
289
Data Set Sent
Commas
Accept
Errors
Rej Miss
EFL Dev 717 474 49 233
EFL Test 683 427 65 232
Native Dev 970 506 86 363
Native Test 839 377 50 314
Table 4: Comma Usage Statistics
use are correct. However, there are a great number
of commas that the annotator inserted (over 80% of
all errors are missing commas) meaning that these
learners are more prone to underusing than overus-
ing commas. Another interesting fact that can be
gleaned from our annotation is that the top five
comma uses, those listed in the first five rows of Ta-
ble 1, account for more than 80% of all commas in
these essays.
7 Error Correction
With a competitive comma restoration system in
place, we turn to the primary task of correcting er-
rors in learner essays. While the task remains simi-
lar to comma restoration, error correction in student
writing brings a new set of challenges, especially
when the writers are non-native. Newswire texts are
most often well-formed, so the system should not
experience interference from other contextual errors
around the missing commas. Sentences taken from
learner texts, though, often contain multiple errors
that can make it difficult to focus on a single problem
at a time. Spelling errors, for example, can exacer-
bate error correction efforts that use contextual lex-
ical features because well-formed text that is often
used for training data is usually free of such noise.
In these experiments, we use the annotated es-
says described in section 6 for evaluation and train
on 40,000 sentences taken from essays written by
both native and non-native high level college stu-
dents. All of the essays are run through automatic
spelling correction to reduce the noise in the test set
before being tagged with the same tagger used in the
comma restoration experiments.
Because we approach comma error correction as
essentially a comma restoration task, we can we use
largely the same system for error correction as we
did for comma restoration. We still employ CRFs
and label each space between words as requiring
a comma or not, however, there is one significant
change to our methodology for this task. Namely,
we can leave the commas that were present in the
text as provided by the writer as we pre-process
the data for error correction, whereas they were re-
moved in the comma restoration task. For error cor-
rection, the task is really comparing the system?s an-
swer to the annotator?s and the learner?s, as opposed
to simply inserting commas into raw text. Leaving
the learners? commas in the text does introduce some
errors to the POS tagging phase. However, since
over 85% of the existing commas in the development
set were judged as acceptable by our annotator (cf.
section 6) , the number of erroneous commas is not
so great as to contaminate the system. Removing all
of the commas would introduce unnecessary errors
in the pre-processing phase.
We also augment the system with three post-
processing filters that we tuned on the development
set. One requires that the classifier be completely
confident before a change is made to an existing
comma; crf++ will give 100% confidence to a single
class in some cases. This filter is based on the fact
that 85% of the existing commas can be expected
to be correct. A similar filter requires that the clas-
sifier be at least 90% confident in a decision to in-
sert a new comma. The final filter, which overrides
any other information provided by the system, does
not allow commas to be inserted before the word be-
cause. These ensure high precision even though they
may reduce recall.
Table 5 provides the accuracy, precision, recall,
F-score, and number of errors in each set for tests on
our 60 annotated EFL and Native essays, and the re-
sult for the combined corpus. The system performs
quite well on the EFL test set, with scores of 94%
precision, 31.7% recall, and 47.4% F-score for the
LexSyn+Dist system. The results for the Native set
are a bit lower, with 84.9% precision, 20% recall,
and 32.4% F-score for the LexSyn+Dist system.
For both data sets, when the distance features are
added to the model, precision increases by 1%, and
in the EFL set, recall also increases. In keeping with
practices established within the field of grammatical
error correction, the system has been optimized for
high precision even at the cost of recall, to ensure
that feedback systems avoid confusing learners by
290
Data System Acc. P R F n
EFL
LexSyn 98.2 92.9 30.9 46.5 297
LexSyn+Dist 98.3 94.0 31.7 47.4 297
Native
LexSyn 97.8 83.9 20.0 32.3 365
LexSyn+Dist 97.8 84.9 20.0 32.4 365
Combined
LexSyn 98.1 88.7 24.9 38.9 662
LexSyn+Dist 98.1 89.8 25.2 39.4 662
Table 5: Comma Error Correction Results (%)
marking correct comma usage as erroneous. Con-
sidering performance over all of the test data, the
system achieves over 89% precision and 25% recall,
results which are comparable to those in other er-
ror correction tasks. For example, the preposition
error detection system described in Tetreault and
Chodorow (2008) achieved 84% precision, 19% re-
call for prepositions.
It is worth noting that the results in Table 5 in-
clude commas that the annotator had marked as
optional. For these, whatever decision the system
makes is scored as correct. Since the grammatical-
ity/readability of the sentence will not be affected by
the presence or absence of a comma in these cases,
we feel this is the fairest assessment of the system.
7.1 Error Analysis
In order to get a sense of what kinds of construc-
tions are difficult for our system, we randomly ex-
tracted 50 sentences from the output that exhibited
at least one wrong comma decision made by the sys-
tem. The 50 sentences contained a combined total of
62 system errors. Among these cases, the most com-
mon context where the system makes the wrong de-
cision is in introductory words and phrases, which is
not surprising given the frequency with which com-
mas occur in these environments in our development
set (about 40% of all commas in the essays). In (4),
for example, the first word, Here, should be followed
by a comma. Since Here is not a common introduc-
tory word in this type of sentence structure in the
training data, this is a difficult case for the system to
correct.
(4) Here we can get specific knowledge in the sci-
ence that we like the most .
The next most common misclassification involves
comma splices, i.e. conjoining complete sentences
with a comma rather than separating them with a full
stop. In (5), for example, there should be a full stop
between college and I, rather than a comma. This
result is not surprising because the system is not
yet equipped to deal with comma splices. Comma
splices are a different type of phenomenon because
correcting them requires removing the comma and
inserting a full stop, essentially two separate steps
rather than the single reject/accept step that the sys-
tem currently handles.
(5) I entered college, I could learn it and make an
effort to achieve my goal.
The next most common context for system errors
was between clauses that are conjoined with a co-
ordinating conjunction as in (6), where there should
not be a comma. In (6), the second clause is actually
a dependent clause, so no comma should precede
the coordinating conjunction. There are a number
of system errors dealing with commas between two
independent clauses. For example in (7), our annota-
tor recommended a comma between things and but,
however the system did not make the insertion. The
problem with these examples likely stems from the
fact that the rule for comma usage in these contexts
is not clearly stated, even in well-respected manu-
als, and therefore likely not clearly understood, even
by high-level native writers. For example, the NYT
style manual (Siegal and Connolly, 1999) states that
?Commas should be used in compound sentences
before conjunctions... When the clauses are excep-
tionally short, however, the comma may be omit-
ted.? Adding a feature that measures clause length
might help, but even then the classifier must rely on
training data that may have considerable variation
as to what length of clauses requires an intervening
comma.
291
(6) They wants to see their portfolio, and what kind
of skill do they have for company.
(7) I have many things but the best is my parents.
Another facet of the data that consistently chal-
lenges the system is the existence of errors other
than the commas in the sentences. Consider the sen-
tence in (8), where erroneous is the original text
from the essay and corrected is a well-formed in-
terpretation.
(8) erroneous: In the other hand , having just
one specific subject , which represents a great
downfall for many students
corrected: On the other hand, knowing only
one subject is a downfall for many students.
The comma after subject is unnecessary, but so is
the word which. In fact, which would normally sig-
nify the beginning of a non-restrictive clause in this
context, which should be set off with a comma. It
is no surprise then, that the system has trouble re-
moving commas in these types of contexts. At least
11 of the 62 system mistakes that we examined have
grammatical errors in the immediate context of the
comma in question, which makes the classification
more difficult.
8 Summary and Conclusion
We presented a novel comma error correction sys-
tem for English that achieves an average of 89% pre-
cision and 25% recall on essays written by learn-
ers of different levels and language backgrounds,
including native English speakers. The system
achieves state-of-the-art performance on the task of
comma restoration, beating previous systems? F-
score and sentence accuracy by 4% and 7%, respec-
tively. We discovered that augmenting lexical fea-
tures, which have been commonly used in previous
work, with the combination and distance features
can improve F-score by as much as 1% in both the
error correction and comma restoration tasks. We
also developed and implemented a novel comma er-
ror annotation scheme.
Additionally, we are interested in the effect of
correct comma placement on other NLP processes.
Jones (1994) and Briscoe and Carroll (1995) show
that adding punctuation to grammars that utilize
part-of-speech (POS) tags, rather than lexical items,
adds more structure and reduces ambiguity as well
as the number of parses for each sentence. Simi-
larly, Doran (1998) and White and Rajkumar (2008)
found that adding punctuation improved parsing re-
sults in tree-adjoining grammar (TAG) and combi-
natorial categorial grammar (CCG) parsing, respec-
tively. These studies all highlight the importance of
correctly inserted punctuation, especially commas,
for parsing. Given these results, we believe that by
enhancing the quality of the text, comma error cor-
rection will improve not only tagging and parsing,
but also the ability of systems to correct many other
forms of grammatical errors, such as those involv-
ing incorrect word order, number disagreement, and
misuse of prepositions, articles, and collocations.
Acknowledgments
We would like to thank Melissa Lopez for help with
annotating our corpora and Michael Flor for kindly
developing an annotation tool for our purposes. We
also thank Aoife Cahill, Robbie Kantor, Markus
Dickinson, Michael Heilman, Nitin Madnani, and
our anonymous reviewers for insightful comments
and discussion.
References
In?aki Alegria, Bertol Arrieta, Arantza Diaz de Ilar-
raza, Eli Izagirre, and Montse Maritxalar. 2006.
Using machine learning techniques to build a
comma checker for Basque. In Proceedings of the
COLING/ACL main conference poster sessions.
Timothy Baldwin and Manuel Paul Anil Kumar
Joseph. 2009. Restoring punctuation and casing
in English text. In Australasian Conference on
Artificial Intelligence?09.
E. Brill. 1993. A Corpus-Based Approach to Lan-
guage Learning. Ph.D. thesis, The University of
Pennsylvania, Philadelpha, PA.
Ted Briscoe and John Carroll. 1995. Developing and
evaluating a probabilistic LR parser of part-of-
speech and punctuation labels. In Proceedings of
the ACL/SIGPARSE 4th International Workshop
on Parsing Technologies.
Jacob Cohen. 1960. A coefficient of agreement for
292
nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Robert J. Connors and Andrea A. Lunsford. 1988.
Frequency of formal errors in current college
writing, or Ma and Pa Kettle do research. Col-
lege Composition and Communication, 39(4).
Daniel Dahlmeier and Hwee Tou Ng. 2011. Gram-
matical error correction with alternating structure
optimization. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies - Volume 1. Association for Computational
Linguistics.
Robert Dale and Adam Kilgarriff. 2010. Helping
our own: Text massaging for computational lin-
guistics as a new shared task. In International
Conference on Natural Language Generation.
Rachele De Felice and Stephen Pulman. 2008. A
classifier-based approach to preposition and de-
terminer error correction in L2 English. In Pro-
ceedings of COLING-08. Manchester.
Markus Dickinson, Ross Israel, and Sun-Hee Lee.
2011. Developing methodology for Korean par-
ticle error detection. In Proceedings of the 6th
Workshop on Innovative Use of NLP for Building
Educational Applications. Portland, Oregon.
Christine Doran. 1998. Incorporating Punctuation
into the Sentence Grammar: A Lexicalized Tree-
Adjoining Grammar Perspective. Ph.D. thesis,
University of Pennsylvania.
Benoit Favre, Dilek Hakkani-Tur, and Elizabeth
Shriberg. 2009. Syntactically-informed models
for comma prediction. In Proceedings of the
2009 IEEE International Conference on Acous-
tics, Speech and Signal Processing.
Michael Gamon. 2010. Using mostly native data
to correct errors in learners? writing: A meta-
classifier approach. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics.
Michael Gamon. 2011. High-order sequence model-
ing for language learner detection high-order se-
quence modeling for language learner error de-
tection. In Proceedings of the 6th Workshop on
Innovative Use of NLP for Building Educational
Applications.
Agustin Gravano, Martin Jansche, and Michiel Bac-
chiani. 2009. Restoring punctuation and capi-
talization in transcribed speech. In Proceedings
of the 2009 IEEE International Conference on
Acoustics, Speech and Signal Processing.
Daniel Hardt. 2001. Comma Checking in Danish. In
Corpus Linguistics.
Robin L. Hill and Wayne S. Murray. 1998. Commas
and spaces: The point of punctuation. In 11th An-
nual CUNY Conference on Human Sentence Pro-
cessing.
Jing Huang and Geoffrey Zweig. 2002. Maximum
entropy model for punctuation annotation from
speech. In Proceedings of ICSLP 2002.
Bernard E. M. Jones. 1994. Exploring the role of
punctuation in parsing natural text. In Proceed-
ings of the 15th conference on Computational lin-
guistics - Volume 1.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning.
Claudia Leacock, Martin Chodorow, Michael Ga-
mon, and Joel R. Tetreault. 2010. Auto-
mated Grammatical Error Detection for Lan-
guage Learners. Synthesis Lectures on Hu-
man Language Technologies. Morgan & Claypool
Publishers.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, and
Mary Harper. 2005. Comparing hmm, maximum
entropy, and conditional random fields for disflu-
ency detection. In In Proceeedings of the Euro-
pean Conference on Speech Communication and
Technology.
Wei Lu and Hwee T. Ng. 2010. Better punctua-
tion prediction with dynamic conditional random
fields. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing.
Helena Moniz, Fernando Batista, Hugo Meinedo,
and Alberto Abad. 2009. Prosodically-based au-
tomatic segmentation and punctuation. In Pro-
293
ceedings of the 5th International Conference on
Speech Prosody.
Diane Nicholls. 1999. The cambridge learner corpus
- error coding and analysis for writing dictionaries
and other books for english learners. In Summer
Workshop on Learner Corpora. Showa Woman?s
University.
Hiromi Oyama. 2010. Automatic error detection
method for japanese particles. Polyglossia, 18.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Eric Brill
and Kenneth Church, editors, Proceedings of the
Empirical Methods in Natural Language Process-
ing.
Alla Rozovskaya, Mark Sammons, Joshua Gioja,
and Dan Roth. 2011. University of Illinois sys-
tem in HOO text correction shared task. In Pro-
ceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 263?266. Association
for Computational Linguistics, Nancy, France.
Stuart M. Shieber and Xiaopeng Tao. 2003. Comma
restoration using constituency information. In
Proceedings of the 2003 Human Language Tech-
nology Conference and Conference of the North
American Chapter of the Association for Compu-
tational Linguistics.
Allan M. Siegal and William G. Connolly. 1999. The
New York Times Manual of Style and Usage : The
Official Style Guide Used by the Writers and Edi-
tors of the World?s Most Authoritative Newspaper.
Crown, rev sub edition.
William Strunk and E. B. White. 1999. The Ele-
ments of Style, Fourth Edition. Longman, fourth
edition.
Joel Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL
writing. In Proceedings of COLING-08. Manch-
ester.
Joel Tetreault, Jennifer Foster, and Martin
Chodorow. 2010. Using parse features for
preposition selection and error detection. In
Proceedings of the ACL 2010 Conference Short
Papers.
University of Chicago. 1993. The Chicago Manual
of Style. University Of Chicago Press, Chicago,
fourteenth edition.
Michael White and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Pro-
ceedings of the Workshop on Grammar Engineer-
ing Across Frameworks.
294
Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 8?16,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Building a Korean Web Corpus for Analyzing Learner Language
Markus Dickinson
Indiana University
md7@indiana.edu
Ross Israel
Indiana University
raisrael@indiana.edu
Sun-Hee Lee
Wellesley College
slee6@wellesley.edu
Abstract
Post-positional particles are a significant
source of errors for learners of Korean. Fol-
lowing methodology that has proven effective
in handling English preposition errors, we are
beginning the process of building a machine
learner for particle error detection in L2 Ko-
rean writing. As a first step, however, we must
acquire data, and thus we present a method-
ology for constructing large-scale corpora of
Korean from the Web, exploring the feasibil-
ity of building corpora appropriate for a given
topic and grammatical construction.
1 Introduction
Applications for assisting second language learners
can be extremely useful when they make learners
more aware of the non-native characteristics in their
writing (Amaral and Meurers, 2006). Certain con-
structions, such as English prepositions, are difficult
to characterize by grammar rules and thus are well-
suited for machine learning approaches (Tetreault
and Chodorow, 2008; De Felice and Pulman, 2008).
Machine learning techniques are relatively portable
to new languages, but new languages bring issues in
terms of defining the language learning problem and
in terms of acquiring appropriate data for training a
machine learner.
We focus in this paper mainly on acquiring data
for training a machine learning system. In partic-
ular, we are interested in situations where the task
is constant?e.g., detecting grammatical errors in
particles?but the domain might fluctuate. This is
the case when a learner is asked to write an essay on
a prompt (e.g., ?What do you hope to do in life??),
and the prompts may vary by student, by semester,
by instructor, etc. By isolating a particular domain,
we can hope for greater degrees of accuracy; see,
for example, the high accuracies for domain-specific
grammar correction in Lee and Seneff (2006).
In this situation, we face the challenge of obtain-
ing data which is appropriate both for: a) the topic
the learners are writing about, and b) the linguistic
construction of interest, i.e., containing enough rel-
evant instances. In the ideal case, one could build
a corpus directly for the types of learner data to
analyze. Luckily, using the web as a data source
can provide such specialized corpora (Baroni and
Bernardini, 2004), in addition to larger, more gen-
eral corpora (Sharoff, 2006). A crucial question,
though, is how one goes about designing the right
web corpus for analyzing learner language (see, e.g.,
Sharoff, 2006, for other contexts)
The area of difficulty for language learners which
we focus on is that of Korean post-positional parti-
cles, akin to English prepositions (Lee et al, 2009;
Ko et al, 2004). Korean is an important language
to develop NLP techniques for (see, e.g., discussion
in Dickinson et al, 2008), presenting a variety of
features which are less prevalent in many Western
languages, such as agglutinative morphology, a rich
system of case marking, and relatively free word or-
der. Obtaining data is important in the general case,
as non-English languages tend to lack resources.
The correct usage of Korean particles relies on
knowing lexical, syntactic, semantic, and discourse
information (Lee et al, 2005), which makes this
challenging for both learners and machines (cf. En-
8
glish determiners in Han et al, 2006). The only
other approach we know of, a parser-based one, had
very low precision (Dickinson and Lee, 2009). A
secondary contribution of this work is thus defin-
ing the particle error detection problem for a ma-
chine learner. It is important that the data represent
the relationships between specific lexical items: in
the comparable English case, for example, interest
is usually found with in: interest in/*with learning.
The basic framework we employ is to train a ma-
chine learner on correct Korean data and then apply
this system to learner text, to predict correct parti-
cle usage, which may differ from the learner?s (cf.
Tetreault and Chodorow, 2008). After describing the
grammatical properties of particles in section 2, we
turn to the general approach for obtaining relevant
web data in section 3, reporting basic statistics for
our corpora in section 4. We outline the machine
learing set-up in section 5 and present initial results
in section 6. These results help evaluate the best way
to build specialized corpora for learner language.
2 Korean particles
Similar to English prepositions, Korean postposi-
tional particles add specific meanings or grammat-
ical functions to nominals. However, a particle can-
not stand alone in Korean and needs to be attached
to the preceding nominal. More importantly, par-
ticles indicate a wide range of linguistic functions,
specifying grammatical functions, e.g., subject and
object; semantic roles; and discourse functions. In
(1), for instance, ka marks both the subject (func-
tion) and agent (semantic role), eykey the dative and
beneficiary; and so forth.1
(1) Sumi-ka
Sumi-SBJ
John-eykey
John-to
chayk-ul
book-OBJ
ilhke-yo
read-polite
?Sumi reads a book to John.?
Particles can also combine with nominals to form
modifiers, adding meanings of time, location, instru-
ment, possession, and so forth, as shown in (2). Note
in this case that the marker ul/lul has multiple uses.2
1We use the Yale Romanization scheme for writing Korean.
2Ul/lul, un/nun, etc. only differ phonologically.
(2) Sumi-ka
Sumi-SBJ
John-uy
John-GEN
cip-eyse
house-LOC
ku-lul
he-OBJ
twu
two
sikan-ul
hours-OBJ
kitaly-ess-ta.
wait-PAST-END
?Sumi waited for John for (the whole) two hours in
his house.?
There are also particles associated with discourse
meanings. For example, in (3) the topic marker nun
is used to indicate old information or a discourse-
salient entity, while the delimiter to implies that
there is someone else Sumi likes. In this paper, we
focus on syntactic/semantic particle usage for nom-
inals, planning to extend to other cases in the future.
(3) Sumi-nun
Sumi-TOP
John-to
John-also
cohahay.
like
?Sumi likes John also.?
Due to these complex linguistic properties, parti-
cles are one of the most difficult topics for Korean
language learners. In (4b), for instance, a learner
might replace a subject particle (as in (4a)) with an
object (Dickinson et al, 2008). Ko et al (2004) re-
port that particle errors were the second most fre-
quent error in a study across different levels of Ko-
rean learners, and errors persist across levels (see
also Lee et al, 2009).
(4) a. Sumi-nun
Sumi-TOP
chayk-i
book-SBJ
philyohay-yo
need-polite
?Sumi needs a book.?
b. *Sumi-nun
Sumi-TOP
chayk-ul
book-OBJ
philyohay-yo
need-polite
?Sumi needs a book.?
3 Approach
3.1 Acquiring training data
Due to the lexical relationships involved, machine
learning has proven to be a good method for sim-
ilar NLP problems like detecting errors in En-
glish preposition use. For example Tetreault and
Chodorow (2008) use a maximum entropy classifier
to build a model of correct preposition usage, with
7 million instances in their training set, and Lee and
Knutsson (2008) use memory-based learning, with
10 million sentences in their training set. In expand-
ing the paradigm to other languages, one problem
9
is a dearth of data. It seems like a large data set is
essential for moving forward.
For Korean, there are at least two corpora pub-
licly available right now, the Penn Korean Treebank
(Han et al, 2002), with hundreds of thousands of
words, and the Sejong Corpus (a.k.a., The Korean
National Corpus, The National Institute of Korean
Language, 2007), with tens of millions of words.
While we plan to include the Sejong corpus in fu-
ture data, there are several reasons we pursue a dif-
ferent tack here. First, not every language has such
resources, and we want to work towards a language-
independent platform of data acquisition. Secondly,
these corpora may not be a good model for the kinds
of topics learners write about. For example, news
texts are typically written more formally than learner
writing. We want to explore ways to quickly build
topic-specific corpora, and Web as Corpus (WaC)
technology gives us tools to do this.3
3.2 Web as Corpus
To build web corpora, we use BootCat (Baroni and
Bernardini, 2004). The process is an iterative algo-
rithm to bootstrap corpora, starting with various seed
terms. The procedure is as follows:
1. Select initial seeds (terms).
2. Combine seeds randomly.
3. Run Google/Yahoo queries.
4. Retrieve corpus.
5. Extract new seeds via corpus comparison.
6. Repeat steps #2-#5.
For non-ASCII languages, one needs to check
the encoding of webpages in order to convert the
text into UTF-8 for output, as has been done for,
e.g., Japanese (e.g., Erjavec et al, 2008; Baroni and
Ueyama, 2004). Using a UTF-8 version of Boot-
Cat, we modified the system by using a simple Perl
module (Encode::Guess) to look for the EUC-
KR encoding of most Korean webpages and switch
it to UTF-8. The pages already in UTF-8 do not need
to be changed.
3.3 Obtaining data
A crucial first step in constructing a web corpus is
the selection of appropriate seed terms for construct-
ing the corpus (e.g., Sharoff, 2006; Ueyama, 2006).
3Tetreault and Chodorow (2009) use the web to derive
learner errors; our work, however, tries to obtain correct data.
In our particular case, this begins the question of
how one builds a corpus which models native Ko-
rean and which provides appropriate data for the task
of particle error detection. The data should be genre-
appropriate and contain enough instances of the par-
ticles learners know and used in ways they are ex-
pected to use them (e.g., as temporal modifiers). A
large corpus will likely satisfy these criteria, but has
the potential to contain distracting information. In
Korean, for example, less formal writing often omits
particles, thereby biasing a machine learner towards
under-guessing particles. Likewise, a topic with dif-
ferent typical arguments than the one in question
may mislead the machine. We compare the effec-
tiveness of corpora built in different ways in training
a machine learner.
3.3.1 A general corpus
To construct a general corpus, we identify words
likely to be in a learner?s lexicon, using a list of 50
nouns for beginning Korean students for seeds. This
includes basic vocabulary entries like the words for
mother, father, cat, dog, student, teacher, etc.
3.3.2 A focused corpus
Since we often know what domain4 learner es-
says are written about, we experiment with building
a more topic-appropriate corpus. Accordingly, we
select a smaller set of 10 seed terms based on the
range of topics covered in our test corpus (see sec-
tion 6.1), shown in figure 1. As a first trial, we select
terms that are, like the aforementioned general cor-
pus seeds, level-appropriate for learners of Korean.
han-kwuk ?Korea? sa-lam ?person(s)?
han-kwuk-e ?Korean (lg.)? chin-kwu ?friend?
kyey-cel ?season? ga-jok ?family?
hayng-pok ?happiness? wun-tong ?exercise?
ye-hayng ?travel? mo-im ?gathering?
Figure 1: Seed terms for the focused corpus
3.3.3 A second focused corpus
There are several issues with the quality of data
we obtain from our focused terms. From an ini-
tial observation (see section 4.1), the difficulty stems
in part from the simplicity of the seed terms above,
4By domain, we refer to the subject of a discourse.
10
leading to, for example, actual Korean learner data.
To avoid some of this noise, we use a second set of
seed terms, representing relevant words in the same
domains, but of a more advanced nature, i.e., topic-
appropriate words that may be outside of a typical
learner?s lexicon. Our hypothesis is that this is more
likely to lead to native, quality Korean. For each
one of the simple words above, we posit two more
advanced words, as given in figure 2.
kyo-sa ?teacher? in-kan ?human?
phyung-ka ?evaluation? cik-cang ?workplace?
pen-yuk ?translation? wu-ceng ?friendship?
mwun-hak ?literature? sin-loy ?trust?
ci-kwu ?earth? cwu-min ?resident?
swun-hwan ?circulation? kwan-kye ?relation?
myeng-sang ?meditation? co-cik ?organization?
phyeng-hwa ?peace? sik-i-yo-pep ?diet?
tham-hem ?exploration? yen-mal ?end of a year?
cwun-pi ?preparation? hayng-sa ?event?
Figure 2: Seed terms for the second focused corpus
3.4 Web corpus parameters
One can create corpora of varying size and general-
ity, by varying the parameters given to BootCaT. We
examine three parameters here.
Number of seeds The first way to vary the type
and size of corpus obtained is by varying the number
of seed terms. The exact words given to BootCaT af-
fect the domain of the resulting corpus, and utilizintg
a larger set of seeds leads to more potential to create
a bigger corpus. With 50 seed terms, for example,
there are 19,600 possible 3-tuples, while there are
only 120 possible 3-tuples for 10 seed terms, limit-
ing the relevant pages that can be returned.
For the general (G) corpus, we use: G1) all 50
seed terms, G2) 5 sets of 10 seeds, the result of split-
ting the 50 seeds randomly into 5 buckets, and G3)
5 sets of 20 seeds, which expand the 10-seed sets in
G2 by randomly selecting 10 other terms from the
remaining 40 seeds. This breakdown into 11 sets (1
G1, 5 G2, 5 G3) allows us to examine the effect of
using different amounts of general terms and facili-
tates easy comparison with the first focused corpus,
which has only 10 seed terms.
For the first focused (F1) corpus, we use: F11) the
10 seed terms, and F12) 5 sets of 20 seeds, obtained
by combining F11 with each seed set from G2. This
second group provides an opportunity to examine
what happens when augmenting the focused seeds
with more general terms; as such, this is a first step
towards larger corpora which retain some focus. For
the second focused corpus (F2), we simply use the
set of 20 seeds. We have 7 sets here (1 F11, 5 F12, 1
F2), giving us a total of 18 seed term sets at this step.
Tuple length One can also experiment with tuple
length in BootCat. The shorter the tuple, the more
webpages that can potentially be returned, as short
tuples are likely to occur in several pages (e.g., com-
pare the number of pages that all of person happi-
ness season occur in vs. person happiness season
exercise travel). On the other hand, longer tuples are
more likely truly relevant to the type of data of inter-
est, more likely to lead to well-formed language. We
experiment with tuples of different lengths, namely
3 and 5. With 2 different tuple lengths and 18 seed
sets, we now have 36 sets.
Number of queries We still need to specify how
many queries to send to the search engine. The max-
imum number is determined by the number of seeds
and the tuple size. For 3-word tuples with 10 seed
terms, for instance, there are 10 items to choose 3
objects from:
(10
3
)
= 10!3!(10?3)! = 120 possibilities.
Using all combinations is feasible for small seed
sets, but becomes infeasible for larger seed sets, e.g.,
(50
5
)
= 2, 118, 760 possibilities. To reduce this, we
opt for the following: for 3-word tuples, we generate
120 queries for all cases and 240 queries for the con-
ditions with 20 and 50 seeds. Similarly, for 5-word
tuples, we generate the maximum 252 queries with
10 seeds, and both 252 and 504 for the other condi-
tions. With the previous 36 sets (12 of which have
10 seed terms), evenly split between 3 and 5-word
tuples, we now have 60 total corpora, as in table 1.
# of seeds
tuple # of General F1 F2
len. queries 10 20 50 10 20 20
3 120 5 5 1 1 5 1
240 n/a 5 1 n/a 5 1
5 252 5 5 1 1 5 1
504 n/a 5 1 n/a 5 1
Table 1: Number of corpora based on parameters
11
Other possibilities There are other ways to in-
crease the size of a web corpus using BootCaT. First,
one can increase the number of returned pages for a
particular query. We set the limit at 20, as anything
higher will more likely result in non-relevant data
for the focused corpora and/or duplicate documents.
Secondly, one can perform iterations of search-
ing, extracting new seed terms with every iteration.
Again, the concern is that by iterating away from the
initial seeds, a corpus could begin to lose focus. We
are considering both extensions for the future.
Language check One other constraint we use is to
specify the particular language of interest, namely
that we want Korean pages. This parameter is set
using the language option when collecting URLs.
We note that a fair amount of English, Chinese, and
Japanese appears in these pages, and we are cur-
rently developing our own Korean filter.
4 Corpus statistics
To gauge the properties of size, genre, and degree of
particle usage in the corpora, independent of appli-
cation, basic statistics of the different web corpora
are given in table 2, where we average over multiple
corpora for conditions with 5 corpora.5
There are a few points to understand in the table.
First, it is hard to count true words in Korean, as
compounds are frequent, and particles have a de-
batable status. From a theory-neutral perspective,
we count ejels, which are tokens occurring between
white spaces. Secondly, we need to know about the
number of particles and number of nominals, i.e.,
words which could potentially bear particles, as our
machine learning paradigm considers any nominal a
test case for possible particle attachment. We use a
POS tagger (Han and Palmer, 2004) for this.
Some significant trends emerge when comparing
the corpora in the table. First of all, longer queries
(length 5) result in not only more returned unique
webpages, but also longer webpages on average than
shorter queries (length 3). This effect is most dra-
matic for the F2 corpora. The F2 corpora also exhibit
a higher ratio of particles to nominals than the other
web corpora, which means there will be more pos-
5For the 252 5-tuple 20 seed General corpora, we average
over four corpora, due to POS tagging failure on the fifth corpus.
itive examples in the training data for the machine
learner based on the F2 corpora.
4.1 Qualitative evaluation
In tandem with the basic statistics, it is also impor-
tant to gauge the quality of the Korean data from
a more qualitative perspective. Thus, we examined
the 120 3-tuple F1 corpus and discovered a number
of problems with the data.
First, there are issues concerning collecting data
which is not pure Korean. We find data extracted
from Chinese travel sites, where there is a mixture of
non-standard foreign words and unnatural-sounding
translated words in Korean. Ironically, we also find
learner data of Korean in our search for correct Ko-
rean data. Secondly, there are topics which, while
exhibiting valid forms of Korean, are too far afield
from what we expect learners to know, including re-
ligious sites with rare expressions; poems, which
commonly drop particles; gambling sites; and so
forth. Finally, there are cases of ungrammatical uses
of Korean, which are used in specific contexts not
appropriate for our purposes. These include newspa-
per titles, lists of personal names and addresses, and
incomplete phrases from advertisements and chats.
In these cases, we tend to find less particles.
Based on these properties, we developed the
aforementioned second focused corpus with more
advanced Korean words and examined the 240 3-
tuple F2 corpus. The F2 seeds allow us to capture a
greater percentage of well-formed data, namely data
from news articles, encyclopedic texts, and blogs
about more serious topics such as politics, literature,
and economics. While some of this data might be
above learners? heads, it is, for the most part, well-
formed native-like Korean. Also, the inclusion of
learner data has been dramatically reduced. How-
ever, some of the same problems from the F1 corpus
persist, namely the inclusion of poetry, newspaper
titles, religious text, and non-Korean data.
Based on this qualitative analysis, it is clear that
we need to filter out more data than is currently be-
ing filtered, in order to obtain valid Korean of a type
which uses a sufficient number of particles in gram-
matical ways. In the future, we plan on restrict-
ing the genre, filtering based on the number of rare
words (e.g., religious words), and using a trigram
language model to check the validity.
12
Ejel Particles Nominals
Corpus Seeds Len. Queries URLs Total Avg. Total Avg. Total Avg.
Gen. 10 3 120 1096.2 1,140,394.6 1044.8 363,145.6 331.5 915,025 838.7
5 252 1388.2 2,430,346.4 1779.9 839,005.8 618.9 1,929,266.0 1415.3
20 3 120 1375.2 1,671,549.2 1222.1 540,918 394.9 1,350,976.6 988.6
3 240 2492.4 2,735,201.6 1099.4 889,089 357.3 2,195,703 882.4
5 252 1989.6 4,533,642.4 2356 1,359,137.2 724.5 3,180,560.6 1701.5
5 504 3487 7,463,776 2193.5 2,515,235.8 741.6 5,795,455.8 1709.7
50 3 120 1533 1,720,261 1122.1 584,065 380.9 1,339,308 873.6
3 240 2868 3,170,043 1105.3 1,049,975 366.1 2,506,995 874.1
5 252 1899.5 4,380,684.2 2397.6 1,501,358.7 821.5 3,523,746.2 1934.6
5 504 5636 5,735,859 1017.7 1,773,596 314.6 4,448,815 789.3
F1 10 3 120 1315 628,819 478.1 172,415 131.1 510,620 388.3
5 252 1577 1,364,885 865.4 436,985 277.1 1,069,898 678.4
20 3 120 1462.6 1,093,772.4 747.7 331,457.8 226.8 885,157.2 604.9
240 2637.2 1,962,741.8 745.2 595,570.6 226.1 1,585,730.4 602.1
5 252 2757.6 2,015,077.8 730.8 616,163.8 223.4 1,621,306.2 588
504 4734 3,093,140.4 652.9 754,610 159.8 1,993,104.4 422.1
F2 20 3 120 1417 1,054,925 744.5 358,297 252.9 829,416 585.3
240 2769 1,898,383 685.6 655,757 236.8 1,469,623 530.7
5 252 1727 4,510,742 2611.9 1,348,240 780.7 2,790,667 1615.9
504 2680 6,916,574 2580.8 2,077,171 775.1 4,380,571 1634.5
Table 2: Basic statistics of different web corpora
Note that one might consider building even larger
corpora from the start and using the filtering step to
winnow down the corpus for a particular application,
such as particle error detection. However, while re-
moving ungrammatical Korean is a process of re-
moving noise, identifying whether a corpus is about
traveling, for example, is a content-based decision.
Given that this is what a search engine is designed
to do, we prefer filtering based only on grammatical
and genre properties.
5 Classification
We describe the classification paradigm used to de-
termine how effective each corpus is for detecting
correct particle usage; evaluation is in section 6.
5.1 Machine learning paradigm
Based on the parallel between Korean particles and
English prepositions, we use preposition error de-
tection as a starting point for developing a classifier.
For prepositions, Tetreault and Chodorow (2008) ex-
tract 25 features to guess the correct preposition (out
of 34 selected prepositions), including features cap-
turing the lexical and grammatical context (e.g., the
words and POS tags in a two-word window around
the preposition) and features capturing various rel-
evant selectional properties (e.g., the head verb and
noun of the preceding VP and NP).
We are currently using TiMBL (Daelemans et al,
2007) for development purposes, as it provides a
range of options for testing. Given that learner
data needs to be processed instantaneously and that
memory-based learning can take a long time to clas-
sify, we will revisit this choice in the future.
5.2 Defining features
5.2.1 Relevant properties of Korean
As discussed in section 2, Korean has major dif-
ferences from English, leading to different features.
First, the base word order of Korean is SOV, which
means that the following verb and following noun
could determine how the current word functions.
However, since Korean allows for freer word order
than English, we do not want to completely disre-
gard the previous noun or verb, either.
Secondly, the composition of words is different
than English. Words contain a stem and an arbitrary
number of suffixes, which may be derivational mor-
13
phemes as well as particles, meaning that we must
consider sub-word features, i.e., segment words into
their component morphemes.
Finally, particles have more functions than prepo-
sitions, requiring a potentially richer space of fea-
tures. Case marking, for example, is even more de-
pendent upon the word?s grammatical function in
a sentence. In order to ensure that our system can
correctly handle all of the typical relations between
words without failing on less frequent constructions,
we need (large amounts of) appropriate data.
5.2.2 Feature set
To begin with, we segment and POS tag the text,
using a hybrid (trigram + rule-based) morphological
tagger for Korean (Han and Palmer, 2004). This seg-
mentation phase means that we can define subword
features and isolate the particles in question. For our
features, we break each word into: a) its stem and b)
its combined affixes (excluding particles), and each
of these components has its own POS, possibly a
combined tag (e.g., EPF+EFN), with tags from the
Penn Korean Treebank (Han et al, 2002).
The feature vector uses a five word window that
includes the target word and two words on either
side for context. Each word is broken down into four
features: stem, affixes, stem POS, and affixes POS.
Given the importance of surrounding noun and verbs
for attachment in Korean, we have features for the
preceding as well as the following noun and verb.
For the noun/verb features, only the stem is used, as
this is largely a semantically-based property.
In terms of defining a class, if the target word?s
affixes contain a particle, it is removed and used as
the basis for the class; otherwise the class is NONE.
We also remove particles in the context affixes, as
we cannot rely on surrounding learner particles.
As an example, consider predicting the particle
for the word Yenge (?English?) in (5a). We gener-
ate the instance in (5b). The first five lines refer
to the previous two words, the target word, and the
following two words, each split into stem and suf-
fixes along with their POS tags, and with particles
removed. The sixth line contains the stems of the
preceding and following noun and verb, and finally,
there is the class (YES/NO).
(5) a. Mikwuk-eyse
America-in
sal-myense
live-while
Yenge-man-ul
English-only-OBJ
cip-eyse
home-at
ss-ess-eyo.
use-Past-Decl
?While living in America, (I/she/he) used only
English at home.?
b. Mikwuk NPR NONE NONE
sal VV myense ECS
Yenge NPR NONE NONE
cip NNC NONE NONE
ss VV ess+eyo EPF+EFN
sal Mikwuk ss cip
YES
For the purposes of evaluating the different cor-
pora, we keep the task simple and only guess YES
or NO for the existence of a particle. We envision
this as a first pass, where the specific particle can
be guessed later. This is also a practical task, in
that learners can benefit from accurate feedback on
knowing whether or not a particle is needed.
6 Evaluation
We evaluate the web corpora for the task of predict-
ing particle usage, after describing the test corpus.
6.1 Learner Corpus
To evaluate, we use a corpus of learner Korean made
up of essays from college students (Lee et al, 2009).
The corpus is divided according to student level (be-
ginner, intermediate) and student background (her-
itage, non-heritage),6 and is hand-annotated for par-
ticle errors. We expect beginners to be less accurate
than intermediates and non-heritage less accurate
than heritage learners. To pick a middle ground, the
current research has been conducted on non-heritage
intermediate learners. The test corpus covers a range
of common language classroom topics such as Ko-
rean language, Korea, friends, family, and traveling.
We run our system on raw learner data, i.e, un-
segmented and with spelling and spacing errors in-
cluded. As mentioned in section 5.2.2, we use a POS
tagger to segment the words into morphemes, a cru-
cial step for particle error detection.7
6Heritage learners have had exposure to Korean at a young
age, such as growing up with Korean spoken at home.
7In the case of segmentation errors, we cannot possibly get
the particle correct. We are currently investigating this issue.
14
Seeds Len. Quer. P R F
Gen. 10 3 120 81.54% 76.21% 78.77%
5 252 82.98% 77.77% 80.28%
20 3 120 81.56% 77.26% 79.33%
3 240 82.89% 78.37% 80.55%
5 252 83.79% 78.17% 80.87%
5 504 84.30% 79.44% 81.79%
50 3 120 82.97% 77.97% 80.39%
3 240 83.62% 80.46% 82.00%
5 252 82.57% 78.45% 80.44%
5 504 84.25% 78.69% 81.36%
F1 10 3 120 81.41% 74.67% 77.88%
5 252 83.82% 77.09% 80.30%
20 3 120 82.23% 76.40% 79.20%
240 82.57% 77.19% 79.78%
5 252 83.62% 77.97% 80.68%
504 81.86% 75.88% 78.73%
F2 20 3 120 81.63% 76.44% 78.93%
240 82.57% 78.45% 80.44%
5 252 84.21% 80.62% 82.37%
504 83.87% 81.51% 82.67%
Table 3: Results of guessing particle existence, training
with different corpora
The non-heritage intermediate (NHI) corpus gives
us 3198 words, with 1288 particles and 1836 nom-
inals. That is, about 70% of the nominals in the
learner corpus are followed by a particle. This is a
much higher average than in the 252 5-tuple F2 cor-
pus, which exhibits the highest average of all of the
web corpora at about 48% ( 7811616 ; see table 2).
6.2 Results
We use the default settings for TiMBL for all the re-
sults we report here. Though we have obtained 4-5%
higher F-scores using different settings, the compar-
isons between corpora are the important measure for
the current task. The results are given in table 3.
The best results were achieved when training
on the 5-tuple F2 corpora, leading to F-scores of
82.37% and 82.67% for the 252 tuple and 504 tu-
ple corpora, respectively. This finding reinforces our
hypothesis that more advanced seed terms result in
more reliable Korean data, while staying within the
domain of the test corpus. Both longer tuple lengths
and greater amounts of queries have an effect on the
reliability of the resulting corpora. Specificaly, 5-
tuple corpora produce better results than similar 3-
tuple corpora, and corpora with double the amount
of queries of n-length perform better than smaller
comparable corpora. Although larger corpora tend
to do better, it is important to note that there is not
a clear relationship. The general 50/5/252 corpus,
for instance, is similarly-sized to the F2 focused
20/5/252 corpus, with over 4 million ejels (see ta-
ble 2). The focused corpus?based on fewer yet
more relevant seed terms?has 2% better F-score.
7 Summary and Outlook
In this paper, we have examined different ways to
build web corpora for analyzing learner language
to support the detection of errors in Korean parti-
cles. This type of investigation is most useful for
lesser-resourced languages, where the error detec-
tion task stays constant, but the topic changes fre-
quently. In order to develop a framework for testing
web corpora, we have also begun developing a ma-
chine learning system for detecting particle errors.
The current web data, as we have demonstrated, is
not perfect, and thus we need to continue improving
that. One approach will be to filter out clearly non-
Korean data, as suggested in section 4.1. We may
also explore instance sampling (e.g., Wunsch et al,
2009) to remove many of the non-particle nominal
(negative) instances, which will reduce the differ-
ence between the ratios of negative-to-positive in-
stances of the web and learner corpora. We still feel
that there is room for improvement in our seed term
selection, and plan on constructing specific web cor-
pora for each topic covered in the learner corpus.
We will also consider adding currently available cor-
pora, such as the Sejong Corpus (The National Insti-
tute of Korean Language, 2007), to our web data.
With better data, we can work on improving the
machine learning system. This includes optimizing
the set of features, the parameter settings, and the
choice of machine learning algorithm. Once the sys-
tem has been optimized, we will need to test the re-
sults on a wider range of learner data.
Acknowledgments
We would like to thank Marco Baroni and Jan
Pomika?lek for kindly providing a UTF-8 version of
BootCat; Chong Min Lee for help with the POS tag-
ger, provided by Chung-Hye Han; and Joel Tetreault
for useful discussion.
15
References
Amaral, Luiz and Detmar Meurers (2006). Where
does ICALL Fit into Foreign Language Teach-
ing? Talk given at CALICO Conference. May
19, 2006. University of Hawaii.
Baroni, Marco and Silvia Bernardini (2004). Boot-
CaT: Bootstrapping Corpora and Terms from the
Web. In Proceedings of LREC 2004. pp. 1313?
1316.
Baroni, Marco and Motoko Ueyama (2004). Re-
trieving Japanese specialized terms and corpora
from the World Wide Web. In Proceedings of
KONVENS 2004.
Daelemans, Walter, Jakub Zavrel, Ko van der Sloot,
Antal van den Bosch, Timbl Tilburg and Memory
based Learner (2007). TiMBL: Tilburg Memory-
Based Learner - version 6.1 - Reference Guide.
De Felice, Rachele and Stephen Pulman (2008). A
classifier-baed approach to preposition and deter-
miner error correction in L2 English. In Proceed-
ings of COLING-08. Manchester.
Dickinson, Markus, Soojeong Eom, Yunkyoung
Kang, Chong Min Lee and Rebecca Sachs (2008).
A Balancing Act: How can intelligent computer-
generated feedback be provided in learner-to-
learner interactions. Computer Assisted Language
Learning 21(5), 369?382.
Dickinson, Markus and Chong Min Lee (2009).
Modifying Corpus Annotation to Support the
Analysis of Learner Language. CALICO Journal
26(3).
Erjavec, Irena Srdanovic`, Tomaz Erjavec and Adam
Kilgarriff (2008). A Web Corpus and Word
Sketches for Japanese. Information and Media
Technologies 3(3), 529?551.
Han, Chung-Hye, Na-Rare Han, Eon-Suk Ko and
Martha Palmer (2002). Development and Eval-
uation of a Korean Treebank and its Application
to NLP. In Proceedings of LREC-02.
Han, Chung-Hye and Martha Palmer (2004). A Mor-
phological Tagger for Korean: Statistical Tag-
ging Combined with Corpus-Based Morphologi-
cal Rule Application. Machine Translation 18(4),
275?297.
Han, Na-Rae, Martin Chodorow and Claudia Lea-
cock (2006). Detecting Errors in English Arti-
cle Usage by Non-Native Speakers. Natural Lan-
guage Engineering 12(2).
Ko, S., M. Kim, J. Kim, S. Seo, H. Chung and S. Han
(2004). An analysis of Korean learner corpora
and errors. Hanguk Publishing Co.
Lee, John and Ola Knutsson (2008). The Role of
PP Attachment in Preposition Generation. In Pro-
ceedings of CICLing 2008. Haifa, Israel.
Lee, John and Stephanie Seneff (2006). Auto-
matic Grammar Correction for Second-Language
Learners. In INTERSPEECH 2006. Pittsburgh,
pp. 1978?1981.
Lee, Sun-Hee, Donna K. Byron and Seok Bae Jang
(2005). Why is Zero Marking Important in Ko-
rean? In Proceedings of IJCNLP-05. Jeju Island,
Korea.
Lee, Sun-Hee, Seok Bae Jang and Sang kyu Seo
(2009). Annotation of Korean Learner Corpora
for Particle Error Detection. CALICO Journal
26(3).
Sharoff, Serge (2006). Creating General-Purpose
Corpora Using Automated Search Engine
Queries. In WaCky! Working papers on the Web
as Corpus. Gedit.
Tetreault, Joel and Martin Chodorow (2008). The
Ups and Downs of Preposition Error Detection
in ESL Writing. In Proceedings of COLING-08.
Manchester.
Tetreault, Joel and Martin Chodorow (2009). Exam-
ining the Use of Region Web Counts for ESL Er-
ror Detection. In Web as Corpus Workshop (WAC-
5). San Sebastian, Spain.
The National Institute of Korean Language (2007).
The Sejong Corpus.
Ueyama, Motoko (2006). Evaluation of Japanese
Web-based Reference Corpora: Effects of Seed
Selection and Time Interval. In WaCky! Working
papers on the Web as Corpus. Gedit.
Wunsch, Holger, Sandra Ku?bler and Rachael
Cantrell (2009). Instance Sampling Methods for
Pronoun Resolution. In Proceedings of RANLP
2009. Borovets, Bulgaria.
16
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 81?86,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Developing Methodology for Korean Particle Error Detection
Markus Dickinson
Indiana University
md7@indiana.edu
Ross Israel
Indiana University
raisrael@indiana.edu
Sun-Hee Lee
Wellesley College
slee6@wellesley.edu
Abstract
We further work on detecting errors in post-
positional particle usage by learners of Korean
by improving the training data and develop-
ing a complete pipeline of particle selection.
We improve the data by filtering non-Korean
data and sampling instances to better match
the particle distribution. Our evaluation shows
that, while the data selection is effective, there
is much work to be done with preprocessing
and system optimization.
1 Introduction
A growing area of research in analyzing learner lan-
guage is to detect errors in function words, namely
categories such as prepositions and articles (see Lea-
cock et al, 2010, and references therein). This work
has mostly been for English, and there are issues,
such as greater morphological complexity, in mov-
ing to other languages (see, e.g., de Ilarraza et al,
2008; Dickinson et al, 2010). Our goal is to build a
machine learning system for detecting errors in post-
positional particles in Korean, a significant source of
learner errors (Ko et al, 2004; Lee et al, 2009b).
Korean postpositional particles are morphemes
that attach to a preceding nominal to indicate a range
of linguistic functions, including grammatical func-
tions, e.g., subject and object; semantic roles; and
discourse functions. In (1), for instance, ka marks
the subject (function) and agent (semantic role).1
Similar to English prepositions, particles can also
have modifier functions, adding meanings of time,
location, instrument, possession, and so forth.
1We use the Yale Romanization scheme for writing Korean.
(1) Sumi-ka
Sumi-SBJ
John-uy
John-GEN
cip-eyse
house-LOC
ku-lul
he-OBJ
twu
two
sikan-ul
hours-OBJ
kitaly-ess-ta.
wait-PAST-END
?Sumi waited for John for (the whole) two hours in
his house.?
We treat the task of particle error detection as
one of particle selection, and we use machine learn-
ing because it has proven effective in similar tasks
for other languages (e.g., Chodorow et al, 2007;
Oyama, 2010). Training on a corpus of well-formed
Korean, we predict which particle should appear af-
ter a given nominal; if this is different from the
learner?s, we have detected an error. Using a ma-
chine learner has the advantage of being able to per-
form well without a researcher having to specify
rules, especially with the complex set of linguistic
relationships motivating particle selection.2
We build from Dickinson et al (2010) in two
main ways: first, we implement a presence-selection
pipeline that has proven effective for English prepo-
sition error detection (cf. Gamon et al, 2008). As
the task is understudied, the work is preliminary, but
it nonetheless is able to highlight the primary ar-
eas of focus for future work. Secondly, we improve
upon the training data, in particular doing a better
job of selecting relevant instances for the machine
learner. Obtaining better-quality training data is a
major issue for machine learning applied to learner
language, as the domain of writing is different from
news-heavy training domains (Gamon, 2010).
2See Dickinson and Lee (2009); de Ilarraza et al (2008);
Oyama (2010) for related work in other languages.
81
2 Particle error detection
2.1 Pre-processing
Korean is an agglutinative language: Korean words
(referred to as ecels) are usually composed of a
root with a number of functional affixes. We thus
first segment and POS tag the text, for both train-
ing and testing, using a hybrid (trigram + rule-
based) morphological tagger for Korean (Han and
Palmer, 2004). The tagger is designed for native
language and is not optimized to make guesses for
ill-formed input. While the POS tags assigned to the
learner corpus are thus often incorrect (see Lee et al,
2009a), there is the more primary problem of seg-
mentation, as discussed in more detail in section 4.
2.2 Machine learning
We use the Maximum Entropy Toolkit (Le, 2004)
for machine learning. Training on a corpus of well-
formed Korean, we predict which particle should ap-
pear after a given nominal; if this is different from
what the learner used, we have detected an error. It
is important that the data represent the relationships
between specific lexical items: in the comparable
English case, for example, interest is usually found
with in: interest in/*with learning.
Treating the ends of nominal elements as possible
particle slots, we break classification into two steps:
1) Is there a particle? (Yes/No); and 2) What is the
exact particle? Using two steps eases the task of ac-
tual particle prediction: with a successful classifica-
tion of negative and positive instances, there is no
need to handle nominals that have no particle in step
2. To evaluate our parameters for obtaining the most
relevant instances, we keep the task simple and per-
form only step 1, as this step provides information
about the usability of the training data. For actual
system performance, we evaluate both steps.
In selecting features for Korean, we have to ac-
count for relatively free word order (Chung et al,
2010). We follow our previous work (Dickinson
et al, 2010) in our feature choices, using a five-
word window that includes the target stem and two
words on either side for context (see also Tetreault
and Chodorow, 2008). Each word is broken down
into: stem, affixes, stem POS, and affixes POS. We
also have features for the preceding and following
noun and verb, thereby approximating relevant se-
lectional properties. Although these are relatively
shallow features, they provide enough lexical and
grammatical context to help select better or worse
training data (section 3) and to provide a basis for a
preliminary system (section 4).
3 Obtaining the most relevant instances
We need well-formed Korean data in order to train
a machine learner. To acquire this, we use web-
based corpora, as this allows us to find data similar
to learner language, and using web as corpus (WaC)
tools allows us to adjust parameters for new data
(Dickinson et al, 2010). However, the methodology
outlined in Dickinson et al (2010) can be improved
in at least three ways, outlined next.
3.1 Using sub-corpora
Web corpora can be built by searching for a set of
seed terms, extracting documents with those terms
(Baroni and Bernardini, 2004). One way to improve
such corpora is to use better seeds, namely, those
which are: 1) domain-appropriate (e.g., about trav-
eling), and 2) of an appropriate level. In Dickinson
et al (2010), we show that basic terms result in poor
quality Korean, but slightly more advanced terms on
the same topics result in better-formed data.
Rather than use all of the seed terms to create a
single corpus, we divide the seed terms into 13 sep-
arate sets, based on the individual topics from our
learner corpus. The sub-corpora are then combined
to create a cohesive corpus covering all the topics.
For example, we use 10 Travel words to build a
subcorpus, 10 Learning Korean words for a differ-
ent subcorpus, and so forth. This means that terms
appropriate for one topic are not mixed with terms
for a different topic, ensuring more coherent web
documents. Otherwise, we might obtain a Health
Management word, such as pyengwen (?hospital?),
mixed with a Generation Gap word, such as kaltung
(?conflict?)?in this case, leading to webpages on
war, a topic not represented in our learner corpus.
3.2 Filtering
One difficulty with our web corpora is that some of
them have large amounts of other languages along
with Korean. The keywords are in the corpora, but
there is additional text, often in Chinese, English, or
Japanese. These types of pages are unreliable for
82
our purposes, as they may not exhibit natural Ko-
rean. By using a simple filter, we check whether a
majority of the characters in a webpage are indeed
from the Korean writing system, and remove pages
beneath a certain threshold.
3.3 Instance sampling
Particles are often dropped in colloquial and even
written Korean, whereas learners are more often
required to use them. It is not always the case
that the web pages contain the same ratio of par-
ticles as learners are expected to use. To alleviate
this over-weighting of having no particle attached
to a noun, we propose to downsample our corpora
for the machine learning experiments, by remov-
ing a randomly-selected proportion of (negative) in-
stances. Instance sampling has been effective for
other NLP tasks, e.g., anaphora resolution (Wunsch
et al, 2009), when the number of negative instances
is much greater than the positive ones. In our web
corpora, nouns have a greater than 50% chance of
having no particle; in section 3.4, we thus downsam-
ple to varying amounts of negative instances from
about 45% to as little as 10% of the total corpus.
3.4 Training data selection
In Dickinson et al (2010), we used a Korean learner
data set from Lee et al (2009b) for development. It
contains 3198 ecels, 1842 of which are nominals,
and 1271 (?70%) of those have particles. We use
this same corpus for development, to evaluate filter-
ing and down-sampling. Evaluating on (yes/no) par-
ticle presence, in tables 1 and 2, recall is the percent-
age of positive instances we correctly find and pre-
cision is the percentage of instances that we classify
as positive that actually are. A baseline of always
guessing a particle gives 100% recall, 69% preci-
sion, and 81.7% F-score.
Table 1 shows the results of the MaxEnt system
for step 1, using training data built for the topics in
the data with filter thresholds of 50%, 70%, 90%,
and 100%?i.e., requiring that percentage of Korean
characters?as well as the unfiltered corpus. The
best F-score is with the filter set at 90%, despite the
size of the filtered corpus being smaller than the full
corpus. Accordingly, we use the 90% filter on our
training corpus for the experiments described below.
Threshold 100% 90% 70% 50% Full
Ecel 67k 9.6m 10.3m 11.1m 12.7m
Instances 37k 5.8m 6.3m 7.1m 8.4m
Accuracy 74.75 81.11 74.64 80.29 80.46
Precision 80.03 86.14 79.65 85.41 85.56
Recall 84.50 86.55 84.97 86.15 86.23
F-score 82.20 86.34 82.22 85.78 85.89
Table 1: Step 1 (particle presence) results with filters
The results for instance sampling are given in ta-
ble 2. We experiment with positive to negative sam-
pling ratios of 1.3/1 (?43% negative instances), 2/1
(?33%), 4/1 (?20%), and 10/1 (?10%). We select
the 90% filter, 1.3/1 downsampling settings and ap-
ply them to the training corpus (section 3.1) for all
experiments below.
P/N ratio 10/1 4/1 2/1 1.3/1 1/1.05
Instances 3.1m 3.5m 4.3m 5m 5.8m
Accuracy 74.75 77.85 80.23 81.59 81.11
Precision 73.38 76.72 80.75 84.26 86.14
Recall 99.53 97.48 93.71 90.17 86.55
F-score 84.47 85.86 86.74 87.12 86.34
Table 2: Step 1 (presence) results with instance sampling
One goal has been to improve the web as corpus
corpus methodology for training a machine learning
system. The results in tables 1 and 2 reinforce our
earlier finding that size is not necessarily the most
important variable in determining the usefulness or
overall quality of data collected from the web for
NLP tasks (Dickinson et al, 2010). Indeed, the cor-
pus producing best results (90% filter, 1.3:1 down-
sampling) is more than 3 million instances smaller
than the unfiltered, unsampled corpus.
4 Initial system evaluation
We have obtained an annotated corpus of 25 essays
from heritage intermediate learners,3 with 299 sen-
tences and 2515 ecels (2676 ecels after correcting
spacing errors). There are 1138 nominals, with 93
particle errors (5 added particles, 35 omissions, 53
substitutions)?in other words, less than 10% of par-
ticles are errors. There are 979 particles after cor-
rection. We focus on 38 particles that intermediate
3Heritage learners have had exposure to Korean at a young
age, such as growing up with Korean spoken at home.
83
students can be reasonably expected to use. A parti-
cle is one of three types (cf. Nam and Ko, 2005): 1)
case markers, 2) adverbials (cf. prepositions), and
3) auxiliary particles.4
Table 3 gives the results for the entire system on
the test corpus, with separate results for each cat-
egory of particle, (Case, Adv., and Aux.) as well
as the concatenation of the three (All). The ac-
curacy presented here is in terms of only the par-
ticle in question, as opposed to the full form of
root+particle(s). Step 2 is presented in 2 ways: Clas-
sified, meaning that all of the instances classified as
needing a particle by step 1 are processed, or Gold,
in which we rely on the annotation to determine par-
ticle presence. It is not surprising, then, that Gold
experiments are more accurate than Classified ex-
periments, due to step 1 errors and also preprocess-
ing issues, discussed next.
Step 1 Step 2
Data # Classified Gold
Case 504 95.83% 71.23% 72.22%
Adv. 205 82.43% 30.24% 32.68%
Aux. 207 89.37% 31.41% 35.74%
All 916 91.37% 53.05% 55.13%
Table 3: Accuracy for step 1 (particle presence) & step 2
(particle selection), with number (#) of instances
Preprocessing For the particles we examine, there
are 135 mis-segmented nominals. The problem is
more conspicuous if we look at the entire corpus:
the tagger identifies 1547 nominal roots, but there
are only 1138. Some are errors in segmentation, i.e.,
mis-identifying the proper root of the ecel, and some
are problems with tagging the root, e.g., a nominal
mistagged as a verb. Table 4 provides results divided
by cases with only correctly pre-processed ecels and
where the target ecel has been mis-handled by the
tagger. This checks whether the system particle is
correct, ignoring whether the whole form is correct;
if full-form accuracy is considered, we have no way
to get the 135 inaccurate cases correct.
Error detection While our goal now is to estab-
lish a starting point, the ultimate, on-going goal of
4Full corpus details will be made available at: http://
cl.indiana.edu/?particles/.
Step 1 Step 2
Data # Classified Gold
Accurate 781 94.24% 55.95% 58.13%
Inaccurate 135 74.81% 36.29% 38.51%
Table 4: Overall accuracy divided by accurate and inac-
curate preprocessing
Case Adv. Aux. All
Precision 28.82% 7.69% 5.51% 15.45%
Recall 87.50% 100% 77.78% 88.00%
Table 5: Error detection (using Gold step 1)
this work is to develop a robust system for automati-
cally detecting errors in learner data. Thus, it is nec-
essary to measure our performance at actually find-
ing the erroneous instances extracted from our test
corpus. Table 5 provides results for step 2 in terms
of our ability to detect erroneous instances. We re-
port precision and recall, calculated as in figure 1.
From the set of erroneous instances:
True Positive (TP) ML class 6= student class
False Negative (FN) ML class = student class
From the set of correct instances:
False Positive (FP) ML class 6= student class
True Negative (TN) ML class = student class
Precision (P) TPTP+FP
Recall (R) TPTP+FN
Figure 1: Precision and recall for error detection
4.1 Discussion and Outlook
One striking aspect about the results in table 3 is the
gap in accuracy between case particles and the other
two categories, particularly in step 2. This points at
a need to develop independent systems for each type
of particle, each relying on different types of linguis-
tic information. Auxiliary particles, for example, in-
clude topic particles which?similar to English arti-
cles (Han et al, 2006)?require discourse informa-
tion to get correct. Still, as case particles comprise
more than half of all particles in our corpus, the sys-
tem is already potentially useful to learners.
Comparing the rows in table 4, the dramatic drop
in accuracy when moving to inaccurately-processed
84
cases shows a clear need for preprocessing adapted
to learner data. While it is disconcerting that nearly
15% (135/916) of the cases have no chance of re-
sulting in a correct full form, the results indicate that
we can obtain reliable accuracy (cf. 94.24%) for pre-
dicting particle presence across all types of particles,
assuming good morphological tagging.
From table 5, it is apparent that we are overguess-
ing errors; recall that only 10% of particles are er-
roneous, whereas we more often guess a different
particle. While this tendency results in high recall,
a tool for learners should have higher precision, so
that correct usage is not flagged. However, this is
a first attempt at error detection, and simply know-
ing that precision is low means we can take steps
to solve this deficiency. Our training data may have
too many possible classes in it, and we have not yet
accounted for phonological alternations; e.g. if the
system guesses ul when lul is correct, we count a
miss, even though they are different realizations of
the same morpheme.
To try and alleviate the over-prediction of errors,
we have begun to explore implementing a confi-
dence filter. As a first pass, we use a simple fil-
ter that compares the probability of the best parti-
cle to the probability of the particle the learner pro-
vided; the absolute difference in probabilities must
be above a certain threshold. Table 6 provides the er-
ror detection results for each type of particle, incor-
porating confidence filters of 10%, 20%, 30%, 40%,
50%, and 60%. The results show that increasing the
threshold at which we accept the classifier?s answer
can significantly increase precision, at the cost of re-
call. As noted above, higher precision is desirable,
so we plan on further developing this confidence fil-
ter. We may also include heuristic-based filters, such
as the ones implemented in Criterion (see Leacock
et al, 2010), as well as a language model approach
(Gamon et al, 2008).
Finally, we are currently working on improving
the POS tagger, testing other taggers in the pro-
cess, and developing optimal feature sets for differ-
ent kinds of particles.
Acknowledgments
We would like to thank the IU CL discussion group
and Joel Tetreault for feedback at various points.
Adv Aux Case All
10
% P 10.0% 6.3% 29.9% 16.3%
R 100% 77.8% 67.8% 73.3%
20
% P 13.5% 7.8% 32.6% 18.0%
R 100% 77.8% 50.0% 60.0%
30
% P 20.0% 8.3% 36.1% 20.8%
R 100% 66.7% 39.3% 50.7%
40
% P 19.4% 14.3% 48.6% 26.9%
R 60.0% 66.7% 30.4% 38.7%
50
% P 23.1% 16.7% 57.9% 32.1%
R 30.0% 44.4% 19.6% 24.0%
60
% P 40.0% 26.7% 72.3% 45.2%
R 20.0% 44.4% 14.3% 18.7%
Table 6: Error detection with confidence filters
References
Marco Baroni and Silvia Bernardini. 2004. Bootcat:
Bootstrapping corpora and terms from the web. In
Proceedings of LREC 2004, pages 1313?1316.
Martin Chodorow, Joel Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involv-
ing prepositions. In Proceedings of the 4th ACL-
SIGSEM Workshop on Prepositions, pages 25?30.
Prague.
Tagyoung Chung, Matt Post, and Daniel Gildea.
2010. Factors affecting the accuracy of korean
parsing. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 49?57.
Los Angeles, CA, USA.
Arantza D??az de Ilarraza, Koldo Gojenola, and
Maite Oronoz. 2008. Detecting erroneous uses
of complex postpositions in an agglutinative lan-
guage. In Proceedings of COLING-08. Manch-
ester.
Markus Dickinson, Ross Israel, and Sun-Hee Lee.
2010. Building a korean web corpus for analyz-
ing learner language. In Proceedings of the 6th
Workshop on the Web as Corpus (WAC-6). Los
Angeles.
Markus Dickinson and Chong Min Lee. 2009. Mod-
ifying corpus annotation to support the analysis of
learner language. CALICO Journal, 26(3).
Michael Gamon. 2010. Using mostly native data
85
to correct errors in learners? writing. In Human
Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the
Association for Computational Linguistics, pages
163?171. Los Angeles, California.
Michael Gamon, Jianfeng Gao, Chris Brockett,
Alexander Klementiev, William Dolan, Dmitriy
Belenko, and Lucy Vanderwende. 2008. Using
contextual speller techniques and language mod-
eling for esl error correction. In Proceedings of
IJCNLP. Hyderabad, India.
Chung-Hye Han and Martha Palmer. 2004. A mor-
phological tagger for korean: Statistical tagging
combined with corpus-based morphological rule
application. Machine Translation, 18(4):275?
297.
Na-Rae Han, Martin Chodorow, and Claudia Lea-
cock. 2006. Detecting errors in english article us-
age by non-native speakers. Natural Language
Engineering, 12(2).
S. Ko, M. Kim, J. Kim, S. Seo, H. Chung, and
S. Han. 2004. An analysis of Korean learner cor-
pora and errors. Hanguk Publishing Co.
Zhang Le. 2004. Maximum Entropy Mod-
eling Toolkit for Python and C++. URL
http://homepages.inf.ed.ac.uk/
s0450736/maxent_toolkit.html.
Claudia Leacock, Martin Chodorow, Michael Ga-
mon, and Joel Tetreault. 2010. Automated Gram-
matical Error Detection for Language Learners.
Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool.
Chong Min Lee, Soojeong Eom, and Markus Dick-
inson. 2009a. Towards analyzing korean learner
particles. Talk given at CALICO ?09 Pre-
Conference Workshop on Automatic Analysis of
Learner Language. Tempe, AZ.
Sun-Hee Lee, Seok Bae Jang, and Sang kyu Seo.
2009b. Annotation of korean learner corpora for
particle error detection. CALICO Journal, 26(3).
Ki-shim Nam and Yong-kun Ko. 2005. Korean
Grammar (phyocwun kwuke mwunpeplon). Top
Publisher, Seoul.
Hiromi Oyama. 2010. Automatic error detection
method for japanese particles. Polyglossia, 18.
Joel Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in esl
writing. In Proceedings of COLING-08. Manch-
ester.
Holger Wunsch, Sandra Ku?bler, and Rachael
Cantrell. 2009. Instance sampling methods for
pronoun resolution. In Proceedings of RANLP
2009. Borovets, Bulgaria.
86
Proceedings of the Second Workshop on Metaphor in NLP, pages 33?41,
Baltimore, MD, USA, 26 June 2014.
c
?2014 Association for Computational Linguistics
Abductive Inference for Interpretation of Metaphors
Ekaterina Ovchinnikova*, Ross Israel*, Suzanne Wertheim
+
,
Vladimir Zaytsev*, Niloofar Montazeri*, Jerry Hobbs*
* USC ISI, 4676 Admiralty Way, CA 90292, USA
{katya,israel,vzaytsev,niloofar,hobbs}@isi.edu
+
Worthwhile Research & Consulting, 430 1/2 N Genesee Av., Los Angeles, CA 90036, USA
worthwhileresearch@gmail.com
Abstract
This paper presents a metaphor interpre-
tation pipeline based on abductive infer-
ence. In this framework following (Hobbs,
1992) metaphor interpretation is modelled
as a part of the general discourse pro-
cessing problem, such that the overall dis-
course coherence is supported. We present
an experimental evaluation of the pro-
posed approach using linguistic data in
English and Russian.
1 Introduction
In this paper, we elaborate on a semantic pro-
cessing framework based on a mode of inference
called abduction, or inference to the best expla-
nation. In logic, abduction is a kind of inference
which arrives at an explanatory hypothesis given
an observation. (Hobbs et al., 1993) describe how
abduction can be applied to the discourse process-
ing problem, viewing the process of interpreting
sentences in discourse as the process of providing
the best explanation of why the sentence would be
true. (Hobbs et al., 1993) show that abductive rea-
soning as a discourse processing technique helps
to solve many pragmatic problems such as refer-
ence resolution, the interpretation of noun com-
pounds, detection of discourse relations, etc. as a
by-product. (Hobbs, 1992) explains how abduc-
tion can be applied to interpretation of metaphors.
The term conceptual metaphor (CM) refers
to the understanding of one concept or concep-
tual domain in terms of the properties of another
(Lakoff and Johnson, 1980; Lakoff, 1987). For ex-
ample, development can be understood as move-
ment (e.g., the economy moves forward, the en-
gine of the economy). In other words, a concep-
tual metaphor consists in mapping a target con-
ceptual domain (e.g., economy) to a source do-
main (e.g., vehicle) by comparing their properties
(e.g., an economy develops like a vehicle moves).
In text, conceptual metaphors are represented by
linguistic metaphors (LMs), i.e. natural language
phrases expressing the implied comparison of two
domains.
We present a metaphor interpretation approach
based on abduction. We developed an end-to-
end metaphor interpretation system that takes text
potentially containing linguistic metaphors as in-
put, detects linguistic metaphors, maps them to
conceptual metaphors, and interprets conceptual
metaphors in terms of both logical predicates and
natural language expressions. Currently, the sys-
tem can process linguistic metaphors mapping
predefined target and source domains.
We perform an experimental evaluation
of the proposed approach using linguistic
data in two languages: English and Rus-
sian. We select target concepts and generate
potential sources for them as described at
github.com/MetaphorExtractionTools/mokujin.
For top-ranked sources, we automatically find cor-
responding linguistic metaphors. These linguistic
metaphors are each then validated by three expert
linguists. For the validated linguistic metaphors,
we generate natural language interpretations,
which are also validated by three experts.
2 Related Work
Automatic interpretation of linguistic metaphors is
performed using two principal approaches: 1) de-
riving literal paraphrases for metaphorical expres-
sions from corpora (Shutova, 2010; Shutova et
al., 2012) and 2) reasoning with manually coded
knowledge (Hobbs, 1992; Narayanan, 1999; Barn-
den and Lee, 2002; Agerri et al., 2007; Veale and
Hao, 2008).
(Shutova, 2010; Shutova et al., 2012) present
methods for deriving paraphrases for linguis-
tic metaphors from corpora. For example, the
metaphorical expression "a carelessly leaked re-
33
port" is paraphrased as "a carelessly disclosed re-
port". This approach currently focuses on single-
word metaphors expressed by verbs only and does
not explain the target?source mapping.
The KARMA (Narayanan, 1999) and the ATT-
Meta (Barnden and Lee, 2002; Agerri et al., 2007)
systems perform reasoning with manually coded
world knowledge and operate mainly in the source
domain. The ATT-Meta system takes logical ex-
pressions that are representations of a small dis-
course fragment as input; i.e., it does not work
with natural language. KARMA focuses on dy-
namics and motion in space. For example, the
metaphorical expression the government is stum-
bling in its efforts is interpreted in terms of motion
in space: stumbling leads to falling, while falling
is a conventional metaphor for failing.
(Veale and Hao, 2008) suggest to derive
common-sense knowledge from WordNet and cor-
pora in order to obtain concept properties that can
be used for metaphor interpretation. Simple in-
ference operations, i.e. insertions, deletions and
substitution, allow the system to establish links be-
tween target and source concepts.
(Hobbs, 1992) understands metaphor interpre-
tation as a part of the general discourse processing
problem. According to Hobbs, a metaphorical ex-
pression should be interpreted in context. For ex-
ample, John is an elephant can be best interpreted
as "John is clumsy" in the context Mary is grace-
ful, but John is an elephant. In order to obtain
context-dependent interpretations, (Hobbs, 1992)
uses abductive inference linking parts of the dis-
course and ensuring discourse coherence.
3 Metaphor Interpretation System
Our abduction-based metaphor interpretation sys-
tem is shown in Fig. 1. Text fragments possibly
containing linguistic metaphors are given as in-
put to the pipeline. The text fragments are parsed
and converted into logical forms (section 3.1).
The logical forms are input to the abductive rea-
soner (section 3.2) that is informed by a knowl-
edge base (section 4). The processing component
labelled "CM extractor & scorer" extracts con-
ceptual metaphors from the logical abductive in-
terpretations and outputs scored CMs and Target-
Source mappings (section 3.3). The Target-Source
mappings are then translated into natural language
expressions by the NL generator module (sec-
tion 3.4).
3.1 Logical Form Generation
A logical form (LF) is a conjunction of propo-
sitions which have argument links showing rela-
tionships among phrase constituents. We use logi-
cal representations of natural language texts as de-
scribed in (Hobbs, 1985). In order to obtain LFs
we convert dependency parses into logical repre-
sentations in two steps: 1) assign arguments to
each lemma, 2) apply rules to dependencies in or-
der to link arguments.
Consider the dependency structure for the sen-
tence, John decided to leave: [PRED decide
[SUBJ John] [OBJ leave]]. First, we
generate unlinked predicates for this structure:
John(e
1
, x
1
)?decide(e
2
, x
2
, x
3
)?leave(e
3
, x
4
).
Then, based on the dependency labels, we link
argument x
1
with x
2
, x
3
with e
3
, and x
1
with
x
4
to obtain the following LF: John(e
1
, x
1
) ?
decide(e
2
, x
1
, e
3
) ? leave(e
3
, x
1
).
LFs are preferable to dependency structures in
this case because they generalize over syntax and
link arguments using long-distance dependencies.
Furthermore, we need logical representations in
order to apply abductive inference.
In order to produce logical forms for English,
we use the Boxer semantic parser (Bos et al.,
2004). As one of the possible formats, Boxer
outputs logical forms of sentences in the style of
(Hobbs, 1985). For Russian, we use the Malt de-
pendency parser (Nivre et al., 2006). We devel-
oped a converter turning Malt dependencies into
logical forms in the style of (Hobbs, 1985).
1
3.2 Abductive Inference
In order to detect conceptual metaphors and in-
fer explicit mappings between target and source
domains, we employ a mode of inference called
weighted abduction (Hobbs et al., 1993). This
framework is appealing because it is a realization
of the observation that we understand new mate-
rial by linking it with what we already know.
Abduction is inference to the best explanation.
Formally, logical abduction is defined as follows:
Given: Background knowledge B, observations
O, where both B and O are sets of first-order log-
ical formulas,
Find: A hypothesis H such that H ?B |= O,H ?
B 6|=?, where H is a set of first-order logical for-
mulas.
1
The converter is freely available at
https://github.com/eovchinn/Metaphor-ADP.
34
Figure 1: Abduction-based metaphor interpretation system.
Typically, there exist several hypotheses H ex-
plaining O. To rank hypotheses according to plau-
sibility and select the best hypothesis, we use
the framework of weighted abduction (Hobbs et
al., 1993). Frequently, the best interpretation re-
sults from identifying two entities with each other,
so that their common properties only need to be
proved or assumed once. Weighted abduction fa-
vors those interpretations that link parts of obser-
vations together and supports discourse coherence,
which is crucial for discourse interpretation.
According to (Hobbs, 1985), metaphor interpre-
tation can be modelled as abductive inference re-
vealing conceptual overlap between the target and
the source domain. Consider the abductive inter-
pretation produced for the sentence We intend to
cure poverty, Fig. 2. In the top line of the figure,
we have the LF (cf. Sec. 3.1), where we can see
that a person (x
1
) is the agent for the verbs intend
(e
1
) and cure (e
2
) and that poverty (x
2
) is the ob-
ject of cure. In the first box in the next row, we
see that cure invokes the source concepts of DIS-
EASE, CURE, and DOCTOR, where DISEASE is
the object of CURE, and DOCTOR is the subject.
In the same row, we see that poverty invokes the
POVERTY concept in the target domain. Impor-
tantly, POVERTY and DISEASE share the same
argument (x
2
), which refers to poverty.
The next row contains two boxes with ellipses,
representing long chains of common-sense infer-
ences in the source and target domains of DIS-
EASE and POVERTY, respectively. For DIS-
EASE we know that linguistic tokens such as ill-
ness, sick, disease, etc. cause the afflicted to expe-
rience loss of health, loss of energy, and a general
lack of productivity. For POVERTY, we know that
tokens such as poor, broke, poverty mean that the
experiencer of poverty lacks money to buy things,
take care of basic needs, or have access to trans-
portation. The end result of both of these frame-
works is that the affected individuals (or commu-
nities) cannot function at a normal level, with re-
spect to unaffected peers. We can use this common
meaning of causing the individual to not function
to link the target to the source.
The next three rows provide the mapping
from the meaning of the source (CURE, DOC-
TOR, DISEASE) concepts to the target concept
(POVERTY). As explained above, we can con-
sider DISEASE as a CAUSING-AGENT that can
CAUSE NOT FUNCTION; POVERTY can be ex-
plained the same way, at a certain level of abstrac-
tion. Essentially, the interpretation of poverty in
this sentence is that it causes some entity not to
function, which is what a DISEASE does as well.
For CURE, we see that cure can CAUSE NOT EX-
IST, while looking for a CAUSING-AGENT (per-
son) and an EXISTING DISEASE (poverty).
In our system, we use the implementation of
weighted abduction based on Integer Linear Pro-
gramming (ILP) (Inoue and Inui, 2012), which
makes the inference scalable.
3.3 CM Extractor and Scorer
The abductive reasoning system produces an inter-
pretation that contains mappings of lexical items
into Target and Source domains. Any Target-
Source pair detected in a text fragment constitutes
a potential CM. For some text fragments, the sys-
tem identifies multiple CMs. We score Target-
Source pairs according to the length of the depen-
dency path linking them in the predicate-argument
structure. Consider the following text fragment:
opponents argue that any state attempting to force
an out-of-state business to do its dirty work of tax
collection violates another state?s right to regulate
its own corporate residents and their commerce
35
Figure 2: Abductive interpretation for the sentence We intend to cure poverty.
Suppose our target domain is TAXATION, trig-
gered by tax collection in the sentence above. In
our corpus, we find realizations of the CM TAXA-
TION is an ENEMY (fight against taxes). The lex-
eme opponent triggers the STRUGGLE/ENEMY
domain. However, the sentence does not trigger
the CM TAXATION is an ENEMY. Instead, it in-
stantiates the CM TAXATION is DIRT (dirty work
of tax collection). The length of the dependency
path between dirty and tax is equal to 2, whereas
the path between opponent and tax is equal to
9. Therefore, our procedure ranks TAXATION is
DIRT higher, which corresponds to the intuition
that target and source words should constitute a
syntactic phrase in order to trigger a CM.
3.4 NL Representation of Metaphor
Interpretation
The output of the abduction engine is similar to
the logical forms provided in Fig. 2. In order to
make the output more reader friendly, we produce
a natural language representation of the metaphor
interpretation using templates for each CM. For
example, the text their rivers of money mean they
can offer far more than a single vote would invoke
the WEALTH is WATER CM, and the abduction
engine would output: LARGE-AMOUNT[river],
THING-LARGE-AMOUNT[money]. We then
take this information and use it as input for the
NL generation module to produce: "river" implies
that there is a large amount of "money".
4 Knowledge Base
In order to process metaphors with abduction, we
need a knowledge base that encodes the informa-
tion about the source domain, the target domain,
and the relationships between sources and targets.
We develop two distinct sets of axioms: lexical ax-
ioms that encode lexical items triggering domains,
and mapping axioms that encode knowledge used
to link source and target domains. We will discuss
the details of each axiom type next.
4.1 Lexical Axioms
Every content word or phrase that can be expected
to trigger a source or target domain is included as a
lexical axiom in the knowledge base. For example,
the STRUGGLE domain contains words like war,
fight, combat, conquer, weapon, etc. An example
of how a lexical axiom encodes the system logic is
given in (1). On the left side, we have the linguistic
token, fight, along with its part-of-speech, vb, and
the argument structure for verbs where e
0
is the
eventuality (see (Hobbs, 1985)) of the action of
fighting, x is the subject of the verb, and y is the
object. On the right side, STRUGGLE is linked to
the action of fighting, the subject is marked as the
AGENT, and the object is marked as the ENEMY.
(1) fight-vb(e
0
, x, y) ? STRUGGLE(e
0
)?
AGENT (x, e
0
) ? ENEMY (y, e
0
)
The lexicon is not limited to single-token en-
tries; phrases can be included as single entries; For
example, the ABYSS domain has phrases such as
climb out of as a single entry. Encoding phrases
often proves useful, as function words can often
help to distinguish one domain from others. In
this case, climbing out of something usually de-
notes an abyss, whereas climbing up or on usually
does not. The lexical axioms also include the POS
36
for each word. Thus a word like fight can be en-
tered as both a noun and a verb. In cases where a
single lexical axiom could be applied to multiple
domains, one can create multiple entries for the
axiom with different domains and assign weights
so that a certain domain is preferred over others.
Initial lexical axioms for each domain were de-
veloped based on intuition about each domain.
We then utilize ConceptNet (Havasi et al., 2007)
as a source for semi-automatically extracting a
large-scale lexicon. ConceptNet is a multilingual
semantic network that establishes links between
words and phrases. We query ConceptNet for
our initial lexical axioms to return a list of related
words and expressions.
4.2 Mapping Axioms
Mapping axioms provide the underlying meanings
for metaphors and link source and target domains.
All of these axioms are written by hand based
on common-sense world knowledge about each
target-source pair. For each CM, we consider a
set of LMs that are realizations of this CM in an
effort to capture inferences that are common for
all of the LMs. We consider the linguistic contexts
of the LMs and overlapping properties of the tar-
get and source domains derived from corpora as
described in section 5.1.
We will outline the process of axiomatizing the
STRUGGLE domain here. We know that a verb
like fight includes concepts for the struggle it-
self, an agent, and an enemy. In the context of
a STRUGGLE, an enemy can be viewed as some
entity a that attempts to, or actually does, inhibit
the functioning of some entity b, often through ac-
tual physical means, but also psychologically, eco-
nomically, etc. The struggle, or fight, itself then,
is an attempt by a to rid itself of b so that a can en-
sure normal functionality. So, given a phrase like
poverty is our enemy, the intended meaning is that
poverty is hindering the functionality of some en-
tity (an individual, a community, a country, etc.)
and is seen as a problem that must be fought,
i.e. eliminated. In a phrase like the war against
poverty, war refers to an effort to stop the exis-
tence of poverty. These inferences are supported
by the overlapping property propositions extracted
from English Gigaword as described in Sec. 5.1,
e.g., scourge of X, country fights X, country pulls
of X, suffer from X, fight against X.
To extend the example in (1), consider (2).
Here, we encode a STRUGGLE action, e.g. fight,
as CAUSE NOT EXIST, the AGENT of the
fight as CAUSING-AGENT, and the ENEMY as
EXISTING-THING. Then, for a verb phrase like
we fight poverty, we is the AGENT that engages in
causing poverty, the ENEMY, to not exist.
(2) STRUGGLE(e
0
) ? AGENT (x, e
0
) ?
ENEMY (y, 2
0
)?CAUSE(e
0
)?CAUSED(n, e
0
)?
NOT (n, ex) ? EXIST (ex) ? CAUSING ?
AGENT (x, e
0
) ? EXISTING? THING(y, ex)
We use 75 mapping axioms to cover the valid
LMs discussed in Sec. 5.2. Some interesting
trends emerge when examining the core meanings
of the LMs. Following (Hobbs, 2005), we found
that over 65% of the valid LMs in this study could
be explained in terms of causality. The next most
prevalent aspect that these metaphors touch upon
is that of functionality (nearly 35%), with some of
these overlapping with the causality aspect where
the meaning has to do with X causing Y to function
or not function.
Many of the CMs covered in this study have
fairly transparent interpretations based on these
ideas of causality and functionality, such as
POVERTY is DISEASE, where the main under-
lying meaning is that a disease causes the suf-
ferer not to function properly. However, for some
CMs, the interpretation can be more difficult to
pin down. For example, the interpretation of
WEALTH is a GAME is quite opaque. Given a
sentence such as, Wealth is a game and you better
start playing the game, there are no obvious con-
nections to concepts such as causality or function-
ality. Instead, game raises such ideas as competi-
tion, winning, and losing. In the literal context of a
game, the competition itself, who the competitors
are, and what it means to win or lose are usually
clearly defined, but this is not so when speaking
metaphorically about wealth. To derive a meaning
of game that can apply to wealth, we must look
at a higher level of abstraction and define game as
the instantiation of a positive or negative outcome,
i.e. to win is to achieve a positive outcome, or
gain wealth. In the same sentence play implies that
some voluntary action must be taken to achieve a
positive outcome.
For some metaphors, a simple transfer of the
source properties to the target does not result in
a coherent interpretation at all. Given, for exam-
ple, the CM POVERTY is a PRICE, one LM from
this study is, poverty is the price of peace. In this
case, the meaning has to do with some notion of
37
an exchange, where a negative consequence must
be accepted in order to achieve a desired outcome.
However, the metaphorical meaning of price dif-
fers from the literal meaning of the word. In literal
contexts, price refers to an amount of money or
goods with inherent value that must be given to ac-
quire something; the buyer has a supply of money
or goods that they willingly exchange for their
desired item. In the metaphorical sense, though,
there often is no buyer, and there is certainly not
an inherent value that can be assigned to poverty,
nor can one use a supply of it to acquire peace.
Another issue concerns cultural differences.
While writing the axioms to deal with English and
Russian source-target pairs we noticed that a ma-
jority of the axioms applied equally well to both
languages. However, there are some subtle dif-
ferences of aspect that impact the interpretation
of similar CMs across the two languages. Look-
ing again at the WEALTH is a GAME metaphor,
the Russian interpretation involves some nuance
of a lack of importance about the subject that
does not seem to be present in English when us-
ing words like game and play. Note that there
may be some notion of carelessness for English
(see Sec. 5.3), but for Russian, the notion of being
carefree, which is not the same as careless, about
wealth has a strong prevalence.
5 Experimental Validation
5.1 Source Generation
Following from the definition of metaphor, the tar-
get and the source domain share certain proper-
ties. In natural language, concepts and properties
are represented by words and phrases. There is
a long-standing tradition for considering compu-
tational models derived from word co-occurrence
statistics as being capable of producing reason-
able property-based descriptions of concepts (Ba-
roni and Lenci, 2008). We use proposition stores
to derive salient properties of concepts that can be
potentially compared in a metaphor.
A proposition store is a collection of proposi-
tions such that each proposition is assigned its fre-
quency in a corpus. Propositions are tuples of
words that have a determined pattern of syntactic
relations among them (Clark and Harrison, 2009;
Pe?as and Hovy, 2010; Tsao and Wible, 2013).
For example, the following propositions can be ex-
tracted from the sentence John decided to go to
school:
(NV John decide)
(NV John go)
(NVPN John go to school)
...
We generated proposition stores from parsed
English Gigaword (Parker et al., 2011) and Rus-
sian ruWac (Sharoff and Nivre, 2011). Given the
proposition stores, we generate potential sources
for a seed target lexeme l in three steps:
1. Find all propositions P
l
containing l.
2. Find all potential source lexemes S such that
for each s ? S there are propositions p, p
?
in the proposition store such that l occurs at
position i in p and s occurs at position i in p
?
.
The set of propositions containing l and s at
the same positions is denoted by P
l,s
.
3. Weight potential sources s ? S using the fol-
lowing equation:
weight
l
(s) =
?
p?P
l,s
weight
l
(t), (1)
The source generation procedure and
its validations are described in detail at
github.com/MetaphorExtractionTools/mokujin.
2
In the experiment described below, we gener-
ated potential sources for the target domains of
POVERTY and WEALTH.
5.2 Linguistic Metaphors Extraction and
Validation
For each potential CM, we look for supporting
LMs in corpora. A a large number of LMs sup-
porting a particular CM suggests that this CM
might be cognitively plausible. We use a simple
method for finding LMs. If a target lexeme and
a source lexeme are connected by a dependency
relation in a sentence, then we assume that this
dependency structure contains a LM. For exam-
ple, in the phrases medicine against poverty and
chronic poverty, the target word (poverty) is re-
lated via dependency arc with the source words
(medicine, chronic). LMs were extracted from En-
glish Gigaword (Parker et al., 2011) and Russian
ruWac (Sharoff and Nivre, 2011).
For the generated CMs, we select seed lexemes
for target and source domains. We expand the
2
The tools for generating proposition stores
and the obtained resources are freely available at
https://ovchinnikova.me/proj/metaphor.html.
38
sets of these target and source lexemes with se-
mantically related lexemes using English and Rus-
sian ConceptNet (Speer and Havasi, 2013) and top
ranked patterns from the proposition stores. For
example, the expansion of the lexeme disease re-
sults in the following set of lexemes: {disease,
symptom, syndrome, illness, unwellness, sickness,
sick, medicine, treatment, treat, cure, doctor, ... }
For each language, we select 20 top-ranked
sources per target. Then we randomly select at
most 10 sentences per each target-source pair.
These sentences are validated by 3 linguist experts
each. For each sentence, the experts are asked if
it contains a metaphor comparing an indicated tar-
get domain with an indicated source domain. The
inter-annotator agreement on the validation task is
defined as the percentage of judgements on which
the three experts agree. Agreement is 81% for En-
glish and 80% for Russian.
Tables 1 and 2 show 10 potential sources per
target with the best agreement. Column ALL pro-
vides the number of sentences per a proposed CM
such that all experts agreed that the sentence con-
tains a metaphor. Column TWO provides the num-
ber of sentences such that any two experts agreed
on, and Column ONE shows the number of sen-
tences such that a single expert thought it con-
tained a metaphor.
target source ALL TWO ONE
w
e
a
l
t
h
blood 10 10 10
water 9 10 10
drug 9 10 10
food 9 9 10
body 9 9 10
power 8 9 10
game 8 9 9
security 7 9 10
resource 7 7 9
disease 7 8 9
p
o
v
e
r
t
y
war 10 10 10
abyss 10 10 10
violence 9 9 10
price 8 9 9
location 7 8 8
disease 7 7 7
crime 4 5 6
crop 3 7 9
terrorism 3 3 5
cost 2 3 7
Table 1: Validation of English linguistic
metaphors found for potential sources.
5.3 Metaphor Interpretation Validation
Metaphor interpretations were generated for posi-
tively validated linguistic metaphors, as described
?
?
?
?
?
?
?
?
?
(
w
e
a
l
t
h
)
??????? (energy) 10 10 10
???? (water) 10 10 10
??????? (freedom) 10 10 10
?????? (power) 9 10 10
??? (god) 9 10 10
????? (blood) 9 10 10
???? (way) 9 10 10
???? (game) 8 10 10
????? (glory) 4 5 5
????? (ware) 3 8 10
?
?
?
?
?
?
?
?
(
p
o
v
e
r
t
y
)
???????? (abyss) 10 10 10
???? (enemy) 9 10 10
??????? (disease) 9 9 9
?????? (power) 8 10 10
???? (body) 6 6 6
???? (pain) 5 10 10
???????? (despair) 5 10 10
???? (price) 4 4 4
?????? (death) 3 5 6
????? (fear) 3 9 10
Table 2: Validation of Russian linguistic
metaphors found for potential sources.
in Sec. 3.4. Each interpretation was validated by
three expert linguists. We calculated strict and
relaxed agreement for the validated data. Strict
agreement is calculated over three categories: cor-
rect (C), partially correct (P), and incorrect (I). Re-
laxed agreement is calculated over two categories:
C/P and I. Partially correct means that the valida-
tor felt that something was missing from the inter-
pretation, but that what was there was not wrong.
Table 3 presents the validation results for both lan-
guages. As can be seen in the table, strict agree-
ment (AgrS) is 62% and 52% and strict system
accuracy (AccS ALL) is 62% and 50% for En-
glish and Russian, respectively. Relaxed agree-
ment (AgrR) results is 93% and 83%, and relaxed
accuracy (AccR ALL) is 91% and 78%.
Validators often marked things as only partially
correct if they felt that the interpretation was lack-
ing some aspect that was critical to the meaning of
the metaphor. A common feeling amongst the val-
idators, for example, is that the interpretation for
people who are terrorized by poverty should in-
clude some mention of "fear" as a crucial aspect
of the metaphor, as the interpretation provided
states only that "terrorize" implies that "poverty"
is causing "people" not to function. However, the
end result of "fear" itself is often that the experi-
encer cannot function, as in paralyzed by fear.
Tables 4 and 5 contain interpretation system ac-
curacy results by CM. We calculated the percent-
age of LMs evoking this CM that were validated
as C vs. I (strict) or P/C vs. I (relaxed) by all three
39
AgrS AgrR AccS ALL AccS TWO AccS ONE AccR ALL AccR TWO AccR ONE
English 0.62 0.93 0.62 0.84 0.98 0.91 0.97 0.99
Russian 0.52 0.83 0.50 0.76 0.96 0.78 0.93 0.99
Table 3: Validation results for metaphor interpretation for English and Russian.
(ALL), or just two (TWO) validators. In most of
the cases, the system performs well on "simple"
CMs related to the concepts of causation and func-
tioning (e.g., WEALTH is POWER), cf. section 4,
whereas its accuracy is lower for richer metaphors
(e.g., WEALTH is a GAME).
target source
ALL TWO
S R S R
w
e
a
l
t
h
blood 0.8 1 1 1
water 1 1 1 1
drug 0.44 0.78 0.89 0.89
food 0.89 1 1 1
body 0.67 0.78 0.78 0.78
power 1 1 1 1
game 0.63 1 1 1
security 0.14 0.88 0.71 1
resource 1 1 1 1
disease 0 1 1 1
p
o
v
e
r
t
y
war 0.9 0.9 1 1
abyss 0 0.5 0.4 1
violence 0 1 0.11 1
price 0.88 0.88 0.88 1
location 1 1 1 1
disease 0.43 0.86 0.86 0.86
crime 0.75 1 1 1
crop 1 1 1 1
terrorism 0 1 0.33 1
cost 1 1 1 1
Table 4: Accuracy of English interpretations for
each CM.
The data used in the described experiments, sys-
tem output, and expert validations are available
at http://ovchinnikova.me/suppl/AbductionSystem-
Metaphor-Validation.7z.
6 Conclusion and Future Work
The developed abduction-based metaphor
interpretation pipeline is available at
https://github.com/eovchinn/Metaphor-ADP
as a free open-source project. This pipeline
produces favorable results, with metaphor in-
terpretations that are rated as at least partially
correct, for over 90% of all valid metaphors it is
given for English, and close to 80% for Russian.
Granted, the current research is performed using a
small, controlled set of metaphors, so these results
could prove difficult to reproduce on a large scale
where any metaphor is possible. Still, the high
accuracies achieved on both languages indicate
T source
ALL TWO
S R S R
?
?
?
?
?
?
?
?
?
(
w
e
a
l
t
h
)
??????? (energy) 0.4 0.8 0.9 1
???? (water) 0 0.9 0.6 0.9
??????? (freedom) 1 1 1 1
?????? (power) 1 1 1 1
??? (god) 0.67 1 0.89 1
????? (blood) 1 1 1 1
???? (way) 0.78 0.78 0.89 0.89
???? (game) 0.1 0.2 0.2 0.3
????? (glory) 0 0.75 0.75 1
????? (ware) 0 0 0 1
?
?
?
?
?
?
?
?
(
p
o
v
e
r
t
y
)
???????? (abyss) 0.7 1 1 1
???? (enemy) 0.56 1 1 1
??????? (disease) 0.33 0.89 0.67 1
?????? (power) 0.5 0.5 1 1
???? (body) 0.17 0.17 0.17 0.83
???? (pain) 1 1 1 1
???????? (despair) 0.6 0.6 1 1
???? (price) 0.75 0.75 1 1
?????? (death) 0 0 0.33 1
????? (fear) 0 1 0.67 1
Table 5: Accuracy of Russian interpretations for
each CM.
that the approach is sound and there is potential
for future work.
The current axiomatization methodology is
based mainly on manually writing mapping ax-
ioms based on the axiom author?s intuition. Ob-
viously, this approach is subject to scrutiny re-
garding the appropriateness of the metaphors and
faces scalability issues. Thus, developing new au-
tomatic methods to construct the domain knowl-
edge bases is a main area for future consideration.
The mapping axioms present a significant chal-
lenge as far producing reliable output automati-
cally. One area for consideration is the afore-
mentioned prevalence of certain underlying mean-
ings such as causality and functionality. Gather-
ing enough examples of these by hand could lead
to generalizations in argument structure that could
then be applied to metaphorical phrases in cor-
pora to extract new metaphors with similar mean-
ings. Crowd-sourcing is another option that could
be applied to both axiom writing tasks in order to
develop a large-scale knowledge base in consid-
erably less time and at a lower cost than having
experts build the knowledge base manually.
40
References
R. Agerri, J.A. Barnden, M.G. Lee, and A.M. Walling-
ton. 2007. Metaphor, inference and domain-
independent mappings. In Proc. of RANLP?07,
pages 17?23.
J. A. Barnden and M. G. Lee. 2002. An artificial intel-
ligence approach to metaphor understanding. Theo-
ria et Historia Scientiarum, 6(1):399?412.
M. Baroni and A. Lenci. 2008. Concepts and proper-
ties in word spaces. Italian Journal of Linguistics,
20(1):55?88.
J. Bos, S. Clark, M. Steedman, J. R. Curran, and
J. Hockenmaier. 2004. Wide-coverage semantic
representations from a ccg parser. In Proc. of COL-
ING?04, pages 1240?1246.
P. Clark and P. Harrison. 2009. Large-scale extrac-
tion and use of knowledge from text. In Proc. of the
5th international conference on Knowledge capture,
pages 153?160. ACM.
Catherine Havasi, Robert Speer, and Jason Alonso.
2007. Conceptnet 3: a flexible, multilingual se-
mantic network for common sense knowledge. In
Recent Advances in Natural Language Processing,
Borovets, Bulgaria, September.
J. R. Hobbs, M. Stickel, P. Martin, and D. Edwards.
1993. Interpretation as abduction. Artificial Intelli-
gence, 63:69?142.
J. R. Hobbs. 1985. Ontological promiscuity. In Proc.
of ACL, pages 61?69, Chicago, Illinois.
J. R. Hobbs. 1992. Metaphor and abduction. In
A. Ortony, J. Slack, and O. Stock, editors, Com-
munication from an Artificial Intelligence Perspec-
tive: Theoretical and Applied Issues, pages 35?58.
Springer, Berlin, Heidelberg.
Jerry R. Hobbs. 2005. Toward a useful concept of
causality for lexical semantics. Journal of Seman-
tics, 22(2):181?209.
N. Inoue and K. Inui. 2012. Large-scale cost-based
abduction in full-fledged first-order predicate logic
with cutting plane inference. In Proc. of JELIA,
pages 281?293.
G. Lakoff and M. Johnson. 1980. Metaphors we Live
by. University of Chicago Press.
G. Lakoff. 1987. Women, fire, and dangerous things:
what categories reveal about the mind. University
of Chicago Press.
S. Narayanan. 1999. Moving right along: A computa-
tional model of metaphoric reasoning about events.
In Proc. of AAAI/IAAI, pages 121?127.
J. Nivre, J. Hall, and J. Nilsson. 2006. Maltparser:
A data-driven parser-generator for dependency pars-
ing. In Proc. of LREC?06, volume 6, pages 2216?
2219.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011. English gigaword fifth edition. LDC.
A. Pe?as and E. H. Hovy. 2010. Filling knowledge
gaps in text for machine reading. In Proc. of COL-
ING?10, pages 979?987.
S. Sharoff and J. Nivre. 2011. The proper place of
men and machines in language technology: Process-
ing Russian without any linguistic knowledge. In
Proc. Dialogue 2011, Russian Conference on Com-
putational Linguistics.
E. Shutova, T. Van de Cruys, and A. Korhonen. 2012.
Unsupervised metaphor paraphrasing using a vector
space model. In COLING (Posters), pages 1121?
1130.
E. Shutova. 2010. Automatic metaphor interpretation
as a paraphrasing task. In Proc. of NAACL?10.
R. Speer and C. Havasi. 2013. Conceptnet 5: A large
semantic network for relational knowledge. In The
People?s Web Meets NLP, pages 161?176. Springer.
N. Tsao and D. Wible. 2013. Word similarity us-
ing constructions as contextual features. In Proc.
JSSP?13, pages 51?59.
T. Veale and Y. Hao. 2008. A fluid knowledge repre-
sentation for understanding and generating creative
metaphors. In Proc. of COLING?08, pages 945?952.
ACL.
41
