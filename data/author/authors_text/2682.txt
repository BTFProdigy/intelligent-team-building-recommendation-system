Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 145?148,
New York, June 2006. c?2006 Association for Computational Linguistics
Selecting relevant text subsets from web-data for building topic specic
language models
Abhinav Sethy, Panayiotis G. Georgiou, Shrikanth Narayanan
Speech Analysis and Interpretation Lab
Integrated Media Systems Center
Viterbi School of Engineering
Department of Electrical Engineering-Systems
University of Southern California
Abstract
In this paper we present a scheme to se-
lect relevant subsets of sentences from a
large generic corpus such as text acquired
from the web. A relative entropy (R.E)
based criterion is used to incrementally se-
lect sentences whose distribution matches
the domain of interest. Experimental re-
sults show that by using the proposed sub-
set selection scheme we can get signif-
icant performance improvement in both
Word Error Rate (WER) and Perplexity
(PPL) over the models built from the en-
tire web-corpus by using just 10% of the
data. In addition incremental data selec-
tion enables us to achieve significant re-
duction in the vocabulary size as well as
number of n-grams in the adapted lan-
guage model. To demonstrate the gains
from our method we provide a compar-
ative analysis with a number of methods
proposed in recent language modeling lit-
erature for cleaning up text.
1 Introduction
One of the main challenges in the rapid deployment
of NLP applications is the lack of in-domain data
required for training statistical models. Language
models, especially n-gram based, are key compo-
nents of most NLP applications, such as speech
recognition and machine translation, where they
serve as priors in the decoding process. To estimate
a n-gram language model we require examples of
in-domain transcribed utterances, which in absence
of readily available relevant corpora have to be col-
lected manually. This poses severe constraints in
terms of both system turnaround time and cost.
This led to a growing interest in using the World
Wide Web (WWW) as a corpus for NLP (Lapata,
2005; Resnik and Smith, 2003). The web can serve
as a good resource for automatically gathering data
for building task-specific language models. Web-
pages of interest can be identified by generating
query terms either manually or automatically from
an initial set of in-domain sentences by measures
such as TFIDF or Relative Entropy (R.E). These
webpages can then be converted to a text corpus
(which we will refer to as web-data) by appropri-
ate preprocessing. However text gathered from the
web will rarely fit the demands or the nature of the
domain of interest completely. Even with the best
queries and web crawling schemes, both the style
and content of the web-data will usually differ sig-
nificantly from the specific needs. For example, a
speech recognition system requires conversational
style text whereas most of the data on the web is
literary.
The mismatch between in-domain data and web-
data can be seen as a semi-supervised learning prob-
lem. We can model the web-data as a mix of sen-
tences from two classes: in-domain (I) and noise
(N) (or out-of-domain). The labels I and N are la-
tent and unknown for the sentences in web-data but
we usually have a small number of examples of in-
domain examples I. Selecting the right labels for
the unlabeled set is important for benefiting from it.
145
Recent research on semi-supervised learning shows
that in many cases (Nigam et al, 2000; Zhu, 2005)
poor preprocessing of unlabeled data might actually
lower the performance of classifiers. We found sim-
ilar results in our language modeling experiments
where the presence of a large set of noisy N ex-
amples in training actually lowers the performance
slightly in both perplexity and WER terms. Recent
literature on building language models from text ac-
quired from the web addresses this issue partly by
using various rank-and-select schemes for identify-
ing the set I (Ostendorf et al, 2005; Sethy, 2005;
Sarikaya, 2005). However we believe that simi-
lar to the question of balance (Zhu, 2005) in semi-
supervised learning for classification, we need to ad-
dress the question of distributional similarity while
selecting the appropriate utterances for building a
language model from noisy data. The subset of sen-
tences from web-data which are selected to build the
adaptation language should have a distribution sim-
ilar to the in-domain data model.
To address the issue of distributional similarity we
present an incremental algorithm which compares
the distribution of the selected set and the in-domain
examples by using a relative entropy (R.E) criterion.
We will review in section 2 some of the ranking
schemes which provide baselines for performance
comparison and in section 3 we describe the pro-
posed algorithm. Experimental results are provided
in section 4, before we conclude with a summary of
this work and directions for the future.
2 Rank and select methods for text
cleaning
The central idea behind text cleanup schemes in re-
cent literature, on using web-data for language mod-
eling, is to use a scoring function that measures the
similarity of each observed sentence in the web-
data to the in-domain set and assigns an appropri-
ate score. The subsequent step is to set a threshold
in terms of either the minimum score or the num-
ber of top scoring sentences. The threshold can usu-
ally be fixed using a heldout set. Ostendorf (2005)
use perplexity from an in-domain n-gram language
model as a scoring function. More recently, a mod-
ified version of the BLEU metric which measures
sentence similarity in machine translation has been
proposed by Sarikaya (2005) as a scoring function.
Instead of explicit ranking and thresholding it is also
possible to design a classifier in a learning from pos-
itive and unlabeled examples framework (LPU) (Liu
et al, 2003). In this system, a subset of the unla-
beled set is selected as the negative or the noise set
N. A two class classifier is then trained using the
in-domain set and the negative set. The classifier
is then used to label the sentences in the web-data.
The classifier can then be iteratively refined by us-
ing a better and larger subset of the I/N sentences
selected in each iteration.
Rank ordering schemes do not address the issue of
distributional similarity and select many sentences
which already have a high probability in the in-
domain text. Adapting models on such data has the
tendency to skew the distribution even further to-
wards the center. For example, in our doctor-patient
interaction task short sentences containing the word
?okay? such as ?okay?,?yes okay?, ?okay okay? were
very frequent in the in-domain data. Perplexity or
other similarity measures give a high score to all
such examples in the web-data boosting the prob-
ability of these words even further while other perti-
nent sentences unseen in the in-domain data such as
?Can you stand up please?? are ranked low and get
rejected.
3 Incremental Selection
To address the issue of distributional similarity we
developed an incremental greedy selection scheme
based on relative entropy which selects a sentence
if adding it to the already selected set of sentences
reduces the relative entropy with respect to the in-
domain data distribution.
Let us denote the language model built from in-
domain data as P and let Pinit be a language modelfor initialization purposes which we estimate by
bagging samples from the same in-domain data. To
describe our algorithm we will employ the paradigm
of unigram probabilities though the method general-
izes to higher n-grams also.
Let W (i) be a initial set of counts for the words
i in the vocabulary V initialized using Pinit. We de-note the count of word i in the j th sentence sj ofweb-data with mij . Let nj = ?i mij be the num-ber of words in the sentence and N = ?i W (i) be
146
the total number of words already selected. The rel-
ative entropy of the maximum likelihood estimate of
the language model of the selected sentences to the
initial model P is given by
H(j ? 1) = ?
?
i
P (i) ln P (i)W (i)/N
If we select the sentence sj , the updated R.E
H(j) = ?
?
i
P (i) ln P (i)(W (i) + mij)/(N + nj)
Direct computation of R.E using the above ex-
pressions for every sentence in the web-data will
have a very high computational cost since O(V )
computations per sentence in the web-data are re-
quired. However given the fact that mij is sparse,we can split the summation H(j) into
H(j) = ?
?
i
P (i) ln P (i) +
+
?
i
P (i) ln W (i) + mijN + nj
= H(j ? 1) + ln N + njN
? ?? ?
T1
?
?
i,mij 6=0
P (i) ln (W (i) + mij)W (i)
? ?? ?
T2
Intuitively, the term T1 measures the decrease
in probability mass because of adding nj wordsmore to the corpus and the term T2 measures the
in-domain distribution P weighted improvement in
probability for words with non-zero mij .For the R.E to decrease with selection of sentence
sj we require T1 < T2. To make the selection morerefined we can impose a condition T1 + thr(j) <
T2 where thr(j) is a function of j. A good choice
for thr(j) based on empirical study is a function that
declines at the same rate as the ratio ln (N+nj)N ?
nj/N ? 1/kj where k is the average number ofwords for every sentence.
The proposed algorithm is sequential and greedy
in nature and can benefit from randomization of the
order in which it scans the corpus. We generate per-
mutes of the corpus by scanning through the corpus
and randomly swapping sentences. Next we do se-
quential selection on each permutation and merge
the selected sets.
The choice of using maximum likelihood estima-
tion for estimating the intermediate language mod-
els for W (j) is motivated by the simplification in
the entropy calculation which reduces the order from
O(V ) to O(k). However, maximum likelihood esti-
mation of language models is poor when compared
to smoothing based estimation. To balance the com-
putation cost and estimation accuracy, we modify
the counts W (j) using Kneser-Ney smoothing pe-
riodically after fixed number of sentences.
4 Experiments
Our experiments were conducted on medical do-
main data collected for building the English ASR
of our English-Persian Speech to Speech translation
project (Georgiou et al, 2003). We have 50K in-
domain sentences for this task available. We down-
loaded around 60GB data from the web using au-
tomatically generated queries which after filtering
and normalization amount to 150M words. The test
set for perplexity evaluations consists of 5000 sen-
tences(35K words) and the heldout set had 2000
sentences (12K words). The test set for word er-
ror rate evaluation consisted of 520 utterances. A
generic conversational speech language model was
built from the WSJ, Fisher and SWB corpora in-
terpolated with the CMU LM. All language models
built from web-data and in-domain data were inter-
polated with this language model with the interpola-
tion weight determined on the heldout set.
We first compare our proposed algorithm against
baselines based on perplexity(PPL), BLEU and LPU
classification in terms of test set perplexity. As the
comparison shows the proposed algorithm outper-
forms the rank-and-select schemes with just 1/10th
of data. Table 1 shows the test set perplexity with
different amounts of initial in-domain data. Table 2
shows the number of sentences selected for the best
perplexity on the heldout set by the above schemes.
The average relative perplexity reduction is around
6%. In addition to the PPL and WER improvements
we were able to acheive a factor of 5 reduction in
the number of estimated language model parameters
(bigram+trigram) and a 30% reduction in the vocab-
147
10K 20K 30K 40K
No Web 60 49.6 42.2 39.7
AllWeb 57.1 48.1 41.8 38.2
PPL 56.1 48.1 41.8 38.2
BLEU 56.3 48.2 42.0 38.3
LPU 56.3 48.2 42.0 38.3
Proposed 54.8 46.8 40.7 38.1
Table 1: Perplexity of testdata with the web adapted
model for different number of initial sentences.
ulary size. No Web refers to the language model built
from just in-domain data with no web-data. All-
Web refers to the case where the entire web-data was
used.
The WER results in Table 3 show that adding data
from the web without proper filtering can actually
harm the performance of the speech recognition sys-
tem when the initial in-domain data size increases.
This can be attributed to the large increase in vo-
cabulary size which increases the acoustic decoder
perplexity. The average reduction in WER using the
proposed scheme is close to 3% relative. It is inter-
esting to note that for our data selection scheme the
perplexity improvments correlate surprisingly well
with WER improvments. A plausible explanation
is that the perplexity improvments are accompanied
by a significant reduction in the number of language
model parameters.
5 Conclusion and Future Work
In this paper we have presented a computationally
efficient scheme for selecting a subset of data from
an unclean generic corpus such as data acquired
from the web. Our results indicate that with this
scheme, we can identify small subsets of sentences
(about 1/10th of the original corpus), with which we
can build language models which are substantially
smaller in size and yet have better performance in
10K 20K 30K 40K
PPL 93 92 91 91
BLEU 91 90 89 89
LPU 90 88 87 87
Proposed 12 11 11 12
Table 2: Percentage of web-data selected for differ-
ent number of initial sentences.
10K 20K 30K 40K
No Web 19.8 18.9 18.3 17.9
AllWeb 19.5 19.1 18.7 17.9
PPL 19.2 18.8 18.5 17.9
BLEU 19.3 18.8 18.5 17.9
LPU 19.2 18.8 18.5 17.8
Proposed 18.3 18.2 18.2 17.3
Table 3: Word Error Rate (WER) with web adapted
models for different number of initial sentences.
both perplexity and WER terms compared to models
built using the entire corpus. Although our focus in
the paper was on web-data, we believe the proposed
method can be used for adaptation of topic specific
models from large generic corpora.
We are currently exploring ways to use multiple
bagged in-domain language models for the selection
process. Instead of sequential scan of the corpus, we
are exploring the use of rank-and-select methods to
give a better search sequence.
References
Abhinav Sethy and Panayiotis Georgiou et al. Building topic
specific language models from web-data using competitive
models. Proceedings of Eurospeech. 2005
Bing Liu and Yang Dai et al. Building Text Classifiers Using
Positive and Unlabeled Examples. Proceedings of ICDM.
2003
Kamal Nigam and Andrew Kachites McCallum et al. Text
Classification from Labeled and Unlabeled Documents using
EM. Journal of Machine Learning. 39(2:3)103?134. 2000
Mirella Lapata and Frank Keller. Web-based models for natu-
ral language processing. ACM Transactions on Speech and
Language Processing. 2(1),2005.
Philip Resnik and Noah A. Smith. The Web as a parallel cor-
pus. Computational Linguistics. 29(3),2003.
P.G. Georgiou and S.Narayanan et al. Transonics: A speech to
speech system for English-Persian Interactions. Proceedings
of IEEE ASRU. 2003
Ruhi Sarikaya and Agustin Gravano et al Rapid Language
Model Development Using External Resources For New
Spoken Dialog Domains Proceedings of ICASSP. 2005
Tim Ng and Mari Ostendorf et al. Web-data Augmented Lan-
guage Model for Mandarin Speech Recognition. Proceed-
ings of ICASSP. 2005
Xiaojin Zhu. Semi-Supervised Learning Literature Survey.
Computer Science, University of Wisconsin-Madison.
148
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 89?92, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Transonics: A Practical Speech-to-Speech Translator for English-Farsi
Medical Dialogues
Emil Ettelaie, Sudeep Gandhe, Panayiotis Georgiou,
Kevin Knight, Daniel Marcu, Shrikanth Narayanan ,
David Traum
University of Southern California
Los Angeles, CA 90089
ettelaie@isi.edu, gandhe@ict.usc.edu,
georgiou@sipi.usc.edu, knight@isi.edu,
marcu@isi.edu, shri@sipi.usc.edu,
traum@ict.use.edu
Robert Belvin
HRL Laboratories, LLC
3011 Malibu Canyon Rd.
Malibu, CA 90265
rsbelvin@hrl.com
Abstract
We briefly describe a two-way speech-to-
speech English-Farsi translation system
prototype developed for use in doctor-
patient interactions.  The overarching
philosophy of the developers has been to
create a system that enables effective
communication, rather than focusing on
maximizing component-level perform-
ance.  The discussion focuses on the gen-
eral approach and evaluation of the
system by an independent government
evaluation team.
1 Introduction
In this paper we give a brief description of a
two-way speech-to-speech translation system,
which was created under a collaborative effort
between three organizations within USC (the
Speech Analysis and Interpretation Lab of the
Electrical Engineering department, the Information
Sciences Institute, and the Institute for Creative
Technologies) and the Information Sciences Lab of
HRL Laboratories.  The system is intended to pro-
vide a means of enabling communication between
monolingual English speakers and monolingual
Farsi (Persian) speakers.  The system is targeted at
a domain which may be roughly characterized as
"urgent care" medical interactions, where the Eng-
lish speaker is a medical professional and the Farsi
speaker is the patient.  In addition to providing a
brief description of the system (and pointers to pa-
pers which contain more detailed information), we
give an overview of the major system evaluation
activities.
2 General Design of the system
Our system is comprised of seven speech and
language processing components, as shown in Fig.
1. Modules communicate using a centralized mes-
sage-passing system. The individual subsystems
are the Automatic Speech Recognition (ASR) sub-
system, which uses n-gram Language Models
(LM) and produces n-best lists/lattices along with
the decoding confidence scores. The output of the
ASR is sent to the Dialog Manager (DM), which
displays the n-best and passes one hypothesis on to
the translation modules, according to a user-
configurable state. The DM sends translation re-
quests to the Machine Translation (MT) unit. The
MT unit works in two modes: Classifier based MT
and a fully Stochastic MT. Depending on the dia-
logue manager mode, translations can be sent to
the unit selection based Text-To-Speech synthe-
sizer (TTS), to provide the spoken output. The
same basic pipeline works in both directions: Eng-
lish ASR, English-Persian MT, Persian TTS, or
Persian ASR, Persian-English MT, English TTS.
There is, however, an asymmetry in the dia-
logue management and control, given the desire for
the English-speaking doctor to be in control of the
device and the primary "director" of the dialog.
The English ASR used the University of Colo-
rado Sonic recognizer, augmented primarily with
LM data collected from multiple sources, including
89
our own large-scale simulated doctor-patient dia-
logue corpus based on recordings of medical stu-
dents examining standardized patients (details in
Belvin et al 2004).
1
 The Farsi acoustic models r e-
quired an eclectic approach due to the lack of ex-
isting labeled speech corpora.  The approach
included borrowing acoustic data from English by
means of developing a sub-phonetic mapping be-
tween the two languages, as detailed in (Srini-
vasamurthy & Narayanan 2003), as well as use of
a small existing Farsi speech corpus (FARSDAT),
and our own team-internally generated acoustic
data.  Language modeling data was also obtained
from multiple sources.  The Defense Language
Institute translated approximately 600,000 words
of English medical dialogue data (including our
standardized patient data mentioned above), and in
addition, we were able to obtain usable Farsi text
from mining the web for electronic news sources.
Other  smaller  amounts of  training  data  were ob
tained from various sources, as detailed in  (Nara-
yanan et al 2003, 2004).  Additional detail on de-
velopment methods for all of these components,
system integration and evaluation can also be
found in the papers just cited.
The MT components, as noted, consist of both a
Classifier and a stochastic translation engine,  both
                                                           
1
 Standardized Patients are typically actors who have been
trained by doctors or nurses to portray symptoms of particular
illnesses or injuries.  They are used extensively in medical
education so that doctors in training don't have to "practice"
on real patients.
developed by USC-ISI team members.  The Eng-
lish Classifier uses approximately 1400 classes
consisting mostly of standard questions used by
medical care providers in medical interviews.
Each class has a large number of paraphrases asso-
ciated with it, such that if the care provider speaks
one of those phrases, the system will identify it
with the class and translate it to Farsi via table-
lookup.  If the Classifier cannot succeed in finding
a match exceeding a confidence threshold, the sto-
chastic MT engine will be employed.  The sto-
chastic MT engine relies on n-gram
correspondences between the source and target
languages.  As with ASR, the performance of the
component is highly dependent on very large
amounts of training data.  Again, there were multi-
ple sources of training data used, the most signifi-
cant being the data generated by our own team's
English collection effort, supported by translation
into Farsi by DLI. Further details of the MT com-
ponents can be found in Narayanan et al, op.cit.
3 Enabling Effective Communication
The approach taken in the development of Tran-
sonics was what can be referred to as the total
communication pathway.  We are not so concerned
with trying to maximize the performance of a
given component of the system, but rather with the
effectiveness of the system as a whole in facilitat-
ing actual communication.  To this end, our design
and development included the following:
MT
English to Farsi
Farsi to English
ASR
English
Prompts or TTS
Farsi
Prompts or TTS
English
ASR
Farsi
GUI:
prompts,
 confirmations,
 ASR switch
Dialog
Manager
SMT
English to Farsi
Farsi to English
Figure 1: Architecture of the Transonics system.  The Dialogue Manager acts as the hub through which the
individual components interact.
90
i. an "educated guess" capability (system
guessing at the meaning of an utterance) from the
Classifier translation mechanism?this proved very
useful for noisy ASR output, especially for the re-
stricted domain of medical interviews.
ii. a flexible and robust SMT good for filling in
where the more accurate Classifier misses.
iii. exploitation of a partial n-best list as part of
the GUI used by the doctor/medic for the English
ASR component and the Farsi-to-English transla-
tion component.
iv. a dialog manager which in essence occa-
sionally makes  "suggestions" (for next questions
for the doctor to ask) based on query sets which are
topically related to the query the system believes it
recognized the doctor to have spoken.
Overall, the system achieves a respectable level of
performance in terms of allowing users to follow a
conversational thread in a fairly coherent way, de-
spite the presence of frequent ungrammatical or
awkward translations (i.e. despite what we might
call non-catastrophic errors).
4 Testing and Evaluation
In addition to our own laboratory tests, the sys-
tem was evaluated by MITRE as part of the
DARPA program.  There were two parts to the
MITRE evaluations, a "live" part, designed pri-
marily to evaluate the overall task-oriented effec-
tiveness of the systems, and a "canned" part,
designed primarily to evaluate individual compo-
nents of the systems.
The live evaluation consisted of six medical
professionals (doctors, corpsmen and physician?s
assistants from the Naval Medical Center at Quan-
tico, and a nurse from a civilian institution) con-
ducting unrehearsed "focused history and physical
exam" style interactions with Farsi speakers play-
ing the role of patients, where the English-speaking
doctor and the Farsi-speaking patient communi-
cated by means of the Transonics system.  Since
the cases were common enough to be within the
realm of general internal medicine, there was no
attempt to align ailments with medical specializa-
tions among the medical professionals.
MITRE endeavored to find primarily monolin-
gual Farsi speakers to play the role of patient, so as
to provide a true test of the system to enable com-
munication between people who would otherwise
have no way to communicate.  This goal was only
partially realized, since one of the two Farsi patient
role-players was partially competent in English.
2
The Farsi-speaking role-players were trained by a
medical education specialist in how to simulate
symptoms of someone with particular injuries or
illnesses.  Each Farsi-speaking patient role-player
received approximately 30 minutes of training for
any given illness or injury.  The approach was
similar to that used in training standardized pa-
tients, mentioned above (footnote 1) in connection
with generation of the dialogue corpus.
MITRE established a number of their own met-
rics for measuring the success of the systems, as
well as using previously established metrics.  A
full discussion of these metrics and the results ob-
tained for the Transonics system is beyond the
scope of this paper, though we will note that one of
the most important of these was task-completion.
There were 5 significant facts (5 distinct facts for
each of 12 different scenarios) that the medical
professional should have discovered in the process
of interviewing/examining each Farsi patient.  The
USC/HRL system averaged 3 out of the 5 facts,
which was a slightly above-average score among
the 4 systems evaluated.  A "significant fact" con-
sisted of determining a fact which was critical for
diagnosis, such as the fact that the patient had been
injured in a fall down a stairway, the fact that the
patient was experiencing blurred vision, and so on.
Significant facts did not include items such as a
patient's age or marital status.
3
  We report on this
measure in that it is perhaps the single most im-
portant component in the assessment, in our opin-
ion, in that it is an indication of many aspects of
the system, including both directions of the trans-
lation system.  That is, the doctor will very likely
conclude correct findings only if his/her question is
translated correctly to the patient, and also if the
patient's answer is translated correctly for the doc-
tor.  In a true medical exam, the doctor may have
                                                           
2
 There were additional difficulties encountered as well, hav-
ing to do with one of the role-players not adequately grasping
the goal of role-playing.  This experience highlighted the
many challenges inherent in simulating domain-specific
spontaneous dialogue.
3
 Unfortunately, there was no baseline evaluation this could be
compared to,  such as assessing whether any of the critical
facts could be determined without the use of the system at all.
91
other means of determining some critical facts
even in the absence of verbal communication, but
in the role-playing scenario described, this is very
unlikely.  Although this measure is admittedly
coarse-grained, it simultaneously shows, in a crude
sense, that the USC/HRL system compared fa-
vorably against the other 3 systems in the evalua-
tion, and also that there is still significant room for
improvement in the state of the art.
As noted, MITRE devised a component evalua-
tion process also consisting of running 5 scripted
dialogs through the systems and then measuring
ASR and MT performance.  The two primary
component measures were a version of BLEU for
the MT component (modified slightly to handle the
much shorter sentences typical of this kind of dia-
log) and a standard Word-Error Rate for the ASR
output.  These scores are shown below.
Table 1:  Farsi BLEU Scores
IBM BLEU
ASR
IBM BLEU
TEXT
English to Farsi
0.2664 0.3059
Farsi  to English 0.2402 0.2935
The reason for the two different BLEU scores is
that one was calculated based on the ASR compo-
nent output being translated to the other language,
while the other was calculated from human tran-
scribed text being translated to the other language.
Table 2:  HRL/USC WER for Farsi and English
English Farsi
WER 11.5% 13.4%
5 Conclusion
In this paper we have given an overview of the
design, implementation and evaluation of the Tran-
sonics speech-to-speech translation system for nar-
row domain two-way translation.  Although there
are still many significant hurdles to be overcome
before this kind of technology can be called truly
robust, with appropriate training and two coopera-
tive interlocutors, we can now see some degree of
genuine communication being enabled.  And this is
very encouraging indeed.
6 Acknowledgements
This work was supported primarily by the DARPA
CAST/Babylon program, contract N66001-02-C-
6023.
References
R. Belvin, W. May, S. Narayanan, P. Georgiou, S. Gan-
javi.  2004. Creation of a Doctor-Patient Dialogue
Corpus Using Standardized Patients. In Proceedings of
the Language Resources and Evaluation Conference
(LREC), Lisbon, Portugal.
S. Ganjavi, P. G. Georgiou, and S. Narayanan. 2003.
Ascii based transcription schemes for languages with
the Arabic script: The case of Persian. In Proc. IEEE
ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Ganjavi, P. Georgiou, C. Hein, S. Kadambe,
K. Knight, D. Marcu, H. Neely, N. Srinivasamurthy,
D. Traum and D. Wang.  2003. Transonics: A speech
to speech system for English-Persian Interactions,
Proc. IEEE ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Gandhe, S. Ganjavi, P. G. Georgiou, C. M.
Hein, S. Kadambe, K. Knight, D. Marcu, H. E.
Neely, N. Srinivasamurthy, D. Traum, and D. Wang.
2004. The Transonics Spoken Dialogue Translator:
An aid for English-Persian Doctor-Patient interviews,
in Working Notes of the AAAI Fall symposium on
Dialogue Systems for Health Communication, pp 97-
-103.
N. Srinivasamurthy, and S. Narayanan. 2003. Language
adaptive Persian speech recognition. In proceedings
of Eurospeech 2003.
92
A Transcription Scheme for Languages Employing the Arabic Script 
Motivated by Speech Processing Application 
Shadi GANJAVI 
*Department of Linguistics  
University of Southern California 
ganajvi@usc.edu 
Panayiotis G. GEORGIOU,  
Shrikanth NARAYANAN* 
Department of Electrical Engineering 
Speech Analysis & Interpretation 
Laboratory (sail.usc.edu) 
[georgiou, shri]@sipi.usc.edu 
 
Abstract 
This paper offers a transcription system for 
Persian, the target language in the Transonics 
project, a speech-to-speech translation system 
developed as a part of the DARPA Babylon 
program (The DARPA Babylon Program; 
Narayanan, 2003).  In this paper, we discuss 
transcription systems needed for automated 
spoken language processing applications in 
Persian that uses the Arabic script for writing.  
This system can easily be modified for Arabic, 
Dari, Urdu and any other language that uses 
the Arabic script. The proposed system has 
two components. One is a phonemic based 
transcription of sounds for acoustic modelling 
in Automatic Speech Recognizers and for Text 
to Speech synthesizer, using ASCII based 
symbols, rather than International Phonetic 
Alphabet symbols.  The other is a hybrid 
system that provides a minimally-ambiguous 
lexical representation that explicitly includes 
vocalic information; such a representation is 
needed for language modelling, text to speech 
synthesis and machine translation. 
1 Introduction 
Speech-to-speech (S2S) translation systems 
present many challenges, not only due to the 
complex nature of the individual technologies 
involved, but also due to the intricate interaction 
that these technologies have to achieve.  A great 
challenge for the specific S2S translation system 
involving Persian and English would arise from 
not only the linguistics differences between the 
two languages but also from the limited amount of 
data available for Persian.  The other major hurdle 
in achieving a S2S system involving these 
languages is the Persian writing system, which is 
based on the Arabic script, and hence lacks the 
explicit inclusion of vowel sounds, resulting in a 
very large amount of one-to-many mappings from 
transcription to acoustic and semantic 
representations.   
In order to achieve our goal, the system that was 
designed comprised of the following components: 
 
 
 Fig 1. Block diagram of the system. Note that the communication server allows interaction between all 
subsystems and the broadcast of messages. Our vision is that only the doctor will have access to the GUI and 
the patient will only be given a phone handset. 
(1) a visual and control Graphical User Interface 
(GUI); (2) an Automatic Speech Recognition 
(ASR) subsystem, which works both using Fixed 
State Grammars (FSG) and Language Models 
(LM), producing n-best lists/lattices along with the 
decoding confidence scores; (3) a Dialog Manager 
(DM), which receives the output of the speech 
recognition and machine translation units and 
subsequently ?re-scores?? the data according to the 
history of the conversation; (4) a Machine 
Translation (MT) unit, which works in two modes: 
Classifier based MT and a fully Stochastic MT; 
and finally  (5) a unit selection based Text To 
Speech synthesizer (TTS), which provides the 
spoken output.  A functional block diagram is 
shown in Figure 1. 
 
1.1 The Language Under Investigation: 
Persian 
Persian is an Indo-European language with a 
writing system based on the Arabic script.  
Languages that use this script have posed a 
problem for automated language processing such 
as speech recognition and translation systems.  For 
instance, the CSLU Labeling Guide (Lander, 
http://cslu.cse.ogi.edu/corpora/corpPublications.ht
ml) offers orthographic and phonetic transcription 
systems for a wide variety of languages, from 
German to Spanish with a Latin-based writing 
system to languages like Mandarin and Cantonese, 
which use Chinese characters for writing.  
However, there seems to be no standard 
transcription system for languages like Arabic, 
Persian, Dari, Urdu and many others, which use 
the Arabic script (ibid; Kaye, 1876; Kachru, 1987, 
among others).   
Because Persian and Arabic are different, 
Persian has modified the writing system and 
augmented it in order to accommodate the 
differences.  For instance, four letters were added 
to the original system in order to capture the 
sounds available in Persian that Arabic does not 
have.  Also, there are a number of homophonic 
letters in the Persian writing system, i.e., the same 
sound corresponding to different orthographic 
representations.  This problem is unique to Persian, 
since in Arabic different orthographic 
representations represent different sounds.  The 
other problem that is common in all languages 
using the Arabic script is the existance of a large 
number of homographic words, i.e., orthographic 
representations that have a similar form but 
different pronunciation.  This problem arises due 
to limited vowel presentation in this writing 
system.   
Examples of the homophones and homographs 
are represented in Table 1.  The words ?six? and 
?lung? are examples of homographs, where the 
identical (transliterated Arabic) orthographic 
representations (Column 3) correspond to different 
pronunciations [SeS] and [SoS] respectively 
(Column 4). The words ?hundred? and ?dam? are 
examples of homophones, where the two words 
have similar pronunciation [sad] (Column 4), 
despite their different spellings (Column 3).   
 
 Persian USCPers USCPron USCPers+ 
?six?  
 
SS SeS SeS 
?lung?  
 
SS SoS SoS 
?100? 
 
$d sad $ad 
?dam? 
 
sd sad sad 
Table 1 Examples of the transcription methods 
and their limitation.  Purely orthographic 
transcription schemes (such as USCPers) fail to 
distinctly represent homographs while purely 
phonetic ones (such as USCPron) fail to distinctly 
represent the homophones. 
The former is the sample of the cases in which 
there is a many-to-one mapping between 
orthography and pronunciation, a direct result of 
the basic characteristic of the Arabic script, viz., 
little to no representation of the vowels.   
As is evident by the data presented in this table, 
there are two major sources of problems for any 
speech-to-speech machine translation.  In other 
words, to employ a system with a direct 1-1 
mapping between Arabic orthography and a Latin 
based transcription system (what we refer to as 
USCPers in our paper) would be highly ambiguous 
and insufficient to capture distinct words as 
required by our speech-to-speech translation 
system, thus resulting in ambiguity at the text-to-
speech output level, and internal confusion in the 
language modelling and machine translation units.  
The latter, on the other hand, is a representative of 
the cases in which the same sequence of sounds 
would correspond to more than one orthographic 
representation.  Therefore, using a pure phonetic 
transcription, e.g., USCPron, would be acceptable 
for the Automatic Speech Recognizer (ASR), but 
not for the Dialog Manager (DM) or the Machine 
Translator (MT).  The goal of this paper is twofold 
(i) to provide an ASCII based phonemic 
transcription system similar to the one used in the 
International Phonetic Alphabet (IPA), in line of 
Worldbet (Hieronymus, 
http://cslu.cse.ogi.edu/corpora/corpPublications.ht
ml) and (ii) to argue for an ASCII based hybrid 
transcription scheme, which provides an easy way 
to transcribe data in languages that use the Arabic 
script. 
We will proceed in Section 2 to provide the 
USCPron ASCII based phonemic transcription 
system that is similar to the one used by the 
International Phonetic Alphabet (IPA), in line of 
Worldbet (ibid).  In Section 3, we will present the 
USCPers orthographic scheme, which has a one-
to-one mapping to the Arabic script.  In Section 4 
we will present and analyze USCPers+, a hybrid 
system that keeps the orthographic information, 
while providing the vowels.  Section 5 discusses 
some further issues regarding the lack of data.   
2 Phonetic Labels (USCPron) 
One of the requirements of an ASR system is a 
phonetic transcription scheme to represent the 
pronunciation patterns for the acoustic models. 
Persian has a total of 29 sounds in its inventory, six 
vowels (Section 2.1) and 23 consonants (Section 
2.2).  The system that we created to capture these 
sounds is a modified version of the International 
Phonetic Alphabet (IPA), called 
USCPron(unciation).  In USCPron, just like the 
IPA, there is a one-to-one correspondence between 
the sounds and the symbols representing them.  
However, this system, unlike IPA does not require 
special fonts and makes use of ASCII characters.  
The advantage that our system has over other 
systems that use two characters to represent a 
single sound is that following IPA, our system 
avoids all ambiguities. 
2.1 Vowels 
Persian has a six-vowel system, high to low and 
front and back.  These vowels are: [i, e, a, u, o, A], 
as are exemplified by the italicized vowels in the 
following English examples: ?beat?, ?bet?, ?bat?, 
?pull?, ?poll? and ?pot?. The high and mid vowels 
are represented by the IPA symbols. The low front 
vowel is represented as [a], while the low back 
vowel is represented as [A].  There are no 
diphthongs in Persian, nor is there a tense/lax 
distinction among the vowels (Windfuhr, Gernot 
L.1987). 
 
  Front Back 
High i u 
Mid e o 
Low a A 
Table 2: Vowels 
2.2 Consonants 
In addition to the six vowels, there are 23 
distinct consonantal sounds in Persian.  Voicing is 
phonemic in Persian, giving rise to a quite 
symmetric system.  These consonants are 
represented in Table 3 based on the place (bilabial 
(BL), lab-dental (LD), dental (DE), alveopalatal 
(AP), velar (VL), uvular (UV) and glottal (GT)) 
and manner of articulation (stops (ST), fricatives 
(FR), affricates (AF), liquids (LQ), nasals (NS) 
and glides (GL)) and their voicing ([-v(oice)] and 
[+v(oice)]. 
 
 BL LD DE AP VL UV GT 
ST [-v] p  t  k  ? 
 [+v] b  d  g q  
FR [-v]  f s S x  h 
 [+v]  v z Z    
AF [-v]    C    
 [+v]    J    
LQ   l, r     
NS m  n     
GL    y    
Table 3: Consonants 
Many of these sounds are similar to English 
sounds. For instance, the stops, [p, b, t, d, k, g] are 
similar to the italicized letters in the following 
English words: ?potato?, ?ball?, ?tree?, ?doll?, ?key? 
and ?dog? respectively.  The glottal stop [?] can be 
found in some pronunciations of ?button?, and the 
sound in between the two syllables of ?uh oh?.  The 
uvular stop [q] does not have a correspondent in 
English.  Nor does the velar fricative [x].  But the 
rest of the fricatives [f, v, s, z, S, Z, h] have a 
corresponding sound in English, as demonstrated 
by the following examples ?fine?, ?value?, ?sand?, 
?zero?, ?shore?, ?pleasure? and ?hello?.  The 
affricates [C] and [J] are like their English 
counterparts in the following examples: ?church? 
and ?judge?.  The same is true of the nasals [m, n] 
as in ?make? and ?no?; liquids [r, l], as in ?rain? and 
?long? and the glide [y], as in ?yesterday?.  (The 
only distinction between Persian and English is 
that in Persian [t, d, s, z, l, r, n] are dental sounds, 
while in English they are alveolar.)  As is evident, 
whenever possible, the symbols used are those of 
the International Phonetic Alphabet (IPA). 
However, as mentioned before because IPA 
requires special fonts, which are not readily 
available for a few of the sounds, we have used an 
ASCII symbol that resembled the relevant IPA 
symbol.  The only difference between our symbols 
and the ones used by IPA are in voiceless and 
voiced alveopalatal fricatives [S] and [Z], the 
voiceless and voiced affricates [C] and [J], and the 
palatal glide [y].  In the case of the latter, we did 
not want to use the lower case ?j?, in order to 
decrease confusion.   
3 Orthographic Labels (USCPers) 
We proceed in this section to present an 
alternative orthographic system for Persian, as a 
first step in the creation of the USCPers+ system 
that will be presented later. The Persian writing 
system is a consonantal system with 32 letters in 
its alphabet (Windfuhr, 1987).  All but four of 
these letters are direct borrowing from the Arabic 
writing system.  It is important to note that this 
borrowing was not a total borrowing, i.e., many 
letters were borrowed without their corresponding 
sound.  This has resulted in having many letters 
with the same sound (homophones).  However, 
before discussing these cases, let us consider the 
cases in which there is no homophony, i.e., the 
cases in which a single letter of the alphabet is 
represented by a single sound. 
In order to assign a symbol to each letter of the 
alphabet, the corresponding letter representing the 
sound of that letter was chosen.  So, for instance 
for the letter ?   ?, which is represented as [p] in 
USCPron, the letter ?p? was used in USCPers(ian).   
These letters are: 
 
ST FR AF LQ NS 

   p     f    C     r     m 

   b     S     J 	    l 
    n 

   d     Z    

   k     x    

   g 
 
 
 
 

   ?     
Table 4: USCPers(ian) Symbols:  
Non-Homophonic Consonants 
As mentioned above, this partial borrowing of the 
Arabic writing system has given rise to many 
homophonic letters.  In fact, thirteen letters of the 
alphabet are represented by only five sounds.  
These sounds and the corresponding letters are 
presented below:   
 
? [t] for ?  ? and ? ?;  
? [q] for ?  ? and ?  ?;  
? [h] for ?  ? and ?  ?;  
? [s] for ?  ?, ?  ? and ?  ? and 
? [z] for ? Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 382?389,
Sydney, July 2006. c?2006 Association for Computational Linguistics
382
383
384
385
386
387
388
389
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 1?4
Manchester, August 2008
Mitigation of data sparsity in classifier-based translation
Emil Ettelaie, Panayiotis G. Georgiou, Shrikanth S. Narayanan
Signal Analysis and Interpretation Laboratory
Ming Hsieh Department of Electrical Engineering
Viterbi School of Engineering
University of Southern California
ettelaie@usc.edu
Abstract
The concept classifier has been used as a
translation unit in speech-to-speech trans-
lation systems. However, the sparsity of
the training data is the bottle neck of its
effectiveness. Here, a new method based
on using a statistical machine translation
system has been introduced to mitigate the
effects of data sparsity for training classi-
fiers. Also, the effects of the background
model which is necessary to compensate
the above problem, is investigated. Exper-
imental evaluation in the context of cross-
lingual doctor-patient interaction applica-
tion show the superiority of the proposed
method.
1 Introduction
Statistical machine translation (SMT) methods
are well established in speech-to-speech transla-
tion systems as the main translation technique
(Narayanan et al, 2003; Hsiao et al, 2006). Due
to their flexibility these methods provide a good
coverage of the dialog domain. The fluency of
the translation, however, is not guaranteed. Dis-
fluencies of spoken utterances plus the speech rec-
ognizer errors degrade the translation quality even
more. All these ultimately affect the quality of the
synthesized speech output in the target language,
and the effectiveness of the concept transfer.
It is quite common, though, to use other means of
translation in parallel to the SMT methods (Gao et
al., 2006; Stallard et al, 2006). Concept classifica-
tion, as an alternative translation method, has been
successfully integrated in speech-to-speech transla-
tors (Narayanan et al, 2003; Ehsani et al, 2006).
A well defined dialog domain, e.g. doctor-patient
dialog, can be partly covered by a number of con-
cept classes. Upon a successful classification of
the input utterance, the translation task reduces to
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
synthesizing a previously created translation of the
concept, as a mere look up. Since the main goal in
such applications is an accurate exchange of con-
cepts, this method would serve the purpose as long
as the input utterance falls within the coverage of
the classifier. This process can be viewed as a quan-
tization of a continuous ?semantic? sub-space. The
classifier is adequate when the quantization error is
small (i.e. the derived concept and input utterance
are good matches), and when the utterance falls in
the same sub-space (domain) as the quantizer at-
tempts to cover. Since it is not feasible to accu-
rately cover the whole dialog domain (since a large
number of quantization levels needed) the classi-
fier should be accompanied by a translation system
with a much wider range such as an SMT engine.
A rejection mechanism can help identify the cases
that the input utterance falls outside the classifier
coverage (Ettelaie et al, 2006).
In spite of this short coming, the classifier-
based translator is an attractive option for speech-
to-speech applications because of its tolerance to
?noisy? input and the fluency of its output, when it
operates close to its design parameters. In practice
this is attainable for structured dialog interactions
with high levels of predictability. In addition, it can
provide the users with both an accurate feedback
and different translation options to choose from.
The latter feature, specially, is useful for applica-
tions like doctor-patient dialog.
Building a concept classifier starts with identify-
ing the desired concepts and representing them with
canonical utterances that express these concepts. A
good set of concepts should consist of the ones that
are more frequent in a typical interaction in the do-
main. For instance in a doctor-patient dialog, the
utterance ?Where does it hurt?? is quite common
and therefore its concept is a good choice. Phrase
books, websites, and experts? judgment are some of
the resources that can be used for concept selection.
Other frequently used concepts include those that
correspond to basic communicative and social as-
pects of the interaction such as greeting, acknowl-
edgment and confirmation.
After forming the concept space, for each class,
1
utterances that convey its concept must be gath-
ered. Hence, this training corpus would consist of
a group of paraphrases for each class. This form of
data are often very difficult to collect as the number
of classes grow. Therefore, the available training
data are usually sparse and cannot produce a classi-
fication accuracy to the degree possible. Since the
classifier range is limited, high accuracy within that
range is quite crucial for its effectiveness. One of
the main issues is dealing with data sparsity. Other
techniques have also been proposed to improve the
classification rates. For example in (Ettelaie et al,
2006) the accuracy has been improved by introduc-
ing a dialog model. Also, a background model has
been used to improve the discrimination ability of a
given concept class model.
In this work a novel method for handling the
sparsity is introduced. This method utilizes an SMT
engine to map a single utterance to a group of them.
Furthermore, the effect of the background model on
classification accuracy is investigated.
Section 2 reviews the concept classification pro-
cess and the background model. In Section 3 the
sparsity handling method using an SMT is intro-
duced. Data and experiments are described in Sec-
tion 4. The results are discussed in Section 5.
2 Concept classifier and background
model
The concept classifier based on the maximum like-
lihood criterion can be implemented as a language
model (LM) scoring process. For each class a lan-
guage model is built using data expressing the class
concept. The classifier scores the input utterance
using the class LM?s and selects the class with high-
est score. In another word if C is the set of concept
classes and e is the input utterance, the classifica-
tion process is,
c? = argmax
c?C
{P
c
(e | c)} (1)
where P
c
(e | c) is the score of e from the LM of
class c. The translation job is concluded by playing
out a previously constructed prompt that expresses
the concept c? in the target language.
It is clear that a class with limited training data
items will have an undertrained associated LM with
poor coverage. In practice such a model fails to pro-
duce a usable LM score and leads to a poor classifi-
cation accuracy. Interpolating the LM with a back-
ground language model results in a smoother model
(Stolcke, 2002) and increases the overall accuracy
of the classifier.
The background model should be built from a
larger corpus that fairly covers the domain vocab-
ulary. The interpolation level can be optimized for
the best performance based on heldout set.
3 Handling sparsity by statistical
machine translation
The goal is to employ techniques that limit the ef-
fects of data sparsity. What is proposed here is to
generate multiple utterances ? possibly with lower
quality ? from a single original one. One approach
is to use an SMT to generate n-best lists of trans-
lation candidates for the original utterances. Such
lists are ranked based on a combination of scores
from different models (Ney et al, 2000). The hy-
pothesis here is that for an SMT trained on a large
corpus, the quality of the candidates would not de-
grade rapidly as one moves down the n-best list.
Therefore a list with an appropriate length would
consist of translations with acceptable quality with-
out containing a lot of poor candidates. This pro-
cess would result in more data, available for train-
ing, at the cost of using noisier data.
Although the source language of the SMT must
be the same as the classifier?s, its target language
can be selected deliberately. It is clear that a lan-
guage with large available resources (in the form of
parallel corpora with the source language) must be
selected. For simplicity this language is called the
?intermediate language? here.
A classifier in the intermediate language can be
built by first generating an n-best list for every
source utterance in the classifier?s training corpus.
Then the n-best lists associated with each class are
combined to form a new training set. The class
LM?s are now built from these training sets rather
than the original sets of the source utterances.
To classify a source utterance e, first the SMT
is deployed to generate an n-best list (in the inter-
mediate language) from it. The list will consist of
candidates f
1
, f
2
,..., f
n
. The classification process
can be reformulated as,
c? = argmax
c?C
{
n
?
i=1
?
P
c
(f
i
| c)
}
(2)
Here,
?
P
c
(f
i
| c) is the score of the i
th
candidate f
i
from the LM of class c. The scores are considered
in the probability domain.
The new class LM?s can also be smoothed by in-
terpolation with a background model in the inter-
mediate language.
4 Data and Experiments
4.1 Data
The data used in this work were originally collected
for, and used in, the Transonics project (Narayanan
et al, 2003) to develop an English/Farsi speech-to-
speech translator in the doctor-patient interaction
domain. For the doctor side, 1,269 concept classes
were carefully chosen using experts? judgment and
medical phrase books. Then, for each concept, En-
glish data were collected from a website, a web-
based game, and multiple paraphrasing sessions at
the Information Sciences Institute of the University
2
Conventional n-best length
(baseline) 100 500 1,000 2,000
Accuracy [%]
74.9 77.4 77.5 76.8 76.4
Relative error
reduction [%]
0.0 10.0 10.4 7.6 6.0
Accuracy in
4-best [%]
88.6 90.7 91.0 91.3 90.5
Relative error
reduction [%]
0.0 18.4 21.1 23.7 16.7
Table 1: Classification accuracy for the conventional method
and the proposed method with different lengths of n-best list
of Southern California. The total size of the data
set consists of 9,893 English phrases.
As the test corpus for this work, 1,000 phrases
were randomly drawn from the above set and the
rest were used for training. To make sure that the
training set covered every class, one phrase per
class was excluded from the test set selection pro-
cess.
To generate the n-best lists, a phrase based SMT
(Koehn et al, 2003) was used. The intermedi-
ate language was Farsi and the SMT was trained
on a parallel English/Farsi corpus with 148K lines
(1.2M words) on the English side. This corpus
was also used to build the classification background
models in both languages. The SMT was opti-
mized using a parallel development set with 915
lines (7.3K words) on the English side.
4.2 Classification Accuracy Measures
Classifier accuracy is often used as the the qual-
ity indicator of the classification task. However, it
is common in the speech-to-speech translation sys-
tems to provide the user with a short list of potential
translations to choose from. For example the user
of system in (Narayanan et al, 2003) is provided
with the top four classifier outputs. In such cases, it
is practically useful to measure the accuracy of the
classifier within its n-best outputs (e.g., n = 4 for
the above system). In this work the classification
accuracy was measured on both the single output
and the 4-best outputs.
4.3 Experiments
To compare the proposed method with the con-
ventional classification, a classifier based on each
method was put to test. In the proposed method,
it is expected that the accuracy is affected by the
length of the n-best lists. To observe that, n-best
lists of lengths 100, 500, 1000, and 2000 were used
in the experiments. The results are shown in Table
1. In all of the above experiments the background
interpolation factor was set to 0.9 which is close
to the optimum value obtained in (Ettelaie et al,
2006).
To examine the effect of the background model,
the conventional and proposed methods were tried
with different values of the interpolation factor ?
(the background model is weighted by 1 ? ?). For
the conventional method the length of the n-best
list was set to 500. Figure 1 shows the accuracy
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.045%
50%55%
60%65%
70%75%
80%85%
90%95%
Conv. 4-bestConv.New 4-bestNew
Background Interpolation Factor
Accu
racy
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00%
5%
Background Interpolation Factor (?)
Accu
racy
Figure 1: The effect of background model on classification
accuracy
changes with respect to the interpolation factor for
these two methods.
5 Discussion
Table 1 shows the advantage of the proposed
method over the conventional classification with a
relative error rate reduction up to 10.4% (achieved
when the length of the SMT n-best list was 500).
However, as expected, this number decreases with
longer SMT n-best lists due to the increased noise
present in lower ranked outputs of the SMT.
Table 1 also shows the accuracy within 4-best
classifier outputs for each method. In that case
the proposed method showed an error rate which
was relatively 23.7% lower than the error rate of
the conventional method. That was achieved at the
peak of the accuracy within 4-best, when the length
of the SMT n-best list was 1,000. In this case too,
further increase in the length of the n-best list led
to an accuracy degradation as the classifier models
became noisier.
The effect of the background model on classifier
accuracy is shown in Figure 1. The figure shows
the one-best accuracy and the accuracy within 4-
best outputs, versus the background interpolation
factor (?) for both conventional and proposed meth-
ods. As the curves indicate, with ? equal to zero the
classifier has no discriminating feature since all the
class scores are driven solely from the background
model. However, a slight increase in ?, leads to
a large jump in the accuracy. The reason is that
the background model was built from a large gen-
eral domain corpus and hence, had no bias toward
any of the classes. With a small ?, the score from
the background model dominates the overall class
scores. In spite of that, the score differences caused
by the class LM?s are notable in improving the clas-
sifier performance.
As ? increases the role of the class LM?s be-
comes more prominent. This makes the classifier
models more discriminative and increases its accu-
racy as shown in Figure 1. When the factor is in
the close vicinity of one, the smoothing effect of
the background model diminishes and leaves the
3
classes with spiky models with very low vocabu-
lary coverage (lots of zeros). This leads to a rapid
drop in accuracy as ? reaches one.
Both the conventional and proposed methods
follow the above trend as Figure 1 shows, al-
though, the proposed method maintains its supe-
riority throughout the range of ? that was exam-
ined. The maximum measured accuracies for con-
ventional and proposed methods were 75.2% and
78.7% respectively and was measured at ? = 0.999
for both methods. Therefore, the error rate of the
proposed method was relatively 14.1% lower than
its counterpart from the conventional method.
Figure 1 also indicates that when the accuracy is
measured within the 4-best outputs, again the pro-
posed method outperforms the conventional one.
The maximum 4-best accuracy for the conventional
method was measured at the sample point ? = 0.9
and was equal to 88.6%. For the proposed method,
that number was measured as 91.5% achieved at the
sample point ? = 0.999. In another words, consid-
ering the 4-best classifier outputs, the error rate of
the proposed method was relatively 25.4% lower.
6 Conclusion
The proposed language model based method can be
used to improve the accuracy of the concept classi-
fiers specially in the case of sparse training data.
It outperformed the conventional classifier, trained
on the original source language paraphrases, in the
experiments. With this method, when the input ut-
terance is within the classification domain, the clas-
sifier can be viewed as a filter that produces fluent
translations (removes the ?noise?) from the SMT
output.
The experiments also emphasized the impor-
tance of the background model, although indicated
that the classification accuracy was not very sen-
sitive to the value of the background interpolation
factor. This relieves the developers from the fine
tuning of that factor and eliminates the need for a
development data set when a suboptimal solution is
acceptable.
We believe that significant improvements to the
technique can be made through the use of weighted
n-best lists based on the SMT scores. In addition
we believe that using a much richer SMT engine
could provide significant gains through increased
diversity in the output vocabulary. We intend to ex-
tend on this work through the use of enriched, mul-
tilingual SMT engines, and the creation of multiple
classifiers (in several intermediate languages).
7 Acknowledgment
This work was supported in part by funds from
DARPA.
References
Ehsani, F., J. Kinzey, D. Master, K. Sudre, D. Domingo,
and H. Park. 2006. S-MINDS 2-way speech-to-
speech translation system. In Proc. of the Medi-
cal Speech Translation Workshop, Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy (NAACL-HLT), pages 44?45, New York, NY,
USA, June.
Ettelaie, E., P. G. Georgiou, and S. Narayanan. 2006.
Cross-lingual dialog model for speech to speech
translation. In Proc. of the Ninth International Con-
ference on Spoken Language Processing (ICLSP),
pages 1173?1176, Pittsburgh, PA, USA, September.
Gao, Y., L. Gu, B. Zhou, R. Sarikaya, M. Afify, H. Kuo,
W. Zhu, Y. Deng, C. Prosser, W. Zhang, and L. Be-
sacier. 2006. IBM MASTOR SYSTEM: Multilin-
gual automatic speech-to-speech translator. In Proc.
of the Medical Speech Translation Workshop, Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology (NAACL-HLT), pages 53?56,
New York, NY, USA, June.
Hsiao, R., A. Venugopal, T. Kohler, Y. Zhang,
P. Charoenpornsawat, A. Zollmann, S. Vogel, A. W.
Black, T. Schultz, and A. Waibel. 2006. Optimiz-
ing components for handheld two-way speech trans-
lation for an English-Iraqi Arabic system. In Proc. of
the Ninth International Conference on Spoken Lan-
guage Processing (ICLSP), pages 765?768, Pitts-
burgh, PA, USA, September.
Koehn, P., F. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of the Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology (NAACL-HLT), volume 1, pages 48?54,
Edmonton, AB, Canada, May-June.
Narayanan, S., S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Ganjavi, P. Georgiou, C. Hein, S. Kadambe,
K. Knight, D. Marcu, H. Neely, N. Srinivasamurthy,
D. Traum, and D. Wang. 2003. Transonics: A
speech to speech system for English-Persian inter-
actions. In Proc. of IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU),
pages 670?675, St.Thomas, U.S. Virgin Islands,
November-Decmeber.
Ney, H., S. Nie?en, F. J. Och, C. Tillmann, H. Sawaf,
and S. Vogel. 2000. Algorithms for statistical trans-
lation of spoken language. IEEE Trans. on Speech
and Audio Processing, Special Issue on Language
Modeling and Dialogue Systems, 8(1):24?36, Jan-
uary.
Stallard, D., F. Choi, K. Krstovski, P. Natarajan,
R. Prasad, and S. Saleem. 2006. A hybrid
phrase-based/statistical speech translation system.
In Proc. of the Ninth International Conference on
Spoken Language Processing (ICLSP), pages 757?
760, Pittsburgh, PA, USA, September.
Stolcke, A. 2002. SRILM - an extensible language
modeling toolkit. In Proc. of the International Con-
ference on Spoken Language Processing (ICSLP),
pages 901?904, Denver, CO, USA, September.
4
Proceedings of the SIGDIAL 2013 Conference, pages 394?403,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Which ASR should I choose for my dialogue system?
Fabrizio Morbini, Kartik Audhkhasi, Kenji Sagae, Ron Artstein,
Dog?an Can, Panayiotis Georgiou, Shri Narayanan, Anton Leuski and David Traum
University of Southern California
Los Angeles, California, USA
{morbini,sagae,artstein,leuski,traum}@ict.usc.edu
{audhkhas,dogancan}@usc.edu {georgiou,shri}@sipi.usc.edu
Abstract
We present an analysis of several pub-
licly available automatic speech recogniz-
ers (ASRs) in terms of their suitability for
use in different types of dialogue systems.
We focus in particular on cloud based
ASRs that recently have become available
to the community. We include features
of ASR systems and desiderata and re-
quirements for different dialogue systems,
taking into account the dialogue genre,
type of user, and other features. We then
present speech recognition results for six
different dialogue systems. The most in-
teresting result is that different ASR sys-
tems perform best on the data sets. We
also show that there is an improvement
over a previous generation of recognizers
on some of these data sets. We also inves-
tigate language understanding (NLU) on
the ASR output, and explore the relation-
ship between ASR and NLU performance.
1 Introduction
Dialogue system developers who are not also
speech recognition experts are in a better posi-
tion than ever before in terms of the ease of in-
tegrating existing speech recognizers in their sys-
tems. While there have been commercial solutions
and toolkits for a number of years, there were a
number of problems in getting these systems to
work. For example, early toolkits relied on spe-
cific machine hardware, software, and firmware
to function properly, often had a difficult instal-
lation process, and moreover often didn?t work
well for complex dialogue domains, or challeng-
ing acoustic environments. Fortunately the situ-
ation has greatly improved in recent years. Now
there are a number of easy to use solutions, in-
cluding open-source systems (like PocketSphinx),
as well as cloud-based approaches.
While this increased choice of quality recogniz-
ers is of great benefit to dialogue system develop-
ers, it also creates a dilemma ? which recognizer
to use? Unfortunately, the answer is not simple ?
it depends on a number of issues, including the
type of dialogue domain, availability and amount
of training data, availability of internet connectiv-
ity for the runtime system, and speed of response
needed. In this paper we assess several freely
available speech recognition engines, and exam-
ine their suitability and performance in several di-
alogue systems. Here we extend the work done in
Yao et al (2010) focusing in particular on cloud
based freely available ASR systems. We include
2 local ASRs for reference, one of which was also
used in the earlier work for easy comparison.
2 Speech Recognizer Features and
Engines
The following are some of the major criteria for
selection of a speech recognizer.
Customization Some of the available speech
recognizers allow the users to tune the recognizer
to the environment it will operate in, by providing
a specialized lexicon, trained language models or
acoustic models. Customization is especially im-
portant for dialogue systems whose input contains
specialized vocabulary (see section 4).
Output options A basic recognizer will output
a string of text, representing its best hypothesis
about the transcription of the speech input. Some
recognizers offer additional outputs which are use-
ful for dialogue systems: ranked n-best hypothe-
ses allow later processing to use context for dis-
ambiguation, and incremental results allow the
system to react while the user is still speaking.
Performance characteristics Dialogue systems
differ in their requirements for response speed; a
394
System Customization Output options Open Source PerformanceN-best Incremental Speed Installation
Pocketsphinx Full Yes Yes Yes realtime Local
Apple No Noa No No network Cloud
Google No Yes Yesb No network Cloud
AT&T Partialc Yes No No network Cloud
Otosense-Kaldi Full Yes No Yesd variablee Local
aSingle output annotated with alternative hypotheses. bOnly for web-delivered applications in a Google Chrome browser.
cCustom language models. dRelease scheduled for Fall 2013. eUser controls trade-off between speed and output quality.
Table 1: Speech recognizer features important for use in dialogue systems
speech recognizer that runs locally can help by
avoiding network latencies.
Output quality Typically, a dialogue system
would want the best recognition accuracy pos-
sible given the constraints. Ultimately, dialogue
systems want the output that would yield the best
performance for Natural Language Understand-
ing and other downstream processes. As a rule,
better speech recognition leads to better language
understanding, though this is not necessarily the
case for specific applications (see section 5).
We evaluated 5 freely available speech recog-
nizers. Their features are summarized in Table 1.
We did not include the MIT WAMI toolkit1 as we
are focused on speech services that can directly
be used by stand alone applications as opposed to
web delivered ones. We did not include commer-
cial recognizers such as Nuance, because licensing
terms can be difficult for research institutions, and
in particular, disallow publishing benchmarks.
Pocketsphinx is a version of the CMU Sphinx
ASR system optimized to run also on embedded
systems (Huggins-Daines et al, 2006). Pocket-
sphinx is fast, runs locally, and requires relatively
modest computational resources. It provides n-
best lists and lattices, and supports incremental
output. It also provides a voice activity detec-
tion functionality for continuous ASR. This ASR
is fully customizable and trainable, but users are
expected to provide language models suitable for
their applications. A few acoustic models are pro-
vided, and can be adapted using the CMUSphinx
tools.2
1http://wami.csail.mit.edu/
2http://cmusphinx.sourceforge.net/wiki/tutorialadapt
Apple Dictation is the OS level feature in both
MacOSX and iOS.3 It is integrated into the text in-
put system pipeline so a user can replace her key-
board with a microphone for entering text in any
application. Dictation is often associated with the
Siri personal assistant feature of iOS. While it is
likely that Dictation and Siri share the same ASR
technology, Dictation only does speech recogni-
tion. Apple states that Dictation learns the charac-
teristics of the user?s voice and adapts to her accent
(Apple Inc, 2012). Dictation requires an internet
connection to send recorded user speech to Ap-
ple?s servers and receive ASR results. Processing
starts as soon as the user starts speaking so the de-
lay of getting the recognition results after the user
finishes speaking is minimal.
To integrate Dictation into a dialogue system,
a system designer needs to include any system de-
fined text input control into her application and use
the control APIs to observe text changes. The user
would need to press a key when starting to speak
and push the key again once she is done speak-
ing. The ASR result is a text string annotated with
alternative interpretations of individual words or
phrases in the text. There is an API for extract-
ing those interpretations from the result. While the
Dictation feature is reasonably fast and easy to in-
tegrate, dialogue system developers have no con-
trol over the ASR process, which must be treated
as a black box. Apple dictation is limited in that
no customization is possible, no partial recogni-
tion results are provided, and there is an unspeci-
fied limit on the number of utterances dictated for
a period of time, which is not a problem for inter-
action between a single user and a dialogue sys-
tem, but may be an issue in dialogue systems that
support multiple concurrent users.
3Dictation was introduced in iOS 5.0 and MacOSX 10.8.
395
Google Speech API provides support for the
HTML 5 speech input feature.4 It is a cloud based
service in which a user submits audio data using
an HTML POST request and receives as reply the
ASR output in the form of an n-best list. The au-
dio data is limited to roughly 10 seconds in length,
longer clips are rejected and return no ASR results.
The user can (1) customize the number of hy-
potheses returned by the ASR, (2) specify which
language the audio file contains and (3) enable a
filter to remove profanities from the output text.
As is the case with Apple Dictation, ASR must be
treated as a black box, and no task customization
is possible for dialogue system developers. Users
cannot specify or provide custom language models
or acoustic models. The service returns only the fi-
nal hypothesis, there is no incremental output.5 In
addition, results for the same inputs may change
unpredictably, since Google may update or other-
wise change its service and models, and models
may be adapted using specific audio data supplied
by users. In our experiments, we observed accu-
racy improvements when submitting the same au-
dio files over repeated trials over two weeks.
AT&T Watson is the ASR engine available
through the AT&T Speech Mashup service.6 It is
a cloud based service that can be accessed through
HTML POST requests, like the Google Speech
API. AT&T Watson is designed to support the de-
mands of online spoken dialogue systems, and can
be customized with data specific to a dialogue sys-
tem. Additionally, in our tests we did not observe
any limitation in the maximum length of the in-
put audio data. However, AT&T does not provide
a default general-purpose language model, and
application-specific models must be built within
the Speech Mashup service using user-provided
text data. The acoustic model must be selected
from a list provided by the AT&T service, and
acoustic models can be further customized within
the Speech Mashup service. The ASR returns an
n-best list of hypotheses but does not provide in-
cremental output.
Otosense-Kaldi Another ASR we employed
was the Kaldi-based OtoSense-Kaldi engine de-
4https://www.google.com/speech-api/v1/recognize
5The demo page shows continuous speech understanding
with incremental results but requires Google Chrome to run
and is specific to web delivered applications:
http://www.google.com/intl/en/chrome/demos/speech.html
6https://service.research.att.com/smm
veloped at SAIL.7 OtoSense-Kaldi8 is an on-line,
multi-threaded architecture based on the Kaldi
toolkit (Povey et al, 2011) that allows for dynam-
ically configurable and distributed ASR.
3 Dialogue Systems, Users, and Data
All spoken dialogue systems are similar in some
respects, in that there is speech by a user (or users)
that needs to be recognized, and this speech is
punctuated by speech from the system. More-
over, the speech is not fully independent, but ut-
terances are connected to other utterances, e.g. an-
swers to questions, or clarifications. There are,
however many ways in which systems can differ,
that have implications for which speech recogniz-
ers are most appropriate. Some of the dimensions
to consider are:
Type of microphone(s) One of the biggest im-
pacts on ASR is the acoustic environment. Will
the audio be clean, coming from a close-talking
head or lapel-mounted microphone, or will it need
to be picked up from a broader directional micro-
phone or microphone array?
Number of speakers/microphones Will there
be one designated microphone per person, or will
speaker identification need to be performed? Will
audio from the system confuse the ASR?
Push to talk or continuous speech Will the
user clearly identify the start and end of speech,
or will the system need to detect speech acousti-
cally?
Type of Users Will there be designated long-
term users, where user-training or system model
adaptation is feasible, or will there be many un-
known users, where training is not feasible? See
also section 3.1 for more on user types.
Genre What kinds of things will people be say-
ing to the system? Is it mostly commands or short
answers to questions, or more open-ended conver-
sation? See section 3.2 for more on genre issues.
Training Data Is within-domain training data
available, and if so how much?
3.1 Types of Users
The type of user is important for the overall
design of the system and has implications for
7http://sail.usc.edu
8OtoSense-Kaldi will be released (BSD license) in 2013.
396
ASR performance as well. One important as-
pect is the broad physical differences among
speakers, such as male vs female, adult vs child
(e.g. Bell and Gustafson, 2003), or language pro-
ficiency/accent, that will have implications for the
acoustics of what is said, and ASR results. Other
aspects of users have implications for what will
be said, and how successful the interface may
be, overall. Many (e.g. Hassel and Hagen, 2006;
Jokinen and Kanto, 2004) have looked at the dif-
ferences between novice and expert users. Ai et
al. (2007a) also points out a difference between
real users and recruited subjects. Real users also
come in many different flavors, depending on their
purposes. E.g. are they interacting with the system
for fun, to do a specific task that they need to get
done, to learn something (specific or general), or
with some other purpose in mind?
We considered the following classes of users,
ordered from easiest to hardest to get to acceptable
performance and robustness levels:
Demonstrators are generally the easiest for a sys-
tem to understand ? a demonstrator is trained in
use of the system, knows what can and can?t be
said, is motivated toward success, and is gener-
ally interested in showing off the most impres-
sive/successful aspects of the system to an audi-
ence rather than using it for its own sake.
Trained/Expert Users are similar to demonstra-
tors, but use the system to achieve specific results
rather than just to show off its capabilities. This
means that users may be forced down lines that
are not ideal for the system, if these are necessary
to accomplish the task.
Motivated Users do not have the training of ex-
pert users, and may say many things that the sys-
tem can not handle as opposed to equivalent ex-
pressions that could be handled. However moti-
vated users do want the system to succeed, and in
general are willing to do whatever they think is
necessary to improve system performance. Unlike
expert users, motivated users might be incorrect
about what will help the system (e.g. hyperarticu-
lation in response to system misunderstanding).
Casual Users are interested in finding out what
the system can do, but do not have particular moti-
vations to help or hinder the system. Casual Users
may also leave in the middle of an interaction, if it
is not engaging enough.
Red Teams are out to test or ?break? the system,
or show it as not-competent, and may try to do
things the system can?t understand or react well
to, even when an alternative formulation is known
to work.
3.2 Types of Dialogue System Genres
Dialogue Genres can be distinguished along many
lines, e.g. the number and relationship of partic-
ipants, specific conversational rules, purposes of
the participants, etc. We distinguish here four gen-
res of dialogue system that have been in use at
the Institute for Creative Technologies and that we
have available corpora for (there are many other
types of dialogue genres, including tutoring, ca-
sual conversation, interviewing,. . . ). Each genre
has implications for the internal representations
and system architectures needed to engage in that
genre of dialogue.
Simple Question-answering This genre in-
volves strong user-initiative and weak global di-
alogue coherence. The user can ask any ques-
tion to the system at any time, and the system
should respond, with an appropriate answer if
able, or with some other reply indicating either
inability or unwillingness to provide the answer.
This genre allows modeling dialogue at a surface-
text level (Gandhe, 2013), without internal se-
mantic representations of the input, and where
the result of ?understanding? input is the system?s
expected output. The NCPEditor9 (Leuski and
Traum, 2011) is a toolkit that provides an author-
ing environment, classification, and dialogue ca-
pability for simple question-answering characters.
The SGT Blackwell, SGT Star, and Twins systems
described below are all systems in this genre.
Advanced Question-answering This genre is
similar to the simple question-answering charac-
ters, in that the main task of the user is to elicit
information from the system character. The differ-
ence is that there is more long-range and interme-
diate dialogue coherence, in that questions can be
answered several utterances after they have been
asked, there can be intervening sub-dialogues, and
characters sometimes take the initiative to pursue
their own goals rather than just responding to the
user. Because of the requirements for somewhat
deeper understanding, and relation of input to con-
9Available free for academic research purposes from
https://confluence.ict.usc.edu/display/VHTK/Home
397
text and character goals and policies, there is a
need of at least a shallow semantic representa-
tion and representation of the dialogue informa-
tion state, and the character must distinguish un-
derstanding of the input from the character out-
put (since the latter will depend on the dialogue
policy and information state, not just the under-
standing of input). The tactical questioning archi-
tecture (Gandhe et al, 2009)10 provides author-
ing and run-time support for advanced question-
answering characters, and has been used to build
over a dozen characters for purposes such as train-
ing tactical questioning, training culture, and psy-
chology experiments (Gandhe et al, 2011). The
Amani character described below is in this genre.
Slot-filling Probably the most common type of
dialogue system (at least in the research commu-
nity) is slot-filling. Here the dialogue is fairly
structured, with an initial greeting phase, then one
or more tasks, which all start with the user se-
lecting the task, and the system taking over ini-
tiative to ?fill? and possibly confirm the needed
slots, before retrieving some information from a
database, or performing a simple service.11 This
genre also requires a semantic representation, at
least of the slots and acceptable values. Gener-
ally, the set of possible values is large enough, that
some form of NLG is needed (at least template
filling), rather than authoring of all full sentences.
There are a number of toolkits and development
frameworks that are well suited to slot-filling sys-
tems, e.g. Ravenclaw (Bohus and Rudnicky, 2003)
or Trindikit (Larsson and Traum, 2000). The Ra-
diobots system, described below is in this genre.
Negotiation and Planning In this genre, the
system is more of an equal partner with the user,
than a servant, as in the slot-filling systems. The
system must not merely understand user requests,
but must also evaluate whether they meet the sys-
tem goals, what the consequences and precondi-
tions of requests are, and whether there are better
alternatives. For this kind of inference, a more de-
tailed semantic representation is required than just
filling in slots. While we are not aware of publicly
available software that makes this kind of system
easy to construct, there have been several built us-
ing an information-state approach, or the soar cog-
10Soon to be released as part of the virtual human toolkit.
11Mixed-initiative versions of this genre exist, where the
user can also provide unsolicited information, which reduces
the number of system queries needed.
nitive architecture. The TRIPS system (Allen et
al., 2001) also has many similarities.
3.3 ICT Dialogue Systems Tested
We tested the recognizers described in section 2
on data sets collected from six different dialogue
domains. Five are the same ones tested in Yao et
al. (2010), to which we added the Twins set. De-
tails on the size of the training and development
sets may be found in Yao et al (2010), here we
report only the numbers relevant to the Twins do-
main and to the NLU analysis, which are not in
Yao et al (2010).
SGT Blackwell was created as a virtual human
technology demonstration for the 2004 Army Sci-
ence Conference. This is a question-answering
character, with no internal semantic representation
and the primary NLU task merged with Dialogue
management as selecting the best response.
The original users were ICT demonstrators.
However, there were also some experiments with
recruited participants (Leuski et al, 2006a; Leuski
et al, 2006b). Later SGT Blackwell became a part
of the ?best design in America? triennial at the
Cooper-Hewitt Museum in New York City, and
the data set here is from visitors to the museum,
who are mostly casual users, but range from expert
to red-team. Users spoke into a mounted direc-
tional microphone (see Robinson et al, 2008 for
more details).
SGT STAR (Artstein et al, 2009a) is a question-
answering character similar to SGT Blackwell, al-
though designed to talk about Army careers rather
than general knowledge. The users are Army per-
sonnel who went to job fairs and visited schools in
the mobile Army adventure vans, speaking using
headset microphones, and performing for an audi-
ence. The users are somewhere between demon-
strators and expert users. They are speaking to
SGT STAR for the benefit of an audience, but their
primary purpose is to convey information to the
audience in a memorable way (through dialogue
with SGT STAR) rather than to show off the high-
lights of the character.
The Twins are two life-size virtual characters
who serve as guides at the Museum of Science
in Boston (Swartout et al, 2010). The charac-
ters promote interest in Science, Technology, En-
gineering and Mathematics (STEM) in children
between the ages of 7 and 14. They are question-
398
answering characters, but unlike SGTs Blackwell
and Star, the response is a whole dialogue se-
quence, potentially involving interchange from
both characters, rather than a single character turn.
There are two types of users for the Twins:
demonstrators, who are museum staff members,
using head-mounted microphones, and museum
visitors, who use a Shure 522 table-top mounted
microphone (Traum et al, 2012). More on analy-
sis of the museum data can be found in (Aggarwal
et al, 2012). We also investigated speech recog-
nition and NLU performance in this domain in
Morbini et al (2012).
This dataset contains 14K audio files each an-
notated with one of the 168 possible response se-
quences. The division in training development and
test is the same used in Morbini et al (2012) (10K
for training, the rest equally divided between de-
velopment and test).
Amani (Artstein et al, 2009b; Artstein et al,
2011) is an advanced question-answering char-
acter used as a prototype for systems meant to
train soldiers to perform tactical questioning. The
users are in between real users and test subjects:
they were cadets at the U.S. Military Academy in
April 2009, who interacted with Amani as a uni-
versity course exercise on negotiation techniques.
They used head-mounted microphones to talk with
Amani.
This dataset comprises of 1.8K audio files each
annotated with one of the 105 possible NLU se-
mantic classes.
Radiobots (Roque et al, 2006) is a training pro-
totype that responds to military calls for artillery
fire in a virtual reality urban combat environment.
This is a domain in the slot-filling genre, where
there is a preferred protocol for the order in which
information is provided and confirmed. Users are
generally trainees, learning how to do calls for fire,
they are motivated users with some training. The
semantic processing involved tagging each word
with the dialogue act and parameter that it was as-
sociated with (Ai et al, 2007b).
This data set was collected during the develop-
ment of the system in 2006 at Fort Sill, Oklahoma,
during two evaluation sessions from recruited vol-
unteer trainees who performed calls for specific
missions (Robinson et al, 2006). These subjects
used head-mounted microphones rather than the
ASTI simulated radios from later data collection.
SASO-EN (Traum et al, 2008) is a negotiation
training prototype in which two virtual characters
negotiate with a human ?trainee? about moving a
medical clinic. The genre is negotiation and plan-
ning, where the human participant must try to form
a coalition, and the characters reason about utili-
ties of different proposals, as well as causes and
effects. The output of NLU is a frame represen-
tation including both semantic elements, like the-
matic argument structure, and pragmatic elements,
such as addressee and referring expressions. Fur-
ther contextual interpretation is performed by each
of the virtual characters to match the (possibly par-
tial) representation to actions and states in their
task model, resolve other referring expressions,
and determine a full set of dialogue acts (Traum,
2003). Speech was collected at the USC Insti-
tute for Creative Technologies (ICT) during 2006?
2009, mostly from visitors and new hires, who
acted as test subjects.
This dataset has 4K audio files each anno-
tated with one of the 117 different NLU semantic
classes.
4 ASR Performance
We tested each of the Datasets described in Sec-
tion 3.3 with some of the recognizers described
in Section 2. All recognizers were tested on the
Amani, SASO-EN, and Twins domains, and we
also tested a natural language understanding com-
ponent on these data sets (Section 5). For SGT
Blackwell, SGT STAR, and Radiobots, we report
the performance on the same development set used
in Yao et al (2010). For Amani and SASO-EN
(where we also report the NLU performance), we
run a 10-fold cross-validation in which 9 folds
where used to train the NLU and ASR language
model and the 10th was used for testing. For the
Twins dialogue system, we used the same partition
into training, development and testing reported in
Morbini et al (2012) and the results reported here
are from the development set. Due to differences
in training/testing regimens, performance of sys-
tems are only comparable within each domain.
Table 2 summarizes the performance of the var-
ious ASR engines on the evaluation data sets. Per-
formance is measured as Word Error Rate and was
obtained using the NIST SCLITE tool.12
Note that only Otosense-Kaldi in the Twins do-
main had adapted acoustic models. In the remain-
12http://www.itl.nist.gov/iad/mig/tools/
399
Speech recognizer Evaluation data setAmani Radiobots SASO-EN SGT Blackwell SGT Star Twins
Pocketsphinx 39.7 11.8 28.4 51 28.6 81
Apple 28 ? 30.9 ? ? 29
AT&T 29 12.1 16.3 27.3 21.7 28.8
Google 23.8 36.3 20 18 26 20.6
Otosense-Kaldi 33.7 ? 22.1 ? ? 18.7
Table 2: Word Error Rates (%) for the various dialogue systems and ASR systems tested.
ing cases only the language model was adapted.
Looking at the results on the development set re-
ported in Yao et al (2010), we have improvements
in 3 out of 5 domains: Amani (?11.8% Google),
SASO-EN (?11.7% AT&T) and SGT Blackwell
(?13% Google). In Radiobots and SGT Star the
performance achieved with just language model
adaptation, when permitted, is worse: +4.8% and
+1.7% respectively.
We find that there is no single best performing
speech recognizer: results vary greatly between
the evaluation test sets. In 4 of the 6 datasets over-
all, and 2 of the 3 datatests tested with Otosense-
Kaldi, the best performer is a cloud-based ser-
vice (Google or AT&T). There are two datasets
for which a local, fully customizable recognizer
performs better than the cloud-based services. Ra-
diobots, consisting of military calls for artillery
fire, has a fairly limited and very specialized vo-
cabulary, and indeed the two recognizers with cus-
tom language models (Pocketsphinx and AT&T)
perform much better than the non-customizable
recognizer (Google).
The Twins dataset is unique in that for the
Otosense-Kaldi system we custom-trained acous-
tic and language models, while standard WSJ
acoustic models and adapted language models
were used for the other dialogue systems. In
both cases the models were triphone based with
a Linear Discriminant Analysis (LDA) front end,
and Maximum Likelihood Linear Transforma-
tion (MLLT) and Maximum Mutual Information
(MMI) training. This reflects on the very good
performance in the Twins domain, decent perfor-
mance on the SASO-EN domain (reasonable mis-
match of WSJ and SASO-EN) and very degraded
performance in Amani (highly mismatched Amani
and WSJ domains). The observed degradation in
performance is accentuated by the MMI discrim-
inative training on the mismatched-WSJ data. As
with PocketSphinx and Watson, and unlike with
Apple Dictation and Google Speech API, with
Kaldi we fully control experimental conditions
and can guarantee no contamination of the train-
test data.
In summary, our evaluation shows that cus-
tomizable recognizers are useful when the ex-
pected speech is highly specialized, or when sub-
stantial resources are available for tuning the rec-
ognizer.
5 NLU Accuracy & Relation between
ASR and NLU
While the different genres of system have different
types of output for NLU: response text, dialogue
act and parameter tags, speech acts, or semantic
frames, many of them can be coerced into a se-
lection task, in which the NLU selects the right
output from a set of possible outputs. This allows
any multiclass classification algorithm to be used
for NLU. A possible drawback is that for some
inputs, the right output might not be available in
the set considered by the training data, even if it
might easily be constructed from known parts us-
ing a generative approach.
A second issue is that even though we can cast
the problem as multi-class classification, classifi-
cation accuracy is not always the most appropriate
metric of NLU quality. For question-answering
characters, getting an appropriate and relevant re-
ply is more important than picking the exact re-
ply selected by a human domain designer or an-
notator: there might be multiple good answers, or
even the best available answer might not be very
good. For that reason, the question-answering
characters allow an ?off-topic? answer and Error-
return plots (Artstein, 2011) might be necessary
to choose an optimal threshold. For the SASO-EN
system, slot-filler metrics such as precision, recall,
and f-score are more appropriate than frame accu-
400
racy, because some frames may have many slots
in common and few that are different (e.g. just a
different addressee). Nonetheless, we begin our
analysis within this common framework. For sim-
plicity, we start with just three domains: Twins,
Amani, and SASO-EN. SGT STAR and Blackwell
are very similar to Twins in terms of NLU. Ra-
diobots is more challenging to coerce to multiclass
classification.
Conventional wisdom in the speech and lan-
guage processing community is that performance
of ASR and NLU are closely tied: improved
speech recognition leads to better language under-
standing, while deficiencies in speech recognition
cause difficulty in understanding. This conven-
tional wisdom is borne out by decades of experi-
ence with speech and dialogue systems, though we
are not aware of attempts to systematically demon-
strate it. The present study shows that the expected
relation between speech recognition and language
understanding holds for the systems we tested.
Accepted assumptions about the relation be-
tween speech recognition and language under-
standing have been repeatedly challenged. Direct
challenges are typically limited to specific appli-
cations. Wang et al (2003) show that for a slot-
filling NLU, ASR can be specifically tuned to rec-
ognize those words that are relevant to the slot-
filling task, resulting in improved understanding
despite a decrease in performance on overall word
recognition. However, Boros et al (1996) found
that when not optimizing the ASR for the specific
slot filling task there is a nearly linear correlation
between word accuracy and NLU accuracy. Al-
shawi (2003) and Huang and Cox (2006) show that
in call-routing applications the word level can be
dispensed with altogether and calls routed based
on phonetic information alone without noticeable
loss in performance. These challenges suggest that
the speech-language divide is not as clean as the
theory suggests.
To investigate the relation between ASR and
NLU, we ran each ASR output from each of
the 5 recognizers through an understanding com-
ponent to obtain an NLU output (each dataset
had a separate NLU component, which was held
constant for all speech recognizers). ASR and
NLU performance are conventionally measured on
scales of opposite polarity: better performance
shows up as lower word error rates but higher
NLU accuracies. For the correlations we invert the
conventional ASR scale and use word accuracy, so
that higher numbers signify better performance on
both scales.13
Figure 1 shows the results obtained in the 3 di-
alogue systems by the various ASR systems. The
figures plot ASR performance against NLU per-
formance; NLU results on manual transcriptions
are included for comparison. There are too few
data points for the correlations between ASR and
NLU performance to be significant, but the trends
are positive, as expected.
Our experiments lend supporting evidence to
the claim that in general, ASR performance is pos-
itively linked to NLU performance (special cases
notwithstanding). The 3 datasets exhibit posi-
tive correlations between speech recognition and
language understanding performance. Thus, we
claim that the basis of the conventional wisdom
is sound: speech recognition directly affects lan-
guage understanding. This conclusion holds when
the speech recognizer has been optimized to pro-
duce the most accurate transcript, rather than for a
specific NLU.
6 Conclusion and Future Work
We have extended here the ASR system evaluation
published in Yao et al (2010) including some new
cloud based ASR services that achieve very good
performance showing an improvement of around
12%. We also showed that ASR and NLU perfor-
mance are correlated.
One possible avenue of future work is to ex-
tract importance weights for each word from the
learnt NLU models and use these weights to try
to explain those cases that diverge from the corre-
lation between ASR and NLU performance. This
may also give us a better measure than WER for
assessing ASR performance in dialogue systems.
Another avenue of future work involves examin-
ing different types of NLU engines, and different
metrics for the different dialogue system genres,
which, again, may lead to a more relevant assess-
ment of ASR performance.
Acknowledgments
The effort described here has been sponsored by
the U.S. Army. Any opinions, content or informa-
tion presented does not necessarily reflect the posi-
13We define ?accuracy? as 1 minus WER, so this number
can in principle dip below zero if there are more errors than
words.
401
Amani
r = 0.54, df = 3, p = 0.345
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
50 60 70 80 90 100
51
54
57
60
63
66
69
?
?
?
? ?
?
SASO-EN
r = 0.77, df = 3, p = 0.130
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
60 70 80 90 100
30
40
50
60
70
80
90
?
???
?
?
Twins
r = 0.99, df = 3, p = 0.002
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
0 20 40 60 80 100
40
50
60
70
80
90
100
?
?
?
?
?
Figure 1: Relation between ASR and NLU performance (red dots are manual transcriptions)
tion or the policy of the United States Government,
and no official endorsement should be inferred.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, Athana-
sios Katsamanis, Shrikanth Narayanan, Angela
Nazarian, and David Traum. 2012. The Twins cor-
pus of museum visitor questions. In LREC-2012,
Istanbul, Turkey, May.
Hua Ai, Antoine Raux, Dan Bohus, Maxine Eskenazi,
and Diane Litman. 2007a. Comparing spoken dia-
log corpora collected with recruited subjects versus
real users. In SIGdial 2007.
Hua Ai, Antonio Roque, Anton Leuski, and David
Traum. 2007b. Using information state to improve
dialogue move identification in a spoken dialogue
system. In Proceedings of the 10th Interspeech Con-
ference, Antwerp, Belgium, August.
James F. Allen, George Ferguson, and Amanda Stent.
2001. An architecture for more realistic conversa-
tional systems. In IUI, pages 1?8.
Hiyan Alshawi. 2003. Effective utterance classifica-
tion with unsupervised phonotactic models. In HLT-
NAACL 2003, pages 1?7, Edmonton, Alberta, May.
Apple Inc. 2012. Mac basics: Dictation (Technote
HT5449), November.
R. Artstein, S. Gandhe, J. Gerten, A. Leuski, and
D. Traum. 2009a. Semi-formal evaluation of con-
versational characters. In O. Grumberg, M. Kamin-
ski, S. Katz, and S. Wintner, editors, Languages:
From Formal to Natural. Essays Dedicated to Nis-
sim Francez on the Occasion of His 65th Birthday,
volume 5533 of Lecture Notes in Computer Science,
pages 22?35. Springer, Berlin.
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009b. Viability of a simple dia-
logue act scheme for a tactical questioning dialogue
system. In DiaHolmia 2009: Proceedings of the
13th Workshop on the Semantics and Pragmatics of
Dialogue, page 43?50, Stockholm, Sweden, June.
Ron Artstein, Michael Rushforth, Sudeep Gandhe,
David Traum, and MAJ Aram Donigian. 2011.
Limits of simple dialogue acts for tactical question-
ing dialogues. In 7th IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
Barcelona, Spain, July.
Ron Artstein. 2011. Error return plots. In 12th SIG-
dial Workshop on Discourse and Dialogue, Port-
land, OR, June.
Linda Bell and Joakim Gustafson. 2003. Child and
adult speaker adaptation during error resolution in a
publicly available spoken dialogue system. In IN-
TERSPEECH 2003.
Dan Bohus and Alexander I. Rudnicky. 2003. Raven-
claw: dialog management using hierarchical task de-
composition and an expectation agenda. In INTER-
SPEECH 2003.
M. Boros, W. Eckert, F. Gallwitz, G. Grz, G. Han-
rieder, and H. Niemann. 1996. Towards understand-
ing spontaneous speech: Word accuracy vs. concept
accuracy. In In Proceedings of (ICSLP 96), pages
1009?1012.
Sudeep Gandhe, Nicolle Whitman, David R. Traum,
and Ron Artstein. 2009. An integrated authoring
tool for tactical questioning dialogue systems. In 6th
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, Pasadena, California, July.
Sudeep Gandhe, Michael Rushforth, Priti Aggarwal,
and David R. Traum. 2011. Evaluation of
an integrated authoring tool for building advanced
question-answering characters. In Proceedings of
Interspeech-11, Florence, Italy, 08/2011.
Sudeep Gandhe. 2013. Rapid prototyping and evalu-
ation of dialogue systems for virtual humans. Ph.D.
thesis, University of Southern California.
402
Liza Hassel and Eli Hagen. 2006. Adaptation of an
automotive dialogue system to users? expertise and
evaluation of the system. Language Resources and
Evaluation, 40(1):67?85.
Quiang Huang and Stephen Cox. 2006. Task-
independent call-routing. Speech Communication,
48(3?4):374?389.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Acoustics,
Speech and Signal Processing, 2006. ICASSP 2006
Proceedings. 2006 IEEE International Conference
on, volume 1, pages I?I.
Kristiina Jokinen and Kari Kanto. 2004. User ex-
pertise modeling and adaptivity in a speech-based
e-mail system. In Donia Scott, Walter Daelemans,
and Marilyn A. Walker, editors, ACL, pages 87?94.
ACL.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language En-
gineering, 6:323?340, September. Special Issue on
Spoken Language Dialogue System Engineering.
Anton Leuski and David R. Traum. 2011. NPCEditor:
Creating virtual human dialogue using information
retrieval techniques. AI Magazine, 32:42?56.
Anton Leuski, Brandon Kennedy, Ronakkumar Patel,
and David Traum. 2006a. Asking questions to
limited domain virtual characters: How good does
speech recognition have to be? In 25th Army Sci-
ence Conference.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006b. Building effective ques-
tion answering characters. In Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue,
pages 18?27.
Fabrizio Morbini, Kartik Audhkhasi, Ron Artstein,
Maarten Van Segbroeck, Kenji Sagae, Panayio-
tis S. Georgiou, David R. Traum, and Shrikanth S.
Narayanan. 2012. A reranking approach for recog-
nition and classification of speech input in conversa-
tional dialogue systems. In SLT, pages 49?54. IEEE.
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luka?s?
Burget, Ondr?ej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motl??c?ek, Yanmin Qian, Petr
Schwarz, Jan Silovsky?, Georg Stemmer, and Karel
Vesely?. 2011. The Kaldi speech recognition
toolkit. In IEEE 2011 Workshop on Automatic
Speech Recognition and Understanding, December.
S.M. Robinson, A. Roque, A. Vaswani, D. Traum,
C. Hernandez, and B. Millspaugh. 2006. Evalua-
tion of a spoken dialogue system for virtual reality
call for fire training. In 25th Army Science Confer-
ence, Orlando, Florida, USA.
S. Robinson, D. Traum, M. Ittycheriah, and J. Hen-
derer. 2008. What would you ask a conversational
agent? Observations of human-agent dialogues in
a museum setting. In Proc. of Sixth International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
A. Roque, A. Leuski, V. Rangarajan, S. Robinson,
A. Vaswani, S. Narayanan, and D. Traum. 2006.
Radiobot-CFF: A spoken dialogue system for mil-
itary training. In Proc. of Interspeech, Pittsburgh,
Pennsylvania, USA.
W. Swartout, D. Traum, R. Artstein, D. Noren, P. De-
bevec, K. Bronnenkant, J. Williams, A. Leuski,
S. Narayanan, D. Piepol, C. Lane, J. Morie, P. Ag-
garwal, M. Liewer, J. Chiang, J. Gerten, S. Chu,
and K. White. 2010. Ada and Grace: Toward
realistic and engaging virtual museum guides. In
J. Allbeck, N. Badler, T. Bickmore, C. Pelachaud,
and A. Safonova, editors, Intelligent Virtual Agents:
10th International Conference, IVA 2010, Philadel-
phia, PA, USA, September 20?22, 2010 Proceed-
ings, volume 6356 of Lecture Notes in Artificial In-
telligence, pages 286?300. Springer, Heidelberg.
David R. Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal
virtual agents. In IVA, pages 117?130.
David Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William Swartout. 2012.
Ada and grace: Direct interaction with museum
visitors. In The 12th International Conference on
Intelligent Virtual Agents (IVA), Santa Cruz, CA,
September.
David Traum. 2003. Semantics and pragmatics of
questions and answers for dialogue agents. In pro-
ceedings of the International Workshop on Compu-
tational Semantics, pages 380?394.
Ye-Yi Wang, A. Acero, and C. Chelba. 2003. Is
word error rate a good indicator for spoken lan-
guage understanding accuracy. In IEEE Workshop
on Automatic Speech Recognition and Understand-
ing (ASRU ?03), pages 577?582.
Xuchen Yao, Pravin Bhutada, Kallirroi Georgila, Kenji
Sagae, Ron Artstein, and David R. Traum. 2010.
Practical evaluation of speech recognizers for vir-
tual human dialogue systems. In Nicoletta Calzo-
lari, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, Stelios Piperidis, Mike Rosner, and
Daniel Tapias, editors, LREC. European Language
Resources Association.
403
