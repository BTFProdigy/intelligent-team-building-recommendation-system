Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1360?1369, Dublin, Ireland, August 23-29 2014.
Sentence Compression for Target-Polarity Word Collocation Extraction
Yanyan Zhao
1
, Wanxiang Che
2
, Honglei Guo
3
, Bing Qin
2
, Zhong Su
3
and Ting Liu
2?
1: Department of Media Technology and Art, Harbin Institute of Technology
2: Department of Computer Science and Technology, Harbin Institute of Technology
3: IBM Research-China
{yyzhao, bqin, tliu}@ir.hit.edu.cn, {guohl, suzhong}@cn.ibm.com
Abstract
Target-polarity word (T-P) collocation extraction, a basic sentiment analysis task, relies primarily
on syntactic features to identify the relationships between targets and polarity words. A major
problem of current research is that this task focuses on customer reviews, which are natural or
spontaneous, thus posing a challenge to syntactic parsers. We address this problem by proposing
a framework of adding a sentiment sentence compression (Sent Comp) step before performing
T-P collocation extraction. Sent Comp seeks to remove the unnecessary information for senti-
ment analysis, thereby compressing a complicated sentence into one that is shorter and easier to
parse. We apply a discriminative conditional random field model, with some special sentiment-
related features, in order to automatically compress sentiment sentences. Experiments show that
Sent Comp significantly improves the performance of T-P collocation extraction.
1 Introduction
Sentiment analysis deals with the computational treatment of opinion, sentiment and subjectivity in tex-
t (Pang and Lee, 2008), and has received considerable attention in recent years (Liu, 2012). Target-
Polarity word (T-P) collocation extraction, which aims to extract the collocation of a target and its cor-
responding polarity word in a sentiment sentence, is a basic task in sentiment analysis. For example,
in a sentiment sentence ????????????? (The camera has a novel appearance), ????
(appearance) is the target, and ???? (novel) is the polarity word that modifies ???? (appearance).
According, ???, ??? (?appearance, novel?) is the T-P collocation. Generally, T-P collocation is a
basic and complete sentiment unit, thus is very useful for many sentiment analysis applications.
Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (Ab-
basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation ?Adj
ATT
x Noun?, where the
ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P
collocation ???, ??? (?appearance, novel?) in the above sentiment sentence (Bloom et al., 2007;
Qiu et al., 2011; Xu et al., 2013).
However, one major problem of these approaches is the ?naturalness? of sentiment sentences, that is,
such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge
to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can
further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a)
as an example, because the word ???? (fortunately) is so chatty,1 the parsing result is wrong. Thus,
are unable to extract the T-P collocation ???,?? (?keyboard, good?).
To solve the ?naturalness? problem, we can train a parser on sentiment sentences. Unfortunately, an-
notating such data will cost us a lot of time and effort. Instead, in this paper we produce a sentence
compression model, Sent Comp, which is designed especially to compress complicated sentiment sen-
tences into formal and easier to parse ones, further improving T-P collocation extraction.
?
Correspondence author: tliu@ir.hit.edu.cn
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Note that, in Figure 1, the Chinese word ???? is chatty, although its translated English word ?fortunately? is not. In this
paper, we focus on processing the Chinese data.
1360
?? ?? ?
fortunately keyboard good
SBV
VOB
ROOT
(a) before compression
?? ?
keyboard good
SBV
ROOT
(b) after compression
Figure 1: Parse trees before and after compression.
This idea is motivated by the observation that, current syntactic parsers usually perform accurately
for short, simple and formal sentences, whereas error rates increase for longer, more complex or more
natural and spontaneous sentences (Finkel et al., 2008). Hence, the improvement in syntactic parsing
performance would have a ripple effect over T-P collocation extraction. For example, we can compress
the sentence in Figure 1(a) into a shortened sentence in Figure 1(b) by removing the chatty part ????
(fortunately). We can see that the shortened sentence is now well-formed (in Chinese) and its parse tree
is correct, making it easier to accurately extract T-P collocation.
Traditional sentence compression aims to obtain a shorter grammatical sentence by retaining impor-
tant information (usually important grammar structure) (Jing, 2000). For example, the sentence ?Overall,
this is a great camera.? can be compressed into ?This is a camera.? by removing the adverbial ?overall?
and the modifier ?great?. However, the modifier ?great? is a polarity word and very important for sen-
timent analysis. Therefore, Sent Comp model for sentiment sentences is different from the traditional
compression models, because it needs to retain the important sentiment information, such as the polarity
word. Hence, using Sent Comp, the above sentence can be compressed into ?This is a great camera.?
We regard Sent Comp as a sequence labeling task, which can be solved by a conditional random
fields (CRF) model. Instead of seeking the manual rules on parse trees for compression, as in other
studies (Vickrey and Koller, 2008), this method is an automatic procedure. In this work, we introduce
some sentiment-related features to retain the sentiment information for Sent Comp.
We apply Sent Comp as the first step in the T-P collocation extraction task. First, we compress the
sentiment sentences into easier to parse ones using Sent Comp, after which we employ the state-of-the-
art T-P collocation extraction approach on the compressed sentences. Experimental results on a Chinese
corpus of four product domains show the effectiveness of our approach.
The main contributions of this paper are as follows:
? We present a framework of using sentiment sentence compression preprocessing step to improve T-
P collocation extraction. This framework can better solve the ?over-natural? problem of sentiment
sentences, which poses a challenge to syntactic parsers. More importantly, the idea of this frame-
work can be applied to some other sentiment analysis tasks that rely heavily on syntactic results.
? We develop a simple yet effective compression model Sent Comp for sentiment sentences. To the
best of our knowledge, this is the first sentiment sentence compression model.
2 Background
For our baseline system, we used the state-of-the-art method to extract T-P collocations introduced by
Qiu et al. (2011), who proposed a double propagation method. This idea is based on the observation
that there is a natural syntactic relationship between polarity words and targets owing to the fact that
polarity words are used to modify targets. Furthermore, they also found that polarity words and targets
themselves have relations in some sentiment sentences (Qiu et al., 2011).
Based on this idea, in the double propagation method, we first used an initial seed polarity word lexicon
and the syntactic relations to extract the targets, which can fall into a new target lexicon. Then we used the
target lexicon and the same syntactic relations to extract the polarity words and to subsequently expand
the polarity word lexicon. This is an iterative procedure, because this method can iteratively produce the
new polarity words and targets back and forth using the syntactic relations.
1361
?? ?? ??
function very powerful
SBV
ADV
ROOT
(a) syntactic structure 1
?? ? ??
powerful function
ATT
RAD
ROOT
(b) syntactic structure 2
?? ? ?? ?
function is powerful
SBV VOB RAD
ROOT
(c) syntactic structure 3
?? ? ?? ?? ??
function and service very powerful
COO
LAD ADV
SBV ROOT
(d) syntactic structure 4
?? ?? ?? ? ??
function very powerful and complete
SBV COO
ADV LAD
ROOT
(e) syntactic structure 5
Figure 2: Example of syntactic structure rules for T-P collocation extraction. We showed five examples
from a total of nine syntactic structures. For each kind of syntactic structure (a) to (e), the target is
shown with a red box and the polarity word is shown with a green box. Syntactic structures (a) to (c)
describe the relations between targets and polarity words. Syntactic structure (d), which is extended
from (a), describes the relation between two targets. Syntactic structure (e), which is also extended from
(a), describes the relation between two polarity words. Similarly, we can summarize the other four rules
extended from (b) and (c) to describe the relations between two targets or two polarity words.
We can see that the syntactic relations are important for this method, and Qiu et al. (2011) proposed
eight rules to describe these relations. However, their work only focused on English sentences, whereas
the relations for Chinese sentences are different. Thus, in accordance with Chinese grammar, we pro-
posed nine syntactic structure rules between target t and polarity word p in a Chinese T-P collocation
?t, p?.
2
The three main rules are shown below and some example rules are illustrated in Figure 2.
Rule 1: t
SBV
x p, the ?subject-verb? structure between t and p, such as the example in Figure 2(a).
Rule 2: p
ATT
x t, that p is an attribute for t, such as the example in Figure 2(b).
Rule 3: t
SBV
x ?
VOB
y p, the ?subject-verb-object? structure between t and p, such as the example in
Figure 2(c). The ? denotes any word.
The other six rules can be extended from the three main rules by obtaining the coordination (COO)
relation of t or p, such as t
SBV
x ?
COO
y p in Figure 2(e). Note that the POS for t should be noun and for p
should be adjective.
As described above, the T-P collocation extraction relies heavily on syntactic parsers. Hence, if we
can use the Sent Comp model to improve the performance of parsers, the performance of T-P collocation
extraction can also be improved accordingly.
3 Sentiment Sentence Compression
3.1 Problem Analysis
First, we conducted an error analysis for the results of current T-P collocation extraction, from which we
observed that the ?naturalness? of sentiment sentences is one of the main problems. For examples:
? Chatty form: some sentiment sentences are so chatty, that they bring many difficulties to the parser.
For example, in the sentence ??????? (fortunately the keyboard is good) shown in Figure 1,
the usage of the chatty word ???? (fortunately) affects the accuracy of the syntactic parser.
2
A Chinese natural language processing toolkit, Language Technology Platform (LTP) (Che et al., 2010), was used as our
dependency parser. More information about the syntactic relations can be found in their paper. The state-of-the-art graph-based
dependency parsing model, in the toolkit, was trained on Chinese Dependency Treebank 1.0 (LDC2012T05).
1362
?? ?? ?
besides photo good
ADV
POB
ROOT
comp
?? ?
photo good
SBV
ROOT
(a) parse tree 1 before and after compression
?? ? ? ? ?? ??screen for people feel good
SBV
ATT
POBRAD SBV
ROOT
comp
?? ??screen good
SBV
ROOT
(b) parse tree 2 before and after compression
Figure 3: ?Naturalness? problem of sentiment sentences.
? Conjunction word usage: conjunction words are often used in sentiment sentences to show the dis-
course relations between two sentences. However, there are so many conjunction words in Chinese,
some of which can cause errors among parsers. For example, in Figure 3(a), the parse tree of sen-
tence ???????? (besides the photo is good) is wrong because of the usage of the conjunction
word ???? (besides).
? Feeling words/phrase usage: in sentiment sentences, people often use some feeling words/phrase,
such as ??????? (feel like) in Figure 3(b) or ????? (smell like). Given that the current
syntactic parser cannot handle the feeling words/phrases very well, the T-P collocation ???, ?
?? (?screen, good?) in Figure 3(b) cannot be extracted correctly.
To address the ?naturalness? problem, we compressed the sentiment sentences into one that are shorter
and easier to parse. Similar to the examples in Figure 1 and 3, the compressed sentences can be easily
and correctly parsed. The above analysis can be used as the criteria to guide us in compressing sentiment
sentences when annotating, and can also help us exploit more useful features for automatic sentiment
sentence compression.
3.2 Task Definition
We focus on studying the methods for extractive sentence compression.
3
Formally, extractive sentence
compression aims to shorten a sentence x = x
1
? ? ?x
n
into a substring y = y
1
? ? ? y
m
, where y
i
?
{x
1
, ? ? ? , x
n
}, m ? n.
In this paper, similar to Nomoto (2007), we also treated the sentence compression as a sequence
labeling task which can be solved by a CRF model. We assigned a compression tag t
i
to each word x
i
in
an original sentence x, where t
i
= N if x
i
? y, else t
i
= Y.
A first-order linear-chain CRF is used which defines the following conditional probability:
P (t|x) =
1
Z(x)
?
i
M
i
(t
i
, t
i?1
|x) (1)
where x and t are the input and output sequences respectively, Z(x) is the partition function, and M
i
is
the clique potential for edge clique i. Here, we used the CRFsuite toolkit to train the CRF model.
4
3.3 Features
The features for Sent Comp are listed in Table 1. Aside from the basic word (w), POS tag (t) and
their combination context features (01 ? 04), we introduced some sentiment-related features (05 ? 06)
and latent semantic features (07 ? 08) to better handle sentiment analysis data and generalize word
features. Then we added the syntactic parse features (09), which are commonly used in traditional
sentence compression task.
One sentiment-related feature (feeling(?)) indicates whether a word is a feeling word, which is inspired
by the naturalness problem in Figure 3(b). As discussed above, the current parser often produces wrong
parse trees because of these feeling words. Therefore, the feeling words tend to be removed from a
3
Generally, there are two kinds of sentence compression methods: extractive method and abstractive method. Because
abstractive method needs more resource and is more complicated, in this paper, we only focus on extractive approach.
4
www.chokkan.org/software/crfsuite/
1363
Basic Features
01: w
i+k
,?1 ? k ? 1
02: w
i+k?1
? w
i+k
, 0 ? k ? 1
03: t
i+k
,?2 ? k ? 2
04: t
i+k?1
? t
i+k
,?1 ? k ? 2
Sentiment-related Features
05: feeling(w
i
)
06: polarity(w
i
)
Latent Semantic Features
07: suffix(w
i
) if t(w
i
) == n else prefix(w
i
)
08: cluster(w
i
)
Syntactic Features
09: dependency(w
i
)
Table 1: Features of sentiment sentence compression
sentiment sentence for Sent Comp. We can obtain a feeling word lexicon from HowNet,
5
a popular
Chinese sentiment thesaurus, where a feeling word is defined by DEF={perception|??} tag. Finally,
we collected 38 feeling words, such as?? (realize),?? (find), and?? (think).
The other sentiment-related feature (polarity(?)) indicates whether a word is a polarity word. One
of the main differences between a sentiment sentence and a formal sentence is that the former often
contains polarity words. In contrast to the features of feeling(?), polarity words (e.g., ?great? in the
sentence ?Overall, this is a great camera?) tend to be retained, because they are important and special
to sentiment analysis. In this paper, we treat polarity words as important features, considering that they
are often tagged as modifiers and are easily removed by common sentence compression methods. We
can obtain the polarity feature (polarity(?)) from a polarity lexicon, which can also be obtained from
HowNet.
To generalize the words in sentiment sentences, we proposed two kinds of semantic features. The
first one is a suffix or prefix character feature (prefix(?) or suffix(?)). In contrast to English, the suffix
(for noun) or prefix (for non noun) characters of a Chinese word often carry that word?s core semantic
information. For example, ??? (bicycle), ?? (car), and ?? (train) are all various kinds of ?
(vehicle), which is also the suffix of the three words. Given that all of them may become targets, they
tend to be retained in compressed sentences. The verbs, ?? and ??, can be denoted by their prefix
feel (?), and can be removed from original sentences because they are feeling words.
We used word clustering features (cluster(?)) as the other latent semantic feature to further improve
the generalization over common words. Word clustering features contain some semantic information
and have been successfully used in several natural language processing tasks, including NER (Miller et
al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words ??
and ?? (appearance) belong to the same word cluster, although they have a different suffix or prefix.
Both words are important for T-P collocation extraction and should be retained. We used the Brown
word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were
obtained from the fifth edition of Chinese Gigaword (LDC2011T13).
Finally, similar to McDonald (2006), we also added the dependency relation between a word and its
parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out sentence
compression. For example, the ROOT relation typically indicates that the word should not be removed
because it is the main verb of a sentence.
4 Experiments
4.1 Experimental Setup
4.1.1 Corpus
We conducted the experiments on a Chinese corpus of four product domains, which came from the Task3
of the Chinese Opinion Analysis Evaluation (COAE) (Zhao et al., 2008).
6
Table 2 describes the corpus,
5
www.keenage.com
6
www.ir-china.org.cn/coae2008.html
1364
Domain # reviews # sentences # collocations
Camera 138 1,249 1,335
Car 161 1,172 1,312
Notebook 56 623 674
Phone 123 1,350 1,479
All 478 4,394 4,800
Table 2: Corpus statistics for the Chinese corpus of four product domains.
where 4,394 sentiment sentences containing 4,800 T-P collocations are manually found and annotated
from 478 reviews.
We ask annotators to manually compress all the sentiment sentences. Specifically, the annotators
removed some words from a sentiment sentence according to two criteria stated as follows: (1) removing
the word should not change the essential content of the sentence, and (2) removing the word should
not change the sentiment orientation of the sentence. In order to assess the quality of the annotation,
we sampled 500 sentences from this corpus and asked two annotators to perform the annotation. The
resulting word-based Cohen?s kappa (Cohen, 1960) (i.e., a measure of inter-annotator agreement ranging
from zero to one) of 0.7 indicated a good strength of agreement.
4.1.2 Evaluation
Generally, compressions are evaluated using three criteria (McDonald, 2006), namely, grammaticality,
importance, and compression rate. Obviously, the former two are difficult to evaluate objectively. Previ-
ous works used human judgment, which entails a difficult and expensive process. In this paper, similar to
a common sequence labeling task, we simply used the F-score metric of removed words to roughly eval-
uate the performance of sentiment sentence compression. Of course, the final effectiveness of sentence
compression model can be reviewed by the derived T-P collocation extraction task.
For T-P collocation extraction, we applied the traditional P, R and F-score for the final evaluations.
Specially, a fuzzy matching evaluation is adopted for the T-P collocation extraction. That is to say,
given an extracted T-P collocation ?t, p?, whose standard result is ?t
s
, p
s
?, if t is the substring of t
s
, and
meanwhile p is the substring of p
s
, we consider the extracted ?t, p? is a correct T-P collocation.
4.2 Sentiment Sentence Compression Results
Features P(%) R(%) F(%)
Basic (01 ? 04) 76.4 57.4 65.5
+ feeling (05) 75.9 57.6 65.5
+ polarity (06) 76.6 57.6 65.7
+ suffix or prefix (07) 78.4 56.9 66.0
+ cluster (08) 74.9 58.9 65.9
+ dependency (09) 75.3 57.2 65.0
All (01 ? 08) 77.3 59.1 67.0
All - feeling (05) 77.1 58.9 66.8
Table 3: The results of sentiment sentence compression with different features.
Results of Sent Comp with different features are shown in Table 3. All results are reported using five-
fold cross validation. We can see that the performance is improved when we added feeling
7
and polarity
features (05 ? 06) respectively, indicating that the sentiment-related features are useful for sentiment
sentence compression. In addition, the latent semantic features (07 ? 08) are also helpful, especially the
suffix or prefix features, which show better performance than the four other kinds of features.
Nonetheless, the dependency features (09) have a negative on compression performance due to the
specificity of compression for sentiment sentences. That is because the lower dependency parsing per-
formance on sentiment sentences introduces many wrong dependency relations, which counteract the
7
In Table 3, although the performance of adding feeling is comparative to the basic system (Basic (01-04)), the system
without feeling (All - feeling (05), the last line) is worse than the system using all the features (All (01-08)). This can illustrate
the effectiveness of the feeling feature.
1365
Domain Method P(%) R(%) F(%)
no Comp 74.7 58.4 65.6
Camera manual Comp 83.4 62.7 71.6
auto Comp 80.4 62.1 70.1
no Comp 68.2 53.1 59.7
Car manual Comp 76.3 57.7 65.7
auto Comp 72.3 56.1 63.2
no Comp 74.1 56.8 64.3
Notebook manual Comp 82.7 64.5 72.5
auto Comp 79.7 62.8 70.2
no Comp 77.3 60.9 68.1
Phone manual Comp 82.7 65.7 73.2
auto Comp 80.3 63.3 70.8
no Comp 73.7 57.5 64.6
All manual Comp 81.2 62.5 70.6
auto Comp 78.1 60.9 68.4
Table 4: Results on T-P collocation extraction for four product domains.
contribution of the dependency relation features. This is also the reason why we need to compress sen-
timent sentences as the first step for T-P collocation extraction. Finally, when we combine all of useful
features (01 ? 08), the performance achieves the highest score.
It is worth noting that sentiment sentence compression is a new task proposed in this paper. For
simplicity, this paper aims to attempt a simple yet effective sentiment sentence compression model. We
will polish the Sent Comp model in the future work.
4.3 Sent Comp for T-P Collocation Extraction
We designed three comparative systems to demonstrate the effectiveness of Sent Comp for T-P collo-
cation extraction. Note that, Sent Comp is the first step to process the corpus before T-P collocation
extraction. The method for T-P collocation extraction was based on the state-of-the-art method proposed
by Qiu et al. (2011) as described in Section 2.
no Comp - This refers to the system that only uses the T-P collocation extraction method and does not
perform sentence compression as the first step.
manual Comp - This system manually compresses the corpus into a new one as the first step, and then
applies the T-P collocation extraction method on the new compressed corpus.
auto Comp - This system uses Sent Comp as the first step to automatically compress the corpus into a
new one, and then applies the T-P collocation extraction method on the new corpus.
From the descriptions above, we can draw a conclusion that the performance of manual Comp can be
considered as the upper bound for the sentiment sentence compression based T-P collocation extraction
task.
Table 4 shows the experimental results of the three systems on T-P collocation extraction for four prod-
uct domains. Here, manual Comp can significantly (p < 0.01) improved the F-score by approximately
6%,
8
compared with no Comp. This illustrates that the idea of sentiment sentence compression is use-
ful for T-P collocation extraction. Specifically, the proposed method can transform some over-natural
sentences into normal ones, further influencing their final syntactic parsers. Evidently, because the T-P
collocation extraction relies heavily on syntactic features, the more correct syntactic parse trees derived
from the compressed sentences can help to increase the performance of this task.
Compared with no Comp, the auto Comp system also yielded a significantly better results (p < 0.01)
that indicated an improvement of 3.8% in the F-score, despite the fact that the automatic sentence com-
pression model Sent Comp may wrongly compress some sentences. This demonstrates the usefulness
of sentiment sentence compression step in the T-P collocation extraction task and further proves the
effectiveness of our proposed model.
8
We use paired bootstrap resampling significance test (Efron and Tibshirani, 1993).
1366
Moreover, we can observe that the idea of sentence compression and our Sent Comp are useful for
all the four product domains on T-P collocation extraction task, indicating that Sent Comp is domain
adaptive. However, we can find a small gap between auto Comp and manual Comp, which indicates
that the Sent Comp model can still be improved further. In the future, we will explore more effective
sentence compression algorithms to bridge the gap between the two systems.
5 Related Works
5.1 Sentiment Analysis
T-P collocation extraction is a basic task in sentiment analysis. In order to solve this task, most methods
focused on identifying relationships between targets and polarity words. In early studies, researcher-
s recognized the target first, and then chose its polarity word within a window of size k (Hu and Liu,
2004). However, considering that this kind of method is too heuristic, the performance proved to be very
limited. To tackle this problem, many researchers found syntactic patterns that can better describe the
relationships between targets and polarity words. For example, Bloom et al. (2007) constructed a link-
age specification lexicon containing 31 patterns, while Qiu et al. (2011) proposed a double propagation
method that introduced eight heuristic syntactic patterns to extract the collocations. Xu et al. (2013) used
the syntactic patterns to extract the collocation candidates in their two-stage framework.
Based on the above, we can conclude that syntactic features are very important for T-P collocation
extraction. However, the ?naturalness? problem can still seriously affect the performance of syntactic
parser. Once our sentiment sentence compression method can improve the quality of parsing, the perfor-
mance of T-P collocation extraction task can be improved as well. Note that, to date, there is no previous
work using a sentence compression model to improve this task.
5.2 Sentence Compression
Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones,
while preserving the essential content (Jing, 2000). There are many applications that can benefit from
a robust compression system, such as summarization systems (Li et al., 2013), semantic role label-
ing (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on.
Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and
Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos,
2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the
syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus,
the compressed tree (after removing constituents from a bad parse) may not produce a good compressed
sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem
by using discriminative models.
Aside from above extractive sentence compression approaches, there is another research line, namely,
abstractive approach, which compresses an original sentence by reordering, substituting, and inserting,
as well as removing (Cohn and Lapata, 2013). This method needs more resource and is more complicat-
ed. Therefore, in this paper, we only focus on extractive approach.
At present, the current sentence compression methods all focus on formal sentences, and few meth-
ods are being proposed to study sentiment sentences. As discussed in the above sections, the current
compression models cannot be directly utilized to T-P collocation extraction owing to the specificity of
sentiment sentences. Therefore, a new compression model for sentiment sentences should be established.
6 Conclusion and Future Work
In this work, we presented a framework that adopted a CRF based sentiment sentence compression mod-
el Sent Comp, as a preprocessing step, to improve the T-P collocation extraction task. Different from
the existing sentence compression models used for formal sentences, Sent Comp incorporated some
sentiment-related features to retain the sentiment information. Experimental results showed that the sys-
tem with the sentence compression step performed better than that without this step, thus demonstrating
the effectiveness of the framework and the compression model Sent Comp.
1367
Generally, the idea of this framework maybe useful for many sentiment analysis tasks that rely heavily
on syntactic results. Thus in the future, we will try to apply the Sent Comp model for these tasks. Besides,
the simplicity and effectiveness of this framework motivates us to pursue the study further. For example,
we will polish the Sent Comp model by exploring more sentiment-related features and exploring other
types of compression models.
Acknowledgments
We thank the anonymous reviewers for their helpful comments. This work was supported by National
Natural Science Foundation of China (NSFC) via grant 61300113, 61133012 and 61273321, the Ministry
of Education Research of Social Sciences Youth funded projects via grant 12YJCZH304, the Fundamen-
tal Research Funds for the Central Universities via grant No.HIT.NSRIF.2013090 and IBM Research-
China Joint Research Project.
References
Ahmed Abbasi, Hsinchun Chen, and Arab Salem. 2008. Sentiment analysis in multiple languages: Feature
selection for opinion classification in web forums. ACM Trans. Inf. Syst., 26(3):12:1?12:34, June.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon. 2007. Extracting appraisal expressions. In HLT-NAACL
2007, pages 308?315.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Comput. Linguist., 18(4):467?479, December.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp: A chinese language technology platform. In Coling 2010:
Demonstrations, pages 13?16, Beijing, China, August. Coling 2010 Organizing Committee.
Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu. 2013. Named entity recognition with
bilingual constraints. In Proceedings of the 2013 Conference of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Language Technologies, pages 52?62, Atlanta, Georgia, June.
Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear program-
ming approach. J. Artif. Intell. Res. (JAIR), 31:399?429.
Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement,
20(1):37 ? 46.
Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelli-
gence Research, 34:637?674.
Trevor Cohn and Mirella Lapata. 2013. An abstractive approach to sentence compression. ACM Transactions on
Intelligent Systems and Technology, 4(3):1?35.
Adnan Duric and Fei Song. 2012. Feature selection for sentiment analysis based on content and syntax models.
Decis. Support Syst., 53(4):704?711, November.
B. Efron and R. J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08: HLT, pages 959?967, Columbus, Ohio, June. Association for
Computational Linguistics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 885?893, Los Angeles, California, June. Association
for Computational Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov grammars for sentence compression. In
Human Language Technologies 2007: The Conference of the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main Conference, pages 180?187, Rochester, New York, April.
Association for Computational Linguistics.
1368
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of KDD-2004,
pages 168?177.
Hongyan Jing. 2000. Sentence reduction for automatic text summarization. In IN PROCEEDINGS OF THE 6TH
APPLIED NATURAL LANGUAGE PROCESSING CONFERENCE, pages 310?315.
Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to
sentence compression. Artif. Intell., 139(1):91?107, July.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus, Ohio, June. Association for Computational Linguistics.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013. Document summarization via guided sentence compression.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 490?500,
Seattle, Washington, USA, October. Association for Computational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natural language. Master?s thesis, MIT.
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers.
Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In In Proc. EACL.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative
training. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,
pages 337?342, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.
Makoto Miwa, Rune Saetre, Yusuke Miyao, and Jun?ichi Tsujii. 2010. Entity-focused sentence simplification
for relation extraction. In Proceedings of the 23rd International Conference on Computational Linguistics
(COLING 2010), pages 788?796.
Tadashi Nomoto. 2007. Discriminative sentence compression with conditional random fields. Information Pro-
cessing and Management, 43(6):1571?1587, November.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Computational Linguistics, 37(1):9?27.
Kapil Thadani and Kathleen McKeown. 2013. Sentence compression with joint structural inference. In Pro-
ceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 65?74, Sofia,
Bulgaria, August. Association for Computational Linguistics.
Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 290?
297, Stroudsburg, PA, USA. Association for Computational Linguistics.
David Vickrey and Daphne Koller. 2008. Sentence simplification for semantic role labeling. In ACL, pages
344?352. The Association for Computer Linguistics.
Kristian Woodsend and Mirella Lapata. 2011. Learning to simplify sentences with quasi-synchronous grammar
and integer programming. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language
Processing, pages 409?420, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Mining opinion words and opinion targets
in a two-stage framework. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1764?1773, Sofia, Bulgaria, August. Association for Computational
Linguistics.
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan, Kang Liu, and Qi Zhang. 2008. Overview of chinese pinion
analysis evaluation 2008. In The First Chinese Opinion Analysis Evaluation (COAE) 2008.
1369
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 160?170, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Collocation Polarity Disambiguation Using Web-based Pseudo Contexts
Yanyan Zhao, Bing Qin and Ting Liu?
Harbin Institute of Technology, Harbin, China
{yyzhao, bqin, tliu}@ir.hit.edu.cn
Abstract
This paper focuses on the task of colloca-
tion polarity disambiguation. The collocation
refers to a binary tuple of a polarity word and
a target (such as ?long, battery life? or ?long,
startup?), in which the sentiment orientation of
the polarity word (?long?) changes along with
different targets (?battery life? or ?startup?).
To disambiguate a collocation?s polarity, pre-
vious work always turned to investigate the
polarities of its surrounding contexts, and then
assigned the majority polarity to the collo-
cation. However, these contexts are limited,
thus the resulting polarity is insufficient to be
reliable. We therefore propose an unsuper-
vised three-component framework to expand
some pseudo contexts from web, to help dis-
ambiguate a collocation?s polarity.Without us-
ing any additional labeled data, experiments
show that our method is effective.
1 Introduction
In recent years, more attention has been paid to
sentiment analysis as it has been widely used in
various natural language processing applications,
such as question answering (Wiebe et al2003;
Yu and Hatzivassiloglou, 2003), information extrac-
tion (Riloff et al2005) and opinion-oriented sum-
marization (Hu and Liu, 2004; Liu et al2005).
Meanwhile, it also brings us lots of interesting and
challenging research topics, such as subjectivity
analysis (Riloff and Wiebe, 2003), sentiment clas-
sification (Pang et al2002; Kim and Hovy, 2005;
?Correspondence author: tliu@ir.hit.edu.cn
Wilson et al2009; He et al2011), opinion re-
trieval (Zhang et al2007; Zhang and Ye, 2008;
Li et al2010) and so on.
One fundamental task for sentiment analysis is
to determine the semantic orientations of words.
For example, the word ?beautiful? is positive, while
?ugly? is negative. Many researchers have devel-
oped several algorithms for this purpose and gener-
ated large static lexicons of words marked with prior
polarities (Hatzivassiloglou and McKeown, 1997;
Turney et al2003; Esuli, 2008; Mohammad et al
2009; Velikovich et al2010). However, there exist
some polarity-ambiguous words, which can dynam-
ically reflect different polarities along with different
contexts. A typical polarity-ambiguous word ???
(?long? in English) is shown with two example sen-
tences as follows.
1. ????[????]t?[?]p?(Positive)
Translated as: The [battery life]t of this camera
is [long]p. (Positive)
2. ????[????]t?[?]p?(Negative)
Translated as: This camera has [long]p
[startup]t. (Negative)
The phrases marked with p superscript are the
polarity-ambiguous words, and the phrases marked
with t superscript are targets modified by the polar-
ity words. In the above two sentences, the sentiment
orientation of the polarity word ??? (?long? in En-
glish) changes along with different targets. When
modifying the target ?????? (?battery life? in
English), its polarity is positive; and when modify-
ing ?????? (?startup? in English), its polarity is
160
negative. In this paper, we especially define the col-
location as a binary tuple of the polarity-ambiguous
word and its modified target, such as ??,?????
(?long, battery life? in English) or ??,?????
(?long, startup? in English). This paper concentrates
on the task of collocation polarity disambiguation.
This is an important task as the problem of
polarity-ambiguity is frequent. We analyze 4,861
common binary tuples of polarity words and their
modified targets from 478 reviews1, and find that
over 20% of them are the collocations defined in this
paper. Therefore, the task of collocation polarity dis-
ambiguation is worthy of study.
For a sentence s containing such a collocation c,
since the in-sentence features are always ambiguous,
it is difficult to disambiguate the polarity of c by us-
ing them. Thus some previous work turned to in-
vestigate its surrounding contexts? polarities (such
as the sentences before or after s), and then assigned
the majority polarity to the collocation c (Hatzivas-
siloglou and McKeown, 1997; Hu and Liu, 2004;
Kanayama and Nasukawa, 2006). However, since
the amount of contexts from the original review is
very limited, the final resulting polarity for the col-
location c is insufficient to be reliable.
Fortunately, most collocations may appear multi-
ple times, in different forms, both within the same
review and within topically-related reviews. Thus
for a collocation, we can collect large amounts of
contexts from other reviews to improve its polarity
disambiguation. These expanded contexts are called
pseudo contexts in this paper. Some previous work
used the similar methods. For example, Ding (Ding
et al2008) expanded some pseudo contexts from
a topically-related review set. But since the review
set is limited, the expanded contexts are still lim-
ited and unreliable. In order to overcome this prob-
lem, we propose an unsupervised three-component
framework to expand more pseudo contexts from
web for the collocation polarity disambiguation.
Without using any labeled data, experiments on
a Chinese data set from four product domains show
that the three-component framework is feasible and
the web-based pseudo contexts are useful for the
collocation polarity disambiguation. Compared to
other previous work, our method achieves an F1
1The dataset will be introduced in Section 4.1 in detail.
score of 72.02%, which is about 15% higher.
The remainder of this paper is organized as fol-
lows. Section 2 introduces the related work. Section
3 shows the proposed approach including three in-
dependent components. Section 4 and 5 presents the
experiments and results. Finally we conclude this
paper in Section 6.
2 Related Work
The key of the collocation polarity disambigua-
tion task is to recognize the polarity word?s sen-
timent orientation of a collocation. There are ba-
sically two types of approaches for word polar-
ity recognition: corpus-based and dictionary-based
approaches. Corpus-based approaches find co-
occurrence patterns of words in the large corpora
to determine the word sentiments, such as the work
in (Hatzivassiloglou and McKeown, 1997; Wiebe,
2000; Riloff and Wiebe, 2003; Turney et al2003;
Kaji and Kitsuregawa, 2007; Velikovich et al
2010). On the other hand, dictionary-based ap-
proaches use synonyms and antonyms in WordNet
to determine word sentiments based on a set of seed
polarity words. Such approaches are studied in (Kim
and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps
et al2004). Overall, most of the above approaches
aim to generate a large static polarity word lexicon
marked with prior polarities.
However, it is not sensible to predict a word?s sen-
timent orientation without considering its context.
In fact, even in the same domain, a word may indi-
cate different polarities depending on what targets it
is applied to, especially for the polarity-ambiguous
words, such as ??? (?long? in English) shown in
Section 1. Based on these, we need to consider both
the polarity words and their modified targets, i.e.,
the collocations mentioned in this paper, rather than
only the polarity words.
To date, the task in this paper is similar with
much previous work. Some researchers exploited
the features of the sentences containing colloca-
tions to help disambiguate the polarity of the
polarity-ambiguous word. For example, Hatzivas-
siloglou (Hatzivassiloglou and McKeown, 1997)
and Kanayama (Kanayama and Nasukawa, 2006)
used conjunction rules to solve this problem from
large domain corpora. Suzuki (Suzuki et al2006)
161
took into account many contextual information of
the word within the sentence, such as exclamation
words, emoticons and so on. However, the experi-
mental results show that these in-sentence features
are not rich enough.
Instead of considering the current sentence alone,
some researchers exploited external information and
evidences in other sentences or other reviews to infer
the collocation?s polarity. For a collocation, Hu (Hu
and Liu, 2004) analyzed its surrounding sentences?
polarities to disambiguate its polarity. Ding (Ding
et al2008) proposed a holistic lexicon-based ap-
proach of using global information to solve this
problem. However, the contexts or evidences from
these two methods are limited and unreliable. Ex-
cept for the above unsupervised methods, some re-
searchers (Wilson et al2005; Wilson et al2009)
proposed supervised methods for this task, which
need large annotated corpora.
In addition, many related works tried to learn
word polarity in a specific domain, but ignored the
problem that even the same word in the same do-
main may indicate different polarities (Jijkoun et al
2010; Bollegala et al2011). And some work (Lu et
al., 2011) combined difference sources of informa-
tion, especially the lexicons and heuristic rules for
this task, but ignored the important role of the con-
text. Besides, there exists some research focusing
on word sense subjectivity disambiguation, which
aims to classify a word sense into subjective or ob-
jective (Wiebe and Mihalcea, 2006; Su and Markert,
2009). Obviously, this task is different from ours.
3 The Proposed Approach
3.1 Overview
The motivation of our approach is to make full use of
web sources to collect more useful pseudo contexts
for a collocation, whose original contexts are lim-
ited or unreliable. The framework of our approach
is illustrated in Figure 1.
In order to disambiguate a collocation?s polarity,
three components are carried out:
1. Query Expansion and Pseudo Context Ac-
quisition: This paper uses the collocation as query.
For a collocation, three heuristic query expansion
strategies are used to generate more flexible queries,
which have the same or completely opposite polar-
A Chinese 
collocation 
in a review
Query 
expansion
Searching
Web 
snippets
Original context 
acquisition
Sentiment 
analysis
Pseudo context 
acquisition
Sentiment 
analysis
Combination Pos/Neg
start
end
Figure 1: The framework of our approach.
ity with this collocation. Searching these queries in
the domain-related websites, lots of snippets can be
acquired. Then we can extract the pseudo contexts
from these snippets.
2. Sentiment Analysis: For both original con-
texts and the expanded pseudo contexts from web, a
simple lexicon-based sentiment computing method
is used to recognize each context?s polarity.
3. Combination: Two strategies are designed to
integrate the polarities of the original and pseudo
contexts, under the assumption that these two kinds
of contexts can be complementary to each other.
It is worth noting that this three-component
framework is flexible and we can try to design dif-
ferent strategies for each component. Next sections
will give a simple example strategy for each compo-
nent to show its feasibility and effectiveness.
3.2 Query Expansion and Pseudo Context
Acquisition
3.2.1 Why Expanding Queries
For a collocation, such as ??,????? (?long,
battery life? in English), the most intuitive query
used for searching is constructed by the form of ?tar-
get + polarity word?, i.e., ????? (battery
life long in English). Even if we search this query
alone, a great many web snippets covering the po-
larity word and target will be retrieved. But why do
we still need to expand the queries?
In fact, for a collocation, though the amount of the
retrieved snippets is large, lots of them cannot pro-
vide accurate pseudo contexts. The reason is that the
162
polarity words in some snippets do not really mod-
ify the targets, such as in the sentence ?The battery
life is short, and finds few buyers for a long time.?
There exist no modifying relation between ?battery
life? and ?long?.
In order to filter these meaningless snippets, we
can simply search with a new query ???????
by surrounding it with quotes (noted as Strategy0).
However, this can drastically decline the amount of
snippets. In addition, as the new query is short, in
many retrieved snippets, there also exist no modify-
ing relations between the polarity words and targets.
As a result, if we just use this query strategy, the ex-
panded pseudo contexts are limited and cannot yield
ideal performance.
Therefore, we need to design some effective
query expansion strategies to ensure that (1) the po-
larity words do modify the targets in the retrieved
web snippets, and (2) the snippets are more enough.
3.2.2 Query Expansion Strategy
We first investigate the modifying relations be-
tween polarity words and the targets, and then con-
struct effective queries.
Observed from previous work (Bloom et al
2007; Kobayashi et al2004; Popescu and Etzioni,
2005), there are two kinds of common relations be-
tween the polarity words and their targets. One is
the ?subject-copula-predicate? relation, such as the
relationship between ?long? and ?battery life? in the
sentence ?The battery life of this camera is long?.
The other is the ?attribute-head? relation, such as
the relationship between them in the sentence ?This
camera has long battery life?.
As a result, three heuristic query expansion strate-
gies are adopted to construct efficient queries for
searching. Take the collocation ??,?????
(?long, battery life? in English) as an example, the
strategies are described as follows.
Strategy1: target + modifier + polarity word:
Such as the query ???????? or ?????
???? (?the battery life is very long? in English).
Different from Strategy0, this strategy adds a mod-
ifier element. It refers to the words that are used to
change the degree of a polarity word, such as ??? or
???? (?very? in English). Due to the usage of the
modifiers, the queries from this strategy can satisfy
the ?subject-copula-predicate? relation.
Strategy2: modifier + polarity word + ?+ tar-
get: Such as the query ????????? or ??
???????? (?very long battery life? in En-
glish). This strategy also uses modifiers to modify
polarity words, and the generated queries can satisfy
the ?attribute-head? relation.
Strategy3: negation word + polarity word +?+
target: Such as the query ????????? or ??
???????? (?not long battery life? in En-
glish). This strategy uses negation words to modify
the polarity words. And the queries from this strat-
egy can satisfy the ?attribute-head? relation. The
only difference is that the polarity of this kind of
queries is opposite to that of the collocation.
Similar to the queries from Strategy0, the queries
generated by Strategy1?3 are all searched with
quotes. In addition, note that the modifier and the
negation word are taken from Modifier Lexicon and
Negation Lexicon introduced in Table 2.
3.2.3 Pseudo Context Acquisition
For each query from Strategy0?3, we search it in
some websites to acquire the related snippets. If we
directly search it using Google without site restric-
tions, it does return all the snippets containing the
query, but lots of them are non-reviews. Further, the
pseudo contexts generated by these non-reviews are
useless or even harmful. To overcome this problem,
the advanced search of Google is used to search the
query within the forum sites of the product domain.
We can flexibly choose several popular forum sites
for each domain. The URLs of the forum sites used
in this paper are listed in Table 1.
Formally, given a collocation c, the expanded
pseudo contexts Conx(c) can be obtained using the
following function:
Conx(c) =
?3
i=0 f(Queryi)
=
?3
i=0
?n
j=1 f(queryij)
(1)
Here, Queryi is the query set generated by the ith
query expansion strategy; queryij is the jth query
generated by the ith strategy. And the parameter n is
the total number of queries from the ith query expan-
sion strategy. From this function, we can collect the
contexts of c by summing up all the pseudo contexts
from every queryij .
163
Domain URL
Camera
http://www.qqdc.com.cn
http://forums.nphoto.net
http://dc.pconline.com.cn
http://photobbs.it168.com
http://club.tech.sina.com.cn/dc
Car
http://bbs.chetx.com
http://bbs.pcauto.com.cn
http://club.autohome.com.cn
http://bbs.cheshi.com
http://www.xcar.com.cn
http://www.autohome.com.cn
Notebook
http://benyouhui.it168.com/index.php
http://nbbbs.zol.com.cn
http://www.ibijiben.com
http://notebook.pconline.com.cn
http://nbbbs.enet.com.cn
Phone
http://bbs.imobile.com.cn
http://sjbbs.zol.com.cn
http://bbs.shouji.com.cn
http://bbs.cnmo.com
http://forum.younet.com
Table 1: The URLs used in context expansion for differ-
ent domains.
In detail, the pseudo context acquisition algorithm
for a collocation c is illustrated in Figure 2. Note
that, the original context acquisition of c can be con-
sidered as a simplified version of the pseudo context
acquisition. That?s because the current review con-
taining c can be considered as only one snippet in
pseudo context acquisition. Thus, we can just carry
out the two steps in (2) of Figure 2 to obtain the orig-
inal contexts.
Analyzing either the pseudo contexts or the orig-
inal contexts, we can find that not all of them are
useful contexts. Thus we will simply filter the noisy
ones by context sentiment computation, and choose
the contexts showing sentiment orientations as the
useful contexts.
3.3 Sentiment Analysis
For both the original and expanded pseudo contexts,
we employ the lexicon-based sentiment computing
method (Hu and Liu, 2004) to compute the polarity
value for each context. This unsupervised approach
is quite straightforward and makes use of the senti-
ment lexicons in Table 2.
The polarity value Polarity(con) for a context con
Algorithm: Pseudo Context Expansion Algorithm
Input: A collocation c and the URL list
Output: The pseudo context set Conx(c)
1. Use Strategy0~3 to expand c and the expanded queries 
are saved as a set Query(c).
2. For any query q Query(c),   acquire its pseudo 
contexts Conx(q) as follows:
(1) search q in the domain-related URL list, the top 100 
retrieved snippets for each URL are collected as Snip(q)
(2) for each snippet sp Snip(q)
find the sentence s containing q
obtain the two sentences before and after s as the 
contexts of q in this sp, noted as Conx(q, sp)
Conx(q) = 
3. Conx(c) =                      =
?
?
U
)(
),(
qSnipsp
spqConx
?
U
)(
)(
cQueryq
qConx
?
U U
)( )(
),(
cQueryq qSnipsp
spqConx
? ?
Figure 2: The algorithm for pseudo context acquisition.
Lexicon Content
Modifier Lexicon
?,??,??,??,?,?,
??,?,??,??,??
(?very? or ?quite? in English)
Negation Lexicon ??,?,??(?no? or ?not? in English)
Positive Lexicon There are 3,730 Chinese wordsare collected from HOWNET1.
Negative Lexicon There are 3,116 Chinese wordsare collected from HOWNET.
1 http://www.keenage.com/html/e index.html.
Table 2: The lexicons used in this paper.
is computed by summing up the polarity values of all
words in con, making use of both the word polarity
defined in the positive and negative lexicons and the
contextual shifters defined in the negation lexicon.
The algorithm is illustrated in Figure 3.
In this algorithm, n is the parameter controlling
the window size within which the negation words
have influence on the polarity words, and here n is
set to 3.
Normally, if the polarity value Polarity(con) is
more than 0, the context con is labeled as positive; if
less than 0, the context is negative. We also consider
the transitional words, such as ???? (?but? in En-
glish). Finally, the contexts with positive/negative
polarities are used as the useful contexts.
164
Domain # of reviews # of c # of single c Sig / All # of multiple c(All) (Sig) (%) / kinds of multiple c
Camera 138 295 183 62.03 112 / 35
Car 161 232 131 56.47 101 / 33
Notebook 56 147 94 63.95 53 / 20
Phone 123 327 192 58.72 135 / 35
Total 478 1001 600 59.94 401 / 123 ? 3.3
Table 3: Statistics for the Chinese collocation corpus.
Algorithm: Sentiment Analysis 
Input: a context con, and three lexicons: Positive_Dic, 
Negative_Dic, Negation_Dic
Output: Polarity value Polarity(con)
1. Segment con into word set W(con)
2. For each word w W(con), compute its polarity value 
Polarity(w) as follows:
(1) if w Positive_Dic, Polarity(w) = 1;
(2) if w Negative_Dic, Polarity(w) = -1;
(3) otherwise, Polarity(w) = 0;
(4) Within the window of n words previous to w, if 
there is a word w? Negation_Dic, 
Polarity(w) = -Polarity(w)
3. Polarity(con)  = 
?
?
?
? )(
)(
conWw
wPolarity
?
?
Figure 3: The algorithm for context polarity computation.
3.4 Combination
After the pseudo context acquisition and polarity
computation, two kinds of effective contexts: orig-
inal contexts and pseudo contexts, and their corre-
sponding polarities can be obtained.
In order to yield a relatively accurate polarity Po-
larity(c) for a collocation c, we exploit the following
combination methods:
1. Majority Voting: Rather than considering the
difference between the two kinds of contexts, this
combination method relies on the polarity tag of
each context. Suppose c has n effective contexts
(including original and pseudo contexts), it can ob-
tain n polarity tags based on the individual sentiment
analysis algorithm. The polarity tag receiving more
votes is chosen as the final polarity of c.
2. Complementation: For a collocation c, we
first employ ?Majority Voting? method just on the
expanded pseudo contexts to obtain the polarity tag.
If the polarity of c cannot be recognized2, the ma-
jority polarity tag voted on the original contexts is
chosen as the final polarity tag.
4 Experimental Setup
4.1 Dataset and Evaluation Metrics
We conduct the experiments on a Chinese colloca-
tion corpus of four product domains, which is from
the Task3 of the Chinese Opinion Analysis Evalua-
tion (COAE)3 (Zhao et al2008). Table 3 describes
the corpus in detail.
From 478 reviews, 1,001 collocations (454 pos-
itive and 547 negative) with polarity-ambiguous
words are found and manually annotated by two an-
notators. Cohen?s kappa (Cohen, 1960), a measure
of inter-annotator agreement ranging from zero to
one, is 0.83, indicating a good strength of agree-
ment 4. In Table 3, Sig of the fourth column denotes
the collocations that appear once in all the domain-
related reviews. And multiple in the last column
denotes the collocations that appear several times.
From Table 3, we can find that among all the re-
views, nearly 60% collocations only appear once.
Even for the multiple collocations, they averagely
appear less than 4 times. Therefore, for a colloca-
tion, if we only consider its original contexts alone
or the expanded pseudo contexts from the domain-
related review set al, the contexts are obviously
limited and unreliable.
Instead of using accuracy, we use precision (P),
recall (R) and F-measure (F1) to measure the perfor-
mance of this task. That?s because two kinds of col-
locations? polarities cannot be disambiguated. One
2The reason will be explained in the last paragraph of Sec-
tion 4.1.
3http://www.ir-china.org.cn/coae2008.html
4A small number of collocations are still difficult to be dis-
ambiguated from contexts.
165
is the sparse collocations, which obtain no effective
contexts. The other is the collocations that acquire
the same amount of positive and negative contexts.
The metrics are defined as follows.
P = correctly disambiguated collocations
disambiguated collocations
(2)
R = correctly disambiguated collocations
all collocations
(3)
F1 = 2PR
P +R
(4)
4.2 System Description
In order to compare our method with previous work,
we build several systems as follows:
NoExp: Following the method proposed by
Hu (Hu and Liu, 2004), without using the expanded
pseudo contexts, we only consider the two original
contexts Senbef and Senaft of a collocation c in the
current review. If Senbef expresses the polarity po-
lar, then Polarity(ac) = polar. Else if Senaft
expresses the polarity polar?, then Polarity(ac) =
polar?. Else, this method cannot disambiguate the
polarity of c. In this method, the transitional words,
such as ???? (?but? in English) are considered.
Expdataset: Following the method proposed by
Ding (Ding et al2008), we solve this task with the
help of the pseudo contexts in the domain-related re-
view dataset. For a collocation c appearing in many
domain-related reviews, this method refers to the po-
larities of the same c in other reviews. The majority
polarity is chosen as final polarity.
Expweb+sig: This method is the same as our
method in this paper, except for (1) not combining
the original contexts, and (2) not using all the three
query expansion strategies, but just using the sin-
gle (abbv. sig) Strategy0. This method expands the
pseudo contexts from the web. The majority polarity
is chosen as the final polarity.
Expweb+exp: This method is the same as our pro-
posed method in this paper, except for not combin-
ing the original contexts. It expands the pseudo con-
texts from the web. And the ?exp? in the subscript
means that this method uses all the query expansion
strategies. The majority polarity of all the pseudo
contexts is chosen as the final polarity.
Expmv/cweb+exp+com: This is the method proposed
in this paper, which combines the original and ex-
panded pseudo contexts. The superscript ?mv/c? is
short for the two combination methods: Majority
Voting and Complementation.
5 Results
5.1 Comparisons among All the Systems
In fact, all the systems shown in Section 4.2 can be
considered as context based methods. The essential
difference among them lies in the contexts they used.
For a collocation, the contexts for NoExp are two
original contexts from the current review. Breaking
down the boundary of the current review,Expdataset
explores the pseudo contexts from other domain-
related reviews. Further, Expweb+sig, Expweb+exp
and Expmv/cweb+exp+com expand the pseudo contexts
from web, which can be considered as a large corpus
and can provide more evidences for the collocation
polarity disambiguation.
System P(%) R(%) F1(%)
NoExp 67.32 41.16 51.08
Expdataset 68.14 47.85 56.22
Expweb+sig 70.00 53.85 60.87
Expweb+exp 74.97 63.14 68.55
Expmvweb+exp+com 75.53 67.83 71.47
Expcweb+exp+com 74.36 69.83 72.02
Table 4: Comparative results for the collocation polarity
disambiguation task.
Table 4 illustrates the comparative results of all
systems for collocation polarity disambiguation. It
can be observed that our system Expmvweb+exp+com
and Expcweb+exp+com outperform all the other sys-
tems. We discuss the experimental results as fol-
lows:
NoExp yields the worst performance, especially
on the recall. The reason is that the original con-
texts used in this system are limited, and some of
them are even noisy. In comparison, Expdataset
adds a post-processing step of expanding pseudo
contexts from the topically-related review dataset,
which achieves a better result with an absolute im-
provement of 5.14% (F1). This suggests that the
contexts expanded from other reviews are helpful in
disambiguating the collocation?s polarity.
166
However, Expdataset is just effective in disam-
biguating the polarity of such a collocation c, which
appears many times in the domain-related reviews.
From Table 3, we can notice that this kind of collo-
cations only accounts for 40% in all the collocations,
and further they appear less than 4 times on average.
Thus, for such a collocation c, the pseudo contexts
expanded from other reviews that contain the same
c are still far from enough, since the review set size
in this system is not very large.
In order to avoid the context limitation problem,
we expand more pseudo contexts from web for each
collocation. We first try to use a simple query
form (Strategy0) for web mining. Table 4 illustrates
that the corresponding system Expweb+sig outper-
forms the system Expdataset. It can demonstrate
that our web mining based pseudo context expan-
sion is useful for disambiguating the collocation?s
polarity, since this system can explore more con-
texts. However, we can find that the performance
is not very ideal. This system can generate some
harmful contexts for the reason of the wrong mod-
ifying relations between polarity words and targets
in the retrieved snippets.
Thus this paper adds three query expansion strate-
gies to generate more and accurate pseudo con-
texts. Table 4 shows that the corresponding sys-
tem Expweb+exp can achieve a better result with F1
= 68.55%, which is significantly (?2 test with p <
0.01) outperforms Expweb+sig. It demonstrates that
the query expansion strategies are useful.
Finally, Table 4 gives the results of our method in
this paper, Expmvweb+exp+com and Expcweb+exp+com,
which combines the original and expanded pseudo
contexts to yield a final polarity. We can ob-
serve that both of these systems outperform the sys-
tem NoExp of just using the original contexts and
the system Expweb+exp of just using the expanded
pseudo contexts. This can illustrate that the two
kinds of contexts are complementary to each other.
In addition, we can also find that the two combi-
nation methods produce similar results. In detail,
Expmvweb+exp+com disambiguates 899 collocations,
679 of them are correct; Expcweb+exp+com disam-
biguates 940 collocations, 699 of them are correct.
We can further find that, although the amount of
original contexts is small, it also plays an important
role in disambiguating the polarities of the collo-
cations that cannot be recognized by the expanded
pseudo contexts.
5.2 The Contributions of the Query Expansion
Strategies
The expanded pseudo contexts from our method can
be partly credited to the query expansion strategies.
Based on this, this section aims to analyze the differ-
ent contributions of the query expansion strategies in
our method.
Strategy P(%) R(%) F1(%) Avg(#)
Strategy0 70.00 53.85 60.87 71
Strategy1 74.14 55.84 63.70 112
Strategy2 61.84 37.56 46.74 26
Strategy3 64.34 33.17 43.77 20
Expweb+exp 74.97 63.14 68.55 229
Table 5: The performance of our method based on each
query expansion strategy for collocation polarity disam-
biguation.
Table 5 provides the performance of our method
based on each query expansion strategy for collo-
cation polarity disambiguation. For each strategy,
?Avg? in Table 5 denotes the average number of
the expanded pseudo contexts for each collocation.
From this table, we can find that the larger the ?Avg?
is, the better (F1) the strategy is. In detail, Strategy1
with the largest ?Avg? has the best performance; and
Strategy3 with the fewest ?Avg? has the worst per-
formance. This can further demonstrate our idea
that more and effective pseudo contexts can improve
the performance of the collocation polarity disam-
biguation task. Expweb+exp integrates all the query
expansion strategies and obtains much more ?Avg?.
Therefore, this can significantly increase the recall
value, and further produce a better result. On the
other hand, the results in Table 5 show that these
heuristic query expansion strategies are effective.
5.3 Deep Experiments in the
Three-Component Framework
In order to do a detailed analysis into our three-
component framework, some deep experiments are
made:
Query Expansion The aim of query expansion
is to retrieve lots of relative snippets, from which
we can extract the useful pseudo contexts. For each
167
Strategy0 Strategy1 Strategy2 Strategy3
(%) (%) (%) (%)
Query Expansion 76.75 94.50 85.50 85.25
Pseudo Context 71.25 73.50 67.50 74.50
Sentiment Analysis 63.00 68.25 59.00 69.75
Table 6: The accuracies of the query expansion, pseudo context and sentiment analysis for each strategy.
snippet, if the polarity word of the collocation does
modify the target, we consider this snippet as a cor-
rect query expansion result.
Pseudo Context For each expanded pseudo con-
text from web, if it shows the same sentiment ori-
entation with the collocation (or opposite with the
collocation?s polarity because of the usage of transi-
tional words), we consider this context as a correct
pseudo context.
Sentiment Analysis For each expanded pseudo
context, if its polarity can be correctly recognized
by the polarity computation method in Figure 3, and
meanwhile it shows the same sentiment orientation
with the collocation, we consider this context as a
correct one.
Table 6 illustrates the accuracy of each experi-
ment for each strategy in detail, where 400 web re-
trieved snippets for Query Expansion and 400 ex-
panded pseudo contexts for Pseudo Context and
Sentiment Analysis are randomly selected and man-
ually evaluated for each strategy.
Seen from Table 6, we can find that:
1. For Query Expansion, all strategies yield good
accuracies except for Strategy0. This can draw a
same conclusion with our analysis in Section 3.2.1.
The queries from Strategy0 are short, thus in many
retrieved snippets, there exist no modifying relations
between the polarity words and targets. Accord-
ingly, the pseudo contexts from these snippets are
incorrect. This can result in the low accuracy of
Strategy0. On the other hand, we can find that the
other three query expansion strategies perform well.
2. Although the final result of our three-
component framework is good, the accuracies of
Pseudo Context and Sentiment Analysis for each
strategy is not very high. This is perhaps caused by
unrefined work on the specific sub-stages. For ex-
ample, we get alhe pseudo contexts using the al-
gorithm in Figure 2. However, in some reviews, the
two sentences before and after the target sentence
have no polarity relation with the target sentence it-
self. This can bring in some noises. On the other
hand, the context polarity computation algorithm in
Figure 3 is just a simple attempt, which is not the
best way to compute the context?s polarity.
In fact, this paper aims to try some simple algo-
rithms for each component to validate the effective-
ness of the three-component framework. We will
polish every component of our framework in future.
6 Conclusion and Future Work
This paper proposes a web-based context expan-
sion framework for collocation polarity disambigua-
tion. The basic assumption of this framework is
that, if a collocation appears in different forms, both
within the same review and within topically-related
reviews, then the large amounts of pseudo contexts
from these reviews can help to disambiguate such
a collocation?s polarity. Based on this assumption,
this framework includes three independent compo-
nents. First, the heuristic query expansion strate-
gies are adopted to expand pseudo contexts from
web; then a simple but effective polarity computa-
tion method is used to recognize the polarities for
both the original contexts and the expanded pseudo
contexts; and finally, we integrate the polarities from
the original and pseudo contexts as the collocation?s
polarity. Without using any additional labeled data,
experiments on a Chinese data set from four product
domains show that the proposed framework outper-
forms other previous work.
This paper can be concluded as follows:
1. A framework including three independent com-
ponents is proposed for collocation polarity
disambiguation. We can try other different al-
gorithms for each component.
2. Web-based pseudo contexts are effective for
disambiguating a collocation?s polarity.
168
3. The query expansion strategies are promising,
which can generate more useful and correct
contexts.
4. The initial contexts from current reviews and
the expanded contexts from web are comple-
mentary to each other.
The immediate extension of our work is to polish
each component of this framework, such as improv-
ing the accuracy of query expansion and pseudo con-
text acquisition, using other effective polarity com-
puting methods for each context and so on. In ad-
dition, we will explore other query expansion strate-
gies to generate more effective contexts.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments. This work was supported by National
Natural Science Foundation of China (NSFC) via
grant 61133012, the National ?863? Leading Tech-
nology Research Project via grant 2012AA011102,
the Ministry of Education Research of Social Sci-
ences Youth funded projects via grant 12YJCZH304
and the Fundamental Research Funds for the Central
Universities via grant No.HIT.NSRIF.2013090.
References
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308?315.
D. Bollegala, D. Weir, and J. Carroll. 2011. Using mul-
tiple sources to construct a sentiment sensitive the-
saurus for cross-domain sentiment classification. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 132?141. Asso-
ciation for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the Conference onWeb Search andWeb
Data Mining (WSDM), pages 231?240.
A. Esuli and F. Sebastiani. 2005. Determining the se-
mantic orientation of terms through gloss analysis. In
Proceedings of the ACM SIGIR Conference on Infor-
mation and Knowledge Management (CIKM), pages
617?624.
A. Esuli. 2008. Automatic generation of lexical re-
sources for opinion mining: models, algorithms and
applications. In ACM SIGIR Forum, volume 42, pages
105?106. ACM.
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proceed-
ings of the eighth conference on European chapter of
the Association for Computational Linguistics, pages
174?181. Association for Computational Linguistics.
Yulan He, Chenghua Lin, and Harith Alani. 2011. Auto-
matically extracting polarity-bearing topics for cross-
domain sentiment classification. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 123?131, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168?177. ACM.
V. Jijkoun, M. De Rijke, and W. Weerkamp. 2010. Gen-
erating focused topic-specific sentiment lexicons. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 585?594.
Association for Computational Linguistics.
N. Kaji and M. Kitsuregawa. 2007. Building lexicon
for sentiment analysis from massive collection of html
documents. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1075?1083.
Jaap Kamps, Maarten Marx, R. ort. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientation of adjectives. In Proceedings of
LREC-2004, pages 1115?1118.
H. Kanayama and T. Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented senti-
ment analysis. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, pages 355?363. Association for Computational
Linguistics.
Soo-Min Kim and Eduard Hovy. 2005. Automatic detec-
tion of opinion bearing words and sentences. In Pro-
ceedings of IJCNLP-2005, pages 61?66.
S.-M. Kim and E. Hovy. 2006. Identifying and analyz-
ing judgment opinions. In Proceedings of the Joint
Human Language Technology/North American Chap-
ter of the ACL Conference (HLT-NAACL), pages 200?
207.
Nozomi Kobayashi, Kentaro Inui, Yuji Matsumoto, Kenji
Tateishi, and Toshikazu Fukushima. 2004. Collecting
evaluative expressions for opinion extraction. In Pro-
ceedings of the International Joint Conference on Nat-
ural Language Processing (IJCNLP), pages 584?589.
169
Binyang Li, Lanjun Zhou, Shi Feng, and Kam-Fai Wong.
2010. A unified graph model for sentence-based opin-
ion retrieval. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
page 1367?1375.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the web. In Proceedings of WWW-2005, pages
342?351.
Y. Lu, M. Castellanos, U. Dayal, and C.X. Zhai. 2011.
Automatic construction of a context-aware sentiment
lexicon: an optimization approach. In Proceedings of
the 20th international conference on World wide web,
pages 347?356. ACM.
S. Mohammad, C. Dunne, and B. Dorr. 2009. Generat-
ing high-coverage semantic orientation lexicons from
overtly marked words and a thesaurus. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 2-Volume 2, pages
599?608. Association for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP-
2002, pages 79?86.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
hltemnlp2005, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP-2003, pages 105?112.
Ellen Riloff, Janyce Wiebe, and William Phillips. 2005.
Exploiting subjectivity classification to improve in-
formation extraction. In Proceedings of AAAI-2005,
pages 1106?1111.
Fangzhong Su and Katja Markert. 2009. Subjectivity
recognition on word senses via semi-supervised min-
cuts. In Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the ACL, pages 1?9.
Yasuhiro Suzuki, Hiroya Takamura, and Manabu Oku-
mura. 2006. Application of semi-supervised learn-
ing to evaluative expression classification. In Com-
putational Linguistics and Intelligent Text Processing,
pages 502?513.
P. Turney, M.L. Littman, et al003. Measuring praise
and criticism: Inference of semantic orientation from
association. ACM Transactions on Information Sys-
tems (TOIS), 21(4):315?346.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and RyanMcDonald. 2010. The viability of web-
derived polarity lexicons. In The 2010 Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 777?785.
Jan Wiebe and Rada Mihalcea. 2006. Word sense and
subjectivity. In Proceedings of the Conference on
Computational Linguistics / Association for Computa-
tional Linguistics (COLING/ACL), pages 1065?1072.
Janyce Wiebe, Eric Breck, and Chris Buckley. 2003.
Recognizing and Organizing Opinions Expressed in
the World Press. In Papers from the AAAI Spring
Symposium on New Directions in Question Answering,
pages 24?26.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of AAAI, pages 735?
740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP-
2005, pages 347?354.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: an exploration
of features for phrase-level sentiment analysis. Com-
putational Linguistics, 35(3).
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP-2003, pages 129?
136.
Min Zhang and Xingyao Ye. 2008. A generation model
to unify topic relevance and lexicon-based sentiment
for opinion retrieval. In Proceedings of the ACM Spe-
cial Interest Group on Information Retrieval (SIGIR),
pages 411?419.
Wei Zhang, Clement Yu, and Weiyi Meng. 2007. Opin-
ion retrieval from blogs. In In proceedings of CIKM,
page 831?840.
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan,
Kang Liu, and Qi Zhang. 2008. Overview of chinese
opinion analysis evaluation 2008.
170
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 377?380,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Generalizing Syntactic Structures for Product Attribute Candidate
Extraction
Yanyan Zhao, Bing Qin, Shen Hu, Ting Liu
Harbin Institute of Technology, Harbin, China
{yyzhao,bqin,shu,tliu}@ir.hit.edu.cn
Abstract
Noun phrases (NP) in a product review are
always considered as the product attribute
candidates in previous work. However, this
method limits the recall of the product at-
tribute extraction. We therefore propose
a novel approach by generalizing syntactic
structures of the product attributes with two
strategies: intuitive heuristics and syntactic
structure similarity. Experiments show that
the proposed approach is effective.
1 Introduction
Product attribute extraction is a fundamental task of
sentiment analysis. It aims to extract the product at-
tributes from a product review, such as ?picture qual-
ity? in the sentence ?The picture quality of Canon is
perfect.? This task is usually performed in two steps:
product attribute candidate extraction and candidate
classification.
Almost all the previous work pays more attention
to the second step, fewer researchers make in-depth
research on the first step. They simply choose the
NPs in a product review as the product attribute can-
didates (Hu and Liu, 2004; Popescu and Etzioni,
2005; Yi et al, 2003). However, this method lim-
its the recall of the product attribute extraction for
two reasons. First, there exist other structures of the
product attributes except NPs. Second, the syntactic
parsing is not perfect, especially for the Non-English
languages, such as Chinese. Experiments on three
Chinese datasets1 show that nearly 15% product at-
tributes are lost, when only using NPs as the can-
didates. Obviously, if using the candidate classifi-
cation techniques on these NP candidates, it would
1It refers to the training data in Section 3.1.
lead to poor performance (especially for recall) for
the final product attribute extraction.
Based on the above discussion, it can be observed
that product attribute candidate extraction is well
worth studying. In this paper, we propose an ap-
proach by generalizing the syntactic structures of the
product attributes to solve this problem. Figure 1
lists some syntactic structure samples from an an-
notated corpus, including the special forms of NPs
in Figure 1(a) and other syntactic structures, such as
VP or IP in Figure 1(b). We can find that the syntac-
tic structures can not only cover more phrase types
besides NP, but also describe the detailed forms of
the product attributes.
NP
NN
??(screen)
NP
NN
??
NP
NN
???(screen    resolution)
NP
QP
CD
?
NP
NN
??(single    track)
NP
ADJP
JJ
?
NP
NN
??(front         seats)
NP
VB
??
NP
NN
??(photographing function)
VP
NP
NN
??
VP
VB
??(screen   display)
IP
(a) syntactic structure samples of NP
(b) syntactic structure samples of other phrases
Figure 1: Syntactic structure samples of the product at-
tributes (acquired by an automatic phrase parser).
In order to exploit more and useful syntactic struc-
tures, two generalization strategies: intuitive heuris-
tics and syntactic structure similarity are used. Ex-
periments on three Chinese domain-specific datasets
show that our approach can significantly improve the
recall of the product attribute candidate extraction,
and furthermore, improve the performance of the fi-
nal product attribute extraction.
377
2 Approach
The standard syntactic structures of the product at-
tributes can be collected from a training set2. Then
a simple method of exact matching can be used to
select the product attribute candidates from the test
set. In particular, for a syntactic structure3 T in
the test set, if T exactly matches with one of the
standard syntactic structures, then its corresponding
string can be treated as a product attribute candidate.
However, this method fails to handle similar syn-
tactic structures, such as the two structures in Fig-
ure 2. Besides, this method treats the syntactic struc-
ture as a whole during exact matching, without con-
sidering any structural information. Therefore, it is
difficult to describe the syntactic structure informa-
tion explicitly. All of these prevent this method from
generalizing unseen data well.
To overcome the above problems, two generaliza-
tion strategies are proposed in this paper. One is to
generalize the syntactic structures with two intuitive
heuristics. The other is to deeply mine the syntactic
structure by decomposing it into several substruc-
tures. Both strategies will be introduced in the fol-
lowing subsections.
2.1 Intuitive Heuristics
Two intuitive heuristics are adopted to generalize the
syntactic structures.
Heu1: For the near-synonymic grammar tags in
syntactic structures, we can generalize them by a
normalized one. Such as the red boxes in Figure 2,
the POSs ?NNS? and ?NN? show the same syntactic
meaning, we can generalize ?NNS? with ?NN?. The
near-synonymic grammar tags are listed in Table 1.
NP
VP NP
VB NNS NP
NN
NP
VP NP
VB NN NN
Heu2
Heu1
Figure 2: Generalizing a syntactic structure with two in-
tuitive heuristics.
Heu2: For the sequence of identical grammar tags
in syntactic structures, we can replace them with
2We use Dan Bikel?s phrase parser for syntactic parsing.
3We simply select the syntactic structures of the strings un-
der three words or four words with ???(?of? in English).
Replaced by Near-synonymic grammar tags
JJ JJR, JJS
NN NNS, NNP, NNPS, CD, NR
RB RBR, RBS
VB VBD, VBG, VBN, VBP, VBZ, VV
S SBAR, SBARQ, SINU, SQ
Table 1: The near-synonymic grammar tags.
one. The reason is that the sequential grammar tags
always describe the same syntactic function as one
grammar tag. Such as the blue circles in Figure 2.
2.2 Syntactic Structure Similarity
The heuristic generalization strategy is too restric-
tive to give a good coverage. Moreover, after this
kind of generalization, the syntactic structure is used
as a whole in exact matching all the same. Thus,
as an alternative to the exact matching, tree kernel
based methods can be used to implicitly explore the
substructures of the syntactic structure in a high-
dimensional space. This kind of methods can di-
rectly calculate the similarity between two substruc-
ture vectors using a kernel function. Tree kernel
based methods are effective in modeling structured
features, which are widely used in many natural
language processing tasks, such as syntactic pars-
ing (Collins and Duffy, 2001) and semantic role la-
beling (Che et al, 2008) and so on.
NP
NN
VP
VB
IP
NP VP
VB
IP
NP
NN
VP
VB NP
NN
VP
VB
IP
NP
NN
VP
IP
NP VP
IP
NP
NN
VP
VB
IP IP
Figure 3: Substructures from a syntactic structure.
In this paper, the syntactic structure for a product
attribute can be decomposed into several substruc-
tures, such as in Figure 3. Correspondingly, the syn-
tactic structure T can be represented by a vector of
integer counts of each substructure type:
?(T ) = (?1(T ), ?2(T ), ..., ?n(T ))
= (# of substructures of type 1,
= # of substructures of type 2,
...,
= # of substructures of type n)
378
After syntactic structure decomposition, we can
count the number of the common substructures as
the similarity between two syntactic structures. The
commonly used convolution tree kernel is applied in
this paper. Its kernel function is defined as follows:
K(T1, T2) = ??(T1),?(T2)?
=
?
i(?i(T1) ? ?i(T2))
Based on these, for a syntactic structure T in the
test set, we can compute the similarity between T
and all the standard syntactic structures by the above
kernel function. A similarity threshold thsim4 is set
to determine whether the string from T is a correct
product attribute candidate.
3 Experiments
3.1 Datasets and Evaluation Metrics
Three domain-specific datasets are used in the ex-
periments, which is from an official Chinese Opin-
ion Analysis Evaluation 2008 (COAE2008) (Zhao et
al., 2008). Table 2 shows the statistics of the three
datasets, each of which is divided into training, de-
velopment and test data in a proportion of 2:1:1.
Domain # of sentences # of standardproduct attributes
Camera 1,780 1,894
Car 2,166 2,504
Phone 2,196 2,293
Table 2: The datasets for three product domains.
Two evaluation metrics, recall and noise ratio, are
designed to evaluate the performance of the prod-
uct attribute candidate extraction. Recall refers to
the proportion of correctly identified attribute candi-
dates in all standard product attributes. Noise ratio
refers to the proportion of incorrectly identified at-
tribute candidates in all candidates.
3.2 Comparative methods
We choose the method, which considers NPs as the
product attribute candidates, as the baseline (shown
as NPs based).
Besides, in order to assess the two generaliza-
tion strategies? effectiveness, four experiments are
designed as follows:
4In the experiments, thsim is set to 0.7, which is tuned on
the development set.
SynStru based: It refers to the syntactic struc-
ture exact matching method, which is implemented
without the two proposed generation strategies.
SynStru h: It refers to the strategy only using the
first generalization.
SynStru kernel: It refers to the strategy only us-
ing the second generalization.
SynStru h+kernel: It refers to the strategy us-
ing both two generalizations, i.e., it refers to our ap-
proach in this paper.
3.3 Results
Table 3 lists the comparative performances on the
test data between our approach and the comparative
methods for product attribute candidate extraction.
Domain Method Recall Noise ratio
Camera
NPs based 81.20% 63.64%
SynStru based 84.80% 67.67%
SynStru h 92.08% 74.74%
SynStru kernel 92.51% 75.92%
SynStru h+kernel 92.72% 76.25%
Car
NPs based 85.25% 69.35%
SynStru based 86.31% 72.66%
SynStru h 93.78% 78.01%
SynStru kernel 94.56% 79.50%
SynStru h+kernel 94.71% 80.44%
Phone
NPs based 84.11% 63.76%
SynStru based 86.26% 67.09%
SynStru h 93.13% 73.62%
SynStru kernel 93.47% 75.11%
SynStru h+kernel 93.63% 75.35%
Table 3: Comparisons between our approach and the
comparative methods for product attribute candidate ex-
traction.
Analyzing the recalls in Table 3, we can find that:
1. The performance of SynStru based method
is better than NPs based method for each domain.
This can illustrate that syntactic structures can cover
more forms of the product attributes. However, the
recall of SynStru based method is not high, either.
2. The two generalization strategies, SynStru h
and SynStru kernel can both significantly improve
the performance for each domain, comparing to the
SynStru based method. This can illustrate that our
two generalization strategies are helpful.
3. Our approach SynStru h+kernel achieves the
best performance. This can illustrate that the two
generalization strategies are complementary to each
379
other. And further, mining and generalizing the syn-
tactic structures is effective for candidate extraction.
However, the noise ratio for each domain is in-
creasing when employing our approach. That?s be-
cause, more kinds of syntactic structures are consid-
ered, more noise is added. However, we can easily
remove the noise in the candidate classification step.
Thus in the next section, we will assess our candi-
date extraction approach by applying it to the prod-
uct attribute extraction task.
4 Application in Product Attribute
Extraction
For the extracted product attribute candidates, we
train a maximum entropy (ME) based binary clas-
sifier to find the correct product attributes. Several
commonly used features are listed in Table 4.
Feature Description
lexical
the words of the product attribute(PA)
the POS for each word of the PA
three words before the PA
three words after the PA
the words? number of the PA
syntactic the syntactic structure of the PA
Is there a stop word in the PA?
binary Is there a polarity word in the PA?
(Y/N) Is there an English word or number in the PA?
Table 4: The feature set for product attribute extraction.
Table 5 shows the product attribute extraction per-
formances on the test data. We can find that the
performance (F1) of our approach is better than
NPs based method for each domain. We discuss the
results as follows:
1. Comparing to the NPs based method, the re-
call of our approach increases a lot for each domain.
This demonstrates that generalized syntactic struc-
tures can cover more forms of product attributes.
2. Comparing to the NPs based method, the pre-
cision of our approach also increases for each do-
main. That?s because syntactic structures are more
specialized than the phrase forms (such as NP, VP)
in the previous work, which can filter some noises
from the phrase(NP) candidates.
5 Conclusion
This paper describes a simple but effective way to
extract the product attribute candidates from product
Domain Method R (%) P (%) F1 (%)
Camera NPs based 59.62 68.38 63.70Our approach 62.96 73.32 67.74
Car NPs based 59.94 64.87 62.31Our approach 67.34 65.90 66.61
Phone NPs based 58.53 71.14 64.22Our approach 67.84 76.13 71.74
Table 5: Comparisons between our approach and the
NPs based method for product attribute extraction.
reviews. The proposed approach is based on deep
analysis into syntactic structures of the product at-
tributes, via intuitive heuristics and syntactic struc-
ture decomposition. Experimental results indicate
that our approach is promising. In future, we will try
more syntactic structure generalization strategies.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, and the ?863?National
High- Tech Research and Development of China via
grant 2008AA01Z144.
References
Wanxiang Che, Min Zhang, AiTi Aw, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. Using a hybrid con-
volution tree kernel for semantic role labeling. ACM
Trans. Asian Lang. Inf. Process., 7(4).
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In NIPS, pages 625?632.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. In Proceedings of AAAI-
2004, pages 755?760.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
hltemnlp2005, pages 339?346.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Extract-
ing sentiments about a given topic using natural lan-
guage processing techniques. In Proceedings of the
IEEE International Conference on Data Mining.
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan,
Kang Liu, and Qi Zhang. 2008. Overview of chinese
opinion analysis evaluation 2008.
380
