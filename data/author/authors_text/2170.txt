In: Proceedings of CoNLL-2000 and LLL-2000, pages 31-36, Lisbon, Portugal, 2000. 
A Comparison between Supervised Learning Algorithms for Word 
Sense Disambiguation* 
Gerard  Escudero  and Lluis Mhrquez  and German R igau  
TALP Research Center. LSI Department. Universitat Polit~cnica de Catalunya (UPC) 
Jordi Girona Salgado 1-3. E-08034 Barcelona. Catalonia 
{escudero, lluism, g.rigau}@Isi.upc.es 
Abst ract  
This paper describes a set of comparative exper- 
iments, including cross-corpus evaluation, be- 
tween five alternative algorithms for supervised 
Word Sense Disambiguation (WSD), namely 
Naive Bayes, Exemplar-based learning, SNOW, 
Decision Lists, and Boosting. Two main conclu- 
sions can be drawn: 1) The LazyBoosting algo- 
rithm outperforms the other four state-of-the- 
art algorithms in terms of accuracy and ability 
to tune to new domains; 2) The domain depen- 
dence of WSD systems eems very strong and 
suggests that some kind of adaptation or tun- 
ing is required for cross-corpus application. 
1 In t roduct ion  
Word Sense Disambiguation (WSD) is the prob- 
lem of assigning the appropriate meaning (or 
sense) to a given word in a text or discourse. 
Resolving the ambiguity of words is a central 
problem for large scale language understanding 
applications and their associate tasks (Ide and 
V4ronis, 1998). Besides, WSD is one of the most 
important open problems in NLP. Despite the 
wide range of approaches investigated (Kilgar- 
rift and Rosenzweig, 2000) and the large effort 
devoted to tackling this problem, to date, no 
large-scale broad-coverage and highly accurate 
WSD system has been built. 
One of the most successful current lines of 
research is the corpus-based approach, in which 
statistical or Machine Learning (M L) algorithms 
have been applied to learn statistical models 
or classifiers from corpora in order to per- 
* This research has been partially funded by the Spanish 
Research Department (CICYT's project TIC98-0423- 
C06), by the EU Commission (NAMIC I8T-1999-12392), 
and by the Catalan Research Department (CIRIT's 
consolidated research group 1999SGR-150 and CIRIT's 
grant 1999FI 00773). 
form WSD. Generally, supervised approaches 
(those that learn from previously semantically 
annotated corpora) have obtained better esults 
than unsupervised methods on small sets of se- 
lected ambiguous words, or artificial pseudo- 
words. Many standard M L algorithms for su- 
pervised learning have been applied, such as: 
Decision Lists (Yarowsky, 1994; Agirre and 
Martinez, 2000), Neural Networks (Towell and 
Voorhees, 1998), Bayesian learning (Bruce and 
Wiebe, 1999), Exemplar-based learning (Ng, 
1997), Boosting (Escudero et al, 2000a), etc. 
Further, in (Mooney, 1996) some of the previ- 
ous methods are compared jointly with Decision 
Trees and Rule Induction algorithms, on a very 
restricted omain. 
Although some published works include the 
comparison between some alternative algo- 
rithms (Mooney, 1996; Ng, 1997; Escudero et 
al., 2000a; Escudero et al, 2000b), none of 
them addresses the issue of the portability of 
supervised ML algorithms for WSD, i.e., testing 
whether the accuracy of a system trained on 
a certain corpus can be extrapolated to other 
corpora or not. We think that the study of the 
domain dependence of WSD -- in the style of 
other studies devoted to parsing (Sekine, 1997; 
Ratnaparkhi, 1999)-- is needed to assure the 
validity of the supervised approach, and to de- 
termine to which extent a tuning pre-process i
necessary to make real WSD systems portable. 
In this direction, this work compares five differ- 
ent M L algorithms and explores their portability 
and tuning ability by training and testing them 
on different corpora. 
2 Learn ing  A lgor i thms Tested  
Naive-Bayes (NB). Naive Bayes is intended 
as a simple representative of statistical learning 
methods. It has been used in its most classi- 
31 
cal setting (Duda and Hart, 1973). That is, 
assuming the independence of features, it clas- 
sifies a new example by assigning the class that 
maximizes the conditional probability of the 
class given the observed sequence of features 
of that example. Model probabilities are esti- 
mated during the training process using relative 
frequencies. To avoid the effect of zero counts, a 
very simple smoothing technique has been used, 
which was proposed in (Ng, 1997). 
Despite its simplicity, Naive Bayes is claimed 
to obtain state-of-the-art accuracy on super- 
vised WSD in many papers (Mooney, 1996; Ng, 
1997; Leacock et al, 1998). 
Exemplar -based  Classif ier (EB). In exem- 
plar, instance, or memory-based learning (Aha 
et al, 1991) no generalization of training ex- 
amples is performed. Instead, the examples are 
simply stored in memory and the classification 
of new examples is based on the most similar 
stored exemplars. In our implementation, all
examples are kept in memory and the classifica- 
tion is based on a k-NN (Nearest-Neighbours) 
algorithm using Hamming distance to measure 
closeness. For k's greater than 1, the resulting 
sense is the weighted majority sense of the k 
nearest neighbours --where each example votes 
its sense with a strength proportional to its 
closeness to the test example. 
Exemplar-based learning is said to be the 
best option for WSD (Ng, 1997). Other au- 
thors (Daelemans et al, 1999) point out that 
exemplar-based methods tend to be superior in 
language learning problems because they do not 
forget exceptions. 
The SNoW Arch i tec ture  (SN). SNoWis a 
Sparse Network of linear separators which uti- 
lizes the Winnow learning algorithm 1. In the 
SNo W architecture there is a winnow node for 
each class, which learns to separate that class 
from all the rest. During training, which is per- 
formed in an on-line fashion, each example is 
considered a positive example for the winnow 
node associated to its class and a negative x- 
ample for all the others. A key point that allows 
a fast learning is that the winnow nodes are not 
connected to all features but only to those that 
1The Winnow algorithm (Littlestone, 1988) consists 
of a linear threshold algorithm with multiplicative weight 
updating for 2-class problems. 
are "relevant" for their class. When classify- 
ing a new example, SNo W is similar to a neural 
network which takes the input features and out- 
puts the class with the highest activation. Our 
implementation f SNo W for WSD is explained 
in (Escudero et al, 2000c). 
SNoW is proven to perform very well in 
high dimensional NLP problems, where both the 
training examples and the target function reside 
very sparsely in the feature space (Roth, 1998), 
e.g: context-sensitive spelling correction, POS 
tagging, PP-attachment disambiguation, etc. 
Decis ion Lists (DL). In this setting, a Deci- 
sion List is a list of features extracted from the 
training examples and sorted by a log-likelihood 
measure. This measure stimates how strong a 
particular feature is as an indicator of a specific 
sense (Yarowsky, 1994). When testing, the deci- 
sion list is checked in order and the feature with 
the highest weight that matches the test exam- 
ple is used to select the winning word sense. 
Thus, only the single most reliable piece of ev- 
idence is used to perform disambiguation. Re- 
garding the details of implementation (smooth- 
ing, pruning of the decision list, etc.) we have 
followed (Agirre and Martinez, 2000). 
Decision Lists were one of the most success- 
ful systems on the 1st Senseval competition for 
WSD (Kilgarriff and Rosenzweig, 2000). 
LazyBoost ing  (LB). The main idea of boost- 
ing algorithms is to combine many simple and 
moderately accurate hypotheses (weak classi- 
fiers) into a single, highly accurate classifier. 
The weak classifiers are trained sequentially 
and, conceptually, each of them is trained on the 
examples which were most difficult to classify by 
the preceding weak classifiers. These weak hy- 
potheses are then linearly combined into a single 
rule called the combined hypothesis. 
Schapire and Singer's real AdaBoost.MH al- 
gorithm for multiclass multi-label classifica- 
tion (Schapire and Singer, 1999) has been used. 
It constructs a combination of very simple 
weak hypotheses that test the value of a single 
boolean predicate and make a real-valued pre- 
diction based on that value. LazyBoosting (Es- 
cudero et al, 2000a) is a simple modification 
of the AdaBoost.MH algorithm, which consists 
in reducing the feature space that is explored 
when learning each weak classifier. This mod- 
ification significantly increases the efficiency of 
32 
the learning process with no loss in accuracy. 
3 Set t ing  
A number of comparative experiments has been 
carried out on a subset of 21 highly ambiguous 
words of the DSO corpus, which is a semanti- 
cally annotated English corpus collected by Ng 
and colleagues (Ng and Lee, 1996). Each word 
is treated as a different classification problem. 
The 21 words comprise 13 nouns (age, art, body, 
car, child, cost, head, interest, line, point, state, 
thing, work) and 8 verbs (become, fall, grow, lose, 
set, speak, strike, tell), which frequently appear 
in the WSD literature. The average number of 
senses per word is close to 10 and the number 
of training examples is around 1,000. 
The DSO corpus contains entences from two 
different corpora, namely Wall Street Journal 
(WSJ) and Brown Corpus (BC). Therefore, it is 
easy to perform experiments about the porta- 
bility of systems by training them on the WSJ 
part (A part, hereinafter) and testing them on 
the BC part (B part, hereinafter), or vice-versa. 
Two kinds of information are used to train 
classifiers: local and topical context. Let 
... " be ~ W-3 W--2 W--1 W W-i_ 1 W+2 W+3. . .  
the context of consecutive words around the 
word w to be disambiguated, and P?i ( -3  < 
i < 3) be the part-of-speech tag of word 
w?i. Attributes referring to local context 
are the following 15: P-3, P-2, P- l ,  P+I, 
P+2, P+3, w- l ,  W-t-1 , (W-2, W-1), (W-i,W+I), 
(W+I ,W+2) ,  (W-3,  W--2, W--1), (W-2, W- i ,W+I ) ,  
(W--l, W+i , W+2), and (W+l, w+2, w+3), where 
the last seven correspond to collocations of two 
and three consecutive words. The topical con- 
text is formed by c l , . . . ,  Cm, which stand for the 
unordered set of open class words appearing in 
the sentence 2. Details about how the different 
algorithms translate this information into fea- 
tures can be found in (Escudero et al, 2000c). 
4 Compar ing  the  five approaches  
The five algorithms, jointly with a naive Most- 
Frequent-sense Classifier (MFC), have been 
tested, by 10-fold cross validation, on 7 different 
combinations of training-test sets 3. Accuracy 
2This set of attributes corresponds to that used in (Ng 
and Lee, 1996), with the exception of the morphology of 
the target word and the verb-object syntactic relation. 
3The combinations of training-test sets are called: 
A+B-A+B, A-I-B-A, A+B-B, A-A, B-B, A-B, and B-A, 
figures, micro-averaged over the 21 words and 
over the ten folds, are reported in table 1. The 
comparison leads to the following conclusions: 
As expected, the five algorithms ignificantly 
outperform the baseline M FC classifier. Among 
them, three groups can be observed: Ni3, DL, 
and SN perform similarly; LB outperforms all 
the other algorithms in all experiments; and EB 
is somewhere in between. The difference be- 
tween \[B and the rest is statistically significant 
in all cases except when comparing \[B to the EB 
approach in the case marked with an asterisk 4. 
Extremely poor results are observed when 
testing the portability of the systems. Restrict- 
ing to LB results, it can be observed that the 
accuracy obtained in A-B is 47.1%, while the 
accuracy in B-B (which can be considered an 
upper bound for LB in B corpus) is 59.0%, that 
is, that there is a difference of 12 points. Fur- 
thermore, 47.1% is only slightly better than the 
most frequent sense in corpus B, 45.5%. 
Apart from accuracy figures, the comparison 
between the predictions made by the five meth- 
ods on the test sets provides interesting infor- 
mation about the relative behaviour of the algo- 
rithms. Table 2 shows the agreement rates and 
the Kappa statistics 5 between all pairs of meth- 
ods in the A+B-A+B experiment. Note that 
'DSO' stands for the annotation of DSO corpus, 
which is taken as the correct one. 
It can be observed that N B obtains the most 
similar results with regard to M FC in agreement 
and Kappa values. The agreement ratio is 74%, 
that is, almost 3 out of 4 times it predicts the 
most frequent sense. On the other extreme, LB 
obtains the most similar results with regard to 
DSO in agreement and Kappa values, and it has 
the least similar with regard to M FC, suggesting 
respectively. In this notation, the training set is placed 
on the left hand side of symbol "-", while the test set 
is on the right hand side. For instance, A-B means that 
the training set is corpus A and the test set is corpus B. 
The symbol "+" stands for set union. 
4Statistical tests of significance applied: McNemar's 
test and 10-fold cross-validation paired Student's t-test 
at a confidence value of 95% (Dietterich, 1998). 
~The Kappa statistic (Cohen, 1960) is a better mea- 
sure of inter-annotator agreement which reduces the ef- 
fect of chance agreement. It has been used for measur- 
ing inter-annotator agreement during the construction 
of semantic annotated corpora (V~ronis, 1998; Ng et al, 
1999). A Kappa value of 1 indicates perfect agreement, 
while 0.8 is considered as indicating ood agreement. 
33 
Accuracy (%) 
LazyBoosting 
A+B-A+B A+B-A A+B-B A-A B-B 
MFC 46.55?o.71 53.90?2.ol 39.21?i.9o 55.94?Mo 45.52?1.27 
Naive Bayes 61.55?1.o4 67.25?1.o7 55.85?1.81 65.86?1.11 56.80?1.12 
Decision Lists 61.58?o.98 67.64?0.94 55.53?1.85 67.57?1.44 56.56?1.59 
SNoW 60.92?1.o9 65.57?1.33 56.28?1.1o 67.12?1.16 56.13?1.23 
Exemplar-based 63.01?o.93 69.08?1.66 56.97?1.22 68.98?1.o6 57.36?1.68 
66.32?1.34 71.79?1.51 60.85~L81 71,26?1.i5 58.96?1.86 
A-B B-A 
36.40 38.71 
41.38 47.66 
43.01 48.83 
44.07 49.76 
45.32 51.13 
47.10 51.99" 
Table 1: Accuracy results (=h standard eviation) of the methods on all training-test combinations 
A+B-A+B 
DSO MFC NB EB SN DL LB 
DSO - -  46.6 61.6 63.0 60.9 61.6 66.3 
MFC -0.19 --  73.9 60.0 55.9 64.9 54.9 
NB 0.24 -0.09 --  76.3 74.5 76.8 71.4 
EB 0.36 -0.15 0.44 - -  69.6 70.7 72.5 
SN 0.36 -0.17 0.44 0.44 - -  67.5 69.0 
DL 0.32 -0.13 0.40 0.41 0.38 --  69.9 
LB 0.44 -0.17 0.37 0.50 0.46 0.42 - -  
Table 2: Kappa statistic (below diagonal) and 
% of agreement (above diagonal) between all 
methods in the A+B-A+B experiment 
that LB is the algorithm that better learns the 
behaviour of the DSO examples. 
In absolute terms, the Kappa values are very 
low. But, as it is suggested in (Vdronis, 1998), 
evaluation measures hould be computed rela- 
tive to the agreement between the human an- 
notators of the corpus and not to a theoreti- 
cal 100%. It seems pointless to expect more 
agreement between the system and the refer- 
ence corpus than between the annotators them- 
selves. Contrary to the intuition that the agree- 
ment between human annotators should be very 
high in the WSD task, some papers report sur- 
prisingly low figures. For instance, (Ng et al, 
1999) reports an accuracy rate of 56.7% and a 
Kappa value of 0.317 when comparing the anno- 
tation of a subset of the DSO corpus performed 
by two independent research groups. From this 
perspective, the Kappa value of 0.44 achieved 
by LB in A+B-A+B could be considered an ex- 
cellent result. Unfortunately, the subset of the 
\[:)SO corpus studied by (Ng et al, 1999) and 
that used in this report are not the same and, 
thus, a direct comparison is not possible. 
4.1 About  the  tun ing  to new domains  
This experiment explores the effect of a sim- 
ple tuning process consisting in adding to the 
original training set A a relatively small sample 
of manually sense-tagged examples of the new 
domain B. The size of this supervised portion 
varies from 10% to 50% of the available corpus 
in steps of 10% (the remaining 50% is kept for 
testing) 6. This experiment will be referred to 
as A+%B-B T. In order to determine to which 
extent the original training set contributes to 
accurately disambiguating in the new domain, 
we also calculate the results for %B-B, that is, 
using only the tuning corpus for training. 
Figure 1 graphically presents the results ob- 
tained by all methods. Each plot contains the 
A+%B-B and %B-B curves, and the straight 
lines corresponding to the lower bound MFC, 
and to the upper bounds B-B and A+B-B. 
As expected, the accuracy of all methods 
grows (towards the upper bound) as more tun- 
ing corpus is added to the training set. How- 
ever, the relation between A+%B-B and %B-B 
reveals some interesting facts. In plots (c) and 
(d), the contribution of the original training cor- 
pus is null, while in plots (a) and (b), a degrada- 
tion onthe  accuracy is observed. Summarizing, 
these results suggest hat for NB, DL, SN, and 
EB methods it is not worth keeping the original 
training examples. Instead, a better (but dis- 
appointing) strategy would be simply using the 
tuning corpus. However, this is not the situa- 
tion of LB - -plot  (d) - -  for which a moderate, 
but consistent, improvement of accuracy is ob- 
served when retaining the original training set. 
6Tuning examples can be weighted more highly than 
the training examples to force the learning algorithm to 
adapt more quickly to the new corpus. Some experi- 
ments in this direction revealed that slightly better e- 
sults can be obtained, though the improvement was not 
statistically significant. 
7The converse xperiment B-F%A-A is not reported 
in this paper due to space limitations. Results can be 
found in (Escudero et al, 2000c). 
34 
58 
56 
54 
g 52 
al 
~ 50  
~ 46 
44 
42  
40  
(a) Naive Bayes 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  M~S o 
........................................................... ~ ................. B...B . . . . .  
A+B-B ........ 
A+%B-B ~-  
%B-B . . . . .  .. 
56 
~ 54  
o 52 
~ 48 
46 
44  
58  
56 
54 
16 
44  
42  
(b) Decision Lists 
. . . . . . . . . . . . . . . . . . .  G . . . . . . . . . . . . . . . . . . .  o . . . . . . . . . . . . . . . . . . .  ~ . . . . . . . . . . . .  AYS:B : : i : : :  .... 
A+%B-B 
%B-B . . . . .  ' 
, , , , , 
5 10 15 20 25 30 35 40 45 50 5 10 15 
(d )  SNoW 
58 
E::E::E.~:E.~:E.~:EZ::Z~:E.~:Z.Z..~YE::LL:::Y.:~.'S::47724:L:; 
B-B . . . . .  
Zoi~:~ '?~ 
5 10 15 20 25 30 35 40 45 50 
0 20 25 30 35 40 45 50 
62 
60 
58 
~ 56 
~ 52 
48 
46  
44  
(c) Exemplar Based 
58 
MFS 
56 =::=:==::=Q:~=:::=:==:::a=:====::==~=::=:::=:=:Q-~,~=--~---~ A+B-B ........ 
A+%B-B 
54 %B-B ....... 
52 
50 ..... ~ 
46  / o  , , , 
' ' ' 44  ' ' ' ' ' ' ' ' ' 
5 10 15 20 25 30 35 40 45 50 
(e) Lazygoosting 
.................. o................... ~ ................... ~ ................ MF$- -~- -  
B-B ...... 
...................................................... A~'B-B . . . . . . .  
A+%B-B ~- -  
%B-B . . . . .  
/ / 
i / 
5 10 15 20 25 30 35 40 45 50 
Figure 1: Results of the tuning experiment 
We observed that part of the poor results 
obtained is explained by: 1) corpus A and 
B have a very different distribution of senses, 
and, therefore, different a-priori biases; further- 
more, 2) examples of corpus A and B contain 
different information and, therefore, the learn- 
ing algorithms acquire different (and non inter- 
changeable) classification clues from both cor- 
pora. The study of the rules acquired by Lazy- 
Boosting from WSJ and BC helped understand- 
ing the differences between corpora. On the one 
hand, the type of features used in the rules was 
significantly different between corpora and, ad- 
ditionally, there were very few rules that applied 
to both sets. On the other hand, the sign of the 
prediction of many of these common rules was 
somewhat contradictory between corpora. See 
(Escudero et al, 2000c) for details. 
4.2 About  the tra in ing data qual i ty 
The observation of the rules acquired by Lazy- 
Boosting could also help improving data quality 
in a semi-supervised fashion. It is known that 
mislabelled examples resulting from annotation 
errors tend to be hard examples to classify cor- 
rectly and, therefore, tend to have large weights 
in the final distribution. This observation al- 
lows both to identify the noisy examples and 
use LazyBoosting as a way to improve the train- 
ing corpus. 
A preliminary experiment has been carried 
out in this direction by studying the rules ac- 
quired by LazyBoosting from the training ex- 
amples of the word state. The manual revi- 
sion, by four different people, of the 50 high- 
est scored rules, allowed us to identify 28 noisy 
training examples. 11 of them were clear tag- 
ging errors, and the remaining 17 were not co- 
herently tagged and very difficult to judge, since 
the four annotators achieved systematic dis- 
agreement (probably due to the extremely fine 
grained sense definitions involved in these ex- 
amples). 
5 Conc lus ions  
This work reports a comparative study of five 
ML algorithms for WSD, and provides some re- 
sults on cross corpora evaluation and domain 
re-tuning. 
Regarding portability, it seems that the per- 
formance of supervised sense taggers is not 
guaranteed when moving from one domain to 
another (e.g. from a balanced corpus, such 
as BC, to an economic domain, such as WSJ). 
35 
These results imply that some kind of adap- 
tation is required for cross-corpus application. 
Consequently, it is our belief that a number of 
issues regarding portability, tuning, knowledge 
acquisition, etc., should be thoroughly studied 
before stating that the supervised M k paradigm 
is able to resolve a realistic WSD problem. 
Regarding the ML algorithms tested, kazy- 
Boosting emerges as the best option, since 
it outperforms the other four state-of-the-art 
methods in all experiments. Furthermore, this 
algorithm shows better properties when tuned 
to new domains. Future work is planned for 
an extensive valuation of kazyBoosting on the 
WSD task. This would include taking into ac- 
count additional/alternative attributes, learn- 
ing curves, testing the algorithm on other cor- 
pora, etc. 
Re ferences  
E. Agirre and D. Martinez. 2000. Decision Lists and 
Automatic Word Sense Disambiguation. In Pro- 
ceedings of the COLING Workshop on Semantic 
Annotation and Intelligent Content. 
D. Aha, D. Kibler, and M. Albert. 1991. Instance- 
based Learning Algorithms. Machine Learning, 
7:37-66. 
R. F. Bruce and J. M. Wiebe. 1999. Decomposable 
Modeling in Natural Language Processing. Com- 
putational Linguistics, 25(2):195-207. 
J. Cohen. 1960. A Coefficient of Agreement for 
Nominal Scales. Journal of Educational and Psy- 
chological Measurement, 20:37-46. 
W. Daelemans, A. van den Bosch, and J. Zavrel. 
1999. Forgetting Exceptions is Harmful in Lan- 
guage Learning. Machine Learning, 34:11-41. 
T. G. Dietterich. 1998. Approximate Statistical 
Tests for Comparing Supervised Classification 
Learning Algorithms. Neural Computation, 10(7). 
R. O. Duda and P. E. Hart. 1973. Pattern Classifi- 
cation and Scene Analysis. Wiley ~: Sons. 
G. Escudero, L. M~rquez, and G. Rigau. 2000a. 
Boosting Applied to Word Sense Disambiguation. 
In Proceedings of the 12th European Conference 
on Machine Learning, ECML. 
G. Escudero, L. M~rquez, and G. Rigau. 2000b. 
Naive Bayes and Exemplar-Based Approaches to 
Word Sense Disambiguation Revisited. In Pro- 
ceedings of the 14th European Conference on Ar- 
tificial Intelligence, ECAL 
G. Escudero, L. M~rquez, and G. Rigau. 2000c. On 
the Portability and Tuning of Supervised Word 
Sense Disambiguation Systems. Research Report 
LSI-00-30-R, Software Department (LSI). Techni- 
cal University of Catalonia (UPC). 
N. Ide and J. V@ronis. 1998. Introduction to the 
Special Issue on Word Sense Disambiguation: 
The State of the Art. Computational Linguistics, 
24(1):1-40. 
A. Kilgarriff and J. Rosenzweig. 2000. English SEN- 
SEVAL: Report and Results. In Proceedings of the 
2nd International Conference on Language Re- 
sources and Evaluation, LREC. 
C. Leacock, M. Chodorow, and G. A. Miller. 1998. 
Using Corpus Statistics and WordNet Relations 
for Sense Identification. Computational Linguis- 
tics, 24(1):147-166. 
N. Littlestone. 1988. Learning Quickly when Ir- 
relevant Attributes Abound. Machine Learning, 
2:285-318. 
R. J. Mooney. 1996. Comparative Experiments on 
Disambiguating Word Senses: An Illustration of 
the Role of Bias in Machine Learning. In Proceed- 
ings of the 1st Conference on Empirical Methods 
in Natural Language Processing, EMNLP. 
H. T. Ng and H. B. Lee. 1996. Integrating Multiple 
Knowledge Sources to Disambiguate Word Senses: 
An Exemplar-based Approach. In Proceedings of 
the 3~th Annual Meeting of the ACL. 
H. T. Ng, C. Lim, and S. Foo. 1999. A Case Study 
on Inter-Annotator Agreement for Word Sense 
Disambiguation. In Procs. of the ACL SIGLEX 
Workshop: Standardizing Lexical Resources. 
H. T. Ng. 1997. Exemplar-Base Word Sense Disam- 
biguation: Some Recent Improvements. In Procs. 
of the 2nd Conference on Empirical Methods in 
Natural Language Processing, EMNLP. 
A. Ratnaparkhi. 1999. Learning to Parse Natural 
Language with Maximum Entropy Models. Ma- 
chine Learning, 34:151-175. 
D. Roth. 1998. Learning to Resolve Natural Lan- 
guage Ambiguities: A Unified Approach. In Pro- 
ceedings of the National Conference on Artificial 
Intelligence, AAAI  '98. 
R. E. Schapire and Y. Singer. 1999. Improved 
Boosting Algorithms Using Confidence-rated Pre- 
dictions. Machine Learning, 37(3):297-336. 
S. Sekine. 1997. The Domain Dependence of Pars- 
ing. In Proceedings of the 5th Conference on Ap- 
plied Natural Language Processing, ANLP. 
G. Towell and E. M. Voorhees. 1998. Disambiguat- 
ing Highly Ambiguous Words. Computational 
Linguistics, 24(1):125-146. 
J. V@ronis. 1998. A study of polysemy judgements 
and inter-annotator agreement. In Programme 
and advanced papers of the Senseval workshop, 
Herstmonceux Castle, England. 
D. Yarowsky. 1994. Decision Lists for Lexical Ambi- 
guity Resolution: Application to Accent Restora- 
tion in Spanish and French. In Proceedings of the 
32nd Annual Meeting of the ACL. 
36 
An Empirical Study of the Domain Dependence of Supervised 
Word Sense Disambiguation Systems* 
Gerard  Escudero ,  L lu is  M~trquez~ and German R igau  
TALP Research Center. LSI Department. Universitat Polit~cnica de Catalunya (UPC) 
Jordi Girona Salgado 1-3. E-08034 Barcelona. Catalonia 
{escudero, lluism, g. rigau}@isi, upc. es 
Abst ract  
This paper describes a set of experiments car- 
ried out to explore the domain dependence 
of alternative supervised Word Sense Disam- 
biguation algorithms. The aim of the work is 
threefold: studying the performance of these 
algorithms when tested on a different cor- 
pus from that they were trained on; explor- 
ing their ability to tune to new domains, 
and demonstrating empirically that the Lazy- 
Boosting algorithm outperforms tate-of-the- 
art supervised WSD algorithms in both previ- 
ous situations. 
Keywords:  Cross-corpus evaluation of Ni_P 
systems, Word Sense Disambiguation, Super- 
vised Machine Learning 
1 In t roduct ion  
Word Sense Disambiguation (WSD) is the 
problem of assigning the appropriate meaning 
(sense) to a given word in a text or discourse. 
Resolving the ambiguity of words is a central 
problem for large scale language understand- 
ing applications and their associate tasks (Ide 
and V4ronis, 1998), e.g., machine transla- 
tion, information retrieval, reference resolu- 
tion, parsing, etc. 
WSD is one of the most important open 
problems in NLP. Despite the wide range of 
approaches investigated and the large effort 
devoted to tackle this problem, to date, no 
large-scale broad-coverage and highly accu- 
rate WSD system has been built --see the 
main conclusions of the first edition of Sen- 
sEval (Kilgarriff and Rosenzweig, 2000). 
One of the most successful current lines 
of research is the corpus-based approach in 
" This research has been partially funded by the Span- 
ish Research Department (CICYT's project TIC98- 
0423-C06). by the EU Commission (NAMIC IST- 
1999-12392), and by the Catalan Research Depart- 
ment (CIRIT's consolidated research group 1999SGR- 
150 and CIRIT's grant 1999FI 00773). 
which statistical or Machine Learning (ML) al- 
gorithms are applied to learn statistical mod- 
els or classifiers from corpora in order to per- 
form WSD. Generally, supervised approaches 1 
have obtained better results than unsuper- 
vised methods on small sets of selected am- 
biguous words, or artificial pseudo-words. 
Many standard M L algorithms for supervised 
learning have been applied, such as: Decision 
Lists (?arowsky, 1994; Agirre and Martinez, 
2000), Neural Networks (Towell and Voorhees, 
1998), Bayesian learning (Bruce and Wiebe, 
1999), Exemplar-Based learning (Ng, 1997a; 
Fujii et al, 1998), Boosting (Escudero et al, 
2000a), etc. Unfortunately, there have been 
very few direct comparisons between alterna- 
tive methods for WSD. 
In general, supervised learning presumes 
that the training examples are somehow re- 
flective of the task that will be performed by 
the trainee on other data. Consequently, the 
performance of such systems is commonly es- 
timated by testing the algorithm on a separate 
part of the set of training examples (say 10- 
20% of them), or by N-fold cross-validation, 
in which the set of examples i partitioned into 
N disjoint sets (or folds), and the training- 
test procedure is repeated N times using all 
combinations of N-1  folds for training and 1 
fold for testing. In both cases, test examples 
are different from those used for training, but 
they belong to the same corpus, and, there- 
fore, they are expected to be quite similar. 
Although this methodology could be valid 
for certain NLP problems, such as English 
Part-of-Speech tagging, we think that there 
exists reasonable vidence to say that, in 
WSD, accuracy results cannot be simply ex- 
trapolated to other domains (contrary to the 
opinion of other authors (Ng, 1997b)): On the 
aSupervised approaches, also known as data-driven 
or corpus-dmven, are those that learn from a previ- 
ously semantically annotated corpus. 
172 
one hand, WSD is very dependant to the do- 
main of application (Gale et al, 1992b) --see 
also (Ng and Lee, 1996; Ng, 1997a), in which 
quite different accuracy figures are obtained 
when testing an exemplar-based WSD classi- 
fier on two different corpora. Oi1 the other 
hand, it does not seem reasonable to think 
that the training material is large and repre- 
sentative nough to cover "all" potential types 
of examples. 
To date, a thorough study of the domain 
dependence of WSD - - in  the style of other 
studies devoted to parsing (Sekine, 1997)-- 
has not been carried out. We think that such 
an study is needed to assess the validity of 
the supervised approach, and to determine to 
which extent a tuning process is necessary to 
make real WSD systems portable. In order 
to corroborate the previous hypotheses, this 
paper explores the portability and tuning of 
four different ML algorithms (previously ap- 
plied to WSD) by training and testing them 
on different corpora. 
Additionally, supervised methods suffer 
from the "knowledge acquisition bottle- 
neck" (Gale et al, 1992a). (Ng, 1997b) esti- 
mates that the manual annotation effort nec- 
essary to build a broad coverage semantically 
annotated English corpus is about 16 person- 
years. This overhead for supervision could be 
much greater if a costly tuning procedure is 
required before applying any existing system 
to each new domain. 
Due to this fact, recent works have focused 
on reducing the acquisition cost as well as the 
need for supervision i  corpus-based methods. 
It is our belief that the research by (Leacock et 
al., 1998; Mihalcea and Moldovan, 1999) 2 pro- 
vide enough evidence towards the "opening" 
of the bottleneck in the near future. For that 
reason, it is worth further investigating the 
robustness and portability of existing super- 
vised ML methods to better resolve the WSD 
problem. 
It is important o note that the focus of 
this work will be on the empirical cross- 
corpus evaluation of several M L supervised al- 
gorithms. Other important issues, such as: 
selecting the best attribute set, discussing an 
appropriate definition of senses for the task, 
etc., are not addressed in this paper. 
eIn the line of using lexical resources and search en- 
gunes to automatically collect training examples from 
large text collections or Internet. 
This paper is organized as follows: Section 2 
presents the four ML algorithms compared. 
In section 3 the setting is presented in de- 
tail, including the corpora and the experimen- 
tal methodology used. Section 4 reports the 
experiments carried out and the results ob- 
tained. Finally, section 5 concludes and out- 
lines some lines for further esearch. 
2 Learn ing  A lgor i thms Tested  
2.1 Naive-Bayes (NB) 
Naive Bayes is intended as a simple represen- 
tative of statistical learning methods. It has 
been used in its most classical setting (Duda 
and Hart, 1973). That is, assuming indepen- 
dence of features, it classifies a new example 
by assigning the class that maximizes the con- 
ditional probability of the class given the ob- 
served sequence of features of that example. 
Model probabilities are estimated uring 
training process using relative frequencies. To 
avoid the effect of zero counts when esti- 
mating probabilities, a very simple smooth- 
ing technique has been used, which was pro- 
posed in (Ng, 1997a). Despite its simplicity, 
Naive Bayes is claimed to obtain state-of-the- 
art accuracy on supervised WSD in many pa- 
pers (Mooney, 1996; Ng, 1997a; Leacock et 
al., 1998). 
2.2 Exemplar -based  Classif ier (EB) 
In Exemplar-based learning (Aha et al, 1991) 
no generalization of training examples is per- 
formed. Instead, the examples are stored 
in memory and the classification of new ex- 
amples is based on the classes of the most 
similar stored examples. In our implemen- 
tation, all examples are kept in memory and 
the classification of a new example is based 
on a k-NN (Nearest-Neighbours) algorithm 
using Hamming distance 3 to measure close- 
ness (in doing so, all examples are examined). 
For k's greater than 1, the resulting sense is 
the weighted majority sense of the k near- 
est neighbours --where each example votes its 
sense with a strength proportional to its close- 
ness to the test example. 
In the experiments explained in section 4, 
the EB algorithm is run several times using 
different number of nearest neighbours (1, 3, 
SAlthough the use of MVDM metric (Cost and 
Salzberg, 1993) could lead to better results, current 
implementations have prohivitive computational over- 
heads(Escudero et al, 2000b) 
173 
5, 7, 10, 15, 20 and 25) and the results corre- 
sponding to the best choice are reported 4.
Exemplar-based learning is said to be the 
best option for VSD (Ng, 1997a). Other au- 
thors (Daelemans et al, 1999) point out that 
exemplar-based methods tend to be superior 
in language learning problems because they 
do not forget exceptions. 
2.3 Snow: A Winnow-based  Classif ier 
Snow stands for Sparse Network Of Winnows, 
and it is intended as a representative of on- 
line learning algorithms. 
The basic component is the Winnow al- 
gorithm (Littlestone, 1988). It consists of a 
linear threshold algorithm with multiplicative 
weight updating for 2-class problems, which 
learns very fast in the presence of many bi- 
nary input features. 
In the Snow architecture there is a winnow 
node for each class, which learns to separate 
that class from all the rest. During training, 
each example is considered a positive xample 
for winnow node associated to its class and 
a negative example for all the rest. A key 
point that allows a fast learning is that the 
winnow nodes are not connected to all features 
but only to those that are "relevant" for their 
class. When classifying a new example, Snow 
is similar to a neural network which takes the 
input features and outputs the class with the 
highest activation. 
Snow is proven to perform very well in 
high dimensional domains, where both, the 
training examples and the target function re- 
side very sparsely in the feature space (Roth, 
1998), e.g: text categorization, context- 
sensitive spelling correction, WSD, etc. 
In this paper, our approach to WSD using 
Snow follows that of (Escudero et al, 2000c). 
2.4 LazyBoost ing  (LB) 
The main idea of boosting algorithms is to 
combine many simple and moderately accu- 
rate hypotheses (called weak classifiers) into 
a single, highly accurate classifier. The weak 
classifiers are trained sequentially and, con- 
ceptually, each of them is trained on the ex- 
amples which were most difficult to classify 
by the preceding weak classifiers. These weak 
4In order to construct a real EB-based system for 
WSD, the k parameter should be estimated by cross- 
validation using only the training set (Ng, 1997a), 
however, in our case, this cross-validation i side the 
cross-validation i volved in the testing process would 
generate a prohibitive overhead. 
hypotheses are then linearly combined into a 
single rule called the combined hypothesis. 
More particularly, the Schapire and Singer's 
real AdaBoost.MH algorithm for multi- 
class multi-label classification (Schapire and 
Singer, to appear) has been used. As in that 
paper, very simple weak hypotheses are used. 
They test the value of a boolean predicate and 
make a real-valued prediction based on that 
value. The predicates used, which are the bi- 
narization of the attributes described in sec- 
tion 3.2, are of the form "f = v", where f is a 
feature and v is a value (e.g: "-r v" p e mus_word 
= hosp i ta l " ) .  Each weak rule uses a single 
feature, and, therefore, they can be seen as 
simple decision trees with one internal node 
(testing the value of a binary feature) and two 
leaves corresponding to the yes/no answers to 
that test. 
LazyBoosting (Escudero et al, 2000a), is a 
simple modification of the AdaBoost.MH al- 
gorithm, which consists of reducing the fea- 
ture space that is explored when learning each 
weak classifier. More specifically, a small pro- 
portion p of attributes are randomly selected 
and the best weak rule is selected only among 
them. The idea behind this method is that 
if the proportion p is not too small, probably 
a sufficiently good rule can be found at each 
iteration. Besides, the chance for a good rule 
to appear in the whole learning process is very 
high. Another important characteristic is that 
no attribute needs to be discarded and, thus, 
the risk of eliminating relevant attributes is 
avoided. The method seems to work quite well 
since no important degradation is observed in 
performance for values of p greater or equal 
to 5% (this may indicate that there are many 
irrelevant or highly dependant attributes in 
the WSD domain). Therefore, this modifica- 
tion significantly increases the efficiency of the 
learning process (empirically, up to 7 times 
faster) with no loss in accuracy. 
3 Set t ing  
3.1 The  DSO Corpus  
The DSO corpus is a semantically annotated 
corpus containing 192,800 occurrences of 121 
nouns and 70 verbs, corresponding to the most 
frequent and ambiguous English words. This 
corpus was collected by Ng and colleagues (Ng 
and Lee, 1996) and it is available from the 
Linguistic Data Consortium (LDC) 5. 
5LDC address: http://www. Idc.upeaa. ed~/ 
174 
The D50 corpus contains sentences from 
two different corpora, namely Wall Street 
Journal (WSJ) and Brown Corpus (BC). 
Therefore, it is easy to perform experiments 
about the portability of alternative systems 
by training them on the WSJ part and testing 
them on the BE part, or vice-versa. Here- 
inafter, the WSJ part of DSO will be referred 
to as corpus A, and the BC part to as corpus B. 
At a word level, we force the number of exam- 
ples of corpus A and B be the same 6 in order 
to have symmetry and allow the comparison 
in both directions. 
From these corpora, a group of 21 words 
which frequently appear in the WSD litera- 
ture has been selected to perform the com- 
parative experiments (each word is treated 
as a different classification problem). These 
words are 13 nouns (age, art, body, car, child, 
cost, head, interest, line, point, state, thing, 
work) and 8 verbs (become, fall, grow, lose, 
set, speak, strike, tell). Table 1 contains in- 
formation about the number of examples, the 
number of senses, and the percentage of the 
most frequent sense (MF5) of these reference 
words, grouped by nouns, verbs, and all 21 
words. 
3.2 At t r ibutes  
Two kinds of information are used to perform 
disambiguation: local and topical context. 
Let "... w-3 w-2 w-1 w W+l w+2 w+3..." 
be the context of consecutive words around 
the word w to be disambiguated, and p?, 
( -3  < i _< 3)be  the part-of-speech tag 
of word w?~. Attributes referring to local 
context are the following 15: P-3, P-2, 
P- l ,  P+i, P+2, P+3, w- l ,  W+l, (W-2,W-1), 
(w-i.w+i), (w+l,w+2), 
(w-2, W-l, w+l), (w-i ,  w+l, w+2), and 
(w+l,w+2, w+3), where the last seven cor- 
respond to collocations of two and three 
consecutive words. 
The topical context is formed by Cl,..., Cm, 
which stand for the unordered set of open class 
words appearing in the sentence 7. 
The four methods tested translate this 
information into features in different ways. 
Snow and LB algorithms require binary fea- 
6This is achieved by ramdomly reducing the size of 
the largest corpus to the size of the smallest. 
7The already described set of attributes contains 
those attributes used in (Ng and Lee, 1996), with the 
exception of the morphology of the target word and 
the verb-object syntactic relation. 
tures. Therefore, local context attributes have 
to be binarized in a preprocess, while the top- 
ical context attributes remain as binary tests 
about the presence/absence of a concrete word 
in the sentence. As a result the number of 
attributes is expanded to several thousands 
(from 1,764 to 9,900 depending on the partic- 
ular word). 
The binary representation of attributes is 
not appropriate for NB and EB algorithms. 
Therefore, the 15 local-context attributes are 
taken straightforwardly. Regarding the binary 
topical-context attributes, we have used the 
variants described in (Escudero et al, 2000b). 
For EB, the topical information is codified as 
a single set-valued attribute (containing all 
words appearing in the sentence) and the cal- 
culation of closeness is modified so as to han- 
dle this type of attribute. For NB, the top- 
ical context is conserved as binary features, 
but when classifying new examples only the 
information of words appearing in the exam- 
ple (positive information) is taken into ac- 
count. In that paper, these variants are called 
positive Exemplar-based (PEB) and positive 
Naive Bayes (PNB), respectively. PNB and 
PEB algorithms are empirically proven to per- 
form much better in terms of accuracy and 
efficiency in the WSD task. 
3.3 Exper imenta l  Methodo logy  
The comparison of algorithms has been per- 
formed in series of controlled experiments us- 
ing exactly the same training and test sets. 
There are 7 combinations of training-test sets 
called: A+B-A+B, A+B-A, A+B-B, A-A, B- 
B, A-B, and B-A, respectively. In this nota- 
tion, the training set is placed at the left hand 
side of symbol "-", while the test set is at the 
right hand side. For instance, A-B means that 
the training set is corpus A and the test set 
is corpus B. The symbol "+" stands for set 
union, therefore A+B-B means that the train- 
ing set is A union B and the test set is B. 
When comparing the performance oftwo al- 
gorithms, two different statistical tests of sig- 
nificance have been apphed depending on the 
case. A-B and B-A combinations represent a 
single training-test experiment. In this cases, 
the McNemar's test of significance is used 
(with a confidence value of: X1,0.952 = 3.842), 
which is proven to be more robust than a sim- 
ple test for the difference of tw0_proportions. 
In the other combinations, a 10-fold cross- 
validation was performed in order to prevent 
175 
nouns 
verbs 
AorB  
examples 
rain 
senses 
min max avg 
A 
i MFS (%) 
min min 
senses 
B 
MFS (%! 
min max max avg max avg max avg avg 
122 714 420 2 24 7.7 37.9 90.7 59.8 3 24 8.8 21.0 87.7 45.3 
101 741 369 4 13 8.9120.8 81.6 49.3 4 14 11.4 28.0 71.7 46.3 
101 741 401 2 24 8.1 J20.8 90.7 56.1 3 24 9.8 21.0 87.7 45.6 
Table 1: Information about the set of 21 words of reference. 
testing on the same material used for training. 
In these cases, accuracy/error rate figures re- 
ported in section 4 are averaged over the re- 
sults of the 10 folds. The associated statistical 
tests of significance is a paired Student's t-test 
with a confidence value of: t9,0.975 = 2.262. 
Information about both statistical tests can 
be found at (Dietterich, 1998). 
4 Exper iments  
4.1 F i rs t  Exper iment  
Table 2 shows the accuracy figures of the four 
methods in all combinations of training and 
test sets . Standard deviation numbers are 
supplied in all cases involving cross valida- 
tion. M FC stands for a Most-Frequent-sense 
Classifier, that is, a naive classifier that learns 
the most frequent sense of the training set 
and uses it to classify all examples of the test 
set. Averaged results are presented for nouns. 
verbs, and overall, and the best results for 
each case are printed in boldface. 
The following conclusions can be drawn: 
? LB outperforms all other methods in 
all cases. Additionally, this superiority 
is statistically significant, except when 
comparing LB to the PEB approach in the 
cases marked with an asterisk. 
? Surprisingly, LB in A+B-A (or A+B-B) 
does not achieve substantial improvement 
to the results of A-A (or B-S) w in  fact, 
the first variation is not statistically sig- 
nificant and the second is only slightly 
significant. That is, the addition of extra 
examples from another domain does not 
necessarily contribute to improve the re- 
sults on the original corpus. This effect is 
also observed in the other methods, spe- 
cially in some cases (e.g. Snow in A+B-A 
vs. A-A) in which the joining of both 
training corpora is even counterproduc- 
tive. 
SThe second and third column correspond to the 
train and test sets used by (Ng and Lee, 1996; Ng, 
1997a) 
? Regarding the portability of the systems, 
very disappointing results are obtained. 
Restricting to \[B results, we observe that 
the accuracy obtained in A-B is 47.1% 
while the accuracy in B-B (which can 
be considered an upper bound for LB in 
B corpus) is 59.0%, that is, a drop of 
12 points. Furthermore, 47.1% is only 
slightly better than the most frequent 
sense in corpus B, 45.5%. The compari- 
son in the reverse direction is even worse: 
a drop from 71.3% (A-A) to 52.0% (B- 
A), which is lower than the most frequent 
sense of corpus A, 55.9%. 
4.2 Second Exper iment  
The previous experiment shows that classi- 
tiers trained on the A corpus do not work well 
on the B corpus, and vice-versa. Therefore, 
it seems that some kind of tuning process is 
necessary to adapt supervised systems to each 
new domain. 
This experiment explores the effect of a sim- 
ple tuning process consisting of adding to the 
original training set a relatively small sarn- 
ple of manually sense tagged examples of the 
new domain. The size of this supervised por- 
tion varies from 10% to 50% of the available 
corpus in steps of 10% (the remaining 50% is 
kept for testing). This set of experiments will 
be referred to as A+%B-B, or conversely, to 
B+%A-A. 
In order to determine to which extent the 
original training set contributes to accurately 
disambiguate in the new domain, we also cal- 
culate the results for %A-A (and %B-B), that 
is, using only the tuning corpus for training. 
Figure 1 graphically presents the results ob- 
tained by all methods. Each plot contains the 
X+%Y-Y and %Y-Y curves, and the straight 
lines corresponding to the lower bound MFC, 
and to the upper bounds Y-Y and X+Y-Y. 
As expected, the accuracy of all methods 
grows (towards the upper bound) as more tun- 
ing corpus is added to the training set. How- 
ever, the relation between X+%Y-Y and %Y- 
Y reveals some interesting facts. In plots 2a, 
176 
nouns 
MFC verbs 
total 
nouns 
PNB verbs 
total  
nouns  
PEB verbs 
total 
nouns  
Snow verbs 
total 
nouns 
LB verbs 
total 
A+B-A+B 
46.59?1.08 
46.49?1.37 
46.55?0.71 
62.29?1.25 
60.18?1.64 
61.55?1.04 
62.66?0.87 
63.67?1.94 
63.01?0.93 
61.24?1.14 
60.35?1.57 
60.92?1.09 
66.00?1.47 
66.91?2.25 
66.32?1.34 
A+B-A 
56.68?2.79 
48.74?1.98 
53.90?2.01 
68.89?0.93 
64.21?2.26 
67.25?1.07 
69.45?1 51 
68.39?3.25 
69.08?1.66 
66.36?1 57 
64.11?2.76 
65.57?1.33 
2.09?1.61 
71.23?2.99 
71.79?1.51 
Accuracy (%) 
A+B-B 
36.49?2.41 
44.23?2.67 
39.21?1.90 
55.69?1.94 
56.14?2.79 
55.85?1.81 
56.09?1.12 
58.58?2.40 
56.97?1.22 
56.11?1.45 
56.58?2.45 
56.28?1.10 
59.92?1.93 
62.58?2.93 
60.85?1.81 
A-A 
59.77?1.44 
48.85?2.09 
55.94?1.10 
66.93?1.44 
63.87?1.80 
65.86?1.11 
69.38?1.24 
68.25?2.84 
68.98?1.06 
68.85?1.36 
63.91?1.51 
67.12?1.16 
71.69?1.54 
70.45?2.14" 
71.26?1.15 
B-B \[ A-B B-A 
45.28?1.81 33.97 39.46 
45.96?2.6O 40.91 37.31 
45.52?1.27 36.40 38.71 
56.17?1.60 36.62 45.99 
57.97?2.86 50.20 50.75 
56.80?1.12 41.38 47.66 
56.17?1.80 42.15 50.53 
59.57?2.86 51.19 52.24 
57.36?1.68 45.32 51.13 
56.55?1.31 42.13 49.96 
55.36?3.27 47.66 49.39 
56.13?1.23 44.07 49.76 
58.33?2.26 43.92 51.28" 
60.14?3.43" 52.99 53.29* 
58.96?1.86 47.10 51.99" 
Table 2: Accuracy results (:i: standard eviation) of the methods on all training-test combina- 
tions 
3a, and lb the contribution of the original 
training corpus is null. Furthermore, in plots 
la, 2b, and 3b a degradation on the accuracy 
performance is observed. Summarizing, these 
six plots show that for Naive Bayes, Exemplar 
Based, and Snow methods it is not worth keep- 
ing the original training examples. Instead, a 
better (but disappointing) strategy would be 
simply using the tuning corpus. 
However, this is not the situation of Lazy- 
Boosting (plots 4a and 4b), for which a mod- 
erate (but consistent) improvement of accu- 
racy is observed when retaining the original 
training set. Therefore, Lazy\[3oosting shows 
again a better behaviour than their competi- 
tors when moving from one domain to an- 
other. 
4.3 Th i rd  Exper iment  
The bad results about portability could be ex- 
plained by, at least, two reasons: 1) Corpus 
A and \[3 have a very different distribution of 
senses, and, therefore, different a-priori bi- 
ases; 2) Examples of corpus A and \[3 con- 
tain different information, and, therefore, the 
learning algorithms acquire different (and non 
interchangeable) classification cues from both 
corpora,. 
The first hypothesis confirmed by observ- 
ing the bar plots of figure 2, which contain the 
distribution of the four most frequent senses 
of some sample words in the corpora A and 
B. respectively. In order to check the second 
hypothesis, two new sense-balanced corpora 
have been generated from the DSO corpus, by 
equilibrating the number of examples of each 
sense between A and B parts. In this way, the 
first difficulty is artificially overrided and the 
algorithms hould be portable if examples of 
both parts are quite similar. 
Table 3 shows the results obtained by Lazy- 
Boosting on these new corpora. 
Regarding portability, we observe a signifi- 
cant accuracy decrease of 7 and 5 points from 
A-A to B-A, and from B-B to A-B, respec- 
tively 9. That is, even when the sazne distri- 
bution of senses is conserved between training 
and test examples, the portability of the su- 
pervised WSD systems is not guaranteed. 
These results imply that examples have to 
be largely different from one corpus to an- 
other. By studying the weak rules generated 
by kazyBoosting in both cases, we could cor- 
roborate this fact. On the one hand, the type 
of features used in the rules were significantly 
different between corpora, and, additionally, 
there were very few rules that apply to both 
sets; On the other hand, the sign of the pre- 
diction of many of these common rules was 
somewhat contradictory between corpora. 
9This loss in accuracy is not as important as m the 
first experiment, due to the simplification provided by 
the balancing ofsense distributions. 
177 
Naive Bayes 
Exemplar Based 
Snow 
LazyBoosting 
58 
56 1 
54 
~?52 
o 
50 
44 
4O 
58 
56 
Af~ 
52 
~o 
~ 48 
46  
58 
56 ' 
54 
o 52 
50 
46 
62 
60 ' 
58  
~o 
48 
46  
4.4 
Test on B corpus 
(la) 
. . . . .  -MF'~ . . . .  
o B,-B 
A+B-B o 
A+%B-B 
%B-B . . . . .  
/ . f "  
5 10 15 20 25 30 35 40 45 50 
(2a) 
. . . . . . .  ? ; . . . .  ; . . . . . .  ~ --:-" ~S-- :~. : : , - -  ~ 
B-B --- 
A+B-B o 
A+%B-B * - -  
%B-B - "u ' -  
/+.- 
J 
5 10 15 20 25 30 355 40 45 50 
(3a) 
MFS 
A+B-B o 
A+%B-B ~- -  
%B-B ==- 
5 10 15 20 25 30 35 40 45 50 
(4a) 
B-B  ~ - -  
A+%B-B . . . .  
%B-B - - - -  
/ /+  / / " "  - 
-/ , /  
.+ 
$+* 
, , = , = , i , , 
5 10 15 20 25 30 35 40 45 50 
72 
70 
68 
~66 
~,64 
62 
60 
58 
56 
54 
72 
70 
68 
66 
64 
62. 
60 
58 
56 
54 
72 
7O 
68 ~66 
60 
58 
56 
54 
72 
7O 
68 
o  .64 
~: eo 
58 
56 
54 
Test on A corpus 
(lb) 
MFS 
A-A - - - -  
B+A-A 
B+%A-A ~ - -  
o o . %,~oA -~- 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
/ 
, , , , , . . . .  
5 10 15 20 25 30 35 40 45 50 
(2b) 
MFS 
A-A~- -  
B+A-A . . . . . . . . . . . . . . . . . . .  ~ . . . . . .  = . -  ~. . , .Z~:~. . .  ~ ~.~ 
%A-A . . . .  
_~. - -  
+ 
+ ? , , , , , , , , , 
5 10 15 20 25 30 35 40 45 50 
(3b) 
MF$ 
A-A . . . .  
B+A-A a 
B+%A-A ~-  
%A-A . . . . .  
j." 
/ 
5 10 15 20 25 30 ,35 40 45 50 
(4b) 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  f~tE~ . . . . .  
A-A . . . .  
B+A=A 
B+%A-A . . . .  
/P  +/"  
+" 1" 
/ 
/ 
/ 
/,' 
, , , , , , , , i 
5 10  15  20  25  30  35  40  45  50  
Figure 1: Results of the tuning experiment 
5 Conc lus ions  and  Fur ther  Work  
This work has pointed out some difficulties 
regarding the portability of supervised WSD 
systems, a very important issue that has been 
paid little attention up to the present. 
According to our experiments, it seems that 
the performance of supervised sense taggers is 
not guaranteed when moving from one domain 
to another (e.g. from a balanced corpus, such 
as BC, to an economic domain, such as WSJ). 
These results implies that some_kind of adap- 
tation is required for cross-corpus application. 
178 
. . . . .  il ii\[ 
"~ head ~a I r l temt  ~o fa l l  oo grow 
mL. 
Figure 2: Distribution of the four most frequent senses for two nouns (head, interest) and two 
verbs (line, state). Black bars = A corpus; Grey bars = B corpus 
nouns 
MFC verbs 
total 
nouns 
LB verbs 
total 
Accuracy (%) 
A+B-A+B A+B-B A-A 
48.75?0.91 
48.22?1 68 
48.55?1 6 
62.82?1.43 
66.82?1.53 
64.35?1.16 
A+B-A 
48.90?1.69 
48.22?1.90 
48.64?1.04 
64.26?2.07 
69.33?2.92 
66.20?2.12 
48.61?0.96 
48.22?3 06 
48.46?1.21 
61.38?2.08 
64.32?3.27 
62.50?1.47 
48.87?1 68 
48.22?1.90 
48.62?1.09 
63.19?1.65 
68.51?2.45 
65.22?1.50 
B-B A-B B-A 
48.61?0.96 48.99 48.99 
48.22?3.06 48.22 48.22 
48.46?1.21 48.70 48.70 
60.65?1.01 53.45 55.27 
63.49?2.27 60.44 62.55 
61.74?1.18 56.12 58.05 
Table 3: Accuracy results (5= standard eviation) of LazyBoosting on the sense-balanced corpora 
Furthermore, these results are in contradic- 
tion with the idea of "robust broad-coverage 
WSD" introduced by (Ng, 1997b), in which a 
supervised system trained on a large enough 
corpora (say a thousand examples per word) 
~hould provide accurate disambiguation on 
any corpora (or, at least significantly better 
than MFS). 
Consequently, it is our belief that a number 
of issues regarding portability, tuning, knowl- 
edge acquisition, etc., should be thoroughly 
studied before stating that the supervised ML 
paradigm is able to resolve a realistic WSD 
problem. 
Regarding the M L algorithms tested, the 
contribution of this work consist of empiri- 
cally demonstrating that the LazyBoosting al- 
gorithm outperforms other three state-of-the- 
art supervised ML methods for WSD. Further- 
more. this algorithm is proven to have better 
properties when is applied to new domains. 
Further work is planned to be done in the 
following directions: 
? Extensively evaluate LazyBoosting on the 
WSD task. This would include tak- 
ing into account additional/alternative 
attributes and testing the algorithm in 
other corpora --specially on sense-tagged 
corpora automatically obtained from In- 
ternet or large text collections using non- 
supervised methods (Leazock et al, 1998; 
Mihalcea and Moldovan, 1999). 
? Since most of the knowledge l arned from 
a domain is not useful when changing 
to a new domain, further investigation is 
needed on tuning strategies, pecially on 
those using non-supervised algorithms. 
? It is known that mislabelled examples re- 
sulting from annotation errors tend to be 
hard examples to classify correctly, and, 
therefore, tend to have large weights in 
the final distribution. This observation 
allows both to identify the noisy exam- 
ples and use LazyBoosting as a way to 
improve data quality. Preliminary exper- 
iments have been already carried out in 
this direction on the DSO corpus. 
? Moreover, the inspection of the rules 
learned by kazyBoosting could provide 
evidence about similar behaviours of a- 
priori different senses. This type of 
knowledge could be useful to perform 
clustering of too fine-grained or artificial 
senses. 
Re ferences  
E. Agirre and D. Martinez. 2000. Decision Lists 
and Automatic Word Sense Disambiguation. In
Proceedings o\] the COLING Workshop on Se- 
mantic Annotation and Intelligent Content 
D. Aha, D. Kibler, and M. Albert. 1991. Instance- 
based Learning Algorithms. Machine Learning, 
7:37-66. 
R. F. Bruce and J. M. Wiebe. 1999. Decompos- 
179 
able Modeling in Natural Language Processing. 
Computatwnal Linguistics. 25(2):195-207. 
S. Cost and S. Salzberg. 1993. A weighted nearest 
neighbor algorithm for learning with symbolic 
features. Machine Learning, 10(1), 57-78. 
W. Daelemans, A. van den Bosch, and J. Zavrel. 
1999. Forgetting Exceptions is Harmful in Lan- 
guage Learning. Machine Learning, 34:11-41. 
T. G. Dietterich. 1998. Approximate Statisti- 
cal Tests for Comparing Supervised Classifi- 
cation Learning Algorithms. Neural Computa- 
tion, 10(7). 
R. O. Duda and P. E. Hart. 1973. Pattern Clas- 
sificatwn and Scene Analysis. Wiley. 
G. Escudero, L. M~rquez, and G. Rigau. 2000a. 
Boosting Applied to Word Sense Disam- 
biguation. In Proceedings of the 12th Euro- 
pean Conference on Machine Learning, ECML, 
Barcelona, Spain. 
G. Escudero. L. M~rquez, and G. Rigau. 2000b. 
Naive Bayes and Exemplar-Based Approaches 
to Word Sense Disambiguation Revisited. In 
To appear in Proceedings of the 14th European 
Conference on Artificial Intelligence, ECAI. 
G. Escudero, L. M~quez, and G. Rigau. 2000c. 
On the Portability and Tuning of Super- 
vised Word Sense Disambiguation Systems. Re- 
search Report LSI-00-30-R, Software Depart- 
ment (LSI). Technical University of Catalonia 
(UPC). 
A. Fujii, K. Inui. T. Tokunaga, and H. Tanaka. 
1998. Selective Sampling for Example-based 
W'ord Sense Disambiguation. Computatwnal 
Linguistics, 24(4):573-598. 
W. Gale, K. W. Church, and D. Yarowsky. 1992a. 
A Method for Disambiguating Word Senses in a 
Large Corpus. Computers and the Humanities, 
26:415-439. 
W. Gale, K. W. Church, and D. Yarowsky. 1992b. 
Estimating Upper and Lower Bounds on the 
Performance of Word Sense Disambiguation. 
In Proceedings of the 30th Annual Meeting of 
the Association for Computational Linguistics. 
ACL. 
N. Ide and J. V@ronis. 1998. Introduction to the 
Special Issue on Word Sense Disambiguation: 
The State of the Art. Computational Linguis- 
tics, 24(1):1-40. 
A. Kilgarriff and J. Rosenzweig. 2000. English 
SENSEVAL: Report and Results. In Proceed- 
ings of the 2nd International Conference on 
Language Resources and Evaluatwn, LREC, 
Athens, Greece. 
C. Leacock, M. Chodorow, and G. A. Miller. 1998. 
Using Corpus Statistics and WordNet Relations 
for Sense Identification. Computatwnal Lin- 
guistwcs, 24(1):147-166. 
N. Littlestone. 1988. Learning Quickly when Irrel- 
evant Attributes Abound. Machine Learning, 
2:285-318. 
R. Mihalcea and I. Moldovan. 1999. An Au- 
tomatic Method for Generating Sense Tagged 
Corpora. In Proceedings of the 16th National 
Conference on Artificial Intelligence. AAAI 
Press. 
R. J. Mooney. 1996. Comparative Experiments 
on Disambiguating Word Senses: An Illustra- 
tion of the Role of Bias in Machine Learning. 
In Proceedings of the 1st Conference on Empir- 
ical Methods m Natural Language Processing, 
EMNLP. 
H. T. Ng and H. B. Lee. 1996. Integrating Multi- 
ple Knowledge Sources to Disambiguate Word 
Sense: An Exemplar-based Approach. In Pro- 
ceedmgs of the 3~th Annual Meeting of the As- 
sociation for Computational Linguistics. ACL. 
H. T. Ng. 1997a. Exemplar-Base Wbrd Sense Dis- 
ambiguation: Some Recent Improvements. In 
Proceedings of the 2nd Conference on Empir- 
zcal Methods in Natural Language Processing, 
EMNLP. 
H. T. Ng. 1997b. Getting Serious about Word 
Sense Disambiguation. In Proceedings of the 
ACL SIGLEX Workshop: Tagging Text with 
Lexical Semantics: Why, what and how?, Wash- 
ington, USA. 
D. Roth. 1998. Learning to Resolve Natural Lan- 
guage Ambiguities: A Unified Approach. In 
Proceedings of the National Conference on Ar- 
tzficial Intelhgence, AAAI 'Y8, July. 
R. E. Schapire and Y. Singer. to appear. Improved 
Boosting Algorithms Using Confidence-rated 
Predictions. Machine Learning. Also appearing 
in Proceedings of the 11th Annual Conference on 
Computatzonal Learning Theory, 1998. 
S. Sekine. 1997. The Domain Dependence ofPars- 
ing. In Proceedings o\] the 5th Conference on 
Applied Natural Language Processing, ANLP, 
Washington DC. ACL. 
G. Towell and E. M. Voorhees. 1998. Disam- 
biguating Highly Ambiguous Words. Computa- 
tional Lingu~stzcs. 24(1):125-146. 
D. Yarowsky. 1994. Decision Lists for Lexical 
Ambiguity Resolution: Application to Accent 
Restoration i  Spanish and French. In Proceed- 
ings of the 32nd Annual Meeting of the Associ- 
ation for Computational Linguistics, pages 88- 
95, Las Cruces, NM. ACL. 
180 
TALP System for the English Lexical Sample Task
Gerard Escudero?, Llu??s Ma`rquez? and German Rigau?
?TALP Research Center. EUETIB. LSI. UPC. escudero@lsi.upc.es
?TALP Research Center. LSI. UPC. lluism@lsi.upc.es
? IXA Group. UPV/EHU. rigau@si.ehu.es
1 Introduction
This paper describes the TALP system on the En-
glish Lexical Sample task of the Senseval-31 event.
The system is fully supervised and relies on a par-
ticular Machine Learning algorithm, namely Sup-
port Vector Machines. It does not use extra exam-
ples than those provided by Senseval-3 organisers,
though it uses external tools and ontologies to ex-
tract part of the representation features.
Three main characteristics have to be pointed out
from the system architecture. The first thing is the
way in which the multiclass classification problem
posed by WSD is addressed using the binary SVM
classifiers. Two different approaches for binarizing
multiclass problems have been tested: one?vs?all
and constraint classification. In a cross-validation
experimental setting the best strategy has been se-
lected at word level. Section 2 is devoted to explain
this issue in detail.
The second characteristic is the rich set of fea-
tures used to represent training and test examples.
Topical and local context features are used as usual,
but also syntactic relations and semantic features in-
dicating the predominant semantic classes in the ex-
ample context are taken into account. A detailed
description of the features is presented in section 3.
And finally, since each word represents a learning
problem with different characteristics, a per?word
feature selection has been applied. This tuning pro-
cess is explained in detail in section 4.
The last two sections discuss the experimental re-
sults (section 5) and present the main conclusions of
the work performed (section 6).
2 Learning Framework
The TALP system belongs to the supervised Ma-
chine Learning family. Its core algorithm is the
Support Vector Machines (SVM) learning algorithm
(Cristianini and Shawe-Taylor, 2000). Given a set
of binary training examples, SVMs find the hy-
perplane that maximizes the margin in a high di-
1http://www.senseval.org
mensional feature space (transformed from the in-
put space through the use of a non-linear function,
and implicitly managed by using the kernel trick),
i.e., the hyperplane that separates with maximal dis-
tance the positive examples from the negatives. This
learning bias has proven to be very effective for pre-
venting overfitting and providing good generalisa-
tion. SVMs have been also widely used in NLP
problems and applications.
One of the problems in using SVM for the WSD
problem is how to binarize the multiclass classifi-
cation problem. The two approximations tested in
the TALP system are the usual one?vs?all and the
recently introduced constraint?classification frame-
work (Har-Peled et al, 2002).
In the one?vs?all approach, the problem is de-
composed into as many binary problems as classes
has the original problem, and one classifier is
trained for each class trying to separate the exam-
ples of that class (positives) from the examples of
all other classes (negatives). This method assumes
the existence of a separator between each class and
the set of all other classes. When classifying a new
example, all binary classifiers predict a class and
the one with highest confidence is selected (winner?
take?all strategy).
2.1 Constraint Classification
Constraint classification (Har-Peled et al, 2002) is
a learning framework that generalises many multi-
class classification and ranking schemes. It consists
of labelling each example with a set of binary con-
straints indicating the relative order between pairs
of classes. For the WSD setting of Senseval-3, we
have one constraint for each correct class (sense)
with each incorrect class, indicating that the clas-
sifier to learn should give highest confidence to the
correct classes than to the negatives. For instance, if
we have 4 possible senses {1, 2, 3, 4} and a training
example with labels 2 and 3, the constraints corre-
sponding to the example are {(2>1), (2>4), (3>1),
and (3>4)}. The aim of the methodology is to learn
a classifier consistent with the partial order defined
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
by the constraints. Note that here we are not as-
suming that perfect separators can be constructed
between each class and the set of all other classes.
Instead, the binary decisions imposed are more con-
servative.
Using Kesler?s construction for multiclass classi-
fication, each training example is expanded into a
set of (longer) binary training examples. Finding
a vector?based separator in this new training set is
equivalent to find a separator for each of the binary
constraints imposed by the problem. The construc-
tion is general, so we can use SVMs directly on the
expanded training set to solve the multiclass prob-
lem. See (Har-Peled et al, 2002) for details.
3 Features
We have divided the features of the system in 4 cat-
egories: local, topical, knowledge-based and syn-
tactic features. First section of table 1 shows the
local features. The basic aim of these features
is to modelize the information of the surrounding
words of the target word. All these features are ex-
tracted from a ?3?word?window centred on the tar-
get word. The features also contain the position of
all its components. To obtain Part?of?Speech and
lemma for each word, we used FreeLing 2. Most
of these features have been doubled for lemma and
word form.
Three types of Topical features are shown in the
second section of table 1. Topical features try to
obtain non?local information from the words of the
context. For each type, two overlapping sets of
redundant topical features are considered: one ex-
tracted from a ?10?word?window and another con-
sidering all the example.
The third section of table 1 presents the
knowledge?based features. These features have
been obtained using the knowledge contained into
the Multilingual Central Repository (MCR) of the
MEANING project3 (Atserias et al, 2004). For each
example, the feature extractor obtains, from each
context, all nouns, all their synsets and their associ-
ated semantic information: Sumo labels, domain la-
bels, WordNet Lexicographic Files, and EuroWord-
Net Top Ontology. We also assign to each label a
weight which depends on the number of labels as-
signed to each noun and their relative frequencies
in the whole WordNet. For each kind of seman-
tic knowledge, summing up all these weights, the
program finally selects those semantic labels with
higher weights.
2http://www.lsi.upc.es/?nlp/freeling
3http://www.lsi.upc.es/?meaning
local feats.
Feat. Description
form form of the target word
locat all part?of?speech / forms / lemmas in
the local context
coll all collocations of two part?of?speech /
forms / lemmas
coll2 all collocations of a form/lemma and a
part?of?speech (and the reverse)
first form/lemma of the first noun / verb /
adjective / adverb to the left/right of the
target word
topical feats.
Feat. Description
topic bag of forms/lemmas
sbig all form/lemma bigrams of the example
comb forms/lemmas of consecutive (or not)
pairs of the open?class?words in the
example
knowledge-based feats.
Feat. Description
f sumo first sumo label
a sumo all sumo labels
f semf first wn semantic file label
a semf all wn semantic file labels
f tonto first ewn top ontology label
a tonto all ewn top ontology labels
f magn first domain label
a magn all domain labels
syntactical feats.
Feat. Description
tgt mnp syntactical relations of the target word
from minipar
rels mnp all syntactical relations from minipar
yar noun NounModifier, ObjectTo, SubjectTo
for nouns
yar verb Object, ObjectToPreposition, Preposi-
tion for verbs
yar adjs DominatingNoun for adjectives
Table 1: Feature Set
Finally, the last section of table 1 describes
the syntactic features which contains features ex-
tracted using two different tools: Dekang Lin?s
Minipar4 and Yarowsky?s dependency pattern ex-
tractor.
It is worth noting that the set of features presented
is highly redundant. Due to this fact, a feature se-
lection process has been applied, which is detailed
in the next section.
4 Experimental Setting
For each binarization approach, we performed a fea-
ture selection process consisting of two consecutive
steps:
4http://www.cs.ualberta.ca/?lindek/minipar.htm
? POS feature selection: Using the Senseval?2
corpus, an exhaustive selection of the best set
of features for each particular Part?of?Speech
was performed. These feature sets were taken
as the initial sets in the feature selection pro-
cess of Senseval-3.
? Word feature selection: We applied a
forward(selection)?backward(deletion) two?
step procedure to obtain the best feature
selection per word. For each word, the process
starts with the best feature set obtained in the
previous step according to its Part?of?Speech.
Now, during selection, we consider those
features not selected during POS feature
selection, adding all features which produce
some improvement. During deletion, we con-
sider only those features selected during POS
feature selection, removing all features which
produces some improvement. Although this
addition?deletion procedure could be iterated
until no further improvement is achieved, we
only performed a unique iteration because
of the computational overhead. One brief
experiment (not reported here) for one?vs?all
achieves an increase of 2.63% in accuracy
for the first iteration and 0.52% for a second
one. First iteration improves the accuracy of
53 words and the second improves only 15.
Comparing the evolution of these 15 words,
the increase in accuracy is of 2.06% for the
first iteration and 1.68% for the second one.
These results may suggest that accuracy could
be increased by this iteration procedure.
The result of this process is the selection of the
best binarization approach and the best feature set
for each individual word.
Considering feature selection, we have inspected
the selected attributes for all the words and we ob-
served that among these attributes there are fea-
tures of all four types. The most selected features
are the local ones, and among them those of ?first
noun/adjective on the left/right?; from topical fea-
tures the most selected ones are the ?comb? and in a
less measure the ?topic?; from the knowledge?based
the most selected feature are those of ?sumo? and
?domains labels?; and from syntactical ones, those
of ?Yarowsky?s patterns?. All the features previ-
ously mentioned where selected at least for 50 of
the 57 Senseval?3 words. Even so, it is useful the
use of all features when a selection procedure is ap-
plied. These general features do not work fine for
all words. Some words make use of the less selected
features; that is, every word is a different problem.
Regarding the implementation details of the sys-
tem, we used SVMlight (Joachims, 2002), a very ro-
bust and complete implementation of Support Vec-
tor Machines learning algorithms, which is freely
available for research purposes5 . A simple lineal
kernel with a regularization C value of 0.1 was
applied. This parameter was empirically decided
on the basis of our previous experiments on the
Senseval?2 corpus. Additionally, previous tests us-
ing non?linear kernels did not provide better results.
The selection of the best feature set and the bi-
narization scheme per word described above, have
been performed using a 5-fold cross validation pro-
cedure on the Senseval-3 training set. The five parti-
tions of the training set were obtained maintaining,
as much as possible, the initial distribution of exam-
ples per sense.
After several experiments considering the ?U? la-
bel as an additional regular class, we found that we
obtained better results by simply ignoring it. Then,
if a training example was tagged only with this la-
bel, it was removed from the training set. If the ex-
ample was tagged with this label and others, the ?U?
label was also removed from the learning example.
In that way, the TALP system do not assigns ?U?
labels to the test examples.
Due to lack of time, the TALP system presented
at the competition time did not include a com-
plete model selection for the constraint classifica-
tion binarization setting. More precisely, 14 words
were processed within the complete model selection
framework, and 43 were adjusted with a fixed one?
vs?all approach but a complete feature selection.
After the competition was closed, we implemented
the constraint classification setting more efficiently
and we reprocessed again the data. Section 5 shows
the results of both variants.
A rough estimation of the complete model selec-
tion time for both approaches is the following. The
training spent about 12 hours (OVA setting) and 5
days (CC setting) to complete6 , suggesting that the
main drawback of these approaches is the computa-
tional overhead. Fortunately, the process time can
be easily reduced: the CC layer could be ported
from Perl to C++ and the model selection could be
easily parallelized (since the treatment of each word
is independent).
5 Results
Table 2 shows the accuracy obtained on the train-
ing set and table 3 the results of our system (SE3,
5http://svmlight.joachims.org
6These figures were calculated using a 800 MHz Pentium
III PC with 320 Mb of memory.
TALP), together with the most frequent sense base-
line (mfs), the recall result of the best system in the
task (best), and the recall median between all par-
ticipant systems (avg). These last three figures were
provided provided by the organizers of the task.
OVA(base) in table 2 stands for the results of the
one?vs?all approach on the starting feature set (5?
fold?cross validation on the training set). CC(base)
refers to the constrain?classification setting on the
starting feature set. OVA(best) and CC(best) mean
one?vs?all and constraint?classification with their
respective feature selection. Finally, SE3 stands for
the system officially presented at competition time7
and TALP stands for the complete architecture.
method accuracy
OVA(base) 72 38%
CC(base) 72.28%
OVA(best) 75.27%
CC(best) 75.70%
SE3 75.62%
TALP 76.02%
Table 2: Overall results of all system variants on the
training set
It can be observed that the feature selection pro-
cess consistently improves the accuracy by around
3 points, both in OVA and CC binarization set-
tings. Constraint?classification is slightly better
than one?vs?all approach when feature selection
is performed, though this improvement is not con-
sistent along all individual words (detailed results
omitted) neither statistically significant (z?test with
0.95 confidence level). Finally, the combined
binarization?feature selection further increases the
accuracy in half a point (again this difference is not
statistically significant).
measure mfs avg best SE3 TALP
fine 55.2 65.1 72.9 71.3 71.6
coarse 64.5 73.7 79.5 78.2 78.2
Table 3: Overall results on the Senseval-3 test set
However, when testing the complete architecture
on the official test set, we obtained an accuracy de-
crease of more than 4 points. It remains to be ana-
lyzed if this difference is due to a possible overfit-
ting to the training corpus during model selection,
or simply is due to the differences between train-
ing and test corpora. Even so, the TALP system
achieves a very good performance, since there is a
7Only 14 words were processed with the full architecture.
difference of only 1.3 points in fine and coarse re-
call respect to the best system of the English lexical
sample task of Senseval?3.
6 Conclusions
Regarding supervised Word Sense Disambiguation,
each word can be considered as a different classi-
fication problem. This implies that each word has
different feature models to describe its senses.
We have proposed and tested a supervised sys-
tem in which the examples are represented through
a very rich and redundant set of features (using the
information content coherently integrated within the
Multilingual Central Repository of the MEANING
project), and which performs a specialized selection
of features and binarization process for each word.
7 Acknowledgments
This research has been partially funded by the Eu-
ropean Commission (Meaning Project, IST-2001-
34460), and by the Spanish Research Department
(Hermes Project: TIC2000-0335-C03-02).
References
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, P. Vossen 2004. The MEAN-
ING Multilingual Central Repository. In Pro-
ceedings of the Second International WordNet
Conference.
N. Cristianini and J. Shawe-Taylor 2000. An Intro-
duction to Support Vector Machines. Cambridge
University Press.
T. Joachims 2002. Learning to Classify Text Using
Support Vector Machines. Dissertation, Kluwer.
S. Har-Peled and D. Roth and D. Zimak 2002. Con-
straint Classification for Multiclass Classification
and Ranking. In Proceedings of the 15th Work-
shop on Neural Information Processing Systems.
