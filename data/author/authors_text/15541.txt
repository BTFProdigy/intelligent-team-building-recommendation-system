Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440?446,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The LIGA (LIG/LIA) Machine Translation System for WMT 2011
Marion Potet1, Raphae?l Rubino2, Benjamin Lecouteux1, Ste?phane Huet2,
Herve? Blanchon1, Laurent Besacier1 and Fabrice Lefe`vre2
1UJF-Grenoble1, UPMF-Grenoble2
LIG UMR 5217
Grenoble, F-38041, France
FirstName.LastName@imag.fr
2Universite? d?Avignon
LIA-CERI
Avignon, F-84911, France
FirstName.LastName@univ-avignon.fr
Abstract
We describe our system for the news com-
mentary translation task of WMT 2011. The
submitted run for the French-English direction
is a combination of two MOSES-based sys-
tems developed at LIG and LIA laboratories.
We report experiments to improve over the
standard phrase-based model using statistical
post-edition, information retrieval methods to
subsample out-of-domain parallel corpora and
ROVER to combine n-best list of hypotheses
output by different systems.
1 Introduction
This year, LIG and LIA have combined their efforts
to produce a joint submission to WMT 2011 for the
French-English translation task. Each group started
by developing its own solution whilst sharing re-
sources (corpora as provided by the organizers but
also aligned data etc) and acquired knowledge (cur-
rent parameters, effect of the size of n-grams, etc.)
with the other. Both LIG and LIA systems are stan-
dard phrase-based translation systems based on the
MOSES toolkit with appropriate carefully-tuned se-
tups. The final LIGA submission is a combination
of the two systems.
We summarize in Section 2 the resources used
and the main characteristics of the systems. Sec-
tions 3 and 4 describe the specificities and report
experiments of resp. the LIG and the LIA system.
Section 5 presents the combination of n-best lists
hypotheses generated by both systems. Finally, we
conclude in Section 6.
2 System overview
2.1 Used data
Globally, our system1 was built using all the French
and English data supplied for the workshop?s shared
translation task, apart from the Gigaword monolin-
gual corpora released by the LDC. Table 1 sums up
the used data and introduces designations that we
follow in the remainder of this paper to refer to cor-
pora. Four corpora were used to build translation
models: news-c, euro, UN and giga, while three
others are employed to train monolingual language
models (LMs). Three bilingual corpora were de-
voted to model tuning: test09 was used for the de-
velopment of the two seed systems (LIG and LIA),
whereas test08 and testcomb08 were used to tune the
weights for system combination. test10 was finally
put aside to compare internally our methods.
2.2 LIG and LIA system characteristics
Both LIG and LIA systems are phrase-based trans-
lation models. All the data were first tokenized with
the tokenizer provided for the workshop. Kneser-
Ney discounted LMs were built from monolingual
corpora using the SRILM toolkit (Stolcke, 2002),
while bilingual corpora were aligned at the word-
level using GIZA++ (Och and Ney, 2003) or its
multi-threaded version MGIZA++ (Gao and Vogel,
2008) for the large corpora UN and giga. Phrase
table and lexicalized reordering models were built
with MOSES (Koehn et al, 2007). Finally, 14 fea-
tures were used in the phrase-based models:
1When not specified otherwise ?our? system refers to the
LIGA system.
440
CORPORA DESIGNATION SIZE (SENTENCES)
English-French Bilingual training
News Commentary v6 news-c 116 k
Europarl v6 euro 1.8 M
United Nation corpus UN 12 M
109 corpus giga 23 M
English Monolingual training
News Commentary v6 mono-news-c 181 k
Shuffled News Crawl corpus (from 2007 to 2011) news-s 25 M
Europarl v6 mono-euro 1.8 M
Development
newstest2008 test08 2,051
newssyscomb2009 testcomb09 502
newstest2009 test09 2,525
Test
newstest2010 test10 2,489
Table 1: Used corpora
? 5 translation model scores,
? 1 distance-based reordering score,
? 6 lexicalized reordering score,
? 1 LM score and
? 1 word penalty score.
The score weights were optimized on the test09 cor-
pus according to the BLEU score with the MERT
method (Och, 2003). The experiments led specifi-
cally with either LIG or LIA system are respectively
described in Sections 3 and 4. Unless otherwise
indicated, all the evaluations were performed using
case-insensitive BLEU and were computed with the
mteval-v13a.pl script provided by NIST. Ta-
ble 2 summarizes the differences between the final
configuration of the systems.
3 The LIG machine translation system
LIG participated for the second time to the WMT
shared news translation task for the French-English
language pair.
3.1 Pre-processing
Training data were first lowercased with the PERL
script provided for the campaign. They were also
processed in order to normalize a special French
form (named euphonious ?t?) as described in (Potet
et al, 2010).
The baseline system was built using a 4-gram LM
trained on the monolingual corpora provided last
year and translation models trained on news-c and
euro (Table 3, System 1). A significant improve-
ment in terms of BLEU is obtained when taking into
account a third corpus, UN, to build translation mod-
els (System 2). The next section describes the LMs
that were trained using the monolingual data pro-
vided this year.
3.2 Language model training
Target LMs are standard 4-gram models trained
on the provided monolingual corpus (mono-news-c,
mono-euro and news-s). We decided to test two dif-
ferent n-gram cut-off settings. The fist set has low
cut-offs: 1-2-3-3 (respectively for 1-gram, 2-gram,
3-gram and 4-gram counts), whereas the second one
(LM2) is more aggressive: 1-5-7-7. Experiment re-
sults (Table 3, Systems 3 and 4) show that resorting
to LM2 leads to an improvement of BLEU with re-
spect to LM1. LM2 was therefore used in the sub-
sequent experiments.
441
FEATURES LIG SYSTEM LIA SYSTEM
Pre-processing Text lowercased Text truecasedNormalization of French euphonious
?t?
Reaccentuation of French words start-
ing with a capital letter
LM Training on mono-news-c, news-s and
mono-euro
Training on mono-news-c and news-s
4-gram models 5-gram models
Translation model
Training on news-c, euro and UN Training on 10 M sentence pairs se-
lected in news-c, euro, UN and giga
Phrase table filtering
Use of -monotone-at-punctuation op-
tion
Table 2: Distinct features between final configurations retained for the LIG and LIA systems
3.3 Translation model training
Translation models were trained from the parallel
corpora news-c, euro and UN. Data were aligned
at the word-level and then used to build standard
phrase-based translation models. We filtered the ob-
tained phrase table using the method described in
(Johnson et al, 2007). Since this technique drasti-
cally reduces the size of the phrase table, while not
degrading (and even slightly improving) the results
on the development and test corpora (System 6), we
decided to employ filtered phrase tables in the final
configuration of the LIG system.
3.4 Tuning
For decoding, the system uses a log-linear com-
bination of translation model scores with the LM
log-probability. We prevent phrase reordering over
punctuation using the MOSES option -monotone-at-
punctuation. As the system can be beforehand tuned
by adjusting the log-linear combination weights on
a development corpus, we used the MERT method
(System 5). Optimizing weights according to BLEU
leads to an improvement with respect to the sys-
tem with MOSES default value weights (System 5
vs System 4).
3.5 Post-processing
We also investigated the interest of a statistical
post-editor (SPE) to improve translation hypotheses.
About 9,000 sentences extracted from the news do-
main test corpora of the 2007?2009 WMT transla-
tion tasks were automatically translated by a sys-
tem very similar to that described in (Potet et al,
2010), then manually post-edited. Manual correc-
tions of translations were performed by means of the
crowd-sourcing platform AMAZON MECHANICAL
TURK2 ($0.15/sent.). These collected data make
a parallel corpus whose source part is MT output
and target part is the human post-edited version of
MT output. This are used to train a phrase-based
SMT (with Moses without the tuning step) that au-
tomatically post-edit the MT output. That aims at
learning how to correct translation hypotheses. Sys-
tem 7 obtained when post-processing MT 1-best out-
put shows a slight improvement. However, SPE was
not used in the final LIG system since we lacked
time to apply SPE on the N-best hypotheses for the
development and test corpora (the N-best being nec-
essary for combination of LIG and LIA systems).
Ths LIGA submission is thus a constrained one.
3.6 Recasing
We trained a phrase-based recaser model on the
news-s corpus using the provided MOSES scripts
and applied it to uppercase translation outputs. A
common and expected loss of around 1.5 case-
sensitive BLEU points was observed on the test cor-
pus (news10) after applying this recaser (System 7)
with respect to the score case-insensitive BLEU pre-
viously measured.
2http://www.mturk.com/mturk/welcome
442
? SYSTEM DESCRIPTION BLEU SCORE
test09 test10
1 Training: euro+news-c 24.89 26.01
2 Training: euro+news-c+UN 25.44 26.43
3 2 + LM1 24.81 27.19
4 2 + LM2 25.37 27.25
5 4 + MERT on test09 26.83 27.53
6 5 + phrase-table filtering 27.09 27.64
7 6 + SPE 27.53 27.74
8 6 + recaser 24.95 26.07
Table 3: Incremental improvement of the LIG system in
terms of case-insensitive BLEU (%), except for line 8
where case-sensitive BLEU (%) are reported
4 The LIA machine translation system
This section describes the particularities of the MT
system which was built at the LIA for its first partic-
ipation to WMT.
4.1 System description
The available corpora were pre-processed using
an in-house script that normalizes quotes, dashes,
spaces and ligatures. We also reaccentuated French
words starting with a capital letter. We significantly
cleaned up the crawled parallel giga corpus, keeping
19.3 M of the original 22.5 M sentence pairs. For ex-
ample, sentence pairs with numerous numbers, non-
alphanumeric characters or words starting with cap-
ital letters were removed. The whole training ma-
terial is truecased, meaning that the words occur-
ing after a strong punctuation mark were lowercased
when they belonged to a dictionary of common all-
lowercased forms; the others were left unchanged.
The training of a 5-gram English LM was re-
strained to the news corpora mono-news-c and news-
s that we consider large enough to ignore other data.
In order to reduce the size of the LM, we first limited
the vocabulary of our model to a 1 M word vocabu-
lary taking the most frequent words in the news cor-
pora. We also resorted to cut-offs to discard infre-
quent n-grams (2-2-3-5 thresholds on 2- to 5-gram
counts) and uses the SRILM option prune, which
allowed us to train the LM on large data with 32 Gb
RAM.
Our translation models are phrase-based models
(PBMs) built with MOSES with the following non-
default settings:
? maximum sentence length of 80 words,
? limit on the number of phrase translations
loaded for each phrase fixed to 30.
Weights of LM, phrase table and lexicalized re-
ordering model scores were optimized on the devel-
opment corpus thanks to the MERT algorithm.
Besides the size of used data, we experimented
with two advanced features made available for
MOSES. Firstly, we filtered phrase tables using the
default setting -l a+e -n 30. This dramatically
reduced phrase tables by dividing their size by a
factor of 5 but did not improve our best configu-
ration from the BLEU score perspective (Table 4,
line 1); the method was therefore not kept in the
LIA system. Secondly, we introduced reordering
constraints in order to consider quoted material as
a block. This method is particularly useful when ci-
tations included in sentences have to be translated.
Two configurations were tested: zone markups in-
clusion around quotes and wall markups inclusion
within zone markups. However, the measured gains
were finally too marginal to include the method in
the final system.
4.2 Parallel corpus subsampling
As the only news parallel corpus provided for the
workshop contains 116 k sentence pairs, we must
resort to parallel out-of-domain corpora in order to
build reliable translation models. Information re-
trieval (IR) methods have been used in the past to
subsample parallel corpora. For example, Hilde-
brand et al (2005) used sentences belonging to the
development and test corpora as queries to select the
k most similar source sentences in an indexed paral-
lel corpus. The retrieved sentence pairs constituted
a training corpus for the translation models.
The RALI submission for WMT10 proposed a
similar approach that builds queries from the mono-
lingual news corpus in order to select sentence pairs
stylistically close to the news domain (Huet et al,
2010). This method has the major interest that it
does not require to build a new training parallel
corpus for each news data set to translate. Fol-
lowing the best configuration tested in (Huet et al,
443
2010), we index the three out-of-domain corpora us-
ing LEMUR3, and build queries from English news-s
sentences where stop words are removed. The 10 top
sentence pairs retrieved per query are selected and
added to the new training corpus if they are not re-
dundant with a sentence pair already collected. The
process is repeated until the training parallel cor-
pus reaches a threshold over the number of retrieved
pairs.
Table 4 reports BLEU scores obtained with the
LIA system using the in-domain corpus news-c and
various amounts of out-of-domain data. MERT was
re-run for each set of training data. The first four
lines display results obtained with the same num-
ber of sentence pairs, which corresponds to the
size of news-c appended to euro. The experiments
show that using euro instead of the first sentences of
UN and giga significantly improves BLEU scores,
which indicates the better adequacy of euro with re-
spect to the test10 corpus. The use of the IR method
to select sentences from euro, UN and giga leads to
a similar BLEU score to the one obtained with euro.
The increase of the collected pairs up to 3 M pairs
generates a significant improvement of 0.9 BLEU
point. A further rise of the amount of collected
pairs does not introduce a major gain since retriev-
ing 10 M sentence pairs only augments BLEU from
29.1 to 29.3. This last configuration which leads to
the best BLEU was used to build the final LIA sys-
tem. Let us note that 2 M, 3 M and 15 M queries
were required to respectively obtain 3 M, 5 M and
10 M sentence pairs because of the removal of re-
dundant sentences in the increased corpus.
For a matter of comparison, a system was also
built taking into account all the training material,
i.e. 37 M sentence pairs4. This last system is out-
performed by our best system built with IR and has
finally close performance to the one obtained with
news-c+euro relatively to the quantity of used data.
5 The system combination
System combination is based on the 500-best out-
puts generated by the LIA and the LIG systems.
3www.lemurproject.org
4For this experiment, the data were split into three parts
to build independent alignment models: news-c+euro, UN and
giga, and they were joined afterwards to build translation mod-
els.
USED PARALLEL CORPORA FILTERING
without with
news-c + euro (1.77 M) 28.1 28.0
news-c + 1.77 M of UN 27.2 -
news-c + 1.77 M of giga 27.1 -
news-c + 1.77 M with IR 28.2 -
news-c + 3 M with IR 29.1 29.0
news-c + 5 M with IR 28.8 -
news-c + 10 M with IR 29.3 29.2
All data 28.9 29.0
Table 4: BLEU (%) on test10 measured with the LIA
system using different training parallel corpora
They both used the MOSES option distinct, en-
suring that the hypotheses produced for a given sen-
tence are different inside an N-best list. Each N-best
list is associated with a set of 14 scores and com-
bined in several steps.
The first step takes as input lowercased 500-best
lists, since preliminary experiments have shown a
better behavior using only lowercased output (with
cased output, combination presents some degrada-
tions). The score combination weights are opti-
mized on the development corpus, in order to max-
imize the BLEU score at the sentence level when
N-best lists are reordered according to the 14 avail-
able scores. To this end, we resorted to the SRILM
nbest-optimize tool to do a simplex-based
Amoeba search (Press et al, 1988) on the error func-
tion with multiple restarts to avoid local minima.
Once the optimized feature weights are com-
puted independently for each system, N-best lists
are turned into confusion networks (Mangu et al,
2000). The 14 features are used to compute poste-
riors relatively to all the hypotheses in the N-best
list. Confusion networks are computed for each sen-
tence and for each system. In Table 5 we present
the ROVER (Fiscus, 1997) results for the LIA and
LIG confusion networks (LIA CNC and LIG CNC).
Then, both confusion networks computed for each
sentence are merged into a single one. A ROVER
is applied on the combined confusion network and
generates a lowercased 1-best.
The final step aims at producing cased hypothe-
ses. The LIA system built from truecased corpora
achieved significantly higher performance than the
444
LIG LIA LIG CNC LIA CNC LIG+LIA
case-insensitive test10 27.6 29.3 28.1 29.4 29.7
BLEU test11 28.5 29.4 28.5 29.3 29.9
case-sensitive test10 26.1 28.4 27.0 28.4 28.7
BLEU test11 26.9 28.4 27.5 28.4 28.8
Table 5: Performance measured before and after combining systems
LIG system trained on lowercased corpora (Table 5,
two last lines). In order to get an improvement when
combining the outputs, we had to adopt the follow-
ing strategy. The 500-best truecased outputs of the
LIA system are first merged in a word graph (and
not a mesh lattice). Then, the lowercased 1-best
previously obtained with ROVER is aligned with the
graph in order to find the closest existing path, which
is equivalent to matching an oracle with the graph.
This method allows for several benefits. The new
hypothesis is based on a ?true? decoding pass gener-
ated by a truecased system and discarded marginal
hypotheses. Moreover, the selected path offers a
better BLEU score than the initial hypothesis with
and without case. This method is better than the one
which consists of applying the LIG recaser (section
3.6) on the combined (un-cased) hypothesis.
The new recased one-best hypothesis is then used
as the final submission for WMT. Our combination
approach improves on test11 the best single sys-
tem by 0.5 case-insensitive BLEU point and by 0.4
case-sensitive BLEU (Table 5). However, it also in-
troduces some mistakes by duplicating in particular
some segments. We plan to apply rules at the seg-
ment level in order to reduce these artifacts.
6 Conclusion
This paper presented two statistical machine trans-
lation systems developed at different sites using
MOSES and the combination of these systems. The
LIGA submission presented this year was ranked
among the best MT system for the French-English
direction. This campaign was the first shot for LIA
and the second for LIG. Beside following the tradi-
tional pipeline for building a phrase-based transla-
tion system, each individual system led to specific
works: LIG worked on using SPE as post-treatment,
LIA focused on extracting useful data from large-
sized corpora. And their combination implied to ad-
dress the interesting issue of matching results from
systems with different casing approaches.
WMT is a great opportunity to chase after perfor-
mance and joining our efforts has allowed to save
considerable amount of time for data preparation
and tuning choices (even when final decisions were
different among systems), yet obtaining very com-
petitive results. This year, our goal was to develop
state-of-the-art systems so as to investigate new ap-
proaches for related topics such as translation with
human-in-the-loop or multilingual interaction sys-
tems (e.g. vocal telephone information-query di-
alogue systems in multiple languages or language
portability of such systems).
References
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates:recognizer output vot-
ing error reduction (ROVER). In Proceedings of the
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 347?354, Santa Barbara, CA,
USA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Proceedings of the
ACL Workshop: Software Engineering, Testing, and
Quality Assurance for Natural Language Processing,
pages 49?57, Columbus, OH, USA.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the 10th confer-
ence of the European Association for Machine Trans-
lation (EAMT), Budapest, Hungary.
Ste?phane Huet, Julien Bourdaillet, Alexandre Patry, and
Philippe Langlais. 2010. The RALI machine trans-
lation system for WMT 2010. In Proceedings of the
ACL Joint 5th Workshop on Statistical Machine Trans-
lation and Metrics (WMT), Uppsala, Sweden.
Howard Johnson, Joel Martin, George Foster, and Roland
445
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of
the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, jun.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), Companion Vol-
ume, pages 177?180, Prague, Czech Republic, June.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics (ACL), Sapporo, Japan.
Marion Potet, Laurent Besacier, and Herve? Blanchon.
2010. The LIG machine translation for WMT 2010.
In Proceedings of the ACL Joint 5th Workshop on Sta-
tistical Machine Translation and Metrics (WMT), Up-
psala, Sweden.
William H. Press, Brian P. Flannery, Saul A. Teukolsky,
and William T. Vetterling. 1988. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th In-
ternational Conference on Spoken Language Process-
ing (ICSLP), Denver, CO, USA.
446
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 386?391,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LIG System for WMT13 QE Task: Investigating the Usefulness of
Features in Word Confidence Estimation for MT
Ngoc-Quang Luong Benjamin Lecouteux
LIG, Campus de Grenoble
41, Rue des Mathe?matiques,
UJF - BP53, F-38041 Grenoble Cedex 9, France
{ngoc-quang.luong,laurent.besacier,benjamin.lecouteux}@imag.fr
Laurent Besacier
Abstract
This paper presents the LIG?s systems
submitted for Task 2 of WMT13 Qual-
ity Estimation campaign. This is a
word confidence estimation (WCE) task
where each participant was asked to la-
bel each word in a translated text as
a binary ( Keep/Change) or multi-class
(Keep/Substitute/Delete) category. We in-
tegrate a number of features of various
types (system-based, lexical, syntactic and
semantic) into the conventional feature
set, for our baseline classifier training.
After the experiments with all features,
we deploy a ?Feature Selection? strategy
to keep only the best performing ones.
Then, a method that combines multiple
?weak? classifiers to build a strong ?com-
posite? classifier by taking advantage of
their complementarity is presented and ex-
perimented. We then select the best sys-
tems for submission and present the offi-
cial results obtained.
1 Introduction
Recently Statistical Machine Translation (SMT)
systems have shown impressive gains with many
fruitful results. While the outputs are more accept-
able, the end users still face the need to post edit
(or not) an automatic translation. Then, the issue
is to be able to accurately identify the correct parts
as well as detecting translation errors. If we fo-
cus on errors at the word level, the issue is called
Word-level Confidence Estimation (WCE).
In WMT 2013, a shared task about quality esti-
mation is proposed. This quality estimation task is
proposed at two levels: word-level and sentence-
level. Our work focuses on the word-level qual-
ity estimation (named Task 2). The objective is to
highlight words needing post-edition and to detect
parts of the sentence that are not reliable. For the
task 2, participants produce for each token a label
according to two sub-tasks:
? a binary classification: good (keep) or bad
(change) label
? a multi-class classification: the label refers to
the edit action needed for the token (i.e. keep,
delete or substitute).
Various approaches have been proposed for
WCE: Blatz et al (2003) combine several features
using neural network and naive Bayes learning al-
gorithms. One of the most effective feature combi-
nations is the Word Posterior Probability (WPP) as
proposed by Ueffing et al (2003) associated with
IBM-model based features (Blatz et al, 2004).
Ueffing and Ney (2005) propose an approach for
phrase-based translation models: a phrase is a se-
quence of contiguous words and is extracted from
word-aligned bilingual training corpus. The con-
fidence value of each word is then computed by
summing over all phrase pairs in which the tar-
get part contains this word. Xiong et al (2010)
integrate target word?s Part-Of-Speech (POS) and
train them by Maximum Entropy Model, allow-
ing significative gains compared to WPP features.
Other approaches are based on external features
(Soricut and Echihabi, 2010; Felice and Specia,
2012) allowing to deal with various MT systems
(e.g. statistical, rule based etc.).
In this paper, we propose to use both internal
and external features into a conditionnal random
fields (CRF) model to predict the label for each
word in the MT hypothesis. We organize the arti-
cle as follows: section 2 explains all the used fea-
tures. Section 3 presents our experimental settings
and the preliminary experiments. Section 4 ex-
plores a feature selection refinement and the sec-
tion 5 presents work using several classifiers asso-
ciated with a boosting decision. Finally we present
386
our systems submissions and propose some con-
clusions and perspectives.
2 Features
In this section, we list all 25 types of features for
building our classifier (see a list in Table 3). Some
of them are already used and described in detail in
our previous paper (Luong, 2012), where we deal
with French - English SMT Quality Estimation.
WMT13 was a good chance to re-investigate their
usefulness for another language pair: English-
Spanish, as well as to compare their contributions
with those from other teams. We categorize them
into two types: the conventional features, which
are proven to work efficiently in numerous CE
works and are inherited in our systems, and the
LIG features which are more specifically sug-
gested by us.
2.1 The conventional features
We describe below the conventional features we
used. They can be found in some previous papers
dealing with WCE.
? Target word features: the target word itself;
the bigram (trigram) it forms with one (two)
previous and one (two) following word(s); its
number of occurrences in the sentence.
? Source word features: all the source words
that align to the target one, represented in
BIO1 format.
? Source alignment context features: the com-
binations of the target word and one word be-
fore (left source context) or after (right source
context) the source word aligned to it.
? Target algnment context features: the com-
binations of the source word and each word
in the window ?2 (two before, two after) of
the target word.
? Target Word?s Posterior Probability (WPP).
? Backoff behaviour: a score assigned to the
word according to how many times the target
Language Model has to back-off in order to
assign a probability to the word sequence, as
described in (Raybaud et al, 2011).
1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
? Part-Of-Speech (POS) features (using Tree-
Tagger2 toolkit): The target word?s POS; the
source POS (POS of all source words aligned
to it); bigram and trigram sequences between
its POS and the POS of previous and follow-
ing words.
? Binary lexical features that indicate whether
the word is a: stop word (based on the stop
word list for target language), punctuation
symbol, proper name or numerical.
2.2 The LIG features
? Graph topology features: based on the N-best
list graph merged into a confusion network.
On this network, each word in the hypothesis
is labelled with its WPP, and belongs to one
confusion set. Every completed path passing
through all nodes in the network represents
one sentence in the N-best, and must con-
tain exactly one link from each confusion set.
Looking into a confusion set, we find some
useful indicators, including: the number of
alternative paths it contains (called Nodes),
and the distribution of posterior probabili-
ties tracked over all its words (most interest-
ing are maximum and minimum probabilities,
called Max and Min).
? Language Model (LM) features: the ?longest
target n-gram length? and ?longest source n-
gram length?(length of the longest sequence
created by the current target (source aligned)
word and its previous ones in the target
(source) LM). For example, with the tar-
get word wi: if the sequence wi?2wi?1wi
appears in the target LM but the sequence
wi?3wi?2wi?1wi does not, the n-gram value
for wi will be 3.
? The word?s constituent label and its depth in
the tree (or the distance between it and the
tree root) obtained from the constituent tree
as an output of the Berkeley parser (Petrov
and Klein, 2007) (trained over a Spanish tree-
bank: AnCora3).
? Occurrence in Google Translate hypothesis:
we check whether this target word appears in
the sentence generated by Google Translate
engine for the same source.
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
3http://clic.ub.edu/corpus/en/ancora
387
? Polysemy Count: the number of senses of
each word given its POS can be a reliable in-
dicator for judging if it is the translation of
a particular source word. Here, we investi-
gate the polysemy characteristic in both tar-
get word and its aligned source word. For
source word (English), the number of senses
can be counted by applying a Perl exten-
sion named Lingua::WordNet4, which pro-
vides functions for manipulating the Word-
Net database. For target word (Spanish), we
employ BabelNet5 - a multilingual semantic
network that works similarly to WordNet but
covers more European languages, including
Spanish.
3 Experimental Setting and Preliminary
Experiment
The WMT13 organizers provide two bilingual
data sets, from English to Spanish: the training
and the test ones. The training set consists of
803 MT outputs, in which each token is anno-
tated with one appropriate label. In the binary
variant, the words are classified into ?K? (Keep)
or ?C? (Change) label, meanwhile in the multi-
class variant, they can belong to ?K? (Keep), ?S?
(Substitution) or ?D? (Deletion). The test set con-
tains 284 sentences where all the labels accompa-
nying words are hidden. For optimizing parame-
ters of the classifier, we extract 50 sentences from
the training set to form a development set. Since
a number of repetitive sentences are observed in
the original training set, the dev set was carefully
chosen to ensure that there is no overlap with the
new training set (753 sentences), keeping the tun-
ing process accurate. Some statistics about each
set can be found in Table 1.
Motivated by the idea of addressing WCE as
a sequence labeling task, we employ the Con-
ditional Random Fields (CRF) model (Lafferty
et al, 2001) and the corresponding WAPITI toolkit
(Lavergne et al, 2010) to train our classifier. First,
we experiment with the combination of all fea-
tures. For the multi-class system, WAPITI?s de-
fault configuration is applied to determine the la-
bel, i.e. label which has the highest score is as-
signed to word. In case of the binary system,
the classification task is then conducted multiple
times, corresponding to a threshold increase from
4http://search.cpan.org/dist/Lingua-Wordnet/Wordnet.pm
5http://babelnet.org
0.300 to 0.975 (step = 0.025). When threshold =
?, all words in the test set which the probability of
?K? class > ? will be labelled as ?K?, and oth-
erwise, ?C?. The values of Precision (Pr), Recall
(Rc) and F-score (F) for K and C label are tracked
along this threshold variation, allowing us to se-
lect the optimal threshold that yields the highest
Favg = F (K)+F (C)2 .
Results for the all-feature binary system
(ALL BIN) at the optimal threshold (0.500) and
the multi-class one (ALL MULT) at the default
threshold, obtained on our dev set, are shown
in Table 2. We can notice that with ALL BIN,
?K? label scores are very promising and ?C? la-
bel reaches acceptable performance. In case of
ALL MULT we obtain the almost similar above
performance for ?K? and ?S?, respectively, ex-
cept the disappointing scores for ?D? (which can
be explained by the fact that very few instances of
?D? words (4%) are observed in the training cor-
pus).
Data set Train Dev Test
#segments 753 50 284
#distinct segments 400 50 163
#words 18435 1306 7827
%K : %C 70: 30 77: 23 -
%K: %S: %D 70:26:4 77:19:4 -
Table 1: Statistics of training, dev and test sets
System Label Pr(%) Rc(%) F(%)
ALL BIN K 85.79 84.68 85.23
C 50.96 53.16 52.04
ALL MULT K 85.30 84.00 84.65
S 43.89 49.00 46.31
D 7.90 6.30 7.01
Table 2: Average Pr, Rc and F for labels of all-
feature binary and multi-class systems, obtained
on dev set.
4 Feature Selection
In order to improve the preliminary scores of all-
feature systems, we conduct a feature selection
which is based on the hypothesis that some fea-
tures may convey ?noise? rather than ?informa-
tion? and might be the obstacles weakening the
other ones. In order to prevent this drawback,
we propose a method to filter the best features
388
based on the ?Sequential Backward Selection? al-
gorithm6. We start from the full set of N features,
and in each step sequentially remove the most use-
less one. To do that, all subsets of (N-1) fea-
tures are considered and the subset that leads to
the best performance gives us the weakest feature
(not involved in the considered set). This proce-
dure is also called ?leave one out? in the litera-
ture. Obviously, the discarded feature is not con-
sidered in the following steps. We iterate the pro-
cess until there is only one remaining feature in
the set, and use the following score for compar-
ing systems: Favg(all) = Favg(K)+Favg(C)2 , where
Favg(K) and Favg(C) are the averaged F scores
for K and C label, respectively, when threshold
varies from 0.300 to 0.975. This strategy enables
us to sort the features in descending order of im-
portance, as displayed in Table 3. Figure 1 shows
the evolution of the performance as more and more
features are removed.
Rank Feature name Rank Feature name
1 Source POS 14? Distance to root
2? Occur in Google Trans. 15 Backoff behaviour
3? Nodes 16? Constituent label
4 Target POS 17 Proper name
5 WPP 18 Number of occurrences
6 Left source context 19? Min
7 Right target context 20? Max
8 Numeric 21 Left target context
9? Polysemy (target) 22? Polysemy (source)
10 Punctuation 23? Longest target gram length
11 Stop word 24? Longest source gram length
12 Right source context 25 Source Word
13 Target Word
Table 3: The rank of each feature (in term of use-
fulness) in the set. The symbol ?*? indicates our
proposed features.
Observations in 10-best and 10-worst perform-
ing features in Table 3 suggest that numerous fea-
tures extracted directly from SMT system itself
(source and target POS, alignment context infor-
mation, WPP, lexical properties: numeric, punc-
tuation) perform very well. Meanwhile, opposite
from what we expected, those from word statis-
tical knowledge sources (target and source lan-
guage models) are likely to be much less ben-
eficial. Besides, three of our proposed features
appear in top 10-best. More noticeable, among
them, the first-time-experimented feature ?Occur-
rence in Google Translation hypothesis? is the
most prominent (rank 2), implying that such an on-
line MT system can be a reliable reference channel
for predicting word quality.
6http://research.cs.tamu.edu/prism/lectures/pr/pr l11.pdf
Figure 1: Evolution of system performance
(Favg(all)) during Feature Selection process, ob-
tained on dev set
The above selection process also brings us the
best-performing feature set (Top 20 in Table 3).
The binary classifier built using this optimal sub-
set of features (FS BIN) reaches the optimal per-
formance at the threshold value of 0.475, and
slightly outperforms ALL BIN in terms of F scores
(0.46% better for ?K? and 0.69% better for ?C?).
We then use this set to build the multi-class one
(FS MULT) and the results are shown to be a
bit more effective compare to ALL MULT (0.37%
better for ?K?, 0.80% better for ?S? and 0.15%
better for ?D?). Detailed results of these two sys-
tems can be found in Table 4.
In addition, in Figure 1, when the size of fea-
ture set is small (from 1 to 7), we can observe
sharply the growth of system scores for both la-
bels. Nevertheless the scores seem to saturate as
the feature set increases from the 8 up to 25. This
phenomenon raises a hypothesis about the learn-
ing capability of our classifier when coping with
a large number of features, hence drives us to an
idea for improving the classification scores. This
idea is detailed in the next section.
System Label Pr(%) Rc(%) F(%)
FS BIN K 85.90 85.48 85.69
C 52.29 53.17 52.73
FS MULT K 85.05 85.00 85.02
S 45.36 49.00 47.11
D 9.1 5.9 7.16
Table 4: The Pr, Rc and F for labels of binary and
multi-class system built from Top 20 features, at
the optimal threshold value, obtained on dev set
389
5 Using Boosting technique to improve
the system?s performance
In this section, we try to answer to the following
question: if we build a number of ?weak? (or ?ba-
sic?) classifiers by using subsets of our features
and a machine learning algorithm (such as Boost-
ing), would we get a single ?strong? classifier?
When deploying this idea, our hope is that multi-
ple models can complement each other as one fea-
ture set might be specialized in a part of the data
where the others do not perform very well.
First, we prepare 23 feature subsets
(F1, F2, ..., F23) to train 23 basic classifiers,
in which: F1 contains all features, F2 is the Top
20 in Table 3 and Fi (i = 3..23) contains 9
randomly chosen features. Next, a 7-fold cross
validation is applied on our training set. We
divide it into 7 subsets (S1, S2, . . . , S7). Each
Si (i = 1..6) contains 100 sentences, and the
remaining 153 sentences constitute S7. In the
loop i (i = 1..7), Si is used as the test set and
the remaining data is trained with 23 feature
subsets. After each loop, we obtain the results
from 23 classifiers for each word in Si. Finally,
the concatenation of these results after 7 loops
gives us the training data for Boosting. Therefore,
the Boosting training file has 23 columns, each
represents the output of one basic classifier for
our training set. The detail of this algorithm is
described below:
Algorithm to build Boosting training data
for i :=1 to 7 do
begin
TrainSet(i) := ?Sk (k = 1..7, k 6= i)
TestSet(i) := Si
for j := 1 to 23 do
begin
Classifier Cj := Train TrainSet(i) with Fj
Result Rj := Use Cj to test Si
Column Pj := Extract the ?probability of word
to be G label? in Rj
end
Subset Di (23 columns) := {Pj} (j = 1..23)
end
Boosting training set D := ?Di (i = 1..7)
Next, the Bonzaiboost toolkit7 (which imple-
ments Boosting algorithm) is used for building
Boosting model. In the training command, we in-
voked: algorithm = ?AdaBoost?, and number of
iterations = 300. The Boosting test set is prepared
as follows: we train 23 feature subsets with the
training set to obtain 23 classifiers, then use them
7http://bonzaiboost.gforge.inria.fr/x1-20001
to test our dev set, finally extract the 23 probabil-
ity columns (like in the above pseudo code). In the
testing phase, similar to what we did in Section 4,
the Pr, Rc and F scores against threshold variation
for ?K? and ?C? labels are tracked, and those cor-
responding to the optimal threshold (0.575 in this
case) are represented in Table 5.
System Label Pr(%) Rc(%) F(%)
BOOST BIN K 86.65 84.45 85.54
C 51.99 56.48 54.15
Table 5: The Pr, Rc and F for labels of Boosting
binary classifier (BOOST BIN)
The scores suggest that using Boosting algo-
rithm on our CRF classifiers? output accounts
for an efficient way to make them predict better:
on the one side, we maintain the already good
achievement on K class (only 0.15% lost), on the
other side we gain 1.42% the performance in C
class. It is likely that Boosting enables different
models to better complement each other, in terms
of the later model becomes experts for instances
handled wrongly by the previous ones. Another
advantage is that Boosting algorithm weights each
model by its performance (rather than treating
them equally), so the strong models (come from
all features, top 20, etc.) can make more dominant
impacts than the rest.
6 Submissions and Official Results
After deploying several techniques to improve the
system?s prediction capability, we select two bests
of each variant (binary and multi-class) to sub-
mit. For the binary task, the submissions in-
clude: the Boosting (BOOST BIN) and the Top
20 (FS BIN) system. For the multi-class task, we
submit: the Top 20 (FS MULT) and the all-feature
(ALL MULT) one. Before the submission, the
training and dev sets were combined to re-train
the prediction models for FS BIN, FS MULT and
ALL MULT. Table 6 reports the official results
obtained by LIG at WMT 2013, task 2. We ob-
tained the best performance among 3 participants.
These results confirm that the feature selection
strategy is efficient (FS MULT slightly better than
ALL MULT) while the contribution of Boosting
is unclear (BOOST BIN better than FS BIN if F-
measure is considered but worse if Accuracy is
considered - the difference is not significant).
390
System Pr Rc F Acc
BOOST BIN 0.777882 0.884325 0.827696 0.737702
FS BIN 0.788483 0.864418 0.824706 0.738213
FS MULT - - - 0.720710
ALL MULT - - - 0.719177
Table 6: Official results of the submitted systems, obtained on test set
7 Discussion and Conclusion
In this paper, we describe the systems submitted
for Task 2 of WMT13 Quality Estimation cam-
paign. We cope with the prediction of quality
at word level, determining whether each word
is ?good? or ?bad? (in the binary variant), or is
?good?, or should be ?substitute? or ?delete? (in
the multi-class variant). Starting with the ex-
isting word features, we propose and add vari-
ous of novel ones to build the binary and multi-
class baseline classifier. The first experiment?s re-
sults show that precision and recall obtained in
?K? label (both in binary and multi-class sys-
tems) are very encouraging, and ?C? (or ?S?) la-
bel reaches acceptable performance. A feature se-
lection strategy is then deployed to enlighten the
valuable features, find out the best performing sub-
set. One more contribution we made is the proto-
col of applying Boosting algorithm, training mul-
tiple ?weak? classifiers, taking advantage of their
complementarity to get a ?stronger? one. These
techniques improve gradually the system scores
(measure with F score) and help us to choose the
most effective systems to classify the test set.
In the future, this work can be extended in the
following ways. Firstly, we take a deeper look into
linguistic features of word, such as the grammar
checker, dependency tree, semantic similarity, etc.
Besides, we would like to reinforce the segment-
level confidence assessment, which exploits the
context relation between surrounding words to
make the prediction more accurate. Moreover, a
methodology to evaluate the sentence confidence
relied on the word- and segment- level confidence
will be also deeply considered.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. Confidence estimation for machine trans-
lation. Technical report, JHU/CLSP Summer Workshop,
2003.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. Confidence estimation for machine trans-
lation. In Proceedings of COLING 2004, pages 315?321,
Geneva, April 2004.
Mariano Felice and Lucia Specia. Linguistic features for
quality estimation. In Proceedings of the 7th Workshop on
Statistical Machine Translation, pages 96?103, Montreal,
Canada, June 7-8 2012.
John Lafferty, Andrew McCallum, and Fernando Pereira.
Conditional random fields: Probabilistic models for seg-
menting et labeling sequence data. In Proceedings of
ICML-01, pages 282?289, 2001.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon. Practi-
cal very large scale crfs. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics,
pages 504?513, 2010.
Ngoc-Quang Luong. Integrating lexical, syntactic and
system-based features to improve word confidence estima-
tion in smt. In Proceedings of JEP-TALN-RECITAL, vol-
ume 3 (RECITAL), pages 43?56, Grenoble, France, June
4-8 2012.
Slav Petrov and Dan Klein. Improved inference for unlexical-
ized parsing. In Proceedings of NAACL HLT 2007, pages
404?411, Rochester, NY, April 2007.
S. Raybaud, D. Langlois, and K. Sma?? li. ?this sentence is
wrong.? detecting errors in machine - translated sentences.
In Machine Translation, pages 1?34, 2011.
Radu Soricut and Abdessamad Echihabi. Trustrank: Inducing
trust in automatic translations via ranking. In Proceedings
of the 48th ACL (Association for Computational Linguis-
tics), pages 612?621, Uppsala, Sweden, July 2010.
Nicola Ueffing and Hermann Ney. Word-level confidence
estimation for machine translation using phrased-based
translation models. In Proceedings of Human Lan-
guage Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages 763?
770, Vancouver, 2005.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. Con-
fidence measures for statistical machine translation. In
Proceedings of the MT Summit IX, pages 394?401, New
Orleans, LA, September 2003.
Deyi Xiong, Min Zhang, and Haizhou Li. Error detection
for statistical machine translation using linguistic features.
In Proceedings of the 48th Association for Computational
Linguistics, pages 604?611, Uppsala, Sweden, July 2010.
391
Workshop on Humans and Computer-assisted Translation, pages 1?9,
Gothenburg, Sweden, 26 April 2014.
c?2014 Association for Computational Linguistics
Word Confidence Estimation for SMT N-best List Re-ranking
Ngoc-Quang Luong Laurent Besacier
LIG, Campus de Grenoble
41, Rue des Math?ematiques,
UJF - BP53, F-38041 Grenoble Cedex 9, France
{ngoc-quang.luong,laurent.besacier,benjamin.lecouteux}@imag.fr
Benjamin Lecouteux
Abstract
This paper proposes to use Word Confi-
dence Estimation (WCE) information to
improve MT outputs via N-best list re-
ranking. From the confidence label as-
signed for each word in the MT hypoth-
esis, we add six scores to the baseline log-
linear model in order to re-rank the N-best
list. Firstly, the correlation between the
WCE-based sentence-level scores and the
conventional evaluation scores (BLEU,
TER, TERp-A) is investigated. Then, the
N-best list re-ranking is evaluated over dif-
ferent WCE system performance levels:
from our real and efficient WCE system
(ranked 1st during last WMT 2013 Quality
Estimation Task) to an oracle WCE (which
simulates an interactive scenario where a
user simply validates words of a MT hy-
pothesis and the new output will be auto-
matically re-generated). The results sug-
gest that our real WCE system slightly (but
significantly) improves the baseline while
the oracle one extremely boosts it; and bet-
ter WCE leads to better MT quality.
1 Introduction
A number of methods to improve MT hypothe-
ses after decoding have been proposed in the past,
such as: post-editing, re-ranking or re-decoding.
Post-editing (Parton et al., 2012) is a human-
inspired task where the machine post edits trans-
lations in a second automatic pass. In re-ranking
(Zhang et al., 2006; Duh and Kirchhoff, 2008;
Bach et al., 2011), more features are used along
with the multiple model scores for re-determining
the 1-best among N-best list. Meanwhile, re-
decoding process (Venugopal et al., 2007) inter-
venes directly into the decoder?s search graph (e.g.
adds more reward or penalty scores), driving it to
another better path.
This work aims at re-ranking the N-best list to im-
prove MT quality. Generally, during the transla-
tion task, the decoder traverses through paths in
its search space, computes the objective function
values for them and outputs the one with high-
est score as the best hypothesis. Besides, those
with lower scores can also be generated in a so-
called N-best list. The decoder?s function consists
of parameters from different models, such as trans-
lation, distortion, word penalties, reordering, lan-
guage models, etc. In the N-best list, although the
current 1-best beats the remains in terms of model
score, it might not be exactly the closest to the hu-
man reference. Therefore, adding more decoder
independent features would be expected to raise
up a better candidate. In this work, we build six
additional features based on the labels predicted
by our Word Confidence Estimation (WCE) sys-
tem, then integrate them with the existing decoder
scores for re-ranking hypotheses in the N-best
list. More precisely, in the second pass, our re-
ranker aggregates over decoder and WCE-based
weighted scores and utilizes the obtained sum to
sort out the best candidate. The novelty of this pa-
per lies on the following contributions: the corre-
lation between WCE-based sentence-level scores
and conventional evaluation scores (BLEU, TER,
TERp-A) is first investigated. Then, we conduct
the N-best list re-ranking over different WCE sys-
tem performance levels: starting by a real WCE,
passing through several gradually improved (sim-
ulated) systems and finally the ?oracle? one. From
these in-depth experiments, the role of WCE in
improving MT quality via re-ranking N-best list
is confirmed and reinforced.
The remaining parts of this article are organized
as follows: in section 2 we summarize some out-
standing approaches in N-best list re-ranking as
well as in WCE. Section 3 describes our WCE sys-
tem construction, followed by proposed features.
1
The experiments along with results and in-depth
analysis of WCE scores? contribution (as WCE
system gets better) are presented in Section 4 and
Section 5. The last section concludes the paper
and points out some ongoing work.
2 Related Work
2.1 N-best List Re-ranking
Walking through various related work concern-
ing this issue, we observe some prominent ideas.
The first attempt focuses on proposing additional
Language Models. Kirchhoff and Yang (2005)
train one word-based 4-gram model (with modi-
fied Kneser-Ney smoothing) and one factored tri-
gram one, then combine them with seven decoder
scores for re-ranking N-best lists of several SMT
systems. Their proposed LMs increase the transla-
tion quality of the baselines (measured by BLEU
score) from 21.6 to 22.0 (Finnish - English), or
from 30.5 to 31.0 (Spanish - English). Meanwhile,
Zhang et al. (2006) experiment a distributed LM
where each server, among the total of 150, hosts a
portion of the data and responses its client, allow-
ing them to exploit an extremely large corpus (2.7
billion word English Gigaword) for estimating N-
gram probability. The quality of their Chinese
- English hypotheses after the re-scoring process
by using this LM is improved 4.8% (from BLEU
31.44 to 32.64, oracle score = 37.48).
In one other direction, several authors propose to
replace the current linear scoring function used by
the decoder by more efficient functions. Sokolov
et al. (2012) learn their non-linear scoring function
in a learning-to-rank paradigm, applying Boosting
algorithm. Their gains on the WMT?{10, 11, 12}
are shown modest yet consistent and higher than
those based on linear scoring functions. Duh and
Kirchhoff (2008) use Minimum Error Rate Train-
ing (MERT) (Och, 2003) as a weak learner and
build their own solution, BoostedMERT, a highly-
expressive re-ranker created by voting among mul-
tiple MERT ones. Their proposed model dramat-
ically beats the decoder?s log-linear model (43.7
vs. 42.0 BLEU) in IWSLT 2007 Arabic - English
task. Applying solely goodness (the sentence con-
fidence) scores, Bach et al. (2011) obtain very con-
sistent TER reductions (0.7 and 0.6 on the dev and
test set) after a 5-list re-ranking for their Arabic -
English SMT hypotheses. This latter work is the
one that is the most related to our paper. However,
the major differences are: (1) our proposed sen-
tence scores are computed based on word confi-
dence labels; and (2) we perform an in-depth study
of the use of WCE for N-best reranking and assess
its usefulness in a simulated interactive scenario.
2.2 Word Confidence Estimation
Confidence Estimation (CE) is the task of iden-
tifying the correct parts and detecting the trans-
lation errors in MT output. If the error is pre-
dicted for each word, this becomes WCE. The in-
teresting uses of WCE include: pointing out the
words that need to be corrected by the post-editor,
telling readers about the reliability of a specific
portion, and selecting the best segments among
options from multiple translation systems for com-
bination.
Dealing with this problem, various approaches
have been proposed: Blatz et al. (2003) combine
several features using neural network and naive
Bayes learning algorithms. One of the most ef-
fective feature combinations is the Word Posterior
Probability (WPP) as suggested by Ueffing et al.
(2003) associated with IBM-model based features
(Blatz et al., 2004). Ueffing and Ney (2005)
propose an approach for phrase-based translation
models: a phrase is a sequence of contiguous
words and is extracted from the word-aligned
bilingual training corpus. The confidence value
of each word is then computed by summing over
all phrase pairs in which the target part contains
this word. Xiong et al. (2010) integrate target
word?s Part-Of-Speech (POS) and train them by
Maximum Entropy Model, allowing significative
gains in comparison to WPP features. The novel
features from source side, alignment context, and
dependency structure (Bach et al., 2011) help to
augment marginally in F-score as well as the Pear-
son correlation with human judgment. Other ap-
proaches are based on external features (Soricut
and Echihabi, 2010; Felice and Specia, 2012) al-
lowing to cope with various MT systems (e.g. sta-
tistical, rule based etc.). Among the numerous
WCE applications, we consider its contribution in
a specific step of SMT pipeline: N-best list re-
ranking. Our WCE system and the proposed re-
ranking features are presented in the next section.
3 Our Approach
Our approach can be expressed in three steps: in-
vestigate the potential of using word-level score in
N-best list re-ranking, build the WCE system and
2
extract additional features to integrate with the ex-
isting log-linear model.
3.1 Investigating the correlation between
?word quality? scores and other metrics
Firstly, we investigate the correlation between
sentence-level scores (obtained from WCE labels)
and conventional evaluation scores (BLEU (Pa-
pineni et al., 2002), TER and TERp-A (Snover
et al., 2008)). For each sentence, a word quality
score (WQS) is calculated by:
WQS =
#
??
G
??
(good) words
#words
(1)
In other words, we are trying to answer the fol-
lowing question: can the high percentage of ?G?
(good) words (predicted by WCE system) in a
MT output ensure its possibility of having a better
BLEU and low TER (TERp-A) value ? This inves-
tigation is a strong prerequisite for further exper-
iments in order to check that WCE scores do not
bring additional ?noise? to the re-ranking process.
In this experiment, we compute WQS over our en-
tire French - English data set (total of 10,881 1-
best translations) for which WCE oracle labels are
available (see Section 3.2 to see how they were ob-
tained). The results are plotted in Figure 1, where
the y axis shows the ?G? (good) word percent-
age, and the x axis shows BLEU (1a), TER (1b) or
TERp-A (1c) scores. It can be seen from Figure 1
that the major parts of points (the densest areas) in
all three cases conform the common tendency: In
Figure 1a, the higher ?G? percentage, the higher
BLEU is; on the contrary, in Figure 1b (Figure
1c), the higher ?G? percentage, the lower TER
(TERp-A) is. We notice some outliers, i.e. sen-
tences with most or almost words labeled ?good?,
yet still have low BLEU or high TER (TERp-A)
scores. This phenomenon is to be expected when
many (unknown) source words are not translated
or when the (unique) reference is simply too far
from the hypothesis. Nevertheless, the informa-
tion extracted from oracle WCE labels seems use-
ful to build an efficient re-ranker.
3.2 WCE System Preparation
Essentially, a WCE system construction consists
of two pivotal elements: the features (the SMT
system dependent or independent information
extracted for each word to represent its char-
acteristics) and the machine learning method
(to train the prediction model). Motivated
Figure 1: The correlation between WQS in a sen-
tence and its overall quality measured by : (a)
BLEU, (b) TER and (c) TERp-A metrics
by the idea of addressing WCE problem as
a sequence labeling process, we employ the
Conditional Random Fields (CRFs) for our model
training, with WAPITI toolkit (Lavergne et al.,
2010). Basically, CRF computes the probabil-
ity of the output sequence Y = (y
1
, y
2
, ..., y
N
)
given the input sequenceX = (x
1
, x
2
, ..., x
N
) by:
3
p?
(Y |X) =
1
Z
?
(X)
exp
{
K
?
k=1
?
k
F
k
(X,Y )
}
(2)
where F
k
(X,Y ) =
?
T
t=1
f
k
(y
t?1
, y
t
, x
t
);
{f
k
} (k = 1,K) is a set of feature functions;
{?
k
} (k = 1,K) are the associated parameter val-
ues; and Z
?
(x) is the normalization function.
In terms of features, a number of knowledge
sources are employed for extracting them, result-
ing in the major types listed below. We briefly
summarize them in this work, further details about
total of 25 features can be referred in (Luong et al.,
2013a).
? Target Side: target word; bigram (trigram)
backward sequences; number of occurrences
? Source Side: source word(s) aligned to the
target word
? Alignment Context: the combinations of the
target (source) word and all aligned source
(target) words in the window ?2
? Word posterior probability
? Pseudo-reference (Google Translate):
whether the current word appears in the
pseudo reference or not
1
?
? Graph topology: number of alternative paths
in the confusion set, maximum and minimum
values of posterior probability distribution
? Language model (LM) based: length of the
longest sequence of the current word and its
previous ones in the target (resp. source) LM.
For example, with the target word w
i
: if the
sequence w
i?2
w
i?1
w
i
appears in the target
LM but the sequence w
i?3
w
i?2
w
i?1
w
i
does
not, the n-gram value for w
i
will be 3.
? Lexical Features: word?s Part-Of-Speech
(POS); sequence of POS of all its aligned
source words; POS bigram (trigram) back-
ward sequences; punctuation; proper name;
numerical
? Syntactic Features: Null link; constituent la-
bel; depth in the constituent tree
? Semantic Features: number of word senses in
WordNet.
Interestingly, this feature set was also used in our
English - Spanish WCE system which got the first
1
This is our first-time experimented feature and does not
appear in (Luong et al., 2013a)
rank in WMT 2013 Quality Estimation Shared
Task (Luong et al., 2013b).
For building the WCE training and test sets, we
use a dataset of 10,881 French sentences (Potet
et al., 2012) , and apply a baseline SMT system
to generate hypotheses (1000-best list). Our base-
line SMT system (presented for WMT 2010 eval-
uation) keeps the Moses?s default setting (Koehn
et al., 2007): log-linear model with 14 weighted
feature functions. The translation model is trained
on the Europarl and News parallel corpora of
WMT10
2
evaluation campaign (1,638,440 sen-
tences). The target language model is trained by
the SRI language modeling toolkit (Stolcke, 2002)
on the news monolingual corpus (48,653,884 sen-
tences).
Translators were then invited to correct MT out-
puts, giving us the same amount of post editions
(Potet et al., 2012). The set of triples (source,
hypothesis, post edition) is then divided into the
training set (10000 first triples) and test set (881
remaining). To train the WCE model, we ex-
tract all above features for words of the 1-best hy-
potheses of the training set. For the test set, the
features are built for all 1000 best translations of
each source sentence. Another essential element
is the word?s confidence labels (or so-called WCE
oracle labels) used to train the prediction model
as well as to judge the WCE results. They are
set by using TERp-A toolkit (Snover et al., 2008)
in one of the following classes: ?I? (insertions),
?S? (substitutions), ?T? (stem matches), ?Y? (syn-
onym matches), ?P? (phrasal substitutions), ?E?
(exact matches) and then simplified into binary
class: ?G? (good word) or ?B? (bad word) (Lu-
ong et al., 2013a).
Once having the prediction model built with all
features, we apply it on the test set (881 x 1000
best = 881000 sentences) and get needed WCE la-
bels. Figure 2 shows an example about the classi-
fication results for one sentence. Comparing with
the reference labels, we can point out easily the
correct classifications for ?G? words (e.g. in case
of operation, added) and for ?B? words (e.g. is,
have), as well as classification errors (e.g. a, com-
bat). According to the Precision (Pr), Recall (Rc)
and F-score (F) shown in Table 1, our WCE sys-
tem reaches very promising performance in pre-
dicting ?G? label, and acceptable for ?B? label.
These labels will be used to calculate our proposed
2
http://www.statmt.org/wmt10/
4
Figure 2: Example of our WCE classification results for one MT hypothesis
features (section 3.3).
Label Pr(%) Rc(%) F(%)
Good (G) 84.36 91.22 87.65
Bad (B) 51.34 35.95 42.29
Table 1: Pr, Rc and F for ?G? and ?B? labels of
our WCE system
3.3 Proposed Features
Since the scores resulted from the WCE system
are for words, we have to synthesize them in sen-
tence level scores for integrating with the 14 de-
coder scores. Six proposed scores involve:
? The ratio of number of good words to total
number of words. (1 score)
? The ratio of number of good nouns (verbs) to
total number of nouns (verbs)
3
. (2 scores)
? The ratio of number of n consecutive good
word sequences to the total number of con-
secutive word sequences ; n=2, n=3 and n=4.
(3 scores)
For instance, in case of the hypothesis in Figure 2:
among the total of 18 words, we have 12 labeled
as ?G?; and 7 out of 17 word pairs (bigram) are
labeled as ?GG?, etc. Hence, some of the above
3
We decide not to experiment with adjectives, adverbs and
conjunctions since their number can be 0 in many cases.
scores can be written as:
#good words
#words
=
12
18
= 0.667
#good bigrams
#bigrams
=
7
17
= 0.4118
#good trigrams
#trigrams
=
3
16
= 0.1875
(3)
With the features simply derived from WCE labels
and not from CRF model scores (i.e. the probabil-
ity p(G), p(B)) , we expect to spread out the eval-
uation up to the ?oracle? setting, where the users
validate a word as ?G? or ?B? without providing
any confidence score.
4 Experiments
4.1 Experimental Settings
As described in Section 3.2, our SMT system gen-
erates 1000-best list for each source sentence, and
among them, the best hypothesis was determined
by using the objective function based on 14 de-
coder scores, including: 7 reordering scores, 1 lan-
guage model score, 5 translation model scores and
1 word penalty score. Initially, all six additional
WCE-based scores are weighted as 1.0. Then,
two optimization methods: MERT and Margin
Infused Relaxed Algorithm (MIRA) (Watanabe
et al., 2007) are applied to optimize the weights of
all 20 scores of the re-ranker. In both methods, we
carry out a 2-fold cross validation on the N-best
5
Systems MERT MIRA
BLEU TER TERp-A BLEU TER TERp-A
BL 52.31 0.2905 0.3058 50.69 0.3087 0.3036
BL+OR 58.10 0.2551 0.2544 55.41 0.2778 0.2682
BL+WCE 52.77 0.2891 0.3025 51.01 0.3055 0.3012
WCE + 25% 53.45 0.2866 0.2903 51.33 0.3010 0.2987
WCE + 50% 55.77 0.2730 0.2745 53.63 0.2933 0.2903
WCE + 75% 56.40 0.2687 0.2669 54.35 0.2848 0.2822
Oracle BLEU score BLEU=60.48
Table 2: Translation quality of the baseline system (only decoder scores) and that with additional scores
from real ?WCE? or ?oracle? WCE system
System MERT
Better Equivalent Worse
BL+WCE 159 601 121
BL+OR 517 261 153
WCE+25% 253 436 192
WCE+50% 320 449 112
WCE+75% 461 243 177
Table 3: Quality comparison (measured by TER) between the baseline and two integrated systems in
details (How many sentences are improved, kept equivalent or degraded, out of 881 test sentences?
test set. In other words, we split our N-best test
set into two equivalent subsets: S1 and S2. Play-
ing the role of a development set, S1 will be used
to optimize the 20 weights for re-ranking S2 (and
vice versa). Finally two result subsets (new 1-best
after re-ranking process) are merged for evalua-
tion. To better acknowledge the impact of the pro-
posed scores, we calculate them not only using our
real WCE system, but also using an oracle WCE
(further called ?WCE scores? and ?oracle scores?,
respectively). To summarize, we experiment with
the three following systems:
? BL: Baseline SMT system with 14 above de-
coder scores
? BL+WCE: Baseline + 6 real WCE scores
? BL+OR: Baseline + 6 oracle WCE scores
(simulating an interactive scenario).
4.2 Results and Analysis
The translation quality of BL, BL+WCE and
BL+OR, optimized by MERT and MIRA method
are reported in Table 2. Meanwhile, Table 3
depicts in details the number of sentences in
the two integrated systems which outperform, re-
main equivalent or degrade the baseline hypoth-
esis (when match against the references, mea-
sured by TER). It can be observed from Table
2 that the integration of oracle scores signifi-
cantly boosts the MT output quality, measured
by all three metrics and optimized by both meth-
ods employed. We gained 5.79 and 4.72 points
in BLEU score, by MERT and MIRA (respec-
tively). With TER, BL+OR helps to gain 0.03
point in both two methods. Meanwhile, in case of
TERp-A, the improvement is 0.05 point for MERT
and 0.03 point for MIRA. It is worthy to mention
that the possibility of obtaining such oracle labels
is definitely doable through a human-interaction
scenario (which could be built from a tool like
PET (Post-Editing Tool) (Aziz et al., 2012) for
instance). In such an environment, once having
the hypothesis produced by the first pass (trans-
lation task), the human editor could simply click
on words considered as bad (B), the other words
being implicitly considered as correct (G).
Breaking down the analysis into sentence level,
as described on Table 3, BL+OR (MERT) yields
nearly 59% (517 over 881) better outputs than the
baseline and only 17% of worse ones. Further-
more, Table 2 shows that in case of our test set, op-
timizing by MERT is pretty more beneficial than
MIRA (we do not have a clear explanation of this
yet).
For more insightful understanding about WCE
scores? acuteness, we make a comparison with
6
the most possible optimal BLEU score that could
be obtained from the N-best list. Applying the
sentence-level BLEU+1 (Nakov et al., 2012) met-
ric over candidates in the list, we are able to se-
lect the one with highest score and aggregate all
of them in an oracle-best translation; the result-
ing performance obtained is 60.48. This score
accounts for a fact that the simulated interactive
scenario (BL+OR) lacks only 2.38 points (in case
of MERT) to be optimal and clearly overpass the
baseline (8.17 points below the best score).
The contribution of a real WCE system seems
more modest: BL+WCE marginally increases
BLEU scores of BL (0.46 gain in case of opti-
mizing by MERT and 0.32 by MIRA). For both
TER and TERp-A metric, the progressions are
also negligible. To verify the significance of this
result, we estimate the p-value between BLEU of
BL+WCE system and BLEU of baseline BL rely-
ing on Approximate Randomization (AR) method
(Clark et al., 2011) which indicates if the improve-
ment yielded by the optimized system is likely
to be generated again by some random processes
(randomized optimizers). After various optimizer
runs, we selected randomly 5 optimizer outputs,
perform the AR test and obtain a p-value of 0.01.
This result reveals that the improvement yielded
by BL+WCE is significative although small, orig-
inated from the contribution of WCE score, not
by any optimizer variance. This modest but pos-
itive change in BLEU score using WCE features,
encourages us to investigate and analyze further
about WCE scores? impact, supposing WCE per-
formance is getting better. More in-depth analysis
is presented in the next section.
5 Further Understanding of WCE scores
role in N-best Re-ranking via
Improvement Simulation
We think it would be very interesting and useful
to answer the following question: do WCE scores
really effectively help to increase MT output qual-
ity when the WCE system is getting better and
better? To do this, our proposition is as follows:
firstly, by using the oracle labels, we filter out all
wrongly classified words in the test set and push
them into a temporary set, called T. Then, we cor-
rect randomly a percentage (25%, 50%, or 75%)
of labels in T. Finally, the altered T will be inte-
grated back with the correctly predicted part (by
the WCE system) in order to form a new ?simu-
lated? result set. This strategy results in three ?vir-
tual? WCE systems called ?WCE+N%? (N=25,
50 or 75), which use 14 decoder scores and 6 ?sim-
ulated? WCE scores. Table 4 shows the perfor-
mance of these systems in term of F score (%).
From each of the above systems, the whole exper-
System F(?G?) F(?B?) Overall F
WCE+25% 89.87 58.84 63.51
WCE+50% 93.21 73.09 76.11
WCE+75% 96.58 86.87 88.33
Oracle labels 100 100 100
Table 4: The performances (Fscore) of simulated
WCE systems
imental setting is identical to what we did with the
original WCE and oracle systems: six scores are
built and combined with existing 14 system scores
for each hypothesis in the N-best list. After that,
MERT and MIRA methods are invoked to opti-
mize their weights, and finally the reordering is
performed thanks to these scores and appropriate
optimal weights. The translation quality measured
by BLEU, TER and TERp-A after re-ranking us-
ing ?WCE+N%? (N=25,50,75) can be seen also
in Table 2. The number of translations which out-
perform, keep intact and decline in comparison to
the baseline are shown in Table 3 for MERT opti-
mization.
We note that all obtained scores fit our guess and
expectation: the better performance WCE system
reaches, the clearer its role in improving MT out-
put quality. Diminishing 25% of the wrongly pre-
dicted words leads to a gain 0.68 point (by MERT)
and 0.32 (by MIRA) in BLEU score. More sig-
nificant increases of BLEU 3.00 and BLEU 3.63
(MERT) can be achieved when prediction errors
are cut off up to 50% and 75%. Figure 3 presents
an overview of the results obtained and helps us
to predict the MT improvements expected if the
WCE system improves in the future. Table 5
shows several examples where WCE scores drive
SMT system to better reference-correlated hypoth-
esis. In the first example, the baseline generates
the hypothesis in which the source phrase ?pour
sa part? remains untranslated. On the contrary,
WCE+50% overcomes this drawback by result-
ing in a correct translation phrase: ?for his part?.
The latter translation needs only one edit opera-
tion (shift for ?Bettencourt-Meyers?) to become
its reference. In example 2, BL+OR selects the
7
Example 1 (from WCE+50%)
Source Pour sa part , l? avocat de Franc?oise Bettencourt-Meyers , Olivier
Metzner , s? est f?elicit?e de la d?ecision du tribunal .
Hypothesis (Baseline SMT) The lawyer of Bettencourt-Meyers Franc?oise , Olivier Metzner ,
welcomed the court ?s decision .
Hypothesis (SMT+WCE
scores)
For his part , the lawyer of Bettencourt-Meyers Franc?oise ,
Olivier Metzner , welcomed the court ?s decision .
Post-edition For his part , the lawyer of Franc?oise Bettencourt-Meyers ,
Olivier Metzner , welcomed the court ?s decision .
Example 2 (from BL+OR)
Source Pour l? otre , l? accord risque ? de creuser la tombe d? un tr`es
grand nombre de pme du secteur dans les 12 prochains mois ? .
Hypothesis (Baseline MT) For the otre the agreement is likely to deepen the grave of a very
large number of smes in the sector in the next 12 months ? .
Hypothesis (SMT+WCE
scores)
For the otre agreement , the risk ? digging the grave of a very
large number of medium-sized businesses in the next 12 months ?
.
Post-edition For the otre , the agreement risks ? digging the grave of a very
large number of small- and medium-sized businesses in the next
12 months ? .
Table 5: Examples of MT hypothesis before and after reranking using the additional scores from
WCE+50% (Example 1) and BL+OR (Example 2) system
Figure 3: Comparison of the performance of var-
ious systems: the integrations of WCE features,
which the quality increases gradually, lead to the
linear improvement of translation outputs.
better hypothesis, in which the phrases ?creuser
la tombe? and ??pme du secteur? are translated
into ?digging the grave? and ?medium-sized busi-
nesses?, respectively, better than those of the base-
line (?deepen the grave? and ?smes in the sec-
tor?).
6 Conclusions And Perspectives
So far, the word confidence scores have been
exploited in several applications, e.g. post-
editing, sentence quality assessment or multiple
MT-system combination, yet very few studies (ex-
cept Bach et al. (2011) ) propose to investigate
them for boosting MT quality. Thus, this pa-
per proposed several features extracted from a
WCE system and combined them with existing de-
coder scores for re-ranking N-best lists. Our WCE
model is built using CRFs, on a variety of types of
features for the French - English SMT task. Due
to its limitations in predicting translation errors
(?B? label), WCE scores ensure only a modest im-
provement in translation quality over the baseline
SMT. Nevertheless, further experiments about the
simulation of WCE performance suggest that such
types of score contribute dramatically if they are
built from an accurate WCE system. They also
show that with the help of an ?ideal? WCE, the
MT system reaches quite close to its most optimal
possible quality. These scores are totally indepen-
dent from the decoder, they can be seen as a way
to introduce lexical, syntactic and semantic infor-
mation (used for WCE) in a SMT pipeline.
As future work, we plan to focus on augmenting
our WCE performance using more linguistic fea-
tures as well as advanced techniques (feature se-
lection, Boosting method...). In the same time, we
would like to integrate the WCE scores in the de-
coder?s search graph to redirect the decoding pro-
cess (preliminary experiments, not reported here
yet, have shown that this is a very promising av-
enue of research).
8
References
Wilker Aziz, Sheila C. M. de Sousa, and Lucia Specia. Pet:
a tool for post-editing and assessing machine translation.
In Proceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Istanbul,
Turkey, May 23-25 2012.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. Goodness:
A method for measuring machine translation confidence.
In Proceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 211?219, Port-
land, Oregon, June 19-24 2011.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. Confidence estimation for machine trans-
lation. Technical report, JHU/CLSP Summer Workshop,
2003.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. Confidence estimation for machine trans-
lation. In Proceedings of COLING 2004, pages 315?321,
Geneva, April 2004.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.
Better hypothesis testing for statistical machine transla-
tion: Controlling for optimizer instability. In Proceedings
of the Association for Computational Lingustics, 2011.
Kevin Duh and Katrin Kirchhoff. Beyond log-linear models:
Boosted minimum error rate training for n-best re-ranking.
In Proc. of ACL, Short Papers, 2008.
Mariano Felice and Lucia Specia. Linguistic features for
quality estimation. In Proceedings of the 7th Workshop on
Statistical Machine Translation, pages 96?103, Montreal,
Canada, June 7-8 2012.
Katrin Kirchhoff and Mei Yang. Improved language model-
ing for statistical machine translation. In Proceedings of
the ACL Workshop on Building and Using Parallel Texts,
pages 125?128, Ann Arbor, Michigan, June 2005.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. Moses: Open source toolkit for statisti-
cal machine translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguistics,
pages 177?180, Prague, Czech Republic, June 2007.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon. Practi-
cal very large scale crfs. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics,
pages 504?513, 2010.
Ngoc Quang Luong, Laurent Besacier, and Benjamin Lecou-
teux. Word confidence estimation and its integration in
sentence quality estimation for machine translation. In
Proceedings of The Fifth International Conference on
Knowledge and Systems Engineering (KSE 2013), Hanoi,
Vietnam, October 17-19 2013a.
Ngoc Quang Luong, Benjamin Lecouteux, and Laurent Be-
sacier. LIG system for WMT13 QE task: Investigating the
usefulness of features in word confidence estimation for
MT. In Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 396?391, Sofia, Bulgaria,
August 2013b. Association for Computational Linguistics.
Preslav Nakov, Francisco Guzman, and Stephan Vogel. Op-
timizing for sentence-level bleu+1 yields short transla-
tions. In Proceedings of COLING 2012, pages 1979?1994,
Mumbai, India, December 8 -15 2012.
Franz Josef Och. Minimum error rate training in statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics,
pages 160?167, July 2003.
Kishore Papineni, Salim Roukos, Todd Ard, and Wei-Jing
Zhu. Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, 2002.
Kristen Parton, Nizar Habash, Kathleen McKeown, Gonzalo
Iglesias, and Adri`a de Gispert. Can automatic post-editing
make mt more meaningful? In Proceedings of the 16th
EAMT, pages 111?118, Trento, Italy, 28-30 May 2012.
M Potet, R Emmanuelle E, L Besacier, and H Blanchon.
Collection of a large database of french-english smt out-
put corrections. In Proceedings of the eighth interna-
tional conference on Language Resources and Evaluation
(LREC), Istanbul, Turkey, May 2012.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. Terp system description. In MetricsMATR
workshop at AMTA, 2008.
Artem Sokolov, Guillaume Wisniewski, and Francois Yvon.
Non-linear n-best list reranking with few features. In Pro-
ceedings of AMTA, 2012.
Radu Soricut and Abdessamad Echihabi. Trustrank: Inducing
trust in automatic translations via ranking. In Proceedings
of the 48th ACL (Association for Computational Linguis-
tics), pages 612?621, Uppsala, Sweden, July 2010.
Andreas Stolcke. Srilm - an extensible language model-
ing toolkit. In Seventh International Conference on Spo-
ken Language Processing, pages 901?904, Denver, USA,
2002.
Nicola Ueffing and Hermann Ney. Word-level confidence
estimation for machine translation using phrased-based
translation models. In Proceedings of Human Lan-
guage Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages 763?
770, Vancouver, 2005.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. Con-
fidence measures for statistical machine translation. In
Proceedings of the MT Summit IX, pages 394?401, New
Orleans, LA, September 2003.
Ashish Venugopal, Andreas Zollmann, and Stephan Vogel.
An efficient two-pass approach to synchronous-cfg driven
statistical mt. In Proceedings of Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguistics,
April 2007.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. Online large-margin training for statistical ma-
chine translation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning,
pages 64?773,, Prague, Czech Republic, June 2007.
Deyi Xiong, Min Zhang, and Haizhou Li. Error detection
for statistical machine translation using linguistic features.
In Proceedings of the 48th Association for Computational
Linguistics, pages 604?611, Uppsala, Sweden, July 2010.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
Distributed language modeling for n-best list re-ranking.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing (EMNLP 2006),
pages 216?223, Sydney, July 2006.
9
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 335?341,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
LIG System for Word Level QE task at WMT14
Ngoc-Quang Luong Laurent Besacier
LIG, Campus de Grenoble
41, Rue des Math?ematiques,
UJF - BP53, F-38041 Grenoble Cedex 9, France
{ngoc-quang.luong,laurent.besacier,benjamin.lecouteux}@imag.fr
Benjamin Lecouteux
Abstract
This paper describes our Word-level QE
system for WMT 2014 shared task on
Spanish - English pair. Compared to
WMT 2013, this year?s task is different
due to the lack of SMT setting information
and additional resources. We report
how we overcome this challenge to retain
most of the important features which
performed well last year in our system.
Novel features related to the availability of
multiple systems output (new point of this
year) are also proposed and experimented
along with baseline set. The system
is optimized by several ways: tuning
the classification threshold, combining
with WMT 2013 data, and refining
using Feature Selection strategy on our
development set, before dealing with the
test set for submission.
1 Introduction
1.1 Overview of task 2 in WMT14
This year WMT calls for methods which predict
the MT output quality at run-time, on both levels:
sentence (Task 1) and word (Task 2). Towards
a SMT system-independent and widely-applied
estimation, MT outputs are collected from
multiple translation means (machine and human),
therefore all SMT specific settings (and the
associated features that could have been extracted
from it) become unavailable. This initiative puts
more challenges on participants, yet motivates
number of SMT-unconventional approaches and
inspires the endeavors aiming at an ?Evaluation
For All?.
We focus our effort on Task 2 (Word-level QE),
where, unlike in WMT2013, participants are
requested to generate prediction labels for words
in three variants:
? Binary: words are judged as Good (no
translation error), or Bad (need for editing).
? Level 1: the Good class is kept intact,
whereas Bad one is further divided into
subcategories: Accuracy issue (the word does
not accurately reflect the source text) and
Fluency issue (the word does not relate to the
form or content of the target text).
? Multi-class: more detailed judgement, where
the translation errors are further decomposed
into 16 labels based on MQM
1
metric.
1.2 Related work
WMT 2013 witnessed several attempts dealing
with this evaluation type in its first launch. Han
et al. (2013); Luong et al. (2013) employed the
Conditional Random Fields (CRF) (Lafferty et al.,
2001) model as their Machine Learning method
to address the problem as a sequence labeling
task. Meanwhile, Bicici (2013) extended the
global learning model by dynamic training with
adaptive weight updates in the perceptron training
algorithm. As far as prediction indicators are
concerned, Bicici (2013) proposed seven word
feature types and found among them the ?common
cover links? (the links that point from the leaf
node containing this word to other leaf nodes
in the same subtree of the syntactic tree) the
most outstanding. Han et al. (2013) focused
only on various n-gram combinations of target
words. Inheriting most of previously-recognized
features, Luong et al. (2013) integrated a number
of new indicators relying on graph topology,
pseudo reference, syntactic behavior (constituent
label, distance to the semantic tree root) and
polysemy characteristic. Optimization endeavors
were also made to enhance the baseline, including
classification threshold tuning, feature selection
and boosting technique (Luong et al., 2013).
1
http://www.qt21.eu/launchpad/content/training
335
1.3 Paper outline
The rest of our paper is structured as follows:
in the next section, we describe 2014 provided
data for Task 2, and the additional data used
to train the system. Section 3 lists the entire
feature set, involving WMT 2013 set as well as
a new feature proposed for this year. Baseline
system experiments and methods for optimizing it
are furthered discussed in Section 4 and Section
5 respectively. Section 6 selects the most
outstanding system for submission. The last
section summarizes the approach and opens new
outlook.
2 Data and Supporting Resources
For English - Spanish language pair in Task 2,
the organizers released two bilingual data sets:
the training and the test ones. The training
set contains 1.957 MT outputs, in which each
token is annotated with one appropriate label.
In the binary variant, the words are classified
into ?OK? (no translation error) or ?BAD? (edit
operators needed) label. Meanwhile, in the level
1 variant, they belong to ?OK?, ?Accuracy?
or ?Fluency? (two latter ones are divided from
?BAD? label of the first subtask). In the last
variant, multi-class, beside ?Accuracy? and
?Fluency? we have further 15 labels based on
MQM metric: Terminology, Mistranslation,
Omission, Addition, Untranslated, Style/register,
Capitalization, Spelling, Punctuation,
Typography, Morphology (word form),
Part of speech, Agreement, Word order,
Function words, Tense/aspect/mood, Grammar
and Unintelligible. The test set consists of 382
sentences where all the labels accompanying
words are hidden. For optimizing parameters of
the classifier, we extract last 200 sentences from
the training set to form a development (dev) set.
Besides, the Spanish - English corpus provided in
WMT 2013 (total of 1087 tuples) is also exploited
to enrich our WMT 2014 system. Unfortunately,
2013 data can only help us in the binary variant,
due to the discrepancy in training labels. Some
statistics about each set can be found in Table 1.
In addition, additional (MT-independent)
resources are used for the feature extraction,
including:
? Spanish and English Word Language Models
(LM)
? Spanish and English POS Language Models
? Spanish - English 2013 MT system
On the contrary, no specific MT setting is provided
(e.g. the code to re-run Moses system like
WMT 2013), leading to the unavailability of some
crucial resources, such as the N-best list and
alignment information. Coping with this, we
firstly thought of using the Moses ?Constrained
Decoding? option as a method to tie our (already
available) decoder?s output to the given target
translations (this feature is supported by the
latest version of Moses (Koehn et al., 2007) in
2013). Our hope was that, by doing so, both
N-best list and alignment information would be
generated during decoding. But the decoder
failed to output all translations (only 1/4 was
obtained) when the number of allowed unknown
words (-max-unknowns) was set as 0. Switching
to non zero value for this option did not help
either since, even if more outputs were generated,
alignment information was biased in that case
due to additional/missing words in the obtained
MT output. Ultimately, we decided to employ
GIZA++ toolkit (Och and Ney, 2003) to obtain
at least the alignment information (and associated
features) between source text and target MT
output. However, no N-best list were extracted
nor available as in last year system. Nevertheless,
we tried to extract some features equivalent to
last year N-best features (details can be found in
Section 3.2).
3 Feature Extraction
In this section, we briefly list out all the
features used in WMT 2013 (Luong et al.,
2013) that were kept for this year, followed
by some proposed features taking advantage of
the provided resources and multiple translation
system outputs (for a same source sentence).
3.1 WMT13 features
? Source word features: all the source words
that align to the target one, represented in
BIO
2
format.
? Source alignment context features: the
combinations of the target word and one
word before (left source context) or after
(right source context) the source word
aligned to it.
2
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
336
Statistics WMT2014 WMT2013
train dev test train dev test
#segments 1757 200 382 753 50 284
#words 40975 6436 9613 18435 1306 7827
%G (OK) : %B (BAD) 67 : 33 58 : 42 - 70 : 30 77 : 23 75 : 25
Table 1: Statistics of corpora used in LIG?s system. We use the notion name+year to indicate the dataset.
For instance, train14 stands for the training set of WMT14
? Target alignment context features: the
combinations of the source word and each
word in the window ?2 (two before, two
after) of the target word.
? Backoff Behaviour: a score assigned to the
word according to how many times the target
Language Model has to back-off in order to
assign a probability to the word sequence, as
described in (Raybaud et al., 2011).
? Part-Of-Speech (POS) features (using
TreeTagger
3
toolkit): The target word?s POS;
the source POS (POS of all source words
aligned to it); bigram and trigram sequences
between its POS and the POS of previous
and following words.
? Binary lexical features that indicate whether
the word is a: stop word (based on the stop
word list for target language), punctuation
symbol, proper name or numerical.
? Language Model (LM) features: the ?longest
target n-gram length? and ?longest source
n-gram length?(length of the longest
sequence created by the current target
(source aligned) word and its previous ones
in the target (source) LM). For example,
with the target word w
i
: if the sequence
w
i?2
w
i?1
w
i
appears in the target LM but
the sequence w
i?3
w
i?2
w
i?1
w
i
does not, the
n-gram value for w
i
will be 3.
? The word?s constituent label and its depth in
the tree (or the distance between it and the
tree root) obtained from the constituent tree
as an output of the Berkeley parser (Petrov
and Klein, 2007) (trained over a Spanish
treebank: AnCora
4
).
? Occurrence in Google Translate hypothesis:
we check whether this target word appears in
3
http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
4
http://clic.ub.edu/corpus/en/ancora
the sentence generated by Google Translate
engine for the same source.
? Polysemy Count: the number of senses of
each word given its POS can be a reliable
indicator for judging if it is the translation
of a particular source word. Here, we
investigate the polysemy characteristic in
both target word and its aligned source word.
For source word (English), the number
of senses can be counted by applying a
Perl extension named Lingua::WordNet
5
,
which provides functions for manipulating
the WordNet database. For target word
(Spanish), we employ BabelNet
6
- a
multilingual semantic network that works
similarly to WordNet but covers more
European languages, including Spanish.
3.2 WMT14 additional features
? POS?s LM based features: we exploit
the Spanish and English LMs of POS
tag (provided as additional resources for
this year?s QE tasks) for calculating the
maximum length of the sequences created
by the current target token?s POS and those
of previous ones. The same score for POS
of aligned source word(s) is also computed.
Besides, the back-off score for word?s POS
tag is also taken into consideration. Actually,
these feature types are listed in Section
3.1 for target word, and we proposed the
similar ones for POS tags. In summary, three
POS LM?s new features are built, including:
?longest target n-gram length?, ?longest
source n-gram length? and back-off score for
POS tag.
? Word Occurrence in multiple translations:
one novel point in this year?s shared task
is that the targets come from multiple MT
5
http://search.cpan.org/dist/Lingua-Wordnet/Wordnet.pm
6
http://babelnet.org
337
outputs (from systems or from humans) for
the same source sentences. Obviously, one
would have a ?natural? intuition that: the
occurrence of a word in all (or almost)
systems implies a higher likelihood of being
a correct translation. Relying on this
observation, we add a new binary-value
feature, telling whether the current token
can be found in more than N% (in our
experiments, we choose N = 50) out
of all translations generated for the same
source sentence. Here, in order to make
the judgments more accurate, we propose
several additional references besides those
provided in the corpora, coming from: (1)
Google Translate system, (2) The baseline
SMT engine provided for WMT2013 English
- Spanish QE task. These two MT outputs
are added to the already available MT outputs
of a given source sentence, before calculating
the (above described) binary feature.
4 Baseline Experiments and
Optimization Strategies
4.1 Machine Learning Method
Motivated by the idea of addressing Word
Confidence Estimation (WCE) problem as
a sequence labeling process, we employ the
Conditional Random Fields (CRFs) for our
model training, with WAPITI toolkit (Lavergne
et al., 2010). Let X = (x
1
, x
2
, . . . , x
N
) be the
random variable over data sequence to be labeled,
Y = (y
1
, y
2
, . . . , y
N
) be the output sequence
obtained after the labeling task. Basically, CRF
computes the probability of the output sequence
Y given the input sequence X by:
p
?
(Y |X) =
1
Z
?
(X)
exp
{
K
?
k=1
?
k
F
k
(X,Y )
}
(1)
where F
k
(X,Y ) =
?
T
t=1
f
k
(y
t?1
, y
t
, x
t
);
{f
k
} (k = 1,K) is a set of feature functions;
{?
k
} (k = 1,K) are the associated parameter
values; and Z
?
(x) is the normalization function.
In the training phase, we set the maximum
number of iterations, the stop window size,
and stop epsilon value at 200; 6 and 0.00005
respectively.
System Label Pr(%) Rc(%) F(%)
BL(bin) OK 66.67 81.92 73.51
Bad 60.69 41.92 49.58
BL(L1) OK 63.86 82.83 72.12
Accuracy 22.14 14.89 17.80
Fluency 50.40 27.98 35.98
BL(mult) OK 63.32 87.56 73.49
Fluency 14.44 10.10 11.88
Mistranslation 9.95 5.69 7.24
Terminology 3.62 3.89 3.75
Unintelligible 52.97 16.56 25.23
Agreement 5.93 11.76 7.88
Untranslated 5.65 7.76 6.53
Punctuation 56.97 25.82 35.53
BL+WMT OK 68.62 82.69 75.01
13(bin) Bad 64.38 45.73 53.47
Table 2: Average Pr, Rc and F for labels
of all-feature binary and multi-class systems,
obtained on our WMT 2014 dev set (200
sentences). In BL(multi), classes with zero value
for Pr or Rc will not be reported
4.2 Experimental Classifiers
We experiment with the following classifiers:
? BL(bin): all features (WMT14+WMT13)
trained on train14 only, using binary labels
(?OK? and ?BAD?)
? BL(L1): all features trained on train14 only,
using level 1 labels (?OK?, ?Accuracy?, and
?Fluency?)
? BL(mult): all features trained on train14
only, using 16 labels
? BL+WMT13(bin): all features trained on
train14 + {train+dev+test}13, using binary
labels.
System quality in Precision (Pr), Recall (Rc) and
F score (F) are shown in Table 2. It can be
observed that promising results are found in binary
variant where both BL(bin) and BL+WMT(bin)
are able to reach at least 50% F score in detecting
errors (BAD class), meanwhile the performances
in ?OK? class go far beyond (73.51% and 75.01%
respectively). Interestingly, the combination
with WMT13 data boosts the baseline prediction
capability in both labels: BL+WMT13(bin)
outperforms BL(bin) in 1.10% ( 3.89%) for OK
(BAD) label. Nevertheless, level 1 and multi-class
systems maintain only good score for ?OK? class.
In addition, BL(mult) seems suffer seriously
from its class imbalance, as well as the lack of
training data for each, resulting in the inability
of prediction for several among them (not all are
reported in Table 2 ).
338
4.3 Decision threshold tuning for binary task
In binary systems BL(bin) and
BL+WMT13(bin), we run the classification
task multiple times, corresponding to a decision
threshold increase from 0.300 to 0.975 (step
= 0.025). The values of Precision (Pr), Recall
(Rc) and F-score (F) for OK and BAD label are
tracked along this threshold variation, allowing
us to select the optimal threshold that yields the
highest F
avg
=
F (OK)+F (BAD)
2
. Figure 1 shows
that BL(bin) reaches the best performance at the
threshold value of 0.95, meanwhile the one for
BL+WMT13(bin) is 0.75. The latter threshold
(0.75) has been used for the primary system
submitted.
Figure 1: Decision threshold tuning on BL(bin)
and BL+WMT2013(bin)
4.4 Feature Selection
In order to improve the preliminary scores
of all-feature systems, we conduct a feature
selection which is based on the hypothesis
that some features may convey ?noise? rather
than ?information? and might be the obstacles
weakening the other ones. In order to prevent
this drawback, we propose a method to filter the
best features based on the ?Sequential Backward
Selection? algorithm
7
. We start from the full set of
N features, and in each step sequentially remove
the most useless one. To do that, all subsets of
(N-1) features are considered and the subset that
leads to the best performance gives us the weakest
feature (not involved in the considered set). This
procedure is also called ?leave one out? in the
literature. Obviously, the discarded feature is not
considered in the following steps. We iterate the
7
http://research.cs.tamu.edu/prism/lectures/pr/pr l11.pdf
process until there is only one remaining feature in
the set, and use the following score for comparing
systems: F
avg
(all) =
F
avg
(OK)+F
avg
(BAD)
2
,
where F
avg
(OK) and F
avg
(BAD) are the
averaged F scores for OK and BAD label,
respectively, when threshold varies from 0.300 to
0.975. This strategy enables us to sort the features
in descending order of importance, as displayed
in Table 3. Figure 2 shows the evolution of
the performance as more and more features are
removed. The feature selection is done from the
BL+WMT2013(bin) system.
We observe in Table 3 four valuable features
which appear in top 10 in both WMT13
and WMT14 systems: Source POS, Occur in
Google Translate, Left source context and Right
target context. Among our proposed features,
?Occurrence in multiple systems? is the most
outstanding one with rank 3, ?longest target POS
gram length? plays an average role with rank 12,
whereas ?longest source POS gram length? is
much less beneficial with the last position in the
list. Figure 2 reveals that the optimal subset of
features is the top 18 in Table 3, after discarding 6
weakest ones. This set will be used to train again
the classifiers in all subtasks and compare to the
baseline ones.
Figure 2: The evolution of the performance
as more and more features are removed (from
BL+WMT2013(bin) system)
5 Submissions
After finishing the optimization process and
comparing systems, we select two most
out-standing ones (of each subtask) for the
submission of this year?s shared task. They are
the following:
? Binary variant: BL+WMT13(bin) and
FS(bin) (feature selection from the same
corresponding system)
? Level 1 variant: BL(L1) and FS(L1) (feature
selection from the same corresponding
system)
339
Rank WMT2014 WMT2013
1 Target POS Source POS
2 Longest target gram length Occur in Google Translate
3 Occurrence in multiple systems Nodes
4 Target word Target POS
5 Occur in Google Translate WPP any
6 Source POS Left source context
7 Numeric Right target context
8 Polysemy count (target) Numeric
9 Left source context Polysemy count(target)
10 Right Target context Punctuation
11 Constituent label Stop word
12 Longest target POS gram length Right source context
13 Punctuation Target word
14 Stop word Distance to root
15 Number of occurrences Backoff behaviour
16 Left target context Constituent label
17 Backoff behaviour Proper name
18 Polysemy count (source) Number of occurrences
19 Source Word Min
20 Proper Name Max
21 Distance to root Left target context
22 Longest source gram length Polysemy count (source)
23 Right source context Longest target gram length
24 Longest source POS gram length Longest source gram length
25 Source Word
Table 3: The rank of each feature (in term of usefulness) in WMT2014 and WMT2013 systems. The
bold ones perform well in both cases. Note that feature sets are not exactly the same for 2013 and 2014
(see explanations in section 3).
? Multi-class variant: BL(mult) and
FS(mult) (feature selection from the
same corresponding system)
The official results can be seen in Table 4. This
year, in order to appreciate the translation error
detection capability of WCE systems, the official
evaluation metric used for systems ranking is the
average F score for all but the ?OK? class. For
the non-binary variant, this average is weighted
by the frequency of the class in the test data.
Nevertheless, we find the F scores for ?OK? class
are also informative, since they reflect how good
our systems are in identifying correct translations.
Therefore, both scores are reported in Table 4.
6 Conclusion and perspectives
We presented our preparation for this year?s shared
task on QE at word level, for the English - Spanish
language pair. The lack of some information
on MT system internals was a challenge. We
made efforts to maintain most of well-performing
System F(?OK?) (%) Average F(%)
FS(bin) (primary) 74.0961 0.444735
FS(L1) 73.9856 0.317814
FS(mult) 76.6645 0.204953
BL+WMT2013(bin) 74.6503 0.441074
BL(L1) 74.0045 0.317894
BL(mult) 76.6645 0.204953
Table 4: The F scores for ?OK? class and the
average F scores for the remaining classes (official
WMT14 metric) , obtained on test set.
2013 features, especially the source side ones,
and propose some novel features based on this
year?s corpus specificities, as well as combine
them with those of last year. Generally, our
results are not able to beat those in WMT13 for
the same language pair, yet still promising under
these constraints. As future work, we are thinking
of using more efficiently the existing references
(coming from provided translations and other
reliable systems) to obtain stronger indicators, as
340
well as examine other ML methods besides CRF.
References
Ergun Bicici. Referential translation machines
for quality estimation. In Proceedings of
the Eighth Workshop on Statistical Machine
Translation, pages 343?351, Sofia, Bulgaria,
August 2013. Association for Computational
Linguistics. URL http://www.aclweb.
org/anthology/W13-2242.
Aaron Li-Feng Han, Yi Lu, Derek F. Wong,
Lidia S. Chao, Liangye He, and Junwen Xing.
Quality estimation for machine translation
using the joint method of evaluation criteria
and statistical modeling. In Proceedings of
the Eighth Workshop on Statistical Machine
Translation, pages 365?372, Sofia, Bulgaria,
August 2013. Association for Computational
Linguistics. URL http://www.aclweb.
org/anthology/W13-2245.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen,
Christine Moran, Richard Zens, Chris Dyer,
Ondrej Bojar, Alexandra Constantin, and
Evan Herbst. Moses: Open source toolkit for
statistical machine translation. In Proceedings
of the 45th Annual Meeting of the Association
for Computational Linguistics, pages 177?180,
Prague, Czech Republic, June 2007.
John Lafferty, Andrew McCallum, and
Fernando Pereira. Conditional random
fields: Probabilistic models for segmenting
et labeling sequence data. In Proceedings of
ICML-01, pages 282?289, 2001.
Thomas Lavergne, Olivier Capp?e, and Franc?ois
Yvon. Practical very large scale crfs. In
Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 504?513, 2010.
Ngoc Quang Luong, Laurent Besacier, and
Benjamin Lecouteux. Word confidence
estimation and its integration in sentence
quality estimation for machine translation.
In Proceedings of the fifth international
conference on knowledge and systems
engineering (KSE), Hanoi, Vietnam, October
2013.
Franz Josef Och and Hermann Ney. A systematic
comparison of various statistical alignment
models. Computational Linguistics, 29(1):
19?51, 2003.
Slav Petrov and Dan Klein. Improved inference
for unlexicalized parsing. In Proceedings of
NAACL HLT 2007, pages 404?411, Rochester,
NY, April 2007.
S. Raybaud, D. Langlois, and K. Sma?? li. ?this
sentence is wrong.? detecting errors in machine
- translated sentences. In Machine Translation,
pages 1?34, 2011.
Matthew Snover, Nitin Madnani, Bonnie Dorr,
and Richard Schwartz. Terp system description.
In MetricsMATR workshop at AMTA, 2008.
341
