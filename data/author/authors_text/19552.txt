Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 290?300, Dublin, Ireland, August 23-29 2014.
Unsupervised Training Set Generation
for Automatic Acquisition of Technical Terminology in Patents
Alex Judea
1
Hinrich Schu?tze
2
So?ren Bru?gmann
3
1
Heidelberg Institute for Theoretical Studies, Heidelberg, Germany
2
Center for Information and Language Processing, University of Munich, Germany
3
Bru?gmann Software GmbH, Papenburg, Germany
Abstract
NLP methods for automatic information access to rich technological knowledge sources like
patents are of great value. One important resource for accessing this knowledge is the tech-
nical terminology of the patent domain. In this paper, we address the problem of automatic
terminology acquisition (ATA), i.e., the problem of automatically identifying all technical terms
in a document. We analyze technical terminology in patents and define the concept of technical
term based on the analysis. We present a novel method for labeling large amounts of high-quality
training data for ATA in an unsupervised fashion. We train two ATA methods on this training
data, a term candidate classifier and a conditional random field (CRF), and investigate the utility
of different types of features. Finally, we show that our method of automatically generating train-
ing data is effective and the two ATA methods successfully generalize, considerably increasing
recall while preserving high precision relative to a state-of-the-art baseline.
1 Introduction
A large part of our technological knowledge is encoded in patents. Methods for automatically finding
information in patents and inferring information from patents are thus of great value. An important
step in getting access to patent information is identification of technical terminology, i.e., finding the
linguistic expressions that denote the technical concepts of a patent: the methods, processes, substances
and objects that are part of the invention or modified by it. In the example ?The present invention
relates to a charging apparatus of a bicycle dynamo?, the bolded compound nouns are the main
content words and refer to specific technological concepts. We call such linguistic expressions (technical)
terms or terms and their totality the (technical) terminology of a document or domain.
We address the task of automatic terminology acquisition (ATA), the task of finding technical terms
in texts without reliance on existing resources that list terms of the domain. In contrast to this stands
automatic terminology recognition (ATR), which we define as finding known terms and their variants
(Jacquemin and Bourigault, 2003). ATA provides input to downstream components like automatic sum-
marization, machine translation, ontology building, information extraction and retrieval. terms ex-
tracted by ATA can be semantically classified or mapped to entries in a semantic database (Krauthammer
and Nenadic, 2004), but we focus on identifying them without further classification in this paper.
Our main contributions are as follows. (i) We present a method for automatically labeling large amounts
of training data for ATA. (ii) We show that two types of statistical classifiers trained on this training
data beat a state-of-the-art baseline, indicating that the automatic labeling is of high quality. (iii) We
study different feature types for ATA and investigate how much they contribute to good performance. We
investigate a semi-supervised setting in which features are selected based on a manually labeled evaluation
set and a completely unsupervised setting where the feature selection is performed on an automatically
produced set. (iv) Finally, we show that performance strongly depends on correct identification of the
boundaries of terms and could be enhanced considerably by improving candidate identification.
The paper is organized as follows. Section 2 gives a definition of technical terminology and provides a
brief analysis of terms in patents. Section 3 presents related work. Section 4 describes the architecture of
our ATA system: preprocessing, linguistic filtering, automatic labeling of training data, feature selection
and postprocessing. Section 5 reports evaluation results and analyzes selected features and errors. Section
6 presents our conclusions.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and
proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.
0/
290
2 Problem Description
Let w
1...k
be a sequence of words w
1
, w
2
, . . . , w
k
and w
k
a head noun. w
1...k
is a term of domain D iff
(i) the head noun w
k
is unmodified (k = 1) or (for k > 1) is modified by sequences of other nouns (?disk
controller?), adjectives (?secondary controller?) or present participles (?writing controller?) and (ii) it
denotes a concept specific to D.
(i) and (ii) describe the syntactic and semantic properties of a term, respectively. Part (i) restricts
terms to parts of noun phrases. This is a reasonable restriction that covers most technical terms
(Daille et al., 1996) and it has been frequently made in the computational terminology literature. We
exclude comparatives and superlatives as modifying adjectives because they are rarely used attributively
in patents and usually modify quantities or qualities of terms(e.g., ?higher shunt currents?); in other
words, only ?positive? (base-form) adjectives are included in our definition. Note that the number of
tokens per term is not restricted by the definition. Our approach aims to find terms of arbitrary length.
Part (ii) of the definition restricts terms to be specific to a domain D. We can set D to a general
domain like ?electricity? and be on a par with many prior definitions (Ananiadou, 1994; Georgantopoulos
and Piperidis, 2000; Zhang and Fang, 2010), but we can also set D to a narrow domain like ?emergency
protective circuit arrangements? (IPC code
1
H02H).
Here, we choose the most general technical domain possible: the domain of all technical subjects. This
is a good setting for many downstream tasks, e.g., information retrieval should benefit from a broad
coverage of D. It also makes annotation easier: Non-experts can carry it out with good agreement
(Section 5.1) because they simply look for all technical expressions.
The syntactic and semantic parts of our definition of term correspond to the concepts of unithood and
termhood , respectively. Unithood is the degree to which a sequence of tokens is a linguistic unit; and
termhood the degree to which a linguistic unit is a term of a domain (Kageura and Umino, 1996). Both
aspects have to be covered by ATA systems.
Terms in Patents
In addition to traditional terms like simple nouns (1, ?voltage?), modified nouns (2, ?secondary arm?)
and nouns modified by prepositional phrases (3, ?trajectory of the lever?), patents provide also coordina-
tions (4, ?constant and variable current?) and complex constructions (5, ?storage device storing a target
temperature value which a battery is intended to reach?).
For ATA, it seems advisable to exclude infrequent and complex nominal expressions from the definition
of term, both from a terminological and a computational point of view. Most nominal expressions that
are generally viewed as terms are single nouns, compound nouns, and nouns with an adjectival modifier
(Daille et al., 1996); our syntactic definition covers these three types. Nominal expressions like (5)
tend to be long; if we were to count such cases as terms, then it would be unclear where the term
ends. When analyzing (5), our first take might be that there is a nucleus (?storage device?) which is
modified by a verbal phrase (?storing a target temperature value?) and that the rest of the phrase is
not part of the term. But it turns out that the whole phrase appears multiple times in its patent; it
is a stable way of denoting a part of the invention. However, the underlying concept is also denoted by
simpler constructions like the nucleus itself, or synonymous terms like ?control circuit?; these simpler
constructions are covered by our definition.
Coordinations like (4) mix multiple concepts (here, ?constant current? and ?variable current?) without
making this explicit on the surface. It is difficult to identify ?constant current? as a potential term
because it is non-contiguous and is only indicated by an adjective. Our treatment of coordinations in
this paper is to only consider sequences satisfying the syntactic definition (i.e., ?variable current?) to be
terms and discard other parts (i.e., ?constant?). Of course, if both conjuncts are complete terms and
satisfy the syntactic definition, both will be identified as terms.
Finally, prepositional phrases like (3) are rather infrequent compared to terms covered by our syntactic
definition. They also tend to be highly ambiguous and the underlying concept is often expressed by terms
covered by our definition (?lever trajectory?).
3 Related Work
Previous work on ATA either employs filtering or sequence models. Filtering combines linguistic and
statistical criteria for (i) extracting a list of candidates (typically word n-grams) based on simple linguistic
criteria, (ii) computing candidate statistics and (iii) using ranking, classification or some other mechanism
for producing a pruned list of terms as output. Because variation of the surface form of terms is limited,
1
wwwcms10.wipo.int/classifications/ipc/en/
291
it makes sense to use word n-grams as the basis for candidate identification ? even though there are cases
that cannot be found this way, e.g., ?constant current? or alternations like ?pressure regulating valve?
vs. ?valve regulating pressure?.
The main difference between ATA methods that rely on filtering is in how they accomplish the rank-
ing/pruning of the candidate list. See Kageura and Umino (1996), Jacquemin (2001) and Pazienza et al.
(2005) for an overview. In this paper, we accomplish this by training a statistical model to classify term
candidates. We also run experiments with a sequence model. Our main innovation is that these models
are trained on automatically labeled training data.
It is difficult to directly compare computational terminology systems because of differences in domain,
language, application and task definition. As an example consider Takeuchi and Collier (2005) who report
an F
1
of .742. However, their task definition includes assigning terms to pre-defined categories such as
DNA and protein as opposed to simply identifying terms. In addition, terminologies in the biomedical
and technological domains are different. In biomedicine, categories like DNA and protein dominate. For
these terms, shape features are informative ? in contrast to terms in patents. Another difference is
that terms in patents tend to be long whereas DNA and proteins are often single-token abbreviations.
3.1 Training Data Collection
One of our main contributions is unsupervised training data generation (Section 4.3). Prior work has used
automatically recognized training data for computational terminology, specifically for ATR (Craven and
Kumlien, 1999; Hatzivassiloglou et al., 2001; Morgan et al., 2003; Zhang et al., 2010) in the biomedical
domain. Given large precompiled term lists they search for occurrences of list elements, e.g., genes, in
texts and use the occurrences they find as training examples. This is similar to distant supervision (Mintz
et al., 2009) which also uses pre-existing resources such as gazetteers for, e.g., relation extraction.
In contrast, our method is applied to ATA for the technological domain and does not rely on precompiled
resources ? we make use of figure references, which are an inherent part of patents. Our method can
be characterized as training data identification: we exploit given conditions in patents for our search of
training data. In contrast, training data recognition methods need precompiled resources as input and
search for instances of resource elements in texts.
3.2 Learning Algorithms and Features
Different learning algorithms and feature sets have been used for computational terminology. Foo and
Merkel (2010) use Ripper (Cohen, 1995) with a variety of features to classify uni- and bigram term
candidates. Hatzivassiloglou et al. (2001) compare C4.5 (Quinlan, 1993) and Naive Bayes (Duda and
Hart, 1973). Zhang et al. (2010) acquire novel terms using CRFs and syntactic features. Takeuchi and
Collier (2005) find that more training data results in higher F scores. Large training sets have the same
positive effect in our experiments. Our approach has the added advantage that the training sets are
generated completely automatically.
4 Approach
As discussed in the introduction, we address the problem of ATA. We use the abbreviation ATAS (au-
tomatic terminology acquisition system) to refer to our approach in general as well as to the specific
implementation we evaluate in this paper.
ATAS consists of three parts: (i) training set generation, (ii) parameter selection and training of the
term candidate classifier (ATAS-TC) and the CRF (ATAS-CRF) and (iii) identification of terminology
in documents.
Processing in step (iii) is document by document because some of our features are document-based.
ATAS takes a document as input and identifies all terms in the document, using the term candidate
classifier or the CRF learned in (ii).
The term candidate classifier (ATAS-TC) decides on entire (multi-token) candidates while the CRF
decides on single tokens. ATAS-TC heavily relies on candidate computation and its decisions are mutually
independent, which is clearly incorrect. In contrast, ATAS-CRF is less dependent on candidate compu-
tation and models dependence of decisions correctly; but it lacks the more ?global? view of ATAS-TC on
entire candidates. We want to investigate which approach is more suited for ATA.
In what follows we describe how we preprocess patents, the linguistic filters used to implement our
syntactic definition of term, automatic labeling of training data (step (i) of ATAS), training of term
candidate classifier and CRF (step (ii) of ATAS), features and feature selection.
292
4.1 Preprocessing
The preprocessing pipeline consists of the ANNIE tokenizer, OpenNLP sentence splitter, Mate POS
tagger (Bohnet (2010), retrained for patents) and Mate lemmatizer. Preprocessing has a big influence
on computational terminology because special domain text poses problems for off-the-shelf components.
For example, patents tend to use common language words in rare functions or meanings, e.g., ?said? as
a de facto determiner in contexts such as ?the structure of said component?. Other problems are the
use of special language words, e.g., substances like ?triphenylphosphine? and acronyms like ?AC?. Such
properties pose serious problems to POS taggers. Patent citations, acronyms and even product names
can include punctuation, confusing sentence splitters. Chemical formulas may confuse tokenizers.
We adapted our POS tagger and sentence splitter for patent language to deal with unusual punctuation
and POS tags ? especially unusual POS tags of common-language words like ?said?. This adaptation
involves training on a manually labeled training set of patent text and some other adjustments; e.g., we
only allow the tag NN for the acronyms ?AC?, ?DC? and ?A/D?.
4.2 Filter
We now describe how we find term candidates that satisfy the syntactic definition; recall that only
(possibly modified) nouns can be terms (Section 2).
In general, candidate identification strategies using linguistic knowledge perform better. There are two
different strategies of this type: (i) parsing the sentence, extracting nominal chunks from the parse and
further processing the nominal chunks and (ii) POS tagging the sentence and extracting word sequences
that satisfy a set of predefined POS patterns. Because many patent sentences are long and difficult to
parse, we adopt the POS pattern approach in this paper. To this end, we define two simple POS-based
rules for finding term candidates.
2
PREMODS. This rule defines a modifier sequence. It matches a sequence of noun pre-modifiers:
(JJ|?/?|VBG|RB|N(N|P))*.3 We include RB because the POS tagger sometimes misclassifies JJ as RB.
We include ?/? because the tokenizer splits abbreviations containing it.
CANDIDATE. This rule defines a term candidate. It matches either a single noun or PREMODS
followed by a noun: (PREMODS N(N|P)). The last noun must be longer than two characters. We
add a flag indicating if the candidate comes before a figure reference. A figure reference consists of an
optional keyword (e.g., ?Figure?, ?Fig.?) and a sequence of numbers and letters, optionally enclosed in
parentheses.
We select the longest match in case of overlapping matches and the first longest match in case of
overlapping matches of the same length.
These simple rules will find all terms ? as well as many non-terms that we will train ATAS to identify ?
with two exceptions. First, due to POS errors some candidates are spurious. Second, unwanted modifiers
may be part of candidates. E.g., the rules will only identify ?same battery? as a candidate and not
?battery?. But only ?battery? is a valid term. To address the latter, we manually compiled a stop list of
67 modifiers, mostly numerals (?first?) and adjectives in anaphoric function (?above-mentioned?). These
modifiers are removed from term candidates.
4.3 Automatic Labeling of Training Data
We view ATA as either a binary classification task where a term candidate classifier decides if a candidate
is a term or not, or as a sequence labeling task where a CRF decides if a token (word) belongs to a
term or not.
Large training sets are needed to train such models. Usually, these sets are produced by expensive
human labeling. We present a method for generating high quality training data in an unsupervised way
without the necessity of precompiled resources. In principle, our method can be used for any language
for which machine-readable patents are available.
Our starting point is that patents typically contain figure references, i.e., pointers to drawings illus-
trating the invention or its parts. Consider the example: ?. . . so that first clamp-holding secondary
arms (1) . . . ? Here, the figure reference (?(1)?) points to the illustration ?Figure 1? and is preceded by
the illustrated term (?clamp-holding secondary arms?). Illustrated terms may be concrete, as in this
example, or abstract, e.g., a diagram illustrating properties of a method.
We call a term candidate that precedes a figure reference a basic figure reference term candidate
(bFRTC). In a manual inspection of bFRTCs in 12 patents we found that almost 95% of bFRTCs were
2
JJ, VBG, and RB are POS tags for positive adjectives, gerunds/present participles, and adverbs, respectively.
3
?*? is the Kleene star, ??? denotes optionality, and ?|? denotes alternation.
293
terms. Thus, bFRTCs can be used as positive training examples because they usually denote technical
concepts; they have the advantage of being identifiable with high precision using simple patterns.
Once the bFRTCs have been identified, there is a simple way to further increase the size of the training
data: we add all extended FRTCs (eFRTCs) to the training set, where we define an eFRTC as a term
candidate whose suffix is a bFRTC. E.g., if we have identified ?shunt current? as a bFRTC, then ?AC
shunt current? is an eFRTC. eFRTCs typically are hyponyms since the modifiers added at the beginning
restrict the bFRTC to a more specific meaning. This kind of hyponymy is a special case of term derivation,
a modification where a base term is further specified by prefixes (Daille et al., 1996). The strategy of
identifying eFRTCs can also be applied to free word order languages because figure references tend to
have a local and fixed occurrence pattern similar to English. We use the term FRTC to refer to both
bFRTCs and eFRTCs.
We identify all FRTCs and add them as positive examples to the training set. We also add the 5%
most frequent candidates as positive examples; most of them are FRTCs, so that this step usually adds
few new training examples.
We label the following candidates as negative training examples: candidates appearing only once in
a patent; patent citations; and measurements. Citations and measurements (?3 cm?) are clear non-
terms. We identify them using regular expressions. Many singletons are non-terms because they denote
common language (i.e., nontechnical) concepts, e.g., ?time?. These heuristics for finding negative training
examples are not applied to a candidate if it has the same head as a positive training example.
We exclude from the training set candidates that do not satisfy any positive or negative criteria.
4.4 Classifiers
We use the L2-regularized logistic regression of LIBLINEAR (Fan et al., 2008) as our term candidate
classifier. We use LIBLINEAR?s default normalization for continuous-valued attributes (normalization
to range [0, 1]) and the default representation for categorical attributes. As LIBLINEAR cannot handle
missing values, we replace them with their means and modes. We set the regularization parameter c = 1.
Our sequence model is CRF++
4
, order 1, with default parameters. The CRF features are adapted from
the ATAS-TC features, e.g., term-level features (e.g., TFIDF) are propagated down to the individual
tokens of the term. We also include word trigrams. We discretize numeric features to three values.
4.5 Features
We developed a set of 74 features for ATA. Some of these features are taken from the literature, some
are specific to our approach and make use of the concept of FRTC and some exploit other properties of
patents (e.g., the importance of the title and the claims in patents). A final group consists of other novel
features that we designed in the course of developing our system. We now provide an overview. c refers
to a term candidate.
Corpus and document statistics. This feature type captures termhood and unithood of c as well as
the position of c?s first occurrence in the document. We use a corpus of technical text C
T
and a general
language corpus C
G
. For every c ? C
T
we collect the number of patents it appears in, its frequency
and its FRTC frequency, i.e., the number of its occurrences that are FRTCs. Features that are intended
to indicate termhood include simple frequencies and distributional characteristics (in C
T
or in a single
patent). Finally, we define a measure of frequency deviation (or ?keywordness?) of h(c), the head of c:
bias(h(c)) =
f
C
G
(h(c))
|C
G
|
|C
T
| ? f
C
T
(h(c))
f
C
G
(resp., f
C
T
) are the frequencies in C
G
(resp., C
T
), |X| is the sum of frequencies of all x ? X.
bias(h(c)) measures the deviation between expected frequency of the head of c (estimated on C
G
) and
its actual frequency. The intuition here is that the frequency of a general language noun like ?time? will
be similar over text types, resulting in a lower bias.
Context. This feature type captures unigrams and bigrams adjacent to c as well as their POS tags.
Part-of-speech. This feature type captures the POS sequence of c.
A patent usually focuses on a narrow technological subdomain. As a result, many of its terms are
semantically related to each other. We would like to include features that directly capture semantic simi-
larity to other terms because a candidate that is semantically similar to several other already recognized
terms is likely to be a term itself.
Our goal in this paper is to address ATA using simple and efficient methods. For this reason, we
approximate semantic similarity using string similarity because a subset of semantically similar terms are
4
crfpp.googlecode.com
294
Tu
tdg
T
l
test
T
l
dev
T
u
sel
patents 365 5 11 25
word tokens 3,422,131 50,007 74,000 152,715
word types 292,994 3711 7391 4141
bFRTCs 119,316 1264 2558 6503
FRTCs 240,240 2371 4942 10,110
candidates 353,238 8836 13,099 27,164
terms 3814 7220
Table 1: Data set statistics
P R F
1
description
1 .704 .797 .748 mean string similarity of c and FRTCs
2 .712 .832 .767 frequency of c as an FRTC in C
T
3 .694 .887 .779 TFIDF of c
4 .703 .888 .784 is c uppercase?
5 .708 .893 .790 is c followed by a figure reference?
6 .710 .896 .792 TFIDF of h(c)
7 .711 .895 .793 frequency of h(c) as an FRTC in C
T
8 .718 .892 .795 bias(h(c))
9 .720 .891 .797 # sentences with FRTCs that c occurs in
10 .720 .893 .797 C-value of c
11 .721 .893 .798 frequency of h(c) in C
G
Table 2: Features selected on T
l
dev
(setting S). c: term candidate. h(c): head of c
also similar on the surface. E.g., the semantic similarity between ?AC power supply source? and ?AC
supply source? also manifests itself as string similarity.
String similarity. When designing a similarity measure, we wanted it to satisfy the following criteria:
(i) more words in common should result in higher scores and (ii) words in common towards the end of the
two strings should be weighted higher than words in common at the beginning. The motivation for (ii)
is that candidates differing only in initial modifiers are often cohyponyms and highly related; conversely,
candidates with different heads are often not related.
To implement this, we represent a candidate c as a vector ~c in |V |-dimensional space where V is the
vocabulary. ~c
i
is set to the position of word w
i
in c if it occurs and 0 otherwise. The string similarity
between c and c
?
is then defined as the cosine of ~c and
~
c
?
. Example: for ?AC power supply source? and
?AC supply source?, we get the vectors (1, 2, 3, 4) and (1, 0, 2, 3) and the cosine .927; comparing the first
string with ?AC power supply? with the vector (1, 2, 3, 0) we get the cosine .683.
Features in our initial set of 74 that make use of this semantic similarity are: maximum similarity of
c to any FRTC, average similarity of c to all FRTCs in the patent and similarity of c to the rightmost
term candidate in the title.
Frantzi and Ananiadou (1997) define C-value(c) as:
C-value(c) = log
2
|c|(f(c)?
1
|T
c
|
?
b?T
c
f(b))
where T
c
is the set of term candidates containing c and f is frequency in C
T
. C-value is high for term
candidates that are frequent and occur as parts of many other term candidates ? this is a good indicator
of termhood.
5 Experiments and Evaluation
5.1 Data Sets
We hired three students with a bachelor degree in computer science to annotate 16 patents. The test
set T
l
test
consists of 5 patents annotated by all three students. We used majority voting to produce the
final gold annotations. The devset T
l
dev
consists of the remaining 11 patents. Each T
l
dev
sentence was
annotated by one student.
Inter-annotator agreement on T
l
test
was .76 (Fleiss? ?). Most disagreements concern modifiers or
common nouns (e.g., the term ?battery? was often not annotated). More extensive training of the
annotators should reduce these problems considerably.
As unlabeled data we randomly selected 390 technology patents. We use 365 as T
u
tdg
for training data
generation and 25 as T
u
sel
for unsupervised feature selection. We made sure the 390 documents are not
295
in T
l
dev
and T
l
test
. We excluded chemical patents because standard preprocessing components often fail
for chemical formulas. Table 1 gives data set statistics.
As our technical corpus C
T
we use T
u
tdg
and as our general corpus C
G
all nouns in the 2000 most
frequent English words from Project Gutenberg
5
. This list contains many general nouns which also
appear in patents (e.g., ?time?) without containing many technical terms (e.g., ?battery?); this way, C
T
and C
G
give us a good contrast between technical and non-technical vocabularies (cf. Section 4.5).
One obstacle to comparing systems for ATA in the technical domain is the lack of publicly available
evaluation benchmarks. We are making our data sets and the annotation guidelines available
6
.
5.2 Baselines
We define the FRTC baseline as the system that labels all FRTCs and only FRTCs as terms. Almost
all FRTCs are terms, but many terms are not FRTCs; thus, the FRTC baseline has high precision and
low recall. Our goal is to preserve high precision while considerably increasing recall, or to generalize
well from FRTCs to other terms.
Our state of the art baseline is Z-CRF, a reimplementation of the CRF described in (Zhang et al.,
2010). Its feature representation includes POS tags, unigrams, bigrams and syntactic information, e.g.,
the number of times a particular token is used in a syntactic function like subject in the training set.
Syntactic information is extracted with Mate (Bohnet, 2010). Z-CRF is trained on T
u
tdg
, just as ATAS.
Our last baseline is the well-known C-value (Frantzi and Ananiadou, 1997). Like our first baseline,
it needs no training data. In contrast to our first baseline, it was specifically designed for terminology
acquisition. It combines observations about statistical and linguistic properties of terms, i.e., a candidate
is preferred as a term if it is long and frequently appears as substring of other candidates. Following
Frantzi and Ananiadou (1997) we regard a candidate as term if its C-value it not zero; unlike them, we
do not restrict the length of terms because the computation of long terms did not pose computational
problems for us.
5.3 Evaluation Setup
We evaluate ATAS using precision, recall and F
1
. Evaluation is based on candidate tokens (as opposed
to candidate types or word tokens); e.g., each instance of a candidate term that is incorrectly classified
as a term is a false positive. Evaluation is strict in the sense that a term is counted as a false positive
if there is a single token that is added or missed.
We evaluate ATAS in two settings. In the system (S) setting, the ATAS pipeline described in Section
4 (ATAS-TC or ATAS-CRF) is used to identify term candidates. This is the real-world setting since
errors in term candidate identification ? misplaced boundaries, missing candidates, etc. ? are a major
source of error in ATA.
We would also like to evaluate candidate classification on gold boundaries (manually verified boundaries
of term candidates); this allows us to quantify by how much performance can be improved if candidate
identification is perfect. However, since gold boundary annotation is expensive, we instead approximated
it: (i) We run automatic term candidate identification. (ii) We remove all term candidates that overlap
with gold (manually annotated) terms. (iii) The set of gold term candidates is then the union of all
remaining automatically identified candidates and the manually annotated terms.
In the gold boundary (G) setting, we provide these gold term candidates to the ATAS pipeline. This
allows us to evaluate the performance of term/non-term classification separately from term candidate
identification.
5.4 Feature Selection
For our feature set of 74, we perform forward feature selection for the term candidate classifier by
selecting the feature in each step that maximizes system F
1
. We perform feature selection (i) on the
manually labeled set T
l
dev
(to gauge performance for an optimal or close-to-optimal feature set) and (ii)
on the automatically labeled set T
u
sel
(to gauge the performance in a completely unsupervised setting).
In the following we explain both settings in more detail.
Table 2 gives the features selected in supervised feature selection, i.e., when features are optimized
on T
l
dev
. Precision remains stable, except for a drop on line 3. Recall rises steadily from .797 to .893. F
1
increases from .748 to .798.
The best feature (line 1) is the mean string similarity of a term candidate c to all FRTCs in a document
(Section 4.5). Together with the next best feature (frequency of c as an FRTC in C
T
) and feature 5 (is
5
en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2006/04/1-10000
6
h-its.org/english/research/nlp/download/terminology.php
296
ATAS-TC ATAS-CRF Baselines
S-SEL U-SEL S-SEL U-SEL Z-CRF C-value FRTC
S G S G S G S G S G S G S G
T
l d
e
v
1 P .721 .838 .690 .796 .732 .844 .727 .854 .867 .891 .384 .749 .839 1.000
2 R .893 .892 .825 .818 .815 .699 .755 .679 .563 .607 .292 .355 .344 .353
3 F
1
.798 .864 .752 .807 .771 .765 .741 .756 .683
?
.722
?
.314
?
.471
?
.488
?
.522
?
T
l t
e
s
t
4 P .696 .753 .627 .692 .774 .832 .664 .745 .813 .840 .388 .726 .864 1.000
5 R .850 .853 .764 .764 .791 .743 .644 .625 .516 .559 .320 .410 .286 .302
6 F
1
.765 .800 .689 .728 .783 .785 .654 .680 .631
?
.674
?
.350
?
.519
?
.430
?
.465
?
Table 3: System (S) and gold boundary (G) results with supervised (S-SEL) and unsupervised (U-SEL)
feature selection.
?
: significantly lower than corresponding ATAS-TC and ATAS-CRF scores.
c followed by a figure reference?) this supports our intuition for using FRTCs for automatic training set
generation because they are indeed strong indicators for termness. Additionally, feature 9 indicates that
candidates occurring often with FRTCs in sentences are probably terms. Feature 4 (is c uppercase?) is
selected because uppercase term candidates are often abbreviations and terms.
Feature 3 (TFIDF of c) hurts precision, but increases recall, resulting in increased F
1
. This feature
models the hypothesis that a term is frequent in some patents but does not occur in many patents.
Patent writers often invent novel terms rather than using standard ones to make finding a patent hard.
Thus, a term candidate that occurs often in a few patents could be such an obfuscating term.
TFIDF is low for terms with small term frequency. Features 6 (TFIDF of h(c)) and 10 (C-value of c)
can help correctly identify such term candidates as terms.
Features 8 and 11 incorporate information from the general purpose corpus C
G
. Feature 8 contrasts
the frequency of c in C
G
with its frequency in C
T
? frequencies of terms are higher in C
T
, frequencies of
non-terms are similar in both corpora. Feature 11 is complementary to this. It makes it more probable
that c is a non-term if its head appears more often in C
G
. Additionally, string similarity with the patent?s
title is an effective feature.
Unsupervised feature selection, i.e., selection on T
u
sel
, selected seven features that are similar
to those selected by supervised selection and that we will discuss now. The best unsupervised feature
(maximum string similarity, 1) and the best supervised feature (mean string similarity) both capture
partial string overlap of c and FRTCs. For similar reasons, the feature ?string similarity of c and rightmost
NP in patent title? (2) ? which exploits the importance of the title in analogy to the importance of figure
references ? is selected.
Other selected features (relative patent frequency of c and its head (3, 4), number of patent sentences
in which c occurs with FRTCs (5), patent frequency of c = 1?(6)) are also similar to the features selected
in the supervised setting. They capture frequency distributions of c. However, while many features in
the supervised setting capture distributions of c in C
T
, in the unsupervised setting, distributions of c in
the patent are more important. The reason may be that C
T
-based features (which use all technical text
as opposed to the relevant patent in question) are harder to recognize as good predictors if the set used
for selection is automatically labeled and hence noisier.
The last unsupervised feature captures the length of c in tokens (7). Manual inspection revealed that
on average terms have more tokens than non-terms (1.9 vs. 1.3).
5.5 ATAS Results
Table 3 gives evaluation results for ATA on T
l
dev
and T
l
test
. We report results for the ATAS versions
(ATAS-TC, ATAS-CRF) and for the baselines (Z-CRF, C-value, FRTC) as well as for using supervised
(S-SEL) and unsupervised feature selection (U-SEL) in system setting (S) and gold boundary setting
(G).
Differences in F
1
between ATAS and baselines (marked with a ?) are significant at p < .01.
7
If not
stated otherwise, numbers below are for the system setting (S).
We note that F
1
of the ATAS versions is consistently and considerably better than all baselines in
all settings. E.g., line 6 shows system F
1
on T
l
test
of ATAS-TC (.765 for S-SEL, .689 for U-SEL) and
ATAS-CRF (.783 for S-SEL, .654 for U-SEL) compared to Z-CRF (.631), FRTC (.430), and the C-value
baseline (.350) . The better results mainly come from higher recall (except for C-value, which is also
beaten in precision). In general, precision of the baselines is higher, but recall much smaller than for
ATAS. This shows that (i) statistical classifiers can be successfully trained for ATA using our method
7
We use approximate randomization (Yeh, 2000) for all significance tests in this paper.
297
Figure 1: System F
1
as a function of training set size (in percent) in setting G.
for automatically generating training data and (ii) these classifiers beat a state-of-the-art system in both
S-SEL and U-SEL settings.
Comparing S-SEL and U-SEL shows that precision and recall for U-SEL are lower than for S-SEL. For
instance, F
1
of ATAS-TC on T
l
test
is .765 for S-SEL and .689 for U-SEL; F
1
of ATAS-CRF is .783 for
S-SEL and .654 for U-SEL (line 6). In general, we note a bigger drop in recall than in precision, indicating
that U-SEL does not generalize as well as S-SEL. However, the U-SEL numbers are significantly better
than the Z-CRF FRTC, and C-value baselines.
When comparing ATAS-TC with ATAS-CRF we note that ATAS-CRF consistently has higher precision
and lower recall. In most cases, ATAS-TC has considerably higher recall, leading to higher F
1
. This is
not surprising given that feature selection was performed for ATAS-TC. Nevertheless, ATAS-CRF can
compete with ATAS-TC in terms of F
1
. Furthermore, ATAS-CRF produces more stable results because
it shows less variance in F
1
across settings.
Comparing S and G scores shows that knowing exact boundaries has a great impact on results, especially
on precision; looking at S-SEL numbers in line 4 in Table 3, precision for ATAS-TC (resp., ATAS-CRF)
is .696 in S vs. .753 in G (resp., .774 in S vs. .832 in G). Similar differences also hold for U-SEL numbers.
In general, ATAS-TC profits more from knowing exact boundaries than ATAS-CRF. This leads us to the
conclusion that the linguistic filter would greatly benefit from a (statistical) measure of unithood. Note
that this also holds for the baselines; deciding about the termness of gold boundary candidates seems to
be easier, especially for C-value.
All observations hold for T
l
dev
and T
l
test
. However, numbers are higher for T
l
dev
because the ratio of
FRTCs to candidates is higher than for T
l
test
(38% vs. 27%) which improves classification performance
on T
l
dev
? this holds for ATAS as well as for the baselines.
To investigate the quality of the extracted training data, consider Figure 1. It shows F
1
in setting G
as a function of training set size in percent of the total training set T
u
tdg
. For each evaluation point,
we randomly add training examples from the full set. F
1
starts at .834 for 0.1% of training data (344
training examples) and rises to .864 for 100% (353,238 examples), with a small drop at 50%. Note that
1000 examples roughly correspond to one annotated patent. The main results of this experiment are that
(i) a modest amount of automatically labeled training data gives good performance and (ii) the more
automatically labeled data the better. The last point is not a trivial finding, given that training data was
generated automatically. The logarithmic graph shows a nearly linear increase in F
1
for each doubling of
the training data.
To further investigate the quality of the generated training data, we compared automatically and
manually produced training examples. We compare results for 13238 manual and 13238 automatic labels
(setting G, ATAS-TC). We get precision and recall of .811 and .805 for manual and .762 and .850 for
automatic annotations, resulting in similar F1 scores: .808 vs. .804 for manual and automatic annotations,
respectively. We believe that the differences in recall are an artifact of the randomization we performed
before removing automatic training samples. Manual labels are entire patents; in contrast, automatic
labels come from all patents in the training set, leaving us with a more diverse set than the manual
version.
5.6 Error Analysis
We found two major types of false negatives. First, infrequent terms are problematic. It is hard to judge
termness when having limited information about a candidate, especially if it appears only once or twice
in a document. Second, POS errors prevent the system from finding some candidates; e.g., the noun
?current? is frequently mistagged as adjective. Incorrect POS tags also lead to incorrect boundaries.
298
We found four major types of false positives. First, incorrect modifiers lead to partially incorrect
terms. 27% of false positives are of this type. Second, incorrectly recognized figure references cause
incorrect system decisions; e.g., our patterns incorrectly parse an expression like ?value PBA? as a figure
reference even though it is instead a named output of a component. Third, very frequent non-terms are
commonly classified as terms. Almost all frequent candidates are terms, so that the term candidate
classifier has difficulty correctly identifying the exceptions from this pattern.
Finally, if a candidate is a term in one context it may be a non-term in another. A good example
for this are general single token terms like ?apparatus?. Before figure references they are terms, e.g.,
?one preferred form of apparatus 22?. In such cases the figure reference serves as a disambiguator.
However, in other positions they are non-terms, e.g., ?They include braces, collars, splints and other
similar apparatus?.
6 Conclusion and Future Work
This paper introduces a method for ATA with two novel aspects: (i) new powerful features for ATA
and (ii) a procedure for generating an ATA training set in an unsupervised fashion. The training set
generation method produces high quality training data, even when compared to manual annotations. It is
language-independent: It can be applied to patents in any language if the definition of term candidates
is modified for the target language. It is also domain-independent: it can be applied to patents of
any domain. The training data can be successfully used to train ATA models, both term candidate
classification as well as CRF models. Even in a completely unsupervised setting the models outperform a
state-of-the-art baseline. We found that using more automatically labeled training data and using better
term boundaries results in better performance.
In future work, we plan to incorporate term variation patterns (Daille et al., 1996; Jacquemin, 2001)
in the expansion process to decrease the number of FNs and increase recall. We would also like to
improve the terminology identification module because we found that incorrect identified boundaries
affect performance greatly.
Finally, we are planning to extend our approach to languages other than English. Our methods are
language-independent to the extent that a body of patents exists for many common languages. Since
we generate the training set automatically, all we need to do to cover another language is to adapt the
linguistic filters for candidate identification.
Acknowledgments. This work was supported by the European Union (Project Topas, FP7-SME-
2011 286639) and by SPP 1335 Scalable Visual Analytics of Deutsche Forschungsgemeinschaft (DFG
grant SCHU 2246/8-2). We would like to thank the anonymous reviewers for their helpful comments and
suggestions, and Bianca and Luca for their support.
References
Sophia Ananiadou. 1994. A methodology for automatic term recognition. In Proceedings of the 15th
conference on Computational linguistics - Volume 2, COLING ?94, pages 1034?1038.
Bernd Bohnet. 2010. Top Accuracy and Fast Dependency Parsing is not a Contradiction. In Proceedings
of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89?97, Beijing,
China, August.
William W. Cohen. 1995. Fast effective rule induction. In Twelft International Conference on Machine
Learning (ML95), pages 115?123.
Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting infor-
mation from text sources. In Thomas Lengauer, Reinhard Schneider, Peer Bork, Douglas L. Brutlag,
Janice I. Glasgow, Hans-Werner Mewes, and Ralf Zimmer, editors, ISMB, pages 77?86. AAAI.
Be?atrice Daille, Beno??t Habert, Christian Jacquemin, and Jean Royaute?. 1996. Empirical Observation
of Term Variations and Principles for their Description. Terminology, 3(2):197?258.
Richard O. Duda and Peter E. Hart. 1973. Pattern Classification and Scene Analysis. John Wiley &
Sons Inc, 1 edition.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of Machine Learning Research, 9:1871?1874.
299
Jody Foo and Magnus Merkel. 2010. Using machine learning to perform automatic term recognition. In
Proceedings of the LREC 2010 Workshop on Methods for automatic acquisition of Language Resources
and their Evaluation Methods, pages 49?54.
Katerina T. Frantzi and Sophia Ananiadou. 1997. Automatic Term Recognition using Contextual Cues.
In Proceedings of 3rd DELOS Workshop, Zurich, Switzerland.
Byron Georgantopoulos and Stelios Piperidis. 2000. Term-based Identification of sentences for Text Sum-
marisation. In Proceedings of Second International Conference on Language Resources and Evaluation
(LREC2000), pages 1067?1070, Athens, Greece.
Vasileios Hatzivassiloglou, Pablo Ariel Dubou, and Andrey Rzhetsky. 2001. Disambiguating proteins,
genes, and rna in text: a machine learning approach. In ISMB (Supplement of Bioinformatics), pages
97?106.
Christian Jacquemin and Didier Bourigault. 2003. Term extraction and automatic indexing. In Ruslan
Mitkov, editor, The Oxford Handbook of Computational Linguistics, chapter 33. Oxford University
Press.
Christian Jacquemin. 2001. Spotting and Discovering Terms Through Natural Language Processing. MIT
Press, April.
Kyo Kageura and Bin Umino. 1996. Methods of automatic term recognition: A review. Terminology,
3(2):259?289.
Michael Krauthammer and Goran Nenadic. 2004. Term identification in the biomedical literature.
Journal of Biomedical Informatics, 37(6):512?526, December.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume
2-Volume 2, pages 1003?1011. Association for Computational Linguistics.
Alex Morgan, Lynette Hirschman, Alexander Yeh, and Marc Colosimo. 2003. Gene Name Extraction
Using FlyBase Resources. In Proceedings of ACL 2003 Workshop on Natural Language Processing in
Biomedicine, pages 1?8, Sapporo, Japan.
Maria Teresa Pazienza, Marco Pennacchiotti, Michele Vindigni, and Fabio Massimo Zanzotto. 2005.
Ai/nlp technologies applied to spacecraft mission design. In Proceedings of the 18th international
conference on Innovations in Applied Artificial Intelligence, IEA/AIE?2005, pages 239?248, London,
UK, UK.
John Ross Quinlan. 1993. C4.5: programs for machine learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Koichi Takeuchi and Nigel Collier. 2005. Bio-medical entity extraction using support vector machines.
Artificial Intelligence in Medicine, 33(2):125?137, February.
Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational linguistics - Volume 2, COLING ?00, pages 947?953,
Stroudsburg, PA, USA.
Xing Zhang and Alex Chengyu Fang. 2010. An ATE system based on probabilistic relations between
terms and syntactic functions. In 10th International Conference on Statistical Analysis of Textual
Data, pages 1135?1143, Sapienza, Italy, June.
Xing Zhang, Yan Song, and Alex Chengyu Fang. 2010. How well conditional random fields can be used in
novel term recognition. In Proceedings of the 24th Pacific Asia Conference on Language, Information
and Computation, pages 583?592, Tohoku University, Sendai, Japan, November.
300
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 183?193, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Local and Global Context
for Supervised and Unsupervised Metonymy Resolution
Vivi Nastase
HITS gGmbH
Heidelberg, Germany
vivi.nastase@h-its.org
Alex Judea
University of Stuttgart
Stuttgart, Germany
alexander.judea@ims.uni-stuttgart.de
Katja Markert
University of Leeds
Leeds, UK
K.Markert@leeds.ac.uk
Michael Strube
HITS gGmbH
Heidelberg, Germany
michael.strube@h-its.org
Abstract
Computational approaches to metonymy res-
olution have focused almost exclusively on
the local context, especially the constraints
placed on a potentially metonymic word by
its grammatical collocates. We expand such
approaches by taking into account the larger
context. Our algorithm is tested on the data
from the metonymy resolution task (Task 8) at
SemEval 2007. The results show that incorpo-
ration of the global context can improve over
the use of the local context alone, depending
on the types of metonymies addressed. As a
second contribution, we move towards unsu-
pervised resolution of metonymies, made fea-
sible by considering ontological relations as
possible readings. We show that such an unsu-
pervised approach delivers promising results:
it beats the supervised most frequent sense
baseline and performs close to a supervised
approach using only standard lexico-syntactic
features.
1 Introduction
With the exception of explicit tasks in metonymy
and metaphor analysis, computational treatment of
language relies on the assumption that the texts to be
processed have a literal interpretation. This contrasts
with the fact that figurative expressions are com-
mon in language, as exemplified by the metonymy
in the excerpt from a Wikipedia article in Exam-
ple 1 and another in Example 2 from the SemEval
2007 metonymy resolution task (Markert and Nis-
sim, 2009).
(1) In the gold medal game, Canada defeated the
American team 2-0 to win their third consecu-
tive gold.
(2) This keyword is only required when your rela-
tional database is Oracle.
The defeating in Example 1 will not be done
by the country as such, but by a team represent-
ing the country in a sporting event. Hence, in a
metonymy a potentially metonymic expression or
word (here Canada) stands for a conceptually re-
lated entity (here, people of Canada). In the sec-
ond Example, a company name (Oracle) stands for
a product (database) developed by the company.
Metonymy resolution can be important for a
variety of tasks. Textual entailment may need
metonymy resolution (Bentivogli et al2007): for
example, we would like to be able to induce from
Example 1 the hypothesis
The Canadian team won . . . .
Leveling and Hartrumpf (2008) show that
metonymy recognition on location proper names
helps geographical information retrieval by ex-
cluding metonymically used place names from
consideration (such as Example 1 or the use of
Vietnam for the Vietnam war). Metonymies also fre-
quently interact with anaphora resolution (Nunberg,
1995; Markert and Hahn, 2002), as in Example 1
where the metonymic use of Canada is referred to
by a plural pronoun afterward (their).
Metonymies can be quite regular: company
names can be used for their management or their
products, country names can be used for associated
sports teams. Following from this, the currently
183
prevalent set-up for metonymy resolution ? as in
the SemEval 2007 task ? provides a manually com-
piled list of frequent readings or metonymic patterns
such as organization-for-product for pre-
specified semantic classes (such as organizations) as
well as annotated examples for these patterns so that
systems can then treat metonymy resolution as a (su-
pervised) word sense disambiguation task. How-
ever, this approach needs novel, manual provision
of readings as well as annotated examples for each
new semantic class.
In contrast, we will see readings as relations be-
tween the potentially metonymic word (PMW) and
other concepts in a large concept network, a priori
allowing all possible relations as readings. We base
this approach on the observation that metonymic
words stand in for concepts that they are related
with ? e.g. the part for the whole, the company
for the product. These readings are obtained on
the fly and are therefore independent of manually
provided, preclassified interpretations or semantic
classes, leading eventually to the possibility of un-
supervised metonymy resolution. We achieve this
by first linking a PMW to an article in Wikipedia.
Then we extract from a large concept network de-
rived from Wikipedia the relations surrounding the
PMW.
As there will be (many) more than one such rela-
tion, these need to be ranked or scored. We achieve
this in a probabilistic framework where we condi-
tion the probability of a relation on the context of
the PMW. This ranking showcases our second major
innovation in that the flexibility of our framework al-
lows us to incorporate a wider context than in most
prior approaches. Let us consider the indications for
metonymic readings and its interpretation in Exam-
ple 1, on the one hand, and Example 2, on the other
hand. In Example 1, the grammatical relation to the
verb defeat and the verb?s selectional preferences in-
dicate the metonymy. We will call all such grammat-
ically related words and the grammatical relations
the local context of the PMW. Such types of local
context have been used by most prior approaches
(Pustejovsky, 1991; Hobbs et al1993; Fass, 1991;
Nastase and Strube, 2009, among others). However,
Example 2 shows that the local context can be am-
biguous or often weak, such as the verb to be. In
these examples, the wider context (database, key-
word) is a better indication for a metonymy but has
not been satisfactorily integrated in prior approaches
(see Section 2). We here call all words surround-
ing the PMW but not grammatically related to it the
global context.
In our approach we integrate both the local and
the global context in our probabilistic framework.
For the local context, we compute the selectional
preferences for the words related to the PMW from a
corpus of English Wikipedia articles and generalize
them in the Wikipedia concept network, thus (auto-
matically) providing a set of abstractions ? general
concepts in the network that capture the semantic
classes required by the local context. In the next
step we compute probabilities of the global con-
text surrounding the PMWs under each (locally re-
quired) abstraction, and combine this with the se-
lectional preferences of the grammatically related
words. That we can integrate local and global con-
text in one probabilistic but also knowledge-based
framework is possible because we combine two de-
scriptions of meaning ? ontological and distribu-
tional ? by exploiting different sources of informa-
tion in Wikipedia (category-article hierarchy and ar-
ticle texts).
We compute the probabilities of the relations (=
readings) between the concept corresponding to the
PMW and its directly related concepts. These can
be used either (i) as additional features in a super-
vised approach or (ii) directly for unsupervised res-
olution. We do both in this paper and show that (i)
the supervised approach using both local and global
context can outperform one using just local con-
text, dependent on the semantic class studied and
(ii) that an unsupervised approach ? although lower
than the supervised one ? outperforms the super-
vised most frequent reading baseline and performs
close to a standard supervised model with the basic
set of lexico-syntactic features (Nissim and Markert,
2005).
2 Related Work
The word sense disambiguation setting for
metonymy resolution as developed by Nissim
and Markert (2005) and used for the SemEval 2007
task (Markert and Nissim, 2009) uses a small, pre-
specified number of frequently occurring readings.
184
The approaches building on this work (Farkas et
al., 2007; Nicolae et al2007, among others) are
supervised, mostly using shallow surface features
as well as grammatical relations.1 Most effective
in the SemEval task as summarized in Markert
and Nissim (2009) has been the local, grammatical
context, with the two systems relying on the global
context or the local/global context in a BOW model
(Leveling, 2007; Poibeau, 2007) not outperforming
the most frequent reading baseline. We believe
that might be due to the lack of a link between the
local and global context in these approaches ? in
our work, we condition the global context on the
abstractions and selectional preferences yielded by
the local context and achieve better results.
Lapata (2003), Shutova (2009) as well as Roberts
and Harabagiu (2011) deal with the issue of logical
metonymy, where the participant stands in for the
full event: e.g. Mary enjoyed the book., where book
stands in for reading the book, and this missing event
(reading) can be inferred from a corpus. Utiyama
et al2000), Lapata (2003) propose a probabilis-
tic model for finding the correct interpretation of
such metonymies in an unsupervised manner. How-
ever, these event type metonymies differ from the
problem dealt with in our paper and the SemEval
2007 task in that their recognition (i.e. their distinc-
tion from literal occurrences) is achieved simply by
grammatical patterns (a noun instead of a gerund or
to-infinitive following the verb) and the problem is
limited to interpretation.
Our view of relations in a concept network being
the interpretations of metonymies is strongly remi-
niscent of older work in metonymy resolution such
as Hobbs et al1993), Fass (1991), Markert and
Hahn (2002) or the use of a generative lexicon and
its relations in Pustejovsky (1991), which also are
unsupervised. However, these approaches lacked
scalability due to the use of small hand-modeled
knowledge bases which our use of a very large
Wikipedia-derived ontology overcomes. In addition,
most of these approaches (Fass, 1991; Hobbs et al
1993; Pustejovsky, 1991; Harabagiu, 1998) rely on
the view that metonymies violate selectional restric-
tions in their immediate, local context, usually those
1Brun et al2007) is semi-supervised but again relies on the
local grammatical context.
imposed by the verbs on their arguments. As can
be seen in the Example 2, this misses metonymies
which do not violate selectional restrictions. Nas-
tase and Strube (2009) use more flexible proba-
bilistic selectional preferences instead of strict con-
straint violations as well as WordNet as a larger tax-
onomy but are also restricted to the local context.
Markert and Hahn (2002) do propose a treatment of
metonymies that takes into account the larger dis-
course in the form of anaphoric relations between
a metonymy and the prior context. However, they
constrain discourse integration to potential PMWs
that are definite NPs and the context to few previous
noun phrases. In addition, their framework uses a
strict rule-based ranking of competing readings that
cannot be easily extended.
The work presented here also relies on a con-
cept network, built automatically from Wikipedia.
This resource provides us with links between enti-
ties in the text, and also a variety of ontological re-
lations for the PMW, that will allow us to identify a
wide variety of metonymic interpretations. Our ap-
proach combines information from the concept net-
work with automatically acquired selectional prefer-
ences as well as a possibility to combine in a prob-
abilistic framework the influence of the local and
global context on the interpretation of a potentially
metonymic word.
3 The Approach
The approach we present takes into account both
the local, grammatical, context and the larger textual
context of a potentially metonymic word. Figure 1
presents a graphical representation of our approach.
On the one hand, the word/term to be interpreted
(the potentially metonymic word/term ? PMW) is
mapped onto a concept in the concept network (Sec-
tion 3.3), which gives us access to the conceptual
relations (Ri) between the PMW and other concepts
(cx ? CRi). On the other hand, any word w gram-
matically related to the PMW via a grammatical re-
lation r provides us with semantic restrictions on the
interpretation of the PMW, namely preferred seman-
tic classes Aj (we call them abstractions) and a se-
lectional preference score.2 These are automatically
2We restrict the grammatical context that provides selec-
tional preferences to verbs or adjectives grammatically related
185
A 1 A 2
1R kR
A n
12c
14c
11c
13c 1n?1c
1nc
k1c k2
c
k4ck3c
kmc
km?1c w1w2w3
wl
w
r
...
...
PMW
p(Ri|Aj) p(Aj|Cont,w,r)
Global context
...
... ...
...
... ...
Figure 1: Metonymy resolution using selectional preferencesAj derived from local contextw and r, semantic relations
Ri to the PMW from a concept network, and the global context surrounding a term to be interpreted
acquired by using a corpus of Wikipedia articles and
a repository of encyclopedic knowledge (presented
in Section 3.1), as described in detail in 3.2. Because
the abstractions Aj and the PMW?s related concepts
(cx) come from the same structured resource, we
can compute the probabilities for each Ri given the
grammatically related word w and the grammatical
relation r. The global context can also easily be
added to the computation, as the probability of each
word in the context relative to an abstraction Aj can
be computed through the resource?s is a hierarchy
and its link to Wikipedia articles. This is detailed in
Section 3.4.
3.1 A concept network obtained from
Wikipedia
We use a Wikipedia article dump (January 2011)
which provided over 3.5 million English articles,
interconnected through a hierarchy of categories
and hyperlinks. This partly structured repository
is transformed into a large-scale multilingual con-
cept network, whose nodes are concepts correspond-
ing to articles or categories in Wikipedia (Nastase
et al2010). Concepts in this network are con-
nected through a variety of semantic relations (e.g.
is a, member of, nationality) derived from category
names and infoboxes. The version of WikiNet used
to the PMW.
had 3,707,718 nodes and 49,931,266 relation in-
stances of 494 types, and is freely available3.
WikiNet is used here as a concept inventory,
and its links and structure to generalize more spe-
cific concepts identified in texts to general concepts.
The fact that nodes in WikiNet correspond to arti-
cles/categories in Wikipedia is used to link article
texts in Wikipedia to general concepts, for the pur-
pose of computing various probability scores (de-
tailed in Section 3.4).
3.2 Selectional preferences and abstractions
To compute selectional preferences we use the set of
English Wikipedia articles, which describe specific
concepts. Wikipedia contributors are encouraged to
insert hyperlinks, which link important terms in an
article to the corresponding articles. A hyperlink
consists of two parts, the actual link (i.e. a URL)
and a phrase to appear in the text. Hyperlinks then
constitute a bridge from the textual level to the con-
ceptual level without the need for word sense dis-
ambiguation. We exploit these links to gather con-
cept arguments for verbs and adjectives, and gen-
eralize these using the concept network built from
Wikipedia.
The corpus of Wikipedia articles was first en-
riched with hyperlinks, making the ?one sense per
3http://www.h-its.org/english/research/
nlp/download/wikinet.php
186
Algorithm 1 computeSelPrefs(G,WkN)
Input: G ? grammatical relation triples
WkN ? WikiNet
M ? maximum number of generalization steps
Output: ?
? = {}
for all (w, r) such that (c, r, w) ? G do
S = {(c, f)|f is the frequency of (c, r, w) in G}
?w,r = S
mdl = MDL(?w,r,S)
for all i = 1,M do
?? = abstract(S,WkN)
mdl?? = MDL(??,S)
if mdl?? < mdl then
?w,r = ??
? = {?w,r} ? ?
return ?
Algorithm 2 MDL(?,S)
Input: ? = {(c, f)} ? a scored list of concepts
S ? the set of observations (concept collocates)
Output: MDL(?,S)
?? =< f1, ..., fn >; (ci, fi) ? ?
remove {(c, f) ? ?|f = 1} // parameter description
length :
L(??|?) = |?|?12 ? log(|S|) // data description length :
for all (c, f) ? ? do
L(S|?, ??) = L(S|?, ??) + f ? log( fhyponyms(c)?|?| )
return L(??|?)? L(S|?, ??)
Algorithm 3 abstract(S,WkN)
Input: S = {(c, f)|(w,R, c) ? G}
WkN ? WikiNet
Output: S ?
S ? = {}
for c|(c, ) ? S do
while c has only one is a link do
c = c?, (c, is a, c?) ?WkN
C = {(c?, c)|(c, is a, c?) ?WkN}
for (c?, c) ? C do
if (c?, f ?) ? S ? then
replace (c?, f ?) with (c?, f ? + f|C| ), (c, f) ? S
in S ?
else
S ?? = {(c?, f)}, (c, f) ? S
// Remove hyponyms.
for all {(c, c?) ? S ?|(c?, is a, c) ?WkN} do
// update frequency f of c
fc = fc + fc? , f ? S
delete c?
return S ?
discourse? assumption ? a phrase that appears as-
sociated with a hyperlink once in the article body
will be associated with the same hyperlink through-
out the article (this applies to the article title as well,
which is not hyperlinked in the article itself). This
new version of the corpus was then split into sen-
tences, and those without hyperlinks were removed.
The remaining 18 million sentences were parsed
with a parallelized version of Ensemble4 (Surdeanu
and Manning, 2010), and we extracted G, the set of
all grammatical relations of the type (verb, depen-
dency, hyperlink) and (adjective, dependency, hy-
perlink), with the hyperlinks resolved to their cor-
responding node (concept) in the network ( |G| =
1,578,413 triples). For each verb and adjective in the
extracted collocations, and for each of their depen-
dency relations, their collocates were generalized in
the network defined by the hypernym/hyponym re-
lations in WikiNet following a method similar to the
Minimum Description Length principle (Li and Abe,
1998).
Essentially, we aimed to determine a small set of
(more general) concepts that describe the set of col-
locates for a word w and grammatical relation r.
Starting from the concept collocates gathered, we
go upwards following WikiNet?s is a links, and for
each node found that covers at least N concept col-
locates (N is a parameter, N=2 in the experiments
presented here), the MDL score of the node is com-
puted (Algorithm 2). We place a limit M on the
number of upward steps in the hierarchy (M=3 in
our experiments). The disjoint set of nodes that has
the lowest overall MDL score is chosen (?), and for
each node in this cut (which we call abstraction),
we compute the selectional preference score, based
on the number of concepts it dominates.
As an example, for the verb defeat, the corpus
leads to collocations such as5:
defeat
nsubj
Earle Page (10357) ? 8, Manuela Maleeva
(1092361) ? 7, New York Yankees
(10128601) ? 5, Tommy Haas (1118005)
? 5, . . .
obj
4http://www.surdeanu.name/mihai/
ensemble/
5The format is:
Article name (Article Id) ? frequency.
187
New York Yankees (10128601) ? 9, Oak-
land Athletics (11641124) ? 6, Phoenix
Suns (11309373) ? 4, Jason Suttie
(10080653) ? 3, Ravana (100234) ? 3, . . .
Determining abstractions and selectional prefer-
ences leads to the following information6:
defeat
nsubj
Martial artists (118977183) ? 0.5, Person
(219599) ? 0.3518, Interest (146738) ?
0.037, . . .
obj
Video games (9570081) ? 0.25, British
games (24489088) ? 0.25, Person (219599)
? 0.1445, Interest (146738) ? 0.1341, . . .
3.3 Linking the PMW to the concept network
In our environment, linking the PMW to the con-
cept network is equivalent to finding its correspond-
ing concept in our ontology, WikiNet. We see this
corresponding concept as the literal reading of the
PMW. Doing so is a non-trivial task (see the Cross-
Lingual Link Discovery task at NTCIR-9 (Tang et
al., 2011) and the Cross-Lingual Entity Linking task
? part of the Knowledge Base Population track ? at
TAC 20117). In our particular setting, where we use
the metonymy data from SemEval 2007, the domain
of the PMW is well defined: locations and compa-
nies, respectively. Using these constraints, finding
the corresponding Wikipedia articles is much sim-
plified, by using the category hierarchy and con-
straining the concepts to fall under the Geography
and Companies categories respectively. When mul-
tiple options are present, we find instead a matching
disambiguation page. In this case we pick the article
that is listed first on this disambiguation page. On
a manually checked random sample, the accuracy of
the approach was 100% (on a sample of 100 PMWs).
3.4 Scoring conceptual relations with local and
global context
We work under the assumption that the concept cor-
responding to the PMW is related to the possible in-
terpretations through a semantic relation, in particu-
lar one that is captured in the concept network. After
6The format is:
Concept name (Concept Id) ? selectional preference score.
7http://nlp.cs.qc.cuny.edu/kbp/2011/
countries : Administrator of, Architect of,
Based in, Built in, Continent, ...
companies : Association, Brand, Company, Dis-
tributed by, Executive of, ...
Table 1: Example conceptual relations
establishing the connection to the resource by link-
ing the PMW to the concept cPMW corresponding to
its literal interpretation (see Section 3.3), we extract
the relations in which it is involved (Ri, i = 1, k),
and the concepts it is connected to through these re-
lations (CRi = {cx|(cPMWRicx)}). Table 1 shows
examples of conceptual relations extracted for com-
panies and countries.
We are interested in computing the likelihood of
a conceptual relation being the correct interpreta-
tion of a PMW, given its local and global context
p(Ri|Cont, w, r).
3.4.1 The local context
The local context considered in this work are all
grammatically related verbs and adjectives w and
their associated grammatical relation r. The gram-
matical analysis (see Section 3.2) provides the set of
abstractions corresponding to the grammatically re-
lated word w and grammatical relation r: Aj , j =
1, n. Remember that these are local context con-
straints on the interpretation of the PMW.
Through the knowledge resource used we can es-
tablish and quantify connections between each cx
and Aj , and thus between eachRi and Aj :
p(Ri|Aj) =
?
x?CRi
p(cx|Aj)(3)
where p(cx|Aj) is the probability of concept cx un-
der abstraction Aj , which is computed based on the
semantic relations in WikiNet:
p(cx|Aj) =
?
H
?
hi?H
p(hi|hi+1)
whereH is in turn each path from cx toAj following
is a links in WikiNet, starting with cx (i.e. h0 = cx)
and ending in Aj . p(hi|hi+1) is the probability of
the child node hi given its ancestor hi+1. Within this
work we assume a uniform probability distribution
in each node:
188
p(hi|hi+1) =
1
|descendants(hi+1)|
Through this, it is straightforward that
?
cx p(cx|Aj) = 1 when cx ranges over all
concepts subsumed by Aj , and is thus a valid
probability distribution.
3.4.2 The global context
The abstractions obtained before are concepts.
We extract all nodes in the network subsumed
by these concepts, and their corresponding articles
in Wikipedia (if they have one). This produces
?abstraction-specific? article sets, based on which
we compute the probability of the global context of
a PMW for each abstraction. We are interested in
the probability of an abstraction, given the context
and the word w and grammatical relation r, which
we compute as:
p(Aj |Cont, w, r) =
p(Cont|Aj , w, r) ? p(Aj , w, r)
p(Cont, w, r)
which, considering that p(Cont, w, r) is the same
for a given context, we approximate as
p(Aj |Cont) ? p(Cont|Aj) ? p(Aj , w, r)
p(Aj , w, r) = p(Aj |w, r)?p(w, r), and we approxi-
mate it through the computed selectional preference
p(Aj |w, r), since p(w, r) is constant for a given ex-
ample to analyze.
p(Cont|Aj , w, r) =
n?
j=1
p(Cont|Aj)p(Aj |w, r)
=
n?
j=1
(
m?
l=1
p(wl|Aj))p(Aj |w, r)
where Cont is the global context consisting of m
words wl, l = 1,m.8
8The global context therefore could be all words in a text
or all words in a sentence or any other token-based definition
in our framework. As the SemEval 2007 data gives metonymic
examples in a three-sentence context we use all the words in the
3 sentences as our global context.
p(wl|Aj) =
count(wl,Aj)
|Aj |
where Aj is the set of articles subsumed by abstrac-
tion Aj , and count(wl,Aj) is the number of times
word wl appears in the article collection Aj .
3.4.3 Putting it all together
This enables us now to compute p(Ri|Cont, w, r)
based on the formulas 3, 4:
p(Ri|Cont, w, r) =
n?
j=1
(p(Ri|Aj)?p(Aj |Cont, w, r))
4 Experiments
The computed probabilities for each conceptual re-
lation (= potential readings) of the PMW in the con-
cept network can be used as features in a supervised
framework or directly as an unsupervised prediction,
returning the most likely conceptual relation given
the context as the required reading.
Although the latter is our ultimate goal, to allow
comparison with related work from the metonymy
resolution task (Task 8) at SemEval 2007, we first
investigate the supervised set-up. We then simulate
the unsupervised setting in Section 4.3.
4.1 Data
We use the data from the metonymy resolution task
(Task 8) at SemEval 2007. It consists of training and
test data for country and company names which are
potentially metonymic. Table 2 shows the statistics
of the data, and the possible interpretations for the
PMWs. The training-test division was achieved ran-
domly so that the test data can have metonymic read-
ings for which no training data exists, showing again
the limitations of a supervised approach of prespec-
ified readings.
Grammatical features The features used by Nis-
sim and Markert (2005), and commonly used for
the supervised classification of metonymy readings
(Markert and Nissim, 2009):
? grammatical role of PMW (subj, obj, ...);
? lemmatized head/modifier of PMW (announce,
say, ...);
189
reading train test
locations 925 908
literal 737 721
mixed 15 20
othermet 9 11
obj-for-name 0 4
obj-for-representation 0 0
place-for-people 161 141
place-for-event 3 10
place-for-product 0 1
organizations 1090 842
literal 690 520
mixed 59 60
othermet 14 8
obj-for-name 8 6
obj-for-representation 1 0
org-for-members 220 161
org-for-event 2 1
org-for-product 74 67
org-for-facility 15 16
org-for-index 7 3
Table 2: Statistics for the Task 8 data
? determiner of PMW (def, indef, bare, demonst,
other, ...);
? grammatical number of PMW (sg, pl);
? number of grammatical roles in which the
PMW appears in its current context;
? number of words in PMW.
All these features can be extracted from the gram-
matically annotated and POS tagged data provided
by the organizers.
The annotations provided are dependency rela-
tions, many of which contain a preposition as an ar-
gument (e.g. (to, pp, UK) from the example ... the
visit to the UK of ...). Such relations are not infor-
mative, but together with the head that dominates the
prepositional complement (e.g. visit to) they may be.
Because of this, we process the provided annotations
and add wherever possible to the simple prepositions
the head of their subsuming constituent. This would
change the above mentioned dependency to (visit,
prep-to, UK).
Semantic relations as features To evaluate the
proposed approach we use the PMW?s conceptual
relations as features. The feature values are the
p(Ri|Cont, w, r) scores.
For the ?countries? portion of the data this adds
109 semantic relation features, and for companies
29 features. Table 1 showed examples of these new
features.
4.2 Supervised learning
We use the SMO classifier in the WEKA machine
learning toolkit (Witten and Frank, 2000) with its
standard settings, training on the SemEval 2007
(Task 8) training set.
Table 3 shows the results of various configura-
tions on the test data, in comparison with a most
frequent reading baseline (assigning literal to all
PMWs) as well as a system M&N that shows the re-
sults computed using only the features proposed by
Nissim and Markert (2005). In addition, we com-
pare to the best results9 at SemEval 2007 (SEmax)
and Nastase and Strube (2009) (N09). Nastase and
Strube (2009) added WordNet supersenses as fea-
tures, and their values are selectional preferences
computed with reference to WordNet. These are
similar to our abstractions, which in our approach
serve to link the local and the global context to the
ontological relations, but do not appear as features.
Our system SP shows the results obtained us-
ing the M&N features plus the conceptual relation
features conditioned on both local and global con-
text whereas SPlocal and SPglobal use conceptual
relations conditioned on local (p(Aj |Cont, w, r) ?
p(Aj |w, r)) or global context (p(Aj |Cont, w, r) ?
p(Aj |Cont) =
?n
j=1(
?m
l=1 p(wl|Aj))) only.
While the differences in overall accuracies are
small, there are significant differences in classifying
individual classes, as shown in Tables 4 ? 510, where
the distrib. column shows the class distribution in
the test data. It is interesting to note that, in our set-
ting, the global context is more useful than the local
9We show the best result for each category, not necessarily
from the overall best performing system. This holds for Tables
4 and 5 as well.
10The detailed results for previous approaches are reproduced
from (Nastase and Strube, 2009). We include only the classes
that have a non-zero F-score for at least one of the presented
approaches.
190
task ? method? baseline SEmax N09 M&N SP SPlocal SPglobal SPunsup
LOCATION-COARSE 79.4 85.2 86.1 83.4 85.8 83.0 85.0 81.6
LOCATION-MEDIUM 79.4 84.8 85.9 82.3 85.7 82.7 84.6 81.5
LOCATION-FINE 79.4 84.4 85.0 81.3 84.7 82.1 83.8 81.0
ORGANIZATION-COARSE 61.8 76.7 74.9 74.0 77.0 76.4 76.8 67.8
ORGANIZATION-MEDIUM 61.8 73.3 72.4 69.4 74.6 74.0 74.4 66.3
ORGANIZATION-FINE 61.8 72.8 71.0 68.5 72.8 71.9 72.7 65.3
Table 3: Accuracy scores
task ? method? distrib. SEmax N09 SP
LOCATION-COARSE
literal 79.4 91.2 91.6 91.4
non-literal 20.6 57.6 59.1 58.5
LOCATION-MEDIUM
literal 79.4 91.2 91.6 91.4
metonymic 18.4 58.0 61.5 61.6
mixed 2.2 8.3 16 9.1
LOCATION-FINE
literal 79.4 91.2 91.6 91.4
place-for-people 15.5 58.9 61.7 61.1
place-for-event 1.1 16.7 0 0
obj-for-name 0.4 66.7 0 0
mixed 2.2 8.3 16 9.1
Table 4: Fine-grained results for each classification task
for countries (F-scores)
one for resolving metonymies. Combining local and
global evidence improves over both, indicating that
the information they provide is not redundant.
For companies the difference is small in terms of
accuracy, but in classification of individual classes
the difference in performance is higher, but because
of the small data size not statistically significant.
Countries in WikiNet have a high number of sur-
rounding relations, because they are used as cat-
egorization criteria for professionals, for example,
which generates fine-grained relations such as Ad-
ministrator of, Ambassador of, Chemist of .... Such
a fine grained distinction between different profes-
sions for people in a country is not necessary, or in-
deed, desirable, for the metonymy resolution task.
The results show that despite this shortcoming, the
results are on par with the state-of-the-art, but in fu-
ture work we plan to explore the task of relation gen-
eralization and its impact on the current task.
task ? method? distrib. SEmax N09 SP
ORGANIZATION-COARSE
literal 61.8 82.5 81.4 82.7
non-literal 38.2 65.2 61.6 65.5
ORGANIZATION-MEDIUM
literal 61.8 82.5 81.4 82.7
metonymic 31.0 60.4 58.7 63.1
mixed 7.2 30.8 26.8 27.4
ORGANIZATION-FINE
literal 61.8 82.6 81.4 82.7
org-for-members 19.1 63.0 59.7 66.5
org-for-product 8.0 50.0 44.4 35.0
org-for-facility 2.0 22.2 36.3 45.5
org-for-name 0.7 80.0 58.8 44.4
mixed 7.2 34.3 27.1 27.4
Table 5: Fine-grained results for each classification task
for companies (F-scores)
4.3 Simulating unsupervised metonymy
resolution
In an unsupervised metonymy resolution approach,
we would assign as interpretation the conceptual re-
lation whose probability given the PMW, global and
local contexts is highest. To simulate then the un-
supervised metonymy resolution task, we make the
relation features (used in the supervised approach)
binary, where for each instance the relation that has
highest probability has the value 1, the others 0.
Using only the relation features simulates an un-
supervised approach ? this set-up learns a map-
ping between the relations used as features and
the metonymy classes in the data used. Column
SPUnsup in Table 3 shows the results obtained in
this configuration. As expected the results are lower,
but still close to the supervised method when using
only grammatical features (M&N) for the location
191
setting. The results also significantly beat the base-
line (apart from the Location-Fine setting). One fea-
ture that contributes greatly to the results, especially
for the company semantic class, is the grammatical
role of the PMW, but we could not incorporate this
in the unsupervised setting.
The results in the simulated unsupervised set-
ting indicate that relations are a viable substitute
for manually provided classes in an unsupervised
framework, while leaving space for improvement.
5 Conclusion
We have explored the usage of local and global con-
text for the task of metonymy resolution in a prob-
abilistic framework. The global context has been
rarely used for the task of determining the intended
reading of a potentially metonymic word (PMW)
in context. We rely on automatically computed se-
lectional preferences, extracted from a corpus of
Wikipedia articles, and generalized based on a con-
cept network also extracted from Wikipedia. De-
spite relying on automatically derived resources, the
presented approach produces results on-a-par with
current state-of-the-art systems. The method de-
scribed here is also a step towards the unsupervised
resolution of metonymic words in context, by tak-
ing into account knowledge about the concept cor-
responding to the literal interpretation of the PMW,
and its relations to other concepts. This frame-
work would also allow for exploring the metonymy
resolution phenomena in various languages (since
Wikipedia and WikiNet are multilingual), and inves-
tigate whether the same relations apply or different
languages have different metonymic patterns.
Acknowledgments
Katja Markert is the recipient of an Alexander-von-
Humboldt Fellowship for Experienced Researchers.
This work was financially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853) and the
Klaus Tschirra Foundation. We thank the review-
ers for the helpful comments, and Helga Kra?mer-
Houska for additional support for conference partic-
ipation.
References
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo Gi-
ampiccolo, Medea Lo Leggio, and Bernardo Magnini.
2007. Building textual entailment specialized data
sets: A methodology for isolating linguistic phenom-
ena relevant to inference. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation, La Valetta, Malta, 17?23 May 2010.
Caroline Brun, Maud Ehrmann, and Guillaume Jacquet.
2007. XRCE-M: A hybrid system for named en-
tity metonymy resolution. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-1), Prague, Czech Republic, 23?24 June
2007, pages 488?491.
Richa?rd Farkas, Eszter Simon, Gyo?rgy Szarvas, and
Da?niel Varga. 2007. GYDER: Maxent metonymy res-
olution. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-1), Prague,
Czech Republic, 23?24 June 2007, pages 161?164.
Dan C. Fass. 1991. met?: A method for discriminating
metonomy and metaphor by computer. Computational
Linguistics, 17(1):49?90.
Sanda M. Harabagiu. 1998. Deriving metonymic co-
ercions from WordNet. In Proceedings of the Work-
shop on the Usage of WordNet in Natural Language
Systems, Montral, Quebec, Canada, 16 August, 1998,
pages 142?148.
Jerry Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1-2):69?142.
Maria Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the 41st Annual Meeting of the Association for Compu-
tational Linguistics, Sapporo, Japan, 7?12 July 2003,
pages 545?552.
Johannes Leveling and Sven Hartrumpf. 2008. On
metonymy recognition for geographic information re-
trieval. International Journal of Geographical Infor-
mation Science, 22(3):289?299.
Johannes Leveling. 2007. FUH (FernUniversita?t in Ha-
gen): Metonymy recognition using different kinds of
context for a memory-based learner. In Proceedings
of the 4th International Workshop on Semantic Eval-
uations (SemEval-1), Prague, Czech Republic, 23?24
June 2007, pages 153?156.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217?244.
Katja Markert and Udo Hahn. 2002. Metonymies in dis-
course. Artificial Intelligence, 135(1/2):145?198.
Katja Markert and Malvina Nissim. 2009. Data and
models for metonymy resolution. Language Re-
sources and Evaluation, 43(2):123?138.
192
Vivi Nastase and Michael Strube. 2009. Combining
collocations, lexical and encyclopedic knowledge for
metonymy resolution. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, Singapore, 6-7 August 2009, pages
910?918.
Vivi Nastase, Michael Strube, Benjamin Bo?rschinger,
Ca?cilia Zirn, and Anas Elghafari. 2010. WikiNet:
A very large scale multi-lingual concept network.
In Proceedings of the 7th International Conference
on Language Resources and Evaluation, La Valetta,
Malta, 17?23 May 2010.
Cristina Nicolae, Gabriel Nicolae, and Sanda Harabagiu.
2007. UTD-HLT-CG: Semantic architecture for
metonymy resolution and classification of nominal re-
lations. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-1), Prague,
Czech Republic, 23?24 June 2007, pages 454?459.
Malvina Nissim and Katja Markert. 2005. Learning to
buy a Renault and talk to BMW: A supervised ap-
proach to conventional metonymy. In Proceedings of
the 6th International Workshop on Computational Se-
mantics, Tilburg, Netherlands, January 12-14, 2005.
Geoffrey Nunberg. 1995. Transfers of meaning. Journal
of Semantics, 12(1):109?132.
Thierry Poibeau. 2007. Up13: Knowledge-poor meth-
ods (sometimes) perform poorly. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations (SemEval-1), Prague, Czech Republic, 23?24
June 2007, pages 418?421.
James Pustejovsky. 1991. The generative lexicon. Com-
putational Linguistics, 17(4):209?241.
Kirk Roberts and Sanda M. Harabagiu. 2011. Unsuper-
vised learning of selectional restrictions and detection
of argument coercions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, Edinburgh, UK, 27-29 July 2011,
pages 980?990.
Ekaterina Shutova. 2009. Sense-based interpretation of
logical metonymy using a statistical method. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the Association for Computational Lin-
guistics and the 4th International Joint Conference on
Natural Language Processing, Singapore, 2?7 August
2009, pages 1?9.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble Models for Dependency Parsing: Cheap and
Good? In Proceedings of Human Language Tech-
nologies 2010: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Los Angeles, Cal., 2?4 June 2010, pages 649?
652.
Ling-Xiang Tang, Shlomo Geva, Andrew Trotman, Yue
Xu, and Kelly Y. Itakura. 2011. Overview of the
NTCIR-9 crosslink task: Cross-lingual link discovery.
In Proceedings of the 9th NII Test Collection for IR
Systems Workshop meeting ? NTCIR-9 Tokyo, Japan,
6?9 December 2011.
Masao Utiyama, Masaki Murata, and Hitoshi Isahara.
2000. A statistical approach to the processing
of metonymy. In Proceedings of the 18th Inter-
national Conference on Computational Linguistics,
Saarbru?cken, Germany, 31 July ? 4 August 2000,
pages 885?891.
Ian H. Witten and Eibe Frank. 2000. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann, San
Diego, CA.
193
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 963?967,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Dependency parsing with latent refinements of part-of-speech tags
Thomas M?uller
?
, Richard Farkas
?
, Alex Judea
?
, Helmut Schmid
?
, and Hinrich Sch?utze
?
?
Center for Information and Language Processing, University of Munich, Germany
?
Department of Informatics, University of Szeged, Hungary
?
Heidelberg Institute for Theoretical Studies, Heidelberg, Germany
muellets@cis.lmu.de
Abstract
In this paper we propose a method to
increase dependency parser performance
without using additional labeled or unla-
beled data by refining the layer of pre-
dicted part-of-speech (POS) tags. We per-
form experiments on English and Ger-
man and show significant improvements
for both languages. The refinement is
based on generative split-merge training
for Hidden Markov models (HMMs).
1 Introduction
Probabilistic Context-free Grammars with latent
annotations (PCFG-LA) have been shown (Petrov
et al., 2006) to yield phrase structure parsers
with state-of-the-art accuracy. While Hidden
Markov Models with latent annotations (HMM-
LA) (Huang et al., 2009), stay somewhat behind
the performance of state-of-the-art discriminative
taggers (Eidelman et al., 2010). In this paper we
address the question of whether the resulting la-
tent POS tags are linguistically meaningful and
useful for upstream tasks such as syntactic pars-
ing. We find that this is indeed the case, lead-
ing to a procedure that significantly increases the
performance of dependency parsers. The proce-
dure is attractive because the refinement of pre-
dicted part-of-speech sequences using a coarse-to-
fine strategy (Petrov and Klein, 2007) is fast and
efficient. More precisely, we show that incorpo-
rating the induced POS into a state-of-the-art de-
pendency parser (Bohnet, 2010) gives increases in
Labeled Attachment Score (LAS): from 90.34 to
90.57 for English and from 87.92 to 88.24 (resp.
88.35 to 88.51) for German without using (resp.
with using) morphological features.
2 Related Work
Petrov et al. (2006) introduce generative split-
merge training for PCFGs and provide a fully au-
tomatic method for training state-of-the-art phrase
structure parsers. They argue that the resulting la-
tent annotations are linguistically meaningful. Sun
et al. (2008) induce latent sub-states into CRFs and
show that noun phrase (NP) recognition can be im-
proved, especially if no part-of-speech features are
available. Huang et al. (2009) apply split-merge
training to create HMMs with latent annotations
(HMM-LA) for Chinese POS tagging. They re-
port that the method outperforms standard gener-
ative bigram and trigram tagging, but do not com-
pare to discriminative methods. Eidelman et al.
(2010) show that a bidirectional variant of latent
HMMs with incorporation of prosodic information
can yield state-of-the-art results in POS tagging of
conversational speech.
3 Split-Merge Training for HMMs
Split-merge training for HMMs (Huang et al.,
2009) iteratively splits every tag into two subtags.
Word emission and tag transition probabilities of
subtags are then initialized close to the values of
the parent tags but with some randomness to break
symmetry. Using expectation?maximization (EM)
training the parameters can then be set to a local
maximum of the training data likelihood. After
this split phase, the merge phase reverts splits that
only lead to small improvements in the likelihood
function in order to increase the robustness of the
model. This approach requires an approximation
of the gain in likelihood of every split analogous
to Petrov et al. (2006) as an exact computation is
not feasible.
We have observed that this procedure is not
963
Universal Tag Feature Tag
0
Tag
1
English Adjectives p(w|t) more (0.05) many (0.03) last (0.03) new (0.03) other (0.03) first (0.02)
(ADJ) p(u|t) VERB (0.32) ADV (0.27) NOUN (0.14) DET (0.39) ADP (0.17) ADJ (0.10)
Particles p(w|t) ?s (0.93) ? (0.07) to (0.89) up (0.04) out (0.02) off (0.01)
(PRT) p(b|t) POS (1.00) TO (0.89) RP (0.10)
Prepositions p(w|t) that (0.11) in (0.10) by (0.09) of (0.43) in (0.19) for (0.11)
(ADP) p(u|t) VERB (0.46) NOUN (0.15) . (0.13) NOUN (0.84) NUM (0.06) ADJ (0.03)
Pronouns p(w|t) its (0.30) their (0.15) his (0.14) it (0.21) he (0.16) they (0.12)
(PRON) p(b|t) PRP$ (0.68) PRP (0.26) WP (0.05) PRP (0.87) WP (0.11) PRP$ (0.02)
Verbs p(w|t) be (0.06) been (0.02) have (0.02) is (0.10) said (0.08) was (0.05)
(VERB) p(u|t) VERB (0.38) PRT (0.22) ADV (0.11) NOUN (0.52) PRON (0.20) . (0.12)
German Conjunctions p(w|t) da? (0.26) wenn (0.08) um (0.06) und (0.76) oder (0.07) als (0.06)
(CONJ) p(b|t) KOUS (0.58) KON (0.30) KOUI (0.06) KON (0.88) KOKOM (0.10) APPR (0.02)
Particles p(w|t) an (0.13) aus (0.10) ab (0.09) nicht (0.49) zu (0.46) Nicht (0.01)
(PRT) p(b|t) PTKVZ (0.92) ADV (0.04) ADJD (0.01) PTKNEG (0.52) PTKZU (0.44) PTKA (0.02)
Pronouns p(w|t) sich (0.13) die (0.08) es (0.07) ihre (0.06) seine (0.05) seiner (0.05)
(PRON) p(b|t) PPER (0.33) PRF (0.14) PRELS (0.14) PPOSAT (0.40) PIAT (0.34) PDAT (0.16)
Verbs p(w|t) werden (0.04) worden (0.02) ist (0.02) ist (0.07) hat (0.04) sind (0.03)
(VERB) p(u|t) NOUN (0.46) VERB (0.22) PRT (0.10) NOUN (0.49) . (0.19) PRON (0.16)
Table 1: Induced sub-tags and their statistics, word forms (p(w|t)), treebank tag (p(b|t)) and preceding
Universal tag probability (p(u|t)). Bold: linguistically interesting differences.
only a way to increase HMM tagger performance
but also yields annotations that are to a consid-
erable extent linguistically interpretable. As an
example we discuss some splits that occurred af-
ter a particular split-merge step for English and
German. For the sake of comparability we ap-
plied the split to the Universal Tagset (Petrov et
al., 2011). Table 1 shows the statistics used for
this analysis. The Universal POS tag set puts the
three Penn-Treebank tags RP (particle), POS (pos-
sessive marker) and TO into one particle tag (see
?PRT? in English part of the table). The training
essentially reverses this by splitting particles first
into possessive and non-possessive markers and in
a subsequent split the non-possessives into TO and
particles. For German we have a similar split into
verb particles, negation particles like nicht ?not?
and the infinitive marker zu ?to? (?PRT?) in the
German part of the table). English prepositions
get split by proximity to verbs or nouns (?ADP?).
Subordinate conjunctions like that, which in the
Penn-Treebank annotation are part of the prepo-
sition tag IN, get assigned to the sub-class next
to verbs. For German we also see a separation
of ?CONJ? into predominantly subordinate con-
junctions (Tag 0) and predominantly coordinating
conjunctions (Tag 1). For both languages adjec-
tives get split by predicative and attributive use.
For English the predicative sub-class also seems
to hold rather atypical adjectives like ?such? and
?last.? For English, verbs (?VERB?) get split into
a predominantly infinite tag (Tag 0) and a predom-
inantly finite tag (Tag 1) while for German we get
a separation by verb position. In German we get a
separation of pronouns (?PRON?) into possessive
and non-possessive; in English, pronouns get split
by predominant usage in subject position (Tag 0)
and as possessives (Tag 1).
Our implementation of HMM-LA has been re-
leased under an open-source licence.
1
In the next section we evaluate the utility of
these annotations for dependency parsing.
4 Dependency Parsing
In this section we investigate the utility of in-
duced POS as features for dependency parsing.
We run our experiments on the CoNLL-2009 data
sets (Haji?c et al., 2009) for English and German.
As a baseline system we use the latest version
of the mate-tools parser (Bohnet, 2010).
3
It was
the highest scoring syntactic parser for German
and English in the CoNLL 2009 shared task eval-
uation. The parser gets automatically annotated
lemmas, POS and morphological features as input
which are part of the CoNLL-2009 data sets.
In this experiment we want to examine the ben-
efits of tag refinements isolated from the improve-
ments caused by using two taggers in parallel,
thus we train the HMM-LA on the automatically
tagged POS sequences of the training set and use
it to add an additional layer of refined POS to the
input data of the parser. We do this by calculating
the forward-backward charts that are also used in
the E-steps during training ? in these charts base
1
https://code.google.com/p/cistern/
1
Unlabeled Attachment Score
3
We use v3.3 of Bohnet?s graph-based parser.
964
#Tags ?
LAS
max
LAS
?
LAS
?
UAS
max
UAS
?
UAS
English Baseline 88.43 91.46
58 88.52 (88.59) 0.06 91.52 (91.61) 0.08
73 88.55 (88.61) 0.05 91.54 (91.59) 0.04
92 88.60 (88.71) 0.08 91.60 (91.72) 0.08
115 88.62 (88.73) 0.07 91.58 (91.71) 0.08
144 88.60 (88.70) 0.07 91.60 (91.71) 0.07
German (no feat.) Baseline 87.06 89.54
85 87.09 (87.18) 0.06 89.61 (89.67) 0.04
107 87.23 (87.36) 0.09 89.74 (89.83) 0.08
134 87.22 (87.31) 0.09 89.75 (89.86) 0.09
German (feat.) Baseline 87.35 89.75
85 87.33 (87.47) 0.11 89.76 (89.88) 0.09
107 87.43 (87.73) 0.16 89.81 (90.14) 0.17
134 87.38 (87.53) 0.08 89.75 (89.89) 0.08
Table 2: LAS and UAS
1
mean (?), best value (max) and std. deviation (?) for the development set for
English and German dependency parsing with (feat.) and without morphological features (no feat.).
tags of the refined tags are constrained to be iden-
tical to the automatically predicted tags.
We use 100 EM iterations after each split and
merge phase. The percentage of splits reverted in
each merge phase is set to .75.
We integrate the tags by adding one additional
feature for every edge: the conjunction of latent
tags of the two words connected by the edge.
Table 2 shows results of our experiments. All
numbers are averages of five independent runs.
For English the smaller models with 58 and 73
tags achieve improvements of ?.1. The improve-
ments for the larger tag sets are ?.2. The best
individual model improves LAS by .3. For the
German experiments without morphological fea-
tures we get only marginal average improvements
for the smallest tag set and improvements of ?.15
for the bigger tag sets. The average ULA scores
for 107 and 134 tags are at the same level as the
ULA scores of the baseline with morph. features.
The best model improves LAS by .3. For German
with morphological features the absolute differ-
ences are smaller: The smallest tag set does not
improve the parser on average. For the tag set
of 107 tags the average improvement is .08. The
best model improves LAS by .38. In all experi-
ments we see the highest improvements for tag set
sizes of roughly the same size (115 for English,
107 for German). While average improvements
are low (esp. for German with morphological fea-
tures), peak improvements are substantial.
Running the best English system on the test set
gives an improvement in LAS from 90.34 to 90.57;
this improvement is significant
4
(p < .02). For
German we get an improvement from 87.92 to
4
Approx. randomization test (Yeh, 2000) on LAS scores
88.24 without and from 88.35 to 88.51 with mor-
phological features. The difference between the
values without morphological features is signifi-
cant (p < .05), but the difference between mod-
els with morphological features is not (p = .26).
However, the difference between the baseline sys-
tem with morphological features and the best sys-
tem without morphological features is also not sig-
nificant (p = .49).
We can conclude that HMM-LA tags can sig-
nificantly improve parsing results. For German we
see that HMM-LA tags can substitute morpholog-
ical features up to an insignificant difference. We
also see that morphological features and HMM-
LA seem to be correlated as combining the two
gives only insignificant improvements.
5 Contribution Analysis
In this section we try to find statistical evidence
for why a parser using a fine-grained tag set might
outperform a parser based on treebank tags only.
The results indicate that an induced latent tag
set as a whole increases parsing performance.
However, not every split made by the HMM-LA
seems to be useful for the parser. The scatter plots
in Figure 1 show that there is no strict correlation
between tagging accuracy of a model and the re-
sulting LAS. This is expected as the latent induc-
tion optimizes a tagging objective function, which
does not directly translate into better parsing per-
formance. An example is lexicalization. Most
latent models for English create a subtag for the
preposition ?of?. This is useful for a HMM as ?of?
is frequent and has a very specific context. A lexi-
calized syntactic parser, however, does not benefit
from such a tag.
965
l l l ll
88.40 88.45 88.50 88.55 88.60 88.65 88.70 88.75
97.5
97.6
97.7
97.8
97.9
98.0
LAS
Taggin
g Accu
racy
87.00 87.05 87.10 87.15 87.20 87.25 87.30 87.35
97.10
97.12
97.14
97.16
97.18
97.20
LAS
Taggin
g Accu
racy
87.2 87.3 87.4 87.5 87.6 87.7
97.10
97.12
97.14
97.16
97.18
97.20
LAS
Taggin
g Accu
racy
Figure 1: Scatter plots of LAS vs tagging accuracy for English (left) and German without (middle) and
with (right) morphological features. English tag set sizes are 58 (squares), 73 (diamonds), 92 (trian-
gles), 115 (triangles pointing downwards) and 144 (circles). German tag set sizes are 85 (squares), 107
(diamonds) and 134 (triangles). The dashed lines indicate the baselines.
We base the remainder of our analysis on the
results of the baseline parser on the English devel-
opment set and the results of the best performing
latent model. The best performing model has a
LAS score of 88.73 vs 88.43 for the baseline, a dif-
ference of .3. If we just look at the LAS of words
with incorrectly predicted POS we see a difference
of 1.49. A look at the data shows that the latent
model helps the parser to identify words that might
have been annotated incorrectly. As an example
consider plural nouns (NNS) and two of their la-
tent subtags NNS
1
and NNS
2
and how often they
get classified correctly and misclassified as proper
nouns (NNPS):
NNS NNPS
NNS 2019 104
NNS
1
90 72
NNS
2
1100 13
. . . . . . . . .
We see that NNS
1
is roughly equally likely to
be a NNPS or NNS while NNS
2
gives much more
confidence of the actual POS being NNS. So one
benefit of HMM-LA POS tag sets are tags of dif-
ferent levels of confidence.
Another positive effect is that latent POS tags
have a higher correlation with certain dependency
relations. Consider proper nouns (NNP):
NAME NMOD SBJ
NNP 962 662 468
NNP
1
10 27 206
NNP
2
24 50 137
. . . . . . . . . . . .
We see that NNP
1
and NNP
2
are more likely
to appear in subject relations. NNP
1
contains sur-
names; the most frequent word forms are Keating,
Papandreou and Kaye. In contrast, NNP
2
con-
tains company names such as Sony, NBC and Key-
stone. This explains why the difference in LAS is
twice as high for NNPs as on average.
For German we see similar effects and the an-
ticipated correlation with morphology. The 5 de-
terminer subtags, for example, strongly correlate
with grammatical case:
Nom Gen Dat Acc
ART 1185 636 756 961
ART
1
367 7 38
ART
2
11 28 682 21
ART
3
6 602 7 3
ART
4
39 43 429
ART
5
762 6 17 470
6 Conclusion and Future Work
We have shown that HMMs with latent anno-
tations (HMMLA) can generate latent part-of-
speech tagsets are linguistically interpretable and
can be used to improve dependency parsing. Our
best systems improve an English parser from a
LAS of 90.34 to 90.57 and a German parser from
87.92 to 88.24 when not using morphological fea-
tures and from 88.35 to 88.51 when using mor-
phological features . Our analysis of the parsing
results shows that the major reasons for the im-
provements are: the separation of POS tags into
more and less trustworthy subtags, the creation of
POS subtags with higher correlation to certain de-
pendency labels and for German a correlation of
tags and morphological features such as case.
7 Future Work
The procedure works well in general. However,
not every split is useful for the parser; e.g., as
966
discussed above lexicalization increases HMM ac-
curacy, but does not help an already lexicalized
parser. We would like to use additional informa-
tion (e.g., from the dependency trees) to identify
useless splits. The different granularities of the hi-
erarchy induced by split-merge training are poten-
tially useful. However, the levels of the hierarchy
are incomparable: a child tag is in general not a
subtag of a parent tag. We think that coupling par-
ents and children in the tag hierarchy might be one
way to force a consistent hierarchy.
Acknowledgments
We would like to thank the anonymous reviewers
for their comments. The first author is a recipient
of the Google Europe Fellowship in Natural Lan-
guage Processing, and this research is supported in
part by this Google Fellowship and by DFG (grant
SFB 732). Most of this work was conducted while
the authors worked at the Institute for Natural Lan-
guage Processing of the University of Stuttgart.
References
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of COLING.
Vladimir Eidelman, Zhongqiang Huang, and Mary
Harper. 2010. Lessons learned in part-of-speech
tagging of conversational speech. In Proceedings of
EMNLP.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, et al. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of CoNLL.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm
part-of-speech tagger by latent annotation and self-
training. In Proceedings of NAACL.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL.
Slav Petrov, Dipanjan Das, and Ryan McDon-
ald. 2011. A universal part-of-speech tagset.
ArXiv:1104.2086v1.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,
and Jun?ichi Tsujii. 2008. Modeling latent-dynamic
in shallow parsing: a latent conditional model with
improved inference. In Proceedings of COLING.
Alexander Yeh. 2000. More accurate tests for the
statistical significance of result differences. In Pro-
ceedings of COLING.
967
