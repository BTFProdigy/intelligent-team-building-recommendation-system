Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 199?207,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Summarizing Definition from Wikipedia
Shiren Ye and Tat-Seng Chua and Jie Lu
Lab of Media Search
National University of Singapore
{yesr|chuats|luj}@comp.nus.edu.sg
Abstract
Wikipedia provides a wealth of knowl-
edge, where the first sentence, infobox
(and relevant sentences), and even the en-
tire document of a wiki article could be
considered as diverse versions of sum-
maries (definitions) of the target topic.
We explore how to generate a series of
summaries with various lengths based on
them. To obtain more reliable associations
between sentences, we introduce wiki con-
cepts according to the internal links in
Wikipedia. In addition, we develop an
extended document concept lattice model
to combine wiki concepts and non-textual
features such as the outline and infobox.
The model can concatenate representative
sentences from non-overlapping salient lo-
cal topics for summary generation. We test
our model based on our annotated wiki ar-
ticles which topics come from TREC-QA
2004-2006 evaluations. The results show
that the model is effective in summariza-
tion and definition QA.
1 Introduction
Nowadays, ?ask Wikipedia? has become as pop-
ular as ?Google it? during Internet surfing, as
Wikipedia is able to provide reliable information
about the concept (entity) that the users want. As
the largest online encyclopedia, Wikipedia assem-
bles immense human knowledge from thousands of
volunteer editors, and exhibits significant contribu-
tions to NLP problems such as semantic related-
ness, word sense disambiguation and question an-
swering (QA).
For a given definition query, many search en-
gines (e.g., specified by ?define:? in Google) often
place the first sentence of the corresponding wiki1
article at the top of the returned list. The use of
1 For readability, we follow the upper/lower case rule
on web (say, ?web pages? and ?on the Web?), and utilize
one-sentence snippets provides a brief and concise
description of the query. However, users often need
more information beyond such a one-sentence de-
finition, while feeling that the corresponding wiki
article is too long. Thus, there is a strong demand
to summarize wiki articles as definitions with vari-
ous lengths to suite different user needs.
The initial motivation of this investigation is to
find better definition answer for TREC-QA task
using Wikipedia (Kor and Chua, 2007). Accord-
ing to past results on TREC-QA (Voorhees, 2004;
Voorhees and Dang, 2005), definition queries are
usually recognized as being more difficult than fac-
toid and list queries. Wikipedia could help to
improve the quality of answer finding and even
provide the answers directly. Its results are bet-
ter than other external resources such as WordNet,
Gazetteers and Google?s define operator, especially
for definition QA (Lita et al, 2004).
Different from the free text used in QA and sum-
marization, a wiki article usually contains valuable
information like infobox and wiki link. Infobox
tabulates the key properties about the target, such
as birth place/date and spouse for a person as well
as type, founder and products for a company. In-
fobox, as a form of thumbnail biography, can be
considered as a mini version of a wiki article?s sum-
mary. In addition, the relevant concepts existing in
a wiki article usually refer to other wiki pages by
wiki internal links, which will form a close set of
reference relations. The current Wikipedia recur-
sively defines over 2 million concepts (in English)
via wiki links. Most of these concepts are multi-
word terms, whereas WordNet has only 50,000 plus
multi-word terms. Any term could appear in the
definition of a concept if necessary, while the total
vocabulary existing in WordNet?s glossary defini-
tion is less than 2000. Wikipedia addresses explicit
semantics for numerous concepts. These special
knowledge representations will provide additional
information for analysis and summarization. We
thus need to extend existing summarization tech-
nologies to take advantage of the knowledge repre-
sentations in Wikipedia.
?wiki(pedia) articles? and ?on (the) Wikipedia?, the latter re-
ferring to the entire Wikipedia.
199
The goal of this investigation is to explore sum-
maries with different lengths in Wikipedia. Our
main contribution lies in developing a summariza-
tion method that can (i) explore more reliable asso-
ciations between passages (sentences) in huge fea-
ture space represented by wiki concepts; and (ii) ef-
fectively combine textual and non-textual features
such as infobox and outline in Wikipedia to gener-
ate summaries as definition.
The rest of this paper is organized as follows: In
the next section, we discuss the background of sum-
marization using both textual and structural fea-
tures. Section 3 presents the extended document
concept lattice model for summarizing wiki arti-
cles. Section 4 describes corpus construction and
experiments are described; while Section 5 con-
cludes the paper.
2 Background
Besides some heuristic rules such as sentence po-
sition and cue words, typical summarization sys-
tems measure the associations (links) between sen-
tences by term repetitions (e.g., LexRank (Erkan
and Radev, 2004)). However, sophisticated authors
usually utilize synonyms and paraphrases in vari-
ous forms rather than simple term repetitions. Fur-
nas et al (1987) reported that two people choose
the same main key word for a single well-known
object less than 20% of the time. A case study by
Ye et al (2007) showed that 61 different words ex-
isting in 8 relevant sentences could be mapped into
16 distinctive concepts by means of grouping terms
with close semantic (such as [British, Britain, UK]
and [war, fought, conflict, military]). However,
most existing summarization systems only consider
the repeated words between sentences, where latent
associations in terms of inter-word synonyms and
paraphrases are ignored. The incomplete data likely
lead to unreliable sentence ranking and selection for
summary generation.
To recover the hidden associations between sen-
tences, Ye et al (2007) compute the semantic simi-
larity using WordNet. The term pairs with semantic
similarity higher than a predefined threshold will be
grouped together. They demonstrated that collect-
ing more links between sentences will lead to bet-
ter summarization as measured by ROUGE scores,
and such systems were rated among the top systems
in DUC (document understanding conference) in
2005 and 2006. This WordNet-based approach has
several shortcomings due to the problems of data
deficiency and word sense ambiguity, etc.
Wikipedia already defined millions of multi-
word concepts in separate articles. Its definition is
much larger than that of WordNet. For instance,
more than 20 kinds of songs and movies called But-
terfly , such as Butterfly (Kumi Koda song), Butter-
fly (1999 film) and Butterfly (2004 film), are listed
in Wikipedia. When people say something about
butterfly in Wikipedia, usually, a link is assigned
to refer to a particular butterfly. Following this
link, we can acquire its explicit and exact seman-
tic (Gabrilovich and Markovitch, 2007), especially
for multi-word concepts. Phrases are more im-
portant than individual words for document re-
trieval (Liu et al, 2004). We hope that the wiki con-
cepts are appropriate text representation for sum-
marization.
Generally, wiki articles have little redundancy
in their contents as they utilize encyclopedia style.
Their authors tend to use wiki links and ?See Also?
links to refer to the involved concepts rather than
expand these concepts. In general, the guideline
for composing wiki articles is to avoid overlong
and over-complicated styles. Thus, the strategy of
?split it? into a series of articles is recommended;
so wiki articles are usually not too long and contain
limited number of sentences. These factors lead to
fewer links between sentences within a wiki article,
as compared to normal documents. However, the
principle of typical extractive summarization ap-
proaches is that the sentences whose contents are
repeatedly emphasized by the authors are most im-
portant and should be included (Silber and McCoy,
2002). Therefore, it is challenging to summarize
wiki articles due to low redundancy (and links)
between sentences. To overcome this problem,
we seek (i) more reliable links between passages,
(ii) appropriate weighting metric to emphasize the
salient concepts about the topic, and (iii) additional
guideline on utilizing non-textual features such as
outline and infobox. Thus, we develop wiki con-
cepts to replace ?bag-of-words? approach for better
link measurements between sentences, and extend
an existing summarization model on free text to in-
tegrate structural information.
By analyzing rhetorical discourse structure of
aim, background, solution, etc. or citation context,
we can obtain appropriate abstracts and the most
influential contents from scientific articles (Teufel
and Moens, 2002; Mei and Zhai, 2008). Similarly,
we believe that the structural information such as
infobox and outline is able to improve summariza-
tion as well. The outline of a wiki article using in-
ner links will render the structure of its definition.
In addition, infobox could be considered as topic
signature (Lin and Hovy, 2000) or keywords about
the topic. Since keywords and summary of a doc-
ument can be mutually boosted (Wan et al, 2007),
infobox is capable of summarization instruction.
When Ahn (2004) and Kor (2007) utilize
Wikipedia for TREC-QA definition, they treat the
Wikipedia as the Web and perform normal search
on it. High-frequency terms in the query snippets
returned from wiki index are used to extend query
and rank (re-rank) passages. These snippets usually
200
come from multiple wiki articles. Here the use-
ful information may be beyond these snippets but
existing terms are possibly irrelevant to the topic.
On the contrary, our approach concentrates on the
wiki article having the exact topic only. We as-
sume that every sentence in the article is used to de-
fine the query topic, no matter whether it contains
the term(s) of the topic or not. In order to extract
some salient sentences from the article as definition
summaries, we will build a summarization model
that describes the relations between the sentences,
where both textual and structural features are con-
sidered.
3 Our Approach
3.1 Wiki Concepts
In this subsection, we address how to find rea-
sonable and reliable links between sentences using
wiki concepts.
Consider a sentence: ?After graduating from
Boston University in 1988, she went to work at a
Calvin Klein store in Boston.? from a wiki article
?Carolyn Bessette Kennedy?2, we can find 11 dis-
tinctive terms, such as after, graduate, Boston, Uni-
versity,1988, go, work, Calvin, Klein, store, Boston,
if stop words are ignored.
However, multi-word terms such as Boston
University and Calvin Klein are linked to the
corresponding wiki articles, where their definitions
are given. Clearly, considering the anchor texts as
two wiki concepts rather than four words is more
reasonable. Their granularity are closer to semantic
content units in a summarization evaluation method
Pyramid (Nenkova et al, 2007) and nuggets in
TREC-QA . When the text is represented by
wiki concepts, whose granularity is similar to the
evaluation units, it is possibly easy to detect the
matching output using a model. Here,
? Two separate words, Calvin and Klein, are
meaningless and should be discarded; oth-
erwise, spurious links between sentences are
likely to occur.
? Boston University and Boston are processed
separately, as they are different named entities.
No link between them is appropriate3.
? Terms such as ?John F. Kennedy, Jr.? and
?John F. Kennedy? will be considered as two
diverse wiki concepts, but we do not account
on how many repeated words there are.
? Different anchor texts, such as U.S.A. and
United States of America, are recognized as
2All sample sentences in this paper come from this article
if not specified.
3Consider new pseudo sentence: ?After graduating from
Stanford in 1988, she went to work ... in Boston.? We do not
need assign link between Stanford and Boston as well.
an identical concept since they refer to the
same wiki article.
? Two concepts, such as money and cash, will
be merged into an identical concept when their
semantics are similar.
In wiki articles, the first occurrence of a wiki
concept is tagged by a wiki link, but there is no
such a link to its subsequent occurrences in the re-
maining parts of the text in most cases. To allevi-
ate this problem, a set of heuristic rules is proposed
to unify the subsequent occurrences of concepts in
normal text with previous wiki concepts in the an-
chor text. These heuristic rules include: (i) edit dis-
tance between linked wiki concept and candidates
in normal text is larger than a predefined threshold;
and (ii) partially overlapping words beginning with
capital letter, etc.
After filtering out wiki concepts, the words re-
maining in wiki articles could be grouped into two
sets: close-class terms like pronouns and preposi-
tions as well as open-class terms like nouns and
verbs. For example, in the sentence ?She died at age
33, along with her husband and sister?, the open-
class terms include die, age, 33, husband and sister.
Even though most open-class terms are defined in
Wikipedia as well, the authors of the article do not
consider it necessary to present their references us-
ing wiki links. Hence, we need to extend wiki con-
cepts by concatenating them with these open-class
terms to form an extended vector. In addition, we
ignore all close-class terms, since we cannot find
efficient method to infer reliable links across them.
As a result, texts are represented as a vector of wiki
concepts.
Once we introduce wiki concepts to replace typ-
ical ?bag-of-words? approach, the dimensions of
concept space will reach six order of magnitudes.
We cannot ignore the data spareness issue and com-
putation cost when the concept space is so huge.
Actually, for a wiki article and a set of relevant arti-
cles, the involved concepts are limited, and we need
to explore them in a small sub-space. For instance,
59 articles about Kennedy family in Wikipedia have
10,399 distinctive wiki concepts only, where 5,157
wiki concepts exist twice and more. Computing the
overlapping among them is feasible.
Furthermore, we need to merge the wiki concepts
with identical or close semantic (namely, building
links between these synonyms and paraphrases).
We measure the semantic similarity between two
concepts by using cosine distance between their
wiki articles, which are represented as the vectors
of wiki concepts as well. For computation effi-
ciency, we calculate semantic similarities between
all promising concept pairs beforehand, and then
retrieve the value in a Hash table directly. We spent
CPU time of about 12.5 days preprocessing the se-
201
mantic calculation. Details are available at our tech-
nical report (Lu et al, 2008).
Following the principle of TFIDF, we define
the weighing metric for the vector represented
by wiki concepts using the entire Wikipedia as
the observation collection. We define the CFIDF
weight of wiki concept i in article j as:
wi,j = cfi,j? idfi = ni,j?
k nk,j
? log |D||dj : ti ? dj| ,
(1)
where cfi,j is the frequency of concept i in arti-
cle j; idfi is the inverse frequency of concept i
in Wikipedia; and D is the number of articles in
Wikipedia. Here, sparse wiki concepts will have
more contribution.
In brief, we represent articles in terms of wiki
concepts using the steps below.
1. Extract the wiki concepts marked by wiki
links in context.
2. Detect the remaining open-class terms as wiki
concepts as well.
3. Merge concepts whose semantic similarity is
larger than predefined threshold (0.35 in our
experiments) into the one with largest idf .
4. Weight all concepts according to Eqn (1).
3.2 Document Concept Lattice Model
Next, we build the document concept lattice (DCL)
for articles represented by wiki concepts. For il-
lustration on how DCL is built, we consider 8 sen-
tences from DUC 2005 Cluster d324e (Ye et al,
2007) as case study. 8 sentences, represented by 16
distinctive concepts A-P, are considered as the base
nodes 1-8 as shown in Figure 1. Once we group
nodes by means of the maximal common concepts
among base nodes hierarchically, we can obtain the
derived nodes 11-41, which form a DCL. A derived
node will annotate a local topic through a set of
shared concepts, and define a sub concept space that
contains the covered base nodes under proper pro-
jection. The derived node, accompanied with its
base nodes, is apt to interpret a particular argument
(or statement) about the involved concepts. Further-
more, one base node among them, coupled with the
corresponding sentence, is capable of this interpre-
tation and could represent the other base nodes to
some degree.
In order to Extract a set of sentences to cover
key distinctive local topics (arguments) as much as
possible, we need to select a set of important non-
overlapping derived nodes. We measure the impor-
tance of node N in DCL of article j in term of rep-
resentative power (RP) as:
RP (N) =
?
ci?N
(|ci|?wi,j)/ log(|N |), (2)
         
	 
     
  
     
	
 
  



      
	
 



 
      
	
 



     
	
 
     
         