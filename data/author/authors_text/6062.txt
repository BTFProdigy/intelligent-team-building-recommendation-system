Rapid Prototyping of Robust Language Understanding Modules
for Spoken Dialogue Systems
?Yuichiro Fukubayashi, ?Kazunori Komatani, ?Mikio Nakano,
?Kotaro Funakoshi, ?Hiroshi Tsujino, ?Tetsuya Ogata, ?Hiroshi G. Okuno
?Graduate School of Informatics, Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto
606-8501, Japan
{fukubaya,komatani}@kuis.kyoto-u.ac.jp
{ogata,okuno}@kuis.kyoto-u.ac.jp
?Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako, Saitama
351-0188, Japan
nakano@jp.honda-ri.com
{funakoshi,tsujino}@jp.honda-ri.com
Abstract
Language understanding (LU) modules for
spoken dialogue systems in the early phases
of their development need to be (i) easy
to construct and (ii) robust against vari-
ous expressions. Conventional methods of
LU are not suitable for new domains, be-
cause they take a great deal of effort to
make rules or transcribe and annotate a suf-
ficient corpus for training. In our method,
the weightings of the Weighted Finite State
Transducer (WFST) are designed on two
levels and simpler than those for conven-
tional WFST-based methods. Therefore,
our method needs much fewer training data,
which enables rapid prototyping of LU mod-
ules. We evaluated our method in two dif-
ferent domains. The results revealed that our
method outperformed baseline methods with
less than one hundred utterances as training
data, which can be reasonably prepared for
new domains. This shows that our method
is appropriate for rapid prototyping of LU
modules.
1 Introduction
The language understanding (LU) of spoken dia-
logue systems in the early phases of their devel-
opment should be trained with a small amount of
data in their construction. This is because large
amounts of annotated data are not available in the
early phases. It takes a great deal of effort and time
to transcribe and provide correct LU results to a
Figure 1: Relationship between our method and con-
ventional methods
large amount of data. The LU should also be robust,
i.e., it should be accurate even if some automatic
speech recognition (ASR) errors are contained in its
input. A robust LU module is also helpful when col-
lecting dialogue data for the system because it sup-
presses incorrect LU and unwanted behaviors. We
developed a method of rapidly prototyping LU mod-
ules that is easy to construct and robust against var-
ious expressions. It makes LU modules in the early
phases easier to develop.
Several methods of implementing an LU mod-
ule in spoken dialogue systems have been proposed.
Using grammar-based ASR is one of the simplest.
Although its ASR output can easily be transformed
into concepts based on grammar rules, complicated
grammars are required to understand the user?s ut-
terances in various expressions. It takes a great deal
of effort to the system developer. Extracting con-
210
Figure 2: Example of WFST for LU
cepts from user utterances by keyword spotting or
heuristic rules has also been proposed (Seneff, 1992)
where utterances can be transformed into concepts
without major modifications to the rules. However,
numerous complicated rules similarly need to be
manually prepared. Unfortunately, neither method
is robust against ASR errors.
To cope with these problems, corpus-based (Su-
doh and Tsukada, 2005; He and Young, 2005) and
Weighted Finite State Transducer (WFST)-based
methods (Potamianos and Kuo, 2000; Wutiwi-
watchai and Furui, 2004) have been proposed as LU
modules for spoken dialogue systems. Since these
methods extract concepts using stochastic analy-
sis, they do not need numerous complicated rules.
These, however, require a great deal of training data
to implement the module and are not suitable for
constructing new domains.
Here, we present a new WFST-based LU module
that has two main features.
1. A statistical language model (SLM) for ASR
and a WFST for parsing that are automatically
generated from the domain grammar descrip-
tion.
2. Since the weighting for the WFST is simpler
than that in conventional methods, it requires
fewer training data than conventional weight-
ing schemes.
Our method accomplishes robust LU with less ef-
fort using SLM-based ASR and WFST parsing. Fig-
ure 1 outlines the relationships between our method
and conventional schemes. Since rule- or grammar-
based approaches do not require a large amount of
data, they take less effort than stochastic techniques.
However, they are not robust against ASR errors.
Stochastic approaches, on the contrary, take a great
deal of effort to collect data but are robust against
ASR errors. Our method is an intermediate approach
that lies between these. That is, it is more robust than
rule- or grammar-based approaches and takes less
effort than stochastic techniques. This characteristic
makes it easier to rapidly prototype LU modules for
a new domain and helps development in the early
phases.
2 Related Work and WFST-based
Approach
A Finite State Transducer (FST)-based LU is ex-
plained here, which accepts ASR output as its in-
put. Figure 2 shows an example of the FST for a
video recording reservation domain. The input, ?,
means that a transition with no input is permitted at
the state transition. In this example, the LU mod-
ule returns the concept [month=2, day=22] for the
utterance ?It is February twenty second please?.
Here, a FILLER transition in which any word is ac-
cepted is appropriately allowed between phrases. In
Figure 2, ?F? represents 0 or more FILLER tran-
sitions. A FILLER transition from the start to the
end is inserted to reject unreliable utterances. This
FILLER transition enables us to ignore unnecessary
words listed in the example utterances in Table 1.
The FILLER transition helps to suppress the inser-
tion of incorrect concepts into LU results.
However, many output sequences are obtained for
one utterance due to the FILLER transitions, be-
cause the utterance can be parsed with several paths.
We used a WFST to select the most appropriate
path from several output sequences. The path with
the highest cumulative weight, w, is selected in a
211
Table 2: Many LU results for input ?It is February twenty second please?
LU output LU result w
It is February twenty second please month=2, day=22 2.0
It is FILLER twenty second please day=22 1.0
It is FILLER twenty second FILLER day=22 1.0
FILLER FILLER FILLER FILLER FILLER FILLER n/a 0
Table 1: Examples of utterances with FILLERs
ASR output
Well, it is February twenty second please
It is uhm, February twenty second please
It is February, twe-, twenty second please
It is February twenty second please, OK?
(LU result = [month=2, day=22])
WFST-based LU. In the example in Table 2, the
concept [month=2, day=22] has been selected, be-
cause its cumulative weight, w, is 2.0, which is the
highest.
The weightings of conventional WFST-based ap-
proaches used an n-gram of concepts (Potamianos
and Kuo, 2000) and that of word-concept pairs (Wu-
tiwiwatchai and Furui, 2004). They obtained the
n-grams from several thousands of annotated ut-
terances. However, it takes a great deal of ef-
fort to transcribe and annotate a large corpus. Our
method enables prototype LU modules to be rapidly
constructed that are robust against various expres-
sions with SLM-based ASR and WFST-based pars-
ing. The SLM and WFST are generated automat-
ically from a domain grammar description in our
toolkit. We need fewer data to train WFST, because
its weightings are simpler than those in conventional
methods. Therefore, it is easy to develop an LU
module for a new domain with our method.
3 Domain Grammar Description
A developer defines grammars, slots, and concepts
in a domain in an XML file. This description en-
ables an SLM for ASR and parsing WFST to be au-
tomatically generated. Therefore, a developer can
construct an LU module rapidly with our method.
Figure 3 shows an example of a descrip-
tion. A definition of a slot is described in
keyphrase-class tags and its keyphrases and
...
<keyphrase-class name="month">
...
<keyphrase>
<orth>February</orth>
<sem>2</sem>
</keyphrase>
...
</keyphrase-class>
...
<action type="specify-attribute">
<sentence> {It is} [*month] *day [please]
</sentence>
</action>
Figure 3: Example of a grammar description
the values are in keyphrase tags. The month is
defined as a slot in this figure. February and 2 are
defined as one of the phrases and values for the slot
month. A grammar is described in a sequence of
terminal and non-terminal symbols. A non-terminal
symbol represents a class of keyphrases, which is
defined in keyphrase-class. It begins with an
asterisk ?*? in a grammar description in sentence
tags. Symbols that can be skipped are enclosed
by brackets []. The FILLER transition described
in Section 2 is inserted between the symbols un-
less they are enclosed in brackets [] or braces {}.
Braces are used to avoid FILLER transitions from
being inserted. For example, the grammar in Figure
3 accepts ?It is February twenty second please.? and
?It is twenty second, OK??, but rejects ?It is Febru-
ary.? and ?It, uhm, is February twenty second.?.
A WFST for parsing can be automatically gener-
ated from this XML file. The WFST in Figure 2 is
generated from the definition in Figure 3. Moreover,
we can generate example sentences from the gram-
mar description. The SLM for the speech recognizer
is generated with our method by using many exam-
ple sentences generated from the defined grammar.
212
4 Weighting for ASR Outputs on Two
Levels
We define weights on two levels for a WFST. The
first is a weighting for ASR outputs, which is set to
select paths that are reliable at a surface word level.
The second is a weighting for concepts, which is
used to select paths that are reliable on a concept
level. The weighting for concepts reflects correct-
ness at a more abstract level than the surface word
level. The weighting for ASR outputs consists of
two categories: a weighting for ASR N-best outputs
and one for accepted words. We will describe the
definitions of these weightings in the following sub-
sections.
4.1 Weighting for ASR N-Best Outputs
The N-best outputs of ASR are used for an input of
a WFST. Weights are assigned to each sentence in
ASR N-best outputs. Larger weights are given to
more reliable sentences, whose ranks in ASR N-best
are higher. We define this preference as
wis =
e??scorei
?N
j e??scorej
,
where wis is a weight for the i-th sentence in ASRN-best outputs, ? is a coefficient for smoothing, and
scorei is the log-scaled score of the i-th ASR out-put. This weighting reflects the reliability of the
ASR output. We set ? to 0.025 in this study after
a preliminary experiment.
4.2 Weighting for Accepted Words
Weights are assigned to word sequences that have
been accepted by the WFST. Larger weights are
given to more reliable sequences of ASR outputs at
the surface word level. Generally, longer sequences
having more words that are not fillers and more re-
liable ASR outputs are preferred. We define these
preferences as the weights:
1. word(const.): ww = 1.0,
2. word(#phone): ww = l(W ), and
3. word(CM): ww = CM(W ) ? ?w.
The word(const.) gives a constant weight to
all accepted words. This means that sequences
with more words are simply preferred. The
word(#phone) takes the length of each accepted
word into consideration. This length is measured by
its number of phonemes, which are normalized by
that of the longest word in the vocabulary. The nor-
malized values are denoted as l(W ) (0 < l(W ) ?
1). By adopting word(#phone), the length of se-
quences is represented more accurately. We also
take the reliability of the accepted words into ac-
count as word(CM). This uses confidence measures
(Lee et al, 2004) for a word, W , in ASR outputs,
which are denoted as CM(W ). The ?w is the thresh-old for determining whether word W is accepted or
not. The ww obtains a negative value for an unreli-able word W when CM(W ) is lower than ?w. Thisrepresents a preference for longer and more reliable
sequences.
4.3 Weighting for Concepts
In addition to the ASR level, weights on a concept
level are also assigned. The concepts are obtained
from the parsing results by the WFST, and contain
several words. Weights for concepts are defined by
using the measures of all words contained in a con-
cept.
We prepared three kinds of weights for the con-
cepts:
1. cpt(const.): wc = 1.0,
2. cpt(avg):
wc =
?
W (CM(W ) ? ?c)
#W , and
3. cpt(#pCM(avg)):
wc =
?
W (CM(W ) ? l(W ) ? ?c)
#W ,
where W is a set of accepted words, W , in the corre-
sponding concept, and #W is the number of words
in W .
The cpt(const.) represents a preference for
sequences with more concepts. The cpt(avg)
is defined as the weight by using the CM(W )
of each word contained in the concept. The
cpt(#pCM(avg)) represents a preference for longer
and reliable sequences with more concepts. The ?cis the threshold for the acceptance of a concept.
213
Table 3: Examples of weightings when parameter set is: word(CM) and cpt(#pCM(avg))
ASR onput No, it is February twenty second
LU output FILLER it is February twenty second
CM(W ) 0.3 0.7 0.6 0.9 1.0 0.9
l(W ) 0.3 0.2 0.2 0.9 0.6 0.5
Concept - - - month=2 day=22
word - 0.7 ? ?w 0.6 ? ?w 0.9 ? ?w 1.0 ? ?w 0.9 ? ?wcpt - - - (0.9 ? 0.9 ? ?c)/1 (1.0 ? 0.6 ? ?c + 0.9 ? 0.5 ? ?c)/2
'
&
$
%
Reference From June third please
ASR output From June third uhm FIT please LU result
CM(W ) 0.771 0.978 0.757 0.152 0.525 0.741
LU reference From June third FILLER FILLER FILLER month:6, day:3
Our method From June third FILLER FILLER FILLER month:6, day:3
Keyword spotting From June third FILLER FIT please month:6, day:3, car:FIT
(?FIT? is the name of a car.)
Figure 4: Example of LU with WFST
4.4 Calculating Cumulative Weight and
Training
The LU results are selected based on the weighted
sum of the three weights in Subsection 4.3 as
wi = wis + ?w
?
ww + ?c
?
wc
The LU module selects an output sequence with
the highest cumulative weight, wi, for 1 ? i ? N .
Let us explain how to calculate cumulative weight
wi by using the example specified in Table 3. Here,
word(CM) and cpt(#pCM(avg)) are selected as pa-
rameters. The sum of weights in this table for ac-
cepted words is ?w(4.1 ? 5?w), when the input se-quence is ?No, it is February twenty second.?.
The sum of weights for concepts is ?c(1.335 ? 2?c)because the weight for ?month=2? is ?c(0.81 ? ?c)and the weight for ?day=22? is ?c(0.525 ? ?c).Therefore, cumulative weight wi for this input se-
quence is wis + ?w(4.1 ? 5?w) + ?c(1.335 ? 2?c).In the training phase, various combinations of pa-
rameters are tested, i.e., which weightings are used
for each of ASR output and concept level, such as
N = 1 or 10, coefficient ?w,c = 1.0 or 0, and thresh-old ?w,c = 0 to 0.9 at intervals of 0.1, on the train-ing data. The coefficient ?w,c = 0 means that acorresponding weight is not added. The optimal pa-
rameter settings are obtained after testing the various
combinations of parameters. They make the concept
error rate (CER) minimum for a training data set.
We calculated the CER in the following equation:
CER = (S +D + I)/N , where N is the number of
concepts in a reference, and S, D, and I correspond
to the number of substitution, deletion, and insertion
errors.
Figure 4 shows an example of LU with our
method, where it rejects misrecognized concept
[car:FIT], which cannot be rejected by keyword
spotting.
5 Experiments and Evaluation
5.1 Experimental Conditions
We discussed our experimental investigation into the
effects of weightings in Section 4. The user utter-
ance in our experiment was first recognized by ASR.
Then, the i-th sentence of ASR output was input to
WFST for 1 ? i ? N , and the LU result for the
highest cumulative weight, wi, was obtained.
We used 4186 utterances in the video recording
reservation domain (video domain), which consisted
of eight different dialogues with a total of 25 differ-
ent speakers. We also used 3364 utterances in the
rent-a-car reservation domain (rent-a-car domain) of
214
eight different dialogues with 23 different speakers.
We used Julius 1 as a speech recognizer with an
SLM. The language model was prepared by using
example sentences generated from the grammars of
both domains. We used 10000 example sentences in
the video and 40000 in the rent-a-car domain. The
number of the generated sentences was determined
empirically. The vocabulary size was 209 in the
video and 891 in the rent-a-car domain. The average
ASR accuracy was 83.9% in the video and 65.7%
in the rent-a-car domain. The grammar in the video
domain included phrases for dates, times, channels,
commands. That of the rent-a-car domain included
phrases for dates, times, locations, car classes, op-
tions, and commands. The WFST parsing mod-
ule was implemented by using the MIT FST toolkit
(Hetherington, 2004).
5.2 Performance of WFST-based LU
We evaluated our method in the two domains: video
and rent-a-car. We compared the CER on test data,
which was calculated by using the optimal settings
for both domains. We evaluated the results with 4-
fold cross validation. The number of utterances for
training was 3139 (=4186*(3/4)) in the video and
2523 (=3364*(3/4)) in the rent-a-car domain.
The baseline method was simple keyword spot-
ting because we assumed a condition where a large
amount of training data was not available. This
method extracts as many keyphrases as possible
from ASR output without taking speech recogni-
tion errors and grammatical rules into consideration.
Both grammar-based and SLM-based ASR outputs
are used for input in keyword spotting (denoted as
?Grammar & spotting? and ?SLM & spotting? in
Table 4). The grammar for grammar-based ASR
was automatically generated by the domain descrip-
tion file. The accuracy of grammar-based ASR was
66.3% in the video and 43.2% in the rent-a-car do-
main.
Table 4 lists the CERs for both methods. In key-
word spotting with SLM-based ASR, the CERs were
improved by 5.2 points in the video and by 22.2
points in the rent-a-car domain compared with those
with grammar-based ASR. This is because SLM-
based ASR is more robust against fillers and un-
1http://julius.sourceforge.jp/
Table 4: Concept error rates (CERs) in each domain
Domain Grammar &spotting SLM &spotting Ourmethod
Video 22.1 16.9 13.5
Rent-a-car 51.1 28.9 22.0
known words than grammar-based ASR. The CER
was improved by 3.4 and 6.9 points by optimal
weightings for WFST. Table 5 lists the optimal pa-
rameters in both domains. The ?c = 0 in the videodomain means that weights for concepts were not
used. This result shows that optimal parameters de-
pend on the domain for the system, and these need
to be adapted for each domain.
5.3 Performance According to Training Data
We also investigated the relationship between the
size of the training data for our method and the CER.
In this experiment, we calculated the CER in the
test data by increasing the number of utterances for
training. We also evaluated the results by 4-fold
cross validation.
Figures 5 and 6 show that our method outper-
formed the baseline methods by about 80 utterances
in the video domain and about 30 utterances in
the rent-a-car domain. These results mean that our
method can effectively be used to rapidly prototype
LU modules. This is because it can achieve robust
LU with fewer training data compared with conven-
tional WFST-based methods, which need over sev-
eral thousand sentences for training.
6 Conclusion
We developed a method of rapidly prototyping ro-
bust LU modules for spoken language understand-
ing. An SLM for a speech recognizer and a WFST
for parsing were automatically generated from a do-
main grammar description. We defined two kinds
of weightings for the WFST at the word and con-
cept levels. These two kinds of weightings were
calculated by ASR outputs. This made it possi-
ble to create an LU module for a new domain with
less effort because the weighting scheme was rel-
atively simpler than those of conventional methods.
The optimal parameters could be selected with fewer
training data in both domains. Our experiment re-
215
Table 5: Optimal parameters in each domain
Domain N ?w ww ?c wc
Video 1 1.0 word(const.) 0 -
Rent-a-car 10 1.0 word(CM)-0.0 1.0 cpt(#pCM(avg))-0.8
 0
 5
 10
 15
 20
 25
 30
 35
 40
 3000 1000 500 250 100 50 10
C
E
R
#utt. for training
Grammar-based ASR & keyword spotting
SLM-based ASR & keyword spotting
Our method
Figure 5: CER in video domain
 0
 5
 10
 15
 20
 25
 30
 50
 55
 3000 1000 500 250 100 50 10
C
E
R
#utt. for training
Grammar-based ASR & keyword spotting
SLM-based ASR & keyword spotting
Our method
Figure 6: CER in rent-a-car domain
vealed that the CER could be improved compared to
the baseline by training optimal parameters with a
small amount of training data, which could be rea-
sonably prepared for new domains. This means that
our method is appropriate for rapidly prototyping
LU modules. Our method should help developers
of spoken dialogue systems in the early phases of
development. We intend to evaluate our method on
other domains, such as database searches and ques-
tion answering in future work.
Acknowledgments
We are grateful to Dr. Toshihiko Ito and Ms. Yuka
Nagano of Hokkaido University for constructing the
rent-a-car domain system.
References
Yulan He and Steve Young. 2005. Spoken Language
Understanding using the Hidden Vector State Model.Speech Communication, 48(3-4):262?275.
Lee Hetherington. 2004. The MIT finite-state trans-ducer toolkit for speech and language processing. InProc. 6th International Conference on Spoken Lan-guage Processing (INTERSPEECH-2004 ICSLP).
Akinobu Lee, Kiyohiro Shikano, and Tatsuya Kawahara.
2004. Real-time word confidence scoring using lo-cal posterior probabilities on tree trellis search. InProc. 2004 IEEE International Conference on Acous-tics, Speech, and Signal Processing (ICASSP 2004),volume 1, pages 793?796.
Alexandors Potamianos and Hong-Kwang J. Kuo. 2000.
Statistical recursive finite state machine parsingfor speech understanding. In Proc. 6th Interna-tional Conference on Spoken Language Processing(INTERSPEECH-2000 ICSLP), pages 510?513.
Stephanie Seneff. 1992. TINA: A natural language sys-tem for spoken language applications. ComputationalLinguistics, 18(1):61?86.
Katsuhito Sudoh and Hajime Tsukada. 2005. Tightly in-
tegrated spoken language understanding using word-to-concept translation. In Proc. 9th European Con-ference on Speech Communication and Technology(INTERSPEECH-2005 Eurospeech), pages 429?432.
Chai Wutiwiwatchai and Sadaoki Furui. 2004. Hybridstatistical and structural semantic modeling for Thaimulti-stage spoken language understanding. In Proc.HLT-NAACL Workshop on Spoken Language Under-standing for Conversational Systems and Higher LevelLinguistic Information for Speech Processing, pages
2?9.
216
Proceedings of NAACL HLT 2009: Short Papers, pages 133?136,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Speech Understanding Framework
that Uses Multiple Language Models and Multiple Understanding Models
?Masaki Katsumaru, ?Mikio Nakano, ?Kazunori Komatani,
?Kotaro Funakoshi, ?Tetsuya Ogata, ?Hiroshi G. Okuno
?Graduate School of Informatics, Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto
606-8501, Japan
{katumaru, komatani}@kuis.kyoto-u.ac.jp
{ogata, okuno}@kuis.kyoto-u.ac.jp
?Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako, Saitama
351-0188, Japan
{nakano, funakoshi}@jp.honda-ri.com
Abstract
The optimal combination of language model
(LM) and language understanding model
(LUM) varies depending on available training
data and utterances to be handled. Usually, a
lot of effort and time are needed to find the op-
timal combination. Instead, we have designed
and developed a new framework that uses
multiple LMs and LUMs to improve speech
understanding accuracy under various situa-
tions. As one implementation of the frame-
work, we have developed a method for select-
ing the most appropriate speech understand-
ing result from several candidates. We use
two LMs and three LUMs, and thus obtain six
combinations of them. We empirically show
that our method improves speech understand-
ing accuracy. The performance of the oracle
selection suggests further potential improve-
ments in our system.
1 Introduction
The speech understanding component in a spoken
dialogue system consists of an automatic speech
recognition (ASR) component and a language un-
derstanding (LU) component. To develop a speech
understanding component, we need to prepare an
ASR language model (LM) and a language under-
standing model (LUM) for the dialogue domain
of the system. There are many types of LMs
such as finite-state grammars and N-grams, and
many types of LUMs such as finite-state transduc-
ers (FST), weighted finite-state transducers (WFST),
and keyphrase-extractors (extractor). Selecting a
suitable combination of LM and LUM is necessary
for robust speech understanding against various user
utterances.
Conventional studies of speech understanding
have investigated which LM and LUM give the best
performance by using fixed training and test data
such as the Air Travel Information System (ATIS)
corpus. However, in real system development, re-
sources such as training data for statistical models
and efforts to write finite-state grammars vary ac-
cording to the available human resources or budgets.
Domain-dependent training data are particularly dif-
ficult to obtain. Therefore, in conventional system
development, system developers determine the types
of LM and LUM by trial and error. Every LM and
LUM has some advantages and disadvantages, so it
is difficult for a single combination of LM and LUM
to gain high accuracy except in a situation involv-
ing a lot of training data and effort. Therefore, using
multiple speech understanding methods is a more ef-
fective approach.
In this paper, we propose a speech understand-
ing framework called ?Multiple Language models
and Multiple Understanding models (MLMU)?, in
which multiple LMs and LUMs are used, to achieve
better performance under the various development
situations. It selects the best speech understanding
result from the multiple results generated by arbi-
trary combinations of LMs and LUMs.
So far there have been several attempts to im-
prove ASR and speech understanding using mul-
tiple speech recognizers and speech understanding
modules. ROVER (Fiscus, 1997) tried to improve
ASR accuracy by integrating the outputs of multi-
ple ASRs with different acoustic and language mod-
133
LM 1utterance LM 2
LM n 
resultconfidenceintegrationcomponent
LUcomponent speechunderstandingresultsASRcomponent
LUM n LUM 2
LUM 1
LM: Language Model
LUM: Language Understanding Model
Figure 1: Flow of speech understanding in MLMU
els. The work is different from our study in the fol-
lowing two points: it does not deal with speech un-
derstanding, and it assumes that each ASR is well-
developed and achieves high accuracy for a variety
of speech inputs. Eckert et al (1996) used multiple
LMs to deal with both in-grammar utterances and
out-of-grammar utterances, but did not mention lan-
guage understanding. Hahn et al (2008) used mul-
tiple LUMs, but just a single language model.
2 Speech Understanding Framework
MLMU
MLMU is a framework by which system developers
can use multiple speech understanding methods by
preparing multiple LMs and multiple LUMs. Fig-
ure 1 illustrates the flow of speech understanding in
MLMU. System developers list available LMs and
LUMs for each system?s domain, and the system
understands utterances by using these models. The
framework selects one understanding result from
multiple results or calculates a confidence score of
the result by using the generated multiple under-
standing results.
MLMU can improve speech understanding for the
following reason. The performance of each speech
understanding (a combination of LM and LUM)
might not be very high when either training data for
the statistical model or available expertise and ef-
fort for writing grammar are insufficient. In such
cases, some utterances might not be covered by the
system?s finite-state grammar LM, and probability
estimation in the statistical models may not be very
good. Using multiple speech understanding mod-
els is expected to solve this problem because each
model has different specialities. For example, finite-
state grammar LMs and FST-based LUMs achieve
high accuracy in recognizing and understanding in-
grammar utterances, whereas out-of-grammar utter-
ances are covered by N-gram models and LUMs
based on WFST and keyphrase-extractors. There-
fore it is more possible that the understanding results
of MLMU will include the correct result than a case
when a single understanding model is used.
The understanding results of MLMU will be help-
ful in many ways. We used them to achieve better
understanding accuracy by selecting the most reli-
able one. This selection is based on features con-
cerning ASR results and language understanding re-
sults. It is also possible to delay the selection, hold-
ing multiple understanding result candidates that
will be disambiguated as the dialogue proceeds (Bo-
hus, 2004). Furthermore, confidence scores, which
enable an efficient dialogue management (Komatani
and Kawahara, 2000), can be calculated by ranking
these results or by voting on them, by using multi-
ple speech understanding results. The understanding
results can be used in the discourse understanding
module and the dialogue management module. They
can choose one of the understanding results depend-
ing on the dialogue situation.
3 Implementation
3.1 Available Language Models and Language
Understanding Models
We implemented MLMU as a library of RIME-
TK, which is a toolkit for building multi-domain
spoken dialogue systems (Nakano et al, 2008).
With the current implementation, developers can use
the following LMs:
1. A LM based on finite-state grammar (FSG)
2. A domain-dependent statistical N-gram model
(N-gram)
and the following LUMs:
1. Finite-state transducer (FST)
2. Weighted FST (WFST)
3. Keyphrase-extractor (extractor).
System developers can use multiple finite-state-
grammar-based LMs or N-gram-based LMs, and
134
also multiple FSTs and WFSTs. They can specify
the combination for each domain by preparing LMs
and LUMs. They can specify grammar models when
sufficient human labor is available for writing gram-
mar, and specify statistical models when a corpus for
training models is available.
3.2 Selecting Understanding Result based on
ASR and LU Features
We also implemented a mechanism for selecting one
of the understanding results as the best hypothesis.
The mechanism chooses the result with the highest
estimated probability of correctness. Probabilities
are estimated for each understanding result by using
logistic regression, which uses several ASR and LU
features.
We define Pi as the probability that speech under-
standing result i is correct, and we select one result
based on argmax
i
Pi. We denote each speech un-
derstanding result as i (i = 1,. . . ,6). We constructed
a logistic regression model for Pi. The regression
function can be written as:
Pi = 11 + exp(?(ai1Fi1 + . . . + aimFim + bi)) .(1)
The coefficients ai1, . . . , aim, bi were fitted us-
ing training data. The independent variables
Fi1, Fi2, ..., Fim are listed in Table 1. In the table,
n indicates the number of understanding results, that
is, n = 6 in this paper?s experiment. Here, we denote
the features as Fi1, Fi2, ..., Fim.
Features from Fi1 to Fi3 represent characteristics
of ASR results. The acoustic scores were normal-
ized by utterance durations in seconds. These fea-
tures are used for verifying its ASR result. Features
from Fi4 to Fi9 represent characteristics of LU re-
sults. Features from Fi4 to Fi6 are defined on the
basis of the concept-based confidence scores (Ko-
matani and Kawahara, 2000).
4 Preliminary Experiment
We conducted a preliminary experiment to show the
potential of the framework by using the two LMs
and three LUMs noted in Section 3.1.
Table 1: Features from speech understanding result i
Fi1: acoustic score of ASR
Fi2: difference between Fi1 and acoustic score
of ASR for utterance verification
Fi3: utterance duration [sec.]
Fi4: average confidence scores for concepts in i
Fi5: average of Fi4 ( 1n
?n
i Fi4)
Fi6: proportion of Fi4 (Fi4 /?ni Fi5)
Fi7: average # concepts ( 1n
?n
i #concepti)
Fi8: max. # concepts (max (#concepti) )
Fi9: min. # concepts (min (#concepti) )
4.1 Preparing LMs and LUMs
The finite-state grammar rules were written in sen-
tence units manually. A domain-dependent statisti-
cal N-gram model was trained on 10,000 sentences
randomly generated from the grammar. The vocab-
ulary sizes of the grammar LM and the domain-
dependent statistical LM were both 278. We
also used a domain-independent statistical N-gram
model for obtaining acoustic scores for utterance
verification, which was trained onWeb texts (Kawa-
hara et al, 2004). Its vocabulary size was 60,250.
The grammar used in the FST was the same as the
FSG used as one of the LMs, which was manually
written by a system developer. The WFST-based LU
was based on a method to estimate WFST parame-
ters with a small amount of data (Fukubayashi et al,
2008). Its parameters were estimated by using 105
utterances of just one user. The keyphrase extrac-
tor extracts as many concepts as possible from an
ASR result on the basis of a grammar while ignor-
ing words that do not match the grammar.
4.2 Target Data for Evaluation
We used 3,055 utterances in the rent-a-car reserva-
tion domain (Nakano et al, 2007). We used Julius
(ver. 4.0.2) as the speech recognizer and a 3000-
state phonetic tied-mixture (PTM) triphone model
as the acoustic model1. ASR accuracy in mora ac-
curacy when using the FSG and the N-gram model
were 71.9% and 75.5% respectively. We used con-
cept error rates (CERs) to represent the speech un-
derstanding accuracy, which is calculated as fol-
1http://julius.sourceforge.jp/
135
Table 2: CERs [%] for each speech understanding
method
speech understanding method
(LM + LUM) CER
(1) FSG + FST 26.9
(2) FSG + WFST 29.9
(3) FSG + extractor 27.1
(4) N-gram + FST 35.2
(5) N-gram + WFST 25.3
(6) N-gram + extractor 26.0
selection from (1) through (6) (our method) 22.7
oracle selection 13.5
lows:
CER = # error concepts#concepts in utterances . (2)
We manually annotated whether an understanding
result of each utterance was correct or not, and
used them as training data to fit the coefficients
ai1, . . . , aim, bi.
4.3 Evaluation in Concept Error Rates
We fitted the coefficients of regression functions and
selected understanding results with a 10-fold cross
validation. Table 2 lists the CERs based on combi-
nations of single LM and LUM and by our method.
Of all combinations of single LM and LUM, the best
accuracy was obtained with (5) (N-gram + WFST).
Our method improved by 2.6 points over (5). Al-
though we achieved a lower CER, we used a lot
of data to estimate logistic regression coefficients.
Such a large amount of data may not be available in a
real situation. We will conduct more experiments by
changing the amount of training data. Table 2 also
shows the accuracy of the oracle selection, which
selected the best speech understanding result man-
ually. The CER of the oracle selection was 13.5%,
a significant improvement compared to all combina-
tions of a LM and LUM. There is no combination of
a LM and LUM whose understanding results were
not selected at all in the oracle selection and our
method?s selection. These results show that using
multiple LMs and multiple LUMs can potentially
improve speech understanding accuracy.
5 Ongoing work
We will conduct more experiments in other domains
or with other resources to evaluate the effectiveness
of our framework. We plan to investigate the case
in which a smaller amount of the training data is
used to estimate the coefficients of the logistic re-
gressions. Furthermore, finding a way to calculate
confidence scores of speech understanding results is
on our agenda.
References
Dan Bohus. 2004. Error awareness and recovery in
task-oriented spoken dialogue systems. Ph.D. thesis,
Carnegie Mellon University.
Wieland Eckert, Florian Gallwitz, and Heinrich Nie-
mann. 1996. Combining stochastic and linguistic lan-
guage models for recognition of spontaneous speech.
In Proc. ICASSP, pages 423?426.
Jonathan G. Fiscus. 1997. A post-processing system
to yield reduced word error rates: Recognizer Out-
put Voting Error Reduction (ROVER). In Proc. ASRU,
pages 347?354.
Yuichiro Fukubayashi, Kazunori Komatani, Mikio
Nakano, Kotaro Funakoshi, Hiroshi Tsujino, Tetsuya
Ogata, and Hiroshi G. Okuno. 2008. Rapid prototyp-
ing of robust language understanding modules for spo-
ken dialogue systems. In Proc. IJCNLP, pages 210?
216.
Stefan Hahn, Patrick Lehnen, and Hermann Ney. 2008.
System combination for spoken language understand-
ing. In Proc. Interspeech, pages 236?239.
Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-
sunobu Itou, and Kiyohiro Shikano. 2004. Recent
progress of open-source LVCSR Engine Julius and
Japanese model repository. In Proc. ICSLP, pages
3069?3072.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management using
concept-level confidence measures of speech recog-
nizer output. In Proc. COLING, volume 1, pages 467?
473.
Mikio Nakano, Yuka Nagano, Kotaro Funakoshi, Toshi-
hiko Ito, Kenji Araki, Yuji Hasegawa, and Hiroshi Tsu-
jino. 2007. Analysis of user reactions to turn-taking
failures in spoken dialogue systems. In Proc. SIGdial,
pages 120?123.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and
Hiroshi Tsujino. 2008. A framework for building con-
versational agents based on a multi-expert model. In
Proc. SIGdial, pages 88?91.
136
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 9?17,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multi-Domain Spoken Dialogue System
with Extensibility and Robustness against Speech Recognition Errors
Kazunori Komatani Naoyuki Kanda Mikio Nakano?
Kazuhiro Nakadai? Hiroshi Tsujino? Tetsuya Ogata Hiroshi G. Okuno
Kyoto University, Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
{komatani,ogata,okuno}@i.kyoto-u.ac.jp
? Honda Research Institute Japan Co., Ltd., 8-1 Honcho, Wako, Saitama 351-0188, Japan
{nakano,nakadai,tsujino}@jp.honda-ri.com
Abstract
We developed a multi-domain spoken dia-
logue system that can handle user requests
across multiple domains. Such systems
need to satisfy two requirements: extensi-
bility and robustness against speech recog-
nition errors. Extensibility is required to
allow for the modification and addition
of domains independent of other domains.
Robustness against speech recognition er-
rors is required because such errors are
inevitable in speech recognition. How-
ever, the systems should still behave ap-
propriately, even when their inputs are er-
roneous. Our system was constructed on
an extensible architecture and is equipped
with a robust and extensible domain selec-
tion method. Domain selection was based
on three choices: (I) the previous domain,
(II) the domain in which the speech recog-
nition result can be accepted with the high-
est recognition score, and (III) other do-
mains. With the third choice we newly
introduced, our system can prevent dia-
logues from continuously being stuck in
an erroneous domain. Our experimental
results, obtained with 10 subjects, showed
that our method reduced the domain selec-
tion errors by 18.3%, compared to a con-
ventional method.
1 Introduction
Many spoken dialogue systems have been devel-
oped for various domains, including: flight reser-
vations (Levin et al, 2000; Potamianos and Kuo,
2000; San-Segundo et al, 2000), train travel in-
formation (Lamel et al, 1999), and bus informa-
tion (Komatani et al, 2005b; Raux and Eskenazi,
2004). Since these systems only handle a sin-
gle domain, users must be aware of the limita-
tions of these domains, which were defined by
the system developer. To handle various domains
through a single interface, we have developed a
multi-domain spoken dialogue system, which is
composed of several single-domain systems. The
system can handle complicated tasks that contain
requests across several domains.
Multi-domain spoken dialogue systems need to
satisfy the following two requirements: (1) exten-
sibility and (2) robustness against speech recog-
nition errors. Many such systems have been de-
veloped on the basis of a master-slave architec-
ture, which is composed of a single master module
and several domain experts handling each domain.
This architecture has the advantage that each do-
main expert can be independently developed, by
modifying existing experts or adding new experts
into the system. In this architecture, the master
module needs to select a domain expert to which
response generation and dialogue management for
the user?s utterance are committed. Hereafter, we
will refer to this selecting process domain selec-
tion.
The second requirement is robustness against
speech recognition errors, which are inevitable in
systems that use speech recognition. Therefore,
these systems must robustly select domains even
when the input may be incorrect due to speech
recognition errors.
We present an architecture for a multi-domain
spoken dialogue system that incorporates a new
domain selection method that is both extensi-
ble and robust against speech recognition errors.
Since our system is based on extensible architec-
ture similar to that developed by O?Neill (O?Neill
et al, 2004), we can add and modify the domain
9
Speech recognition
Domain selection
Utterance generation
Speech synthesis
User
utterance
System
response
User System
Central module Expert for domain A
method
Language understanding
Dialogue state update
Dialogue management
Dialogue statesvariable
Expert for domain B
method
variable
Expert for domain B
method
variable
Expert for domain B
method
Language understanding
Dialogue state update
Dialogue management
Dialogue statesvariable
Figure 1: Distributed-type architecture for multi-domain spoken dialogue systems
experts easily. In order to maintain robustness,
domain selection takes into consideration vari-
ous features concerning context and situations of
the dialogues. We also designed a new selection
framework that satisfies the extensibility issue by
abstracting the transitions between the current and
next domains. Specifically, our system selects the
next domain based on: (I) the previous domain,
(II) the domain in which the speech recognition
result can be accepted with the highest recognition
score, and (III) other domains. Conventional meth-
ods cannot select the correct domain when neither
the previous domain nor the speech recognition re-
sults for a current utterance are correct. To over-
come this drawback, we defined another choice as
(III) that enables the system to detect an erroneous
situation and thus prevent the dialogue from con-
tinuing to be incorrect. We modeled this frame-
work as a classification problem using machine
learning, and showed it is effective by perform-
ing an experimental evaluation of 2,205 utterances
collected from 10 subjects.
2 Architecture used for Multi-Domain
Spoken Dialogue Systems
In multi-domain spoken dialogue systems, the sys-
tem design is more complicated than in single do-
main systems. When the designed systems are
closely related to each other, a modification in a
certain domain may affect the whole system. This
type of a design makes it difficult to modify ex-
isting domains or to add new domains. Therefore,
a distributed-type architecture has been previously
proposed (Lin et al, 2001), which enables system
developers to design each domain independently.
In this architecture, the system is composed of
two kinds of components: a part that can be de-
signed independently of all other domains, and a
part in which relations among domains should be
considered. By minimizing the latter component,
a system developer can design each domain semi-
independently, which enables domains to be eas-
ily added or modified. Many existing systems are
based on this architecture (Lin et al, 2001; O?Neill
et al, 2004; Pakucs, 2003; Nakano et al, 2005).
Thus, we adopted the distributed-type architec-
ture (Nakano et al, 2005). Our system is roughly
composed of two parts, as shown in Figure 1: sev-
eral experts that control dialogues in each domain,
and a central module that controls each expert.
When a user speaks to the system, the central mod-
ule drives a speech recognizer, and then passes
the result to each domain expert. Each expert,
which controls its own domains, executes a lan-
guage understanding module, updates its dialogue
states based on the speech recognition result, and
returns the information required for domain selec-
tion1. Based on the information obtained from
the experts, the central module selects an appro-
priate domain for giving the response. An expert
then takes charge of the selected domain and deter-
mines the next dialogue act based on its dialogue
state. The central module generates a response
based on the dialogue act obtained from the expert,
and outputs the synthesized speech to the user.
Communications between the central module and
each expert are realized using method-calls in the
central module. Each expert is required to have
several methods, such as utterance understanding
or response selection, to be considered an expert
1Dialogue states in a domain that are not selected during
domain selection are returned to their previous states.
10
in this architecture.
As was previously described, the central mod-
ule is not concerned with processing the speech
recognition results; instead, the central module
leaves this task to each expert. Therefore, it is
important that the central module selects an ex-
pert that is committed to the process of the speech
recognition result. Furthermore, information used
during domain selection should also be domain
independent, because this allows easier domain
modification and addition, which is, after all, the
main advantage of distributed-type architecture.
3 Extensible and Robust Domain
Selection
Domain selection in the central module should
also be performedwithin an extensible framework,
and also should be robust against speech recogni-
tion errors.
In many conventional methods, domain selec-
tion is based on estimating the most likely do-
mains based on the speech recognition results.
Since these methods are heavily dependent on
the performance of the speech recognizers, they
are not robust because the systems will fail when
a speech recognizer fails. To behave robustly
against speech recognition errors, the success of
speech recognition and of domain selection should
be treated separately. Furthermore, in some con-
ventional methods, accurate language models are
required to construct the domain selection parts
before new domains are added to a multi-domain
system. This means that they are not extensible.
When selecting a domain, other studies have
used the information on the domain in which a pre-
vious response was made. Lin et al (2001) gave
preference to the domain selected in the previous
turn by adding a certain score as an award when
comparing the N-best candidates of the speech
recognition for each domain. Lane and Kawa-
hara (2005) also assigned a similar preference in
the classification with Support Vector Machine
(SVM). A system described in (O?Neill et al,
2004) does not change its domain until its sub-task
is completed, which is a constraint similar to keep-
ing dialogue in one domain. Since these methods
assume that the previous domain is most likely the
correct domain, it is expected that these methods
keep a system in the domain despite errors due
to speech recognition problems. Thus, should do-
main selection be erroneous, the damage due to the
Same domain as
previous response
Domain having 
the highest score in
speech recognizer
User utterancePrevious turn Current turn
(I)
(II)
(III)
?????
????
?
???
Other domains
except (I), (II)
Selected
domain
Figure 2: Overview of domain selection
error is compounded, as the system assumes that
the previous domain is always correct. Therefore,
we solve this problem by considering features that
represent the confidence of the previously selected
domain.
We define domain selection as being based on
the following 3-class categorization: (I) the previ-
ous domain, (II) the domain in which the speech
recognition results can be accepted with the high-
est recognition score, which is different from the
previous domain, and (III) other domains. Figure
2 depicts the three choices. This framework in-
cludes the conventional methods as choices (I) and
(II). Furthermore, it considers the possibility that
the current interpretations may be wrong, which
is represented as choice (III). This framework also
has extensibility for adding new domains, since it
treats domain selection not by detecting each do-
main directly, but by defining only a relative re-
lationship between the previous and current do-
mains.
Since our framework separates speech recogni-
tion results and domain selection, it can keep di-
alogues in the correct domain even when speech
recognition results are wrong. This situation is
represented as choice (I). An example is shown
in Figure 3. Here, the user?s first utterance (U1)
is about the restaurant domain. Although the sec-
ond utterance (U2) is also about the restaurant do-
main, an incorrect interpretation for the restaurant
domain is obtained because the utterance contains
an out-of-vocabulary word and is incorrectly rec-
ognized. Although a response for utterance U2
should ideally be in the restaurant domain, the sys-
tem control shifts to the temple sightseeing infor-
mation domain, in which an interpretation is ob-
tained based on the speech recognition result. This
11
? ?
U1: Tell me bars in Kawaramachi area.
(domain: restaurant)
S1: Searching for bars in Kawaramachi area.
30 items found.
U2: I want Tamanohikari (name of liquor).
(domain: restaurant)
Tamanohikari is out-of-vocabulary word, and
misrecognized as Tamba-bashi (name of place).
(domain: temple)
S2 (bad): Searching spots near Tamba-bashi. 10 items
found. (domain: temple)
S2 (good): I do not understand what you said. Do you
have any other preferences? (domain: restaurant)
? ?
Figure 3: Example in which choice (I) is appropri-
ate in spite of speech recognition error
is shown as utterance S2 (bad). In such cases, our
framework is capable of behaving appropriately.
This is shown as S2 (good), which is made by
selecting choice (I). Accepting erroneous recogni-
tion results is more harmful than rejecting correct
ones for the following reasons: 1) a user needs to
solve the misunderstanding as a result of the false
acceptance, and 2) an erroneous utterance affects
the interpretation of the utterances following it.
Furthermore, we define choice (III), which de-
tects the cases where normal dialogue manage-
ment is not suitable, in which case the central
module selects an expert based on either the pre-
vious domain or the domain based on the speech
recognition results. The situation corresponds to
a succession of recognition errors. However, this
problem is more difficult to solve than merely de-
tecting a simple succession of the errors because
the system needs to distinguish between speech
recognition errors and domain selection errors in
order to generate appropriate next utterances. Fig-
ure 4 shows an example of such a situation. Here,
the user?s utterances U1 and U2 are about the tem-
ple domain, but a speech recognition error oc-
curred in U2, and system control shifts to the hotel
domain. The user again says (U3), but this results
in the same recognition error. In this case, a do-
main that should ideally be selected is neither the
domain in the previous turn nor the domain deter-
mined based on the speech recognition results. If
this situation can be detected, the system should be
able to generate an appropriate response, like S3
(good), and prevent inappropriate responses based
? ?
U1: Tell me the address of Horin-ji (temple name).
(domain: temple)
S1: The address of Horin-ji is ...
U2: Then, what is the fee for Horin-ji?
(domain: temple)
misrecognized as ?the fee of Holiday Inn?.
(domain: hotel)
S2: The fee of Holiday Inn is ...
U3: The fee of Horin-ji. (domain: temple)
again misrecognized as ?the fee of Holiday Inn?.
(domain: hotel)
S3 (bad): The fee of Holiday Inn is ...
S3 (good): Are you asking about hotel information?
U4: No.
S4: Would you like to return to the temple information
service?
? ?
Figure 4: Example in which choice (III) should be
selected
on an incorrect domain determination. It is pos-
sible for the system to restart from two utterances
before (U1), after asking a confirmatory question
(S4) about whether to return to it or not. After that,
repetition of similar errors can also be avoided if
the system prohibits transition to the hotel domain.
4 Domain Selection using Dialogue
History
We constructed a classifier that selects the appro-
priate domains using various features, including
dialogue histories. The selected domain candi-
dates are based on: (I) the previous domain, (II)
the domain in which the speech recognition results
can be accepted with the highest recognition score,
or (III) other domains. Here, we describe the fea-
tures present in our domain selection method.
In order to not spoil the system?s extensibility,
an advantage of the distributed-type architecture,
the features used in the domain selection should
not depend on the specific domains. We categorize
the features used into three categories listed below:
? Features representing the confidence with
which the previous domain can be considered
correct (Table 1)
? Features about a user?s speech recognition re-
sult (Table 2)
12
Table 1: Features representing confidence in pre-
vious domain
P1: number of affirmatives after entering the domain
P2: number of negations after entering the domain
P3: whether tasks have been completed in the domain
(whether to enter ?requesting detailed information?
in database search task)
P4: whether the domain appeared before
P5: number of changed slots after entering the domain
P6: number of turns after entering the domain
P7: ratio of changed slots (= P5/P6)
P8: ratio of user?s negative answers (= P2/(P1 + P2))
P9: ratio of user?s negative answers in the domain (=
P2/P6)
P10: states in tasks
Table 2: Features of speech recognition results
R1: best posteriori probability of the N-best candidates
interpreted in the previous domain
R2: best posteriori probability for the speech recogni-
tion result interpreted in the domain, that is the do-
main with the highest score
R3: average of word?s confidence scores for the best
candidate of speech recognition results in the do-
main, that is, the domain with the highest score
R4: difference of acoustic scores between candidates
selected as (I) and (II)
R5: ratio of averages of words? confidence scores be-
tween candidates selected as (I) and (II)
? Features representing the situation after do-
main selection (Table 3)
We can take into account the possibility that a
current estimated domain might be erroneous, by
using features representing the confidence in the
previous domain. Each feature from P1 to P9 is
defined to represent the determination of whether
an estimated domain is reliable or not. Specifi-
cally, if there are many affirmative responses from
a user or many changes of slot values during in-
teractions in the domain, we regard the current do-
main as reliable. Conversely, the domain is not
reliable if there are many negative answers from a
user after entering the domain.
We also adopted the feature P10 to represent
the state of the task, because the likelihood that
a domain is changed depends on the state of the
task. We classified the tasks that we treat into two
categories using the following classifications first
made by Araki et al (1999). For a task catego-
rized as a ?slot-filling type?, we defined the di-
alogue states as one of the following two types:
?not completed?, if not all of the requisite slots
have been filled; and ?completed?, if all of the
Table 3: Features representing situations after do-
main selection
C1: dialogue state after the domain selection after se-
lecting previous domain
C2: whether the interpretation of the user?s utterance is
negative in previous domain
C3: number of changed slots after selecting previous
domain
C4: dialogue state after selecting the domain with the
highest speech recognition score
C5: whether the interpretation of the user?s utterance
is negative in the domain with the highest speech
recognition score
C6: number of changed slots after selecting the domain
with the highest speech recognition score
C7: number of common slots (name of place, here)
changed after selecting the domain with the high-
est speech recognition score
C8: whether the domain with the highest speech recog-
nition score has appeared before
requisite slots have been filled. For a task catego-
rized as a ?database search type?, we defined the
dialogue states as one of the following two types:
?specifying query conditions? and ?requesting de-
tailed information?, which were defined in (Ko-
matani et al, 2005a).
The features which represent the user?s speech
recognition result are listed in Table 2 and corre-
spond to those used in conventional studies. R1
considers the N-best candidates of speech recogni-
tion results that can be interpreted in the previous
domain. R2 and R3 represent information about a
domain with the highest speech recognition score.
R4 and R5 represent the comparisons between the
above-mentioned two groups.
The features that characterize the situations af-
ter domain selection correspond to the information
each expert returns to the central module after un-
derstanding the speech recognition results. These
are listed in Table 3. Features listed from C1 to
C3 represent a situation in which the previous do-
main (choice (I)) is selected. Those listed from
C4 to C8 represent a situation in which a domain
with the highest recognition score (choice (II)) is
selected.
Note that these features listed here have sur-
vived after feature selection. A feature survives
if the performance in the domain classification is
degradedwhen it is removed from a feature set one
by one. We had prepared 32 features for the initial
set.
13
Table 4: Specifications of each domain
Name of Class of # of vocab. # of
domain task in ASR slots
restaurant database search 1,562 10
hotel database search 741 9
temple database search 1,573 4
weather slot filling 87 3
bus slot filling 1,621 3
total - 7,373 -
5 Experimental Evaluation
5.1 Implementation
We implemented a Japanese multi-domain spoken
dialogue system with five domain experts: restau-
rant, hotel, temple, weather, and bus. Specifica-
tions of each expert are listed in Table 4. If there
is any overlapping slot between the vocabularies
of the domains, our architecture can treat it as a
common slot, whose value is shared among the
domains when interacting with the user. In our
system, place names are treated as a common slot.
We adopted Julian as the grammar-based
speech recognizer (Kawahara et al, 2004). The
grammar rules for the speech recognizer can be
automatically generated from those used in the
language understanding modules in each domain.
As a phonetic model, we adopted a 3000-states
PTM triphone model (Kawahara et al, 2004).
5.2 Collecting Dialogue Data
We collected dialogue data using a baseline sys-
tem from 10 subjects. First, the subjects used the
system by following a sample scenario, to get ac-
customed to the timing to speak. They, then, used
the system by following three scenarios, where at
least three domains were mentioned, but neither
an actual temple name nor domain was explicitly
mentioned. One of the scenarios is shown in Fig-
ure 5. Domain selection in the baseline system
was performed on the basis of the baseline method
that will be mentioned in Section 5.4, in which ?
was set to 40 after preliminary experiments.
In the experiments, we obtained 2,205 utter-
ances (221 per subject, 74 per dialogue). The
accuracy of the speech recognition was 63.3%,
which was rather low. This was because the sub-
jects tended to repeat similar utterances even after
misrecognition occurred due to out-of-grammar or
out-of-vocabulary utterances. Another reason was
that the dialogues for subjects with worse speech
recognition results got longer, which resulted in an
increase in the total number of misrecognition.
? ?
Tomorrow or the day after, you are planning a sightsee-
ing tour of Kyoto. Please find a shrine you want to visit
in the Arashiyama area, and determine, after consider-
ing the weather, on which day you will visit the shrine.
Please, ask for a temperature on the day of travel. Also
find out how to go to the shrine, whether you can take a
bus from the Kyoto station to there, when the shrine is
closing, and what the entrance fee is.
? ?
Figure 5: Example of scenarios
5.3 Construction of the Domain Classifier
We used the data containing 2,205 utterances col-
lected using the baseline system, to construct a do-
main classifier. We used C5.0 (Quinlan, 1993) as
a classifier. The features used were described in
Section 4. Reference labels were given by hand
for each utterance based on the domains the sys-
tem had selected and transcriptions of the user?s
utterances, as follows2.
Label (I): When the correct domain for a user?s
utterance is the same as the domain in which
the previous system?s response was made.
Label (II): Except for case (I), when the correct
domain for a user?s utterance is the domain
in which a speech recognition result in the N-
best candidates with the highest score can be
interpreted.
Label (III): Domains other than (I) and (II).
5.4 Evaluation of Domain Selection
We compared the performance of our domain se-
lection with that of the baseline method described
below.
Baseline method: A domain having an interpre-
tation with the highest score in the N-best
candidates of the speech recognition was se-
lected, after adding ? for the acoustic likeli-
hood of the speech recognizer if the domain
was the same as the previous one. We calcu-
lated the accuracies of domain selections for
various ?.
2Although only one of the authors assigned the labels,
they could be easily assigned without ambiguity, since the
labels were automatically defined as previously described.
Thus, the annotator only needs to judge whether a user?s re-
quest was about the same domain as the previous system?s re-
sponse or whether it was about a domain in the speech recog-
nition result.
14
0100
200
300
400
500
600
700
800
900
0 10 20 30 40 50 60
?
#
 
o
f
 
e
r
r
o
r
s
 
i
n
 
d
o
m
a
i
n
 
s
e
l
e
c
t
i
o
n
 total
 domain in previous utt.
 domain with highest score
 other domain
Figure 6: Accuracy of domain selection in the
baseline method
Our method: A domain was selected based on
our method. The performance was calculated
with a 10-fold cross validation, that is, one
tenth of the 2,205 utterances were used as test
data, and the remainder was used as training
data. The process was repeated 10 times, and
the average of the accuracies was computed.
Accuracies for domain selection were calculated
per utterance. When there were several domains
that had the same score after domain selection, one
domain was randomly selected among them as an
output.
Figure 6 shows the number of errors for do-
main selection in the baseline method, categorized
by their reference labels as ? changed. As ? in-
creases, so does the system desire to keep the pre-
vious domain. A condition where ? = 0 cor-
responds to a method in which domains are se-
lected based only on the speech recognition re-
sults, which implies that there are no constraints
on keeping the current domain. As we can see
in Figure 6, the number of errors whose refer-
ence labels are ?a domain in the previous response
(choice (I))? decreases as ? gets larger. This is be-
cause incorrect domain transitions due to speech
recognition errors were suppressed by the con-
straint to keep the domains. Conversely, we can
see an increase in errors whose labels are ?a do-
main with the highest speech recognition score
(choice (II))?. This is because there is too much
incentive for keeping the previous domain. The
smallest number of errors was 634 when ? = 35,
and the error rate of domain selection was 28.8%
(= 634/2205). There were 371 errors whose refer-
ence labels were neither ?a domain in the previous
response? nor ?a domain with the highest speech
recognition score?, which cannot be detected even
when ? is changed based on conventional frame-
works.
We also calculated the classification accuracy of
our method. Table 5 shows the results as a con-
fusion matrix. The left hand figure denotes the
number of outputs in the baseline method, while
the right hand figure denotes the number of out-
puts in our method. Correct outputs are in the
diagonal cells, while the domain selection errors
are in the off diagonal cells. Total accuracy in-
creased by 5.3%, from 71.2% to 76.5%, and the
number of errors in domain selection was reduced
from 634 to 518, so the error reduction rate was
18.3% (= 116/634). There was no output in the
baseline method for ?other domains (III)?, which is
in the third column, because conventional frame-
works have not taken this choice into considera-
tion. Our method was able to detect this kind of
error in 157 of 371 utterances, which allows us
to prevent further errors from continuing. More-
over, accuracies for (I) and (II) did not get worse.
Precision for (I) improved from 0.77 to 0.83, and
the F-measure for (I) also improved from 0.83 to
0.86. Although recall for (II) got worse, its preci-
sion improved from 0.52 to 0.62, and consequently
the F-measure for (II) improved slightly from 0.61
to 0.62. These results show that our method can
detect choice (III), which was newly introduced,
without degrading the existing classification accu-
racies.
The features that follow played an important
role in the decision tree. The features that repre-
sent confidence in the previous domain appeared
in the upper part of the tree, including ?the num-
ber of affirmatives after entering the domain (P1)?,
?the ratio of user?s negative answers in the do-
main (P9)?, ?the number of turns after entering the
domain (P6)?, and ?the number of changed slots
based on the user?s utterances after entering the
domain (P5)?. These were also ?whether a domain
with the highest score has appeared before (C8)?
and ?whether an interpretation of a current user?s
utterance is negative (C2)?.
6 Conclusion
We constructed a multi-domain spoken dialogue
system using an extensible framework. Domain
selection in conventional studies is based on ei-
ther the domain based on the speech recognition
15
Table 5: Confusion matrix in domain selection (baseline / our method)
reference label \ output in previous response (I) with highest score (II) others (III) # total label (recall)
in previous response (I) 1289 / 1291 162 / 85 0 / 75 1451 (0.89 / 0.89)
with highest score (II) 84 / 99 299? / 256? 0 / 28 383 (0.74 / 0.62)
others (III) 293 / 172 78 / 42 0 / 157 371 ( 0 / 0.42)
total 1666 / 1562 539 / 383 0 / 260 2205
(precision) (0.77) / (0.83) (0.52) / (0.62) ( - ) / (0.60) (0.712 / 0.765)
?: These include 17 errors because of random selection when there were several domains having the same highest scores.
results or the previous domain. However, we no-
ticed that these conventional frameworks cannot
cope with situations where neither of these do-
mains is correct. Detection of such situations
can prevent dialogues from staying in the incor-
rect domain, which allows our domain selection
method to be robust against speech recognition er-
rors. Furthermore, our domain selection method
is also extensible. Our method does not select the
domains directly, but, by categorizing them into
three classes, it can cope with an increase or de-
crease in the number of domains. Based on the re-
sults of an experimental evaluation using 10 sub-
jects, our method was able to reduce domain se-
lection errors by 18.3% compared to a baseline
method. This means our system is robust against
speech recognition errors.
There are still some issues that could make
our system more robust, and this is included in
future work. For example, in this study, we
adopted a grammar-based speech recognizer to
construct each domain expert easily. However,
other speech recognition methods could be used,
such as a statistical language model. As well,
multiple speech recognizers employing different
domain-dependent grammars could be run in par-
allel. Thus, we need to investigate how to integrate
these approaches into our framework, without de-
stroying the extensibility.
References
Masahiro Araki, Kazunori Komatani, Taishi Hirata,
and Shuji Doshita. 1999. A dialogue library for
task-oriented spoken dialogue systems. In Proc.
IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, pages 1?7.
Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-
sunobu Itou, and Kiyohiro Shikano. 2004. Re-
cent progress of open-source LVCSR engine Julius
and japanese model repository. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), pages 3069?
3072.
Kazunori Komatani, Naoyuki Kanda, Tetsuya Ogata,
and Hiroshi G. Okuno. 2005a. Contextual
constraints based on dialogue models in database
search task for spoken dialogue systems. In Proc.
European Conf. Speech Commun. & Tech. (EU-
ROSPEECH), pages 877?880, Sep.
Kazunori Komatani, Shinichi Ueno, Tatsuya Kawa-
hara, and Hiroshi G. Okuno. 2005b. User model-
ing in spoken dialogue systems to generate flexible
guidance. User Modeling and User-Adapted Inter-
action, 15(1):169?183.
Lori Lamel, Sophie Rosset, Jean-Luc Gauvain, and
Samir Bennacef. 1999. The LIMSI ARISE sys-
tem for train travel information. In IEEE Int?l Conf.
Acoust., Speech & Signal Processing (ICASSP),
pages 501?504, Phoenix, AZ.
Ian R. Lane and Tatsuya Kawahara. 2005. Utterance
verification incorporating in-domain confidence and
discourse coherence measures. In Proc. European
Conf. Speech Commun. & Tech. (EUROSPEECH),
pages 421?424.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov,
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky,M. Rahim, P. Ruscitti, andM.Walker.
2000. The AT&T-DARPA communicator mixed-
initiative spoken dialogue system. In Proc. Int?l
Conf. Spoken Language Processing (ICSLP).
Bor-shen Lin, Hsin-min Wang, and Lin-shan Lee.
2001. A distributed agent architecture for intelli-
gent multi-domain spoken dialogue systems. IEICE
Trans. on Information and Systems, E84-D(9):1217?
1230, Sept.
Mikio Nakano, Yuji Hasegawa, Kazuhiro Nakadai,
Takahiro Nakamura, Johane Takeuchi, Toyotaka
Torii, Hiroshi Tsujino, Naoyuki Kanda, and Hi-
roshi G. Okuno. 2005. A two-layer model for be-
havior and dialogue planning in conversational ser-
vice robots. In 2005 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems (IROS),
pages 1542?1548.
Ian O?Neill, Philip Hanna, Xingkun Liu, and Michael
McTear. 2004. Cross domain dialogue modelling:
An object-based approach. In Proc. Int?l Conf. Spo-
ken Language Processing (ICSLP).
Botond Pakucs. 2003. Towards dynamic multi-
domain dialogue processing. In Proc. European
16
Conf. Speech Commun. & Tech. (EUROSPEECH),
pages 741?744.
Alexandros Potamianos and Hong-Kwang J. Kuo.
2000. Statistical recursive finite state machine pars-
ing for speech understanding. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), volume 3,
pages 510?513.
J. Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo,
CA. http://www.rulequest.com/see5-info.html.
Antoine Raux and Maxine Eskenazi. 2004. Non-
native users in the let?s go!! spoken dialogue sys-
tem: Dealing with linguistic mismatch. In Proc. of
HLT/NAACL.
Ruben San-Segundo, Bryan Pellom, Wayne Ward, and
Jose M. Pardo. 2000. Confidence measures for di-
alogue management in the CU communicator sys-
tem. In IEEE Int?l Conf. Acoust., Speech & Signal
Processing (ICASSP).
17
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 314?321,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Ranking Help Message Candidates Based on Robust Grammar
Verification Results and Utterance History in Spoken Dialogue Systems
Kazunori Komatani Satoshi Ikeda Yuichiro Fukubayashi
Tetsuya Ogata Hiroshi G. Okuno
Graduate School of Informatics
Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
{komatani,sikeda,fukubaya,ogata,okuno}@kuis.kyoto-u.ac.jp
Abstract
We address an issue of out-of-grammar
(OOG) utterances in spoken dialogue sys-
tems by generating help messages for
novice users. Help generation for OOG
utterances is a challenging problem be-
cause language understanding (LU) re-
sults based on automatic speech recogni-
tion (ASR) results for such utterances are
always erroneous as important words are
often misrecognized or missed from such
utterances. We first develop grammar ver-
ification for OOG utterances on the ba-
sis of a Weighted Finite-State Transducer
(WFST). It robustly identifies a grammar
rule that a user intends to utter, even when
some important words are missed from the
ASR result. We then adopt a ranking algo-
rithm, RankBoost, whose features include
the grammar verification results and the
utterance history representing the user?s
experience.
1 Introduction
Studies on spoken dialogue systems have recently
proceeded from in-laboratory systems to ones de-
ployed to the open public (Raux et al, 2006; Ko-
matani et al, 2007; Nisimura et al, 2005). Ac-
cordingly, opportunities are increasing as general
citizens use the systems. This situation means
that novice users directly access the systems with
no instruction, which is quite different from in-
laboratory experiments where some instructions
can be given. In such cases, users often experi-
ence situations where their utterances are not cor-
rectly recognized. This is because of a gap be-
tween the actual system and a user?s mental model,
that is, a user?s expectation of the system. Ac-
tually, a user?s utterance often cannot be inter-
preted by the system because of the system?s lim-
ited grammar for language understanding (LU).
We call such an unacceptable utterance an ?out-
of-grammar (OOG) utterance.? When users? ut-
terances are OOG, they cannot change their ut-
terances into acceptable ones unless they are in-
formed what expressions are acceptable by the
system.
We aim to manage the problem of OOG utter-
ances by providing help messages showing an ex-
ample of acceptable language expressions when a
user utterance is not acceptable. We prepare help
messages corresponding to each grammar rule the
system has. We therefore assume that appropri-
ate help messages can be provided if a user?s in-
tention, i.e., a grammar rule the user originally
intends to use by his utterance, is correctly esti-
mated.
Issues for generating such help messages in-
clude:
1. Estimating a grammar rule corresponding to
user intention even from OOG utterances,
and
2. Complementing missing information in a sin-
gle utterance.
The first issue focuses on the fact that automatic
speech recognition (ASR) results, used as main in-
put data, are erroneous for OOG utterances. Es-
timating a grammar rule that the user intends to
use becomes accordingly difficult especially when
content words, which correspond to database en-
tries such as place names and their attributes, are
not correctly recognized. That is, any type of ASR
error in any position should be taken into consid-
eration in ASR results of OOG utterances. On the
314
other hand, the second issue focuses on the fact
that an ASR result for an OOG utterance does not
necessarily contain sufficient information to esti-
mate the user intention. This is because of ASR
errors or that users may omit some elements from
their utterances because they are in context.
We develop a grammar verification method
based on Weighted Finite-State Transducer
(WFST) as a solution to the first issue. The
grammar verification method robustly estimates
which a grammar rule is intended to use by a
user?s utterance. The WFST is automatically
generated to represent an ASR result in which any
possibility of error is taken into consideration. We
furthermore adopt a boosting algorithm, Rank-
Boost (Freund et al, 2003), to put help messages
in order of probability to address the second issue.
Because it is difficult even for human annotators
to uniquely determine which help message should
be provided for each case, we adopt an algorithm
that can be used for training on several data
examples that have a certain order of priority.
We also incorporate features representing the
user?s utterance history for preventing message
repetition.
2 Related Work
Various studies have been done on generating help
messages in spoken dialogue systems. Gorrell et
al. (2002) trained a decision tree to classify causes
of errors for OOG utterances. Hockey et al (2003)
also classified OOG utterances into the three cate-
gories of endpointing errors, unknown vocabulary,
and subcategorization mistakes, by comparing two
kinds of ASR results. This was called Targeted
Help and provided a user with immediate feedback
tailored to what the user said. Lee et al (2007) also
addressed error recovery by generating help mes-
sages in an example-based dialog modeling frame-
work. These studies, however, determined what
help messages should be provided mainly on the
basis of literal ASR results. Therefore, help mes-
sages would be degraded by ASR results in which
a lot of information was missing, especially for
OOG utterances. The same help messages would
be repeated when the same ASR results were ob-
tained.
An example dialogue enabled by our method,
especially the part of the method described in Sec-
tion 4, is shown in Figure 1. Here, user utter-
ances are transcriptions, and utterance numbers
U1: Tell me your recommending sites.
Underlined parts are not in-vocabulary and no
valid LU result is obtained. The estimated gram-
mar is [Obtaining info on a site] although the most
appropriate help message is that corresponding to
[Searching tourist sites].
S1: I did not understand. You can say ?Tell me
the address of Kiyomizu Temple? for example,
if getting information on a site.
The help message corresponding to [Obtaining info
on a site] is provided.
U2: Tell me your recommending sites.
The user repeats the same utterance probably be-
cause the help message (S1) was not helpful. The
estimated grammar is [Obtaining info on a site]
again.
S2: I did not understand. You can say ?Search
shrines or museums? for example, if searching
tourist sites.
Another help message corresponding to [Searching
tourist sites] is provided after ranking candidates
by also using the user?s utterance history.
[] denotes grammar rules.
Figure 1: Example dialogue enabled by our
method
start with ?S? and ?U? denote system and user
utterances, respectively. In this example, ASR
results for the user utterances (U1 and U2) do
not contain sufficient information because the ut-
terances are short and contain out-of-vocabulary
words. These two results are similar, and ac-
cordingly, the help message after U2 provided by
methods like Targeted Help (Gorrell et al, 2002;
Hockey et al, 2003) is the same as Utterance S1
because they are only based on ASR results. Our
method can provide different help messages as Ut-
terance S2 after ranking candidates by consider-
ing the utterance history and grammar verification
results. Because the candidates are arranged in
the order of probability, the most appropriate help
message can be provided in fewer attempts.
This ranking method for help message candi-
dates is also useful in multimodal interfaces with
speech input. Help messages are necessary when
ASR is used as its input modality, and such mes-
sages were actually implemented in City Browser
(Gruenstein and Seneff, 2007), for example. This
system lists template-based help messages on the
screen by using ASR results and internal states of
the system. The order of help messages is impor-
tant, especially in portable devices with a small
screen, on which the number of help messages dis-
315
played at one time is limited, as Hartmann and
Schreiber (2008) pointed out. Even in cases where
sufficiently large screens are available, too many
help messages without any order will distract the
user?s attention and thus spoil its usability.
3 Grammar Verification based on WFST
We estimate a user?s intention even from OOG ut-
terances as a grammar rule that the user intends
to use by his utterance. We call this estimation
grammar verification. This process is applied to
ASR outputs based on a statistical language model
(LM) in this paper. We use two transducers: a
finite-state transducer (FST) representing the task
grammar, and weighted FST (WFST) representing
an ASR result and its confidence score. Hereafter,
we denote these two as ?grammar FST? and ?input
WFST? and depict examples in Figure 2.
A strong point of our method is that it takes
all three types of ASR error into consideration.
The input WFST is designed to represent all cases
where any word in an ASR result is an inserted or
substituted error, or any word is deleted. Its weight
is designed to reflect confidence scores of ASR re-
sults. By composing this WFST and the gram-
mar FST, we can obtain all possible sequences
and their accumulated weights when arbitrary se-
quences represented by the input WFST are input
into the grammar FST. The optimal results having
the maximum accumulated weight consist of the
LU result and the grammar rule that is the nearest
to the ASR result. The result can be obtained even
when any element in it is misrecognized or absent
from the ASR result.
An LU result is a set of concepts that consist
of slots and their values corresponding to database
entries the system handles. For example, an LU
result ?month=2, day=22? consists of two con-
cepts, such as the value of slotmonth is 2, and the
value of slot day is 22.
3.1 Design of input WFST and grammar FST
In input WFSTs and grammar FSTs, each arc rep-
resenting state transitions has a label in the form of
?a:b/c? denoting its input symbol, output symbol,
and weight, in this order. Input symbol ? means a
state transition without any input symbol, that is,
an epsilon transition. Output symbol ? means no
output in the state transition. For example, a state
transition ?please:?/1.0? is executed when an in-
put symbol is ?please,? no output symbol is gen-
erated, and 1.0 is added to the accumulated weight.
Weights are omitted in the grammar FST because
no weight is given in it.
An input WFST is automatically constructed
from an ASR result. Sequential state transitions
are assigned to each word in the ASR result, and
each of them is paralleled by filler transitions, as
shown in Figure 2 where the ASR result was ?Ev-
ery Monday please? for example. Filler transitions
such as INS, DEL, and SUB are assigned to each
state for representing every kind of error such as
insertion, deletion, and substitution errors. All in-
put symbols in the input WFST are ?, by which the
WFST represents all possible sequences contain-
ing arbitrary errors. For example, the input WFST
in Figure 2 represents all possible sequences such
as ?Every Monday please,? ?Every Monday F,? ?F
Monday F,? and so on. Here, every word can be
replaced by the symbol F that represents an inser-
tion or substitution error. Moreover, the error sym-
bol DEL can be inserted into its output symbol se-
quence at any position, which corresponds to dele-
tion errors in ASR results. Each weight per state
transition is summed up and then the optimal re-
sult is determined. The weights will be explained
in Section 3.2.
A grammar FST is generated from a task gram-
mar, which is written by a system developer for
each task. It determines whether an input se-
quence conforms to the task grammar. We also
assign filler transitions to each state for handling
each type of error of ASR results considered in
the input WFST. A filler transition, either of INS,
DEL, or SUB, is added to each state in the FST
except for states within keyphrases, which are ex-
plicitly indicated by a system developer. In the
example shown in Figure 2, ?SUB $ Monday
date-repeat=Mon please? is output for an input
sequence ?SUB Monday please?. Here, date-
repeat=Mon denotes an LU result, and $ is a sym-
bol for marking words corresponding to a concept.
3.2 Weights assigned to input WFST
We defined two kinds of weights:
1. Rewards for accepted words (wacc), and
2. Penalties for each kind of error (wsub, wdel,
wins).
An accumulated weight for a single utterance is
defined as the sum of these weights as shown be-
316
Input WFST
Every:
Every
Monday:
Monday
please:
please
Grammar FST
input:output/weight
ASR result: ?Every Monday please?
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !!
  !"#$%& :
 !!
  !"#$"%& :
 !!
  !"#$%&' :
 !"# !"
 !"# !"
 !"# !"
$:?
repeat-date:?
Mon=
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"# !"  !"# !"
 !"# !"
 !"# !"
 !"# !"
Figure 2: Example of input WFST and grammar FST
low.
w =
?
Eaccepted
wacc +
?
Eerror
(wsub + wdel + wins)
Here, Eaccepted denotes a set of accepted words
corresponding to elements of each grammar rule,
and Eerror denotes a set of words that are not ac-
cepted and that have either error symbol. Note that
the weights are not given beforehand but are cal-
culated and given to the input WFST in runtime
according to each ASR result.
A weight for an accepted word easr is defined
by using its confidence score CM(easr) (Lee et
al., 2004) and its word length. A word length in
mora is denoted as l(?), which is normalized by
that of the longest word in the vocabulary.
wacc = CM(easr)l(easr)
This weight wacc gives preference to sequences
containing longer words with higher confidence
scores.
Weights for each type of error have negative val-
ues because they are penalties:
wsub = ?{CM(easr)l(easr) + l(egram)}/2
wdel = ?{l(e) + l(egram)}/2
wins = ?{CM(easr)l(easr) + l(e)}/2
where l(e) is the average word length in the vocab-
ulary and egram is a grammar element i.e., either a
word or a class. A deletion error is a case when a
grammar element does not correspond to any word
in the ASR result. A substitution error is a case
when an element is replaced by another word in
the ASR result. An insertion error is a case when
no grammar element corresponds to the ASR re-
sult. Every weight is defined as an average of a
word length of a grammar element and the corre-
sponding one in the ASR result multiplied by its
confidence score. When correspondences cannot
be defined in insertion and deletion errors, l(e) is
used instead. In the case when egram is a class in
the grammar, the average word length in that class
is used as l(egram).
3.3 Example of calculating the weights
We show how a weight is calculated by using the
example in Figure 3. In this example, the user ut-
terance was ?Tell me a liaison of Koetsu-ji (a tem-
ple name).? The word ?liaison? was not in the sys-
tem vocabulary. The ASR result accordingly con-
tained errors for that part; the result was ?Tell me
all Sakyo-ward Koetsu-ji.?
Weights are calculated for each grammar rule
the system has. This example shows calcula-
tions for two grammar rules: [get info] accept-
ing ?Tell me ?item name? of ?temple name?,? and
[search ward] accepting ?Tell me ?facility name?
of ?ward name?.? Here, [] and ?? denote a gram-
mar rule and a class in grammars. Two alignment
results are also shown for grammar [get info] in
this example. Weights are calculated for any align-
ment as shown here, and the alignment result with
the largest weight is selected. In this example,
weight +0.16 for the grammar [get info] was the
largest.
We consequently obtained the result that gram-
mar rule [get info] had the highest score for this
OOG utterance and its accumulated weight was
317
User utterance: ?Tell me a liaison of Koetsu-ji?. (Underlined parts denote OOG.)
ASR result tell me all Sakyo-ward Koetsu-ji
(ward) (temple)
grammar [get info] tell me ?item name? of ?temple name?
WFST output tell me INS SUB DEL Koetsu-ji
weights +0.09 +0.06 ?0.04 ?0.11 ?0.02 +0.18 +0.16
grammar [get info] tell me ?item name ? of ?temple name?
WFST output tell me SUB SUB Koetsu-ji
weights +0.09 +0.06 ?0.21 ?0.10 +0.18 +0.02
grammar [search ward] tell me ?facility type? in ?ward name?
WFST output tell me INS SUB DEL SUB
weights +0.09 +0.06 ?0.04 ?0.12 ?0.02 ?0.21 ?0.24
Figure 3: Example of calculating weights in our grammar verification
+0.16. The result also indicated each type of er-
ror as a result of the alignment: ?item name? was
substituted by ?Sakyo-ward?, ?of? in the grammar
[get info] was deleted, and ?all? in the ASR result
was inserted.
4 Ranking Help Message Candidates by
Integrating Dialogue Context
We furthermore develop a method to rank help
message candidates per grammar rule by integrat-
ing the grammar verification result and the user?s
utterance history. This complements information
that is often absent from utterances or misrecog-
nized in ASR and prevents that the same help mes-
sages are repeated. An outline of the method is
depicted in Figure 4.
4.1 Features used in Ranking
Features used in our methods are listed in Table
1. These features are calculated for each help
message candidate corresponding to each gram-
mar rule. Features H1 to H5 represent how reli-
able a grammar verification result is. Feature H1 is
a grammar verification score, that is, the resulting
accumulated weight described in Section 3. Fea-
ture H2 is calculated by normalizing H1 by the
total score of all grammar rules. This represents
how reliable the grammar verification result is rel-
atively compared to others. Features H3 to H5
represent how partially the user utterance matches
with the grammar rule.
Features H6 and H7 correspond to a dialogue
context. Feature H6 reflects the case in which
users tend to repeat similar utterances when their
utterances were not understood by the system.
Feature H7 represents whether and how the user
knows about the language expression of the gram-
mar rule. This feature corresponds to the known
degree we previously proposed (Fukubayashi et
Table 1: Features of each instance (help message
candidate)
H1: accumulated weight of GV (GV score)
H2: GV score normalized by the total GV score of other
instances
H3: ratio of # of accepted words in GV result to # of all
words
H4: maximum number of successively accepted words
in GV result
H5: number of accepted slots in GV result
H6: how before the grammar rule was selected as GV
result (in # of utterances)
H7: maximum GV score for the grammar rule until then
H8: whether it belongs to the ?command? class
H9: whether it belongs to the ?query? class
H10: whether it belongs to the ?request-info? class
H11-H17: products of H8 and each of H1 to H7
H18-H24: products of H9 and each of H1 to H7
H25-H31: products of H10 and each of H1 to H7
GV: grammar verification
al., 2006), and prevents a help message the user
already knows from being provided repeatedly.
Features H8 to H10 represent properties of
utterances corresponding to the grammar rules,
which are categorized into three classes such as
?command,? ?query,? and ?request-info.? In the
sightseeing task, the numbers of grammar rules for
the three classes were 8, 4, and 11, respectively.
More specifically, utterances in either ?query? or
?request-info? class tend to appear successively
because they are used when users try and com-
pare several query conditions; on the other hand,
utterances in ?command? class tend to appear in-
dependently of the context. Features H11 to H31
are the products of features H8, H9, and H10 and
each feature from H1 to H7. These were defined to
consider combinations of properties of utterances
represented by H8, H9, and H10 and their reliabil-
ity represented by H1 to H7, because RankBoost
318
Help candidate 
Help candidate
Ranking
(RankBoost)
?
=
T
t
tt
xhxH )()( ?
1
x
LL )()(
111
xfxf
i
LL )()(
1 nin
xfxf
n
x
User
utterance
Context
deft
qi,,,??
Parameters
Training 
data
p
x
q
x
Grammar
verification
Calculating features
Sorted by H(x)
Statistical LM-based
ASR outputs
Figure 4: Outline of our ranking method for help message candidates
does not consider them.
4.2 Ranking Algorithm
We adopt RankBoost (Freund et al, 2003), a
boosting algorithm based on machine learning, to
rank help message candidates. This algorithm can
be used for training on several data examples hav-
ing a certain order of priority. This attribute fits
for the problem in this paper; it is difficult even
for human annotators to determine the unique ap-
propriate help message to be provided. Target in-
stances x of the algorithm are help message can-
didates corresponding to grammar rules in this pa-
per.
RankBoost trains a score function H(x) and ar-
ranges instances x in the order. Here, H(x?) <
H(x??) means x?? is ranked higher than x?. This
score function is defined as a linear combination
of weak rankers giving partial information regard-
ing the order:
H(x) =
T
?
t
?tht(x)
where T , ht(), and ?t denote the number of boost-
ing iterations, a weak ranker, and its associated
weight, respectively. The weak ranker ht is de-
fined by comparing the value of a feature fi of an
instance x with a threshold ?. That is,
ht(x) =
?
?
?
?
?
1 if fi(x) > ?
0 if fi(x) ? ?
qdef if fi(x) = ?
(1)
where qdef ? {0, 1}. Here, fi(x) denotes the
value of the i-th feature of instance x, and ? de-
notes that no value is given in fi(x).
5 Experimental Evaluation
5.1 Target Data
Data were collected by 30 subjects in total by us-
ing a multi-domain spoken dialogue system that
handles five domains such as restaurant, hotel,
sightseeing, bus, and weather (Komatani et al,
2008). The data consisted of 180 dialogues and
11,733 utterances. Data from five subjects were
used to determine the number of boosting iter-
ations and to improve LMs for ASR. We used
utterances in the restaurant, hotel, and sightsee-
ing domains because the remaining two, bus and
weather, did not have many grammar rules. We
then extracted OOG utterances on the basis of the
grammar verification results to evaluate the per-
formance of our method for such utterances. We
regarded an utterance whose accumulated weight
was negative as OOG. As a result, 1,349 OOG ut-
terances by 25 subjects were used for evaluation,
hereafter. These consisted of 363 utterances in the
restaurant domain, 563 in the hotel domain, and
423 in the sightseeing domain. These data were
collected under the following conditions: subjects
were given no instructions on concrete language
expressions the system accepts. System responses
were made only by speech, and no screen for dis-
playing outputs was used. Subjects were given six
scenarios describing tasks to be completed.
We used Julius1 that is a statistical-LM-based
ASR engine. We constructed class 3-gram LMs
for ASR by using 10,000 sentences generated
from the task grammars and the 600 utterances
collected by the five subjects. The vocabulary
sizes for the restaurant, hotel, and sightseeing do-
mains were 3,456, 2,625, and 3,593, and ASR ac-
curacies for them were 45.8%, 57.1%, and 43.5%,
respectively. These ASR accuracies were not very
high because the target utterances were all OOG.
A set of possible thresholds in the weak rankers
described in Section 4.2 consisted of all feature
values that appeared in the training data. The num-
bers of boosting iterations were determined on the
basis of accuracies for the data by the five sub-
1http://julius.sourceforge.jp/
319
!"#
$"#
%"#
&"#
'"#
("#
)"#
*""#
*+,-./ 0+,-./ !+,-./ $+,-./ %+,-./
1+,-./
!
"
"
#
$
%
"
&
!"#$%&'$ ()*+,$-.(/
Figure 5: Accuracy when N candidates were pro-
vided in sightseeing domain (1 ? N ? 5)
jects. The numbers were 400, 100, and 500 for the
restaurant, hotel, and sightseeing domains.
5.2 Evaluation Criterion
We manually gave five help messages correspond-
ing to grammar rules as reference labels per ut-
terance in the order of having a strong relation to
the utterance. The numbers of candidate help mes-
sages were 28, 27, and 23 for the restaurant, hotel
and sightseeing domains, respectively.
We evaluated our ranking method as the accu-
racy where at least one of the reference labels was
contained in its top N candidates. This corre-
sponds to a probability where at least one appro-
priate help message was contained in a list of N
candidates. The accuracy was calculated by 5-fold
cross validation. In the baseline method we set,
help messages were provided only by using the
grammar verification scores.
5.3 Results
Results in the sightseeing domain are plotted in
Figure 5. We can see that our method outper-
formed the baseline in the accuracies for all N
values. All these differences were statistically sig-
nificant (p < 0.05) by the McNemar test. The ac-
curacies were also better in the other two domains
for all N values, and the average differences for
the three domains were 11.7 points for N=1, 9.7
points for N=2, and 6.7 points for N=3. The dif-
ferences were large especially for small N values.
This result indicates that we can successfully re-
duce the number of help messages when providing
several ones for users. The improvements were
derived from the features we incorporated such as
the estimated user knowledge in addition to gram-
mar verification results. The baseline method was
only based on grammar verification results for sin-
gle utterances, which contained insufficient infor-
mation because OOG utterances were often mis-
recognized or misunderstood.
Table 2: Sum of absolute values of weight ? for
each feature
H7 H17 H19 H2 H6
(H7*H8) (H2*H9)
9.58 6.91 6.61 6.02 6.01
We also investigated dominant features by cal-
culating the sum of absolute values of final weight
? for each feature in RankBoost. Five dominant
features based on the sums are shown in Table
2. These five features include a feature obtained
from grammar verification result (H2), a feature
about the user?s utterance history (H6), a feature
representing estimated user knowledge (H7), and
features representing properties of the utterances.
The most dominant feature was H7, which ap-
peared twice in this table. This was because user
utterances were not likely to be OOG utterances
again after the user had already known an expres-
sion corresponding to the grammar rule, which can
be detected when user utterances for it were cor-
rectly accepted, that is, its grammar verification
score was high. The second dominant feature was
H2, which showed that grammar verification re-
sults worked effectively.
6 Conclusion
We addressed an issue of OOG utterances in spo-
ken dialogue systems by generating help mes-
sages. To manage situations when a user utter-
ance could not be accepted, we robustly estimated
a user?s intention as a grammar rule that the user
intends to use. We furthermore integrated various
information as well as the grammar verification
results for complementing missing information in
single utterances, and then ranked help message
candidates corresponding to the grammar rules for
efficiently providing them.
Our future work includes the following. The
evaluation in this paper was taken place only on
the basis of utterances collected beforehand. Pro-
viding help messages itself should be evaluated by
another experiment through dialogues. Further-
more, we assumed that language expressions of
help messages to show an example language ex-
pression were fixed. We also need to investigate
what kind of expression is more helpful to novice
users.
320
References
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and
Yoram Singer. 2003. An efficient boosting algo-
rithm for combining preferences. Journal of Ma-
chine Learning Research, 4:933?969.
Yuichiro Fukubayashi, Kazunori Komatani, Tetsuya
Ogata, and Hiroshi G. Okuno. 2006. Dynamic
help generation by estimating user?s mental model in
spoken dialogue systems. In Proc. Int?l Conf. Spo-
ken Language Processing (INTERSPEECH), pages
1946?1949.
Genevieve Gorrell, Ian Lewin, and Manny Rayner.
2002. Adding intelligent help to mixed-initiative
spoken dialogue systems. In Proc. Int?l Conf. Spo-
ken Language Processing (ICSLP), pages 2065?
2068.
Alexander Gruenstein and Stephanie Seneff. 2007.
Releasing a multimodal dialogue system into the
wild: User support mechanisms. In Proc. 8th SIG-
dial Workshop on Discourse and Dialogue, pages
111?119.
Melanie Hartmann and Daniel Schreiber. 2008. Proac-
tively adapting interfaces to individual users for mo-
bile devices. In Adaptive Hypermedia and Adap-
tive Web-Based Systems, 5th International Confer-
ence (AH 2008), volume 5149 of Lecture Notes in
Computer Science, pages 300?303. Springer.
Beth A. Hockey, Oliver Lemon, Ellen Campana, Laura
Hiatt, Gregory Aist, James Hieronymus, Alexander
Gruenstein, and John Dowding. 2003. Targeted
help for spoken dialogue systems: intelligent feed-
back improves naive users? performance. In Proc.
10th Conf. of the European Chapter of the ACL
(EACL2003), pages 147?154.
Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G.
Okuno. 2007. Analyzing temporal transition of real
user?s behaviors in a spoken dialogue system. In
Proc. INTERSPEECH, pages 142?145.
Kazunori Komatani, Satoshi Ikeda, Tetsuya Ogata,
and Hiroshi G. Okuno. 2008. Managing out-of-
grammar utterances by topic estimation with domain
extensibility in multi-domain spoken dialogue sys-
tems. Speech Communication, 50(10):863?870.
Akinobu Lee, Kiyohiro Shikano, and Tatsuya Kawa-
hara. 2004. Real-time word confidence scoring us-
ing local posterior probabilities on tree trellis search.
In IEEE Int?l Conf. Acoust., Speech & Signal Pro-
cessing (ICASSP), volume 1, pages 793?796.
Cheongjae Lee, Sangkeun Jung, Donghyeon Lee, and
Gary Guenbae Lee. 2007. Example-based error re-
covery strategy for spoken dialog system. In Proc.
of IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU), pages 538?543.
Ryuichi Nisimura, Akinobu Lee, Masashi Yamada, and
Kiyohiro Shikano. 2005. Operating a public spo-
ken guidance system in real environment. In Proc.
European Conf. Speech Commun. & Tech. (EU-
ROSPEECH), pages 845?848.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year of
Let?s Go! experience. In Proc. INTERSPEECH.
321
Coling 2010: Poster Volume, pages 579?587,
Beijing, August 2010
Automatic Allocation of Training Data for Rapid Prototyping
of Speech Understanding based on Multiple Model Combination
Kazunori Komatani? Masaki Katsumaru? Mikio Nakano?
Kotaro Funakoshi? Tetsuya Ogata? Hiroshi G. Okuno?
? Graduate School of Informatics, Kyoto University
{komatani,katumaru,ogata,okuno}@kuis.kyoto-u.ac.jp
? Honda Research Institute Japan Co., Ltd.
{nakano,funakoshi}@jp.honda-ri.com
Abstract
The optimal choice of speech understand-
ing method depends on the amount of
training data available in rapid prototyp-
ing. A statistical method is ultimately
chosen, but it is not clear at which point
in the increase in training data a statisti-
cal method become effective. Our frame-
work combines multiple automatic speech
recognition (ASR) and language under-
standing (LU) modules to provide a set
of speech understanding results and se-
lects the best result among them. The
issue is how to allocate training data to
statistical modules and the selection mod-
ule in order to avoid overfitting in training
and obtain better performance. This paper
presents an automatic training data alloca-
tion method that is based on the change
in the coefficients of the logistic regres-
sion functions used in the selection mod-
ule. Experimental evaluation showed that
our allocation method outperformed base-
line methods that use a single ASR mod-
ule and a single LU module at every point
while training data increase.
1 Introduction
Speech understanding in spoken dialogue systems
is the process of extracting a semantic represen-
tation from a user?s speech. That is, it consists
of automatic speech recognition (ASR) and lan-
guage understanding (LU). Because vocabularies
and language expressions depend on individual
systems, it needs to be constructed for each sys-
tem, and accordingly, training data are required
for each. To collect more real training data, which
will lead to higher performance, it is more desir-
able to use a prototype system than that based on
the Wizard-of-Oz (WoZ) method where real ASR
errors cannot be observed, and to use a more ac-
curate speech understanding module. That is, in
the bootstrapping phase, spoken dialogue systems
need to operate before sufficient real data have
been collected.
We have been addressing the issue of rapid pro-
totyping on the basis of the ?Multiple Language
model for ASR and Multiple language Under-
standing (MLMU)? framework (Katsumaru et al,
2009). In MLMU, the most reliable speech un-
derstanding result is selected from candidates pro-
duced by various combinations of multiple ASR
and LU modules using hand-crafted grammar and
statistical models. A grammar-based method is
still effective at an early stage of system devel-
opment because it does not require training data;
Schapire et al (2005) also incorporated human-
crafted prior knowledge into their boosting al-
gorithm. By combining multiple understanding
modules, complementary results can be obtained
by different kinds of ASR and LU modules.
We propose a novel method to allocate avail-
able training data to statistical modules when the
amount of training data increases. The training
data need to be allocated adaptively because there
are several modules to be trained, and they would
cause overfitting without data allocation. There
are speech understanding modules that have lan-
guage models (LMs) for ASR and LU models
579
(LUMs), and a selection module that selects the
most reliable speech understanding result from
multiple candidates in the MLMU framework.
When the amount of available training data is
small, and an LUM and the selection module are
trained on the same data set, they are trained un-
der a closed-set condition, and thus the training
data for the selection module include too many
correct understanding results. In such cases, the
data need to be divided into subdata sets to avoid
overfitting. On the other hand, when the amount
of available training data is large, so that overfit-
ting does not occur, all available data should be
used to train each statistical module to prepare as
much training data as possible.
We therefore develop a method for switching
data allocation policies. More specifically, two
points are automatically determined at which sta-
tistical modules with more parameters start to be
trained. As a result, better overall performance
is achieved at every point while the amount of
training data increases, compared with all combi-
nations of a single ASR module and a single LU
module.
2 Related Work
It is important to consider the amount of available
training data when designing a speech understand-
ing module. Many statistical LU methods have
been studied, e.g., (Wang and Acero, 2006; Jeong
and Lee, 2006; Raymond and Riccardi, 2007;
Hahn et al, 2008; Dinarelli et al, 2009). They
generally outperform grammar-based LU meth-
ods when a sufficient amount of training data is
available; but sufficient training data are not nec-
essarily available during rapid prototyping. Sev-
eral LU methods were constructed using a small
amount of training data (Fukubayashi et al, 2008;
Dinarelli et al, 2009). Fukubayashi et al (2008)
constructed an LU method based on the weighted
finite state transducer (WFST), in which filler
transitions accepting arbitrary inputs and transi-
tion weights were added to a hand-crafted FST.
This method is placed between a grammar-based
method and a statistical method because a sta-
tistically selected weighting scheme is applied
to a hand-crafted grammar model. Therefore,
the amount of training data can be smaller com-
pared with general statistical LU methods, but this
method does not outperform them when plenty of
training data are available. Dinarelli et al (2009)
used a generative model for which overfitting is
less prone to occur than discriminative models
when the amount of training data is small, but
they did not use a grammar-based model, which is
expected to achieve reasonable performance even
when the amount of training data is very small.
Raymond et al (2007) compared the perfor-
mances of statistical LU methods for various
amounts of training data. They used a statis-
tical finite-state transducer (SFST) as a genera-
tive model and a support vector machine (SVM)
and conditional random fields (CRF) as discrim-
inative models. The generative model was more
effective when the amount of data was small,
and the discriminative models were more effec-
tive when it was large. This shows that the perfor-
mance of an LUmethod depends on the amount of
training data available, and therefore, LU meth-
ods need to be switched automatically. Wang et
al. (2002) developed a two-stage speech under-
standing method by applying statistical methods
first and then grammatical rules. They also ex-
amined the performance of the statistical methods
at their first stage for various amounts of train-
ing data and confirmed that the performance is not
very high when a small amount of data is used.
Schapire et al (2005) showed that accuracy
of call classification in spoken dialogue systems
improved by incorporating hand-crafted prior
knowledge into their boosting algorithm. Their
idea is the same as ours in that they improve the
system?s performance by using hand-crafted hu-
man knowledge while only a small amount of
training data is available. We furthermore solve
the data allocation problem because there are mul-
tiple statistical models to be trained in speech
understanding, while their call classification has
only one statistical model.
3 MLMU Framework
MLMU is the framework for selecting the most
reliable speech understanding result from multi-
ple speech understanding modules (Katsumaru et
al., 2009). In this paper, we furthermore adapt the
selection module to the amount of available train-
580
LU model
#1
Language
model #1
LU
modules
ASR
modules
Result:
1
CM
N
CM
MN
CM
?
i
i
CMmaxarg
N
1
Selection module
LU
results
ASR
results
Utterance
ASR: automatic speech recognition
LU: language understanding
CM: confidence measure
M ?
M ?
Speech understanding
Language
model #2
Language
model #N
LU model
#2
LU model
#M
Logistic
regression #1
Logistic
regression #N
Logistic
regression #
Figure 1: Overview of speech understanding framework MLMU
ing data. More specifically, the allocation policy
of training data is changed and thus appropriate
LMs and LUMs are selected as its result.
An overview of MLMU is shown in Figure 1.
MLMU uses multiple LMs for ASR and multi-
ple LUMs and selects the most reliable speech un-
derstanding result from all combinations of them.
We denote a speech understanding module as SUi
(i = 1, . . . , n). Its result is a semantic representa-
tion consisting of a set of concepts. The concept is
either a semantic slot and its value or an utterance
type. Note that n = N ? M , when N LMs and
M LUMs are used. The confidence measure per
utterance for a result of i-th speech understanding
module SUi is denoted as CMi. The speech un-
derstanding result having the highest confidence
measure is selected as the final result for the ut-
terance. That is, the result is the output of SUm
where m = argmaxi CMi.
The confidence measure is calculated by logis-
tic regression based on the features of each speech
understanding result. A logistic regression func-
tion is constructed for each speech understanding
module SUi:
CMi =
1
1 + e?(ai1Fi1+...+ai7Fi7+bi) . (1)
Parameters ai1, . . . , ai7 and bi are determined by
using training data. In the training phase, teacher
signal 1 is given when a speech understanding re-
sult is completely correct; that is, when no error is
contained in the result. Otherwise, 0 is given. We
use seven features, Fi1, Fi2, . . . , Fi7, as indepen-
dent variables. Each feature value is normalized
Table 1: Features of speech understanding result
obtained from SUi
Fi1: Acoustic score normalized by utterance length
Fi2: Difference between Fi1 and normalized acoustic
scores of verification ASR
Fi3: Average concept CM in understanding result
Fi4: Minimum concept CM in understanding result
Fi5: Number of concepts in understanding result
Fi6: Whether any understanding result is obtained
Fi7: Whether understanding result is yes/no
CM: confidence measure
so as to make its mean zero and its variance one.
The features used are listed in Table 1. Com-
pared with those used in our previous paper (Kat-
sumaru et al, 2009), we deleted ones that were
highly correlated with other features and added
ones regarding content of the speech understand-
ing results. Features Fi1 and Fi2 are obtained
from an ASR result. Another ASR with a gen-
eral large vocabulary LM is executed for verifying
the i-th ASR result. Fi2 is the difference between
its score and Fi1 (Komatani et al, 2007). These
two features represent the reliability of the ASR
result. Fi3 and Fi4 are calculated for each concept
in the LU result on the basis of the posterior prob-
ability of the 10-best ASR candidates (Komatani
and Kawahara, 2000). Fi5 is the number of con-
cepts in the LU result. This feature is effective be-
cause the LU results of lengthy utterances tend to
be erroneous in a grammar-based LU. Fi6 repre-
sents the case when an ASR result is not accepted
by the subsequent LU module. In such cases, no
speech understanding result is obtained, which is
581
U1: It is June ninth.
ASR result:
- grammar ?It is June ninth.?
- N-gram ?It is June noon and?
LU result:
- grammar + FST ?month:6 day:9 type:refer-time?
- N-gram + WFST ?month:6 type:refer-time?
U2: I will borrow it on twentieth.
(Underlined part is out-of-grammar.)
ASR result:
- grammar ?Around two pm on twentieth.?
- N-gram ?Around two at ten on twentieth.?
LU result:
- grammar + FST ?day:20 hour:14 type:refer-time?
- N-gram + WFST ?day:20 type:refer-time?
Combination of LM and LUM is denoted as ?LM+LUM?.
Figure 2: Example of speech understanding re-
sults in MLMU framework
regarded as an error. Fi7 is added because affirma-
tive and negative responses, typically ?Yes? and
?No?, tend to be correctly recognized and under-
stood.
Figure 2 depicts an example when multiple
ASRs based on LMs and multiple LUs are used.
In short, the correct speech understanding result is
obtained from a different combination of LMs and
LUMs.
4 Automatic Allocation of Training Data
Using Change in Coefficients
The training data need to be allocated to the
speech understanding modules (i.e., statistical LM
and statistical LUM) and the selection module. If
more data are allocated to the ASR and LU mod-
ules, the performances of these modules are im-
proved, but the overall performance is degraded
because of the low performance of the selection
module. On the other hand, even if more training
data are allocated to the selection module, the per-
formance of each ASR and LU module remains
low.
4.1 Allocation Policy
We focus on the convergence of the logistic re-
gression functions when the amount of training
data increases. The convergence is defined as
the change in their coefficients, which will appear
later as Equation 2, and determines two points
1. All data are used to
train selection modules
2. Data are allocated to SU
and selection modules
3. Data are
not divided
No No
Yes
Yes
Selection module 
first converges?
No over-fitting
occurs?
Amount of training data increases
SU: speech understanding
Figure 3: Flowchart of data allocation
during the increase in training data, and thus three
phases are defined. The flowchart of data alloca-
tion is depicted in Figure 3. The three phases are
explained below.
In the first phase, the first priority is given to
the selection module. This is because the lo-
gistic regression functions used in the selection
module converge with relatively less training data
than those in the statistical ASR and LU mod-
ules for speech understanding; there are eight pa-
rameters for each logistic regression function as
shown in Equation 1, far fewer than for other sta-
tistical models such as N-gram and CRF. The out-
put from a speech understanding module that em-
ploys grammar-based LM and LUM would be the
most reliable in many cases because its perfor-
mance is better than that of other statistical mod-
ules when a very small amount of training data is
available. As a result, equivalent or better perfor-
mance would be achieved than methods using a
single ASR module and a single LU module.
In the second phase, the training data are also
allocated to the speech understanding modules af-
ter the selection module converges. This aims
to improve the performance of the speech under-
standing modules by allocating as much training
data to them as possible. The amount of train-
ing data is fixed in this phase to the amount al-
located to the selection module determined in the
first phase. The remaining data are used to train
the speech understanding modules.
When the performances of all the speech under-
standing modules stabilize, the allocation phase
proceeds to the third one. After this point, we
hypothesize that overfitting does not occur in this
phase because plenty of training data are avail-
able. All available data are used to train all mod-
582
ules without dividing the data in this phase.
4.2 Determining When to Switch Allocation
Policies
Automatic switching from one phase to the next
requires the determination of two points in the
number of training utterances: when the selec-
tion module first converges (konlysel) and when
the speech understanding modules all become sta-
ble (knodiv). These points are determined by fo-
cusing on the changes in the coefficients of the
logistic regression functions when the number of
utterances used as training data increases. We ob-
serve the sum of the changes in the coefficients of
the functions and then identify the points at which
the changes converge. The points are determined
individually by the following algorithm.
Step 1 Construct two logistic regression func-
tions for speech understanding module SUi
by using k and (k + ?k) utterances out of
kmax utterances, where kmax is the amount
of training data available.
Step 2 Calculate the change in coefficients from
the two logistic regression functions by
?i(k) =
?
j
|aij(k + ?k) ? aij(k)|
+|bi(k + ?k) ? bi(k)|, (2)
where aij(k) and bi(k) denote the param-
eters of the logistic regression functions,
shown in Equation 1, for speech understand-
ing module SUi, when k utterances are used
to train the functions.
Step 3 If ?i(k) becomes smaller than threshold
?, consider that the training of the functions
has converged, and record this k as the point
of convergence. If not, return to Step 1 after
k ? k + ?k.
The ?k is the minimum unit of training data con-
taining various utterances. We set it as the number
of utterances in one dialogue session, whose aver-
age was 17. Threshold ? was set to 8, which corre-
sponds to the number of parameters in the logistic
regression functions. No experiments were con-
ducted to determine if better performance could
be achieved with other choices of ?1.
The first point, konlysel, is determined using the
speech understanding module that uses no training
data. Specifically, we used ?grammar+FST? as
method SUi. Here, ?LM+LUM? denotes a com-
bination of LM for ASR and LUM. If the func-
tion converges at k utterances, we set konlysel to
k and fix the k utterances as training data used by
the selection module. The remaining (kmax ? k)
utterances are allocated to the speech understand-
ing modules, that is, the LMs and LUMs. Note
that if k becomes equal to kmax before ?i con-
verges, all training data are allocated to the selec-
tion module; that is, no data are allocated to the
LMs and LUMs. In this case, no output is ob-
tained from statistical speech understanding mod-
ules, and only outputs from the grammar-based
modules are used.
The second point, knodiv , is determined on the
basis of the speech understanding module that
needs the largest amount of data for training. The
amount of data needed depends on the number of
parameters. Specifically, we used ?N-gram+CRF?
as SUi in Equation 2. If the function converges,
we hypothesize that the performance of all the
speech understanding modules stabilize and thus
overfitting does not occur. We then stop the divi-
sion of training data, and use all available data to
train the statistical modules.
5 Experimental Evaluation
5.1 Target Data and Implementation
We used a data set previously collected through
actual dialogues with a rent-a-car reservation sys-
tem (Nakano et al, 2007) with 39 participants.
Each participant performed 8 dialogue sessions,
and 5900 utterances were collected in total. Out
of these utterances, we used 5240 for which the
automatic voice activity detection (VAD) results
agreed with manual annotation. We divided the
utterances into two sets: 2121 with 16 participants
as training data and 3119 with 23 participants as
the test data.
1We do not think the value is very critical after seeing the
results shown in Figure 4.
583
We constructed another rent-a-car reservation
system to evaluate our allocation method. The
system included two language models (LMs)
and four language understanding models (LUMs).
That is, eight speech understanding results in total
were obtained. The two LMs were a grammar-
based LM (?grammar?, hereafter) and a domain-
specific statistical LM (?N-gram?). The grammar
model was described by hand to be equivalent to
the FST model used in LU. The N-gram model
was a class 3-gram and was trained on a tran-
scription of the available training data. The vo-
cabulary size was 281 for the grammar model and
420 for the N-gram model when all the training
data were used. The ASR accuracies of the gram-
mar and N-gram models were 67.8% and 90.5%
for the training data and 66.3% and 85.0% for the
test data when all the training data were used. We
used Julius (ver. 4.1.2) as the speech recognizer
and a gender-independent phonetic-tied mixture
model as the acoustic model (Kawahara et al,
2004). We also used a domain-independent statis-
tical LM with a vocabulary size of 60250, which
was trained on Web documents (Kawahara et al,
2004), as the verification model.
The four LUMs were a finite-state transducer
(FST) model, a weighted FST (WFST) model,
a keyphrase-extractor (Extractor) model, and a
conditional random fields (CRF) model. In the
FST-based LUM, the FST was constructed by
hand. The WFST-based LUM is based on the
method developed by Fukubayashi et al (2008).
The WFSTs were constructed by using the MIT
FST Toolkit (Hetherington, 2004). The weight-
ing scheme used for the test data was selected by
using training data (Fukubayashi et al, 2008). In
the extractor-based LUM, as many parts as pos-
sible in the ASR result were simply transformed
into concepts. As the CRF-based LUM, we used
open-source software, CRF++2, to construct the
LUM. As its features, we use a word in the ASR
result, its first character, its last character, and the
ASR confidence of the word. Its parameters were
estimated by using training data.
The metric used for speech understanding per-
formance was concept understanding accuracy,
2http://crfpp.sourceforge.net/
Table 2: Absolute degradation in oracle accuracy
when each module was removed
Case (A) (B)
With all modules (%) 86.6 90.1
w/o grammar ASR -12.0 -1.1
w/o N-gram ASR -6.1 -7.7
w/o FST LUM -0.4 0.0
w/o WFST LUM -1.2 -0.5
w/o Extractor LUM -0.1 0.0
w/o CRF LUM -0.6 -3.7
(w/o FST & Extractor LUMs) -1.0 -0.1
(A): 141 utterances with 1 participant
(B): 2121 utterances with 16 participants
defined as
1 ? SUB + INS + DEL
no. of concepts in correct results,
where SUB, INS, and DEL denote the numbers of
substitution, insertion, and deletion errors.
5.2 Effectiveness of Using Multiple LMs and
LUMs
We investigated how much the performance of our
framework degraded when one ASR or LU mod-
ule was removed. We used the oracle accuracies,
i.e., when the most appropriate result was selected
by hand. The result reveals the contribution of
each ASR and LU module to the performance of
the framework. A module is regarded as more im-
portant when the accuracy is degraded more when
it is removed than when another one is removed.
Two cases (A) and (B) were defined: when the
amount of available training data was (A) small
and (B) large. We used 141 utterances with 1 par-
ticipant for case (A) and 2121 utterances with 16
participants for case (B). The results are shown in
Table 2.
When a small amount of training data was
available (case (A)), the accuracy was degraded by
12.0 points when the grammar-based ASRmodule
was removed and 6.1 points when the N-gram-
based ASR module was removed. The accuracy
was thus degraded substantially when either ASR
module was removed. This indicates that the two
ASR modules work complementarily.
584
020
40
60
80
100
120
140
160
180
200
0 100 200 300 400 500
C
h
a
n
g
e
s
 
i
n
 
c
o
e
f
f
i
c
i
e
n
t
s
Number of training utterances available
(a) grammar+FST
0
20
40
60
80
100
120
140
160
180
200
0 100 200 300 400 500
C
h
a
n
g
e
s
 
i
n
 
c
o
e
f
f
i
c
i
e
n
t
s
Number of training utterances available
(b) N-gram+CRF
Figure 4: Change in the sum of coefficients ?i when amount of training data increases (?LM+LUM?
denotes combination of LM and LUM)
On the other hand, when a large amount of
training data was available (case (B)), the ac-
curacy was degraded by 1.1 points when the
grammar-based ASR was removed. This means
that it became less important when there are
plenty of training data because the coverage of the
N-gram-based ASR became wider. In short, espe-
cially when the amount of training data is smaller,
speech understanding modules based on a hand-
crafted grammar are more important because of
the low performance of statistical modules.
Concerning the LUMs, the accuracy was de-
graded when any of the LUM modules was re-
moved when a small amount of training data was
available. When a large amount of training data
was available, the module based on CRF in par-
ticular became more important.
5.3 Results and Evaluation of Automatic
Allocation
Figure 4 shows the change in the sum of the co-
efficients, ?i, with the increase in the amount of
training data. In Figure 4(a), the change was very
large while the amount of training data was small,
and decreased dramatically and converged around
one hundred utterances. By applying ? (=8) to ?i,
we set 111 utterances as the first point, konlysel,
up to which all the training data are allocated to
the selection module, as described in Section 4.1.
Similarly, from the results shown in Figure 4(b),
we set 207 utterances as the second point, knodiv,
from which the training data are not divided.
To evaluate our method for allocating training
55
60
65
70
75
80
85
90
50 100 200 400 800 1600
C
o
n
c
e
p
t
 
u
n
d
e
r
s
t
a
n
d
i
n
g
 
a
c
c
u
r
a
c
y
 
[
%
]
Number of training utterances available
Our method
Na?ve allocation
No division
Figure 5: Results of allocation methods
data, we compared it with two baseline methods:
? No-division method: All data available at
each point were used to train both the speech
understanding modules and the selection
module. That is, the same data set was used
to train them.
? Naive-allocation method: Training data
available at each point were allocated equally
to the speech understanding modules and the
selection module.
As shown in Figure 5, our method had the best
concept understanding accuracy when the amount
of training data was small, that is, up to about
278 utterances. This indicates that our method for
allocating the available training data is effective
when the amount of training data is small.
This result is explained more specifically by us-
585
Table 3: Concept understanding accuracy for 141
utterances
Accuracy (%)
Our method 77.9
Naive allocation 73.5
No division 74.1
ing the case in which 141 utterances were used as
the training data. 111 (= konlysel) were secured to
train the selection module and 30 utterances were
allocated to train the speech understanding mod-
ules. As shown in Table 3, the accuracy with our
method was 3.8 points higher than that with the
no-division baseline method. This was achieved
by avoiding the overfitting of the logistic regres-
sion functions; i.e., the data input to the functions
became similar to the test data due to allocation,
so the concept understanding accuracy for the test
set was improved. The accuracy with our method
was 4.4 points higher than that with the naive al-
location baseline method. This was because the
amount of training data allocated to the selection
module was less than our method, and accordingly
the selection module was not trained sufficiently.
5.4 Comparison with methods using a single
ASR and a single LU
Figure 6 plots concept understanding accuracy
with our method against baseline methods using
a single ASR module and a single LU module for
various amounts of training data. Each module for
comparison was constructed by using all available
training data at each point while training data in-
creased; i.e., the same condition as our method.
The accuracies of only three speech understand-
ing modules are shown in the figure, out of the
eight obtained by combining two LMs for ASR
and four LUMs. These three are the ones with the
highest accuracies while the amount of training
data increased. Our method switched the alloca-
tion phase at 111 and 207 utterances, as described
in Section 5.3.
Our method performed equivalently or better
than all baseline methods even when only a small
amount of training data was available. As a result,
our method outperformed all the baseline methods
55
60
65
70
75
80
85
50 100 200 400 800 1600
C
o
n
c
e
p
t
 
u
n
d
e
r
s
t
a
n
d
i
n
g
 
a
c
c
u
r
a
c
y
 
[
%
]
Number of training utterances available
our method
grammar+FST
N-gram+WFST
N-gram+CRF
Figure 6: Comparison with baseline methods us-
ing single speech understanding
at every point while training data increase.
6 Conclusion
We developed a method to automatically allo-
cate training data to statistical modules so as to
avoid performance degradation caused by overfit-
ting. Experimental evaluation showed that speech
understanding accuracies achieved by our method
were equivalent or better than the baseline meth-
ods based on all combinations of a single ASR
module and a single LU module at every point
while training data increase. This includes a case
when a very small amount of training data is avail-
able. We also showed empirically that the training
data should be allocated while an amount of train-
ing data is not sufficient. Our method allocated
available training data on the basis of our alloca-
tion policy described in Section 4.1, and outper-
formed the two baselines where the training data
were equivalently allocated and not allocated.
When plenty of training data were available,
there was no difference between our method and
the speech understanding method that requires the
most training data, i.e., N-gram+CRF, as shown in
Figure 6. It is possible that our method combin-
ing multiple speech understanding modules would
outperform it as Schapire et al (2005) reported.
In their data, there were some examples that only
a hand-crafted rules can parse. Including such a
task as more complicated language understanding
grammar is required, verification of our method in
other tasks is one of the future works.
586
References
Dinarelli, Marco, Alessandro Moschitti, and Giuseppe
Riccardi. 2009. Re-Ranking Models for Spoken
Language Understanding. In Proc. European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 202?210.
Fukubayashi, Yuichiro, Kazunori Komatani, Mikio
Nakano, Kotaro Funakoshi, Hiroshi Tsujino, Tet-
suya Ogata, and Hiroshi G. Okuno. 2008. Rapid
prototyping of robust language understanding mod-
ules for spoken dialogue systems. In Proc. Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP), pages 210?216.
Hahn, Stefan, Patrick Lehnen, and Hermann Ney.
2008. System Combination for Spoken Language
Understanding. In Proc. Annual Conference of the
International Speech Communication Association
(INTERSPEECH), pages 236?239.
Hetherington, Lee. 2004. The MIT Finite-State Trans-
ducer Toolkit for Speech and Language Processing.
In Proc. Int?l Conf. Spoken Language Processing
(ICSLP), pages 2609?2612.
Jeong, Minwoo and Gary Geunbae Lee. 2006. Ex-
ploiting non-local features for spoken language un-
derstanding. In Proc. COLING/ACL 2006 Main
Conference Poster Sessions, pages 412?419.
Katsumaru, Masaki, Mikio Nakano, Kazunori Ko-
matani, Kotaro Funakoshi, Tetsuya Ogata, and Hi-
roshi G. Okuno. 2009. Improving speech un-
derstanding accuracy with limited training data us-
ing multiple language models and multiple under-
standing models. In Proc. Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pages 2735?2738.
Kawahara, Tatsuya, Akinobu Lee, Kazuya Takeda,
Katsunobu Itou, and Kiyohiro Shikano. 2004. Re-
cent progress of open-source LVCSR engine Julius
and Japanese model repository. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), pages 3069?
3072.
Komatani, Kazunori and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management us-
ing concept-level confidence measures of speech
recognizer output. In Proc. Int?l Conf. Computa-
tional Linguistics (COLING), pages 467?473.
Komatani, Kazunori, Yuichiro Fukubayashi, Tetsuya
Ogata, and Hiroshi G. Okuno. 2007. Introducing
utterance verification in spoken dialogue system to
improve dynamic help generation for novice users.
In Proc. 8th SIGdial Workshop on Discourse and
Dialogue, pages 202?205.
Nakano, Mikio, Yuka Nagano, Kotaro Funakoshi,
Toshihiko Ito, Kenji Araki, Yuji Hasegawa, and Hi-
roshi Tsujino. 2007. Analysis of user reactions to
turn-taking failures in spoken dialogue systems. In
Proc. 8th SIGdial Workshop on Discourse and Dia-
logue, pages 120?123.
Raymond, Christian and Giuseppe Riccardi. 2007.
Generative and Discriminative Algorithms for Spo-
ken Language Understanding. In Proc. Annual
Conference of the International Speech Communi-
cation Association (INTERSPEECH), pages 1605?
1608.
Shapire, Robert E., Marie Rochery, Mazin Rahim, and
Narendra Gupta. 2005. Boosting with prior knowl-
edge for call classification. IEEE Trans. on Speech
and Audio Processing, 13(2):174?181.
Wang, Ye-Yi and Alex Acero. 2006. Discrimina-
tive models for spoken language understanding. In
Proc. Int?l Conf. Spoken Language Processing (IN-
TERSPEECH), pages 2426?2429.
Wang, Ye-Yi, Alex Acero, Ciprian Chelba, Brendan
Frey, and Leon Wong. 2002. Combination of Sta-
tistical and Rule-based Approaches for Spoken Lan-
guage Understanding. In Proc. Int?l Conf. Spoken
Language Processing (ICSLP), pages 609?612.
587
