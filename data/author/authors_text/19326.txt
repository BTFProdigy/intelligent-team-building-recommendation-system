Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 674?679,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Polylingual Topic Models from
Code-Switched Social Media Documents
Nanyun Peng Yiming Wang Mark Dredze
Human Language Technology Center of Excellence
Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD USA
{npeng1,freewym,mdredze}@jhu.edu
Abstract
Code-switched documents are common
in social media, providing evidence for
polylingual topic models to infer aligned
topics across languages. We present
Code-Switched LDA (csLDA), which in-
fers language specific topic distributions
based on code-switched documents to fa-
cilitate multi-lingual corpus analysis. We
experiment on two code-switching cor-
pora (English-Spanish Twitter data and
English-Chinese Weibo data) and show
that csLDA improves perplexity over
LDA, and learns semantically coherent
aligned topics as judged by human anno-
tators.
1 Introduction
Topic models (Blei et al, 2003) have become stan-
dard tools for analyzing document collections, and
topic analyses are quite common for social media
(Paul and Dredze, 2011; Zhao et al, 2011; Hong
and Davison, 2010; Ramage et al, 2010; Eisen-
stein et al, 2010). Their popularity owes in part to
their data driven nature, allowing them to adapt to
new corpora and languages. In social media espe-
cially, there is a large diversity in terms of both the
topic and language, necessitating the modeling of
multiple languages simultaneously. A good candi-
date for multi-lingual topic analyses are polylin-
gual topic models (Mimno et al, 2009), which
learn topics for multiple languages, creating tuples
of language specific distributions over monolin-
gual vocabularies for each topic. Polylingual topic
models enable cross language analysis by group-
ing documents by topic regardless of language.
Training of polylingual topic models requires
parallel or comparable corpora: document tuples
from multiple languages that discuss the same
topic. While additional non-aligned documents
User 1: ?Don Samuel es un crack! #VamosM?exico #DaleTri
RT @User4: Arriba! Viva Mexico! Advanced to GOLD.
medal match in ?Football?!
User 2: @user1 rodo que tal el nuevo Mountain ?
User 3: @User1 @User4 wow this is something !! Ja ja ja
Football well said
Figure 1: Three users discuss Mexico?s football
team advancing to the Gold medal game in the
2012 Olympics in code-switched Spanish and En-
glish.
can be folded in during training, the ?glue? doc-
uments are required to aid in the alignment across
languages. However, the ever changing vocabu-
lary and topics of social media (Eisenstein, 2013)
make finding suitable comparable corpora diffi-
cult. Standard techniques ? such as relying on ma-
chine translation parallel corpora or comparable
documents extracted from Wikipedia in different
languages ? fail to capture the specific terminol-
ogy of social media. Alternate methods that rely
on bilingual lexicons (Jagarlamudi and Daum?e,
2010) similarly fail to adapt to shifting vocabular-
ies. The result: an inability to train polylingual
models on social media.
In this paper, we offer a solution: utilize code-
switched social media to discover correlations
across languages. Social media is filled with ex-
amples of code-switching, where users switch be-
tween two or more languages, both in a conversa-
tion and even a single message (Ling et al, 2013).
This mixture of languages in the same context sug-
gests alignments between words across languages
through the common topics discussed in the con-
text.
We learn from code-switched social media by
extending the polylingual topic model framework
to infer the language of each token and then auto-
matically processing the learned topics to identify
aligned topics. Our model improves both in terms
of perplexity and a human evaluation, and we pro-
vide some example analyses of social media that
rely on our learned topics.
674
2 Code-Switching
Code-switched documents has received consider-
able attention in the NLP community. Several
tasks have focused on identification and analysis,
including mining translations in code-switched
documents (Ling et al, 2013), predicting code-
switched points (Solorio and Liu, 2008a), identi-
fying code-switched tokens (Lignos and Marcus,
2013; Yu et al, 2012; Elfardy and Diab, 2012),
adding code-switched support to language mod-
els (Li and Fung, 2012), linguistic processing of
code switched data (Solorio and Liu, 2008b), cor-
pus creation (Li et al, 2012; Diab and Kamboj,
2011), and computational linguistic analyses and
theories of code-switching (Sankofl, 1998; Joshi,
1982).
Code-switching specifically in social media has
also received some recent attention. Lignos and
Marcus (2013) trained a supervised token level
language identification system for Spanish and
English code-switched social media to study code-
switching behaviors. Ling et al (2013) mined
translation spans for Chinese and English in code-
switched documents to improve a translation sys-
tem, relying on an existing translation model to aid
in the identification and extraction task. In contrast
to this work, we take an unsupervised approach,
relying only on readily available document level
language ID systems to utilize code-switched data.
Additionally, our focus is not on individual mes-
sages, rather we aim to train a model that can be
used to analyze entire corpora.
In this work we consider two types of code-
switched documents: single messages and conver-
sations, and two language pairs: Chinese-English
and Spanish-English. Figure 1 shows an exam-
ple of a code-switched Spanish-English conversa-
tion, in which three users discuss Mexico?s foot-
ball team advancing to the Gold medal game in
the 2012 Summer Olympics. In this conversation,
some tweets are code-switched and some are in a
single language. By collecting the entire conver-
sation into a single document we provide the topic
model with additional content. An example of a
Chinese-English code-switched messages is given
by Ling et al (2013):
watup Kenny Mayne!! - Kenny Mayne
??????!!
Here a user switches between languages in a single
message. We empirically evaluate our model on
both conversations and messages. In the model
presentation we will refer to both as ?documents.?
3 csLDA
To train a polylingual topic model on social me-
dia, we make two modifications to the model of
Mimno et al (2009): add a token specific language
variable, and a process for identifying aligned top-
ics.
First, polylingual topic models require paral-
lel or comparable corpora in which each docu-
ment has an assigned language. In the case of
code-switched social media data, we require a per-
token language variable. However, while docu-
ment level language identification (LID) systems
are common place, very few languages have per-
token LID systems (King and Abney, 2013; Lig-
nos and Marcus, 2013).
To address the lack of available LID systems,
we add a per-token latent language variable to the
polylingual topic model. For documents that are
not code-switched, we observe these variables to
be the output of a document level LID system. In
the case of code-switched documents, these vari-
ables are inferred during model inference.
Second, polylingual topic models assume the
aligned topics are from parallel or comparable cor-
pora, which implicitly assumes that a topics pop-
ularity is balanced across languages. Topics that
show up in one language necessarily show up in
another. However, in the case of social media,
we can make no such assumption. The topics
discussed are influenced by users, time, and lo-
cation, all factors intertwined with choice of lan-
guage. For example, English speakers will more
likely discuss Olympic basketball while Spanish
speakers football. There may be little or no docu-
ments on a given topic in one language, while they
are plentiful in another. In this case, a polylin-
gual topic model, which necessarily infers a topic-
specific word distribution for each topic in each
language, would learn two unrelated word dis-
tributions in two languages for a single topic.
Therefore, naively using the produced topics as
?aligned? across languages is ill-advised.
Our solution is to automatically identify aligned
polylingual topics after learning by examining
a topic?s distribution across code-switched docu-
ments. Our metric relies on distributional proper-
ties of an inferred topic across the entire collec-
tion.
675
To summarize, based on the model of Mimno et
al. (2009) we will learn:
? For each topic, a language specific word distri-
bution.
? For each (code-switched) token, a language.
? For each topic, an identification as to whether
the topic captures an alignment across lan-
guages.
The first two goals are achieved by incorporat-
ing new hidden variables in the traditional polylin-
gual topic model. The third goal requires an auto-
mated post-processing step. We call the resulting
model Code-Switched LDA (csLDA). The gener-
ative process is as follows:
? For each topic z ? T
? For each language l ? L
? Draw word distribution
?
l
z
?Dir(?
l
)
? For each document d ? D:
? Draw a topic distribution ?
d
? Dir(?)
? Draw a language distribution
?
d
?Dir(?)
? For each token i ? d:
? Draw a topic z
i
? ?
d
? Draw a language l
i
? ?
d
? Draw a word w
i
? ?
l
z
For monolingual documents, we fix l
i
to the LID
tag for all tokens. Additionally, we use a single
background distribution for each language to cap-
ture stopwords; a control variable pi, which fol-
lows a Dirichlet distribution with prior parameter-
ized by ?, is introduced to decide the choice be-
tween background words and topic words follow-
ing (Chemudugunta et al, 2006)
1
. We use asym-
metric Dirichlet priors (Wallach et al, 2009), and
let the optimization process learn the hyperparam-
eters. The graphical model is shown in Figure 2.
3.1 Inference
Inference for csLDA follows directly from LDA.
A Gibbs sampler learns the word distributions ?
l
z
for each language and topic. We use a block Gibbs
sampler to jointly sample topic and language vari-
ables for each token. As is customary, we collapse
out ?, ? and ?. The sampling posterior is:
P (z
i
, l
i
|w, z
?i
, l
?i
, ?, ?, ?) ?
(n
l,z
w
i
)
?i
+ ?
n
l,z
?i
+W?
?
m
z,d
?i
+ ?
m
d
?i
+ T ?
?
o
l,d
?i
+ ?
o
d
?i
+ L?
(1)
where (n
l,z
w
i
)
?i
is the number of times the type for
word w
i
assigned to topic z and language l (ex-
1
Omitted from the generative process but shown in Fig. 2.
?
?
l
i
?
d
?
d
?
l
z
?
l
b
?
B?
z
i
b
i
w
i
D
N
L
T
Figure 2: The graphical model for csLDA.
cluding current word w
i
), m
z,d
?i
is the number of
tokens assigned to topic z in document d (exclud-
ing current word w
i
), o
l,d
?i
is the number of tokens
assigned to language l in document d (excluding
current word w
i
), and these variables with super-
scripts or subscripts omitted are totals across all
values for the variable. W is the number of words
in the corpus. All counts omit words assigned
to the background. During sampling, words are
first assigned to the background/topic distribution
and then topic and language are sampled for non-
background words.
We optimize the hyperparameters ?, ?, ? and ?
by interleaving sampling iterations with a Newton-
Raphson update to obtain the MLE estimate for
the hyperparameters. Taking ? as an example, one
step of the Newton-Raphson update is:
?
new
= ?
old
?H
?1
?L
??
(2)
where H is the Hessian matrix and
?L
??
is the gra-
dient of the likelihood function with respect to
the optimizing hyperparameter. We interleave 200
sampling iterations with one Newton-Raphson up-
date.
3.2 Selecting Aligned Topics
We next identify learned topics (a set of related
word-distributions) that truly represent an aligned
topic across languages, as opposed to an unrelated
set of distributions for which there is no support-
ing alignment evidence in the corpus. We begin by
measuring how often each topic occurs in code-
switched documents. If a topic never occurs in
a code-switched document, then there can be no
evidence to support alignment across languages.
For the topics that appear at least once in a code-
switched document, we estimate their probability
676
in the code-switched documents by a MAP esti-
mate of ?. Topics appearing in at least one code-
switched document with probability greater than
a threshold p are selected as candidates for true
cross-language topics.
4 Data
We used two datasets: a Sina Weibo Chinese-
English corpus (Ling et al, 2013) and a Spanish-
English Twitter corpus.
Weibo Ling et al (2013) extracted over 1m
Chinese-English parallel segments from Sina
Weibo, which are code-switched messages. We
randomly sampled 29,705 code-switched mes-
sages along with 42,116 Chinese and 42,116 En-
glish messages from the the same time frame. We
used these data for training. We then sampled
an additional 2475 code-switched messages, 4221
English and 4211 Chinese messages as test data.
Olympics We collected tweets from July 27,
2012 to August 12, 2012, and identified 302,775
tweets about the Olympics based on related hash-
tags and keywords (e.g. olympics, #london2012,
etc.) We identified code-switched tweets using
the Chromium Language Detector
2
. This system
provides the top three possible languages for a
given document with confidence scores; we iden-
tify a tweet as code-switched if two predicted lan-
guages each have confidence greater than 33%.
We then used the tagger of Lignos and Marcus
(2013) to obtain token level LID tags, and only
tweets with tokens in both Spanish and English are
used as code-switched tweets. In total we iden-
tified 822 Spanish-English code-switched tweets.
We further expanded the mined tweets to full con-
versations, yielding 1055 Spanish-English code-
switched documents (including both tweets and
conversations), along with 4007 English and 4421
Spanish tweets composes our data set. We reserve
10% of the data for testing.
5 Experiments
We evaluated csLDA on the two datasets and eval-
uated each model using perplexity on held out data
and human judgements. While our goal is to learn
polylingual topics, we cannot compare to previous
polylingual models since they require comparable
data, which we lack. Instead, we constructed a
baseline from LDA run on the entire dataset (no
2
https://code.google.com/p/chromium-compact-language-detector/
language information.) For each model, we mea-
sured the document completion perplexity (Rosen-
Zvi et al, 2004) on the held out data. We ex-
perimented with different numbers of topics (T ).
Since csLDA duplicates topic distributions (T ?L)
we used twice as many topics for LDA.
Figure 3 shows test perplexity for varying T and
perplexity for the best setting of csLDA (T =60)
and LDA (T =120). The table lists both mono-
lingual and code-switched test data; csLDA im-
proves over LDA in almost every case, and across
all values of T . The background distribution (-bg)
has mixed results for LDA, whereas for csLDA
it shows consistent improvement. Table 4 shows
some csLDA topics. While there are some mis-
takes, overall the topics are coherent and aligned.
We use the available per-token LID system
(Lignos and Marcus, 2013) for Spanish/English
to justify csLDA?s ability to infer the hidden lan-
guage variables. We ran csLDA-bg with l
i
set to
the value provided by the LID system for code-
switched documents (csLDA-bg with LID), which
gives csLDA high quality LID labels. While we
see gains for the code-switched data, overall the
results for csLDA-bg and csLDA-bg with LID are
similar, suggesting that the model can operate ef-
fectively even without a supervised per-token LID
system.
5.1 Human Evaluation
We evaluate topic alignment quality through a hu-
man judgements (Chang et al, 2009). For each
aligned topic, we show an annotator the 20 most
frequent words from the foreign language topic
(Chinese or Spanish) with the 20 most frequent
words from the aligned English topic and two ran-
dom English topics. The annotators are asked to
select the most related English topic among the
three; the one with the most votes is considered
the aligned topic. We count how often the model?s
alignments agree.
LDA may learn comparable topics in different
languages but gives no explicit alignments. We
create alignments by classifying each LDA topic
by language using the KL-divergence between the
topic?s words distribution and a word distribution
for the English/foreign language inferred from the
monolingual documents. Language is assigned to
a topic by taking the minimum KL. For Weibo
data, this was not effective since the vocabularies
of each language are highly unbalanced. Instead,
677
20/40 30/60 40/80 50/100 60/120 70/140
# Topics
8000
8500
9000
9500
10000
P
e
r
p
l
e
x
i
t
y
LDA
LDA-bg
csLDA-bg with LID
csLDA-bg
csLDA
20/40 30/60 40/80 50/100 60/120 70/140
# Topics
18000
20000
22000
24000
26000
28000
30000
P
e
r
p
l
e
x
i
t
y
LDA
LDA-bg
csLDA-bg
csLDA
T =60/120 Olympics Weibo
En Es CS En Cn CS
LDA 11.32 9.44 6.97 29.19 23.06 11.69
LDA-bg 11.35 9.51 6.79 40.87 27.56 10.91
csLDA 8.72 7.94 6.17 18.20 17.31 12.72
csLDA-bg 8.72 7.73 6.04 18.25 17.74 12.46
csLDA-bg 8.73 7.93 4.91 - - -
with LID
Figure 3: Plots show perplexity for different T (Olympics left, Weibo right). Perplexity in the table are
in magnitude of 1? 10
3
.
Football Basketball
English Spanish English Spanish
mexico mucho game espa?na
brazil argentina basketball baloncesto
soccer m?exico year basketball
vs brasil finals bronce
womens ganar?a gonna china
football tri nba final
mens yahel castillo obama rusia
final delpo lebron espa?nola
Social Media Transportation
English Chinese English Chinese
twitter ??? car ??
bitly ?? drive ??
facebook ?? road ??
check ?? line ??
use ?? train ???
blog ?? harry ??
free pm ?? ??
post ?? bus ??
Figure 4: Examples of aligned topics from Olympics (left) and Weibo (right).
we manually labeled the topics by language. We
then pair topics across languages using the cosine
similarity of their co-occurrence statistics in code-
switched documents. Topic pairs with similarity
above t are considered aligned topics. We also
used a threshold p (?3.2) to select aligned topics
in csLDA. To ensure a fair comparison, we select
the same number of aligned topics for LDA and
csLDA.
3
. We used the best performing setting:
csLDA T =60, LDA T =120, which produced 12
alignments from Olympics and 28 from Weibo.
Using Mechanical Turk we collected multiple
judgements per alignment. For Spanish, we re-
moved workers who disagreed with the majority
more than 50% of the time (83 deletions), leav-
ing 6.5 annotations for each alignment (85.47%
inter-annotator agreement.) For Chinese, since
quality of general Chinese turkers is low (Pavlick
et al, 2014) we invited specific workers and
obtained 9.3 annotations per alignment (78.72%
inter-annotator agreement.) For Olympics, LDA
alignments matched the judgements 25% of the
time, while csLDA matched 50% of the time.
While csLDA found 12 alignments and LDA 29,
the 12 topics evaluated from both models show
that csLDA?s alignments are higher quality. For
the Weibo data, LDA matched judgements 71.4%,
while csLDA matched 75%. Both obtained high
3
We used thresholds p = 0.2 and t = 0.0001. We limited
the model with more alignments to match the one with less.
quality alignments ? likely due both to the fact
that the code-switched data is curated to find trans-
lations and we hand labeled topic language ? but
csLDA found many more alignments: 60 as com-
pared to 28. These results confirm our automated
results: csLDA finds higher quality topics that
span both languages.
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research (JMLR), 3:993?1022.
Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L
Boyd-graber, and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Advances in neural information processing systems,
pages 288?296.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2006. Modeling general and specific as-
pects of documents with a probabilistic topic model.
In NIPS.
Mona Diab and Ankit Kamboj. 2011. Feasibility of
leveraging crowd sourcing for the creation of a large
scale annotated resource for Hindi English code
switched data: A pilot annotation. In Proceedings
of the 9th Workshop on Asian Language Resources,
pages 36?40, Chiang Mai, Thailand, November.
Asian Federation of Natural Language Processing.
Jacob Eisenstein, Brendan O?Connor, Noah A Smith,
and Eric P Xing. 2010. A latent variable model
678
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1277?1287. Asso-
ciation for Computational Linguistics.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In NAACL.
Heba Elfardy and Mona Diab. 2012. Token level
identification of linguistic code switching. In Pro-
ceedings of COLING 2012: Posters, pages 287?296,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Liangjie Hong and Brian D Davison. 2010. Empirical
study of topic modeling in twitter. In Proceedings of
the First Workshop on Social Media Analytics, pages
80?88. ACM.
Jagadeesh Jagarlamudi and Hal Daum?e. 2010. Ex-
tracting multilingual topics from unaligned compa-
rable corpora. Advances in Information Retrieval,
pages 444?456.
Aravind K Joshi. 1982. Processing of sentences
with intra-sentential code-switching. In Proceed-
ings of the 9th Conference on Computational lin-
guistics (COLING), pages 145?150.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In NAACL.
Ying Li and Pascale Fung. 2012. Code-switch lan-
guage model with inversion constraints for mixed
language speech recognition. In Proceedings of
COLING 2012, pages 1671?1680, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
mandarin-english code-switching corpus. In Nico-
letta Calzolari, Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 2515?2519, Istanbul, Turkey, May. European
Language Resources Association (ELRA). ACL
Anthology Identifier: L12-1573.
Constantine Lignos and Mitch Marcus. 2013. To-
ward web-scale analysis of codeswitching. In An-
nual Meeting of the Linguistic Society of America.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Proceedings of the 51st Annual Meeting
on Association for Computational Linguistics, ACL
?13. Association for Computational Linguistics.
David Mimno, Hanna M Wallach, Jason Naradowsky,
David A Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2-Volume 2, pages
880?889. Association for Computational Linguis-
tics.
Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing twitter for public health. In
ICWSM.
Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev,
and Chris Callison-Burch. 2014. The language de-
mographics of Amazon Mechanical Turk. Transac-
tions of the Association for Computational Linguis-
tics, 2(Feb):79?92.
Daniel Ramage, Susan T Dumais, and Daniel J
Liebling. 2010. Characterizing microblogs with
topic models. In ICWSM.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model
for authors and documents. In Proceedings of the
20th conference on Uncertainty in artificial intelli-
gence, pages 487?494. AUAI Press.
David Sankofl. 1998. The production of code-mixed
discourse. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics, Volume 1, pages 8?21, Montreal,
Quebec, Canada, August. Association for Computa-
tional Linguistics.
Thamar Solorio and Yang Liu. 2008a. Learning to
predict code-switching points. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 973?981, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Thamar Solorio and Yang Liu. 2008b. Part-of-Speech
tagging for English-Spanish code-switched text. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
1051?1060, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Hanna M Wallach, David M Mimno, and Andrew Mc-
Callum. 2009. Rethinking lda: Why priors matter.
In NIPS, volume 22, pages 1973?1981.
Liang-Chih Yu, Wei-Cheng He, and Wei-Nan Chien.
2012. A language modeling approach to identify-
ing code-switched sentences and words. In Pro-
ceedings of the Second CIPS-SIGHAN Joint Confer-
ence on Chinese Language Processing, pages 3?8,
Tianjin, China, December. Association for Compu-
tational Linguistics.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing
He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li.
2011. Comparing twitter and traditional media us-
ing topic models. In Advances in Information Re-
trieval, pages 338?349. Springer.
679
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 414?421,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
A Description of Tunable Machine Translation Evaluation Systems in 
WMT13 Metrics Task 
 
 
Aaron L.-F. Han 
hanlifengaaron@gmail.com 
Derek F. Wong 
derekfw@umac.mo 
Lidia S. Chao 
lidiasc@umac.mo 
   
Yi Lu 
mb25435@umac.mo 
Liangye He 
wutianshui0515@gmail.com 
Yiming Wang 
mb25433@umac.mo 
Jiaji Zhou 
mb25473@uamc.mo 
    
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory 
Department of Computer and Information Science 
University of Macau, Macau S.A.R. China 
 
  
 
Abstract 
This paper is to describe our machine transla-
tion evaluation systems used for participation 
in the WMT13 shared Metrics Task. In the 
Metrics task, we submitted two automatic MT 
evaluation systems nLEPOR_baseline and 
LEPOR_v3.1. nLEPOR_baseline is an n-gram 
based language independent MT evaluation 
metric employing the factors of modified sen-
tence length penalty, position difference penal-
ty, n-gram precision and n-gram recall. 
nLEPOR_baseline measures the similarity of 
the system output translations and the refer-
ence translations only on word sequences. 
LEPOR_v3.1 is a new version of LEPOR met-
ric using the mathematical harmonic mean to 
group the factors and employing some linguis-
tic features, such as the part-of-speech infor-
mation. The evaluation results of WMT13 
show LEPOR_v3.1 yields the highest average-
score 0.86 with human judgments at system-
level using Pearson correlation criterion on 
English-to-other (FR, DE, ES, CS, RU) lan-
guage pairs. 
1 Introduction 
Machine translation has a long history since the 
1950s (Weaver, 1955) and gains a fast develop-
ment in the recent years because of the higher 
level of computer technology. For instances, Och 
(2003) presents Minimum Error Rate Training 
(MERT) method for log-linear statistical ma-
chine translation models to achieve better trans-
lation quality; Menezes et al (2006) introduce a 
syntactically informed phrasal SMT system for 
English-to-Spanish translation using a phrase 
translation model, which is based on global reor-
dering and the dependency tree; Su et al (2009) 
use the Thematic Role Templates model to im-
prove the translation; Costa-juss? et al (2012) 
develop the phrase-based SMT system for Chi-
nese-Spanish translation using a pivot language. 
With the rapid development of Machine Transla-
tion (MT), the evaluation of MT has become a 
challenge in front of researchers. However, the 
MT evaluation is not an easy task due to the fact 
of the diversity of the languages, especially for 
the evaluation between distant languages (Eng-
lish, Russia, Japanese, etc.). 
2 Related works 
The earliest human assessment methods for ma-
chine translation include the intelligibility and 
fidelity used around 1960s (Carroll, 1966), and 
the adequacy (similar as fidelity), fluency and 
comprehension (improved intelligibility) (White 
et al, 1994). Because of the expensive cost of 
manual evaluations, the automatic evaluation 
metrics and systems appear recently. 
The early automatic evaluation metrics in-
clude the word error rate WER (Su et al, 1992) 
and position independent word error rate PER 
(Tillmann et al, 1997) that are based on the Le-
venshtein distance. Several promotions for the 
MT and MT evaluation literatures include the 
ACL?s annual workshop on statistical machine 
translation WMT (Koehn and Monz, 2006; Calli-
son-Burch et al, 2012), NIST open machine 
translation (OpenMT) Evaluation series (Li, 
2005) and the international workshop of spoken 
language translation IWSLT, which is also orga-
nized annually from 2004 (Eck and Hori, 2005; 
414
Paul, 2008, 2009; Paul, et al, 2010; Federico et 
al., 2011). 
BLEU (Papineni et al, 2002) is one of the 
commonly used evaluation metrics that is de-
signed to calculate the document level precisions. 
NIST (Doddington, 2002) metric is proposed 
based on BLEU but with the information weights 
added to the n-gram approaches. TER (Snover et 
al., 2006) is another well-known MT evaluation 
metric that is designed to calculate the amount of 
work needed to correct the hypothesis translation 
according to the reference translations. TER in-
cludes the edit categories such as insertion, dele-
tion, substitution of single words and the shifts of 
word chunks. Other related works include the 
METEOR (Banerjee and Lavie, 2005) that uses 
semantic matching (word stem, synonym, and 
paraphrase), and (Wong and Kit, 2008), (Popovic, 
2012), and (Chen et al, 2012) that introduces the 
word order factors, etc. The traditional evalua-
tion metrics tend to perform well on the language 
pairs with English as the target language. This 
paper will introduce the evaluation models that 
can also perform well on the language pairs that 
with English as source language. 
3 Description of Systems 
3.1 Sub Factors 
Firstly, we introduce the sub factor of modified 
length penalty inspired by BLEU metric. 
 
    {
   
 
            
                  
   
 
            
 (1) 
 
In the formula,    means sentence length 
penalty that is designed for both the shorter or 
longer translated sentence (hypothesis translation) 
as compared to the reference sentence. Parame-
ters   and   represent the length of candidate 
sentence and reference sentence respectively. 
Secondly, let?s see the factors of n-gram pre-
cision and n-gram recall. 
 
    
              
                           
 (2) 
 
    
              
                          
  (3) 
 
The variable                represents the 
number of matched n-gram chunks between hy-
pothesis sentence and reference sentence. The n-
gram precision and n-gram recall are firstly cal-
culated on sentence-level instead of corpus-level 
that is used in BLEU (  ). Then we define the 
weighted n-gram harmonic mean of precision 
and recall (WNHPR). 
 
          (?       (        
 
    (4) 
 
Thirdly, it is the n-gram based position differ-
ence penalty (NPosPenal). This factor is de-
signed to achieve the penalty for the different 
order of successfully matched words in reference 
sentence and hypothesis sentence. The alignment 
direction is from the hypothesis sentence to the 
reference sentence. It employs the  -gram meth-
od into the matching period, which means that 
the potential matched word will be assigned 
higher priority if it also has nearby matching. 
The nearest matching will be accepted as a back-
up choice if there are both nearby matching or 
there is no other matched word around the poten-
tial pairs. 
 
                 (5) 
 
     
 
          
?      
         
    (6) 
 
                             (7) 
 
The variable           means the length of 
the hypothesis sentence; the variables 
          and           represent the posi-
tion number of matched words in hypothesis sen-
tence and reference sentence respectively.  
3.2 Linguistic Features 
The linguistic features could be easily employed 
into our evaluation models. In the submitted ex-
periment results of WMT Metrics Task, we used 
the part of speech information of the words in 
question. In grammar, a part of speech, which is 
also called a word class, a lexical class, or a lexi-
cal category, is a linguistic category of lexical 
items. It is generally defined by the syntactic or 
morphological behavior of the lexical item in 
question. The POS information utilized in our 
metric LEPOR_v3.1, an enhanced version of 
LEPOR (Han et al, 2012), is extracted using the 
Berkeley parser (Petrov et al, 2006) for English, 
German, and French languages, using COM-
POST Czech morphology tagger (Collins, 2002) 
for Czech language, and using TreeTagger 
(Schmid, 1994) for Spanish and Russian lan-
guages respectively. 
415
Ratio 
other-to-English English-to-other 
CZ-EN DE-EN ES-EN FR-EN EN-CZ EN-DE EN-ES EN-FR 
HPR:LP:NPP(word) 7:2:1 3:2:1 7:2:1 3:2:1 7:2:1 1:3:7 3:2:1 3:2:1 
HPR:LP:NPP(POS) NA 3:2:1 NA 3:2:1 7:2:1 7:2:1 NA 3:2:1 
    (      1:9 9:1 1:9 9:1 9:1 9:1 9:1 9:1 
    (     NA 9:1 NA 9:1 9:1 9:1 NA 9:1 
        NA 1:9 NA 9:1 1:9 1:9 NA 9:1 
Table 1. The tuned weight values in LEPOR_v3.1 system 
 
System 
Correlation Score with Human Judgment 
other-to-English English-to-other Mean 
score CZ-EN DE-EN ES-EN FR-EN EN-CZ EN-DE EN-ES EN-FR 
LEPOR_v3.1 0.93 0.86 0.88 0.92 0.83 0.82 0.85 0.83 0.87 
nLEPOR_baseline 0.95 0.61 0.96 0.88 0.68 0.35 0.89 0.83 0.77 
METEOR 0.91 0.71 0.88 0.93 0.65 0.30 0.74 0.85 0.75 
BLEU 0.88 0.48 0.90 0.85 0.65 0.44 0.87 0.86 0.74 
TER 0.83 0.33 0.89 0.77 0.50 0.12 0.81 0.84 0.64 
Table 2. The performances of nLEPOR_baseline and LEPOR_v3.1 systems on WMT11 corpora 
 
3.3 The nLEPOR_baseline System 
The nLEPOR_baseline system utilizes the simple 
product value of the factors: modified length 
penalty, n-gram position difference penalty, and 
weighted n-gram harmonic mean of precision 
and recall. 
 
                           (8) 
 
The system level score is the arithmetical 
mean of the sentence level evaluation scores. In 
the experiments of Metrics Task using the 
nLEPOR_baseline system, we assign N=1 in the 
factor WNHPR, i.e. weighted unigram harmonic 
mean of precision and recall. 
3.4 The LEPOR_v3.1 System 
The system of LEPOR_v3.1 (also called as 
hLEPOR) combines the sub factors using 
weighted mathematical harmonic mean instead 
of the simple product value. 
 
        
                   
   
  
 
          
         
 
    
   
 (9) 
 
Furthermore, this system takes into account 
the linguistic features, such as the POS of the 
words. Firstly, we calculate the hLEPOR score 
on surface words            (the closeness of 
the hypothesis translation and the reference 
translation). Then, we calculate the hLEPOR 
score on the extracted POS sequences 
          (the closeness of the corresponding 
POS tags between hypothesis sentence and refer-
ence sentence). The final score             is 
the combination of the two sub-scores 
           and          . 
 
             
 
       
(              
               (10) 
 
4 Evaluation Method 
In the MT evaluation task, the Spearman rank 
correlation coefficient method is usually used by 
the authoritative ACL WMT to evaluate the cor-
relation of different MT evaluation metrics. So 
we use the Spearman rank correlation coefficient 
  to evaluate the performances of 
nLEPOR_baseline and LEPOR_v3.1 in system 
level correlation with human judgments. When 
there are no ties,   is calculated using: 
 
     
 ?  
 
 (     
  (11) 
 
The variable    is the difference value be-
tween the ranks for         and   is the number 
of systems. We also offer the Pearson correlation 
coefficient information as below. Given a sample 
of paired data (X, Y) as (      ,         , the 
Pearson correlation coefficient is: 
 
     
? (      (      
 
   
?? (       
 
   
?? (     )
 
     
 (12) 
416
where    and    specify the mean of discrete 
random variable X and Y respectively. 
 
Directions 
EN-
FR 
EN-
DE 
EN-
ES 
EN-
CS 
EN-
RU 
Av 
LEPOR_v3.1 .91 .94 .91 .76 .77 .86 
nLEPOR_baseline .92 .92 .90 .82 .68 .85 
SIMP-
BLEU_RECALL 
.95 .93 .90 .82 .63 .84 
SIMP-
BLEU_PREC 
.94 .90 .89 .82 .65 .84 
NIST-mteval-
inter 
.91 .83 .84 .79 .68 .81 
Meteor .91 .88 .88 .82 .55 .81 
BLEU-mteval-
inter 
.89 .84 .88 .81 .61 .80 
BLEU-moses .90 .82 .88 .80 .62 .80 
BLEU-mteval .90 .82 .87 .80 .62 .80 
CDER-moses .91 .82 .88 .74 .63 .80 
NIST-mteval .91 .79 .83 .78 .68 .79 
PER-moses .88 .65 .88 .76 .62 .76 
TER-moses .91 .73 .78 .70 .61 .75 
WER-moses .92 .69 .77 .70 .61 .74 
TerrorCat .94 .96 .95 na na .95 
SEMPOS na na na .72 na .72 
ACTa .81 -.47 na na na .17 
ACTa5+6 .81 -.47 na na na .17 
Table 3. System-level Pearson correlation scores 
on WMT13 English-to-other language pairs 
5 Experiments 
5.1 Training 
In the training stage, we used the officially re-
leased data of past WMT series. There is no Rus-
sian language in the past WMT shared tasks. So 
we trained our systems on the other eight lan-
guage pairs including English to other (French, 
German, Spanish, Czech) and the inverse transla-
tion direction. In order to avoid the overfitting 
problem, we used the WMT11 corpora as train-
ing data to train the parameter weights in order to 
achieve a higher correlation with human judg-
ments at system-level evaluations. For the 
nLEPOR_baseline system, the tuned values of   
and   are 9 and 1 respectively for all language 
pairs except for (   ,    ) for CS-EN lan-
guage pair. For the LEPOR_v3.1 system, the 
tuned values of weights are shown in Table 1. 
The evaluation scores of the training results on 
WMT11 corpora are shown in Table 2. The de-
signed methods have shown promising correla-
tion scores with human judgments at system lev-
el, 0.87 and 0.77 respectively for 
nLEPOR_baseline and LEPOR_v3.1 of the mean 
score on eight language pairs. As compared to 
METEOR, BLEU and TER, we have achieved 
higher correlation scores with human judgments.  
5.2 Testing 
In the WMT13 shared Metrics Task, we also 
submitted our system performances on English-
to-Russian and Russian-to-English language 
pairs. However, since the Russian language did 
not appear in the past WMT shared tasks, we 
assigned the default parameter weights to Rus-
sian language for the submitted two systems. The 
officially released results on WMT13 corpora are 
shown in Table 3, Table 4 and Table 5 respec-
tively for system-level and segment-level per-
formance on English-to-other language pairs. 
 
Directions 
EN-
FR 
EN-
DE 
EN-
ES 
EN-
CS 
EN-
RU 
Av 
SIMP-
BLEU_RECALL 
.92 .93 .83 .87 .71 .85 
LEPOR_v3.1 .90 .9 .84 .75 .85 .85 
NIST-mteval-
inter 
.93 .85 .80 .90 .77 .85 
CDER-moses .92 .87 .86 .89 .70 .85 
nLEPOR_baseline .92 .90 .85 .82 .73 .84 
NIST-mteval .91 .83 .78 .92 .72 .83 
SIMP-
BLEU_PREC 
.91 .88 .78 .88 .70 .83 
Meteor .92 .88 .78 .94 .57 .82 
BLEU-mteval-
inter 
.92 .83 .76 .90 .66 .81 
BLEU-mteval .89 .79 .76 .90 .63 .79 
TER-moses .91 .85 .75 .86 .54 .78 
BLEU-moses .90 .79 .76 .90 .57 .78 
WER-moses .91 .83 .71 .86 .55 .77 
PER-moses .87 .69 .77 .80 .59 .74 
TerrorCat .93 .95 .91 na na .93 
SEMPOS na na na .70 na .70 
ACTa5+6 .81 -.53 na na na .14 
ACTa .81 -.53 na na na .14 
Table 4. System-level Spearman rank correlation 
scores on WMT13 English-to-other language 
pairs 
 
Table 3 shows LEPOR_v3.1 and 
nLEPOR_baseline yield the highest and the sec-
ond highest average Pearson correlation score 
0.86 and 0.85 respectively with human judg-
ments at system-level on five English-to-other 
language pairs. LEPOR_v3.1 and 
417
nLEPOR_baseline also yield the highest Pearson 
correlation score on English-to-Russian (0.77) 
and English-to-Czech (0.82) language pairs re-
spectively. The testing results of LEPOR_v3.1 
and nLEPOR_baseline show better correlation 
scores as compared to METEOR (0.81), BLEU 
(0.80) and TER-moses (0.75) on English-to-other 
language pairs, which is similar with the training 
results.  
On the other hand, using the Spearman rank 
correlation coefficient, SIMPBLEU_RECALL 
yields the highest correlation score 0.85 with 
human judgments. Our metric LEPOR_v3.1 also 
yields the highest Spearman correlation score on 
English-to-Russian (0.85) language pair, which 
is similar with the result using Pearson correla-
tion and shows its robust performance on this 
language pair.  
 
Directions 
EN-
FR 
EN-
DE 
EN-
ES 
EN-
CS 
EN-
RU Av 
SIMP-
BLEU_RECALL 
.16 .09 .23 .06 .12 .13 
Meteor .15 .05 .18 .06 .11 .11 
SIMP-
BLEU_PREC 
.14 .07 .19 .06 .09 .11 
sentBLEU-moses .13 .05 .17 .05 .09 .10 
LEPOR_v3.1 .13 .06 .18 .02 .11 .10 
nLEPOR_baseline .12 .05 .16 .05 .10 .10 
dfki_logregNorm-
411 
na na .14 na na .14 
TerrorCat .12 .07 .19 na na .13 
dfki_logregNormS
oft-431 
na na .03 na na .03 
Table 5. Segment-level Kendall?s tau correlation 
scores on WMT13 English-to-other language 
pairs 
 
However, we find a problem in the Spearman 
rank correlation method. For instance, let two 
evaluation metrics MA and MB with their evalu-
ation scores   ??????                   and  
  ???? ??                   respectively reflecting 
three MT systems  
 ??            . Before the calculation of cor-
relation with human judgments, they will be 
converted into   ??????  ?          and   ???? ??  ?          
with the same rank sequence using Spearman 
rank method; thus, the two evaluation systems 
will get the same correlation with human judg-
ments whatever are the values of human judg-
ments. But the two metrics reflect different re-
sults indeed: MA gives the outstanding score 
(0.95) to M1 system and puts very low scores 
(0.50 and 0.45) on the other two systems M2 and 
M3 while MB thinks the three MT systems have 
similar performances (scores from 0.74 to 0.77). 
This information is lost using the Spearman rank 
correlation methodology. 
The segment-level performance of 
LEPOR_v3.1 is moderate with the average Ken-
dall?s tau correlation score 0.10 on five English-
to-other language pairs, which is due to the fact 
that we trained our metrics at system-level in this 
shared metrics task. Lastly, the officially released 
results on WMT13 other-to-English language 
pairs are shown in Table 6, Table 7 and Table 8 
respectively for system-level and segment-level 
performance.  
 
Directions 
FR-
EN 
DE-
EN 
ES-
EN 
CS-
EN 
RU-
EN Av 
Meteor .98 .96 .97 .99 .84 .95 
SEMPOS .95 .95 .96 .99 .82 .93 
Depref-align .97 .97 .97 .98 .74 .93 
Depref-exact .97 .97 .96 .98 .73 .92 
SIMP-
BLEU_RECALL 
.97 .97 .96 .94 .78 .92 
UMEANT .96 .97 .99 .97 .66 .91 
MEANT .96 .96 .99 .96 .63 .90 
CDER-moses .96 .91 .95 .90 .66 .88 
SIMP-
BLEU_PREC 
.95 .92 .95 .91 .61 .87 
LEPOR_v3.1 .96 .96 .90 .81 .71 .87 
nLEPOR_baseline .96 .94 .94 .80 .69 .87 
BLEU-mteval-
inter 
.95 .92 .94 .90 .61 .86 
NIST-mteval-inter .94 .91 .93 .84 .66 .86 
BLEU-moses .94 .91 .94 .89 .60 .86 
BLEU-mteval .95 .90 .94 .88 .60 .85 
NIST-mteval .94 .90 .93 .84 .65 .85 
TER-moses .93 .87 .91 .77 .52 .80 
WER-moses .93 .84 .89 .76 .50 .78 
PER-moses .84 .88 .87 .74 .45 .76 
TerrorCat .98 .98 .97 na na .98 
Table 6. System-level Pearson correlation scores 
on WMT13 other-to-English language pairs 
 
METEOR yields the highest average correla-
tion scores 0.95 and 0.94 respectively using 
Pearson and Spearman rank correlation methods 
on other-to-English language pairs. The average 
performance of nLEPOR_baseline is a little bet-
ter than LEPOR_v3.1 on the five language pairs 
of other-to-English even though it is also moder-
ate as compared to other metrics. However, using 
418
the Pearson correlation method, 
nLEPOR_baseline yields the average correlation 
score 0.87 which already wins the BLEU (0.86) 
and TER (0.80) as shown in Table 6. 
 
Directions 
FR-
EN 
DE-
EN 
ES-
EN 
CS-
EN 
RU-
EN Av 
Meteor .98 .96 .98 .96 .81 .94 
Depref-align .99 .97 .97 .96 .79 .94 
UMEANT .99 .95 .96 .97 .79 .93 
MEANT .97 .93 .94 .97 .78 .92 
Depref-exact .98 .96 .94 .94 .76 .92 
SEMPOS .94 .92 .93 .95 .83 .91 
SIMP-
BLEU_RECALL 
.98 .94 .92 .91 .81 .91 
BLEU-mteval-
inter 
.99 .90 .90 .94 .72 .89 
BLEU-mteval .99 .89 .89 .94 .69 .88 
BLEU-moses .99 .90 .88 .94 .67 .88 
CDER-moses .99 .88 .89 .93 .69 .87 
SIMP-
BLEU_PREC 
.99 .85 .83 .92 .72 .86 
nLEPOR_baseline .95 .95 .83 .85 .72 .86 
LEPOR_v3.1 .95 .93 .75 0.8 .79 .84 
NIST-mteval .95 .88 .77 .89 .66 .83 
NIST-mteval-inter .95 .88 .76 .88 .68 .83 
TER-moses .95 .83 .83 0.8 0.6 
0.8
0 
WER-moses .95 .67 .80 .75 .61 .76 
PER-moses .85 .86 .36 .70 .67 .69 
TerrorCat .98 .96 .97 na na .97 
Table 7. System-level Spearman rank correlation 
scores on WMT13 other-to-English language 
pairs 
 
Once again, our metrics perform moderate at 
segment-level on other-to-English language pairs 
due to the fact that they are trained at system-
level. We notice that some of the evaluation met-
rics do not submit the results on all the language 
pairs; however, their performance on submitted 
language pair is sometimes very good, such as 
the dfki_logregFSS-33 metric with a segment-
level correlation score 0.27 on German-to-
English language pair. 
6 Conclusion 
This paper describes our participation in the 
WMT13 Metrics Task. We submitted two sys-
tems nLEPOR_baseline and LEPOR_v3.1. Both 
of the two systems are trained and tested using 
the officially released data. LEPOR_v3.1 yields 
the highest Pearson correlation average-score 
0.86 with human judgments on five English-to-
other language pairs, and nLEPOR_baseline 
yields better performance than LEPOR_v3.1 on 
other-to-English language pairs. Furthermore, 
LEPOR_v3.1 shows robust system-level perfor-
mance on English-to-Russian language pair, and 
nLEPOR_baseline shows best system-level per-
formance on English-to-Czech language pair us-
ing Pearson correlation criterion. As compared to 
nLEPOR_baseline, the experiment results of 
LEPOR_v3.1 also show that the proper use of 
linguistic information can increase the perfor-
mance of the evaluation systems. 
 
Directions 
FR-
EN 
DE-
EN 
ES-
EN 
CS-
EN 
RU-
EN Av 
SIMP-
BLEU_RECALL 
.19 .32 .28 .26 .23 .26 
Meteor .18 .29 .24 .27 .24 .24 
Depref-align .16 .27 .23 .23 .20 .22 
Depref-exact .17 .26 .23 .23 .19 .22 
SIMP-
BLEU_PREC 
.15 .24 .21 .21 .17 .20 
nLEPOR_baseline .15 .24 .20 .18 .17 .19 
sentBLEU-moses .15 .22 .20 .20 .17 .19 
LEPOR_v3.1 .15 .22 .16 .19 .18 .18 
UMEANT .10 .17 .14 .16 .11 .14 
MEANT .10 .16 .14 .16 .11 .14 
dfki_logregFSS-
33 
na .27 na na na .27 
dfki_logregFSS-
24 
na .27 na na na .27 
TerrorCat .16 .30 .23 na na .23 
Table 8. Segment-level Kendall?s tau correlation 
scores on WMT13 other-to-English language 
pairs 
Acknowledgments 
The authors wish to thank the anonymous re-
viewers for many helpful comments. The authors 
are grateful to the Science and Technology De-
velopment Fund of Macau and the Research 
Committee of the University of Macau for the 
funding support for our research, under the refer-
ence No. 017/2009/A and RG060/09-
10S/CS/FST.  
References  
Banerjee, Satanjeev and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 
Proceedings of the 43th Annual Meeting of the 
419
Association of Computational Linguistics 
(ACL- 05), pages 65?72, Ann Arbor, US, June. 
Association of Computational Linguistics. 
Callison-Burch, Chris, Philipp Koehn, Christof Monz, 
and Omar F. Zaidan. 2011. Findings of the 2011 
Workshop on Statistical Machine Translation. 
In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation (WMT '11). Asso-
ciation for Computational Linguistics, Stroudsburg, 
PA, USA, 22-64. 
Callison-Burch, Chris, Philipp Koehn, Christof Monz, 
Matt Post, Radu Soricut, and Lucia Specia. 2012. 
Findings of the 2012Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh 
Workshop on Statistical Machine Translation, 
pages 10?51, Montreal, Canada. Association for 
Computational Linguistics. 
Carroll, John B. 1966. An Experiment in Evaluating 
the Quality of Translations, Mechanical Transla-
tion and Computational Linguistics, vol.9, 
nos.3 and 4, September and December 1966, page 
55-66, Graduate School of Education, Harvard 
University. 
Chen, Boxing, Roland Kuhn and Samuel Larkin. 2012. 
PORT: a Precision-Order-Recall MT Evaluation 
Metric for Tuning, Proceedings of the 50th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 930?939, Jeju, Republic 
of Korea, 8-14 July 2012. 
Collins, Michael. 2002. Discriminative Training 
Methods for Hidden Markov Models: Theory and 
Experiments with Perceptron Algorithms. In Pro-
ceedings of the ACL-02 conference on Empiri-
cal methods in natural language processing, 
Volume 10 (EMNLP 02), pages 1-8. Association 
for Computational Linguistics, Stroudsburg, PA, 
USA. 
Costa-juss?, Marta R., Carlos A. Henr?quez and Ra-
fael E. Banchs. 2012. Evaluating Indirect Strategies 
for Chinese-Spanish Statistical Machine Transla-
tion. Journal of artificial intelligence research, 
Volume 45, pages 761-780. 
Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Lan-
guage Technology Research (HLT '02). Morgan 
Kaufmann Publishers Inc., San Francisco, CA, 
USA, 138-145. 
Eck, Matthias and Chiori Hori. 2005. Overview of the 
IWSLT 2005 Evaluation Campaign. Proceedings 
of IWSLT 2005. 
Federico, Marcello, Luisa Bentivogli, Michael Paul, 
and Sebastian Stiiker. 2011. Overview of the 
IWSLT 2011 Evaluation Campaign. In Proceed-
ings of IWSLT 2011, 11-27. 
Han, Aaron Li-Feng, Derek F. Wong and Lidia S. 
Chao. 2012. LEPOR: A Robust Evaluation Metric 
for Machine Translation with Augmented Factors. 
Proceedings of the 24th International Confer-
ence on Computational Linguistics (COLING 
2012: Posters), Mumbai, India. 
Koehn, Philipp and Christof Monz. 2006. Manual and 
Automatic Evaluation of Machine Translation be-
tween European Languages. Proceedings of the 
ACLWorkshop on Statistical Machine Trans-
lation, pages 102?121, New York City. 
Li, A. (2005). Results of the 2005 NIST machine 
translation evaluation. In Machine Translation 
Workshop. 
Menezes, Arul, Kristina Toutanova and Chris Quirk. 
2006. Microsoft Research Treelet Translation Sys-
tem: NAACL 2006 Europarl Evaluation, Proceed-
ings of the ACLWorkshop on Statistical Ma-
chine Translation, pages 158-161, New York 
City. 
Och, Franz Josef. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation.  In Pro-
ceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL-
2003). pp. 160-167. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. 
In Proceedings of the 40th Annual Meeting on 
Association for Computational Linguis-
tics (ACL '02). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 311-318. 
Paul, Michael. 2008. Overview of the IWSLT 2008 
Evaluation Campaign. Proceeding of IWLST 
2008, Hawaii, USA. 
Paul, Michael. 2009. Overview of the IWSLT 2009 
Evaluation Campaign. In Proc. of IWSLT 2009, 
Tokyo, Japan, pp. 1?18. 
Paul, Michael, Marcello Federico and Sebastian 
Stiiker. 2010. Overview of the IWSLT 2010 Eval-
uation Campaign, Proceedings of the 7th Inter-
national Workshop on Spoken Language 
Translation, Paris, December 2nd and 3rd, 2010, 
page 1-25. 
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 
21st International Conference on Computa-
tional Linguistics and the 44th annual meeting 
of the Association for Computational Linguis-
420
tics (ACL-44). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 433-440. 
Popovic, Maja. 2012. Class error rates for evaluation 
of machine translation output. Proceedings of the 
7th Workshop on Statistical Machine Transla-
tion, pages 71?75, Canada. 
Schmid, Helmut. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings of 
International Conference on New Methods in 
Language Processing, Manchester, UK. 
Snover, Matthew, Bonnie J. Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
Study of Translation Edit Rate with Targeted Hu-
man Annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Trans-
lation in the Americas (AMTA-06), pages 223?
231, USA. Association for Machine Translation in 
the Americas. 
Su, Hung-Yu and Chung-Hsien Wu. 2009. Improving 
Structural Statistical Machine Translation for Sign 
Language With Small Corpus Using Thematic 
Role Templates as Translation Memory, IEEE 
TRANSACTIONS ON AUDIO, SPEECH, AND 
LANGUAGE PROCESSING, VOL. 17, NO. 7. 
Su, Keh-Yih, Wu Ming-Wen and Chang Jing-Shin. 
1992. A New Quantitative Quality Measure for 
Machine Translation Systems. In Proceedings of 
the 14th International Conference on Compu-
tational Linguistics, pages 433?439, Nantes, 
France. 
Tillmann, Christoph, Stephan Vogel, Hermann Ney, 
Arkaitz Zubiaga, and Hassan Sawaf. 1997. Accel-
erated DP Based Search For Statistical Translation. 
In Proceedings of the 5th European Confer-
ence on Speech Communication and Technol-
ogy (EUROSPEECH-97). 
Weaver, Warren. 1955. Translation. In William Locke 
and A. Donald Booth, editors, Machine Transla-
tion of Languages: Fourteen Essays. John 
Wiley & Sons, New York, pages 15?23. 
White, John S., Theresa O?Connell, and Francis 
O?Mara. 1994. The ARPA MT evaluation method-
ologies: Evolution, lessons, and future approaches. 
In Proceedings of the Conference of the Asso-
ciation for Machine Translation in the Ameri-
cas (AMTA 1994). pp193-205. 
Wong, Billy and Chunyu Kit. 2008. Word choice and 
word position for automatic MT evaluation. In 
Workshop: MetricsMATR of the Association for 
Machine Translation in the Americas (AMTA), 
short paper, Waikiki, Hawai?I, USA. Association 
for Machine Translation in the Americas. 
 
421
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 83?90,
Baltimore, Maryland, 26-27 July 2014. c?2014 Association for Computational Linguistics
Factored Statistical Machine Translation for Grammatical Error 
Correction 
 
 
Yiming Wang, Longyue Wang, Derek F. Wong, Lidia S. Chao, Xiaodong Zeng, Yi Lu 
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory,  
Department of Computer and Information Science, 
University of Macau, Macau S.A.R., China 
{wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo, 
lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.mo 
 
  
 
Abstract 
This paper describes our ongoing work on 
grammatical error correction (GEC). Focusing 
on all possible error types in a real-life 
environment, we propose a factored statistical 
machine translation (SMT) model for this task. 
We consider error correction as a series of 
language translation problems guided by 
various linguistic information, as factors that 
influence translation results. Factors included 
in our study are morphological information, i.e. 
word stem, prefix, suffix, and Part-of-Speech 
(PoS) information. In addition, we also 
experimented with different combinations of 
translation models (TM), phrase-based and 
factor-based, trained on various datasets to 
boost the overall performance. Empirical 
results show that the proposed model yields an 
improvement of 32.54% over a baseline 
phrase-based SMT model. The system 
participated in the CoNLL 2014 shared task 
and achieved the 7
th
 and 5
th
 F0.5 scores
1 on the 
official test set among the thirteen 
participating teams. 
 
1 Introduction 
The task of grammatical error detection and 
correction (GEC) is to make use of 
computational methods to fix the mistakes in a 
written text. It is useful in two aspects. For a 
non-native English learner it may help to 
improve the grammatical quality of the written 
text. For a native speaker the tool may help to 
remedy mistakes automatically. Automatic 
                                                          
1  These two rankings are based on gold-standard edits 
without and with alternative answers, respectively. 
correction of grammatical errors is an active 
research topic, aiming at improving the writing 
process with the help of artificial intelligent 
techniques. Second language learning is a user 
group of particular interest. 
Recently, Helping Our Own (HOO) and 
CoNLL held a number of shared tasks on this 
topic (Dale et al., 2012, Ng et al., 2013, Ng et al., 
2014). Previous studies based on rules (Sidorov 
et al., 2013), data-driven methods (Berend et al., 
2013, Yi et al., 2013) and hybrid methods (Putra 
and Szab?, 2013, Xing et al., 2013) have shown 
substantial gains for some frequent error types 
over baseline methods. Most proposed methods 
share the commonality that a sub-model is built 
for a specific type of error, on top of which a 
strategy is applied to combine a number of these 
individual models. Also, detection and correction 
are often split into two steps. For example, Xing 
et al. (2013) presented the UM-Checker for five 
error types in the CoNLL 2013 shared task. The 
system implements a cascade of five individual 
detection-and-correction models for different 
types of error. Given an input sentence, errors are 
detected and corrected one-by-one by each sub-
model at the level of its corresponding error type.  
The specifics of an error type are fully 
considered in each sub-model, which is easier to 
realize for a single error type than for multiple 
types in a single model. In addition, dividing the 
error detection and correction into two steps 
alleviates the application of machine learning 
classifiers. However, an approach that considers 
error types individually may have negative 
effects: 
? This approach assumes independence 
between each error type. It ignores the 
interaction of neighboring errors. Results 
(Xing et al., 2013) have shown that 
83
consecutive errors of multiple types tend to 
hinder solving these errors individually. 
? As the number of error types increases, the 
complexities of analyzing, designing, and 
implementing the model increase, in 
particular when combinatorial errors are 
taken into account. 
? Looking for an optimal model combination 
becomes complex. A simple pipeline 
approach would result in interference and the 
generation of new errors, and hence to 
propagating those errors to the subsequent 
processes. 
? Separating the detection and correction tasks 
may result in more errors. For instance, once 
a candidate is misidentified as an error, it 
would be further revised and turned into an 
error by the correction model. In this 
scenario the model risks losing precision. 
In the shared task of this year (Ng et la., 
2014), two novelties are introduced: 1) all types 
of errors present in an essay are to be detected 
and corrected (i.e., there is no restriction on the 
five error types of the 2013 shared task); 2) the 
official evaluation metric of this year adopts F0.5, 
weighting precision twice as much as recall. This 
requires us to explore an alternative universal 
joint model that can tackle various kinds of 
grammatical errors as well as join the detection 
and correction processes together. Regarding 
grammatical error correction as a process of 
translation has been shown to be effective (Ehsan 
and Faili, 2013, Mizumoto et al., 2011, 
Yoshimoto et al., 2013, Yuan and Felice, 2013). 
We treat the problematic sentences and golden 
sentences as pairs of source and target sentences. 
In SMT, a translation model is trained on a 
parallel corpus that consists of the source 
sentences (i.e. sentences that may contain 
grammatical errors) and the targeted translations 
(i.e. the grammatically well-formed sentences). 
The challenge is that we need a large amount of 
these parallel sentences for constructing such a 
data-driven SMT system. Some researches 
(Brockett et al., 2006, Yuan and Felice, 2013) 
explore generating artificial errors to resolve this 
sparsity problem. Other studies (Ehsan and Faili, 
2013, Yoshimoto et al., 2013, Yuan and Felice, 
2013) focus on using syntactic information (such 
as PoS or tree structure) to enhance the SMT 
models.  
In this paper, we propose a factored SMT 
model by taking into account not only the surface 
information contained in the sentence, but also 
morphological and syntactic clues (i.e., word 
stem, prefix, suffix and finer PoS information). 
To counter the sparsity problem we do not use 
artificial or manual approaches to enrich the 
training data. Instead we apply factored and 
transductive learning techniques to enhance the 
model on a small dataset. In addition, we also 
experimented with different combinations of 
translation models (TM), phrase- and factor-
based, that are trained on different datasets to 
boost the overall performance. Empirical results 
show that the proposed model yields an 
improvement of 32.54% over a baseline phrase-
based SMT model. 
The remainder of this paper is organized as 
follows: Section 2 describes our proposed 
methods. Section 3 reports on the design of our 
experiments. We discuss the result, including the 
official shared task results, in Section 4,. We 
summarize our conclusions in Section 5. 
2 Methodology 
In contrast with phrase-based translation models, 
factored models make use of additional linguistic 
clues to guide the system such that it generates 
translated sentences in which morphological and 
syntactic constraints are met (Koehn and Hoang, 
2007). The linguistic clues are taken as factors in 
a factored model; words are represented as 
vectors of factors rather than as a single token. 
This requires us to pre-process the training data 
to factorize all words. In this study, we explore 
the use of various types of morphological 
information and PoS as factors. For each possible 
factor we build an individual translation model. 
The effectiveness of all factors is analyzed by 
comparing the performance of the corresponding 
models on the grammatical error correction task. 
Furthermore, two approaches are proposed to 
combine those models. One adopts the model 
cascading method based on transductive learning. 
The second approach relies on learning and 
decoding multiple factors learning. The details of 
each approach are discussed in the following 
sub-sections. 
2.1 Data Preparation 
In order to construct a SMT model, we convert 
the training data into a parallel corpus where the 
problematic sentences that ought to be corrected 
are regarded as source sentences, while the 
reference sentences are treated as the 
corresponding target translations. We discovered 
that a number of sentences is absent at the target 
side due to incorrect annotations in the golden 
84
data. We removed these unparalleled sentences 
from the data. Secondly, the initial 
capitalizations of sentences are converted to their 
most probable casing using the Moses truecaser2. 
URLs are quite common in the corpus, but they 
are not useful for learning and even may cause 
the model to apply unnecessary correction on it. 
Thus, we mark all of the ULRs with XML 
markups, signaling the SMT decoder not to 
analyze an URL and output it as is.  
2.2 Model Construction 
In this study we explore four different factors: 
prefix, suffix, stem, and PoS. This linguistic 
information not only helps to capture the local 
constraints of word morphologies and the 
interaction of adjacent words, but also helps to 
prevent data sparsity caused by inflected word 
variants and insufficient training data.  
Word stem: Instead of lemmas, we prefer  
word stemming as one of the factors, considering 
that stemming does not requires deep 
morphological analysis and is easier to obtain. 
Second, during the whole error detection and 
correction process, stemming information is used 
as auxiliary information in addition to the 
original word form. Third, for grammatical error 
correction using word lemmas or word stems in 
factored translation model shows no significant 
difference. This is because we are translating text 
of the same language, and the translation of this 
factor, stem or lemma, is straightforwardly 
captured by the model. Hence, we do not rely on 
the word lemma. In this work, we use the 
English Porter stemmer (Porter, 1980) for 
generating word stems.  
Prefix: The second type of morphological 
information we explored is the word prefix. 
Although a prefix does not present strong 
evidence to be useful to the grammatical error 
correction, we include it in our study in order to 
fully investigate all types of morphological 
information. We believe the prefix can be an 
important factor in the correction of initial 
capitalization, e.g. ?In this era, engineering 
designs?? should be changed to ?In this era, 
engineering designs?? In model construction, 
we take the first three letters of a word as its 
prefix. If the length of a word is less than three, 
we use the word as the prefix factor. 
Suffix: Suffix, one of the important factors, 
helps to capture the grammatical agreements 
between predicates and arguments within a 
                                                          
2 After decoding, we will de-truecase all these words. 
sentence. Particularly the endings of plural nouns 
and inflected verb variants are useful for the 
detection of agreement violations that shown up 
in word morphologies. Similar to how we 
represent the prefix, we are interested in the last 
three characters of a word.  
 Examples 
Sentence 
this card contains biometric data to 
add security and reduce the risk of 
falsification 
Original 
POS 
DT NN BVZ JJ NNS TO VB NN 
CC VB DT NN IN NN 
Specific 
POS 
DT NN VBZ JJ NNS TO_to VB 
NN CC VB DT_the NN IN_of 
NN 
Table 1: Example of modified PoS. 
According to the description of factors, Figure 
1 illustrates the forms of various factors 
extracted from a given example sentence.  
Surface 
constantly combining ideas will 
result in better solutions being 
formulated 
Prefix con com ide wil res in bet sol bei for 
Suffix tly ing eas ill ult in ter ons ing ted 
Stem 
constantli combin idea will result in 
better solut be formul 
Specific 
POS 
RB VBG NNS MD VB IN JJR NNS 
VBG VBN 
Figure 1: The factorized sentence. 
PoS: Part-of-Speech tags denote the morpho-
syntactic category of a word. The use of PoS 
sequences enables us to some extent to recover 
missing determiners, articles, prepositions, as 
well as the modal verb in a sentence. Empirical 
studies (Yuan and Felice, 2013) have 
demonstrated that the use of this information can 
greatly improve the accuracy of the grammatical 
error correction. To obtain the PoS, we adopt the 
Penn Treebank tag set (Marcus et al., 1993), 
which contains 45 PoS tags. The Stanford parser 
(Klein and Manning, 2002) is used to extract the 
PoS information. Inspired by Yuan and Felice 
(2013), who used preposition-specific tags to fix 
the problem of being unable to distinguish 
between prepositions and obtained good 
performance, we create specific tags both for 
determiners (i.e., a, an, the) and prepositions. 
Table 1 provides an example of this modification, 
where prepositions, TO and IN, and determiner, 
85
DT, are revised to TO_to, IN_of and DT_the, 
respectively. 
2.3 Model Combination 
In addition to the design of different factored 
translation models, two model combination 
strategies are designed to treat grammatical error 
correction problem as a series of translation 
processes, where an incorrect sentence is 
translated into the correct one. In both 
approaches we pipeline two translation models, 
    and    . In the first approach, we derive 
four combinations of different models that 
trained on different sources.  
? In case I,    
  and    
  are both factored 
models but trained on different factors, e.g. 
for     
 training on ?surface + factori? and 
    
  on ?surface + factori?j?. Both models 
use the same training sentences, but different 
factors.  
? In case II,     
  is trained on sentences that 
paired with the output from the previous 
model,     
 , and the golden correct sentences. 
We want to create a second model that can 
also tackle the new errors introduced by the 
first model. 
? In case III, similar to case II, the second 
translation model,    
  is replaced by a 
phrase-based translation model.  
? In case IV, the quality of training data is 
considered vital to the construction of a good 
translation model. The present training dataset 
is not large enough. To complement this, the 
second model,     
 , is trained on an enlarged 
data set, by combining the training data of 
both models, i.e. the original parallel data 
(official incorrect and correct sentence pairs) 
and the supplementary parallel data 
(sentences output from the first model,     
 , 
and the correct sentences). Note that we do 
not de-duplicate sentences.  
In all cases, the testing process is carried out 
as follows. The test set is translated by the first 
translation model,     
 . The output from the first 
model is then fed into the second translation 
model,     
 . The output of the second model is 
used as the final corrections. 
The second combination approach is to make 
use of multiple factors for model construction. 
The question is whether multiple factors when 
used together may improve the correction results. 
In this setting we combine two factors together 
with the word surface form to build a multi-
factored translation model. All pairs of factors 
are used, e.g. stem and PoS. The decoding 
sequence is as follows: translate the input stems 
into target stems; translate the PoS; and generate 
the surface form given the factors of stem and 
PoS. 
3 Experiment Setup  
3.1 Dataset 
We pre-process the NUCLE corpus (Dahlmeier 
et al., 2013) as described in Section 2 for training 
different translation models. We use both the 
official golden sentences and additional 
WMT2014 English monolingual data3 to train an 
in-domain and a general-domain language model 
(LM), respectively. These language models are 
linearly interpolated in the decoding phase. We 
also randomly select a number of sentence pairs 
from the parallel corpus as a development set and 
a test set, disjoint from the training data. Table 2 
summarizes the statistics of all the datasets.  
Corpus Sentences Tokens 
Parallel 
Corpus 
55,503 
1,124,521 / 
1,114,040 
Additional 
Monolingual 
85,254,788 2,033,096,800 
Dev. Set 500 10,532 / 10,438 
Test Set 900 18,032 / 17,906 
Table 2: Statistics of used corpora. 
The experiments were carried out with 
MOSES 1.04 (Philipp Koehn et al., 2007). The 
translation and the re-ordering model utilizes the 
?grow-diag-final? symmetrized word-to-word 
alignments created with GIZA++5 (Och and Ney, 
2003) and the training scripts of MOSES. A 5-
gram LM was trained using the SRILM toolkit6 
(Stolcke et al., 2002), exploiting the improved 
modified Kneser-Ney smoothing (Kneser and 
Ney, 1995), and quantizing both probabilities 
and back-off weights. For the log-linear model 
training, we take minimum-error-rate training 
(MERT) method as described in (Och, 2003). 
The result is evaluated by M2 Scorer (Dahlmeier 
and Ng, 2012) computing precision, recall and 
F0.5.  
                                                          
3 http://www.statmt.org/wmt14/translation-task.html. 
4 http://www.statmt.org/moses/. 
5 http://code.google.com/p/giza-pp/. 
6 http://www.speech.sri.com/projects/srilm/. 
86
In total, one baseline system, five individual 
systems, and four combination systems are 
evaluated in this study. The baseline system 
(Baseline) is trained on the words-only corpus 
using a phrase-based translation model. For the 
individual systems we adopt the factored 
translation model that are trained respectively on 
1) surface and stem factors (Sys+stem), 2) surface 
and suffix factors (Sys+suf), 3) surface and prefix 
factors (Sys+pref), 4) surface and PoS factors 
(Sys+PoS), and 5) surface and modified-PoS 
factors (Sys+MPoS). The combination systems 
include: 1) the combination of ?factored + 
phrase-based? and ?factored + factored? for 
models cascading; and 2) the factors of surface, 
stem and modified-PoS (Sys+stem+MPoS) are 
combined for constructing a correction system 
based on a multi-factor model. 
4 Results and Discussions 
We report our results in terms of the precision, 
recall and F0.5 obtained by each of the individual 
models and combined models.  
4.1 Individual Model 
Table 3 shows the absolute measures for the 
baseline system, while the other individual 
models are listed with values relative to the 
baseline.  
Model Precision  Recall  F0.5 
Baseline 25.58 3.53 11.37 
Sys+stem -14.84 +13.00 +0.18 
Sys+suf -14.57 +14.77 +0.60 
Sys+pref -15.74 +12.20 -0.77 
Sys+PoS -11.63 +9.79 +2.45 
Sys+MPoS -10.25 +10.60 +3.70 
Table 3: Performance of various models. 
The baseline system has the highest precision 
score but the lowest recall. Nearly all individual 
models except Sys+pref show improvements in the 
correction result (F0.5) over the baseline. Overall, 
Sys+MPoS achieves the best result for the 
grammatical error correction task. It shows a 
significant improvement over the other models 
and outperforms the baseline model by 3.7 F0.5 
score. The Sys+stem and Sys+suf models obtain an 
improvement of 0.18 and 0.60 in F0.5 scores, 
respectively, compared to the baseline. Although 
the differences are not significant, it confirms our 
hypothesis that morphological clues do help to 
improve error correction. The F0.5 score of 
Sys+pref is the lowest among the models including 
the baseline, showing a drop of 0.77 in F0.5 score 
against the baseline. One possible reason is that 
few errors (in the training corpus) involve word 
prefixes. Thus, the prefix does not seem to be a 
suitable factor for tackling the GEC problem. 
Type 
Sys+stem 
(%) 
Sys+suf 
(%) 
Sys+MPoS 
(%) 
Error 
Num. 
Vt 17.07 12.20 12.20 41 
ArtOrDet 37.65 36.47 29.41 85 
Nn 33.33 19.61 23.53 51 
Prep 10.26 10.26 12.82 39 
Wci 9.10 10.61 6.10 66 
Rloc- 15.20 13.92 10.13 79 
Table 4: The capacity of different models in 
handling six frequent error types. 
We analyze the capacities of the models on 
different types of errors. Sys+PoS and Sys+MPoS are 
built by using the PoS and modified PoS. Both of 
them yield an improvement in F0.5 score. Overall, 
Sys+MPoS produces more accurate results than 
Sys+pref. Therefore, we specifically compare and 
evaluate the best three models, Sys+stem, Sys+suf 
and Sys+MPoS. Table 4 presents evaluation scores 
of these models for the six most frequent error 
types, which take up a large part of the training 
and test data. Among them, Sys+stem displays a 
powerful capacity to handle determiner and 
noun/number agreement errors, up to 37.65% 
and 33.33%. Sys+suf shows the ability to correct 
determiner errors at 36.47%; Sys+MPoS yields a 
similar performance to Sys+suf. All three 
individual models exhibit a relatively high 
capacity to handle determiner errors. The likely 
reason is that this mistake constitutes the largest 
portion in training data and test set, giving the 
learning models many examples to capture this 
problem well. In the case of preposition errors, 
Sys+MPoS demonstrates a better performance. This, 
once again, confirms the result (Yuan and Felice, 
2013) that the modified PoS factor is effective 
for every preposition word. For these six error 
types, the individual models show a weak 
capacity to handle the word collocation or idiom 
error category (Wci). Although Sys+MPoS 
achieves the highest F0.5 score in the overall 
evaluation, it only achieves 6.10% in handling 
this error type. The likely reason is that idioms 
are not frequent in the training data, and also that 
in most of the cases they contain out-of-
vocabulary words never seen in training data. 
4.2 Model Combination 
We intend to further boost the overall 
performance of the correction system by 
87
combining the strengths of individual models 
through model combination, and compare against 
the baseline. The systems compared here cover 
three pipelined models and a multi-factored 
model, as described earlier in Section 3. The 
combined systems include: 1) CSyssuf+phrase: the 
combination of Sys+suf and the baseline phrase-
based translation model; 2) CSyssuf+suf: we 
combine two similar factored models with suffix 
factors, Sys+suf, which is trained on the same 
corpus; and 3) TSyssuf+phrase: similar to 
CSyssuf+phrase, but the training data for the second 
phrase-based model is augmented by adding the 
output sentences from the previous model (paired 
with the correct sentences). Our intention is to 
enlarge the size of the training data. The 
evaluation results are presented in Table 5. 
Model Precision Recall F0.5 
Baseline 25.58 3.53 11.37 
CSyssuf+phrase -14.70 +14.61 +0.45 
CSyssuf+suf -15.04 +14.13 +0.09 
TSyssuf+phrase -14.76 +14.61 +0.40 
Sys+stem+MPoS -15.87 +11.72 -0.90 
Table 5: Evaluation results of combined models. 
In Table 5 we observe that Sys+stem+MPoS hurts 
performance and shows a drop of 0.9% in F0.5 
score. Both the CSyssuf+phrase and CSyssuf+suf 
show minor improvements over the baseline 
system. Even when we enrich the training data 
for the second model in TSyssuf+phrase, it cannot 
help in boosting the overall performance of the 
system. One of the problems we observe is that, 
with this combination structure, new incorrect 
sentences are introduced by the model at each 
step. The errors are propagated and accumulated 
to the final result. Although CSyssuf+phrase and 
CSyssuf+suf produce a better F0.5 score over the 
baseline, they are not as good as the individual 
models, Sys+PoS and Sys+MPoS, which are trained 
on PoS and modified-PoS, respectively. 
4.3 The Official Result 
After fully evaluating the designed individual 
models as well as the integrated ones, we adopt 
Sys+MPoS as our designated system for this 
grammatical error correction task. The official 
test set consists of 50 essays, and 2,203 errors. 
Table 6 shows the final result obtained by our 
submitted system.  
Table 7 details the correction rate of the five 
most frequent error types obtained by our system. 
The result suggests that the proposed system has 
a better ability in handling the verb, article and 
determiner error than other error types. 
Criteria Result Alt. Result 
P 0.3127 0.4317 
R 0.1446 0.1972 
F0.5 0.2537 0.3488 
Table 6: The official correction results of our 
submitted system. 
Type Error Correct % 
Vt 203/201 21/22 10.34/10.94 
V0 57/54 9/9 15.79/16.67 
Vform 156/169 11/18 7.05/10.65 
ArtOrDet 569/656 84/131 14.76/19.97 
Nn 319/285 31/42 9.72/10.91 
Table 7: Detailed error information of evaluation 
system (with alternative result). 
5 Conclusion 
This paper describes our proposed grammatical 
error detection and correction system based on a 
factored statistical machine translation approach. 
We have investigated the effectiveness of models 
trained with different linguistic information 
sources, namely morphological clues and 
syntactic PoS information. In addition, we also 
explore some ways to combine different models 
in the system to tackle the correction problem. 
The constructed models are compared against the 
baseline model, a phrase-based translation model. 
Results show that PoS information is a very 
effective factor, and the model trained with this 
factor outperforms the others. One difficulty of 
this year?s shared task is that participants have to 
tackle all 28 types of errors, which is five times 
more than last year. From the results, it is 
obvious there are still many rooms for improving 
the current system. 
Acknowledgements 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for their research, 
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS. 
The authors also wish to thank the anonymous 
reviewers for many helpful comments with 
special thanks to Antal van den Bosch for his 
generous help on this manuscript. 
  
88
References  
 G?bor Berend, Veronika Vincze, Sina Zarriess, 
and Rich?rd Farkas. 2013. LFG-based 
Features for Noun Number and Article 
Grammatical Errors. CoNLL-2013. 
Chris Brockett, William B. Dolan, and Michael 
Gamon. 2006. Correcting ESL errors using 
phrasal SMT techniques. Proceedings of the 
21st International Conference on 
Computational Linguistics and the 44th 
annual meeting of the Association for 
Computational Linguistics pages 249?256. 
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei 
Wu. 2013. Building a Large Annotated 
Corpus of Learner English: The NUS Corpus 
of Learner English. Proceedings of the Eighth 
Workshop on Innovative Use of NLP for 
Building Educational Applications. pages 22-
31. 
Robert Dale, Ilya Anisimoff, and George 
Narroway. 2012. HOO 2012: A report on the 
preposition and determiner error correction 
shared task. Proceedings of the Seventh 
Workshop on Building Educational 
Applications Using NLP pages 54?62. 
Nava Ehsan, and Heshaam Faili. 2013. 
Grammatical and context-sensitive error 
correction using a statistical machine 
translation framework. Software: Practice and 
Experience. Wiley Online Library. 
D. Klein, and C. D. Manning. 2002. Fast exact 
inference with a factored model for natural 
language parsing. Advances in neural 
information processing systems. 
Reinhard Kneser, and Hermann Ney. 1995. 
Improved backing-off for m-gram language 
modeling. Acoustics, Speech, and Signal 
Processing, 1995. ICASSP-95., 1995 
International Conference on Vol. 1, pages 
181?184. 
P. Koehn, and H. Hoang. 2007. Factored 
translation models. Proceedings of the Joint 
Conference on Empirical Methods in Natural 
Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL) 
Vol. 868, pages 876?876. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, 
Chris Callison-Burch, Marcello Federico, 
Nicola Bertoldi, Brooke Cowan, et al. 2007. 
Moses: Open source toolkit for statistical 
machine translation. Proceedings of the 45th 
Annual Meeting of the ACL on Interactive 
Poster and Demonstration Sessions pages 
177?180. 
M. P. Marcus, M. A. Marcinkiewicz, and B. 
Santorini. 1993. Building a large annotated 
corpus of English: The Penn Treebank. 
Computational linguistics. MIT Press. 
Tomoya Mizumoto, Mamoru Komachi, Masaaki 
Nagata, and Yuji Matsumoto. 2011. Mining 
Revision Log of Language Learning SNS for 
Automated Japanese Error Correction of 
Second Language Learners. IJCNLP pages 
147?155. 
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, 
Christian Hadiwinoto, Raymond Hendy 
Susanto, and Bryant Christopher. 2014. The 
conll-2014 shared task on grammatical error 
correction. Proceedings of CoNLL. Baltimore, 
Maryland, USA. 
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, 
Christian Hadiwinoto, and Joel Tetreault. 
2013. The conll-2013 shared task on 
grammatical error correction. Proceedings of 
CoNLL. 
Franz Josef Och. 2003. Minimum Error Rate 
Training in Statistical Machine Translation, 
160?167. 
Franz Josef Och, and Hermann Ney. 2003. A 
systematic comparison of various statistical 
alignment models. Computational linguistics. 
MIT Press. 
Martin F. Porter. 1980. An algorithm for suffix 
stripping. Program: electronic library and 
information systems. MCB UP Ltd. 
Desmond Darma Putra, and Lili Szab?. 2013. 
UdS at the CoNLL 2013 Shared Task. 
CoNLL-2013. 
Grigori Sidorov, Anubhav Gupta, Martin Tozer, 
Dolors Catala, Angels Catena, and Sandrine 
Fuentes. 2013. Rule-based System for 
Automatic Grammar Correction Using 
Syntactic N-grams for English Language 
Learning (L2). CoNLL-2013. 
Andreas Stolcke, and others. 2002. SRILM-an 
extensible language modeling toolkit. 
INTERSPEECH. 
Junwen Xing, Longyue Wang, Derek F. Wong, 
Lidia S. Chao, and Xiaodong Zeng. 2013. 
UM-Checker: A Hybrid System for English 
Grammatical Error Correction. Proceedings of 
the Seventeenth Conference on Computational 
Natural Language Learning: Shared Task, 
34?42. Sofia, Bulgaria: Association for 
Computational Linguistics. Retrieved from 
http://www.aclweb.org/anthology/W13-3605 
Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang 
Rim. 2013. KUNLP Grammatical Error 
Correction System For CoNLL-2013 Shared 
89
Task. CoNLL-2013. 
Ippei Yoshimoto, Tomoya Kose, Kensuke 
Mitsuzawa, Keisuke Sakaguchi, Tomoya 
Mizumoto, Yuta Hayashibe, Mamoru 
Komachi, et al. 2013. NAIST at 2013 CoNLL 
grammatical error correction shared task. 
CoNLL-2013. 
Zheng Yuan, and Mariano Felice. 2013. 
Constrained grammatical error correction 
using Statistical Machine Translation. 
CoNLL-2013. 
  
90
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 233?238,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Domain Adaptation for Medical Text Translation Using Web Re-
sources
Yi Lu, Longyue Wang, Derek F. Wong, Lidia S. Chao, Yiming Wang, Francisco Oliveira
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory,
Department of Computer and Information Science,
University of Macau, Macau, China
takamachi660@gmail.com, vincentwang0229@hotmail.com,
derekfw@umac.mo, lidiasc@umac.mo, wang2008499@gmail.com, 
olifran@umac.mo
Abstract
This paper describes adapting statistical 
machine translation (SMT) systems to 
medical domain using in-domain and 
general-domain data as well as web-
crawled in-domain resources. In order to 
complement the limited in-domain corpo-
ra, we apply domain focused web-
crawling approaches to acquire in-
domain monolingual data and bilingual 
lexicon from the Internet. The collected 
data is used for adapting the language 
model and translation model to boost the 
overall translation quality. Besides, we 
propose an alternative filtering approach
to clean the crawled data and to further 
optimize the domain-specific SMT sys-
tem. We attend the medical summary
sentence unconstrained translation task of 
the Ninth Workshop on Statistical Ma-
chine Translation (WMT2014). Our sys-
tems achieve the second best BLEU 
scores for Czech-English, fourth for 
French-English, English-French language 
pairs and the third best results for re-
minding pairs.
1 Introduction
In this paper, we report the experiments carried 
out by the NLP2CT Laboratory at University of 
Macau for WMT2014 medical sentence transla-
tion task on six language pairs: Czech-English 
(cs-en), French-English (fr-en), German-English 
(de-en) and the reverse direction pairs (i.e., en-cs, 
en-fr and en-de). 
As data in specific domain are usually rela-
tively scarce, the use of web resources to com-
plement the training resources provides an effec-
tive way to enhance the SMT systems (Resnik 
and smith, 2003; Espl?-Gomis and Forcada, 2010; 
Pecina et al., 2011; Pecina et al., 2012; Pecina et 
al., 2014). In our experiments, we not only use 
all available training data provided by the
WMT2014 standard translation task 1 (general-
domain data) and medical translation task2 (in-
domain data), but also acquire addition in-
domain bilingual translations (i.e. dictionary) and 
monolingual data from online sources.
First of all, we collect the medical terminolo-
gies from the web. This tiny but significant par-
allel data are helpful to reduce the out-of-
vocabulary words (OOVs) in translation models. 
In addition, the use of larger language models 
during decoding is aided by more efficient stor-
age and inference (Heafield, 2011). Thus, we 
crawl more in-domain monolingual data from the 
Internet based on domain focused web-crawling
approach. In order to detect and remove out-
domain data from the crawled data, we not only 
explore text-to-topic classifier, but also propose 
an alternative filtering approach combined the 
existing one (text-to-topic classifier) with per-
plexity. After carefully pre-processing all the 
available training data, we apply language model 
adaptation and translation model adaptation us-
ing various kinds of training corpora. Experi-
mental results show that the presented approach-
es are helpful to further boost the baseline system.
The reminder of this paper is organized as fol-
lows. In Section 2, we detail the workflow of 
web resources acquisition. Section 3 describes 
the pre-processing steps for the corpora. Section 
5 presents the baseline system. Section 6 reports 
the experimental results and discussions. Finally, 
                                                
1 http://www.statmt.org/wmt14/translation-task.html.
2 http://www.statmt.org/wmt14/medical-task/.
233
the submitted systems and the official results are 
reported in Section 7.
2 Domain Focused Web-Crawling
In this section, we introduce our domain focused 
web-crawling approaches on acquisition of in-
domain translation terminologies and monolin-
gual sentences. 
2.1 Bilingual Dictionary
Terminology is a system of words used to name 
things in a particular discipline. The in-domain 
vocabulary size directly affects the performance 
of domain-specific SMT systems. Small size of 
in-domain vocabulary may result in serious 
OOVs problem in a translation system. Therefore, 
we crawl medical terminologies from some 
online sources such as dict.cc3, where the vocab-
ularies are divided into different subjects. We 
obtain the related bilingual entries in medicine 
subject by using Scala build-in XML parser and 
XPath. After cleaning, we collected 28,600, 
37,407, and 37,600 entries in total for cs-en, de-
en, and fr-en respectively.
2.2 Monolingual Data
The workflow for acquiring in-domain resources 
consists of a number of steps such as domain 
identification, text normalization, language iden-
tification, noise filtering, and post-processing as 
well as parallel sentence identification.
Firstly we use an open-source crawler, Com-
bine4, to crawl webpages from the Internet. In 
order to classify these webpages as relevant to 
the medical domain, we use a list of triplets 
<term, relevance weight, topic class> as the 
basic entries to define the topic. Term is a word 
or phrase. We select terms for each language 
from the following sources: 
? The Wikipedia title corpus, a WMT2014 of-
ficial data set consisting of titles of medical 
articles. 
? The dict.cc dictionary, as is described in Sec-
tion 2.1.
? The DrugBank corpus, which is a WMT2014 
official data set on bioinformatics and 
cheminformatics.
For the parallel data, i.e. Wikipedia and dict.cc 
dictionary, we separate the source and target text 
into individual text and use either side of them
for constructing the term list for different lan-
                                                
3 http://www.dict.cc/.
4 http://combine.it.lth.se/.
guages. Regarding the DrugBank corpus, we di-
rectly extract the terms from the ?name? field. 
The vocabulary size of collected text for each 
language is shown in Table 1.
EN CS DE FR
Wikipedia Titles 12,684 3,404 10,396 8,436
dict.cc 29,294 16,564 29,963 22,513
DrugBank 2,788
Total 44,766 19,968 40,359 30,949
Table 1: Size of terms used for topic definition.
Relevance weight is the score for each occur-
rence of the term, which is assigned by its length, 
i.e., number of tokens. The topic class indicates 
the topics. In this study, we are interested in 
medical domain, the topic class is always marked 
with ?MED? in our topic definition. 
The topic relevance of each document is cal-
culated5 as follows:
  ? ?      
   
  
   
 
   (1)
where is the amount of terms in the topic defi-
nition;   
 is the weight of term  ;   
 is the 
weight of term at location  .    is the number of 
occurrences of term  at  position. In implemen-
tation, we use the default values for setting and
parameters. Another input required by the crawl-
er is a list of seed URLs, which are web sites that 
related to medical topic. We limit the crawler 
from getting the pages within the http domain 
guided by the seed links. We acquired the list 
from the Open Directory Project6, which is a re-
pository maintained by volunteer editors. Totally, 
we collected 12,849 URLs from the medicine
category.
Text normalization is to convert the text of 
each HTML page into UTF-8 encoding accord-
ing to the content_charset of the header. In addi-
tion, HTML pages often consist of a number of 
irrelevant contents such as the navigation links, 
advertisements disclaimers, etc., which may neg-
atively affect the performance of SMT system. 
Therefore, we use the Boilerpipe tool 
(Kohlsch?tter et al., 2010) to filter these noisy
data and preserve the useful content that is 
marked by the tag, <canonicalDocument>. The 
resulting text is saved in an XML file, which will 
be further processed by the subsequent tasks. For 
language identification, we use the language-
detection7 toolkit to determine the possible lan-
                                                
5
http://combine.it.lth.se/documentation/DocMain/node6.html.
6 http://www.dmoz.org/Health/Medicine/.
7 https://code.google.com/p/language-detection/.
234
guage of the text, and discard the articles which 
are in the right language we are interested.
2.3 Data Filtering
The web-crawled documents (described in Sec-
tion 2.2) may consist a number of out-domain 
data, which would harm the domain-specific lan-
guage and translation models. We explore and 
propose two filtering approaches for this task. 
The first one is to filter the documents based on 
their relative score, Eq. (1). We rank all the doc-
uments according to their relative scores and se-
lect top K percentage of entire collection for fur-
ther processing. 
Second, we use a combination method, which 
takes both the perplexity and relative score into 
account for the selection. Perplexity-based data 
selection has shown to be a powerful mean on 
SMT domain adaptation (Wang et al., 2013; 
Wang et al., 2014; Toral, 2013; Rubino et al., 
2013; Duh et al., 2013). The combination method 
is carried out as follows: we first retrieve the 
documents based on their relative scores. The 
documents are then split into sentences, and
ranked according to their perplexity using Eq. (2)
(Stolcke et al., 2002). The used language model 
is trained on the official in-domain data. Finally, 
top N percentage of ranked sentences are consid-
ered as additional relevant in-domain data. 
    ( )        
 ( )
    (2)
where  is a input sentence or document,  ( ) is 
the probability of  -gram segments estimated 
from the training set.     is the number of 
tokens of an input string.
3 Pre-processing
Both official training data and web-crawled re-
sources are processed using the Moses scripts8, 
this includes the text tokenization, truecasing and 
length cleaning. For trusecasing, we use both the 
target side of parallel corpora and monolingual 
data to train the trucase models. We consider the 
target system is intended for summary translation, 
the sentences tend to be short in length. We re-
move sentence pairs which are more than 80 
words at length in either sides of the parallel text.
In addition to these general data filtering steps,
we introduce some extra steps to pre-process the 
training data. The first step is to remove the du-
plicate sentences. In data-driven methods, the 
more frequent a term occurs, the higher probabil-
                                                
8 http://www.statmt.org/moses/?n=Moses.Baseline.
ity it biases. Duplicate data may lead to unpre-
dicted behavior during the decoding. Therefore, 
we keep only the distinct sentences in monolin-
gual corpus. By taking into account multiple 
translations in parallel corpus, we remove the 
duplicate sentence pairs. We also use a biomedi-
cal sentence splitter9 (Rune et al., 2007) to split 
sentences in monolingual corpora. The statistics 
of the data are provided in Table 2.
4 Baseline System
We built our baseline system on an optimized 
level. It is trained on all official in-domain train-
ing corpora and a portion of general-domain data. 
We apply the Moore-Lewis method (Moore and 
Lewis, 2010) and modified Moore-Lewis method 
(Axelrod et al., 2011) for selecting in-domain 
data from the general-domain monolingual and 
parallel corpora, respectively. The top M per-
centages of ranked sentences are selected as a 
pseudo in-domain data to train an additional LM
and TM. For LM, we linearly interpolate the ad-
ditional LM with in-domain LM. For TM, the 
additional model is log-linearly interpolated with 
the in-domain model using the multi-decoding 
method described in (Koehn and Schroeder, 
2007). Finally, LM adaptation and TM adapta-
tion are combined to further improve the transla-
tion quality of baseline system.
5 Experiments and Results
The official medical summary development sets 
(dev) are used for tuning and evaluating the 
comparative systems. The official medical sum-
mary test sets (test) are only used in our final 
submitted systems.
The experiments were carried out with the 
Moses 1.010 (Koehn et al., 2007). The translation 
and the re-ordering model utilizes the ?grow-
diag-final? symmetrized word-to-word align-
ments created with MGIZA++11 (Och and Ney, 
2003; Gao and Vogel, 2008) and the training 
scripts from Moses. A 5-gram LM was trained 
using the SRILM toolkit12 (Stolcke et al., 2002), 
exploiting improved modified Kneser-Ney 
smoothing, and quantizing both probabilities and 
back-off weights. For the log-linear model train-
ing, we take the minimum-error-rate training 
(MERT) method as described in (Och, 2003).
                                                
9 http://www.nactem.ac.uk/y-matsu/geniass/.
10 http://www.statmt.org/moses/.
11 http://www.kyloo.net/software/doku.php/mgiza:overview.
12 http://www.speech.sri.com/projects/srilm/.
235
In the following sub-sections, we describe the
results of baseline systems, which are trained on 
the official corpora. We also present the en-
hanced systems that make use of the web-
crawled bilingual dictionary and monolingual 
data as the additional training resources. Two
variants of enhanced system are constructed 
based on different filtering criteria.
5.1 Baseline System
The baseline systems is constructed based on the 
combination of TM adaptation and LM adapta-
tion, where the corresponding selection thresh-
olds ( ) are manually tuned. Table 3 shows the 
BLEU scores of baseline systems as well as the
threshold values of for general-domain mono-
lingual corpora and parallel corpora selection, 
respectively.
By looking into the results, we find that en-cs 
system performs poorly, because of the limited 
in-domain parallel and monolingual corpora 
(shown in Table 2). While the fr-en and en-fr 
systems achieve the best scores, due the availa-
bility of the high volume training data. We ex-
periment with different values of ={0, 25, 50, 
75, 100} that indicates the percentages of sen-
tences out of the general corpus used for con-
structing the LM adaptation and TM adaptation. 
After tuning the parameter  , we find that
BLEU scores of different systems peak at differ-
ent values of . LM adaptation can achieve the 
best translation results for cs-en, en-fr and de-en 
pairs when  =25, en-cs and en-de pairs when 
 =50, and fr-en pair when  =75. While TM 
adaptation yields the best scores for en-fr and en-
de pairs at  =25 and cs-en and fr-en pairs at 
 =50, de-en pair when =75 and en-cs pair at 
 =100.
Lang. Pair BLEU
Mono.
(M%)
Parallel
(M%)
en-cs 17.57 50% 100%
cs-en 31.29 25% 50%
en-fr 38.36 25% 25%
fr-en 44.36 75% 50%
en-de 18.01 50% 25%
de-en 32.50 25% 75%
Table 3: BLEU scores of baseline systems for 
different language pairs.
5.2 Based on Relevance Score Filtering
As described in Section 2.3, we use the relevance
score to filter out the non-in-domain documents. 
Once again, we evaluate different values of 
Data Set Lang. Sent. Words Vocab. Ave. Len. Sites Docs
In-domain 
Parallel Data
cs/en 1,770,421
9,373,482/
10,605,222
134,998/
156,402
5.29/
5.99
de/en 3,894,099
52,211,730/
58,544,608
1,146,262/
487,850
13.41/
15.03
fr/en 4,579,533
77,866,237/
68,429,649
495,856/
556,587
17.00/
14.94
General-
domain 
Parallel Data
cs/en 12,426,374
180,349,215/
183,841,805
1,614,023/
1,661,830
14.51/
14.79
de/en 4,421,961
106,001,775/
112,294,414
1,912,953/
919,046
23.97/
25.39
fr/en 36,342,530
1,131,027,766/
953,644,980
3,149,336/
3,324,481
31.12/
26.24
In-domain 
Mono. Data
cs 106,548 1,779,677 150,672 16.70
fr 1,424,539 53,839,928 644,484 37.79
de 2,222,502 53,840,304 1,415,202 24.23
en 7,802,610 199430649 1,709,594 25.56
General-
domain 
Mono. Data
cs 33,408,340 567,174,266 3,431,946 16.98
fr 30,850,165 780,965,861 2,142,470 25.31
de 84,633,641 1,548,187,668 10,726,992 18.29
en 85,254,788 2,033,096,800 4,488,816 23.85
Web-crawled 
In-domain 
Mono. Data
en 8,448,566 280,211,580 3,047,758 33.16 26 1,601
cs 44,198 1,280,326 137,179 28.96 4 388
de 473,171 14,087,687 728,652 29.77 17 968
fr 852,036 35,339,445 718,141 41.47 10 683
Table 2: Statistics summary of corpora after pre-processing.
236
 ={0, 25, 50, 75, 100} that represents the per-
centages of crawled documents we used for 
training the LMs. In Table 4, we show the abso-
lute BLEU scores of the evaluated systems, listed 
with the optimized thresholds, and the relative 
improvements (?%) in compared to the baseline 
system. The size of additional training data (for 
LM) is displayed at the last column.
Lang. 
Pair
Docs
( %)
BLEU
? 
(%)
Sent.
en-cs 50 17.59 0.11 31,065 
en-de 75 18.52 2.83 435,547 
en-fr 50 39.08 1.88 743,735 
cs-en 75 32.22 2.97 7,943,931
de-en 25 33.50 3.08 4,951,189
fr-en 100 45.45 2.46 8,448,566
Table 4: Evaluation results for systems that 
trained on relevance-score-filtered documents.
The relevance score filtering approach yields 
an improvement of 3.08% of BLEU score for de-
en pair that is the best result among the language 
pairs. On the other hand, en-cs pair obtains a 
marginal gain. The reason is very obvious that 
the training data is very insufficient. Empirical 
results of all language pairs expect fr-en indicate
that data filtering is the necessity to improve the 
system performance.
5.3 Based on Moore-Lewis Filtering
In this approach, we need to determine the values 
of two parameters, top  documents and top  
sentences, where  ={100, 75, 50} and  ={75, 
50, 25},    . When  =100, it is a conven-
tional perplexity-based data selection method, i.e. 
no document will be filtered. Table 5 shows the 
combination of different  and  that gives the 
best translation score for each language pair. We 
provide the absolute BLEU for each system, to-
gether with relative improvements (?%) that 
compared to the baseline system.
Lang.  
Pair
Docs
( %)
Target 
Size ( %)
BLEU ? (%)
en-cs 50 25 17.69 0.68
en-de 100 50 18.03 0.11
en-fr 100 50 38.73 0.96
cs-en 100 25 32.20 2.91
de-en 100 25 33.10 1.85
fr-en 100 25 45.22 1.94
Table 5: Evaluation results for systems that 
trained on combination filtering approach.
In this shared task, we have a quality and 
quantity in-domain monolingual training data for 
English. All the systems that take English as the 
target translation always outperform the other
reverse pairs. Besides, we found the systems 
based on the perplexity data selection method
tend to achieve a better scores in BLEU.
6 Official Results and Conclusions
We described our study on developing uncon-
strained systems in the medical translation task 
of 2014 Workshop on Statistical Machine Trans-
lation. In this work, we adopt the web crawling 
strategy for acquiring the in-domain monolingual 
data.  In detection the domain data, we exploited 
Moore-Lewis data selection method to filter the 
collected data in addition to the build-in scoring 
model provided by the crawler toolkit. However, 
after investigation, we found that the two meth-
ods are very competitive to each other.
The systems we submitted to the shared task 
were built using the language models and trans-
lation models that yield the best results in the 
individual testing. The official test set is convert-
ed into the recased and detokenized SGML for-
mat. Table 9 presents the official results of our 
submissions for every language pair.
Lang. 
Pair
BLEU of Combined 
systems
Official 
BLEU
en-cs 23.16 (+5.59) 22.10
cs-en 36.8 (+5.51) 37.40
en-fr 40.34 (+1.98) 40.80
fr-en 45.79 (+1.43) 43.80
en-de 19.36 (+1.35) 18.80
de-en 34.17 (+1.67) 32.70
Table 6: BLEU scores of the submitted systems 
for the medical translation task in six language 
pairs.
Acknowledgments
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for their research, 
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS.
References 
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 
2011. Domain adaptation via pseudo in-domain da-
ta selection. In Proceedings of EMNLP, pages 355-
362.
237
K. Duh, G. Neubig, K. Sudoh, H. Tsukada. 2013. Ad-
aptation data selection using neural language mod-
els: Experiments in machine translation. In Pro-
ceedings of the Annual Meeting of the Association 
for Computational Linguistics, pages, 678?683.
M. Espl?-Gomis and M. L. Forcada. 2010. Combining 
Content-Based and URL-Based Heuristics toHar-
vest Aligned Bitexts from Multilingual Sites with 
Bitextor. The Prague Bulletin of Mathemathical 
Lingustics, 93:77?86.
Qin Gao and Stephan Vogel. 2008. Parallel Imple-
mentations of Word Alignment Tool. Software En-
gineering, Testing, and Quality Assurance for Nat-
ural Language Processing, pp. 49-57.
Kenneth Heafield. 2011. KenLM: Faster and smaller 
language model queries. In Proceedings of the 
Sixth Workshop on Statistical Machine Translation, 
pages 187-197.
Papineni, Kishore, Salim Roukos, ToddWard, and-
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In 40th Annu-
al Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 311?318, Philadelphia, 
USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran et al. 
2007. Moses: Open source toolkit for statistical 
machine translation. In Proceedings of ACL, pages
177-180.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine 
translation. In Proceedings of the 2nd ACL Work-
shop on Statistical Machine Translation, pages 
224-227.
Christian Kohlsch?tter, Peter Fankhauser, and Wolf-
gang Nejdl. 2010. Boilerplate detection using shal-
low text features. In Proceedings of the 3rd ACM
International Conference on Web Search and Data
Mining, pages 441-450.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In 
Proceedings of ACL: Short Papers, pages 220-224.
Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. Proceedings of 
ACL, pp. 160-167.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, 29:19-51.
P. Pecina, A. Toral, A. Way, V. Papavassiliou, P. 
Prokopidis, and M. Giagkou. 2011. Towards Using 
WebCrawled Data for Domain Adaptation in Sta-
tistical Machine Translation. In Proceedings of the 
15th Annual Conference of the European Associta-
tion for Machine Translation, pages 297-304.
P. Pecina, A. Toral, V. Papavassiliou, P. Prokopidis, J. 
van Genabith,  and R. I. C. Athena. 2012. Domain 
adaptation of statistical machine translation using 
web-crawled resources: a case study. In Proceed-
ings of the 16th Annual Conference of the Europe-
an Association for Machine Translation, pp. 145-
152.
P. Pecina, O. Du?ek, L. Goeuriot, J. Haji?, J. Hla-
v??ov?, G. J. Jones, and Z. Ure?ov?. 2014. Adapta-
tion of machine translation for multilingual infor-
mation retrieval in the medical domain. Artificial 
intelligence in medicine, pages 1-25.
Philip Resnik and Noah A. Smith. 2003. The Web as 
a parallel corpus. Computational Linguistics, 
29:349?380
Raphael Rubino, Antonio Toral, Santiago Cort?s 
Va?llo, Jun Xie, Xiaofeng Wu, Stephen Doherty, 
and Qun Liu. 2013. The CNGL-DCU-Prompsit 
translation systems for WMT13. In Proceedings of 
the Eighth Workshop on Statistical Machine Trans-
lation, pages 213-218.
S?tre Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiro Matsubayashi and Tomo-
ko Ohta. 2007. AKANE System: Protein-Protein 
Interaction Pairs in BioCreAtIvE2 Challenge, PPI-
IPS subtask. In Proceedings of the Second BioCre-
ative Challenge Evaluation Workshop, pp. 209-212.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. Proceedings of the Inter-
national Conference on Spoken Language Pro-
cessing, pp. 901-904.
Antonio Toral. 2013. Hybrid selection of language 
model training data using linguistic information 
and perplexity. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation.
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi Lu, 
and Junwen Xing. 2014. A Systematic Com-
parison of Data Selection Criteria for SMT Domain 
Adaptation. The Scientific World Journal, vol. 
2014, Article ID 745485, 10 pages.
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi Lu, 
Junwen Xing. 2013. iCPE: A Hybrid Data Selec-
tion Model for SMT Domain Adaptation. Chinese 
Computational Linguistics and Natural Language 
Processing Based on Naturally Annotated Big Da-
ta. Springer Berlin Heidelberg. pages, 280-290
238
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 254?259,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Combining Domain Adaptation Approaches for Medical Text Transla-
tion 
 
Longyue Wang, Yi Lu, Derek F. Wong, Lidia S. Chao, Yiming Wang, Francisco Oliveira 
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory, 
Department of Computer and Information Science, 
University of Macau, Macau, China 
vincentwang0229@hotmail.com,  
{mb25435, derekfw, lidiasc, mb25433, olifran}@umac.mo 
 
 
Abstract 
This paper explores a number of simple 
and effective techniques to adapt statisti-
cal machine translation (SMT) systems in 
the medical domain. Comparative exper-
iments are conducted on large corpora for 
six language pairs. We not only compare 
each adapted system with the baseline, 
but also combine them to further improve 
the domain-specific systems. Finally, we 
attend the WMT2014 medical summary 
sentence translation constrained task and 
our systems achieve the best BLEU 
scores for Czech-English, English-
German, French-English language pairs 
and the second best BLEU scores for re-
minding pairs. 
 
1. Introduction 
This paper presents the experiments conducted 
by the NLP2CT Laboratory at the University of 
Macau for WMT2014 medical sentence transla-
tion task on six language pairs: Czech-English 
(cs-en), French-English (fr-en), German-English 
(de-en) and the reverse direction pairs, i.e., en-cs, 
en-fr and en-de.  
By comparing the medical text with common 
text, we discovered some interesting phenomena 
in medical genre. We apply domain-specific 
techniques in data pre-processing, language 
model adaptation, translation model adaptation, 
numeric and hyphenated words translation.  
Compared to the baseline systems (detailed in 
Section 2 & 3), the results of each method show 
reasonable gains. We combine individual ap-
proach to further improve the performance of our 
systems. To validate the robustness and lan-
guage-independency of individual and combined 
systems, we conduct experiments on the official 
training data (detailed in Section 3) in all six lan-
guage pairs. We anticipate the numeric compari-
son (BLEU scores) on these individual and com-
bined domain adaptation approaches that could 
be valuable for others on building a real-life do-
main-specific system. 
The reminder of this paper is organized as fol-
lows. In Section 2, we detail the configurations 
of our experiments as well as the baseline sys-
tems. Section 3 presents the specific pre-
processing for medical data. In Section 4 and 5, 
we describe the language model (LM) and trans-
lation model (TM) adaptation, respectively. Be-
sides, the techniques for numeric and hyphenated 
words translation are reported in Section 6 and 7. 
Finally, the performance of design systems and 
the official results are reported in Section 8. 
2. Experimental Setup 
All available training data from both WMT2014 
standard translation task1 (general-domain data) 
and medical translation task 2  (in-domain data) 
are used in this study. The official medical sum-
mary development sets (dev) are used for tuning 
and evaluating all the comparative systems. The 
official medical summary test sets (test) are only 
used in our final submitted systems.  
The experiments were carried out with the 
Moses 1.03 (Koehn et al., 2007). The translation 
and the re-ordering model utilizes the ?grow-
diag-final? symmetrized word-to-word align-
ments created with MGIZA++4 (Och and Ney, 
                                                 
1 http://www.statmt.org/wmt14/translation-task.html. 
2 http://www.statmt.org/wmt14/medical-task/. 
3 http://www.statmt.org/moses/. 
4 http://www.kyloo.net/software/doku.php/mgiza:overview. 
254
2003; Gao and Vogel, 2008) and the training 
scripts from Moses. A 5-gram LM was trained 
using the SRILM toolkit5 (Stolcke et al., 2002), 
exploiting improved modified Kneser-Ney 
smoothing, and quantizing both probabilities and 
back-off weights. For the log-linear model train-
ing, we take the minimum-error-rate training 
(MERT) method as described in (Och, 2003). 
3. Task Oriented Pre-processing 
A careful pre-processing on training data is sig-
nificant for building a real-life SMT system. In 
addition to the general data preparing steps used 
for constructing the baseline system, we intro-
duce some extra steps to pre-process the training 
data. 
The first step is to remove the duplicate sen-
tences. In data-driven methods, the more fre-
quent a term occurs, the higher probability it bi-
ases. Duplicate data may lead to unpredicted be-
havior during the decoding. Therefore, we keep 
only the distinct sentences in monolingual cor-
pus. By taking into account multiple translations 
in parallel corpus, we remove the duplicate sen-
tence pairs. The second concern in pre-
processing is symbol normalization. Due to the 
nature of medical genre, symbols such as num-
bers and punctuations are commonly-used to pre-
sent chemical formula, measuring unit, terminol-
ogy and expression. Fig. 1 shows the examples 
of this case. These symbols are more frequent in 
medical article than that in the common texts. 
Besides, the punctuations of apostrophe and sin-
gle quotation are interchangeably used in French 
text, e.g. ?l?effet de l'inhibition?. We unify it by 
replacing with the apostrophe. In addition, we 
observe that some monolingual training subsets 
(e.g., Gene Regulation Event Corpus) contain 
sentences of more than 3,000 words in length. To 
avoid the long sentences from harming the true-
case model, we split them into sentences with a 
sentence splitter6 (Rune et al., 2007) that is opti-
mized for biomedical texts. On the other hand, 
we consider the target system is intended for 
summary translation, the sentences tend to be 
short in length. For instance, the average sen-
tence lengths in development sets of cs, fr, de 
and en are around 15, 21, 17 and 18, respective-
ly. We remove sentence pairs which are more 
than 80 words at length. In order to that our ex-
periments are reproducible, we give the detailed 
                                                 
5 http://www.speech.sri.com/projects/srilm/. 
6 http://www.nactem.ac.uk/y-matsu/geniass/. 
statistics of task oriented pre-processed training 
data in Table 2. 
1,25-OH 
47 to 80% 
10-20 ml/kg 
A&E department 
Infective endocarditis (IE) 
Figure 1. Examples of the segments with sym-
bols in medical texts. 
To validate the effectiveness of the pre-
processing, we compare the SMT systems 
trained on original data 7 (Baseline1) and task-
oriented-processed data (Baseline2), respective-
ly. Table 1 shows the results of the baseline sys-
tems. We found all the Baseline2 systems outper-
form the Baseline1 models, showing that the sys-
tems can benefit from using the processed data. 
For cs-en and en-cs pairs, the BLEU scores im-
prove quite a lot. For other language pairs, the 
translation quality improves slightly.  
By analyzing the Baseline2 results (in Table 1) 
and the statistics of training corpora (in Table 2), 
we can further elaborate and explain the results. 
The en-cs system performs poorly, because of 
the short average length of training sentences, as 
well as the limited size of in-domain parallel and 
monolingual corpora. On the other hand, the fr-
en system achieves the best translation score, as 
we have sufficient training data. The translation 
quality of cs-en, en-fr, fr-en and de-en pairs is 
much higher than those in the other pairs. Hence, 
Baseline2 will be used in the subsequent compar-
isons with the proposed systems described in 
Section 4, 5, 6 and 7. 
Lang. Pair Baseline1 Baseline2 Diff. 
en-cs 12.92 17.57 +4.65 
cs-en 20.85 31.29 +10.44 
en-fr 38.31 38.36 +0.05 
fr-en 44.27 44.36 +0.09 
en-de 17.81 18.01 +0.20 
de-en 32.34 32.50 +0.16 
Table 1: BLEU scores of two baseline systems 
trained on original and processed corpora for 
different language pairs. 
4. Language Model Adaptation 
The use of LMs (trained on large data) during 
decoding is aided by more efficient storage and 
inference (Heafield, 2011). Therefore, we not 
                                                 
7 Data are processed according to Moses baseline tutorial: 
http://www.statmt.org/moses/?n=Moses.Baseline. 
255
Data Set Lang. Sent. Words Vocab. Ave. Len. 
In-domain  
Parallel Data 
cs/en 1,770,421 
9,373,482/ 
10,605,222 
134,998/ 
156,402 
5.29/ 
5.99 
de/en 3,894,099 
52,211,730/ 
58,544,608 
1,146,262/ 
487,850 
13.41/ 
15.03 
fr/en 4,579,533 
77,866,237/ 
68,429,649 
495,856/ 
556,587 
17.00/ 
14.94 
General-domain  
Parallel Data 
cs/en 12,426,374 
180,349,215/ 
183,841,805 
1,614,023/ 
1,661,830 
14.51/ 
14.79 
de/en 4,421,961 
106,001,775/ 
112,294,414 
1,912,953/ 
919,046 
23.97/ 
25.39 
fr/en 36,342,530 
1,131,027,766/ 
953,644,980 
3,149,336/ 
3,324,481 
31.12/ 
26.24 
In-domain  
Mono. Data 
cs 106,548 1,779,677 150,672 16.70 
fr 1,424,539 53,839,928 644,484 37.79 
de 2,222,502 53,840,304 1,415,202 24.23 
en 7,802,610 199430649 1,709,594 25.56 
General-domain  
Mono. Data 
cs 33,408,340 567,174,266 3,431,946 16.98 
fr 30,850,165 780,965,861 2,142,470 25.31 
de 84,633,641 1,548,187,668 10,726,992 18.29 
en 85,254,788 2,033,096,800 4,488,816 23.85 
Table 2: Statistics summary of corpora after pre-processing. 
only use the in-domain training data, but also the 
selected pseudo in-domain data 8  from general-
domain corpus to enhance the LMs (Toral, 2013; 
Rubino et al., 2013; Duh et al., 2013). Firstly, 
each sentence s in general-domain monolingual 
corpus is scored using the cross-entropy differ-
ence method in (Moore and Lewis, 2010), which 
is calculated as follows: 
 ( ) ( ) ( )I Gscore s H s H s? ? (1) 
where H(s) is the length-normalized cross-
entropy. I and G are the in-domain and general-
domain corpora, respectively. G is a random sub-
set (same size as the I) of the general-domain 
corpus. Then top N percentages of ranked data 
sentences are selected as a pseudo in-domain 
subset to train an additional LM. Finally, we lin-
early interpolate the additional LM with in-
domain LM.  
We use the top N% of ranked results, where 
N={0, 25, 50, 75, 100} percentages of sentences 
out of the general corpus. Table 3 shows the ab-
solute BLEU points for Baseline2 (N=0), while 
the LM adapted systems are listed with values 
relative to the Baseline2. The results indicate that 
LM adaptation can gain a reasonable improve-
ment if the LMs are trained on more relevant 
data for each pair, instead of using the whole 
training data. For different systems, their BLEU 
                                                 
8 Axelrod et al. (2011) names the selected data as pseudo in-
domain data. We adopt both terminologies in this paper. 
scores peak at different values of N. It gives the 
best results for cs-en, en-fr and de-en pairs when 
N=25, en-cs and en-de pairs when N=50, and fr-
en pair when N=75. Among them, en-cs and en-
fr achieve the highest BLEU scores. The reason 
is that their original monolingual (in-domain) 
data for training the LMs are not sufficient. 
When introducing the extra pseudo in-domain 
data, the systems improve the translation quality 
by around 2 BLEU points. While for cs-en, fr-en 
and de-en pairs, the gains are small. However, it 
can still achieve a significant improvement of 
0.60 up to 1.12 BLEU points. 
Lang. N=0 N=25 N=50 N=75 N=100 
en-cs 17.57 +1.66 +2.08 +1.72 +2.04 
cs-en 31.29 +0.94 +0.60 +0.66 +0.47 
en-fr 38.36 +1.82 +1.66 +1.60 +0.08 
fr-en 44.36 +0.91 +1.09 +1.12 +0.92 
en-de 18.01 +0.57 +1.02 -4.48 -4.54 
de-en 32.50 +0.60 +0.50 +0.56 +0.38 
Table 3: BLEU scores of LM adapted systems. 
5. Translation Model Adaptation 
As shown in Table 2, general-domain parallel 
corpora are around 1 to 7 times larger than the 
in-domain ones. We suspect if general-domain 
corpus is broad enough to cover some in-domain 
sentences. To observe the domain-specificity of 
general-domain corpus, we firstly evaluate sys-
tems trained on general-domain corpora. In Ta-
256
ble 4, we show the BLEU scores of general-
domain systems9 on translating the medical sen-
tences. The BLEU scores of the compared sys-
tems are relative to the Baseline2 and the size of 
the used general-domain corpus is relative to the 
corresponding in-domain one. For en-cs, cs-en, 
en-fr and fr-en pairs, the general-domain parallel 
corpora we used are 6 times larger than the orig-
inal ones and we obtain the improved BLEU 
scores by 1.72 up to 3.96 points. While for en-de 
and de-en pairs, the performance drops sharply 
due to the limited training corpus we used. 
Hence we can draw a conclusion: the general-
domain corpus is able to aid the domain-specific 
translation task if the general-domain data is 
large and broad enough in content.  
Lang. Pair BLEU Diff. Corpus 
en-cs 21.53 +3.96 
+601.89% 
cs-en 33.01 +1.72 
en-fr 41.57 +3.21 
+693.59% 
fr-en 47.33 +2.97 
en-de 16.54 -1.47 
+13.63% 
de-en 27.35 -5.15 
Table 4: The BLEU scores of systems trained on 
general-domain corpora. 
Taking into account the performance of gen-
eral-domain system, we explore various data se-
lection methods to derive the pseudo in-domain 
sentence pairs from general-domain parallel cor-
pus for enhancing the TMs (Wang et al., 2013; 
Wang et al., 2014). Firstly, sentence pair in cor-
responding general-domain corpora is scored by 
the modified Moore-Lewis (Axelrod et al., 
2011), which is calculated as follows: 
 ? ?
g g
( ) ( ) ( )
( ) ( )
I src G src
I t t G t t
score s H s H s
H s H s
? ?
? ?
? ?
? ?? ?? ?
 (2) 
which is similar to Eq. (1) and the only differ-
ence is that it considers the both the source (src) 
and target (tgt) sides of parallel corpora. Then 
top N percentage of ranked sentence pairs are 
selected as a pseudo in-domain subset to train an 
individual translation model. The additional 
model is log-linearly interpolated with the in-
domain model (Baseline2) using the multi-
decoding method described in (Koehn and 
Schroeder, 2007). 
Similar to LM adaptation, we use the top N% 
of ranked results, where N={0, 25, 50, 75, 100} 
percentages of sentences out of the general cor-
                                                 
9  General-domain systems are trained only on genera-
domain training corpora (i.e., parallel, monolingual). 
pus. Table 5 shows the absolute BLEU points for 
Baseline2 (N=0), while for the TM adapted sys-
tems we show the values relative to the Base-
line2. For different systems, their BLEU peak at 
different N. For en-fr and en-de pairs, it gives the 
best translation results at N=25. Regarding cs-en 
and fr-en pairs, the optimal performance is 
peaked at N=50. While the best results for de-en 
and en-cs pairs are N=75 and N=100 respective-
ly. Besides, performance of TM adapted system 
heavily depends on the size and (domain) broad-
ness of the general-domain data. For example, 
the improvements of en-de and de-en systems are 
slight due to the small general-domain corpora. 
While the quality of other systems improve about 
3 BLEU points, because of their large and broad 
general-domain corpora.  
Lang. N=0 N=25 N=50 N=75 N=100 
en-cs 17.57 +0.84 +1.53 +1.74 +2.55 
cs-en 31.29 +2.03 +3.12 +3.12 +2.24 
en-fr 38.36 +3.87 +3.66 +3.53 +2.88 
fr-en 44.36 +1.29 +3.36 +1.84 +1.65 
en-de 18.01 +0.02 -0.13 -0.07 0 
de-en 32.50 -0.12 +0.06 +0.31 +0.24 
Table 5: BLEU scores of TM adapted systems. 
6. Numeric Adaptation 
As stated in Section 3, numeric occurs frequently 
in medical texts. However, numeric expression in 
dates, time, measuring unit, chemical formula are 
often sparse, which may lead to OOV problems 
in phrasal translation and reordering. Replacing 
the sparse numbers with placeholders may pro-
duce more reliable statistics for the MT models.  
Moses has support using placeholders in train-
ing and decoding. Firstly, we replace all the 
numbers in monolingual and parallel training 
corpus with a common symbol (a sample phrase 
is illustrated in Fig. 2). Models are then trained 
on these processed data. We use the XML 
markup translation method for decoding.  
Original: Vitamin D 1,25-OH  
Replaced: Vitamin D @num@, @num@-OH 
Figure 2. Examples of placeholders. 
Table 6 shows the results on this number ad-
aptation approach as well as the improvements 
compared to the Baseline2. The method im-
proves the Baseline2 systems by 0.23 to 0.40 
BLEU scores. Although the scores increase 
slightly, we still believe this adaptation method is 
significant for medical domain. The WMT2014 
medical task only focuses on the summary of 
257
medical text, which may contain fewer chemical 
expression in compared with the full article. As 
the used of numerical instances increases, place-
holder may play a more important role in domain 
adaptation.  
Lang. Pair BLEU (Dev) Diff. 
en-cs 17.80 +0.23 
cs-en 31.52 +0.23 
en-fr 38.72 +0.36 
fr-en 44.69 +0.33 
en-de 18.41 +0.40 
de-en 32.88 +0.38 
Table 6: BLEU scores of numeric adapted sys-
tems. 
7. Hyphenated Word Adaptation 
Medical texts prefer a kind of compound words, 
hyphenated words, which is composed of more 
than one word. For instance, ?slow-growing? and 
?easy-to-use? are composed of words and linked 
with hyphens. These hyphenated words occur 
quite frequently in medical texts. We analyze the 
development sets of cs, fr, en and de respective-
ly, and observe that there are approximately 
3.2%, 11.6%, 12.4% and 19.2% of sentences that 
contain one or more hyphenated words. The high 
ratio of such compound words results in Out-Of-
Vocabulary words (OOV) 10 , and harms the 
phrasal translation and reordering. However, a 
number of those hyphenated words still have 
chance to be translated, although it is not precise-
ly, when they are tokenized into individual 
words.  
Algorithm: Alternative-translation Method 
Input: 
1. A sentence, s, with M hyphenated words 
2. Translation lexicon 
Run: 
1. For i = 1, 2, ?, M 
2.   Split the ith hyphenated word (Ci) into 
Pi 
3.   Translate  Pi into Ti 
4.   If (Ti are not OOVs): 
5.      Put alternative translation Ti in XML 
6.    Else: keep Ci unchanged 
Output: 
Sentence, s?, embedded with alternative 
translations for all Ti. 
End 
Table 7: Alternative-translation algorithm. 
                                                 
10 Default tokenizer does not handle the hyphenated words. 
To resolve this problem, we present an alter-
native-translation method in decoding. Table 7 
shows the proposed algorithm. 
In the implementation, we apply XML markup 
to record the translation (terminology) for each 
compound word. During the decoding, a hyphen-
ated word delimited with markup will be re-
placed with its corresponding translation. Table 8 
shows the BLEU scores of adapted systems ap-
plied to hyphenated translation. This method is 
effective for most language pairs. While the 
translation systems for en-cs and cs-en do not 
benefit from this adaptation, because the hy-
phenated words ratio in the en and cs dev are 
asymmetric. Thus, we only apply this method for 
en-fr, fr-en, de-en and en-de pairs. 
Lang. Pair BLEU (Dev) Diff. 
en-cs 16.84 -0.73 
cs-en 31.23 -0.06 
en-fr 39.12 +0.76 
fr-en 45.02 +0.66 
en-de 18.64 +0.63 
de-en 33.01 +0.51 
Table 8: BLEU scores of hyphenated word 
adapted systems. 
3. Final Results and Conclusions 
According to the performance of each individual 
domain adaptation approach, we combined the 
corresponding models for each language pair. In 
Table 10, we show the BLEU scores and its in-
crements (compared to the Baseline2) of com-
bined systems in the second column. The official 
test set is converted into the recased and deto-
kenized SGML format. The official results of our 
submissions are given in the last column of Table 
9. 
Lang. 
Pair 
BLEU of Com-
bined systems 
Official 
BLEU 
en-cs 23.66 (+6.09) 22.60 
cs-en 38.05 (+6.76) 37.60 
en-fr 42.30 (+3.94) 41.20 
fr-en 48.25 (+3.89) 47.10 
en-de 21.14 (+3.13) 20.90 
de-en 36.03 (+3.53) 35.70 
Table 9: BLEU scores of the submitted systems 
for the medical translation task. 
This paper presents a set of experiments con-
ducted on all available training data for six lan-
guage pairs. We explored various domain adap-
tation approaches for adapting medical transla-
258
tion systems. Compared with other methods, lan-
guage model adaptation and translation model 
adaptation are more effective. Other adapted 
techniques are still necessary and important for 
building a real-life system. Although all individ-
ual methods are not fully additive, combining 
them together can further boost the performance 
of the overall domain-specific system. We be-
lieve these empirical approaches could be valua-
ble for SMT development. 
Acknowledgments 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for their research, 
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS. 
The authors also wish to thank the colleagues in 
CNGL, Dublin City University (DCU) for their 
helpful suggestion and guidance on related work. 
Reference 
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 
2011. Domain adaptation via pseudo in-domain da-
ta selection. In Proceedings of EMNLP, pages 355-
362. 
K. Duh, G. Neubig, K. Sudoh, H. Tsukada. 2013. Ad-
aptation data selection using neural language mod-
els: Experiments in machine translation. In Pro-
ceedings of the Annual Meeting of the Association 
for Computational Linguistics, pages, 678?683. 
Qin Gao and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49-57. 
Kenneth Heafield. 2011. KenLM: Faster and smaller 
language model queries. In Proceedings of the 
Sixth Workshop on Statistical Machine Transla-
tion, pages 187-197. 
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine 
translation. In Proceedings of the 2nd ACL Work-
shop on Statistical Machine Translation, pages 
224-227. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine Moran 
et al. 2007. Moses: open source toolkit for statisti-
cal machine translation. In Proceedings of ACL, 
pages 177-180. 
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In 
Proceedings of ACL: Short Papers, pages 220-224. 
S?tre Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiro Matsubayashi and Tomo-
ko Ohta. 2007. AKANE system: protein-protein in-
teraction pairs in BioCreAtIvE2 challenge, PPI-IPS 
subtask. In Proceedings of the Second BioCreative 
Challenge Evaluation Workshop, pages 209-212.  
Raphael Rubino, Antonio Toral, Santiago Cort?s 
Va?llo, Jun Xie, Xiaofeng Wu, Stephen Doherty, 
and Qun Liu. 2013. The CNGL-DCU-Prompsit 
translation systems for WMT13. In Proceedings of 
the Eighth Workshop on Statistical Machine Trans-
lation, pages 213-218. 
Andreas Stolcke and others. 2002. SRILM-An exten-
sible language modeling toolkit. In Proceedings of 
the International Conference on Spoken Language 
Processing, pages 901-904. 
Antonio Toral. 2013. Hybrid selection of language 
model training data using linguistic information 
and perplexity. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation. 
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, 29:19-51. 
Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. In Proceedings of 
the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160-167. 
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi 
Lu, and Junwen Xing. 2014 ?A Systematic Com-
parison of Data Selection Criteria for SMT Domain 
Adaptation,? The Scientific World Journal, vol. 
2014, Article ID 745485, 10 pages. 
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi 
Lu, Junwen Xing. 2013. iCPE: A Hybrid Data Se-
lection Model for SMT Domain Adaptation. Chi-
nese Computational Linguistics and Natural Lan-
guage Processing Based on Naturally Annotated 
Big Data. Springer Berlin Heidelberg. pages, 280-
290. 
 
259
