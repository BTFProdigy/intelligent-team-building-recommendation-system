Left-To-Right Parsing 
and Bilexical Context-Free Grammars 
Mark- Jan  Nederhof  
DFK I  
Stuhlsatzenhausweg 3 
D-66123 Saarbrficken 
Germany 
nederhofOdfk?,  de 
Giorgio Sat ta  
Dipart imento di E lettronica e Informat ica 
Universit~ di Padova 
via Gradenigo,  6 /A  
1-35131 Padova, I ta ly  
satta~dei, unipd, it 
Abst rac t  
We compare the asymptotic time complexity of 
left-to-right and bidirectional parsing techniques for 
bilexical context-free grammars, a grammar formal- 
ism that is an abstraction oflanguage models used in 
several state-of-the-art real-world parsers. We pro- 
vide evidence that left-to-right parsing cannot be re- 
alised within acceptable time-bounds if the so called 
correct-prefix property is to be ensured. Our evi- 
dence is based on complexity results for the repre- 
sentation of regular languages. 
1 Introduction 
Traditionally, algorithms for natural language pars- 
ing process the input string strictly from left to right. 
In contrast, several algorithms have been proposed 
in the literature that process the input in a bidi- 
rectional fashion; see (van Noord, 1997; Satta and 
Stock, 1994) and references therein. The issue of 
parsing efficiency for left-to-right vs. bidirectional 
methods has longly been debated. On the basis of 
experimental results, it has been argued that the 
choice of the most favourable strategy should depend 
on the grammar at hand. With respect o grammar 
formalisms based upon context-free grammars, and 
when the rules of these formalisms strongly depend 
on lexical information, (van Noord, 1997) shows that 
bidirectional strategies are more efficient than left- 
to-right strategies. This is because bidirectional 
strategies are most effective in reducing the parsing 
search space, by activating as early as possible the 
maximum number of lexical constraints available in 
the grammar. 
In this paper we present mathematical arguments 
in support of the above empirically motivated the- 
sis. We investigate a class of lexicalized grammars 
that, in their probabilistic versions, have been widely 
adopted as language models in state-of-the-art real- 
world parsers. The size of these grammars usually 
grows with the square of the size of the working lex- 
icon, and thus can be very large. In these cases, the 
primary goal in the design of a parsing algorithm 
is to achieve asymptotic time performance sublinear 
in the size of the working grammar and indepen- 
dent of the size of the lexicon. These desiderata are 
met by existing bidirectional algorithms (Alshawi, 
1996; Eisner, 1997; Eisner and Satta, 1999). In con- 
trast, we show the following two main results for 
the asymptotic time performance of left-to-right al- 
gorithms atisfying the so called correct-prefix prop- 
erty. 
? In case off-line compilation of the working ram- 
mar is not allowed, left-to-right parsing cannot 
be realised within time bounds independent of
the size of the lexicon. 
? In case polynomial-time, off-line compilation of 
the working grammar is allowed, left-to-right 
parsing cannot be realised in polynomial time, 
and independently of the size of the lexicon, un- 
less a strong conjecture based on complexity re- 
sults for the representation f regular languages 
is falsified. 
The first result implies that the well known Earley 
algorithm and related standard parsing techniques 
that do not require grammar precompilation can- 
not be directly extended to process the above men- 
tioned grammars (resp. language models) within an 
acceptable time bound. The second result provides 
evidence that well known parsing techniques as left- 
corner parsing, requiring polynomial-time prepro- 
cessing of the grammar, also cannot be directly ex- 
tended to process these formalisms within an accept- 
able time bound. 
The grammar formalisms we investigate are based 
upon context-free grammars and are called bilex- 
ical context-free grammars. Bilexical context-free 
grammars have been presented in (Eisner and Satta, 
1999) as an abstraction oflanguage models that have 
been adopted in several recent real-world parsers, 
improving state-of-the-art parsing accuracy (A1- 
shawl, 1996; Eisner, 1996; Charniak, 1997; Collins, 
1997). Our results directly transfer to all these lan- 
guage models. In a bilexical context-free grammar, 
possible arguments of a word are always specified 
along with possible head words for those arguments. 
Therefore abilexical grammar requires the grammar 
writer to make stipulations about the compatibil- 
272 
ity of particular pairs of words in particular oles, 
something that was not necessarily true of general 
context-free grammars. 
The remainder of this paper is organized as fol- 
lows. We introduce bilexical context-free grammars 
in Section 2, and discuss parsing with the correct- 
prefix property in Section 3. Our results for parsing 
with on-line and off-line grammar compilation are 
presented in Sections 4 and 5, respectively. To com- 
plete the presentation, Appendix A shows that left- 
to-right parsing in time independent of the size of 
the lexicon is indeed possible when an off-line com- 
pilation of the working grammar is allowed that has 
an exponential time complexity. 
2 B i lex ica l  context - f ree  grammars  
In this section we introduce the grammar formalism 
we investigate in this paper. This formalism, origi- 
nally presented in (Eisner and Satta, 1999), is an ab- 
straction of the language models adopted by several 
state-of-the-art eal-world parsers (see Section 1). 
We specify a non-stochastic version of the formal- 
ism, noting that probabilities may be attached to 
the rewrite rules exactly as in stochastic CFG (Gon- 
zales and Thomason, 1978; Wetherell, 1980). We 
assume that the reader is familiar with context-free 
grammars. Here we follow the notation of (Harrison, 
1978; Hopcroft and Ullman, 1979). 
A context-free grammar (CFG) is a tuple G = 
(VN, VT, P, S), where VN and VT are finite, disjoint 
sets of nonterminal and terminal symbols, respec- 
tively, S E VN is the start symbol, and P is a finite 
set of productions having the form A -+ a, where 
A E VN and a E (VN t3 VT)*. A "derives" relation, 
written ::~, is associated with a CFG as usual. We 
use the reflexive and transitive closure of =~, writ- 
ten =~*, and define L(G) accordingly. The size of a 
CFG G is defined as IGI = ~(A--+a)EP \]Aal. If every 
production in P has the form A --+ BC or A --+ a, 
for A,B ,C  E VN,a E VT, then G is said to be in 
Chomsky Normal Form (CNF). 
A CFG G = (VN, VT, P, S\[$\]) in CNF is called a 
bilexical context - f ree  grammar  if there exists a 
set VD, called the set of delexical ized nontermi -  
nals, such that nonterminals from VN are of the form 
A\[a\], consisting of A E VD and a E VT, and every 
production in P has one of the following two forms: 
(i) A\[a\] ~ B\[b\] C\[c\], a E {b, c); 
(ii) A\[a\] ~ a. 
A nonterminal A\[a\] is said to have terminal symbol 
a as its lexlcal head. Note that in a parse tree for 
G, the lexical head of a nonterminal is always "in- 
herited" from some daughter symbol (i.e., from some 
symbol in the right-hand side of a production). In 
the sequel, we also refer to the set VT as the lexicon 
of the grammar. 
A bilexical CFG can encode lexically specific pref- 
erences in the form of binary relations on lexi- 
cal items. For instance, one might specify P as 
to contain the production VP\[solve\] -+ V\[solve\] 
NP\[puzzles\] but not the production VP\[eat\] --4 
V\[eat\] NP\[puzzles\]. This will allow derivation of 
some VP constituents such as "solve two puzzles", 
while forbidding "eat two puzzles". See (Eisner and 
Satta, 1999) for further discussion. 
The cost of this expressiveness is a very large 
grammar. Indeed, we have IGI = O(\]VD\[ 3" IVT\[2), 
and in practical applications \]VTI >> IVDI > 1. Thus, 
the grammar size is dominated in its growth by the 
square of the size of the working lexicon. Even if we 
conveniently group lexical items with distributional 
similarities into the same category, in practical ap- 
plications the resulting rammar might have several 
thousand productions. Parsing strategies that can- 
not work in sublinear time with respect o the size of 
the lexicon and with respect o the size of the whole 
input grammar are very inefficient in these cases. 
3 Cor rect -pre f ix  p roper ty  
So called left-to-right strategies are standa~dly 
adopted in algorithms for natural language pars- 
ing. Although intuitive, the notion of left-to-right 
parsing is a concept with no precise mathematical 
meaning. Note that in fact, in a pathological way, 
one could read the input string from left-to-right, 
storing it into some data structure, and then per- 
form syntactic analysis with a non-left-to-right strat- 
egy. In this paper we focus on a precise definition 
of left-to-right parsing, known in the literature as 
correct-prefix property parsing (Sippu and Soisalon- 
Soininen, 1990). Several algorithms commonly used 
in natural anguage parsing satisfy this property, as 
for instance Earley's algor!thm (Earley, 1970), tab- 
ular left-corner and PLR parsing (Nederhof, 1994) 
and tabular LR parsing (Tomita, 1986). 
Let VT be some alphabet. A generic string over VT 
is denoted as w = al " -an ,  with n _> 0 and ai E VT 
(1 < i < n); in case n = 0, w equals the empty 
string e. For integers i and j with 1 < i < j < n, we 
write w\[i,j\] to denote string aiai+l"  .aj; if i > j,  
we define w\[i, j\] = c. 
Let G -- (VN,VT,P,S) be a CFG and let w = 
al ... an with n _> 0 be some string over VT. A rec- 
ognizer  for the CFG class is an algorithm R that, 
on input (G,w), decides whether w E L(G). We 
say that R satisfies the cor rect -pre f ix  p roper ty  
(CPP) if the following condition holds. Algorithm 
R processes the input string from left-to-right, "con- 
suming" one symbol ai at a time. If for some i, 
0 < i < n, the set of derivations in G having the 
form S ~*  w\[1, i\]7, 7 E (VN U VT)*, is empty, then 
R rejects and halts, and it does so before consuming 
symbol ai+l, if i < n. In this case, we say that R 
273 
has detected an error at position i in w. Note that 
the above property forces the recognizer to do rele- 
vant computation for each terminal symbol that is 
consumed. 
We say that w\[1,i\] is a cor rect -pre f ix  for a lan- 
guage L if there exists a string z such that w\[1,i\]z E 
L. In the natural language parsing literature, the 
CPP is sometimes defined with the following condi- 
tion in place of the above. If for some i, 0 < i < n, 
w\[1, if is not a correct prefix for L(G), then R rejects 
and halts, and it does so before consuming symbol 
ai+i,~ if i < n. Note that the latter definition asks 
for a stronger condition, and the two definitions are 
equivalent only in case the input grammar G is re- 
duced, i While the above mentioned parsing algo- 
rithms satisfy the former definition of CPP, they do 
not satisfy the latter. Actually, we are not aware of 
any practically used parsing algorithm that satisfies 
the latter definition of CPP. 
One needs to distinguish CPP parsing from Some 
well known parsing algorithms in the literature that 
process ymbols in the right-hand sides of each gram- 
mar production from left to right, but that do not 
exhibit any left-to-right dependency between differ- 
ent productions. In particular, processing of the 
right-hand side of some production may be initi- 
ated at some input position without consultation of 
productions or parts of productions that may have 
been found to cover parts of the input to the left 
of that position. These algorithms may also consult 
input symbols from left to right, but the processing 
that takes place to the right of some position i does 
not strictly depend on the processing that has taken 
place to the left of i. Examples are pure bottom-up 
methods, such as left-corner parsing without top- 
down filtering (Wiren, 1987). 
Algorithms that do satisfy the CPP make use of 
some form of top-down prediction. Top-down pre- 
diction can be implemented at parse-time as in the 
case of Earley's algorithm by means of the "predic- 
tor" step, or can be precompiled, as in the case of 
left-corner parsing (Rosenkrantz and Lewis, 1970), 
by means of the left-corner relation, or as in the case 
of LR parsers (Sippu and Soisalon-Soininen, 1990), 
through the closure function used in the construc- 
tion of LR states. 
4 Recognit ion without 
precompilat ion 
In this section we consider recognition algorithms 
that do not require off-line compilation of the input 
grammar. Among algorithms that satisfy the CPP, 
the most popular example of a recognizer that does 
i A context-free grammar G is reduced if every nonterminal 
of G can be part of at least one derivation that rewrites the 
start symbol into some string of terminal symbols. 
not require grammar precompilation is perhaps Ear- 
ley's algorithm (Earley, 1970). We show here that 
methods in this family cannot be extended to work 
in time independent of the size of the lexicon, in 
contrast with bidirectional recognition algorithms. 
The result presented below rests on the follow- 
ing, quite obvious, assumption. There exists a con- 
stant c, depending on the underlying computation 
model, such that in k > 0 elementary computation 
steps any recognizer can only read up to c.  k pro- 
ductions from set P. In what follows, and without 
any loss of generality, we assume c = 1. Apart from 
this assumption, no other restriction is imposed on 
the representation f the input grammar or on the 
access to the elements of sets VN, VT and P. 
Theorem 1 Let f be any function of two variables 
defined on natural numbers. No recognizer for bilexi- 
cal context-free grammars that satisfies the CPP can 
run on input (G, wl in an amount of time bounded 
by f(\[VDI, \[W\[), where VD is the set of delexicatized 
nonterminals of G. 
Proof. Assume the existence of a recognizer R sat- 
isfying the CPP and running in I(IVDI, IwL) steps or 
less. We show how to derive a contradiction. 
Let q >_ 1 be an integer. Define a bilexical CFG 
G a = (Vr~ , V~, Pq, d\[bi\]) where V~ contains q + 2 
distinct symbols {bi,. . . ,bq+2} and 
V~ = {A\[b,\]l l<i<q+l}u{T\[b\]lbeV~}, 
and where set pa contains all and only the following 
productions: 
(i) A\[b,\] --+ A\[b,+i\] T\[b,\], 1 < i < q; 
(ii) A\[bq+i\] -+ T\[bq+2\] T\[bq+l\]; 
(iii) T\[b\] -+ b, b E V~. 
Productions in (i) are called bridging productions. 
Note that there are q bridging productions in Gq. 
Also, note that V~ = {A,T} does not depend on 
the choice of q. Thus, we will simply write VD. 
Choose q > max{f(IVD\[,2), l  }. On input 
(Gq, bq+2bq+i), R does not detect any error at posi- 
tion 1, that is after having read the first symbol bq+2 
of the input string. This is because A\[bl\] ~* bq+2~/ 
with 3' =- T\[ba+i\]T\[ba\]T\[bq- i \ ] '"T\[bi \]  is a valid 
derivation in G. Since R executes no more than 
f(IVD\] ,2) steps, from our assumption that reading 
a production takes unit time it follows that there 
must be an integer k, 1 < k < q, such that bridging 
production A\[bk\] --+ A\[bk+i\] T\[bk\] is not read from 
Gq. Construct hen a new grammar GI~ by replacing 
in Gq the production A\[bk\] --+ A\[bk+l\] T\[bk\] with 
the new production A\[bk\] --+ T\[bk\] A\[bk+i\], leaving 
everything else unchanged. It follows that, on in- 
put (G~, ba+2ba+i), R behaves exactly as before and 
does not detect any error at position 1. But this is 
274 
a contradiction, since there is no derivation in G~ of 
the form A\[bl\] =~* bq+2"Y, 7 E (VN U VT)*, as can be 
easily verified. ? 
We can use the above result in the comparison 
of left-to-right and bidirectional recognizers. The 
recognition of bilexical context-free languages can 
be carried out by existing bidirectional algorithms 
in time independent of the size of the lexicon and 
without any precompilation of the input bilexical 
grammar. For instance, the algorithms presented 
in (Eisner and Satta, 1999) allow recognition i  time 
O(IVDI 3 IwI4). 2 Theorem 1 states that this time 
bound cannot be met if we require the CPP and if 
the input grammar is not precompiled. In the next 
section, we will consider the possibility that the in- 
put grammar is in a precompiled form. 
5 Recogn i t ion  w i th  precompi la t ion  
In this section we consider recognition algorithms 
that satisfy the CPP and allow off-line, polynomial- 
time compilation of the working grammar. We focus 
on a class of bilexical context-free grammars where 
recognition requires the stacking of a number of un- 
resolved lexical dependencies that is proportional to 
the length of the input string. We provide evidence 
that the above class of recognizers perform much less 
efficiently for these grammars than existing bidirec- 
tional recognizers. 
We assume that the reader is familiar with the 
notions of deterministic and nondeterministic finite 
automata. We follow here the notation in (Hopcroft 
and Ullman, 1979). A nondeterministic finite au- 
tomaton (FA) is a tuple M = (Q, E, 5, q0, F), where 
Q and P. are finite, disjoint sets of state and alphabet 
symbols, respectively, qo E Q and F _C Q are the ini- 
tial state and the set of final states, respectively, and 
is a total function mapping Q x ~ to 2 Q, the power- 
set of Q. Function 5 represents he transitions of the 
automaton. Given a string w = al " "an ,  n > O, an 
accept ing  computat ion  in M for w is a sequence 
qo, a l ,q l ,a2,q2 . . . .  ,an,q,, such that qi E 5(q i - l ,a i )  
for 1 < i  < n, and q~ E F. The languageL(M) is 
the set of all strings in E* that admit at least one 
accepting computation in M. The size of M is de- 
fined as \]M\] = ~qeQ,ae~ I~(q,a)l. The automaton 
M is deterministic f, for every q E Q and a E ~, we 
have IS(q, a)\] = 1. 
We call quas i -determin izer  any algorithm A 
that satisfies the following two conditions: 
1. A takes as input a nondeterministic FA M --= 
(Q, ~, 5, qo, F) and produces as output a device 
DM that, when given a string w as input, de- 
cides whether w E L(M); and 
2More precisely, the running time for these algorithms is 
O(IVDI 3 Iw\[3min{\[VT\[, \[w\[}). In cases of practical interest, 
we always have Iw\[ < IVT\[. 
2. there exists a polynomial PA such that every 
DM runs in an amount of time bounded by 
PA(Iwl). 
We remark that, given a nondeterministic FA M 
specified as above, known algorithms allow simula- 
tion of M on an input string w in time O( IM I IwI) 
(see for instance (Aho et al, 1974, Thin. 9.5) 
or (Sippu and Soisalon-Soininen, 1988, Thm. 3.38)). 
In contrast, a quasi-determinizer produces a device 
that simulates M in an amount of time independent 
of the size of M itself. 
A standard example of a quasi-determinizer is the 
so called power-set construction, used to convert 
a nondeterministic FA into a language-equivalent 
deterministic FA (see for instance (Hopcroft and 
Ullman, 1979, Thin. 2.1) or (Sippu and Soisalon- 
Soininen, 1988, Thm. 3.30)). In fact, there exist 
constants c and d such that any deterministic FA 
can be simulated on input string w in an amount of 
time bounded by c \]w I + d. This requires function 
to be stored as a IQ\] x \]El, 2-dimensional rray with 
values in Q. This is a standard representation for 
automata-like structures; see (Gusfield, 1997, :Sect. 
6.5) for discussion. 
We now pose the question of the time efficiency 
of a quasi-determinizer, and consider the amount of 
time needed in the construction of DM. In (Meyer 
and Fisher, 1971; Stearns and Hunt, 1981) it is 
shown that there exist (infinitely many) nonde- 
terministic FAs with state set Q, such that any 
language-equivalent deterministic FA must have at 
least 2 IQ} states. This means that the power-set con- 
struction cannot work in polynomial time in the size 
of the input FA. Despite of much effort, no algo- 
rithm has been found, up to the authors' knowledge, 
that can simulate a nondeterministic FA on an input 
string w in linear time in' Iwl and independently of
IMI, if only polynomial-time precompilation of M 
is allowed. Even in case we relax the linear-time re- 
striction and consider ecognition of w in polynomial 
time, for some fixed polynomial, it seems unlikely 
that the problem can be solved if only polynomial- 
time precompilation of M is allowed. Furthermore, 
if we consider precompilation of nondeterministic 
FAs into "partially determinized" FAs that would 
allow recognition in polynomial (or even exponen- 
tial) time in Iw\], it seems unlikely that the analysis 
required for this precompilation could consider less 
than exponentially many combinations ofstates that 
may be active at the same time for the original non- 
deterministic FA. Finally, although more powerful 
formalisms have been shown to represent some regu- 
lar languages much more succinctly than FAs (Meyer 
and Fisher, 1971), while allowing polynomial-time 
parsing, it seem unlikely that this could hold for reg- 
ular languages in general. 
275 
Conjecture There is no quasi-determinizer that 
works in polynomial time in the size of the input 
automaton. 
Before turning to our main result, we need to 
develop some additional machinery. Let M = 
(Q,E,6, qo, F) be a nondeterministic FA and let 
w = a l . . -an  E L(M),  where n > 0. Let 
qo, al , ql , . . . , an, qn be an accepting computation for 
w in M, and choose some symbol $ ? E. We can 
now encode the accepting computation as 
($, q0)(al, ql) ? ? ? (an, qa) 
where we pair alphabet symbols to states, prepend- 
ing $ to make up for the difference in the number 
of alphabet symbols and states. We now provide 
a construction that associates M with a bilexical 
CFG GM. Strings in L(GM) are obtained by pair- 
ing strings in L(M)  with encodings of their accepting 
computations ( ee below for an example). 
Def in i t ion 1 Let M = (Q,E, J ,  qo,F) be a nonde- 
terministic FA. Choose two symbols $, # ~ E, and 
let A = {(a,q) I a e EU{$},  q ? O}. A bilexi- 
cat CFG GM -- (VN, VT, e~ C\[($, qo)\]) is specified as 
follows: 
(i) vN = {TIff I ~ ? VT} U {C\[~\],C'\[.\] I ~ ? a} ;  
(ii) V T = A U ~ U {}; 
(iii) P contains all and only the following produc- 
tions: 
(a) for each a ? VT, 
T\[a\] - ,  a; 
(b) for each (a,q),(a',q*) ? A such that q* ? 
5(q,a'), 
C\[(a, q)\] ~ C'\[(a', 4)\] T\[(a, q)\]; 
(C) for each (a,q) ? A, 
C'\[(a,q)\] ~ T\[a\] C\[(a,q)\]; 
(d) .for each (a, q) ? A such that q ? F, 
C\[(a, q)\] - ,  T\[#\] T\[(a, q)\]. 
We give an example of the above construc- 
tion. Consider an automaton M and a string 
w = ala2a3 such that w ? L(M).  Let 
($,qo)(al,ql)(a2,q2)(a3,q3) be the encoding of an 
accepting computation in M for w. Then the 
string ala2a3~(a3, q3)(a2, q2)(al, ql)($, qo) belongs 
to L(GM). The tree depicted in Figure I represents 
a derivation in GM of such a string. 
The following fact will be used below. 
Lemma 1 For each w ? E*, w# is a correct-prefix 
for L(GM) if and only if w ? L (M) .  
Outline of the proof. We claim the following 
fact. For each k > 0, a l ,a2 , . . . ,ak  ? ~ and 
qo, ql , . . . , qk ? Q we have 
qi ? 6(qi- l ,ai),  for all i (1 < i < k), 
c\[($,q0)l 
C'\[(al, ql)\] T\[($, q0)\] 
T\[a,\] C\[(al,ql)\] ($, q0) 
al C'\[(a2, q2)\] r \ [ (a l ,  ql)l 
T\[a2\] C\[(a2, q2)\] (al, qz ) 
a~ c'\[(a~,q~)\] T\[(a~,q2)\] 
T\[a3\] C\[(a3, q3)\] (a2, (12) 
a~ T\[#1 T\[(a~,q3)\] 
I I 
# (a3, q3) 
Figure I: A derivation in GM for string 
ala2a3#(a3, qs)(a2, q2)(ax, ql)($, q0). 
ff and only ff 
C\[($, qo)\] ~*  
a l ' . "  akg\[Cak, qk)\](at?-l, qk-x) ? ? ? ($, qo). 
The claim can be proved by induction on k, using 
productions (a) to (c) from Definition 1. 
Let R denote the reverse operator on strings. 3
From the above claim and using production (d) from 
Definition 1, one can easily show that 
L(GM) = {w#u \[ w E L (M) ,  u R encodes an 
accepting computation for w}. 
The lemma directly follows from this relation. ? 
We can now provide the main result of this sec- 
tion. To this end, we refine the definition of rec- 
ognizer presented in Section 3. A recognizer for the 
CFG class is an algorithm R that has random access 
to some data structure C(G) obtained by means of 
some off-line precompilation of a CFG G. On in- 
put w, which is a string on the terminal symbols of 
G, R decides whether w E L(G). The definition of 
the CPP extends in the obvious way to recognizers 
working with precompiled grammars. 
Theorem 2 Let p be any polynomial in two vari- 
ables. 1\] the conjecture about quasi-determinizers 
holds true, then no recognizer exists that 
3Note that R does not affect individual symbols in a string. 
Thus (a, q)R = (a, q). 
276 
(i) has random access to data structure C(G) pre- 
compiled from a bilexical CFG G in polynomial 
time in IGI, 
(ii) runs in an amount of time bounded by 
p(IVDI, Iwl), where VD is the set of delexicalized 
nonterminals of G and w is the input string, 
and 
(iii) satisfies the CPP. 
Proo/. Assume there exists a recognizer R that sat- 
isfies conditions (i) to (iii) in the statement of the 
theorem. We show how this entails that the conjec- 
ture about quasi-determinizers is false. 
We use algorithm R to specify a quasi- 
determinizer A. Given a nondeterministic FA M, 
A goes through the following steps. 
1. A constructs grammar GM as in Definition 1. 
2. A precompiles GM as required by R, producing 
data structure C(GM). 
3. A returns a device DM specified as follows. 
Given a string w as input, DM runs R on string 
w~. If R detects an error at any position i, 
0 < i < Iw#l, then DM rejects and halts, oth- 
erwise DM accepts and halts. 
From Lemma 1 we have that DM accepts w if and 
only if w E L(M). Since R runs in time P(\]VDI, Iwl) 
and since GM has a set of delexicalized nonterminals 
independent of M, we have that there exists a poly- 
nomial PA such that every DM works in an amount 
of time bounded by pA(IWl). We therefore conclude 
that A is a quasi-determinizer. 
It remains to be shown that A works in polyno- 
mial time in IMI. Step 1 can be carried out in time 
O(IM\[). The compilation at Step 2 takes polynomial 
time in IGM\], following our hypotheses on R, and 
hence polynomial time in IMI, since IGMI = O(IMI). 
Finally, the construction of DM at Step 3 can easily 
be carried out in time O(IMI) as well. ? 
In addition to Theorem 1, Theorem 2 states that, 
even in case the input grammar is compiled off- 
line and in polynomial time, we cannot perform 
CPP recognition for bilexical context-free grammars 
in time polynomial in the grammar and the input 
string but independent of the lexicon size. This 
is true with at least the same evidence that sup- 
ports the conjecture on quasi-determinizers. Again, 
this should be contrasted with the time performance 
of existing bidirectional algorithms, allowing recog- 
nition for bilexical context-free grammars in time 
O(IVDI 3 Iwl ). 
In order to complete our investigation of the above 
problem, in Appendix A we show that, when we drop 
the polynomial-time r striction on the grammar pre- 
compilation, it is indeed possible to get rid of any 
IVT\] factor from the running time of the recognizer. 
6 Conc lus ion  
Empirical results presented in the literature show 
that bidirectional parsing strategies can be more 
time efficient in cases of grammar formalisms whose 
rules are specialized for one or more lexical items. 
In this paper we have provided an original mathe- 
matical argument in favour of this thesis. Our re- 
sults hold for bilexical context-free grammars and 
directly transfer to several language models that can 
be seen as stochastic versions of this formalism (see 
Section 1). We perceive that these results can be ex- 
tended to other language models that properly em- 
bed bilexical context-free grammars, as for instance 
the more general history-based models used in (Rat- 
naparkhi, 1997) and (Chelba and Jelinek, 1998). We 
leave this for future work. 
Acknowledgements 
We would like to thank Jason Eisner and Mehryar 
Mohri for fruitful discussions. The first author is 
supported by the German Federal Ministry of Edu- 
cation, Science, Research and Technology (BMBF) 
in the framework of the VERBMOBIL Project under 
Grant 01 IV 701 V0, and was employed at AT&T 
Shannon Laboratory during a part of the period this 
paper was written. The second author is supported 
by MURST under project PRIN." BioInformatica e
Ricerca Genomica and by University of Padua, un- 
der project Sviluppo di Sistemi ad Addestramento 
Automatico per l'Analisi del Linguaggio Naturale. 
References 
A. V. Aho, J. E. Hopcroft, and J. D. Ullman. 1974. 
The Design and Analysis of Computer Algorithms. 
Addison-Wesley, Reading, MA. 
H. Alshawi. 1996. Head automata and bilingual 
tiling: Translation with minimal representations. 
In Proc. of the 3~ th ACL, pages 167-176, Santa 
Cruz, CA. 
E. Charniak. 1997. Statistical parsing with a 
context-free grammar and word statistics. In 
Proc. of AAAI-97, Menlo Park, CA. 
C. Chelba and F. Jelinek. 1998. Exploiting syntactic 
structure for language modeling. In Proc. o\] the 
36 th A CL, Montreal, Canada. 
M. Collins. 1997. Three generative, lexicalised mod- 
els for statistical parsing. In Proc. of the 35 th 
ACL, Madrid, Spain. 
J. Earley. 1970. An efficient context-free parsing al- 
gorithm. Communications of the Association for 
Computing Machinery, 13(2):94-102. 
J. Eisner and G. Satta. 1999. Efficient parsing for 
bilexical context-free grammars and head automa- 
ton grammars. In Proc. of the 37 th A CL, pages 
457-464, College Park, Maryland. 
277 
J. Eisner. 1996. An empirical comparison of proba- 
bility models for dependency grammar. Technical 
Report IRCS-96-11, IRCS, Univ. of Pennsylvania. 
J. Eisner. 1997. Bilexical grammars and a cubic- 
time probabilistic parser. In Proceedings of the 
~th Int. Workshop on Parsing Technologies, MIT, 
Cambridge, MA, September. 
R. C. Gonzales and M. G. Thomason. 1978. Syntac- 
tic Pattern Recognition. Addison-Wesley, Read- 
ing, MA. 
D. Gusfield. 1997. Algorithms on Strings, Trees and 
Sequences. Cambridge University Press, Cam- 
bridge, UK. 
M. A. Harrison. 1978. Introduction to Formal Lan- 
guage Theory. Addison-Wesley, Reading, MA. 
J. E. Hopcroft and J. D. Ullman. 1979. Introduc- 
tion to Automata Theory, Languages and Compu- 
tation. Addison-Wesley, Reading, MA. 
A. R. Meyer and M. J. Fisher. 1971. Economy of de- 
scription by automata, grammars and formal sys- 
tems. In 12th Annual Symp. on Switching and Au- 
tomata Theory, pages 188-190, New York. IEEE. 
M.-J. Nederhof and G. Satta. 1996. Efficient abular 
LR parsing. In Proc. of the 3~ th A CL, pages 239- 
246, Santa Cruz, CA. 
M.-J. Nederhof. 1994. An optimal tabular parsing 
algorithm. In Proc. of the 32 ~ ACL, pages 117- 
124, Las Cruces, New Mexico. 
A. Ratnaparkhi. 1997. A linear observed time sta- 
tistical parser based on maximum entropy mod- 
els. In Second Conference on Empirical Methods 
in Natural Language Processing, Brown Univer- 
sity, Providence, Rhode Island. 
D. J. Rosenkrantz and P. M. Lewis. 1970. Determin- 
istic left corner parsing. In IEEE Conf. Record 
11 th Annual Symposium on Switching and Au- 
tomata Theory, pages 139-152. 
G. Satta and O. Stock. 1994. Bidirectional context- 
free grammar parsing for natural anguage pro- 
cessing. Artificial Intelligence, 69:123-164. 
S. Sippu and E. Soisalon-Soininen. 1988. Pars- 
ing Theory: Languages and Parsing, volume 1. 
Springer-Verlag, Berlin, Germany. 
S. Sippu and E. Soisalon-Soininen. 1990. Parsing 
Theory: LR(k) and LL(k) Parsing, volume 2. 
Springer-Verlag, Berlin, Germany. 
R. E. Stearns and H. B. Hunt. 1981. On the equiva- 
lence and containment problem for unambiguous 
regular expressions, grammars, and automata. In 
22nd Annual Syrup. on Foundations of Computer 
Science, pages 74-81, New York. IEEE. 
M. Tomita. 1986. Ej~icient Parsing \]or Natural Lan- 
guage. Kluwer, Boston, Mass. 
G. van Noord. 1997. An efficient implementation f 
the head-corner parser. Computational Linguis- 
tics, 23(3):425-456. 
C. S. Wetherell. 1980. Probabilistic languages: A 
review and some open questions. Computing Sur- 
veys, 12(4):361-379. 
M. Wiren. 1987. A comparison of rule-invocation 
strategies in parsing. In Proc. of the 3 ~d EACL, 
pages 226-233, Copenhagen, Denmark. 
A Recogn i t ion  in  t ime independent  
o f  the  lex icon  
In Section 5 we have shown that it is unlikely that 
correct-prefix property parsing for a bilexical CFG 
can be carried out in polynomial time and indepen- 
dently of the lexicon size, when only polynomial- 
time off-line compilation of the grammar is allowed. 
To complete our presentation, we show here that 
correct-prefix property parsing in time independent 
of the lexicon size is indeed possible if we spend ex- 
ponential time on grammar precompilation. 
We first consider tabular LR parsing (Tomita, 
1986), a technique which satisfies the correct-prefix 
property, and apply it to bilexical CFGs. Our pre- 
sentation relies on definitions from (Nederhof and 
Satta, 1996). Let w E V~ be some input string. A 
property of LR parsing is that any state that can be 
reached after reading prefix w\[1,j\], j < \]w\], must be 
of the form 
goto(goto(. . (goto( q~n, X1),...), Xm-1), Xm) 
where q~ is the initial LR state, and X I , . . . ,  X,~ are 
terminals or nonterminals such that X I ' . 'Xm o*  
w\[1, if. For a bilexical CFG, each X~ is of the form b~ 
or of the form B~\[b~\], where bl , . . . ,  bm is some subse- 
quence of wIl , j  \]. This means that there are at most 
(2+ IVDI)" distinct states that can be reached by the 
recognizer, apart from qin. In the algorithm, the tab- 
ulation prevents repeated manipulation ofstates for 
a triple of input positions, leading to a time complex- 
ity of O(n 3 IvDIn), where n = Iwl. Hence, when we 
apply precompilation f the grammar, we can carry 
out recognition i  time exponential in the length of 
the input string, yet independent ofthe lexicon size. 
Note however that the precompilation for LR pars- 
ing takes exponential time. 
The second algorithm with the CPP we will con- 
sider can be derived from Earley's algorithm (Ear- 
ley, 1970). For this new recognizer, we achieve a 
time complexity completely independent of the size 
of the whole grammar, not merely independent of
the size of the lexicon as in the case of tabular LR 
parsing. Furthermore, the input grammar can be 
any general CFG, not necessarily a bilexical one. In 
terms of the length of the input, the complexity is
polynomial rather than exponential. 
Earley's algorithm is outlined in what follows, 
with minor modifications with respect o its origi- 
nal presentation. An i tem is an object of the form 
278 
\[A -+ a ,, j3\], where A -~ a~ is a production from 
the grammar. The recognition algorithm consists in 
an incremental construction of a (n + 1) x (n + 1), 
2-dimensional table T, where n is the length of the 
input string. At each stage, each entry T\[i,j\] in 
the table contains a set of items, which is initially 
the empty set. After an initial item is added to en- 
try T\[0, 0\] in the table, other items in other entries 
are derived from it, directly or indirectly, using three 
steps called predictor, scanner and completer. When 
no more new items can be derived, the presence of 
a final item in entry T\[0, n\] indicates whether the 
input is recognized. 
The recognition process can be precompiled, 
based on the observation that for any grammar the 
set of all possible items is finite, and thereby all po- 
tential contents of T's entries can be enumerated. 
Furthermore, the dependence of entries on one an- 
other is not cyclic; one item in T\[i, j\] may be derived 
from a second item in the same entry, but it is not 
possible that, for example, an item in T\[i,j\] is de- 
rived from an item in T\[i',j'\], with (i,j) ~ (i',j'), 
which is in turn derived from an item in T\[i,j\]. 
A consequence is that entries can be computed 
in a strict order, and an operation that involves the 
combination of, say, the items from two entries T\[i, j\] 
and T\[j, k\] by means of the completer step can be 
implemented by a simple table lookup. More pre- 
cisely, each set of items is represented by an atomic 
state, and combining two sets of items according 
to the completer step is implemented by indexing 
a 2-dimensional rray by the two states representing 
those two sets, yielding a third state representing 
the resulting set of items. Similarly, the scanner 
and predictor steps and the union operation on sets 
of items can all be implemented by table lookup. 
The time complexity of recognition can straight- 
forwardly be shown to be (9(n3), independent of 
the size of the grammar. However, massive pre- 
compilation is involved in enumerating all possi- 
ble sets of items and precomputing the operations 
on them. The motivation for discussing this algo- 
rithm is therefore purely theoretical: it illustrates 
the unfavourable complexity properties that The- 
orem 2, together with the conjecture about quasi- 
determinizers, attributes to the recognition problem 
if the correct-prefix property is to be ensured. 
279 
Kullback-Leibler Distance
between Probabilistic Context-Free Grammars
and Probabilistic Finite Automata
Mark-Jan Nederhof
Faculty of Arts
University of Groningen
P.O. Box 716
NL-9700 AS Groningen, The Netherlands
markjan@let.rug.nl
Giorgio Satta
Department of Information Engineering
University of Padua
via Gradenigo, 6/A
I-35131 Padova, Italy
satta@dei.unipd.it
Abstract
We consider the problem of computing the
Kullback-Leibler distance, also called the
relative entropy, between a probabilistic
context-free grammar and a probabilistic fi-
nite automaton. We show that there is
a closed-form (analytical) solution for one
part of the Kullback-Leibler distance, viz.
the cross-entropy. We discuss several ap-
plications of the result to the problem of
distributional approximation of probabilis-
tic context-free grammars by means of prob-
abilistic finite automata.
1 Introduction
Among the many formalisms used for descrip-
tion and analysis of syntactic structure of natu-
ral language, the class of context-free grammars
(CFGs) is by far the best understood and most
widely used. Many formalisms with greater gen-
erative power, in particular the different types
of unification grammars, are ultimately based
on CFGs.
Regular expressions, with their procedural
counter-part of finite automata (FAs), are not
able to describe hierarchical, tree-shaped struc-
ture, and thereby seem less suitable than CFGs
for full analysis of syntactic structure. How-
ever, there are many applications where only
partial or approximated analysis of structure is
needed, and where full context-free processing
could be prohibitively expensive. Such appli-
cations can for example be found in real-time
speech recognition systems: of the many hy-
potheses returned by a speech recognizer, shal-
low syntactic analysis may be used to select a
small subset of those that seem most promis-
ing for full syntactic processing in a next phase,
thereby avoiding further computational costs
for the less promising hypotheses.
As FAs cannot describe structure as such, it
is impractical to write the automata required
for such applications by hand, and even diffi-
cult to derive them automatically by training.
For this reason, the used FAs are often derived
from CFGs, by means of some form of approx-
imation. An overview of different methods of
approximating CFGs by FAs, along with an ex-
perimental comparison, was given by (Nederhof,
2000).
The next step is to assign probabilities to the
transitions of the approximating FA, as the ap-
plication outlined above requires a qualitative
distinction between hypotheses rather than the
purely boolean distinction of language member-
ship. Under certain circumstances, this may be
done by carrying over the probabilities from an
input probabilistic CFG (PCFG), as shown for
the special case of n-grams by (Rimon and Herz,
1991; Stolcke and Segal, 1994), or by training
of the FA on a corpus generated by the PCFG
(Jurafsky et al, 1994). See also (Mohri and
Nederhof, 2001) for discussion of related ideas.
An obvious question to ask is then how
well the resulting PFA approximates the input
PCFG, possibly for different methods of deter-
mining an FA and different ways of attaching
probabilities to the transitions. Until now, any
direct way of measuring the distance between
a PCFG and a PFA has been lacking. As we
will argue in this paper, the natural distance
measure between probability distributions, the
Kullback-Leibler (KL) distance, is difficult to
compute. (The KL distance is also called rela-
tive entropy.) We can however derive a closed-
form (analytical) solution for the cross entropy
of a PCFG and a PFA, provided the FA under-
lying the PFA is deterministic. The difference
between the cross-entropy and the KL distance
is the entropy of the PCFG, which does not rely
on the PFA. This means that if we are interested
in the relative quality of different approximat-
ing PFAs with respect to a single input PCFG,
the cross-entropy may be used instead of the
KL distance. The constraint of determinism is
not a problem in practice, as any FA can be
determinized, and FAs derived by approxima-
tion algorithms are normally determinized (and
minimized).
As a second possible application, we now look
more closely into the matter of determinization
of finite-state models. Not all PFAs can be de-
terminized, as discussed by (Mohri, 1997). This
is unfortunate, as deterministic (P)FAs process
input with time and space costs independent
of the size of the automaton, whereas these
costs are linear in the size of the automaton
in the nondeterministic case, which may be too
high for some real-time applications. Instead
of distribution-preserving determinization, we
may therefore approximate a nondeterministic
PFA by a deterministic PFA whose probability
distribution is close to, but not necessarily iden-
tical to, that of the first PFA. Again, an impor-
tant question is how close the two models are to
each other. It was argued before by (Juang and
Rabiner, 1985; Falkhausen et al, 1995; Vihola
et al, 2002) that the KL distance between finite-
state models is difficult to compute in general.
The theory developed in this paper shows how-
ever that the cross-entropy between the input
PFA and the approximating deterministic PFA
can be expressed in closed form, relying on the
fact that a PFA can be seen as a special case of
a PCFG. Thereby, different approximating de-
terministic PFAs can be compared for closeness
to the input PFA. We can even compute the
KL distance between two unambiguous PFAs,
in closed form. (It is not difficult to see that
ambiguity is a decidable property for FAs.)
The structure of this paper is as follows.
We provide some preliminary definitions in Sec-
tion 2. Section 3 discusses the expected fre-
quency of a rule in derivations allowed by a
PCFG, and explains how such values can be ef-
fectively computed. The KL distance between
a PCFG and a PFA is closely related to the
entropy of the PCFG, which we discuss in Sec-
tion 4. Essential to our approach is the inter-
section of PCFGs and PFAs, to be discussed in
Section 5. As we show in Section 6, the part
of the KL distance expressing the cross-entropy
can be computed in closed form, based on this
intersection. Section 7 concludes this paper.
2 Preliminaries
Throughout the paper we use mostly stan-
dard formal language notation, as for instance
in (Hopcroft and Ullman, 1979; Booth and
Thompson, 1973), which we summarize below.
A context-free grammar (CFG) is a 4-tuple
G = (?,N, S,R) where ? and N are finite dis-
joint sets of terminals and nonterminals, respec-
tively, S ? N is the start symbol and R is a fi-
nite set of rules. Each rule has the form A? ?,
where A ? N and ? ? (? ?N)?.
The ?derives? relation ? associated with G
is defined on triples consisting of two strings
?, ? ? (? ? N)? and a rule pi ? R. We write
? pi? ? if and only if ? is of the form uA?
and ? is of the form u??, for some u ? ??,
? ? (? ? N)?, and pi = (A ? ?). A left-most
derivation (for G) is a string d = pi1 ? ? ?pim,
m ? 0, such that ?0
pi1? ?1
pi2? ? ? ?
pim? ?m, for
some ?0, . . . , ?m ? (? ? N)?; d =  (where 
denotes the empty string) is also a left-most
derivation. In the remainder of this paper,
we will let the term ?derivation? refer to ?left-
most derivation?, unless specified otherwise. If
?0
pi1? ? ? ?
pim? ?m for some ?0, . . . , ?m ? (??N)?,
then we say that d = pi1 ? ? ?pim derives ?m from
?0 and we write ?0
d? ?m; d =  derives any
?0 ? (? ?N)? from itself.
A (left-most) derivation d such that S d? w,
w ? ??, is called a complete derivation. If d is
a complete derivation, we write y(d) to denote
the (unique) string w ? ?? such that S d? w.
The language generated by G is the set of all
strings y(d) derived by complete derivations,
i.e., L(G) = {w |S d? w, d ? R?, w ? ??}.
It is well-known that there is a one-to-one cor-
respondence between complete derivations and
parse trees for strings in L(G).
A probabilistic CFG (PCFG) is a pair Gp =
(G, pG), where G is a CFG and pG is a function
from R to real numbers in the interval [0, 1].
A PCFG is proper if
?
pi=(A??) pG(pi) = 1 for
all A ? N . Function pG can be used to as-
sociate probabilities to derivations of the un-
derlying CFG G, in the following way. For
d = pi1 ? ? ?pim ? R?, m ? 0, we define pG(d) =
?m
i=1 pG(pii) if S
d? w for some w ? ??, and
pG(d) = 0 otherwise. The probability of a string
w ? ?? is defined as pG(w) =
?
d:y(d)=w pG(d).
A PCFG is consistent if
?
w pG(w) = 1. Con-
sistency implies that the PCFG defines a proba-
bility distribution on the set of terminal strings
as well as on the set of grammar derivations. If
a PCFG is proper, then consistency means that
no probability mass is lost in ?infinite? deriva-
tions.
A finite automaton (FA) is a 5-tuple M = (?,
Q, q0, Qf , T ), where ? and Q are two finite sets
of terminals and states, respectively, q0 is the
initial state, Qf ? Q is the set of final states,
and T is a finite set of transitions, each of the
form s a7? t, where s, t ? Q and a ? ?. A
probabilistic finite automaton (PFA) is a pair
Mp = (M,pM ), where M is an FA and pM is a
function from T to real numbers in the interval
[0, 1].1
For a fixed (P)FA M , we define a configu-
ration to be an element of Q ? ??, and we
define the relation ` on triples consisting of
two configurations and a transition ? ? T by:
(s, w)
?
` (t, w?) if and only if w is of the form aw?,
for some a ? ?, and ? = (s a7? t). A complete
computation is a string c = ?1 ? ? ? ?m, m ? 0,
such that (s0, w0)
?1
` (s1, w1)
?2
` ? ? ?
?m
` (sm, wm),
for some (s0, w0), . . . , (sm, wm) ? Q???, with
s0 = q0, sm ? Qf and wm = , and we write
(s0, w0)
c
` (sm, wm). The language accepted by
M is L(M) = {w ? ?? | (q?, w)
c
` (s, ), c ?
T ?, s ? Qf}.
For a PFA Mp = (M,pM ), and c = ?1 ? ? ? ?m ?
T ?, m ? 0, we define pM (c) =
?m
i=1 pM (?i) if
c is a complete computation, and pM (c) = 0
otherwise. A PFA is consistent if
?
c pM (c) = 1.
We say M is unambiguous if for each w ? ??,
?s?Qf [(q0, w)
c
` (s, )] for at most one c ? T ?.
We say M is deterministic if for each s and a,
there is at most one transition s a7? t. Deter-
minism implies unambiguity. It can be more
readily checked whether an FA is determinis-
tic than whether it is unambiguous. Further-
more, any FA can be effectively turned into a
deterministic FA accepting the same language.
Therefore, this paper will assume that FAs are
deterministic, although technically, unambigu-
ity is sufficient for our constructions to apply.
3 Expectation of rule frequency
Here we discuss how we can compute the ex-
pectation of the frequency of a rule or a non-
terminal over all derivations of a probabilistic
context-free grammar. These quantities will be
used later by our algorithms.
1Our definition of PFAs amounts to a slight loss of
generality with respect to standard definitions, in that
there are no epsilon transitions and no probability func-
tion on states being final. We want to avoid these con-
cepts as they would cause some technical complications
later in this article. There is no loss of generality how-
ever if we may assume an end-of-sentence marker, which
is often the case in practice.
Let (A ? ?) ? R be a rule of PCFG Gp,
and let d ? R? be a complete derivation in Gp.
We define f(A? ?; d) as the number of occur-
rences, or frequency , of A ? ? in d. Similarly,
the frequency of nonterminal A in d is defined
as f(A; d) =
?
? f(A? ?; d). We consider the
following related quantities
EpG f(A? ?; d) =
?
d
pG(d) ? f(A? ?; d),
EpG f(A; d) =
?
d
pG(d) ? f(A; d)
=
?
?
EpG f(A? ?; d).
A method for the computation of these quan-
tities is reported in (Hutchins, 1972), based on
the so-called momentum matrix. We propose
an alternative method here, based on an idea
related to the inside-outside algorithm (Baker,
1979; Lari and Young, 1990; Lari and Young,
1991). We observe that we can factorize a
derivation d at each occurrence of rule A ? ?
into an ?innermost? part d2 and two ?outermost?
parts d1 and d3. We can then write
EpG f(A? ?; d) =
?
d=pi1???pim,m1,m2,w,?,v,x:
S
d1?wA?, with d1=pi1???pim1?1,
(A??)=pim1 ,
?
d2?v, with d2=pim1+1???pim2 ,
?
d3?x, with d3=pim2+1???pim
m?
i=1
pG(pii).
Next we group together all of the innermost and
all of the outermost derivations and write
EpG f(A? ?; d) =
outGp(A) ? pG(A? ?) ? inGp(?)
where
outGp(A) =
?
d=pi1???pim,d?=pi?1???pi
?
m?
,w,?,x:
S
d
?wA?, ?
d?
?x
m?
i=1
pG(pii) ?
m??
i=1
pG(pi
?
i)
and
inGp(?) =
?
d=pi1???pim,v:
?
d
?v
m?
i=1
pG(pii).
Both outGp(A) and inGp(?) can be described in
terms of recursive equations, of which the least
fixed-points are the required values. If Gp is
proper and consistent, then inGp(?) = 1 for
each ? ? (? ? N)?. Quantities outGp(A) for
every A can all be (exactly) calculated by solv-
ing a linear system, requiring an amount of time
proportional to the cube of the size of Gp; see
for instance (Corazza et al, 1991).
On the basis of all the above quantities, a
number of useful statistical properties of Gp can
be easily computed, such as the expected length
of derivations, denoted EDL(Gp) and the ex-
pected length of sentences, denoted EWL(Gp),
discussed before by (Wetherell, 1980). These
quantities satisfy the relations
EDL(Gp) = EpG |d| =
?
A??
outGp(A) ? pG(A? ?) ? inGp(?),
EWL(Gp) = EpG |y(d)| =
?
A??
outGp(A) ? pG(A? ?) ? inGp(?) ? |?|? ,
where for a string ? ? (N ? ?)? we write |?|?
to denote the number of occurrences of terminal
symbols in ?.
4 Entropy of PCFGs
In this section we introduce the notion of deriva-
tional entropy of a PCFG, and discuss an algo-
rithm for its computation.
Let Gp = (G, pG) be a PCFG. For a nonter-
minal A of G, let us define the entropy of A as
the entropy of the distribution pG on all rules
of the form A? ?, i.e.,
H(A) = EpG log
1
pG(A? ?)
=
?
?
pG(A? ?) ? log
1
pG(A? ?)
.
The derivational entropy of Gp is defined as
the expectation of the information of the com-
plete derivations generated by Gp, i.e.,
Hd(Gp) = EpG log
1
pG(d)
=
?
d
pG(d) ? log
1
pG(d)
. (1)
We now characterize derivational entropy using
expected rule frequencies as
Hd(Gp) =
?
d
pG(d) ? log
1
pG(d)
=
?
d
pG(d) ? log
?
A??
(
1
pG(A? ?)
)f(A??;d)
=
?
d
pG(d) ?
?
A??
f(A? ?; d) ? log
1
pG(A? ?)
=
?
A??
log
1
pG(A? ?)
?
?
d
pG(d) ? f(A? ?; d) =
?
A??
log
1
pG(A? ?)
? EpG f(A? ?; d) =
?
A
?
?
log
1
pG(A? ?)
? outGp(A) ? pG(A? ?)?
inGp(?) =
?
A
outGp(A) ?
?
?
pG(A? ?) ? log
1
pG(A? ?)
?
inGp(?).
As already discussed, under the assumption
that Gp is proper and consistent we have
inGp(?) = 1 for every ?. Thus we can write
Hd(Gp) =
?
A
outGp(A) ?H(A). (2)
The computation of outGp(A) was discussed
in Section 3, and also H(A) can easily be calcu-
lated.
Under the restrictive assumption that a
PCFG is proper and consistent, the characteri-
zation in (2) was already known from (Grenan-
der, 1976, Theorem 10.7, pp. 90?92). The proof
reported in that work is different from ours and
uses a momentum matrix (Section 3). Our char-
acterization above is more general and uses sim-
pler notation than the one in (Grenander, 1976).
The sentential entropy , or entropy for short,
of Gp is defined as the expectation of the infor-
mation of the strings generated by Gp, i.e.,
H(Gp) = EpG log
1
pG(w)
=
?
w
pG(w) ? log
1
pG(w)
, (3)
assuming 0 ? log 10 = 0, for strings w not gen-
erated by Gp. It is not difficult to see that
H(Gp) ? Hd(Gp) and equality holds if and only
if G is unambiguous (Soule, 1974, Theorem 2.2).
As ambiguity of CFGs is undecidable, it follows
that we cannot hope to obtain a closed-form
solution for H(Gp) for which equality to (2) is
decidable. We will return to this issue in Sec-
tion 6.
5 Weighted intersection
In order to compute the cross-entropy defined
in the next section, we need to derive a sin-
gle probabilistic model that simultaneously ac-
counts for both the computations of an under-
lying FA and the derivations of an underlying
PCFG. We start from a construction originally
presented in (Bar-Hillel et al, 1964), that com-
putes the intersection of a context-free language
and a regular language. The input consists of a
CFG G = (?, N, S, R) and an FA M = (?, Q,
q0, Qf , T ); note that we assume, without loss
of generality, that G and M share the same set
of terminals ?.
The output of the construction is CFG G? =
(?, N?, S?, R?), where N? = Q ? (? ? N) ?
Q ? {S?}, and R? consists of the set of rules
that is obtained as follows.
? For each s ? Qf , let S? ? (q0, S, s) be a
rule of G?.
? For each rule A ? X1 ? ? ?Xm of G
and each sequence of states s0, . . . , sm
of M , with m ? 0, let (s0, A, sm) ?
(s0, X1, s1) ? ? ? (sm?1, Xm, sm) be a rule of
G?; form = 0, G? has a rule (s0, A, s0)? 
for each state s0.
? For each transition s a7? t of M , let
(s, a, t)? a be a rule of G?.
Note that for each rule (s0, A, sm) ?
(s0, X1, s1) ? ? ? (sm?1, Xm, sm) there is a unique
rule A ? X1 ? ? ?Xm from which it has been
constructed by the above. Similarly, each rule
(s, a, t) ? a uniquely identifies a transition
s a7? t. This means that if we take a complete
derivation d? in G?, we can extract a sequence
h1(d?) of rules from G and a sequence h2(d?) of
transitions from M , where h1 and h2 are string
homomorphisms that we define point-wise as
? h1(pi?) = , if pi? is S? ? (q0, S, s);
h1(pi?) = pi, if pi? is (s0, A, sm) ?
(s0, X1, s1) ? ? ? (sm?1, Xm, sm) and pi is
(A? X1 ? ? ?Xm);
h1(pi?) = , if pi? is (s, a, t)? a;
? h2(pi?) = , if pi? is S? ? (q0, S, s);
h2(pi?) = ? , if pi? is (s, a, t) ? a and ? is
s a7? t;
h2(pi?) = , if pi? is (s0, A, sm) ?
(s0, X1, s1) ? ? ? (sm?1, Xm, sm).
We define h(d?) = (h1(d?), h2(d?)). It can be
easily shown that if S?
d?? w and h(d?) = (d, c),
then for the same w we have S d? w and
(q0, w)
c
` (s, ), some s ? Qf . Conversely,
if for some w, d and c we have S d? w and
(q0, w)
c
` (s, ), some s ? Qf , then there is pre-
cisely one derivation d? such that h(d?) = (d, c)
and S?
d?? w.
As noted before by (Nederhof and Satta,
2003), this construction can be extended to ap-
ply to a PCFG Gp = (G, pG) and an FA M . The
output is a PCFG G?,p = (G?, pG?), where G?
is defined as above and pG? is defined by:
? pG?(S? ? (q0, S, s)) = 1;
? pG?((s0, A, sm) ? (s0, X1, s1) ? ? ?
(sm?1, Xm, sm)) = pG(A? X1 ? ? ?Xm);
? pG?((s, a, t)? a) = 1.
Note that G?,p is non-proper. More specifically,
probabilities of rules with left-hand side S? or
(s0, A, sm) might not sum to one. This is not
a problem for the algorithms presented in this
paper, as we have never assumed properness for
our PCFGs. What is most important here is the
following property of G?,p. If d?, d and c are
such that h(d?) = (d, c), then pG?(d?) = pG(d).
Let us now assume that M is deterministic.
(In fact, the weaker condition of M being unam-
biguous is sufficient for our purposes, but unam-
biguity is not a very practical condition.) Given
a string w and a transition s a7? t of M we define
f(s a7? t;w) as the frequency (number of occur-
rences) of s a7? t in the unique computation of
M , if it exists, that accepts w; this frequency is
0 if w is not accepted by M . On the basis of the
above construction of G?,p and of Section 3, we
find
EpG f(s
a7? t; y(d)) =
?
d
pG(d) ? f(s
a7? t; y(d)) =
outG?,p((s, a, t)) ? pG?((s, a, t)?a) ? inG?,p(a) =
outG?,p((s, a, t)) (4)
6 Kullback-Leibler distance
In this section we consider the Kullback-Leibler
distance between a PCFGs and a PFA, and
present a method for its optimization under cer-
tain assumptions. Let Gp = (G, pG) be a consis-
tent PCFG and let Mp = (M,pM ) be a consis-
tent PFA. We demand that M be deterministic
(or more generally, unambiguous). Let us first
assume that L(G) ? L(M); we will later drop
this constraint.
The cross-entropy of Gp and Mp is defined as
usual for probabilistic models, viz. as the expec-
tation under distribution pG of the information
of the strings generated by M , i.e.,
H(Gp ||Mp) = EpG log
1
pM (w)
=
?
w
pG(w) ? log
1
pM (w)
.
The Kullback-Leibler distance of Gp and Mp is
defined as
D(Gp ||Mp) = EpG log
pG(w)
pM (w)
=
?
w
pG(w) ? log
pG(w)
pM (w)
.
Quantity D(Gp ||Mp) can also be expressed as
the difference between the cross-entropy of Gp
and Mp and the entropy of Gp, i.e.,
D(Gp ||Mp) = H(Gp ||Mp)?H(Gp). (5)
Let G?,p be the PCFG obtained by intersecting
Gp with the non-probabilistic FA M underlying
Mp, as in Section 5. Using (4) the cross-entropy
of Gp and Mp can be expressed as
H(Gp ||Mp) =
?
w
pG(w) ? log
1
pM (w)
=
?
d
pG(d) ? log
1
pM (y(d))
=
?
d
pG(d) ? log
?
s
a
7?t
(
1
pM (s
a7? t)
)f(s
a
7?t;y(d))
=
?
d
pG(d) ?
?
s
a
7?t
f(s a7? t; y(d)) ? log
1
pM (s
a7? t)
=
?
s
a
7?t
log
1
pM (s
a7? t)
?
?
d
pG(d) ? f(s
a7? t; y(d)) =
?
s
a
7?t
log
1
pM (s
a7? t)
? EpG f(s
a7? t; y(d)) =
?
s
a
7?t
log
1
pM (s
a7? t)
? outG?,p((s, a, t)).
We can combine the above with (5) to obtain
D(Gp ||Mp) =
?
s
a
7?t
outG?,p((s, a, t)) ? log
1
pM (s
a7? t)
?H(Gp).
The values of outG?,p can be calculated eas-
ily, as discussed in Section 3. Computation of
H(Gp) in closed-form is problematic, as already
pointed out in Section 4. However, for many
purposes computation of H(Gp) is not needed.
For example, assume that the non-
probabilistic FA M underlying Mp is given, and
our goal is to measure the distance between Gp
and Mp, for different choices of pM . Then the
choice that minimizes H(Gp ||Mp) determines
the choice that minimizes D(Gp ||Mp), irre-
spective of H(Gp). Formally, we can use the
above characterization to compute
p?M = argmaxpM
D(Gp ||Mp)
= argmax
pM
H(Gp ||Mp).
When L(G) ? L(M) is non-empty, both
D(Gp ||Mp) and H(Gp ||Mp) are undefined, as
their definitions imply a division by pM (w) = 0
for w ? L(G)? L(M). In cases where the non-
probabilistic FA M is given, and our goal is to
compare the relative distances between Gp and
Mp for different choices of pM , it makes sense
to ignore strings in L(G) ? L(M), and define
D(Gp ||Mp), H(Gp ||Mp) and H(Gp) on the do-
main L(G) ? L(M). Our equations above then
still hold. Note that strings in L(M)?L(G) can
be ignored since they do not contribute non-zero
values to D(Gp ||Mp) and H(Gp ||Mp).
7 Conclusions
We have discussed the computation of the
KL distance between PCFGs and deterministic
PFAs. We have argued that exact computation
is difficult in general, but for determining the
relative qualities of different PFAs, with respect
to their closeness to an input PCFG, it suffices
to compute the cross-entropy. We have shown
that the cross-entropy between a PCFG and a
deterministic PFA can be computed exactly.
These results can also be used for comparing
a pair of PFAs, one of which is deterministic.
Generalization of PCFGs to probabilistic tree-
adjoining grammars (PTAGs) is also possible,
by means of the intersection of a PTAG and a
PFA, along the lines of (Lang, 1994).
Acknowledgements
Helpful comments from Zhiyi Chi are gratefully
acknowledged. The first author is supported by
the PIONIER Project Algorithms for Linguis-
tic Processing , funded by NWO (Dutch Orga-
nization for Scientific Research). The second
author is partially supported by MIUR (Italian
Ministry of Education) under project PRIN No.
2003091149 005.
References
J.K. Baker. 1979. Trainable grammars for
speech recognition. In J.J. Wolf and D.H.
Klatt, editors, Speech Communication Papers
Presented at the 97th Meeting of the Acousti-
cal Society of America, pages 547?550.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964.
On formal properties of simple phrase struc-
ture grammars. In Y. Bar-Hillel, editor,
Language and Information: Selected Essays
on their Theory and Application, chapter 9,
pages 116?150. Addison-Wesley.
T.L. Booth and R.A. Thompson. 1973. Ap-
plying probabilistic measures to abstract lan-
guages. IEEE Transactions on Computers,
C-22(5):442?450, May.
A. Corazza, R. De Mori, R. Gretter, and
G. Satta. 1991. Computation of probabilities
for an island-driven parser. IEEE Transac-
tions on Pattern Analysis and Machine In-
telligence, 13(9):936?950.
M. Falkhausen, H. Reininger, and D. Wolf.
1995. Calculation of distance measures be-
tween Hidden Markov Models. In Proceedings
of Eurospeech ?95, pages 1487?1490, Madrid.
U. Grenander. 1976. Lectures in Pattern The-
ory, Vol. I: Pattern Synthesis. Springer-
Verlag.
J.E. Hopcroft and J.D. Ullman. 1979. Intro-
duction to Automata Theory, Languages, and
Computation. Addison-Wesley.
S.E. Hutchins. 1972. Moments of strings and
derivation lengths of stochastic context-free
ggrammars. Information Sciences, 4:179?
191.
B.-H. Juang and L.R. Rabiner. 1985. A prob-
abilistic distance measure for hidden Markov
models. AT&T Technical Journal, 64(2):391?
408.
D. Jurafsky, C. Wooters, G. Tajchman, J. Se-
gal, A. Stolcke, E. Fosler, and N. Morgan.
1994. The Berkeley Restaurant Project. In
Proceedings of the International Conference
on Spoken Language Processing (ICSLP-94),
pages 2139?2142, Yokohama, Japan.
B. Lang. 1994. Recognition can be harder
than parsing. Computational Intelligence,
10(4):486?494.
K. Lari and S.J. Young. 1990. The estimation
of stochastic context-free grammars using the
Inside-Outside algorithm. Computer Speech
and Language, 4:35?56.
K. Lari and S.J. Young. 1991. Applications of
stochastic context-free grammars using the
Inside-Outside algorithm. Computer Speech
and Language, 5:237?257.
M. Mohri and M.-J. Nederhof. 2001. Regu-
lar approximation of context-free grammars
through transformation. In J.-C. Junqua and
G. van Noord, editors, Robustness in Lan-
guage and Speech Technology, pages 153?163.
Kluwer Academic Publishers.
M. Mohri. 1997. Finite-state transducers in
language and speech processing. Computa-
tional Linguistics, 23(2):269?311.
M.-J. Nederhof and G. Satta. 2003. Proba-
bilistic parsing as intersection. In 8th Inter-
national Workshop on Parsing Technologies,
pages 137?148, LORIA, Nancy, France, April.
M.-J. Nederhof. 2000. Practical experi-
ments with regular approximation of context-
free languages. Computational Linguistics,
26(1):17?44.
M. Rimon and J. Herz. 1991. The recogni-
tion capacity of local syntactic constraints.
In Fifth Conference of the European Chap-
ter of the Association for Computational Lin-
guistics, Proceedings of the Conference, pages
155?160, Berlin, Germany, April.
S. Soule. 1974. Entropies of probabilistic gram-
mars. Information and Control, 25:57?74.
A. Stolcke and J. Segal. 1994. Precise N -
gram probabilities from stochastic context-
free grammars. In 32nd Annual Meeting of
the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 74?
79, Las Cruces, New Mexico, USA, June.
M. Vihola, M. Harju, P. Salmela, J. Suon-
tausta, and J. Savela. 2002. Two dissimilar-
ity measures for HMMs and their application
in phoneme model clustering. In ICASSP
2002, volume I, pages 933?936.
C.S. Wetherell. 1980. Probabilistic languages:
A review and some open questions. Comput-
ing Surveys, 12(4):361?379, December.
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 478?486,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Treebank Grammar Techniques for Non-Projective Dependency Parsing
Marco Kuhlmann
Uppsala University
Uppsala, Sweden
marco.kuhlmann@lingfil.uu.se
Giorgio Satta
University of Padua
Padova, Italy
satta@dei.unipd.it
Abstract
An open problem in dependency parsing
is the accurate and efficient treatment of
non-projective structures. We propose to
attack this problem using chart-parsing
algorithms developed for mildly context-
sensitive grammar formalisms. In this pa-
per, we provide two key tools for this ap-
proach. First, we show how to reduce non-
projective dependency parsing to parsing
with Linear Context-Free Rewriting Sys-
tems (LCFRS), by presenting a technique
for extracting LCFRS from dependency
treebanks. For efficient parsing, the ex-
tracted grammars need to be transformed
in order to minimize the number of nonter-
minal symbols per production. Our second
contribution is an algorithm that computes
this transformation for a large, empirically
relevant class of grammars.
1 Introduction
Dependency parsing is the task of predicting the
most probable dependency structure for a given
sentence. One of the key choices in dependency
parsing is about the class of candidate structures
for this prediction. Many parsers are confined to
projective structures, in which the yield of a syn-
tactic head is required to be continuous. A major
benefit of this choice is computational efficiency:
an exhaustive search over all projective structures
can be done in cubic, greedy parsing in linear time
(Eisner, 1996; Nivre, 2003). A major drawback of
the restriction to projective dependency structures
is a potential loss in accuracy. For example, around
23% of the analyses in the Prague Dependency
Treebank of Czech (Hajic? et al, 2001) are non-
projective, and for German and Dutch treebanks,
the proportion of non-projective structures is even
higher (Havelka, 2007).
The problem of non-projective dependency pars-
ing under the joint requirement of accuracy and
efficiency has only recently been addressed in the
literature. Some authors propose to solve it by tech-
niques for recovering non-projectivity from the out-
put of a projective parser in a post-processing step
(Hall and Nov?k, 2005; Nivre and Nilsson, 2005),
others extend projective parsers by heuristics that
allow at least certain non-projective constructions
to be parsed (Attardi, 2006; Nivre, 2007). McDon-
ald et al (2005) formulate dependency parsing as
the search for the most probable spanning tree over
the full set of all possible dependencies. However,
this approach is limited to probability models with
strong independence assumptions. Exhaustive non-
projective dependency parsing with more powerful
models is intractable (McDonald and Satta, 2007),
and one has to resort to approximation algorithms
(McDonald and Pereira, 2006).
In this paper, we propose to attack non-project-
ive dependency parsing in a principled way, us-
ing polynomial chart-parsing algorithms developed
for mildly context-sensitive grammar formalisms.
This proposal is motivated by the observation that
most dependency structures required for the ana-
lysis of natural language are very nearly projective,
differing only minimally from the best projective
approximation (Kuhlmann and Nivre, 2006), and
by the close link between such ?mildly non-project-
ive? dependency structures on the one hand, and
grammar formalisms with mildly context-sensitive
generative capacity on the other (Kuhlmann and
M?hl, 2007). Furthermore, as pointed out by Mc-
Donald and Satta (2007), chart-parsing algorithms
are amenable to augmentation by non-local inform-
ation such as arity constraints and Markovization,
and therefore should allow for more predictive stat-
istical models than those used by current systems
for non-projective dependency parsing. Hence,
mildly non-projective dependency parsing prom-
ises to be both efficient and accurate.
478
Contributions In this paper, we contribute two
key tools for making the mildly context-sensitive
approach to accurate and efficient non-projective
dependency parsing work.
First, we extend the standard technique for ex-
tracting context-free grammars from phrase-struc-
ture treebanks (Charniak, 1996) to mildly con-
text-sensitive grammars and dependency treebanks.
More specifically, we show how to extract, from
a given dependency treebank, a lexicalized Linear
Context-Free Rewriting System (LCFRS) whose
derivations capture the dependency analyses in the
treebank in the same way as the derivations of
a context-free treebank grammar capture phrase-
structure analyses. Our technique works for arbit-
rary, even non-projective dependency treebanks,
and essentially reduces non-projective dependency
to parsing with LCFRS. This problem can be solved
using standard chart-parsing techniques.
Our extraction technique yields a grammar
whose parsing complexity is polynomial in the
length of the sentence, but exponential in both a
measure of the non-projectivity of the treebank and
the maximal number of dependents per word, re-
flected as the rank of the extracted LCFRS. While
the number of highly non-projective dependency
structures is negligible for practical applications
(Kuhlmann and Nivre, 2006), the rank cannot eas-
ily be bounded. Therefore, we present an algorithm
that transforms the extracted grammar into a nor-
mal form that has rank 2, and thus can be parsed
more efficiently. This contribution is important
even independently of the extraction procedure:
While it is known that a rank-2 normal form of
LCFRS does not exist in the general case (Rambow
and Satta, 1999), our algorithm succeeds for a large
and empirically relevant class of grammars.
2 Preliminaries
We start by introducing dependency trees and
Linear Context-Free Rewriting Systems (LCFRS).
Throughout the paper, for positive integers i and j ,
we write ?i; j ? for the interval f k j i  k  j g,
and use ?n? as a shorthand for ?1; n?.
2.1 Dependency Trees
Dependency parsing is the task to assign depend-
ency structures to a given sentence w. For the
purposes of this paper, dependency structures are
edge-labelled trees. More formally, let w be a sen-
tence, understood as a sequence of tokens over
some given alphabet T , and let L be an alphabet
of edge labels. A dependency tree for w is a con-
structD D .w;E; /, where E forms a rooted tree
(in the standard graph-theoretic sense) on the set
?jwj?, and  is a total function that assigns every
edge in E a label in L. Each node of D represents
a (position of a) token in w.
Example 1 Figure 2 shows a dependency tree for
the sentence A hearing is scheduled on the issue
today, which consists of 8 tokens and the edges
f .2; 1/; .2; 5/; .3; 2/; .3; 4/; .4; 8/; .5; 7/; .7; 6/ g.
The edges are labelled with syntactic functions
such as sbj for ?subject?. The root node is marked
by a dotted line. 
Let u be a node of a dependency treeD. A node u0
is a descendant of u, if there is a (possibly empty)
path from u to u0. A block of u is a maximal
interval of descendants of u. The number of blocks
of u is called the block-degree of u. The block-
degree of a dependency tree is the maximum among
the block-degrees of its nodes. A dependency tree
is projective, if its block-degree is 1.
Example 2 The tree shown in Figure 2 is not
projective: both node 2 (hearing) and node 4
(scheduled) have block-degree 2. Their blocks are
f 2 g; f 5; 6; 7 g and f 4 g; f 8 g, respectively.
2.2 LCFRS
Linear Context-Free Rewriting Systems (LCFRS)
have been introduced as a generalization of sev-
eral mildly context-sensitive grammar formalisms.
Here we use the standard definition of LCFRS
(Vijay-Shanker et al, 1987) and only fix our nota-
tion; for a more thorough discussion of this formal-
ism, we refer to the literature.
Let G be an LCFRS. Recall that each nonter-
minal symbol A of G comes with a positive integer
called the fan-out of A, and that a production p
of G has the form
A! g.A1; : : : ; Ar/ I g.Ex1; : : : ; Exr/ D E? ;
whereA;A1; : : : ; Ar are nonterminals with fan-out
f; f1; : : : ; fr , respectively, g is a function symbol,
and the equation to the right of the semicolon spe-
cifies the semantics of g. For each i 2 ?r?, Exi is
an fi -tuple of variables, and E? D h?1; : : : ; f? i is a
tuple of strings over the variables on the left-hand
side of the equation and the alphabet of terminal
symbols in which each variable appears exactly
once. The production p is said to have rank r ,
fan-out f , and length j?1jC   C j f? jC .f  1/.
479
3 Grammar Extraction
We now explain how to extract an LCFRS from a
dependency treebank, in very much the same way
as a context-free grammar can be extracted from a
phrase-structure treebank (Charniak, 1996).
3.1 Dependency Treebank Grammars
A simple way to induce a context-free grammar
from a phrase-structure treebank is to read off the
productions of the grammar from the trees. We will
specify a procedure for extracting, from a given
dependency treebank, a lexicalized LCFRS G that
is adequate in the sense that for every analysis D
of a sentencew in the treebank, there is a derivation
tree of G that is isomorphic to D, meaning that
it becomes equal to D after a suitable renaming
and relabelling of nodes, and has w as its derived
string. Here, a derivation tree of an LCFRS G is
an ordered tree such that each node u is labelled
with a production p of G, the number of children
of u equals the rank r of p, and for each i 2 ?r?,
the i th child of u is labelled with a production that
has as its left-hand side the i th nonterminal on the
right-hand side of p.
The basic idea behind our extraction procedure
is that, in order to represent the compositional struc-
ture of a possibly non-projective dependency tree,
one needs to represent the decomposition and relat-
ive order not of subtrees, but of blocks of subtrees
(Kuhlmann and M?hl, 2007). We introduce some
terminology. A component of a node u in a de-
pendency tree is either a block B of some child u0
of u, or the singleton interval that contains u; this
interval will represent the position in the string that
is occupied by the lexical item corresponding to u.
We say that u0 contributes B , and that u contrib-
utes ?u; u? to u. Notice that the number of com-
ponents that u0 contributes to its parent u equals
the block-degree of u0. Our goal is to construct
for u a production of an LCFRS that specifies how
each block of u decomposes into components, and
how these components are ordered relative to one
another. These productions will make an adequate
LCFRS, in the sense defined above.
3.2 Annotating the Components
The core of our extraction procedure is an efficient
algorithm that annotates each node u of a given de-
pendency tree with the list of its components, sor-
ted by their left endpoints. It is helpful to think of
this algorithm as of two independent parts, one that
1: Function Annotate-L.D/
2: for each u of D, from left to right do
3: if u is the first node of D then
4: b WD the root node of D
5: else
6: b WD the lca of u and its predecessor
7: for each u0 on the path b   u do
8: left?u0? WD left?u0?  u
Figure 1: Annotation with components
annotates each node u with the list of the left end-
points of its components (Annotate-L) and one
that annotates the corresponding right endpoints
(Annotate-R). The list of components can then
be obtained by zipping the two lists of endpoints
together in linear time.
Figure 1 shows pseudocode for Annotate-L;
the pseudocode for Annotate-R is symmetric. We
do a single left-to-right sweep over the nodes of the
input treeD. In each step, we annotate all nodes u0
that have the current node u as the left endpoint of
one of their components. Since the sweep is from
left to right, this will get us the left endpoints of u0
in the desired order. The nodes that we annotate are
the nodes u0 on the path between u and the least
common ancestor (lca) b of u and its predecessor,
or the path from the root node to u, in case that u
is the leftmost node of D.
Example 3 For the dependency tree in Figure 2,
Annotate-L constructs the following lists left?u?
of left endpoints, for u D 1; : : : ; 8:
1; 1  2  5; 1  3  4  5  8; 4  8; 5  6; 6; 6  7; 8
The following Lemma establishes the correctness
of the algorithm:
Lemma 1 Let D be a dependency tree, and let u
and u0 be nodes of D. Let b be the least common
ancestor of u and its predecessor, or the root node
in case that u is the leftmost node of D. Then u is
the left endpoint of a component of u0 if and only
if u0 lies on the path from b to u. 
Proof It is clear that u0 must be an ancestor of u.
If u is the leftmost node of D, then u is the left
endpoint of the leftmost component of all of its
ancestors. Now suppose that u is not the leftmost
node of D, and let Ou be the predecessor of u. Dis-
tinguish three cases: If u0 is not an ancestor of Ou,
then Ou does not belong to any component of u0;
therefore, u is the left endpoint of a component
480
of u0. If u0 is an ancestor of Ou but u0 ? b, then Ou
and u belong to the same component of u0; there-
fore, u is not the left endpoint of this component.
Finally, if u0 D b, then Ou and u belong to different
components of u0; therefore, u is the left endpoint
of the component it belongs to. 
We now turn to an analysis of the runtime of the
algorithm. Let n be the number of components
of D. It is not hard to imagine an algorithm that
performs the annotation task in time O.n logn/:
such an algorithm could construct the components
for a given node u by essentially merging the list of
components of the children of u into a new sorted
list. In contrast, our algorithm takes time O.n/.
The crucial part of the analysis is the assignment
in line 6, which computes the least common an-
cestor of u and its predecessor. Using markers for
the path from the root node to u, it is straightfor-
ward to implement this assignment in time O.jj/,
where  is the path b   u. Now notice that, by our
correctness argument, line 8 of the algorithm is ex-
ecuted exactly n times. Therefore, the sum over the
lengths of all the paths  , and hence the amortized
time of computing all the least common ancest-
ors in line 6, is O.n/. This runtime complexity is
optimal for the task we are solving.
3.3 Extraction Procedure
We now describe how to extend the annotation al-
gorithm into a procedure that extracts an LCFRS
from a given dependency tree D. The basic idea is
to transform the list of components of each node u
of D into a production p. This transformation will
only rename and relabel nodes, and therefore yield
an adequate derivation tree. For the construction
of the production, we actually need an extended
version of the annotation algorithm, in which each
component is annotated with the node that contrib-
uted it. This extension is straightforward, and does
not affect the linear runtime complexity.
Let D be a dependency tree for a sentence w.
Consider a single node u of D, and assume that u
has r children, and that the block-degree of u is f .
We construct for u a production p with rank r
and fan-out f . For convenience, let us order the
children of u, say by their leftmost descendants,
and let us write ui for the i th child of u according
to this order, and fi for the block-degree of ui ,
i 2 ?r?. The production p has the form
L! g.L1; : : : ; Lr/ I g.Ex1; : : : ; Exr/ D E? ;
where L is the label of the incoming edge of u
(or the special label root in case that u is the root
node of D) and for each i 2 ?r?: Li is the label of
the incoming edge of ui ; Exi is a fi -tuple of vari-
ables of the form xi;j , where j 2 ?fi ?; and E? is
an f -tuple that is constructed in a single left-to-
right sweep over the list of components computed
for u as follows. Let k 2 ?fi ? be a pointer to a cur-
rent segment of E?; initially, k D 1. If the current
component is not adjacent (as an interval) to the
previous component, we increase k by one. If the
current component is contributed by the child ui ,
i 2 ?r?, we add the variable xi;j to ?k , where j
is the number of times we have seen a component
contributed by ui during the sweep. Notice that
j 2 ?fi ?. If the current component is the (unique)
component contributed by u, we add the token cor-
responding to u to ?k . In this way, we obtain a
complete specification of how the blocks of u (rep-
resented by the segments of the tuple E?) decompose
into the components of u, and of the relative order
of the components. As an example, Figure 2 shows
the productions extracted from the tree above.
3.4 Parsing the Extracted Grammar
Once we have extracted the grammar for a depend-
ency treebank, we can apply any parsing algorithm
for LCFRS to non-projective dependency parsing.
The generic chart-parsing algorithm for LCFRS
runs in timeO.jP j  jwjf .rC1//, where P is the set
of productions of the input grammar G, w is the in-
put string, r is the maximal rank, and f is the max-
imal fan-out of a production inG (Seki et al, 1991).
For a grammar G extracted by our technique, the
number f equals the maximal block-degree per
node. Hence, without any further modification, we
obtain a parsing algorithm that is polynomial in the
length of the sentence, but exponential in both the
block-degree and the rank. This is clearly unaccept-
able in practical systems. The relative frequency
of analyses with a block-degree  2 is almost neg-
ligible (Havelka, 2007); the bigger obstacle in ap-
plying the treebank grammar is the rank of the res-
ulting LCFRS. Therefore, in the remainder of the
paper, we present an algorithm that can transform
the productions of the input grammar G into an
equivalent set of productions with rank at most 2,
while preserving the fan-out. This transformation,
if it succeeds, yields a parsing algorithm that runs
in time O.jP j  r  jwj3f /.
481
1A 2hearing 3is 4scheduled 5on 6the 7issue 8today
nmod sbj
root node
vc
pp
nmod
np
tmp
nmod! g1 g1 D hAi
sbj! g2.nmod; pp/ g2.hx1;1i; hx2;1i/ D hx1;1 hearing; x2;1i
root! g3.sbj; vc/ g3.hx1;1; x1;2i; hx2;1; x2;2i/ D hx1;1 is x2;1 x1;2 x2;2i
vc! g4.tmp/ g4.hx1;1i/ D hscheduled; x1;1i
pp! g5.np/ g5.hx1;1i/ D hon x1;1i
nmod! g6 g6 D hthei
np! g7.nmod/ g7.hx1;1i/ D hx1;1 issuei
tmp! g8 g8 D htodayi
Figure 2: A dependency tree, and the LCFRS extracted for it
4 Adjacency
In this section we discuss a method for factorizing
an LCFRS into productions of rank 2. Before start-
ing, we get rid of the ?easy? cases. A production p
is connected if any two strings ?i , j? in p?s defini-
tion share at least one variable referring to the same
nonterminal. It is not difficult to see that, when p is
not connected, we can always split it into new pro-
ductions of lower rank. Therefore, throughout this
section we assume that LCFRS only have connec-
ted productions. We can split p into its connected
components using standard methods for finding the
strongly connected components of an undirected
graph. This can be implemented in time O.r  f /,
where r and f are the rank and the fan-out of p,
respectively.
4.1 Adjacency Graphs
Let p be a production with length n and fan-out f ,
associated with function a g. The set of positions
of p is the set ?n?. Informally, each position rep-
resents a variable or a lexical element in one of the
components of the definition of g, or else a ?gap?
between two of these components. (Recall that n
also accounts for the f   1 gaps in the body of g.)
Example 4 The set of positions of the production
for hearing in Figure 2 is ?4?: 1 for variable x1, 2
for hearing, 3 for the gap, and 4 for y1. 
Let i1; j1; i2; j2 2 ?n?. An interval ?i1; j1? is ad-
jacent to an interval ?i2; j2? if either j1 D i2   1
(left-adjacent) or i1 D j2 C 1 (right-adjacent). A
multi-interval, or m-interval for short, is a set v of
pairwise disjoint intervals such that no interval in v
is adjacent to any other interval in v. The fan-out
of v, written f .v/, is defined as jvj.
We use m-intervals to represent the nonterminals
and the lexical element heading p. The i th nonter-
minal on the right-hand side of p is represented by
the m-interval obtained by collecting all the pos-
itions of p that represent a variable from the i th
argument of g. The head of p is represented by the
m-interval containing the associated position. Note
that all these m-intervals are pairwise disjoint.
Example 5 Consider the production for is in
Figure 2. The set of positions is ?5?. The
first nonterminal is represented by the m-inter-
val f ?1; 1?; ?4; 4? g, the second nonterminal by
f ?3; 3?; ?5; 5? g, and the lexical head by f ?2; 2? g. 
For disjoint m-intervals v1; v2, we say that v1 is
adjacent to v2, denoted by v1 ! v2, if for every
interval I1 2 v1, there is an interval I2 2 v2 such
that I1 is adjacent to I2. Adjacency is not symmet-
ric: if v1 D f ?1; 1?; ?4; 4? g and v2 D f ?2; 2? g, then
v2 ! v1, but not vice versa.
Let V be some collection of pairwise disjoint
m-intervals representing p as above. The ad-
jacency graph associated with p is the graph
G D .V;!G/ whose vertices are the m-intervals
in V , and whose edges!G are defined by restrict-
ing the adjacency relation! to the set V .
For m-intervals v1; v2 2 V , the merger of v1
and v2, denoted by v1 ? v2, is the (uniquely
determined) m-interval whose span is the union
of the spans of v1 and v2. As an example, if
v1 D f ?1; 1?; ?3; 3? g and v2 D f ?2; 2? g, then
v1 ? v2 D f ?1; 3? g. Notice that the way in which
we defined m-intervals ensures that a merging oper-
ation collapses all adjacent intervals. The proof of
the following lemma is straightforward and omitted
for space reasons:
482
1: Function Factorize.G D .V;!G//
2: R WD ;;
3: while!G ? ; do
4: choose .v1; v2/ 2 !G ;
5: R WD R [ f .v1; v2/ g;
6: V WD V   f v1; v2 g [ f v1 ? v2 g;
7: !G WD f .v; v0/ j v; v0 2 V; v ! v0 g;
8: if jV j D 1 then
9: output R and accept;
10: else
11: reject;
Figure 3: Factorization algorithm
Lemma 2 If v1 ! v2, then f .v1 ? v2/  f .v2/.
4.2 The Adjacency Algorithm
Let G D .V;!G/ be some adjacency graph, and
let v1!G v2. We can derive a new adjacency
graph from G by merging v1 and v2. The resulting
graph G0 has vertices V 0 D V  f v1; v2 g[ f v1?
v2 g and set of edges!G0 obtained by restricting
the adjacency relation ! to V 0. We denote the
derive relation as G ).v1;v2/ G
0.
Informally, ifG represents some LCFRS produc-
tion p and v1; v2 represent nonterminals A1; A2,
thenG0 represents a production p0 obtained from p
by replacing A1; A2 with a fresh nonterminal A. A
new production p00 can also be constructed, expand-
ing A into A1; A2, so that p0; p00 together will be
equivalent to p. Furthermore, p0 has a rank smaller
than the rank of p and, from Lemma 2, A does not
increase the overall fan-out of the grammar.
In order to simplify the notation, we adopt the
following convention. Let G ).v1;v2/ G
0 and
let v!G v1, v ? v2. If v!G0 v1 ? v2, then
edges .v; v1/ and .v; v1 ? v2/ will be identified,
and we say that G0 inherits .v; v1 ? v2/ from G.
If v 6!G0 v1?v2, then we say that .v; v1/ does not
survive the derive step. This convention is used for
all edges incident upon v1 or v2.
Our factorization algorithm is reported in Fig-
ure 3. We start from an adjacency graph repres-
enting some LCFRS production that needs to be
factorized. We arbitrarily choose an edge e of the
graph, and push it into a set R, in order to keep
a record of the candidate factorization. We then
merge the two m-intervals incident to e, and we
recompute the adjacency relation for the new set
of vertices. We iterate until the resulting graph has
an empty edge set. If the final graph has one one
vertex, then we have managed to factorize our pro-
duction into a set of productions with rank at most
two that can be computed from R.
Example 6 Let V D f v1; v2; v3 g with v1 D
f ?4; 4? g, v2 D f ?1; 1?; ?3; 3? g, and v3 D
f ?2; 2?; ?5; 5? g. Then !G D f .v1; v2/ g. After
merging v1; v2 we have a new graph G with V D
f v1 ? v2; v3 g and!G D f .v1 ? v2; v3/ g. We
finally merge v1 ? v2; v3 resulting in a new graph
G with V D f v1 ? v2 ? v3 g and!G D ;. We
then accept and stop. 
4.3 Mathematical Properties
We have already argued that, if the algorithm ac-
cepts, then a binary factorization that does not
increase the fan-out of the grammar can be built
from R. We still need to prove that the algorithm
answers consistently on a given input, despite of
possibly different choices of edges at line 4. We do
this through several intermediate results.
A derivation for an adjacency graph G is a se-
quence of edges d D he1; : : : ; eni, n  1, such
that G D G0 and Gi 1 )ei Gi for every i with
1  i  n. For short, we write G0 )d Gn.
Two derivations for G are competing if one is a
permutation of the other.
Lemma 3 If G )d1 G1 and G )d2 G2 with d1
and d2 competing derivations, then G1 D G2.
Proof We claim that the statement of the lemma
holds for jd1j D 2. To see this, let G )e1
G01 )e2 G1 and G )e2 G
0
2 )e1 G2 be valid
derivations. We observe that G1 and G2 have the
same set of vertices. Since the edges of G1 and G2
are defined by restricting the adjacency relation to
their set of vertices, our claim immediately follows.
The statement of the lemma then follows from
the above claim and from the fact that we can al-
ways obtain the sequence d2 starting from d1 by
repeatedly switching consecutive edges. 
We now consider derivations for the same adja-
cency graph that are not competing, and show that
they always lead to isomorphic adjacency graphs.
Two graphs are isomorphic if they become equal
after some suitable renaming of the vertices.
Lemma 4 The out-degree of G is bounded by 2.
Proof Assume v!G v1 and v!G v2, with v1 ?
v2, and let I 2 v. I must be adjacent to some in-
terval I1 2 v1. Without loss of generality, assume
that I is left-adjacent to I1. I must also be adja-
cent to some interval I2 2 v2. Since v1 and v2
483
are disjoint, I must be right-adjacent to I2. This
implies that I cannot be adjacent to an interval in
any other m-interval v0 of G. 
A vertex v of G such that v!G v1 and v!G v2
is called a bifurcation.
Example 7 Assume v D f ?2; 2? g, v1 D
f ?3; 3?; ?5; 5? g, v2 D f ?1; 1? g with v!G v1 and
v!G v2. The m-interval v? v1 D f ?2; 3?; ?5; 5? g
is no longer adjacent to v2. 
The example above shows that, when choosing one
of the two outgoing edges in a bifurcation for mer-
ging, the other edge might not survive. Thus, such
a choice might lead to distinguishable derivations
that are not competing (one derivation has an edge
that is not present in the other). As we will see (in
the proof of Theorem 1), bifurcations are the only
cases in which edges might not survive a merging.
Lemma 5 Let v be a bifurcation of G with outgo-
ing edges e1; e2, and let G )e1 G1, G )e2 G2.
Then G1 and G2 are isomorphic.
Proof (Sketch) Assume e1 has the form
v!G v1 and e2 has the form v!G v2. Let
also VS be the set of vertices shared by G1 and
G2. We show that the statement holds under the
isomorphism mapping v ? v1 and v2 in G1 to v1
and v ? v2 in G2, respectively.
When restricted to VS , the graphs G1 and G2
are equal. Let us then consider edges from G1 and
G2 involving exactly one vertex in VS . We show
that, for v0 2 VS , v0!G1 v ? v1 if and only if
v0!G2 v1. Consider an arbitrary interval I
0 2 v0.
If v0!G1 v?v1, then I
0 must be adjacent to some
interval I1 2 v ? v1. If I1 2 v1 we are done.
Otherwise, I1 must be the concatenation of two
intervals I1v and I1v1 with I1v 2 v and I1v1 2
v1. Since v!G2 v2, I1v is also adjacent to some
interval in v2. However, v0 and v2 are disjoint.
Thus I 0 must be adjacent to I1v1 2 v1. Conversely,
if v0!G2 v1, then I
0 must be adjacent to some
interval I1 2 v1. Because v0 and v are disjoint, I 0
must also be adjacent to some interval in v ? v1.
Using very similar arguments, we can conclude
that G1 and G2 are isomorphic when restricted to
edges with at most one vertex in VS .
Finally, we need to consider edges from G1 and
G2 that are not incident upon vertices in VS . We
show that v ? v1!G1 v2 only if v1!G2 v ? v2;
a similar argument can be used to prove the con-
verse. Consider an arbitrary interval I1 2 v?v1. If
v ? v1!G1 v2, then I1 must be adjacent to some
interval I2 2 v2. If I1 2 v1 we are done. Other-
wise, I1 must be the concatenation of two adjacent
intervals I1v and I1v1 with I1v 2 v and I1v1 2 v1.
Since I1v is also adjacent to some interval I 02 2 v2
(here I 02 might as well be I2), we conclude that
I1v1 2 v1 is adjacent to the concatenation of I1v
and I 02, which is indeed an interval in v? v2. Note
that our case distinction is exhaustive. We thus
conclude that v1!G2 v ? v2.
A symmetrical argument can be used to show
that v2!G1 v ? v1 if and only if v ? v2!G2 v1,
which concludes our proof. 
Theorem 1 Let d1 and d2 be derivations for G,
describing two different computations c1 and c2 of
the algorithm of Figure 3 on input G. Computation
c1 is accepting if and only if c2 is accepting.
Proof First, we prove the claim that if e is not an
edge outgoing from a bifurcation vertex, then in the
derive relation G )e G0 all of the edges of G but
e and its reverse are inherited by G0. Let us write
e in the form v1!G v2. Obviously, any edge of
G not incident upon v1 or v2 will be inherited by
G0. If v!G v2 for some m-interval v ? v1, then
every interval I 2 v is adjacent to some interval
in v2. Since v and v1 are disjoint, I will also be
adjacent to some interval in v1?v2. Thus we have
v!G0 v1 ? v2. A similar argument shows that
v!G v1 implies v!G0 v1 ? v2.
If v2!G v for some v ? v1, then every in-
terval I 2 v2 is adjacent to some interval in v.
From v1!G v2 we also have that each interval
I12 2 v1 ? v2 is either an interval in v2 or else
the concatenation of exactly two intervals I1 2 v1
and I2 2 v2. (The interval I2 cannot be adjacent
to more than an interval in v1, because v2!G v).
In both cases I12 is adjacent to some interval in
v, and hence v1 ? v2!G0 v. This concludes the
proof of our claim.
Let d1, d2 be as in the statement of the the-
orem, with G )d1 G1 and G )d2 G2. If d1
and d2 are competing, then the theorem follows
from Lemma 3. Otherwise, assume that d1 and d2
are not competing. From our claim above, some
bifurcation vertices must appear in these deriva-
tions. Let us reorder the edges in d1 in such a way
that edges outgoing from a bifurcation vertex are
processed last and in some canonical order. The
resulting derivation has the form dd 01, where d
0
1
involves the processing of all bifurcation vertices.
We can also reorder edges in d2 to obtain dd 02,
where d 02 involves the processing of all bifurcation
484
not context-free 102 687 100.00%
not binarizable 24 0.02%
not well-nested 622 0.61%
Table 1: Properties of productions extracted from
the CoNLL 2006 data (3 794 605 productions)
vertices in exactly the same order as in d 01, but with
possibly different choices for the outgoing edges.
Let G )d Gd )d 01 G
0
1 and G )d Gd )d 02
G02. Derivations dd
0
1 and d1 are competing. Thus,
by Lemma 3, we haveG01 D G1. Similarly, we can
conclude that G02 D G2. Since bifurcation vertices
in d 01 and in d
0
2 are processed in the same canonical
order, from repeated applications of Lemma 5 we
have that G01 and G
0
2 are isomorphic. We then con-
clude that G1 and G2 are isomorphic as well. The
statement of the theorem follows immediately. 
We now turn to a computational analysis of the
algorithm of Figure 3. Let G be the representation
of an LCFRS production p with rank r . G has
r vertices and, following Lemma 4, O.r/ edges.
Let v be an m-interval of G with fan-out fv. The
incoming and outgoing edges for v can be detected
in time O.fv/ by inspecting the 2  fv endpoints of
v. Thus we can compute G in time O.jpj/.
The number of iterations of the while cycle in the
algorithm is bounded by r , since at each iteration
one vertex of G is removed. Consider now an
iteration in which m-intervals v1 and v2 have been
chosen for merging, with v1!G v2. (These m-
intervals might be associated with nonterminals
in the right-hand side of p, or else might have
been obtained as the result of previous merging
operations.) Again, we can compute the incoming
and outgoing edges of v1?v2 in time proportional
to the number of endpoints of such an m-interval.
By Lemma 2, this number is bounded by O.f /, f
the fan-out of the grammar. We thus conclude that
a run of the algorithm on G takes time O.r  f /.
5 Discussion
We have shown how to extract mildly context-
sensitive grammars from dependency treebanks,
and presented an efficient algorithm that attempts
to convert these grammars into an efficiently par-
seable binary form. Due to previous results (Ram-
bow and Satta, 1999), we know that this is not
always possible. However, our algorithm may fail
even in cases where a binarization exists?our no-
tion of adjacency is not strong enough to capture
all binarizable cases. This raises the question about
the practical relevance of our technique.
In order to get at least a preliminary answer to
this question, we extracted LCFRS productions
from the data used in the 2006 CoNLL shared task
on data-driven dependency parsing (Buchholz and
Marsi, 2006), and evaluated how large a portion
of these productions could be binarized using our
algorithm. The results are given in Table 1. Since it
is easy to see that our algorithm always succeeds on
context-free productions (productions where each
nonterminal has fan-out 1), we evaluated our al-
gorithm on the 102 687 productions with a higher
fan-out. Out of these, only 24 (0.02%) could not be
binarized using our technique. We take this number
as an indicator for the usefulness of our result.
It is interesting to compare our approach
with techniques for well-nested dependency trees
(Kuhlmann and Nivre, 2006). Well-nestedness is
a property that implies the binarizability of the
extracted grammar; however, the classes of well-
nested trees and those whose corresponding pro-
ductions can be binarized using our algorithm are
incomparable?in particular, there are well-nested
productions that cannot be binarized in our frame-
work. Nevertheless, the coverage of our technique
is actually higher than that of an approach that
relies on well-nestedness, at least on the CoNLL
2006 data (see again Table 1).
We see our results as promising first steps in a
thorough exploration of the connections between
non-projective and mildly context-sensitive pars-
ing. The obvious next step is the evaluation of our
technique in the context of an actual parser.
As a final remark, we would like to point out
that an alternative technique for efficient non-pro-
jective dependency parsing, developed by G?mez
Rodr?guez et al independently of this work, is
presented elsewhere in this volume.
Acknowledgements We would like to thank
Ryan McDonald, Joakim Nivre, and the anonym-
ous reviewers for useful comments on drafts of this
paper, and Carlos G?mez Rodr?guez and David J.
Weir for making a preliminary version of their pa-
per available to us. The work of the first author
was funded by the Swedish Research Council. The
second author was partially supported by MIUR
under project PRIN No. 2007TJNZRE_002.
485
References
Giuseppe Attardi. 2006. Experiments with a mul-
tilanguage non-projective dependency parser. In
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 166?170, New
York, USA.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency pars-
ing. In Tenth Conference on Computational Natural
Language Learning (CoNLL), pages 149?164, New
York, USA.
Eugene Charniak. 1996. Tree-bank grammars. In 13th
National Conference on Artificial Intelligence, pages
1031?1036, Portland, Oregon, USA.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In 16th In-
ternational Conference on Computational Linguist-
ics (COLING), pages 340?345, Copenhagen, Den-
mark.
Carlos G?mez-Rodr?guez, David J. Weir, and John
Carroll. 2009. Parsing mildly non-projective de-
pendency structures. In Twelfth Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL), Athens, Greece.
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevov?,
Eva Hajic?ov?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. Linguistic Data
Consortium, 2001T10.
Keith Hall and V?clav Nov?k. 2005. Corrective mod-
elling for non-projective dependency grammar. In
Ninth International Workshop on Parsing Technolo-
gies (IWPT), pages 42?52, Vancouver, Canada.
Jir?? Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In 45th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 608?615, Prague, Czech Republic.
Marco Kuhlmann and Mathias M?hl. 2007. Mildly
context-sensitive dependency languages. In 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 160?167, Prague, Czech
Republic.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In 21st In-
ternational Conference on Computational Linguist-
ics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), Main
Conference Poster Sessions, pages 507?514, Sydney,
Australia.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Eleventh Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 81?88, Trento, Italy.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Tenth International Conference on Pars-
ing Technologies (IWPT), pages 121?132, Prague,
Czech Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Human Lan-
guage Technology Conference (HLT) and Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 523?530, Vancouver,
Canada.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 99?106, Ann Arbor, USA.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Eighth International
Workshop on Parsing Technologies (IWPT), pages
149?160, Nancy, France.
Joakim Nivre. 2007. Incremental non-projective
dependency parsing. In Human Language Tech-
nologies: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 396?403, Rochester,
NY, USA.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theoretical Computer Science, 223(1?2):87?
120.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On Multiple Context-
Free Grammars. Theoretical Computer Science,
88(2):191?229.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In 25th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 104?111, Stanford,
CA, USA.
486
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 803?810, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Some Computational Complexity Results
for Synchronous Context-Free Grammars
Giorgio Satta
Dept. of Information Engineering
University of Padua
via Gradenigo, 6/A
I-35131 Padova
Italy
satta@dei.unipd.it
Enoch Peserico
Dept. of Information Engineering
University of Padua
via Gradenigo, 6/A
I-35131 Padova
Italy
enoch@dei.unipd.it
Abstract
This paper investigates some computa-
tional problems associated with proba-
bilistic translation models that have re-
cently been adopted in the literature on
machine translation. These models can be
viewed as pairs of probabilistic context-
free grammars working in a ?synchronous?
way. Two hardness results for the class
NP are reported, along with an exponen-
tial time lower-bound for certain classes
of algorithms that are currently used in the
literature.
1 Introduction
State of the art architectures for machine transla-
tion are all based on mathematical models called
translation models. Generally speaking, a transla-
tion model accounts for all the elementary opera-
tions that rule the process of translation between the
words and the different word orderings of the source
and target languages. Translation models are usu-
ally enriched with statistical parameters, to drive the
search toward the most likely translation(s). Special-
ized algorithms are provided for the automatic esti-
mation of these parameters from corpora of trans-
lation pairs. Besides the task of natural language
translation, statistical translation models are also ex-
ploited in other applications, such as word align-
ment, multilingual document retrieval and automatic
dictionary construction.
The most successful translation models that are
found in the literature exploit finite-state machinery.
The approach started with the so-called IBM mod-
els (Brown et al, 1988), implementing a set of ele-
mentary operations, such as movement, duplication
and translation, that independently act on individ-
ual words in the source sentence. These word-to-
word models have been later enriched with the in-
troduction of larger units such as phrases; see for
instance (Och et al, 1999; Och and Ney, 2002).
Still, the generative capacity of these models lies
within the realm of finite-state machinery (Kumar
and Byrne, 2003), so they are unable to handle
nested structures and do not provide the expressivity
required to process language pairs with very differ-
ent word orderings.
Recently, more sophisticated translation models
have been proposed, borrowing from the theory of
compilers and making use of synchronous rewrit-
ing. In synchronous rewriting, two formal gram-
mars are exploited, one describing the source lan-
guage and the other describing the target language.
Furthermore, the productions of the two gram-
mars are paired and, in the rewriting process, such
pairs are always applied synchronously. Formalisms
based on synchronous rewriting have been empow-
ered with the use of statistical parameters, and spe-
cialized estimation and translation (decoding) algo-
rithms were newly developed. Among the several
proposals, we mention here the models presented
in (Wu, 1997; Wu and Wong, 1998), (Alshawi et al,
2000), (Yamada and Knight, 2001), (Gildea, 2003)
and (Melamed, 2003).
In this paper we consider synchronous models
based on context-free grammars and probabilistic
extensions thereof. This is the most common choice
803
in statistical translation models that exceed the gen-
erative power of finite-state machinery. We focus
on two associated computational problems that have
been defined in the literature. One is the member-
ship problem, which involves testing whether an in-
put string pair can be generated by the model. The
other is the translation problem (also called the de-
coding problem) which involves the search for a
suitable translation of an input string/structure. It
has been often informally stated in the literature
that the use of structured models results in efficient,
polynomial time algorithms for the above problems.
We show here that sometimes this is not the case.
The contribution of this paper can be stated as fol-
lows:
? we show that the membership problem is NP-
hard, unless a constant bound is imposed on the
length of the productions (Section 3);
? we show an exponential time lower bound for
the membership problem, in case chart parsing
is adopted (Section 3);
? we show that translating an input string into
the best parse tree in the target language is NP-
hard, even in case productions are bounded in
length (Section 4).
Investigation of the computational complexity of
translation models has started in (Knight, 1999) for
word-to-word models. This paper can be seen as the
continuation of that line of research.
2 Synchronous context-free grammars
Several definitions for synchronous context-free
grammars have been proposed in the literature; see
for instance (Chiang, 2004; Chiang, 2005). Our
definition is based on syntax-directed translation
schemata (SDTS; Aho and Ullman, 1972), with the
difference that we do not impose the restriction that
two paired context-free productions have the same
left-hand side. As it will be discussed in Section 4,
this results in an enriched generative capacity when
probabilistic extensions are considered. We assume
the reader is familiar with the definition of context-
free grammar (CFG) and with the associated notion
of derivation.
Let VN and VT be sets of nonterminal and termi-
nal symbols, respectively. In what follows we need
to represent bijections between all the occurrences
of nonterminals in two strings over VN ? VT . This
can be done by annotating nonterminals with indices
from an infinite set. We define I(VN ) = {A(t) |
A ? VN , t ? N} and VI = I(VN ) ? VT . We
write index(?), ? ? V ?I , to denote the set of all in-
dices (the integers t) that appear in symbols in ?.
Two strings ?, ?? ? V ?I are synchronous if each in-
dex in index(?) occurs only once in ?, each index
in index(??) occurs only once in ??, and index(?) =
index(??). Therefore synchronous strings have the
general form
u10A
(t1)
11 u11A
(t2)
12 u12 ? ? ? u1r?1A
(tr)
1r u1r,
u20A
(tpi(1))
21 u21A
(tpi(2))
22 u22 ? ? ? u2r?1A
(tpi(r))
2r u2r,
where r ? 0, u1i, u2i ? V ?T , A
(ti)
1i , A
(tpi(i))
2i ?
I(VN ), ti 6= tj for i 6= j and pi is some permuta-
tion defined on set {1, . . . , r}.
Definition 1 A synchronous context-free gram-
mar (SCFG) is a tuple G = (VN , VT , P, S), where
VN , VT are finite, disjoint sets of nonterminal and
terminal symbols, respectively, S ? VN is the start
symbol and P is a finite set of synchronous produc-
tions, each of the form [A1 ? ?1, A2 ? ?2], with
A1, A2 ? VN and ?1, ?2 ? V ?I synchronous strings.
The size of a SCFG G is defined as |G| =
?
[A1??1, A2??2]?P |A1?1A2?2|. Based on an ex-
ample from (Yamada and Knight, 2001), we provide
a sample SCFG fragment translating from English to
Japanese, specified by means of the following syn-
chronous productions:
s1 : [VB ? PRP(1) VB1(2) VB2(3),
VB ? PRP(1) VB2(3) VB1(1)]
s2 : [VB2 ? VB(1) TO(2),
VB2 ? TO(2) VB(1) ga]
s3 : [TO ? TO(1) NN(2), TO ? NN(2) TO(1)]
s4 : [PRP ? he, PRP ? kare ha]
s5 : [VB1 ? adores, VB1 ? daisuki desu]
s6 : [VB ? listening, VB ? kiku no]
s7 : [TO ? to, TO ? wo]
s8 : [NN ? music, NN ? ongaku]
Note that in production s2 above, the nonterminals
VB and TO generated from nonterminal VB2 in
804
the English component are inverted in the Japanese
component, where some additional lexical material
is also added.
In a SCFG, the ?derives? relation is defined on
synchronous strings in terms of simultaneous rewrit-
ing of two nonterminals with the same index. Some
additional notation will help us defining this rela-
tion precisely. A reindexing is a one-to-one func-
tion on N. We extend a reindexing f to VI by letting
f(A(t)) = A(f(t)) for A(t) ? I(VN ) and f(a) = a
for a ? VT . We also extend f to strings in V ?I by
letting f(?) = ? and f(X?) = f(X)f(?), for each
X ? VI and ? ? V ?I . We say that strings ?1, ?2 ?
V ?I are independent if index(?1) ? index(?2) = ?.
Definition 2 Let G = (VN , VT , P, S) be a SCFG
and let ?1, ?2 be synchronous strings in V ?I . The
derives relation [?1, ?2] ?G [?1, ?2] holds
whenever there exist an index t in index(?1), a syn-
chronous production [A1 ? ?1, A2 ? ?2] in P
and some reindexing f such that
(i) f(?1?2) and ?1?2 are independent; and
(ii) ?i = ??iA
(t)
i ?
??
i , ?i = ?
?
if(?i)?
??
i , for i = 1, 2.
We also write [?1, ?2] ?sG [?1, ?2] to explicitly
indicate that the derives relation holds through some
synchronous production s ? P .
Since ?1 and ?2 in Definition 2 are synchronous
strings, we can define the reflexive and transitive
closure of ?G, written ??G. This relation is used
to represent derivations in G. In case we have
[?1i?1, ?2i?1] ?
si
G [?1i, ?2i] for 1 ? i ? n,
n ? 1, we also write [?10, ?20] ??G [?1n, ?2n],
where ? = s1s2 ? ? ? sn. We always assume some
canonical form for derivations (as for instance left-
most derivation on the left component). Similarly to
the case of context-free grammars, each derivation
in G can be associated with a pair of parse trees, that
is, one parse tree for each dimension.
Back to our example, we report a fragment of a
derivation of the string pair [he adores listening to
music, kare ha ongaku wo kiku no ga daisuki desu]:
[VB(1), VB(1)]
?s1G [PRP
(2) VB1(3) VB2(4),
PRP(2) VB2(4) VB1(3)]
?s4G [he VB1
(3) VB2(4),
kare ha VB2(4) VB1(3)]
?s5G [he adores VB2
(4),
kare ha VB2(4) daisuki desu]
?s2G [he adores VB
(5) TO(6),
kare ha TO(6) VB(5) ga daisuki desu].
The translation generated by a SCFG G is a bi-
nary relation over V ?T defined as
T (G) = {[w1, w2] | [S
(1), S(1)] ??G [w1, w2],
w1, w2 ? V
?
T }.
The set of strings that are translations of a given
string w1 is defined as:
T (G,w1) = {w2 | [w1, w2] ? T (G)}.
A probabilistic SCFG (PSCFG) is a pair (G, pG)
where G = (VN , VT , P, S) is a SCFG and pG is a
function from P to real numbers in [0, 1] such that,
for each A1, A2 ? VN , we have:
?
?1,?2
pG([A1 ? ?1, A2 ? ?2] = 1.
If for n ? 1 and si ? P , 1 ? i ? n, string
? = s1s2 ? ? ? sn is a canonical derivation of the form
[S(1), S(1)] ??G [w1, w2], we write pG(?) =?n
i=1 pG(si). If D([w1, w2]) is the set of all canon-
ical derivations in G for pair [w1, w2], we write
pG([w1, w2]) =
?
??D([w1,w2]) pG(?).
3 The membership problem
We consider here the membership problem for
SCFG, defined as follows: for input instance a
SCFG G and a pair [w1, w2], decide whether
[w1, w2] is in T (G). This problem has been con-
sidered for instance in (Wu, 1997) for his inver-
sion transduction grammars and has applications in
the support of several tasks of automatic annotation
of parallel corpora, as for instance segmentation,
bracketing, phrasal and word alignment. We show
that the membership problem for SCFGs is NP-
hard. The result could be derived from the findings
in (Melamed et al, 2004) that synchronous rewriting
systems as SCFGs are related to the class of so called
linear context-free rewriting systems (LCFRSs) and
from the result that the membership problem for
805
LCFRSs is NP-hard (Satta, 1992; Kaji and others,
1994). However, we provide here a direct proof, to
simplify the presentation.
Theorem 1 The membership problem for SCFGs is
NP-hard.
Proof. We reduce from the three-satisfiability
problem (3SAT, Garey and Johnson, 1979). Let
?U,C? be an instance of the 3SAT problem, where
U = {u1, . . . , up} is a set of variables and C =
{c1, . . . , cn} is a set of clauses. Each clause is a set
of three literals from {u1, u1, . . . , up, up}.
The general idea of the proof is to use a string
pair [w1w2 ? ? ?wp, wc], where wc is a string repre-
sentation of C and each wi is a string controlling the
truth assignment for the variable ui. We then con-
struct a SCFG G such that each wi can be derived
in two possible ways only, using some specialized
productions of G, encoding the truth assignment of
variable ui. In this way the derivation of the whole
string w1 ? ? ?wp in the left dimension corresponds to
a guess of a truth assignment for U . Accordingly, on
the right dimension only those symbols of wc will
be derived that represent clauses that hold true un-
der the guessed assignment.
We need some additional notation. Below we
treat C as an alphabet of atomic symbols. We use
a function d such that, for every i with 1 ? i ?
p, cd(i,1), cd(i,2), . . . , cd(i,si) is the sequence of all
clauses that include literal ui, in the left to right
order in which they appear within c1c2 ? ? ? cn, and
cd(i,si+1), cd(i,si+2), . . . , cd(i,ti) is the sequence of all
clauses that include literal ui, again as they appear
within c1c2 ? ? ? cn from left to right. Note that we
must have
?p
i=1 ti = 3n. We also use a function
e such that, for every 1 ? i ? p and 1 ? j ? ti,
e(i, j) = j +
?i?1
k=1 tk (assume
?0
k=1 tk = 0).
Consider the alphabet {ai, bi | 1 ? i ? p}. For
every i, 1 ? i ? p, let wi denote a sequence of
exactly ti + 1 alternating symbols ai and bi, i.e.,
wi ? (aibi)+ ? (aibi)?ai. For every 1 ? i ? p,
let x(i, 1) = aibi and let x(i, h) = ai (resp. bi)
if h is even (resp. odd), 2 ? h ? ti. Let
also x(i, h) = ai (resp. bi) if h is odd (resp.
even), 1 ? h ? ti ? 1, and let x(i, ti) = aibi
(resp. biai) if ti is odd (resp. even). There-
fore we can write wi = x(i, 1)x(i, 2) ? ? ?x(i, t1) =
x(i, 1)x(i, 2) ? ? ?x(i, t1).
Finally, we need a permutation pi defined on the
set {1, . . . , 3n} as follows. Fix i and j with 1 ? i ?
p and 1 ? j ? ti, and let h be the number of oc-
currences of the clause cd(i,j) found in the sequence
cd(1,1), cd(1,2), . . ., cd(1,t1), cd(2,1), . . ., cd(i,j). Note
that we must have 1 ? h ? 3. Then we set
pi(e(i, j)) = 3 ? [d(i, j)? 1] + h.
We can now define the target instance
?G, [w,w?]? of our reduction. Let [w,w?] =
[w1w2 ? ? ?wp, c1c2 ? ? ? cn]. Let alo G = (VN , VT ,
P, S), with VN = {S} ? {Ai | 1 ? i ? 3n} and
VT = C ? {ai, bi | 1 ? i ? p}. The productions
below define set P :
(i) for every 1 ? i ? p:
(a) for 1 ? h ? si:
[Ae(h,i) ? x(i, h), Ae(h,i) ? ce(i,h)],
[Ae(h,i) ? x(i, h), Ae(h,i) ? ?],
[Ae(h,i) ? x(i, h), Ae(h,i) ? ?];
(b) for si + 1 ? h ? ti:
[Ae(h,i) ? x(i, h), Ae(h,i) ? ?],
[Ae(h,i) ? x(i, h), Ae(h,i) ? ce(i,h)],
[Ae(h,i) ? x(i, h), Ae(h,i) ? ?];
(ii) [S ? A(e(1,1))e(1,1) A
(e(1,2))
e(1,2) ? ? ?
A(e(1,t1))e(1,t1) A
(e(2,1))
e(2,1) ? ? ?A
(e(p,tp))
e(p,tp)
,
S ? A(pi(e(1,1)))pi(e(1,1)) A
(pi(e(1,2)))
pi(e(1,2)) ? ? ?
A(pi(e(1,t1)))pi(e(1,t1)) A
(pi(e(2,1)))
pi(e(2,1)) ? ? ?A
(pi(e(p,tp)))
pi(e(p,tp))
].
It is easy to see that |G|, |w| and |w?| are polyno-
mially related to |U | and |C|. From a derivation of
[w,w?] ? T (G), we can exhibit a truth assignment
that satisfies C simply by reading off the derivation
of the left string w1w2 ? ? ?wp. Conversely, starting
from a truth assignment that satisfiesC we can prove
w ? L(G) by means of (finite) induction on |U |: this
part requires a careful inspection of all items in the
definition of G.
From Theorem 1 we may conclude that algo-
rithms for the membership problem for SCFGs are
very unlikely to run in polynomial time. In the
literature, several algorithms for this problem have
been proposed using tabular methods (chart pars-
ing). In the worst case, all these algorithms run in
time ?(|G| ? nk(G)), with G an SCFG and n the
806
length of the input string pair. We know that, un-
less P = NP, k(G) cannot be a constant. We now
prove a lower bound on k(G), providing thereby an
exponential time lower bound result for our problem
under the assumption of the tabular paradigm.
Tabular methods for the membership problem are
based on the following representation. Given a syn-
chronous production
s : [A1 ? B
(1)
11 ? ? ?B
(r)
1r ,
A2 ? B
(pi(1))
21 ? ? ?B
(pi(r))
2r ], (1)
the already recognized constituent pairs B1i, B2pi(i)
are gather together in several steps, keeping a record
of the spanned substrings of the input. To pro-
vide a concrete example, if we gather all the B1i?s
on the left dimension from left to right, the partial
analysis we obtain after the first step can be repre-
sented as a state ?s(1), (i11, j11), (i21, j21)?, mean-
ing that B11 and B2pi(1) span substrings w1[i11, j11]
and w2[i21, j21], respectively.1 At the second
step we have a state ?s(2), (i11, j12), (i21, j21),
(i22, j22)?, meaning that B11B12 together span
w1[i11, j12], B2pi(1) spans w2[i21, j21] and B2pi(2)
spans w2[i22, j22]. We can see that, for some worst
case permutations, the left-to-right strategy demands
for increasingly more pairs of indices, so that the ex-
ponent in the time complexity linearly grows with r.
How much better can we do, if we exploit some
strategy other than the left-to-right above? More
precisely, we ask how many unconnected spannings
a state may require for some worst case permutation
pi, under the choice of the best possible parsing strat-
egy for pi itself.
Theorem 2 In the worst case, standard tabular
methods for the SCFG membership problem require
an amount of time ?(|G|nc?
?
r), with r the length of
the longest production in G and c a constant.
Proof. For any r ? 8 we let q = b
?
r/2c ?
b
?
8/2c = 2, and define a permutation pir on
{1, . . . , r}. We view the domain of pir as composed
of 2q blocks with q adjacent integers each, possi-
bly followed by r ? 2q2 additional ?padding? in-
tegers, and its codomain as composed of q blocks
1For a string w = a1 ? ? ? an, we write w[i, j] to denote the
substring ai+1 ? ? ? aj .
with 2q adjacent integers each, again possibly fol-
lowed by r ? 2q2 ?padding? integers. Permutation
pir transposes all blocks by sending the j-th element
of the i-th block in the domain into the i-th element
of the j-th block in the codomain, while mapping
each padding integer identically into itself. For-
mally, for all positive integers i ? 2q and j ? q,
pir(q ? (i ? 1) + j) = 2q ? (j ? 1) + i, and for all
integers i with 2q2 < i ? r, pir(i) = i.
We count below how many spans are instanti-
ated by a state that has gathered p constituent pairs,
1 ? p ? r, in parsing production (1) under any pos-
sible strategy. When a constituent pair B1i, B2pir(i)
is gathered, we say integer i in the domain of pir and
integer pir(i) in the codomain have been pebbled. In
this way each span (i, j) in a state corresponds to
some run i, i + 1, . . . j of pebbled integers, with ei-
ther i = 1 or i? 1 unpebbled, and with either j = r
or j + 1 unpebbled. We call each such run a seg-
ment, and show that every parsing strategy demands
at least q = b
?
r/2c segments either in the domain
or in the codomain of pir.
We say that a block in the domain of pir is empty,
full, or mixed if, respectively, none, all, or some but
not all of its elements have been pebbled. Assume
that, for a given parsing strategy, the last block that
becomes mixed does so when we place the i-th peb-
ble, and the first block that becomes full does so
when we place the j-th pebble. Obviously i 6= j:
the first pebble placed in a previously empty block
can not make it full since every block contains at
least 2 elements.
If i < j, after placing the i-th pebble and before
placing the j-th pebble every block in the domain of
pir is mixed. Each of these 2q blocks then contains
at least one pebbled element which is adjacent to an
unpebbled one and must therefore be either the first
or the last element of a segment. The domain of pir
then contains at least 2q/2 = q segments.
If j < i, after placing the j-th pebble and be-
fore placing the i-th pebble at least one block in the
domain of pir (e.g., the h-th block) is full, and at
least one (e.g., the k-th) is empty. Then, in each
of the q blocks in the codomain of pir, the h-th el-
ement is pebbled while the k-th is not. Therefore
the h-th elements of any two consecutive blocks in
the codomain of pir must belong to two distinct seg-
ments, since at least one intermediate element is not
807
pebbled. The codomain of pir then contains at least
q segments.
4 The translation problem
In this section we consider some formulations of the
translation problem for PSCFG that have been pro-
posed in the literature. The most general definition
of the translation problem for PSCFG is this: for
an input PSCFG Gp = (G, pG) and an input string
w, produce a representation of all possible parse
trees, along with their probabilities, that are assigned
byG to a string in the set T (G,w) under some trans-
lation of w.
Variant of this definition can be found where the
input is a single parse tree for w (Yamada and
Knight, 2001), or where the output is a single parse
tree, chosen according to some specific criteria (Wu
andWong, 1998). To formally study these problems,
in what follows we focus on single parse trees asso-
ciated with derivations in Gp. For a derivation ? of
the form [S(1), S(1)] ??G [w1, w2], we write t?,l and
t?,r to denote the left and the right parse trees, re-
spectively, associated with ?. The probability that
t?,r is obtained as a translation of t?,l through Gp is
thus pG([t?,l, t?,r]) = pG(?). Let t be some parse
tree; we write y(t) to denote the string in the yield
of t. For a string w ? V ?T and a parse tree t, we
also consider the probability that t is obtained from
w through Gp, defined as:
pG([w, t]) =
?
y(t?)=w
pG([t
?, t]). (2)
We can now precisely define the variants of the
translation problem we are interested in. Given
as input a PSCFG Gp = (G, pG) and two strings
w1, w2 ? V ?T , output the pair of parse trees
argmax
y(t1) = w1,
y(t2) = w2
pG([t1, t2]). (3)
If the synchronous productions in the underlying
SCFG G have length bounded by some constant,
then the above problem can be solved in polynomial
time using extensions of the Viterbi search strategy
to parse forests. This has been shown for instance
in (Wu and Wong, 1998; Yamada and Knight, 2001;
Melamed, 2004).
A second interesting problem is defined as fol-
lows. Given as input a PSCFG Gp = (G, pG) and a
string w ? V ?T , output the parse tree
argmax
t
pG([w, t]). (4)
Even in case we impose some constant bound on
the length of the synchronous productions in G, the
above problem is NP-hard, as we show in what fol-
lows.
We assume the reader is familiar with the defini-
tion of probabilistic context-free grammar (PCFG)
and with the associated notion of derivation prob-
ability (Wetherell, 1980). We denote a PCFG as
a pair (G, pG), with G = (VN , VT , P, S) the un-
derlying context-free grammar and pG the associ-
ated function providing the probability distributions
for the productions in P , conditioned on their left-
hand side. A probabilistic regular grammar (PRG)
is a PCFG with underlying productions of the form
A ? aB or A ? ?, with A,B nonterminal symbols
and a a terminal symbol.
We consider below a decision problem associated
with PRG, called the consensus problem, defined as
follows: Given as input a PRG (G, pG) and a ra-
tional number d ? [0, 1], decide whether there ex-
ists a string w in the language generated by G such
that pG(w) ? d. It has been shown in (Casacuberta
and de la Higuera, 2000) that, for a PRG G whose
productions have all probabilities expressed by ra-
tional numbers, the above problem is NP-complete.
(Essentially the same result is also reported in (Lyn-
gso and Pedersen, 2002), stated in terms of hidden
Markov models.) We reduce the consensus problem
for PRG to a decision version of the problem in (4),
called the best translated derivation problem and
defined as follows. Given as input a PCFG Gp =
(G, pG), a string w ? V ?T and a rational number
d ? [0, 1], decide whether maxt pG([w, t]) ? d.
Theorem 3 The best translated derivation problem
for the class PSCFG is NP-hard.
Proof. We provide a reduction from the consensus
problem for the class PRG with rational production
probabilities. The main idea is described in what fol-
lows. Given the input PRG Gp, we construct a target
PSCFG G?p that translates string $ into $, with $ a
special symbol. Given as input the string $, G?p sim-
ulates all possible derivations of Gp through its own
808
derivations. This is done by encoding the nontermi-
nals appearing in a derivation ? of Gp within the left
component of some derivation ? of G?p, and by en-
coding the terminal string generated by ? within the
right component of ?. The probability of ? is also
preserved by ?.
Let Gp = (G, pG), d be an instance of the con-
sensus problem as above, with G = (VN , VT , P, S).
We specify a PSCFG G?p = (G
?, pG?) with G? =
(V ?N , {$}, P
?, S) and V ?N = VN ? VT . Set P
? is con-
structed as follows:
(i) for every (S ? aA) ? P , s : [S ? A(1), S ?
a(1)] is added to P ?, with pG?(s) = pG(S ?
aA);
(ii) for every (S ? ?) ? P , s : [S ? $, S ? $] is
added to P ?, with pG?(s) = pG(S ? ?);
(iii) for every a ? VT and (A ? bB) ? P , s :
[A ? B(1), a ? b(1)] is added to P ?, with
pG?(s) = pG(A ? bB)
(iv) for every a ? VT and (A ? ?) ? P ,
s : [A ? $, a ? $] is added to P ?, with
pG?(s) = pG(A ? ?).
Note that the construction of G?p can be carried out
in quadratic time in the size of Gp. It is not diffi-
cult to see that there exists a derivation of the form
S ?G a1A1 ?G a1a2A2 ? ? ? ?G a1a2 ? ? ? anAn
if and only if there exist a derivation in G? asso-
ciated with unary trees t1 and t2, such that string
SA1A2 ? ? ?An is read from the spine of t1 and string
Sa1a2 ? ? ? an is read from the spine of t2. Further-
more, the two derivations are composed of ?corre-
sponding? productions with the same probabilities.
We conclude that there exists a string w in L(G)
with pG(w) > d if and only if there exists a unary
tree t with string Sw$ read from the spine such that
pG?([$, t]) > d.
We discuss below an interesting consequence of
Theorem 3. The SDTS formalism discussed in Sec-
tion 1 has been extended to the probabilistic case
in (Maryanski and Thomason, 1979), called stochas-
tic SDTS (SSDTS). As a corollary to the proof of
Theorem 3, we obtain that one can define, through
some PSCFG Gp and some fixed string w, a proba-
bility distribution pG([w, t]) on parse trees that can-
not be obtained through any SSDTS. Without pro-
viding the details of the definition of SSDTS, we
give here only an outline of the proof. We also as-
sume that the reader is familiar with probabilistic
finite automata and with their distributional equiv-
alence with PRG.
Consider the PSCFG G?p = (G
?, pG?) defined in
the proof of Theorem 3, and assume there exists
some SSDTS G??p = (G
??, pG??) such that, for every
tree t, we have pG??([$, t]) = pG?([$, t]). Since in a
derivation of an SDTS the generated trees are always
isomorphic, up to some reordering of sibling nodes,
we obtain that the productions of G?? must have the
form [S ? a(1), S ? a(1)], [a ? b(1), a ? b(1)]
and [a ? $, a ? $]. From these productions we
can construct a probabilistic deterministic finite au-
tomaton generating the same language as the PRG
Gp, and with the same distribution. But this is im-
possible since there are string distributions defined
by some PRG that cannot be obtained through prob-
abilistic deterministic finite automata; see for in-
stance (Vidal et al, 2005).
We conclude by remarking that in (Casacuberta
and de la Higuera, 2000) it is shown that finding
the best output string for a given input string is NP-
hard for stochastic SDTS with a single nonterminal
in each production?s right-hand side. Our result in
Theorem 3, stated for PSCFG, is stronger, since it in-
vestigates individual parse trees rather than strings.
5 Concluding remarks
The presented results are based on worst case analy-
sis: further experimental evaluation needs to be car-
ried out on multilingual corpora in order to asses the
practical impact of these findings.
Acknowledgment
We are indebted to Dan Melamed and Mark-Jan
Nederhof for technical discussion on topics related
to this paper. Dan Melamed also suggested to us the
problem investigated by Theorem 2. The first author
is partially supported by MIUR under project PRIN
No. 2003091149 005.
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation and Compiling, volume 1. Prentice-
Hall, Englewood Cliffs, NJ.
809
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60, March.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, Robert L.
Mercer, and Paul Roossin. 1988. A statistical ap-
proach to language translation. In Proceedings of
the International Conference on Computational Lin-
guistics (COLING) 1988, pages 71?76, Budapest,
Hungary, August.
F. Casacuberta and C. de la Higuera. 2000. Computa-
tional complexity of problems on probabilistic gram-
mars and transducers. In L. Oliveira, editor, Gram-
matical Inference: Algorithms and Applications; 5th
International Colloquium, ICGI 2000, pages 15?24.
Springer.
D. Chiang. 2004. Evaluating Grammar Formalisms for
Applications to Natural Language Processing and By-
ological Sequence Analysis. Ph.D. thesis, Department
of Computer and Information Science, University of
Pennsylvania.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of the 43rd
ACL, pages 263?270.
M. R. Garey and D. S. Johnson. 1979. Computers and
Intractability. Freeman and Co., New York, NY.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL), Sapporo, Japan, July.
Y. Kaji et al 1994. The computational complexity of
the universal recognition problem for parallel multiple
context-free grammars. Computational Intelligence,
10(4):440?452.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, Squibs and Discussion, 25(4).
S. Kumar and W. Byrne. 2003. A weighted finite state
transducer implementation of the alignment template
model for statistical machine translation. In Proceed-
ings of HLT-NAACL.
R. B. Lyngso and C. N. S. Pedersen. 2002. The con-
sensus string problem and the complexity of compar-
ing hidden markov models. Journal of Computing and
System Science, 65:545?569.
Fred J. Maryanski and Michael G. Thomason. 1979.
Properties of stochastic syntax-directed translation
schemata. International Journal of Computer and In-
formation Sciences, 8(2):89?110.
I. Dan Melamed, Giorgio Satta, and Benjamin Welling-
ton. 2004. Generalized multitext grammars. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Barcelona,
Spain.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of the Human Lan-
guage Technology Conference and the North Ameri-
can Association for Computational Linguistics (HLT-
NAACL), pages 158?165, Edmonton, Canada.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Barcelona, Spain.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, July.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 4nd Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 20?28, College Park, Mary-
land.
G. Satta. 1992. Recognition of linear context-free rewrit-
ing systems. In Proc. of the 30th ACL, Newark,
Delaware.
E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,
and R. C. Carrasco. 2005. Probabilistic finite-state
machines ? Part I. IEEE Trans. on Pattern analysis
and Machine Intelligence. To appear.
C. S. Wetherell. 1980. Probabilistic languages: A re-
view and some open questions. Computing Surveys,
12(4):361?379.
Dekai Wu and Hongsing Wong. 1998. Machine trans-
lation with a stochastic grammatical channel. In Pro-
ceedings of the 35th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Montreal,
Canada, July.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Septem-
ber.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 531?538, Toulouse,
July.
810
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 335?342,
New York, June 2006. c?2006 Association for Computational Linguistics
Cross-Entropy and Estimation of
Probabilistic Context-Free Grammars
Anna Corazza
Department of Physics
University ?Federico II?
via Cinthia
I-80126 Napoli, Italy
corazza@na.infn.it
Giorgio Satta
Department of Information Engineering
University of Padua
via Gradenigo, 6/A
I-35131 Padova, Italy
satta@dei.unipd.it
Abstract
We investigate the problem of training
probabilistic context-free grammars on
the basis of a distribution defined over
an infinite set of trees, by minimizing
the cross-entropy. This problem can be
seen as a generalization of the well-known
maximum likelihood estimator on (finite)
tree banks. We prove an unexpected the-
oretical property of grammars that are
trained in this way, namely, we show
that the derivational entropy of the gram-
mar takes the same value as the cross-
entropy between the input distribution and
the grammar itself. We show that the re-
sult also holds for the widely applied max-
imum likelihood estimator on tree banks.
1 Introduction
Probabilistic context-free grammars are able to de-
scribe hierarchical, tree-shaped structures underly-
ing sentences, and are widely used in statistical nat-
ural language processing; see for instance (Collins,
2003) and references therein. Probabilistic context-
free grammars seem also more suitable than finite-
state devices for language modeling, and several
language models based on these grammars have
been recently proposed in the literature; see for in-
stance (Chelba and Jelinek, 1998), (Charniak, 2001)
and (Roark, 2001).
Empirical estimation of probabilistic context-free
grammars is usually carried out on tree banks, that
is, finite samples of parse trees, through the max-
imization of the likelihood of the sample itself. It
is well-known that this method also minimizes the
cross-entropy between the probability distribution
induced by the tree bank, also called the empirical
distribution, and the tree probability distribution in-
duced by the estimated grammar.
In this paper we generalize the maximum like-
lihood method, proposing an estimation technique
that works on any unrestricted tree distribution de-
fined over an infinite set of trees. This generalization
is theoretically appealing, and allows us to prove un-
expected properties of the already mentioned maxi-
mum likelihood estimator for tree banks, that were
not previously known in the literature on statistical
natural language parsing. More specifically, we in-
vestigate the following information theoretic quanti-
ties
? the cross-entropy between the unrestricted tree
distribution given as input and the tree distri-
bution induced by the estimated probabilistic
context-free grammar; and
? the derivational entropy of the estimated prob-
abilistic context-free grammar.
These two quantities are usually unrelated. We show
that these two quantities take the same value when
the probabilistic context-free grammar is trained us-
ing the minimal cross-entropy criterion. We then
translate back this property to the method of max-
imum likelihood estimation. Our general estima-
tion method also has practical applications in cases
one uses a probabilistic context-free grammar to ap-
proximate strictly more powerful rewriting systems,
335
as for instance probabilistic tree adjoining gram-
mars (Schabes, 1992).
Not much is found in the literature about the
estimation of probabilistic grammars from infinite
distributions. This line of research was started
in (Nederhof, 2005), investigating the problem of
training an input probabilistic finite automaton from
an infinite tree distribution specified by means of an
input probabilistic context-free grammar. The prob-
lem we consider in this paper can then be seen as
a generalization of the above problem, where the in-
put model to be trained is a probabilistic context-free
grammar and the input distribution is an unrestricted
tree distribution. In (Chi, 1999) an estimator that
maximizes the likelihood of a probability distribu-
tion defined over a finite set of trees is introduced,
as a generalization of the maximum likelihood es-
timator. Again, the problems we consider here can
be thought of as generalizations of such estimator to
the case of distributions over infinite sets of trees or
sentences.
The remainder of this paper is structured as fol-
lows. Section 2 introduces the basic notation and
definitions and Section 3 discusses our new esti-
mation method. Section 4 presents our main re-
sult, which is transferred in Section 5 to the method
of maximum likelihood estimation. Section 6 dis-
cusses some simple examples, and Section 7 closes
with some further discussion.
2 Preliminaries
Throughout this paper we use standard notation and
definitions from the literature on formal languages
and probabilistic grammars, which we briefly sum-
marize below. We refer the reader to (Hopcroft and
Ullman, 1979) and (Booth and Thompson, 1973) for
a more precise presentation.
A context-free grammar (CFG) is a tuple G =
(N,?,R, S), where N is a finite set of nonterminal
symbols, ? is a finite set of terminal symbols dis-
joint from N , S ? N is the start symbol and R is a
finite set of rules. Each rule has the form A ? ?,
where A ? N and ? ? (? ? N)?. We denote by
L(G) and T (G) the set of all strings, resp., trees,
generated by G. For t ? T (G), the yield of t is
denoted by y(t).
For a nonterminal A and a string ?, we write
f(A,?) to denote the number of occurrences of A
in ?. For a rule (A ? ?) ? R and a tree t ? T (G),
f(A ? ?, t) denotes the number of occurrences of
A ? ? in t. We let f(A, t) =?? f(A ? ?, t).
A probabilistic context-free grammar (PCFG) is
a pair G = (G, pG), with G a CFG and pG a func-
tion from R to the real numbers in the interval [0, 1].
A PCFG is proper if for every A ? N we have
?
? pG(A ? ?) = 1. The probability of t ? T (G)
is the product of the probabilities of all rules in t,
counted with their multiplicity, that is,
pG(t) =
?
A??
pG(A ? ?)f(A??,t). (1)
The probability of w ? L(G) is the sum of the prob-
abilities of all the trees that generate w, that is,
pG(w) =
?
y(t)=w
pG(t). (2)
A PCFG is consistent if
?
t?T (G) pG(t) = 1.
In this paper we write log for logarithms in base 2
and ln for logarithms in the natural base e. We also
assume 0 ? log 0 = 0. We write Ep to denote the
expectation operator under distribution p. In case G
is proper and consistent, we can define the deriva-
tional entropy of G as the expectation of the infor-
mation of parse trees in T (G), computed under dis-
tribution pG, that is,
Hd(pG) = EpG log
1
pG(t)
= ?
?
t?T (G)
pG(t) ? log pG(t). (3)
Similarly, for each A ? N we also define the non-
terminal entropy of A as
HA(pG) =
= EpG log
1
pG(A ? ?)
= ?
?
?
pG(A ? ?) ? log pG(A ? ?). (4)
3 Estimation based on cross-entropy
Let T be an infinite set of (finite) trees with inter-
nal nodes labeled by symbols in N , root nodes la-
beled by S ? N and leaf nodes labeled by symbols
336
in ?. We assume that the set of rules that are ob-
served in the trees in T is drawn from some finite set
R. Let pT be a probability distribution defined over
T , that is, a function from T to set [0, 1] such that
?
t?T pT (t) = 1.
The skeleton CFG underlying T is defined as
G = (N,?,R, S). Note that we have T ? T (G)
and, in the general case, there might be trees in T (G)
that do not appear in T . We wish anyway to approx-
imate distribution pT the best we can, by turning
G into some proper PCFG G = (G, pG) and set-
ting parameters pG(A ? ?) appropriately, for each
(A ? ?) ? R.
One possible criterion is to choose pG in such a
way that the cross-entropy between pT and pG is
minimized, where we now view pG as a probability
distribution defined over T (G). The cross-entropy
between pT and pG is defined as the expectation un-
der distribution pT of the information, computed un-
der distribution pG, of the trees in T (G)
H(pT || pG) = EpT log
1
pG(t)
= ?
?
t?T
pT (t) ? log pG(t). (5)
Since G should be proper, the minimization of (5) is
subject to the constraints
?
? pG(A ? ?) = 1, for
each A ? N .
To solve the minimization problem above, we use
Lagrange multipliers ?A for each A ? N and define
the form
? =
?
A?N
?A ? (
?
?
pG(A ? ?)? 1) +
?
?
t?T
pT (t) ? log pG(t). (6)
We now view ? as a function of all the ?A and the
pG(A ? ?), and consider all the partial derivatives
of ?. For each A ? N we have
??
??A
=
?
?
pG(A ? ?)? 1.
For each (A ? ?) ? R we have
??
?pG(A ? ?)
=
= ?A ?
?
?pG(A ? ?)
?
t?T
pT (t) ? log pG(t)
= ?A ?
?
t?T
pT (t) ?
?
?pG(A ? ?)
log pG(t)
= ?A ?
?
t?T
pT (t) ?
?
?pG(A ? ?)
log
?
(B??)?R
pG(B ? ?)f(B??,t)
= ?A ?
?
t?T
pT (t) ?
?
?pG(A ? ?)
?
(B??)?R
f(B ? ?, t) ? log pG(B ? ?)
= ?A ?
?
t?T
pT (t) ?
?
(B??)?R
f(B ? ?, t) ?
?
?pG(A ? ?)
log pG(B ? ?)
= ?A ?
?
t?T
pT (t) ? f(A ? ?, t) ?
? 1ln(2) ?
1
pG(A ? ?)
= ?A ?
1
ln(2) ?
1
pG(A ? ?)
?
?
?
t?T
pT (t) ? f(A ? ?, t)
= ?A ?
1
ln(2) ?
1
pG(A ? ?)
?
?EpT f(A ? ?, t).
We now need to solve a system of |N |+ |R| equa-
tions obtained by setting to zero all of the above par-
tial derivatives. From each equation ???pG(A??) = 0
we obtain
?A ? ln(2) ? pG(A ? ?) =
= EpT f(A ? ?, t). (7)
We sum over all strings ? such that (A ? ?) ? R
?A ? ln(2) ?
?
?
pG(A ? ?) =
=
?
?
EpT f(A ? ?, t)
=
?
?
?
t?T
pT (t) ? f(A ? ?, t)
=
?
t?T
pT (t) ?
?
?
f(A ? ?, t)
=
?
t?T
pT (t) ? f(A, t)
= EpT f(A, t). (8)
337
From each equation ????A = 0 we obtain
?
? pG(A ? ?) = 1 for each A ? N (our origi-
nal constraints). Combining with (8) we obtain
?A ? ln(2) = EpT f(A, t). (9)
Replacing (9) into (7) we obtain, for every rule
(A ? ?) ? R,
pG(A ? ?) =
EpT f(A ? ?, t)
EpT f(A, t)
. (10)
The equations in (10) define the desired estimator
for our PCFG, assigning to each rule A ? ? a prob-
ability specified as the ratio between the expected
number of A ? ? and the expected number of A,
under the distribution pT . We remark here that the
minimization of the cross-entropy above is equiva-
lent to the minimization of the Kullback-Leibler dis-
tance between pT and pG, viewed as tree distribu-
tions. Also, note that the likelihood of an infinite set
of derivations would always be zero and therefore
cannot be considered here.
To be used in the next section, we now show that
the PCFG G obtained as above is consistent. The
line of our argument below follows a proof provided
in (Chi and Geman, 1998) for the maximum like-
lihood estimator based on finite tree distributions.
Without loss of generality, we assume that in G the
start symbol S is never used in the right-hand side
of a rule.
For each A ? N , let qA be the probability that a
derivation in G rooted in A fails to terminate. We
can then write
qA ?
?
B?N
qB ?
?
?
pG(A ? ?)f(B,?).(11)
The inequality follows from the fact that the events
considered in the right-hand side of (11) are not mu-
tually exclusive. Combining (10) and (11) we obtain
qA ? EpT f(A, t) ?
?
?
B?N
qB ?
?
?
EpT f(A ? ?, t)f(B,?).
Summing over all nonterminals we have
?
A?N
qA ? EpT f(A, t) ?
?
?
B?N
qB ?
?
A?N
?
?
EpT f(A ? ?, t)f(B,?)
=
?
B?N
qB ? EpT fc(B, t), (12)
where fc(B, t) indicates the number of times a node
labeled by nonterminal B appears in the derivation
tree t as a child of some other node.
From our assumptions on the start symbol S, we
have that S only appears at the root of the trees
in T (G). Then it is easy to see that, for every
A 6= S, we have EpT fc(A, t) = EpT f(A, t), while
EpT fc(S, t) = 0 and EpT f(S, t) = 1. Using these
relations in (12) we obtain
qS ? EpT f(S, T ) ? qS ? EpT fc(S, T ),
from which we conclude qS = 0, thus implying the
consistency of G.
4 Cross-entropy and derivational entropy
In this section we present the main result of the pa-
per. We show that, when G = (G, pG) is estimated
by minimizing the cross-entropy in (5), then such
cross-entropy takes the same value as the deriva-
tional entropy of G, defined in (3).
In (Nederhof and Satta, 2004) relations are de-
rived for the exact computation ofHd(pG). For later
use, we report these relations below, under the as-
sumption that G is consistent (see Section 3). We
have
Hd(pG) =
?
A?N
outG(A) ?HA(pG). (13)
Quantities HA(pG), A ? N , have been defined
in (4). For eachA ? N , quantity outG(A) is the sum
of the probabilities of all trees generated by G, hav-
ing root labeled by S and having a yield composed
of terminal symbols with an unexpanded occurrence
of nonterminal A. Again, we assume that symbol
S does not appear in any of the right-hand sides of
the rules in R. This means that S only appears at
the root of the trees in T (G). Under this condi-
tion, quantities outG(A) can be exactly computed
by solving the following system of linear equations
(see also (Nederhof, 2005))
outG(S) = 1; (14)
for each A 6= S
outG(A) =
=
?
B??
outG(B) ? f(A, ?) ? pG(B ? ?).(15)
338
We can now prove the equality
Hd(pG) = H(pT || pG), (16)
where G is the PCFG estimated by minimizing the
cross-entropy in (5), as described in Section 3.
We start from the definition of cross-entropy
H(pT || pG) =
= ?
?
t?T
pT (t) ? log pG(t)
= ?
?
t?T
pT (t) ? log
?
A??
pG(A ? ?)f(A??,t)
= ?
?
t?T
pT (t) ?
?
?
A??
f(A ? ?, t) ? log pG(A ? ?)
= ?
?
A??
log pG(A ? ?) ?
?
?
t?T
pT (t) ? f(A ? ?, t)
= ?
?
A??
log pG(A ? ?) ?
?EpT f(A ? ?, t). (17)
From our estimator in (10) we can write
EpT f(A ? ?, t) =
= pG(A ? ?) ? EpT f(A, t). (18)
Replacing (18) into (17) gives
H(pT || pG) =
= ?
?
A??
log pG(A ? ?) ?
?pG(A ? ?) ? EpT f(A, t)
= ?
?
A?N
EpT f(A, t) ?
?
?
?
pG(A ? ?) ? log pG(A ? ?)
=
?
A?N
EpT f(A, t) ?H(pG, A). (19)
Comparing (19) with (13) we see that, in order to
prove the equality in (16), we need to show relations
EpT f(A, t) = outG(A), (20)
for every A ? N . We have already observed in Sec-
tion 3 that, under our assumption on the start symbol
S, we have
EpT f(S, t) = 1. (21)
We now observe that, for any A ? N with A 6= S
and any t ? T (G), we have
f(A, t) =
=
?
B??
f(B ? ?, t) ? f(A, ?). (22)
For each A ? N with A 6= S we can then write
EpT f(A, t) =
=
?
t?T
pT (t) ? f(A, t)
=
?
t?T
pT (t) ?
?
B??
f(B ? ?, t) ? f(A, ?)
=
?
B??
?
t?T
pT (t) ? f(B ? ?, t) ? f(A, ?)
=
?
B??
EpT f(B ? ?, t) ? f(A, ?). (23)
Once more we use relation (18), which replaced
in (23) provides
EpT f(A, t) =
=
?
B??
EpT f(B, t) ?
?f(A, ?) ? pG(B ? ?). (24)
Notice that the linear system in (14) and (15) and the
linear system in (21) and (24) are the same. Thus we
conclude that quantities EpT f(A, t) and outG(A)
are the same for each A ? N . This completes our
proof of the equality in (16). Some examples will be
discussed in Section 6.
Besides its theoretical significance, the equality
in (16) can also be exploited in the computation of
the cross-entropy in practical applications. In fact,
cross-entropy is used as a measure of tightness in
comparing different models. In case of estimation
from an infinite distribution pT , the definition of the
cross-entropy H(pT || pG) contains an infinite sum-
mation, which is problematic for the computation of
such quantity. In standard practice, this problem is
overcome by generating a finite sample T (n) of large
size n, through the distribution pT , and then comput-
ing the approximation (Manning and Schu?tze, 1999)
H(pT || pG) ? ?
1
n
?
t?T
f(t, T (n)) ? log pG(t),
339
where f(t, T (n)) indicates the multiplicity, that is,
the number of occurrences, of t in T (n). However, in
practical applications n must be very large in order
to have a small error. Based on the results in this
section, we can instead compute the exact value of
H(pT || pG) by computing the derivational entropy
Hd(pG), using relation (13) and solving the linear
system in (14) and (15), which takes cubic time in
the number of nonterminals of the grammar.
5 Estimation based on likelihood
In natural language processing applications, the es-
timation of a PCFG is usually carried out on the ba-
sis of a finite sample of trees, called tree bank. The
so-called maximum likelihood estimation (MLE)
method is exploited, which maximizes the likeli-
hood of the observed data. In this section we show
that the MLE method is a special case of the esti-
mation method presented in Section 3, and that the
results of Section 4 also hold for the MLE method.
Let T be a tree sample, and let T be the under-
lying set of trees. For t ? T , we let f(t, T ) be the
multiplicity of t in T . We define
f(A ? ?, T ) =
=
?
t?T
f(t, T ) ? f(A ? ?, t), (25)
and let f(A, T ) = ?? f(A ? ?, T ). We can in-
duce from T a probability distribution pT , defined
over T , by letting for each t ? T
pT (t) =
f(t, T )
|T | . (26)
Note that
?
t?T pT (t) = 1. Distribution pT is called
the empirical distribution of T .
Assume that the trees in T have internal nodes
labeled by symbols in N , root nodes labeled by
S and leaf nodes labeled by symbols in ?. Let
also R be the finite set of rules that are observed
in T . We define the skeleton CFG underlying T as
G = (N,?,R, S). In the MLE method we proba-
bilistically extend the skeleton CFG G by means of
a function pG that maximizes the likelihood of T ,
defined as
pG(T ) =
?
t?T
pG(t)f(t,T ), (27)
subject to the usual properness conditions on pG.
Such maximization provides the estimator (see for
instance (Chi and Geman, 1998))
pG(A ? ?) =
f(A ? ?, T )
f(A, T ) . (28)
Let us consider the estimator in (10). If we replace
distribution pT with our empirical distribution pT ,
we derive
pG(A ? ?) =
= EpT f(A ? ?, t)EpT f(A, t)
=
?
t?T
f(t,T )
|T | ? f(A ? ?, t)
?
t?T
f(t,T )
|T | ? f(A, t)
=
?
t?T f(t, T ) ? f(A ? ?, t)
?
t?T f(t, T ) ? f(A, t)
= f(A ? ?, T )f(A, T ) . (29)
This is precisely the estimator in (28).
From relation (29) we conclude that the MLE
method can be seen as a special case of the general
estimator in Section 3, with the input distribution de-
fined over a finite set of trees. We can also derive
the well-known fact that, in the finite case, the maxi-
mization of the likelihood pG(T ) corresponds to the
minimization of the cross-entropy H(pT || pG).
Let now G = (G, pG) be a PCFG trained on T us-
ing the MLE method. Again from relation (29) and
Section 3 we have that G is consistent. This result
has been firstly shown in (Chaudhuri et al, 1983)
and later, with a different proof technique, in (Chi
and Geman, 1998). We can then transfer the results
of Section 4 to the supervised MLE method, show-
ing the equality
Hd(pG) = H(pT || pG). (30)
This result was not previously known in the litera-
ture on statistical parsing of natural language. Some
examples will be discussed in Section 6.
6 Some examples
In this section we discuss a simple example with the
aim of clarifying the theoretical results in the previ-
ous sections. For a real number q with 0 < q < 1,
340
Figure 1: Derivational entropy of Gq and cross-
entropies for three different corpora.
consider the CFG G defined by the two rules S ?
aS and S ? a, and let Gq = (G, pG,q) be the proba-
bilistic extension of G with pG,q(S ? aS) = q and
pG,q(S ? a) = 1 ? q. This grammar is unambigu-
ous and consistent, and each tree t generated by G
has probability pG,q(t) = qi ? (1 ? q), where i ? 0
is the number of occurrences of rule S ? aS in t.
We use below the following well-known relations
(0 < r < 1)
+?
?
i=0
ri = 11? r , (31)
+?
?
i=1
i ? ri?1 = 1(1? r)2 . (32)
The derivational entropy of Gq can be directly
computed from its definition as
Hd(pG,q) = ?
+?
?
i=0
qi ? (1? q) ? log
(
qi ? (1? q)
)
= ?(1? q)
+?
?
i=0
qi log qi +
?(1? q) ? log(1? q) ?
+?
?
i=0
qi
= ?(1? q) ? log q ?
+?
?
i=0
i ? qi ? log(1? q)
= ? q1? q ? log q ? log(1? q). (33)
See Figure 1 for a plot of Hd(pG,q) as a function
of q.
If a tree bank is given, composed of occurrences
of trees generated by G, the value of q can be es-
timated by applying the MLE or, equivalently, by
minimizing the cross-entropy. We consider here sev-
eral tree banks, to exemplify the behaviour of the
cross-entropy depending on the structure of the sam-
ple of trees. The first tree bank T contains a single
tree t with a single occurrence of rule S ? aS and
a single occurrence of rule S ? a. We then have
pT (t) = 1 and pG,q(t) = q ? (1 ? q). The cross-
entropy between distributions pT and pG,q is then
H(pT , pG,q) = ? log q ? (1? q)
= ? log q ? log(1? q). (34)
The cross-entropy H(pT , pG,q), viewed as a func-
tion of q, is a convex-? function and is plotted in
Figure 1 (line indicated by Kd = 1, see below). We
can obtain its minimum by finding a zero for the first
derivative
d
dqH(pT , pG,q) = ?
1
q +
1
1? q
= 2q ? 1q ? (1? q) = 0, (35)
which gives q = 0.5. Note from Figure 1 that
the minimum of H(pT , pG,q) crosses the line cor-
responding to the derivational entropy, as should be
expected from the result in Section 4.
More in general, for integers d > 0 and K > 0,
consider a tree sample Td,K consisting of d trees ti,
1 ? i ? d. Each ti contains ki ? 0 occurrences
of rule S ? aS and one occurrence of rule S ? a.
Thus we have pTd,K (ti) = 1d and pG,q(ti) = qki ?
(1? q). We let?di=1 ki = K. The cross-entropy is
H(pTd,K , pG,q) =
= ?
d
?
i=0
1
d ? log q
ki ? log(1? q)
= ?Kd log q ? log(1? q). (36)
In Figure 1 we plot H(pTd,K , pG,q) in the case Kd =
0.5 and in the case Kd = 1.5. Again, we have that
these curves intersect with the curve corresponding
to the derivational entropy Hd(pG,q) at the points
were they take their minimum values.
341
7 Conclusions
We have shown in this paper that, when a PCFG is
estimated from some tree distribution by minimiz-
ing the cross-entropy, then the cross-entropy takes
the same value as the derivational entropy of the
PCFG itself. As a special case, this result holds for
the maximum likelihood estimator, widely applied
in statistical natural language parsing. The result
also holds for the relative weighted frequency esti-
mator introduced in (Chi, 1999) as a generalization
of the maximum likelihood estimator, and for the es-
timator introduced in (Nederhof, 2005) already dis-
cussed in the introduction. In a journal version of the
present paper, which is under submission, we have
also extended the results of Section 4 to the unsuper-
vised estimation of a PCFG from a distribution de-
fined over an infinite set of (unannotated) sentences
and, as a particular case, to the well-knonw inside-
outside algorithm (Manning and Schu?tze, 1999).
In practical applications, the results of Section 4
can be exploited in the computation of model tight-
ness. In fact, cross-entropy indicates how much the
estimated model fits the observed data, and is com-
monly exploited in comparison of different models
on the same data set. We can then use the given
relation between cross-entropy and derivational en-
tropy to compute one of these two quantities from
the other. For instance, in the case of the MLE
method we can choose between the computation of
the derivational entropy and the cross-entropy, de-
pending basically on the instance of the problem at
hand. As already mentioned, the computation of the
derivational entropy requires cubic time in the num-
ber of nonterminals of the grammar. If this num-
ber is large, direct computation of (5) on the corpus
might be more efficient. On the other hand, if the
corpus at hand is very large, one might opt for direct
computation of (3).
Acknowledgements
Helpful comments from Zhiyi Chi, Alberto lavelli,
Mark-Jan Nederhof and Khalil Simaan are grate-
fully acknowledged.
References
T.L. Booth and R.A. Thompson. 1973. Applying prob-
abilistic measures to abstract languages. IEEE Trans-
actions on Computers, C-22(5):442?450, May.
E. Charniak. 2001. Immediate-head parsing for language
models. In 39th Annual Meeting and 10th Conference
of the European Chapter of the Association for Com-
putational Linguistics, Proceedings of the Conference,
pages 116?123, Toulouse, France, July.
R. Chaudhuri, S. Pham, and O. N. Garcia. 1983. Solution
of an open problem on probabilistic grammars. IEEE
Transactions on Computers, 32(8):748?750.
C. Chelba and F. Jelinek. 1998. Exploiting syntactic
structure for language modeling. In 36th Annual Meet-
ing of the Association for Computational Linguistics
and 17th International Conference on Computational
Linguistics, volume 1, pages 225?231, Montreal, Que-
bec, Canada, August.
Z. Chi and S. Geman. 1998. Estimation of probabilis-
tic context-free grammars. Computational Linguistics,
24(2):299?305.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131?160.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
pages 589?638.
J.E. Hopcroft and J.D. Ullman. 1979. Introduction
to Automata Theory, Languages, and Computation.
Addison-Wesley.
C.D. Manning and H. Schu?tze. 1999. Foundations
of Statistical Natural Language Processing. Mas-
sachusetts Institute of Technology.
M.-J. Nederhof and G. Satta. 2004. Kullback-Leibler
distance between probabilistic context-free grammars
and probabilistic finite automata. In Proc. of the 20th
COLING, volume 1, pages 71?77, Geneva, Switzer-
land.
M.-J. Nederhof. 2005. A general technique to train lan-
guage models on language models. Computational
Linguistics, 31(2):173?185.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Y. Schabes. 1992. Stochastic lexicalized tree-adjoining
grammars. In Proc. of the fifteenth International
Conference on Computational Linguistics, volume 2,
pages 426?432, Nantes, August.
342
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 343?350,
New York, June 2006. c?2006 Association for Computational Linguistics
Estimation of Consistent
Probabilistic Context-free Grammars
Mark-Jan Nederhof
Max Planck Institute
for Psycholinguistics
P.O. Box 310
NL-6500 AH Nijmegen
The Netherlands
MarkJan.Nederhof@mpi.nl
Giorgio Satta
Dept. of Information Engineering
University of Padua
via Gradenigo, 6/A
I-35131 Padova
Italy
satta@dei.unipd.it
Abstract
We consider several empirical estimators
for probabilistic context-free grammars,
and show that the estimated grammars
have the so-called consistency property,
under the most general conditions. Our
estimators include the widely applied ex-
pectation maximization method, used to
estimate probabilistic context-free gram-
mars on the basis of unannotated corpora.
This solves a problem left open in the lit-
erature, since for this method the consis-
tency property has been shown only under
restrictive assumptions on the rules of the
source grammar.
1 Introduction
Probabilistic context-free grammars are one of the
most widely used formalisms in current work in sta-
tistical natural language parsing and stochastic lan-
guage modeling. An important property for a proba-
bilistic context-free grammar is that it be consistent,
that is, the grammar should assign probability of one
to the set of all finite strings or parse trees that it
generates. In other words, the grammar should not
lose probability mass with strings or trees of infinite
length.
Several methods for the empirical estimation of
probabilistic context-free grammars have been pro-
posed in the literature, based on the optimization of
some function on the probabilities of the observed
data, such as the maximization of the likelihood of
a tree bank or a corpus of unannotated sentences. It
has been conjectured in (Wetherell, 1980) that these
methods always provide probabilistic context-free
grammars with the consistency property. A first re-
sult in this direction was presented in (Chaudhuri et
al., 1983), by showing that a probabilistic context-
free grammar estimated by maximizing the likeli-
hood of a sample of parse trees is always consistent.
In later work by (Sa?nchez and Bened??, 1997)
and (Chi and Geman, 1998), the result was in-
dependently extended to expectation maximization,
which is an unsupervised method exploited to es-
timate probabilistic context-free grammars by find-
ing local maxima of the likelihood of a sample of
unannotated sentences. The proof in (Sa?nchez and
Bened??, 1997) makes use of spectral analysis of ex-
pectation matrices, while the proof in (Chi and Ge-
man, 1998) is based on a simpler counting argument.
Both these proofs assume restrictions on the un-
derlying context-free grammars. More specifically,
in (Chi and Geman, 1998) empty rules and unary
rules are not allowed, thus excluding infinite ambi-
guity, that is, the possibility that some string in the
input sample has an infinite number of derivations in
the grammar. The treatment of general form context-
free grammars has been an open problem so far.
In this paper we consider several estimation meth-
ods for probabilistic context-free grammars, and we
show that the resulting grammars have the consis-
tency property. Our proofs are applicable under
the most general conditions, and our results also
include the expectation maximization method, thus
solving the open problem discussed above. We use
an alternative proof technique with respect to pre-
343
vious work, based on an already known renormal-
ization construction for probabilistic context-free
grammars, which has been used in the context of
language modeling.
The structure of this paper is as follows. We pro-
vide some preliminary definitions in Section 2, fol-
lowed in Section 3 by a brief overview of the esti-
mation methods we investigate in this paper. In Sec-
tion 4 we prove some properties of a renormaliza-
tion technique for probabilistic context-free gram-
mars, and use this property to show our main results
in Section 5. Section 6 closes with some concluding
remarks.
2 Preliminaries
In this paper we use mostly standard notation, as for
instance in (Hopcroft and Ullman, 1979) and (Booth
and Thompson, 1973), which we summarize below.
A context-free grammar (CFG) is a 4-tupleG =
(N,?, S,R) where N and ? are finite disjoint sets
of nonterminal and terminal symbols, respectively,
S ? N is the start symbol and R is a finite set of
rules. Each rule has the form A ? ?, where A ? N
and ? ? (? ?N)?. We write V for set ? ?N .
Each CFG G is associated with a left-most de-
rive relation ?, defined on triples consisting of two
strings ?, ? ? V ? and a rule pi ? R. We write ? pi? ?
if and only if ? = uA?? and ? = u???, for some
u ? ??, ?? ? V ?, and pi = (A ? ?). A left-
most derivation for G is a string d = pi1 ? ? ?pim,
m ? 0, such that ?0 pi1? ?1 pi2? ? ? ? pim? ?m, for
some ?0, . . . , ?m ? V ?; d = ? (where ? denotes
the empty string) is also a left-most derivation. In
the remainder of this paper, we will let the term
derivation always refer to left-most derivation. If
?0
pi1? ? ? ? pim? ?m for some ?0, . . . , ?m ? V ?, then
we say that d = pi1 ? ? ?pim derives ?m from ?0 and
we write ?0 d? ?m; d = ? derives any ?0 ? V ? from
itself.
A (left-most) derivation d such that S d? w,
w ? ??, is called a complete derivation. If d is
a complete derivation, we write y(d) to denote the
(unique) string w ? ?? such that S d? w. We
define D(G) to be the set of all complete deriva-
tions for G. The language generated by G is the set
of all strings derived by complete derivations, i.e.,
L(G) = {y(d) | d ? D(G)}. It is well-known that
there is a one-to-one correspondence between com-
plete derivations and parse trees for strings in L(G).
For X ? V and ? ? V ?, we write f(X,?) to
denote the number of occurrences of X in ?. For
(A ? ?) ? R and a derivation d, f(A ? ?, d)
denotes the number of occurrences of A ? ? in d.
We let f(A, d) =?? f(A ? ?, d).
A probabilistic CFG (PCFG) is a pair G =
(G, pG), where G is a CFG and pG is a function
from R to real numbers in the interval [0, 1]. We
say that G is proper if, for every A ? N , we have
?
A??
pG(A ? ?) = 1. (1)
Function pG can be used to assign probabilities to
derivations of the underlying CFG G, in the follow-
ing way. For d = pi1 ? ? ?pim ? R?,m ? 0, we define
pG(d) =
m
?
i=1
pG(pii). (2)
Note that pG(?) = 1. The probability of a string
w ? ?? is defined as
pG(w) =
?
y(d)=w
pG(d). (3)
A PCFG is consistent if
?
w
pG(w) = 1. (4)
Consistency implies that the PCFG defines a proba-
bility distribution over both sets D(G) and L(G).
If a PCFG is proper, then consistency means that
no probability mass is lost in derivations of infinite
length. All PCFGs in this paper are implicitly as-
sumed to be proper, unless otherwise stated.
3 Estimation of PCFGs
In this section we give a brief overview of some esti-
mation methods for PCFGs. These methods will be
later investigated to show that they always provide
consistent PCFGs.
In natural language processing applications, esti-
mation of a PCFG is usually carried out on the ba-
sis of a tree bank, which in this paper we assume to
be a sample, that is, a finite multiset, of complete
derivations. Let D be such a sample, and let D be
344
the underlying set of derivations. For d ? D, we
let f(d,D) be the multiplicity of d in D, that is, the
number of occurrences of d in D. We define
f(A ? ?,D) =
?
d?D
f(d,D) ? f(A ? ?, d), (5)
and let f(A,D) =?? f(A ? ?,D).
Consider a CFG G = (N,?,R, S) defined by
all and only the nonterminals, terminals and rules
observed in D. The criterion of maximum likeli-
hood estimation (MLE) prescribes the construction
of a PCFG G = (G, pG) such that pG maximizes the
likelihood of D, defined as
pG(D) =
?
d?D
pG(d)f(d,D), (6)
subject to the properness conditions
?
? pG(A ?
?) = 1 for eachA ? N . The maximization problem
above has a unique solution, provided by the estima-
tor (see for instance (Chi and Geman, 1998))
pG(A ? ?) =
f(A ? ?,D)
f(A,D) . (7)
We refer to this as the supervised MLE method.
In applications in which a tree bank is not avail-
able, one might still use the MLE criterion to train
a PCFG in an unsupervised way, on the basis of a
sample of unannotated sentences, also called a cor-
pus. Let us call C such a sample and C the underly-
ing set of sentences. For w ? C, we let f(w, C) be
the multiplicity of w in C.
Assume a CFG G = (N,?,R, S) that is able
to generate all of the sentences in C, and possibly
more. The MLE criterion prescribes the construc-
tion of a PCFG G = (G, pG) such that pG maxi-
mizes the likelihood of C, defined as
pG(C) =
?
w?C
pG(w)f(w,C), (8)
subject to the properness conditions as in the super-
vised case above. The above maximization prob-
lem provides a system of |R| nonlinear equations
(see (Chi and Geman, 1998))
pG(A ? ?) =
?
w?C f(w, C) ? EpG(d |w) f(A ? ?, d)
?
w?C f(w, C) ? EpG(d |w) f(A, d)
, (9)
where Ep denotes an expectation computed under
distribution p, and pG(d |w) is the probability of
derivation d conditioned by sentence w (so that
pG(d |w) > 0 only if y(d) = w). The solution to
the above system is not unique, because of the non-
linearity. Furthermore, each solution of (9) identi-
fies a point where the curve in (8) has partial deriva-
tives of zero, but this does not necessarily corre-
spond to a local maximum, let alne an absolute
maximum. (A point with partial derivatives of zero
that is not a local maximum could be a local min-
imum or even a so-called saddle point.) In prac-
tice, this system is typically solved by means of an
iterative algorithm called inside/outside (Charniak,
1993), which implements the expectation maximiza-
tion (EM) method (Dempster et al, 1977). Starting
with an initial function pG that probabilistically ex-
tends G, a so-called growth transformation is com-
puted, defined as
pG(A ? ?) =
?
w?C f(w, C)?
?
y(d)=w
pG(d)
pG(w) ?f(A ? ?, d)
?
w?C f(w, C)?
?
y(d)=w
pG(d)
pG(w) ?f(A, d)
. (10)
Following (Baum, 1972), one can show that
pG(C) ? pG(C). Thus, by iterating the growth trans-
formation above, we are guaranteed to reach a local
maximum for (8), or possibly a saddle point. We
refer to this as the unsupervised MLE method.
We now discuss a third estimation method for
PCFGs, which was proposed in (Corazza and Satta,
2006). This method can be viewed as a general-
ization of the supervised MLE method to probabil-
ity distributions defined over infinite sets of com-
plete derivations. Let D be an infinite set of com-
plete derivations using nonterminal symbols in N ,
start symbol S ? N and terminal symbols in ?.
We assume that the set of rules that are observed
in D is drawn from some finite set R. Let pD be
a probability distribution defined over D, that is,
a function from set D to interval [0, 1] such that
?
d?D pD(d) = 1.
Consider the CFG G = (N,?,R, S). Note that
D ? D(G). We wish to extend G to some PCFG
G = (G, pG) in such a way that pD is approxi-
mated by pG (viewed as a distribution over complete
derivations) as well as possible according to some
criterion. One possible criterion is minimization of
345
the cross-entropy between pD and pG, defined as
the expectation, under distribution pD, of the infor-
mation of the derivations in D computed under dis-
tribution pG, that is
H(pD || pG) = EpD log
1
pG(d)
= ?
?
d?D
pD(d) ? log pG(d). (11)
We thus want to assign to the parameters pG(A ?
?), A ? ? ? R, the values that minimize (11), sub-
ject to the conditions
?
? pG(A ? ?) = 1 for each
A ? N . Note that minimization of the cross-entropy
above is equivalent to minimization of the Kullback-
Leibler distance between pD and pG. Also note that
the likelihood of an infinite set of derivations would
always be zero and therefore cannot be considered
here.
The solution to the above minimization problem
provides the estimator
pG(A ? ?) =
EpD f(A ? ?, d)
EpD f(A, d)
. (12)
A proof of this result appears in (Corazza and Satta,
2006), and is briefly summarized in Appendix A,
in order to make this paper self-contained. We call
the above estimator the cross-entropy minimization
method.
The cross-entropy minimization method can be
viewed as a generalization of the supervised MLE
method in (7), as shown in what follows. Let D and
D be defined as for the supervisedMLEmethod. We
define a distribution over D as
pD(d) =
f(d,D)
|D| . (13)
Distribution pD is usually called the empirical dis-
tribution associated withD. Applying the estimator
in (12) to pD, we obtain
pG(A ? ?) =
=
?
d?D pD(d) ? f(A ? ?, d)
?
d?D pD(d) ? f(A, d)
=
?
d?D
f(d,D)
|D| ? f(A ? ?, d)
?
d?D
f(d,D)
|D| ? f(A, d)
=
?
d?D f(d,D) ? f(A ? ?, d)
?
d?D f(d,D) ? f(A, d)
. (14)
This is the supervised MLE estimator in (7). This re-
minds us of the well-known fact that maximizing the
likelihood of a (finite) sample through a PCFG dis-
tribution amounts to minimizing the cross-entropy
between the empirical distribution of the sample and
the PCFG distribution itself.
4 Renormalization
In this section we recall a renormalization technique
for PCFGs that was used before in (Abney et al,
1999), (Chi, 1999) and (Nederhof and Satta, 2003)
for different purposes, and is exploited in the next
section to prove our main results. In the remainder
of this section, we assume a fixed, not necessarily
proper PCFG G = (G, pG), with G = (N,?, S,R).
We define the renormalization of G as the PCFG
R(G) = (G, pR) with pR specified by
pR(A ? ?) =
pG(A ? ?) ?
?
d,w pG(?
d? w)
?
d,w pG(A
d? w)
. (15)
It is not difficult to see that R(G) is a proper PCFG.
We now show an important property of R(G), dis-
cussed before in (Nederhof and Satta, 2003) in the
context of so-called weighted context-free gram-
mars.
Lemma 1 For each derivation d with A d? w, A ?
N and w ? ??, we have
pR(A d? w) =
pG(A d? w)
?
d?,w? pG(A
d?? w?)
. (16)
Proof. The proof is by induction on the length of d,
written |d|. If |d| = 1 we must have d = (A ? w),
and thus pR(d) = pR(A ? w). In this case, the
statement of the lemma directly follows from (15).
Assume now |d| > 1 and let pi = (A ? ?)
be the first rule used in d. Note that there must
be at least one nonterminal symbol in ?. We can
then write ? as u0A1u1A2 ? ? ?uq?1Aquq, for q ? 1,
Ai ? N , 1 ? i ? q, and uj ? ??, 0 ?
j ? q. In words, A1, . . . , Aq are all of the occur-
rences of nonterminals in ?, as they appear from
left to right. Consequently, we can write d in the
form d = pi ? d1 ? ? ? dq for some derivations di,
1 ? i ? q, with Ai di? wi, |di| ? 1 and with
346
w = u0w1u1w2 ? ? ?uq?1wquq. Below we use the
fact that pR(uj ?? uj) = pG(uj ?? uj) = 1 for
each j with 0 ? j ? q, and further using the def-
inition of pR and the inductive hypothesis, we can
write
pR(A d? w) =
= pR(A ? ?) ?
q
?
i=1
pR(Ai di? wi)
= pG(A ? ?) ?
?
d?,w? pG(?
d?? w?)
?
d?,w? pG(A
d?? w?)
?
?
q
?
i=1
pR(Ai di? wi)
= pG(A ? ?) ?
?
d?,w? pG(?
d?? w?)
?
d?,w? pG(A
d?? w?)
?
?
q
?
i=1
pG(Ai di? wi)
?
d?,w? pG(Ai
d?? w?)
= pG(A ? ?) ?
?
d?,w? pG(?
d?? w?)
?
d?,w? pG(A
d?? w?)
?
?
?q
i=1 pG(Ai
di? wi)
?q
i=1
?
d?,w? pG(Ai
d?? w?)
= pG(A ? ?) ?
?
d?,w? pG(?
d?? w?)
?
d?,w? pG(A
d?? w?)
?
?
?q
i=1 pG(Ai
di? wi)
?
d?,w? pG(?
d?? w?)
= pG(A ? ?) ?
?q
i=1 pG(Ai
di? wi)
?
d?,w? pG(A
d?? w?)
?
= pG(A
d? w)
?
d?,w? pG(A
d?? w?)
. (17)
As an easy corollary of Lemma 1, we have that
R(G) is a consistent PCFG, as we can write
?
d,w
pR(S d? w) =
=
?
d,w
pG(S d? w)
?
d?,w? pG(S
d?? w?)
=
?
d,w pG(S
d? w)
?
d?,w? pG(S
d?? w?)
= 1. (18)
5 Consistency
In this section we prove the main results of this
paper, namely that all of the estimation methods
discussed in Section 3 always provide consistent
PCFGs. We start with a technical lemma, central
to our results, showing that a PCFG that minimizes
the cross-entropy with a distribution over any set of
derivations must be consistent.
Lemma 2 Let G = (G, pG) be a proper PCFG
and let pD be a probability distribution defined over
some set D ? D(G). If G minimizes function
H(pD || pG), then G is consistent.
Proof. LetG = (N,?, S,R), and assume that G is
not consistent. We establish a contradiction. Since G
is not consistent, we must have
?
d,w pG(S
d? w) <
1. Let then R(G) = (G, pR) be the renormalization
of G, defined as in (15). For any derivation S d? w,
w ? ??, with d in D, we can use Lemma 1 and
write
pR(S d? w) =
= 1
?
d?,w? pG(S
d?? w?)
? pG(S d? w)
> pG(S d? w). (19)
In words, every complete derivation d in D has a
probability in R(G) that is strictly greater than in
G. But this means H(pD || pR) < H(pD || pG),
against our hypothesis. Therefore, G is consistent
and pG is a probability distribution over set D(G).
Thus function H(pD || pG) can be interpreted as the
cross-entropy. (Observe that in the statement of the
lemma we have avoided the term ?cross-entropy?,
since cross-entropies are only defined for probability
distributions.)
Lemma 2 directly implies that the cross-entropy
minimization method in (12) always provides a con-
sistent PCFG, since it minimizes cross-entropy for a
distribution defined over a subset of D(G). We have
already seen in Section 3 that the supervised MLE
method is a special case of the cross-entropy min-
imization method. Thus we can also conclude that
a PCFG trained with the supervised MLE method is
347
always consistent. This provides an alternative proof
of a property that was first shown in (Chaudhuri et
al., 1983), as discussed in Section 1.
We now prove the same result for the unsuper-
vised MLE method, without any restrictive assump-
tion on the rules of our CFGs. This solves a problem
that was left open in the literature (Chi and Geman,
1998); see again Section 1 for discussion. Let C and
C be defined as in Section 3. We define the empiri-
cal distribution of C as
pC(w) =
f(w, C)
|C| . (20)
Let G = (N,?, S,R) be a CFG such that C ?
L(G). Let D(C) be the set of all complete deriva-
tions for G that generate sentences in C, that is,
D(C) = {d | d ? D(G), y(d) ? C}.
Further, assume some probabilistic extension G =
(G, pG) of G, such that pG(d) > 0 for every d ?
D(C). We define a distribution over D(C) by
pD(C)(d) = pC(y(d)) ?
pG(d)
pG(y(d))
. (21)
It is not difficult to verify that
?
d?D(C)
pD(C)(d) = 1. (22)
We now apply to G the estimator in (12), in order
to obtain a new PCFG G? = (G, p?G) that minimizes
the cross-entropy between pD(C) and p?G. According
to Lemma 2, we have that G? is a consistent PCFG.
Distribution p?G is specified by
p?G(A ? ?) =
=
?
d?D(C) pD(C)(d)?f(A ? ?, d)
?
d?D(C) pD(C)(d)?f(A, d)
=
?
d?D(C)
f(y(d),C)
|C| ?
pG(d)
pG(y(d)) ?f(A ? ?, d)
?
d?D(C)
f(y(d),C)
|C| ?
pG(d)
pG(y(d)) ?f(A, d)
=
?
w?C f(w, C)?
?
y(d)=w
pG(d)
pG(w) ?f(A ? ?, d)
?
w?C f(w, C)?
?
y(d)=w
pG(d)
pG(w) ?f(A, d)
=
?
w?C f(w, C)?EpG(d |w)f(A ? ?, d)
?
w?C f(w, C)?EpG(d |w)f(A, d)
. (23)
Since distribution pG was arbitrarily chosen, sub-
ject to the only restriction that pG(d) > 0 for ev-
ery d ? D(C), we have that (23) is the growth
estimator (10) already discussed in Section 3. In
fact, for each w ? L(G) and d ? D(G), we have
pG(d |w) = pG(d)pG(w) . We conclude with the desired
result, namely that a general form PCFG obtained at
any iteration of the EM method for the unsupervised
MLE is always consistent.
6 Conclusions
In this paper we have investigated a number of
methods for the empirical estimation of probabilis-
tic context-free grammars, and have shown that the
resulting grammars have the so-called consistency
property. This property guarantees that all the prob-
ability mass of the grammar is used for the finite
strings it derives. Thus if the grammar is used in
combination with other probabilistic models, as for
instance in a speech processing system, consistency
allows us to combine or compare scores from differ-
ent modules in a sound way.
To obtain our results, we have used a novel proof
technique that exploits an already known construc-
tion for the renormalization of probabilistic context-
free grammars. Our proof technique seems more
intuitive than arguments previously used in the lit-
erature to prove the consistency property, based on
counting arguments or on spectral analysis. It is
not difficult to see that our proof technique can
also be used with probabilistic rewriting formalisms
whose underlying derivations can be characterized
by means of context-free rewriting. This is for
instance the case with probabilistic tree-adjoining
grammars (Schabes, 1992; Sarkar, 1998), for which
consistency results have not yet been shown in the
literature.
A Cross-entropy minimization
In order to make this paper self-contained, we sketch
a proof of the claim in Section 3 that the estimator
in (12) minimizes the cross entropy in (11). A full
proof appears in (Corazza and Satta, 2006).
Let D, pD and G = (N,?,R, S) be defined as
in Section 3. We want to find a proper PCFG G =
(G, pG) such that the cross-entropy H(pD || pG) is
minimal. We use Lagrange multipliers ?A for each
A ? N and define the form
? =
?
A?N
?A ? (
?
?
pG(A ? ?)? 1) +
348
?
?
d?D
pD(d) ? log pG(d). (24)
We now consider all the partial derivatives of?. For
each A ? N we have
??
??A
=
?
?
pG(A ? ?)? 1. (25)
For each (A ? ?) ? R we have
??
?pG(A ? ?)
=
= ?A ?
?
?pG(A ? ?)
?
d?D
pD(d) ? log pG(d)
= ?A ?
?
d?D
pD(d) ?
?
?pG(A ? ?)
log pG(d)
= ?A ?
?
d?D
pD(d) ?
?
?pG(A ? ?)
log
?
(B??)?R
pG(B ? ?)f(B??,d)
= ?A ?
?
d?D
pD(d) ?
?
?pG(A ? ?)
?
(B??)?R
f(B ? ?, d) ? log pG(B ? ?)
= ?A ?
?
d?D
pD(d) ?
?
(B??)?R
f(B ? ?, d) ?
?
?pG(A ? ?)
log pG(B ? ?)
= ?A ?
?
d?D
pD(d) ? f(A ? ?, d) ?
? 1ln(2) ?
1
pG(A ? ?)
= ?A ?
1
ln(2) ?
1
pG(A ? ?)
?
?
?
d?D
pD(d) ? f(A ? ?, d)
= ?A ?
1
ln(2) ?
1
pG(A ? ?)
?
? EpD f(A ? ?, d). (26)
By setting to zero all of the above partial derivatives,
we obtain a system of |N |+|R| equations, which we
must solve. From ???pG(A??) = 0 we obtain
?A ? ln(2) ? pG(A ? ?) =
EpDf(A ? ?, d). (27)
We sum over all strings ? such that (A ? ?) ? R,
deriving
?A ? ln(2) ?
?
?
pG(A ? ?) =
=
?
?
EpD f(A ? ?, d)
=
?
?
?
d?D
pD(d) ? f(A ? ?, d)
=
?
d?D
pD(d) ?
?
?
f(A ? ?, d)
=
?
d?D
pD(d) ? f(A, d)
= EpD f(A, d). (28)
From each equation ????A = 0 we obtain
?
? pG(A ? ?) = 1 for each A ? N (our original
constraints). Combining this with (28) we obtain
?A ? ln(2) = EpD f(A, d). (29)
Replacing (29) into (27) we obtain, for every rule
(A ? ?) ? R,
pG(A ? ?) =
EpD f(A ? ?, d)
EpD f(A, d)
. (30)
This is the estimator introduced in Section 3.
References
S. Abney, D. McAllester, and F. Pereira. 1999. Relating
probabilistic grammars and automata. In 37th Annual
Meeting of the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 542?549,
Maryland, USA, June.
L. E. Baum. 1972. An inequality and associated max-
imization technique in statistical estimations of prob-
abilistic functions of Markov processes. Inequalities,
3:1?8.
T.L. Booth and R.A. Thompson. 1973. Applying prob-
abilistic measures to abstract languages. IEEE Trans-
actions on Computers, C-22(5):442?450, May.
E. Charniak. 1993. Statistical Language Learning. MIT
Press.
R. Chaudhuri, S. Pham, and O. N. Garcia. 1983. Solution
of an open problem on probabilistic grammars. IEEE
Transactions on Computers, 32(8):748?750.
Z. Chi and S. Geman. 1998. Estimation of probabilis-
tic context-free grammars. Computational Linguistics,
24(2):299?305.
349
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131?160.
A. Corazza and G. Satta. 2006. Cross-entropy and es-
timation of probabilistic context-free grammars. In
Proc. of HLT/NAACL 2006 Conference (this volume),
New York.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, B,
39:1?38.
J.E. Hopcroft and J.D. Ullman. 1979. Introduction
to Automata Theory, Languages, and Computation.
Addison-Wesley.
M.-J. Nederhof and G. Satta. 2003. Probabilistic pars-
ing as intersection. In 8th International Workshop on
Parsing Technologies, pages 137?148, LORIA, Nancy,
France, April.
J.-A. Sa?nchez and J.-M. Bened??. 1997. Consistency
of stochastic context-free grammars from probabilis-
tic estimation based on growth transformations. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 19(9):1052?1055, September.
A. Sarkar. 1998. Conditions on consistency of proba-
bilistic tree adjoining grammars. In Proc. of the 36th
ACL, pages 1164?1170, Montreal, Canada.
Y. Schabes. 1992. Stochastic lexicalized tree-adjoining
grammars. In Proc. of the 14th COLING, pages 426?
432, Nantes, France.
C. S. Wetherell. 1980. Probabilistic languages: A re-
view and some open questions. Computing Surveys,
12(4):361?379.
350
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 539?547,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Optimal Reduction of Rule Length
in Linear Context-Free Rewriting Systems
Carlos Go?mez-Rodr??guez1, Marco Kuhlmann2, Giorgio Satta3 and David Weir4
1 Departamento de Computacio?n, Universidade da Corun?a, Spain (cgomezr@udc.es)
2 Department of Linguistics and Philology, Uppsala University, Sweden (marco.kuhlmann@lingfil.uu.se)
3 Department of Information Engineering, University of Padua, Italy (satta@dei.unipd.it)
4 Department of Informatics, University of Sussex, United Kingdom (davidw@sussex.ac.uk)
Abstract
Linear Context-free Rewriting Systems
(LCFRS) is an expressive grammar formalism
with applications in syntax-based machine
translation. The parsing complexity of an
LCFRS is exponential in both the rank
of a production, defined as the number of
nonterminals on its right-hand side, and a
measure for the discontinuity of a phrase,
called fan-out. In this paper, we present
an algorithm that transforms an LCFRS
into a strongly equivalent form in which
all productions have rank at most 2, and
has minimal fan-out. Our results generalize
previous work on Synchronous Context-Free
Grammar, and are particularly relevant for
machine translation from or to languages that
require syntactic analyses with discontinuous
constituents.
1 Introduction
There is currently considerable interest in syntax-
based models for statistical machine translation that
are based on the extraction of a synchronous gram-
mar from a corpus of word-aligned parallel texts;
see for instance Chiang (2007) and the references
therein. One practical problem with this approach,
apart from the sheer number of the rules that result
from the extraction procedure, is that the parsing
complexity of all synchronous formalisms that we
are aware of is exponential in the rank of a rule,
defined as the number of nonterminals on the right-
hand side. Therefore, it is important that the rules
of the extracted grammar are transformed so as to
minimise this quantity. Not only is this beneficial in
terms of parsing complexity, but smaller rules can
also improve a translation model?s ability to gener-
alize to new data (Zhang et al, 2006).
Optimal algorithms exist for minimising the size
of rules in a Synchronous Context-Free Gram-
mar (SCFG) (Uno and Yagiura, 2000; Zhang et al,
2008). However, the SCFG formalism is limited
to modelling word-to-word alignments in which a
single continuous phrase in the source language is
aligned with a single continuous phrase in the tar-
get language; as defined below, this amounts to
saying that SCFG have a fan-out of 2. This re-
striction appears to render SCFG empirically inad-
equate. In particular, Wellington et al (2006) find
that the coverage of a translation model can increase
dramatically when one allows a bilingual phrase to
stretch out over three rather than two continuous
substrings. This observation is in line with empir-
ical studies in the context of dependency parsing,
where the need for formalisms with higher fan-out
has been observed even in standard, single language
texts (Kuhlmann and Nivre, 2006).
In this paper, we present an algorithm that com-
putes optimal decompositions of rules in the for-
malism of Linear Context-Free Rewriting Systems
(LCFRS) (Vijay-Shanker et al, 1987). LCFRS was
originally introduced as a generalization of sev-
eral so-called mildly context-sensitive grammar for-
malisms. In the context of machine translation,
LCFRS is an interesting generalization of SCFG be-
cause it does not restrict the fan-out to 2, allow-
ing productions with arbitrary fan-out (and arbitrary
rank). Given an LCFRS, our algorithm computes a
strongly equivalent grammar with rank 2 and min-
539
imal increase in fan-out.1 In this context, strong
equivalence means that the derivations of the orig-
inal grammar can be reconstructed using some sim-
ple homomorphism (c.f. Nijholt, 1980). Our contri-
bution is significant because the existing algorithms
for decomposing SCFG, based on Uno and Yagiura
(2000), cannot be applied to LCFRS, as they rely
on the crucial property that components of biphrases
are strictly separated in the generated string: Given a
pair of synchronized nonterminal symbols, the ma-
terial derived from the source nonterminal must pre-
cede the material derived from the target nontermi-
nal, or vice versa. The problem that we solve has
been previously addressed by Melamed et al (2004),
but in contrast to our result, their algorithm does not
guarantee an optimal (minimal) increase in the fan-
out of the resulting grammar. However, this is essen-
tial for the practical applicability of the transformed
grammar, as the parsing complexity of LCFRS is ex-
ponential in both the rank and the fan-out.
Structure of the paper The remainder of the pa-
per is structured as follows. Section 2 introduces the
terminology and notation that we use for LCFRS.
In Section 3, we present the technical background
of our algorithm; the algorithm itself is discussed
in Section 4. Section 5 concludes the paper by dis-
cussing related work and open problems.
General notation The set of non-negative integers
is denoted by N. For i, j ? N, we write [i, j] to
denote the interval { k ? N | i ? k ? j }, and use
[i] as a shorthand for [1, i]. Given an alphabet V , we
write V ? for the set of all (finite) strings over V .
2 Preliminaries
We briefly summarize the terminology and notation
that we adopt for LCFRS; for detailed definitions,
see Vijay-Shanker et al (1987).
2.1 Linear, non-erasing functions
Let V be an alphabet. For natural numbers r ? 0
and f, f1, . . . , fr ? 1, a function
g : (V ?)f1 ? ? ? ? ? (V ?)fr ? (V ?)f
1Rambow and Satta (1999) show that without increasing
fan-out it is not always possible to produce even weakly equiv-
alent grammars.
is called a linear, non-erasing function over V of
type f1 ? ? ? ? ? fr ? f , if it can be defined by an
equation of the form
g(?x1,1, . . . , x1,f1?, . . . , ?xr,1, . . . , xr,fr?) = ?g ,
where ?g = ??g,1, . . . , ?g,f ? is an f -tuple of strings
over the variables on the left-hand side of the equa-
tion and symbols in V that contains exactly one oc-
currence of each variable. We call the value r the
rank of g, the value f its fan-out, and write ?(g)
and ?(g), respectively, to denote these quantities.
Note that, if we assume the variables on the left-
hand side of the defining equation of g to be named
according to the specific schema given above, then g
is uniquely determined by ?g.
2.2 Linear context-free rewriting systems
A linear context-free rewriting system (LCFRS)
is a construct G = (VN , VT , P, S), where: VN is
an alphabet of nonterminal symbols in which each
symbol A ? VN is associated with a value ?(A),
called its fan-out; VT is an alphabet of terminal
symbols; S ? N is a distinguished start symbol with
?(S) = 1; and P is a set of productions of the form
p : A? g(B1, B2, . . . , Br) ,
where A,B1, . . . , Br ? VN , and g is a linear, non-
erasing function over the terminal alphabet VT of
type ?(B1) ? ? ? ? ? ?(Br) ? ?(A). In a deriva-
tion of an LCFRS, the production p can be used to
transform a sequence of r tuples of strings, gener-
ated by the nonterminals B1, . . . , Br, into a single
?(A)-tuple of strings, associated with the nonter-
minal A. The values ?(g) and ?(g) are called the
rank and fan-out of p, respectively, and we write
?(p) and ?(p), respectively, to denote these quan-
tities. The rank and fan-out of G, written ?(G)
and ?(G), respectively, are the maximum rank and
fan-out among all of its productions. Given that
?(S) = 1, a derivation will associate S with a set of
one-component tuples of strings over VT ; this forms
the string language generated by G.
Example 1 The following LCFRS generates the
string language { anbncndn | n ? N }. We only
specify the set of productions; the remaining com-
540
ponents of the grammar are obvious from that.
S ? g1(R) g1(?x1,1, x1,2?) = ?x1,1x1,2?
R? g2(R) g2(?x1,1, x1,2?) = ?ax1,1b, cx1,2d?
R? g3 g3 = ??, ??
The functions g1 and g2 have rank 1; the function g3
has rank 0. The functions g2 and g3 have fan-out 2;
the function g1 has fan-out 1. 2
3 Technical background
The general idea behind our algorithm is to replace
each production of an LCFRS with a set of ?shorter?
productions that jointly are equivalent to the original
production. Before formalizing this idea, we first in-
troduce a specialized representation for the produc-
tions of an LCFRS.
We distinguish between occurrences of symbols
within a string by exploiting two different notations.
Let ? = a1a2 ? ? ? an be a string. The occurrence ai
in ? can be denoted by means of its position index
i ? [n], or else by means of its two (left and right)
endpoints, i?1 and i; here, the left (right) endpoint
denotes a boundary between occurrence ai and the
previous (subsequent) occurrence, or the beginning
(end) of the string ?. Similarly, a substring ai ? ? ? aj
of ? with i ? j can be denoted by the positions
i, i+ 1, . . . , j of its occurrences, or else by means of
its left and right endpoints, i? 1 and j.
3.1 Production representation
For the remainder of this section, let us fix an
LCFRS G = (VN , VT , P, S) and a production
p : A ? g(B1, . . . , Br) of G, with g defined as
in Section 2.1. We define
|p| = ?(g) +
?(g)?
i=1
|?g,i|.
Let $ be a fresh symbol that does not occur inG. We
define the characteristic string of the production p
as
?(p) = ?g,1$ ? ? ? $?g,?(g) ,
and the variable string of p as the string ?N (p) ob-
tained from ?(p) by removing all the occurrences of
symbols in VT .
Example 2 We will illustrate the concepts intro-
duced in this section using the concrete production
p0 : A? g(B1, B2, B3), where
?g = ?x1,1ax2,1x1,2, x3,1bx3,2? .
In this case, we have
?(p0) = x1,1ax2,1x1,2$x3,1bx3,2 , and
?N (p0) = x1,1x2,1x1,2$x3,1x3,2 . 2
Let I be an index set, I ? [r]. Consider the set B of
occurrences Bi in the right-hand side of p such that
i ? I .2 We define the position set of B, denoted
by ?B, as the set of all positions 1 ? j ? |?N (p)|
such that the jth symbol in ?N (p) is a variable of the
form xi,h, for i ? I and some h ? 1.
Example 3 Some position sets of p0 are
?{B1} = {1, 3} ,?{B2} = {2} ,?{B3} = {5, 6} .
2
A position set ?B can be uniquely expressed as the
union of f ? 1 intervals [l1 + 1, r1], . . . , [lf + 1, rf ]
such that ri?1 < li for every 1 < i ? f . Thus we
define the set of endpoints of ?B as
?B = { lj | j ? [f ] } ? { rj | j ? [f ] } .
The quantity f is called the fan-out of ?B, writ-
ten ?(?B). Notice that the fan-out of a position set
?{B} does not necessarily coincide with the fan-out
of the non-terminal B in the underlying LCFRS. A
set with 2f endpoints always corresponds to a posi-
tion set of fan-out f .
Example 4 For our running example, we have
?{B1} = {0, 1, 2, 3}, ?{B2} = {1, 2}, ?{B3} =
{4, 6}. Consequently, the fan-out of ?{B1} is 2, and
the fan-out of ?{B2} and ?{B3} is 1. Notice that the
fan-out of the non-terminal B3 is 2. 2
We drop B from ?B and ?B whenever this set is
understood from the context or it is not relevant.
Given a set of endpoints ? = {i1, . . . , i2f} with
i1 < ? ? ? < i2f , we obtain its corresponding position
set by calculating the closure of ?, defined as
[?] = ?fj=1[i2j?1 + 1, i2j ] .
2To avoid clutter in our examples, we abuse the notation by
not making an explicit distinction between nonterminals and oc-
currences of nonterminals in productions.
541
3.2 Reductions
Assume that r > 2. The reduction of p by the non-
terminal occurrencesBr?1, Br is the ordered pair of
productions (p1, p2) that is defined as follows. Let
?1, . . . , ?n be the maximal substrings of ?(p) that
contain only variables xi,j with r ? 1 ? i ? r and
terminal symbols, and at least one variable. Then
p1 : A? g1(B1, . . . , Br?2, X) and
p2 : X ? g2(Br?1, Br) ,
where X is a fresh nonterminal symbol, the char-
acteristic string ?(p1) is the string obtained from
?(p) by replacing each substring ?i by the vari-
able xr?1,i, and the characteristic string ?(p2) is the
string ?1$ ? ? ? $?n.
Note that the defining equations of neither g1
nor g2 are in the specific form discussed in Sec-
tion 2.1; however, they can be brought into this form
by a consistent renaming of the variables. We will
silently assume this renaming to take place.
Example 5 The reduction of p0 by the nonterminal
occurrences B2 and B3 has p1 : A ? g1(B1, X)
and p2 : X ? g2(B2, B3) with
?(p1) = x1,1x2,1x1,2$x2,2
?(p2) = ax2,1$x3,1bx3,2
or, after renaming and in standard notation,
g1(?x1,1, x1,2?, ?x2,1, x2,2?) = ?x1,1x2,1x1,2, x2,2?
g2(?x1,1?, ?x2,1, x2,2?) = ?ax1,1, x2,1bx2,2? .2
It is easy to check that a reduction provides us with a
pair of productions that are equivalent to the original
production p, in terms of generative capacity, since
g1(B1, . . . , Br?2, g2(Br?1, Br)) = g(B1, . . . , Br)
for all tuples of strings generated from the nontermi-
nalsB1, . . . , Br, respectively. Note also that the fan-
out of production p1 equals the fan-out of p. How-
ever, the fan-out of p2 (the value n) may be greater
than the fan-out of p, depending on the way vari-
ables are arranged in ?(p). Thus, a reduction does
not necessarily preserve the fan-out of the original
production. In the worst case, the fan-out of p2 can
be as large as ?(Br?1) + ?(Br).
1: Function NAIVE-BINARIZATION(p)
2: result? ?;
3: currentProd? p;
4: while ?(currentProd) > 2 do
5: (p1, p2)? any reduction of currentProd;
6: result? result ? p2;
7: currentProd? p1;
8: return result ? currentProd;
Figure 1: The naive algorithm
We have defined reductions only for the last two
occurrences of nonterminals in the right-hand side of
a production p. However, it is easy to see that we can
also define the concept for two arbitrary (not neces-
sarily adjacent) occurrences of nonterminals, at the
cost of making the notation more complicated.
4 The algorithm
Let G be an LCFRS with ?(G) = f and ?(G) = r,
and let f ? ? f be a target fan-out. We will now
present an algorithm that computes an equivalent
LCFRS G? of fan-out at most f ? whose rank is at
most 2, if such an LCFRS exists in the first place.
The algorithm works by exhaustively reducing all
productions in G.
4.1 Naive algorithm
Given an LCFRS production p, a naive algorithm
to compute an equivalent set of productions whose
rank is at most 2 is given in Figure 1. By ap-
plying this algorithm to all the productions in the
LCFRSG, we can obtain an equivalent LCFRS with
rank 2. We will call such an LCFRS a binarization
of G.
The fan-out of the obtained LCFRS will depend
on the nonterminals that we choose for the reduc-
tions in line 5. It is not difficult to see that, in the
worst case, the resulting fan-out can be as high as
d r2e ? f . This occurs when we choose d r2e nonter-minals with fan-out f that have associated variables
in the string ?N (p) that do not occur at consecutive
positions.
The algorithm that we develop in Section 4.3 im-
proves on the naive algorithm in that it can be ex-
ploited to find a sequence of reductions that results
in a binarization of G that is optimal, i.e., leads to
542
an LCFRS with minimal fan-out. The algorithm is
based on a technical concept called adjacency.
4.2 Adjacency
Let p be some production in the LCFRS G, and let
?1,?2 be sets of endpoints, associated with some
sets of nonterminal occurrences in p. We say that?1
and ?2 overlap if the intersection of their closures
is nonempty, that is, if [?1]? [?2] 6= ?. Overlapping
holds if and only if the associated sets of nontermi-
nal occurrences are not disjoint. If ?1 and ?2 do
not overlap, we define their merge as
?(?1,?2) = (?1 ??2) \ (?1 ??2) .
It is easy to see that [?(?1,?2)] = [?1] ? [?2].
We say that ?1 and ?2 are adjacent for a given fan-
out f , written ?1 ?f ?2, if ?1 and ?2 do not
overlap, and ?([?(?1,?2)]) ? f .
Example 6 For the production p0 from Example 2,
we have ?(?{B1},?{B2}) = {0, 3}, showing that?{B1} ?1 ?{B2}. Similarly, we have
?(?{B1},?{B3}) = {0, 1, 2, 3, 4, 6} ,
showing that ?{B1} ?3 ?{B3}, but that neither?{B1} ?2 ?{B3} nor ?{B1} ?1 ?{B3} holds. 2
4.3 Bounded binarization algorithm
The adjacency-based binarization algorithm is given
in Figure 2. It starts with a working set contain-
ing the endpoint sets corresponding to each non-
terminal occurrence in the input production p. Re-
ductions of p are only explored for nonterminal oc-
currences whose endpoint sets are adjacent for the
target fan-out f ?, since reductions not meeting this
constraint would produce productions with fan-out
greater than f ?. Each reduction explored by the al-
gorithm produces a new endpoint set, associated to
the fresh nonterminal that it introduces, and this new
endpoint set is added to the working set and poten-
tially used in further reductions.
From the definition of the adjacency relation?f ,
it follows that at lines 9 and 10 of BOUNDED-
BINARIZATION we only pick up reductions for p
that do not exceed the fan-out bound of f ?. This
implies soundness for our algorithm. Completeness
means that the algorithm fails only if there exists no
binarization for p of fan-out not greater than f ?. This
1: Function BOUNDED-BINARIZATION(p, f ?)
2: workingSet? ?;
3: agenda? ?;
4: for all i from 1 to ?(p) do
5: workingSet? workingSet ? {?{Bi}};
6: agenda? agenda ? {?{Bi}};
7: while agenda 6= ? do
8: ?? pop some endpoint set from agenda;
9: for all ?1 ? workingSet with ?1 ?f ? ? do
10: ?2 = ?(?,?1);
11: if ?2 /? workingSet then
12: workingSet? workingSet ? {?2};
13: agenda? agenda ? {?2};
14: if ?{B1,B2,...,B?(p))} ? workingSet then
15: return true;
16: else
17: return false;
Figure 2: Algorithm to compute a bounded binarization
property is intuitive if one observes that our algo-
rithm is a specialization of standard algorithms for
the computation of the closure of binary relations.
A formal proof of this fact is rather long and te-
dious, and will not be reported here. We notice that
there is a very close similarity between algorithm
BOUNDED-BINARIZATION and the deduction pro-
cedure proposed by Shieber et al (1995) for parsing.
We discuss this more at length in Section 5.
Note that we have expressed the algorithm as a
decision function that will return true if there exists
a binarization of p with fan-out not greater than f ?,
and false otherwise. However, the algorithm can
easily be modified to return a reduction producing
such a binarization, by adding to each endpoint set
? ? workingSet two pointers to the adjacent end-
point sets that were used to obtain it. If the algorithm
is successful, the tree obtained by following these
pointers from the final endpoint set ?{B1,...,B?(p)} ?workingSet gives us a tree of reductions that will
produce a binarization of p with fan-out not greater
than f ?, where each node labeled with the set ?{Bi}
corresponds to the nonterminal Bi, and nodes la-
beled with other endpoint sets correspond to the
fresh nonterminals created by the reductions.
543
4.4 Implementation
In order to implement BOUNDED-BINARIZATION,
we can represent endpoint sets in a canonical way
as 2f ?-tuples of integer positions in ascending order,
and with some special null value used to fill posi-
tions for endpoint sets with fan-out strictly smaller
than f ?. We will assume that the concrete null value
is larger than any other integer.
We also need to provide some appropriate repre-
sentation for the set workingSet, in order to guar-
antee efficient performance for the membership test
and the insertion operation. Both operations can be
implemented in constant time if we represent work-
ingSet as an (2?f ?)-dimensional table with Boolean
entries. Each dimension is indexed by values in
[0, n] plus our special null value; here n is the length
of the string ?N (p), and thus n = O(|p|). However,
this has the disadvantage of using space ?(n2f ?),
even in case workingSet is sparse, and is affordable
only for quite small values of f ?. Alternatively, we
can more compactly represent workingSet as a trie
data structure. This representation has size certainly
smaller than 2f ? ? q, where q is the size of the set
workingSet. However, both membership and inser-
tion operations take now an amount of time O(2f ?).
We now analyse the time complexity of algorithm
BOUNDED-BINARIZATION for inputs p and f ?. We
first focus on the while-loop at lines 7 to 13. As
already observed, the number of possible endpoint
sets is bounded by O(n2f ?). Furthermore, because
of the test at line 11, no endpoint set is ever inserted
into the agenda variable more than once in a sin-
gle run of the algorithm. We then conclude that our
while-loop cycles a number of times O(n2f ?).
We now focus on the choice of the endpoint set
?1 in the inner for-loop at lines 9 to 13. Let us fix ?
as in line 8. It is not difficult to see that any ?1 with
?1 ?f ? ? must satisfy
?(?) + ?(?1)? |? ??1| ? f ?. (1)
Let I ? ?, and consider all endpoint sets ?1 with
? ??1 = I . Given (1), we also have
?(?1) ? f ? + |I| ? ?(?). (2)
This means that, for each ? coming out of the
agenda, at line 9 we can choose all endpoint sets ?1
such that ?1 ?f ? ? by performing the following
steps:
? arbitrarily choose a set I ? ?;
? choose endpoints in set ?1\I subject to (2);
? test whether ?1 belongs to workingSet and
whether ?, ?1 do not overlap.
We claim that, in the above steps, the number
of involved endpoints does not exceed 3f ?. To
see this, we observe that from (2) we can derive
|I| ? ?(?) + ?(?1) ? f ?. The total number
of (distinct) endpoints in a single iteration step is
e = 2?(?) + 2?(?1) ? |I|. Combining with the
above inequality we have
e ? 2?(?) + 2?(?1)? ?(?)? ?(?1) + f ?
= ?(?) + ?(?1) + f ? ? 3f ? ,
as claimed. Since each endpoint takes values in
the set [0, n], we have a total of O(n3f ?) different
choices. For each such choice, we need to clas-
sify an endpoint as belonging to either ?\I , ?1\I ,
or I . This amounts to an additional O(33f ?) dif-
ferent choices. Overall, we have a total number of
O((3n)3f ?) different choices. For each such choice,
the test for membership in workingSet for ?1 takes
constant time in case we use a multi-dimensional ta-
ble, or else O(|p|) in case we use a trie. The ad-
jacency test and the merge operations can easily be
carried out in time O(|p|).
Putting all of the above observations together, and
using the already observed fact that n = O(|p|),
we can conclude that the total amount of time re-
quired by the while-loop at lines 7 to 13 is bounded
byO(|p| ? (3|p|)3f ?), both under the assumption that
workingSet is represented as a multi-dimensional ta-
ble or as a trie. This is also a bound on the running
time of the whole algorithm.
4.5 Minimal binarization of a complete LCFRS
The algorithm defined in Section 4.3 can be used
to binarize an LCFRS in such a way that each rule
in the resulting binarization has the minimum pos-
sible fan-out. This can be done by applying the
BOUNDED-BINARIZATION algorithm to each pro-
duction p, until we find the minimum value for the
544
1: Function MINIMAL-BINARIZATION(G)
2: pb = ? {Set of binarized productions}
3: for all production p of G do
4: f ? = fan-out(p);
5: while not BOUNDED-BINARIZATION(p, f ?)
do
6: f ? = f ? + 1;
7: add result of BOUNDED-BINARIZATION(p,
f ?) to pb; {We obtain the tree from
BOUNDED-BINARIZATION as explained in
Section 4.3 and use it to binarize p}
8: return pb;
Figure 3: Minimal binarization by sequential search
bound f ? for which this algorithm finds a binariza-
tion. For a production with rank r and fan-out f ,
we know that this optimal value of f ? must be in
the interval [f, d r2e ? f ] because binarizing a pro-duction cannot reduce its fan-out, and the NAIVE-
BINARIZATION algorithm seen in Section 4.1 can
binarize any production by increasing fan-out to
d r2e ? f in the worst case.
The simplest way of finding out the optimal value
of f ? for each production is by a sequential search
starting with ?(p) and going upwards, as in the algo-
rithm in Figure 3. Note that the upper bound d r2e ? fthat we have given for f ? guarantees that the while-
loop in this algorithm always terminates.
In the worst case, we may need f ? (d r2e ? 1) + 1executions of the BOUNDED-BINARIZATION algo-
rithm to find the optimal binarization of a production
in G. This complexity can be reduced by changing
the strategy to search for the optimal f ?: for exam-
ple, we can perform a binary search within the inter-
val [f, d r2e ? f ], which lets us find the optimal bina-rization in blog(f ? (d r2e?1)+1)c+1 executions ofBOUNDED-BINARIZATION. However, this will not
result in a practical improvement, since BOUNDED-
BINARIZATION is exponential in the value of f ? and
the binary search will require us to run it on val-
ues of f ? larger than the optimal in most cases. An
intermediate strategy between the two is to apply
exponential backoff to try the sequence of values
f?1+2i (for i = 0, 1, 2 . . .). When we find the first
i such that BOUNDED-BINARIZATION does not fail,
if i > 0, we apply the same strategy to the interval
[f?1+2i?1, f?2+2i], and we repeat this method to
shrink the interval until BOUNDED-BINARIZATION
does not fail for i = 0, giving us our optimal f ?.
With this strategy, the amount of executions of the
algorithm that we need in the worst case is
1
2(dlog(?)e+ dlog(?)e
2) + 1 ,
where ? = f ? (d r2e ? 1) + 1, but we avoid usingunnecessarily large values of f ?.
5 Discussion
To conclude this paper, we now discuss a number of
aspects of the results that we have presented, includ-
ing various other pieces of research that are particu-
larly relevant to this paper.
5.1 The tradeoff between rank and fan-out
The algorithm introduced in this paper can be used
to transform an LCFRS into an equivalent form
with rank 2. This will result into a more effi-
ciently parsable LCFRS, since rank exponentially
affects parsing complexity. However, we must take
into account that parsing complexity is also influ-
enced by fan-out. Our algorithm guarantees a min-
imal increase in fan-out. In practical cases it seems
such an increase is quite small. For example, in
the context of dependency parsing, both Go?mez-
Rodr??guez et al (2009) and Kuhlmann and Satta
(2009) show that all the structures in several well-
known non-projective dependency treebanks are bi-
narizable without any increase in their fan-out.
More in general, it has been shown by Seki et al
(1991) that parsing of LCFRS can be carried out in
time O(n|pM |), where n is the length of the input
string and pM is the production in the grammar with
largest size.3 Thus, there may be cases in which one
has to find an optimal tradeoff between rank and fan-
out, in order to minimize the size of pM . This re-
quires some kind of Viterbi search over the space of
all possible binarizations, constructed as described
at the end of Subsection 4.3, for some appropriate
value of the fan-out f ?.
3The result has been shown for the formalism of multiple
context-free grammars (MCFG), but it also applies to LCFRS,
which are a special case of MCFG.
545
5.2 Extension to general LCFRS
This paper has focussed on string-based LCFRS.
As discussed in Vijay-Shanker et al (1987), LCFRS
provide a more general framework where the pro-
ductions are viewed as generating a set of abstract
derivation trees. These trees can be used to specify
how structures other than tuples of strings are com-
posed. For example, LCFRS derivation trees can be
used to specify how the elementary trees of a Tree
Adjoining Grammar can be composed to produced
derived tree. However, the results in this paper also
apply to non-string-based LCFRS, since by limit-
ing attention to the terminal string yield of whatever
structures are under consideration, the composition
operations can be defined using the string-based ver-
sion of LCFRS that is discussed here.
5.3 Similar algorithmic techniques
The NAIVE-BINARIZATION algorithm given in Fig-
ure 1 is not novel to this paper: it is similar to
an algorithm developed in Melamed et al (2004)
for generalized multitext grammars, a formalism
weakly equivalent to LCFRS that has been intro-
duced for syntax-based machine translation. How-
ever, the grammar produced by our algorithm has
optimal (minimal) fan-out. This is an important im-
provement over the result in (Melamed et al, 2004),
as this quantity enters into the parsing complexity
of both multitext grammars and LCFRS as an expo-
nential factor, and therefore must be kept as low as
possible to ensure practically viable parsing.
Rank reduction is also investigated in Nesson
et al (2008) for synchronous tree-adjoining gram-
mars, a synchronous rewriting formalism based on
tree-adjoining grammars Joshi and Schabes (1992).
In this case the search space of possible reductions
is strongly restricted by the tree structures specified
by the formalism, resulting in simplified computa-
tion for the reduction algorithms. This feature is not
present in the case of LCFRS.
There is a close parallel between the technique
used in the MINIMAL-BINARIZATION algorithm
and deductive parsing techniques as proposed by
Shieber et al (1995), that are usually implemented
by means of tabular methods. The idea of exploit-
ing tabular parsing in production factorization was
first expressed in Zhang et al (2006). In fact, the
particular approach presented here has been used
to improve efficiency of parsing algorithms that use
discontinuous syntactic models, in particular, non-
projective dependency grammars, as discussed in
Go?mez-Rodr??guez et al (2009).
5.4 Open problems
The bounded binarization algorithm that we have
presented has exponential run-time in the value of
the input fan-out bound f ?. It remains an open ques-
tion whether the bounded binarization problem for
LCFRS can be solved in deterministic polynomial
time. Even in the restricted case of f ? = ?(p), that
is, when no increase in the fan-out of the input pro-
duction is allowed, we do not know whether p can be
binarized using only deterministic polynomial time
in the value of p?s fan-out. However, our bounded
binarization algorithm shows that the latter problem
can be solved in polynomial time when the fan-out
of the input LCFRS is bounded by some constant.
Whether the bounded binarization problem can
be solved in polynomial time in the value of the
input bound f ? is also an open problem in the re-
stricted case of synchronous context-free grammars,
a special case of an LCFRS of fan-out two with
a strict separation between the two components of
each nonterminal in the right-hand side of a produc-
tion, as discussed in the introduction. An interesting
analysis of this restricted problem can be found in
Gildea and Stefankovic (2007).
Acknowledgements The work of Carlos Go?mez-
Rodr??guez was funded by Ministerio de Educacio?n
y Ciencia and FEDER (HUM2007-66607-C04) and
Xunta de Galicia (PGIDIT07SIN005206PR, IN-
CITE08E1R104022ES, INCITE08ENA305025ES,
INCITE08PXIB302179PR and Rede Galega de
Procesamento da Linguaxe e Recuperacio?n de Infor-
macio?n). The work of Marco Kuhlmann was funded
by the Swedish Research Council. The work of
Giorgio Satta was supported by MIUR under project
PRIN No. 2007TJNZRE 002. We are grateful to an
anonymous reviewer for a very detailed review with
a number of particularly useful suggestions.
546
References
David Chiang. 2007. Hierarchical phrase-
based translation. Computational Linguistics,
33(2):201?228.
Daniel Gildea and Daniel Stefankovic. 2007. Worst-
case synchronous grammar rules. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for
Computational Linguistics; Proceedings of the
Main Conference, pages 147?154. Association
for Computational Linguistics, Rochester, New
York.
Carlos Go?mez-Rodr??guez, David J. Weir, and John
Carroll. 2009. Parsing mildly non-projective de-
pendency structures. In Twelfth Conference of the
European Chapter of the Association for Compu-
tational Linguistics (EACL). To appear.
A. K. Joshi and Y. Schabes. 1992. Tree adjoining
grammars and lexicalized grammars. In M. Nivat
and A. Podelsky, editors, Tree Automata and Lan-
guages. Elsevier, Amsterdam, The Netherlands.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-
ACL), Main Conference Poster Sessions, pages
507?514. Sydney, Australia.
Marco Kuhlmann and Giorgio Satta. 2009. Tree-
bank grammar techniques for non-projective de-
pendency parsing. In Twelfth Conference of the
European Chapter of the Association for Compu-
tational Linguistics (EACL). To appear.
I. Dan Melamed, Benjamin Wellington, and Gior-
gio Satta. 2004. Generalized multitext gram-
mars. In 42nd Annual Meeting of the Association
for Computational Linguistics (ACL), pages 661?
668. Barcelona, Spain.
Rebecca Nesson, Giorgio Satta, and Stuart M.
Shieber. 2008. Optimal k-arization of syn-
chronous tree-adjoining grammar. In Proceedings
of ACL-08: HLT, pages 604?612. Association for
Computational Linguistics, Columbus, Ohio.
A. Nijholt. 1980. Context-Free Grammars: Cov-
ers, Normal Forms, and Parsing, volume 93.
Springer-Verlag, Berlin, Germany.
Owen Rambow and Giorgio Satta. 1999. Indepen-
dent parallelism in finite copying parallel rewrit-
ing systems. Theoretical Computer Science,
223(1?2):87?120.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On Multiple Context-
Free Grammars. Theoretical Computer Science,
88(2):191?229.
Stuart M. Shieber, Yves Schabes, and Fernando
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Program-
ming, 24(1?2):3?36.
Takeaki Uno and Mutsunori Yagiura. 2000. Fast al-
gorithms to enumerate all common intervals of
two permutations. Algorithmica, 26(2):290?309.
K. Vijay-Shanker, David J. Weir, and Aravind K.
Joshi. 1987. Characterizing structural descrip-
tions produced by various grammatical for-
malisms. In 25th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), pages
104?111. Stanford, CA, USA.
Benjamin Wellington, Sonjia Waxmonsky, and
I. Dan Melamed. 2006. Empirical lower bounds
on the complexity of translational equivalence. In
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-
ACL), pages 977?984. Sydney, Australia.
Hao Zhang, Daniel Gildea, and David Chiang.
2008. Extracting synchronous grammar rules
from word-level alignments in linear time. In
22nd International Conference on Computational
Linguistics (Coling), pages 1081?1088. Manch-
ester, England, UK.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Human Language Technol-
ogy Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 256?263. New York, USA.
547
Parsing Non-Recursive Context-Free Grammars
Mark-Jan Nederhof  
Faculty of Arts
University of Groningen
P.O. Box 716
NL-9700 AS Groningen, The Netherlands
markjan@let.rug.nl
Giorgio Satta
Dip. di Elettronica e Informatica
Universita` di Padova
via Gradenigo, 6/A
I-35131 Padova, Italy
satta@dei.unipd.it
Abstract
We consider the problem of parsing
non-recursive context-free grammars, i.e.,
context-free grammars that generate finite
languages. In natural language process-
ing, this problem arises in several areas
of application, including natural language
generation, speech recognition and ma-
chine translation. We present two tabu-
lar algorithms for parsing of non-recursive
context-free grammars, and show that they
perform well in practical settings, despite
the fact that this problem is PSPACE-
complete.
1 Introduction
Several applications in natural language processing
require ?parsing? of a large but finite set of candidate
strings. Here parsing means some computation that
selects those strings out of the finite set that are well-
formed according to some grammar, or that are most
likely according to some language model. In these
applications, the finite set is typically encoded in a
compact way as a context-free grammar (CFG) that
is non-recursive. This is motivated by the fact that
non-recursive CFGs allow very compact represen-
tations for finite languages, since the strings deriv-
able from single nonterminals may be substrings of
many different strings in the language. Unfolding
such a grammar and parsing the generated strings

Secondary affiliation is the German Research Center for
Artificial Intelligence (DFKI).
one by one then leads to an unnecessary duplica-
tion of subcomputations, since each occurrence of
a repeated substring has to be independently parsed.
As this approach may be prohibitively expensive, it
is preferable to find a parsing algorithm that shares
subcomputations among different strings by work-
ing directly on the nonterminals and the rules of the
non-recursive CFG. In this way, ?parsing? a nonter-
minal of the grammar amounts to shared parsing of
all the substrings encoded by that nonterminal.
To give a few examples, in some natural lan-
guage generation systems (Langkilde, 2000) non-
recursive CFGs are used to encode very large sets
of candidate sentences realizing some input con-
ceptual representation (Langkilde calls such gram-
mars forests). Each CFG is later ?parsed? using a
language model, in order to rank the sentences in
the set according to their likelyhood. Similarly, in
some approaches to automatic speech understand-
ing (Corazza and Lavelli, 1994) the  -best sen-
tences obtained from the speech recognition module
are ?compressed? into a non-recursive CFG gram-
mar, which is later provided as input to a parser. Fi-
nally, in some machine translation applications re-
lated techniques are exploited to obtain sentences
that simultaneously realize two different conceptual
representations (Knight and Langkilde, 2000). This
is done in order to produce translations that preserve
syntactic or semantic ambiguity in cases where the
ambiguity could not be resolved when processing
the source sentence.
To be able to describe the above applications in an
abstract way, let us first fix some terminology. The
term ?recognition? refers to the process of deciding
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 112-119.
                         Proceedings of the 40th Annual Meeting of the Association for
whether an input string is in the language described
by a grammar, the parsing grammar  . We will
generalize this notion in a natural way to input rep-
resenting a set of strings, and here the goal of recog-
nition is to decide whether at least one of the strings
in the set is in the language described by  . If the
input is itself given in the form of a grammar, the
input grammar  , then recognition amounts to de-
termining whether the intersection of the languages
described by  and 	 is non-empty. In this paper
we use the term parsing as synonymous to recog-
nition, since the recognition algorithms we present
can be easily extended to yield parse trees (with as-
sociated probabilities if either 
 or 	 or both are
probabilistic).
In what follows we consider the case where both
	 and  are CFGs. General CFGs have un-
favourable computational properties with respect to
intersection. In particular, the problem of deciding
whether the intersection of two CFGs is non-empty
is undecidable (Harrison, 1978). Following the ter-
minology adopted above, this means that parsing
a context-free input grammar  on the basis of a
context-free parsing grammar  is not possible in
general.
One way to make the parsing problem decidable
is to place some additional restrictions on 
 or
	 . This direction is taken by Langkilde (2000),
where   is a non-recursive CFG and   repre-
sents a regular language, more precisely an

-gram
model. In this way the problem can be solved us-
ing a stochastic variant of an algorithm presented
by Bar-Hillel et al (1964), where it is shown that the
intersection of a general context-free language and a
regular language is still context-free.
In the present paper we leave the theoretical
framework of Bar-Hillel et al (1964), and consider
parsing grammars  that are unrestricted CFGs,
and input grammars  that are non-recursive
context-free grammars. In this case the parsing (in-
tersection) problem becomes PSPACE-complete.1
Despite of this unfavourable theoretical result, algo-
rithms for the problem at hand have been proposed
in the literature and are currently used in practical
applications. In (Knight and Langkilde, 2000) 
 is
1The PSPACE-hardness result has been shown by Harry B.
Hunt III and Dan Rosenkrantz (Harry B. Hunt III, p.c.). Mem-
bership in PSPACE is shown by Nederhof and Satta (2002).
unfolded into a lattice (acyclic finite automaton) and
later parsed with  using an algorithm close to the
one by Bar-Hillel et al (1964). The algorithm pro-
posed by Corazza and Lavelli (1994) involves copy-
ing of charts, and this makes it very similar in be-
haviour to the former approach. Thus in both al-
gorithms parts of the input grammar  are copied
where a nonterminal occurs more than once, which
destroys the compactness of the representation. In
this paper we propose two alternative tabular algo-
rithms that exploit the compactness of 
 as much
as possible. Although a limited amount of copying
is also done by our algorithms, this never happens in
cases where the resulting structure is ungrammatical
with respect to the parsing grammar   .
The structure of this paper is as follows. In Sec-
tion 2 we introduce some preliminary definitions,
followed in Section 3 by a first algorithm based on
CKY parsing. A more sophisticated algorithm, sat-
isfying the equivalent of the correct-prefix property
and based on Earley?s algorithm, is presented in Sec-
tion 4. Section 5 presents our experimental results
and Section 6 closes with some discussion.
2 Preliminaries
In this section we briefly recall some standard no-
tions from formal language theory. For more details
we refer the reader to textbooks such as (Harrison,
1978).
A context-free grammar is a 4-tuple 

, where  is a finite set of terminals, called the
alphabet,  is a finite set of nonterminals, including
the start symbol  , and  is a finite set of rules hav-
ing the form Probabilistic Parsing Strategies
Mark-Jan Nederhof
Faculty of Arts
University of Groningen
P.O. Box 716
NL-9700 AS Groningen
The Netherlands
markjan@let.rug.nl
Giorgio Satta
Dept. of Information Engineering
University of Padua
via Gradenigo, 6/A
I-35131 Padova
Italy
satta@dei.unipd.it
Abstract
We present new results on the relation between
context-free parsing strategies and their probabilis-
tic counter-parts. We provide a necessary condition
and a sufficient condition for the probabilistic exten-
sion of parsing strategies. These results generalize
existing results in the literature that were obtained
by considering parsing strategies in isolation.
1 Introduction
Context-free grammars (CFGs) are standardly used
in computational linguistics as formal models of the
syntax of natural language, associating sentences
with all their possible derivations. Other computa-
tional models with the same generative capacity as
CFGs are also adopted, as for instance push-down
automata (PDAs). One of the advantages of the use
of PDAs is that these devices provide an operational
specification that determines which steps must be
performed when parsing an input string, something
that is not offered by CFGs. In other words, PDAs
can be associated to parsing strategies for context-
free languages. More precisely, parsing strategies
are traditionally specified as constructions that map
CFGs to language-equivalent PDAs. Popular ex-
amples of parsing strategies are the standard con-
structions of top-down PDAs (Harrison, 1978), left-
corner PDAs (Rosenkrantz and Lewis II, 1970),
shift-reduce PDAs (Aho and Ullman, 1972) and LR
PDAs (Sippu and Soisalon-Soininen, 1990).
CFGs and PDAs have probabilistic counterparts,
called probabilistic CFGs (PCFGs) and probabilis-
tic PDAs (PPDAs). These models are very popular
in natural language processing applications, where
they are used to define a probability distribution
function on the domain of all derivations for sen-
tences in the language of interest. In PCFGs and
PPDAs, probabilities are assigned to rules or tran-
sitions, respectively. However, these probabilities
cannot be chosen entirely arbitrarily. For example,
for a given nonterminalA in a PCFG, the sum of the
probabilities of all rules rewritingAmust be 1. This
means that, out of a total of saym rules rewritingA,
only m? 1 rules represent ?free? parameters.
Depending on the choice of the parsing strategy,
the constructed PDA may allow different probabil-
ity distributions than the underlying CFG, since the
set of free parameters may differ between the CFG
and the PDA, both quantitatively and qualitatively.
For example, (Sornlertlamvanich et al, 1999) and
(Roark and Johnson, 1999) have shown that a prob-
ability distribution that can be obtained by training
the probabilities of a CFG on the basis of a corpus
can be less accurate than the probability distribution
obtained by training the probabilities of a PDA con-
structed by a particular parsing strategy, on the basis
of the same corpus. Also the results from (Chitrao
and Grishman, 1990), (Charniak and Carroll, 1994)
and (Manning and Carpenter, 2000) could be seen
in this light.
The question arises of whether parsing strate-
gies can be extended probabilistically, i.e., whether
a given construction of PDAs from CFGs can be
?augmented? with a function defining the probabili-
ties for the target PDA, given the probabilities asso-
ciated with the input CFG, in such a way that the ob-
tained probabilistic distributions on the CFG deriva-
tions and the corresponding PDA computations are
equivalent. Some first results on this issue have been
presented by (Tendeau, 1995), who shows that the
already mentioned left-corner parsing strategy can
be extended probabilistically, and later by (Abney et
al., 1999) who show that the pure top-down parsing
strategy and a specific type of shift-reduce parsing
strategy can be probabilistically extended.
One might think that any ?practical? parsing
strategy can be probabilistically extended, but this
turns out not to be the case. We briefly discuss
here a counter-example, in order to motivate the ap-
proach we have taken in this paper. Probabilistic
LR parsing has been investigated in the literature
(Wright and Wrigley, 1991; Briscoe and Carroll,
1993; Inui et al, 2000) under the assumption that
it would allow more fine-grained probability distri-
butions than the underlying PCFGs. However, this
is not the case in general. Consider a PCFG with
rule/probability pairs:
S ? AB , 1 B ? bC , 23
A? aC , 13 B ? bD ,
1
3
A? aD , 23 C ? xc, 1
D ? xd , 1
There are two key transitions in the associated LR
automaton, which represent shift actions over c and
d (we denote LR states by their sets of kernel items
and encode these states into stack symbols):
?c : {C ? x ? c,D ? x ? d}
c7?
{C ? x ? c,D ? x ? d} {C ? xc ?}
?d : {C ? x ? c,D ? x ? d}
d7?
{C ? x ? c,D ? x ? d} {D ? xd ?}
Assume a proper assignment of probabilities to the
transitions of the LR automaton, i.e., the sum of
transition probabilities for a given LR state is 1. It
can be easily seen that we must assign probabil-
ity 1 to all transitions except ?c and ?d, since this
is the only pair of distinct transitions that can be ap-
plied for one and the same top-of-stack symbol, viz.
{C ? x ? c,D ? x ? d}. However, in the PCFG
model we have
Pr(axcbxd)
Pr(axdbxc) =
Pr(A?aC )?Pr(B?bD)
Pr(A?aD)?Pr(B?bC ) =
1
3 ?
1
3
2
3 ?
2
3
= 14
whereas in the LR PPDA model we have
Pr(axcbxd)
Pr(axdbxc) =
Pr(?c)?Pr(?d)
Pr(?d)?Pr(?c)
= 1 6= 14 .
Thus we conclude that there is no proper assignment
of probabilities to the transitions of the LR automa-
ton that would result in a distribution on the gener-
ated language that is equivalent to the one induced
by the source PCFG. Therefore the LR strategy does
not allow probabilistic extension.
One may seemingly solve this problem by drop-
ping the constraint of properness, letting each tran-
sition that outputs a rule have the same probability
as that rule in the PCFG, and letting other transitions
have probability 1. However, the properness condi-
tion for PDAs has been heavily exploited in pars-
ing applications, in doing incremental left-to-right
probability computation for beam search (Roark
and Johnson, 1999; Manning and Carpenter, 2000),
and more generally in integration with other lin-
ear probabilistic models. Furthermore, commonly
used training algorithms for PCFGS/PPDAs always
produce proper probability assignments, and many
desired mathematical properties of these methods
are based on such an assumption (Chi and Geman,
1998; Sa?nchez and Bened??, 1997). We may there-
fore discard non-proper probability assignments in
the current study.
However, such probability assignments are out-
side the reach of the usual training algorithms for
PDAs, which always produce proper PDAs. There-
fore, we may discard such assignments in the cur-
rent study, which investigates aspects of the poten-
tial of training algorithms for CFGs and PDAs.
What has been lacking in the literature is a theo-
retical framework to relate the parameter space of a
CFG to that of a PDA constructed from the CFG by
a particular parsing strategy, in terms of the set of
allowable probability distributions over derivations.
Note that the number of free parameters alone is
not a satisfactory characterization of the parameter
space. In fact, if the ?nature? of the parameters is
ill-chosen, then an increase in the number of param-
eters may lead to a deterioration of the accuracy of
the model, due to sparseness of data.
In this paper we extend previous results, where
only a few specific parsing strategies were consid-
ered in isolation, and provide some general char-
acterization of parsing strategies that can be prob-
abilistically extended. Our main contribution can
be stated as follows.
? We define a theoretical framework to relate the
parameter space defined by a CFG and that de-
fined by a PDA constructed from the CFG by a
particular parsing strategy.
? We provide a necessary condition and a suffi-
cient condition for the probabilistic extension
of parsing strategies.
We use the above findings to establish new results
about probabilistic extensions of parsing strategies
that are used in standard practice in computational
linguistics, as well as to provide simpler proofs of
already known results.
We introduce our framework in Section 3 and re-
port our main results in Sections 4 and 5. We discuss
applications of our results in Section 6.
2 Preliminaries
In this paper we assume some familiarity with def-
initions of (P)CFGs and (P)PDAs. We refer the
reader to standard textbooks and publications as for
instance (Harrison, 1978; Booth and Thompson,
1973; Santos, 1972).
A CFG G is a tuple (?, N, S, R), with ? and N
the sets of terminals and nonterminals, respectively,
S the start symbol and R the set of rules. In this
paper we only consider left-most derivations, repre-
sented as strings d ? R? and simply called deriva-
tions. For ?, ? ? (? ?N)?, we write ??d ? with
the usual meaning. If ? = S and ? = w ? ??, we
call d a complete derivation of w. We say a CFG is
reduced if each rule in R occurs in some complete
derivation.
A PCFG is a pair (G, p) consisting of a CFG G
and a probability function p from R to real num-
bers in the interval [0, 1]. A PCFG is proper
if
?
pi=(A??)?R p(pi) = 1 for each A ? N .
The probability of a (left-most) derivation d =
pi1 ? ? ?pim, pii ? R for 1 ? i ? m, is p(d) =?m
i=1 p(pii). The probability of a string w ? ??
is p(w) =
?
S?dw p(d). A PCFG is consistent if
?w??? p(w) = 1. A PCFG (G, p) is reduced if G is
reduced.
In this paper we will mainly consider push-down
transducers rather than push-down automata. Push-
down transducers not only compute derivations of
the grammar while processing an input string, but
they also explicitly produce output strings from
which these derivations can be obtained. We use
transducers for two reasons. First, constraints on
the output strings allow us to restrict our attention
to ?reasonable? parsing strategies. Those strategies
that cannot be formalized within these constraints
are unlikely to be of practical interest. Secondly,
mappings from input strings to derivations, such as
those realized by push-down transducers, turn out
to be a very powerful abstraction and allow direct
proofs of several general results.
Contrary to many textbooks, our push-down de-
vices do not possess states next to stack symbols.
This is without loss of generality, since states can
be encoded into the stack symbols, given the types
of transitions that we allow. Thus, a PDT A is a
6-tuple (??, ??, Q, Xin, Xfin, ?), with ?? and
?? the input and output alphabets, respectively, Q
the set of stack symbols, including the initial and fi-
nal stack symbols Xin and Xfin, respectively, and
? the set of transitions. Each transition has one
of the following three forms: X 7? XY , called a
push transition, YX 7? Z, called a pop transition,
or X
x,y
7? Y , called a swap transition; here X , Y ,
Z ? Q, x ? ?? ? {?} is the input read by the tran-
sition and y ? ??? is the written output. Note that
in our notation, stacks grow from left to right, i.e.,
the top-most stack symbol will be found at the right
end. A configuration of a PDT is a triple (?,w, v),
where ? ? Q? is a stack, w ? ??? is the remain-
ing input, and v ? ??? is the output generated so
far. Computations are represented as strings c ?
??. For configurations (?,w, v) and (?,w?, v?), we
write (?,w, v) `c (?,w?, v?) with the usual mean-
ing, and write (?,w, v) `? (?,w?, v?) when c is of
no importance. If (Xin, w, ?) `c (Xfin, ?, v), then
c is a complete computation of w, and the output
string v is denoted out(c). A PDT is reduced if
each transition in ? occurs in some complete com-
putation.
Without loss of generality, we assume that com-
binations of different types of transitions are not al-
lowed for a given stack symbol. More precisely,
for each stack symbol X 6= Xfin, the PDA can
only take transitions of a single type (push, pop or
swap). A PDT can easily be brought in this form
by introducing for each X three new stack symbols
Xpush , Xpop and Xswap and new swap transitions
X
?,?
7? Xpush , X
?,?
7? Xpop and X
?,?
7? Xswap . In
each existing transition that operates on top-of-stack
X , we then replace X by one from Xpush , Xpop or
Xswap , depending on the type of that transition. We
also assume that Xfin does not occur in the left-
hand side of a transition, again without loss of gen-
erality.
A PPDT is a pair (A, p) consisting of a PDT A
and a probability function p from? to real numbers
in the interval [0, 1]. A PPDT is proper if
? ??=(X 7?XY )?? p(?) = 1 for each X ? Q
such that there is at least one transition X 7?
XY , Y ? Q;
? ?
?=(X
x,y
7?Y )??
p(?) = 1 for each X ? Q such
that there is at least one transition X x,y7? Y ,
x ? ?? ? {?}, y ? ??? , Y ? Q; and
? ??=(Y X 7?Z)?? p(?) = 1, for each X,Y ? Q
such that there is at least one transition Y X 7?
Z, Z ? Q.
The probability of a computation c = ?1 ? ? ? ?m,
?i ? ? for 1 ? i ? m, is p(c) =?m
i=1 p(?i). The probability of a stringw is p(w) =?
(Xin,w,?)`c(Xfin,?,v) p(c). A PPDT is consistent
if ?w??? p(w) = 1. A PPDT (A, p) is reduced if
A is reduced.
3 Parsing Strategies
The term ?parsing strategy? is often used informally
to refer to a class of parsing algorithms that behave
similarly in some way. In this paper, we assign a
formal meaning to this term, relying on the obser-
vation by (Lang, 1974) and (Billot and Lang, 1989)
that many parsing algorithms for CFGs can be de-
scribed in two steps. The first is a construction of
push-down devices from CFGs, and the second is
a method for handling nondeterminism (e.g. back-
tracking or dynamic programming). Parsing algo-
rithms that handle nondeterminism in different ways
but apply the same construction of push-down de-
vices from CFGs are seen as realizations of the same
parsing strategy.
Thus, we define a parsing strategy to be a func-
tion S that maps a reduced CFG G = (??, N, S,
R) to a pair S(G) = (A, f) consisting of a reduced
PDT A = (??, ??, Q, Xin, Xfin, ?), and a func-
tion f that maps a subset of ??? to a subset of R?,
with the following properties:
? R ? ??.
? For each string w ? ??? and each complete
computation c on w, f(out(c)) = d is a (left-
most) derivation of w. Furthermore, each sym-
bol from R occurs as often in out(c) as it oc-
curs in d.
? Conversely, for each string w ? ??? and
each derivation d of w, there is precisely
one complete computation c on w such that
f(out(c)) = d.
If c is a complete computation, we will write f(c)
to denote f(out(c)). The conditions above then im-
ply that f is a bijection from complete computations
to complete derivations. Note that output strings of
(complete) computations may contain symbols that
are not in R, and the symbols that are in R may
occur in a different order in v than in f(v) = d.
The purpose of the symbols in ?? ? R is to help
this process of reordering of symbols from R in v,
as needed for instance in the case of the left-corner
parsing strategy (see (Nijholt, 1980, pp. 22?23) for
discussion).
A probabilistic parsing strategy is defined to
be a function S that maps a reduced, proper and
consistent PCFG (G, pG) to a triple S(G, pG) =
(A, pA, f), where (A, pA) is a reduced, proper and
consistent PPDT, with the same properties as a
(non-probabilistic) parsing strategy, and in addition:
? For each complete derivation d and each com-
plete computation c such that f(c) = d, pG(d)
equals pA(c).
In other words, a complete computation has the
same probability as the complete derivation that it
is mapped to by function f . An implication of
this property is that for each string w ? ??? , the
probabilities assigned to that string by (G, pG) and
(A, pA) are equal.
We say that probabilistic parsing strategy S ? is an
extension of parsing strategy S if for each reduced
CFG G and probability function pG we have S(G) =
(A, f) if and only if S ?(G, pG) = (A, pA, f) for
some pA.
4 Correct-Prefix Property
In this section we present a necessary condition for
the probabilistic extension of a parsing strategy. For
a given PDT, we say a computation c is dead if
(Xin, w1, ?) `c (?, ?, v1), for some ? ? Q?, w1 ?
??? and v1 ? ??? , and there are no w2 ? ??? and
v2 ? ??? such that (?,w2, ?) `? (Xfin, ?, v2). In-
formally, a dead computation is a computation that
cannot be continued to become a complete compu-
tation. We say that a PDT has the correct-prefix
property (CPP) if it does not allow any dead com-
putations. We also say that a parsing strategy has
the CPP if it maps each reduced CFG to a PDT that
has the CPP.
Lemma 1 For each reduced CFG G, there is a
probability function pG such that PCFG (G, pG) is
proper and consistent, and pG(d) > 0 for all com-
plete derivations d.
Proof. Since G is reduced, there is a finite set D
consisting of complete derivations d, such that for
each rule pi in G there is at least one d ? D in
which pi occurs. Let npi,d be the number of occur-
rences of rule pi in derivation d ? D, and let npi be
?d?D npi,d, the total number of occurrences of pi in
D. Let nA be the sum of npi for all rules pi withA in
the left-hand side. A probability function pG can be
defined through ?maximum-likelihood estimation?
such that pG(pi) = npinA for each rule pi = A? ?.
For all nonterminals A, ?pi=A?? pG(pi) =
?pi=A?? npinA =
nA
nA
= 1, which means that the
PCFG (G, pG) is proper. Furthermore, it has been
shown in (Chi and Geman, 1998; Sa?nchez and
Bened??, 1997) that a PCFG (G, pG) is consistent if
pG was obtained by maximum-likelihood estimation
using a set of derivations. Finally, since npi > 0 for
each pi, also pG(pi) > 0 for each pi, and pG(d) > 0
for all complete derivations d.
We say a computation is a shortest dead compu-
tation if it is dead and none of its proper prefixes is
dead. Note that each dead computation has a unique
prefix that is a shortest dead computation. For a
PDT A, let TA be the union of the set of all com-
plete computations and the set of all shortest dead
computations.
Lemma 2 For each proper PPDT (A, pA),
?c?TA pA(c) ? 1.
Proof. The proof is a trivial variant of the proof
that for a proper PCFG (G, pG), the sum of pG(d) for
all derivations d cannot exceed 1, which is shown by
(Booth and Thompson, 1973).
From this, the main result of this section follows.
Theorem 3 A parsing strategy that lacks the CPP
cannot be extended to become a probabilistic pars-
ing strategy.
Proof. Take a parsing strategy S that does not have
the CPP. Then there is a reduced CFG G = (??, N,
S, R), with S(G) = (A, f) for some A and f , and
a shortest dead computation c allowed by A.
It follows from Lemma 1 that there is a proba-
bility function pG such that (G, pG) is a proper and
consistent PCFG and pG(d) > 0 for all complete
derivations d. Assume we also have a probability
function pA such that (A, pA) is a proper and con-
sistent PPDT and pA(c?) = pG(f(c?)) for each com-
plete computation c?. SinceA is reduced, each tran-
sition ? must occur in some complete computation
c?. Furthermore, for each complete computation c?
there is a complete derivation d such that f(c?) = d,
and pA(c?) = pG(d) > 0. Therefore, pA(?) > 0
for each transition ? , and pA(c) > 0, where c is the
above-mentioned dead computation.
Due to Lemma 2, 1 ? ?c??TA pA(c?) ?
?w???? pA(w) + pA(c) > ?w???? pA(w) =
?w???? pG(w). This is in contradiction with the con-
sistency of (G, pG). Hence, a probability function
pA with the properties we required above cannot ex-
ist, and therefore S cannot be extended to become a
probabilistic parsing strategy.
5 Strong Predictiveness
In this section we present our main result, which is a
sufficient condition allowing the probabilistic exten-
sion of a parsing strategy. We start with a technical
result that was proven in (Abney et al, 1999; Chi,
1999; Nederhof and Satta, 2003).
Lemma 4 Given a non-proper PCFG (G, pG), G =
(?,N, S,R), there is a probability function p?G such
that PCFG (G, p?G) is proper and, for every com-
plete derivation d, p?G(d) = 1C ? pG(d), where C =?
S?d?w,w??? pG(d
?).
Note that if PCFG (G, pG) in the above lemma is
consistent, then C = 1 and (G, p?G) and (G, pG) de-
fine the same distribution on derivations. The nor-
malization procedure underlying Lemma 4 makes
use of quantities
?
A?dw,w??? pG(d) for each A ?
N . These quantities can be computed to any degree
of precision, as discussed for instance in (Booth and
Thompson, 1973) and (Stolcke, 1995). Thus nor-
malization of a PCFG can be effectively computed.
For a fixed PDT, we define the binary relation ;
on stack symbols by: Y ; Y ? if and only if
(Y,w, ?) `? (Y ?, ?, v) for some w ? ??? and
v ? ??? . In words, some subcomputation of the
PDT may start with stack Y and end with stack Y ?.
Note that all stacks that occur in such a subcompu-
tation must have height of 1 or more. We say that a
(P)PDA or a (P)PDT has the strong predictiveness
property (SPP) if the existence of three transitions
X 7? XY , XY1 7? Z1 and XY2 7? Z2 such that
Y ; Y1 and Y ; Y2 implies Z1 = Z2. Infor-
mally, this means that when a subcomputation starts
with some stack ? and some push transition ? , then
solely on the basis of ? we can uniquely determine
what stack symbol Z1 = Z2 will be on top of the
stack in the firstly reached configuration with stack
height equal to |?|. Another way of looking at it is
that no information may flow from higher stack el-
ements to lower stack elements that was not already
predicted before these higher stack elements came
into being, hence the term ?strong predictiveness?.
We say that a parsing strategy has the SPP if it maps
each reduced CFG to a PDT with the SPP.
Theorem 5 Any parsing strategy that has the CPP
and the SPP can be extended to become a proba-
bilistic parsing strategy.
Proof. Consider a parsing strategy S that has the
CPP and the SPP, and a proper, consistent and re-
duced PCFG (G, pG), G = (??, N, S, R). Let
S(G) = (A, f), A = (??, ??, Q, Xin, Xfin, ?).
We will show that there is a probability function pA
such that (A, pA) is a proper and consistent PPDT,
and pA(c) = pG(f(c)) for all complete computa-
tions c.
We first construct a PPDT (A, p?A) as follows.
For each scan transition ? = X x,y7? Y in ?, let
p?A(?) = pG(y) in case y ? R, and p?A(?) = 1
otherwise. For all remaining transitions ? ? ?, let
p?A(?) = 1. Note that (A, p?A) may be non-proper.
Still, from the definition of f it follows that, for each
complete computation c, we have
p?A(c) = pG(f(c)), (1)
and so our PPDT is consistent.
We now map (A, p?A) to a language-equivalent
PCFG (G?, pG?), G? = (??, Q,Xin, R?), where R?
contains the following rules with the specified asso-
ciated probabilities:
? X ? YZ with pG?(X ? YZ ) = p ?A(X 7?
XY ), for each X 7? XY ? ? with Z the
unique stack symbol such that there is at least
one transition XY ? 7? Z with Y ; Y ?;
? X ? xY with pG?(X ? xY ) = p ?A(X
x7?
Y ), for each transition X x7? Y ? ?;
? Y ? ? with pG?(X ? ?) = 1, for each stack
symbol Y such that there is at least one transi-
tion XY 7? Z ? ? or such that Y = Xfin.
It is not difficult to see that there exists a bijection
f ? from complete computations of A to complete
derivations of G?, and that we have
pG?(f
?(c)) = p?A(c), (2)
for each complete computation c. Thus (G?, pG?)
is consistent. However, note that (G?, pG?) is not
proper.
By Lemma 4, we can construct a new PCFG
(G?, p?G?) that is proper and consistent, and such that
pG?(d) = p?G?(d), for each complete derivation d of
G?. Thus, for each complete computation c ofA, we
have
p?G?(f
?(c)) = pG?(f
?(c)). (3)
We now transfer back the probabilities of rules of
(G?, p?G?) to the transitions ofA. Formally, we define
a new probability function pA such that, for each
? ? ?, pA(?) = p?G?(pi), where pi is the rule in R?
that has been constructed from ? as specified above.
It is easy to see that PPDT (A, pA) is now proper.
Furthermore, for each complete computation c ofA
we have
pA(c) = p
?
G?(f
?(c)), (4)
and so (A, pA) is also consistent. By combining
equations (1) to (4) we conclude that, for each com-
plete computation c of A, pA(c) = p?G?(f ?(c)) =
pG?(f ?(c)) = p?A(c) = pG(f(c)). Thus our parsing
strategy S can be probabilistically extended.
Note that the construction in the proof above can
be effectively computed (see discussion in Section 4
for effective computation of normalized PCFGs).
The definition of p?A in the proof of Theorem 5
relies on the strings output by A. This is the main
reason why we needed to consider PDTs rather
than PDAs. Now assume an appropriate probabil-
ity function pA has been computed, such that the
source PCFG and (A, pA) define equivalent dis-
tributions on derivations/computations. Then the
probabilities assigned to strings over the input al-
phabet are also equal. We may subsequently ignore
the output strings if the application at hand merely
requires probabilistic recognition rather than proba-
bilistic transduction, or in other words, we may sim-
plify PDTs to PDAs.
The proof of Theorem 5 also leads to the obser-
vation that parsing strategies with the CPP and the
SPP as well as their probabilistic extensions can be
described as grammar transformations, as follows.
A given (P)CFG is mapped to an equivalent (P)PDT
by a (probabilistic) parsing strategy. By ignoring
the output components of swap transitions we ob-
tain a (P)PDA, which can be mapped to an equiva-
lent (P)CFG as shown above. This observation gives
rise to an extension with probabilities of the work on
covers by (Nijholt, 1980; Leermakers, 1989).
6 Applications
Many well-known parsing strategies with the CPP
also have the SPP. This is for instance the case
for top-down parsing and left-corner parsing. As
discussed in the introduction, it has already been
shown that for any PCFG G, there are equiva-
lent PPDTs implementing these strategies, as re-
ported in (Abney et al, 1999) and (Tendeau, 1995),
respectively. Those results more simply follow
now from our general characterization. Further-
more, PLR parsing (Soisalon-Soininen and Ukko-
nen, 1979; Nederhof, 1994) can be expressed in our
framework as a parsing strategy with the CPP and
the SPP, and thus we obtain as a new result that this
strategy allows probabilistic extension.
The above strategies are in contrast to the LR
parsing strategy, which has the CPP but lacks the
SPP, and therefore falls outside our sufficient condi-
tion. As we have already seen in the introduction, it
turns out that LR parsing cannot be extended to be-
come a probabilistic parsing strategy. Related to LR
parsing is ELR parsing (Purdom and Brown, 1981;
Nederhof, 1994), which also lacks the SPP. By an
argument similar to the one provided for LR, we can
show that also ELR parsing cannot be extended to
become a probabilistic parsing strategy. (See (Ten-
deau, 1997) for earlier observations related to this.)
These two cases might suggest that the sufficient
condition in Theorem 5 is tight in practice.
Decidability of the CPP and the SPP obviously
depends on how a parsing strategy is specified. As
far as we know, in all practical cases of parsing
strategies these properties can be easily decided.
Also, observe that our results do not depend on the
general behaviour of a parsing strategy S, but just
on its ?point-wise? behaviour on each input CFG.
Specifically, if S does not have the CPP and the
SPP, but for some fixed CFG G of interest we ob-
tain a PDT A that has the CPP and the SPP, then
we can still apply the construction in Theorem 5.
In this way, any probability function pG associated
with G can be converted into a probability function
pA, such that the resulting PCFG and PPDT induce
equivalent distributions. We point out that decid-
ability of the CPP and the SPP for a fixed PDT can
be efficiently decided using dynamic programming.
One more consequence of our results is this. As
discussed in the introduction, the properness condi-
tion reduces the number of parameters of a PPDT.
However, our results show that if the PPDT has the
CPP and the SPP then the properness assumption is
not restrictive, i.e., by lifting properness we do not
gain new distributions with respect to those induced
by the underlying PCFG.
7 Conclusions
We have formalized the notion of CFG parsing strat-
egy as a mapping from CFGs to PDTs, and have in-
vestigated the extension to probabilities. We have
shown that the question of which parsing strategies
can be extended to become probabilistic heavily re-
lies on two properties, the correct-prefix property
and the strong predictiveness property. As far as we
know, this is the first general characterization that
has been provided in the literature for probabilistic
extension of CFG parsing strategies. We have also
shown that there is at least one strategy of practical
interest with the CPP but without the SPP, namely
LR parsing, that cannot be extended to become a
probabilistic parsing strategy.
Acknowledgements
The first author is supported by the PIO-
NIER Project Algorithms for Linguistic Process-
ing, funded by NWO (Dutch Organization for
Scientific Research). The second author is par-
tially supported by MIUR under project PRIN No.
2003091149 005.
References
S. Abney, D. McAllester, and F. Pereira. 1999. Re-
lating probabilistic grammars and automata. In
37th Annual Meeting of the Association for Com-
putational Linguistics, Proceedings of the Con-
ference, pages 542?549, Maryland, USA, June.
A.V. Aho and J.D. Ullman. 1972. Parsing, vol-
ume 1 of The Theory of Parsing, Translation and
Compiling. Prentice-Hall.
S. Billot and B. Lang. 1989. The structure of
shared forests in ambiguous parsing. In 27th
Annual Meeting of the Association for Com-
putational Linguistics, Proceedings of the Con-
ference, pages 143?151, Vancouver, British
Columbia, Canada, June.
T.L. Booth and R.A. Thompson. 1973. Apply-
ing probabilistic measures to abstract languages.
IEEE Transactions on Computers, C-22(5):442?
450, May.
T. Briscoe and J. Carroll. 1993. Generalized prob-
abilistic LR parsing of natural language (cor-
pora) with unification-based grammars. Compu-
tational Linguistics, 19(1):25?59.
E. Charniak and G. Carroll. 1994. Context-
sensitive statistics for improved grammatical lan-
guage models. In Proceedings Twelfth National
Conference on Artificial Intelligence, volume 1,
pages 728?733, Seattle, Washington.
Z. Chi and S. Geman. 1998. Estimation of prob-
abilistic context-free grammars. Computational
Linguistics, 24(2):299?305.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguis-
tics, 25(1):131?160.
M.V. Chitrao and R. Grishman. 1990. Statistical
parsing of messages. In Speech and Natural Lan-
guage, Proceedings, pages 263?266, Hidden Val-
ley, Pennsylvania, June.
M.A. Harrison. 1978. Introduction to Formal Lan-
guage Theory. Addison-Wesley.
K. Inui, V. Sornlertlamvanich, H. Tanaka, and
T. Tokunaga. 2000. Probabilistic GLR parsing.
In H. Bunt and A. Nijholt, editors, Advances
in Probabilistic and other Parsing Technologies,
chapter 5, pages 85?104. Kluwer Academic Pub-
lishers.
B. Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In Automata,
Languages and Programming, 2nd Colloquium,
volume 14 of Lecture Notes in Computer Science,
pages 255?269, Saarbru?cken. Springer-Verlag.
R. Leermakers. 1989. How to cover a grammar.
In 27th Annual Meeting of the Association for
Computational Linguistics, Proceedings of the
Conference, pages 135?142, Vancouver, British
Columbia, Canada, June.
C.D. Manning and B. Carpenter. 2000. Proba-
bilistic parsing using left corner language mod-
els. In H. Bunt and A. Nijholt, editors, Ad-
vances in Probabilistic and other Parsing Tech-
nologies, chapter 6, pages 105?124. Kluwer Aca-
demic Publishers.
M.-J. Nederhof and G. Satta. 2003. Probabilis-
tic parsing as intersection. In 8th International
Workshop on Parsing Technologies, pages 137?
148, LORIA, Nancy, France, April.
M.-J. Nederhof. 1994. An optimal tabular parsing
algorithm. In 32nd Annual Meeting of the Associ-
ation for Computational Linguistics, Proceedings
of the Conference, pages 117?124, Las Cruces,
New Mexico, USA, June.
A. Nijholt. 1980. Context-Free Grammars: Cov-
ers, Normal Forms, and Parsing, volume 93 of
Lecture Notes in Computer Science. Springer-
Verlag.
P.W. Purdom, Jr. and C.A. Brown. 1981. Pars-
ing extended LR(k) grammars. Acta Informatica,
15:115?127.
B. Roark and M. Johnson. 1999. Efficient proba-
bilistic top-down and left-corner parsing. In 37th
Annual Meeting of the Association for Compu-
tational Linguistics, Proceedings of the Confer-
ence, pages 421?428, Maryland, USA, June.
D.J. Rosenkrantz and P.M. Lewis II. 1970. Deter-
ministic left corner parsing. In IEEE Conference
Record of the 11th Annual Symposium on Switch-
ing and Automata Theory, pages 139?152.
J.-A. Sa?nchez and J.-M. Bened??. 1997. Consis-
tency of stochastic context-free grammars from
probabilistic estimation based on growth trans-
formations. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 19(9):1052?1055,
September.
E.S. Santos. 1972. Probabilistic grammars and au-
tomata. Information and Control, 21:27?47.
S. Sippu and E. Soisalon-Soininen. 1990. Parsing
Theory, Vol. II: LR(k) and LL(k) Parsing, vol-
ume 20 of EATCS Monographs on Theoretical
Computer Science. Springer-Verlag.
E. Soisalon-Soininen and E. Ukkonen. 1979. A
method for transforming grammars into LL(k)
form. Acta Informatica, 12:339?369.
V. Sornlertlamvanich, K. Inui, H. Tanaka, T. Toku-
naga, and T. Takezawa. 1999. Empirical sup-
port for new probabilistic generalized LR pars-
ing. Journal of Natural Language Processing,
6(3):3?22.
A. Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):167?201.
F. Tendeau. 1995. Stochastic parse-tree recognition
by a pushdown automaton. In Fourth Interna-
tional Workshop on Parsing Technologies, pages
234?249, Prague and Karlovy Vary, Czech Re-
public, September.
F. Tendeau. 1997. Analyse syntaxique et
se?mantique avec e?valuation d?attributs dans
un demi-anneau. Ph.D. thesis, University of
Orle?ans.
J.H. Wright and E.N. Wrigley. 1991. GLR pars-
ing with probability. In M. Tomita, editor, Gen-
eralized LR Parsing, chapter 8, pages 113?128.
Kluwer Academic Publishers.
An alternative method of training probabilistic LR parsers
Mark-Jan Nederhof
Faculty of Arts
University of Groningen
P.O. Box 716
NL-9700 AS Groningen
The Netherlands
markjan@let.rug.nl
Giorgio Satta
Dept. of Information Engineering
University of Padua
via Gradenigo, 6/A
I-35131 Padova
Italy
satta@dei.unipd.it
Abstract
We discuss existing approaches to train LR parsers,
which have been used for statistical resolution of
structural ambiguity. These approaches are non-
optimal, in the sense that a collection of probability
distributions cannot be obtained. In particular, some
probability distributions expressible in terms of a
context-free grammar cannot be expressed in terms
of the LR parser constructed from that grammar,
under the restrictions of the existing approaches to
training of LR parsers. We present an alternative
way of training that is provably optimal, and that al-
lows all probability distributions expressible in the
context-free grammar to be carried over to the LR
parser. We also demonstrate empirically that this
kind of training can be effectively applied on a large
treebank.
1 Introduction
The LR parsing strategy was originally devised
for programming languages (Sippu and Soisalon-
Soininen, 1990), but has been used in a wide range
of other areas as well, such as for natural language
processing (Lavie and Tomita, 1993; Briscoe and
Carroll, 1993; Ruland, 2000). The main difference
between the application to programming languages
and the application to natural languages is that in
the latter case the parsers should be nondetermin-
istic, in order to deal with ambiguous context-free
grammars (CFGs). Nondeterminism can be han-
dled in a number of ways, but the most efficient
is tabulation, which allows processing in polyno-
mial time. Tabular LR parsing is known from the
work by (Tomita, 1986), but can also be achieved
by the generic tabulation technique due to (Lang,
1974; Billot and Lang, 1989), which assumes an in-
put pushdown transducer (PDT). In this context, the
LR parsing strategy can be seen as a particular map-
ping from context-free grammars to PDTs.
The acronym ?LR? stands for ?Left-to-right pro-
cessing of the input, producing a Right-most deriva-
tion (in reverse)?. When we construct a PDTA from
a CFG G by the LR parsing strategy and apply it on
an input sentence, then the set of output strings ofA
represents the set of all right-most derivations that G
allows for that sentence. Such an output string enu-
merates the rules (or labels that identify the rules
uniquely) that occur in the corresponding right-most
derivation, in reversed order.
If LR parsers do not use lookahead to decide be-
tween alternative transitions, they are called LR(0)
parsers. More generally, if LR parsers look ahead k
symbols, they are called LR(k) parsers; some sim-
plified LR parsing models that use lookahead are
called SLR(k) and LALR(k) parsing (Sippu and
Soisalon-Soininen, 1990). In order to simplify the
discussion, we abstain from using lookahead in this
article, and ?LR parsing? can further be read as
?LR(0) parsing?. We would like to point out how-
ever that our observations carry over to LR parsing
with lookahead.
The theory of probabilistic pushdown automata
(Santos, 1972) can be easily applied to LR parsing.
A probability is then assigned to each transition, by
a function that we will call the probability function
pA, and the probability of an accepting computa-
tion of A is the product of the probabilities of the
applied transitions. As each accepting computation
produces a right-most derivation as output string, a
probabilistic LR parser defines a probability distri-
bution on the set of parses, and thereby also a prob-
ability distribution on the set of sentences generated
by grammar G. Disambiguation of an ambiguous
sentence can be achieved on the basis of a compari-
son between the probabilities assigned to the respec-
tive parses by the probabilistic LR model.
The probability function can be obtained on the
basis of a treebank, as proposed by (Briscoe and
Carroll, 1993) (see also (Su et al, 1991)). The
model by (Briscoe and Carroll, 1993) however in-
corporated a mistake involving lookahead, which
was corrected by (Inui et al, 2000). As we will not
discuss lookahead here, this matter does not play a
significant role in the current study. Noteworthy is
that (Sornlertlamvanich et al, 1999) showed empir-
ically that an LR parser may be more accurate than
the original CFG, if both are trained on the basis
of the same treebank. In other words, the resulting
probability function pA on transitions of the PDT
allows better disambiguation than the correspond-
ing function pG on rules of the original grammar.
A plausible explanation of this is that stack sym-
bols of an LR parser encode some amount of left
context, i.e. information on rules applied earlier, so
that the probability function on transitions may en-
code dependencies between rules that cannot be en-
coded in terms of the original CFG extended with
rule probabilities. The explicit use of left con-
text in probabilistic context-free models was inves-
tigated by e.g. (Chitrao and Grishman, 1990; John-
son, 1998), who also demonstrated that this may
significantly improve accuracy. Note that the prob-
ability distributions of language may be beyond the
reach of a given context-free grammar, as pointed
out by e.g. (Collins, 2001). Therefore, the use of left
context, and the resulting increase in the number of
parameters of the model, may narrow the gap be-
tween the given grammar and ill-understood mech-
anisms underlying actual language.
One important assumption that is made by
(Briscoe and Carroll, 1993) and (Inui et al, 2000)
is that trained probabilistic LR parsers should be
proper, i.e. if several transitions are applicable for
a given stack, then the sum of probabilities as-
signed to those transitions by probability function
pA should be 1. This assumption may be moti-
vated by pragmatic considerations, as such a proper
model is easy to train by relative frequency estima-
tion: count the number of times a transition is ap-
plied with respect to a treebank, and divide it by
the number of times the relevant stack symbol (or
pair of stack symbols) occurs at the top of the stack.
Let us call the resulting probability function prfe .
This function is provably optimal in the sense that
the likelihood it assigns to the training corpus is
maximal among all probability functions pA that are
proper in the above sense.
However, properness restricts the space of prob-
ability distributions that a PDT allows. This means
that a (consistent) probability function pA may ex-
ist that is not proper and that assigns a higher like-
lihood to the training corpus than prfe does. (By
?consistent? we mean that the probabilities of all
strings that are accepted sum to 1.) It may even
be the case that a (proper and consistent) probabil-
ity function pG on the rules of the input grammar G
exists that assigns a higher likelihood to the corpus
than prfe , and therefore it is not guaranteed that LR
parsers allow better probability estimates than the
CFGs from which they were constructed, if we con-
strain probability functions pA to be proper. In this
respect, LR parsing differs from at least one other
well-known parsing strategy, viz. left-corner pars-
ing. See (Nederhof and Satta, 2004) for a discus-
sion of a property that is shared by left-corner pars-
ing but not by LR parsing, and which explains the
above difference.
As main contribution of this paper we establish
that this restriction on expressible probability dis-
tributions can be dispensed with, without losing the
ability to perform training by relative frequency es-
timation. What comes in place of properness is
reverse-properness, which can be seen as proper-
ness of the reversed pushdown automaton that pro-
cesses input from right to left instead of from left to
right, interpreting the transitions of A backwards.
As we will show, reverse-properness does not re-
strict the space of probability distributions express-
ible by an LR automaton. More precisely, assume
some probability distribution on the set of deriva-
tions is specified by a probability function pA on
transitions of PDT A that realizes the LR strat-
egy for a given grammar G. Then the same prob-
ability distribution can be specified by an alterna-
tive such function p?A that is reverse-proper. In ad-
dition, for each probability distribution on deriva-
tions expressible by a probability function pG for G,
there is a reverse-proper probability function pA for
A that expresses the same probability distribution.
Thereby we ensure that LR parsers become at least
as powerful as the original CFGs in terms of allow-
able probability distributions.
This article is organized as follows. In Sec-
tion 2 we outline our formalization of LR pars-
ing as a construction of PDTs from CFGs, making
some superficial changes with respect to standard
formulations. Properness and reverse-properness
are discussed in Section 3, where we will show
that reverse-properness does not restrict the space
of probability distributions. Section 4 reports on ex-
periments, and Section 5 concludes this article.
2 LR parsing
As LR parsing has been extensively treated in exist-
ing literature, we merely recapitulate the main defi-
nitions here. For more explanation, the reader is re-
ferred to standard literature such as (Harrison, 1978;
Sippu and Soisalon-Soininen, 1990).
An LR parser is constructed on the basis of a CFG
that is augmented with an additional rule S? ?` S,
where S is the former start symbol, and the new
nonterminal S? becomes the start symbol of the
augmented grammar. The new terminal ` acts as
an imaginary start-of-sentence marker. We denote
the set of terminals by ? and the set of nontermi-
nals by N . We assume each rule has a unique label
r.
As explained before, we construct LR parsers as
pushdown transducers. The main stack symbols
of these automata are sets of dotted rules, which
consist of rules from the augmented grammar with
a distinguished position in the right-hand side in-
dicated by a dot ???. The initial stack symbol is
pinit = {S? ? ` ? S}.
We define the closure of a set p of dotted rules as
the smallest set closure(p) such that:
1. p ? closure(p); and
2. for (B ? ? ? A?) ? closure(p) and A ?
? a rule in the grammar, also (A ? ? ?) ?
closure(p).
We define the operation goto on a set p of dotted
rules and a grammar symbol X ? ? ?N as:
goto(p,X) = {A? ?X ? ? |
(A? ? ? X?) ? closure(p)}
The set of LR states is the smallest set such that:
1. pinit is an LR state; and
2. if p is an LR state and goto(p,X) = q 6= ?, for
some X ? ? ?N , then q is an LR state.
We will assume that PDTs consist of three types
of transitions, of the form P a,b7? P Q (a push tran-
sition), of the form P a,b7? Q (a swap transition), and
of the form P Q a,b7? R (a pop transition). Here P , Q
and R are stack symbols, a is one input terminal or
is the empty string ?, and b is one output terminal or
is the empty string ?. In our notation, stacks grow
from left to right, so that P a,b7? P Q means that Q is
pushed on top of P . We do not have internal states
next to stack symbols.
For the PDT that implements the LR strategy, the
stack symbols are the LR states, plus symbols of the
form [p;X], where p is an LR state andX is a gram-
mar symbol, and symbols of the form (p,A,m),
where p is an LR state, A is the left-hand side of
some rule, and m is the length of some prefix of the
right-hand side of that rule. More explanation on
these additional stack symbols will be given below.
The stack symbols and transitions are simultane-
ously defined in Figure 1. The final stack symbol
is pfinal = (pinit , S?, 0). This means that an input
a1 ? ? ? an is accepted if and only if it is entirely read
by a sequence of transitions that take the stack con-
sisting only of pinit to the stack consisting only of
pfinal . The computed output consists of the string of
terminals b1 ? ? ? bn? from the output components of
the applied transitions. For the PDTs that we will
use, this output string will consist of a sequence of
rule labels expressing a right-most derivation of the
input. On the basis of the original grammar, the cor-
responding parse tree can be constructed from such
an output string.
There are a few superficial differences with LR
parsing as it is commonly found in the literature.
The most obvious difference is that we divide re-
ductions into ?binary? steps. The main reason is that
this allows tabular interpretation with a time com-
plexity cubic in the length of the input. Otherwise,
the time complexity would be O(nm+1), where m
is the length of the longest right-hand side of a rule
in the CFG. This observation was made before by
(Kipps, 1991), who proposed a solution similar to
ours, albeit formulated differently. See also a related
formulation of tabular LR parsing by (Nederhof and
Satta, 1996).
To be more specific, instead of one step of the
PDT taking stack:
?p0p1 ? ? ? pm
immediately to stack:
?p0q
where (A ? X1 ? ? ?Xm ?) ? pm, ? is a string
of stack symbols and goto(p0, A) = q, we have
a number of smaller steps leading to a series of
stacks:
?p0p1 ? ? ? pm?1pm
?p0p1 ? ? ? pm?1(A,m?1)
?p0p1 ? ? ? (A,m?2)
.
.
.
?p0(A, 0)
?p0q
There are two additional differences. First, we
want to avoid steps of the form:
?p0(A, 0)
?p0q
by transitions p0 (A, 0)
?,?
7? p0 q, as such transitions
complicate the generic definition of ?properness?
for PDTs, to be discussed in the following section.
For this reason, we use stack symbols of the form
[p;X] next to p, and split up p0 (A, 0)
?,?
7? p0 q into
pop [p0;X0] (A, 0)
?,?
7? [p0;A] and push [p0;A]
?,?
7?
[p0;A] q. This is a harmless modification, which in-
creases the number of steps in any computation by
at most a factor 2.
Secondly, we use stack symbols of the form
(p,A,m) instead of (A,m). This concerns the con-
ditions of reverse-properness to be discussed in the
? For LR state p and a ? ? such that goto(p, a) 6= ?:
p
a,?
7? [p; a] (1)
? For LR state p and (A? ?) ? p, where A? ? has label r:
p
?,r
7? [p;A] (2)
? For LR state p and (A? ? ?) ? p, where |?| = m > 0 and A? ? has label r:
p
?,r
7? (p,A,m? 1) (3)
? For LR state p and (A? ? ? X?) ? p, where |?| = m > 0, such that goto(p,X) = q 6= ?:
[p;X] (q, A,m)
?,?
7? (p,A,m? 1) (4)
? For LR state p and (A? ? X?) ? p, such that goto(p,X) = q 6= ?:
[p;X] (q, A, 0)
?,?
7? [p;A] (5)
? For LR state p and X ? ? ?N such that goto(p,X) = q 6= ?:
[p;X]
?,?
7? [p;X] q (6)
Figure 1: The transitions of a PDT implementing LR(0) parsing.
following section. By this condition, we consider
LR parsing as being performed from right to left, so
backwards with regard to the normal processing or-
der. If we were to omit the first components p from
stack symbols (p,A,m), we may obtain ?dead ends?
in the computation. We know that such dead ends
make a (reverse-)proper PDT inconsistent, as proba-
bility mass lost in dead ends causes the sum of prob-
abilities of all computations to be strictly smaller
than 1. (See also (Nederhof and Satta, 2004).) It
is interesting to note that the addition of the compo-
nents p to stack symbols (p,A,m) does not increase
the number of transitions, and the nature of LR pars-
ing in the normal processing order from left to right
is preserved.
With all these changes together, reductions
are implemented by transitions resulting in the
following sequence of stacks:
??[p0;X0][p1;X1] ? ? ? [pm?1;Xm?1]pm
??[p0;X0][p1;X1] ? ? ? [pm?1;Xm?1](pm, A,m?1)
??[p0;X0][p1;X1] ? ? ? (pm?1, A,m?2)
.
.
.
??[p0;X0](p1, A, 0)
??[p0;A]
??[p0;A]q
Please note that transitions of the form
[p;X] (q, A,m)
?,?
7? (p,A,m? 1) may corre-
spond to several dotted rules (A ? ? ? X?) ? p,
with different ? of length m and different ?. If we
were to multiply such transitions for different ? and
?, the PDT would become prohibitively large.
3 Properness and reverse-properness
If a PDT is regarded to process input from left to
right, starting with a stack consisting only of pinit ,
and ending in a stack consisting only of pfinal , then
it seems reasonable to cast this process into a prob-
abilistic framework in such a way that the sum of
probabilities of all choices that are possible at any
given moment is 1. This is similar to how the notion
of ?properness? is defined for probabilistic context-
free grammars (PCFGs); we say a PCFG is proper if
for each nonterminalA, the probabilities of all rules
with left-hand side A sum to 1.
Properness for PCFGs does not restrict the space
of probability distributions on the set of parse trees.
In other words, if a probability distribution can be
defined by attaching probabilities to rules, then we
may reassign the probabilities such that that PCFG
becomes proper, while preserving the probability
distribution. This even holds if the input grammar
is non-tight, meaning that probability mass is lost
in ?infinite derivations? (Sa?nchez and Bened??, 1997;
Chi and Geman, 1998; Chi, 1999; Nederhof and
Satta, 2003).
Although CFGs and PDTs are weakly equiva-
lent, they behave very differently when they are ex-
tended with probabilities. In particular, there seems
to be no notion similar to PCFG properness that
can be imposed on all types of PDTs without los-
ing generality. Below we will discuss two con-
straints, which we will call properness and reverse-
properness. Neither of these is suitable for all types
of PDTs, but as we will show, the second is more
suitable for probabilistic LR parsing than the first.
This is surprising, as only properness has been de-
scribed in existing literature on probabilistic PDTs
(PPDTs). In particular, all existing approaches to
probabilistic LR parsing have assumed properness
rather than anything related to reverse-properness.
For properness we have to assume that for each
stack symbol P , we either have one or more tran-
sitions of the form P a,b7? P Q or P a,b7? Q, or one
or more transitions of the form Q P a,b7? R, but no
combination thereof. In the first case, properness
demands that the sum of probabilities of all transi-
tions P a,b7? P Q and P a,b7? Q is 1, and in the second
case properness demands that the sum of probabili-
ties of all transitions Q P a,b7? R is 1 for each Q.
Note that our assumption above is without loss
of generality, as we may introduce swap transitions
P
?,?
7? P1 and P
?,?
7? P2, where P1 and P2 are new
stack symbols, and replace transitions P a,b7? P Q
and P a,b7? Q by P1
a,b
7? P1 Q and P1
a,b
7? Q, and
replace transitions Q P a,b7? R by Q P2
a,b
7? R.
The notion of properness underlies the normal
training process for PDTs, as follows. We assume
a corpus of PDT computations. In these computa-
tions, we count the number of occurrences for each
transition. For each P we sum the total number of
all occurrences of transitions P a,b7? P Q or P a,b7? Q.
The probability of, say, a transition P a,b7? P Q is
now estimated by dividing the number of occur-
rences thereof in the corpus by the above total num-
ber of occurrences of transitions with P in the left-
hand side. Similarly, for each pair (Q,P ) we sum
the total number of occurrences of all transitions of
the formQ P a,b7? R, and thereby estimate the proba-
bility of a particular transitionQ P a,b7? R by relative
frequency estimation. The resulting PPDT is proper.
It has been shown that imposing properness is
without loss of generality in the case of PDTs
constructed by a wide range of parsing strategies,
among which are top-down parsing and left-corner
parsing. This does not hold for PDTs constructed by
the LR parsing strategy however, and in fact, proper-
ness for such automata may reduce the expressive
power in terms of available probability distributions
to strictly less than that offered by the original CFG.
This was formally proven by (Nederhof and Satta,
2004), after (Ng and Tomita, 1991) and (Wright and
Wrigley, 1991) had already suggested that creating
a probabilistic LR parser that is equivalent to an in-
put PCFG is difficult in general. The same difficulty
for ELR parsing was suggested by (Tendeau, 1997).
For this reason, we investigate a practical alter-
native, viz. reverse-properness. Now we have to as-
sume that for each stack symbol R, we either have
one or more transitions of the form P a,b7? R or
Q P
a,b
7? R, or one or more transitions of the form
P
a,b
7? P R, but no combination thereof. In the first
case, reverse-properness demands that the sum of
probabilities of all transitions P a,b7? R or Q P a,b7? R
is 1, and in the second case reverse-properness de-
mands that the sum of probabilities of transitions
P
a,b
7? P R is 1 for each P . Again, our assumption
above is without loss of generality.
In order to apply relative frequency estimation,
we now sum the total number of occurrences of tran-
sitions P a,b7? R or Q P a,b7? R for each R, and we
sum the total number of occurrences of transitions
P
a,b
7? P R for each pair (P,R).
We now prove that reverse-properness does not
restrict the space of probability distributions, by
means of the construction of a ?cover? grammar
from an input CFG, as reported in Figure 2. This
cover CFG has almost the same structure as the PDT
resulting from Figure 1. Rules and transitions al-
most stand in a one-to-one relation. The only note-
worthy difference is between transitions of type (6)
and rules of type (12). The right-hand sides of those
rules can be ? because the corresponding transitions
are deterministic if seen from right to left. Now it
becomes clear why we needed the components p in
stack symbols of the form (p,A,m). Without it, one
could obtain an LR state q that does not match the
underlying [p;X] in a reversed computation.
We may assume without loss of generality that
rules of type (12) are assigned probability 1, as a
probability other than 1 could be moved to corre-
sponding rules of types (10) or (11) where state
q was introduced. In the same way, we may as-
sume that transitions of type (6) are assigned prob-
ability 1. After making these assumptions, we ob-
tain a bijection between probability functions pA for
the PDT and probability functions pG for the cover
CFG. As was shown by e.g. (Chi, 1999) and (Neder-
hof and Satta, 2003), properness for CFGs does not
restrict the space of probability distributions, and
thereby the same holds for reverse-properness for
PDTs that implement the LR parsing strategy.
It is now also clear that a reverse-proper LR
parser can describe any probability distribution that
the original CFG can. The proof is as follows.
Given a probability function pG for the input CFG,
we define a probability function pA for the LR
parser, by letting transitions of types (2) and (3)
? For LR state p and a ? ? such that goto(p, a) 6= ?:
[p; a]? p (7)
? For LR state p and (A? ?) ? p, where A? ? has label r:
[p;A]? p r (8)
? For LR state p and (A? ? ?) ? p, where |?| = m > 0 and A? ? has label r:
(p,A,m? 1)? p r (9)
? For LR state p and (A? ? ? X?) ? p, where |?| = m > 0, such that goto(p,X) = q 6= ?:
(p,A,m? 1)? [p;X] (q, A,m) (10)
? For LR state p and (A? ? X?) ? p, such that goto(p,X) = q 6= ?:
[p;A]? [p;X] (q, A, 0) (11)
? For LR state q:
q ? ? (12)
Figure 2: A grammar that describes the set of computations of the LR(0) parser. Start symbol is pfinal =
(pinit , S?, 0). Terminals are rule labels. Generated language consists of right-most derivations in reverse.
have probability pG(r), and letting all other transi-
tions have probability 1. This gives us the required
probability distribution in terms of a PPDT that is
not reverse-proper in general. This PPDT can now
be recast into reverse-proper form, as proven by the
above.
4 Experiments
We have implemented both the traditional training
method for LR parsing and the novel one, and have
compared their performance, with two concrete ob-
jectives:
1. We show that the number of free parameters
is significantly larger with the new training
method. (The number of free parameters is
the number of probabilities of transitions that
can be freely chosen within the constraints of
properness or reverse-properness.)
2. The larger number of free parameters does not
make the problem of sparse data any worse,
and precision and recall are at least compara-
ble to, if not better than, what we would obtain
with the established method.
The experiments were performed on the Wall
Street Journal (WSJ) corpus, from the Penn Tree-
bank, version II. Training was done on sections 02-
21, i.e., first a context-free grammar was derived
from the ?stubs? of the combined trees, taking parts
of speech as leaves of the trees, omitting all af-
fixes from the nonterminal names, and removing ?-
generating subtrees. Such preprocessing of the WSJ
corpus is consistent with earlier attempts to derive
CFGs from that corpus, as e.g. by (Johnson, 1998).
The obtained CFG has 10,035 rules. The dimen-
sions of the LR parser constructed from this gram-
mar are given in Table 1.
The PDT was then trained on the trees from the
same sections 02-21, to determine the number of
times that transitions are used. At first sight it is not
clear how to determine this on the basis of the tree-
bank, as the structure of LR parsers is very differ-
ent from the structure of the grammars from which
they are constructed. The solution is to construct a
second PDT from the PDT to be trained, replacing
each transition ? a,b7? ? with label r by transition
?
b,r
7? ?. By this second PDT we parse the tree-
bank, encoded as a series of right-most derivations
in reverse.1 For each input string, there is exactly
one parse, of which the output is the list of used
transitions. The same method can be used for other
parsing strategies as well, such as left-corner pars-
ing, replacing right-most derivations by a suitable
alternative representation of parse trees.
By the counts of occurrences of transitions, we
may then perform maximum likelihood estimation
to obtain probabilities for transitions. This can
be done under the constraints of properness or of
reverse-properness, as explained in the previous
section. We have not applied any form of smooth-
1We have observed an enormous gain in computational ef-
ficiency when we also incorporate the ?shifts? next to ?reduc-
tions? in these right-most derivations, as this eliminates a con-
siderable amount of nondeterminism.
total # transitions 8,340,315
# push transitions 753,224
# swap transitions 589,811
# pop transitions 6,997,280
Table 1: Dimensions of PDT implementing LR
strategy for CFG derived from WSJ, sect. 02-21.
proper rev.-prop.
# free parameters 577,650 6,589,716
# non-zero probabilities 137,134 137,134
labelled precision 0.772 0.777
labelled recall 0.747 0.749
Table 2: The two methods of training, based on
properness and reverse-properness.
ing or back-off, as this could obscure properties in-
herent in the difference between the two discussed
training methods. (Back-off for probabilistic LR
parsing has been proposed by (Ruland, 2000).) All
transitions that were not seen during training were
given probability 0.
The results are outlined in Table 2. Note that the
number of free parameters in the case of reverse-
properness is much larger than in the case of normal
properness. Despite of this, the number of transi-
tions that actually receive non-zero probabilities is
(predictably) identical in both cases, viz. 137,134.
However, the potential for fine-grained probability
estimates and for smoothing and parameter-tying
techniques is clearly greater in the case of reverse-
properness.
That in both cases the number of non-zero prob-
abilities is lower than the total number of parame-
ters can be explained as follows. First, the treebank
contains many rules that occur a small number of
times. Secondly, the LR automaton is much larger
than the CFG; in general, the size of an LR automa-
ton is bounded by a function that is exponential in
the size of the input CFG. Therefore, if we use the
same treebank to estimate the probability function,
then many transitions are never visited and obtain a
zero probability.
We have applied the two trained LR automata
on section 22 of the WSJ corpus, measuring la-
belled precision and recall, as done by e.g. (John-
son, 1998).2 We observe that in the case of reverse-
properness, precision and recall are slightly better.
2We excluded all sentences with more than 30 words how-
ever, as some required prohibitive amounts of memory. Only
one of the remaining 1441 sentences was not accepted by the
parser.
The most important conclusion that can be drawn
from this is that the substantially larger space of
obtainable probability distributions offered by the
reverse-properness method does not come at the ex-
pense of a degradation of accuracy for large gram-
mars such as those derived from the WSJ. For com-
parison, with a standard PCFG we obtain labelled
precision and recall of 0.725 and 0.670, respec-
tively.3
We would like to stress that our experiments
did not have as main objective the improvement of
state-of-the-art parsers, which can certainly not be
done without much additional fine-tuning and the
incorporation of some form of lexicalization. Our
main objectives concerned the relation between our
newly proposed training method for LR parsers and
the traditional one.
5 Conclusions
We have presented a novel way of assigning proba-
bilities to transitions of an LR automaton. Theoreti-
cal analysis and empirical data reveal the following.
? The efficiency of LR parsing remains unaf-
fected. Although a right-to-left order of read-
ing input underlies the novel training method,
we may continue to apply the parser from left
to right, and benefit from the favourable com-
putational properties of LR parsing.
? The available space of probability distributions
is significantly larger than in the case of the
methods published before. In terms of the
number of free parameters, the difference that
we found empirically exceeds one order of
magnitude. By the same criteria, we can now
guarantee that LR parsers are at least as pow-
erful as the CFGs from which they are con-
structed.
? Despite the larger number of free parameters,
no increase of sparse data problems was ob-
served, and in fact there was a small increase
in accuracy.
Acknowledgements
Helpful comments from John Carroll and anony-
mous reviewers are gratefully acknowledged. The
first author is supported by the PIONIER Project
Algorithms for Linguistic Processing, funded by
NWO (Dutch Organization for Scientific Research).
The second author is partially supported by MIUR
under project PRIN No. 2003091149 005.
3In this case, all 1441 sentences were accepted.
References
S. Billot and B. Lang. 1989. The structure of shared
forests in ambiguous parsing. In 27th Annual
Meeting of the Association for Computational
Linguistics, pages 143?151, Vancouver, British
Columbia, Canada, June.
T. Briscoe and J. Carroll. 1993. Generalized prob-
abilistic LR parsing of natural language (cor-
pora) with unification-based grammars. Compu-
tational Linguistics, 19(1):25?59.
Z. Chi and S. Geman. 1998. Estimation of prob-
abilistic context-free grammars. Computational
Linguistics, 24(2):299?305.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguis-
tics, 25(1):131?160.
M.V. Chitrao and R. Grishman. 1990. Statistical
parsing of messages. In Speech and Natural Lan-
guage, Proceedings, pages 263?266, Hidden Val-
ley, Pennsylvania, June.
M. Collins. 2001. Parameter estimation for sta-
tistical parsing models: Theory and practice of
distribution-free methods. In Proceedings of the
Seventh International Workshop on Parsing Tech-
nologies, Beijing, China, October.
M.A. Harrison. 1978. Introduction to Formal Lan-
guage Theory. Addison-Wesley.
K. Inui, V. Sornlertlamvanich, H. Tanaka, and
T. Tokunaga. 2000. Probabilistic GLR parsing.
In H. Bunt and A. Nijholt, editors, Advances
in Probabilistic and other Parsing Technologies,
chapter 5, pages 85?104. Kluwer Academic Pub-
lishers.
M. Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
J.R. Kipps. 1991. GLR parsing in time O(n3). In
M. Tomita, editor, Generalized LR Parsing, chap-
ter 4, pages 43?59. Kluwer Academic Publishers.
B. Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In Automata,
Languages and Programming, 2nd Colloquium,
volume 14 of Lecture Notes in Computer Science,
pages 255?269, Saarbru?cken. Springer-Verlag.
A. Lavie and M. Tomita. 1993. GLR? ? an efficient
noise-skipping parsing algorithm for context free
grammars. In Third International Workshop on
Parsing Technologies, pages 123?134, Tilburg
(The Netherlands) and Durbuy (Belgium), Au-
gust.
M.-J. Nederhof and G. Satta. 1996. Efficient tab-
ular LR parsing. In 34th Annual Meeting of the
Association for Computational Linguistics, pages
239?246, Santa Cruz, California, USA, June.
M.-J. Nederhof and G. Satta. 2003. Probabilis-
tic parsing as intersection. In 8th International
Workshop on Parsing Technologies, pages 137?
148, LORIA, Nancy, France, April.
M.-J. Nederhof and G. Satta. 2004. Probabilis-
tic parsing strategies. In 42nd Annual Meeting
of the Association for Computational Linguistics,
Barcelona, Spain, July.
S.-K. Ng and M. Tomita. 1991. Probabilistic LR
parsing for general context-free grammars. In
Proc. of the Second International Workshop on
Parsing Technologies, pages 154?163, Cancun,
Mexico, February.
T. Ruland. 2000. A context-sensitive model for
probabilistic LR parsing of spoken language
with transformation-based postprocessing. In
The 18th International Conference on Compu-
tational Linguistics, volume 2, pages 677?683,
Saarbru?cken, Germany, July?August.
J.-A. Sa?nchez and J.-M. Bened??. 1997. Consis-
tency of stochastic context-free grammars from
probabilistic estimation based on growth trans-
formations. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 19(9):1052?1055,
September.
E.S. Santos. 1972. Probabilistic grammars and au-
tomata. Information and Control, 21:27?47.
S. Sippu and E. Soisalon-Soininen. 1990. Parsing
Theory, Vol. II: LR(k) and LL(k) Parsing, vol-
ume 20 of EATCS Monographs on Theoretical
Computer Science. Springer-Verlag.
V. Sornlertlamvanich, K. Inui, H. Tanaka, T. Toku-
naga, and T. Takezawa. 1999. Empirical sup-
port for new probabilistic generalized LR pars-
ing. Journal of Natural Language Processing,
6(3):3?22.
K.-Y. Su, J.-N. Wang, M.-H. Su, and J.-S. Chang.
1991. GLR parsing with scoring. In M. Tomita,
editor, Generalized LR Parsing, chapter 7, pages
93?112. Kluwer Academic Publishers.
F. Tendeau. 1997. Analyse syntaxique et
se?mantique avec e?valuation d?attributs dans
un demi-anneau. Ph.D. thesis, University of
Orle?ans.
M. Tomita. 1986. Efficient Parsing for Natural
Language. Kluwer Academic Publishers.
J.H. Wright and E.N. Wrigley. 1991. GLR pars-
ing with probability. In M. Tomita, editor, Gen-
eralized LR Parsing, chapter 8, pages 113?128.
Kluwer Academic Publishers.
Generalized Multitext Grammars
I. Dan Melamed
Computer Science Department
New York University
715 Broadway, 7th Floor
New York, NY, 10003, USA
 
lastname  @cs.nyu.edu
Giorgio Satta
Dept. of Information Eng?g
University of Padua
via Gradenigo 6/A
I-35131 Padova, Italy
 
lastname  @dei.unipd.it
Benjamin Wellington
Computer Science Department
New York University
715 Broadway, 7th Floor
New York, NY, 10003, USA
 
lastname  @cs.nyu.edu
Abstract
Generalized Multitext Grammar (GMTG) is a syn-
chronous grammar formalism that is weakly equiv-
alent to Linear Context-Free Rewriting Systems
(LCFRS), but retains much of the notational and in-
tuitive simplicity of Context-Free Grammar (CFG).
GMTG allows both synchronous and independent
rewriting. Such flexibility facilitates more perspic-
uous modeling of parallel text than what is possible
with other synchronous formalisms. This paper in-
vestigates the generative capacity of GMTG, proves
that each component grammar of a GMTG retains
its generative power, and proposes a generalization
of Chomsky Normal Form, which is necessary for
synchronous CKY-style parsing.
1 Introduction
Synchronous grammars have been proposed for
the formal description of parallel texts representing
translations of the same document. As shown by
Melamed (2003), a plausible model of parallel text
must be able to express discontinuous constituents.
Since linguistic expressions can vanish in transla-
tion, a good model must be able to express inde-
pendent (in addition to synchronous) rewriting. In-
version Transduction Grammar (ITG) (Wu, 1997)
and Syntax-Directed Translation Schema (SDTS)
(Aho and Ullman, 1969) lack both of these prop-
erties. Synchronous Tree Adjoining Grammar
(STAG) (Shieber, 1994) lacks the latter and allows
only limited discontinuities in each tree.
Generalized Multitext Grammar (GMTG) offers
a way to synchronize Mildly Context-Sensitive
Grammar (MCSG), while satisfying both of the
above criteria. The move to MCSG is motivated
by our desire to more perspicuously account for
certain syntactic phenomena that cannot be easily
captured by context-free grammars, such as clitic
climbing, extraposition, and other types of long-
distance movement (Becker et al, 1991). On the
other hand, MCSG still observes some restrictions
that make the set of languages it generates less ex-
pensive to analyze than the languages generated by
(properly) context-sensitive formalisms.
More technically, our proposal starts from Mul-
titext Grammar (MTG), a formalism for synchro-
nizing context-free grammars recently proposed by
Melamed (2003). In MTG, synchronous rewriting
is implemented by means of an indexing relation
that is maintained over occurrences of nonterminals
in a sentential form, using essentially the same ma-
chinery as SDTS. Unlike SDTS, MTG can extend
the dimensionality of the translation relation be-
yond two, and it can implement independent rewrit-
ing by means of partial deletion of syntactic struc-
tures. Our proposal generalizes MTG by moving
from component grammars that generate context-
free languages to component grammars whose gen-
erative power is equivalent to Linear Context-Free
Rewriting Systems (LCFRS), a formalism for de-
scribing a class of MCSGs. The generalization is
achieved by allowing context-free productions to
rewrite tuples of strings, rather than single strings.
Thus, we retain the intuitive top-down definition of
synchronous derivation original in SDTS and MTG
but not found in LCFRS, while extending the gen-
erative power to linear context-free rewriting lan-
guages. In this respect, GMTG has also been in-
spired by the class of Local Unordered Scattered
Context Grammars (Rambow and Satta, 1999). A
syntactically very different synchronous formalism
involving LCFRS has been presented by Bertsch
and Nederhof (2001).
This paper begins with an informal description of
GMTG. It continues with an investigation of this
formalism?s generative capacity. Next, we prove
that in GMTG each component grammar retains its
generative power, a requirement for synchronous
formalisms that Rambow and Satta (1996) called
the ?weak language preservation property.? Lastly,
we propose a synchronous generalization of Chom-
sky Normal Form, which lays the groundwork for
synchronous parsing under GMTG using a CKY-
style algorithm (Younger, 1967; Melamed, 2004).
2 Informal Description and Comparisons
GMTG is a generalization of MTG, which is itself
a generalization of CFG to the synchronous case.
Here we present MTG in a new notation that shows
the relation to CFG more clearly. For example, the
following MTG productions can generate the multi-
text [(I fed the cat), (ya kota kormil)]:1
  (S)  (S)    PN  VP 
	  PN  VP 	 (1)
 
PN 	

PN 	
 
I 	

ya 	 (2)
 
VP 	

VP 	
 
V  NP  	

NP  V  	 (3)
 
V 	

V 	
 
fed 	

kormil 	 (4)
 
NP 	

NP 	
 
D  N  	

N  	 (5)
 
D 	

	
 
the 	

	 (6)
 
N 	

N 	
 
cat 	

kota 	 (7)
Each production in this example has two com-
ponents, the first modeling English and the sec-
ond (transliterated) Russian. Nonterminals with the
same index must be rewritten together (synchronous
rewriting). One strength of MTG, and thus also
GMTG, is shown in Productions (5) and (6). There
is a determiner in English, but not in Russian, so
Production (5) does not have the nonterminal D in
the Russian component and (6) applies only to the
English component (independent rewriting). For-
malisms that do not allow independent rewriting re-
quire a corresponding  to appear in the second
component on the right-hand side (RHS) of Produc-
tion (5), and this  would eventually generate the
empty string. This approach has the disadvantage
that it introduces spurious ambiguity about the po-
sition of the ?empty? nonterminal with respect to
the other nonterminals in its component. Spurious
ambiguity leads to wasted effort during parsing.
GMTG?s implementation of independent rewrit-
ing through the empty tuple () serves a very differ-
ent function from the empty string. Consider the
following GMTG:
 
	

	
 
	

	 (8)
 
	

	
 

	


	 (9)
 
	

	
 ffProceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279?286,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Factoring Synchronous Grammars By Sorting
Daniel Gildea
Computer Science Dept.
University of Rochester
Rochester, NY 14627
Giorgio Satta
Dept. of Information Eng?g
University of Padua
I-35131 Padua, Italy
Hao Zhang
Computer Science Dept.
University of Rochester
Rochester, NY 14627
Abstract
Synchronous Context-Free Grammars
(SCFGs) have been successfully exploited
as translation models in machine trans-
lation applications. When parsing with
an SCFG, computational complexity
grows exponentially with the length of the
rules, in the worst case. In this paper we
examine the problem of factorizing each
rule of an input SCFG to a generatively
equivalent set of rules, each having the
smallest possible length. Our algorithm
works in time O(n log n), for each rule
of length n. This improves upon previous
results and solves an open problem about
recognizing permutations that can be
factored.
1 Introduction
Synchronous Context-Free Grammars (SCFGs)
are a generalization of the Context-Free Gram-
mar (CFG) formalism to simultaneously produce
strings in two languages. SCFGs have a wide
range of applications, including machine transla-
tion, word and phrase alignments, and automatic
dictionary construction. Variations of SCFGs go
back to Aho and Ullman (1972)?s Syntax-Directed
Translation Schemata, but also include the In-
version Transduction Grammars in Wu (1997),
which restrict grammar rules to be binary, the syn-
chronous grammars in Chiang (2005), which use
only a single nonterminal symbol, and the Multi-
text Grammars in Melamed (2003), which allow
independent rewriting, as well as other tree-based
models such as Yamada and Knight (2001) and
Galley et al (2004).
When viewed as a rewriting system, an SCFG
generates a set of string pairs, representing some
translation relation. We are concerned here with
the time complexity of parsing such a pair, accord-
ing to the grammar. Assume then a pair with each
string having a maximum length of N , and con-
sider an SCFG G of size |G|, with a bound of n
nonterminals in the right-hand side of each rule in
a single dimension, which we call below the rank
of G. As an upper bound, parsing can be carried
out in time O(|G|Nn+4) by a dynamic program-
ming algorithm maintaining continuous spans in
one dimension. As a lower bound, parsing strate-
gies with discontinuous spans in both dimensions
can take time ?(|G|N c?n) for unfriendly permu-
tations (Satta and Peserico, 2005). A natural ques-
tion to ask then is: What if we could reduce the
rank of G, preserving the generated translation?
As in the case of CFGs, one way of doing this
would be to factorize each single rule into several
rules of rank strictly smaller than n. It is not diffi-
cult to see that this would result in a new grammar
of size at most 2 ? |G|. In the time complexities
reported above, we see that such a size increase
would be more than compensated by the reduction
in the degree of the polynomial in N . We thus
conclude that a reduction in the rank of an SCFG
would result in more efficient parsing algorithms,
for most common parsing strategies.
In the general case, normal forms with bounded
rank are not admitted by SCFGs, as shown in (Aho
and Ullman, 1972). Nonetheless, an SCFG with a
rank of n may not necessarily meet the worst case
of Aho and Ullman (1972). It is then reasonable
to ask if our SCFG G can be factorized, and what
is the smallest rank k < n that can be obtained
in this way. This paper answers these two ques-
tions, by providing an algorithm that factorizes the
rules of an input SCFG, resulting in a new, genera-
tively equivalent, SCFG with rank k as low as pos-
sible. The algorithm works in time O(n log n) for
each rule, regardless of the rank k of the factorized
rules. As discussed above, in this way we achieve
an improvement of the parsing time for SCFGs,
obtaining an upper bound of O(|G|N k+4) by us-
ing a parsing strategy that maintains continuous
279
1,2
1,2
2,1
2 1
1,2
3 4
3,1,4,2
7 5 8 6
4,1,3,5,2
7 1 2,4,1,3
4 6 3 5
8 2
Figure 1: Two permutation trees. The permuta-
tions associated with the leaves can be produced
by composing the permutations at the internal
nodes.
spans in one dimension.
Previous work on this problem has been pre-
sented in Zhang et al (2006), where a method is
provided for casting an SCFG to a form with rank
k = 2. If generalized to any value of k, that algo-
rithm would run in time O(n2). We thus improve
existing factorization methods by almost a factor
of n. We also solve an open problem mentioned
by Albert et al (2003), who pose the question of
whether irreducible, or simple, permutations can
be recognized in time less than ?(n2).
2 Synchronous CFGs and permutation
trees
We begin by describing the synchronous CFG for-
malism, which is more rigorously defined by Aho
and Ullman (1972) and Satta and Peserico (2005).
Let us consider strings defined over some set of
nonterminal and terminal symbols, as defined for
CFGs. We say that two such strings are syn-
chronous if some bijective relation is given be-
tween the occurrences of the nonterminals in the
two strings. A synchronous context-free gram-
mar (SCFG) is defined as a CFG, with the dif-
ference that it uses synchronous rules of the form
[A1 ? ?1, A2 ? ?2], with A1, A2 nonterminalsand ?1, ?2 synchronous strings. We can use pro-duction [A1 ? ?1, A2 ? ?2] to rewrite any syn-chronous strings [?11A1?12, ?21A2?22] into thesynchronous strings [?11?1?12, ?21?2?22], un-der the condition that the indicated occurrences
of A1 and A2 be related by the bijection asso-ciated with the source synchronous strings. Fur-
thermore, the bijective relation associated with the
target synchronous strings is obtained by compos-
ing the relation associated with the source syn-
chronous strings and the relation associated with
synchronous pair [?1, ?2], in the most obviousway.
As in standard constructions that reduce the
rank of a CFG, in this paper we focus on each
single synchronous rule and factorize it into syn-
chronous rules of lower rank. If we view the bijec-
tive relation associated with a synchronous rule as
a permutation, we can further reduce our factoriza-
tion problem to the problem of factorizing a per-
mutation of arity n into the composition of several
permutations of arity k < n. Such factorization
can be represented as a tree of composed permuta-
tions, called in what follows a permutation tree.
A permutation tree can be converted into a set of
k-ary SCFG rules equivalent to the input rule. For
example, the input rule:
[ X ? A(1)B(2)C(3)D(4)E(5)F (6)G(7)H(8),
X ? B(2)A(1)C(3)D(4)G(7)E(5)H(8)F (6) ]
yields the permutation tree of Figure 1(left). In-
troducing a new grammar nonterminal Xi for eachinternal node of the tree yields an equivalent set of
smaller rules:
[ X ? X(1)1 X
(2)
2 , X ? X
(1)
1 X
(2)
2 ]
[ X1 ? X(1)3 X
(2)
4 , X1 ? X
(1)
3 X
(2)
4 ]
[ X3 ? A(1)B(2), X3 ? B(2)A(1) ]
[ X4 ? C(1)D(2), X4 ? C(1)D(2) ]
[ X2 ? E(1)F (2)G(3)H(4),
X2 ? G(3)E(1)H(4)F (2) ]
In the case of stochastic grammars, the rule cor-
responding to the root of the permutation tree is
assigned the original rule?s probability, while all
other rules, associated with new grammar nonter-
minals, are assigned probability 1. We process
each rule of an input SCFG independently, pro-
ducing an equivalent grammar with the smallest
possible arity.
3 Factorization Algorithm
In this section we specify and discuss our factor-
ization algorithm. The algorithm takes as input a
permutation defined on the set {1, ? ? ? , n}, repre-
senting a rule of some SCFG, and provides a per-
mutation tree of arity k ? n for that permutation,
with k as small as possible.
Permutation trees covering a given input permu-
tation are unambiguous with the exception of se-
quences of binary rules of the same type (either
inverted or straight) (Albert et al, 2003). Thus,
when factorizing a permutation into a permutation
280
tree, it is safe to greedily reduce a subsequence
into a new subtree as soon as a subsequence is
found which represents a continuous span in both
dimensions of the permutation matrix1 associated
with the input permutation. For space reasons, we
omit the proof, but emphasize that any greedy re-
duction turns out to be either necessary, or equiv-
alent to the other alternatives.
Any sequences of binary rules can be rear-
ranged into a normalized form (e.g. always left-
branching) as a postprocessing step, if desired.
The top-level structure of the algorithm exploits
a divide-and-conquer approach, and is the same as
that of the well-known mergesort algorithm (Cor-
men et al, 1990). We work on subsequences of
the original permutation, and ?merge? neighbor-
ing subsequences into successively longer subse-
quences, combining two subsequences of length
2i into a subsequence of length 2i+1 until we have
built one subsequence spanning the entire permu-
tation. If each combination of subsequences can
be performed in linear time, then the entire permu-
tation can be processed in time O(n log n). As in
the case of mergesort, this is an application of the
so-called master theorem (Cormen et al, 1990).
As the algorithm operates, we will maintain the
invariant that we must have built all subtrees of
the target permutation tree that are entirely within
a given subsequence that has been processed. This
is analogous to the invariant in mergesort that all
processed subsequences are in sorted order. When
we combine two subsequences, we need only build
nodes in the tree that cover parts of both sub-
sequences, but are entirely within the combined
subsequence. Thus, we are looking for subtrees
that span the midpoint of the combined subse-
quence, but have left and right boundaries within
the boundaries of the combined subsequence. In
what follows, this midpoint is called the split
point.
From this invariant, we will be guaranteed to
have a complete, correct permutation tree at the
end of last subsequence combination. An example
of the operation of the general algorithm is shown
in Figure 2. The top-level structure of the algo-
rithm is presented in function KARIZE of Figure 3.
There may be more than one reduction neces-
sary spanning a given split point when combin-
ing two subsequences. Function MERGE in Fig-
1A permutation matrix is a way of representing a permuta-
tion, and is obtained by rearranging the row (or the columns)
of an identity matrix, according to the permutation itself.
2 1 3 4 7 5 8 6
2,1
2 1
1,2
3 4 7 5 8 6
1,2
2,1
2 1
1,2
3 4
3,1,4,2
7 5 8 6
1,2
1,2
2,1
2 1
1,2
3 4
3,1,4,2
7 5 8 6
Figure 2: Recursive combination of permutation
trees. Top row, the input permutation. Second row,
after combination into sequences of length two, bi-
nary nodes have been built where possible. Third
row, after combination into sequences of length
four; bottom row, the entire output tree.
ure 3 initializes certain data structures described
below, and then checks for reductions repeatedly
until no further reduction is possible. It looks first
for the smallest reduction crossing the split point
of the subsequences being combined. If SCAN,
described below, finds a valid reduction, it is com-
mitted by calling REDUCE. If a reduction is found,
we look for further reductions crossing either the
left or right boundary of the new reduction, repeat-
ing until no further reductions are possible. Be-
cause we only need to find reductions spanning
the original split point at a given combination step,
this process is guaranteed to find all reductions
needed.
We now turn to the problem of identifying a
specific reduction to be made across a split point,
which involves identifying the reduction?s left and
right boundaries. Given a subsequence and can-
didate left and right boundaries for that subse-
quence, the validity of making a reduction over
this span can be tested by verifying whether the
span constitutes a permuted sequence, that is,
a permutation of a contiguous sequence of inte-
gers. Since the starting permutation is defined
on a set {1, 2, ? ? ? , n}, we have no repeated in-
tegers in our subsequences, and the above condi-
281
function KARIZE(pi)
. initialize with identity mapping
h? hmin? hmax? (0..|pi|);
. mergesort core
for size? 1; size ? |pi|; size? size * 2 do
for min? 0;
min < |pi|-size+1;
min? min + 2 * size do
div = min + size - 1;
max? min(|pi|, min + 2*size - 1);
MERGE(min, div, max);
function MERGE(min, div, max)
. initialize h
sort h[min..max] according to pi[i];
sort hmin[min..max] according to pi[i];
sort hmax[min..max] according to pi[i];
. merging sorted list takes linear time
. initialize v
for i? min; i ? max; i? i + 1 do
v [ h[i] ]? i;
. check if start of new reduced block
if i = min or
hmin[i] 6= hmin[i-1] then
vmin? i;
vmin[ h[i] ]? vmin;
for i? max; i ? min; i? i - 1 do
. check if start of new reduced block
if i = max or
hmax[i] 6= hmax[i+1] then
vmax? i ;
vmax[ h[i] ]? vmax;
. look for reductions
if SCAN(div) then
REDUCE(scanned reduction);
while SCAN(left) or SCAN(right) do
REDUCE(smaller reduction);
function REDUCE(left, right, bot, top)
for i? bot..top do
hmin[i]? left;
hmax[i]? right;
for i? left..right do
vmin[i]? bot;
vmax[i]? top;
print ?reduce:? left..right ;
Figure 3: KARIZE: Top level of algorithm, iden-
tical to that of mergesort. MERGE: combines two
subsequences of size 2i into new subsequence of
size 2i+1. REDUCE: commits reduction by updat-
ing min and max arrays.
tion can be tested by scanning the span in ques-
tion, finding the minimum and maximum integers
in the span, and checking whether their difference
is equal to the length of the span minus one. Be-
low we call this condition the reduction test. As
an example of the reduction test, consider the sub-
sequence (7, 5, 8, 6), and take the last three ele-
ments, (5, 8, 6), as a candidate span. We see that
5 and 8 are the minimum and maximum integers
in the corresponding span, respectively. We then
compute 8 ? 5 = 3, while the length of the span
minus one is 2, implying that no reduction is possi-
ble. However, examining the entire subsequence,
the minimum is 5 and the maximum is 8, and
8 ? 5 = 3, which is the length of the span minus
one. We therefore conclude that we can reduce
that span by means of some permutation, that is,
parse the span by means of a node in the permuta-
tion tree. This reduction constitutes the 4-ary node
in the permutation tree of Figure 2.
A trivial implementation of the reduction test
would be to tests all combinations of left and right
boundaries for the new reduction. Unfortunately,
this would take time ?(n2) for a single subse-
quence combination step, whereas to achieve the
overall O(n log n) complexity we need linear time
for each combination step.
It turns out that the boundaries of the next re-
duction, covering a given split point, can be com-
puted in linear time with the technique shown in
function SCAN of Figure 5. We start with left and
right candidate boundaries at the two points imme-
diately to the left and right of the split point, and
then repeatedly check whether the current left and
right boundaries identify a permuted sequence by
applying the reduction test, and move the left and
right boundaries outward as necessary, as soon as
?missing? integers are identified outside the cur-
rent boundaries, as explained below. We will show
that, as we move outward, the number of possible
configurations achieved for the positions of the left
and the right boundaries is linearly bounded in the
length of the combined subsequence (as opposed
to quadratically bounded).
In order to efficiently implement the above idea,
we will in fact maintain four boundaries for the
candidate reduction, which can be visualized as
the left, right, top and bottom boundaries in the
permutation matrix. No explicit representation
of the permutation matrix itself is constructed, as
that would require quadratic time. Rather, we
282
7 1 4 6 3 5 8 2pi 4
7
2
1
1
3
2
4
4
3
6
1
6
3
8
7
5
5
8
8
6
5
2
7
v
pi
h
Figure 4: Permutation matrix for input permuta-
tion pi (left) and within-subsequence permutation
v (right) for subsequences of size four.
maintain two arrays: h, which maps from vertical
to horizontal positions within the current subse-
quence, and v which maps from horizontal to ver-
tical positions. These arrays represent the within-
subsequence permutation obtained by sorting the
elements of each subsequence according to the
input permutation, while keeping each element
within its block, as shown in Figure 4.
Within each subsequence, we alternate between
scanning horizontally from left to right, possibly
extending the top and bottom boundaries (Figure 5
lines 9 to 14), and scanning vertically from bottom
to top, possibly extending the left and right bound-
aries (lines 20 to 26). Each extension is forced
when, looking at the within-subsequence permuta-
tion, we find that some element is within the cur-
rent boundaries in one dimension but outside the
boundaries in the other. If the distance between
vertical boundaries is larger in the input permu-
tation than in the subsequence permutation, nec-
essary elements are missing from the current sub-
sequence and no reduction is possible at this step
(line 18). When all necessary elements are present
in the current subsequence and no further exten-
sions are necessary to the boundaries (line 30), we
have satisfied the reduction test on the input per-
mutation, and make a reduction.
The trick used to keep the iterative scanning lin-
ear is that we skip the subsequence scanned on the
previous iteration on each scan, in both the hori-
zontal and vertical directions. Lines 13 and 25 of
Figure 5 perform this skip by advancing the x and y
counters past previously scanned regions. Consid-
ering the horizontal scan of lines 9 to 14, in a given
iteration of the while loop, we scan only the items
between newleft and left and between right and
newright. On the next iteration of the while loop,
the newleft boundary has moved further to the left,
1: function SCAN (div)
2: left???;
3: right???;
4: newleft? div;
5: newright? div + 1 ;
6: newtop???;
7: newbot??;
8: while 1 do
. horizontal scan
9: for x? newleft; x ? newright ; do
10: newtop? max(newtop, vmax[x]);
11: newbot? min(newbot, vmin[x]);
. skip to end of reduced block
12: x? hmax[vmin[x]] + 1;
. skip section scanned on last iter
13: if x = left then
14: x? right + 1;
15: right? newright;
16: left? newleft;
. the reduction test
17: if newtop - newbot <
18: pi[h[newtop]] - pi[h[newbot]] then
19: return (0);
. vertical scan
20: for y? newbot; y ? newtop ; do
21: newright?
22: max(newright, hmax[y]);
23: newleft? min(newleft, hmin[y]);
. skip to end of reduced block
24: y? vmax[hmin[y]] + 1;
. skip section scanned on last iter
25: if y = bot then
26: y? top + 1;
27: top? newtop;
28: bot? newbot;
. if no change to boundaries, reduce
29: if newright = right
30: and newleft = left then
31: return (1, left, right, bot, top);
Figure 5: Linear time function to check for a sin-
gle reduction at split point div.
283
while the variable left takes the previous value of
newleft, ensuring that the items scanned on this it-
eration are distinct from those already processed.
Similarly, on the right edge we scan new items,
between right and newright. The same analysis
applies to the vertical scan. Because each item in
the permutation is scanned only once in the verti-
cal direction and once in the horizontal direction,
the entire call to SCAN takes linear time, regard-
less of the number of iterations of the while loop
that are required.
We must further show that each call to MERGE
takes only linear time, despite that fact that it
may involve many calls to SCAN. We accom-
plish this by introducing a second type of skipping
in the scans, which advances past any previously
reduced block in a single step. In order to skip
past previous reductions, we maintain (in func-
tion REDUCE) auxiliary arrays with the minimum
and maximum positions of the largest block each
point has been reduced to, in both the horizontal
and vertical dimensions. We use these data struc-
tures (hmin, hmax, vmin, vmax) when advancing to
the next position of the scan in lines 12 and 24 of
Figure 5. Because each call to SCAN skips items
scanned by previous calls, each item is scanned
at most twice across an entire call to MERGE,
once when scanning across a new reduction?s left
boundary and once when scanning across the right
boundary, guaranteeing that MERGE completes in
linear time.
4 An Example
In this section we examine the operation of the
algorithm on a permutation of length eight, re-
sulting in the permutation tree of Figure 1(right).
We will build up our analysis of the permutation
by starting with individual items of the input per-
mutation and building up subsequences of length
2, 4, and finally 8. In our example permutation,
(7, 1, 4, 6, 3, 5, 8, 2), no reductions can be made
until the final combination step, in which one per-
mutation of size 4 is used, and one of size 5.
We begin with the input permutation along the
bottom of Figure 6a. We represent the interme-
diate data structures h, hmin, and hmax along the
vertical axis of the figure; these three arrays are all
initialized to be the sequence (1, 2, ? ? ? , 8).
Figure 6b shows the combination of individual
items into subsequences of length two. Each new
subsequence of the h array is sorted according to
a)
7
1
1
1
111
1
2
2
2
222
4
3
3
3
333
6
4
4
4
444
3
5
5
5
555
5
6
6
6
666
8
7
7
7
777
2
8
8
8
888
pi
v
vmin
vmax
hhm
in
hm
ax
1
7
1
2
1
2
3
4
3
4
6
4
5
3
5
6
5
6
7
8
7
8
2
8
v
pi
h
b)
7
2
2
2
222
1
1
1
1
111
4
3
3
3
333
6
4
4
4
444
3
5
5
5
555
5
6
6
6
666
8
8
8
8
888
2
7
7
7
777
pi
v
vmin
vmax
hhm
in
hm
ax
2
7
2
1
1
1
3
4
3
4
6
4
5
3
5
6
5
6
8
8
8
7
2
7
v
pi
h
c)
7
4
4
4
222
1
1
1
1
333
4
2
2
2
444
6
3
3
3
111
3
6
6
6
888
5
7
7
7
555
8
8
8
8
666
2
5
5
5
777
pi
v
vmin
vmax
hhm
in
hm
ax
4
7
2
1
1
3
2
4
4
3
6
1
6
3
8
7
5
5
8
8
6
5
2
7
v
pi
h
Figure 6: Steps in an example computation,
with input permutation pi on left and within-
subsequence permutation described by v array on
right. Panel (a) shows initial blocks of unit size,
(b) shows combination of unit blocks into blocks
of size two, and (c) size two into size four. No
reductions are possible in these stages; example
continued in next figure.
284
a)
7
7
7
7
222
1
1
1
1
888
4
4
4
4
555
6
6
6
6
333
3
3
3
3
666
5
5
5
5
444
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax b)
7
7
7
7
222
1
1
1
1
888
4
4
3
6
536
6
6
3
6
336
3
3
3
6
636
5
5
3
6
436
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax
Left and right boundaries are initialized
to be adjacent to horizontal split point.
Vertical scan shows left and right bound-
aries must be extended. Permutation of
size four is reduced.
c)
7
7
7
7
222
1
1
1
1
888
4
4
3
6
536
6
6
3
6
336
3
3
3
6
636
5
5
3
6
436
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax d)
7
7
7
7
222
1
1
1
1
888
4
4
3
6
536
6
6
3
6
336
3
3
3
6
636
5
5
3
6
436
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax
Search for next reduction: left and right
boundaries initialized to be adjacent to
left edge of previous reduction.
Vertical scan shows right boundary must
be extended.
e)
7
7
7
7
222
1
1
1
1
888
4
4
3
6
536
6
6
3
6
336
3
3
3
6
636
5
5
3
6
436
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax f)
7
7
1
8
218
1
1
1
8
818
4
4
1
8
518
6
6
1
8
318
3
3
1
8
618
5
5
1
8
418
8
8
1
8
118
2
2
1
8
718
pi
v
vmin
vmax
hhm
in
hm
ax
Horizontal scan shows top boundary must
be extended.
Vertical scan shows left boundary must
be extended. Permutation of size five is
reduced.
Figure 7: Steps in scanning for final combination of subsequences, where v = pi. Area within current
left, right, top and bottom boundaries is shaded; darker shading indicates a reduction. In each scan, the
span scanned in the previous panel is skipped over.
285
the vertical position of the dots in the correspond-
ing columns. Thus, because pi[7] = 8 > pi[8] = 2,
we swap 7 and 8 in the h array. The algorithm
checks whether any reductions can be made at this
step by computing the difference between the in-
tegers on each side of each split point. Because
none of the pairs of integers in are consecutive, no
reductions are made at this step.
Figure 6c shows the combination the pairs
into subsequences of length four. The two split
points to be examined are between the second and
third position, and the sixth and seventh position.
Again, no reductions are possible.
Finally we combine the two subsequences of
length four to complete the analysis of the entire
permutation. The split point is between the fourth
and fifth positions of the input permutation, and
in the first horizontal scan of these two positions,
we see that pi[4] = 6 and pi[5] = 3, meaning our
top boundary will be 6 and our bottom boundary
3, shown in Figure 7a. Scanning vertically from
position 3 to 6, we see horizontal positions 5, 3,
6, and 4, giving the minimum, 3, as the new left
boundary and the maximum, 6, as the new right
boundary, shown in Figure 7b. We now perform
another horizontal scan starting at position 3, but
then jumping directly to position 6, as horizontal
positions 4 and 5 were scanned previously. Af-
ter this scan, the minimum vertical position seen
remains 3, and the maximum vertical position is
still 6. At this point, because we have the same
boundaries as on the previous scan, we can stop
and verify whether the region determined by our
current boundaries has the same length in the ver-
tical and horizontal dimensions. Both dimensions
have length four, meaning that we have found a
subsequence that is continuous in both dimensions
and can safely be reduced, as shown in Figure 6d.
After making this reduction, we update the hmin
array to have all 3?s for the newly reduced span,
and update hmax to have all sixes. We then check
whether further reductions are possible covering
this split point. We repeat the process of scan-
ning horizontally and vertically in Figure 7c-f,
this time skipping the span just reduced. One fur-
ther reduction is possible, covering the entire input
permutation, as shown in Figure 7f.
5 Conclusion
The algorithm above not only identifies whether
a permutation can be factored into a composi-
tion of permutations, but also returns the factor-
ization that minimizes the largest rule size, in time
O(n log n). The factored SCFG with rules of size
at most k can be used to synchronously parse
in time O(Nk+4) by dynamic programming with
continuous spans in one dimension.
As mentioned in the introduction, the optimal
parsing strategy for SCFG rules with a given
permutation may involve dynamic programming
states with discontinuous spans in both dimen-
sions. Whether these optimal parsing strategies
can be found efficiently remains an interesting
open problem.
Acknowledgments This work was partially sup-
ported by NSF ITR IIS-09325646 and NSF ITR
IIS-0428020.
References
Albert V. Aho and Jeffery D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume 1. Prentice-Hall, Englewood Cliffs, NJ.
M. H. Albert, M. D. Atkinson, and M. Klazar. 2003.
The enumeration of simple permutations. Journal
of Integer Sequences, 6(03.4.4):18 pages.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL-05, pages 263?270.
Thomas H. Cormen, Charles E. Leiserson, and
Ronald L. Rivest. 1990. Introduction to algorithms.
MIT Press, Cambridge, MA.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT/NAACL.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of HLT/NAACL.
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous
context-free grammars. In Proceedings of
HLT/EMNLP, pages 803?810.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of ACL-01.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of HLT/NAACL.
286
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760?767,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Guided Learning for Bidirectional Sequence Classification
Libin Shen
BBN Technologies
Cambridge, MA 02138, USA
lshen@bbn.com
Giorgio Satta
Dept. of Inf. Eng?g.
University of Padua
I-35131 Padova, Italy
satta@dei.unipd.it
Aravind K. Joshi
Department of CIS
University of Pennsylvania
Philadelphia, PA 19104, USA
joshi@seas.upenn.edu
Abstract
In this paper, we propose guided learning,
a new learning framework for bidirectional
sequence classification. The tasks of learn-
ing the order of inference and training the
local classifier are dynamically incorporated
into a single Perceptron like learning algo-
rithm. We apply this novel learning algo-
rithm to POS tagging. It obtains an error rate
of 2.67% on the standard PTB test set, which
represents 3.3% relative error reduction over
the previous best result on the same data set,
while using fewer features.
1 Introduction
Many NLP tasks can be modeled as a sequence clas-
sification problem, such as POS tagging, chunking,
and incremental parsing. A traditional method to
solve this problem is to decompose the whole task
into a set of individual tasks for each token in the in-
put sequence, and solve these small tasks in a fixed
order, usually from left to right. In this way, the out-
put of the previous small tasks can be used as the
input of the later tasks. HMM and MaxEnt Markov
Model are examples of this method.
Lafferty et al (2001) showed that this approach
suffered from the so called label bias problem (Bot-
tou, 1991). They proposed Conditional Random
Fields (CRF) as a general solution for sequence clas-
sification. CRF models a sequence as an undirected
graph, which means that all the individual tasks are
solved simultaneously. Taskar et al (2003) improved
the CRF method by employing the large margin
method to separate the gold standard sequence la-
beling from incorrect labellings. However, the com-
plexity of quadratic programming for the large mar-
gin approach prevented it from being used in large
scale NLP tasks.
Collins (2002) proposed a Perceptron like learn-
ing algorithm to solve sequence classification in the
traditional left-to-right order. This solution does not
suffer from the label bias problem. Compared to the
undirected methods, the Perceptron like algorithm
is faster in training. In this paper, we will improve
upon Collins? algorithm by introducing a bidirec-
tional searching strategy, so as to effectively utilize
more context information at little extra cost.
When a bidirectional strategy is used, the main
problem is how to select the order of inference. Tsu-
ruoka and Tsujii (2005) proposed the easiest-first ap-
proach which greatly reduced the computation com-
plexity of inference while maintaining the accuracy
on labeling. However, the easiest-first approach only
serves as a heuristic rule. The order of inference is
not incorporated into the training of the MaxEnt clas-
sifier for individual labeling.
Here, we will propose a novel learning frame-
work, namely guided learning, to integrate classifi-
cation of individual tokens and inference order selec-
tion into a single learning task. We proposed a Per-
ceptron like learning algorithm (Collins and Roark,
2004; Daume? III and Marcu, 2005) for guided learn-
ing. We apply this algorithm to POS tagging, a clas-
sic sequence learning problem. Our system reports
an error rate of 2.67% on the standard PTB test set,
a relative 3.3% error reduction of the previous best
system (Toutanova et al, 2003) by using fewer fea-
tures. By using deterministic search, it obtains an
error rate of 2.73%, a 5.9% relative error reduction
760
over the previous best deterministic algorithm (Tsu-
ruoka and Tsujii, 2005).
The new POS tagger is similar to (Toutanova et
al., 2003; Tsuruoka and Tsujii, 2005) in the way
that we employ context features. We use a bidi-
rectional search strategy (Woods, 1976; Satta and
Stock, 1994), and our algorithm is based on Percep-
tron learning (Collins, 2002). A unique contribution
of our work is on the integration of individual clas-
sification and inference order selection, which are
learned simultaneously.
2 Guided Learning for Bidirectional
Labeling
We first present an example of POS tagging to show
the idea of bidirectional labeling. Then we present
the inference algorithm and the learning algorithm.
2.1 An Example of POS tagging
Suppose that we have an input sentence
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 0)
If we scan from left to right, we may find it
difficult to resolve the ambiguity of the label for
that, which could be either DT (determiner), or
IN (preposition or subordinating conjunction) in the
Penn Treebank. However, if we resolve the labels for
book and interesting, it would be relatively easy to
figure out the correct label for that.
Now, we show how bidirectional inference works
on this sample. Suppose we use beam search with
width of 2, and we use a window of (-2, 2) for con-
text features.
For the first step, we enumerate hypotheses for
each word. For example, found could have a label
VBN or VBD. Suppose that at this point the most
favorable action, out of the candidate hypotheses, is
the assignment of NN to book, according to the con-
text features defined on words. Then, we resolve the
label for book first. We maintain the top two hy-
potheses as shown below. Here, the second most fa-
vorable label for book is VB.
NN
VB
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 1)
At the second step, assume the most favorable ac-
tion is the assignment of label JJ to interesting in
the context of NN for book. Then we maintain the
top two hypotheses for span book interesting as
shown below. The second most favorable label for
interesting is still JJ, but in the context of VB for
book.
NN------JJ
VB------JJ
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 2)
Then, suppose we are most confident for assigning
labels VBD and VBN to found, in that order. We get
two separated tagged spans as shown below.
VBD NN------JJ
VBN VB------JJ
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 3)
In the next step, suppose we are most confident for
assigning label DT to that under the context of VBD
on the left and NN-JJ on the right side, as shown
below (second most favorable action, not discussed
here, is also displayed). After tagging w3, two sep-
arated spans merge into one, starting from found to
interesting.
VBD---DT---NN------JJ
VBD---IN---NN------JJ
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 4)
For the last step, we assign label NNP to Agatha,
which could be an out-of-vocabulary word, under the
context of VBD-DT on the right.
NNP---VBD---DT---NN------JJ
NNP---VBD---IN---NN------JJ
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 5)
This simple example has shown the advantage of
adopting a flexible search strategy. However, it is
still unclear how we maintain the hypotheses, how
we keep candidates and accepted labels and spans,
and how we employ dynamic programming. We will
answer these questions in the formal definition of the
inference algorithm in the next section.
761
2.2 Inference Algorithm
Terminology: Let the input sequence be
w1w2 ? ? ?wn. For each token wi, we are expected
to assign a label ti ? T, with T the label set.
A subsequence wi ? ? ?wj is called a span, and is
denoted [i, j]. Each span p considered by the al-
gorithm is associated with one or more hypotheses,
that is, sequences over T having the same length as
p. Part of the label sequence of each hypothesis is
used as a context for labeling tokens outside the span
p. For example, if a tri-gram model is adopted, we
use the two labels on the left boundary and the two
labels on the right boundary of the hypothesis for la-
beling outside tokens. The left two labels are called
the left interface, and the right two labels are called
the right interface. Left and right interfaces have
only one label in case of spans of length one.
A pair s = (Ileft , Iright) with a left and a right
interface is called a state. We partition the hypothe-
ses associated with span p into sets compatible with
the same state. In practice, for span p, we use a ma-
trix Mp indexed by states, so that Mp(s), s = (Ileft ,
Iright), is the set of all hypotheses associated with p
that are compatible with Ileft and Iright .
For a span p and a state s, we denote the associated
top hypothesis as
s.T = argmax
h?Mp(s)
V (h),
where V is the score of a hypothesis (defined in (1)
below). Similarly, we denote the top state for p as
p.S = argmax
s: Mp(s) 6=?
V (s.T ).
Therefore, for each span p, we have a top hypothe-
sis p.S.T , whose score is the highest among all the
hypotheses for span p.
Hypotheses are started and grown by means of
labeling actions. For each hypothesis h associated
with a span p we maintain its most recent labeling
action h.A, involving some token within p, as well
as the states h.SL and h.SR that have been used as
context by such an action, if any. Note that h.SL and
h.SR refer to spans that are subsequences of p. We
recursively compute the score of h as
V (h) = V (h.SL.T ) + V (h.SR.T ) + U(h.A), (1)
Algorithm 1 Inference Algorithm
Require: token sequence w1 ? ? ?wn;
Require: beam width B;
Require: weight vector w;
1: Initialize P , the set of accepted spans;
2: Initialize Q, the queue of candidate spans;
3: repeat
4: span p? ? argmaxp?Q U(p.S.T.A);
5: Update P with p?;
6: Update Q with p? and P ;
7: until (Q = ?)
where U is the score of an action. In other words,
the score of an hypothesis is the sum of the score
of the most recent action h.A and the scores of the
top hypotheses of the context states. The score of
an action h.A is computed through a linear function
whose weight vector is w, as
U(h.A) = w ? f(h.A), (2)
where f(h.A) is the feature vector of action h.A,
which depends on h.SL and h.SR.
Algorithm: Algorithm 1 is the inference algorithm.
We are given the input sequence and two parame-
ters, beam width B to determine the number of states
maintained for each span, and weight vector w used
to compute the score of an action.
We first initialize the set P of accepted spans with
the empty set. Then we initialize the queue Q of
candidate spans with span [i, i] for each token wi,
and for each t ? T assigned to wi we set
M[i,i]((t, t)) = {i? t},
where i ? t represents the hypothesis consisting of
a single action which assigns label t to wi. This pro-
vides the set of starting hypotheses.
As for the example Agatha found that book
interesting in the previous subsection, we have
? P = ?
? Q = {[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]}
Suppose NN and VB are the two possible POS tags
for w4 book. We have
? M[4,4](NN, NN) = {h441 = 4? NN}
? M[4,4](VB, VB) = {h442 = 4? VB}
The most recent action of hypothesis h441 is to as-
sign NN to w4. According to Equation (2), the score
762
of this action U(h441.A) depends on the features de-
fined on the local context of action. For example,
f1001(h441.A) =
{
1 if t = NN ? w?1 = that
0 otherwise,
where w?1 represents the left word. It should be
noted that, for all the features depending on the
neighboring tags, the value is always 0, since those
tags are still unknown in the step of initialization.
Since this operation does not depend on solved tags,
we have V (h441) = U(h411.A), according to Equa-
tion (1).
The core of the algorithm repeatedly selects a can-
didate span from Q, and uses it to update P and Q,
until a span covering the whole sequence is added to
P and Q becomes empty. This is explained in detail
below.
At each step, we remove from Q the span p? such
that the action (not hypothesis) score of its top hy-
pothesis, p?.S.T , is the highest. This represents the
labeling action for the next move that we are most
confident about. Now we need to update P and Q
with the selected span p?. We add p? to P , and re-
move from P the spans included in p?, if any. Let
S be the set of removed spans. We remove from Q
each span which takes one of the spans in S as con-
text, and replace it with a new candidate span taking
p? (and another accepted span) as context. We always
maintain B different states for each span.
Back to the previous example, after Step 3 is com-
pleted, w2 found, w4 book and w5 interesting
have been tagged and we have
? P = {[2, 2], [4, 5]}
? Q = {[1, 2], [2, 5]}
There are two candidate spans in Q, each with its as-
sociated hypotheses and most recent actions. More
specifically, we can either solve w1 based on the con-
text hypotheses for [2, 2], resulting in span [1, 2], or
else solve w3 based on the context hypotheses in
[2, 2] and [4, 5], resulting in span [2, 5].
The top two states for span [2, 2] are
? M[2,2](VBD, VBD) = {h221 = 2? VBD}
? M[2,2](VBN, VBN) = {h222 = 2? VBN}
and the top two states for span [4, 5] are
? M[4,5](NN-JJ, NN-JJ)
= {h451 = (NN,NN)5? JJ}
? M[4,5](VB-JJ, VB-JJ)
= {h452 = (VB,VB)5? JJ}
Here (NN,NN)5 ? JJ represents the hypothesis
coming from the action of assigning JJ to w5 under
the left context state of (NN,NN). (VB,VB)5 ? JJ
has a similar meaning.1
We first compute the hypotheses resulting from all
possible POS tag assignments to w3, under all possi-
ble state combinations of the neighboring spans [2, 2]
and [4, 5]. Suppose the highest score action consists
in the assignment of DT under the left context state
(VBD, VBD) and the right context state (NN-JJ, NN-
JJ). We obtain hypothesis h251 = (VBD,VBD)3 ?
DT(NN-JJ, NN-JJ) with
V (h251) = V ((VBD,VBD).T ) +
V ((NN-JJ,NN-JJ).T ) + U(h251.A)
= V (h221) + V (h451) +w ? f(h251.A)
Here, features for action h251.A may depend on
the left tag VBD and right tags NN-JJ, which have
been solved before. More details of the feature func-
tions are given in Section 4.2. For example, we can
have features like
f2002(h251.A) =
{
1 if t = DT ? t+2 = JJ
0 otherwise,
We maintain the top two states with the highest
hypothesis scores, if the beam width is set to two.
We have
? M[2,5](VBD-DT, NN-JJ) = {h251 =
(VBD,VBD)3? DT(NN-JJ,NN-JJ)}
? M[2,5](VBD-IN, NN-JJ) = {h252 =
(VBD,VBD)3? IN(NN-JJ,NN-JJ)}
Similarly, we compute the top hypotheses and
states for span [1, 2]. Suppose now the hypothesis
with the highest action score is h251. Then we up-
date P by adding [2, 5] and removing [2, 2] and [4, 5],
which are covered by [2, 5]. We also update Q by re-
moving [2, 5] and [1, 2],2 and add new candidate span
[1, 5] resulting in
? P = {[2, 5]}
? Q = {[1, 5]}
1It should be noted that, in these cases, each state con-
tains only one hypothesis. However, if the span is longer than
4 words, there may exist multiple hypotheses for the same
state. For example, hypotheses DT-NN-VBD-DT-JJ and DT-
NN-VBN-DT-JJ have the same left interface DT-NN and right
interface DT-JJ.
2Span [1, 2] depends on [2, 2] and [2, 2] has been removed
from P . So it is no longer a valid candidate given the accepted
spans in P .
763
The algorithm is especially designed in such a way
that, at each step, some new span is added to P or
else some spans already present in P are extended
by some token(s). Furthermore, no pair of overlap-
ping spans is ever found in P , and the number of
pairs of overlapping spans that may be found in Q is
always bounded by a constant. This means that the
algorithm performs at most n iterations, and its run-
ning time is therefore O(B2n), that is, linear in the
length of the input sequence.
2.3 Learning Algorithm
In this section, we propose guided learning, a Per-
ceptron like algorithm, to learn the weight vector w,
as shown in Algorithm 2. We use p?.G to represent
the gold standard hypothesis on span p?.
For each input sequence Xr and the gold standard
sequence of labeling Yr, we first initialize P and Q
as in the inference algorithm. Then we select the
span for the next move as in Algorithm 1. If p?.S.T ,
the top hypothesis of the selected span p?, is com-
patible with the gold standard, we update P and Q
as in Algorithm 1. Otherwise, we update the weight
vector in the Perceptron style, by promoting the fea-
tures of the gold standard action, and demoting the
features of the action of the top hypothesis. Then
we re-generate the queue Q with P and the updated
weight vector w. Specifically, we first remove all the
elements in Q, and then generate hypotheses for all
the possible spans based on the context spans in P .
Hypothesis scores and action scores are calculated
with the updated weight vector w.
A special aspect of Algorithm 2 is that we main-
tain two scores: the score of the action represents the
confidence for the next move, and the score of the
hypothesis represents the overall quality of a partial
result. The selection for the next action directly de-
pends on the score of the action, but not on the score
of the hypothesis. On the other hand, the score of the
hypothesis is used to maintain top partial results for
each span.
We briefly describe the soundness of the Guided
Learning Algorithm in terms of two aspects. First,
in Algorithm 2 weight update is activated whenever
there exists an incorrect state s, the action score of
whose top hypothesis s.T is higher than that of any
state in each span. We demote this action and pro-
mote the gold standard action on the same span.
Algorithm 2 Guided Learning Algorithm
Require: training sequence pairs {(Xr, Yr)}1?r?R;
Require: beam width B and iterations I;
1: w? 0;
2: for (i? 1; i ? I; i++) do
3: for (r ? 1; r ? R; r++) do
4: Load sequence Xr and gold labeling Yr.
5: Initialize P , the set of accepted spans
6: Initialize Q, the queue of candidate spans;
7: repeat
8: p? ? argmaxp?Q U(p.S.T.A);
9: if (p?.S.T = p?.G) then
10: Update P with p?;
11: Update Q with p? and P ;
12: else
13: promote(w, f(p?.G.A));
14: demote(w, f(p?.S.T.A));
15: Re-generate Q with w and P ;
16: end if
17: until (Q = ?)
18: end for
19: end for
However, we do not automatically adopt the gold
standard action on this span. Instead, in the next
step, the top hypothesis of another span might be se-
lected based on the score of action, which means that
it becomes the most favorable action according to the
updated weights.
As a second aspect, if the action score of a gold
standard hypothesis is higher than that of any oth-
ers, this hypothesis and the corresponding span are
guaranteed to be selected at line 8 of Algorithm 2.
The reason for this is that the scores of the context
hypotheses of a gold standard hypothesis must be
no less than those of other hypotheses of the same
span. This could be shown recursively with respect
to Equation 1, because the context hypotheses of a
gold standard hypothesis are also compatible with
the gold standard.
Furthermore, if we take
(xi = f(p
?.G.A)? f(p?.S.T.A), yi = +1)
as a positive sample, and
(xj = f(p
?.S.T.A)? f(p?.G.A), yj = ?1)
as a negative sample, the weight updates at lines 13
764
and 14 are a stochastic approximation of gradient de-
scent that minimizes the squared errors of the mis-
classified samples (Widrow and Hoff, 1960). What
is special with our learning algorithm is the strategy
used to select samples for training.
In general, this novel learning framework lies be-
tween supervised learning and reinforcement learn-
ing. Guided learning is more difficult than super-
vised learning, because we do not know the order of
inference. The order is learned automatically, and
partial output is in turn used to train the local clas-
sifier. Therefore, the order of inference and the lo-
cal classification are dynamically incorporated in the
learning phase.
Guided learning is not as hard as reinforcement
learning. At each local step in learning, we always
know the undesirable labeling actions according to
the gold standard, although we do not know which
is the most desirable. In this approach, we can eas-
ily collect the automatically generated negative sam-
ples, and use them in learning. These negative sam-
ples are exactly those we will face during inference
with the current weight vector.
In our experiments, we have used Averaged Per-
ceptron (Collins, 2002; Freund and Schapire, 1999)
and Perceptron with margin (Krauth and Me?zard,
1987) to improve performance.
3 Related Works
Tsuruoka and Tsujii (2005) proposed a bidirectional
POS tagger, in which the order of inference is han-
dled with the easiest-first heuristic. Gime?nez and
Ma`rquez (2004) combined the results of a left-to-
right scan and a right-to-left scan. In our model, the
order of inference is dynamically incorporated into
the training of the local classifier.
Toutanova et al (2003) reported a POS tagger
based on cyclic dependency network. In their work,
the order of inference is fixed as from left to right. In
this approach, large beam width is required to main-
tain the ambiguous hypotheses. In our approach, we
can handle tokens that we are most confident about
first, so that our system does not need a large beam.
As shown in Section 4.2, even deterministic infer-
ence shows rather good results.
Our guided learning can be modeled as a search
algorithm with Perceptron like learning (Daume? III
and Marcu, 2005). However, as far as we know,
Data Set Sections Sentences Tokens
Training 0-18 38,219 912,344
Develop 19-21 5,527 131,768
Test 22-24 5,462 129,654
Table 1: Data set splits
the mechanism of bidirectional search with an on-
line learning algorithm has not been investigated be-
fore. In (Daume? III and Marcu, 2005), as well
as other similar works (Collins, 2002; Collins and
Roark, 2004; Shen and Joshi, 2005), only left-to-
right search was employed. Our guided learning al-
gorithm provides more flexibility in search with an
automatically learned order. In addition, our treat-
ment of the score of action and the score of hypoth-
esis is unique (see discussion in Section 2.3).
Furthermore, compared to the above works, our
guided learning algorithm is more aggressive on
learning. In (Collins and Roark, 2004; Shen and
Joshi, 2005), a search stops if there is no hypothe-
sis compatible with the gold standard in the queue
of candidates. In (Daume? III and Marcu, 2005), the
search is resumed after some gold standard compat-
ible hypotheses are inserted into a queue for future
expansion, and the weights are updated correspond-
ingly. However, there is no guarantee that the up-
dated weights assign a higher score to those inserted
gold standard compatible hypotheses. In our algo-
rithm, the gold standard compatible hypotheses are
used for weight update only. As a result, after each
sentence is processed, the weight vector can usually
successfully predict the gold standard parse. There-
fore our learning algorithm is aggressive on weight
update.
As far as this aspect is concerned, our algorithm
is similar to the MIRA algorithm in (Crammer and
Singer, 2003). In MIRA, one always knows the cor-
rect hypothesis. In our case, we do not know the
correct order of operations. So we use our form of
weight update to implement aggressive learning.
4 Experiments on POS Tagging
4.1 Settings
We apply our guided learning algorithm to POS tag-
ging. We carry out experiments on the standard
data set of the Penn Treebank (PTB) (Marcus et al,
1994). Following (Ratnaparkhi, 1996; Collins, 2002;
Toutanova et al, 2003; Tsuruoka and Tsujii, 2005),
765
Feature Sets Templates Error%
A Ratnaparkhi?s 3.05
B A + [t0, t1], [t0, t?1, t1], [t0, t1, t2] 2.92
C B + [t0, t?2], [t0, t2], [t0, t?2, w0], [t0, t?1, w0], [t0, t1, w0],
[t0, t2, w0], [t0, t?2, t?1, w0], [t0, t?1, t1, w0], [t0, t1, t2, w0]
2.84
D C + [t0, w?1, w0], [t0, w1, w0] 2.78
E D + [t0, X = prefix or suffix of w0], 4 < |X| ? 9 2.72
Table 2: Experiments on the development data with beam width of 3
we cut the PTB into the training, development and
test sets as shown in Table 1. We use tools provided
by CoNLL-2005 3 to extract POS tags from the mrg
files of PTB. So the data set is the same as previous
work. We use the development set to select features
and estimate the number of iterations in training. In
our experiments, we enumerate all the POS tags for
each word instead of using a dictionary as in (Ratna-
parkhi, 1996), since the size of the tag set is tractable
and our learning algorithm is efficient enough.
4.2 Results
Effect of Features: We first run the experiments to
evaluate the effect of features. We use templates to
define features. For this set of experiments, we set
the beam width B = 3 as a balance between speed
and accuracy. The guided learning algorithm usually
converges on the development data set in 4-8 itera-
tions over the training data.
Table 2 shows the error rate on the development
set with different features. We first use the same fea-
ture set used in (Ratnaparkhi, 1996), which includes
a set of prefix, suffix and lexical features, as well
as some bi-gram and tri-gram context features. Fol-
lowing (Collins, 2002), we do not distinguish rare
words. On set A, Ratnaparkhi?s feature set, our sys-
tem reports an error rate of 3.05% on the develop-
ment data set.
With set B, we include a few feature templates
which are symmetric to those in Ratnaparkhi?s set,
but are only available with bidirectional search. With
set C, we add more bi-gram and tri-gram features.
With set D, we include bi-lexical features. With set
E, we use prefixes and suffixes of length up to 9, as in
(Toutanova et al, 2003; Tsuruoka and Tsujii, 2005).
We obtain 2.72% of error rate. We will use this fea-
ture set on our final experiments on the test data.
Effect of Search and Learning Strategies: For the
second set of experiments, we evaluate the effect of
3http://www.lsi.upc.es/?srlconll/soft.html, package srlconll-
1.1.tgz.
Search Aggressive? Beam=1 Beam=3
L-to-R Yes 2.94 2.82
L-to-R No 3.24 2.75
Bi-Dir Yes 2.84 2.72
Bi-Dir No does not converge
Table 3: Experiments on the development data
search methods, learning strategies, and beam width.
We use feature set E for this set of experiments. Ta-
ble 3 shows the error rates on the development data
set with both left-to-right (L-to-R) and bidirectional
(Bi-Dir) search methods. We also tested both aggres-
sive learning and non-aggressive learning strategies
with beam width of 1 and 3.
First, with non-aggressive learning on bidirec-
tional search, the error rate does not converge to a
comparable number. This is due to the fact that the
search space is too large in bidirectional search, if
we do not use aggressive learning to constrain the
samples for learning.
With aggressive learning, the bidirectional ap-
proach always shows advantages over left-to-right
search. However, the gap is not large. This is
due to the fact that the accuracy of POS tagging
is very high. As a result, we can always keep the
gold-standard tags in the beam even with left-to-right
search in training.
This can also explain why the performance of left-
to-right search with non-aggressive learning is close
to bidirectional search if the beam is large enough.
However, with beam width = 1, non-aggressive
learning over left-to-right search performs much
worse, because in this case it is more likely that the
gold-standard tag is not in the beam.
This set of experiments show that guided learn-
ing is more preferable for tasks with higher ambi-
guities. In our recent work (Shen and Joshi, 2007),
we have applied a variant of this algorithm to depen-
dency parsing, and showed significant improvement
over left-to-right non-aggressive learning strategy.
Comparison: Table 4 shows the comparison with
the previous works on the PTB test sections.
766
System Beam Error%
(Ratnaparkhi, 1996) 5 3.37
(Tsuruoka and Tsujii, 2005) 1 2.90
(Collins, 2002) - 2.89
Guided Learning, feature B 3 2.85
(Tsuruoka and Tsujii, 2005) all 2.85
(Gime?nez and Ma`rquez, 2004) - 2.84
(Toutanova et al, 2003) - 2.76
Guided Learning, feature E 1 2.73
Guided Learning, feature E 3 2.67
Table 4: Comparison with the previous works
According to the experiments shown above, we
build our best system by using feature set E with
beam width B = 3. The number of iterations on
the training data is estimated with respect to the de-
velopment data. We obtain an error rate of 2.67%
on the test data. With deterministic search, or beam
with B = 1, we obtain an error rate of 2.73%.
Compared to previous best result on the same data
set, 2.76% by (Toutanova et al, 2003), our best re-
sult shows a relative error reduction of 3.3%. This
result is very promising, since we have not used any
specially designed features in our experiments. It is
reported in (Toutanova et al, 2003) that a crude com-
pany name detector was used to generate features,
and it gave rise to significant improvement in per-
formance. However, it is difficult for us to duplicate
exactly the same feature for the purpose of compari-
son, although it is convenient to use features like that
in our framework.
5 Conclusions
In this paper, we propose guided learning, a new
learning framework for bidirectional sequence clas-
sification. The tasks of learning the order of infer-
ence and training the local classifier are dynamically
incorporated into a single Perceptron like algorithm.
We apply this novel algorithm to POS tagging. It
obtains an error rate of 2.67% on the standard PTB
test set, which represents 3.3% relative error reduc-
tion over the previous best result (Toutanova et al,
2003) on the same data set, while using fewer fea-
tures. By using deterministic search, it obtains an
error rate of 2.73%, a 5.9% relative error reduction
over the previous best deterministic algorithm (Tsu-
ruoka and Tsujii, 2005). It should be noted that the
error rate is close to the inter-annotator discrepancy
on PTB, the standard test set for POS tagging, there-
fore it is very difficult to achieve improvement.
References
L. Bottou. 1991. Une approche the?orique de l?apprentissage
connexionniste: Applications a` la reconnaissance de la pa-
role. Ph.D. thesis, Universite? de Paris XI.
M. Collins and B. Roark. 2004. Incremental parsing with the
perceptron algorithm. In ACL-2004.
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In EMNLP-2002.
K. Crammer and Y. Singer. 2003. Ultraconservative online
algorithms for multiclass problems. Journal of Machine
Learning Research, 3:951?991.
H. Daume? III and D. Marcu. 2005. Learning as search opti-
mization: Approximate large margin methods for structured
prediction. In ICML-2005.
Y. Freund and R. E. Schapire. 1999. Large margin classifi-
cation using the perceptron algorithm. Machine Learning,
37(3):277?296.
J. Gime?nez and L. Ma`rquez. 2004. Svmtool: A general pos tag-
ger generator based on support vector machines. In LREC-
2004.
W. Krauth and M. Me?zard. 1987. Learning algorithms with
optimal stability in neural networks. Journal of Physics A,
20:745?752.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmentation and la-
beling sequence data. In ICML-2001.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1994.
Building a large annotated corpus of English: The Penn Tree-
bank. Computational Linguistics, 19(2):313?330.
A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tag-
ger. In EMNLP-1996.
G. Satta and O. Stock. 1994. Bi-Directional Context-Free
Grammar Parsing for Natural Language Processing. Artifi-
cial Intelligence, 69(1-2).
L. Shen and A. K. Joshi. 2005. Incremental LTAG Parsing. In
EMNLP-2005.
L. Shen and A. K. Joshi. 2007. Bidirectional LTAG Depen-
dency Parsing. Technical Report 07-02, IRCS, UPenn.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
markov networks. In NIPS-2003.
K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic dependency
network. In NAACL-2003.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional inference
with the easiest-first strategy for tagging sequence data. In
EMNLP-2005.
B. Widrow and M. E. Hoff. 1960. Adaptive switching circuits.
IRE WESCON Convention Record, part 4.
W. Woods. 1976. Parsers in speech understanding systems.
Technical Report 3438, Vol. 4, 1?21, BBN Inc.
767
Proceedings of ACL-08: HLT, pages 604?612,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Optimal k-arization of Synchronous Tree-Adjoining Grammar
Rebecca Nesson
School of Engineering
and Applied Sciences
Harvard University
Cambridge, MA 02138
nesson@seas.harvard.edu
Giorgio Satta
Department of
Information Engineering
University of Padua
I-35131 Padova, Italy
satta@dei.unipd.it
Stuart M. Shieber
School of Engineering
and Applied Sciences
Harvard University
Cambridge, MA 02138
shieber@seas.harvard.edu
Abstract
Synchronous Tree-Adjoining Grammar
(STAG) is a promising formalism for syntax-
aware machine translation and simultaneous
computation of natural-language syntax and
semantics. Current research in both of these
areas is actively pursuing its incorporation.
However, STAG parsing is known to be
NP-hard due to the potential for intertwined
correspondences between the linked nonter-
minal symbols in the elementary structures.
Given a particular grammar, the polynomial
degree of efficient STAG parsing algorithms
depends directly on the rank of the grammar:
the maximum number of correspondences that
appear within a single elementary structure.
In this paper we present a compile-time
algorithm for transforming a STAG into a
strongly-equivalent STAG that optimally
minimizes the rank, k, across the grammar.
The algorithm performs inO(|G|+ |Y | ? L3
G
)
time where L
G
is the maximum number of
links in any single synchronous tree pair in
the grammar and Y is the set of synchronous
tree pairs of G.
1 Introduction
Tree-adjoining grammar is a widely used formal-
ism in natural-language processing due to its mildly-
context-sensitive expressivity, its ability to naturally
capture natural-language argument substitution (via
its substitution operation) and optional modifica-
tion (via its adjunction operation), and the existence
of efficient algorithms for processing it. Recently,
the desire to incorporate syntax-awareness into ma-
chine translation systems has generated interest in
the application of synchronous tree-adjoining gram-
mar (STAG) to this problem (Nesson, Shieber, and
Rush, 2006; Chiang and Rambow, 2006). In a par-
allel development, interest in incorporating seman-
tic computation into the TAG framework has led
to the use of STAG for this purpose (Nesson and
Shieber, 2007; Han, 2006b; Han, 2006a; Nesson
and Shieber, 2006). Although STAG does not in-
crease the expressivity of the underlying formalisms
(Shieber, 1994), STAG parsing is known to be NP-
hard due to the potential for intertwined correspon-
dences between the linked nonterminal symbols in
the elementary structures (Satta, 1992; Weir, 1988).
Without efficient algorithms for processing it, its po-
tential for use in machine translation and TAG se-
mantics systems is limited.
Given a particular grammar, the polynomial de-
gree of efficient STAG parsing algorithms depends
directly on the rank of the grammar: the maximum
number of correspondences that appear within a sin-
gle elementary structure. This is illustrated by the
tree pairs given in Figure 1 in which no two num-
bered links may be isolated. (By ?isolated?, we
mean that the links can be contained in a fragment
of the tree that contains no other links and domi-
nates only one branch not contained in the fragment.
A precise definition is given in section 3.)
An analogous problem has long been known
to exist for synchronous context-free grammars
(SCFG) (Aho and Ullman, 1969). The task of
producing efficient parsers for SCFG has recently
been addressed by binarization or k-arization of
SCFG grammars that produce equivalent grammars
in which the rank, k, has been minimized (Zhang
604
AB
C
D
w
A
B
C
DE F G
1
2
3
4
A
B C
D E F G
A
B C
D
2
3
1
4
1 2 3 4 2 4 31
w ?w w ?x x ? y ?y z z ?
A
B C
D 1
w
3 4
E 2
x
5 A
B C
D
1
3 4E
2
5
w ? x ?
?1 : ?2 : ?3 :
Figure 1: Example of intertwined links that cannot be binarized. No two links can be isolated in both trees in a tree
pair. Note that in tree pair ?
1
, any set of three links may be isolated while in tree pair ?
2
, no group of fewer than four
links may be isolated. In ?
3
no group of links smaller than four may be isolated.
S
V P
V
likes
red candies
aime
les b o n b o n srouges
Det
N P?
S
V P
V N P?
N P
N
N P
NN ?
N
A d j N ?
N
A d j
S
N P V P
J o h n V
likes
J ean
aime
S
N P V P
V
les
Det
N PN P
red
N
A d j
candies
N
b o n b o n s
N
rouges
N
A d j
2
1
2
1 J ean
N PN P
J o h n
N P? 1 N P? 1
likes
J o h n candies
red
1 2
1
( a ) ( b ) ( c )
Figure 2: An example STAG derivation of the English/French sentence pair ?John likes red candies?/?Jean aime les
bonbons rouges?. The figure is divided as follows: (a) the STAG grammar, (b) the derivation tree for the sentence
pair, and (c) the derived tree pair for the sentences.
and Gildea, 2007; Zhang et al, 2006; Gildea, Satta,
and Zhang, 2006). The methods for k-arization
of SCFG cannot be directly applied to STAG be-
cause of the additional complexity introduced by
the expressivity-increasing adjunction operation of
TAG. In SCFG, where substitution is the only avail-
able operation and the depth of elementary struc-
tures is limited to one, the k-arization problem re-
duces to analysis of permutations of strings of non-
terminal symbols. In STAG, however, the arbitrary
depth of the elementary structures and the lack of
restriction to contiguous strings of nonterminals in-
troduced by adjunction substantially complicate the
task.
In this paper we offer the first algorithm address-
ing this problem for the STAG case. We present
a compile-time algorithm for transforming a STAG
into a strongly-equivalent STAG that optimally min-
imizes k across the grammar. This is a critical mini-
mization because k is the feature of the grammar that
appears in the exponent of the complexity of parsing
algorithms for STAG. Following the method of Seki
et al (1991), an STAG parser can be implemented
with complexity O(n4?(k+1) ? |G|). By minimizing
k, the worst-case complexity of a parser instanti-
ated for a particular grammar is optimized. The k-
arization algorithm performs in O(|G|+ |Y | ? L3G)
time where LG is the maximum number of links in
any single synchronous tree pair in the grammar and
Y is the set of synchronous tree pairs of G. By com-
parison, a baseline algorithm performing exhaustive
search requires O(|G|+ |Y | ? L6G) time.
1
The remainder of the paper proceeds as follows.
In section 2 we provide a brief introduction to the
STAG formalism. We present the k-arization algo-
rithm in section 3 and an analysis of its complexity
in section 4. We prove the correctness of the algo-
rithm in section 5.
1In a synchronous tree pair with L links, there are O(L4)
pairs of valid fragments. It takes O(L) time to check if the two
components in a pair have the same set of links. Once the syn-
chronous fragment with the smallest number of links is excised,
this process iterates at most L times, resulting in time O(L6G).
605
DE F
A
B
C
1
2
3 4
y z
5
H
I J2 3
1
NM 4
w ? x ?
5
L
y ?
K
? :
x
G
z ?
n 1 :
n 2 :
n 3 :
n 4 :
n 5 :
Figure 3: A synchronous tree pair containing frag-
ments ?
L
= ?
L
(n
1
, n
2
) and ?
R
= ?
R
(n
3
). Since
links(n
1
, n
2
) = links(n
3
) = { 2 , 4 , 5}, we can de-
fine synchronous fragment ? = ??
L
, ?
R
?. Note also
that node n
3
is a maximal node and node n
5
is not.
?(n
1
) = 2 5 5 3 3 2 4 4 ; ?(n
3
) = 2 5 5 4 4 2 .
2 Synchronous Tree-Adjoining Grammar
A tree-adjoining grammar (TAG) consists of a set of
elementary tree structures of arbitrary depth, which
are combined by substitution, familiar from context-
free grammars, or an operation of adjunction that is
particular to the TAG formalism. Auxiliary trees
are elementary trees in which the root and a frontier
node, called the foot node and distinguished by the
diacritic ?, are labeled with the same nonterminalA.
The adjunction operation involves splicing an auxil-
iary tree in at an internal node in an elementary tree
also labeled with nonterminal A. Trees without a
foot node, which serve as a base for derivations, are
called initial trees. For further background, refer to
the survey by Joshi and Schabes (1997).
We depart from the traditional definition in nota-
tion only by specifying adjunction and substitution
sites explicitly with numbered links. Each link may
be used only once in a derivation. Operations may
only occur at nodes marked with a link. For sim-
plicity of presentation we provisionally assume that
only one link is permitted at a node. We later drop
this assumption.
In a synchronous TAG (STAG) the elementary
structures are ordered pairs of TAG trees, with a
linking relation specified over pairs of nonterminal
nodes. Each link has two locations, one in the left
tree in a pair and the other in the right tree. An ex-
ample of an STAG derivation including both substi-
tution and adjunction is given in Figure 2. For fur-
ther background, refer to the work of Shieber and
Schabes (1990) and Shieber (1994).
3 k-arization Algorithm
For a synchronous tree pair ? = ??L, ?R?, a frag-
ment of ?L (or ?R) is a complete subtree rooted at
some node n of ?L, written ?L(n), or else a subtree
rooted at n with a gap at node n?, written ?L(n, n?);
see Figure 3 for an example. We write links(n) and
links(n, n?) to denote the set of links of ?L(n) and
?L(n, n
?
), respectively. When we do not know the
root or gap nodes of some fragment ?L, we also
write links(?L).
We say that a set of links ? from ? can be iso-
lated if there exist fragments ?L and ?R of ?L
and ?R, respectively, both with links ?. If this is
the case, we can construct a synchronous fragment
? = ??L, ?R?. The goal of our algorithm is to de-
compose ? into synchronous fragments such that the
maximum number of links of a synchronous frag-
ment is kept to a minimum, and ? can be obtained
from the synchronous fragments by means of the
usual substitution and adjunction operations. In or-
der to simplify the presentation of our algorithm we
assume, without any loss of generality, that all ele-
mentary trees of the source STAG have nodes with
at most two children.
3.1 Maximal Nodes
A node n of ?L (or ?R) is called maximal if
(i) links(n) 6= ?, and (ii) it is either the root node
of ?L or, for its parent node n?, we have links(n?) 6=
links(n). Note that for every node n? of ?L such
that links(n?) 6= ? there is always a unique maxi-
mal node n such that links(n?) = links(n). Thus,
for the purpose of our algorithm, we need only look
at maximal nodes as places for excising tree frag-
ments. We can show that the number of maxi-
mal nodes Mn in a subtree ?L(n) always satisfies
|links(n)| ?Mn ? 2? |links(n)| ? 1.
Let n be some node of ?L, and let l(n) be the
(unique) link impinging on n if such a link exists,
and l(n) = ? otherwise. We associate n with a
string ?(n), defined by a pre- and post-order traver-
sal of fragment ?L(n). The symbols of ?(n) are the
links in links(n), viewed as atomic symbols. Given
a node n with p children n1, . . . , np, 0 ? p ? 2,
we define ?(n) = l(n)?(n1) ? ? ??(np) l(n). See
again Figure 3 for an example. Note that |?(n)| =
2? |links(n)|.
606
31
1
1
1
2
2
2
2
X X X X
R
R
R
R
R
R
G
G
G
G
G
G
X
?
X
?
X
?
?
X
?
X
?
excise adjoin transform
?
L
:
n
1
:
n
2
:
Figure 4: A diagram of the tree transformation performed
when fragment ?
L
(n
1
, n
2
) is removed. In this and the
diagrams that follow, patterned or shaded triangles rep-
resent segments of the tree that contain multiple nodes
and at least one link. Where the pattern or shading corre-
sponds across trees in a tree pair, the set of links contained
within those triangles are equivalent.
3.2 Excision of Synchronous Fragments
Although it would be possible to excise synchronous
fragments without creating new nonterminal nodes,
for clarity we present a simple tree transforma-
tion when a fragment is excised that leaves exist-
ing nodes intact. A schematic depiction is given in
Figure 4. In the figure, we demonstrate the exci-
sion process on one half of a synchronous fragment:
?L(n1, n2) is excised to form two new trees. The
excised tree is not processed further. In the exci-
sion process the root and gap nodes of the original
tree are not altered. The material between them is
replaced with a single new node with a fresh non-
terminal symbol and a fresh link number. This non-
terminal node and link form the adjunction or sub-
stitution site for the excised tree. Note that any link
impinging on the root node of the excised fragment
is by our convention included in the fragment and
any link impinging on the gap node is not.
To regenerate the original tree, the excised frag-
ment can be adjoined or substituted back into the
tree from which it was excised. The new nodes that
were generated in the excision may be removed and
the original root and gap nodes may be merged back
together retaining any impinging links, respectively.
Note that if there was a link on either the root or gap
node in the original tree, it is not lost or duplicated
1 1 0 0 0 0 0 0 0 0 1
2 0 1 0 0 0 0 1 0 1 0
5 0 0 1 1 0 0 0 0 0 0
5 0 0 1 1 0 0 0 0 0 0
3 0 0 0 0 0 0 0 1 1 0
3 0 0 0 0 0 0 0 1 1 0
2 0 1 0 0 0 0 1 0 0 0
4 0 0 0 0 1 1 0 0 0 0
4 0 0 0 0 1 1 0 0 0 0
1 1 0 0 0 0 0 0 0 0 1
1 2 5 5 4 4 2 3 3 1
0
Figure 5: Table pi with synchronous fragment
??
L
(n
1
, n
2
), ?
R
(n
3
)? from Figure 3 highlighted.
in the process.
3.3 Method
Let nL and nR be the root nodes of trees ?L and ?R,
respectively. We know that links(nL) = links(nR),
and |?(nL)| = |?(nR)|, the second string being a
rearrangement of the occurrences of symbols in the
first one. The main data structure of our algorithm is
a Boolean matrix pi of size |?(nL)|?|?(nL)|, whose
rows are addressed by the occurrences of symbols in
?(nL), in the given order, and whose columns are
similarly addressed by ?(nR). For occurrences of
links x1 , x2 , the element of pi at a row addressed by
x1 and a column addressed by x2 is 1 if x1 = x2,
and 0 otherwise. Thus, each row and column of pi
has exactly two non-zero entries. See Figure 5 for
an example.
For a maximal node n1 of ?L, we let pi(n1) de-
note the stripe of adjacent rows of pi addressed by
substring ?(n1) of ?(nL). If n1 dominates n2 in ?L,
we let pi(n1, n2) denote the rows of pi addressed by
?(n1) but not by ?(n2). This forms a pair of hori-
zontal stripes in pi. For nodes n3, n4 of ?R, we sim-
ilarly define pi(n3) and pi(n3, n4) as vertical stripes
of adjacent columns. See again Figure 5.
Our algorithm is reported in Figure 6. For each
synchronous tree pair ? = ??L, ?R? from the in-
put grammar, we maintain an agenda B with all
candidate fragments ?L from ?L having at least
two links. These fragments are processed greed-
ily in order of increasing number of links. The
function ISOLATE(), described in more detail be-
607
1: Function KARIZE(G) {G a binary STAG}
2: G? ? STAG with empty set of synch trees;
3: for all ? = ??L, ?R? in G do
4: init pi and B;
5: while B 6= ? do
6: ?L ? next fragment from B;
7: ?R ? ISOLATE(?L, pi, ?R);
8: if ?R 6= null then
9: add ??L, ?R? to G?;
10: ? ? excise ??L, ?R? from ?;
11: update pi and B;
12: add ? to G?;
13: return G?
Figure 6: Main algorithm.
low, looks for a right fragment ?R with the same
links as ?L. Upon success, the synchronous frag-
ment ? = ??L, ?R? is added to the output grammar.
Furthermore, we excise ? from ? and update data
structures pi and B. The above process is iterated
until B becomes empty. We show in section 5 that
this greedy strategy is sound and complete.
The function ISOLATE() is specified in Figure 7.
We take as input a left fragment ?L, which is asso-
ciated with one or two horizontal stripes in pi, de-
pending on whether ?L has a gap node or not. The
left boundary of ?L in pi is the index x1 of the col-
umn containing the leftmost occurrence of a 1 in the
horizontal stripes associated with ?L. Similarly, the
right boundary of ?L in pi is the index x2 of the col-
umn containing the rightmost occurrence of a 1 in
these stripes. We retrieve the shortest substring ?(n)
of ?(nR) that spans over indices x1 and x2. This
means that n is the lowest node from ?R such that
the links of ?L are a subset of the links of ?R(n).
If the condition at line 3 is satisfied, all of the ma-
trix entries of value 1 that are found from column
x1 to column x2 fall within the horizontal stripes
associated with ?L. In this case we can report the
right fragment ?R = ?R(n). Otherwise, we check
whether the entries of value 1 that fall outside of
the two horizontal stripes in between columns x1
and x2 occur within adjacent columns, say from col-
umn x3 ? x1 to column x4 ? x2. In this case,
we check whether there exists some node n? such
that the substring of ?(n) from position x3 to x4 is
1: Function ISOLATE(?L, pi, ?R)
2: select n ? ?R such that ?(n) is the shortest
string within ?(nR) including left/right bound-
aries of ?L in pi;
3: if |?(n)| = 2? |links(?L)| then
4: return ?R(n);
5: select n? ? ?R such that ?(n?) is the gap string
within ?(n) for which links(n) ? links(n?) =
links(?L);
6: if n? is not defined then
7: return null; {more than one gap}
8: return ?R(n, n?);
Figure 7: Find synchronous fragment.
an occurrence of string ?(n?). This means that n?
is the gap node, and we report the right fragment
?L = ?R(n, n
?
). See again Figure 5.
We now drop the assumption that only one link
may impinge on a node. When multiple links im-
pinge on a single node n, l(n) is an arbitrary order
over those links. In the execution of the algorithm,
any stripe that contains one link in l(n) it must in-
clude every link in l(n). This prevents the excision
of a proper subset of the links at any node. This pre-
serves correctness because excising any proper sub-
set would impose an order over the links at n that
is not enforced in the input grammar. Because the
links at a node are treated as a unit, the complexity
of the algorithm is not affected.
4 Complexity
We discuss here an implementation of the algo-
rithm of section 3 resulting in time complexity
O(|G|+ |Y | ? L3G), where Y is the set of syn-
chronous tree pairs of G and LG is the maximum
number of links in a synchronous tree pair in Y .
Consider a synchronous tree pair ? = ??L, ?R?
with L links. If M is the number of maximal nodes
in ?L or ?R, we have M = ?(L) (Section 3.1). We
implement the sparse table pi inO(L) space, record-
ing for each row and column the indices of its two
non-zero entries. We also assume that we can go
back and forth between maximal nodes n and strings
?(n) in constant time. Here each ?(n) is represented
by its boundary positions within ?(nL) or ?(nR),
nL and nR the root nodes of ?L and ?R, respectively.
608
At line 2 of the function ISOLATE() (Figure 7) we
retrieve the left and right boundaries by scanning the
rows of pi associated with input fragment ?L. We
then retrieve node n by visiting all maximal nodes
of ?L spanning these boundaries. Under the above
assumptions, this can be done in time O(L). In a
similar way we can implement line 5, resulting in
overall run time O(L) for function ISOLATE().
In the function KARIZE() (Figure 6) we use buck-
ets Bi, 1 ? i ? L, where each Bi stores the candi-
date fragments ?L with |links(?L)| = i. To populate
these buckets, we first process fragments ?L(n) by
visiting bottom up the maximal nodes of ?L. The
quantity |links(n)| is computed from the quantities
|links(ni)|, where ni are the highest maximal nodes
dominated by n. (There are at most two such nodes.)
Fragments ?L(n, n?) can then be processed using
the relation |links(n, n?)| = |links(n)| ? |links(n?)|.
In this way each fragment is processed in constant
time, and population of all the buckets takes O(L2)
time.
We now consider the while loop at lines 5 to 11 in
function KARIZE(). For a synchronous tree pair ?,
the loop iterates once for each candidate fragment
?L in some bucket. We have a total of O(L2) it-
erations, since the initial number of candidates in
the buckets is O(L2), and the possible updating of
the buckets after a synchronous fragment is removed
does not increase the total size of all the buckets. If
the links in ?L cannot be isolated, one iteration takes
time O(L) (the call to function ISOLATE()). If the
links in ?L can be isolated, then we need to restruc-
ture pi and to repopulate the buckets. The former
can be done in time O(L) and the latter takes time
O(L2), as already discussed. Crucially, the updat-
ing of pi and the buckets takes place no more than
L ? 1 times. This is because each time we excise
a synchronous fragment, the number of links in ? is
reduced by at least one.
We conclude that function KARIZE() takes time
O(L3) for each synchronous tree ?, and the total
running time is O(|G|+ |Y | ? L3G), where Y is the
set of synchronous tree pairs of G. The term |G| ac-
counts for the reading of the input, and dominates
the complexity of the algorithm only in case there
are very few links in each synchronous tree pair.
A
B C
D 1
w
3 4
E 2
x
5
B
D 1
w
3
6
n 1 :
n 2 :
n 3 :
n 4 :
? : ?
?
:
A?
A
Figure 8: In ? links 3 and 5 cannot be isolated because
the fragment would have to contain two gaps. However,
after the removal of fragment ?(n
1
, n
2
), an analogous
fragment ??(n
3
, n
4
) may be removed.
5 Proof of Correctness
The algorithm presented in the previous sections
produces an optimal k-arization for the input gram-
mar. In this section we sketch a proof of correctness
of the strategy employed by the algorithm.2
The k-arization strategy presented above is
greedy in that it always chooses the excisable frag-
ment with the smallest number of links at each step
and does not perform any backtracking. We must
therefore show that this process cannot result in a
non-optimal solution. If fragments could not overlap
each other, this would be trivial to show because the
excision process would be confluent. If all overlap-
ping fragments were cases of complete containment
of one fragment within another, the proof would also
be trivial because the smallest-to-largest excision or-
der would guarantee optimality. However, it is pos-
sible for fragments to partially overlap each other,
meaning that the intersection of the set of links con-
tained in the two fragments is non-empty and the dif-
ference between the set of links in one fragment and
the other is also non-empty. Overlapping fragment
configurations are given in Figure 9 and discussed in
detail below.
The existence of partially overlapping fragments
complicates the proof of optimality for two reasons.
First, the excision of a fragment ? that is partially
overlapped with another fragment ? necessarily pre-
cludes the excision of ? at a later stage in the ex-
2Note that the soundness of the algorithm can be easily veri-
fied from the fact that the removal of fragments can be reversed
by performing standard STAG adjunction and substitution oper-
ations until a single STAG tree pair is produced. This tree pair
is trivially homomorphic to the original tree pair and can easily
be mapped to the original tree pair.
609
(1, 1?)
[ [
A
B
C
D
n 1 :
n 2 :
n 3 :
n 4 :
A
B C
n 5 :
n 6 : n 7 :
A
B
C D
n 8 :
n 9 :
n 10 : n 11 :
(2) (3 )
Figure 9: The four possible configurations of overlapped
fragments within a single tree. For type 1, let ? =
?(n
1
, n
3
) and ? = ?(n
2
, n
4
). The roots and gaps of the
fragments are interleaved. For type 1?, let ? = ?(n
1
, n
3
)
and ? = ?(n
2
). The root of ? dominates the gap of ?.
For type 2, let ? = ?(n
5
, n
6
) and ? = ?(n
5
, n
7
). The
fragments share a root and have gap nodes that do not
dominate each other. For type 3 let ? = ?(n
8
, n
10
) and
? = ?(n
9
, n
11
). The root of ? dominates the root of ?,
both roots dominate both gaps, but neither gap dominates
the other.
cision process. Second, the removal of a fragment
may cause a previously non-isolatable set of links to
become isolatable, effectively creating a new frag-
ment that may be advantageous to remove. This is
demonstrated in Figure 8. These possibilities raise
the question of whether the choice between remov-
ing fragments ? and ? may have consequences at a
later stage in the excision process. We demonstrate
that this choice cannot affect the k found for a given
grammar.
We begin by sketching the proof of a lemma that
shows that removal of a fragment ? that partially
overlaps another fragment ? always leaves an anal-
ogous fragment that may be removed.
5.1 Validity Preservation
Consider a STAG tree pair ? containing the set of
links ? and two synchronous fragments ? and ?
with ? containing links links(?) and ? containing
links(?) (links(?), links(?) ( ?).
If ? and ? do not overlap, the removal of ? is
defined as validity preserving with respect to ?.
If ? and ? overlap, removal of ? from ? is valid-
ity preserving with respect to ? if after the removal
there exists a valid synchronous fragment (contain-
ing at most one gap on each side) that contains all
and only the links (links(?)? links(?))?{x}where
x is the new link added to ?.
remove ?
remove ?
A
B
C
D
E
F G
n 1 :
n 2 :
n 3 :
n 4 :
n 5 :
n 6 : n 7 :
An 1 : En 5 :
Cn 3 :
x x
Dn 4 :
Fn 6 :
H I
An 1 :
Bn 2 :
J x
Dn 4 :
En 5 :
K x
Dn 4 :
Figure 10: Removal from a tree pair ? containing type 1?
type 2 fragment overlap. The fragment ? is represented
by the horizonal-lined pieces of the tree pair. The frag-
ment ? is represented by the vertical-lined pieces of the
tree pair. Cross-hatching indicates the overlapping por-
tion of the two fragments.
We prove a lemma that removal of any syn-
chronous fragment from an STAG tree pair is va-
lidity preserving with respect to all of the other syn-
chronous fragments in the tree pair.
It suffices to show that for two arbitrary syn-
chronous fragments ? and ?, the removal of ? is
validity preserving with respect to ?. We show this
by examination of the possible configurations of ?
and ?.
Consider the case in which ? is fully contained
within ?. In this case links(?) ( links(?). The re-
moval of ? leaves the root and gap of ? intact in both
trees in the pair, so it remains a valid fragment. The
new link is added at the new node inserted where
? was removed. Since ? is fully contained within
?, this node is below the root of ? but not below
its gap. Thus, the removal process leaves ? with the
links (links(?)?links(?))?{x}, where x is the link
added in the removal process; the removal is validity
preserving.
Synchronous fragments may partially overlap in
several different ways. There are four possible con-
figurations for an overlapped fragment within a sin-
gle tree, depicted in Figure 9. These different single-
tree overlap types can be combined in any way to
form valid synchronous fragments. Due to space
constraints, we consider two illustrative cases and
leave the remainder as an exercise to the reader.
An example of removing fragments from
a tree set containing type 1?type 2 over-
lapped fragments is given in Figure 10.
Let ? = ??L(n1, n3), ?R(n5, n6)?. Let
610
? = ??L(n2, n4), ?R(n5, n7)?. If ? is re-
moved, the validity preserving fragment for ? is
???L(n1, n4), ?
?
R(n5)?. It contains the links in the
vertical-lined part of the tree and the new link x .
This forms a valid fragment because both sides con-
tain at most one gap and both contain the same set
of links. In addition, it is validity preserving for ?
because it contains exactly the set of links that were
in links(?) and not in links(?) plus the new link
x . If we instead choose to remove ?, the validity
preserving fragment for ? is ???L(n1, n4), ?
?
R(n5)?.
The links in each side of this fragment are the same,
each side contains at most one gap, and the set of
links is exactly the set left over from links(?) once
links(?) is removed plus the newly generated link x .
An example of removing fragments from a tree
set containing type 1??type 3 (reversed) overlapped
fragments is given in Figure 11. If ? is re-
moved, the validity preserving fragment for ? is
???L(n1), ?
?
R(n4)?. If ? is removed, the validity pre-
serving fragment for ? is ???L(n1, n8), ?
?
R(n4)?.
Similar reasoning follows for all remaining types
of overlapped fragments.
5.2 Proof Sketch
We show that smallest-first removal of fragments is
optimal. Consider a decision point at which a choice
is made about which fragment to remove. Call the
size of the smallest fragments at this pointm, and let
the set of fragments of size m be X with ?, ? ? X .
There are two cases to consider. First, consider
two partially overlapped fragments ? ? X and
? /? X . Note that |links(?)| < |links(?)|. Valid-
ity preservation of ? with respect to ? guarantees
that ? or its validity preserving analog will still be
available for excision after ? is removed. Excising
? increases k more than excising ? or any fragment
that removal of ? will lead to before ? is considered.
Thus, removal of ? cannot result in a smaller value
for k if it is removed before ? rather than after ?.
Second, consider two partially overlapped frag-
ments ?, ? ? X . Due to the validity preservation
lemma, we may choose arbitrarily between the frag-
ments in X without jeopardizing our ability to later
remove other fragments (or their validity preserving
analogs) in that set. Removal of fragment ? cannot
increase the size of any remaining fragment.
Removal of ? or ? may generate new fragments
remove ?
remove ?
A
B
C
n 1 :
n 2 :
n 3 :
E
F G
n 5 :
n 6 : n 7 :
Dn 4 : An 1 :
Cn 3 :
xH En 5 :
x
Fn 6 :
I
Dn 4 : An 1 :
Bn 2 :
xJ ?
Dn 4 :
K x
Gn 7 :
n 8 :
Figure 11: Removal from a tree pair ? containing a type
1
??type 3 (reversed) fragment overlap. The fragment ? is
represented by the horizontal lined pieces of the tree pair.
The fragment ? is represented by the vertical-lined pieces
of the tree pair. Cross-hatching indicates the overlapping
portion of the two fragments.
that were not previously valid and may reduce the
size of existing fragments that it overlaps. In addi-
tion, removal of ?may lead to availability of smaller
fragments at the next removal step than removal of ?
(and vice versa). However, since removal of either ?
or ? produces a k of size at leastm, the later removal
of fragments of size less than m cannot affect the k
found by the algorithm. Due to validity preservation,
removal of any of these smaller fragments will still
permit removal of all currently existing fragments or
their analogs at a later step in the removal process.
If the removal of ? generates a new fragment ? of
size larger thanm all remaining fragments inX (and
all others smaller than ?) will be removed before ?
is considered. Therefore, if removal of ? generates a
new fragment smaller than ?, the smallest-first strat-
egy will properly guarantee its removal before ?.
6 Conclusion
In order for STAG to be used in machine translation
and other natural-language processing tasks it must
be possible to process it efficiently. The difficulty in
parsing STAG stems directly from the factor k that
indicates the degree to which the correspondences
are intertwined within the elementary structures of
the grammar. The algorithm presented in this pa-
per is the first method available for k-arizing a syn-
chronous TAG grammar into an equivalent grammar
with an optimal value for k. The algorithm operates
offline and requires only O(|G|+ |Y | ? L3G) time.
Both the derivation trees and derived trees produced
are trivially homomorphic to those that are produced
by the original grammar.
611
References
Aho, Alfred V. and Jeffrey D. Ullman. 1969. Syntax di-
rected translations and the pushdown assembler. Jour-
nal of Computer and System Sciences, 3(1):37?56.
Chiang, David and Owen Rambow. 2006. The hid-
den TAG model: synchronous grammars for parsing
resource-poor languages. In Proceedings of the 8th
International Workshop on Tree Adjoining Grammars
and Related Formalisms (TAG+ 8), pages 1?8.
Gildea, Daniel, Giorgio Satta, and Hao Zhang. 2006.
Factoring synchronous grammars by sorting. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics and the Association for Computa-
tional Linguistics (COLING/ACL-06), July.
Han, Chung-Hye. 2006a. Pied-piping in relative clauses:
Syntax and compositional semantics based on syn-
chronous tree adjoining grammar. In Proceedings
of the 8th International Workshop on Tree Adjoining
Grammars and Related Formalisms (TAG+ 8), pages
41?48, Sydney, Australia.
Han, Chung-Hye. 2006b. A tree adjoining grammar
analysis of the syntax and semantics of it-clefts. In
Proceedings of the 8th International Workshop on Tree
Adjoining Grammars and Related Formalisms (TAG+
8), pages 33?40, Sydney, Australia.
Joshi, Aravind K. and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Sa-
lomaa, editors, Handbook of Formal Languages.
Springer, pages 69?124.
Nesson, Rebecca and Stuart M. Shieber. 2006. Sim-
pler TAG semantics through synchronization. In Pro-
ceedings of the 11th Conference on Formal Grammar,
Malaga, Spain, 29?30 July.
Nesson, Rebecca and Stuart M. Shieber. 2007. Extrac-
tion phenomena in synchronous TAG syntax and se-
mantics. In Proceedings of Syntax and Structure in
Statistical Translation (SSST), Rochester, NY, April.
Nesson, Rebecca, Stuart M. Shieber, and Alexander
Rush. 2006. Induction of probabilistic synchronous
tree-insertion grammars for machine translation. In
Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas (AMTA
2006), Boston, Massachusetts, 8-12 August.
Satta, Giorgio. 1992. Recognition of linear context-free
rewriting systems. In Proceedings of the 10th Meet-
ing of the Association for Computational Linguistics
(ACL92), pages 89?95, Newark, Delaware.
Seki, H., T. Matsumura, M. Fujii, and T. Kasami. 1991.
On multiple context-free grammars. Theoretical Com-
puter Science, 88:191?229.
Shieber, Stuart M. 1994. Restricting the weak-generative
capacity of synchronous tree-adjoining grammars.
Computational Intelligence, 10(4):371?385, Novem-
ber.
Shieber, Stuart M. and Yves Schabes. 1990. Syn-
chronous tree adjoining grammars. In Proceedings of
the 13th International Conference on Computational
Linguistics (COLING ?90), Helsinki, August.
Weir, David. 1988. Characterizing mildly context-
sensitive grammar formalisms. PhD Thesis, Depart-
ment of Computer and Information Science, Univer-
sity of Pennsylvania.
Zhang, Hao and Daniel Gildea. 2007. Factorization of
synchronous context-free grammars in linear time. In
NAACL Workshop on Syntax and Structure in Statisti-
cal Translation (SSST), April.
Zhang, Hao, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference/North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
612
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 985?993,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
An Optimal-Time Binarization Algorithm
for Linear Context-Free Rewriting Systems with Fan-Out Two
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a, Spain
cgomezr@udc.es
Giorgio Satta
Department of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
Linear context-free rewriting systems
(LCFRSs) are grammar formalisms with
the capability of modeling discontinu-
ous constituents. Many applications use
LCFRSs where the fan-out (a measure of
the discontinuity of phrases) is not allowed
to be greater than 2. We present an ef-
ficient algorithm for transforming LCFRS
with fan-out at most 2 into a binary form,
whenever this is possible. This results
in asymptotical run-time improvement for
known parsing algorithms for this class.
1 Introduction
Since its early years, the computational linguistics
field has devoted much effort to the development
of formal systems for modeling the syntax of nat-
ural language. There has been a considerable in-
terest in rewriting systems that enlarge the generat-
ive power of context-free grammars, still remain-
ing far below the power of the class of context-
sensitive grammars; see (Joshi et al, 1991) for dis-
cussion. Following this line, (Vijay-Shanker et al,
1987) have introduced a formalism called linear
context-free rewriting systems (LCFRSs) that has
received much attention in later years by the com-
munity.
LCFRSs allow the derivation of tuples of
strings,1 i.e., discontinuous phrases, that turn out
to be very useful in modeling languages with rel-
atively free word order. This feature has recently
been used for mapping non-projective depend-
ency grammars into discontinuous phrase struc-
tures (Kuhlmann and Satta, 2009). Furthermore,
LCFRSs also implement so-called synchronous
1In its more general definition, an LCFRS provides a
framework where abstract structures can be generated, as for
instance trees and graphs. Throughout this paper we focus on
so-called string-based LCFRSs, where rewriting is defined
over strings only.
rewriting, up to some bounded degree, and have
recently been exploited, in some syntactic vari-
ant, in syntax-based machine translation (Chiang,
2005; Melamed, 2003) as well as in the modeling
of syntax-semantic interface (Nesson and Shieber,
2006).
The maximum number f of tuple components
that can be generated by an LCFRS G is called
the fan-out of G, and the maximum number r of
nonterminals in the right-hand side of a production
is called the rank of G. As an example, context-
free grammars are LCFRSs with f = 1 and r
given by the maximum length of a production
right-hand side. Tree adjoining grammars (Joshi
and Levy, 1977), or TAG for short, can be viewed
as a special kind of LCFRS with f = 2, since
each elementary tree generates two strings, and r
given by the maximum number of adjunction sites
in an elementary tree.
Several parsing algorithms for LCFRS or equi-
valent formalisms are found in the literature; see
for instance (Seki et al, 1991; Boullier, 2004; Bur-
den and Ljunglo?f, 2005). All of these algorithms
work in time O(|G| ? |w|f ?(r+1)). Parsing time is
then exponential in the input grammar size, since
|G| depends on both f and r. In the develop-
ment of efficient algorithms for parsing based on
LCFRS the crucial goal is therefore to optimize
the term f ? (r + 1).
In practical natural language processing applic-
ations the fan-out of the grammar is typically
bounded by some small number. As an example,
in the case of discontinuous parsing discussed
above, we have f = 2 for most practical cases.
On the contrary, LCFRS productions with a rel-
atively large number of nonterminals are usually
observed in real data. The reduction of the rank of
a LCFRS, called binarization, is a process very
similar to the reduction of a context-free grammar
into Chomsky normal form. While in the special
case of CFG and TAG this can always be achieved,
985
binarization of an LCFRS requires, in the gen-
eral case, an increase in the fan-out of the gram-
mar much larger than the achieved reduction in
the rank. Worst cases and some lower bounds have
been discussed in (Rambow and Satta, 1999; Satta,
1998).
Nonetheless, in many cases of interest binariza-
tion of an LCFRS can be carried out without any
extra increase in the fan-out. As an example, in
the case where f = 2, binarization of a LCFRS
would result in parsing time of O(|G| ? |w|6).
With the motivation of parsing efficiency, much
research has been recently devoted to the design
of efficient algorithms for rank reduction, in cases
in which this can be carried out at no extra increase
in the fan-out. (Go?mez-Rodr??guez et al, 2009) re-
ports a general binarization algorithm for LCFRS.
In the case where f = 2, this algorithm works
in time O(|p|7), where p is the input production.
A more efficient algorithm is presented in (Kuhl-
mann and Satta, 2009), working in time O(|p|) in
case of f = 2. However, this algorithm works
for a restricted typology of productions, and does
not cover all cases in which some binarization is
possible. Other linear time algorithms for rank re-
duction are found in the literature (Zhang et al,
2008), but they are restricted to the case of syn-
chronous context-free grammars, a strict subclass
of the LCFRS with f = 2.
In this paper we focus our attention on LCFRS
with a fan-out of two. We improve upon all
of the above mentioned results, by providing
an algorithm that computes a binarization of an
LCFRS production in all cases in which this is
possible and works in time O(|p|). This is an
optimal result in terms of time complexity, since
?(|p|) is also the size of any output binarization
of an LCFRS production.
2 Linear context-free rewriting systems
We briefly summarize here the terminology and
notation that we adopt for LCFRS; for detailed
definitions, see (Vijay-Shanker et al, 1987). We
denote the set of non-negative integers by N. For
i, j ? N, the interval {k | i ? k ? j} is denoted
by [i, j]. We write [i] as a shorthand for [1, i]. For
an alphabet V , we write V ? for the set of all (fi-
nite) strings over V .
As already mentioned in Section 1, linear
context-free rewriting systems generate tuples of
strings over some finite alphabet. This is done by
associating each production p of a grammar with
a function g that rearranges the string compon-
ents in the tuples generated by the nonterminals
in p?s right-hand side, possibly adding some al-
phabet symbols. Let V be some finite alphabet.
For natural numbers r ? 0 and f, f1, . . . , fr ? 1,
consider a function g : (V ?)f1 ? ? ? ? ? (V ?)fr ?
(V ?)f defined by an equation of the form
g(?x1,1, . . . , x1,f1?, . . . , ?xr,1, . . . , xr,fr?) = ~?,
where ~? = ??1, . . . , ?f ? is an f -tuple of strings
over g?s argument variables and symbols in V . We
say that g is linear, non-erasing if ~? contains ex-
actly one occurrence of each argument variable.
We call r and f the rank and the fan-out of g, re-
spectively, and write r(g) and f(g) to denote these
quantities.
A linear context-free rewriting system
(LCFRS) is a tuple G = (VN , VT , P, S), where
VN and VT are finite, disjoint alphabets of nonter-
minal and terminal symbols, respectively. Each
A ? VN is associated with a value f(A), called its
fan-out. The nonterminal S is the start symbol,
with f(S) = 1. Finally, P is a set of productions
of the form
p : A? g(A1, A2, . . . , Ar(g)) ,
where A,A1, . . . , Ar(g) ? VN , and g : (V
?
T )
f(A1)
? ? ? ?? (V ?T )
f(Ar(g)) ? (V ?T )
f(A) is a linear, non-
erasing function.
A production p of G can be used to transform
a sequence of r(g) string tuples generated by the
nonterminals A1, . . . , Ar(g) into a tuple of f(A)
strings generated by A. The values r(g) and f(g)
are called the rank and fan-out of p, respectively,
written r(p) and f(p). The rank and fan-out of G,
written r(G) and f(G), respectively, are the max-
imum rank and fan-out among all of G?s produc-
tions. Given that f(S) = 1, S generates a set of
strings, defining the language of G.
Example 1 Consider the LCFRS G defined by
the productions
p1 : S ? g1(A), g1(?x1,1, x1,2?) = ?x1,1x1,2?
p2 : A? g2(A), g2(?x1,1, x1,2?) = ?ax1,1b, cx1,2d?
p3 : A? g3(), g3() = ??, ??
We have f(S) = 1, f(A) = f(G) = 2, r(p3) = 0
and r(p1) = r(p2) = r(G) = 1. G generates
the string language {anbncndn |n ? N}. For in-
stance, the string a3b3c3d3 is generated by means
986
of the following bottom-up process. First, the
tuple ??, ?? is generated by A through p3. We
then iterate three times the application of p2 to
??, ??, resulting in the tuple ?a3b3, c3d3?. Finally,
the tuple (string) ?a3b3c3d3? is generated by S
through application of p1. 2
3 Position sets and binarizations
Throughout this section we assume an LCFRS
production p : A? g(A1, . . . , Ar) with g defined
through a tuple ~? as in section 2. We also assume
that the fan-out ofA and the fan-out of eachAi are
all bounded by two.
3.1 Production representation
We introduce here a specialized representation for
p. Let $ be a fresh symbol that does not occur
in p. We define the characteristic string of p as
the string
?N (p) = ?
?
1$?
?
2$ ? ? ? $?
?
f(A),
where each ??j is obtained from ?j by removing all
the occurrences of symbols in VT . Consider now
some occurrence Ai of a nonterminal symbol in
the right-hand side of p. We define the position set
of Ai, written XAi , as the set of all non-negative
integers j ? [|?N (p)|] such that the j-th symbol in
?N (p) is a variable of the form xi,h for some h.
Example 2 Let p : A ? g(A1, A2, A3), where
g(?x1,1, x1,2?, ?x2,1?, ?x3,1, x3,2?) = ~? with
~? = ?x1,1ax2,1x1,2, x3,1bx3,2? .
We have ?N (p) = x1,1x2,1x1,2$x3,1x3,2, XA1 =
{1, 3}, XA2 = {2} and XA3 = {5, 6}. 2
Each position set X ? [|?N (p)|] can be repres-
ented by means of non-negative integers i1 < i2 <
? ? ? < i2k satisfying
X =
k?
j=1
[i2j?1 + 1, i2j ].
In other words, we are decomposing X into the
union of k intervals, with k as small as possible.
It is easy to see that this decomposition is always
unique. We call set E = {i1, i2, . . . , i2k} the en-
dpoint set associated with X , and we call k the
fan-out of X , written f(X). Throughout this pa-
per, we will represent p as the collection of all
the position sets associated with the occurrences
of nonterminals in its right-hand side.
Let X1 and X2 be two disjoint position sets
(i.e., X1 ? X2 = ?), with f(X1) = k1 and
f(X2) = k2 and with associated endpoint sets E1
and E2, respectively. We define the merge of X1
and X2 as the set X1 ? X2. We extend the po-
sition set and end-point set terminology to these
merge sets as well. It is easy to check that the en-
dpoint set associated to position set X1 ? X2 is
(E1?E2)\ (E1?E2). We say thatX1 andX2 are
2-combinable if f(X1 ?X2) ? 2. We also say
that X1 and X2 are adjacent, written X1 ? X2,
if f(X1 ?X2) ? max(k1, k2). It is not difficult
to see that X1 ? X2 if and only if X1 and X2 are
disjoint and |E1 ? E2| ? min(k1, k2). Note also
that X1 ? X2 always implies that X1 and X2 are
2-combinable (but not the other way around).
Let X be a collection of mutually disjoint posi-
tion sets. A reduction of X is the process of mer-
ging two position sets X1, X2 ? X , resulting in a
new collectionX ? = (X \{X1, X2})?{X1?X2}.
The reduction is 2-feasible if X1 and X2 are 2-
combinable. A binarization of X is a sequence
of reductions resulting in a new collection with
two or fewer position sets. The binarization is
2-feasible if all of the involved reductions are 2-
feasible. Finally, we say that X is 2-feasible if
there exists at least one 2-feasible binarization for
X .
As an important remark, we observe that when
a collection X represents the position sets of all
the nonterminals in the right-hand side of a pro-
duction p with r(p) > 2, then a 2-feasible reduc-
tion merging XAi , XAj ? X can be interpreted
as follows. We replace p by means of a new pro-
duction p? obtained from p by substituting Ai and
Aj with a fresh nonterminal symbol B, so that
r(p?) = r(p) ? 1. Furthermore, we create a new
production p?? with Ai and Aj in its right-hand
side, such that f(p??) = f(B) ? 2 and r(p??) = 2.
Productions p? and p?? together are equivalent to p,
but we have now achieved a local reduction in rank
of one unit.
Example 3 Let p be defined as in example 2 and
let X = {XA1 , XA2 , XA3}. We have that XA1
and XA2 are 2-combinable, and their merge is the
new position set X = XA1 ? XA2 = {1, 2, 3}.
This merge corresponds to a 2-feasible reduction
of X resulting in X ? = {X,XA3}. Such a re-
duction corresponds to the construction of a new
production p? : A? g?(B,A3) with
g?(?x1,1?, ?x3,1, x3,2?) = ?x1,1, x3,1bx3,2? ;
987
and a new production p?? : B ? g??(A1, A2) with
g??(?x1,1, x1,2?, ?x2,1?) = ?x1,1ax2,1x1,2? . 2
It is easy to see that X is 2-feasible if and only
if there exists a binarization of p that does not in-
crease its fan-out.
Example 4 It has been shown in (Rambow
and Satta, 1999) that binarization of an
LCFRS G with f(G) = 2 and r(G) = 3
is always possible without increasing the
fan-out, and that if r(G) ? 4 then this is
no longer true. Consider the LCFRS pro-
duction p : A ? g(A1, A2, A3, A4), with
g(?x1,1, x1,2?, ?x2,1, x2,2?, ?x3,1, x3,2?, ?x4,1, x4,2?) =
~?, ~? = ?x1,1x2,1x3,1x4,1, x2,2x4,2x1,2x3,2?. It is
not difficult to see that replacing any set of two or
three nonterminals in p?s right-hand side forces
the creation of a fresh nonterminal of fan-out
larger than two. 2
3.2 Greedy decision theorem
The binarization algorithm presented in this paper
proceeds by representing each LCFRS production
p as a collection of disjoint position sets, and then
finding a 2-feasible binarization of p. This binariz-
ation is computed deterministically, by an iterative
process that greedily chooses merges correspond-
ing to pairs of adjacent position sets.
The key idea behind the algorithm is based on a
theorem that guarantees that any merge of adjacent
sets preserves the property of 2-feasibility:
Theorem 1 LetX be a 2-feasible collection of po-
sition sets. The reduction of X by merging any
two adjacent position sets D1, D2 ? X results in
a new collection X ? which is 2-feasible.
To prove Theorem 1 we consider that, sinceX is
2-feasible, there must exist at least one 2-feasible
binarization for X . We can write this binariza-
tion ? as a sequence of reductions, where each re-
duction is characterized by a pair of position sets
(X1, X2) which are merged into X1 ?X2, in such
a way that both each of the initial sets and the res-
ult of the merge have fan-out at most 2.
We will show that, under these conditions, for
every pair of adjacent position sets D1 and D2,
there exists a binarization that starts with the re-
duction merging D1 with D2.
Without loss of generality, we assume that
f(D1) ? f(D2) (if this inequality does not hold
we can always swap the names of the two position
sets, since the merging operation is commutative),
and we define a function hD1?D2 : 2
N ? 2N as
follows:
? hD1?D2(X) = X; if D1 * X ?D2 * X .
? hD1?D2(X) = X; if D1 ? X ?D2 ? X .
? hD1?D2(X) = X ?D1; if D1 * X ?D2 ?
X .
? hD1?D2(X) = X \D1; if D1 ? X ?D2 *
X .
With this, we construct a binarization ?? from ?
as follows:
? The first reduction in ?? merges the pair of
position sets (D1, D2),
? We consider the reductions in ? in or-
der, and for each reduction o merging
(X1, X2), if X1 6= D1 and X2 6=
D1, we append a reduction o? merging
(hD1?D2(X1), hD1?D2(X2)) to ?
?.
We will now prove that, if ? is a 2-feasible bin-
arization, then ?? is also a 2-feasible binarization.
To prove this, it suffices to show the following:2
(i) Every position set merged by a reduction in
?? is either one of the original sets in X , or
the result of a previous merge in ??.
(ii) Every reduction in ?? merges a pair of posi-
tion sets (X1, X2) which are 2-combinable.
To prove (i) we note that by construction of ??,
if an operand of a merging operation in ?? is not
one of the original position sets in X , then it must
be an hD1?D2(X) for some X that appears as an
operand of a merging operation in ?. Since the
binarization ? is itself valid, this X must be either
one of the position sets in X , or the result of a
previous merge in the binarization ?. So we divide
the proof into two cases:
? If X ? X : First of all, we note that X can-
not be D1, since the merging operations of ?
that have D1 as an operand do not produce
2It is also necessary to show that no position set is merged
in two different reductions, but this easily follows from the
fact that hD1?D2(X) = hD1?D2(Y ) if and only if X ?
D1 = Y ?D1. Thus, two reductions in ? can only produce
conflicting reductions in ?? if they merge two position sets
differing only by D1, but in this case, one of the reductions
must merge D1 so it does not produce any reduction in ??.
988
a corresponding operation in ??. If X equals
D2, then hD1?D2(X) is D1 ? D2, which is
the result of the first merging operation in ??.
Finally, if X is one of the position sets in X ,
and not D1 or D2, then hD1?D2(X) = X ,
so our operand is also one of the position sets
in X .
? If X is the result of a previous merging oper-
ation o in binarization ?: Then, hD1?D2(X)
is the result of a previous merging operation
o? in binarization ??, which is obtained by ap-
plying the function hD1?D2 to the operands
and result of o. 3
To prove (ii), we show that, under the assump-
tions of the theorem, the function hD1?D2 pre-
serves 2-combinability. Since two position sets of
fan-out ? 2 are 2-combinable if and only if they
are disjoint and the fan-out of their union is at most
2, it suffices to show that, for everyX,X1, X2 uni-
ons of one or more sets of X , having fan-out ? 2,
such that X1 6= D1, X2 6= D1 and X 6= D1;
(a) The function hD1?D2 preserves disjointness,
that is, if X1 and X2 are disjoint, then
hD1?D2(X1) and hD1?D2(X2) are disjoint.
(b) The function hD1?D2 is distributive with
respect to the union of position sets, that
is, hD1?D2(X1 ? X2) = hD1?D2(X1) ?
hD1?D2(X2).
(c) The function hD1?D2 preserves the property
of having fan-out? 2, that is, ifX has fan-out
? 2, then hD1?D2(X) has fan-out ? 2.
If X1 and X2 do not contain D1 or D2, or if
one of the two unionsX1 orX2 containsD1?D2,
properties (a) and (b) are trivial, since the function
hD1?D2 behaves as the identity function in these
cases.
It remains to show that (a) and (b) are true in the
following cases:
? X1 contains D1 but not D2, and X2 does not
contain D1 or D2:
3Except if one of the operands of the operation o was D1.
But in this case, if we call the other operand Z, then we have
that X = D1 ? Z. If Z contains D2, then X = D1 ?
Z = hD1?D2(X) = hD1?D2(Z), so we apply this same
reasoning with hD1?D2(Z) where we cannot fall into this
case, since there can be only one merge operation in ? that
uses D1 as an operand. If Z does not contain D2, then we
have that hD1?D2(X) = X \D1 = Z = hD1?D2(Z), so
we can do the same.
In this case, ifX1 andX2 are disjoint, we can
writeX1 = Y1?D1, such that Y1, X2, D1 are
pairwise disjoint. By definition, we have that
hD1?D2(X1) = Y1, and hD1?D2(X2) =
X2, which are disjoint, so (a) holds.
Property (b) also holds because, with these
expressions for X1 and X2, we can calcu-
late hD1?D2(X1 ? X2) = Y1 ? X2 =
hD1?D2(X1) ? hD1?D2(X2).
? X1 containsD2 but notD1,X2 does not con-
tain D1 or D2:
In this case, if X1 and X2 are disjoint,
we can write X1 = Y1 ? D2, such that
Y1, X2, D1, D2 are pairwise disjoint. By
definition, hD1?D2(X1) = Y1 ? D2 ? D1,
and hD1?D2(X2) = X2, which are disjoint,
so (a) holds.
Property (b) also holds, since we can check
that hD1?D2(X1 ? X2) = Y1 ? X2 ? D2 ?
D1 = hD1?D2(X1) ? hD1?D2(X2).
? X1 contains D1 but not D2, X2 contains D2
but not D1:
In this case, ifX1 andX2 are disjoint, we can
writeX1 = Y1?D1 andX2 = Y2?D2, such
that Y1, Y2, D1, D2 are pairwise disjoint. By
definition, we know that hD1?D2(X1) = Y1,
and hD1?D2(X2) = Y2 ? D1 ? D2, which
are disjoint, so (a) holds.
Finally, property (b) also holds in this case,
since hD1?D2(X1 ?X2) = Y1 ?X2 ?D2 ?
D1 = hD1?D2(X1) ? hD1?D2(X2).
This concludes the proof of (a) and (b).
To prove (c), we consider a position set X ,
union of one or more sets of X , with fan-out ? 2
and such that X 6= D1. First of all, we observe
that if X does not contain D1 or D2, or if it con-
tains D1 ? D2, (c) is trivial, because the function
hD1?D2 behaves as the identity function in this
case. So it remains to prove (c) in the cases where
X contains D1 but not D2, and where X contains
D2 but not D1. In any of these two cases, if we
call E(Y ) the endpoint set associated with an ar-
bitrary position set Y , we can make the following
observations:
1. Since X has fan-out ? 2, E(X) contains at
most 4 endpoints.
2. SinceD1 has fan-out f(D1),E(D1) contains
at most 2f(D1) endpoints.
989
3. SinceD2 has fan-out f(D2),E(D2) contains
at most 2f(D2) endpoints.
4. Since D1 and D2 are adjacent, we know
that E(D1) ? E(D2) contains at least
min(f(D1), f(D2)) = f(D1) endpoints.
5. Therefore, E(D1) \ (E(D1) ? E(D2)) can
contain at most 2f(D1) ? f(D1) = f(D1)
endpoints.
6. On the other hand, sinceX contains only one
of D1 and D2, we know that the endpoints
where D1 is adjacent to D2 must also be en-
dpoints of X , so that E(D1) ? E(D2) ?
E(X). Therefore, E(X)\(E(D1)?E(D2))
can contain at most 4? f(D1) endpoints.
Now, in the case where X contains D1 but not
D2, we know that hD1?D2(X) = X\D1. We cal-
culate a bound for the fan-out ofX\D1 as follows:
we observe that all the endpoints in E(X \ D1)
must be either endpoints of X or endpoints of
D1, since E(X) = (E(X \ D1) ? E(D1)) \
(E(X \ D1) ? E(D1)), so every position that is
in E(X \D1) but not in E(D1) must be in E(X).
But we also observe that E(X \ D1) cannot con-
tain any of the endpoints where D1 is adjacent to
D2 (i.e., the members of E(D1) ? E(D2)), since
X \D1 does not contain D1 or D2. Thus, we can
say that any endpoint of X \D1 is either a mem-
ber of E(D1) \ (E(D1) ? E(D2)), or a member
of E(X) \ (E(D1) ? E(D2)).
Thus, the number of endpoints in E(X \ D1)
cannot exceed the sum of the number of endpoints
in these two sets, which, according to the reason-
ings above, is at most 4 ? f(D1) + f(D1) = 4.
Since E(X \D1) cannot contain more than 4 en-
dpoints, we conclude that the fan-out of X \ D1
is at most 2, so the function hD1?D2 preserves the
property of position sets having fan-out? 2 in this
case.
In the other case, where X contains D2 but not
D1, we follow a similar reasoning: in this case,
hD1?D2(X) = X ? D1. To bound the fan-out
of X ? D1, we observe that all the endpoints in
E(X ?D1) must be either in E(X) or in E(D1),
since E(X ?D1) = (E(X)?E(D1)) \ (E(X)?
E(D1)). But we also know that E(X ?D1) can-
not contain any of the endpoints where D1 is adja-
cent to D2 (i.e., the members of E(D1)?E(D2)),
since X ?D1 contains both D1 and D2. Thus, we
can say that any endpoint of X ? D1 is either a
1: Function BINARIZATION(p)
2: A ? ?; {working agenda}
3: R ? ??; {empty list of reductions}
4: for all i from 1 to r(p) do
5: A ? A? {XAi};
6: while |A| > 2 and A contains two adjacent
position sets do
7: choose X1, X2 ? A such that X1 ? X2;
8: X ? X1 ?X2;
9: A ? (A \ {X1, X2}) ? {X};
10: append (X1, X2) toR;
11: if |A| = 2 then
12: return R;
13: else
14: return fail;
Figure 1: Binarization algorithm for a production
p : A ? g(A1, . . . , Ar(p)). Result is either a list
of reductions or failure.
member of E(D1)\ (E(D1)?E(D2)), or a mem-
ber of E(X) \ (E(D1) ? E(D2)). Reasoning as
in the previous case, we conclude that the fan-out
of X ? D1 is at most 2, so the function hD1?D2
also preserves the property of position sets having
fan-out ? 2 in this case.
This concludes the proof of Theorem 1.
4 Binarization algorithm
Let p : A ? g(A1, . . . , Ar(p)) be a production
with r(p) > 2 from some LCFRS with fan-out
not greater than 2. Recall from Subsection 3.1 that
each occurrence of nonterminal Ai in the right-
hand side of p is represented as a position setXAi .
The specification of an algorithm for finding a 2-
feasible binarization of p is reported in Figure 1.
The algorithm uses an agenda A as a working
set, where all position sets that still need to be pro-
cessed are stored. A is initialized with the posi-
tion sets XAi , 1 ? i ? r(p). At each step in the
algorithm, the size of A represents the maximum
rank among all productions that can be obtained
from the reductions that have been chosen so far in
the binarization process. The algorithm also uses
a list R, initialized as the empty list, where all re-
ductions that are attempted in the binarization pro-
cess are appended.
At each iteration, the algorithm performs a re-
duction by arbitrarily choosing a pair of adjacent
endpoint sets from the agenda and by merging
them. As already discussed in Subsection 3.1, this
990
corresponds to some specific transformation of the
input production p that preserves its generative ca-
pacity and that decreases its rank by one unit.
We stop the iterations of the algorithm when we
reach a state in which there are no more than two
position sets in the agenda. This means that the
binarization process has come to an end with the
reduction of p to a set of productions equivalent
to p and with rank and fan-out at most 2. This
set of productions can be easily constructed from
the output list R. We also stop the iterations in
case no adjacent pair of position sets can be found
in the agenda. If the agenda has more than two
position sets, this means that no binarization has
been found and the algorithm returns a failure.
4.1 Correctness
To prove the correctness of the algorithm in Fig-
ure 1, we need to show that it produces a 2-feasible
binarization of the given production p whenever
such a binarization exists. This is established by
the following theorem:
Theorem 2 LetX be a 2-feasible collection of po-
sition sets, such that the union of all sets in X is a
position set with fan-out ? 2. The procedure:
while ( X contains any pair of adjacent sets
X1, X2 ) reduce X by merging X1 with X2;
always finds a 2-feasible binarization of X .
In order to prove this, the loop invariant is that
X is a 2-feasible set, and that the union of all po-
sition sets in X has fan-out ? 2: reductions can
never change the union of all sets in X , and The-
orem 1 guarantees us that every change to the state
of X maintains 2-feasibility. We also know that
the algorithm eventually finishes, because every
iteration reduces the amount of position sets in X
by 1; and the looping condition will not hold when
the number of sets gets to be 1.
So it only remains to prove that the loop is only
exited if X contains at most two position sets. If
we show this, we know that the sequence of re-
ductions produced by this procedure is a 2-feasible
binarization. Since the loop is exited when X is 2-
feasible but it contains no pair of adjacent position
sets, it suffices to show the following:
Proposition 1 Let X be a 2-feasible collection of
position sets, such that the union of all the sets in
X is a position set with fan-out? 2. IfX has more
than two elements, then it contains at least a pair
of adjacent position sets. 2
Let X be a 2-feasible collection of more than
two position sets. Since X is 2-feasible, we know
that there must be a 2-feasible binarization of X .
Suppose that ? is such a binarization, and let D1
and D2 be the two position sets that are merged in
the first reduction of ?. Since ? is 2-feasible, D1
and D2 must be 2-combinable.
If D1 and D2 are adjacent, our proposition is
true. If they are not adjacent, then, in order to be 2-
combinable, the fan-out of both position sets must
be 1: if any of them had fan-out 2, their union
would need to have fan-out > 2 for D1 and D2
not to be adjacent, and thus they would not be 2-
combinable. Since D1 and D2 have fan-out 1 and
are not adjacent, their sets of endpoints are of the
form {b1, b2} and {c1, c2}, and they are disjoint.
If we call EX the set of endpoints correspond-
ing to the union of all the position sets in X and
ED1D2 = {b1, b2, c1, c2}, we can show that at
least one of the endpoints in ED1D2 does not ap-
pear in EX , since we know that EX can have at
most 4 elements (as the union has fan-out ? 2)
and that it cannot equalED1D2 because this would
mean that X = {D1, D2}, and by hypothesis X
has more than two position sets. If we call this
endpoint x, this means that there must be a posi-
tion set D3 in X , different from D1 and D2, that
has x as one of its endpoints. Since D1 and D2
have fan-out 1, this implies that D3 must be ad-
jacent either to D1 or to D2, so we conclude the
proof.
4.2 Implementation and complexity
We now turn to the computational analysis of the
algorithm in Figure 1. We define the length of an
LCFRS production p, written |p|, as the sum of
the length of all strings ?j in ~? in the definition
of the linear, non-erasing function associated with
p. Since we are dealing with LCFRS of fan-out at
most two, we easily derive that |p| = O(r(p)).
In the implementation of the algorithm it is con-
venient to represent each position set by means of
the corresponding endpoint set. Since at any time
in the computation we are only processing posi-
tion sets with fan-out not greater than two, each
endpoint set will contain at most four integers.
The for-loop at lines 4 and 5 in the algorithm
can be easily implemented through a left-to-right
scan of the characteristic string ?N (p), detecting
the endpoint sets associated with each position set
XAi . This can be done in constant time for each
991
XAi , and thus in linear time in |p|.
At each iteration of the while-loop at lines 6
to 10 we have that A is reduced in size by one
unit. This means that the number of iterations is
bounded by r(p). We will show below that each
iteration of this loop can be executed in constant
time. We can therefore conclude that our binariz-
ation algorithm runs in optimal time O(|p|).
In order to run in constant time each single it-
eration of the while-loop at lines 6 to 10, we need
to perform some additional bookkeeping. We use
two arrays Ve and Va, whose elements are in-
dexed by the endpoints associated with character-
istic string ?N (p), that is, integers i ? [0, |?N (p)|].
For each endpoint i, Ve[i] stores all the endpoint
sets that share endpoint i. Since each endpoint can
be shared by at most two endpoint sets, such a data
structure has sizeO(|p|). If there exists some posi-
tion setX inAwith leftmost endpoint i, then Va[i]
stores all the position sets (represented as endpoint
sets) that are adjacent to X . Since each position
set can be adjacent to at most four other position
sets, such a data structure has size O(|p|). Finally,
we assume we can go back and forth between po-
sition sets in the agenda and their leftmost end-
points.
We maintain arrays Ve and Va through the fol-
lowing simple procedures.
? Whenever a new position set X is added to
A, for each endpoint i of X we add X to
Ve[i]. We also check whether any position set
in Ve[i] other than X is adjacent to X , and
add these position sets to Va[il], where il is
the leftmost end point of X .
? Whenever some position set X is removed
from A, for each endpoint i of X we remove
X from Ve[i]. We also remove all of the posi-
tion sets in Va[il], where il is the leftmost end
point of X .
It is easy to see that, for any position set X which
is added/removed from A, each of the above pro-
cedures can be executed in constant time.
We maintain a set I of integer numbers i ?
[0, |?N (p)|] such that i ? I if and only if Va[i] is
not empty. Then at each iteration of the while-loop
at lines 6 to 10 we pick up some index in I and re-
trieve at Va[i] some pairX,X ? such thatX ? X ?.
Since X,X ? are represented by means of endpoint
sets, we can compute the endpoint set ofX?X ? in
constant time. Removal of X,X ? and addition of
X?X ? in our data structures Ve and Va is then per-
formed in constant time, as described above. This
proves our claim that each single iteration of the
while loop can be executed in constant time.
5 Discussion
We have presented an algorithm for the binariza-
tion of a LCFRS with fan-out 2 that does not in-
crease the fan-out, and have discussed how this
can be applied to improve parsing efficiency in
several practical applications. In the algorithm of
Figure 1, we can modify line 14 to return R even
in case of failure. If we do this, when a binariza-
tion with fan-out ? 2 does not exist the algorithm
will still provide us with a list of reductions that
can be converted into a set of productions equival-
ent to p with fan-out at most 2 and rank bounded
by some rb, with 2 < rb ? r(p). In case rb <
r(p), we are not guaranteed to have achieved an
optimal reduction in the rank, but we can still ob-
tain an asymptotic improvement in parsing time if
we use the new productions obtained in the trans-
formation.
Our algorithm has optimal time complexity,
since it works in linear time with respect to the
input production length. It still needs to be invest-
igated whether the proposed technique, based on
determinization of the choice of the reduction, can
also be used for finding binarizations for LCFRS
with fan-out larger than two, again without in-
creasing the fan-out. However, it seems unlikely
that this can still be done in linear time, since the
problem of binarization for LCFRS in general, i.e.,
without any bound on the fan-out, might not be
solvable in polynomial time. This is still an open
problem; see (Go?mez-Rodr??guez et al, 2009) for
discussion.
Acknowledgments
The first author has been supported by Ministerio
de Educacio?n y Ciencia and FEDER (HUM2007-
66607-C04) and Xunta de Galicia (PGIDIT-
07SIN005206PR, INCITE08E1R104022ES,
INCITE08ENA305025ES, INCITE08PXIB-
302179PR and Rede Galega de Procesamento
da Linguaxe e Recuperacio?n de Informacio?n).
The second author has been partially supported
by MIUR under project PRIN No. 2007TJN-
ZRE 002.
992
References
Pierre Boullier. 2004. Range concatenation grammars.
In H. Bunt, J. Carroll, and G. Satta, editors, New
Developments in Parsing Technology, volume 23 of
Text, Speech and Language Technology, pages 269?
289. Kluwer Academic Publishers.
Ha?kan Burden and Peter Ljunglo?f. 2005. Parsing lin-
ear context-free rewriting systems. In IWPT05, 9th
International Workshop on Parsing Technologies.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd ACL, pages 263?270.
Carlos Go?mez-Rodr??guez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in linear context-free rewriting systems.
In Proc. of the North American Chapter of the Asso-
ciation for Computational Linguistics - Human Lan-
guage Technologies Conference (NAACL?09:HLT),
Boulder, Colorado. To appear.
Aravind K. Joshi and Leon S. Levy. 1977. Constraints
on local descriptions: Local transformations. SIAM
J. Comput., 6(2):272?284.
Aravind K. Joshi, K. Vijay-Shanker, and David Weir.
1991. The convergence of mildly context-sensitive
grammatical formalisms. In P. Sells, S. Shieber, and
T. Wasow, editors, Foundational Issues in Natural
Language Processing. MIT Press, Cambridge MA.
Marco Kuhlmann and Giorgio Satta. 2009. Tree-
bank grammar techniques for non-projective de-
pendency parsing. In Proc. of the 12th Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL-09), pages 478?486,
Athens, Greece.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of HLT-NAACL
2003.
Rebecca Nesson and Stuart M. Shieber. 2006. Simpler
TAG semantics through synchronization. In Pro-
ceedings of the 11th Conference on Formal Gram-
mar, Malaga, Spain, 29?30 July.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theoretical Computer Science, 223:87?120.
Giorgio Satta. 1998. Trading independent for syn-
chronized parallelism in finite copying parallel re-
writing systems. Journal of Computer and System
Sciences, 56(1):27?45.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science, 88:191?
229.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In Pro-
ceedings of the 25th Meeting of the Association for
Computational Linguistics (ACL?87).
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In 22nd Inter-
national Conference on Computational Linguistics
(Coling), pages 1081?1088, Manchester, England,
UK.
993
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 994?1002,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Polynomial-Time Parsing Algorithm for TT-MCTAG
Laura Kallmeyer
Collaborative Research Center 441
Universita?t Tu?bingen
Tu?bingen, Germany
lk@sfs.uni-tuebingen.de
Giorgio Satta
Department of Information Engineering
University of Padua
Padova, Italy
satta@dei.unipd.it
Abstract
This paper investigates the class of Tree-
Tuple MCTAG with Shared Nodes, TT-
MCTAG for short, an extension of Tree
Adjoining Grammars that has been pro-
posed for natural language processing, in
particular for dealing with discontinuities
and word order variation in languages such
as German. It has been shown that the uni-
versal recognition problem for this formal-
ism is NP-hard, but so far it was not known
whether the class of languages generated
by TT-MCTAG is included in PTIME. We
provide a positive answer to this ques-
tion, using a new characterization of TT-
MCTAG.
1 Introduction
For a large range of linguistic phenomena, exten-
sions of Tree Adjoining Grammars (Joshi et al,
1975), or TAG for short, have been proposed based
on the idea of separating the contribution of a lex-
ical item into several components. Instead of sin-
gle trees, these grammars contain (multi-)sets of
trees. Examples are tree-local and set-local mul-
ticomponent TAG (Joshi, 1985; Weir, 1988), MC-
TAG for short, non-local MCTAG with dominance
links (Becker et al, 1991), Vector-TAG with dom-
inance links (Rambow, 1994) and, more recently,
Tree-Tuple MCTAG with Shared Nodes (Lichte,
2007)), or TT-MCTAG for short.
For some of the above formalisms the word
recognition problem is NP-hard. This has been
shown for non-local MCTAG (Rambow and Satta,
1992), even in the lexicalized case (Champollion,
2007). Some others generate only polynomial lan-
guages but their generative capacity is too limited
to deal with all natural language phenomena. This
has been argued for tree-local and even set-local
MCTAG on the basis of scrambling data from lan-
guages such as German (Becker et al, 1992; Ram-
bow, 1994).
In this paper, we focus on TT-MCTAG (Lichte,
2007). So far, it has been shown that the univer-
sal recognition problem for TT-MCTAG is NP-
hard (S?gaard et al, 2007). A restriction on TT-
MCTAG has been proposed in (Kallmeyer and
Parmentier, 2008): with such a restriction, the uni-
versal recognition problem is still NP-hard, but
the class of generated languages is included in
PTIME, i.e., all these languages can be recognized
in deterministic polynomial time. In this paper, we
address the question of whether for general TT-
MCTAG, i.e., TT-MCTAG without the constraint
from (Kallmeyer and Parmentier, 2008), the class
of generated languages is included in PTIME. We
provide a positive answer to this question.
The TT-MCTAG definition from (Lichte, 2007;
Kallmeyer and Parmentier, 2008) imposes a con-
dition on the way different tree components from a
tree tuple in the grammar combine with each other.
This condition is formulated in terms of mapping
between argument and head trees, i.e., in order to
test such a condition one has to guess some group-
ing of the tree components used in a derivation into
instances of tree tuples from the grammar. This re-
sults in a combinatorial explosion of parsing anal-
yses. In order to obtain a polynomial parsing al-
gorithm, we need to avoid this effect.
On this line, we propose an alternative charac-
terization of TT-MCTAG that only requires (i) a
counting of tree components and (ii) the check of
some local conditions on these counts. This allows
for parsing in polynomial deterministic time.
TT-MCTAG uses so-called ?parallel unordered?
rewriting. The first polynomial time parsing
results on this class were presented in (Ram-
bow and Satta, 1994; Satta, 1995) for some
string-based systems, exploiting counting tech-
niques closely related to those we use in this pa-
per. In contrast to string-based rewriting, the tree
994
rewriting formalisms we consider here are struc-
turally more complex and require specializations
of the above techniques. Polynomial parsing re-
sults for tree rewriting systems based on paral-
lel unordered rewriting have also been reported
in (Rambow, 1994; Rambow et al, 1995). How-
ever, in the approach proposed by these authors,
tree-based grammars are first translated into equiv-
alent string-based systems, and the result is again
provided on the string domain.
2 Tree Adjoining Grammars
Tree Adjoining Grammars (Joshi et al, 1975) are
a formalism based on tree rewriting. We briefly
summarize here the relevant definitions and refer
the reader to (Joshi and Schabes, 1997) for a more
complete introduction.
Definition 1 A Tree Adjoining Grammar
(TAG) is a tuple G = (VN , VT , S, I,A) where
VN and VT are disjoint alphabets of non-terminal
and terminal symbols, respectively, S ? VN is the
start symbol, and I and A are finite sets of initial
and auxiliary trees, respectively. 2
Trees in I ?A are called elementary trees. The
internal nodes in the elementary trees are labeled
with non-terminal symbols, the leaves with non-
terminal or terminal symbols. As a special prop-
erty, each auxiliary tree ? has exactly one of its
leaf nodes marked as the foot node, having the
same label as the root. Such a node is denoted by
Ft(?). Leaves with non-terminal labels that are
not foot nodes are called substitution nodes.
In a TAG, larger trees can be derived from the
elementary trees by subsequent applications of the
operations substitution and adjunction. The sub-
stitution operation replaces a substitution node ?
with an initial tree having root node with the same
label as ?. The adjunction operation replaces
an internal node ? in a previously derived tree ?
with an auxiliary tree ? having root node with the
same label as ?. The subtree of ? rooted at ? is
then placed below the foot node of ?. Only inter-
nal nodes can allow for adjunction, adjunction at
leaves is not possible. See figure 1 for an example
of a tree derivation.
Usually, a TAG comes with restrictions on the
two operations, specified at each node ? by sets
Sbst(?) and Adj (?) listing all elementary trees
that can be substituted or adjoined, respectively.
Furthermore, adjunction at ? might be obligatory.
NP
John
S
NP VP
V
laughs
VP
ADV VP?
always
derived tree:
S
NP VP
John ADV VP
always V
laughs
derivation tree:
laugh
1 2
john always
Figure 1: TAG derivation for John always laughs
TAG derivations are represented by derivation
trees that record the history of how the elemen-
tary trees are put together. A derivation tree is
an unordered tree whose nodes are labeled with
elements in I ? A and whose edges are labeled
with Gorn addresses of elementary trees.1 Each
edge in a derivation tree stands for an adjunction
or a substitution. E.g., the derivation tree in fig-
ure 1 indicates that the elementary tree for John is
substituted for the node at address 1 and always is
adjoined at node address 2.
In the following, we write a derivation tree D
as a directed graph ?V,E, r? where V is the set of
nodes, E ? V ? V is the set of arcs and r ? V is
the root. For every v ? V , Lab(v) gives the node
label and for every ?v1, v2? ? E, Lab(?v1, v2?)
gives the edge label.
A derived tree is the result of carrying out the
substitutions and the adjunctions in a derivation
tree, i.e., the derivation tree describes uniquely the
derived tree; see again figure 1.
3 TT-MCTAG
3.1 Introduction to TT-MCTAG
For a range of linguistic phenomena, multicompo-
nent TAG (Weir, 1988) have been proposed, also
called MCTAG for short. The underlying motiva-
tion is the desire to split the contribution of a single
lexical item (e.g., a verb and its arguments) into
several elementary trees. An MCTAG consists of
(multi-)sets of elementary trees, called tree sets.
If an elementary tree from some set is used in a
derivation, then all of the remaining trees in the
set must be used as well. Several variants of MC-
TAGs can be found the literature, differing on the
1In this convention, the root address is ? and the jth child
of a node with address p has address p ? j.
995
specific definition of the derivation process.
The particular MCTAG variant we are con-
cerned with is Tree-Tuple MCTAG with Shared
Nodes, TT-MCTAG (Lichte, 2007). TT-MCTAG
were introduced to deal with free word order phe-
nomena in languages such as German. An exam-
ple is (1) where the argument es of reparieren pre-
cedes the argument der Mann of versucht and is
not adjacent to the predicate it depends on.
(1) ... dass es der Mann zu reparieren versucht
... that it the man to repair tries
?... that the man tries to repair it?
A TT-MCTAG is slightly different from stan-
dard MCTAGs since each elementary tree set con-
tains one specially marked lexicalized tree called
the head, and all of the remaining trees in the set
function as arguments of the head. Furthermore, in
a TT-MCTAG derivation the argument trees must
either adjoin directly to their head tree, or they
must be linked in the derivation tree to an elemen-
tary tree that attaches to the head tree, by means
of a chain of adjunctions at root nodes. In other
words, in the corresponding TAG derivation tree,
the head tree must dominate the argument trees in
such a way that all positions on the path between
them, except the first one, must be labeled by ?.
This captures the notion of adjunction under node
sharing from (Kallmeyer, 2005).2
Definition 2 A TT-MCTAG is a tuple G = (VN ,
VT , S, I,A,T ) where GT = (VN , VT , S, I,A) is
an underlying TAG and T is a finite set of tree
tuples of the form ? = ??, {?1, . . . , ?r}? where
? ? (I ? A) has at least one node with a terminal
label, and ?1, . . . , ?n ? A. 2
For each ? = ??, {?1, . . . , ?r}? ? T , we call ?
the head tree and the ?j?s the argument trees.
We informally say that ? and the ?j?s belong to ?,
and write |?| = r + 1.
As a remark, an elementary tree ? from the un-
derlying TAG GT can be found in different tree tu-
ples in G, or there could even be multiple instances
of such a tree within the same tree tuple ?. In these
cases, we just treat these tree instances as distinct
trees that are isomorphic and have identical labels.
2The intuition is that, if a tree ?? adjoins to some ?, its
root in the resulting derived tree somehow belongs both to ?
and ?? or, in other words, is shared by them. A further tree ?
adjoining to this node can then be considered as adjoining to
?, not only to ?? as in standard TAG. Note that we assume that
foot nodes do not allow adjunctions, otherwise node sharing
would also apply to them.
For a given argument tree ? in ?, h(?) denotes the
head of ? in ?. For a given ? ? I?A, a(?) denotes
the set of argument trees of ?, if there are any, or
the empty set otherwise. Furthermore, for a given
TT-MCTAG G, H(G) is the set of head trees and
A(G) is the set of argument trees. Finally, a node
v in a derivation tree for G with Lab(v) = ? is
called a ?-node.
Definition 3 Let G = (VN , VT , S, I,A,T ) be
some TT-MCTAG. A derivation tree D =
?V,E, r? in the underlying TAG GT is licensed in
G if and only if the following conditions (MC) and
(SN-TTL) are both satisfied.
? (MC): For all ? from G and for all ?1, ?2
in ?, we have |{v | v ? V, Lab(v) = ?1}| =
|{v | v ? V, Lab(v) = ?2}|.
? (SN-TTL): For all ? ? A(G) and n ? 1,
let v1, . . . , vn ? V be pairwise different
h(?)-nodes, 1 ? i ? n. Then there are
pairwise different ?-nodes u1, . . . , un ? V ,
1 ? i ? n. Furthermore, for 1 ? i ?
n, either ?vi, ui? ? E, or else there are
ui,1, . . . , ui,k, k ? 2, with auxiliary tree la-
bels, such that ui = ui,k, ?vi, ui,1? ? E and,
for 1 ? j ? k ? 1, ?ui,j, ui,j+1? ? E with
Lab(?ui,j , ui,j+1?) = ?. 2
The separation between (MC) and (SN-TTL)
in definition 3 is motivated by the desire to
separate the multicomponent property that TT-
MCTAG shares with a range of related formalisms
(e.g., tree-local and set-local MCTAG, Vector-
TAG, etc.) from the notion of tree-locality with
shared nodes that is peculiar to TT-MCTAG.
Figure 2 shows a TT-MCTAG derivation for (1).
Here, the NPnom auxiliary tree adjoins directly to
versucht (its head) while the NPacc tree adjoins to
the root of a tree that adjoins to the root of a tree
that adjoins to reparieren.
TT-MCTAG can generate languages that, in
a strong sense, cannot be generated by Linear
Context-Free Rewriting Systems (Vijay-Shanker
et al, 1987; Weir, 1988), or LCFRS for
short. An example is the language of all strings
pi(n[1] . . . n[m])v[1] . . . v[m] with m ? 1, pi a per-
mutation, and n[i] = n is a nominal argument of
v[i] = v for 1 ? i ? m, i.e., these occurrences
come from the same tree set in the grammar. Such
a language has been proposed as an abstract de-
scription of the scrambling phenomenon as found
in German and other free word order languages,
996
*VP
VP? versucht
,
(
VP
NPnom VP?
) + *
NPnom
der Mann
, {}
+
*
VP
zu reparieren
,
(
VP
NPacc VP?
) + *
NPacc
es
, {}
+
derivation tree:
reparieren
?
versucht
?
NPnom
1 ?
Mann NPacc
1
es
Figure 2: TT-MCTAG derivation of (1)
* ? VP
v
,
( ?1 VPv=?
n VP?NA
)+
* ?2 VP
v VP?NAv=+
,
( ?3 VPv=?
n VP?NA
)+
Figure 3: TT-MCTAG
and cannot be generated by a LCFRS (Becker et
al., 1992; Rambow, 1994). Figure 3 reports a TT-
MCTAG for this language.
Concerning the other direction, at the time of
writing it is not known whether there are lan-
guages generated by LCFRS but not by TT-
MCTAG. It is well known that LCFRS is closed
under the finite-copy operator. This means that,
for any fixed k > 1, if L is generated by a LCFRS
then the language {w |w = uk, u ? L} can
also be generated by a LCFRS. We conjecture that
TT-MCTAG does not have such a closure prop-
erty. However, from a first inspection of the MC-
TAG analyses proposed for natural languages (see
Chen-Main and Joshi (2007) for an overview), it
seems that there are no important natural language
phenomena that can be described by LCFRS and
not by TT-MCTAG. Any construction involving
some kind of component stacking along the VP
projection such as subject-auxiliary inversion can
be modelled with TT-MCTAG. Unbounded extra-
position phenomena cannot be described with TT-
MCTAG but they constitute a problem for any lo-
cal formalism and so far the nature of these phe-
nomena is not sufficiently well-understood.
Note that, in contrast to non-local MCTAG, in
TT-MCTAG the trees coming from the same in-
stance of a tuple in the grammar are not required
to be added at the same time. TT-MCTAGs share
this property of ?non-simultaneity? with other vec-
tor grammars such as Unordered Vector Gram-
mars (Cremers and Mayer, 1973) and Vector-
TAG (Rambow, 1994), V-TAG for short, and it
is crucial for the polynomial parsing algorithm.
The non-simultaneity seems to be an advantage
when using synchronous grammars to model the
syntax-semantics interface (Nesson and Shieber,
2008). The closest formalism to TT-MCTAG is
V-TAG. However, there are fundamental differ-
ences between the two. Firstly, they make a dif-
ferent use of dominance links: In V-TAG domi-
nance links relate different nodes in the trees of
a tree set from the grammar. They present domi-
nance requirements that constrain the derived tree.
In TT-MCTAG, there are no dominance links be-
tween nodes in elementary trees. Instead, the node
of a head tree in the derivation tree must domi-
nate all its arguments. Furthermore, even though
TT-MCTAG arguments can adjoin with a delay
to their head, their possible adjunction site is re-
stricted with respect to their head. As a result,
one obtains a slight degree of locality that can
be exploited for natural language phenomena that
are unbounded only in a limited domain. This is
proposed in (Lichte and Kallmeyer, 2008) where
the fact that substitution nodes block argument ad-
junction to higher heads is used to model the lim-
ited domain of scrambling in German. V-TAG
does not have any such notion of locality. Instead,
it uses explicit constraints, so-called integrity con-
straints, to establish islands.
3.2 An alternative characterization of
TT-MCTAG
The definition of TT-MCTAG in subsection 3.1 is
taken from (Lichte, 2007; Kallmeyer and Parmen-
tier, 2008). The condition (SN-TTL) on the TAG
derivation tree is formulated in terms of heads and
arguments belonging together, i.e., coming from
the same tuple instance. For our parsing algo-
rithm, we want to avoid grouping the instances
of elementary trees in a derivation tree into tu-
ple instances. In other words, we want to check
whether a TAG derivation tree is a valid TT-
997
MCTAG derivation tree without deciding, for ev-
ery occurrence of some argument ?, which of the
h(?)-nodes represents its head. Therefore we pro-
pose to reformulate (SN-TTL).
For a node v in a derivation tree D, we write
Dv to represent the subtree of D rooted at v. For
? ? (I ? A), we define Dom(v, ?) as the set of
nodes of Dv that are labeled by ?. Furthermore,
for an argument tree ? ? A(G), we let pi(v, ?) =
|Dom(v, ?)| ? |Dom(v, h(?))|.
Lemma 1 Let G be a TT-MCTAG with underlying
TAG GT , and let D = ?V,E, r? be a derivation
tree in GT that satisfies (MC). D satisfies (SN-
TTL) if and only if, for every v ? V and every
? ? A(G), the following conditions both hold.
(i) pi(v, ?) ? 0.
(ii) If pi(v, ?) > 0, then one of the following con-
ditions must be satisfied:
(a) Lab(v) = ? and pi(v, ?) = 1;
(b) Lab(v) = ? and pi(v, ?) > 1, and there
is some ?v, v?? ? E with Lab(?v, v??) =
? and pi(v?, ?) + 1 = pi(v, ?);
(c) Lab(v) /? {?, h(?)} and there is some
?v, v?? ? E with Lab(?v, v??) = ? and
pi(v?, ?) = pi(v, ?);
(d) Lab(v) = h(?) and there is some
?v, v?? ? E with Lab(?v, v??) = ? and
pi(v, ?) ? pi(v?, ?) ? pi(v, ?) + 1.
Intuitively, condition (i) in lemma 1 captures the
fact that heads always dominate their arguments
in the derivation tree. Condition (ii)b states that,
if v is a ?-node and if v is not the only ?pend-
ing? ?-node in Dv, then all pending ?-nodes in
Dv, except v itself, must be below the root adjoin-
ing node. Here pending means that the node is
not matched to a head-node within Dv. Condition
(ii)c treats the case in which there are pending ?-
nodes in Dv for some node v whose label is neither
? nor h(?). Then the pending nodes must all be
below the root adjoining node. Finally, condition
(ii)d deals with the case of a h(?)-node v where,
besides the ?-node that serves as an argument of
v, there are other pending ?-nodes in Dv. These
other pending ?-nodes must all be in Dv? , where
v? is the (unique) root adjoining node, if it exists.
The argument of v might as well be below v?, and
then the number of pending ?-nodes in Dv? is the
number of pending nodes in Dv, incremented by
1, since the argument of v is not pending in Dv
but it is pending in Dv? . Otherwise, the argument
of v is a pending ?-node below some other daugh-
ter of v. Then the number of pending ?-nodes in
Dv? is the same as in Dv.
PROOF We first show that (SN-TTL) implies both
(i) and (ii).
Condition (i): Assume that there is a v ? V
and a ? ? A(G) with pi(v, ?) < 0. Then for
some n and for pairwise different v1, . . . , vn with
?v, vi? ? E?, Lab(vi) = h(?) (1 ? i ? n),
we cannot find pairwise different u1, . . . , un with
?vi, ui? ? E?, Lab(ui) = ?. This is in contradic-
tion with (SN-TTL). Consequently, condition (i)
must be satisfied.
Condition (ii): Assume ? and v as in the state-
ment of the lemma, with pi(v, ?) > 0. Let
v1, . . . , vn be all the h(?)-nodes in D. There
is a bijection f? from these nodes to n pairwise
distinct ?-nodes in D, such that every pair vi,
f?(vi) = ui satisfies the conditions in (SN-TTL).
Because of (MC), the nodes u1, . . . , un must be
all the ?-nodes in D. There must be at least one vi
(1 ? i ? n) with ?vi, v? ? E+, ?v, f?(vi)? ? E?.
Then we have one of the following cases.
(a) ui = v and vi is the only h(?)-node dominat-
ing v with a corresponding ?-node dominated by
v. In this case (ii)a holds.
(b) Lab(v) = ?, i.e., ?f?1? (v), v? ? E+ and there
are other nodes u ? Dom(v, ?), u 6= v with
?f?1? (u), v? ? E+. Then, with (SN-TTL), there
must be a v? with ?v, v?? ? E, Lab(?v, v??) = ?
and for all such nodes u, ?v?, u? ? E?. Conse-
quently, (ii)b holds.
(c) Lab(v) /? {?, h(?)}. Then, as in (b), there
must be a v? with ?v, v?? ? E, Lab(?v, v??) = ?
and for all u ? Dom(v, ?) with ?f?1? (u), v? ?
E+, ?v?, u? ? E?. Consequently, (ii)c holds.
(d) Lab(v) = h(?). If f?(v) is dominated by a v?
that is a daughter of v with Lab(?v, v??) = ?, then
for all u ? Dom(v, ?) with ?f?1? (u), v? ? E+
we have ?v?, u? ? E?. Consequently, pi(v?, ?) =
pi(v, ?) + 1. Alternatively, f?(v) is dominated by
some other daughter v? of v with Lab(?v, v??) 6=
?. In this case v? must still exist and, for all
u ? Dom(v, ?) with u 6= f?(v) and with
?f?1? (u), v? ? E+, we have ?v?, u? ? E?. Conse-
quently, pi(v?, ?) = pi(v, ?).
Now we show that (i) and (ii) imply (SN-TTL).
With (MC), the number of ?-nodes and h(?)-
nodes in V are the same, for every ? ? A(G). For
every ? ? A(G), we construct a bijection f? of the
998
same type as in the first part of the proof, and show
that (SN-TTL) is satisfied. To construct f?, for ev-
ery v ? V we define sets V?,v ? Dom(v, ?) of ?-
nodes v? that have a matching head f?(v?) domi-
nating v. The definition satisfies |V?,v| = pi(v, ?).
For every v with v1, . . . , vn being all its daughters:
a) If Lab(v) = ?, then (by (ii)) for every 1 ? j ?
n with Lab(?v, vj?) 6= ?, V?,vj = ?. If there is a
vi with Lab(?v, vi?) = ?, then V?,v = V?,vi ?{v},
else V?,v = {v}.
b) If Lab(v) /? {?, h(?)}, then (by (ii)) V?,vj = ?
for every 1 ? j ? n with Lab(?v, vj?) 6= ?. If
there is a vi with Lab(?v, vi?) = ?, then V?,v =
V?,vi , else V?,v = ?.
c) If Lab(v) = h(?), then there must be some i,
1 ? i ? n, such that V?,vi 6= ?. We need to
distinguish two cases. In the first case we have
Lab(?v, vi?) 6= ?, |V?,vi | = 1 and, for every
1 ? j ? n with j 6= i, either V?,vj = ? or
Lab(?v, vj?) = ?. In this case we define f?(v) =
v? for {v?} = V?,vi . In the second case we have
Lab(?v, vi?) = ? and, for every 1 ? j ? n with
j 6= i, V?,vj = ?. In this case we pick an arbitrary
v? ? V?,vi and let f?(v) = v?. In both cases we let
V?,v = (
?n
i=1 V?,vi) \ {f?(v)}.
With this mapping, (SN-TTL) is satisfied when
choosing for each h(?)-node vi the ?-node ui =
f?(vi) as its corresponding node. 
4 Parsing algorithm
In this section we present a recognition algorithm
for TT-MCTAG working in polynomial time in the
size of the input string. The algorithm can be eas-
ily converted into a parsing algorithm. The ba-
sic idea is to use a parsing algorithm for TAG,
and impose on-the-fly additional restrictions on
the underlying derivation trees that are being con-
structed, in order to fulfill the definition of valid
TT-MCTAG derivation. To simplify the presenta-
tion, we assume without loss of generality that all
elementary trees in our grammars are binary trees.
The input string has the form w = a1 ? ? ? an with
each ai ? VT and n ? 0 (n = 0 means w = ?).
4.1 TAG recognition
We start with the discussion of a baseline recogni-
tion algorithm for TAG, along the lines of (Vijay-
Shanker and Joshi, 1985). The algorithm is
specified by means of deduction rules, follow-
ing (Shieber et al, 1995), and can be implemented
using standard tabular techniques. Items have the
form [?, pt, i, f1, f2, j] where ? ? I ? A, p is the
address of a node in ?, subscript t ? {?,?} speci-
fies whether substitution or adjunction has already
taken place (?) or not (?) at p, and 0 ? i ? f1 ?
f2 ? j ? n are indices with i, j indicating the left
and right edges of the span recognized by p and
f1, f2 indicating the span of a gap in case a foot
node is dominated by p. We write f1 = f2 = ? if
no gap is involved. For combining indices, we use
the operator f ??f ?? = f where f = f ? if f ?? = ?,
f = f ?? if f ? = ?, and f is undefined otherwise.
The deduction rules are shown in figure 4.
The algorithm walks bottom-up on the deriva-
tion tree. Rules (1) and (2) process leaf nodes
in elementary trees and require precondition
Lab(?, p) = wi+1 and Lab(?, p) = ?, respec-
tively. Rule (3) processes the foot node of aux-
iliary tree ? ? A by guessing the portion of w
spanned by the gap. Note that we use p? in the
consequent item in order to block adjunction at
foot nodes, as usually required in TAG.
We move up along nodes in an elementary
tree by means of rules (4) and (5), depending on
whether the current node has no sibling or has a
single sibling, respectively.
Rule (6) substitutes initial tree ? at p in ?, un-
der the precondition ? ? Sbst(?, p). Similarly,
rule (7) adjoins auxiliary tree ? at p in ?, under the
precondition ? ? Adj (?, p). Both these rules use
p? in the consequent item in order to block mul-
tiple adjunction or substitution at p, as usually re-
quired in TAG. Rule (8) processes nodes at which
adjunction is not obligatory.
The algorithm recognizes w if and only if some
item [?, ??, 0,?,?, n] can be inferred with ? ? I
and Lab(?, ?) = S.
4.2 TT-MCTAG recognition
We now extend the recognition algorithm of fig-
ure 4 to TT-MCTAG. Let G be an input TT-
MCTAG. We assume that the tuples in T are num-
bered from 1 to |T |, and that the elementary trees
in each ?i are also numbered from 1 to |?i|, with
the first element being the head. We then write ?q,r
for the r-th elementary tree in the q-th tuple in T .
A t-counter is a ragged array T of integers with
primary index q ranging over {1, . . . , |T |} and
with secondary index r ranging over {1, . . . , |?i|}.
We write T (q,r) to denote the t-counter with
T [q, r] = 1 and zero everywhere else. We also use
the sum and the difference of t-counters, which are
999
[?, p?, i,?,?, i + 1] (1)
[?, p?, i,?,?, i] (2)
[?,Ft(?)?, i, i, j, j] (3)
[?, (p ? 1)?, i, f1, f2, j]
[?, p?, i, f1, f2, j] (4)
[?, (p ? 1)?, i, f1, f2, k]
[?, (p ? 2)?, k, f ?1, f ?2, j]
[?, p?, i, f1 ? f ?1, f2 ? f ?2, j]
(5)
[?, ??, i,?,?, j]
[?, p?, i,?,?, j] (6)
[?, ??, i, f1, f2, j]
[?, p?, f1, f ?1, f ?2, f2]
[?, p?, i, f ?1, f ?2, j]
(7)
[?, p?, i, f1, f2, j]
[?, p?, i, f1, f2, j] (8)
Figure 4: A baseline recognition algorithm for TAG. Rule preconditions and goal item are described in
the text.
[?q,r, p?, i,?,?, i + 1, T (q,r)] (9)
[?q,r, p?, i,?,?, i, T (q,r)] (10)
[?q,r,Ft(?q,r)?, i, i, j, j, T (q,r)] (11)
[?q,r, (p ? 1)?, i, f1, f2, j, T ]
[?q,r, p?, i, f1, f2, j, T ] (12)
[?q,r, (p ? 1)?, i, f1, f2, k, T1]
[?q,r, (p ? 2)?, k, f ?1, f ?2, j, T2]
[?q,r, p?, i, f1 ? f ?1, f2 ? f ?2, j, T1 + T2 ? T (q,r)]
(13)
[?q?,r? , ??, i,?,?, j, T ?]
[?q,r, p?, i,?,?, j, T ? + T (q,r)] (14)
[?q?,r? , ??, i, f1, f2, j, T ?]
[?q,r, p?, f1, f ?1, f ?2, f2, T ]
[?q,r, p?, i, f ?1, f ?2, j, T + T ?]
(15)
[?, p?, i, f1, f2, j, T ]
[?, p?, i, f1, f2, j, T ] (16)
Figure 5: A recognition algorithm for TT-MCTAG. Rule preconditions are the same as for figure 4,
filtering conditions on rules are described in the text.
defined elementwise in the obvious way.
Let D be a derivation tree generated by the TAG
underlying G. We associate D with the t-counter
T such that T [q, r] equals the count of all occur-
rences of elementary tree ?q,r appearing in D. In-
tuitively, we use t-counters to represent informa-
tion about TAG derivation trees that are relevant
to the licensing of such trees by the input TT-
MCTAG G.
We are now ready to present a recognizer based
on TT-MCTAG. To simplify the presentation, we
first discuss how to extend the algorithm of fig. 4
in order to compute t-counters, and will later spec-
ify how to apply TT-MCTAG filtering conditions
through such counters. The reader should however
keep in mind that the two processes are strictly
interleaved, with filtering conditions being tested
right after the construction of each new t-counter.
We use items of the form [?q,r, pt, i, f1, f2, j,
T ], where the first six components are defined as
in the case of TAG items, and the last component is
a t-counter associated with the constructed deriva-
tions. Our algorithm is specified in figure 5.
The simplest case is that of rules (12) and (16).
These rules do not alter the underlying derivation
tree, and thus the t-counter is simply copied from
the antecedent item to the consequent item.
Rules (9), (10) and (11) introduce ?q,r as the
first elementary tree in the analysis (?q,r ? A in
case of rule (11)). Therefore we set the associated
t-counter to T (q,r).
In rule (14) we substitute initial tree ?q?,r? at
node p in ?q,r. In terms of derivation structures,
we extend a derivation tree D? rooted at node v?
with Lab(v?) = ?q?,r? to a new derivation tree D
with root node v, Lab(v) = ?q,r. Node v has a sin-
gle child represented by the root of D?. Thus the
t-counter associated with D should be T ? +T (q,r).
A slightly different operation needs to be per-
formed when applying rule (15). Here we have
a derivation tree D with root node v, Lab(v) =
?q,r and a derivation tree D? with root node v?,
Lab(v?) = ?q?,r? . When adjoining ?q?,r? into ?q,r,
we need to add to the root of D a new child node,
represented by the root of D?. This means that
the t-counter associated with the consequent item
should be the sum of the t-counters associated with
D and D?.
Finally, rule (13) involves derivation trees D1
and D2, rooted at nodes v1 and v2, respectively.
Nodes v1 and v2 have the same label ?q,r. The ap-
plication of the rule corresponds to the ?merging?
of v1 and v2 into a new node v with label ?q,r as
well, Node v inherits all of the children of v1 and
v2. In this case the t-counter associated with the
consequent item is T1 + T2 ? T (q,r). Here T (q,r)
1000
needs to be subtracted because the contribution of
tree ?q,r is accounted for in both v1 and v2.
We can now discuss the filtering conditions that
need to be applied when using the above deduc-
tion rules. We start by observing that the algo-
rithm in figure 5 might not even stop if there is an
infinite set of derivation trees for the input string
w = a1 ? ? ? an in the underlying TAG GT . This
is because each derivation can have a distinct t-
counter. However, the definition of TT-MCTAG
imposes that the head tree of each tuple contains
at least one lexical element. Together with con-
dition (MC), this implies that no more than n tu-
ple instances can occur in a derivation tree for w
according to G. To test for such a condition, we
introduce a norm for t-counters
||T ||m =
|T |
?
q=1
max|?q|r=1 T [q, r] .
We then impose ||T ||m ? n for each t-counter con-
structed by our deduction rule, and block the cor-
responding derivation if this is not satisfied.
We also need to test conditions (i) and (ii) from
lemma 1. Since these conditions apply to nodes
of the derivation tree, this testing is done at each
deduction rule in which a consequent item may be
constructed for a node ??, that is, rules (14), (15)
and (16). We introduce two specialized predicates
F?(T ) ? ?(q, r) : T [q, 1] ? T [q, r] ;
F=(T ) ? ?(q, r) : T [q, 1] = T [q, r] .
We then test F?(T ), which amounts to testing
condition (i) for each argument tree in A(G).
Furthermore, if at some rule we have F?(T ) ?
?F=(T ), then we need to test for condition (ii).
To do this, we consider each argument tree ?q,r,
r 6= 1, and compare the elementary tree ?q,r in the
consequent item of the current rule with ?q,r and
h(?q,r) = ?q,1, to select the appropriate subcondi-
tion of (ii).
As an example, assume that we are applying
rule (15) as in figure 5, with p = ?. Let Tc =
T + T ? be the t-counter associated with the con-
sequent item. When we come to process some ar-
gument tree ?q,r such that Tc[q, r] ? Tc[q, 1] > 0
and ?q,r 6? {?q,r, ?q,1}, we need to test (ii)c. This
is done by requiring
T ?[q, r]? T ?[q, 1] = Tc[q, r]? Tc[q, 1].
If we are instead applying rule (16) with p = ?
and T [q, r] ? T [q, 1] > 0, then we test (ii)a, since
there is no adjunction at the root node, by requir-
ing ?q,r = ?q,r and T [q, r] ? T [q, 1] = 1.
We block the current derivation whenever the
conditions in lemma 1 are not satisfied.
The algorithm recognizes w if and only if some
item [?q,1, ??, 0,?,?, n, T ] can be inferred sat-
isfying ?q,1 ? I , Lab(?q,1, ?) = S and F=(T ).
The correctness immediately follows from the cor-
rectness of the underlying TAG parser and from
lemma 1.
Finally, we turn to the computational analysis
of the algorithm. We assume a tabular implemen-
tation of the process of item inference using our
deduction rules. Our algorithm clearly stops after
some finite amount of time, because of the filtering
condition ||T ||m ? n. We then need to derive an
upper bound on the number of applications of de-
duction rules. To do this, we use an argument that
is rather standard in the tabular parsing literature.
The number of t-counters satisfying ||T ||m ? n
is O(ncG), with cG =
?|T |
i=1 |?i|. Since all of
the other components in an item are bounded by
O(n4), there are polynomially (in n) many items
that can be constructed for an input w. It is not dif-
ficult to see that each individual item can be con-
structed by a number of rule applications bounded
by a polynomial as well. Therefore, the total num-
ber of applications of our deduction rules is also
bounded by some polynomial in n. We thus con-
clude that the languages generated by the class TT-
MCTAG are all included in PTIME.
5 Conclusion and open problems
We have shown in this paper that the class of lan-
guages generated by TT-MCTAG is included in
PTIME, by characterizing the definition of TT-
MCTAG through some conditions that can be
tested locally. PTIME is one of the required
properties in the definition of the class of Mildly
Context-Sensitive (MCS) formalisms (Joshi et al,
1991). In order to settle membership in MCS for
TT-MCTAG, what is still missing is the constant-
growth property or, more generally, the semilin-
earity property.
Acknowledgments
The work of the first author has been sup-
ported by the DFG within the Emmy-Noether
Program. The second author has been partially
supported by MIUR under project PRIN No.
2007TJNZRE 002.
1001
References
Tilman Becker, Aravind K. Joshi, and Owen Rambow.
1991. Long-distance scrambling and tree adjoining
grammars. In Proceedings of ACL-Europe.
Tilman Becker, Owen Rambow, and Michael Niv.
1992. The Derivationel Generative Power of Formal
Systems or Scrambling is Beyond LCFRS. Tech-
nical Report IRCS-92-38, Institute for Research in
Cognitive Science, University of Pennsylvania.
Lucas Champollion. 2007. Lexicalized non-local MC-
TAG with dominance links is NP-complete. In Ger-
ald Penn and Ed Stabler, editors, Proceedings of
Mathematics of Language (MOL) 10, CSLI On-Line
Publications.
Joan Chen-Main and Aravind Joshi. 2007. Some
observations on a graphical model-theoretical ap-
proach and generative models. In Model Theoretic
Syntax at 10. Workshop, ESSLLI 2007, Dublin, Ire-
land.
Armin B. Cremers and Otto Mayer. 1973. On matrix
languages. Information and Control, 23:86?96.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoning Grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, pages
69?123. Springer, Berlin.
Aravind K. Joshi, Leon S. Levy, and Masako Taka-
hashi. 1975. Tree Adjunct Grammars. Journal of
Computer and System Science, 10:136?163.
A. Joshi, K. Vijay-Shanker, and D. Weir. 1991. The
convergence of mildly context-sensitive grammati-
cal formalisms. In P. Sells, S. Shieber, and T. Wa-
sow, editors, Foundational Issues in Natural Lan-
guage Processing. MIT Press, Cambridge MA.
Aravind K. Joshi. 1985. Tree adjoining grammars:
How much contextsensitivity is required ro provide
reasonable structural descriptions? In D. Dowty,
L. Karttunen, and A. Zwicky, editors, Natural Lan-
guage Parsing, pages 206?250. Cambridge Univer-
sity Press.
Laura Kallmeyer and Yannick Parmentier. 2008. On
the relation between Multicomponent Tree Adjoin-
ing Grammars with Tree Tuples (TT-MCTAG) and
Range Concatenation Grammars (RCG). In Carlos
Mart??n-Vide, Friedrich Otto, and Henning Fernaus,
editors, Language and Automata Theory and Ap-
plications. Second International Conference, LATA
2008, number 5196 in Lecture Notes in Computer
Science, pages 263?274. Springer-Verlag, Heidel-
berg Berlin.
Laura Kallmeyer. 2005. Tree-local multicomponent
tree adjoining grammars with shared nodes. Com-
putational Linguistics, 31(2):187?225.
Timm Lichte and Laura Kallmeyer. 2008. Factorizing
Complementation in a TT-MCTAG for German. In
Proceedings of the Ninth International Workshop on
Tree Adjoining Grammars and Related Formalisms
(TAG+9), pages 57?64, Tu?bingen, June.
Timm Lichte. 2007. An MCTAG with Tuples for Co-
herent Constructions in German. In Proceedings
of the 12th Conference on Formal Grammar 2007,
Dublin, Ireland.
Rebecca Nesson and Stuart Shieber. 2008. Syn-
chronous Vector TAG for Syntax and Semantics:
Control Verbs, Relative Clauses, and Inverse Link-
ing. In Proceedings of the Ninth International Work-
shop on Tree Adjoining Grammars and Related For-
malisms (TAG+9), Tu?bingen, June.
Owen Rambow and Giorgio Satta. 1992. Formal prop-
erties of non-locality. In Proceedings of 1st Interna-
tional Workshop on Tree Adjoining Grammars.
Owen Rambow and Giorgio Satta. 1994. A rewrit-
ing system for free word order syntax that is non-
local and mildly context sensitive. In C. Mart??n-
Vide, editor, Current Issues in Mathematical Lin-
guistics, North-Holland Linguistic series, Volume
56. Elsevier-North Holland, Amsterdam.
Owen Rambow, K. Vijay-shanker, and David Weir.
1995. Parsing d-Ttree grammars. In Proceedings of
the Fourth International Workshop on Parsing Tech-
nologies, Prague, pages 252?259.
Owen Rambow. 1994. Formal and Computational
Aspects of Natural Language Syntax. Ph.D. thesis,
University of Pennsylvania.
Giorgio Satta. 1995. The membership problem for un-
ordered vector languages. In Developments in Lan-
guage Theory, pages 267?275.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and Implementation of
Deductive Parsing. Journal of Logic Programming,
24(1&2):3?36.
Anders S?gaard, Timm Lichte, and Wolfgang Maier.
2007. The complexity of linguistically motivated
extensions of tree-adjoining grammar. In Recent
Advances in Natural Language Processing 2007,
Borovets, Bulgaria.
K. Vijay-Shanker and Aravind K. Joshi. 1985. Some
computational properties of Tree Adjoining Gram-
mars. In Proceedings of the 23rd Annual Meeting
of the Association for Computational Linguistics,
pages 82?93.
K. Vijay-Shanker, D. J. Weir, and A. K. Joshi. 1987.
Characterizing structural descriptions produced by
various grammatical formalisms. In 25th Meet-
ing of the Association for Computational Linguistics
(ACL?87).
David J. Weir. 1988. Characterizing mildly context-
sensitive grammar formalisms. Ph.D. thesis, Uni-
versity of Pennsylvania.
1002
Proceedings of the 10th Conference on Parsing Technologies, pages 121?132,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
On the Complexity of Non-Projective Data-Driven Dependency Parsing
Ryan McDonald
Google Inc.
76 Ninth Avenue
New York, NY 10028
ryanmcd@google.com
Giorgio Satta
University of Padua
via Gradenigo 6/A
I-35131 Padova, Italy
satta@dei.unipd.it
Abstract
In this paper we investigate several non-
projective parsing algorithms for depen-
dency parsing, providing novel polynomial
time solutions under the assumption that
each dependency decision is independent of
all the others, called here the edge-factored
model. We also investigate algorithms for
non-projective parsing that account for non-
local information, and present several hard-
ness results. This suggests that it is unlikely
that exact non-projective dependency pars-
ing is tractable for any model richer than the
edge-factored model.
1 Introduction
Dependency representations of natural language are
a simple yet flexible mechanism for encoding words
and their syntactic dependencies through directed
graphs. These representations have been thoroughly
studied in descriptive linguistics (Tesnie`re, 1959;
Hudson, 1984; Sgall et al, 1986; Mel?c?uk, 1988) and
have been applied in numerous language process-
ing tasks. Figure 1 gives an example dependency
graph for the sentence Mr. Tomash will remain as a
director emeritus, which has been extracted from the
Penn Treebank (Marcus et al, 1993). Each edge in
this graph represents a single syntactic dependency
directed from a word to its modifier. In this rep-
resentation all edges are labeled with the specific
syntactic function of the dependency, e.g., SBJ for
subject and NMOD for modifier of a noun. To sim-
plify computation and some important definitions,
an artificial token is inserted into the sentence as the
left most word and will always represent the root of
the dependency graph. We assume all dependency
graphs are directed trees originating out of a single
node, which is a common constraint (Nivre, 2005).
The dependency graph in Figure 1 is an exam-
ple of a nested or projective graph. Under the as-
sumption that the root of the graph is the left most
word of the sentence, a projective graph is one where
the edges can be drawn in the plane above the sen-
tence with no two edges crossing. Conversely, a
non-projective dependency graph does not satisfy
this property. Figure 2 gives an example of a non-
projective graph for a sentence that has also been
extracted from the Penn Treebank. Non-projectivity
arises due to long distance dependencies or in lan-
guages with flexible word order. For many lan-
guages, a significant portion of sentences require
a non-projective dependency analysis (Buchholz et
al., 2006). Thus, the ability to learn and infer non-
projective dependency graphs is an important prob-
lem in multilingual language processing.
Syntactic dependency parsing has seen a num-
ber of new learning and inference algorithms which
have raised state-of-the-art parsing accuracies for
many languages. In this work we focus on data-
drivenmodels of dependency parsing. These models
are not driven by any underlying grammar, but in-
stead learn to predict dependency graphs based on
a set of parameters learned solely from a labeled
corpus. The advantage of these models is that they
negate the need for the development of grammars
when adapting the model to new languages.
One interesting class of data-driven models are
121
Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and neighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and Pereira, 2006). How-
ever, in the data-driven parsing setting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our current
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known to have exact non-projective
implementations.
We then switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related Work
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorithms (Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). In the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, as
is the case for edge-factored models (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
122
mal dependency grammar is hard. In addition, the
work of Kahane et al (1998) provides a polynomial
parsing algorithm for a constrained class of non-
projective structures. Non-projective dependency
parsing can be related to certain parsing problems
defined for phrase structure representations, as for
instance immediate dominance CFG parsing (Barton
et al, 1987) and shake-and-bake translation (Brew,
1992).
Independently of this work, Koo et al (2007) and
Smith and Smith (2007) showed that the Matrix-
Tree Theorem can be used to train edge-factored
log-linear models of dependency parsing. Both stud-
ies constructed implementations that compare favor-
ably with the state-of-the-art. The work of Meila?
and Jaakkola (2000) is also of note. In that study
they use the Matrix Tree Theorem to develop a
tractable bayesian learning algorithms for tree belief
networks, which in many ways are closely related
to probabilistic dependency parsing formalisms and
the problems we address here.
2 Preliminaries
Let L = {l1, . . . , l|L|} be a set of permissible syn-
tactic edge labels and x = x0x1 ? ? ?xn be a sen-
tence such that x0=root. From this sentence we con-
struct a complete labeled directed graph (digraph)
Gx = (Vx, Ex) such that,
? Vx = {0, 1, . . . , n}
? Ex = {(i, j)k | ? i, j ? Vx and 1 ? k ? |L|}
Gx is a graph where each word in the sentence is a
node, and there is a directed edge between every pair
of nodes for every possible label. By its definition,
Gx is a multi-digraph, which is a digraph that may
have more than one edge between any two nodes.
Let (i, j)k represent the kth edge from i to j. Gx en-
codes all possible labeled dependencies between the
words of x. Thus every possible dependency graph
of x must be a subgraph of Gx.
Let i ?+ j be a relation that is true if and only
if there is a non-empty directed path from node i to
node j in some graph under consideration. A di-
rected spanning tree1 of a graph G, that originates
1A directed spanning tree is commonly referred to as a ar-
borescence in the graph theory literature.
out of node 0, is any subgraph T = (VT , ET ) such
that,
? VT = Vx and ET ? Ex
? ?j ? VT , 0 ?+ j if and only if j 6= 0
? If (i, j)k ? ET , then (i?, j)k
?
/? ET , ?i? 6= i
and/or k? 6= k.
Define T (G) as the set of all directed spanning trees
for a graph G. As McDonald et al (2005b) noted,
there is a one-to-one correspondence between span-
ning trees of Gx and labeled dependency graphs
of x, i.e., T (Gx) is exactly the set of all possible
projective and non-projective dependency graphs for
sentence x. Throughout the rest of this paper, we
will refer to any T ? T (Gx) as a valid dependency
graph for a sentence x. Thus, by definition, every
valid dependency graph must be a tree.
3 Edge-factored Models
In this section we examine the class of models that
assume each dependency decision is independent.
Within this setting, every edge in an induced graph
Gx for a sentence x will have an associated weight
wkij ? 0 that maps the k
th directed edge from node
i to node j to a real valued numerical weight. These
weights represents the likelihood of a dependency
occurring from word wi to word wj with label lk.
Define the weight of a spanning tree T = (VT , ET )
as the product of the edge weights
w(T ) =
?
(i,j)k?ET
wkij
It is easily shown that this formulation includes
the projective model of Paskin (2001) and the non-
projective model of McDonald et al (2005b).
The definition of wkij depends on the context in
which it is being used. For example, in the work of
McDonald et al (2005b) it is simply a linear classi-
fier that is a function of the words in the dependency,
the label of the dependency, and any contextual fea-
tures of the words in the sentence. In a generative
probabilistic model (such as Paskin (2001)) it could
represent the conditional probability of a word wj
being generated with a label lk given that the word
being modified is wi (possibly with some other in-
formation such as the orientation of the dependency
123
or the number of words betweenwi andwj). We will
attempt to make any assumptions about the formwkij
clear when necessary.
For the remainder of this section we discuss three
crucial problems for learning and inference while
showing that each can be computed tractably for the
non-projective case.
3.1 Finding the Argmax
The first problem of interest is finding the highest
weighted tree for a given input sentence x
T = argmax
T?T (Gx)
?
(i,j)k?ET
wkij
McDonald et al (2005b) showed that this can be
solved in O(n2) for unlabeled parsing using the
Chu-Liu-Edmonds algorithm for standard digraphs
(Chu and Liu, 1965; Edmonds, 1967). Unlike most
exact projective parsing algorithms, which use effi-
cient bottom-up chart parsing algorithms, the Chu-
Liu-Edmonds algorithm is greedy in nature. It be-
gins by selecting the single best incoming depen-
dency edge for each node j. It then post-processes
the resulting graph to eliminate cycles and then con-
tinues recursively until a spanning tree (or valid
dependency graph) results (see McDonald et al
(2005b) for details).
The algorithm is trivially extended to the multi-
digraph case for use in labeled dependency parsing.
First we note that if the maximum directed spanning
tree of a multi-digraph Gx contains any edge (i, j)k,
then we must have k = k? = argmaxk w
k
ij . Oth-
erwise we could simply substitute (i, j)k
?
in place
of (i, j)k and obtain a higher weighted tree. There-
fore, without effecting the solution to the argmax
problem, we can delete all edges in Gx that do not
satisfy this property. The resulting digraph is no
longer a multi-digraph and the Chu-Liu-Edmonds
algorithm can be applied directly. The new runtime
is O(|L|n2).
As a side note, the k-best argmax problem for di-
graphs can be solved in O(kn2) (Camerini et al,
1980). This can also be easily extended to the multi-
digraph case for labeled parsing.
3.2 Partition Function
A common step in many learning algorithms is to
compute the sum over the weight of all the possi-
ble outputs for a given input x. This value is often
referred to as the partition function due to its sim-
ilarity with a value by the same name in statistical
mechanics. We denote this value as Zx,
Zx =
?
T?T (Gx)
w(T ) =
?
T?T (Gx)
?
(i,j)k?ET
wki,j
To compute this sum it is possible to use the Matrix
Tree Theorem for multi-digraphs,
Matrix Tree Theorem (Tutte, 1984): Let G be a
multi-digraph with nodes V = {0, 1, . . . , n} and
edges E. Define (Laplacian) matrix Q as a (n +
1)?(n+1) matrix indexed from 0 to n. For all i and
j, define:
Qjj =
?
i6=j,(i,j)k?E
wkij & Qij =
?
i6=j,(i,j)k?E
?wkij
If the ith row and column are removed from Q to
produce the matrixQi, then the sum of the weights of
all directed spanning trees rooted at node i is equal
to |Qi| (the determinant of Qi).
Thus, if we construct Q for a graph Gx, then the de-
terminant of the matrix Q0 is equivalent to Zx. The
determinant of an n?n matrix can be calculated in
numerous ways, most of which takeO(n3) (Cormen
et al, 1990). The most efficient algorithms for cal-
culating the determinant of a matrix use the fact that
the problem is no harder than matrix multiplication
(Cormen et al, 1990). Matrix multiplication cur-
rently has known O(n2.38) implementations and it
has been widely conjectured that it can be solved in
O(n2) (Robinson, 2005). However, most algorithms
with sub-O(n3) running times require constants that
are large enough to negate any asymptotic advantage
for the case of dependency parsing. As a result, in
this work we use O(n3) as the runtime for comput-
ing Zx.
Since it takes O(|L|n2) to construct the matrix Q,
the entire runtime to compute Zx is O(n3 + |L|n2).
3.3 Edge Expectations
Another important problem for various learning
paradigms is to calculate the expected value of each
edge for an input sentence x,
?(i, j)k?x =
?
T?T (Gx)
w(T )? I((i, j)k, T )
124
Input: x = x0x1 ? ? ?xn
1. Construct Q O(|L|n2)
2. for j : 1 .. n O(n)
3. Q?jj = Qjj and Q
?
ij = Qij , 0 ? ?i ? n O(n)
4. Qjj = 1 and Qij = 0, 0 ? ?i ? n O(n)
5. for i : 0 .. n & i 6= j O(n)
6. Qij = ?1 O(1)
7. Zx = |Q0| O(n3)
8. ?(i, j)k?x = wkijZx, ?1 ? k ? |L| O(|L|)
9. end for
10. Qjj = Q?jj and Qij = Q
?
ij , 0 ? ?i ? n O(n)
11. end for
Figure 3: Algorithm to calculate ?(i, j)k?x in
O(n5 + |L|n2).
where I((i, j)k, T ) is an indicator function that is
one when the edge (i, j)k is in the tree T .
To calculate the expectation for the edge (i, j)k,
we can simply eliminate all edges (i?, j)k
?
6= (i, j)k
from Gx and calculate Zx. Zx will now be equal
to the sum of the weights of all trees that con-
tain (i, j)k. A naive implementation to compute
the expectation of all |L|n2 edges takes O(|L|n5 +
|L|2n4), since calculating Zx takes O(n3 + |L|n2)
for a single edge. However, we can reduce this con-
siderably by constructing Q a single time and only
making modifications to it when necessary. An al-
gorithm is given in Figure 3.3 that has a runtime of
O(n5 + |L|n2). This algorithm works by first con-
structing Q. It then considers edges from the node i
to the node j. Now, assume that there is only a single
edge from i to j and that that edge has a weight of 1.
Furthermore assume that this edge is the only edge
directed into the node j. In this case Q should be
modified so that Qjj = 1, Qij = ?1, and Qi?j = 0,
?i? 6= i, j (by the Matrix Tree Theorem). The value
of Zx under this new Q will be equivalent to the
weight of all trees containing the single edge from i
to j with a weight of 1. For a specific edge (i, j)k its
expectation is simplywkijZx, since we can factor out
the weight 1 edge from i to j in all the trees that con-
tribute to Zx and multiply through the actual weight
for the edge. The algorithm then reconstructs Q and
continues.
Following the work of Koo et al (2007) and Smith
and Smith (2007), it is possible to compute all ex-
pectations in O(n3 + |L|n2) through matrix inver-
sion. To make this paper self contained, we report
here their algorithm adapted to our notation. First,
consider the equivalence,
? logZx
?wkij
=
? logZx
?Zx
?Zx
?wkij
=
1
Zx
?
T?T (Gx)
w(T )
wkij
? I((i, j)k, T )
As a result, we can re-write the edge expectations as,
?(i, j)k? = Zxw
k
ij
? logZx
?wkij
= Zxw
k
ij
? log |Q0|
?wkij
Using the chain rule, we get,
? log |Q0|
?wkij
=
?
i?,j??1
? log |Q0|
?(Q0)i?j?
?(Q0)i?j?
?wkij
We assume the rows and columns of Q0 are in-
dexed from 1 so that the indexes of Q and Q0 co-
incide. To calculate ?(i, j)k? when i, j > 0, we can
use the fact that ? log |X|/Xij = (X?1)ji and that
?(Q0)i?j?/?wkij is non zero only when i
? = i and
j? = j or i? = j? = j to get,
?(i, j)k? = Zxw
k
ij [((Q
0)?1)jj ? ((Q
0)?1)ji]
When i = 0 and j > 0 the only non zero term of
this sum is when i? = j? = j and so
?(0, j)k? = Zxw
k
0j((Q
0)?1)jj
Zx and (Q0)?1 can both be calculated a single time,
each taking O(n3). Using these values, each expec-
tation is computed in O(1). Coupled with with the
fact that we need to construct Q and compute the
expectation for all |L|n2 possible edges, in total it
takes O(n3 + |L|n2) time to compute all edge ex-
pectations.
3.4 Comparison with Projective Parsing
Projective dependency parsing algorithms are well
understood due to their close connection to phrase-
based chart parsing algorithms. The work of Eis-
ner (1996) showed that the argmax problem for di-
graphs could be solved in O(n3) using a bottom-
up dynamic programming algorithm similar to CKY.
Paskin (2001) presented an O(n3) inside-outside al-
gorithm for projective dependency parsing using the
Eisner algorithm as its backbone. Using this al-
gorithm it is trivial to calculate both Zx and each
125
Projective Non-Projective
argmax O(n3 + |L|n2) O(|L|n2)
Zx O(n3 + |L|n2) O(n3 + |L|n2)
?(i, j)k?x O(n3 + |L|n2) O(n3 + |L|n2)
Table 1: Comparison of runtime for non-projective
and projective algorithms.
edge expectation. Crucially, the nested property of
projective structures allows edge expectations to be
computed in O(n3) from the inside-outside values.
It is straight-forward to extend the algorithms of Eis-
ner (1996) and Paskin (2001) to the labeled case
adding only a factor of O(|L|n2).
Table 1 gives an overview of the computational
complexity for the three problems considered here
for both the projective and non-projective case. We
see that the non-projective case compares favorably
for all three problems.
4 Applications
To motivate the algorithms from Section 3, we
present some important situations where each cal-
culation is required.
4.1 Inference Based Learning
Many learning paradigms can be defined as
inference-based learning. These include the per-
ceptron (Collins, 2002) and its large-margin vari-
ants (Crammer and Singer, 2003; McDonald et al,
2005a). In these settings, a models parameters are
iteratively updated based on the argmax calculation
for a single or set of training instances under the
current parameter settings. The work of McDon-
ald et al (2005b) showed that it is possible to learn
a highly accurate non-projective dependency parser
for multiple languages using the Chu-Liu-Edmonds
algorithm for unlabeled parsing.
4.2 Non-Projective Min-Risk Decoding
In min-risk decoding the goal is to find the depen-
dency graph for an input sentence x, that on average
has the lowest expected risk,
T = argmin
T?T (Gx)
?
T ??T (Gx)
w(T ?)R(T, T ?)
where R is a risk function measuring the error be-
tween two graphs. Min-risk decoding has been
studied for both phrase-structure parsing and depen-
dency parsing (Titov and Henderson, 2006). In that
work, as is common with many min-risk decoding
schemes, T (Gx) is not the entire space of parse
structures. Instead, this set is usually restricted to
a small number of possible trees that have been pre-
selected by some baseline system. In this subsection
we show that when the risk function is of a specific
form, this restriction can be dropped. The result is
an exact min-risk decoding procedure.
Let R(T, T ?) be the Hamming distance between
two dependency graphs for an input sentence x =
x0x1 ? ? ?xn,
R(T, T ?) = n ?
?
(i,j)k?ET
I((i, j)k, T ?)
This is a common definition of risk between two
graphs as it corresponds directly to labeled depen-
dency parsing accuracy (McDonald et al, 2005a;
Buchholz et al, 2006). Some algebra reveals,
T = argmin
T?T (Gx)
X
T ??T (Gx)
w(T ?)R(T, T ?)
= argmin
T?T (Gx)
X
T ??T (Gx)
w(T ?)[n ?
X
(i,j)k?ET
I((i, j)k, T ?)]
= argmin
T?T (Gx)
?
X
T ??T (Gx)
w(T ?)
X
(i,j)k?ET
I((i, j)k, T ?)
= argmin
T?T (Gx)
?
X
(i,j)k?ET
X
T ??T (Gx)
w(T ?)I((i, j)k, T ?)
= argmax
T?T (Gx)
X
(i,j)k?ET
X
T ??T (Gx)
w(T ?)I((i, j)k, T ?)
= argmax
T?T (Gx)
Y
(i,j)k?ET
e
P
T ??T (Gx)
w(T ?)I((i,j)k,T ?)
= argmax
T?T (Gx)
Y
(i,j)k?ET
e?(i,j)
k?x
By setting the edge weights to wkij = e
?(i,j)k?x we
can directly solve this problem using the edge ex-
pectation algorithm described in Section 3.3 and the
argmax algorithm described in Section 3.1.
4.3 Non-Projective Log-Linear Models
Conditional Random Fields (CRFs) (Lafferty et al,
2001) are global discriminative learning algorithms
for problems with structured output spaces, such as
dependency parsing. For dependency parsing, CRFs
would define the conditional probability of a depen-
dency graph T for a sentence x as a globally nor-
126
malized log-linear model,
p(T |x) =
?
(i,j)k?ET
ew?f(i,j,k)
?
T ??T (Gx)
?
(i,j)k?ET ?
ew?f(i,j,k)
=
?
(i,j)k?ET
wkij
?
T ??T (Gx)
?
(i,j)k?ET ?
wkij
=
w(T )
Zx
Here, the weights wkij are potential functions over
each edge defined as an exponentiated linear classi-
fier with weight vector w ? RN and feature vector
f(i, j, k) ? RN , where fu(i, j, k) ? R represents a
single dimension of the vector f. The denominator,
which is exactly the sum over all graph weights, is a
normalization constant forcing the conditional prob-
ability distribution to sum to one.
CRFs set the parameters w to maximize the log-
likelihood of the conditional probability over a train-
ing set of examples T = {(x?, T?)}
|T |
?=1,
w = argmax
w
?
?
log p(T?|x?)
This optimization can be solved through a vari-
ety of iterative gradient based techniques. Many
of these require the calculation of feature expecta-
tions over the training set under model parameters
for the previous iteration. First, we note that the
feature functions factor over edges, i.e., fu(T ) =?
(i,j)k?ET
fu(i, j, k). Because of this, we can use
edge expectations to compute the expectation of ev-
ery feature fu. Let ?fu?x? represent the expectation
of feature fu for the training instance x?,
?fu?x? =
X
T?T (Gx? )
p(T |x?)fu(T )
=
X
T?T (Gx? )
p(T |x?)
X
(i,j)k?ET
fu(i, j, k)
=
X
T?T (Gx? )
w(T )
Zx
X
(i,j)k?ET
fu(i, j, k)
=
1
Zx
X
(i,j)k?Ex?
X
T?T (Gx)
w(T )I((i, j)k, T )fu(i, j, k)
=
1
Zx
X
(i,j)k?Ex?
?(i, j)k?x?fu(i, j, k)
Thus, we can calculate the feature expectation per
training instance using the algorithms for comput-
ing Zx and edge expectations. Using this, we can
calculate feature expectations over the entire train-
ing set,
?fu?T =
?
?
p(x?)?fu?x?
where p(x?) is typically set to 1/|T |.
4.4 Non-projective Generative Parsing Models
A generative probabilistic dependency model over
some alphabet ? consists of parameters pkx,y asso-
ciated with each dependency from word x ? ? to
word y ? ? with label lk ? L. In addition, we im-
pose 0 ? pkx,y ? 1 and the normalization conditions?
y,k p
k
x,y = 1 for each x ? ?. We define a gen-
erative probability model p over trees T ? T (Gx)
and a sentence x = x0x1 ? ? ?xn conditioned on the
sentence length, which is always known,
p(x, T |n) = p(x|T, n)p(T |n)
=
?
(i,j)k?ET
pkxi,xj p(T |n)
We assume that p(T |n) = ? is uniform. This model
is studied specifically by Paskin (2001). In this
model, one can view the sentence as being generated
recursively in a top-down process. First, a tree is
generated from the distribution p(T |n). Then start-
ing at the root of the tree, every word generates all of
its modifiers independently in a recursive breadth-
first manner. Thus, pkx,y represents the probability
of the word x generating its modifier y with label
lk. This distribution is usually smoothed and is of-
ten conditioned on more information including the
orientation of x relative to y (i.e., to the left/right)
and distance between the two words. In the super-
vised setting this model can be trained with maxi-
mum likelihood estimation, which amounts to sim-
ple counts over the data. Learning in the unsuper-
vised setting requires EM and is discussed in Sec-
tion 4.4.2.
Another generative dependency model of interest
is that given by Klein and Manning (2004). In this
model the sentence and tree are generated jointly,
which allows one to drop the assumption that p(T |n)
is uniform. This requires the addition to the model
of parameters px,STOP for each x ? ?, with the nor-
malization condition px,STOP +
?
y,k p
k
x,y = 1. It is
possible to extend the model of Klein and Manning
127
(2004) to the non-projective case. However, the re-
sulting distribution will be over multisets of words
from the alphabet instead of strings. The discus-
sion in this section is stated for the model in Paskin
(2001); a similar treatment can be developed for the
model in Klein and Manning (2004).
4.4.1 Language Modeling
A generative model of dependency structure
might be used to determine the probability of a sen-
tence x by marginalizing out all possible depen-
dency trees,
p(x|n) =
?
T?T (Gx)
p(x, T |n)
=
?
T?T (Gx)
p(x|T, n)p(T |n)
= ?
?
T?T (Gx)
?
(i,j)k?ET
pkxi,xj = ?Zx
This probability can be used directly as a non-
projective syntactic language model (Chelba et al,
1997) or possibly interpolated with a separate n-
gram model.
4.4.2 Unsupervised Learning
In unsupervised learning we train our model on
a sample of unannotated sentences X = {x?}
|X |
?=1.
Let |x?| = n? and p(T |n?) = ??. We choose the
parameters that maximize the log-likelihood
|X |?
?=1
log(p(x?|n?)) =
=
|X |?
?=1
log(
?
T?T (Gx? )
p(x?|T, n?)) +
|X |?
?=1
log(??),
viewed as a function of the parameters and subject
to the normalization conditions, i.e.,
?
y,k p
k
x,y = 1
and pkx,y ? 0.
Let x?i be the ith word of x?. By solving the
above constrained optimization problem with the
usual Lagrange multipliers method one gets
pkx,y =
=
?|X |
?=1
1
Zx?
?
i : x?i = x,
j : x?j = y
?(i, j)k?x?
?|X |
?=1
1
Zx?
?
y?,k?
?
i : x?i = x,
j? : x?j? = y
?
?(i, j?)k??x?
,
where for each x? the expectation ?(i, j)k?x? is de-
fined as in Section 3, but with the weight w(T ) re-
placed by the probability distribution p(x?|T, n?).
The above |L| ? |?|2 relations represent a non-
linear system of equations. There is no closed form
solution in the general case, and one adopts the ex-
pectation maximization (EM) method, which is a
specialization of the standard fixed-point iteration
method for the solution of non-linear systems. We
start with some initial assignment of the parameters
and at each iteration we use the induced distribu-
tion p(x?|T, n?) to compute a refined value for the
parameters themselves. We are always guaranteed
that the Kullback-Liebler divergence between two
approximated distributions computed at successive
iterations does not increase, which implies the con-
vergence of the method to some local maxima (with
the exception of saddle points).
Observe that at each iteration we can compute
quantities ?(i, j)k?x? and Zx? in polynomial time
using the algorithms from Section 3 with pkx?i,x?j
in place of wki,j . Furthermore, under some standard
conditions the fixed-point iteration method guaran-
tees a constant number of bits of precision gain for
the parameters at each iteration, resulting in overall
polynomial time computation in the size of the input
and in the required number of bits for the precision.
As far as we know, this is the first EM learning algo-
rithm for the model in Paskin (2001) working in the
non-projective case. The projective case has been
investigated in Paskin (2001).
5 Beyond Edge-factored Models
We have shown that several computational problems
related to parsing can be solved in polynomial time
for the class of non-projective dependency models
with the assumption that dependency relations are
mutually independent. These independence assump-
tions are unwarranted, as it has already been estab-
lished that modeling non-local information such as
arity and nearby parsing decisions improves the ac-
curacy of dependency models (Klein and Manning,
2002; McDonald and Pereira, 2006).
In the spirit of our effort to understand the nature
of exact non-projective algorithms, we examine de-
pendency models that introduce arity constraints as
well as permit edge decisions to be dependent on a
128
limited neighbourhood of other edges in the graph.
Both kinds of models can no longer be considered
edge-factored, since the likelihood of a dependency
occurring in a particular analysis is now dependent
on properties beyond the edge itself.
5.1 Arity
One feature of the edge-factored models is that no
restriction is imposed on the arity of the nodes in the
dependency trees. As a consequence, these models
can generate dependency trees of unbounded arity.
We show below that this is a crucial feature in the
development of the complexity results we have ob-
tained in the previous sections.
Let us assume a graph G(?)x = (Vx, Ex) defined
as before, but with the additional condition that each
node i ? Vx is associated with an integer value
?(i) ? 0. T (G(?)x ) is now defined as the set of all
directed spanning trees for G(?)x rooted in node 0,
such that every node i ? Vx has arity smaller than or
equal to ?(i). We now introduce a construction that
will be used to establish several hardness results for
the computational problems discussed in this paper.
Recall that a Hamiltonian path in a directed graph
G is a directed path that visits all of the nodes of G
exactly once.
Let G be some directed graph with set of nodes
V = {1, 2, . . . , n}. We construct a target graph
G(?)x = (Vx, Ex) with Vx = V ? {0} (0 the root
node) and |L| = 1. For each i, j ? Vx with i 6= j,
we add an edge (i, j)1 to Ex. We set w1i,j = 1 if
there is an edge from i to j in G, or else if i or j
is the root node 0, and w1i,j = 0 otherwise. Fur-
thermore, we set ?(i) = 1 for each i ? Vx. This
construction can be clearly carried out in log-space.
Note that each T ? T (G(?)x ) must be a monadic
tree with weight equal to either 0 or 1. It is not dif-
ficult to see that if w(T ) = 1, then when we remove
the root node 0 from T we obtain a Hamiltonian path
in G. Conversely, each Hamiltonian path in G can
be extended to a spanning tree T ? T (G(?)x ) with
w(T ) = 1, by adding the root node 0.
Using the above observations, it can be shown that
the solution of the argmax problem for G(?)x pro-
vides some Hamiltonian directed path in G. The lat-
ter search problem is FNP-hard, and is unlikely to
be solved in polynomial time. Furthermore, quan-
tity Zx provides the count of the Hamiltonian di-
rected paths in G, and for each i ? V , the expecta-
tion ?(0, i)1?x provides the count of the Hamiltonian
directed paths in G starting from node i. Both these
counting problems are #P-hard, and very unlikely to
have polynomial time solutions.
This result helps to relate the hardness of data-
driven models to the commonly known hardness
results in the grammar-driven literature given by
Neuhaus and Bo?ker (1997). In that work, an arity
constraint is included in their minimal grammar.
5.2 Vertical and Horizontal Markovization
In general, we would like to say that every depen-
dency decision is dependent on every other edge in
a graph. However, modeling dependency parsing in
such a manner would be a computational nightmare.
Instead, we would like to make a Markov assump-
tion over the edges of the tree, in a similar way that
a Markov assumption can be made for sequential
classification problems in order to ensure tractable
learning and inference.
Klein and Manning (2003) distinguish between
two kinds of Markovization for unlexicalized CFG
parsing. The first is vertical Markovization, which
makes the generation of a non-terminal dependent
on other non-terminals that have been generated at
different levels in the phrase-structure tree. The
second is horizontal Markovization, which makes
the generation of a non-terminal dependent on other
non-terminals that have been generated at the same
level in the tree.
For dependency parsing there are analogous no-
tions of vertical and horizontal Markovization for a
given edge (i, j)k. First, let us define the vertical and
horizontal neighbourhoods of (i, j)k. The vertical
neighbourhood includes all edges in any path from
the root to a leaf that passes through (i, j)k. The hor-
izontal neighbourhood contains all edges (i, j?)k
?
.
Figure 4 graphically displays the vertical and hor-
izontal neighbourhoods for an edge in the depen-
dency graph from Figure 1.
Vertical and horizontal Markovization essentially
allow the score of the graph to factor over a larger
scope of edges, provided those edges are in the same
vertical or horizontal neighbourhood. A dth order
factorization is one in which the score factors only
over the d nearest edges in the neighbourhoods. In
129
Figure 4: Vertical and Horizontal neighbourhood for
the edge from will to remain.
McDonald and Pereira (2006), it was shown that
non-projective dependency parsing with horizontal
Markovization is FNP-hard. In this study we com-
plete the picture and show that vertical Markoviza-
tion is also FNP-hard.
Consider a first-order vertical Markovization in
which the score for a dependency graph factors over
pairs of vertically adjacent edges2,
w(T ) =
?
(h,i)k,(i,j)k??ET
k
hiw
k?
ij
where khiw
k?
ij is the weight of including both edges
(h, i)k and (i, j)k
?
in the dependency graph. Note
that this formulation does not include any contribu-
tions from dependencies that have no vertically adja-
cent neighbours, i.e., any edge (0, i)k such that there
is no edge (i, j)k
?
in the graph. We can easily rec-
tify this by inserting a second root node, say 0?, and
including the weights k0?0w
k?
0i . To ensure that only
valid dependency graphs get a weight greater than
zero, we can set khiw
k?
ij = 0 if i = 0
? and k0?iw
k?
ij = 0
if i 6= 0.
Now, consider the NP-complete 3D-matching
problem (3DM). As input we are given three sets
of size m, call them A, B and C, and a set S ?
A?B ?C. The 3DM problem asks if there is a set
S? ? S such that |S?| = m and for any two tuples
(a, b, c), (a?, b?, c?) ? S? it is the case that a 6= a?,
b 6= b?, and c 6= c?.
2McDonald and Pereira (2006) define this as a second-order
Markov assumption. This is simply a difference in terminology
and does not represent any meaningful distinction.
We can reduce the 3D-matching problem to the
first-order vertical Markov parsing problem by con-
structing a graph G = (V,E), such that L =
A ? B ? C, V = {0?, 0} ? A ? B ? C and E =
{(i, j)k | i, j ? V, k ? L}. The set E contains mul-
tiple edges between ever pair of nodes, each edge
taking on a label representing a single element of
the set A ? B ? C. Now, define k0?0w
k?
0a = 1, for all
a ? A and k, k? ? A ? B ? C, and b0aw
c
ab = 1, for
all a ? A and b ? B and c ? C, and cabw
c
bc = 1, for
all (a, b, c) ? S. All other weights are set to zero.
We show below that there exists a bijection be-
tween the set of valid 3DMs for S and the set of non-
zero weighted dependency graphs in T (G). First, it
is easy to show that for any 3DM S?, there is a rep-
resentative dependency graph that has a weight of
1. This graph simply consists of the edges (0, a)b,
(a, b)c, and (b, c)c, for all (a, b, c) ? S?, plus an ar-
bitrarily labeled edge from 0? to 0.
To prove the reverse, consider a graph with weight
1. This graph must have a weight 1 edge into the
node a of the form (0, a)b since the graph must be
spanning. By the definition of the weight function,
in any non-zero weighted tree, a must have a sin-
gle outgoing edge, and that edge must be directed
into the node b. Let?s say that this edge is (a, b)c.
Then again by the weighting function, in any non-
zero weighted graph, b must have a single outgoing
edge that is directed into c, in particular the edge
(b, c)c. Thus, for any node a, there is a single path
directed out of it to a single leaf c ? C. We can
then state that the only non-zero weighted depen-
dency graph is one where each a ? A, b ? B and
c ? C occurs in exactly one ofm disjoint paths from
the root of the form 0 ? a ? b ? c. This is be-
cause the label of the single edge going into node a
will determine exactly the node b that the one outgo-
ing edge from a must go into. The label of that edge
determines exactly the single outgoing edge from b
into some node c. Now, since the weighting func-
tion ensures that the only non-zero weighted paths
into any leaf node c correspond directly to elements
of S, each of the m disjoint paths represent a single
tuple in a 3DM. Thus, if there is a non-zero weighted
graph in T (G), then it must directly correspond to a
valid 3DM, which concludes the proof.
Note that any dth order Markovization can be em-
bedded into a d + 1th Markovization. Thus, this re-
130
sult also holds for any arbitrary Markovization.
6 Discussion
In this paper we have shown that many important
learning and inference problems can be solved effi-
ciently for non-projective edge-factored dependency
models by appealing to the Matrix Tree Theorem
for multi-digraphs. These results extend the work
of McDonald et al (2005b) and help to further our
understanding of when exact non-projective algo-
rithms can be employed. When this analysis is cou-
pled with the projective parsing algorithms of Eisner
(1996) and Paskin (2001) we begin to get a clear pic-
ture of the complexity for data-driven dependency
parsing within an edge-factored framework. To fur-
ther justify the algorithms presented here, we out-
lined a few novel learning and inference settings in
which they are required.
However, for the non-projective case, moving
beyond edge-factored models will almost certainly
lead to intractable parsing problems. We have pro-
vided further evidence for this by proving the hard-
ness of incorporating arity constraints and hori-
zontal/vertical edge Markovization, both of which
incorporate information unavailable to an edge-
factored model. The hardness results provided
here are also of interest since both arity constraints
and Markovization can be incorporated efficiently
in the projective case through the straight-forward
augmentation of the underlying chart parsing algo-
rithms used in the projective edge-factored models.
This highlights a fundamental difference between
the nature of projective parsing algorithms and non-
projective parsing algorithms. On the projective
side, all algorithms use a bottom-up chart parsing
framework to search the space of nested construc-
tions. On the non-projective side, algorithms are
either greedy-recursive in nature (i.e., the Chu-Liu-
Edmonds algorithm) or based on the calculation of
the determinant of a matrix (i.e., the partition func-
tion and edge expectations).
Thus, the existence of bottom-up chart parsing
algorithms for projective dependency parsing pro-
vides many advantages. As mentioned above, it
permits simple augmentation techniques to incorpo-
rate non-local information such as arity constraints
and Markovization. It also ensures the compatibility
of projective parsing algorithms with many impor-
tant natural language processing methods that work
within a bottom-up chart parsing framework, includ-
ing information extraction (Miller et al, 2000) and
syntax-based machine translation (Wu, 1996).
The complexity results given here suggest that
polynomial chart-parsing algorithms do not exist
for the non-projective case. Otherwise we should
be able to augment them and move beyond edge-
factored models without encountering intractability
? just like the projective case. An interesting line
of research is to investigate classes of non-projective
structures that can be parsed with chart-parsing algo-
rithms and how these classes relate to the languages
parsable by other syntactic formalisms.
Acknowledgments
Thanks to Ben Taskar for pointing out the work of
Meila? and Jaakkola (2000). Thanks to David Smith,
Noah Smith and Michael Collins for making drafts
of their EMNLP papers available.
References
G. E. Barton, R. C. Berwick, and E. S. Ristad. 1987.
Computational Complexity and Natural Language.
MIT Press, Cambridge, MA.
C. Brew. 1992. Letting the cat out of the bag: Generation
for Shake-and-Bake MT. In Proc. COLING.
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. CoNLL-X shared task on multilingual depen-
dency parsing. In Proc. CoNLL.
P. M. Camerini, L. Fratta, and F. Maffioli. 1980. The k
best spanning arborescences of a network. Networks,
10(2):91?110.
C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khudan-
pur, L. Mangu, H. Printz, E.S. Ristad, R. Rosenfeld,
A. Stolcke, and D. Wu. 1997. Structure and per-
formance of a dependency language model. In Eu-
rospeech.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
T.H. Cormen, C.E. Leiserson, and R.L. Rivest. 1990. In-
troduction to Algorithms. MIT Press/McGraw-Hill.
131
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. JMLR.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING.
K. Hall and V. No?va?k. 2005. Corrective modeling for
non-projective dependency parsing. In Proc. IWPT.
H. Hirakawa. 2006. Graph branch algorithm: An opti-
mum tree search method for scored dependency graph
with arc co-occurrence constraints. In Proc. ACL.
R. Hudson. 1984. Word Grammar. Blackwell.
S. Kahane, A. Nasr, and O Rambow. 1998. Pseudo-
projectivity: A polynomially parsable non-projective
dependency grammar. In Proc. ACL.
D. Klein and C.D. Manning. 2002. Fast exact natu-
ral language parsing with a factored model. In Proc.
NIPS.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Proc. ACL.
D. Klein and C. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. ACL.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the matrix-tree theo-
rem. In Proc. EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc
EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT/EMNLP.
M. Meila? and T. Jaakkola. 2000. Tractable Bayesian
learning of tree belief networks. In Proc. UAI.
I.A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
S. Miller, H. Fox, L.A. Ramshaw, and R.M. Weischedel.
2000. A novel use of statistical parsing to extract in-
formation from text. In Proc NAACL, pages 226?233.
P. Neuhaus and N. Bo?ker. 1997. The complexity
of recognition of linguistically adequate dependency
grammars. In Proc. ACL.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. ACL.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of english text. In Proc. COLING.
J. Nivre. 2005. Dependency grammar and dependency
parsing. Technical Report MSI report 05133, Va?xjo?
University: School of Mathematics and Systems Engi-
neering.
M.A. Paskin. 2001. Cubic-time parsing and learning al-
gorithms for grammatical bigram models. Technical
Report UCB/CSD-01-1148, Computer Science Divi-
sion, University of California Berkeley.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proc. EMNLP.
S. Robinson. 2005. Toward an optimal algorithm for
matrix multiplication. News Journal of the Society for
Industrial and Applied Mathematics, 38(9).
P. Sgall, E. Hajic?ova?, and J. Panevova?. 1986. The Mean-
ing of the Sentence in Its Pragmatic Aspects. Reidel.
D.A. Smith and N.A. Smith. 2007. Probabilistic models
of nonprojective dependency trees. In Proc. EMNLP.
L. Tesnie`re. 1959. E?le?ments de syntaxe structurale. Edi-
tions Klincksieck.
I. Titov and J. Henderson. 2006. Bayes risk minimiza-
tion in natural language parsing. University of Geneva
technical report.
W.T. Tutte. 1984. Graph Theory. Cambridge University
Press.
W. Wang and M. P. Harper. 2004. A statistical constraint
dependency grammar (CDG) parser. In Workshop on
Incremental Parsing: Bringing Engineering and Cog-
nition Together (ACL).
D. Wu. 1996. A polynomial-time algorithm for statisti-
cal machine translation. In Proc. ACL.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
IWPT.
132
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 1?12,
Paris, October 2009. c?2009 Association for Computational Linguistics
Parsing Algorithms based on Tree Automata
Andreas Maletti
Departament de Filologies Roma`niques
Universitat Rovira i Virgili, Tarragona, Spain
andreas.maletti@urv.cat
Giorgio Satta
Department of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
We investigate several algorithms related
to the parsing problem for weighted au-
tomata, under the assumption that the in-
put is a string rather than a tree. This
assumption is motivated by several natu-
ral language processing applications. We
provide algorithms for the computation of
parse-forests, best tree probability, inside
probability (called partition function), and
prefix probability. Our algorithms are ob-
tained by extending to weighted tree au-
tomata the Bar-Hillel technique, as defined
for context-free grammars.
1 Introduction
Tree automata are finite-state devices that recog-
nize tree languages, that is, sets of trees. There
is a growing interest nowadays in the natural
language parsing community, and especially in
the area of syntax-based machine translation, for
probabilistic tree automata (PTA) viewed as suit-
able representations of grammar models. In fact,
probabilistic tree automata are generatively more
powerful than probabilistic context-free gram-
mars (PCFGs), when we consider the latter as de-
vices that generate tree languages. This difference
can be intuitively understood if we consider that a
computation by a PTA uses hidden states, drawn
from a finite set, that can be used to transfer infor-
mation within the tree structure being recognized.
As an example, in written English we can em-
pirically observe different distributions in the ex-
pansion of so-called noun phrase (NP) nodes, in
the contexts of subject and direct-object positions,
respectively. This can be easily captured using
some states of a PTA that keep a record of the dif-
ferent contexts. In contrast, PCFGs are unable to
model these effects, because NP node expansion
should be independent of the context in the deriva-
tion. This problem for PCFGs is usually solved by
resorting to so-called parental annotations (John-
son, 1998), but this, of course, results in a different
tree language, since these annotations will appear
in the derived tree.
Most of the theoretical work on parsing and es-
timation based on PTA has assumed that the in-
put is a tree (Graehl et al, 2008), in accordance
with the very definition of these devices. How-
ever, both in parsing as well as in machine transla-
tion, the input is most often represented as a string
rather than a tree. When the input is a string, some
trick is applied to map the problem back to the
case of an input tree. As an example in the con-
text of machine translation, assume a probabilistic
tree transducer T as a translation model, and an
input string w to be translated. One can then inter-
mediately construct a tree automaton Mw that rec-
ognizes the set of all possible trees that have w as
yield, with internal nodes from the input alphabet
of T . This automaton Mw is further transformed
into a tree transducer implementing a partial iden-
tity translation, and such a transducer is composed
with T (relation composition). This is usually
called the ?cascaded? approach. Such an approach
can be easily applied also to parsing problems.
In contrast with the cascaded approach above,
which may be rather inefficient, in this paper we
investigate a more direct technique for parsing
strings based on weighted and probabilistic tree
automata. We do this by extending to weighted
tree automata the well-known Bar-Hillel construc-
tion defined for context-free grammars (Bar-Hillel
et al, 1964) and for weighted context-free gram-
mars (Nederhof and Satta, 2003). This provides
an abstract framework under which several pars-
ing algorithms can be directly derived, based on
weighted tree automata. We discuss several appli-
cations of our results, including algorithms for the
computation of parse-forests, best tree probability,
inside probability (called partition function), and
prefix probability.
1
2 Preliminary definitions
Let S be a nonempty set and ? be an associative
binary operation on S. If S contains an element 1
such that 1 ? s = s = s ? 1 for every s ? S, then
(S, ?, 1) is a monoid. A monoid (S, ?, 1) is com-
mutative if the equation s1 ? s2 = s2 ? s1 holds
for every s1, s2 ? S. A commutative semiring
(S,+, ?, 0, 1) is a nonempty set S on which a bi-
nary addition + and a binary multiplication ? have
been defined such that the following conditions are
satisfied:
? (S,+, 0) and (S, ?, 1) are commutative
monoids,
? ? distributes over + from both sides, and
? s ? 0 = 0 = 0 ? s for every s ? S.
A weighted string automaton, abbreviated WSA,
(Schu?tzenberger, 1961; Eilenberg, 1974) is a sys-
tem M = (Q,?,S, I, ?, F ) where
? Q is a finite alphabet of states,
? ? is a finite alphabet of input symbols,
? S = (S,+, ?, 0, 1) is a semiring,
? I : Q? S assigns initial weights,
? ? : Q???Q? S assigns a weight to each
transition, and
? F : Q? S assigns final weights.
We now proceed with the semantics of M . Let
w ? ?? be an input string of length n. For each
integer i with 1 ? i ? n, we write w(i) to denote
the i-th character of w. The set Pos(w) of posi-
tions of w is {i | 0 ? i ? n}. A run of M on w
is a mapping r : Pos(w) ? Q. We denote the set
of all such runs by RunM (w). The weight of a
run r ? RunM (w) is
wtM (r) =
n?
i=1
?(r(i? 1), w(i), r(i)) .
We assume the right-hand side of the above equa-
tion evaluates to 1 in case n = 0. The WSA M
recognizes the mapping M : ?? ? S, which is
defined for every w ? ?? of length n by1
M(w) = ?
r?RunM (w)
I(r(0)) ?wtM (r) ?F (r(n)) .
In order to define weighted tree automata (Bers-
tel and Reutenauer, 1982; E?sik and Kuich, 2003;
Borchardt, 2005), we need to introduce some addi-
tional notation. Let ? be a ranked alphabet, that
1We overload the symbolM to denote both an automaton
and its recognized mapping. However, the intended meaning
will always be clear from the context.
is, an alphabet whose symbols have an associated
arity. We write ?k to denote the set of all k-ary
symbols in ?. We use a special symbol e ? ?0
to syntactically represent the empty string ?. The
set of ?-trees, denoted by T?, is the smallest set
satisfying both of the following conditions
? for every ? ? ?0, the single node labeled ?,
written ?(), is a tree of T?,
? for every ? ? ?k with k ? 1 and for every
t1, . . . , tk ? T?, the tree with a root node la-
beled ? and trees t1, . . . , tk as its k children,
written ?(t1, . . . , tk), belongs to T?.
As a convention, throughout this paper we assume
that ?(t1, . . . , tk) denotes ?() if k = 0. The size
of the tree t ? T?, written |t|, is defined as the
number of occurrences of symbols from ? in t.
Let t = ?(t1, . . . , tk). The yield of t is recur-
sively defined by
yd(t) =
?
??
??
? if ? ? ?0 \ {e}
? if ? = e
yd(t1) ? ? ? yd(tk) otherwise.
The set of positions of t, denoted by Pos(t), is
recursively defined by
Pos(?(t1, . . . , tk)) =
{?} ? {iw | 1 ? i ? k,w ? Pos(ti)} .
Note that |t| = |Pos(t)| and, according to our con-
vention, when k = 0 the above definition provides
Pos(?()) = {?}. We denote the symbol of t at
position w by t(w) and its rank by rkt(w).
A weighted tree automaton (WTA) is a system
M = (Q,?,S, ?, F ) where
? Q is a finite alphabet of states,
? ? is a finite ranked alphabet of input symbols,
? S = (S,+, ?, 0, 1) is a semiring,
? ? is an indexed family (?k)k?N of mappings
?k : ?k ? SQ?Qk , and
? F : Q? S assigns final weights.
In the above definition, Qk is the set of all strings
over Q having length k, with Q0 = {?}. Fur-
ther note that SQ?Qk is the set of all matrices
with elements in S, row index set Q, and column
index set Qk. Correspondingly, we will use the
common matrix notation and write instances of ?
in the form ?k(?)q0,q1???qk . Finally, we assume
q1 ? ? ? qk = ? if k = 0.
We define the semantics also in terms of runs.
Let t ? T?. A run of M on t is a mapping
r : Pos(t)? Q. We denote the set of all such runs
2
by RunM (t). The weight of a run r ? RunM (t)
is
wtM (r) =
?
w?Pos(t)
rkt(w)=k
?k(t(w))r(w),r(w1)???r(wk) .
Note that, according to our convention, the string
r(w1) ? ? ? r(wk) denotes ? when k = 0. The
WTA M recognizes the mapping M : T? ? S,
which is defined by
M(t) = ?
r?RunM (t)
wtM (r) ? F (r(?))
for every t ? T?. We say that t is recognized
by M if M(t) 6= 0.
In our complexity analyses, we use the follow-
ing measures. The size of a transition (p, ?, q) in
(the domain of ? in) a WSA is |p?q| = 3. The size
of a transition in a WTA, viewed as an instance
(?, q0, q1 ? ? ? qk) of some mapping ?k, is defined
as |?q0 ? ? ? qk|, that is, the rank of the input symbol
occurring in the transition plus two. Finally, the
size |M | of an automaton M (WSA or WTA) is
defined as the sum of the sizes of its nonzero tran-
sitions. Note that this does not take into account
the size of the representation of the weights.
3 Binarization
We introduce in this section a specific transfor-
mation of WTA, called binarization, that reduces
the transitions of the automaton to some normal
form in which no more than three states are in-
volved. This transformation maps the set of rec-
ognized trees into a special binary form, in such a
way that the yields of corresponding trees and their
weights are both preserved. We use this transfor-
mation in the next section in order to guarantee
the computational efficiency of the parsing algo-
rithm we develop. The standard ?first-child, next-
sibling? binary encoding for trees (Knuth, 1997)
would eventually result in a transformed WTA of
quadratic size. To obtain instead a linear size
transformation, we introduce a slightly modified
encoding (Ho?gberg et al, 2009, Section 4), which
is inspired by (Carme et al, 2004) and the classical
currying operation.
Let ? be a ranked alphabet and assume a
fresh symbol @ /? ? (corresponding to the ba-
sic list concatenation operator). Moreover, let
? = ?2 ? ?1 ? ?0 be the ranked alphabet such
that ?2 = {@}, ?1 = ?k?1 ?k, and ?0 = ?0. In
?
?
?
?
? ?
? ?
?
?
@
?
?
?
@
? @
?
@
? ?
?
Figure 1: Input tree t and encoded tree enc(t).
words, all the original non-nullary symbols from
? are now unary, @ is binary, and the original
nullary symbols from ? have their rank preserved.
We encode each tree of T? as a tree of T? as fol-
lows:
? enc(?) = ?() for every ? ? ?0,
? enc(?(t)) = ?(enc(t)) for every ? ? ?1 and
t ? T?, and
? for k ? 2, ? ? ?k, and t1, . . . , tk ? T?
enc(?(t1, . . . , tk)) =
?(@(enc(t1), . . .@(enc(tk?1), enc(tk)) ? ? ? )).
An example of the above encoding is illustrated
in Figure 1. Note that |enc(t)| ? O(|t|) for every
t ? T?. Furthermore, t can be easily reconstructed
from enc(t) in linear time.
Definition 1 LetM = (Q,?,S, ?, F ) be a WTA.
The encoded WTA enc(M) is (P,?,S, ??, F ?)
where
P = {[q] | q ? Q} ?
? {[w] |?k(?)q,uw 6= 0, u ? Q?, w ? Q+},
F ?([q]) = F (q) for every q ? Q, and the transi-
tions are constructed as follows:
(i) ??0(?)[q],? = ?0(?)q,? for every ? ? ?0,
(ii) ??1(?)[q],[w] = ?k(?)q,w for every ? ? ?k,
k ? 1, q ? Q, and w ? Qk, and
(iii) ??2(@)[qw],[q][w] = 1 for every [qw] ? P with
|w| ? 1 and q ? Q.
All remaining entries in F ? and ?? are 0. 2
Notice that each transition of enc(M) involves no
more than three states from P . Furthermore, we
have |enc(M)| ? O(|M |). The following result is
rather intuitive (Ho?gberg et al, 2009, Lemma 4.2);
its proof is therefore omitted.
3
Theorem 1 Let M = (Q,?,S, ?, F ) be a WTA,
and let M ? = enc(M). Then M(t) = M ?(enc(t))
for every t ? T?. 2
4 Bar-Hillel construction
The so-called Bar-Hillel construction was pro-
posed in (Bar-Hillel et al, 1964) to show that
the intersection of a context-free language and
a regular language is still a context-free lan-
guage. The proof of the result consisted in an
effective construction of a context-free grammar
Prod(G,N) from a context-free grammar G and
a finite automaton N , such that Prod(G,N) gen-
erates the intersection of the languages generated
by G and N .
It was later recognized that the Bar-Hillel con-
struction constitutes one of the foundations of the
theory of tabular parsing based on context-free
grammars. More precisely, by taking the finite
automaton N to be of some special kind, accept-
ing only a single string, the Bar-Hillel construction
provides a framework under which several well-
known tabular parsing algorithms can easily be de-
rived, that were proposed much later in the litera-
ture.
In this section we extend the Bar-Hillel con-
struction to WTA, with a similar purpose of es-
tablishing an abstract framework under which one
could easily derive parsing algorithms based on
these devices. In order to guarantee computational
efficiency, we avoid here stating the Bar-Hillel
construction for WTA with alphabets of arbitrary
rank. The next result therefore refers to WTA with
alphabet symbols of rank at most 2. These may,
but need not, be automata obtained through the bi-
nary encoding discussed in Section 3.
Definition 2 Let M = (Q,?,S, ?, F ) be a WTA
such that the maximum rank of a symbol in ? is 2,
and let N = (P,?0 \ {e},S, I, ?,G) be a WSA
over the same semiring. We construct the WTA
Prod(M,N) = (P ?Q? P,?,S, ??, F ?)
as follows:
(i) For every ? ? ?2, states p0, p1, p2 ? P , and
states q0, q1, q2 ? Q let
??2(?)(p0,q0,p2),(p0,q1,p1)(p1,q2,p2) = ?2(?)q0,q1q2 .
(ii) For every symbol ? ? ?1, states p0, p1 ? P ,
and states q0, q1 ? Q let
??1(?)(p0,q0,p1),(p0,q1,p1) = ?1(?)q0,q1 .
p0 p2
p0 p1 p1 p2
?
= = =
p0 p1
p0 p1
?
= =
p0 ? p1
?(p0, ?, p1)
p0 e p0
=
Figure 2: Information transport in the first and
third components of the states in our Bar-Hillel
construction.
(iii) For every symbol ? ? ?0, states p0, p1 ? P ,
and q ? Q let
??0(?)(p0,q,p1),? = ?0(?)q,? ? s
where
s =
{
?(p0, ?, p1) if ? 6= e
1 if ? = e and p0 = p1 .
(iv) F ?(p0, q, p1) = I(p0) ?F (q) ?G(p1) for every
p0, p1 ? P and q ? Q.
All remaining entries in ?? are 0. 2
Theorem 2 Let M and N be as in Definition 2,
and let M ? = Prod(M,N). If S is commutative,
thenM ?(t) = M(t) ?N(yd(t)) for every t ? T?.2
PROOF For a state q ? P ? Q ? P , we write qi
to denote its i-th component with i ? {1, 2, 3}.
Let t ? T? and r ? RunM ?(t) be a run of M ?
on t. We call the run r well-formed if for every
w ? Pos(t):
(i) if t(w) = e, then r(w)1 = r(w)3,
(ii) if t(w) /? ?0, then:
(a) r(w)1 = r(w1)1,
(b) r(w rkt(w))3 = r(w)3, and
(c) if rkt(w) = 2, then r(w1)3 = r(w2)1.
Note that no conditions are placed on the second
components of the states in r. We try to illustrate
the conditions in Figure 2.
A standard proof shows that wtM ?(r) = 0 for
all runs r ? RunM ?(t) that are not well-formed.
We now need to map runs of M ? back into ?cor-
responding? runs for M and N . Let us fix some
t ? T? and some well-formed run r ? RunM ?(t).
4
We define the run piM (r) ? RunM (t) by letting
piM (r)(w) = r(w)2,
for every w ? Pos(t). Let {w1, . . . , wn} =
{w? | w? ? Pos(t), t(w?) ? ?0 \ {e}}, with
w1 < ? ? ? < wn according to the lexico-
graphic order on Pos(t). We also define the run
piN (r) ? RunN (yd(t)) by letting
piN (r)(i? 1) = r(wi)1,
for every 1 ? i < n, and
piN (r)(n) = r(wn)3 .
Note that conversely every run of M on t and ev-
ery run of N on yd(t) yield a unique run of M ?
on t.
Now, we claim that
wtM ?(r) = wtM (piM (r)) ? wtN (piN (r))
for every well-formed run r ? RunM ?(t). To
prove the claim, let t = ?(t1, . . . , tk) for some
? ? ?k, k ? 2, and t1, . . . , tk ? T?. Moreover,
for every 1 ? i ? k let ri(w) = r(iw) for every
w ? Pos(ti). Note that ri ? RunM ?(ti) and that
ri is well-formed for every 1 ? i ? k.
For the induction base, let ? ? ?0; we can write
wtM ?(r)
= ??0(?)r(?),?
=
{
?0(?)r(?)2,? ? ?(r(?)1, ?, r(?)3) if ? 6= e
?0(?)r(?)2,? if ? = e
= wtM (piM (r)) ? wtN (piN (r)) .
In the induction step (i.e., k > 0) we have
wtM ?(r)
= ?
w?Pos(t)
rkt(w)=n
??n(t(w))r(w),r(w1)???r(wn)
= ??k(?)r(?),r(1)???r(k) ?
k?
i=1
wtM ?(ri) .
Using the fact that r is well-formed, commutativ-
ity, and the induction hypothesis, we obtain
= ?k(?)r(?)2,r(1)2???r(k)2 ?
?
k?
i=1
(
wtM (piM (ri)) ? wtN (piN (ri))
)
= wtM (pi2(r)) ? wtN (piN (r)) ,
where in the last step we have again used the fact
that r is well-formed. Using the auxiliary state-
ment wtM ?(r) = wtM (piM (r)) ?wtN (piN (r)), the
main proof now is easy.
M ?(t)
= ?
r?RunM? (t)
wtM ?(r) ? F ?(r(?))
= ?
r?RunM? (t)
r well-formed
wtM (piM (r)) ? wtN (piN (r)) ?
? I(r(?)1) ? F (r(?)2) ?G(r(?)3)
=
( ?
r?RunM (t)
wtM (r) ? F (r(?))
)
?
?
( ?
w=yd(t)
r?RunN (w)
I(r(0)) ? wtN (r) ?G(r(|w|))
)
= M(t) ?N(yd(t)) 
Let us analyze now the computational complex-
ity of a possible implementation of the construc-
tion in Definition 2. In step (i), we could restrict
the computation by considering only those transi-
tions inM satisfying ?2(?)q0,q1q2 6= 0, which pro-
vides a number of choices in O(|M |). Combined
with the choices for the states p0, p1, p2 of N ,
this provides O(|M | ? |P |3) non-zero transitions
in Prod(M,N). This is also a bound on the over-
all running time of step (i). Since we additionally
assume that weights can be multiplied in constant
time, it is not difficult to see that all of the remain-
ing steps can be accommodated within such a time
bound. We thus conclude that the construction in
Definition 2 can be implemented to run in time and
space O(|M | ? |P |3).
5 Parsing applications
In this section we discuss several applications of
the construction presented in Definition 2 that are
relevant for parsing based on WTA models.
5.1 Parse forest
Parsing is usually defined as the problem of con-
structing a suitable representation for the set of all
possible parse trees that are assigned to a given in-
put string w by some grammar model. The set of
all such parse trees is called parse forest. The ex-
tension of the Bar-Hillel construction that we have
5
presented in Section 4 can be easily adapted to ob-
tain a parsing algorithm for WTA models. This is
described in what follows.
First, we should represent the input string w in
a WSA that recognizes the language {w}. Such
an automaton has a state set P = {p0, . . . , p|w|}
and transition weights ?(pi?1, w(i), pi) = 1 for
each i with 1 ? i ? |w|. We also set I(p0) = 1
and F (p|w|) = 1. Setting all the weights to 1 for
a WSA N amounts to ignoring the weights, i.e.,
those weights will not contribute in any way when
applying the Bar-Hillel construction.
Assume now that M is our grammar model,
represented as a WTA. The WTA Prod(M,N)
constructed as in Definition 2 is not necessarily
trim, meaning that it might contain transitions
with non-zero weight that are never used in the
recognition. Techniques for eliminating such use-
less transitions are well-known, see for instance
(Ge?cseg and Steinby, 1984, Section II.6), and can
be easily implemented to run in linear time. Once
Prod(M,N) is trim, we have a device that rec-
ognizes all and only those trees that are assigned
by M to the input string w, and the weights of
those trees are preserved, as seen in Theorem 2.
The WTA Prod(M,N) can then be seen as a rep-
resentation of a parse forest for the input string w,
and we conclude that the construction in Defini-
tion 2, combined with some WTA reduction al-
gorithm, represents a parsing algorithm for WTA
models working in cubic time on the length of the
input string and in linear time on the size of the
grammar model.
More interestingly, from the framework devel-
oped in Section 4, one can also design more effi-
cient parsing algorithms based on WTA. Borrow-
ing from standard ideas developed in the litera-
ture for parsing based on context-free grammars,
one can specialize the construction in Definition 2
in such a way that the number of useless transi-
tions generated for Prod(M,N) is considerably
reduced, resulting in a more efficient construction.
This can be done by adopting some search strat-
egy that guides the construction of Prod(M,N)
using knowledge of the input string w as well as
knowledge about the source model M .
As an example, we can apply step (i) only on de-
mand, that is, we process a transition ??2(?)q0,q1q2
in Prod(M,N) only if we have already computed
non-zero transitions of the form ??k1(?1)q1,w1 and
??k2(?2)q2,w2 , for some ?1 ? ?k1 , w1 ? Qk1 and
?2 ? ?k2 , w2 ? Qk2 where Q is the state set
of Prod(M,N). The above amounts to a bottom-
up strategy that is also used in the Cocke-Kasami-
Younger recognition algorithm for context-free
grammars (Younger, 1967).
More sophisticated strategies are also possible.
For instance, one could adopt the Earley strategy
developed for context-free grammar parsing (Ear-
ley, 1970). In this case, parsing is carried out in
a top-down left-to-right fashion, and the binariza-
tion construction of Section 3 is carried out on the
flight. This has the additional advantage that it
would be possible to use WTA models that are not
restricted to the special normal form of Section 3,
still maintaining the cubic time complexity in the
length of the input string. We do not pursue this
idea any further in this paper, since our main goal
here is to outline an abstract framework for pars-
ing based on WTA models.
5.2 Probabilistic tree automata
Let us now look into specific semirings that are
relevant for statistical natural language process-
ing. The semiring of non-negative real numbers
is R?0 = (R?0,+, ?, 0, 1). For the remainder of
the section, let M = (Q,?,R?0, ?, F ) be a WTA
over R?0. M is convergent if
?
t?T?
M(t) < ?.
We say that M is a probabilistic tree automa-
ton (Ellis, 1971; Magidor and Moran, 1970),
or PTA for short, if ?k(?)q,q1???qk ? [0, 1]
and F (q) ? [0, 1], for every ? ? ?k and
q, q1, . . . , qk ? Q. In other words, in a PTA all
weights are in the range [0, 1] and can be inter-
preted as probabilities. For a PTA M we therefore
write pM (r) = wt(r) and pM (t) = M(t), for
each t ? T? and r ? RunM (t).
A PTA is proper if?q?Q F (q) = 1 and
?
???k,k?0,w?Qk
?k(?)q,w = 1
for every q ? Q. Since the set of symbols is finite,
we could have only required that the sum over all
weights as shown with w ? Qk equals 1 for every
q ? Q and ? ? ?k. A simple rescaling would then
be sufficient to arrive at our notion. Furthermore, a
PTA is consistent if ?t?T? pM (t) = 1. If a PTAis consistent, then pM is a probability distribution
over the set T?.
6
The WTAM is unambiguous if for every input
tree t ? T?, there exists at most one r ? RunM (t)
such that r(?) ? F and wtM (r) 6= 0. In other
words, in an unambiguous WTA, there exists at
most one successful run for each input tree. Fi-
nally, M is in final-state normal form if there ex-
ists a state qS ? Q such that
? F (qS) = 1,
? F (q) = 0 for every q ? Q \ {qS}, and
? ?k(?)q,w = 0 if w(i) = qS for some
1 ? i ? k.
We commonly denote the unique final state by qS .
For the following result we refer the reader
to (Droste et al, 2005, Lemma 4.8) and (Bozapa-
lidis, 1999, Lemma 22). The additional properties
mentioned in the items of it are easily seen.
Theorem 3 For every WTA M there exists an
equivalent WTA M ? in final-state normal form.
? If M is convergent (respectively, proper, con-
sistent), then M ? is such, too.
? If M is unambiguous, then M ? is
also unambiguous and for every
t ? T? and r ? RunM (t) we have
wtM ?(r?) = wtM (r) ? F (r(?)) where
r?(?) = qS and r?(w) = r(w) for every
w ? Pos(t) \ {?}. 2
It is not difficult to see that a proper PTA in
final-state normal form is always convergent.
In statistical parsing applications we use gram-
mar models that induce a probability distribution
on the set of parse trees. In these applications,
there is often the need to visit a parse tree with
highest probability, among those in the parse for-
est obtained from the input sentence. This imple-
ments a form of disambiguation, where the most
likely tree under the given model is selected, pre-
tending that it provides the most likely syntactic
analysis of the input string. In our setting, the
above approach reduces to the problem of ?unfold-
ing? a tree from a PTA Prod(M,N), that is as-
signed the highest probability.
In order to find efficient solutions for the above
problem, we make the following two assumptions.
? M is in final-state normal form. By Theo-
rem 3 this can be achieved without loss of
generality.
? M is unambiguous. This restrictive assump-
tion avoids the so-called ?spurious? ambigu-
ity, that would result in several computations
in the model for an individual parse tree.
It is not difficult to see that PTA satisfying these
1: Function BESTPARSE(M)
2: E ? ?
3: repeat
4: A ? {q |?k(?)q,q1???qk > 0, q /? E ,
q1, . . . , qk ? E}
5: for all q ? A do
6: ?(q)? max
???k,k?0
q1,...,qk?E
?k(?)q,q1???qk ?
k?
i=1
?(qi)
7: E ? E ? {argmax
q?A
?(q)}
8: until qS ? E
9: return ?(qS)
Figure 3: Search algorithm for the most probable
parse in an unambiguous PTAM in final-state nor-
mal form.
two properties are still more powerful than the
probabilistic context-free grammar models that are
commonly used in statistical natural language pro-
cessing.
Once more, we borrow from the literature on
parsing for context-free grammars, and adapt a
search algorithm developed by Knuth (1977); see
also (Nederhof, 2003). The basic idea here is
to generalize Dijkstra?s algorithm to compute the
shortest path in a weighted graph. The search al-
gorithm is presented in Figure 3.
The algorithm takes as input a trim PTA M that
recognizes at least one parse tree. We do not im-
pose any bound on the rank of the alphabet sym-
bols forM . Furthermore,M needs not be a proper
PTA. In order to simplify the presentation, we pro-
vide the algorithm in a form that returns the largest
probability assigned to some tree by M .
The algorithm records into the ?(q) variables
the largest probability found so far for a run that
brings M into state q, and stores these states into
an agenda A. States for which ?(q) becomes opti-
mal are popped from A and stored into a set E .
Choices are made on a greedy base. Note that
when a run has been found leading to an optimal
probability ?(q), from our assumption we know
that the associated tree has only one run that ends
up in state q.
Since E is initially empty (line 2), only weights
satisfying ?0(?)q,? > 0 are considered when line 4
is executed for the first time. Later on (line 7)
the largest probability is selected among all those
that can be computed at this time, and the set E is
populated. As a consequence, more states become
7
available in the agenda in the next iteration, and
new transitions can now be considered. The algo-
rithm ends when the largest probability has been
calculated for the unique final state qS .
We now analyze the computational complexity
of the algorithm in Figure 3. The ?repeat-until?
loop runs at most |Q| times. Entirely reprocess-
ing setA at each iteration would be too expensive.
We instead implement A as a priority heap and
maintain a clock for each weight ?k(?)q,q1???qk ,
initially set to k. Whenever a new optimal proba-
bility ?(q) becomes available through E , we decre-
ment the clock associated with each ?k(?)q,q1???qk
by d, in case d > 0 occurrences of q are found
in the string q1 ? ? ? qk. In this way, at each it-
eration of the ?repeat-until? loop, we can con-
sider only those weights ?k(?)q,q1???qk with asso-
ciated clock of zero, compute new values ?(q),
and update the heap. For each ?k(?)q,q1???qk > 0,
all clock updates and the computation of quan-
tity ?k(?)q,q1???qk ?
?k
i=1 ?(qi) (when the associ-
ated clock becomes zero) both take an amount of
time proportional to the length of the transition
itself. The overall time to execute these opera-
tions is therefore linear in |M |. Accounting for
the heap, the algorithm has overall running time
in O(|M |+ |Q| log|Q|).
The algorithm can be easily adapted to return a
tree having probability ?(qS), if we keep a record
of all transitions selected in the computation along
with links from a selected transition and all of the
previously selected transitions that have caused its
selection. If we drop the unambiguity assump-
tion for the PTA, then the problem of comput-
ing the best parse tree becomes NP-hard, through
a reduction from similar problems for finite au-
tomata (Casacuberta and de la Higuera, 2000). In
contrast, the problem of computing the probability
of all parse trees of a string, also called the inside
probability, can be solved in polynomial time in
most practical cases and will be addressed in Sub-
section 5.4.
5.3 Normalization
Consider the WTA Prod(M,N) obtained as in
Definition 2. If N is a WSA encoding an in-
put string w as in Subsection 5.1 and if M is a
proper and consistent PTA, then Prod(M,N) is
a PTA as well. However, in general Prod(M,N)
will not be proper, nor consistent. Properness and
consistency of Prod(M,N) are convenient in all
those applications where a statistical parsing mod-
ule needs to be coupled with other statistical mod-
ules, in such a way that the composition of the
probability spaces still induces a probability dis-
tribution. In this subsection we deal with the more
general problem of how to transform a WTA that
is convergent into a PTA that is proper and con-
sistent. This process is called normalization. The
normalization technique we propose here has been
previously explored, in the context of probabilis-
tic context-free grammars, in (Abney et al, 1999;
Chi, 1999; Nederhof and Satta, 2003).
We start by introducing some new notions. Let
us assume that M is a convergent WTA. For every
q ? Q, we define
wtM (q) =
?
t?T?,r?RunM (t)
r(?)=q
wtM (r) .
Note that quantity wtM (q) equals the sum of the
weights of all trees in T? that would be recognized
by M if we set F (q) = 1 and F (p) = 0 for each
p ? Q \ {q}, that is, if q is the unique final state
of M . It is not difficult to show that, since M is
convergent, the sum in the definition of wtM (q)
converges for each q ? Q. We will show in Sub-
section 5.4 that the quantities wtM (q) can be ap-
proximated to any desired precision.
To simplify the presentation, and without any
loss of generality, throughout this subsection we
assume that our WTA are in final-state normal
form. We can now introduce the normalization
technique.
Definition 3 Let M = (Q,?,R?0, ?, F ) be a
convergent WTA in final-state normal form. We
construct the WTA
Norm(M) =(Q,?,R?0, ??, F ) ,
where for every ? ? ?k, k ? 0, and
q, q1, . . . , qk ? Q
??k(?)q,q1???qk = ?k(?)q,q1???qk ?
? wtM (q1) ? . . . ? wtM (qk)wtM (q) . 2
We now show the claimed property for our
transformation.
Theorem 4 Let M be as in Definition 3, and let
M ? = Norm(M). Then M ? is a proper and
consistent PTA, and for every t ? T? we have
M ?(t) = M(t)wtM (qS) . 2
8
PROOF Clearly, M ? is again in final-state normal
form. An easy derivation shows that
wtM (q) =
?
???k
q1,...,qk?Q
?k(?)q,q1???qk ?
k?
i=1
wtM (qi)
for every q ? Q. Using the previous remark, we
obtain
?
???k,q1,...,qk?Q
??k(?)q,q1???qk
= ?
???k,q1,...,qk?Q
?k(?)q,q1???qk ?
? wtM (q1) ? . . . ? wtM (qk?)wtM (q)
=
?
???k,
q1,...,qk?Q
?k(?)q,q1???qk ?
k?
i=1
wtM (qi)
?
???k,
p1,...,pk?Q
?k(?)q,p1???pk ?
k?
i=1
wtM (pi)
= 1 ,
which proves that M ? is a proper PTA.
Next, we prove an auxiliary statement. Let
t = ?(t1, . . . , tk) for some ? ? ?k, k ? 0, and
t1, . . . , tk ? T?. We claim that
wtM ?(r) = wtM (r)wtM (r(?))
for every r ? RunM (t) = RunM ?(t). For ev-
ery 1 ? i ? k, let ri ? RunM (ti) be such that
ri(w) = r(iw) for every w ? Pos(ti). Then
wtM ?(r) =
?
w?Pos(t)
rkt(w)=n
??n(t(w))r(w),r(w1)???r(wn)
= ??k(?)r(?),r(1)???r(k) ?
k?
i=1
wtM ?(ri)
= ??k(?)r(?),r1(?)???rk(?) ?
k?
i=1
wtM (ri)
wtM (ri(?))
= ?k(?)r(?),r(1)???r(k) ? wtM (r1) ? ? ? ? ? wtM (rk)wtM (r(?))
= wtM (r)wtM (r(?)) .
Consequently,
M ?(t) = ?
r?RunM? (t)
r(?)=qS
wtM ?(r)
= ?
r?RunM (t)
r(?)=qS
wtM (r)
wtM (qS) =
M(t)
wtM (qS)
and
?
t?T?
M ?(t) = ?
t?T?,r?RunM? (t)
r(?)=qS
wtM ?(r)
= ?
t?T?,r?RunM (t)
r(?)=qS
wtM (r)
wtM (qS)
= wtM (qS)wtM (qS) = 1 ,
which prove the main statement and the consis-
tency of M ?, respectively. 
5.4 Probability mass of a state
AssumeM is a convergent WTA. We have defined
quantities wtM (q) for each q ? Q. Note that when
M is a proper PTA in final-state normal form, then
wtM (q) can be seen as the probability mass that
?rests? on state q. When dealing with such PTA,
we use the notation ZM (q) in place of wtM (q),
and call ZM the partition function of M . This
terminology is borrowed from the literature on ex-
ponential or Gibbs probabilistic models.
In the context of probabilistic context-free
grammars, the computation of the partition func-
tion has several applications, including the elim-
ination of epsilon rules (Abney et al, 1999) and
the computation of probabilistic distances between
probability distributions realized by these for-
malisms (Nederhof and Satta, 2008). Besides
what we have seen in Subsection 5.3, we will pro-
vide one more application of partition functions
for the computations of so-called prefix probabil-
ities in Subsection 5.5 We also add that, when
computed on the Bar-Hillel automata of Section 4,
the partition function provides the so-called inside
probabilities of (Graehl et al, 2008) for the given
states and substrings.
Let |Q| = n and let us assume an arbitrary or-
dering q1, . . . , qn for the states in Q. We can then
rewrite the definition of wtM (q) as
wtM (q) =
?
???k,k?0
qi1 ,...,qik?Q
?k(?)q,qi1 ???qik ?
k?
j=1
wtM (qij )
(see proof of Theorem 4). We rename wtM (qi)
with the unknown Xqi , 1 ? i ? n, and derive a
9
system of n nonlinear polynomial equations of the
form
Xqi =
?
???k,k?0
qi1 ,...,qik?Q
?k(?)q,qi1 ???qik ?Xqi1 ? . . . ?Xqik
= fqi(Xq1 , . . . , Xqn) , (1)
for each i with 1 ? i ? n.
Throughout this subsection, we will consider
solutions of the above system in the extended non-
negative real number semiring
R??0 = (R?0 ? {?},+, ?, 0, 1)
with the usual operations extended to ?. We
can write the system in (1) in the compact form
X = F (X), where we represent the unknowns
as a vector X = (Xq1 , . . . , Xqn) and F is a map-
ping of type (R??0)n ? (R??0)n consisting of the
polynomials fqi(X).
We denote the vector (0, . . . , 0) ? (R??0)n as
X0. Let X,X ? ? (R??0)n. We write X ? X ?
if Xqi ? X ?qi for every 1 ? i ? n. Sinceeach polynomial fqi(X) has coefficients repre-
sented by positive real numbers, it is not difficult
to see that, for each X,X ? ? (R??0)n, we have
F (X) ? F (X ?) whenever X0 ? X ? X ?. This
means that F is an order preserving, or monotone,
mapping.
We observe that ((R??0)n,?) is a complete
lattice with least element X0 and greatest el-
ement (?, . . . ,?). Since F is monotone on
a complete lattice, by the Knaster-Tarski theo-
rem (Knaster, 1928; Tarski, 1955) there exists a
least and a greatest fixed-point of F that are solu-
tions ofX = F (X).
The Kleene theorem states that the least fixed-
point solution of X = F (X) can be obtained
by iterating F starting with the least element X0.
In other words, the sequence Xk = F (Xk?1),
k = 1, 2, . . . converges to the least fixed-point so-
lution. Notice that each Xk provides an approxi-
mation for the partition function of M where only
trees of depth not larger than k are considered.
This means that limk??Xk converges to the par-
tition function of M , and the least fixed-point so-
lution is also the sought solution. Thus, we can
approximate wtM (q) with q ? Q to any degree by
iterating F a sufficiently large number of times.
The fixed-point iteration method discussed
above is also well-known in the numerical calcu-
lus literature, and is frequently applied to systems
of nonlinear equations in general, because it can
be easily implemented. When a number of stan-
dard conditions are met, each iteration of the algo-
rithm (corresponding to the value of k above) adds
a fixed number of bits to the precision of the ap-
proximated solution; see (Kelley, 1995) for further
discussion.
Systems of the form X = F (X) where all
fqi(X) are polynomials with nonnegative real co-
efficients are called monotone system of poly-
nomials. Monotone systems of polynomials as-
sociated with proper PTA have been specifically
investigated in (Etessami and Yannakakis, 2005)
and (Kiefer et al, 2007), where worst case results
on exponential rate of convergence are reported
for the fixed-point method.
5.5 Prefix probability
In this subsection we deal with one more applica-
tion of the Bar-Hillel technique presented in Sec-
tion 4. We show how to compute the so-called
prefix probabilities, that is, the probability that a
tree recognized by a PTA generates a string start-
ing with a given prefix. Such probabilities have
several applications in language modeling. As an
example, prefix probabilities can be used to com-
pute the probability distribution on the terminal
symbol that follows a given prefix (under the given
model).
For probabilistic context-free grammars, the
problem of the computation of prefix probabili-
ties has been solved in (Jelinek et al, 1992); see
also (Persoon and Fu, 1975). The approach we
propose here, originally formulated for probabilis-
tic context-free grammars in (Nederhof and Satta,
2003; Nederhof and Satta, 2009), is more abstract
than the previous ones, since it entirely rests on
properties of the Bar-Hillel construction that we
have already proved in Section 4.
Let M = (Q,?,R?0, ?, F ) be a proper
and consistent PTA in final-state normal form,
? = ?0 \ {e}, and let u ? ?+ be some string.
We assume here that M is in the binary form
discussed in Section 3. In addition, we assume
that M has been preprocessed in order to remove
from its recognized trees all of the unary branches
as well as those branches that generate the null
string ?. Although we do not discuss this con-
struction at length in this paper, the result follows
from a transformation casting weighted context-
free grammars into Chomsky Normal Form (Fu
10
and Huang, 1972; Abney et al, 1999).
We define
Pref(M,u) = {t | t ? T?, M(t) > 0,
yd(t) = uv, v ? ??} .
The prefix probability of u underM is defined as
?
t?Pref(M,u)
pM (t) .
Let |u| = n. We define a WSA Nu with state
set P = {p0, . . . , pn} and transition weights
?(pi?1, u(i), pi) = 1 for each i with 1 ? i ? n,
and ?(pn, ?, pn) = 1 for each ? ? ?. We also
set I(p0) = 1 and F (pn) = 1. It is easy to see
that Nu recognizes the language {uv | v ? ??}.
Furthermore, the PTA Mp = Prod(M,Nu) spec-
ified as in Definition 2 recognizes the desired tree
set Pref(M,u), and it preserves the weights of
those trees with respect to M . We therefore con-
clude that ZMp(qS) is the prefix probability of u
under M . Prefix probabilities can then be approx-
imated using the fixed-point iteration method of
Subsection 5.4. Rather than using an approxima-
tion method, we discuss in what follows how the
prefix probabilities can be exactly computed.
Let us consider more closely the product au-
tomaton Mp, assuming that it is trim. Each state
of Mp has the form pi = (pi, q, pj), pi, pj ? P and
q ? Q, with i ? j. We distinguish three, mutually
exclusive cases.
(i) j < n: From our assumption that M (and
thus Mp) does not have unary or ? branches,
it is not difficult to see that all ZMp(pi) can be
exactly computed in time O((j ? i)3).
(ii) i = j = n: We have pi = (pn, q, pn).
Then the equations for ZMp(pi) exactly
mirror the equations for ZM (q), and
ZMp(pi) = ZMp(q). Because M is proper
and consistent, this means that ZMp(pi) = 1.
(iii) i < j = n: A close inspection of Definition 2
reveals that in this case the equations (1) are
all linear, assuming that we have already re-
placed the solutions from (i) and (ii) above
into the system. This is because any weight
?2(?)pi0,pi1pi > 0 in Mp with pi = (pi, q, pn)
and i < n must have (pi1)3 < n. Quanti-
ties ZMp(pi) can then be exactly computed as
the solution of a linear system of equations in
time O(n3).
Putting together all of the observations above,
we obtain that for a proper and consistent PTA that
has been preprocessed, the prefix probability of u
can be computed in cubic time in the length of the
prefix itself.
6 Concluding remarks
In this paper we have extended the Bar-Hillel con-
struction to WTA, closely following the method-
ology proposed in (Nederhof and Satta, 2003) for
weighted context-free grammars. Based on the ob-
tained framework, we have derived several parsing
algorithms for WTA, under the assumption that the
input is a string rather than a tree.
As already remarked in the introduction, WTA
are richer models than weighted context-free
grammar, since the formers use hidden states in
the recognition of trees. This feature makes it
possible to define a product automaton in Defini-
tion 2 that generates exactly those trees of interest
for the input string. In contrast, in the context-
free grammar case the Bar-Hillel technique pro-
vides trees that must be mapped to the tree of in-
terest using some homomorphism. For the same
reason, one cannot directly convert WTA into
weighted context-free grammars and then apply
existing parsing algorithms for the latter formal-
ism, unless the alphabet of nonterminal symbols
is changed. Finally, our main motivation in de-
veloping a framework specifically based on WTA
is that this can be extended to classes of weighted
tree transducers, in order to deal with computa-
tional problems that arise in machine translation
applications. We leave this for future work.
Acknowledgments
The first author has been supported by the Minis-
terio de Educacio?n y Ciencia (MEC) under grant
JDCI-2007-760. The second author has been par-
tially supported by MIUR under project PRIN No.
2007TJNZRE 002.
References
S. Abney, D. McAllester, and F. Pereira. 1999. Relat-
ing probabilistic grammars and automata. In 37th
Annual Meeting of the Association for Computa-
tional Linguistics, Proceedings of the Conference,
pages 542?549, Maryland, USA, June.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On
formal properties of simple phrase structure gram-
mars. In Y. Bar-Hillel, editor, Language and Infor-
mation: Selected Essays on their Theory and Appli-
cation, chapter 9, pages 116?150. Addison-Wesley,
Reading, Massachusetts.
11
J. Berstel and C. Reutenauer. 1982. Recognizable for-
mal power series on trees. Theoret. Comput. Sci.,
18(2):115?148.
B. Borchardt. 2005. The Theory of Recognizable Tree
Series. Ph.D. thesis, Technische Universita?t Dres-
den.
S. Bozapalidis. 1999. Equational elements in additive
algebras. Theory Comput. Systems, 32(1):1?33.
J. Carme, J. Niehren, and M. Tommasi. 2004. Query-
ing unranked trees with stepwise tree automata. In
Proc. RTA, volume 3091 of LNCS, pages 105?118.
Springer.
F. Casacuberta and C. de la Higuera. 2000. Com-
putational complexity of problems on probabilis-
tic grammars and transducers. In L. Oliveira, edi-
tor, Grammatical Inference: Algorithms and Appli-
cations; 5th International Colloquium, ICGI 2000,
pages 15?24. Springer.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131?160.
M. Droste, C. Pech, and H. Vogler. 2005. A Kleene
theorem for weighted tree automata. Theory Com-
put. Systems, 38(1):1?38.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102,
February.
S. Eilenberg. 1974. Automata, Languages, and Ma-
chines, volume 59 of Pure and Applied Math. Aca-
demic Press.
C. A. Ellis. 1971. Probabilistic tree automata. Infor-
mation and Control, 19(5):401?416.
Z. E?sik and W. Kuich. 2003. Formal tree series. J.
Autom. Lang. Combin., 8(2):219?285.
K. Etessami and M. Yannakakis. 2005. Recursive
Markov chains, stochastic grammars, and monotone
systems of nonlinear equations. In 22nd Interna-
tional Symposium on Theoretical Aspects of Com-
puter Science, volume 3404 of Lecture Notes in
Computer Science, pages 340?352, Stuttgart, Ger-
many. Springer-Verlag.
K.S. Fu and T. Huang. 1972. Stochastic grammars and
languages. International Journal of Computer and
Information Sciences, 1(2):135?170.
F. Ge?cseg and M. Steinby. 1984. Tree Automata.
Akade?miai Kiado?, Budapest.
J. Graehl, K. Knight, and J. May. 2008. Training tree
transducers. Comput. Linguist., 34(3):391?427.
J. Ho?gberg, A. Maletti, and H. Vogler. 2009. Bisim-
ulation minimisation of weighted automata on un-
ranked trees. Fundam. Inform. to appear.
F. Jelinek, J.D. Lafferty, and R.L. Mercer. 1992. Basic
methods of probabilistic context free grammars. In
P. Laface and R. De Mori, editors, Speech Recogni-
tion and Understanding ? Recent Advances, Trends
and Applications, pages 345?360. Springer-Verlag.
M. Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
C. T. Kelley. 1995. Iterative Methods for Linear and
Nonlinear Equations. Society for Industrial and Ap-
plied Mathematics, Philadelphia, PA.
S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On
the convergence of Newton?s method for monotone
systems of polynomial equations. In Proceedings of
the 39th ACM Symposium on Theory of Computing,
pages 217?266.
B. Knaster. 1928. Un the?ore`me sur les fonctions
d?ensembles. Ann. Soc. Polon. Math., 6:133?134.
D. E. Knuth. 1977. A generalization of Dijkstra?s al-
gorithm. Information Processing Letters, 6(1):1?5,
February.
D. E. Knuth. 1997. Fundamental Algorithms. The Art
of Computer Programming. Addison Wesley, 3rd
edition.
M. Magidor and G. Moran. 1970. Probabilistic tree
automata and context free languages. Israel Journal
of Mathematics, 8(4):340?348.
M.-J. Nederhof and G. Satta. 2003. Probabilistic pars-
ing as intersection. In 8th International Workshop
on Parsing Technologies, pages 137?148, LORIA,
Nancy, France, April.
M.-J. Nederhof and G. Satta. 2008. Computation of
distances for regular and context-free probabilistic
languages. Theoretical Computer Science, 395(2-
3):235?254.
M.-J. Nederhof and G. Satta. 2009. Computing parti-
tion functions of PCFGs. Research on Language &
Computation, 6(2):139?162.
M.-J. Nederhof. 2003. Weighted deductive parsing
and Knuth?s algorithm. Computational Linguistics,
29(1):135?143.
E. Persoon and K.S. Fu. 1975. Sequential classi-
fication of strings generated by SCFG?s. Interna-
tional Journal of Computer and Information Sci-
ences, 4(3):205?217.
M. P. Schu?tzenberger. 1961. On the definition of a
family of automata. Information and Control, 4(2?
3):245?270.
A. Tarski. 1955. A lattice-theoretical fixpoint theorem
and its applications. Pacific J. Math., 5(2):285?309.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10:189?208.
12
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 69?72,
Paris, October 2009. c?2009 Association for Computational Linguistics
Synchronous Rewriting in Treebanks
Laura Kallmeyer
University of Tu?bingen
Tu?bingen, Germany
lk@sfs.uni-tuebingen.de
Wolfgang Maier
University of Tu?bingen
Tu?bingen, Germany
wo.maier@uni-tuebingen.de
Giorgio Satta
University of Padua
Padova, Italy
satta@dei.unipd.it
Abstract
Several formalisms have been proposed
for modeling trees with discontinuous
phrases. Some of these formalisms allow
for synchronous rewriting. However, it
is unclear whether synchronous rewriting
is a necessary feature. This is an impor-
tant question, since synchronous rewrit-
ing greatly increases parsing complexity.
We present a characterization of recursive
synchronous rewriting in constituent tree-
banks with discontinuous annotation. An
empirical investigation reveals that syn-
chronous rewriting is actually a neces-
sary feature. Furthermore, we transfer this
property to grammars extracted from tree-
banks.
1 Introduction
Discontinuous phrases are frequent in natural
language, particularly in languages with a rela-
tively free word order. Several formalisms have
been proposed in the literature for modeling trees
containing such phrases. These include non-
projective dependency grammar (Nivre, 2006),
discontinuous phrase structure grammar (DPSG)
(Bunt et al, 1987), as well as linear context-
free rewriting systems (LCFRS) (Vijay-Shanker et
al., 1987) and the equivalent formalism of sim-
ple range concatenation grammar (sRCG) (Boul-
lier, 2000). Kuhlmann (2007) uses LCFRS for
non-projective dependency trees. DPSG have
been used in Plaehn (2004) for data-driven pars-
ing of treebanks with discontinuous constituent
annotation. Maier and S?gaard (2008) extract
sRCGs from treebanks with discontinuous con-
stituent structures.
Both LCFRS and sRCG can model discontinu-
ities and allow for synchronous rewriting as well.
We speak of synchronous rewriting when two or
more context-free derivation processes are instan-
tiated in a synchronous way. DPSG, which has
also been proposed for modeling discontinuities,
does not allow for synchronous rewriting because
the different discontinuous parts of the yield of a
non-terminal are treated locally, i.e., their deriva-
tions are independent from each other. So far, syn-
chronous rewriting has not been empirically mo-
tivated by linguistic data from treebanks. In this
paper, we fill this gap by investigating the exis-
tence of structures indicating synchronous rewrit-
ing in treebanks with discontinuous annotations.
The question of whether we can find evidence for
synchronous rewriting has consequences for the
complexity of parsing. In fact, parsing with syn-
chronous formalisms can be carried out in time
polynomial in the length of the input string, with
a polynomial degree depending on the maximum
number of synchronous branches one can find in
derivations (Seki et al, 1991).
In this paper, we characterize synchronous
rewriting as a property of trees with crossing
branches and in an empirical evaluation, we con-
firm that treebanks do contain recursive syn-
chronous rewriting which can be linguistically
motivated. Furthermore, we show how this char-
acterization transfers to the simple RCGs describ-
ing these trees.
2 Synchronous Rewriting Trees in
German treebanks
By synchronous rewriting we indicate the syn-
chronous instantiation of two or more context-free
derivation processes. As an example, consider the
language L = {anbncndn | n ? 1}. Each
of the two halves of some w ? L can be ob-
tained through a stand-alone context-free deriva-
tion, but for w to be in L the two derivations must
be synchronized somehow. For certain tasks, syn-
chronous rewriting is a desired property for a for-
malism. In machine translation, e.g., synchronous
69
rewriting is extensively used to model the syn-
chronous dependence between the source and tar-
get languages (Chiang, 2007). The question we
are concerned with in this paper is whether we can
find instances of recursive synchronous rewriting
in treebanks that show discontinuous phrases.
We make the assumption that, if the annota-
tion of a treebank allows to express synchronous
rewriting, then all cases of synchronous rewriting
are present in the annotation. This means that, on
the one hand, there are no cases of synchronous
rewriting that the annotator ?forgot? to encode.
Therefore unrelated cases of parallel iterations in
different parts of a tree are taken to be truly unre-
lated. On the other hand, if synchronous rewrit-
ing is annotated explicitely, then we take it to be a
case of true synchronous rewriting, even if, based
on the string, it would be possible to find an anal-
ysis that does not require synchronous rewriting.
This assumption allows us to concentrate only on
explicit cases of synchronous rewriting .
We concentrate on German treebanks annotated
with trees with crossing branches. In such trees,
synchronous rewriting amounts to cases where dif-
ferent components of a non-terminal category de-
velop in parallel. In particular, we search for cases
where the parallelism can be iterated. An exam-
ple is the relative clause in (1), found in TIGER.
Fig. 1 gives the annotation. As can be seen in
the annotation, we have two VP nodes, each of
which has a discontinuous span consisting of two
parts. The two parts are separated by lexical ma-
terial not belonging to the VPs. The two com-
ponents of the second VP (Pop-Idol and werden)
are included in the two components of the first,
higher, VP (genausogut auch Pop-Idol and wer-
den ko?nnen). In other words, the two VP compo-
nents are rewritten in parallel containing again two
smaller VP components.
(1) . . . der
. . . who
genausogut
as well
auch
also
Pop-Idol
pop-star
ha?tte
AUX
werden
become
ko?nnen
could
?who could as well also become a pop-star?
Let us assume the following definitions: We
map the elements of a string to their positions. We
then say that the yield ? of a node n in a tree is
the set of all indices i such that n dominates the
leaf labeled with the ith terminal. A yield ? has a
gap if there are i1 < i2 < i3 such that i1, i3 ? ?
and i2 /? ?. For all i, j ? ? with i < j, the set
??i,j? = {k | i ? k ? j} is a component of ? if
??i,j? ? ? and i?1 /? ? and j+1 /? ?. We order
the components of ? such that ??i1,j1? < ??i2,j2?
if i1 < i2.
Trees showing recursive synchronous rewrit-
ing can be characterized as follows: We have a
non-terminal node n1 with label A whose yield
has a gap. n1 dominates another node n2 with la-
bel A such that for some i 6= j, the ith component
of the yield of n2 is contained in the ith component
of the yield of n1 and similar for the jth compo-
nent. We call the path from n1 to n2 a recursive
synchronous rewriting segment (RSRS).
Table 1 shows the results obtained from search-
ing for recursive synchronous rewriting in the Ger-
man TIGER and NeGra treebanks. In a prepro-
cessing step, punctuation has been removed, since
it is directly attached to the root node and therefore
not included in the annotation.
TIGER NeGra
number of trees 40,013 20,597
total num. of RSRS in all trees 1476 600
av. RSRS length in all trees 2.13 2.12
max. RSRS length in all trees 5 4
Table 1: Synchronous rewriting in treebanks
Example (1) shows that we find instances of re-
cursive synchronous rewriting where each of the
rewriting steps adds something to both of the par-
allel components. (1) was not an isolated case.
The annotation of (1) in Fig. 1 could be turned
into a context-free structure if the lowest node
dominating the material in the gap while not
dominating the synchronous rewriting nodes (here
VAFIN) is attached lower, namely below the lower
VP node. (Note however that there is good linguis-
tic motivation for attaching it high.) Besides such
cases, we even encountered cases where the dis-
continuity cannot be removed this way. An exam-
ple is (2) (resp. Fig. 2) where we have a gap con-
taining an NP such that the lowest node dominat-
ing this NP while not dominating the synchronous
rewriting nodes has a daughter to the right of the
yields of the synchronous rewriting nodes, namely
the extraposed relative clause. This structure is of
the type ancbnd, where a and b depend on each
other in a left-to-right order and can be nested,
and c and d also depend on each other and must
be generated together. This is a structure that re-
quires synchronous rewriting, even on the basis of
the string language. Note that the nesting of VPs
can be iterated, as can be seen in (3).
(2) . . . ob
. . . whether
auf
on
deren
their
Gela?nde
premises
der
the
Typ
type
von
of
70
S
VP
VP
PRELS ADV ADV NN VAFIN VAINF VMINF
der genausogut auch Pop-Idol ha?tte werden ko?nnen
Figure 1: Example for recursive synchronous rewriting
Abstellanlage
parking facility
gebaut
built
werden
be
ko?nne,
could,
der
which
. . .
. . .
?whether on their premises precisely the type of parking
facility could be built, which . . . ?
(3) . . . ob
. . . whether
auf
on
deren
their
Gela?nde
premises
der
the
Typ
type
von
of
Abstellanlage
parking facility
eigentlich
actually
ha?tte
had
schon
already
gebaut
built
werden
be
sollen,
should,
der
which
. . .
. . .
?whether on their premises precisely the type of parking
facility should actually already have been built, which
. . . ?
As a conclusion from these empirical results,
we state that to account for the data we can find in
treebanks with discontinuities, i.e., with crossing
branches, we need a formalism that can express
synchronous rewriting.
3 Synchronous Rewriting in Grammars
Extracted from Treebanks
In the following, we will use simple RCG (which
are equivalent to LCFRS) to model our treebank
annotations. We extract simple RCG rewriting
rules from NeGra and TIGER and check them for
the possibility to generate recursive synchronous
rewriting.
A simple RCG (Boullier, 2000) is a tuple G =
(N,T, V, P, S) where a) N is a finite set of pred-
icate names with an arity function dim: N ? N,
b) T and V are disjoint finite sets of terminals and
variables, c) P is a finite set of clauses of the form
A(?1, . . . , ?dim(A)) ? A1(X(1)1 , . . . ,X(1)dim(A1))
? ? ?Am(X(m)1 , . . . ,X(m)dim(Am))
for m ? 0 where A,A1, . . . , Am ? N , X(i)j ?
V for 1 ? i ? m, 1 ? j ? dim(Ai) and ?i ?
(T ? V )? for 1 ? i ? dim(A), and e) S ? N is
the start predicate name with dim(S) = 1. For all
c ? P , it holds that every variable X occurring in
c occurs exactly once in the left-hand side (LHS)
and exactly once in the RHS. A simple RCG G =
(N,T, V, P, S) is a simple k-RCG if for all A ?
N, dim(A) ? k.
For the definition of the language of a simple
RCG, we borrow the LCFRS definitions here: Let
G = ?N,T, V, P, S? be a simple RCG. For every
A ? N , we define the yield of A, yield(A) as
follows:
a) For every A(~?) ? ?, ~? ? yield(A);
b) For every clause
A(?1, . . . , ?dim(A)) ? A1(X(1)1 , . . . ,X(1)dim(A1))
? ? ?Am(X(m)1 , . . . ,X(m)dim(Am))
and all ~?i ? yield(Ai) for 1 ? i ? m,
?f(?1), . . . , f(?dim(A))? ? yield(A) where
f is defined as follows:
(i) f(t) = t for all t ? T ,
(ii) f(X(i)j ) = ~?i(j) for all 1 ? i ? m, 1 ?
j ? dim(Ai) and
(iii) f(xy) = f(x)f(y) for all x, y ? (T ?
V )+.
c) Nothing else is in yield(A).
The language is then {w | ?w? ? yield(S)}.
We are using the algorithm from Maier and
S?gaard (2008) to extract simple RCGs from Ne-
Gra and TIGER. For the tree in Fig. 1, the algo-
rithm produces for instance the following clauses:
PRELS(der) ? ?
ADV(genausogut) ? ?
. . .
S(X1X2X3X4) ? PRELS(X1)VP2(X1,X4) VAFIN(X3)
VP2(X1X2X3,X4X5) ? ADV(X1) ADV(X2)
VP2(X3,X4) VMINF(X5)
VP2(X1,X2) ? NN(X1) VAINF(X2)
We distinguish different usages of the same cat-
egory depending on their numbers of yield com-
ponents. E.g., we distinguish non-terminals VP1,
VP2, . . . depending on the arity of the VP. We de-
fine cat(A) for A ? N as the category of A, inde-
pendent from the arity, e.g., cat(VP2) =VP.
In terms of simple RCG, synchronous rewrit-
ing means that in a single clause distinct variables
occurring in two different arguments of the LHS
predicate are passed to two different arguments of
the same RHS predicate. We call this recursive
71
S
NP
VP
VP
VP
PP NP
ob auf dem Gela?nde der Typ von Abstellanlage . . . ha?tte . . . gebaut werden sollen, der. . .
Figure 2: Iterable treebank example for synchronous rewriting
if, by a sequence of synchronous rewriting steps,
we can reach the same two arguments of the same
predicate again. Derivations using such cycles of
synchronous rewriting lead exactly to the recursive
synchronous rewriting trees characterized in sec-
tion 2. In the following, we check to which extent
the extracted simple RCG allows for such cycles.
In order to detect synchronous rewriting in a
simple k-RCG G, we build a labeled directed
graph G = (VG , EG , l) from the grammar with
VG a set of nodes, EG a set of arcs and l :
VG ? N ? ? {0, . . . , k} ? {0, . . . , k} where N ? =
{cat(A) |A ? N} a labeling function. G is con-
structed as follows. For each clause A0(~?) ?
A1( ~?1) . . . Am( ~?m) ? P we consider all pairs of
variables Xs,Xt for which the following condi-
tions hold: (i) Xs and Xt occur in different argu-
ments i and j of A0, 1 ? i < j ? dim(A0); and
(ii) Xs and Xt occur in different arguments q and
r of the same occurrence of predicate Ap in the
RHS, 1 ? q < r ? dim(Ap) and 1 ? p ? m.
For each of these pairs, two nodes with labels
[cat(A0), i, j] and [cat(Ap), q, r], respectively, are
added to VG (if they do not yet exist, otherwise we
take the already existing nodes) and a directed arc
from the first node to the second node is added to
EG . The intuition is that an arc in G represents
one or more clauses from the grammar in which
a gap between two variables in the LHS predicate
is transferred to the same RHS predicate. To de-
tect recursive synchronous rewriting, we then need
to discover all elementary cycles in G, i.e., all cy-
cles in which no vertex appears twice. In order to
accomplish this task efficiently, we exploit the al-
gorithm presented in Johnson (1975). On a gram-
mar extracted from NeGra (19,100 clauses), the
algorithm yields a graph with 28 nodes containing
206,403 cycles of an average length of 12.86 and
a maximal length of 28.
4 Conclusion
The starting point of this paper was the question
whether synchronous rewriting is a necessary fea-
ture of grammer formalisms for modelling natu-
ral languages. In order to answer this question,
we have characterized synchronous rewriting in
terms of properties of treebank trees with crossing
branches. Experiments have shown that recursive
cases of synchronous rewriting occur in treebanks
for German which leads to the conclusion that,
in order to model these data, we need formalisms
that allow for synchronous rewriting. In a second
part, we have extracted a simple RCG from these
treebanks and we have characterized the grammar
properties that are necessary to obtain recursive
synchronous rewriting. We then have investigated
the extent to which a grammar extracted from Ne-
Gra allows for recursive synchronous rewriting.
References
Pierre Boullier. 2000. Range concatenation grammars.
In Proceedings of IWPT.
Harry Bunt, Jan Thesingh, and Ko van der Sloot. 1987.
Discontinuous constituents in trees, rules and pars-
ing. In Proceedings of EACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
Donald B. Johnson. 1975. Finding all the elementary
circuits of a directed graph. SIAM Journal on Com-
puting.
Marco Kuhlmann. 2007. Dependency Structures and
Lexicalized Grammars. Dissertation, Saarland Uni-
versity.
Wolfgang Maier and Anders S?gaard. 2008. Tree-
banks and mild context-sensitivity. In Proceedings
of Formal Grammar.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Oliver Plaehn. 2004. Computing the most probable
parse for a discontinuous phrase-structure grammar.
In New developments in parsing technology. Kluwer.
H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991.
On multiple context-free grammars. Theoretical
Computer Science.
K. Vijay-Shanker, David Weir, and Aravind Joshi.
1987. Characterising structural descriptions used by
various formalisms. In Proceedings of ACL.
72
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1213?1221,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Computation of Infix Probabilities
for Probabilistic Context-Free Grammars
Mark-Jan Nederhof
School of Computer Science
University of St Andrews
United Kingdom
markjan.nederhof@gmail.com
Giorgio Satta
Dept. of Information Engineering
University of Padua
Italy
satta@dei.unipd.it
Abstract
The notion of infix probability has been intro-
duced in the literature as a generalization of
the notion of prefix (or initial substring) prob-
ability, motivated by applications in speech
recognition and word error correction. For the
case where a probabilistic context-free gram-
mar is used as language model, methods for
the computation of infix probabilities have
been presented in the literature, based on vari-
ous simplifying assumptions. Here we present
a solution that applies to the problem in its full
generality.
1 Introduction
Probabilistic context-free grammars (PCFGs for
short) are a statistical model widely used in natural
language processing. Several computational prob-
lems related to PCFGs have been investigated in
the literature, motivated by applications in model-
ing of natural language syntax. One such problem is
the computation of prefix probabilities for PCFGs,
where we are given as input a PCFG G and a string
w, and we are asked to compute the probability that
a sentence generated by G starts with w, that is, has
w as a prefix. This quantity is defined as the possi-
bly infinite sum of the probabilities of all strings of
the form wx, for any string x over the alphabet of G.
The problem of computation of prefix probabili-
ties for PCFGs was first formulated by Persoon and
Fu (1975). Efficient algorithms for its solution have
been proposed by Jelinek and Lafferty (1991) and
Stolcke (1995). Prefix probabilities can be used to
compute probability distributions for the next word
or part-of-speech, when a prefix of the input has al-
ready been processed, as discussed by Jelinek and
Lafferty (1991). Such distributions are useful for
speech recognition, where the result of the acous-
tic processor is represented as a lattice, and local
choices must be made for a next transition. In ad-
dition, distributions for the next word are also useful
for applications of word error correction, when one
is processing ?noisy? text and the parser recognizes
an error that must be recovered by operations of in-
sertion, replacement or deletion.
Motivated by the above applications, the problem
of the computation of infix probabilities for PCFGs
has been introduced in the literature as a generaliza-
tion of the prefix probability problem. We are now
given a PCFG G and a string w, and we are asked
to compute the probability that a sentence generated
by G has w as an infix. This probability is defined
as the possibly infinite sum of the probabilities of
all strings of the form xwy, for any pair of strings x
and y over the alphabet of G. Besides applications
in computation of the probability distribution for the
next word token and in word error correction, in-
fix probabilities can also be exploited in speech un-
derstanding systems to score partial hypotheses in
algorithms based on beam search, as discussed by
Corazza et al (1991).
Corazza et al (1991) have pointed out that the
computation of infix probabilities is more difficult
than the computation of prefix probabilities, due to
the added ambiguity that several occurrences of the
given infix can be found in a single string generated
by the PCFG. The authors developed solutions for
the case where some distribution can be defined on
1213
the distance of the infix from the sentence bound-
aries, which is a simplifying assumption. The prob-
lem is also considered by Fred (2000), which pro-
vides algorithms for the case where the language
model is a probabilistic regular grammar. However,
the algorithm in (Fred, 2000) does not apply to cases
with multiple occurrences of the given infix within
a string in the language, which is what was pointed
out to be the problematic case.
In this paper we adopt a novel approach to the
problem of computation of infix probabilities, by re-
moving the ambiguity that would be caused by mul-
tiple occurrences of the given infix. Although our
result is obtained by a combination of well-known
techniques from the literature on PCFG parsing and
pattern matching, as far as we know this is the first
algorithm for the computation of infix probabilities
that works for general PCFG models without any re-
strictive assumption.
The remainder of this paper is structured as fol-
lows. In Section 2 we explain how the sum of the
probabilities of all trees generated by a PCFG can
be computed as the least fixed-point solution of a
non-linear system of equations. In Section 3 we re-
call the construction of a new PCFG out of a given
PCFG and a given finite automaton, such that the
language generated by the new grammar is the in-
tersection of the languages generated by the given
PCFG and the automaton, and the probabilities of
the generated strings are preserved. In Section 4
we show how one can efficiently construct an un-
ambiguous finite automaton that accepts all strings
with a given infix. The material from these three
sections is combined into a new algorithm in Sec-
tion 5, which allows computation of the infix prob-
ability for PCFGs. This is the main result of this
paper. Several extensions of the basic technique are
discussed in Section 6. Section 7 discusses imple-
mentation and some experiments.
2 Sum of probabilities of all derivations
Assume a probabilistic context-free grammar G, rep-
resented by a 5-tuple (?, N, S, R, p), where ? and
N are two finite disjoint sets of terminals and non-
terminals, respectively, S ? N is the start symbol,
R is a finite set of rules, each of the form A ? ?,
whereA ? N and ? ? (??N)?, and p is a function
from rules in R to real numbers in the interval [0, 1].
The concept of left-most derivation in one step is
represented by the notation ? pi?G ?, which means
that the left-most occurrence of any nonterminal in
? ? (? ? N)? is rewritten by means of some rule
pi ? R. If the rewritten nonterminal is A, then pi
must be of the form (A ? ?) and ? is the result
of replacing the occurrence of A in ? by ?. A left-
most derivation with any number of steps, using a
sequence d of rules, is denoted as ? d?G ?. We omit
the subscript G when the PCFG is understood. We
also write ? ?? ? when the involved sequence of
rules is of no relevance. Henceforth, all derivations
we discuss are implicitly left-most.
A complete derivation is either the empty se-
quence of rules, or a sequence d = pi1 ? ? ?pim, m ?
1, of rules such that A d? w for some A ? N and
w ? ??. In the latter case, we say the complete
derivation starts with A, and in the former case, with
d an empty sequence of rules, we assume the com-
plete derivation starts and ends with a single termi-
nal, which is left unspecified. It is well-known that
there exists a bijective correspondence between left-
most complete derivations starting with nonterminal
A and parse trees derived by the grammar with root
A and a yield composed of terminal symbols only.
The depth of a complete derivation d is the length
of the longest path from the root to a leaf in the parse
tree associated with d. The length of a path is de-
fined as the number of nodes it visits. Thus if d = pi
for some rule pi = (A ? w) with w ? ??, then the
depth of d is 2.
The probability p(d) of a complete derivation d =
pi1 ? ? ?pim, m ? 1, is:
p(d) =
m?
i=1
p(pii).
We also assume that p(d) = 1 when d is an empty
sequence of rules. The probability p(w) of a string
w is the sum of all complete derivations deriving that
string from the start symbol:
p(w) =
?
d: S d?w
p(d).
With this notation, consistency of a PCFG is de-
1214
fined as the condition:
?
d,w: S d?w
p(d) = 1.
In other words, a PCFG is consistent if the sum
of probabilities of all complete derivations starting
with S is 1. An equivalent definition of consistency
considers the sum of probabilities of all strings:
?
w
p(w) = 1.
See (Booth and Thompson, 1973) for further discus-
sion.
In practice, PCFGs are often required to satisfy
the additional condition:
?
pi=(A??)
p(pi) = 1,
for each A ? N . This condition is called proper-
ness. PCFGs that naturally arise by parameter es-
timation from corpora are generally consistent; see
(Sa?nchez and Bened??, 1997; Chi and Geman, 1998).
However, in what follows, neither properness nor
consistency is guaranteed.
We define the partition function of G as the func-
tion Z that assigns to each A ? N the value
Z(A) =
?
d,w
p(A d? w). (1)
Note that Z(S) = 1 means that G is consistent.
More generally, in later sections we will need to
compute the partition function for non-consistent
PCFGs.
We can characterize the partition function of a
PCFG as a solution of a specific system of equa-
tions. Following the approach in (Harris, 1963; Chi,
1999), we introduce generating functions associated
with the nonterminals of the grammar. For A ? N
and ? ? (N ? ?)?, we write f(A,?) to denote the
number of occurrences of symbol A within string ?.
Let N = {A1, A2, . . . , A|N |}. For each Ak ? N , let
mk be the number of rules in R with left-hand side
Ak, and assume some fixed order for these rules. For
each i with 1 ? i ? mk, let Ak ? ?k,i be the i-th
rule with left-hand side Ak.
For each k with 1 ? k ? |N |, the generating
function associated with Ak is defined as
gAk(z1, z2, . . . , z|N |) =
mk?
i=1
(
p(Ak ? ?k,i) ?
|N |?
j=1
zf(Aj ,?k,i)j
)
. (2)
Furthermore, for each i ? 1 we recursively define
functions g(i)Ak(z1, z2, . . . , z|N |) by
g(1)Ak (z1, z2, . . . , z|N |) = gAk(z1, z2, . . . , z|N |), (3)
and, for i ? 2, by
g(i)Ak(z1, z2, . . . , z|N |) = (4)
gAk( g
(i?1)
A1 (z1, z2, . . . , z|N |),
g(i?1)A2 (z1, z2, . . . , z|N |), . . . ,
g(i?1)A|N| (z1, z2, . . . , z|N |) ).
Using induction it is not difficult to show that, for
each k and i as above, g(i)Ak(0, 0, . . . , 0) is the sum ofthe probabilities of all complete derivations fromAk
having depth not exceeding i. This implies that, for
i = 0, 1, 2, . . ., the sequence of the g(i)Ak(0, 0, . . . , 0)monotonically converges to Z(Ak).
For each k with 1 ? k ? |N | we can now write
Z(Ak) =
= lim
i??
g(i)Ak(0, . . . , 0)
= lim
i??
gAk( g
(i?1)
A1 (0, 0, . . . , 0), . . . ,
g(i?1)A|N| (0, 0, . . . , 0) )
= gAk( limi?? g
(i?1)
A1 (0, 0, . . . , 0), . . . ,
limi?? g(i?1)A|N| (0, 0, . . . , 0) )
= gAk(Z(A1), . . . , Z(A|N |)).
The above shows that the values of the partition
function provide a solution to the system of the fol-
lowing equations, for 1 ? k ? |N |:
zk = gAk(z1, z2, . . . , z|N |). (5)
In the case of a general PCFG, the above equa-
tions are non-linear polynomials with positive (real)
coefficients. We can represent the resulting system
in vector form and write X = g(X). These systems
1215
are called monotone systems of polynomial equa-
tions and have been investigated by Etessami and
Yannakakis (2009) and Kiefer et al (2007). The
sought solution, that is, the partition function, is the
least fixed point solution of X = g(X).
For practical reasons, the set of nonterminals of
a grammar is usually divided into maximal subsets
of mutually recursive nonterminals, that is, for each
A and B in such a subset, we have A ?? uB?
and B ?? vA?, for some u, v, ?, ?. This corre-
sponds to a strongly connected component if we
see the connection between the left-hand side of a
rule and a nonterminal member in its right-hand side
as an edge in a directed graph. For each strongly
connected component, there is a separate system of
equations of the formX = g(X). Such systems can
be solved one by one, in a bottom-up order. That
is, if one strongly connected component contains
nonterminalA, and another contains nonterminalB,
where A ?? uB? for some u, ?, then the system for
the latter component must be solved first.
The solution for a system of equations such as
those described above can be irrational and non-
expressible by radicals, even if we assume that all
the probabilities of the rules in the input PCFG are
rational numbers, as observed by Etessami and Yan-
nakakis (2009). Nonetheless, the partition function
can still be approximated to any degree of preci-
sion by iterative computation of the relation in (4),
as done for instance by Stolcke (1995) and by Ab-
ney et al (1999). This corresponds to the so-called
fixed-point iteration method, which is well-known
in the numerical calculus literature and is frequently
applied to systems of non-linear equations because
it can be easily implemented.
When a number of standard conditions are met,
each iteration of (4) adds a fixed number of bits
to the precision of the solution; see Kelley (1995,
Chapter 4). Since each iteration can easily be im-
plemented to run in polynomial time, this means
that we can approximate the partition function of a
PCFG in polynomial time in the size of the PCFG
itself and in the number of bits of the desired preci-
sion.
In practical applications where large PCFGs are
empirically estimated from data sets, the standard
conditions mentioned above for the polynomial time
approximation of the partition function are usually
met. However, there are some degenerate cases for
which these standard conditions do not hold, result-
ing in exponential time behaviour of the fixed-point
iteration method. This has been firstly observed
in (Etessami and Yannakakis, 2005).
An alternative iterative algorithm for the approx-
imation of the partition function has been proposed
by Etessami and Yannakakis (2009), based on New-
ton?s method for the solution of non-linear systems
of equations. From a theoretical perspective, Kiefer
et al (2007) have shown that, after a certain number
of initial iterations, Newton?s method adds a fixed
number of bits to the precision of the approximated
solution, even in the above mentioned cases in which
the fixed-point iteration method shows exponential
time behaviour. However, these authors also show
that, in some degenerate cases, the number of itera-
tions needed to compute the first bit of the solution
can be at least exponential in the size of the system.
Experiments with Newton?s method for the ap-
proximation of the partition functions of PCFGs
have been carried out in several application-oriented
settings, by Wojtczak and Etessami (2007) and by
Nederhof and Satta (2008), showing considerable
improvements over the fixed-point iteration method.
3 Intersection of PCFG and FA
It was shown by Bar-Hillel et al (1964) that context-
free languages are closed under intersection with
regular languages. Their proof relied on the con-
struction of a new CFG out of an input CFG and
an input finite automaton. Here we extend that con-
struction by letting the input grammar be a proba-
bilistic CFG. We refer the reader to (Nederhof and
Satta, 2003) for more details.
To avoid a number of technical complications, we
assume the finite automaton has no epsilon transi-
tions, and has only one final state. In the context
of our use of this construction in the following sec-
tions, these restrictions are without loss of general-
ity. Thus, a finite automaton (FA) M is represented
by a 5-tuple (?, Q, q0, qf , ?), where ? and Q are
two finite sets of terminals and states, respectively,
q0 is the initial state, qf is the final state, and ? is
a finite set of transitions, each of the form s a7? t,
where s, t ? Q and a ? ?.
A complete computation of M accepting string
1216
w = a1 ? ? ? an is a sequence c = ?1 ? ? ? ?n of tran-
sitions such that ?i = (si?1 ai7? si) for each i (1 ?
i ? n), for some s0, s1, . . . , sn with s0 = q0 and
sn = qf . The language of all strings accepted byM
is denoted by L(M). A FA is unambiguous if at
most one complete computation exists for each ac-
cepted string. A FA is deterministic if there is at
most one transition s a7? t for each s and a.
For a FAM as above and a PCFG G = (?, N, S,
R, p) with the same set of terminals, we construct
a new PCFG G? = (?, N ?, S?, R?, p?), where N ? =
Q? (??N)?Q, S? = (q0, S, qf ), and R? is the set
of rules that is obtained as follows.
? For each A ? X1 ? ? ?Xm in R and each se-
quence s0, . . . , sm with si ? Q, 0 ? i ? m,
and m ? 0, let (s0, A, sm) ? (s0, X1, s1) ? ? ?
(sm?1, Xm, sm) be in R?; if m = 0, the new
rule is of the form (s0, A, s0) ? . Function p?
assigns the same probability to the new rule as
p assigned to the original rule.
? For each s a7? t in ?, let (s, a, t) ? a be in R?.
Function p? assigns probability 1 to this rule.
Intuitively, a rule of G? is either constructed out of
a rule of G or out of a transition of M. On the basis
of this correspondence between rules and transitions
of G?, G and M, it is not difficult to see that each
derivation d? in G? deriving string w corresponds to a
unique derivation d in G deriving the same string and
a unique computation c in M accepting the same
string. Conversely, if there is a derivation d in G
deriving string w, and some computation c in M
accepting the same string, then the pair of d and c
corresponds to a unique derivation d? in G? deriving
the same string w. Furthermore, the probabilities of
d and d? are equal, by definition of p?.
Let us now assume that each string w is accepted
by at most one computation, i.e. M is unambigu-
ous. If a string w is accepted by M, then there are
as many derivations deriving w in G? as there are in
G. If w is not accepted by M, then there are zero
derivations deriving w in G?. Consequently:
?
d?,w:
S? d
?
?G?w
p?(d?) =
?
d,w:
S d?Gw?w?L(M)
p(d),
or more succinctly:
?
w
p?(w) =
?
w?L(M)
p(w).
Note that the above construction of G? is exponen-
tial in the largest value of m in any rule from G. For
this reason, G is usually brought in binary form be-
fore the intersection, i.e. the input grammar is trans-
formed to let each right-hand side have at most two
members. Such a transformation can be realized in
linear time in the size of the grammar. We will return
to this issue in Section 7.
4 Obtaining unambiguous FAs
In the previous section, we explained that unambigu-
ous finite automata have special properties with re-
spect to the grammar G? that we may construct out
of a FA M and a PCFG G. In this section we dis-
cuss how unambiguity can be obtained for the spe-
cial case of finite automata accepting the language
of all strings with given infix w ? ??:
Linfix (w) = {xwy | x, y ? ??}.
Any deterministic automaton is also unambigu-
ous. Furthermore, there seem to be no practical al-
gorithms that turn FAs into equivalent unambiguous
FAs other than the algorithms that also determinize
them. Therefore, we will henceforth concentrate on
deterministic rather than unambiguous automata.
Given a string w = a1 ? ? ? an, a finite automaton
accepting Linfix (w) can be straightforwardly con-
structed. This automaton has states s0, . . . , sn, tran-
sitions s0 a7? s0 and sn a7? sn for each a ? ?, and
transition si?1 ai7? si for each i (1 ? i ? n). The
initial state is s0 and the final state is sn. Clearly,
there is nondeterminism in state s0.
One way to make this automaton deterministic is
to apply the general algorithm of determinization of
finite automata; see e.g. (Aho and Ullman, 1972).
This algorithm is exponential for general FAs. An
alternative approach is to construct a deterministic
finite automaton directly from w, in line with the
Knuth-Morris-Pratt algorithm (Knuth et al, 1977;
Gusfield, 1997). Both approaches result in the same
deterministic FA, which we denote by Iw. However,
the latter approach is easier to implement in such a
1217
way that the time complexity of constructing the au-
tomaton is linear in |w|.
The automaton Iw is described as follows. There
are n + 1 states t0, . . . , tn, where as before n is
the length of w. The initial state is t0 and the final
state is tn. The intuition is that Iw reads a string
x = b1 ? ? ? bm from left to right, and when it has
read the prefix b1 ? ? ? bj (0 ? j ? m), it is in state
ti (0 ? i < n) if and only if a1 ? ? ? ai is the longest
prefix of w that is also a suffix of b1 ? ? ? bj . If the
automaton is in state tn, then this means that w is an
infix of b1 ? ? ? bj .
In more detail, for each i (1 ? i ? n) and each
a ? ?, there is a transition ti?1 a7? tj , where j is
the length of the longest string that is both a prefix
of w and a suffix of a1 ? ? ? ai?1a. If a = ai, then
clearly j = i, and otherwise j < i. To ensure that we
remain in the final state once an occurrence of infix
w has been found, we also add transitions tn a7? tn
for each a ? ?. This construction is illustrated in
Figure 1.
5 Infix probability
With the material developed in the previous sections,
the problem of computing the infix probabilities can
be effectively solved. Our goal is to compute for
given infix w ? ?? and PCFG G = (?, N, S, R,
p):
?infix (w,G) =
?
z?Linfix (w)
p(z).
In Section 4 we have shown the construction of finite
automaton Iw accepting Linfix (w), by which we ob-
tain:
?infix (w,G) =
?
z?L(Iw)
p(z).
As Iw is deterministic and therefore unambiguous,
the results from Section 3 apply and if G? = (?, N ?,
S?, R?, p?) is the PCFG constructed out of G and Iw
then:
?infix (w,G) =
?
z
p?(z).
Finally, we can compute the above sum by applying
the iterative method discussed in Section 2.
6 Extensions
The approach discussed above allows for a number
of generalizations. First, we can replace the infix w
by a sequence of infixes w1, . . . , wm, which have to
occur in the given order, one strictly after the other,
with arbitrary infixes in between:
?island (w1, . . . , wm,G) =?
x0,...,xm???
p(x0w1x1 ? ? ?wmxm).
This problem was discussed before by (Corazza et
al., 1991), who mentioned applications in speech
recognition. Further applications are found in com-
putational biology, but their discussion is beyond the
scope of this paper; see for instance (Apostolico et
al., 2005) and references therein. In order to solve
the problem, we only need a small addition to the
procedures we discussed before. First we construct
separate automata Iwj (1 ? j ? m) as explained in
Section 4. These automata are then composed into
a single automaton I(w1,...,wm). In this composition,
the outgoing transitions of the final state of Iwj , for
each j (1 ? j < m), are removed and that final state
is merged with the initial state of the next automaton
Iwj+1 . The initial state of the composed automaton
is the initial state of Iw1 , and the final state is the
final state of Iwm . The time costs of constructing
I(w1,...,wm) are linear in the sum of the lengths of the
strings wj .
Another way to generalize the problem is to re-
place w by a finite set L = {w1, . . . , wm}. The ob-
jective is to compute:
?infix (L,G) =
?
w?L,x,y???
p(xwy)
Again, this can be solved by first constructing a de-
terministic FA, which is then intersected with G.
This FA can be obtained by determinizing a straight-
forward nondeterministic FA accepting L, or by di-
rectly constructing a deterministic FA along the lines
of the Aho-Corasick algorithm (Aho and Corasick,
1975). Construction of the automaton with the latter
approach takes linear time.
Further straightforward generalizations involve
formalisms such as probabilistic tree adjoining
grammars (Schabes, 1992; Resnik, 1992). The tech-
nique from Section 3 is also applicable in this case,
1218
t0 t1 t2 t3 t4a b a c
b, c a a, b, cc
b, c a
b
Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac.
as the construction from Bar-Hillel et al (1964) car-
ries over from context-free grammars to tree ad-
joining grammars, and more generally to the linear
context-free rewriting systems of Vijay-Shanker et
al. (1987).
7 Implementation
We have conducted experiments with the computa-
tion of infix probabilities. The objective was to iden-
tify parts of the computation that have a high time
or space demand, and that might be improved. The
experiments were run on a desktop with a 3.0 GHz
Pentium 4 processor. The implementation language
is C++.
The set-up of the experiments is similar to that in
(Nederhof and Satta, 2008). A probabilistic context-
free grammar was extracted from sections 2-21 of
the Penn Treebank version II. Subtrees that gener-
ated the empty string were systematically removed.
The result was a CFG with 10,035 rules, 28 nonter-
minals and 36 parts-of-speech. The rule probabili-
ties were determined by maximum likelihood esti-
mation. The grammar was subsequently binarized,
to avoid exponential behaviour, as explained in Sec-
tion 3.
We have considered 10 strings of length 7, ran-
domly generated, assuming each of the parts-of-
speech has the same probability. For all prefixes of
those strings from length 2 to length 7, we then com-
puted the infix probability. The duration of the full
computation, averaged over the 10 strings of length
7, is given in the first row of Table 1.
In order to solve the non-linear systems of equa-
tions, we used Broyden?s method. It can be seen
as an approximation of Newton?s method. It re-
quires more iterations, but seems to be faster over-
all, and more scalable to large problem sizes, due to
the avoidance of matrix inversion, which sometimes
makes Newton?s method prohibitively expensive. In
our experiments, Broyden?s method was generally
faster than Newton?s method and much faster than
the simple iteration method by the relation in (4).
For further details on Broyden?s method, we refer
the reader to (Kelley, 1995).
The main obstacle to computation for infixes sub-
stantially longer than 7 symbols is the memory con-
sumption rather than the running time. This is due
to the required square matrices, the dimension of
which is the number of nonterminals. The number
of nonterminals (of the intersection grammar) natu-
rally grows as the infix becomes longer.
As explained in Section 2, the problem is divided
into smaller problems by isolating disjoint sets of
mutually recursive nonterminals, or strongly con-
nected components. We found that for the applica-
tion to the automata discussed in Section 4, there
were exactly three strongly connected components
that contained more than one element, throughout
the experiments. For an infix of length n, these com-
ponents are:
? C1, which consists of nonterminals of the form
(ti, A, tj), where i < n and j < n,
? C2, which consists of nonterminals of the form
(ti, A, tj), where i = j = n, and
? C3, which consists of nonterminals of the form
(ti, A, tj), where i < j = n.
This can be easily explained by looking at the struc-
ture of our automata. See for example Figure 1, with
cycles running through states t0, . . . , tn?1, and cy-
cles through state tn. Furthermore, the grammar ex-
tracted from the Penn Treebank is heavily recursive,
1219
infix length 2 3 4 5 6 7
total running time 1.07 1.95 5.84 11.38 23.93 45.91
Broyden?s method for C1 0.46 0.90 3.42 6.63 12.91 24.38
Broyden?s method for C2 0.08 0.04 0.07 0.04 0.03 0.09
Broyden?s method for C3 0.20 0.36 0.81 1.74 5.30 9.02
Table 1: Running time for infixes from length 2 to length 7. The infixes are prefixes of 10 random strings of length 7,
and reported CPU times (in seconds) are averaged over the 10 strings.
so that almost every nonterminal can directly or in-
directly call any other.
The strongly connected component C2 is always
the same, consisting of 2402 nonterminals, for each
infix of any length. (Note that binarization of the
grammar introduced artificial nonterminals.) The
last three rows of Table 1 present the time costs of
Broyden?s method, for the three strongly connected
components.
The strongly connected componentC3 happens to
correspond to a linear system of equations. This is
because a rule in the intersection grammar with a
left-hand side (ti, A, tj), where i < j = n, must
have a right-hand side of the form (ti, A?, tj), or of
the form (ti, A1, tk) (tk, A2, tj), with k ? n. If k <
n, then only the second member can be in C3. If
k = n, only first member can be in C3. Hence,
such a rule corresponds to a linear equation within
the system of equations for the entire grammar.
A linear system of equations can be solved an-
alytically, for example by Gaussian elimination,
rather than approximated through Newton?s method
or Broyden?s method. This means that the running
times in the last row of Table 1 can be reduced by
treating C3 differently from the other strongly con-
nected components. However, the running time for
C1 dominates the total time consumption.
The above investigations were motivated by two
questions, namely whether any part of the computa-
tion can be precomputed, and second, whether infix
probabilities can be computed incrementally, for in-
fixes that are extended to the left or to the right. The
first question can be answered affirmatively for C2,
as it is always the same. However, as we can see in
Table 1, the computation of C2 amounts to a small
portion of the total time consumption.
The second question can be rephrased more pre-
cisely as follows. Suppose we have computed the
infix probability of a string w and have kept inter-
mediate results in memory. Can the computation of
the infix probability of a string of the form aw orwa,
a ? ?, be computed by relying on the existing re-
sults, so that the computation is substantially faster
than if the computation were done from scratch?
Our investigations so far have not found a posi-
tive answer to this second question. In particular,
the systems of equations for C1 and C3 change fun-
damentally if the infix is extended by one more sym-
bol, which seems to at least make incremental com-
putation very difficult, if not impossible. Note that
the algorithms for the computation of prefix prob-
abilities by Jelinek and Lafferty (1991) and Stolcke
(1995) do allow incrementality, which contributes to
their practical usefulness for speech recognition.
8 Conclusions
We have shown that the problem of infix probabili-
ties for PCFGs can be solved by a construction that
intersects a context-free language with a regular lan-
guage. An important constraint is that the finite
automaton that is input to this construction be un-
ambiguous. We have shown that such an automa-
ton can be efficiently constructed. Once the input
probabilistic PCFG and the FA have been combined
into a new probabilistic CFG, the infix probability
can be straightforwardly solved by iterative algo-
rithms. Such algorithms include Newton?s method,
and Broyden?s method, which was used in our exper-
iments. Our discussion ended with an open question
about the possibility of incremental computation of
infix probabilities.
References
S. Abney, D. McAllester, and F. Pereira. 1999. Relating
probabilistic grammars and automata. In 37th Annual
1220
Meeting of the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 542?549,
Maryland, USA, June.
A.V. Aho and M.J. Corasick. 1975. Efficient string
matching: an aid to bibliographic search. Communi-
cations of the ACM, 18(6):333?340, June.
A.V. Aho and J.D. Ullman. 1972. Parsing, volume 1
of The Theory of Parsing, Translation and Compiling.
Prentice-Hall, Englewood Cliffs, N.J.
A. Apostolico, M. Comin, and L. Parida. 2005. Con-
servative extraction of overrepresented extensible mo-
tifs. In Proceedings of Intelligent Systems for Molecu-
lar Biology (ISMB05).
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Y. Bar-Hillel, editor, Language and Information: Se-
lected Essays on their Theory and Application, chap-
ter 9, pages 116?150. Addison-Wesley, Reading, Mas-
sachusetts.
T.L. Booth and R.A. Thompson. 1973. Applying prob-
abilistic measures to abstract languages. IEEE Trans-
actions on Computers, C-22:442?450.
Z. Chi and S. Geman. 1998. Estimation of probabilis-
tic context-free grammars. Computational Linguistics,
24(2):299?305.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131?160.
A. Corazza, R. De Mori, R. Gretter, and G. Satta.
1991. Computation of probabilities for an island-
driven parser. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 13(9):936?950.
K. Etessami and M. Yannakakis. 2005. Recursive
Markov chains, stochastic grammars, and monotone
systems of nonlinear equations. In 22nd International
Symposium on Theoretical Aspects of Computer Sci-
ence, volume 3404 of Lecture Notes in Computer Sci-
ence, pages 340?352, Stuttgart, Germany. Springer-
Verlag.
K. Etessami and M. Yannakakis. 2009. Recursive
Markov chains, stochastic grammars, and monotone
systems of nonlinear equations. Journal of the ACM,
56(1):1?66.
A.L.N. Fred. 2000. Computation of substring proba-
bilities in stochastic grammars. In A. Oliveira, edi-
tor, Grammatical Inference: Algorithms and Applica-
tions, volume 1891 of Lecture Notes in Artificial Intel-
ligence, pages 103?114. Springer-Verlag.
D. Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences. Cambridge University Press, Cambridge.
T.E. Harris. 1963. The Theory of Branching Processes.
Springer-Verlag, Berlin, Germany.
F. Jelinek and J.D. Lafferty. 1991. Computation of the
probability of initial substring generation by stochas-
tic context-free grammars. Computational Linguistics,
17(3):315?323.
C.T. Kelley. 1995. Iterative Methods for Linear and
Nonlinear Equations. Society for Industrial and Ap-
plied Mathematics, Philadelphia, PA.
S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On the
convergence of Newton?s method for monotone sys-
tems of polynomial equations. In Proceedings of the
39th ACM Symposium on Theory of Computing, pages
217?266.
D.E. Knuth, J.H. Morris, Jr., and V.R. Pratt. 1977. Fast
pattern matching in strings. SIAM Journal on Comput-
ing, 6:323?350.
M.-J. Nederhof and G. Satta. 2003. Probabilistic pars-
ing as intersection. In 8th International Workshop on
Parsing Technologies, pages 137?148, LORIA, Nancy,
France, April.
M.-J. Nederhof and G. Satta. 2008. Computing parti-
tion functions of PCFGs. Research on Language and
Computation, 6(2):139?162.
E. Persoon and K.S. Fu. 1975. Sequential classification
of strings generated by SCFG?s. International Journal
of Computer and Information Sciences, 4(3):205?217.
P. Resnik. 1992. Probabilistic tree-adjoining grammar as
a framework for statistical natural language process-
ing. In Proc. of the fifteenth International Conference
on Computational Linguistics, Nantes, August, pages
418?424.
J.-A. Sa?nchez and J.-M. Bened??. 1997. Consistency
of stochastic context-free grammars from probabilis-
tic estimation based on growth transformations. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 19(9):1052?1055, September.
Y. Schabes. 1992. Stochastic lexicalized tree-adjoining
grammars. In Proc. of the fifteenth International Con-
ference on Computational Linguistics, Nantes, Au-
gust, pages 426?432.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):167?201.
K. Vijay-Shanker, D.J. Weir, and A.K. Joshi. 1987.
Characterizing structural descriptions produced by
various grammatical formalisms. In 25th Annual
Meeting of the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 104?111,
Stanford, California, USA, July.
D. Wojtczak and K. Etessami. 2007. PReMo: an an-
alyzer for Probabilistic Recursive Models. In Tools
and Algorithms for the Construction and Analysis of
Systems, 13th International Conference, volume 4424
of Lecture Notes in Computer Science, pages 66?71,
Braga, Portugal. Springer-Verlag.
1221
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234?1245,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Exact Inference for Generative Probabilistic
Non-Projective Dependency Parsing
Shay B. Cohen
School of Computer Science
Carnegie Mellon University, USA
scohen@cs.cmu.edu
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a, Spain
cgomezr@udc.es
Giorgio Satta
Dept. of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
We describe a generative model for non-
projective dependency parsing based on a sim-
plified version of a transition system that has
recently appeared in the literature. We then
develop a dynamic programming parsing al-
gorithm for our model, and derive an inside-
outside algorithm that can be used for unsu-
pervised learning of non-projective depend-
ency trees.
1 Introduction
Dependency grammars have received considerable
attention in the statistical parsing community in
recent years. These grammatical formalisms of-
fer a good balance between structural expressiv-
ity and processing efficiency. Most notably, when
non-projectivity is supported, these formalisms can
model crossing syntactic relations that are typical in
languages with relatively free word order.
Recent work has reduced non-projective parsing
to the identification of a maximum spanning tree in a
graph (McDonald et al, 2005; Koo et al, 2007; Mc-
Donald and Satta, 2007; Smith and Smith, 2007).
An alternative to this approach is to use transition-
based parsing (Yamada and Matsumoto, 2003; Nivre
and Nilsson, 2005; Attardi, 2006; Nivre, 2009;
Go?mez-Rodr??guez and Nivre, 2010), where there is
an incremental processing of a string with a model
that scores transitions between parser states, condi-
tioned on the parse history. This paper focuses on
the latter approach.
The above work on transition-based parsing has
focused on greedy algorithms set in a statistical
framework (Nivre, 2008). More recently, dynamic
programming has been successfully used for pro-
jective parsing (Huang and Sagae, 2010; Kuhlmann
et al, 2011). Dynamic programming algorithms for
parsing (also known as chart-based algorithms) al-
low polynomial space representations of all parse
trees for a given input string, even in cases where
the size of this set is exponential in the length of
the string itself. In combination with appropriate
semirings, these packed representations can be ex-
ploited to compute many values of interest for ma-
chine learning, such as best parses and feature ex-
pectations (Goodman, 1999; Li and Eisner, 2009).
In this paper we move one step forward with re-
spect to Huang and Sagae (2010) and Kuhlmann et
al. (2011) and present a polynomial dynamic pro-
gramming algorithm for non-projective transition-
based parsing. Our algorithm is coupled with a
simplified version of the transition system from At-
tardi (2006), which has high coverage for the type
of non-projective structures that appear in various
treebanks. Instead of an additional transition oper-
ation which permits swapping of two elements in
the stack (Titov et al, 2009; Nivre, 2009), Attardi?s
system allows reduction of elements at non-adjacent
positions in the stack. We also present a generat-
ive probabilistic model for transition-based parsing.
The implication for this, for example, is that one can
now approach the problem of unsupervised learning
of non-projective dependency structures within the
transition-based framework.
Dynamic programming algorithms for non-
projective parsing have been proposed by Kahane et
al. (1998), Go?mez-Rodr??guez et al (2009) and Kuhl-
mann and Satta (2009), but they all run in exponen-
tial time in the ?gap degree? of the parsed structures.
To the best of our knowledge, this paper is the first to
1234
introduce a dynamic programming algorithm for in-
ference with non-projective structures of unbounded
gap degree.
The rest of this paper is organized as follows. In
?2 and ?3 we outline the transition-based model we
use, together with a probabilistic generative inter-
pretation. In ?4 we give the tabular algorithm for
parsing, and in ?5 we discuss statistical inference
using expectation maximization. We then discuss
some other aspects of the work in ?6 and conclude
in ?7.
2 Transition-based Dependency Parsing
In this section we briefly introduce the basic defini-
tions for transition-based dependency parsing. For a
more detailed presentation of this subject, we refer
the reader to Nivre (2008). We then define a spe-
cific transition-based model for non-projective de-
pendency parsing that we investigate in this paper.
2.1 General Transition Systems
Assume an input alphabet ? with a special symbol
$ ? ? , which we use as the root of our parse struc-
tures. Throughout this paper we denote the input
string as w = a0 ? ? ? an?1, n ? 1, where a0 = $ and
ai ? ? \ {$} for each i with 1 ? i ? n? 1.
A dependency tree for w is a directed tree Gw =
(Vw, Aw), where Vw = {0, . . . , n ? 1} is the set of
nodes, and Aw ? Vw ? Vw is the set of arcs. The
root of Gw is the node 0. The intended meaning
is that each node in Vw encodes the position of a
token in w. Furthermore, each arc in Aw encodes a
dependency relation between two tokens. We write
i ? j to denote a directed arc (i, j) ? Aw, where
node i is the head and node j is the dependent.
A transition system (for dependency parsing) is a
tuple S = (C, T, I, Ct), whereC is a set of configur-
ations, defined below, T is a finite set of transitions,
which are partial functions t:C ? C, I is a total
initialization function mapping each input string to
a unique initial configuration, and Ct ? C is a set of
terminal configurations.
A configuration is defined relative to some input
string w, and is a triple (?, ?,A), where ? and ? are
disjoint lists called stack and buffer, respectively,
and A ? Vw ? Vw is a set of arcs. Elements of
? and ? are nodes from Vw and, in the case of the
stack, a special symbol ? that we will use as initial
stack symbol. If t is a transition and c1, c2 are con-
figurations such that t(c1) = c2, we write c1 `t c2,
or simply c1 ` c2 if t is understood from the context.
Given an input string w, a parser based on S in-
crementally processes w from left to right, starting
in the initial configuration I(w). At each step, the
parser nondeterministically applies one transition, or
else it stops if it has reached some terminal config-
uration. The dependency graph defined by the arc
set associated with a terminal configuration is then
returned as one possible analysis for w.
Formally, a computation of S is a sequence ? =
c0, . . . , cm, m ? 1, of configurations such that, for
every iwith 1 ? i ? m, ci?1 `ti ci for some ti ? T .
In other words, each configuration in a computa-
tion is obtained as the value of the preceding con-
figuration under some transition. A computation is
called complete whenever c0 = I(w) for some in-
put string w, and cm ? Ct.
We can view a transition-based dependency
parser as a device mapping strings into graphs (de-
pendency trees). Without any restriction on trans-
ition functions in T , these functions might have an
infinite domain, and could thus encode even non-
recursively enumerable languages. However, in
standard practice for natural language parsing, trans-
itions are always specified by some finite mean. In
particular, the definition of each transition depends
on some finite window at the top of the stack and
some finite window at the beginning of the buffer
in each configuration. In this case, we can view a
transition-based dependency parser as a notational
variant of a push-down transducer (Hopcroft et al,
2000), whose computations output sequences that
directly encode dependency trees. These transducers
are nondeterministic, meaning that several trans-
itions can be applied to some configurations. The
transition systems we investigate in this paper fol-
low these principles.
We close this subsection with some additional
notation. We denote the stack with its topmost ele-
ment to the right and the buffer with its first ele-
ment to the left. We indicate concatenation in the
stack and buffer by a vertical bar. For example, for
k ? Vw, ?|k denotes some stack with topmost ele-
ment k and k|? denotes some buffer with first ele-
ment k. For 0 ? i ? n ? 1, ?i denotes the buffer
1235
[i, i + 1, . . . , n ? 1]; for i ? n, ?i denotes [] (the
empty buffer).
2.2 A Non-projective Transition System
We now turn to give a description of our trans-
ition system for non-projective parsing. While a
projective dependency tree satisfies the requirement
that, for every arc in the tree, there is a direc-
ted path between its headword and each of the
words between the two endpoints of the arc, a non-
projective dependency tree may violate this condi-
tion. Even though some natural languages exhibit
syntactic phenomena which require non-projective
expressive power, most often such a resource is used
in a limited way.
This idea is demonstrated by Attardi (2006), who
proposes a transition system whose individual trans-
itions can deal with non-projective dependencies
only to a limited extent, depending on the distance
in the stack of the nodes involved in the newly con-
structed dependency. The author defines this dis-
tance as the degree of the transition, with transitions
of degree one being able to handle only projective
dependencies. This formulation permits parsing a
subset of the non-projective trees, where this subset
depends on the degree of the transitions. The repor-
ted coverage in Attardi (2006) is already very high
when the system is restricted to transitions of degree
two or three. For instance, on training data for Czech
containing 28,934 non-projective relations, 27,181
can be handled by degree two transitions, and 1,668
additional dependencies can be handled by degree
three transitions. Table 1 gives additional statistics
for treebanks from the CoNLL-X shared task (Buch-
holz and Marsi, 2006).
We now turn to describe our variant of the trans-
ition system of Attardi (2006), which is equivalent to
the original system restricted to transitions of degree
two. Our results are based on such a restriction. It is
not difficult to extend our algorithms (?4) to higher
degree transitions, but this comes at the expense of
higher complexity. See ?6 for more discussion on
this issue.
Let w = a0 ? ? ? an?1 be an input string over ?
defined as in ?2.1, with a0 = $. Our transition sys-
tem for non-projective dependency parsing is
S(np) = (C, T (np), I(np), C(np)t ),
Language Deg. 2 Deg. 3 Deg. 4
Arabic 180 21 7
Bulgarian 961 41 10
Czech 27181 1668 85
Danish 876 136 53
Dutch 9072 2119 171
German 15827 2274 466
Japanese 1484 143 9
Portuguese 3104 424 37
Slovene 601 48 13
Spanish 66 7 0
Swedish 1566 226 79
Turkish 579 185 8
Table 1: The number of non-projective relations of vari-
ous degrees for several treebanks (training sets), as repor-
ted by the parser of Attardi (2006). Deg. stands for ?de-
gree.? The parser did not detect non-projective relations
of degree higher than 4.
where C is the same set of configurations defined
in ?2.1. The initialization function I(np) maps each
string w to the initial configuration ([?], ?0, ?). The
set of terminal configurationsC(np)t contains all con-
figurations of the form ([?, 0], [], A), for any set of
arcs A.
The set of transition functions is defined as
T (np) = {shb | b ? ?} ? {la1, ra1, la2, ra2},
where each transition is specified below. We let vari-
ables i, j, k, l range over Vw, and variable ? is a list
of stack elements from Vw ? {?}:
shb : (?, k|?,A) ` (?|k, ?,A) if ak = b;
la1 : (?|i|j, ?,A) ` (?|j, ?,A ? {j ? i});
ra1 : (?|i|j, ?,A) ` (?|i, ?, A ? {i? j});
la2 : (?|i|j|k, ?,A) ` (?|j|k, ?,A ? {k ? i});
ra2 : (?|i|j|k, ?,A) ` (?|i|j, ?,A ? {i? k}).
Each of the above transitions is undefined on config-
urations that do not match the forms specified above.
As an example, transition la2 is not defined for a
configuration (?, ?,A) with |?| ? 2, and transition
shb is not defined for a configuration (?, k|?,A)
with b 6= ak, or for a configuration (?, [], A).
Transition shb removes the first node from the buf-
fer, in case this node represents symbol b ? ? ,
1236
and pushes it into the stack. These transitions are
called shift transitions. The remaining four trans-
itions are called reduce transitions, i.e., transitions
that consume nodes from the stack. Notice that in
the transition system at hand all the reduce trans-
itions decrease the size of the stack by one ele-
ment. Transition la1 creates a new arc with the top-
most node on the stack as the head and the second-
topmost node as the dependent, and removes the
latter from the stack. Transition ra1 is symmetric
with respect to la1. Transitions la1 and ra1 have
degree one, as already explained. When restricted
to these three transitions, the system is equivalent
to the so-called stack-based arc-standard model of
Nivre (2004). Transition la2 and transition ra2 are
very similar to la1 and ra1, respectively, but with
the difference that they create a new arc between
the topmost node in the stack and a node which is
two positions below the topmost node. Hence, these
transitions have degree two, and are the key com-
ponents in parsing of non-projective dependencies.
We turn next to describe the equivalence between
our system and the system in Attardi (2006). The
transition-based parser presented by Attardi pushes
back into the buffer elements that are in the top pos-
ition of the stack. However, a careful analysis shows
that only the first position in the buffer can be af-
fected by this operation, in the sense that elements
that are pushed back from the stack are never found
in buffer positions other than the first. This means
that we can consider the first element of the buffer
as an additional stack element, always sitting on the
top of the top-most stack symbol.
More formally, we can define a function mc :
C ? C that maps configurations in the original al-
gorithm to those in our variant as follows:
mc((?, k|?,A)) = (?|k, ?,A)
By applying this mapping to the source and target
configuration of each transition in the original sys-
tem, it is easy to check that c1 ` c2 in that parser if
and only if mc(c1) ` mc(c2) in our variant. We ex-
tend this and define an isomorphism between com-
putations in both systems, such that a computation
c0, . . . , cm in the original parser is mapped to a com-
putation mc(c0), . . . ,mc(cm) in the variant, with
both generating the same dependency graph A. This
???
??? 2n2n? 12n? 21 2 3
Figure 1: A dependency structure of arbitrary gap degree
that can be parsed with Attardi?s parser.
proves that our notational variant is in fact equival-
ent to Attardi?s parser.
A relevant property of the set of dependency
structures that can be processed by Attardi?s parser,
even when restricted to transitions of degree two, is
that the number of discontinuities present in each of
their subtrees, defined as the gap degree by Bod-
irsky et al (2005), is not bounded. For example, the
dependency graph in Figure 1 has gap degree n? 1,
and it can be parsed by the algorithm for any arbit-
rary n ? 1 by applying 2n shb transitions to push
all the nodes into the stack, followed by (2n ? 2)
ra2 transitions to create the crossing arcs, and finally
one ra1 transition to create the dependency 1? 2.
As mentioned in ?1, the computational complex-
ity of the dynamic programming algorithm that will
be described in later sections does not depend on the
gap degree, contrary to the non-projective depend-
ency chart parsers presented by Go?mez-Rodr??guez et
al. (2009) and by Kuhlmann and Satta (2009), whose
running time is exponential in the maximum gap de-
gree allowed by the grammar.
3 A Generative Probabilistic Model
In this section we introduce a generative probabil-
istic model based on the transition system of ?2.2.
In formal language theory, there is a standard way
of giving a probabilistic interpretation to a non-
deterministic parser whose computations are based
on sequences of elementary operations such as trans-
itions. The idea is to define conditional probability
distributions over instances of the transition func-
tions, and to ?combine? these probabilities to assign
probabilities to computations and strings.
One difficulty we have to face with when dealing
with transition systems is that the notion of compu-
tation, defined in ?2.1, depends on the input string,
because of the buffer component appearing in each
configuration. This is a pitfall to generative model-
1237
ing, where we are interested in a system whose com-
putations lead to the generation of any string. To
overcome this problem, we observe that each com-
putation, defined as a sequence of stacks and buffers
(the configurations) can equivalently be expressed as
a sequence of stacks and transitions.
More precisely, consider a computation ? =
c0, . . . , cm, m ? 1. Let ?i, be the stack associated
with ci, for each i with 0 ? i ? m. Let alo C? be
the set of all stacks associated with configurations in
C. We can make explicit the transitions that have
been used in the computation by rewriting ? in the
form ?0 `t1 ?1 ? ? ??m?1 `tm ?m. In this way, ?
generates a string that is composed by all symbols
that are pushed into the stack by transitions shb, in
the left to right order.
We can now associate a probability to (our repres-
entation of) sequence ? by setting
p(?) =
m?
i=1
p(ti | ?i?1). (1)
To assign probabilities to complete computations we
should further multiply p(?) by factors ps(?0) and
pe(?m), where ps and pe are start and end probabil-
ity distributions, respectively, both defined over C?.
Note however that, as defined in ?2.2, all initial con-
figurations are associated with stack [?] and all final
configurations are associated with stack [?, 0], thus
ps and pe are deterministic. Note that the Markov
chain represented in Eq. 1 is homogeneous, i.e., the
probabilities of the transition operations do not de-
pend on the time step.
As a second step we observe that, according to the
definition of transition system, each t ? T has an in-
finite domain. A commonly adopted solution is to
introduce a special function, called history function
and denoted by H , defined over the set C? and tak-
ing values over some finite set. For each t ? T and
?, ?? ? C?, we then impose the condition
p(t | ?) = p(t | ??)
whenever H(?) = H(??). Since H is finitely val-
ued, and since T is a finite set, the above condition
guarantees that there will only be a finite number of
parameters p(t | ?) in our model.
So far we have presented a general discussion of
how to turn a transition-based parser into a gener-
ative probabilistic model, and have avoided further
specification of the history function. We now turn
our attention to the non-projective transition system
of ?2.2. To actually transform that system into a
parametrized probabilistic model, and to develop an
associated efficient inference procedure as well, we
need to balance between the amount of information
we put into the history function and the computa-
tional complexity which is required for inference.
We start the discussion with a na??ve model using a
history function defined by a fixed size window over
the topmost portion of the stack. More precisely,
each transition is conditioned on the lexical form of
the three symbols at the top of the stack ?, indic-
ated as b3, b2, b1 ? ? below, with b1 referring to the
topmost symbol. The parameters of the model are
defined as follows.
p(shb | b3, b2, b1) = ?shbb3,b2,b1 , ?b ? ? ,
p(la1 | b3, b2, b1) = ?la1b3,b2,b1 ,
p(ra1 | b3, b2, b1) = ?ra1b3,b2,b1 ,
p(la2 | b3, b2, b1) = ?la2b3,b2,b1 ,
p(ra2 | b3, b2, b1) = ?ra2b3,b2,b1 .
The parameters above are subject to the follow-
ing normalization conditions, for every choice of
b3, b2, b1 ? ? :
?la1b3,b2,b1 + ?
ra1
b3,b2,b1 + ?
la2
b3,b2,b1+
?ra2b3,b2,b1 +
?
b??
?shbb3,b2,b1 = 1 .
This na??ve model presents two practical problems.
The first problem relates to the efficiency of an in-
ference algorithm, which has a quite high computa-
tional complexity, as it will be discussed in ?5. A
second problem arises in the probabilistic setting.
Using this model would require estimating many
parameters which are based on trigrams. This leads
to higher sample complexity to avoid sparse counts:
we would need more samples to accurately estimate
the model.
We therefore consider a more elaborated model,
which tackles both of the above problems. Again,
let b3, b2, b1 ? ? indicate the lexical form of the
three symbols at the top of the stack. We define the
1238
distributions p(t | ?) as follows:
p(shb | b1) = ?shbb1 , ?b ? ? ,
p(la1 | b2, b1) = ?rdb1 ? ?
la1
b2,b1 ,
p(ra1 | b2, b1) = ?rdb1 ? ?
ra1
b2,b1 ,
p(la2 | b3, b2, b1) = ?rdb1 ? ?
rd2
b2,b1 ? ?
la2
b3,b2,b1 ,
p(ra2 | b3, b2, b1) = ?rdb1 ? ?
rd2
b2,b1 ? ?
ra2
b3,b2,b1 .
The parameters above are subject to the following
normalization conditions, for every b3, b2, b1 ? ? :
?
b??
?shbb1 + ?
rd
b1 = 1 , (2)
?la1b2,b1 + ?
ra1
b2,b1 + ?
rd2
b2,b1 = 1 , (3)
?la2b3,b2,b1 + ?
ra2
b3,b2,b1 = 1 . (4)
Intuitively, parameter ?rdb denotes the probability
that we perform a reduce transition instead of a shift
transition, given that we have seen lexical form b at
the top of the stack. Similarly, parameter ?rd2b2,b1 de-notes the probability that we perform a reduce trans-
ition of degree 2 (see ?2.2) instead of a reduce trans-
ition of degree 1, given that we have seen lexical
forms b1 and b2 at the top of the stack.
We observe that the above model has a num-
ber of parameters |? | + 4 ? |? |2 + 2 ? |? |3 (not
all independent). This should be contrasted with
the na??ve model, that has a number of parameters
4 ? |? |3 + |? |4.
4 Tabular parsing
We present here a dynamic programming algorithm
for simulating the computations of the system from
?2?3. Given an input string w, our algorithm pro-
duces a compact representation of the set ? (w),
defined as the set of all possible computations of
the model when processing w. In combination with
the appropriate semirings, this method can provide
for instance the highest probability computation in
? (w), or else the probability of w, defined as the
sum of all probabilities of computations in ? (w).
We follow a standard approach in the literature
on dynamic programming simulation of stack-based
automata (Lang, 1974; Tomita, 1986; Billot and
Lang, 1989). More recently, this approach has also
been applied by Huang and Sagae (2010) and by
??????
c 0
c 1
c m
? h 1
h 1? i
? h 2 h 3
minimum
stack
length in
c 1 , . . . , cm
i i + 1
i + 1
buffer size
stack size
st
ac
k
b
u
ff
er
j
Figure 2: Schematic representation of the computations
? associated with item [h1, i, h2h3, j].
Kuhlmann et al (2011) to the simulation of pro-
jective transition-based parsers. The basic idea in
this approach is to decompose computations of the
parser into smaller parts, group them into equival-
ence classes and recombine to obtain larger parts of
computations.
Let w = a0 ? ? ? an?1, Vw and S(np) be defined as
in ?2. We use a structure called item, defined as
[h1, i, h2h3, j],
where 0 ? i < j ? n and h1, h2, h3 ? Vw must
satisfy h1 < i and i ? h2 < h3 < j. The intended
interpretation of an item can be stated as follows; see
also Figure 2.
? There exists a computation ? of S(np) on w hav-
ing the form c0, . . . , cm, m ? 1, with c0 =
(?|h1, ?i, A) and cm = (?|h2|h3, ?j , A?) for
some stack ? and some arc sets A and A?;
? For each iwith 1 ? i < m, the stack ?i associated
with configuration ci has the list ? at the bottom
and satisfies |?i| ? |?|+ 2.
Some comments on the above conditions are in
order here. Let t1, ? ? ? , tm be the sequence of trans-
itions in T (np) associated with computation ?. Then
we have t1 = shai , since |?1| ? |?| + 2. Thus we
conclude that |?1| = |?|+ 2.
The most important consequence of the definition
of item is that each transition ti with 2 ? i ? m
does not depend on the content of the ? portion of
the stack ?i. To see this, consider transition ci?1 `ti
ci. If ti = shai , the content of ? is irrelevant at
1239
this step, since in our model shai is conditioned only
on the topmost stack symbol of ?i?1, and we have
|?i?1| ? |?|+ 2.
Consider now the case of ti = la2. From |?i| ?
|?| + 2 we have that |?i?1| ? |?| + 3. Again, the
content of ? is irrelevant at this step, since in our
model la2 is conditioned only on the three topmost
stack symbols of ?i?1. A similar argument applies
to the cases of ti ? {ra2, la1, ra1}.
From the above, we conclude that if we apply the
transitions t1, . . . , tm to stacks of the form ?|h1, the
resulting computations have all identical probabilit-
ies, independently of the choice of ?.
Each computation satisfying the two conditions
above will be called an I-computation associ-
ated with item [h1, i, h2h3, j]. Notice that an I-
computation has the overall effect of replacing node
h1 sitting above a stack ? with nodes h2 and h3.
This is the key property in the development of our
algorithm below.
We specify our dynamic programming algorithm
as a deduction system (Shieber et al, 1995). The
deduction system starts with axiom [?, 0, ?0, 1], cor-
responding to an initial stack [?] and to the shift of
a0 = $ from the buffer into the stack. The set ? (w)
is non-empty if and only if item [?, 0, ?0, n] can be
derived using the inference rules specified below.
Each inference rule is annotated with the type of
transition it simulates, along with the arc constructed
by the transition itself, if any.
[h1, i, h2h3, j]
[h3, j, h3j, j + 1]
(shaj )
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h2h5, j]
(la1;h5 ? h4)
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h2h4, j]
(ra1;h4 ? h5)
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h4h5, j]
(la2;h5 ? h2)
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h2h4, j]
(ra2;h2 ? h5)
The above deduction system infers items in a
bottom-up fashion. This means that longer compu-
tations over substrings of w are built by combining
shorter ones. In particular, the inference rule shaj
asserts the existence of I-computations consisting of
a single shaj transition. Such computations are rep-
resented by the consequent item [h3, j, h3j, j + 1],
indicating that the index of the shifted word aj is
added to the stack by pushing it on top of h3.
The remaining four rules implement the reduce
transitions of the model. We have already ob-
served in ?2.2 that all available reduce transitions
shorten the size of the stack by one unit. This al-
lows us to combine pairs of I-computations with
a reduce transition, resulting in a computation that
is again an I-computation. More precisely, if we
concatenate an I-computation asserted by an item
[h1, i, h2h3, k] with an I-computation asserted by an
item [h3, k, h4h5, j], we obtain a computation that
has the overall effect of increasing the size of the
stack by 2, replacing the topmost stack element h1
with stack elements h2, h4 and h5. If we now apply
any of the reduce transitions from the inventory of
the model, we will remove one of these three nodes
from the stack, and the overall result will be again
an I-computation, which can then be asserted by a
certain item. For example, if we apply the reduce
transition la1, the consequent item is [h1, i, h2h5, j],
since an la1 transition removes the second topmost
element from the stack (h4). The other reduce trans-
itions remove a different element, and thus their
rules produce different consequent items.
The above argument shows the soundness of the
deduction system, i.e., an item I = [h1, i, h2h3, j]
is only generated if there exists an I-computation
? = c0, . . . , cm with c0 = (?|h1, ?i, A) and cm =
(?|h2|h3, ?j , A?). To prove completeness, we must
show the converse result, i.e., that the existence of
an I-computation ? implies that item I is inferred.
We first do this under the assumption that the infer-
ence rule for the shift transitions do not have an ante-
cedent, i.e., items [h1, j, h1j, j + 1] are considered
as axioms. We proceed by using strong induction on
the length m of the computation ?.
For m = 1, ? consists of a single transition shaj ,
and the corresponding item I = [h1, j, h1j, j + 1]
is constructed as an axiom. For m > 1, let ? be
as specified above. The transition that produced
1240
cm must have been a reduce transition, otherwise
? would not be an I-computation. Let ck be the
rightmost configuration in c0, . . . , cm?1 whose stack
size is |?| + 2. Then it can be shown that the com-
putations ?1 = c0, . . . , ck and ?2 = ck, . . . , cm?1
are again I-computations. Since ?1 and ?2 have
strictly fewer transitions than ?, by the induction hy-
pothesis, the system constructs items [h1, i, h2h3, k]
and [h3, k, h4h5, j], where h2 and h3 are the stack
elements at the top of ck. Applying to these items
the inference rule corresponding to the reduce trans-
ition at hand, we can construct item I .
When the inference rule for the shift transition has
an antecedent [h1, i, h2h3, j], as indicated above, we
have the overall effect that I-computations consist-
ing of a single transition shifting aj on the top of h3
are simulated only in case there exists a computation
starting with configuration ([?], ?0) and reaching a
configuration of the form (?|h2|h3, ?j). This acts as
a filter on the search space of the algorithm, but does
not invalidate the completeness property. However,
in this case the proof is considerably more involved,
and we do not report it here.
An important property of the deduction system
above, which will be used in the next section, is
that the system is unambiguous, that is, each I-
computation is constructed by the system in a
unique way. This can be seen by observing that, in
the sketch of the completeness proof reported above,
there always is an unique choice of ck that decom-
poses I-computation ? into I-computations ?1 and
?2. In fact, if we choose a configuration ck? other
than ck with stack size |?| + 2, the computation
??2 = ck? , . . . , cm?1 will contain ck as an interme-
diate configuration, which violates the definition of
I-computation because of an intervening stack hav-
ing size not larger than the size of the stack associ-
ated with the initial configuration.
As a final remark, we observe that we can keep
track of all inference rules that have been applied
in the computation of each item by the above al-
gorithm, by encoding each application of a rule as
a reference to the pair of items that were taken as
antecedent in the inference. In this way, we ob-
tain a parse forest structure that can be viewed as a
hypergraph or as a non-recursive context-free gram-
mar, similar to the case of parsing based on context-
free grammars. See for instance Klein and Manning
(2001) or Nederhof (2003). Such a parse forest en-
codes all valid computations in ? (w), as desired.
The algorithm runs in O(n8) time. Using meth-
ods similar to those specified in Eisner and Satta
(1999), we can reduce the running time to O(n7).
However, we do not further pursue this idea here,
and proceed with the discussion of exact inference,
found in the next section.
5 Inference
We turn next to specify exact inference with our
model, for computing feature expectations. Such
inference enables, for example, the derivation of
an expectation-maximization algorithm for unsuper-
vised parsing.
Here, a feature is a function over computations,
providing the count of a pattern related to a para-
meter. We denote by f la2b3,b2,b1(?), for instance,the number of occurrences of transition la2 within
? with topmost stack symbols having word forms
b3, b2, b1 ? ? , with b1 associated with the topmost
stack symbol.
Feature expectations are computed by using an
inside-outside algorithm for the items in the tabu-
lar algorithm. More specifically, given a string w,
we associate each item [h1, i, h2h3, j] defined as in
?4 with two quantities:
I([h1, i, h2h3, j]) =
?
?=([h1],?i),...,([h2,h3],?j)
p(?) ; (5)
O([h1, i, h2h3, j]) =
?
?,?=([?],?0),...,(?|h1,?i)
??=(?|h2|h3,?j),...,([?,0],?n)
p(?) ? p(??) . (6)
I([h1, i, h2h3, j]) and O([h1, i, h2h3, j]) are called
the inside and the outside probabilities, respect-
ively, of item [h1, i, h2h3, j]. The tabular algorithm
of ?4 can be used to compute the inside probabilit-
ies. Using the gradient transformation (Eisner et al,
2005), a technique for deriving outside probabilities
from a set of inference rules, we can also compute
O([h1, i, h2h3, j]). The use of the gradient trans-
formation is valid in our case because the tabular al-
gorithm is unambiguous (see ?4).
Using the inside and outside probabilities, we can
now efficiently compute feature expectations for our
1241
Ep(?|w)[f la2b3,b2,b1(?)] =
?
??? (w)
p(? | w) ? f la2b3,b2,b1(?) =
1
p(w) ?
?
??? (w)
p(?) ? f la2b3,b2,b1(?)
= 1p(w) ?
?
?,i,k,j,
h1,h2,h3,h4,h5,
s.t. ah2=b3,
ah4=b2, ah5=b1
?
?0=([?],?0),...,(?|h1,?i),
?1=(?|h1,?i),...,(?|h2|h3,?k),
?2=(?|h2|h3,?k),...,(?|h2|h4|h5,?j),
?3=(?|h2|h5,?j),...,([?,0],?n)
p(?0) ? p(?1) ? p(?2) ? p(la2 | b3, b2, b1) ? p(?3)
=
?rdb1 ? ?
rd2
b2,b1 ? ?
la2
b3,b2,b1
p(w) ?
?
?,i,j,
h1,h2,h5, s.t.
ah2=b3, ah5=b1
?
?0=([?],?0),...,(?|h1,?i),
?3=(?|h2|h5,?j),...,([?,0],?n)
p(?0) ? p(?3) ?
?
?
k,h3,h4,
s.t. ah4=b2
?
?1=(?|h1,?i),...,(?|h2|h3,?k)
p(?1) ?
?
?2=(?|h2|h3,?k),...,(?|h2|h4|h5,?j)
p(?2)
Figure 3: Decomposition of the feature expectationEp(?|w)[f la2b3,b2,b1(?)] into a finite summation. Quantity p(w) aboveis the sum over all probabilities of computations in ? (w).
model. Figure 3 shows how to express the expect-
ation of feature f la2b3,b2,b1(?) by means of a finitesummation. Using Eq. 5 and 6 and the relation
p(w) = I([?, 0, ?0, n]) we can then write:
Ep(?|w)[f la2b3,b2,b1(?)] =
?rdb1 ? ?
rd2
b2,b1 ? ?
la2
b3,b2,b1
I([?, 0, ?0, n]) ?
?
?
i,j,h1,h4,h5,
s.t. ah4=b2, ah5=b1
O([h1, i, h4h5, j]) ?
?
?
k,h2,h3,
s.t. ah2=b3
I([h1, i, h2h3, k]) ? I([h3, k, h4h5, j]) .
Very similar expressions can be derived for the ex-
pectations for features f ra2b3,b2,b1(?), f la1b2,b1(?), and
f ra1b2,b1(?). As for feature f shbb1 (?), b ? ? , the aboveapproach leads to
Ep(?|w)[f shbb1 (?)] =
=
?shbb1
I([?, 0, ?0, n]) ?
?
?,i,h, s.t.
ah=b1, ai=b
O([h, i, hi, i+ 1]) .
As mentioned above, these expectations can be
used, for example, to derive an EM algorithm for our
model. The EM algorithm in our case is not com-
pletely straightforward because of the way we para-
metrize the model. We give now the re-estimation
steps for such an EM algorithm. We assume that all
expectations below are taken with respect to a set of
parameters ? from iteration s ? 1 of the algorithm,
and we are required to update these ?. To simplify
notation, let us assume that there is only one stringw
in the training corpus. For each b1 ? ? , we define:
Zb1 =
?
b2??
Ep(?|w)
[
f la1b2,b1(?) + f
ra1
b2,b1(?)
]
+
?
b3,b2??
Ep(?|w)
[
f la2b3,b2,b1(?) + f
ra2
b3,b2,b1(?)
]
;
Zb2,b1 =
?
b3??
Ep(?|w)
[
f la2b3,b2,b1(?) + f
ra2
b3,b2,b1(?)
]
.
We then have, for every b ? ? :
?shbb1 (s)?
Ep(?|w)[f shbb1 (?)]
Zb1 +
?
b??? Ep(?|w)[f
shb?
b1 (?)]
.
1242
Furthermore, we have:
?la1b2,b1(s)?
Ep(?|w)[f la1b2,b1(?)]
Zb2,b1 + Ep(?|w)
[
f la1b2,b1(?) + f
ra1
b2,b1(?)
] ,
and:
?la2b3,b2,b1(s)?
Ep(?|w)[f la2b3,b2,b1(?)]
Ep(?|w)
[
f la2b3,b2,b1(?) + f
ra2
b3,b2,b1(?)
] .
The rest of the parameter updates can easily be de-
rived using the above updates because of the sum-
to-1 constraints in Eq. 2?4.
6 Discussion
We note that our model inherits spurious ambigu-
ity from Attardi?s model. More specifically, we can
have different derivations, corresponding to differ-
ent system computations, that result in identical de-
pendency graphs and strings. While running our
tabular algorithm with the Viterbi semiring effi-
ciently computes the highest probability computa-
tion in ? (w), spurious ambiguity means that find-
ing the highest probability dependency tree is NP-
hard. This latter result can be shown using proof
techniques similar to those developed by Sima?an
(1996). We leave it for future work how to eliminate
spurious ambiguity from the model.
While in the previous sections we have described
a tabular method for the transition system of Attardi
(2006) restricted to transitions of degree up to two, it
is possible to generalize the model to include higher-
degree transitions. In the general formulation of At-
tardi parser, transitions of degree d create links in-
volving nodes located d positions beneath the top-
most position in the stack:
lad : (?|i1|i2| . . . |id+1, ?, A) `
(?|i2| . . . |id+1, ?, A ? {id+1 ? i1});
rad : (?|i1|i2| . . . |id+1, ?, A) `
(?|i1|i2| . . . |id, ?, A ? {i1 ? id+1}).
To define a transition system that supports trans-
itions up to degree D, we use a set of
items of the form [s1 . . . sD?1, i, e1 . . . eD, j], cor-
responding (in the sense of ?4) to compu-
tations of the form c0, . . . , cm, m ? 1,
with c0 = (?|s1| . . . |sD?1, ?i, A) and cm =
(?|e1| . . . |eD, ?j , A?). The deduction steps corres-
ponding to reduce transitions in this general system
have the general form
[s1 . . . sD?1, i, e1m1 . . .mD?1, j]
[m1 . . .mD?1, j, e2 . . . eD+1, w]
[s1 . . . sD?1, i, e1 . . . ec?1ec+1 . . . eD+1, w]
(ep ? ec)
where the values of p and c differ for each transition:
to obtain the inference rule corresponding to a lad
transition, we make p = D + 1 and c = D + 1? d;
and to obtain the rule for a rad transition, we make
p = D + 1? d and c = D + 1. Note that the parser
runs in timeO(n3D+2), whereD stands for the max-
imum transition degree, so each unit increase in the
transition degree adds a cubic factor to the parser?s
polynomial time complexity. This is in contrast to a
previous tabular formulation of the Attardi parser by
Go?mez-Rodr??guez et al (2011), which ran in expo-
nential time.
The model for the transition system we give in this
paper is generative. It is not hard to naturally extend
this model to the discriminative setting. In this case,
we would condition the model on the input string to
get a conditional distribution over derivations. It is
perhaps more natural in this setting to use arbitrary
weights for the parameter values, since the compu-
tation of a normalization constant (the probability of
a string) is required in any case. Arbitrary weights
in the generative setting could be more problematic,
because it would require computing a normalization
constant corresponding to a sum over all strings and
derivations.
7 Conclusion
We presented in this paper a generative probabilistic
model for non-projective parsing, together with the
description of an efficient tabular algorithm for pars-
ing and doing statistical inference with the model.
Acknowledgments
The authors thank Marco Kuhlmann for helpful
comments on an early draft of the paper. The authors
also thank Giuseppe Attardi for the help received to
extract the parsing statistics. The second author has
been partially supported by Ministerio de Ciencia e
Innovacio?n and FEDER (TIN2010-18552-C03-02).
1243
References
Giuseppe Attardi. 2006. Experiments with a multil-
anguage non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL), pages 166?170,
New York, USA.
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 143?151,
Vancouver, Canada.
Manuel Bodirsky, Marco Kuhlmann, and Mathias Mo?hl.
2005. Well-nested drawings as models of syntactic
structure. In Tenth Conference on Formal Gram-
mar and Ninth Meeting on Mathematics of Language,
pages 195?203, Edinburgh, UK.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning (CoNLL), pages
149?164, New York, USA.
Jason Eisner and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and Head Auto-
maton Grammars. In Proceedings of the 37th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 457?464, College Park, MD,
USA.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005.
Compiling Comp Ling: Practical weighted dynamic
programming and the Dyna language. In Proceedings
of HLT-EMNLP, pages 281?290.
Carlos Go?mez-Rodr??guez and Joakim Nivre. 2010. A
transition-based parser for 2-planar dependency struc-
tures. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1492?1501, Uppsala, Sweden.
Carlos Go?mez-Rodr??guez, David J. Weir, and John Car-
roll. 2009. Parsing mildly non-projective dependency
structures. In Twelfth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 291?299, Athens, Greece.
Carlos Go?mez-Rodr??guez, John Carroll, and David Weir.
2011. Dependency parsing schemata and mildly non-
projective dependency parsing. Computational Lin-
guistics (in press), 37(3).
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull-
man. 2000. Introduction to Automata Theory.
Addison-Wesley, 2nd edition.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 1077?1086,
Uppsala, Sweden.
Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In 36th Annual
Meeting of the Association for Computational Lin-
guistics and 18th International Conference on Compu-
tational Linguistics (COLING-ACL), pages 646?652,
Montre?al, Canada.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the IWPT, pages
123?134.
Terry Koo, Amir Globerson, Xavier Carreras, and Mi-
chael Collins. 2007. Structured prediction models
via the matrix-tree theorem. In Proceedings of the
EMNLP-CoNLL, pages 141?150.
Marco Kuhlmann and Giorgio Satta. 2009. Tree-
bank grammar techniques for non-projective depend-
ency parsing. In Twelfth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 478?486, Athens, Greece.
Marco Kuhlmann, Carlos Go?mez-Rodr??guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), Portland, Oregon,
USA.
Bernard Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In Jacques Loecx,
editor, Automata, Languages and Programming, 2nd
Colloquium, University of Saarbru?cken, July 29?
August 2, 1974, number 14 in Lecture Notes in Com-
puter Science, pages 255?269. Springer.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 40?51, Singa-
pore.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In Tenth International Conference on Parsing
Technologies (IWPT), pages 121?132, Prague, Czech
Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Human Language
Technology Conference (HLT) and Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 523?530, Vancouver, Canada.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and knuth?s algorithm. Computational Linguistics,
29(1):135?143.
1244
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In 43rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 99?106, Ann Arbor, USA.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In Workshop on Incremental Pars-
ing: Bringing Engineering and Cognition Together,
pages 50?57, Barcelona, Spain.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguist-
ics, 34(4):513?553.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the 47th An-
nual Meeting of the ACL and the Fourth International
Joint Conference on Natural Language Processing of
the AFNLP, pages 351?359, Singapore.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24(1?2):3?
36.
Khalil Sima?an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In Proceedings of COLING, pages 1175?
1180.
David A. Smith and Noah A. Smith. 2007. Probab-
ilistic models of nonprojective dependency trees. In
Proceedings of the EMNLP-CoNLL, pages 132?140.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proceedings of IJCAI, pages 281?290.
Masaru Tomita. 1986. Efficient Parsing for Natural
Language: A Fast Algorithm for Practical Systems.
Springer.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of the Eighth International Workshop on
Parsing Technologies (IWPT), pages 195?206, Nancy,
France.
1245
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 917?927,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
A Polynomial-Time Dynamic Oracle
for Non-Projective Dependency Parsing
Carlos G
?
omez-Rodr??guez
Departamento de
Computaci?on
Universidade da Coru?na, Spain
cgomezr@udc.es
Francesco Sartorio
Department of
Information Engineering
University of Padua, Italy
sartorio@dei.unipd.it
Giorgio Satta
Department of
Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
The introduction of dynamic oracles has
considerably improved the accuracy of
greedy transition-based dependency pars-
ers, without sacrificing parsing efficiency.
However, this enhancement is limited to
projective parsing, and dynamic oracles
have not yet been implemented for pars-
ers supporting non-projectivity. In this
paper we introduce the first such oracle,
for a non-projective parser based on At-
tardi?s parser. We show that training with
this oracle improves parsing accuracy over
a conventional (static) oracle on a wide
range of datasets.
1 Introduction
Greedy transition-based parsers for dependency
grammars have been pioneered by Yamada and
Matsumoto (2003) and Nivre (2003). These meth-
ods incrementally process the input sentence from
left to right, predicting the next parsing action,
called transition, on the basis of a compact rep-
resentation of the derivation history.
Greedy transition-based parsers can be very
efficient, allowing web-scale parsing with high
throughput. However, the accuracy of these meth-
ods still falls behind that of transition-based pars-
ers using beam-search, where the accuracy im-
provement is obtained at the cost of a decrease
in parsing efficiency; see for instance Zhang and
Nivre (2011), Huang and Sagae (2010), Choi and
McCallum (2013). As an alternative to beam-
search, recent research on transition-based parsing
has therefore explored possible ways of improving
accuracy at no extra cost in parsing efficiency.
The training of transition-based parsers relies
on a component called the parsing oracle, which
maps parser configurations to optimal transitions
with respect to a gold tree. A discriminative model
is then trained to simulate the oracle?s behavior,
and is later used for decoding. Traditionally, so-
called static oracles have been exploited in train-
ing, where a static oracle is defined only for con-
figurations that have been reached by computa-
tions with no mistake, and it returns a single ca-
nonical transition among those that are optimal.
Very recently, Goldberg and Nivre (2012),
Goldberg and Nivre (2013) and Goldberg et al.
(2014) showed that the accuracy of transition-
based parsers can be substantially improved using
dynamic oracles. A dynamic oracle returns the
set of all transitions that are optimal for a given
configuration, with respect to the gold tree, and
is well-defined and correct for every configuration
that is reachable by the parser.
Na??ve implementations of dynamic oracles run
in exponential time, since they need to simulate
all possible computations of the parser for the in-
put configuration. Polynomial-time implementa-
tions of dynamic oracles have been proposed by
the above mentioned authors for several project-
ive dependency parsers. To our knowledge, no
polynomial-time algorithm has been published for
transition-based parsers based on non-projective
dependency grammars.
In this paper we consider a restriction of a
transition-based, non-projective parser originally
presented by Attardi (2006). This restriction
was further investigated by Kuhlmann and Nivre
(2010) and Cohen et al. (2011). We provide an im-
plementation for a dynamic oracle for this parser
running in polynomial time.
We experimentally compare the parser trained
with the dynamic oracle to a baseline obtained
by training with a static oracle. Significant ac-
curacy improvements are achieved on many lan-
guages when using our dynamic oracle. To our
knowledge, these are the first experimental results
on non-projective parsing based on a dynamic or-
acle.
917
2 Preliminary Definitions
Transition-based dependency parsing was origin-
ally introduced by Yamada and Matsumoto (2003)
and Nivre (2003). In this section we briefly sum-
marize the notation we use for this framework and
introduce the notion of dynamic oracle.
2.1 Transition-Based Dependency Parsing
We represent an input sentence as a string w =
w
0
? ? ?w
n
, n ? 1, where each w
i
with i 6= 0 is a
lexical symbol and w
0
is a special symbol called
root. Set V
w
= {i | 0 ? i ? n} denotes the sym-
bol occurrences in w. For i, j ? V
w
with i 6= j,
we write i ? j to denote a grammatical depend-
ency of some unspecified type betweenw
i
andw
j
,
where w
i
is the head and w
j
is the dependent.
A dependency tree t for w is a directed tree
with node set V
w
and with root node 0. An arc of t
is a pair (i, j), encoding a dependency i ? j; we
will often use the latter notation to denote arcs.
A transition-based dependency parser typically
uses a stack data structure to process the input
string from left to right, in a way very similar
to the classical push-down automaton for context-
free languages (Hopcroft et al., 2006). Each stack
element is a node from V
w
, representing the root
of a dependency tree spanning some portion of the
input w, and no internal state is used. At each step
the parser applies some transition that updates the
stack and/or consumes one symbol from the input.
Transitions may also construct new dependencies,
which are added to the current configuration of the
parser.
We represent the stack as an ordered sequence
? = [h
d
, . . . , h
1
], d ? 0, of nodes h
i
? V
w
, with
the topmost element placed at the right. When
d = 0, we have the empty stack ? = []. We use
the vertical bar to denote the append operator for
?, and write ? = ?
?
|h
1
to indicate that h
1
is the
topmost element of ?.
The portion of the input string still to be pro-
cessed by the parser is called the buffer. We
represent the buffer as an ordered sequence ? =
[i, . . . , n] of nodes from V
w
, with i the first ele-
ment of the buffer. We denote the empty buffer
as ? = []. Again, we use the vertical bar to de-
note the append operator, and write ? = i|?
?
to
indicate that i is the first symbol occurrence of ?;
consequently, we have ?
?
= [i+ 1, . . . , n].
In a transition-based parser, the parsing pro-
cess is defined through the technical notions of
configuration and transition. A configuration of
the parser relative to w is a triple c = (?, ?,A),
where ? and ? are a stack and a buffer, respect-
ively, and A is the set of arcs that have been built
so far. A transition is a partial function map-
ping the set of parser configurations into itself.
Each transition-based parser is defined by means
of some finite inventory of transitions. We will
later introduce the specific inventory of transitions
for the parser that we investigate in this paper.
We use the symbol ` to denote the binary relation
formed by the union of all transitions of a parser.
With the notions of configuration and transition
in place, we can define a computation of the
parser on w as a sequence c
0
, c
1
, . . . , c
m
, m ? 0,
of configurations relative tow, under the condition
that c
i?1
` c
i
for each i with 1 ? i ? m. We use
the reflexive and transitive closure of `, written
`
?
, to represent computations.
2.2 Configuration Loss and Dynamic Oracles
A transition-based dependency parser is a non-
deterministic device, meaning that a given con-
figuration can be mapped into several configur-
ations by the available transitions. However, in
several implementations the parser is associated
with a discriminative model that, on the basis of
some features of the current configuration, always
chooses a single transition. In other words, the
model is used to run the parser as a pseudo-de-
terministic device. The training of the discriminat-
ive model relies on a component called the parsing
oracle, which maps parser configurations to ?op-
timal? transitions with respect to some reference
dependency tree, which we call the gold tree.
Traditionally, so-called static oracles have been
used which return a single, canonical transition
and they do so only for configurations that can
reach the gold tree, that is, configurations repres-
enting parsing histories with no mistake. In re-
cent work, Goldberg and Nivre (2012), Goldberg
and Nivre (2013) and Goldberg et al. (2014) have
introduced dynamic oracles, which return the set
of all transitions that are optimal with respect to
a gold tree, and are well-defined and correct for
every configuration that is reachable by the parser.
These authors have shown that the accuracy of
transition-based dependency parsers can be sub-
stantially improved if dynamic oracles are used in
place of static ones. In what follows, we provide
a mathematical definition of dynamic oracles, fol-
lowing Goldberg et al. (2014).
918
(?, k|?,A) `
sh
(?|k, ?,A)
(?|i|j, ?,A) `
la
(?|j, ?,A ? {j ? i})
(?|i|j, ?,A) `
ra
(?|i, ?, A ? {i? j})
(?|i|j|k, ?,A) `
la
2
(?|j|k, ?,A ? {k ? i})
(?|i|j|k, ?,A) `
ra
2
(?|i|j, ?,A ? {i? k})
Figure 1: Transitions of the non-projective parser.
Let t
1
and t
2
be dependency trees for w, with
arc sets A
1
and A
2
, respectively. The loss of t
1
with respect to t
2
is defined as
L(t
1
, t
2
) = |A
1
\A
2
| . (1)
Note that L(t
1
, t
2
) = L(t
2
, t
1
), since |A
1
| = |A
2
|.
Furthermore L(t
1
, t
2
) = 0 if and only if t
1
and t
2
are the same tree.
Let c be a configuration of a transition-based
parser relative to w. Let also D(c) be the set of all
dependency trees that can be obtained in a com-
putation of the form c `
?
c
f
, where c
f
is a final
configuration, that is, a configuration that has con-
structed a dependency tree for w. We extend the
loss function in (1) to configurations by letting
L(c, t
2
) = min
t
1
?D(c)
L(t
1
, t
2
) . (2)
Let t
G
be the gold tree for w. Quantity L(c, t
G
)
can be used to define a dynamic oracle as follows.
For any transition `
?
in the finite inventory of our
parser, we use the functional notation ?(c) = c
?
in
place of c `
?
c
?
. We then let
oracle(c, t
G
) =
{? | L(?(c), t
G
)? L(c, t
G
) = 0} . (3)
In words, (3) provides the set of transitions that do
not increase the loss of c; we call these transitions
optimal for c.
A na??ve way of implementing (3) would be
to explicitly compute the set D(c) in (2), which
has exponential size. More interestingly, the im-
plementation of dynamic oracles proposed by the
above cited authors all run in polynomial time.
These oracles are all defined for projective pars-
ing. In this paper, we present a polynomial-time
oracle for a non-projective parser.
3 Non-Projective Dependency Parsing
In this section we introduce a parser for non-
projective dependency grammars that is derived
from the transition-based parser originally presen-
ted by Attardi (2006), and was further investigated
by Kuhlmann and Nivre (2010) and Cohen et al.
(2011). Our definitions follow the framework in-
troduced in Section 2.1.
We start with some additional notation. Let t be
a dependency tree for w and let k be a node of t.
Consider the complete subtree t
?
of t rooted at k,
that is, the subtree of t induced by k and all of the
descendants of k in t. The span of t
?
is the sub-
sequence of tokens in w represented by the nodes
of t
?
. Node k has gap-degree 0 if the span of t
?
forms a (contiguous) substring of w. A depend-
ency tree is called projective if all of its nodes
have gap-degree 0; a dependency tree which is not
projective is called non-projective.
Given w as input, the parser starts with the ini-
tial configuration ([], [0, . . . , n], ?), consisting of
an empty stack, a buffer with all the nodes repres-
enting the symbol occurrences in w, and an empty
set of constructed dependencies (arcs). The parser
stops when it reaches a final configuration of the
form ([0], [], A), consisting of a stack with only the
root node and of an empty buffer; in any such con-
figuration, set A always implicitly defines a valid
dependency tree (rooted in node 0).
The core of the parser consists of an invent-
ory of five transitions, defined in Figure 1. Each
transition is specified using the free variables ?,
?, A, i, j and k. As an example, the schema
(?|i|j, ?,A) `
la
(?|j, ?,A? {j ? i}) means that
if a configuration c matches the antecedent, then a
new configuration is obtained by instantiating the
variables in the consequent accordingly.
The transition `
sh
, called shift, reads a new
token from the input sentence by removing it from
the buffer and pushing it into the stack. Each
of the other transitions, collectively called reduce
transitions, has the effect of building a dependency
between two nodes in the stack, and then removing
the dependent node from the stack. The removal
of the dependent ensures that the output depend-
ency tree is built in a bottom-up order, collecting
all of the dependents of each node i before linking
i to its head.
The transition `
la
, called left-arc, creates a left-
ward arc where the topmost stack node is the
head and the second topmost node is the depend-
ent, and removes the latter from the stack. The
transition `
ra
, called right-arc, is defined sym-
metrically, so that the topmost stack node is at-
919
? ? ?
h h h
h
1 1 2
3minimum
stack length
at c     c1
c c c0 1
stack length
...
...
m
m
Figure 2: General form of the computations asso-
ciated with an item [h
1
, h
2
, h
3
].
tached as a dependent of the second topmost node.
The combination of the shift, left-arc and right-
arc transitions provides complete coverage of pro-
jective dependency trees, but no support for non-
projectivity, and corresponds to the so-called arc-
standard parser introduced by Nivre (2004).
Support for non-projective dependencies is
achieved by adding the transitions `
la
2
and `
ra
2
,
which are variants of the left-arc and right-arc
transitions, respectively. These new transitions
create dependencies involving the first and the
third topmost nodes in the stack. The creation of
dependencies between non-adjacent stack nodes
might produce crossing arcs and is the key to the
construction of non-projective trees.
Recall that transitions are partial functions,
meaning that they might be undefined for some
configurations. Specifically, the shift transition is
only defined for configurations with a non-empty
buffer. Similarly, the left-arc and right-arc trans-
itions can only be applied if the length of the stack
is at least 2, while the transitions `
la
2
and `
ra
2
re-
quire at least 3 nodes in the stack.
Transitions `
la
2
and `
ra
2
were originally intro-
duced by Attardi (2006) together with other, more
complex transitions. The parser we define here
is therefore more restrictive than Attardi (2006),
meaning that it does not cover all the non-pro-
jective trees that can be processed by the ori-
ginal parser. However, the restricted parser has re-
cently attracted some research interest, as it covers
the vast majority of non-projective constructions
appearing in standard treebanks (Attardi, 2006;
Kuhlmann and Nivre, 2010), while keeping sim-
plicity and interesting properties like being com-
patible with polynomial-time dynamic program-
ming (Cohen et al., 2011).
4 Representation of Computations
Our oracle algorithm exploits a dynamic program-
ming technique which, given an input string, com-
bines certain pieces of a computation of the parser
from Section 3 to obtain larger pieces. In order
to efficiently encode pieces of computations, we
borrow a representation proposed by Cohen et al.
(2011), which is introduced in this section.
Let w = a
0
? ? ? a
n
and V
w
be specified as in
Section 2, and let w
?
be some substring of w. (The
specification of w
?
is not of our concern in this
section.) Let also h
1
, h
2
, h
3
? V
w
. We are inter-
ested in computations of the parser processing the
substring w
?
and having the form c
0
, c
1
, . . . , c
m
,
m ? 1, that satisfy both of the following condi-
tions, exemplified in Figure 2.
? For some sequence of nodes ? with |?| ? 0,
the stack associated with c
0
has the form ?|h
1
and the stack associated with c
m
has the form
?|h
2
|h
3
.
? For each intermediate configuration c
i
, 1 ?
i ? m ? 1, the stack associated with c
i
has
the form ??
i
, where ?
i
is a sequence of nodes
with |?
i
| ? 2.
An important property of the above definition
needs to be discussed here, which is at the heart of
the polynomial-time algorithm in the next section.
If in c
0
, c
1
, . . . , c
m
we replace ? with a different
sequence ?
?
, we obtain a valid computation for w
?
constructing exactly the same dependencies as the
original computation. To see this, let c
i?1
`
?
i
c
i
for each i with 1 ? i ? m. Then `
?
1
must be a
shift, otherwise |?
1
| ? 2 would be violated. Con-
sider now a transition `
?
i
with 2 ? i ? m that
builds some dependency. From |?
i
| ? 2 we derive
|?
i?1
| ? 3. We can easily check from Figure 1
that none of the nodes in ? can be involved in the
constructed dependency.
Intuitively, the above property asserts that the
sequence of transitions `
?
1
,`
?
2
, . . . ,`
?
m
can be
applied to parse substring w
?
independently of the
context ?. This suggests that we can group into
an equivalence class all the computations satisfy-
ing the conditions above, for different values of
?. We indicate such class by means of the tuple
[h
1
, h
2
h
3
], called item. It is easy to see that each
item represents an exponential number of compu-
tations. In the next section we will show how we
can process items with the purpose of obtaining an
efficient computation for dynamic oracles.
920
5 Dynamic Oracle Algorithm
Our algorithm takes as input a gold tree t
G
for
string w and a parser configuration c = (?, ?,A)
relative to w, specified as in Section 2. We assume
that t
G
can be parsed by the non-projective parser
of Section 3 starting from the initial configuration.
5.1 Basic Idea
The algorithm consists of two separate stages, in-
formally discussed in what follows. In the first
stage we identify some tree fragments of t
G
that
can be constructed by the parser after reaching
configuration c, in a way that does not depend on
the content of ?. This means that these fragments
can be precomputed by looking only into ?. Fur-
thermore, since these fragments are subtrees of t
G
,
their computation has no effect on the overall loss
of a computation on w.
For each fragment t with the above properties,
we replace all the nodes in ? that are also nodes
of t with the root node of t itself. The result of the
first stage is therefore a new node sequence shorter
than ?, which we call the reduced buffer ?
R
.
In the second stage of the algorithm we use a
variant of the tabular method developed by Co-
hen et al. (2011), which was originally designed
to simulate all computations of the parser in Sec-
tion 3 on an input string w. We run the above
method on the concatenation of the stack and the
reduced buffer, with some additional constraints
that restrict the search space in two respects. First,
we visit only those computations of the parser
that step through configuration c. Second, we
reach only those dependency trees that contain all
the tree fragments precomputed in the first stage.
We can show that such search space always con-
tains at least one dependency tree with the desired
loss, which we then retrieve performing a Viterbi
search.
5.2 Preprocessing of the Buffer
Let t be a complete subtree of t
G
, having root
node k in ?. Consider the following two condi-
tions, defined on t.
? Bottom-up completeness: No arc i ? j in t
is such that i is a node in ?, i 6= k, and j is a
node in ?.
? Zero gap-degree: The nodes of t that are in ?
form a (contiguous) substring of w.
We claim that if t satisfies the above conditions,
then we can safely reduce the nodes of t appearing
in ?, replacing them with node k. We only report
here an informal discussion of this claim, and omit
a formal proof.
As a first remark, recall that our parser imple-
ments a purely bottom-up strategy. This means
that after a tree has been constructed, all of its
nodes but the root are removed from the parser
configuration. Then the Bottom-up completeness
condition guarantees that if we remove from ? all
nodes of t but k, the nodes of t that are in ? can still
be processed in a way that does not affect the loss,
since their parent must be either k or a node that is
neither in ? nor in ?. Note that the nodes of t that
are neither in ? nor in ? are irrelevant to the pre-
computation of t from ?, since these nodes have
already been attached and are no longer available
to the parser.
As a second remark, the Zero gap-degree con-
dition guarantees that the span of t over the nodes
of ? is not interleaved by nodes that do not belong
to t. This is also an important requirement for the
precomputation of t from ?, since a tree fragment
having a discontinuous span over ? might not be
constructable independently of ?. More specific-
ally, parsing such fragment implies dealing with
the nodes in the discontinuities, and this might re-
quire transitions involving nodes from ?.
We can now use the sufficient condition above
to compute ?
R
. We process ? from left to right.
For each node k, we can easily test the Bottom-up
completeness condition and the Zero gap-degree
condition for the complete subtree t of t
G
rooted
at k, and perform the reduction if both conditions
are satisfied. Note that in this process a node k
resulting from the reduction of t might in turn be
removed from ? if, at some later point, we reduce
a supertree of t.
5.3 Computation of the Loss
We describe here our dynamic programming al-
gorithm for the computation of the loss of an in-
put configuration c. We start with some additional
notation. Let ? = ??
R
be the concatenation of ?
and ?
R
, which we treat as a string of nodes. For
integers i with 0 ? i ? |?| ? 1, we write ?[i] to
denote the (i + 1)-th node of ?. Let also ` = |?|.
Symbol ` is used to mark the boundary between
the stack and the reduced buffer in ?, thus ?[i] with
i < ` is a node of ?, while ?[i] with i ? ` is a node
of ?
R
.
Algorithm 1 computes the loss of c by pro-
cessing the sequence ? in a way quite similar to the
921
standard nested loop implementation of the CKY
parser for context-free grammars (Hopcroft et al.,
2006). The algorithm uses a two-dimensional ar-
ray T whose indexes range from 0 to |?| = ` +
|?
R
|, and only the cells T [i, j] with i < j are
filled.
We view each T [i, j] as an association list
whose keys are items [h
1
, h
2
h
3
], defined in the
context of the substring ?[i] ? ? ? ?[j ? 1] of ?; see
Section 4. The value stored at T [i, j]([h
1
, h
2
h
3
])
is the minimum loss contribution due to the com-
putations represented by [h
1
, h
2
h
3
]. For technical
reasons, we assume that our parser starts with a
symbol $ 6? V
w
in the stack, denoting the bottom
of the stack.
We initialize the table by populating the cells
of the form T [i, i + 1] with information about
the trivial computations consisting of a single `
sh
transition that shifts the node ?[i] into the stack.
These computations are known to have zero loss
contribution, because a `
sh
transition does not cre-
ate any arcs. In the case where the node ?[i] be-
longs to ?, i.e., i < `, we assign loss contribution
0 to the entry T [i, i + 1]([?[i? 1], ?[i? 1]?[i]])
(line 3 of Algorithm 1), because ?[i] is shifted with
?[i? 1] at the top of the stack. On the other hand,
if ?[i] is in ?, i.e., i ? `, we assign loss contri-
bution 0 to several entries in T [i, i + 1] (line 6)
because, at the time ?[i] is shifted, the content of
the stack depends on the transitions executed be-
fore that point.
After the above initialization, we consider
pairs of contiguous substrings ?[i] ? ? ? ?[k ? 1] and
?[k] ? ? ? ?[j ? 1] of ?. At each inner iteration
of the nested loops of lines 7-11 we update cell
T [i, j] based on the content of the cells T [i, k] and
T [k, j]. We do this through the procedure PRO-
CESSCELL(T , i, k, j), which considers all pairs
of keys [h
1
, h
2
h
3
] in T [i, k] and [h
3
, h
4
h
5
] in
T [k, j]. Note that we require the index h
3
to match
between both items, meaning that their computa-
tions can be concatenated. In this way, for each
reduce transition ? in our parser, we compute the
loss contribution for a new piece of computation
defined by concatenating a computation with min-
imum loss contribution in the first item and a com-
putation with minimum loss contribution in the
second item, followed by the transition ? . The fact
that the new piece of computation can be repres-
ented by an item is exemplified in Figure 3 for the
case ? = `
ra
2
.
?
h1
c0
?
h
h
2
3
c
?
h
h
2
3
c
?
h
h
2
c
h5
4
?
h
h
4
5
c +1
la  : create arc
h     h  and remove
h   from stack
2
5 2
2
[h , h h ]1 2 3 la 2?
?
h1
c0
?
h
h
4
5
c +1
... ...
+ +[h , h h ]3 4 5
[h , h h ]1 4 5
...
?
m m r r
r
Figure 3: Concatenation of two computa-
tions/items and transition `
ra
2
, resulting in a new
computation/item.
The computed loss contribution is used to up-
date the entry in T [i, j] corresponding to the item
associated with the new computation. Observe
how the loss contribution provided by the arc cre-
ated by ? is computed by the ?
G
function at lines
17, 20, 23 and 26, which is defined as:
?
G
(i? j) =
{
0, if i? j is in t
G
;
1, otherwise.
(4)
We remark that the nature of our problem al-
lows us to apply several shortcuts and optimiza-
tions that would not be possible in a setting where
we actually needed to parse the string ?. First, the
range of variable i in the loop in line 8 starts at
max{0, `?d}, rather than at 0, because we do not
need to combine pairs of items originating from
nodes in ? below the topmost node, as the items
resulting from such combinations correspond to
computations that do not contain our input config-
uration c. Second, when we have set values for i
such that i+2 < `, we can omit calling PROCESS-
CELL for values of the parameter k ranging from
i+2 to `?1, as those calls would use as their input
one of the items described above, which are not of
interest. Finally, when processing substrings that
are entirely in ?
R
(i ? `) we can restrict the trans-
itions that we explore to those that generate arcs
that either are in the gold tree t
G
, or have a parent
node which is not present in ? (see conditions in
922
Algorithm 1 Computation of the loss function
1: T [0, 1]([$, $0])? 0 . shift node 0 on top of empty stack symbol $
2: for i? 1 to `? 1 do
3: T [i, i+ 1]([?[i? 1], ?[i? 1]?[i]])? 0 . shift node ?[i] with ?[i? 1] on top of the stack
4: for i? ` to |?| do
5: for h? 0 to i? 1 do
6: T [i, i+ 1]([?[h], ?[h]?[i]])? 0 . shift node ?[i] with ?[h] on top of the stack
7: for d? 2 to |?| do . consider substrings of length d
8: for i? max{0, `? d} to |?| ? d do . i = beginning of substring
9: j ? i+ d . j ? 1 = end of substring
10: PROCESSCELL(T , i, i+ 1, j) . We omit the range k = i+ 2 to max{i+ 2, `} ? 1
11: for k ? max{i+ 2, `} to j do . factorization of substring at k
12: PROCESSCELL(T , i, k, j)
13: return T [0, |?|]([$, $0]) +
?
i?[0,`?1]
L
c
(?[i], t
G
)
14: procedure PROCESSCELL(T , i, k, j)
15: for each key [h
1
, h
2
h
3
]) defined in T [i, k] do
16: for each key [h
3
, h
4
h
5
]) defined in T [k, j] do . h
3
must match between the two entries
17: loss
la
? T [i, k]([h
1
, h
2
h
3
]) + T [k, j]([h
3
, h
4
h
5
]) + ?
G
(h
5
? h
4
)
18: if (i < `) ? ?
G
(h
5
? h
4
) = 0 ? (h
5
6? ?) then
19: T [i, j]([h
1
, h
2
h
5
])? min{loss
la
, T [i, j]([h
1
, h
2
h
5
])} . cell update `
la
20: loss
ra
? T [i, k]([h
1
, h
2
h
3
]) + T [k, j]([h
3
, h
4
h
5
]) + ?
G
(h
4
? h
5
)
21: if (i < `) ? ?
G
(h
4
? h
5
) = 0 ? (h
4
6? ?) then
22: T [i, j]([h
1
, h
2
h
4
])? min{loss
ra
, T [i, j]([h
1
, h
2
h
4
])} . cell update `
ra
23: loss
la
2
? T [i, k]([h
1
, h
2
h
3
]) + T [k, j]([h
3
, h
4
h
5
]) + ?
G
(h
5
? h
2
)
24: if (i < `) ? ?
G
(h
5
? h
2
) = 0 ? (h
5
6? ?) then
25: T [i, j]([h
1
, h
4
h
5
])? min{loss
la
2
, T [i, j]([h
1
, h
4
h
5
])} . cell update `
la
2
26: loss
ra
2
? T [i, k]([h
1
, h
2
h
3
]) + T [k, j]([h
3
, h
4
h
5
]) + ?
G
(h
2
? h
5
)
27: if (i < `) ? ?
G
(h
2
? h
5
) = 0 ? (h
2
6? ?) then
28: T [i, j]([h
1
, h
2
h
4
])? min{loss
ra
2
, T [i, j]([h
1
, h
2
h
4
])} . cell update `
ra
2
lines 18, 21, 24, 27), because we know that incor-
rectly attaching a buffer node as a dependent of an-
other buffer node, when the correct head is avail-
able, can never be an optimal decision in terms of
loss.
Once we have filled the table T , the loss for
the input configuration c can be obtained from the
value of the entry T [0, |?|]([$, $0]), representing
the minimum loss contribution among computa-
tions that reach the input configuration c and parse
the whole input string. To obtain the total loss,
we add to this value the loss contribution accu-
mulated by the dependency trees with root in the
stack ? of c. This is represented in Algorithm 1 as
?
i?[0,`?1]
L
c
(?[i], t
G
), where L
c
(?[i], t
G
) is the
count of the descendants of ?[i] (the (i+1)-th ele-
ment of ?) that had been assigned the wrong head
by the parser with respect to t
G
.
5.4 Sample Run
Consider the Czech sentence and the gold depend-
ency tree t
G
shown in Figure 4(a). Given the con-
figuration c = (?, ?,A) where ? = [0, 1, 3, 4],
? = [5, . . . , 13] and A = {3 ? 2}, we trace the
two stages of the algorithm.
Preprocessing of the buffer The complete sub-
tree rooted at node 7 satisfies the Bottom-up com-
pleteness and the Zero gap-degree conditions in
Section 5.2, so the nodes 5, . . . , 12 in ? can be
replaced with the root 7. Note that all the nodes in
the span 5, . . . , 12 have all their (gold) dependents
in that span, with the exception of the root 7, with
its dependent node 1 still in the stack. No other
reduction is possible, and we have ?
R
= [7, 13].
The corresponding fragment of t
G
is represented
in Figure 4(b).
Computation of the loss Let ? = ??
R
. Al-
gorithm 1 builds the two-dimensional array T in
Figure 4(c). Each cell T [i, j] contains an asso-
ciation list, whose (key:value) pairs map items to
their loss contribution. Figure 4(c) only shows the
pairs involved in the minimum-loss computation.
Lines 1-6 of Algorithm 1 initialize the cells in
the diagonal, T [0, 1], . . . , T [5, 6]. The boundary
between stack and buffer is ` = 4, thus cells
T [0, 1], T [1, 2], and T [2, 3] contain only one ele-
ment, while T [3, 4], T [4, 5] and T [5, 6] contain as
many as the previous elements in ?, although not
all of them are shown in the figure.
Lines 7-12 fill the superdiagonals until T [0, 6]
is reached. The cells T [0, 2], T [0, 3] and T [1, 3]
923
-Root- V be?z?ne?m provozu vs?ak telefonn?? linky nermaj?? takivou kvalitu jako v laborator?i .
0 1 2 3 4 5 6 7 8 9 10 11 12 13
(a) Non-projective dependency tree from the Prague Dependency Treebank.
-Root- V provozu vs?ak nermaj?? .
0 1 3 4 7 13
? ?R
(b) Fragment of dependency tree in (a) after buffer
reduction.
i j 1 2 3 4 5 6
0 [$,$ 0]:0 ? ? . . . [$,$ 0]:1 [$,$ 0]:1
1 [0,0 1]:0 ? . . . [0,0 4]:1 . . .
2 [1,1 3]:0 [1,1 4]:1 [1,4 7]:1 . . .
3 [3,3 4]:0 [3,4 7]:1 . . .
4 [4,4 7]:0 . . .
5 [0,0 13]:0
(c) Relevant portion of T computed by Algorithm 1, with the
loss of c in the yellow entry.
Figure 4: Example of loss computation given the sentence in (a) and considering a configuration c with
? = [0, 1, 3, 4] and ? = [5, . . . , 13].
are left empty because ` = 4. Once T [0, 6]
is calculated, it contains only the entry with key
[$, $, 0], with the associated value 1 representing
the minimum number of wrong arcs that the pars-
ing algorithm has to build to reach a final con-
figuration from c. Then, Line 13 retrieves the
loss of the configuration, computed as the sum of
T [0, 6]([$, $, 0]) with the termL
c
, representing the
erroneous arcs made before reaching c.
Note that in our example the loss of c is 1, even
though L
c
= 0, meaning that there are no wrong
arcs in A. Indeed, given c, there is no single com-
putation that builds all the remaining arcs in t
G
.
This is reflected in T , where the path to reach the
item with minimum loss has to go through either
T [3, 5] or T [2, 4], which implies building the erro-
neous arc (w
7
? w
3
) or (w
4
? w
3
), respectively.
6 Computational Analysis
The first stage of our algorithm can be easily im-
plemented in time O(|?| |t
G
|), where |t
G
| is the
number of nodes in t
G
, which is equal to the length
n of the input string.
For the worst-case complexity of the second
stage (Algorithm 1), note that the number
of cell updates made by calling PROCESS-
CELL(T , i, k, j) with k < ` is O(|?|
3
|?|
2
|?
R
|).
This is because these updates can only be caused
by procedure calls on line 10 (as those on line 12
always set k ? `) and therefore the index k always
equals i + 1, while h
2
must equal h
1
because the
item [h
1
, h
2
h
3
] is one of the initial items created
on line 3. The variables i, h
1
and h
3
must index
nodes on the stack ? as they are bounded by k,
while j ranges over ?
R
and h
4
and h
5
can refer to
nodes either on ? or on ?
R
.
On the other hand, the number of cell updates
triggered by calls to PROCESSCELL such that k ?
` is O(|?|
4
|?
R
|
4
), as they happen for four indices
referring to nodes of ?
R
(k, j, h
4
, h
5
) and four
indices that can range over ? or ?
R
(i, h
1
, h
2
, h
3
).
Putting everything together, we conclude that
the overall complexity of our algorithm is
O(|?| |t
G
|+ |?|
3
|?|
2
|?
R
|+ |?|
4
|?
R
|
4
).
In practice, quantities |?|, |?
R
| and |?| are signi-
ficantly smaller than n, providing reasonable train-
ing times as we will see in Section 7. For instance,
when measured on the Czech treebank, the aver-
age value of |?| is 7.2, with a maximum of 87.
Even more interesting, the average value of |?
R
|
is 2.6, with a maximum of 23. Comparing this to
the average and maximum values of |?|, 11 and
192, respectively, we see that the buffer reduction
is crucial in reducing training time.
Note that, when expressed as a function of n,
our dynamic oracle has a worst-case time com-
plexity of O(n
8
). This is also the time complexity
of the dynamic programming algorithm of Cohen
et al. (2011) we started with, simulating all com-
putations of our parser. In contrast, the dynamic
oracle of Goldberg et al. (2014) for the projective
case achieves a time complexity ofO(n
3
) from the
dynamic programming parser by Kuhlmann et al.
(2011) running in time O(n
5
).
924
The reason why we do not achieve any asymp-
totic improvement is that some helpful properties
that hold with projective trees are no longer satis-
fied in the non-projective case. In the projective
(arc-standard) case, subtrees that are in the buf-
fer can be completely reduced. As a consequence,
each oracle step always combines an inferred entry
in the table with either a node from the stack or a
node from the reduced buffer, asymptotically re-
ducing the time complexity. However, in the non-
projective (Attardi) case, subtrees in the buffer can
not always be completely reduced, for the reasons
mentioned in the second-to-last paragraph of Sec-
tion 5.2. As a consequence, the oracle needs to
make cell updates in a more general way, which
includes linking pairs of elements in the reduced
buffer or pairs of inferred entries in the table.
-Root- John was not as good for the job as Kate .
0 1 2 3 4 5 6 7 8 9 10 11
Figure 5: Non-projective dependency tree adapted
from the Penn Treebank.
An example of why this is needed is provided
by the gold tree in Figure 5. Assume a config-
uration c = (?, ?,A) where ? = [0, 1, 2, 3, 4],
? = [5, . . . , 11], and A = ?. It is easy to see that
the loss of c is greater than zero, since the gold tree
is not reachable from c: parsing the subtree rooted
at node 5 requires shifting 6 into the stack, and
this makes it impossible to build the arcs 2 ? 5
and 2? 6. However, if we reduced the subtree in
the buffer with root 5, we would incorrectly obtain
a loss of 0, as the resulting tree is parsable if we
start with `
sh
followed by `
la
and `
ra
2
. Note that
there is no way of knowing whether it is safe to
reduce the subtree rooted at 5 without using non-
local information. For example, the arc 2 ? 6 is
crucial here: if 6 depended on 5 or 4 instead, the
loss would be zero. These complications are not
found in the projective case, allowing for the men-
tioned asymptotic improvement.
7 Experimental Evaluation
For comparability with previous work on dynamic
oracles, we follow the experimental settings repor-
ted by Goldberg et al. (2014) for their arc-standard
dynamic oracle. In particular, we use the same
training algorithm, features, and root node posi-
tion. However, we train the model for 20 itera-
static dynamic
UAS LAS UAS LAS
Arabic 80.90 71.56 82.23 72.63
Basque 75.96 66.74 74.32 65.59
Catalan 90.55 85.20 89.94 84.96
Chinese 84.72 79.93 85.34 81.00
Czech 79.83 72.69 82.08 74.44
English 85.52 84.46 87.38 86.40
Greek 79.84 72.26 81.55 74.14
Hungarian 78.13 68.90 76.27 68.14
Italian 83.08 78.94 84.43 80.45
Turkish 79.57 69.44 79.41 70.32
Bulgarian 89.46 85.99 89.32 85.92
Danish 85.58 81.25 86.03 81.59
Dutch 79.05 75.69 80.13 77.22
German 88.34 86.48 88.86 86.94
Japanese 93.06 91.64 93.56 92.18
Portuguese 84.80 81.38 85.36 82.10
Slovene 76.33 68.43 78.20 70.22
Spanish 79.88 76.84 80.25 77.45
Swedish 87.26 82.77 87.24 82.49
PTB 89.55 87.18 90.47 88.18
Table 1: Unlabelled Attachment Score (UAS) and
Labelled Attachment Score (LAS) using a static
and a dynamic oracle. Evaluation on CoNLL 2007
(first block) and CoNLL 2006 (second block) data-
sets is carried out including punctuation, evalu-
ation on the Penn Treebank excludes it.
tions rather than 15, as the increased search space
and spurious ambiguity of Attardi?s non-project-
ive parser implies that more iterations are required
to converge to a stable model. A more detailed
description of the experimental settings follows.
7.1 Experimental Setup
Training We train a global linear model using
the averaged perceptron algorithm and a labelled
version of the parser described in Section 3. We
perform on-line training using the oracle defined
in Section 5: at each parsing step, the model?s
weights are updated if the predicted transition res-
ults into an increase in configuration loss, but
the process continues by following the predicted
transition independently of the loss increase.
As our baseline we train the model using the
static oracle defined by (Cohen et al., 2012). This
oracle follows a canonical computation that cre-
ates arcs as soon as possible, and prioritizes the
`
la
transition over the `
la
2
transition in situations
925
where both create a gold arc. The static oracle
is not able to deal with configurations that can-
not reach the gold dependency tree, so we con-
strain the training algorithm to follow the zero-loss
transition provided by the oracle.
While this version of Attardi?s parser has been
shown to cover the vast majority of non-projective
sentences in several treebanks (Attardi, 2006; Co-
hen et al., 2012), there still are some sentences
which are not parsable. These sentences are
skipped during training, but not during test and
evaluation of the model.
Datasets We evaluate the parser performance
over CoNLL 2006 and CoNLL 2007 datasets.
If a language is present in both datasets, we
use the latest version. We also include res-
ults over the Penn Treebank (PTB) (Marcus et
al., 1993) converted to Stanford basic dependen-
cies (De Marneffe et al., 2006). For the CoNLL
datasets we use the provided part-of-speech tags
and the standard training/test partition; for the
PTB we use automatically assigned tags, we train
on sections 2-21 and test on section 23.
7.2 Results and Analysis
In Table 1 we report the unlabelled (UAS) and la-
belled (LAS) attachment scores for the static and
the dynamic oracles. Each figure is an average
over the accuracy provided by 5 models trained
with the same setup but using a different random
seed. The seed is only used to shuffle the sentences
in random order during each iteration of training.
Our results are consistent with the results re-
ported by Goldberg and Nivre (2013) and Gold-
berg et al. (2014). For most of the datasets, we
obtain a relevant improvement in both UAS and
LAS. For Dutch, Czech and German, we achieve
an error reduction of 5.2%, 11.2% and 4.5%, re-
spectively. Exceptions to this general trend are
Swedish and Bulgarian, where the accuracy differ-
ences are negligible, and the Basque, Catalan and
Hungarian datasets, where the performance actu-
ally decreases.
If instead of testing on the standard test sets we
use 10-fold cross-validation and average the res-
ulting accuracies, we obtain improvements for all
languages in Table 1 but Basque and Hungarian.
More specifically, measured (UAS, LAS) pairs for
Swedish are (86.85, 82.17) with dynamic oracle
against (86.6, 81.93) with static oracle; for Bul-
garian (88.42, 83.91) against (88.20, 83.55); and
for Catalan (88.33, 83.64) against (88.06, 83.13).
This suggests that the negligible or unfavourable
results in Table 1 for these languages are due to
statistical variability given the small size of the test
sets.
As for Basque, we measure (75.54, 67.58)
against (76.77, 68.20); similarly, for Hungarian
we measure (75.66, 67.66) against (77.22, 68.42).
Unfortunately, we have no explanation for these
performance decreases, in terms of the typology
of the non-projective patterns found in these two
datasets. Note that Goldberg et al. (2014) also
observed a performance decrease on the Basque
dataset in the projective case, although not on
Hungarian.
The parsing times measured in our experiments
for the static and the dynamic oracles are the same,
since the oracle algorithm is only used during the
training stage. Thus the reported improvements in
parsing accuracy come at no extra cost for parsing
time. In the training stage, the extra processing
needed to compute the loss and to explore paths
that do not lead to a gold tree made training about
4 times slower, on average, for the dynamic oracle
model. This confirms that our oracle algorithm is
fast enough to be of practical interest, in spite of its
relatively high worst-case asymptotic complexity.
8 Conclusions
We have presented what, to our knowledge, are
the first experimental results for a transition-based
non-projective parser trained with a dynamic or-
acle. We have also shown significant accuracy im-
provements on many languages over a static oracle
baseline.
The general picture that emerges from our ap-
proach is that dynamic programming algorithms
originally conceived for the simulation of trans-
ition-based parsers can effectively be used in the
development of polynomial-time algorithms for
dynamic oracles.
Acknowledgments
The first author has been partially funded by Min-
isterio de Econom??a y Competitividad/FEDER
(Grant TIN2010-18552-C03-02) and by Xunta de
Galicia (Grant CN2012/008). The third author has
been partially supported by MIUR under project
PRIN No. 2010LYA9RH 006.
926
References
Giuseppe Attardi. 2006. Experiments with a multil-
anguage non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL), pages 166?
170, New York, USA.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1052?
1062, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Shay B. Cohen, Carlos G?omez-Rodr??guez, and Giorgio
Satta. 2011. Exact inference for generative prob-
abilistic non-projective dependency parsing. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1234?
1245, Edinburgh, Scotland, UK., July. Association
for Computational Linguistics.
Shay B. Cohen, Carlos G?omez-Rodr??guez, and Gior-
gio Satta. 2012. Elimination of spurious ambigu-
ity in transition-based dependency parsing. CoRR,
abs/1206.6735.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC),
volume 6, pages 449?454.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing. In Proc. of
the 24
th
COLING, Mumbai, India.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the association for Computational
Linguistics, 1.
Yoav Goldberg, Francesco Sartorio, and Giorgio Satta.
2014. A tabular method for dynamic oracles in
transition-based parsing. Transactions of the Associ-
ation for Computational Linguistics, 2(April):119?
130.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull-
man. 2006. Introduction to Automata Theory, Lan-
guages, and Computation (3rd Edition). Addison-
Wesley Longman Publishing Co., Inc., Boston, MA,
USA.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, July.
Marco Kuhlmann and Joakim Nivre. 2010. Transition-
based techniques for non-projective dependency
parsing. Northern European Journal of Language
Technology, 2(1):1?19.
Marco Kuhlmann, Carlos G?omez-Rodr??guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 673?682, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
Eighth International Workshop on Parsing Techno-
logies (IWPT), pages 149?160, Nancy, France.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Workshop on Incremental
Parsing: Bringing Engineering and Cognition To-
gether, pages 50?57, Barcelona, Spain.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Stat-
istical dependency analysis with support vector ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT), pages
195?206.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
927
Complexity, Parsing, and Factorization
of Tree-Local Multi-Component
Tree-Adjoining Grammar
Rebecca Nesson?
School of Engineering and Applied
Sciences,
Harvard University
Giorgio Satta??
Department of Information Engineering
University of Padua
Stuart M. Shieber?
School of Engineering and Applied
Sciences
Harvard University
Tree-Local Multi-Component Tree-Adjoining Grammar (TL-MCTAG) is an appealing formal-
ism for natural language representation because it arguably allows the encapsulation of the
appropriate domain of locality within its elementary structures. Its multicomponent structure
allows modeling of lexical items that may ultimately have elements far apart in a sentence, such
as quantifiers and wh-words. When used as the base formalism for a synchronous grammar, its
flexibility allows it to express both the close relationships and the divergent structure necessary
to capture the links between the syntax and semantics of a single language or the syntax of
two different languages. Its limited expressivity provides constraints on movement and, we
posit, may have generated additional popularity based on a misconception about its parsing
complexity.
Although TL-MCTAG was shown to be equivalent in expressivity to TAG when it was
first introduced, the complexity of TL-MCTAG is still not well understood. This article offers
a thorough examination of the problem of TL-MCTAG recognition, showing that even highly
restricted forms of TL-MCTAG are NP-complete to recognize. However, in spite of the provable
difficulty of the recognition problem, we offer several algorithms that can substantially improve
processing efficiency. First, we present a parsing algorithm that improves on the baseline parsing
? School of Engineering and Applied Sciences, Harvard University, 38 Plymouth St., Cambridge, MA
02141. E-mail: nesson@seas.harvard.edu.
?? Department of Information Engineering, University of Padua, via Gradenigo 6/A, 1-35131 Padova, Italy.
E-mail: satta@dei.unipd.it.
? School of Engineering and Applied Sciences, Harvard University, Maxwell Dworkin Laboratory,
33 Oxford Street, Cambridge, MA 02138. E-mail: shieber@seas.harvard.edu.
Submission received: 4 November 2008; revised submission received: 13 November 2009; accepted for
publication: 18 March 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
method and runs in polynomial time when both the fan-out and rank of the input grammar are
bounded. Second, we offer an optimal, efficient algorithm for factorizing a grammar to produce a
strongly equivalent TL-MCTAG grammar with the rank of the grammar minimized.
1. Introduction
Tree-Local Multi-Component Tree-Adjoining Grammar (TL-MCTAG) is an appealing
formalism for natural language representation because it arguably allows the encapsu-
lation of the appropriate domain of locality within its elementary structures (Kallmeyer
and Romero 2007). Its flexible multicomponent structure allows modeling of lexical
items that may ultimately have elements far apart in a sentence, such as quantifiers and
wh-words. Its limited expressivity provides constraints on movement and, we posit,
may have generated additional popularity based on a misconception about its parsing
complexity.
TL-MCTAG can model highly structurally divergent but closely related elementary
structures, such as the syntax and the semantics of a single word or construction or the
syntax of a single word or construction and its translation into another language, with a
pair of elementary trees. This flexibility permits conceptually simple, highly expressive,
and tightly coupled modeling of the relationship between the syntax and semantics of
a language or the syntax and semantics of two languages. As a result, it has frequently
been put to use in a growing body of research into incorporating semantics into the Tree-
Adjoining Grammar (TAG) framework (Kallmeyer and Joshi 2003; Han 2006; Nesson
and Shieber 2006, 2007). It is also under investigation as a possible base formalism
for use in synchronous-grammar based machine translations systems (Nesson 2009).
Similar pairing of elementary structures of the TAG formalism is too constrained to
capture the inherent divergence in structure between different languages or even be-
tween the syntax and semantics of a language. Pairing of more expressive formalisms is
too flexible to provide appropriate constraints and has unacceptable consequences for
processing efficiency.
Although TL-MCTAG was first introduced by Weir (1988) and shown at that time
to be equivalent in expressivity to TAG, the complexity of TL-MCTAG is still not
well understood. Perhaps because of its equivalence to TAG, questions of processing
efficiency have not been adequately addressed. This article offers a thorough exami-
nation of the problem of TL-MCTAG recognition, showing that even highly restricted
forms of TL-MCTAG are NP-complete to recognize. However, in spite of the provable
difficulty of the recognition problem, we offer several algorithms that can substantially
improve processing efficiency. First, we present a parsing algorithm that improves
on the baseline parsing method and runs in polynomial time when both the fan-out
(the maximum number of trees in a tree set) and rank (the maximum number of
trees that may be substituted or adjoined into a given tree) of the input grammar are
bounded. Second, we offer an optimal, efficient algorithm for factorizing a grammar
to produce a strongly equivalent TL-MCTAG grammar with the rank of the grammar
minimized.
1.1 Summary of Results
TAG is a mildly context-sensitive grammar formalism widely used in natural language
processing. Multicomponent TAG (MCTAG) refers to a group of formalisms that
444
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
generalize TAG by allowing elementary structures to be sets of TAG trees. One member
of the MCTAG formalism group is Tree-Local MCTAG (TL-MCTAG), in which all
trees from a single elementary tree set are constrained to adjoin or substitute into a
single tree in another elementary tree set. Weir (1988) shows that this constraint is
sufficient to guarantee that TL-MCTAG has weak generative capacity equivalent to the
polynomially parsable TAG.
Recent work on the complexity of several TAG variants has demonstrated indirectly
that the universal recognition problem for TL-MCTAG is NP-hard. This result calls
into question the practicality of systems that employ TL-MCTAG as the formalism for
expressing a natural language grammar. In this article we present a more fine-grained
analysis of the processing complexity of TL-MCTAG. We demonstrate (Section 3) that
even under restricted definitions where either the rank or the fan-out of the grammar is
bounded, the universal recognition problem is NP-complete.
We define a novel variant of multi-component TAG formalisms that treats the
elementary structures as vectors of trees rather than as unordered sets (Section 4). We
demonstrate that this variant of the definition of the formalism (the vector definition) is
consistent with the linguistic applications of the formalism presented in the literature.
Universal recognition of the vector definition of TL-MCTAG is NP-complete when
both the rank and fan-out are unbounded. However, when the rank is bounded, the
universal recognition problem is polynomial in both the length of the input string and
the grammar size.
We present a novel parsing algorithm for TL-MCTAG (Section 5) that accommo-
dates both the set and vector definitions of TL-MCTAG. Although no algorithms for
parsing TL-MCTAG have previously been published, the standard method for parsing
linear context-free rewriting systems (LCFRS)?equivalent formalisms can be applied
directly to TL-MCTAG to produce a quite inefficient baseline algorithm in which the
polynomial degree of the length of the input string depends on the input grammar.
We offer an alternative parser for TL-MCTAG in which the polynomial degree of the
length of the input string is constant, though the polynomial degree of the grammar size
depends on the input grammar. This alternative parsing algorithm is more appealing
than the baseline algorithm because it performs universal recognition of TL-MCTAG
(vector definition) with constant polynomial degree in both the length of the input string
and the grammar size when rank is bounded.
It may not be generally desirable to impose an arbitrary rank bound on TL-MCTAGs
to be used for linguistic applications. However, it is possible given a TL-MCTAG to
minimize the rank of the grammar. In the penultimate section of the paper (Section 6)
we offer a novel and efficient algorithm for transforming an arbitrary TL-MCTAG into
a strongly equivalent TL-MCTAG where the rank is minimized.
1.2 Related Work
Our work on TL-MCTAG complexity bears comparison to that of several others.
Kallmeyer (2009) provides a clear and insightful breakdown of the different charac-
teristics of MCTAG variants and the effect of these characteristics on expressivity and
complexity. That work clarifies the definitions of MCTAG variants and the relationship
between them rather than presenting new complexity results. However, it suggests
the possibility of proving results such as ours in its assertion that, after a standard
TAG parse, a check of whether particular trees belong to the same tree set cannot
be performed in polynomial time. Kallmeyer also addresses the problem of parsing
445
Computational Linguistics Volume 36, Number 3
MCTAG, although not specifically for TL-MCTAG. The method proposed differs from
ours in that MCTAGs are parsed first as a standard TAG, with any conditions on tree
or set locality checked on the derivation forest as a second step. No specific algorithm
is presented for performing the check of tree-locality on a TAG derivation forest, so it is
difficult to directly compare the methods. However, that method cannot take advantage
of the gains in efficiency produced by discarding inappropriate partial parses at the
time that they are first considered. Aside from Kallmeyer?s work, little attention has
been paid to the problem of directly parsing TL-MCTAG.
S?gaard, Lichte, and Maier (2007) present several proofs regarding the complexity
of the recognition problem for some linguistically motivated extensions of TAG that are
similar to TL-MCTAG. Their work shows the NP-hardness of the recognition problem
for these variants and, as an indirect result, also demonstrates the NP-hardness of TL-
MCTAG recognition. This work differs from ours in that it does not directly show the
NP-hardness of TL-MCTAG recognition and does not further locate and constrain the
source of the NP-hardness of the problem to the rank of the input grammar, nor does it
provide mitigation through rank reduction of the grammar or by other means.
Our work on TL-MCTAG factorization is thematically though not formally related
to the body of work on induction of TAGs from a treebank exemplified by Chen and
Shanker (2004). The factorization performed in their work is done on the basis of syn-
tactic constraints rather than with the goal of reducing complexity. Working from a
treebank of actual natural language sentences, their work does not have the benefit of
explicitly labeled adjunction sites but rather must attempt to reconstruct a derivation
from complete derived trees.
The factorization problem we address is more closely related to work on factorizing
synchronous context-free grammars (CFGs) (Gildea, Satta, and Zhang 2006; Zhang and
Gildea 2007) and on factorizing synchronous TAGs (Nesson, Satta, and Shieber 2008).
Synchronous grammars are a special case of multicomponent grammars, so the prob-
lems are quite similar to the TL-MCTAG factorization problem. However, synchronous
grammars are fundamentally set-local rather than tree-local formalisms, which in some
cases simplifies their analysis. In the case of CFGs, the problem reduces to one of
identifying problematic permutations of non-terminals (Zhang and Gildea 2007) and
can be done efficiently by using a sorting algorithm to binarize any non-problematic per-
mutations until only the intractable correspondences remain (Gildea, Satta, and Zhang
2006). This method is unavailable in the TAG case because the elementary structures
may have depth greater than one and therefore the concept of adjacency relied upon
in their work is inapplicable. The factorization algorithm of Nesson, Satta, and Shieber
(2008) is the most closely related to this one but is not directly applicable to TL-MCTAG
because each link is presumed to have exactly two locations and all adjunctions occur
in a set-local rather than tree-local manner.
2. Technical Background
A tree-adjoining grammar consists of a set of elementary tree structures of arbitrary
depth, which are combined by the operations of adjunction and substitution. Auxiliary
trees are elementary trees in which the root and a frontier node, called the foot node
and distinguished by the diacritic ?, are labeled with the same nonterminal A. The
adjunction operation entails splicing in an auxiliary tree in an internal node within
an elementary tree also labeled with nonterminal A. Trees without a foot node, which
serve as a base case for derivations and may combine with other trees by substitution,
446
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
are called initial trees. Examples of the adjunction and substitution operations are
given in Figure 1. For further background, we refer the reader to the survey by Joshi
and Schabes (1997).
A TAG derivation can be fully specified by a derivation tree, which records how
the elementary structures are combined using the TAG operations to form the derived
tree. The nodes of the derivation tree are labeled by the names of the elementary trees
and the edges are labeled by the addresses at which the child trees substitute or adjoin.
In contrast to CFGs, the derivation and derived trees are distinct.
We depart from the traditional definition in notation only by specifying adjunc-
tion sites explicitly with numbered links in order to simplify the presentation of the
issues raised by multi-component adjunctions. Each link may be used only once in a
derivation. Adjunctions may only occur at nodes marked with a link. A numbered link
at a single site in a tree specifies that a single adjunction is available at that site. An
obligatory adjunction constraint indicates that at least one link at a given node must
be used (Joshi, Levy, and Takahashi, 1975; Vijay-Shanker and Joshi 1985). We notate
obligatory adjunction constraints by underlining the label of the node to which the
constraint applies. Because we use explicit links, the edges in the derivation tree are
labeled with the number of the link used rather than the traditional label of the address
at which the operation takes place.
Multiple adjunction refers to permitting an unbounded number of adjunctions to
occur at a single adjunction site (Vijay-Shanker 1987; Shieber and Schabes 1994). In
the standard definition of TAG, multiple adjunction is disallowed to ensure that each
derivation tree unambiguously specifies a single derived tree (Vijay-Shanker 1987). Be-
cause each available adjunction is explicitly notated with a numbered link, our notation
implicitly disallows multiple adjunction but permits a third possibility: bounded mul-
tiple adjunction. Bounded multiple adjunction permits the formalism to obtain some
of the potential linguistic advantages of allowing multiple adjunction while preventing
unbounded multiple adjunction. The usual constraint of allowing only one adjunction
at a given adjunction site may be enforced in our link notation by permitting only one
link at a particular link site to be used.
MCTAG generalizes TAG by allowing the elementary items to be sets of trees rather
than single trees (Joshi and Schabes 1997). The basic operations are the same but all trees
in a set must adjoin (or substitute) into another tree set in a single step in the derivation.
To allow for multi-component adjunction, a numbered link may appear on two or more
nodes in a tree, signifying that the adjoining trees must be members of the same tree
set. Any tree in a set may adjoin at any link location if it meets other adjunction or
substitution conditions such as a matching node label. Thus a single multicomponent
Figure 1
An example of TAG operations substitution and adjunction used here to model natural
language syntax.
447
Computational Linguistics Volume 36, Number 3
Figure 2
An example of the way in which two tree sets may produce several different derived trees when
combined under the standard definition of multicomponent TAG.
link may give rise to many distinct derived trees even when the link is always used
by the same multicomponent tree set. An example is given in Figure 2. This standard
definition of multicomponent adjunction we will call the set definition for contrast with
a variation we introduce in Section 4. A derivation tree for a multicomponent TAG is
the same as for TAG except that the nodes are labeled with the names of elementary
tree sets.
An MCTAG is tree-local if tree sets are required to adjoin within a single elementary
tree (Weir 1988). Using the numbered link notation introduced earlier for adjunction
sites, a tree-local MCTAG (TL-MCTAG) is one in which the scope of the link numbers
is a single elementary tree. An example TL-MCTAG operation is given in Figure 3. In
contrast, an MCTAG is set-local if the trees from a single tree set are required to adjoin
within a single elementary tree set and an MCTAG is non-local if the trees from a single
tree set may adjoin to trees that are not within a single tree set. In a set-local MCTAG
the scope of a link is a single elementary tree set, and in a non-local MCTAG the scope
of a link is the entire grammar.
Weir (1988) noted in passing that TL-MCTAG has generative capacity equivalent to
TAG; a combination of well-chosen additional constraints and additions of duplicates
of trees to the grammar can produce a weakly equivalent TAG. Alternatively, a feature-
based TAG where the features enforce the same constraints may be used. Although the
generative capacity of the formalism is not increased, any such conversion from TL-
MCTAG to TAG may require an exponential increase in the size of the grammar as we
prove in Section 3.
3. Complexity
We present several complexity results for TL-MCTAG. S?gaard, Lichte, and Maier (2007)
show indirectly that TL-MCTAG membership is NP-hard. For clarity, we present a direct
Figure 3
An example TL-MCTAG operation demonstrating the use of TL-MCTAG to model wh-question
syntax.
448
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
proof here. We then present several novel results demonstrating that the hardness result
holds under significant restrictions of the formalism.
For a TL-MCTAG G we write |G| to denote the size of G, defined as the total number
of nodes appearing in all elementary trees in the tree sets of the grammar. Fan-out, f ,
measures the number of trees in the largest tree set in the grammar. We show that even
when the fan-out is bounded to a maximum of two, the NP-hardness result still holds.
The rank, r, of a grammar is the maximum number of derivational children possible
for any tree in the grammar, or in other words, the maximum number of links in any
tree in the grammar. We show that when rank is bounded, the NP-hardness result also
holds.
A notable aspect of all of the proofs given here is that they do not make use of
the additional expressive power provided by the adjunction operation of TAG. Put
simply, the trees in the tree sets used in our constructions meet the constraints of Tree
Insertion Grammar (TIG), a known context-free?equivalent formalism (Schabes and
Waters 1995). As a result, we can conclude that the increase in complexity stems from
the multi-component nature of the formalism rather than from the power added by an
unconstrained adjunction operation.
3.1 Universal Recognition of TL-MCTAG is NP-Complete
In this section we prove that universal recognition of TL-MCTAG is NP-complete when
neither the rank nor the fan-out of the grammar is bounded.
Recall the 3SAT decision problem, which is known to be NP-complete. Let V =
{v1, . . . , vp} be a set of variables and C = {c1, . . . , cn} be a set of clauses. Each clause in C
is a disjunction of three literals over the alphabet of all literals LV = {v1, v1, . . . , vp, vp}.
We represent each clause by a set of three literals. The language 3SAT is defined as the
set of all conjunctive formulas over the members of C that are satisfiable.
Theorem 1
The universal recognition problem for TL-MCTAG with unbounded rank and fan-out
is NP-hard.
Proof
Let ?V, C? be an arbitrary instance of the 3SAT problem.1 We use the derivations of the
grammar to guess the truth assignments for V and use the tree sets to keep track of the
dependencies among different clauses in C. Two tree sets are constructed for each vari-
able, one corresponding to an assignment of true to the variable and one corresponding
to an assignment of false. The links in the single initial tree permit only one of these two
sets to be used. The tree set for a particular truth assignment for a particular variable vi
makes it possible to introduce, by means of another adjunction, terminal symbols taken
from the set {1, . . . , n} that correspond to each clause in C that would be satisfied by the
given assignment to vi. In this way, the string w = 1 ? ? ? n can be generated if and only if
all clauses are satisfied by the truth assignment to some variable they contain.
1 We follow the proof strategy of Satta and Peserico (2005) in this and the proof of Theorem 3.
449
Computational Linguistics Volume 36, Number 3
We define a tree-local MCTAG G containing the following tree sets. The initial tree
set S contains the single tree:
In this tree, the ?rows? correspond to the variables and the ?columns? to the clauses.
Each non-terminal node within a row is labeled with the same link to ensure that a tree
set representing a single variable?s effect on each clause will adjoin at each link.
For every variable vi, 1 ? i ? p, tree set Ti, used when representing an assignment of
the value true to vi, contains n trees, one for each clause cj, 1 ? j ? n, defined as follows:
For every variable vi, 1 ? i ? p, tree set Fi ? used when representing an assignment
of the value false to vi ? contains n trees, one for each clause cj, 1 ? j ? n, defined as
follows:
For every clause cj, 1 ? j ? n, tree set Cj contains a single tree as shown here. This
tree allows the corresponding clause number terminal symbol to be recognized by an
appropriate variable instance.2
2 Note that because adjunction is not obligatory, the tree from Cj need not adjoin into the tree for a
particular variable. In fact, to generate w, exactly one instance of Cj must adjoin for each clause even if
more than one variable satisfies the clause. If w can be generated, however, we can conclude that at least
one variable must have satisfied each clause.
450
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
From the definition of G it directly follows that w ? L(G) implies the existence of a
truth assignment that satisfies C. A satisfying truth assignment can be read directly off
of any derivation tree for w. If Ti (respectively, Fi) is a child of S in the derivation tree,
then vk is true (respectively, false). The converse can be shown by using a satisfying
truth assignment for C to construct a derivation for w ? L(G).
?G, w? can be constructed in deterministic polynomial time because the number of
tree sets in the grammar is 2p + 2n + 1, the total number of trees in the grammar is
bounded by n(2p + 2n + 1), and the length of w is n. All trees in the grammar have
constant size except for the initial tree, which has size np. 
Theorem 2
The universal recognition problem for TL-MCTAG with unbounded rank and fan-out
is in NP.
Proof
We show that given an arbitrary TL-MCTAG grammar G and any input string w, the
determination of w ? L(G) can be performed in non-deterministic polynomial time.
Note that the collection of elementary tree sets of G that can generate the empty
string, E , can be generated in time polynomial in |G| using the standard graph reach-
ability algorithm used for context-free grammars in time polynomial in |G| (Sippu and
Soisalon-Soininen 1988).
We begin by showing that given an arbitrary input string w and derivation tree D
for w ? L(G), there must exist a truncated derivation tree for w that has size no larger
than |G| ? |w|. We define a truncated derivation tree as a derivation tree in which the
children of elementary tree sets in E are optionally removed.
Consider D. Each node in D represents an elementary structure of G: a tuple of
one or more TAG trees. We call a node n of D a non-splitting node if a single one
of its children in the derivation tree, ni, generates the same lexical material from the
input string as n itself.3 We call it a splitting node if more than one of its children
generates a non-empty part of the portion of the input string generated by n itself
or if n itself contributes lexical material. We proceed from the root of D examining
chains of non-splitting nodes. Assume that the root of D is a non-splitting node. This
means that it has a single child node, ni, that generates the lexical material for the entire
input string. Its other children all generate the empty string (and therefore must also be
members of E). We truncate the derivation tree at each child of n other than ni. We now
iterate the process on node ni. If during the examination of a chain of non-splitting
nodes we encounter a node identical to one that we have already seen, we remove
the entire cycle from the derivation tree because it is not essential to the derivation.
Because all cycles are removed, the longest possible chain of non-splitting nodes we
can find before encountering a splitting node or reaching the bottom of the derivation
tree is |G|.
If a splitting node is encountered, we truncate all child nodes that generate the
empty string and then iterate the process of non-splitting node identification on those
3 The child tree tuple ni may generate the same lexical material in several distinct pieces, which are
arranged into the string generated by n when the adjunction occurs. Because the adjunction necessarily
connects all of these pieces into a single string in a single predetermined way, it does not matter for our
proof that the lexical material derived by the child may be in any order before the adjunctions.
451
Computational Linguistics Volume 36, Number 3
children that generate lexical material. In the worst case, the process encounters w ? 1
splitting nodes, each of which may be separated by a chain of non-splitting nodes
of maximum length bounded by |G|. This process, therefore, produces a truncated
derivation tree with size bounded by |G| ? |w|.
The truncation of the tree at each node that generates the empty string is necessary
because the size of the subderivation tree generating the empty string may not be
bounded by a polynomial in the size of the grammar. However, the content of the part
of the derivation tree used to generate the empty string is not necessary for determining
membership of w ? L(G) because we know that each truncated node is a member of E .
To show that TL-MCTAG membership is in NP, we construct a Turing machine
that will non-deterministically guess a truncated derivation tree of size no larger than
|G| ? |w|. It then checks that the guessed derivation successfully derives w. Because the
correctness of the derivation can be checked in linear time, this is sufficient to show that
TL-MCTAG membership is in NP. 
We know from the equivalence of LCFRS and SL-MCTAG (and the rule-to-tree-
tuple conversion method used to prove equivalency) (Weir 1988) and the fact that
LCFRS membership is PSPACE-complete that SL-MCTAG membership is also PSPACE-
complete (Kaji et al 1992, 1994). Until the results shown in Theorems 1 and 2 it was
not known whether TL-MCTAG was in NP. Although the difference in generative
capacity between TL-MCTAG and SL-MCTAG is well known, this proven difference
in complexity (assuming NP = PSPACE) is novel.
To understand the reason underlying the difference, we note that the bound on the
length of non-splitting chains does not hold for set-local MCTAG. In set-local MCTAG
a tree tuple may be non-splitting while also performing a permutation of the order of
the lexical output generated by its children. Permutation is possible because set-locality
allows the tuple of strings generated by a tree tuple to be held separate for an arbitrary
number of steps in a derivation. This directly follows the basis of the reasoning of Kaji
et al (1992) in their proof that LCFRS is PSPACE-complete.
3.2 Universal Recognition of TL-MCTAG with Bounded Fan-Out is NP-Complete
The grammar constructed in the proof of Theorem 1 has fan-out n, the number of
clauses. However, the hardness result proved herein holds even if we restrict tree sets
to have at most two elements (TL-MCTAG(2)).4 The result provided here is as tight
as possible. If tree sets are restricted to a maximum size of one (TL-MCTAG(1)), the
formalism reduces to TAG and the hardness result does not hold.
Theorem 3
The universal recognition problem for TL-MCTAG(2) with fan-out limited to two and
unbounded rank is NP-complete.
Proof
Let ?V, C? be an arbitrary instance of the 3SAT problem. We define a more complex
string w = w(1)w(2) ? ? ?w(p)wc where wc is a representation of C and w(i) controls the truth
assignment for the variable vi, 1 ? i ? p. The proof strategy is as follows. We construct
4 We use the postfix (2) to indicate the restriction on the fan-out.
452
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
a TL-MCTAG(2) grammar G such that each w(i) can be derived from G in exactly two
ways using the left members of tree sets of size 2 that correspond to the variables (and a
single initial tree set of size 1). We call the part of w comprising w(1)w(2) ? ? ?w(p) the prefix
string. The prefix string enforces the constraint of permitting only two derivations by
requiring a strictly alternating string of terminal symbols that can only be generated
by the grammar when the truth assignment is stable for a particular variable. The
derivation of the prefix string w(1)w(2) ? ? ?w(p) therefore corresponds to a guess of a truth
assignment for V. The right trees from the tree sets derive the components of wc that are
compatible with the guessed truth assignments for v1, . . . , vp. Subsequently we explain
how ?G, w? is constructed given an instance of 3SAT ?V, C?.
For every variable vi, 1 ? i ? p, let Ai = {cj | vi ? cj} and Ai = {cj | vi ? cj} be the
sets of clauses in which vi occurs positively and negatively, respectively; let alo mi =
|Ai| + |Ai| be the number of occurrences of the variable vi. Let ?? = {ai, bi | 1 ? i ? p}
be an alphabet of not already used symbols; let w(i) (again for 1 ? i ? p) denote a
sequence of mi + 1 alternating symbols ai and bi such that if mi is even w(i) = (aibi)mi/2ai
and if mi is odd w(i) = (aibi)(mi+1)/2. We define three functions, ?, ?, and ?, to aid in
the construction. The functions ? and ? are used to produce pieces of the prefix string
and will only produce the correct prefix string for a variable if the truth assignment is
consistent within the derivation. The function ? is used to produce strings representing
the clauses satisfied by a particular truth assignment to a variable. For every variable
vi, 1 ? i ? p, the clauses ?(i, 1), ?(i, 2), . . . , ?(i, |Ai|) are all the clauses in Ai and the
clauses ?(i, |Ai| + 1), . . . , ?(i, mi) are all the clauses in Ai. Further, for every 1 ? i ? p, let
?(i, 1) = aibi and let ?(i, h) = ai if h is even and ?(i, h) = bi if h is odd, for 2 ? h ? mi. For
every 1 ? i ? p, let ?(i, h) = ai if h is odd, and ?(i, h) = bi if h is even for 1 ? h ? mi ? 1
and let ?(i, mi) = aibi if mi is odd and biai if mi is even. The crucial property of ? and
? is that a string w(i) can be parsed either as a sequence of ?(i, ?) or ?(i, ?) strings, not
intermixed elements. The grammar must ?commit? to parsing the string one way or the
other, corresponding to committing to a value for the variable vi.
We define a TL-MCTAG(2) G to consist of the tree sets described herein. We con-
struct: (1) a tree set of length two for each combination of a variable and clause that the
variable can satisfy under some truth assignment, (2) two filler tree sets for each variable
(one for each truth assignment) of length two that only contribute the string indicating
the truth assignment of the variable but no satisfied clause, and (3) a singleton tree
set containing only an initial tree rooted in S. The initial tree has n + 1 branches with
the first branch intended to yield the prefix string w(1) ? ? ?w(p) and the (k + 1)-st branch
intended to yield ck where 1 ? k ? n. Although it is possible to generate strings not of
the form of w using this construction, given a pair ?G, w? where w respects the definition
above, we show that w ? L(G) if and only if C is satisfiable.
The initial tree set S contains the single tree pictured in Figure 4.5 The name of each
link in the initial tree set is composed of three indices that indicate the role of the link.
The first index, i, corresponds to variable vi. The second is an index into the series 1 ? ? ?mi
where mi is defined from vi as described previously. The third index, j, corresponds to a
clause cj. The use of multiple indices to name the links is for clarity only. They may be
renamed freely.
5 Although we permit the presence of multiple links at a single node in the S tree, we follow the usual TAG
convention of disallowing multiple adjunction. If one of the links at a node is used, the other links at that
node are assumed to be unavailable in the derivation.
453
Computational Linguistics Volume 36, Number 3
Figure 4
The start tree for TL-MCTAG(2) grammar G. The multiply-indexed link numbers are for clarity
only and are treated as simple link names.
For every variable vi, 1 ? i ? p, and index h, 1 ? h ? mi:
 if h ? |Ai|, tree set T(h)+i contains the following two trees:
 if h > |Ai|, tree set F(h)+i contains the following two trees:
454
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
 for all h, tree set T(h)?i contains the following two trees:
 for all h, tree set F(h)?i contains the following two trees:
An illustrative example is provided in Figure 5. In this example we demonstrate
derivations of two possible satisfying truth assignments for Boolean formula (x ? y ?
z) ? (x ? y ? z) ? (y ? y ? z). The truth assignments correspond to whether the T or F tree
sets are used in the derivation of the prefix string for a particular variable. As can be seen
from the example, the structure of the prefix string enforces the requirement that either
all T tree sets or all F tree sets are chosen for a particular variable. Each tree set marked
with a + is used to satisfy a single clause. Which clause a tree set satisfies can be read
off the link number at which it adjoins.
Inspection of the grammar and construction of the input string show that |G| and
|w| are polynomially related to p and n. The sum of the mi is maximally 3n. There are no
Figure 5
Example derivations of two satisfying assignments for the boolean formula
(x ? y ? z) ? (x ? y ? z) ? (y ? y ? z).
455
Computational Linguistics Volume 36, Number 3
more than 9pn + 1 tree sets and no more than 18pn + 1 total trees. The size of the initial
tree is bounded by 3pn and all other trees have constant size.
From a derivation of w ? L(G) we can find a truth assignment satisfying C by
examining the derivation. If the tree sets T(h)+i or T
(h)?
i are children of S for some i
and all h where 1 ? i ? p and 1 ? h ? mi, then vi is true. If the tree sets F(h)+i or F
(h)?
i
are children of S for some i and all h where 1 ? i ? p and 1 ? h ? mi, then vi is false.
By the construction, if w is of the form just described, for a given variable vi only
two derivations of w(i) will be possible, one in which all tree sets corresponding to
that variable are T tree sets and one in which all are F tree sets. Starting from a truth
assignment that satisfies C, we can prove that w ? L(G) by induction on |V|.
That this problem is in NP can be seen from the same reasoning as in the proof of
Theorem 2. 
3.3 Universal Recognition of TL-MCTAG with Bounded Rank is NP-Complete
We now show that universal recognition of TL-MCTAG is NP-complete even when the
rank is bounded.
We briefly recall here the definition of a decision problem called 3PAR. Let t and si ?
t be positive integers, 1 ? i ? 3m, m ? 1. The language 3PAR is defined as the set of all
tuples ?s1, . . . , s3m, t?, satisfying the following condition: The multiset Q = {s1, . . . , s3m}
can be partitioned into multisets Qi, 1 ? i ? m, such that for every 1 ? i ? m, |Qi| = 3
and
?
s?Qi s = t.
Language 3PAR is strongly NP-complete (Garey and Johnson 1979). This means that
3PAR is NP-complete even in case the integers si are all represented in unary notation.
Theorem 4
The universal recognition problem for TL-MCTAG with rank 1 and unbounded fan-out
is NP-complete.
Proof
We provide a reduction from 3PAR.6 Let ?s1, . . . , s3m, t? be an input instance of the 3PAR
problem, with all of the integers si represented in unary notation. Our target grammar G
is defined as follows. We use a set of nonterminal symbols {S, A}, with S being the start
symbol. We take the set of terminal symbols to be {a, $}. G contains two elementary
tree sets. The first set has a single elementary tree ?, corresponding to a context-free
production of the form S ? (AAA$)m?1AAA:
6 We follow the proof strategy of Barton (1985) in this proof.
456
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
Tree ? has a unique link impinging on all of the 3m occurrences of nonterminal
A. The second (multi)set of G contains elementary trees ?i , 1 ? i ? 3m. Each ?i corre-
sponds to a context-free production of the form A ? asi :
We also construct a string w = (at$)m?1at.
If there exists a partition for multiset Q = {s1, . . . , s3m} satisfying the 3PAR require-
ment, we can directly construct a derivation for w in G, by sorting the elementary trees in
the second set accordingly, and by inserting these trees into the link of the elementary
tree ?. Conversely, from any derivation of w in G, we can read off a partition for Q
satisfying the requirement for membership in 3PAR for the input instance of the 3PAR
problem.
Finally, it is easy to see that G and w can be constructed in linear deterministic time
with respect to the size of the input instance of the 3PAR problem.
That this problem is in NP can be seen from the same reasoning as in the proof of
Theorem 2. 
3.4 Universal Recognition of TL-MCTAG with Fixed Input String is NP-Complete
We now show the unusual complexity result that universal recognition of TL-MCTAG is
NP-complete even when the input string is fixed. Although it is uncommon to require
this result, we rely on it in Section 5 to demonstrate that our parser has better time
complexity than the baseline parsing method for TL-MCTAG that we generalize from
the standard parsing method for LCFRS-equivalent formalisms.
We reduce from a variant of the 3SAT problem introduced above in which each
variable occurs in at most four clauses with no repeats in a clause. This problem was
shown to be NP-complete by Tovey (1984).
Theorem 5
Universal recognition of TL-MCTAG is NP-complete when the input string is fixed.
Proof
Let ?V, C? be an arbitrary instance of the 3SAT problem where each variable occurs in
no more than four clauses and does not repeat within a single clause. As in the proof of
Theorem 1, we use the derivations of the grammar to guess the truth assignments for
V and use the tree sets to keep track of the dependencies among different clauses in C.
Two tree sets are constructed for each variable, one corresponding to a true assignment
and one corresponding to a false assignment. The prohibition on multiple adjunction
ensures that only one of these two tree sets can be used for each variable. The tree set of
a particular truth assignment for a particular variable vi makes it possible to satisfy the
obligatory adjunction constraints for the nonterminal symbols representing each of the
clauses that vi satisfies in the 3SAT formula.7 Additional adjunction sites for each clause
7 Obligatory adjunction constraints are standard in the definition of TAG and MCTAG (Joshi, Levy, and
Takahashi, 1975; Weir 1988). However, obligatory adjunction may be avoided in this proof by creating a
457
Computational Linguistics Volume 36, Number 3
provide overflow space in the event that more than one variable satisfies a particular
clause. We fix the input string w to be the empty string. None of the trees of the grammar
contain any terminal symbols. However, a successful parse of the empty string can only
be achieved if all of the obligatory adjunction constraints are satisfied and this occurs
if and only if all clauses of the formula are satisfied by the truth assignment to some
variable.
We define a tree-local MCTAG G containing the following tree sets. We notate
obligatory adjunction constraints by underlining the nodes at which they apply. The
initial tree set S contains the single tree:
For every variable vi, 1 ? i ? p, tree set Ti (respectively, Fi) is used when represent-
ing an assignment of the value true (respectively, false) to vi. Ti (respectively, Fi) contains
at most five trees, one for the variable itself and one for each clause cj, 1 ? j ? n, such
that when vi is true (respectively false) cj is satisfied. More formally, tree set Ti contains
trees Vi? and Cj? if and only if vi ? cj, for 1 ? j ? n. Tree set Fi contains trees Vi? and Cj?
if and only if vi ? cj, for 1 ? j ? n.
Note that the diagram of the initial tree does not show the explicitly notated link
locations that we have used throughout the article. We omit the link locations to avoid
cluttering the diagram. However, because each variable occurs at most four times in the
formula, the total number of links is bounded by pn12.
From the definition of G it directly follows that ? ? L(G) implies the existence of a
truth assignment that satisfies C. A satisfying truth assignment can be read directly off
of any derivation tree for w. If Ti (respectively, Fi) is a child of S in the derivation tree,
then vk is true (respectively, false). The converse can be shown by using a satisfying
truth assignment for C to construct a derivation for w ? L(G).
?G, w? can be constructed in deterministic polynomial time because the number of
tree sets in the grammar is 2p + 1, the total number of trees in the grammar is bounded
by n(2p + 1), and the length of w is 0. All trees in the grammar have constant size except
for the initial tree, which has size 3n + p.
That this problem is in NP can be seen from the same reasoning as in the proof of
Theorem 2. 
larger grammar in which a separate tree set is created for each combination of clauses that may be
satisfied by a given variable. Because each variable may appear in no more than four clauses, this
increases the number of tree sets in the grammar by 24. We leave the details of this alternative proof
strategy to the reader.
458
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
4. An Alternative Definition of TL-MCTAG: Tree Vectors
The proof of NP-hardness of TL-MCTAG in the bounded rank case given in Theorem 4
depends crucially on the treatment of the elementary structures of the TL-MCTAG as
unordered sets. In order to produce the satisfying partitions for the 3PAR problem, any
tree from the second tree set must be able to adjoin at any location of link 1 in the first
tree set. This is in accordance with the usual definition of multi-component TAG. An
alternative definition of multi-component TAG in which the elementary structures are
treated as vectors is suggested by the explicit use of numbered links at the available
adjunction sites. Under this definition, each location of a link is also given an index and
only the tree at that index in a given vector may adjoin at that link location. An example
contrasting the two definitions is given in Figure 6.
The dependence of our bounded-rank proof on the set definition of TL-MCTAG
does not in itself show that vector-definition TL-MCTAG is polynomial in the bounded
rank case. We show this constructively in Section 5 by presenting a parser for vector
definition TL-MCTAG for which the polynomial degree of both the length of the in-
put string and the grammar size is constant when the rank of the input grammar is
bounded.
The difference in complexity between the set and vector definitions of TL-MCTAG
makes the vector definition an appealing possibility for research using TL-MCTAG for
natural language applications. Although all uses of TL-MCTAG in the computational
linguistics literature assume the set definition of TL-MCTAG, the linguistic analyses
therein do not require the additional flexibility provided by the set definition (Kallmeyer
and Joshi 2003; Nesson and Shieber 2006, 2007; Kallmeyer and Romero 2007; Nesson
2009). This is not a coincidence. Multicomponent tree sets are generally used to model
syntactic and semantic constructs in which one tree in the set strictly dominates another
and has a different syntactic or semantic type: for instance, a quantifier and its bound
variable. The locations at which the trees in these sets adjoin are not interchangeable
both because of the dominance constraint and because of the difference in type (and,
correspondingly, root node label). As a result, these grammars may be converted to the
Figure 6
An example contrasting the set definition of MCTAG (shown in Figure 2) with the vector
definition.
459
Computational Linguistics Volume 36, Number 3
Figure 7
The deductive rule generated for tree ? using the naive TAG parsing method.
vector definition without any change in the elementary trees, the generated language, or
grammar size but with crucial gains in the worst case bounds on processing efficiency.8
5. Parsing
Although no algorithms for parsing TL-MCTAG have previously been published, the
standard method for parsing LCFRS-equivalent formalisms can be applied directly to
TL-MCTAG to produce an algorithm with complexity O(|G|p|w|q) (Seki et al 1991). We
offer a novel parser for TL-MCTAG for which q is constant. With our algorithm, for
the set definition of TL-MCTAG p depends on both the rank and fan-out of the input
grammar. For the vector definition of TL-MCTAG p depends on the rank of the input
grammar but contains no index of the fan-out.
We begin with a brief introduction to TAG parsing before discussing our novel TL-
MCTAG parsing algorithm.
5.1 CKY-Style TAG Parsing
Following the method of Seki et al (1991), a naive parser for TAG may be constructed
by generating a single inference rule for each tree in the grammar. For a tree containing
r links, the rule will have r antecedents with each antecedent item representing a tree
that can adjoin at one of the links. Each adjoining tree will cover a span of the input
string that can be represented by four indices, indicating the left and right edges of the
span and of the subspan that will ultimately be dominated by its foot node. Because the
location of the links within the consequent tree is known, the indices in the antecedent
items are not entirely independent. An example is given in Figure 7. Observation shows
that there will be a worst case of 2(r + 1) independent indices in a given rule. Because
each adjoining tree is independent, there may be r + 1 different trees represented in a
single rule. This results in a time complexity of O(n2(r+1)|G|r+1) where n is the length
of the input string, |G| is a representation of the grammar size, and r is the rank of the
input grammar.
8 Various sorts of multicomponent TAGs have been proposed for analysis of scrambling (Rambow 1994;
Kallmeyer 2005). Scrambling entails several different trees of the same type adjoining in different orders,
and therefore seems like a candidate for making use of the flexibility provided by the set definition.
However, in these analyses the elementary tree structures are composed of one VP-rooted auxiliary tree
and one VP-rooted initial tree. Because auxiliary trees and initial trees cannot adjoin at the same link
location for structural reasons, these analyses do not ultimately make use of the flexibility in the selection
of an adjunction site that the set definition provides. The different VP-rooted auxiliary trees which could
benefit from interchanging adjunction sites achieve this flexibility because they appear in different tree
sets, not because they are members of a single set using the set definition.
460
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
Following Graham, Harrison, and Ruzzo (1980) in their optimization of the Earley
parser (Earley 1970), the identifiers of specific trees need not be represented in the items
of the parser. Rather the tree identifiers may be replaced by the labels of the root nodes
of those trees, effectively bundling items of trees that share a root node label and cover
the same span. This modification to the algorithm reduces the time complexity of the
parser to O(n2(r+1)|G|).
We refer to this method of reducing complexity by removing unnecessary informa-
tion about specific elementary structures from the items of the parser as the GHR opti-
mization. When applied, it reduces the time complexity in the grammar size but does
not alter the basic form of the time complexity expression. There remains a single term
consisting of the product of a polynomial in the input string length and a polynomial in
the grammar size. We will return to this observation when examining the complexity of
TL-MCTAG parsing.
Shieber, Schabes, and Pereira (1995) and Vijay-Shanker (1987) apply the Cocke-
Kasami-Younger (CKY) algorithm, first introduced for use with CFGs in Chomsky
normal form (Kasami 1965; Younger 1967), to the TAG parsing problem to generate
parsers with a time complexity of O(n6|G|2). The speed-up in the parser comes from
traversing elementary trees bottom-up, handling only one link at a time. As a result, no
inference rule needs to maintain information about more than one link at a time. If the
GHR optimization is applied, the time complexity is reduced to O(n6|G|).
In order to clarify the presentation of our TL-MCTAG parser, we briefly review the
algorithm of Shieber, Schabes, and Pereira (1995) with minor modifications, using the
deductive inference rule notation from that paper. As shown in Figure 8, items in CKY-
style TAG parsing consist of a node in an elementary tree and the indices that mark
the edges of the span dominated by that node. Nodes, notated ?@a  , are specified by
three pieces of information: the identifier ? of the elementary tree the node is in, the
Gorn address a of the node in that tree,9 and the link  available at that node if there
is one. When no link is present, it is indicated by an underscore, . The node notation
?@a   may be read as ?node ? at address a with link ?.
Each item has four indices, indicating the left and right edges of the span covered by
the node as well as any gap in the node that may be the result of a foot node dominated
by the node. The indices are constrained to be non-decreasing from left to right in an
item. Nodes that do not dominate a foot node will have no gap in them, which we
indicate by the use of underscores in place of the indices for the gap. To limit the number
of inference rules needed, we define the following function i ? j for combining indices:
i ? j =
?
?
?
?
?
?
?
i j =
j i =
i i = j
undefined otherwise
The Adjoin rule has two indices, p and q, that appear in the antecedent but not in
the consequent. These indices specify the gap in one antecedent item and the edges of
the span in the other antecedent item, indicating that one antecedent item will fill the
gap in the span of the other antecedent item. The Foot Axiom similarly makes use of
unbound indices p and q. In this rule the entire span of the item is the gap that must be
9 A Gorn address uniquely identifies a node within a tree. The Gorn address of the root node is ?. The jth
child of the node with address i has address i ? j.
461
Computational Linguistics Volume 36, Number 3
Figure 8
The CKY algorithm for binary-branching TAG.
filled when the item adjoins to another item. As noted in Shieber, Schabes, and Pereira
(1995), the parser can be made more efficient by only introducing foot items of this sort
once an appropriate tree to adjoin into has been parsed for the span from p to q.
Each item of the form ??@a  , i, , , l? maintains the invariant that the input gram-
mar can derive a subtree rooted at ?@a with no foot node that spans wi+1 . . .wl. Items
of the form ??@a  , i, j, k, l? maintain the invariant that the input grammar can derive
a subtree rooted at ?@a with a foot node such that the fringe of the tree is the string
wi+1 . . . wjLabel(Foot(?))wk+1 . . . wl. The invariants for items of the form ??@a  , i, , , l?
and ??@a  , i, j, k, l? are similar except that no adjunction operation may occur at ?@a.
The side conditions Init(?) and Aux(?) hold if ? is an initial tree or an auxiliary
tree, respectively. Label(?@a) specifies the label of the node in tree ? at address a. Ft(?)
specifies the address of the foot node of tree ?. Link(?@a) specifies the link available at
node ?@a if there is one and null (represented as in the inference rules) otherwise.
Adj(?@a  , ?) holds if  is a link at which tree ? may adjoin into tree ? at address a.
Subst(?@a  , ?) holds if  is a link at which tree ? may substitute into tree ? at address
a. If  is null or the adjunction or substitution is prevented by other constraints such as
mismatched node labels, these conditions fail.
462
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
Figure 9
The deductive rule generated for tree ? using the baseline TL-MCTAG parsing method.
Consistent with the usual definition of TAG, only one link is permitted at a given
node. This effectively rules out multiple adjunction. Bounded multiple adjunction may
be permitted without affecting the complexity of the parsing algorithm by allowing a
list of links at a node. Although it first appears that the introduction of multiple links at
a single node could result in an exponential increase in the number of derivations, this
is not the case. The link diacritics themselves carry no information about the trees which
may adjoin at the associated adjunction site. Any restrictions, such as the requirement
of a matching node label, arise from the node itself. As a result, the links are fully
interchangeable and serve only as counters of the number of available adjunctions at a
node.10
5.2 CKY-Style Tree-Local MCTAG Parsing
As shown in Figure 9, the naive algorithm for parsing TAG may also be applied to
TL-MCTAG. The only difference is that each link may have multiple locations within a
given tree. Let r and f represent the rank and fan-out of the input grammar, respectively.
The time complexity of the naive parser will therefore be O(n2(rf+1)|G|r+1). However,
the GHR optimization cannot straightforwardly be applied because the maintenance of
tree locality requires items to carry information about the identities of the specific trees
involved rather than just the labels of the root nodes. Theorem 5 addresses the case in
which the input string length is 0. Therefore, in this case, any factor in the complexity
including the input string length cannot contribute to the overall time complexity. By
showing that the problem is NP-complete when the input string length is 0, Theorem 5
demonstrates that there must be some exponential factor or term in the time complex-
ity expression other than the input string length factor. Due to the earlier observation
that the GHR optimization does not change the form of the time complexity expression,
Theorem 5 therefore shows that the GHR optimization cannot reduce the exponent
of the grammar size term to a constant unless P = NP. This leaves open the possibility
of the existence of an algorithm that is polynomial in the grammar size but has an addi-
tional exponential term in the time complexity expression. However, such an algorithm,
if it exists, cannot be generated by application of the GHR optimization to the baseline
parser.
We can generalize the CKY TAG parsing algorithm presented above to the TL-
MCTAG case. This is an improvement over the standard LCFRS algorithm because it
reduces the q in the |w|q factor of the complexity to a constant. The direct specification
10 Note, however, that the finite length of the lists of links is necessary for multiple adjunction to remain
benign.
463
Computational Linguistics Volume 36, Number 3
of a CKY-style tree-local MCTAG parser is given in Figures 10 and 11. For a tree set or
vector ? from G, we notate the trees in the set or vector using indices that are indicated
as subscripts on the tree set identifier. A tree set or vector ? from G with length two will
therefore contain trees ?1 and ?2. Under the set definition these indices serve only as a
way of differentiating the members of the tree set. Under the vector definition, the index
must match the index of the link location where the tree will adjoin.
In order to directly parse tree-local MCTAG, items must keep track of the trees that
adjoin at each multicomponent link. We handle this by adding a link history to each
item. Under the set definition, a link history is an associative array of links notated with
indices and tree set identifiers notated with indices to identify a unique tree within the
set. Note that because under the set definition a tree may adjoin at any location of a
link, the indices of the link and tree set need not match. The axioms introduce empty
link histories, indicating that no adjunctions have yet occurred. When an adjunction
takes place, the tree identifier of the adjoining tree is associated with the link at which it
adjoins. In order for an adjunction to take place at a multicomponent link, the adjoining
tree?s tree set must be the same as that of any tree identifier already stored for that
link. This is enforced by the Valid(?) condition (Figure 12) defined on link histories. The
Filter(?, ?@a  ) function removes links that are completely used from the argument
link history. An empty link history indicates that tree locality has been enforced for the
subtree specified by the item; thus no additional information need be maintained or
passed on to later stages of the parse.
For the vector definition, the link histories may be simplified because each location
of a link fully specifies which tree from within a vector may adjoin there. As a result, the
link history is an associative array of links (not annotated with indices) and tree vector
identifiers. An example contrasting the link histories for the set and vector definitions
is given in Figure 13.
Figure 10
Modified item form, goal, and axioms for the CKY algorithm for tree-local MCTAG. Inference
rules of the algorithm are given in Figure 11.
464
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
Figure 11
Modified inference rules for the CKY algorithm for tree-local MCTAG. Alternative Adjoin,
Substitute, and No Adjoin rules are given for the set and vector definitions of TL-MCTAG. The
item form, goal item, and axioms are given in Figure 10.
The addition of a link history to each item increases the complexity of the algorithm.
The maximum link history length is bounded by the rank of the input grammar, r.
Under the set definition, the number of possible values for each element of a link history
is on the order of the number of tree sets in the grammar multiplied by the power
set of the fan-out: |G| ? 2f . Thus, for the set definition, the complexity of the algorithm
465
Computational Linguistics Volume 36, Number 3
Figure 12
Definition of the Valid condition, which ensures that all locations of a link are used by unique
trees from the same tree set. Under the set definition there is an entry for each link location and
both the identity of the tree set and the uniqueness of the tree from that tree set must be checked.
Under the vector definition only the link name and the tree vector identifier are stored because
the link locations uniquely select trees from within tree vectors.
Figure 13
A sample TL-MCTAG with examples of the possible link histories under the set and vector
definitions when the parser reaches the top of the circled node. Although the tree sets are
notated in set definition, the reader may substitute angle braces to get the corresponding vector
definition items.
is O(n6|G|r+22rf ). Under the vector definition, the number of possible values for each
element of a link history is on the order of the number of tree sets in the grammar. Thus,
for the vector definition, the complexity of the algorithm is O(n6|G|r+2). Note that the
variable representing fan-out, f , is present only in the complexity of the set definition.
This demonstrates the novel result that when rank is bounded, even with unbounded
fan-out, parsing the vector definition of TL-MCTAG is polynomial.
Permitting multiple adjunction may be accomplished by a method similar to the
one described for the TAG algorithm. Rather than associating each node with at most
one link, we permit nodes to be accompanied by a set of links. In contrast to the
TAG case, here we must use a set rather than a list to allow for the expressivity that
multiple adjunction can provide. In the TAG case a list is sufficient because the links
at a node are fully interchangeable. In the TL-MCTAG case, because the links are
defined not just by the node where they appear but by the full set of nodes at which
locations of that link appear, the links at a given node are not interchangeable. It must
be possible to use them in any order.11 Because the links can be used in any order, the
11 For links that share all locations it is still possible to enforce a strict order over them without
compromising expressivity.
466
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
addition of multiple adjunction adds a factor of 2r to the time complexity of the parsing
algorithm.
6. Link Factorization
The parser presented in the previous section has the advantage of running in polyno-
mial time if the elementary structures of the input TL-MCTAG are defined as vectors
and if the rank of the grammar is bounded by some constant. Bounding the rank by
a constant might be too strong a limitation in natural language parsing applications,
however. Thus, in the general case the running time of our algorithm contains a factor
that is an exponential function of the rank of the input grammar. To optimize parsing
time, then, we seek a method to ?factorize? the elementary trees of the grammar in
such a way that the rank is effectively reduced and the set of derived trees is preserved.
Although the precise meaning of factorization should be inferred from the subsequent
definitions, informally, by factorize we mean splitting a single elementary tree into sev-
eral smaller elementary trees without violating the locality constraints of the grammar
formalism. In this section we present a novel and efficient algorithm for factorizing a
TL-MCTAG into a strongly equivalent TL-MCTAG in which rank is minimized across
the grammar. Here, strongly equivalent means that the two grammars generate the
same set of derived trees.12
6.1 Preliminaries
Let ? be some elementary tree. We write |?| to denote the number of nodes of ?. For a
link l, we write |l| to denote the number of nodes of l.
For an elementary tree ?, we call a fragment of ? a complete subtree rooted at some
node n of ?, written ?(n), or else a subtree rooted at n with a gap at node n? in its
yield, written ?(n, n?). See Figure 14 for an example. We also use ? to denote a generic
fragment with or without a gap node in its yield.
Consider some fragment ? of ?. Let N? be the set of all nodes of ? and let N? be the
set of nodes of ? with the exclusion of the gap node, in case ? has such a node. We say
that ? is an isolated fragment iff ? includes at least one link and no link in ? impinges
both on nodes in N? and on nodes in N? ? N?. Figure 14 provides an example.
Intuitively, we can ?excise? an isolated fragment from ? without splitting apart
the links of ? itself, and therefore preserving the tree locality. This operation may also
reduce the number of links in ?, which is our main goal. The factorization algorithm we
present in Section 6.2 is based on the detection and factorization of isolated fragments.
Let n be a node from some elementary tree ?. We write lnodes(n) to denote the set
of all nodes from fragment ?(n) that are part of some link from ?. Node n is maximal if
 lnodes(n) = ?; and
 n is either the root node of ? or, for its parent node n?, we have
lnodes(n?) = lnodes(n).
12 The trees are not actually the same because of the small, reversible transformation that we make to ensure
that the factorized trees obey the TAG constraint that auxiliary trees must have matching root and foot
node labels. This transformation adds additional nodes into the tree structure but does not change the
shape of the trees and can be reversed to produce trees that are actually the same as the derived trees of
the original grammar.
467
Computational Linguistics Volume 36, Number 3
Figure 14
An elementary tree ? demonstrating fragments, isolation, and maximal nodes. Fragment
?1 = ?(n1, n2) contains all locations of links 2 and 3 , because links at the root node of a
fragment are contained within that fragment. It does not contain any locations of link 4 , because
links at the gap node of a fragment are not contained within that fragment. Because links 2 and
3 impinge only on nodes in ?1 and all other links impinge only on nodes not in ?1, ?1 is an
isolated fragment. Fragment ?2 = ?(n4) is not an isolated fragment because it contains only one
of the link locations of 4 . Note also that n4 is a maximal node but n5 is not.
Note that for every node n? of ? such that lnodes(n?) = ? there is always a unique
maximal node n such that lnodes(n?) = lnodes(n) (see Figure 14). Thus, for the purpose
of TL-MCTAG factorization, we can consider only maximal nodes. The first criterion
in the definition of maximal node, stating that a maximal node always dominates
(possibly reflexively) some node involved in a link, will often be implicitly used in the
following.
We need to distinguish the nodes in lnodes(n) depending on their impinging links.
Assume that {l1, l2, . . . , lr} is the set of all links occurring in ?. For 1 ? j ? r, we write
lnodes(n, lj) to denote the set of all nodes from fragment ?(n) with impinging link lj.
Thus,
?r
j=1 lnodes(n, lj) = lnodes(n). We associate with each maximal node n of ? a
signature ?(n), defined as a vector of size r and taking values over the subsets of
lnodes(n). For each j, 1 ? j ? r, we define
?(n)[j] =
?
?
?
lnodes(n, lj), if 0 < |lnodes(n, lj)| < |lj|;
?, if |lnodes(n, lj)| = 0 or
|lnodes(n, lj)| = |lj|.
Observe that, in this definition, ?(n)[j] = ? means that none or all of the nodes of lj are
found within fragment ?(n). The empty signature, written 0, is the signature with all of
its components set to ?.
Consider maximal nodes n1 and n2 such that n1 = n2, ?(n1) = 0, and ?(n2) = 0.
It is not difficult to see that ?(n1) = ?(n2) always implies that one of the two nodes
dominates the other. This observation is implicitly used in several places subsequently.
When visiting nodes of ? in a path from some leaf node to the root node,13 one
may encounter several maximal nodes having the same non-empty signature. In our
factorization algorithm, we need to consider pairs of such nodes that are as close as
possible. Consider two maximal nodes n1 and n2, n1 = n2, such that n1 dominates n2.
13 We view trees as directed graphs with arcs directed from each node to its parent.
468
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
The ordered pair (n1, n2) is called a minimal pair if ?(n1) = ?(n2) = 0 and, for every
maximal node n3 in the path from n2 to n1 with n3 = n1 and n3 = n2, we have ?(n3) =
?(n1). Consider now a sequence ?n1, n2, . . . , nq?, q ? 2, of nodes from ?. Such a sequence
is called a maximal chain if each pair (ni?1, ni) is a minimal pair, 2 ? i ? q, and all nodes
n from ? with ?(n) = ?(n1) are included in the sequence itself.
Notice that two maximal nodes belonging to two different maximal chains must
have different signatures, and thus one maximal node cannot belong to more than one
maximal chain. We now prove some basic properties of the notions just introduced that
will be used later in the development of our factorization algorithm and in the proof of
some of its mathematical properties.
Lemma 1
Let ? be an elementary tree and let n, n? be maximal nodes, with n properly dominating
n? in (ii).
(i) ?(n) = 0 if and only if ?(n) is an isolated fragment;
(ii) ?(n) = ?(n?) if and only if ?(n, n?) is an isolated fragment.
Proof
(i). If ?(n) = 0, then for each link l we have that either all nodes impinged on by l are
dominated (possibly reflexively) by n or none of these nodes is dominated by n. Because
n is maximal, we further conclude that at least some link l is found within ?(n).
Conversely, if ?(n) is an isolated fragment then all or none of the nodes impinged
on by some link l are dominated by n, and thus ?(n) = 0.
(ii). Let ?(n) = ?(n?), with n properly dominating n?. For each link lj, there are two
possible cases. First consider the case where ?(n)[j] = ?(n?)[j] = ?. In order for this to be
true, the link must be in one of three configurations, all of which satisfy the requirement
that the locations of lj must be all inside or all outside of the fragment ?(n1, n2).
 lnodes(n, j) = ?. In this configuration no one of the nodes on which lj
impinges is dominated by n.
 |lnodes(n, j)| = |lj|. We distinguish two possible cases.
? lnodes(n?, j) = ?. In this configuration all the nodes on which lj
impinges are within the fragment ?(n1, n2).
? |lnodes(n?, j)| = |lj|. In this configuration all the nodes on which lj
impinges are ?below? the fragment ?(n, n?).
Now consider the case where ?(n)[j] = ?(n?)[j] = ?. The nodes in lnodes(n?, j) are dom-
inated (possibly reflexively) by n? and therefore fall ?below? ?(n, n?). The remaining
nodes on which lj impinges cannot be dominated (possibly reflexively) by n. We thus
conclude that no nodes impinged on by lj occur within the fragment ?(n, n?).
Assume now that ?(n, n?) can be isolated. We can use exactly the same arguments
in the analysis of sets lnodes(n, j) and lnodes(n?, j), and conclude that ?(n) = ?(n?). 
The next lemma will be useful later in establishing that the factorization found by
our algorithm is optimal, namely, that it achieves the smallest rank under the imposed
conditions.
469
Computational Linguistics Volume 36, Number 3
Lemma 2
Let (n1, n2) be some minimal pair. Then
(i) for any node n3 in the path from n2 to n1, ?(n3) = 0;
(ii) for any minimal pair (n3, n4), neither or both of n3 and n4 are found in the
path from n2 to n1.
Proof
(i). Because ?(n2) = 0, there is some link lj for which ?(n2)[j] = lnodes(n2, j) = ?. Be-
cause n3 dominates n2, n3 dominates the nodes in lnodes(n2, j). Therefore, the only way
?(n3) could equal 0 is if |lnodes(n3, j)| = |lj|. But then ?(n1)[j] = ? because n1 dominates
n3. This is a contradiction.
(ii). Assume that n4 is on the path from n2 to n1. From the definition of minimal
pair, there must exist a link lk such that ?(n4)[k] = ?(n2)[k]. By the same reasoning
as in the proof of statement (i) for any link lj such that ?(n2)[j] = ?, we must have
?(n2)[j] = ?(n4)[j] = ?(n1)[j]. We thus conclude that ?(n2)[k] = ? and ?(n4)[k] = ?. Be-
cause ?(n4)[k] = ?(n3)[k] = ? and ?(n2)[k] = ?(n1)[k] = ?, node n3 must be in the path
from n2 to n1.
By a similar argument, we can argue that if n3 is on the path from n2 to n1, then
node n4 must be in that path as well. 
6.2 Factorization Algorithm
Let G be an input TL-MCTAG grammar. In this subsection we provide a method for the
construction of a TL-MCTAG that produces a grammar that generates the same derived
trees as G and that has minimal rank. We start with the discussion of some preprocessing
of the input.
We annotate each elementary tree ? as follows: We compute sets lnodes(n, lj)
for all nodes n and all links lj of ?. This can easily be done with a bottom up
visit of ?, by observing that if an internal node n has children n1, n2, . . . , nk then
lnodes(n, lj) =
?k
i=1 lnodes(ni, lj) ? Xj, where Xj = ? if lj does not impinge on n and Xj =
{n} if it does. Using sets lnodes(n, lj), we can then mark all nodes n in ? that are maximal,
and compute the associated signatures ?(n).
We also mark all maximal chains within ?. This simple procedure is reported in
Figure 15. We maintain an associative array with node signatures as entries and node
lists as values. We visit all maximal nodes of ? in a top?down fashion, creating a list for
each different signature and appending to such a list all nodes having that signature.
In the following algorithm we excise isolated fragments from each elementary tree
?. We now introduce some conventions for doing this. Although it would be possible to
Figure 15
Construction of maximal chains in the factorization algorithm.
470
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
excise fragments without the introduction of additional tree structure, we adopt instead
two simple tree transformations that preserve auxiliary tree root and foot label matching
and result in some simplification of the notation used by the algorithm, particularly in
case the root node of a fragment is the same as the gap node of a second fragment within
?. A schematic depiction of both transformations is given in Figure 16.
When a fragment ?(n) is excised, we leave a copy of the root node n without its
impinging links that dominates a fresh node n? with a fresh link indicating obligatory
substitution of the excised fragment. The excised fragment consists of ?(n) including
any links impinging on n, but has a fresh root node immediately dominating n with the
same label as n?. This is shown in the top row of Figure 16.
A similar transformation is used to excise a fragment ?(n, n?). Nodes n and n? of
the original tree are not altered, and thus they retain their names. The material between
them is replaced with a single new node with a fresh nonterminal symbol and a fresh
link. This link indicates the obligatory adjunction of the excised fragment. A new root
and gap node are added to ?(n, n?) to form the excised fragment. This is shown in the
bottom row of Figure 16. We remark that any link impinging on the root node of the
excised fragment is by our convention included in the excised fragment, and any link
impinging on the gap node is not.
To regenerate the original tree, the excised fragment ?(n, n?) can be adjoined back
into the tree from which it was excised. The new nodes that have been generated in
the excision may be removed and the original root and gap nodes may be merged back
together retaining any impinging links.
We need to introduce one more convention for tree excision. Consider a maximal
chain c = ?n1, n2, . . . , nq? in ?, q ? 2. In case q = 2, our algorithm processes c by excis-
ing a fragment ?(n1, n2) from ?, exactly as explained above. In case q > 2, a special
processing is required for c. Chain c represents q ? 1 minimal pairs, corresponding to
fragments ?(ni?1, ni), 2 ? i ? q. We do not excise these q ? 1 fragments one by one,
Figure 16
Diagrams of the tree transformations performed when fragments ?(n) and ?(n, n?) are removed.
471
Computational Linguistics Volume 36, Number 3
because this would create q ? 1 > 1 new links within ?. We follow instead a procedure
that ?binarizes? c, as explained here.
Let us recursively define an elementary tree ?c as follows, for |c| = q and q ? 3:
 In case q = 3, ?c is a tree composed of two nodes besides the root and the
gap nodes, n and n?, with n immediately dominating n?. Node n hosts
the (obligatory) adjunction of the fragment ?(n1, n2) and node n? hosts
the (obligatory) adjunction of ?(n2, n3). Both fragments are transformed
as previously discussed.
 In case q > 3, ?c is a tree composed of two nodes besides the root and the
gap nodes specified as above, with n? hosting the (obligatory) adjunction
of the transformed fragment ?(nq?1, nq). Node n hosts the adjunction of
tree ?c? , with c? = ?n1, . . . , nq?1?.
Note that each tree ?c has rank two.
When processing a maximal chain c with q > 2, the whole fragment ?(n1, nq) is
excised, using this convention. This results in a single fresh link added to ?. In this case
the link refers to the adjunction of a newly created elementary tree ?c, defined as above.
An example of the binarization of a maximal chain with q = 4 is reported in Figure 17.
We can now discuss the factorization algorithm, reported in Figure 18. For a maxi-
mal node n in an elementary tree ?, we write links(n) to denote the number of links from
? that are entirely contained in fragment ?(n). We process each tree set I? of the source
grammar and each elementary tree ? in I? as follows.
Figure 17
The binarization procedure applied to a maximal chain c = ?n1, n2, n3, n4?.
472
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
Figure 18
The factorization algorithm for tree-local MCTAG.
In the first phase, we add to an agenda A each maximal node n different from the
root of ? such that ?(n) = 0. We associate this agenda item with the score links(n). At
the same time, each maximal chain ?n1, n2, . . . , nq?, q ? 2, is added to A, with associated
score links(n1) ? links(nq).
In the second phase, we process all items in A, in order of increasing score, ignoring
those items that have a score of one. If the current item is a maximal node n, we excise
the fragment ?(n) from ?, leaving in place a fresh node with a single node link denoting
obligatory substitution. If the current item is a maximal chain of the form ?n1, n2?, we
excise from ? the fragment ?(n1, n2), leaving in place a fresh node with a single node
link denoting obligatory adjunction of the excised fragment. Finally, if the current item
is a maximal chain c = ?n1, . . . , nq? with q > 2, we excise from ? the whole fragment
?(n1, nq), and we apply to the chain the binarization procedure described in this sub-
section. This results in the addition to the output grammar of fragments ?(ni?1, ni), for
2 ? i ? q, and of newly created elementary tree ?c and elementary trees ?c? for each
chain c? that is a proper prefix of c. After the processing of all elementary trees in tree
set I? is completed, the resulting version of set I? is also added to the output grammar.
As a simple example of a run of the factorization algorithm, we discuss the process-
ing of the elementary tree ? depicted in Figure 19. Tree ? has four links, called li,
1 ? i ? 4. Link l1 impinges on nodes n11 and n12, link l2 impinges on nodes n21 and
n22. Links l3 and l4 impinge on a single node each, and the impinging nodes are called
n3 and n4, respectively. In Figure 19 we have outlined the maximal nodes n, n?, and n??
473
Computational Linguistics Volume 36, Number 3
Figure 19
An example tree to be processed by the factorization algorithm.
that are relevant to this example. Node n dominates both n? and n?? but none of n? and n??
dominates the other. Note that within ? there must exist maximal nodes other than n, n?,
and n??. For instance, there must be a maximal node dominating (possibly reflexively)
node n3 but not node n4. However, this node dominates a single link, and will not be
processed by the algorithm because of the requirement at line 12 in Figure 18. We thus
ignore this and other maximal nodes in what follows.
We have
lnodes(n?, l1) = {n11}, lnodes(n, l1) = {n11},
lnodes(n?, li) = ?, 2 ? i ? 4, lnodes(n, l2) = {n21, n22},
lnodes(n??, li) = ?, 1 ? i ? 2, lnodes(n, l3) = {n3},
lnodes(n??, l3) = {n3}, lnodes(n, l4) = {n4},
lnodes(n??, l4) = {n4},
and
?(n?) = [{n11}, ?, ?, ?],
?(n??) = 0,
?(n) = ?(n?).
The algorithm in Figure 15 will then mark the chain ?n, n??. When processing the elemen-
tary tree ?, the algorithm in Figure 18 will add to its agenda an item n?? with a score of
links(n??) = 2, as well as the above chain, with a score of links(n) ? links(n?) = 3 ? 0 = 3.
Node n?? is processed first, and fragment ?(n??) is excised from ? leaving in its place
a fresh link l5. Later on, the algorithm pops the chain ?n, n?? from the agenda, and
fragment ?(n, n?) is excised from ? leaving in its place a fresh link l6. The algorithm
then stops. The resulting factorization consists in fragment ?(n??) with links l3 and l4,
fragment ?(n, n?) with links l2 and l5, and what is left of the elementary tree ?, with
links l1 and l6.
The discussion of the correctness of the algorithm is reported in the next section,
along with some other mathematical properties.
474
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
6.3 Mathematical Properties
We discuss in this section some mathematical properties of our factorization algorithm.
Let G be the input TL-MCTAG and let G? be the output of the algorithm. We start with
the issue of correctness. First, notice that our algorithm stops after a finite number of
steps, because the number of possible excisions for G is finite. Assume now that ? and
?? are two isolated fragments within some elementary tree ?, and ?? is itself a fragment
within ?. It is easy to see that excising ?? from ? results in a new fragment of ? that is
still an isolated fragment. Using this observation together with Lemma 1, we can then
conclude that all fragments that are excised by the algorithm are isolated fragments.
This in turn implies that each fragment excision in our algorithm preserves tree locality,
and G? is still a TL-MCTAG.
Each fragment that is excised from some source tree must obligatorily be adjoined
back into that tree, at the point from which it was removed. Thus, G? generates the same
derived trees as G, modulo our trivial tree transformation for the root and the gap nodes.
This proves the correctness of our factorization algorithm.
One remark is in order here. Note that we always excise fragments that have at
least two links. This can be shown inductively as follows. Consider first the smallest
fragments that are excised from some elementary tree ?, that is, those fragments that
do not contain any other fragment within themselves. These fragments always have
at least two links, because of the requirement stated in line 12 in the algorithm. In the
inductive case, let ? be some fragment of ? from which a second fragment ?? has been
already excised in some iteration of the loop at lines from 11 to 23. Fragment ?? is thus
replaced by some link l?. Because of the definition of maximal node, ? must contain at
least one link l that is not contained in ??. In case l itself is part of some excised fragment
???, there will still be some other fresh link replacing ???. We thus conclude that, when
excised, ? always has at least two links. Because excised fragments always have at least
two links and since we never consider elementary trees as candidate fragments (line 6),
we can conclude that our algorithm always finds a non-trivial factorization of G.
We can now turn to an analysis of the computational complexity of our algorithm.
Consider an elementary tree ? of G with r links and with a maximum of f nodes per
link. In the preprocessing phase of the algorithm, the computation of sets lnodes(n, lj)
can be carried out in time O(|?| ? r ? f ). To see this, notice that there are no more than
|?| ? r such sets. Furthermore, we have |lnodes(n, lj)| ? f for each j, and each node in
lnodes(n, lj) is processed in constant time through the union operator, when constructing
the set lnodes(n?, lj) for the parent node n? of n. Clearly, O(|?| ? r ? f ) is also a time upper
bound for the computation of quantities ?(n) and links(n) for all nodes in ?, and for
extracting a list of the maximal nodes therein as well.14
In what follows, we will need to compare signatures of different nodes for equality.
Despite the fact that each signature has r elements, and each element of a signature is a
set with O( f ) elements, there are at most |?| different signatures. We can therefore use
an atomic symbol to name each signature (perfect hashing). In this way, signatures can
be compared in constant time.
The marking of all maximal chains within ?, as specified by the algorithm in
Figure 15, can be implemented in time O(|?|). This is done by encoding the associative
14 We remark here that a further improvement in efficiency could be achieved by replacing the sets of nodes
in a signature with the single node that is the least common ancestor of the set of nodes. However, using
the set of nodes substantially improves the clarity of the presentation of the algorithm, so we do not
pursue this optimization here.
475
Computational Linguistics Volume 36, Number 3
array L in the algorithm through a one-dimensional array indexed by signature names.
Each element of the array points to a linked list of nodes, representing a maximal chain.
We now analyze the running time of the factorization function in Figure 18. Let us
first consider a single elementary tree ?. We implement the priority queue A through
a heap data structure. The loops at lines 6 and 9 run in time O(|?| ? log(|?|)): This is
the standard result for populating a heap; see for instance Cormen et al (2001). At
each iteration of the while loop at lines 11 to 23, we extract some fragment ?(n) or
?(n1, nq). The processing of each such fragment ? takes an amount of time O(|?|),
where |?| is the number of nodes of ?. In such an iteration, ? needs to be re-edited
into a new elementary tree with the number of nodes |?| ? |?| + c, where c ? 3 is a
constant that depends on the specific transformation in Figure 16 that was applied in
the excision of the fragment tree. Nonetheless, if a suitable representation is maintained
for ?, making use of nodes and pointers, the re-editing of ? can be done in constant
time. Then a single iteration of the while loop takes time O(|?|), where ? is the excised
fragment. We can then conclude that all iterations of the while loop take an amount of
time O(|?| ? log(|?|)).15
Now let ?M be the elementary tree of G with the largest size, and let rG and fG be the
rank and fan-out of G, respectively. Putting everything together, the total running time
of the factorization algorithm is O(|G| ? (rG ? fG + log(|?M|))), where |G|, the size of the
input grammar, is defined as the sum of terms |?| for all elementary trees ? of G. Because
we always have fG ? |?M|, this upper bound can be rewritten as O(|G| ? |?M| ? rG).
A special case is worth discussing here. If the maximum number of links impinging
on a node of our elementary trees is bounded by some constant, we have rG ? fG =
O(|?M|). In this case, the above bound reduces to O(|G| ? |?M|). The constant bound on
the number of links impinging on the nodes of a grammar holds for all of the grammars
we have studied in Section 3.
We now argue that our algorithm provides the factorization G? of G with the
smallest possible rank, under the assumption that G and G? are strongly equivalent,
that is, that they generate the same derived trees.
A factorization f of G is called maximal if no one of its fragments has a smaller
isolated fragment within itself. We start by observing that the factorization of G found
by our algorithm is maximal. To see this, consider the excision by our algorithm of
a maximal chain ?n1, . . . , nq? within an elementary tree ?. This item is added to the
priority heap at line 10, with a score of links(n1) ? links(nq). This score is the number of
links found in fragment ?(n1, nq), with the exclusion of the links at the gap node nq.
The chain is then factorized into fragments ?(ni?1, ni), for each i with 2 ? i ? q. Assume
that some fragment ?(ni?1, ni) contains in turn a maximal chain ?n?1, . . . , n?q?? or else
an isolated fragment of the form ?(n?). In the first case we have links(n?1) ? links(n?q? ) <
links(n1) ? links(nq) and in the second case we have links(n?) < links(n1) ? links(nq).
Thus the smaller chain or fragment is processed earlier than our maximal chain, and
by the time our maximal chain is processed, the smaller chain or fragment has already
been excised. A similar argument shows that the excision by our algorithm of an isolated
fragment of the form ?(n) happens after the excision of any maximal chain or fragment
included within ?(n) itself.
15 We mention here a second possible optimization of the algorithm. The priority queue allows us to excise
tree segments always from the input elementary tree ?, making the algorithm easier to analyze.
However, as one of the reviewers has pointed out to us, we could do away with the use of the priority
queue and process fragment trees in any order. This results in running time O(|?|) for the factorization
function in Figure 18.
476
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
We now show that the maximal factorization of G is unique. Let ? and ?? be two
isolated fragments of some elementary tree ?. We say that ? and ?? partially overlap if
the set of nodes shared by ? and ?? is not empty and is a proper subset of the nodes of
both fragments. It is not difficult to see that if ? and ?? partially overlap, then at least
one among ? and ?? must have the form ?(n1, n2).
Without any loss of generality, we assume that the elementary trees of G are always
factorized at their maximal nodes, as discussed in Section 6.1. Let us assume that f
and f ? are two distinguishable maximal factorizations of G. Because no fragment of one
factorization can be a sub-fragment of some fragment of the other factorization, there
must be some fragment ? of f and some fragment ?? of f ? such that ? and ?? partially
overlap.
Assume that ? has the form ?(n1). Then ?? must have the form ?(n2, n3), and n1
must be in the path from n3 to n2. Because ?? is as small as possible, (n2, n3) must be a
minimal pair. We have then established a violation of Lemma 2(i). Assume now that ?
has the form ?(n1, n2). Again, (n1, n2) must be a minimal pair. If ?? has the form ?(n3),
this argument applies again, resulting in a violation of Lemma 2(i). If ?? has the form
?(n3, n4), then (n3, n4) must be a minimal pair. Furthermore, n1, n2, n3, and n4 must all
be on the same path within ?, with n1, n2 in alternation with n3, n4. This establishes a
violation of Lemma 2(ii). The assumption that f and f ? partially overlap then leads to a
contradiction, and we must conclude that the maximal factorization of G is unique.
We can also use this argument against the existence of overlapping fragments to
show that any factorization f of G other than the unique maximal factorization fM must
be coarser than fM, meaning that each fragment ? of f is also a fragment of fM, or else
? can be represented as a combination of the fragments of fM (through substitution and
adjunction). This means that no factorization of G can have rank smaller than the rank
of the maximal factorization fM. We conclude that our algorithm is optimal.
This discussion on the optimality of the factorization algorithm crucially assumes
strong equivalence with the source TL-MCTAG G. Of course there might be TL-
MCTAGs that are weakly equivalent to G, that is, they generate the same language,
and have rank strictly smaller than the rank of G?. However, finding such structurally
different grammars is a task that seems to require techniques quite different from the
factorization techniques we have developed in this section. Furthermore, the task might
be computationally unfeasible, considering the fact that the weak equivalence problem
for TL-MCTAG is undecidable. (Such a problem is undecidable even for CFGs.)
We remark here that if we are allowed to change G by recasting its elementary trees
in some suitable way, we might be able to further reduce the rank with respect to the
algorithm we have presented in this section. In this case the output grammar would
not preserve the derived trees, that is, we lose the strong equivalence, but still retain
the derivation trees unaltered. Although this is likely not desirable for applications
in which the input grammar consists of linguistically motivated trees, there may be
other applications for which the preservation of the internal structure of the trees is
less important than the processing efficiency that can be gained by more aggressive
factorization. Furthermore, it is well known that the desired derived tree for the source
grammar can be easily reconstructed from the derivation tree.
Consider for instance cases in which the input TL-MCTAG is not in binary form,
that is, some nodes have more than two children. Currently, the definition of fragment
does not allow splitting apart a subset of the children of a given node from the remaining
ones. However, if we allow binarization of the elementary trees of the source grammar,
then we might be able to isolate sets of links that could not be factorized in the source
grammar itself. It is not difficult to construct an elementary tree ? with r links such
477
Computational Linguistics Volume 36, Number 3
that no factorization of ? is possible if we are required to preserve ??s structure, but if
we drop such a requirement then we could binarize ? in such a way that a factorization
can be obtained through the application of the algorithm above, such that any tree in the
factorization has no more than two links. However, the general problem of restructuring
elementary trees in such a way that an optimal factorization is possible is not trivial and
requires further research. We leave this problem for future work.
A second case arises when multiple links impinge on the same node of an elemen-
tary tree. As presented, the factorization algorithm is designed to handle grammars in
which multiple adjunction is permitted. However, if multiple adjunction is disallowed
and the grammar contains trees in which multiple links impinge on the same node,
the use of one link at a node will disqualify any other impinging links from use. This
opens up the possibility of further reducing the rank of the grammar by producing
tree sets that do not contain any nodes on which multiple links impinge. This can
be accomplished by performing a first-pass grammar transformation in which a copy
of each elementary tree set is added to the grammar for each distinct, maximal, non-
conflicting set of links appearing in the tree set. This transformation in itself may result
in a reduction of the rank of the source grammar. The factorization algorithm can then be
applied to the new grammar. However, if the elementary trees in the source grammar
contain clusters of links that are mutually overlapping, the suggested transformation
may blow up the size of the input grammar in a way that is not bounded by any
polynomial function.
7. Conclusion
This paper explores the complexity of TL-MCTAG, showing that recognition is NP-
complete under a range of interesting restrictions. It then provides a parsing algorithm
that performs better than the extrapolation of the standard multiple CFG parsing
method to TL-MCTAG. As shown by our proofs, the difficulty in parsing TL-MCTAG
stems from the rank of the input grammar. We offer a novel and efficient algorithm for
minimizing the rank of the input grammar while preserving its strong generative capac-
ity. It fits into an active line of research into efficient processing of multicomponent and
synchronous formalisms that appear computationally intractable but have desirable
characteristics for meeting the expressive needs of natural language. It presents novel
complexity results and algorithms for TL-MCTAG, a widely known and used formalism
in computational linguistics that may be applied more effectively in natural-language
processing using algorithms that process it as efficiently as possible.
Acknowledgments
This work was supported in part by the
National Science Foundation under award
BCS-0827979. The second author has been
partially supported by MIUR under project
PRIN No. 2007TJNZRE 002.
References
Barton, G. Edward. 1985. On the complexity
of ID/LP parsing. Computational
Linguistics, 11(4):205?218.
Chen, John and Vijay K. Shanker. 2004.
Automated extraction of TAGs from the
Penn treebank. In H. Blunt, J. Carroll, and
G. Satta, editors, New Developments in
Parsing Technology. Kluwer Academic,
Amsterdam, pages 73?89.
Cormen, Thomas H., Charles E. Leiserson,
Ronald L. Rivest, and Clifford Stein. 2001.
Introduction to Algorithms. The MIT Press,
Cambridge, MA.
Earley, J. 1970. An Efficient Context-free
Parsing Algorithm. Ph.D. thesis, University
of California, Berkeley, CA.
Garey, M. R. and D. S. Johnson. 1979.
Computers and Intractability. Freeman and
Co., New York, NY.
Gildea, Daniel, Giorgio Satta, and Hao
Zhang. 2006. Factoring synchronous
478
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
grammars by sorting. In The International
Conference on Computational Linguistics/
Association for Computational Linguistics
(COLING/ACL-06) Poster Session
pages 279?286, Sydney.
Graham, S. L., M. A. Harrison, and W. L.
Ruzzo. 1980. An improved context-free
recognizer. ACM Transactions on
Programming Languages and Systems,
2:415?462.
Han, Chung-Hye. 2006. Pied-piping in
relative clauses: Syntax and compositional
semantics based on synchronous tree
adjoining grammar. In Proceedings of the 8th
International Workshop on Tree Adjoining
Grammars and Related Formalisms (TAG+ 8),
pages 41?48, Sydney.
Joshi, A. K., L. S. Levy, and M. Takahashi.
1975. Tree adjunct grammars. Journal of
Computer and System Sciences,
10(1):136?163.
Joshi, Aravind K. and Yves Schabes. 1997.
Tree-adjoining grammars. In G. Rozenberg
and A. Salomaa, editors, Handbook of
Formal Languages, volume 3. Springer,
Berlin, pages 69?124.
Kaji, Yuichi, Ryuchi Nakanishi, Hiroyuki
Seki, and Tadao Kasami. 1992. The
universal recognition problems for
multiple context-free grammars and for
linear context-free rewriting systems.
IEICE Transactions on Information and
Systems, E75-D(1):78?88.
Kaji, Yuichi, Ryuchi Nakanishi, Hiroyuki
Seki, and Tadao Kasami. 1994. The
computational complexity of the universal
recognition problem for parallel multiple
context-free grammars. Computational
Intelligence, 10(4):440?452.
Kallmeyer, Laura. 2005. Tree-local
multicomponent tree adjoining grammars
with shared nodes. Computational
Linguistics, 31(2):187?225.
Kallmeyer, Laura. 2009. A declarative
characterization of different types of
multicomponent tree adjoining grammars.
Research on Language and Computation,
7(1):55?99.
Kallmeyer, Laura and Aravind K. Joshi.
2003. Factoring predicate argument and
scope semantics: Underspecified
semantics with LTAG. Research on
Language and Computation, 1:3?58.
Kallmeyer, Laura and Maribel Romero.
2007. Reflexives and reciprocals in
LTAG. In Proceedings of the Seventh
International Workshop on Computational
Semantics ICWS-7, pages 271?282,
Tilburg.
Kasami, T. 1965. An efficient recognition
and syntax algorithm for context-free
languages. Technical Report
AF-CRL-65-758, Air Force Cambridge
Research Laboratory, Bedford, MA.
Nesson, Rebecca. 2009. Synchronous and
Multicomponent Tree-Adjoining Grammars:
Complexity, Algorithms and Linguistic
Applications. Ph.D. thesis, Harvard
University, Cambridge, MA.
Nesson, Rebecca, Giorgio Satta, and Stuart
Shieber. 2008. Optimal k-arization of
synchronous tree-adjoining grammar. In
the Association for Computational Linguistics
(ACL-2008), pages 604?612, Columbus, OH.
Nesson, Rebecca and Stuart M. Shieber.
2006. Simpler TAG semantics through
synchronization. In Proceedings of the
11th Conference on Formal Grammar,
pages 129?142, Malaga.
Nesson, Rebecca and Stuart M. Shieber.
2007. Extraction phenomena in
synchronous TAG syntax and semantics.
In Proceedings of Syntax and Structure in
Statistical Translation (SSST), pages 9?16,
Rochester, NY.
Rambow, Owen. 1994. Formal and
computational aspects of natural language
syntax. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Satta, Giorgio and Enoch Peserico. 2005.
Some computational complexity results for
synchronous context-free grammars. In
Proceedings of Human Language Technology
Conference and Conference on Empirical
Methods in Natural Language Processing
(HLT05/EMNLP05), pages 803?810,
Vancouver.
Schabes, Yves and Richard C. Waters. 1995.
Tree insertion grammar: A cubic-time
parsable formalism that lexicalizes
context-free grammar without changing
the trees produced. Computational
Linguistics, 21(4):479?513.
Seki, H., T. Matsumura, M. Fujii, and
T. Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science,
88:191?229.
Shieber, Stuart M. and Yves Schabes. 1994.
An alternative conception of tree-adjoining
derivation. Computational Linguistics,
20(1):91?124.
Shieber, Stuart M., Yves Schabes, and
Fernando C. N. Pereira. 1995. Principles
and implementation of deductive parsing.
Journal of Logic Programming, 24(1?2):3?36.
Sippu, S. and E. Soisalon-Soininen. 1988.
Parsing Theory: Languages and Parsing.
Springer-Verlag, Berlin.
479
Computational Linguistics Volume 36, Number 3
S?gaard, Anders, Timm Lichte, and
Wolfgang Maier. 2007. On the complexity
of linguistically motivated extensions of
tree-adjoining grammar. In Recent
Advances in Natural Language Processing
2007, Borovets.
Tovey, C. A. 1984. A simplified NP-complete
satisfiability problem. Discrete Applied
Mathematics, 8(1):85?90.
Vijay-Shanker, K. 1987. A study of
tree-adjoining grammars. Ph.D. thesis,
Department of Computer and Information
Science, University of Pennsylvania,
Philadelphia, PA.
Vijay-Shanker, K. and Aravind K. Joshi. 1985.
Some computational properties of
tree-adjoining grammars. In Proceedings of
the 23rd Annual Meeting of the Association
for Computational Linguistics, pages 82?93,
Chicago, IL.
Weir, David. 1988. Characterizing mildly
context-sensitive grammar formalisms.
Ph.D. thesis, Department of Computer and
Information Science, University of
Pennsylvania, Philadelphia, PA.
Younger, D. H. 1967. Recognition and
parsing of context-free languages in time
n3. Information and Control, 10(2):189?208.
Zhang, Hao and Daniel Gildea. 2007.
Factorization of synchronous context-free
grammars in linear time. In NAACL
Workshop on Syntax and Structure in
Statistical Translation (SSST), pages 25?32,
Rochester, NY.
480
Tree-Adjoining Grammars Are Not
Closed Under Strong Lexicalization
Marco Kuhlmann?
Uppsala University
Giorgio Satta??
University of Padua
A lexicalized tree-adjoining grammar is a tree-adjoining grammar where each elementary tree
contains some overt lexical item. Such grammars are being used to give lexical accounts of
syntactic phenomena, where an elementary tree defines the domain of locality of the syntactic
and semantic dependencies of its lexical items. It has been claimed in the literature that for
every tree-adjoining grammar, one can construct a strongly equivalent lexicalized version. We
show that such a procedure does not exist: Tree-adjoining grammars are not closed under strong
lexicalization.
1. Introduction
Many contemporary linguistic theories give lexical accounts of syntactic phenomena,
where complex syntactic structures are analyzed as the combinations of elementary
structures taken from a finite lexicon. In the computational linguistics community, this
trend has been called lexicalization, and has been extensively investigated since the
1990s. From a mathematical perspective, the main question that arises in the context of
lexicalization is whether the restriction of a given class of grammars to lexicalized form
has any impact on the generative or computational properties of the formalism.
As a simple example, consider the class of context-free grammars (CFGs). Recall
that a CFG is in Greibach normal form if the right-hand side of every rule in the gram-
mar starts with a terminal symbol, representing an overt lexical item. Although several
procedures for casting a CFG in Greibach normal form exist, all of them substantially
alter the structure of the parse trees of the source grammar. In technical terms, these
procedures provide a weak lexicalization of the source grammar (because the string
language is preserved) but not a strong lexicalization (because the sets of parse trees
that the two grammars assign to the common string language are not the same). Strong
lexicalization is highly relevant for natural language processing, however, where the
parse tree assigned by a grammar represents the syntactic analysis of interest, and is
used by other modules such as semantic interpretation or translation. In this article, we
investigate the problem of strong lexicalization.
? Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden.
E-mail: marco.kuhlmann@lingfil.uu.se.
?? Department of Information Engineering, via Gradenigo 6/A, 35131 Padova, Italy.
E-mail: satta@dei.unipd.it.
Submission received: 16 July 2011; accepted for publication: 10 September 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 3
Two important results about strong lexicalization have been obtained by Schabes
(1990). The first result is that CFGs are not closed under strong lexicalization. (The
author actually shows a stronger result involving a formalism called tree substitution
grammar, as will be discussed in detail in Section 3.) Informally, this means that we
cannot cast a CFG G in a special form in which each rule has an overt lexical item in its
right-hand side, under the restriction that the new grammar generates exactly the same
set of parse trees as G. As a special case, this entails that no procedure can cast a CFG
in Greibach normal form, under the additional condition that the generated parse trees
are preserved.
The second result obtained by Schabes concerns the relation between CFGs and the
class of tree-adjoining grammars (TAGs) (Joshi, Levy, and Takahashi 1975; Joshi and
Schabes 1997). A TAG consists of a finite set of elementary trees, which are phrase
structure trees of unbounded depth, and allows for the combination of these trees by
means of two operations called substitution and adjunction (described in more detail in
the next section). A lexicalized TAG is one where each elementary tree contains at least
one overt lexical item called the anchor of the tree; the elementary tree is intended to
encapsulate the syntactic and semantic dependencies of its anchor. Because CFG rules
can be viewed as elementary trees of depth one, and because context-free rewriting can
be simulated by the substitution operation defined for TAGs, we can view any CFG as
a special TAG. Under this view, one can ask whether lexicalized TAGs can provide a
strong lexicalization of CFGs. Schabes? second result is that this is indeed the case. This
means that, given a CFG G, one can always construct a lexicalized TAG generating the
same set of parse trees as G, and consequently the same string language.
Following from this result, there arose the possibility of establishing a third result,
stating that TAGs are closed under strong lexicalization. Schabes (1990) states that this
is the case, and provides an informal argument to justify the claim. The same claim
still appears in two subsequent publications (Joshi and Schabes 1992, 1997), but no
precise proof of it has appeared until now. We speculate that the claim could be due
to the fact that adjunction is more powerful than substitution with respect to weak
generative capacity. It turns out, however, that when it comes to strong generative
capacity, adjunction also shares some of the restrictions of substitution. This observation
leads to the main result of this article: TAGs are not closed under strong lexicalization.
In other words, there are TAGs that lack a strongly equivalent lexicalized version.
In the same line of investigation, Schabes and Waters (1995) introduce a restricted
variant of TAG called tree insertion grammars (TIGs). This formalism severely restricts
the adjunction operation originally defined for TAGs, in such a way that the class of
generated string languages, as well as the class of generated parse trees, are the same
as those of CFGs. Schabes and Waters then conjecture that TIGs are closed under strong
lexicalization. In this article we also disprove their conjecture.
2. Preliminaries
We assume familiarity with the TAG formalism; for a survey, we refer the reader to
Joshi and Schabes (1997). We briefly introduce here the basic terminology and notation
for TAG that we use in this article.
2.1 Basic Definitions
A TAG is a rewriting system that derives trees starting from a finite set of elementary
trees. Elementary trees are trees of finite but arbitrary depth, with internal nodes labeled
618
Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalization
with nonterminal symbols and frontier nodes labeled with terminal and nonterminal
symbols. Each elementary tree is either an initial tree or else an auxiliary tree. Initial
trees serve as the starting point for derivations, and may combine with other trees by
means of an operation called substitution. Tree substitution replaces a node labeled
with a nonterminal A in the frontier of some target tree with an initial tree whose
root is labeled with A. The nodes that are the target of the substitution operation are
identified by a down arrow (?). The substitution operation is illustrated in the left half
of Figure 1.
Auxiliary trees are elementary trees in which a special node in the frontier has the
same nonterminal label as the root node. This special node is called the foot node and is
identified by an asterisk (?). Auxiliary trees may combine with other trees by means of
an operation called adjunction. The adjunction operation entails splitting some target
tree at an internal node with label A, and inserting an auxiliary tree whose root (and
foot) node is labeled with A. The adjunction operation is illustrated in the right half of
Figure 1.
A derivation in a TAG can be specified by a derivation tree d; this is a rooted
tree whose nodes are labeled with (instances of) elementary trees, and whose edges
are labeled with (addresses of) nodes at which substitution or adjunction takes place.
More specifically, an edge v ?u v? in d represents the information that the elementary
tree at v? is substituted at or adjoined into node u of the elementary tree at v. When
we combine the elementary trees of our TAG as specified by d, we obtain a (unique)
phrase structure tree called the derived tree associated with d, which we denote
as t(d).
We use the symbol ? as a variable ranging over elementary trees, ? as a variable
ranging over initial trees, and ? as a variable ranging over auxiliary trees. We also
use the symbols u and v as variables ranging over nodes of generic trees (elementary,
derived, or derivation trees). For an elementary tree ?, a derivation tree d is said to have
type ? if the root node of d is labeled with ?. A derivation tree d is called sentential if d is
of some type ?, and the root node of ? is labeled with the start symbol of the grammar,
denoted as S.
A node u in an elementary tree ?may be annotated with an adjunction constraint,
which for purposes here is a label in the set {NA,OA}. The label NA denotes Null
Adjunction, forbidding adjunction at u; the label OA denotes Obligatory Adjunction,
forcing adjunction at u. A derivation tree d is called saturated if, at each node v of d
there is an arc v ?u v?, for some v?, for every node u of the elementary tree at v that
requires substitution or is annotated with an OA constraint.
For a TAGG, we denote by T(G) the set of all the derived trees t such that t = t(d) for
some sentential and saturated derivation tree d obtained in G. Each such derived tree is
Figure 1
Combination operations in TAG.
619
Computational Linguistics Volume 38, Number 3
(uniquely) associated with a string y(t) called the yield of t, obtained by concatenating
all terminal symbols labeling the frontier of t, from left to right. The string language
generated by G is the set
L(G) = { y(t) | t ? T(G) }
A TAG G is said to be finitely ambiguous if, for every string w ? L(G), the subset of
those trees in T(G) that have w as their yield is finite.
An elementary tree ? of G is called useless if ? never occurs in a sentential and sat-
urated derivation tree of G, that is, if no sentential and saturated derivation of G uses ?.
A grammar G is called reduced if none of its elementary trees is useless. Throughout
this article we shall assume that the grammars that we deal with are reduced.
2.2 Lexicalization
In a tree, a node labeled with a terminal symbol is called a lexical node. A TAG is
called lexicalized if each of its elementary trees has at least one lexical node. Observe
that a lexicalized grammar cannot generate the empty string, denoted by ?, because
every derived tree yields at least one lexical element. Similarly, a lexicalized grammar
is always finitely ambiguous, because the length of the generated strings provides an
upper bound on the size of the associated derived trees. Let G and G? be two subclasses
of the class of all TAGs. We say that G? strongly lexicalizes G, if, for every grammar
G ? G that is finitely ambiguous and that satisfies ? 	? L(G), there exists a lexicalized
grammar G? ? G? such that T(G?) = T(G). We also say that G is closed under strong
lexicalization if the class G strongly lexicalizes itself.
Using this terminology, we can now restate the two main results obtained by
Schabes (1990) about strong lexicalization for subclasses of TAGs, already mentioned in
the Introduction. The first result states that the class of CFGs is not closed under strong
lexicalization. Here we view a CFG as a special case of a TAG using only substitution
and elementary trees of depth one. Informally, this means that we cannot cast a CFG
G in a special form in which each rule has an overt lexical item in its right-hand side,
under the restriction that the new grammar generates exactly the same tree set as G. The
second result is that the class of TAGs strongly lexicalizes the class of tree substitution
grammars (TSGs). The latter class is defined as the class of all TAGs that use substitution
as the only tree combination operation, and thus includes all context-free grammars.
This means that, given a TSG or a CFG G, we can always construct a TAG that is
lexicalized and that generates exactly the same tree set as G.
3. Tree Substitution Grammars Are Not Closed Under Strong Lexicalization
Before turning to our main result in Section 4, we find it useful to technically revisit the
related result for TSGs.
Theorem 1
Tree substitution grammars are not closed under strong lexicalization.
To prove this result, Schabes (1990) uses a proof by contradiction: The author considers a
specific TSGG1, reported in Figure 2. It is not difficult to see thatG1 is finitely ambiguous
and that ? 	? L(G1). The author then assumes that G1 can be lexicalized by another TSG,
620
Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalization
Figure 2
The counterexample tree substitution grammar G1.
and derives a contradiction. We provide here an alternative, direct proof of Theorem 1.
This alternative proof will be generalized in Section 4 to obtain the main result of this
article.
We use the following notation. For a derived tree t and a terminal symbol a, we
write Nodes(a, t) to denote the set of all nodes in t that are labeled with a. Furthermore,
for a node u of t we write depth(u, t) to denote the length of the unique path from the
root node of t leading to u.
3.1 Intuition
In order to convey the basic idea behind Schabes?s proof and our alternative version
herein, we first consider a specific candidate grammar for the lexicalization of G1. For
example, one might think that the following TSG G?1 lexicalizes G1:
This grammar is obtained from G1 by taking the lexicalized tree ?1, as well as every
elementary tree that can be obtained by substituting ?1 into the non-lexicalized tree ?2.
The grammar G?1 only generates a subset of the trees generated by G1, however. The
following tree, for example, cannot be generated by G?1:
To see this, we reason as follows. Consider a lexical node v in an elementary tree ?
of G?1, and let t be a tree obtained by substituting some elementary tree into ?. Because
substitution takes place at the frontier of ?, depth(v, t) must be the same as depth(v,?).
More generally, the depth of a lexical node in an elementary tree ? is the same in all trees
derived starting from ?. Because the maximal depth of a lexical node in an elementary
621
Computational Linguistics Volume 38, Number 3
tree of G?1 is 2, we deduce that every tree generated by G
?
1 contains a lexical node with
depth at most 2. In contrast, all lexical nodes in the tree t1 have depth 3. Therefore the
tree t1 is not generated by G
?
1.
3.2 Main Part
We now generalize this argument to arbitrary candidate grammars. For this, we are
interested in the following class G1 of all (reduced) TSGs that derive a subset of the trees
derived by G1:
G1 = {G | G is a TSG, T(G) ? T(G1) }
For a grammar G ? G1, we define the d-index of G as the maximum in N ? {?} of the
minimal depths of a-labeled nodes in trees derived by G:
d-index(G) = max
t?T(G)
min
v?Nodes(a,t)
depth(v, t)
Note that, for two grammars G,G? ? G1, T(G) = T(G?) implies that G and G? have the
same d-index. This means that two grammars in G1 with different d-indices cannot gen-
erate the same tree language. Then Theorem 1 directly follows from the two statements
in the next lemma.
Lemma 1
The grammar G1 has infinite d-index. Every lexicalized grammar in G1 has finite
d-index.
Proof
The first statement is easy to verify: Using longer and longer derivations, the mini-
mal depth of an a-labeled node in the corresponding tree can be pushed beyond any
bound.
To prove the second statement, let G be a lexicalized grammar in G1, and let
t ? T(G). The tree t is derived starting from some initial tree; call this tree ?. Because G
is lexicalized, at least one of the a-labeled nodes in Nodes(a, t) is contributed by ?. Let va
be any such node in t, and let ua be the node of ? that corresponds to va. Remember
that the only tree combination operation allowed in a TSG derivation is substitution.
Because substitution can only take place at the frontier of a derived tree, we must
conclude that depth(va, t) = depth(ua,?). There are only finitely many initial trees in G,
therefore depth(ua,?) must be upper bounded by some constant depending only on G,
and the same must hold for depth(va, t). Lastly, because t has been arbitrarily chosen in
T(G), we must conclude that d-index(G) is finite. 
3.3 Lexicalization of Tree Substitution Grammars
What we have just seen is that lexicalized TSGs are unable to derive the tree structures
generated by the grammar G1 in Figure 2. This is essentially because tree substitution
cannot stretch the depth of a lexical node in an elementary tree. In contrast, tree adjunc-
tion allows the insertion of additional structure at internal nodes of elementary trees,
622
Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalization
and enables TAGs to provide a strong lexicalization of TSGs. For example, the following
TAG G??1 lexicalizes G1.
Note that this grammar looks almost like G?1, except that adjunction now is allowed at
internal nodes, and substitution nodes have become foot nodes. The following deriva-
tion tree witnesses that the tree t1 can be derived in G
??
1 . We write 0 to denote the root
node of an elementary tree, and 1 to denote its leftmost child.
?6 ?0 ?1 ?0 ?1 ?1 ?1
Schabes (1990) provides a general procedure for constructing a lexicalized TAG for a
given context-free grammar.
4. Tree-Adjoining Grammars Are Not Closed Under Strong Lexicalization
In this section we develop the proof of the main result of this article.
Theorem 2
Tree-adjoining grammars are not closed under strong lexicalization.
4.1 Proof Idea
The basic idea underlying the proof of Theorem 2 is essentially the same as the one used
in the proof of Theorem 1 in Section 3. Some discussion of this issue is in order at this
point. In the previous section, we have seen that adjunction, in contrast to substitution,
allows the insertion of additional structure at internal nodes of elementary trees, and
enables TAGs to provide a strong lexicalization of TSGs. One might now be tempted to
believe that, because the depth-based argument that we used in the proof of Lemma 1
can no longer be applied to TAGs, they might be closed under strong lexicalization.
There is a perspective under which adjunction quite closely resembles substitution,
however. Let us first look at substitution as an operation on the yield of the derived
tree. Under this view, substitution is essentially context-free rewriting: It replaces a non-
terminal symbol in the yield of a derived tree with a new string consisting of terminals
and nonterminals, representing the yield of the tree that is substituted. Under the same
perspective, adjunction is more powerful than tree substitution, as is well known. But
just as substitution can be seen as context-free rewriting on tree yields, adjunction can
be seen as context-free rewriting on the paths of trees: It replaces a nonterminal symbol
in some path of a derived tree with a string representing the spine of the tree that is
adjoined?the unique path from the root node of the tree to the foot node.
This observation gives us the following idea for how to lift the proof of Theorem 1
to TAGs. We will specify a TAG G2 such that the paths of the derived trees of G2 encode
in a string form the derived trees of the counterexample grammar G1. This encoding
is exemplified in Figure 3. Each internal node of a derived tree of G1 is represented in
623
Computational Linguistics Volume 38, Number 3
Figure 3
A derived tree of G1, and the corresponding encoding, drawn from left to right. Every
internal node of the original tree is represented by a pair of matching brackets [S (, )S].
The correspondence is indicated by the numerical subscripts.
the spine of the corresponding derived tree of G2 as a pair of matching brackets. By
our encoding, any TAG generating trees from T(G2) will have to exploit adjunction at
nodes in the spine of its elementary trees, and will therefore be subject to essentially the
same restrictions as the grammar G1 which used substitution at nodes in the yield. This
will allow us to lift our argument from Lemma 1. The only difference is that instead of
working with the actual depth of a lexical node in a tree t ? T(G2), we will now need
to work with the depth of the node in the encoded tree. As will be explained later, this
measure can be recovered as the excess of left parentheses over right parentheses in the
spine above the lexical node.
4.2 Preliminaries
As alreadymentioned, our proof of Theorem 2 follows the same structure as our proof of
Theorem 1. As our counterexample grammar, we use the grammar G2 given in Figure 4;
this grammar generates the encodings of the derived trees of G1 that we discussed
previously. Note that the left parenthesis symbol ?(? and the right parenthesis symbol
?)? are nonterminal symbols. As with the grammar G1 before, it is not difficult to see
that G2 is finitely ambiguous and that ? /? L(G2).
Figure 4
The counterexample TAG G2.
624
Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalization
Grammar G2 derives trees that we call right spinal: Each node in such a tree has
at most two children, and the left child of every node with two children is always a
leaf node. The path from the root node of a right spinal tree t to the rightmost leaf
of t is called spine. To save some space, in the following we write right spinal trees
horizontally and from left to right, as already done in Figure 3. Thus the grammar G2
can alternatively be written as follows:
For a node u in a right spinal tree derived by G2, we define
c(u) =
?
?
?
?
?
+1 if u is labeled with (
0 if u is labeled with S or a
?1 if u is labeled with )
We exploit this function to compute the excess of left parentheses over right parentheses
in a sequence of nodes, and write:
excess(?u1, . . . ,un?) =
n
?
i=1
c(ui)
Let t be some right spinal tree in T(G2), and let v be some node in t. Assume that
?u1, . . . ,un = v? is the top?down sequence of all the nodes in the path from t?s root u1
to v. We write excess(v, t) as a shorthand notation for excess(?u1, . . . ,un?). If ?u1, . . . ,un?
is the top?down sequence of all the nodes in the spine of t, we also write excess(t) as a
short hand notation for excess(?u1, . . . ,un?).
It is easy to prove by induction that, for each tree t ? T(G2), the excess of the
sequence of nodes in the spine of t is always zero. Thus, we omit the proof of the
following statement.
Lemma 2
Every derived tree t ? T(G2) is a right spinal tree, and excess(t) = 0.
In order to get a better understanding of the construction used in the following
proofs, it is useful at this point to come back to our discussion of the relation between
that construction and the construction presented in Section 3. We observe that for each
tree t1 generated by G1 there is a tree t2 ? T(G2) such that the sequence of labels in t2?s
spine encodes t1, following the scheme exemplified in Figure 3. Using such encoding,
we can establish a bijection between the a-labeled nodes in the frontier of t1 and the
a-labeled nodes in the frontier of t2. Furthermore, if v1 in t1 and v2 in t2 are two
nodes related by such a correspondence, then it is not difficult to see that depth(v1, t1) =
excess(v2, t2).
4.3 Intuition
Before we give the actual proof of Theorem 2, let us attempt to get some intuition
about why our counterexample grammar G2 cannot be strongly lexicalized by some
625
Computational Linguistics Volume 38, Number 3
other TAG. One might think that the following TAG G?2 is a lexicalized version
of G2:
This grammar is obtained from G2 by taking the lexicalized tree ?3 (repeated here
as ?5), as well as all trees that can be obtained by adjoining ?3 into some non-lexicalized
elementary tree. G?2 does not generate all trees generated by G2, however. The following
tree t2 for example is not generated by G
?
2:
Note that this tree is the encoded version of the counterexample tree t1 from the previous
section (cf. Figure 3).
To see that t2 is not generated by G
?
2, we reason as follows. Consider a lexical node u
in an elementary tree ? of G?2, and let t be a tree obtained by adjoining some elementary
tree into ?. Although this adjunction increases the depth of u, it does not increase its
excess, as it adds a balanced sequence of parentheses into the spine of ?. More generally,
the excess of a lexical node in an elementary ? is constant in all trees derived starting
from ?. From this we conclude that every tree generated by G?2 contains a lexical node
with excess at most 2; this is the maximal excess of a lexical node in an elementary tree
of G?2. In contrast, all lexical nodes in the tree t2 have excess 3. This shows that t2 is not
generated by G?2.
4.4 Main Part
In what follows, we consider the class G2 of (reduced) TAGs that generate subsets of the
trees derived by G2:
G2 = {G | G is a TAG, T(G) ? T(G2) }
For a grammar G ? G2, we define the e-index of G as the maximum in N ? {?} of the
minimal excess of a-labeled nodes in trees derived by G:
e-index(G) = max
t?T(G)
min
v?Nodes(a,t)
excess(v, t)
As we will see, the notion of e-index plays exactly the same role as the notion of d-index
in Section 3.
626
Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalization
There is one last obstacle that we need to overcome. For TSGs we noted (in the proof
of Lemma 1) that the minimal depth of lexical nodes in a derived tree t is bounded by
the minimal depth of lexical nodes in the elementary tree ? from which t was derived.
For the TAGs in G2, the situation is not quite as simple, as an adjunction of an auxiliary
tree ? into an elementary tree ? might affect the excess of a lexical node of ?. It turns
out, however, that this potential variation in the excess of a lexical node of ? is bounded
by a grammar-specific constant. This observation is expressed in the following lemma.
It is the correspondent of Lemma 4 in Knuth?s paper on parenthesis languages (Knuth
1967), and is proved in essentially the same way. Recall that a derivation tree d is of
type ?, ? some elementary tree, if d is derived starting from ?.
Lemma 3
Let G ? G2. For each elementary tree ? of G, there exists a number e(?) such that, for
every saturated derivation tree d of type ?, excess(t(d)) = e(?).
Proof
Because ? is not useless, we can find at least one sentential and saturated derivation tree
of G that contains an occurrence of ?. Let d be any such derivation tree, and let v be any
node of d labeled with ?. Let d1 be the subtree of d rooted at v. Observe that t(d1) must
be a spinal tree. We then let e(?) = excess(t(d1)).
If d1 is the only derivation tree of type ? available inG, then we are done. Otherwise,
let d2 	= d1 be some derivation tree of type ? occurring within some other sentential
and saturated derivation tree of G. We can replace d1 with d2 in d at v to obtain a new
sentential and saturated derivation tree d? 	= d. Every derived tree in T(G) must be a
right spinal tree: This follows from the assumption that G ? G2 and from Lemma 2. We
can then write
excess(t(d?)) = excess(t(d))? excess(t(d1))+ excess(t(d2))
Because excess(t(d)) = 0 and excess(t(d?)) = 0 (by Lemma 2), we conclude that
excess(t(d2)) = excess(t(d1)) = e(?)

Using Lemma 3, we can now prove the following result.
Lemma 4
The grammarG2 has infinite e-index. Every lexicalized grammar in G2 has finite e-index.
Proof
As in the case of Lemma 1, the first statement is easy to verify and we omit its proof. To
prove the second statement, let G ? G2. Let ? be the set of all elementary trees of G, and
let s be the maximal number of nodes in an elementary tree in ?. We show that
e-index(G) ? k , where k = s+ s ?max
???
|e(?)|
Note that k is a constant that only depends on G.
627
Computational Linguistics Volume 38, Number 3
Let d be a sentential and saturated derivation tree of G. It has the following shape:
Here ? is some initial tree, m ? 0, each ui is a node of ? at which a tree combination
operation takes place, each ?i is an elementary tree, and each di is a derivation tree of
type ?i that is a subtree of d. According to this derivation tree, the derived tree t(d) is
obtained by substituting or adjoining the derived trees t(di) at the respective nodes ui
of ?.
Because G is lexicalized, at least one a-labeled node on the frontier of t(d) is con-
tributed by ?. Let va be any such node, and let ua be the node of ? that corresponds
to va. The quantity excess(va, t(d)), representing the excess of the path in t(d) from its
root to the node va, can be computed as follows. Let ?u?1, . . . ,u
?
n = ua? be the top?down
sequence of nodes in the path from the root node of ? to ua. For each i with 1 ? i ? n
we define
c?(u?i ) =
{
excess(t(dj)) if u
?
i = uj for some 1 ? j ? m
c(u?i ) otherwise
Because G ? G2 and because t(d) is a right spinal tree (Lemma 2), we can write
excess(va, t(d)) =
n
?
i=1
c?(u?i )
By Lemma 3, we have excess(t(dj)) = e(?j), for each jwith 1 ? j ? m. We can then write
excess(va, t(d)) ? n+
m
?
i=1
|e(?i)| ? s+ s ?max
???
|e(?)| = k
Thus, every derived tree t in T(G) contains at least one node va in its frontier such that
excess(va, t) ? k. Therefore, e-index(G) ? k. 
Two grammars in G2 that have a different e-index cannot generate the same tree lan-
guage, thus we have concluded the proof of Theorem 2.
5. Tree Insertion Grammars Are Not Closed Under Strong Lexicalization
As mentioned earlier Schabes and Waters (1995) introduce a restricted variant of TAG
called TIG. The essential restriction in that formalism is the absence of wrapping trees,
which are trees derived starting from auxiliary trees with overt lexical material on both
sides of the foot node. Schabes and Waters (1995, Section 5.1.4) conjecture that the class
of all TIGs is closed under strong lexicalization.
628
Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalization
It is easy to see that the counterexample grammar G2 that we gave in Figure 4
does not derive wrapping trees; this means that G2 actually is a TIG. Using the proof
of Section 4, we then obtain the following result.
Theorem 3
Tree insertion grammars are not closed under strong lexicalization.
In fact, we have even proved the stronger result that the class of TAGs does not lexicalize
the class of TIGs.
6. Conclusion
We have shown that, in contrast to what has been claimed in the literature, TAGs are not
closed under strong lexicalization: The restriction to lexicalized TAGs involves a loss in
strong generative capacity.
In this article we have only considered TAGs with Null Adjunction and Obligatory
Adjunction constraints. A third kind of adjunction constraint that has been used in the
literature is Selective Adjunction, where a set of trees is provided thatmay be adjoined at
some node. It is not difficult to see that the proofs of Lemma 3, Lemma 4, and Theorem 3
still hold if Selective Adjunction constraints are used.
Our result triggers a number of follow-up questions. First, are TAGs closed under
weak lexicalization, defined in Section 1? We know that, in the case of CFGs, this ques-
tion can be answered affirmatively, because Greibach normal form is a special case of
lexicalized form, and for every CFG there is a weakly equivalent grammar in Greibach
normal form. But to our knowledge, no comparable result exists for TAG. Second, if
TAGs cannot strongly lexicalize themselves, what would a grammar formalism look
like that is capable of providing strong lexicalization for TAGs?
Acknowledgments
We are grateful to Aravind Joshi for
discussion on previous versions of this
article and for helping us in shaping
the text in the Introduction of the
current version. We also acknowledge
three anonymous reviewers for their
helpful comments.
References
Joshi, Aravind K., Leon S. Levy, and
Masako Takahashi. 1975. Tree Adjunct
Grammars. Journal of Computer and
System Sciences, 10(2):136?163.
Joshi, Aravind K. and Yves Schabes. 1992.
Tree-adjoining grammars and lexicalized
grammars. In Maurice Nivat and
Andreas Podelski, editors, Tree Automata
and Languages. North-Holland,
Amsterdam, pages 409?431.
Joshi, Aravind K. and Yves Schabes. 1997.
Tree-adjoining grammars. In Grzegorz
Rozenberg and Arto Salomaa, editors,
Handbook of Formal Languages, volume 3.
Springer, Berlin, pages 69?123.
Knuth, Donald E. 1967. A characterization
of parenthesis languages. Information
and Control, 11(3):269?289.
Schabes, Yves. 1990.Mathematical and
Computational Aspects of Lexicalized
Grammars. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
Schabes, Yves and Richard C. Waters.
1995. Tree insertion grammar:
A cubic-time parsable formalism that
lexicalizes context-free grammars
without changing the trees produced.
Computational Linguistics, 21(4):479?513.
629

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 276?284,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Efficient Parsing of Well-Nested Linear Context-Free Rewriting Systems
Carlos G?mez-Rodr?guez1, Marco Kuhlmann2, and Giorgio Satta3
1Departamento de Computaci?n, Universidade da Coru?a, Spain, cgomezr@udc.es
2Department of Linguistics and Philology, Uppsala University, Sweden, marco.kuhlmann@lingfil.uu.se
3Department of Information Engineering, University of Padua, Italy, satta@dei.unipd.it
Abstract
The use of well-nested linear context-free
rewriting systems has been empirically moti-
vated for modeling of the syntax of languages
with discontinuous constituents or relatively
free word order. We present a chart-based pars-
ing algorithm that asymptotically improves the
known running time upper bound for this class
of rewriting systems. Our result is obtained
through a linear space construction of a binary
normal form for the grammar at hand.
1 Introduction
Since its earliest years, one of the main goals of
computational linguistics has been the modeling of
natural language syntax by means of formal gram-
mars. Following results by Huybregts (1984) and
Shieber (1985), special attention has been given to
formalisms that enlarge the generative power of con-
text-free grammars, but still remain below the full
generative power of context-sensitive grammars. On
this line of investigation, mildly context-sensitive
grammar formalisms have been introduced (Joshi,
1985), including, among several others, the tree ad-
joining grammars (TAGs) of Joshi et al (1975).
Linear context-free rewriting system (LCFRS), in-
troduced by Vijay-Shanker et al (1987), is a mildly
context-sensitive formalism that allows the deriva-
tion of tuples of strings, i.e., discontinuous phrases.
This feature has been used to model phrase structure
treebanks with discontinuous constituents (Maier and
S?gaard, 2008), as well as to map non-projective de-
pendency trees into discontinuous phrase structures
(Kuhlmann and Satta, 2009).
Informally, in an LCFRS G, each nonterminal can
generate string tuples with a fixed number of compo-
nents. The fan-out of G is defined as the maximum
number of tuple components generated by G. During
a derivation of an LCFRS, tuple components gener-
ated by the nonterminals in the right-hand side of
a production are concatenated to form new tuples,
possibly adding some terminal symbols. The only re-
striction applying to these generalized concatenation
operations is linearity, that is, components cannot be
duplicated or deleted.
The freedom in the rearrangement of components
has specific consequences in terms of the computa-
tional and descriptional complexity of LCFRS. Even
for grammars with bounded fan-out, the universal
recognition problem is NP-hard (Satta, 1992), and
these systems lack Chomsky-like normal forms for
fixed fan-out (Rambow and Satta, 1999) that are es-
pecially convenient in tabular parsing. This is in con-
trast with other mildly context-sensitive formalisms,
and TAG in particular: TAGs can be parsed in poly-
nomial time both with respect to grammar size and
string size, and they can be cast in normal forms
having binary derivation trees only.
It has recently been argued that LCFRS might be
too powerful for modeling languages with discontin-
uous constituents or with relatively free word order,
and that additional restrictions on the rearrangement
of components might be needed. More specifically,
analyses of both dependency and constituency tree-
banks (Kuhlmann and Nivre, 2006; Havelka, 2007;
Maier and Lichte, 2009) have shown that rearrange-
ments of argument tuples almost always satisfy the
so-called well-nestedness condition, a generalization
276
of the standard condition on balanced brackets. This
condition states that any two components x1, x2 of
some tuple will never be composed with any two
components y1, y2 of some other tuple in such a way
that a ?crossing? configuration is realized.
In this paper, we contribute to a better understand-
ing of the formal properties of well-nested LCFRS.
We show that, when fan-out is bounded by any inte-
ger ? ? 1, these systems can always be transformed,
in an efficient way, into a specific normal form with
no more than two nonterminals in their productions?
right-hand sides. On the basis of this result, we
then develop an efficient parsing algorithm for well-
nested LCFRS, running in timeO(? ? |G| ? |w|2?+2),
where G and w are the input grammar and string,
respectively. Well-nested LCFRS with fan-out ? = 2
are weakly equivalent to TAG, and our complex-
ity result reduces to the well-known upper bound
O(|G| ? |w|6) for this class. For ? > 2, our upper
bound is asymptotically better than the one obtained
from existing parsing algorithms for general LCFRS
or equivalent formalisms (Seki et al, 1991).
Well-nested LCFRS are generatively equivalent
to (among others) coupled context-free grammars
(CCFG), introduced by Hotz and Pitsch (1996).
These authors also provide a normal form and de-
velop a parsing algorithm for CCFGs. One difference
with respect to our result is that the normal form for
CCFGs allows more than two nonterminals to appear
in the right-hand side of a production, even though no
nonterminal may contribute more than two tuple com-
ponents. Also, the construction in (Hotz and Pitsch,
1996) results in a blow-up of the grammar that is ex-
ponential in its fan-out, and the parsing algorithm that
is derived runs in time O(4? ? |G| ? |w|2?+2). Our
result is therefore a considerable asymptotic improve-
ment over the CCFG result, both with respect to the
normal form construction and the parsing efficiency.
Finally, under a practical perspective, our parser is a
simple chart-based algorithm, while the algorithm in
(Hotz and Pitsch, 1996) involves two passes and is
considerably more complex to analyze and to imple-
ment than ours.
Kanazawa and Salvati (2010) mention a normal
form for well-nested multiple context-free grammars.
Structure In Section 2, we introduce LCFRS and
the class of well-nested LCFRS that is the focus of
this paper. In Section 3, we discuss the parsing com-
plexity of LCFRS, and show why grammars using
our normal form can be parsed efficiently. Section 4
presents the transformation of a well-nested LCFRS
into the normal form. Section 5 concludes the paper.
2 Linear Context-Free Rewriting Systems
We write [n] to denote the set of positive integers up
to and including n: [n] = {1, . . . , n}.
2.1 Linear, non-erasing functions
Let ? be an alphabet. For integers m ? 0 and
k1, . . . , km, k ? 1, a total function
f : (??)k1 ? ? ? ? ? (??)km ? (??)k
is called a linear, non-erasing function over ? with
type k1 ? ? ? ? ? km ? k, if it can be defined by an
equation of the form
f(?x1,1, . . . , x1,k1?, . . . , ?xm,1, . . . , xm,km?) = ~? ,
where ~? is a k-tuple of strings over the variables on
the left-hand side of the equation and ? with the
property that each variable occurs in ~? exactly once.
The values m and k are called the rank and the fan-
out of f , and denoted by ?(f) and ?(f).
2.2 Linear Context-Free Rewriting Systems
For the purposes of this paper, a linear context-free
rewriting system, henceforth LCFRS, is a construct
G = (N,T, P, S), where N is an alphabet of nonter-
minal symbols in which each symbol A is associated
with a positive integer ?(A) called its fan-out, T is
an alphabet of terminal symbols, S ? N is a distin-
guished start symbol with ?(S) = 1; and P is a finite
set of productions of the form
p = A? f(A1, . . . , Am) ,
where m ? 0, A,A1, . . . , Am ? N , and f is a linear,
non-erasing function over the terminal alphabet T
with type ?(A1)? ? ? ? ??(Am)? ?(A), called the
composition operation associated with p. The rank
of G and the fan-out of G are defined as the maximal
rank and fan-out of the composition operations of G,
and are denoted by ?(G) and ?(G).
The sets of derivation trees of G are the smallest
indexed family of sets DA, A ? N , such that, if
p = A? f(A1, . . . , Am)
277
N = {S,R} , T = {a, b, c, d} , P = { p1 = S ? f1(R), p2 = R? f2(R), p3 = R? f3 } ,
where: f1(?x1,1, x1,2?) = ?x1,1 x1,2? , f2(?x1,1, x1,2?) = ?a x1,1 b, c x1,2 d? , f3 = ??, ?? .
Figure 1: An LCFRS that generates the string language { anbncndn | n ? 0 }.
is a production of G and ti ? DAi for all i ? [m],
then t = p(t1, . . . , tm) ? DA. By interpreting pro-
ductions as their associated composition operations
in the obvious way, a derivation tree t ? DA evalu-
ates to a ?(A)-tuple of strings over T ; we denote this
tuple by val(t). The string language generated by G,
denoted by L(G), is then defined as
L(G) = {w ? T ? | t ? DS , ?w? = val(t) } .
Two LCFRS are called weakly equivalent, if they
generate the same string language.
Example Figure 1 shows a sample LCFRS G with
?(G) = 1 and ?(G) = 2. The sets of its deriva-
tion trees are DR = { pn2 (p3) | n ? 0 } and
DS = { p1(t) | t ? DR }. The string language
generated by G is { anbncndn | n ? 0 }.
2.3 Characteristic strings
In the remainder of this paper, we use the following
convenient syntax for tuples of strings. Instead of
?v1, . . . , vk? , we write v1 $ ? ? ? $ vk ,
using the $-symbol to mark the component bound-
aries. We call this the characteristic string of the tu-
ple, and an occurrence of the symbol $ a gap marker.
We also use this notation for composition operations.
For example, the characteristic string of the operation
f(?x1,1, x1,2?, ?x2,1?) = ?a x1,1 x2,1, x1,2 b?
is a x1,1 x2,1 $ x1,2 b. If we assume the variables on
the left-hand side of an equation to be named ac-
cording to the schema used in Section 2.1, then the
characteristic string of a composition operation deter-
mines that operation completely. We will therefore
freely identify the two, and write productions as
p = A? [v1 $ ? ? ? $ vk](A1, . . . , Am) ,
where the string inside the brackets is the charac-
teristic string of some composition operation. The
substrings v1, . . . , vk are called the components of
the characteristic string. Note that the character-
istic string of a composition operation with type
k1 ? ? ? ? ? km ? k is a sequence of terminal
symbols, gap markers, and variables from the set
{xi,j | i ? [m], j ? [ki] } in which the number of
gap markers is k?1, and each variable occurs exactly
once. When in the context of such a composition op-
eration we refer to ?a variable of the form xi,j?, then
it will always be the case that i ? [m] and j ? [ki].
The identification of composition operations and
their characteristic strings allows us to construct new
operations by string manipulations: if, for example,
we delete some variables from a characteristic string,
then the resulting string still defines a composition
operation (after a suitable renaming of the remaining
variables, which we leave implicit).
2.4 Canonical LCFRS
To simplify our presentation, we will assume that
LCFRS are given in a certain canonical form. Intu-
itively, this canonical form requires the variables in
the characteristic string of a composition operation
to be ordered in a certain way.
Formally, the defining equation of a composition
operation f with type k1 ? ? ? ? ? km ? k is called
canonical, if (i) the sequence obtained from f by
reading variables of the form xi,1 from left to right
has the form x1,1 ? ? ?xm,1; and (ii) for each i ? [m],
the sequence obtained from f by reading variables
of the form xi,j from left to right has the form
xi,1 ? ? ?xi,ki . An LCFRS is called canonical, if each
of its composition operations is canonical.
We omit the proof that every LCFRS can be trans-
formed into a weakly equivalent canonical LCFRS.
However, we point out that both the normal form and
the parsing algorithm that we present in this paper
can be applied also to general LCFRS. This is in con-
trast to some left-to-right parsers in the literature on
LCFRS and equivalent formalisms (de la Clergerie,
2002; Kallmeyer and Maier, 2009), which actually
depend on productions in canonical form.
2.5 Well-nested LCFRS
We now characterize the class of well-nested LCFRS
that are the focus of this paper. Well-nestedness
was first studied in the context of dependency gram-
mars (Kuhlmann and M?hl, 2007). Kanazawa (2009)
278
defines well-nested multiple context-free grammars,
which are weakly equivalent to well-nested LCFRS.
A composition operation is called well-nested, if it
does not contain a substring of the form
xi,i1 ? ? ?xj,j1 ? ? ?xi,i2 ? ? ?xj,j2 , where i 6= j .
For example, the operation x1,1 x2,1$x2,2 x1,2 is well-
nested, while x1,1 x2,1 $ x1,2 x2,2 is not. An LCFRS
is called well-nested, if it contains only well-nested
composition operations.
The class of languages generated by well-nested
LCFRS is properly included in the class of languages
generated by general LCFRS; see Kanazawa and Sal-
vati (2010) for further discussion.
3 Parsing LCFRS
We now discuss the parsing complexity of LCFRS,
and motivate our interest in a normal form for well-
nested LCFRS.
3.1 General parsing schema
A bottom-up, chart-based parsing algorithm for the
class of (not necessarily well-nested) LCFRS can be
defined by using the formalism of parsing schemata
(Sikkel, 1997). The parsing schemata approach con-
siders parsing as a deduction process (as in Shieber
et al (1995)), generating intermediate results called
items. Starting with an initial set of items obtained
from each input sentence, a parsing schema defines
a set of deduction steps that can be used to infer
new items from existing ones. Each item contains
information about the sentence?s structure, and a suc-
cessful parsing process will produce at least one final
item containing a full parse for the input.
The item set used by our bottom-up algorithm to
parse an input string w = a1 ? ? ? an with an LCFRS
G = (N,T, P, S) will be
I = {[A, (l1, r1), . . . , (lk, rk)] | A ? N ?
0 ? li ? ri ? n ?i ? [k]},
where an item [A, (l1, r1), . . . , (lk, rk)] can be inter-
preted as the set of those derivation trees t ? DA
of G for which
val(t) = al1+1 ? ? ? ar1 $ ? ? ? $ alk+1 ? ? ? ark .
The set of final items is thus F = {[S, (0, n)]}, con-
taining full derivation trees that evaluate to w.
For simplicity of definition of the sets of initial
items and deduction steps, let us assume that pro-
ductions of rank > 0 in our grammar do not contain
terminal symbols in their right-hand sides. This can
be easily achieved from a starting grammar by cre-
ating a nonterminal Aa for each terminal a ? T , a
corresponding rank-0 production pa = Aa ? [a](),
and then changing each occurrence of a in the char-
acteristic string of a production to the single variable
associated with the fan-out 1 nonterminal Aa. With
this, our initial item set for a string a1 ? ? ? an will be
H = {[Aai , (i? 1, i)] | i ? [n]} ,
and each production p = A0 ? f(A1, . . . , Am) of
G (excluding the ones we created for the terminals)
will produce a deduction step of the form given in
Figure 2a, where the indexes are subject to the fol-
lowing constraints, imposed by the semantics of f .
1. If the kth component of the characteristic string
of f starts with xi,j , then l0,k = li,j .
2. If the kth component of the characteristic string
of f ends with xi,j , then r0,k = ri,j .
3. If xi,jxi?,j? is an infix of the characteristic string
of f , then ri,j = li?,j? .
4. If the kth component of the characteristic string
of f is the empty string, then l0,k = r0,k.
3.2 General complexity
The time complexity of parsing LCFRS with respect
to the length of the input can be analyzed by counting
the maximum number of indexes that can appear in
an instance of the inference rule above. Although the
total number of indexes is
?m
i=0 2 ? ?(Ai), some of
these indexes are equated by the constraints.
To count the number of independent indexes, con-
sider all the indexes of the form l0,i (corresponding to
the left endpoints of each component of the character-
istic string of f ) and those of the form rj,k for j > 0
(corresponding to the right endpoints of each vari-
able in the characteristic string). By the constraints
above, these indexes are mutually independent, and it
is easy to check that any other index is equated to one
of these: indexes r0,i are equated to the index rj,k
corresponding to the last variable xj,k of the ith com-
ponent of the characteristic string, or to l0,i if there
is no such variable; while indexes lj,k with j > 0
are equated to an index l0,i if the variable xj,k is at
the beginning of a component of the characteristic
string, or to an index rj?,k?(j? > 1) if the variable xj,k
follows another variable xj?,k? .
279
[A1, (l1,1, r1,1), . . . , (l1,?(A1), r1,?(A1))] ? ? ? [Am, (lm,1, rm,1), . . . , (lm,?(Am), rm,?(Am))]
[A0, (l0,1, r0,1), . . . , (l0,?(A0), r0,?(A0))]
(a) The general rule for a parsing schema for LCFRS
[B, (l1, r1), . . . , (lm, rm)] [C, (l
?
1, r
?
1), . . . (l
?
n, r
?
n)]
[A, (l1, r1), . . . , (lm, r
?
1), . . . (l
?
n, r
?
n)]
rm = l?1
(b) Deduction step for concatenation
[B, (l1, r1), . . . , (lm, rm)] [C, (l
?
1, r
?
1), . . . (l
?
n, r
?
n)]
[A, (l1, r1), . . . , (li, r
?
1), . . . (l
?
n, ri+1), . . . , (lm, rm)]
ri = l?1, r
?
n = li+1
(c) Deduction step for wrapping
Figure 2: Deduction steps for parsing LCFRS.
Thus, the parsing complexity (Gildea, 2010) of a
production p = A0 ? f(A1, . . . , Am) is determined
by ?(A0) l-indexes and
?
i?[m] ?(Ai) r-indexes, for
a total complexity of
O(|w|?(A0)+
?
i?[m] ?(Ai))
where |w| is the length of the input string. The pars-
ing complexity of an LCFRS will correspond to the
maximum parsing complexity among its productions.
Note that this general complexity matches the result
given by Seki et al (1991).
In an LCFRS of rank ? and fan-out ?, the maxi-
mum possible parsing complexity is O(|w|?(?+1)),
obtained by applying the above expression to a pro-
duction of rank ? and where each nonterminal has fan-
out ?. The asymptotic time complexity of LCFRS
parsing is therefore exponential both in its rank and
its fan-out. This means that it is interesting to trans-
form LCFRS into equivalent forms that reduce their
rank while preserving the fan-out. For sets of LCFRS
that can be transformed into a binary form (i.e., such
that all its rules have rank at most 2), the ? factor in
the complexity is reduced to a constant, and complex-
ity is improved to O(|w|3?) (see G?mez-Rodr?guez
et al (2009) for further discussion). Unfortunately,
it is known by previous results (Rambow and Satta,
1999) that it is not always possible to convert an
LCFRS into such a binary form without increasing
the fan-out. However, we will show that it is always
possible to build such a binarization for well-nested
LCFRS. Combining this result with the inference
rule and complexity analysis given above, we would
obtain a parser for well-nested LCFRS running in
O(|w|3?) time. But the construction of our binary
normal form additionally restricts binary composition
operations in the binarized LCFRS to be of two spe-
cific forms, concatenation and wrapping, which fur-
ther improves the parsing complexity to O(|w|2?+2),
as we will see below.
3.3 Concatenation and wrapping
A composition operation is called a concatenation
operation, if its characteristic string has the form
x1,1 $ ? ? ? $ x1,m x2,1 $ ? ? ? $ x2,n ,
where m,n ? 1. Intuitively, such an operation corre-
sponds to the bottom-up combination of two adjacent
discontinuous constituents into one. An example of
a concatenation operation is the binary parsing rule
used by the standard CKY parser for context-free
grammars, which combines continuous constituents
(represented as 1-tuples of strings in the LCFRS nota-
tion). In the general case, a concatenation operation
will take an m-tuple and an n-tuple and return an
(m + n ? 1)-tuple, as the joined constituents may
have gaps that will also appear in the resulting tuple.
If we apply the general parsing rule given in Fig-
ure 2a to a production A? conc(B,C), where conc
is a concatenation operation, then we obtain the de-
duction step given in Figure 2b. This step uses 2m
different l- and r-indexes, and 2n? 1 different l?-
and r?-indexes (excluding l?1 which must equal rm),
for a total of 2m+2n?1 = 2(m+n?1)+1 indexes.
Since m+ n? 1 is the fan-out of the nonterminal A,
we conclude that the maximum number of indexes in
the step associated with a concatenation operation in
an LCFRS of fan-out ? is 2?+ 1.
280
before: p
? ? ?
t1 tm
after: p?
q
? ? ?
tq,1 tq,mq
r
? ? ?
tr,1 tr,mr
Figure 3: Transformation of derivation trees
A linear, non-erasing function is called a wrapping
operation, if its characteristic string has the form
x1,1 $ ? ? ? $ x1,i x2,1 $ ? ? ? $ x2,n x1,i+1 $ ? ? ? $ x1,m ,
where m,n ? 1 and i ? [m? 1]. Intuitively, such an
operation wraps the tuple derived from a nontermi-
nal B around the tuple derived from a nonterminal C,
filling the ith gap in the former. An example of a
wrapping operation is the adjunction of an auxiliary
tree in tree-adjoining grammar. In the general case, a
wrapping operation will take an m-tuple and an n-tu-
ple and return an (m + n ? 2)-tuple of strings: the
gaps of the argument tuples appear in the obtained
tuple, except for one gap in the tuple derived from B
which is filled by the tuple derived from C.
By applying the general parsing rule in Figure 2a
to a production A ? wrapi(B,C), where wrapi is
a wrapping operation, then we obtain the deduction
step given in Figure 2c. This step uses 2m different l-
and r-indexes, and 2n? 2 different l?- and r?-indexes
(discounting l?1 and r
?
n which are equal to other in-
dexes), for a total of 2m+2n?2 = 2(m+n?2)+2
indexes. Since the fan-out of A is m + n ? 2, this
means that a wrapping operation needs at most 2?+2
indexes for an LCFRS of fan-out ?.
From this, we conclude that an LCFRS of fan-
out ? in which all composition operations are ei-
ther concatenation operations, wrapping operations,
or operations of rank 0 or 1, can be parsed in time
O(|w|2?+2). In particular, nullary and unary compo-
sition operations do not affect this worst-case com-
plexity, since their associated deduction steps can
never have more than 2? indexes.
4 Transformation
We now show how to transform a well-nested LCFRS
into the normal form that we have just described.
4.1 Informal overview
Consider a production p = A ? f(A1, . . . , Am),
where m ? 2 and f is neither a concatenation nor a
wrapping operation. We will construct new produc-
tions p?, q, r such that every derivation that uses p can
be rewritten into a derivation that uses the new pro-
ductions, and the new productions do not license any
other derivations. Formally, this can be understood as
implementing a tree transformation, where the input
trees are derivations of the original grammar, and the
output trees are derivations of the new grammar. The
situation is illustrated in Figure 3. The tree on top
represents a derivation in the original grammar; this
derivation starts with the rewriting of the nontermi-
nal A using the production p, and continues with the
subderivations t1, . . . , tm. The tree at the bottom rep-
resents a derivation in the transformed grammar. This
derivation starts with the rewriting ofA using the new
production p?, and continues with two independent
subderivations that start with the new productions q
and r, respectively. The sub-derivations t1, . . . , tm
have been partitioned into two sequences
t1,1, . . . , t1,m1 and t2,1, . . . , t2,m2 .
The new production p? will be either a concatenation
or a wrapping operation, and the rank of both q and r
will be strictly smaller than the rank of p. The trans-
formation will continue with q and r, unless these
have rank one. By applying this strategy exhaustively,
we will thus eventually end up with a grammar that
only has productions with rank at most 2, and in
which all productions with rank 2 are either concate-
nation or wrapping operations.
4.2 Constructing the composition operations
To transform the production p, we first factorize the
composition operation f associated with p into three
new composition operations f ?, g, h as follows. Re-
call that we represent composition operations by their
characteristic strings.
In the following, we will assume that no charac-
teristic string starts or ends with a gap marker, or
contains immediate repetitions of gap markers. This
281
property can be ensured, without affecting the asymp-
totic complexity, by adding intermediate steps to the
transformation that we report here; we omit the de-
tails due to space reasons. When this property holds,
we are left with the following two cases. Let us call a
sequence of variables joint, if it contains all and only
variables associated with a given nonterminal.
Case 1 f = x1 f1 x2 ? ? ?xk?1 fk?1 xk f? ,
where k ? 1, x1, . . . , xk are joint variables, and the
suffix f? contains at least one variable. Let
g = x1 f1 x2 ? ? ?xk?1 fk?1 xk ,
let h = f?, and let f ? = conc. As f is well-nested,
both g and h define well-nested composition opera-
tions. By the specific segmentation of f , the ranks of
these operations are strictly smaller than the rank of f .
Furthermore, we have ?(f) = ?(g) + ?(h)? 1 .
Case 2 f = x1 f1 x2 ? ? ?xk?1 fk?1 xk ,
where k ? 2, x1, . . . , xk are joint variables, and there
exist at least one i such that the sequence fi contains
at least one variable. Choose an index j as follows:
if there is at least one i such that fi contains at least
one variable and one gap marker, let j be the minimal
such i; otherwise, let j be the minimal i such that fi
contains at least one variable. Now, let
g = x1 f1 x2 ? ? ?xj $ xj+1 ? ? ?xk?1 fk?1 xk ,
let h = fj , and let f ? = wrapj . As in Case 1, both g
and h define well-nested composition operations
whose ranks are strictly smaller than the rank of f .
Furthermore, we have ?(f) = ?(g) + ?(h)? 2 .
Note that at most one of the two cases can apply
to f . Furthermore, since f is well-nested, it is also
true that at least one of the two cases applies. This
is so because for two distinct nonterminals Ai, Ai? ,
either all variables associated with Ai? precede the
leftmost variable associated with Ai, succeed the
rightmost variable associated with Ai, or are placed
between two variables associated with Ai without an-
other variable associated with Ai intervening. (Here,
we have left out the symmetric cases.)
4.3 Constructing the new productions
Based on the composition operations, we now con-
struct three new productions p?, q, r as follows. LetB
and C be two fresh nonterminals with ?(B) = ?(g)
and ?(C) = ?(h), and let p? = A ? f ?(B,C).
The production p? rewrites A into B and C and
combines the two subderivations that originate at
these nonterminals using either a concatenation or a
wrapping operation. Now, let Aq,1, . . . , Aq,mq and
Ar,1, . . . , Ar,mr be the sequences of nonterminals
that are obtained from the sequence A1, . . . , Am by
deleting those nonterminals that are not associated
with any variable in g or h, respectively. Then, let
q = B ? g(Aq,1, . . . , Aq,mq) and
r = C ? h(?Ar,1, . . . , Ar,mr) .
4.4 Example
We now illustrate the transformation using the con-
crete production p = A? f(A1, A2, A3), where
f = x1,1 x2,1 $ x1,2 $ x3,1 .
Note that this operation has rank 3 and fan-out 3.
The composition operations are constructed as fol-
lows. The operation f matches the pattern of Case 1,
and hence induces the operations
g1 = x1,1 x2,1 $ x1,2 , h1 = $ x3,1 , f ?1 = conc .
The productions constructed from these are
p?1 = A? conc(B1, C1) ,
q1 = B1 ? g1(A1, A2) , r1 = C1 ? h1(A3) .
where B1 and C1 are fresh nonterminals with fan-
out 2. The production r1 has rank one, so it does not
require any further transformations. The transforma-
tion thus continues with q1. The operation g1 matches
the pattern of Case 2, and induces the operations
g2 = x1,1 $ x1,2 , h2 = x2,1$ , f ?2 = wrap1 .
The productions constructed from these are
p?2 = B1 ? wrap1(B2, C2) ,
q2 = B2 ? g2(A1) , r2 = C2 ? h2(A2) ,
where B2 and C2 are fresh nonterminals with fan-
out 2. At this point, the transformation terminates.
We can now delete p from the original grammar, and
replace it with the productions {p?1, r1, p
?
2, q2, r2}.
4.5 Correctness
To see that the transformation is correct, we need to
verify that each production of the original grammar
is transformed into a set of equivalent normal-form
productions, and that the fan-out of the new grammar
does not exceed the fan-out of the old grammar.
For the first point, we note that the transformation
preserves well-nestedness, decreases the rank of a
production, and is always applicable as long as the
282
rank of a production is at most 2 and the production
does not use a concatenation or wrapping operation.
That the new productions are equivalent to the old
ones in the sense of Figure 3 can be proved by induc-
tion on the length of a derivation in the original and
the new grammar, respectively.
Let us now convince ourselves that the fan-out of
the new grammar does not exceed the fan-out of the
old grammar. This is clear in Case 1, where
?(f) = ?(g) + ?(h)? 1
implies that both ?(g) ? ?(f) and ?(h) ? ?(f).
For Case 2, we reason as follows. The fan-out of the
operation h, being constructed from an infix of the
characteristic string of the original operation f , is
clearly bounded by the fan-out of f . For g, we have
?(g) = ?(f)? ?(h) + 2 ,
Now suppose that the index j was chosen according
to the first alternative. In this case, ?(h) ? 2, and
?(g) ? ?(f)? 2 + 2 = ?(f) .
For the case where j was chosen according to the
second alternative, ?(f) < k (since there are no
immediate repetitions of gap markers), ?(h) = 1,
and ?(g) ? k. If we assume that each nonterminal
is productive, then this means that the underlying
LCFRS has at least one production with fan-out k or
more; therefore, the fan-out of g does not increase
the fan-out of the original grammar.
4.6 Complexity
To conclude, we now briefly discuss the space com-
plexity of the normal-form transformation. We mea-
sure it in terms of the length of a production, defined
as the length of its string representation, that is, the
string A? [v1 $ ? ? ? $ vk](A1, . . . , Am) .
Looking at Figure 3, we note that the normal-form
transformation of a production p can be understood
as the construction of a (not necessarily complete)
binary-branching tree whose leaves correspond to the
productions obtained by splitting the characteristic
string of p and whose non-leaf nodes are labeled with
concatenation and wrapping operations. By construc-
tion, the sum of the lengths of leaf-node productions
is O(|p|). Since the number of inner nodes of a bi-
nary tree with n leaves is bounded by n ? 1, we
know that the tree hasO(?(p)) inner nodes. As these
nodes correspond to concatenation and wrapping
operations, each inner-node production has length
O(?(p)). Thus, the sum of the lengths of the produc-
tions created from |p| is O(|p|+ ?(p)?(p)). Since
the rank of a production is always smaller than its
length, this is reduced to O(|p|?(p)).
Therefore, the size of the normal-form transfor-
mation of an LCFRS G of fan-out ? is O(?|G|) in
the worst case, and linear space in practice, since
the fan-out is typically bounded by a small integer.
Taking the normal-form transformation into account,
our parser therefore runs in timeO(? ? |G| ? |w|2?+2)
where |G| is the original grammar size.
5 Conclusion
In this paper, we have presented an efficient parsing
algorithm for well-nested linear context-free rewrit-
ing systems, based on a new normal form for this
formalism. The normal form takes up linear space
with respect to grammar size, and the algorithm is
based on a bottom-up process that can be applied
to any LCFRS, achieving O(? ? |G| ? |w|2?+2) time
complexity when applied to LCFRS of fan-out ?
in our normal form. This complexity is an asymp-
totic improvement over existing results for this class,
both from parsers specifically geared to well-nested
LCFRS or equivalent formalisms (Hotz and Pitsch,
1996) and from applying general LCFRS parsing
techniques to the well-nested case (Seki et al, 1991).
The class of well-nested LCFRS is an interest-
ing syntactic formalism for languages with discon-
tinuous constituents, providing a good balance be-
tween coverage of linguistic phenomena in natu-
ral language treebanks (Kuhlmann and Nivre, 2006;
Maier and Lichte, 2009) and desirable formal prop-
erties (Kanazawa, 2009). Our results offer a further
argument in support of well-nested LCFRS: while
the complexity of parsing general LCFRS depends
on two dimensions (rank and fan-out), this bidimen-
sional hierarchy collapses into a single dimension
in the well-nested case, where complexity is only
conditioned by the fan-out.
Acknowledgments G?mez-Rodr?guez has been
supported by MEC/FEDER (HUM2007-66607-C04)
and Xunta de Galicia (PGIDIT07SIN005206PR, Re-
des Galegas de PL e RI e de Ling. de Corpus, Bolsas
Estad?as INCITE/FSE cofinanced). Kuhlmann has
been supported by the Swedish Research Council.
283
References
?ric Villemonte de la Clergerie. 2002. Parsing mildly
context-sensitive languages with thread automata. In
19th International Conference on Computational Lin-
guistics (COLING), pages 1?7, Taipei, Taiwan.
Daniel Gildea. 2010. Optimal parsing strategies for linear
context-free rewriting systems. In Human Language
Technologies: The Eleventh Annual Conference of the
North American Chapter of the Association for Compu-
tational Linguistics, Los Angeles, USA.
Carlos G?mez-Rodr?guez, Marco Kuhlmann, Giorgio
Satta, and David J. Weir. 2009. Optimal reduction
of rule length in linear context-free rewriting systems.
In Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 539?547,
Boulder, CO, USA.
Jir?? Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
608?615.
G?nter Hotz and Gisela Pitsch. 1996. On parsing coupled-
context-free languages. Theoretical Computer Science,
161(1?2):205?233.
Riny Huybregts. 1984. The weak inadequacy of context-
free phrase structure grammars. In Ger de Haan, Mieke
Trommelen, and Wim Zonneveld, editors, Van periferie
naar kern, pages 81?99. Foris, Dordrecht, The Nether-
lands.
Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.
1975. Tree Adjunct Grammars. Journal of Computer
and System Sciences, 10(2):136?163.
Aravind K. Joshi. 1985. Tree Adjoining Grammars: How
much context-sensitivity is required to provide reason-
able structural descriptions? In Natural Language
Parsing, pages 206?250. Cambridge University Press.
Laura Kallmeyer and Wolfgang Maier. 2009. An incre-
mental Earley parser for simple range concatenation
grammar. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT 2009), pages
61?64. Association for Computational Linguistics.
Makoto Kanazawa and Sylvain Salvati. 2010. The copy-
ing power of well-nested multiple context-free gram-
mars. In Fourth International Conference on Language
and Automata Theory and Applications, Trier, Ger-
many.
Makoto Kanazawa. 2009. The pumping lemma for well-
nested multiple context-free languages. In Develop-
ments in Language Theory. 13th International Confer-
ence, DLT 2009, Stuttgart, Germany, June 30?July 3,
2009. Proceedings, volume 5583 of Lecture Notes in
Computer Science, pages 312?325.
Marco Kuhlmann and Mathias M?hl. 2007. Mildly
context-sensitive dependency languages. In 45th An-
nual Meeting of the Association for Computational Lin-
guistics (ACL), pages 160?167.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In 21st International
Conference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational Lin-
guistics (COLING-ACL), Main Conference Poster Ses-
sions, pages 507?514, Sydney, Australia.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Twelfth Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 478?486, Athens, Greece.
Wolfgang Maier and Timm Lichte. 2009. Characterizing
discontinuity in constituent treebanks. In 14th Confer-
ence on Formal Grammar, Bordeaux, France.
Wolfgang Maier and Anders S?gaard. 2008. Treebanks
and mild context-sensitivity. In 13th Conference on
Formal Grammar, pages 61?76, Hamburg, Germany.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting systems.
Theoretical Computer Science, 223(1?2):87?120.
Giorgio Satta. 1992. Recognition of Linear Context-
Free Rewriting Systems. In 30th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 89?95, Newark, DE, USA.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On Multiple Context-Free Gram-
mars. Theoretical Computer Science, 88(2):191?229.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive pars-
ing. Journal of Logic Programming, 24(1?2):3?36.
Stuart M. Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Philoso-
phy, 8(3):333?343.
Klaas Sikkel. 1997. Parsing Schemata: A Framework
for Specification and Analysis of Parsing Algorithms.
Springer.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In 25th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 104?111, Stanford, CA, USA.
284
Proceedings of NAACL-HLT 2013, pages 487?496,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Approximate PCFG Parsing Using Tensor Decomposition
Shay B. Cohen
Department of Computer Science
Columbia University, USA
scohen@cs.columbia.edu
Giorgio Satta
Department of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Michael Collins
Department of Computer Science
Columbia University, USA
mcollins@cs.columbia.edu
Abstract
We provide an approximation algorithm for
PCFG parsing, which asymptotically im-
proves time complexity with respect to the in-
put grammar size, and prove upper bounds on
the approximation quality. We test our algo-
rithm on two treebanks, and get significant im-
provements in parsing speed.
1 Introduction
The problem of speeding-up parsing algorithms
based on probabilistic context-free grammars
(PCFGs) has received considerable attention in
recent years. Several strategies have been proposed,
including beam-search, best-first and A?. In this
paper we focus on the standard approach of approx-
imating the source PCFG in such a way that parsing
accuracy is traded for efficiency.
Nederhof (2000) gives a thorough presentation
of old and novel ideas for approximating non-
probabilistic CFGs by means of finite automata,
on the basis of specialized preprocessing of self-
embedding structures. In the probabilistic domain,
approximation by means of regular grammars is also
exploited by Eisner and Smith (2005), who filter
long-distance dependencies on-the-fly.
Beyond finite automata approximation, Charniak
et al (2006) propose a coarse-to-fine approach in
which an approximated (not necessarily regular)
PCFG is used to construct a parse forest for the in-
put sentence. Some statistical parameters are then
computed on such a structure, and exploited to filter
parsing with the non-approximated grammar. The
approach can also be iterated at several levels. In
the non-probabilistic setting, a similar filtering ap-
proach was also proposed by Boullier (2003), called
?guided parsing.?
In this paper we rely on an algebraic formulation
of the inside-outside algorithm for PCFGs, based on
a tensor formulation developed for latent-variable
PCFGs in Cohen et al (2012). We combine the
method with known techniques for tensor decompo-
sition to approximate the source PCFG, and develop
a novel algorithm for approximate PCFG parsing.
We obtain improved time upper bounds with respect
to the input grammar size for PCFG parsing, and
provide error upper bounds on the PCFG approxi-
mation, in contrast with existing heuristic methods.
2 Preliminaries
This section introduces the special representation for
probabilistic context-free grammars that we adopt in
this paper, along with the decoding algorithm that
we investigate. For an integer i ? 1, we let [i] =
{1, 2, . . . , i}.
2.1 Probabilistic Context-Free Grammars
We consider context-free grammars (CFGs) in
Chomsky normal form, and denote them as
(N ,L,R) where:
? N is the finite set of nonterminal symbols, with
m = |N |, and L is the finite set of words (lexi-
cal tokens), with L?N = ? and with n = |L|.
? R is a set of rules having the form a? b c,
a, b, c ? N , or the form a? x, a ? N and
x ? L.
A probabilistic CFG (PCFG) is a CFG associated
with a set of parameters defined as follows:
? For each (a? b c) ? R, we have a parameter
p(a? b c | a).
487
? For each (a? x) ? R, we have a parameter
p(a? x | a).
? For each a ? N , we have a parameter pia,
which is the probability of a being the root
symbol of a derivation.
The parameters above satisfy the following nor-
malization conditions:
?
(a?b c)?R
p(a? b c | a) +
?
(a?x)?R
p(a? x | a) = 1,
for each a ? N , and
?
a?N pia = 1.
The probability of a tree ? deriving a sentence in
the language, written p(?), is calculated as the prod-
uct of the probabilities of all rule occurrences in ? ,
times the parameter pia where a is the symbol at the
root of ? .
2.2 Tensor Form of PCFGs
A three-dimensional tensor C ? R(m?m?m) is a
set of m3 parameters Ci,j,k for i, j, k ? [m]. In what
follows, we associate with each tensor three func-
tions, each mapping a pair of vectors in Rm into a
vector in Rm.
Definition 1 Let C ? R(m?m?m) be a tensor.
Given two vectors y1, y2 ? Rm, we let C(y1, y2)
be them-dimensional row vector with components:
[C(y1, y2)]i =
?
j?[m],k?[m]
Ci,j,ky
1
j y
2
k .
We also let C(1,2)(y1, y2) be them-dimensional col-
umn vector with components:
[C(1,2)(y
1, y2)]k =
?
i?[m],j?[m]
Ci,j,ky
1
i y
2
j .
Finally, we let C(1,3)(y1, y2) be the m-dimensional
column vector with components:
[C(1,3)(y
1, y2)]j =
?
i?[m],k?[m]
Ci,j,ky
1
i y
2
k .
For two vectors x, y ? Rm we denote by x y ?
Rm the Hadamard product of x and y, i.e., [xy]i =
xiyi. Finally, for vectors x, y, z ? Rm, xy>z> is the
tensor D ? Rm?m?m where Di,j,k = xiyjzk (this
is analogous to the outer product: [xy>]i,j = xiyj).
We extend the parameter set of our PCFG such
that p(a? b c | a) = 0 for all a? b c not in R,
and p(a? x | a) = 0 for all a? x not in R. We
also represent each a ? N by a unique index in [m],
and we represent each x ? L by a unique index in
[n]: it will always be clear from the context whether
these indices refer to a nonterminal inN or else to a
word in L.
In this paper we assume a tensor representation
for the parameters p(a? b c | a), and we denote by
T ? Rm?m?m a tensor such that:
Ta,b,c , p(a? b c | a).
Similarly, we denote by Q ? Rm?n a matrix such
that:
Qa,x , p(a? x | a).
The root probabilities are denoted using a vector pi ?
Rm?1 such that pia is defined as before.
2.3 Minimum Bayes-Risk Decoding
Let z = x1 ? ? ?xN be some input sentence; we write
T (z) to denote the set of all possible trees for z. It
is often the case that parsing aims to find the high-
est scoring tree ?? for z according to the underlying
PCFG, also called the ?Viterbi parse:?
?? = argmax
??T (z)
p(?)
Goodman (1996) noted that Viterbi parsers do not
optimize the same metric that is usually used for
parsing evaluation (Black et al, 1991). He sug-
gested an alternative algorithm, which he called the
?Labelled Recall Algorithm,? which aims to fix this
issue.
Goodman?s algorithm has two phases. In the first
phase it computes, for each a ? N and for each sub-
string xi ? ? ?xj of z, the marginal ?(a, i, j) defined
as:
?(a, i, j) =
?
??T (z) : (a,i,j)??
p(?).
Here we write (a, i, j) ? ? if nonterminal a spans
words xi ? ? ?xj in the parse tree ? .
488
Inputs: Sentence x1 ? ? ?xN , PCFG (N ,L,R), pa-
rameters T ? R(m?m?m), Q ? R(m?n), pi ?
R(m?1).
Data structures:
? Each ?(a, i, j) ? R for a ? N , i, j ? [N ],
i ? j, is a marginal probability.
? Each ?i,j ? R for i, j ? [N ], i ? j, is the high-
est score for a tree spanning substring xi ? ? ?xj .
Algorithm:
(Marginals) ?a ? N ,?i, j ? [N ], i ? j, compute
the marginals ?(a, i, j) using the inside-outside
algorithm.
(Base case) ?i ? [N ],
?i,i = max
(a?xi)?R
?(a, i, i)
(Maximize Labelled Recall) ?i, j ? [N ], i < j,
?i,j = max
a?N
?(a, i, j) + max
i?k<j
(
?i,k + ?k+1,j
)
Figure 1: The labelled recall algorithm from Goodman
(1996). The algorithm in this figure finds the highest
score for a tree which maximizes labelled recall. The ac-
tual parsing algorithm would use backtrack pointers in
the score computation to return a tree. These are omitted
for simplicity.
The second phase includes a dynamic program-
ming algorithm which finds the tree ?? that maxi-
mizes the sum over marginals in that tree:
?? = argmax
??T (z)
?
(a,i,j)??
?(a, i, j).
Goodman?s algorithm is described in Figure 1.
As Goodman notes, the complexity of the second
phase (?Maximize Labelled Recall,? which is also
referred to as ?minimum Bayes risk decoding?) is
O(N3 +mN2). There are two nested outer loops,
each of order N , and inside these, there are two sep-
arate loops, one of order m and one of order N ,
yielding this computational complexity. The reason
for the linear dependence on the number of nonter-
minals is the lack of dependence on the actual gram-
mar rules, once the marginals are computed.
In its original form, Goodman?s algorithm does
not enforce that the output parse trees are included in
the tree language of the PCFG, that is, certain com-
binations of children and parent nonterminals may
violate the rules in the grammar. In our experiments
we departed from this, and changed Goodman?s al-
gorithm by incorporating the grammar into the dy-
namic programming algorithm in Figure 1. The rea-
son this is important for our experiments is that we
binarize the grammar prior to parsing, and we need
to enforce the links between the split nonterminals
(in the binarized grammar) that refer to the same
syntactic category. See Matsuzaki et al (2005) for
more details about the binarization scheme we used.
This step changes the dynamic programming equa-
tion of Goodman to be linear in the size of the gram-
mar (figure 1). However, empirically, it is the inside-
outside algorithm which takes most of the time to
compute with Goodman?s algorithm. In this paper
we aim to asymptotically reduce the time complex-
ity of the calculation of the inside-outside probabili-
ties using an approximation algorithm.
3 Tensor Formulation of the
Inside-Outside Algorithm
At the core of our approach lies the observation that
there is a (multi)linear algebraic formulation of the
inside-outside algorithm. It can be represented as a
series of tensor, matrix and vector products. A sim-
ilar observation has been made for latent-variable
PCFGs (Cohen et al, 2012) and hidden Markov
models, where only matrix multiplication is required
(Jaeger, 2000). Cohen and Collins (2012) use this
observation together with tensor decomposition to
improve the speed of latent-variable PCFG parsing.
The representation of the inside-outside algorithm
in tensor form is given in Figure 2. For example,
if we consider the recursive equation for the inside
probabilities (where ?i,j is a vector varying over the
nonterminals in the grammar, describing the inside
probability for each nonterminal spanning words i
to j):
?i,j =
j?1?
k=i
T (?i,k, ?k+1,j)
489
Inputs: Sentence x1 ? ? ?xN , PCFG (N ,L,R), pa-
rameters T ? R(m?m?m), Q ? R(m?n), pi ?
R(m?1).
Data structures:
? Each ?i,j ? R1?m, i, j ? [N ], i ? j, is a row
vector of inside terms ranging over a ? N .
? Each ?i,j ? Rm?1, i, j ? [N ], i ? j, is a
column vector of outside terms ranging over
a ? N .
? Each ?(a, i, j) ? R for a ? N , i, j ? [N ],
i ? j, is a marginal probability.
Algorithm:
(Inside base case) ?i ? [N ], ?(a? xi) ? R,
[?i,i]a = Qa,x
(Inside recursion) ?i, j ? [N ], i < j,
?i,j =
j?1?
k=i
T (?i,k, ?k+1,j)
(Outside base case) ?a ? N ,
[?1,N ]a = pia
(Outside recursion) ?i, j ? [N ], i ? j,
?i,j =
i?1?
k=1
T(1,2)(?
k,j , ?k,i?1)+
N?
k=j+1
T(1,3)(?
i,k, ?j+1,k)
(Marginals) ?a ? N ,?i, j ? [N ], i ? j,
?(a, i, j) = [?i,j ]a ? [?i,j ]a
Figure 2: The tensor form of the inside-outside algorithm,
for calculation of marginal terms ?(a, i, j).
and then apply the tensor product from Definition 1
to this equation, we get that coordinate a in ?i,j is
defined recursively as follows:
[?i,j ]a =
j?1?
k=i
?
b,c
Ta,b,c ? ?
i,k
b ? ?
k+1,j
c
=
j?1?
k=i
?
b,c
p(a? b c | a)? ?i,kb ? ?
k+1,j
c ,
which is exactly the recursive definition of the inside
algorithm. The correctness of the outside recursive
equations follows very similarly.
The time complexity of the algorithm in this case
is O(m3N3). To see this, observe that each tensor
application takes timeO(m3). Furthermore, the ten-
sor T is applied O(N) times in the computation of
each vector ?i,j and ?i,j . Finally, we need to com-
pute a total ofO(N2) inside and outside vectors, one
for each substring of the input sentence.
4 Tensor Decomposition for the
Inside-Outside Algorithm
In this section, we detail our approach to approxi-
mate parsing using tensor decomposition.
4.1 Tensor Decomposition
In the formulation of the inside-outside algorithm
based on tensor T , each vector ?i,j and ?i,j consists
of m elements, where computation of each element
requires timeO(m2). Therefore, the algorithm has a
O(m3) multiplicative factor in its time complexity,
which we aim to reduce by means of an approximate
algorithm.
Our approximate method relies on a simple ob-
servation. Given an integer r ? 1, assume that
the tensor T has the following special form, called
?Kruskal form:?
T =
r?
i=1
?iuiv>i w
>
i . (1)
In words, T is the sum of r tensors, where each
tensor is obtained as the product of three vectors
ui, vi and wi, together with a scalar ?i. Exact
Kruskal decomposition of a tensor is not necessarily
unique. See Kolda and Bader (2009) for discussion
of uniqueness of tensor decomposition.
490
Consider now two vectors y1, y2 ? Rm, associ-
ated with the inside probabilities for the left (y1) and
right child (y2) of a given node in a parse tree. Let
us introduce auxiliary arrays U, V,W ? Rr?m, with
the i-th row being ui, vi and wi, respectively. Let
also ? = (?1, . . . , ?r). Using the decomposition in
Eq. (1) within Definition 1 we can express the array
T (y1, y2) as:
T (y1, y2) =
[
r?
i=1
?iuiv>i w
>
i
]
(y1, y2) =
r?
i=1
?iui(v>i y
1)(w>i y
2) =
(
U>(? V y1 Wy2)
)
. (2)
The total complexity of the computation in Eq. (2)
is nowO(rm). It is well-known that an exact tensor
decomposition for T can be achieved with r = m2
(Kruskal, 1989). In this case, there is no computa-
tional gain in using Eq. (2) for the inside calculation.
The minimal r required for an exact tensor decom-
position can be smaller than m2. However, identify-
ing that minimal r is NP-hard (H?astad, 1990).
In this section we focused on the computa-
tion of the inside probabilities through vectors
T (?i,k, ?k+1,j). Nonetheless, the steps above can
be easily adapted for the computation of the outside
probabilities through vectors T(1,2)(?k,j , ?k,i?1)
and T(1,3)(?i,k, ?j+1,k).
4.2 Approximate Tensor Decomposition
The PCFG tensor T will not necessarily have the ex-
act decomposed form in Eq. (1). We suggest to ap-
proximate the tensor T by finding the closest tensor
according to some norm over Rm?m?m.
An example of such an approximate decom-
position is the canonical polyadic decomposition
(CPD), also known as CANDECOMP/PARAFAC
decomposition (Carroll and Chang, 1970; Harsh-
man, 1970; Kolda and Bader, 2009). Given an in-
teger r, least squares CPD aims to find the nearest
tensor in Kruskal form, minimizing squared error.
More formally, for a given tensor D ? Rm?m?m,
let ||D||F =
??
i,j,kD
2
i,j,k. Let the set of tensors in
Kruskal form Cr be:
Cr ={C ? Rm?m?m | C =
r?
i=1
?iuiv>i w
>
i
s.t. ?i ? R, ui, vi, wi ? Rm,
||ui||2 = ||vi||2 = ||wi||2 = 1}.
The least squares CPD of C is a tensor C? such
that C? ? argminC??Cr ||C ? C?||F . Here, we treat
the argmin as a set because there could be multiple
solutions which achieve the same accuracy.
There are various algorithms to perform CPD,
such as alternating least squares, direct linear de-
composition, alternating trilinear decomposition and
pseudo alternating least squares (Faber et al, 2003)
and even algorithms designed for sparse tensors (Chi
and Kolda, 2011). Most of these algorithms treat
the problem of identifying the approximate tensor as
an optimization problem. Generally speaking, these
optimization problems are hard to solve, but they
work quite well in practice.
4.3 Parsing with Decomposed Tensors
Equipped with the notion of tensor decomposition,
we can now proceed with approximate tensor pars-
ing in two steps. The first is approximating the ten-
sor using a CPD algorithm, and the second is apply-
ing the algorithms in Figure 1 and Figure 2 to do
parsing, while substituting all tensor product com-
putations with the approximate O(rm) operation of
tensor product.
This is not sufficient to get a significant speed-up
in parsing time. Re-visiting Eq. (2) shows that there
are additional ways to speed-up the tensor applica-
tion T in the context of the inside-outside algorithm.
The first thing to note is that the projections V y1
and Wy2 in Eq. (2) can be cached, and do not have
to be re-calculated every time the tensor is applied.
Here, y1 and y2 will always refer to an outside or
an inside probability vector over the nonterminals in
the grammar. Caching these projections means that
after each computation of an inside or outside proba-
bility, we can immediately project it to the necessary
r-dimensional space, and then re-use this computa-
tion in subsequent application of the tensor.
The second thing to note is that the U projection
in T can be delayed, because of rule of distributiv-
ity. For example, the step in Figure 2 that computes
491
the inside probability ?i,j can be re-formulated as
follows (assuming an exact decomposition of T ):
?i,j =
j?1?
k=i
T (?i,k, ?k+1,j)
=
j?1?
k=1
U>(? V ?i,k W?k+1,j)
= U>
(j?1?
k=1
(? V ?i,k W?k+1,j)
)
. (3)
This means that projection through U can be done
outside of the loop over splitting points in the sen-
tence. Similar reliance on distributivity can be used
to speed-up the outside calculations as well.
The caching speed-up and the delayed projection
speed-up make the approximate inside-outside com-
putation asymptotically faster. While na??ve applica-
tion of the tensor yields an inside algorithm which
runs in time O(rmN3), the improved algorithm
runs in time O(rN3 + rmN2).
5 Quality of Approximate Tensor Parsing
In this section, we give the main approximation re-
sult, that shows that the probability distribution in-
duced by the approximate tensor is close to the orig-
inal probability distribution, if the distance between
the approximate tensor and the rule probabilities is
not too large.
Denote by T (N) the set of trees in the tree lan-
guage of the PCFG with N words (any nontermi-
nal can be the root of the tree). Let T (N) be the
set of pairs of trees ? = (?1, ?2) such that the to-
tal number of binary rules combined in ?1 and ?2 is
N ? 2 (this means that the total number of words
combined is N ). Let T? be the approximate ten-
sor for T . Denote the probability distribution in-
duced by T? by p?.1 Define the vector ?(?) such that
[?(?)]a = Ta,b,c ? p(?1 | b) ? p(?2 | c) where the root
?1 is nonterminal b and the root of ?2 is c. Similarly,
define [??(?)]a = T?a,b,c ? p?(?1 | b) ? p?(?2 | c).
Define Z(a,N) =
?
??T (N)[??(?)]a. In addition,
define D(a,N) =
?
??T (N) |[??(?)]a ? [?(?)]a|
1Here, p? does not have to be a distribution, because T? could
have negative values, in principle, and its slices do not have to
normalize to 1. However, we just treat p? as a function that maps
trees to products of values according to T? .
and define F (a,N) = D(a,N)/Z(a,N). De-
fine ? = ||T? ? T ||F . Last, define ? =
min(a?b c)?R p(a? b c | a). Then, the following
lemma holds:
Lemma 1 For any a and any N , it holds:
D(a,N) ? Z(a,N)
(
(1 + ?/?)N ? 1
)
Proof sketch: The proof is by induction on N .
Assuming that 1 + F (b, k) ? (1 + ?/?)k and
1 + F (c,N ? k ? 1) ? (1 + ?/?)N?k?1 for F
defined as above (this is the induction hypothesis), it
can be shown that the lemma holds. 
Lemma 2 The following holds for any N :
?
??T (N)
|p?(?)? p(?)| ? m
(
(1 + ?/?)N ? 1
)
Proof sketch: Using Ho?lder?s inequality and
Lemma 1 and the fact that Z(a,N) ? 1, it follows
that:
?
??T (N)
|p?(?)? p(?)| ?
?
??T (N),a
|[?(?)]a ? [??(?)]a|
?
(
?
a
Z(a,N)
)
(
(1 + ?/?)N ? 1
)
? m
(
(1 + ?/?)N ? 1
)

Then, the following is a result that explains how
accuracy changes as a function of the quality of the
tensor approximation:
Theorem 1 For anyN , and  < 1/4, it holds that if
? ?
?
2Nm
, then:
?
??T (N)
|p?(?)? p(?)| ? 
Proof sketch: This is the result of applying Lemma 2
together with the inequality (1 + y/t)t? 1 ? 2y for
any t > 0 and y ? 1/2. 
492
We note that Theorem 1 also implicitly bounds
the difference between a marginal ?(a, i, j) and its
approximate version. A marginal corresponds to a
sum over a subset of summands in Eq. (1).
A question that remains at this point is to decide
whether for a given grammar, the optimal ? that can
be achieved is large or small. We define:
??r = min
T??Cr
||T ? T? ||F (4)
The following theorem gives an upper bound on
the value of ??r based on intrinsic property of the
grammar, or more specifically T . It relies on the
fact that for three-dimensional tensors, where each
dimension is of length m, there exists an exact de-
composition of T using m2 components.
Theorem 2 Let:
T =
m2?
i=1
??iu
?
i (v
?
i )
>(w?i )
>
be an exact Kruskal decomposition of T such that
||u?i ||2 = ||v
?
i ||2 = ||w
?
i || = 1 and ?
?
i ? ?
?
i+1 for
i ? [m2 ? 1]. Then, for a given r, it holds:
??r ?
m2?
i=r+1
|??i |
Proof: Let T? be a tensor that achieves the minimum
in Eq. (4). Define:
T ?r =
r?
i=1
??iu
?
i (v
?
i )
>(w?i )
>
Then, noting that ??r is a minimizer of the norm
difference between T and T? and then applying the
triangle inequality and then Cauchy-Schwartz in-
equality leads to the following chain of inequalities:
??r = ||T ? T? ||F ? ||T ? T
?
r||F
= ||
m2?
i=r+1
??iu
?
i (v
?
i )
>(w?i )
>||F
?
m2?
i=r+1
|??i | ? ||u
?
i (v
?
i )
>(w?i )
>||F =
m2?
i=r+1
|??i |
as required. 
6 Experiments
In this section, we describe experiments that demon-
strate the trade-off between the accuracy of the ten-
sor approximation (and as a consequence, the accu-
racy of the approximate parsing algorithm) and pars-
ing time.
Experimental Setting We compare the tensor ap-
proximation parsing algorithm versus the vanilla
Goodman algorithm. Both algorithms were imple-
mented in Java, and the code for both is almost iden-
tical, except for the set of instructions which com-
putes the dynamic programming equation for prop-
agating the beliefs up in the tree. This makes the
clocktime comparison reliable for drawing conclu-
sions about the speed of the algorithms. Our im-
plementation of the vanilla parsing algorithm is lin-
ear in the size of the grammar (and not cubic in the
number of nonterminals, which would give a worse
running time).
In our experiments, we use the method described
in Chi and Kolda (2011) for tensor decomposition.2
This method is fast, even for large tensors, as long
as they are sparse. Such is the case with the tensors
for our grammars.
We use two treebanks for our comparison: the
Penn treebank (Marcus et al, 1993) and the Arabic
treebank (Maamouri et al, 2004). With the Penn
treebank, we use sections 2?21 for training a max-
imum likelihood model and section 22 for parsing,
while for the Arabic treebank we divide the data into
two sets, of size 80% and 20%, one is used for train-
ing a maximum likelihood model and the other is
used for parsing.
The number of binary rules in the treebank gram-
mar is 7,240. The number of nonterminals is 112
and the number of preterminals is 2593Unary rules
are removed by collapsing non-terminal chains. This
increased the number of preterminals. The number
of binary rules in the Arabic treebank is significantly
smaller and consists of 232 rules. We run all parsing
experiments on sentences of length ? 40. The num-
ber of nonterminals is 48 and the number of preter-
2We use the implementation given in Sandia?s Mat-
lab Tensor Toolbox, which can be downloaded at http:
//www.sandia.gov/?tgkolda/TensorToolbox/
index-2.5.html.
3.
493
rank (r) baseline 20 60 100 140 180 220 260 300 340
Ara
bic speed 0.57 0.04 0.06 0.1 0.12 0.16 0.19 0.22 0.26 0.28
F1 63.78 51.80 58.39 63.63 63.77 63.88 63.82 63.84 63.80 63.88
Eng
lish speed 3.89 0.15 0.21 0.30 0.37 0.44 0.52 0.60 0.70 0.79
F1 71.07 57.83 61.67 68.28 69.63 70.30 70.82 71.42 71.28 71.13
Table 1: Results for the Arabic and English treebank of parsing using a vanilla PCFG with and without tensor decom-
position. Speed is given in seconds per sentence.
minals is 81.
Results Table 1 describes the results of compar-
ing the tensor decomposition algorithm to the vanilla
PCFG parsing algorithm.
The first thing to note is that the running time of
the parsing algorithm is linear in r. This indeed
validates the asymptotic complexity of the inside-
outside component in Goodman?s algorithm with the
approximate tensors. It also shows that most of the
time during parsing is spent on the inside-outside al-
gorithm, and not on the dynamic programming algo-
rithm which follows it.
In addition, compared to the baseline which uses
a vanilla CKY algorithm (linear in the number of
rules), we get a speed up of a factor of 4.75 for
Arabic (r = 140) and 6.5 for English (r = 260)
while retaining similar performance. Perhaps more
surprising is that using the tensor approximation ac-
tually improves performance in several cases. We
hypothesize that the cause of this is that the tensor
decomposition requires less parameters to express
the rule probabilities in the grammar, and therefore
leads to better generalization than a vanilla maxi-
mum likelihood estimate.
We include results for a more complex model for
Arabic, which uses horizontal Markovization of or-
der 1 and vertical Markovization of order 2 (Klein
and Manning, 2003). This grammar includes 2,188
binary rules. Parsing exhaustively using this gram-
mar takes 1.30 seconds per sentence (on average)
with an F1 measure of 64.43. Parsing with tensor
decomposition for r = 280 takes 0.62 seconds per
sentence (on average) with an F1 measure of 64.05.
7 Discussion
In this section, we briefly touch on several other top-
ics related to tensor approximation.
7.1 Approximating the Probability of a String
The probability of a sentence z under a PCFG is de-
fined as p(z) =
?
??T (z) p(?), and can be approx-
imated using the algorithm in Section 4.3, running
in time O(rN3 + rmN2). Of theoretical interest,
we discuss here a time O(rN3 + r2N2) algorithm,
which is more convenient when r < m.
Observe that in Eq. (3) vector ?i,j always appears
within one of the two terms V ?i,j and W?i,j in
Rr?1, whose dimensions are independent of m.
We can therefore use Eq. (3) to compute V ?i,j as
V ?i,j = V U>
(?j?1
k=1(? V ?
i,k W?k+1,j)
)
,
where V U> is a Rr?r matrix that can be
computed off-line, i.e., independently of
z. A symmetrical relation can be used
to compute W?i,j . Finally, we can write
p(z) = pi>U
(?N?1
k=1 (? V ?
1,k W?k+1,N )
)
,
where pi>U is a R1?r vector that can again be
computed off-line. This algorithm then runs in time
O(rN3 + r2N2).
7.2 Applications to Dynamic Programming
The approximation method presented in this paper is
not limited to PCFG parsing. A similar approxima-
tion method has been used for latent-variable PCFGs
(Cohen and Collins, 2012), and in general, ten-
sor approximation can be used to speed-up inside-
outside algorithms for general dynamic program-
ming algorithms or weighted logic programs (Eisner
et al, 2004; Cohen et al, 2011). In the general case,
the dimension of the tensors will not be necessarily
just three (corresponding to binary rules), but can be
of a higher dimension, and therefore the speed gain
can be even greater. In addition, tensor approxima-
tion can be used for computing marginals of latent
variables in graphical models.
For example, the complexity of the forward-
494
backward algorithm for HMMs can be reduced to
be linear in the number of states (as opposed to
quadratic) and linear in the rank used in an approxi-
mate singular-value decomposition (instead of ten-
sor decomposition) of the transition and emission
matrices.
7.3 Tighter (but Slower) Approximation Using
Singular Value Decomposition
The accuracy of the algorithm depends on the ability
of the tensor decomposition algorithm to decompose
the tensor with a small reconstruction error. The de-
composition algorithm is performed on the tensor T
which includes all rules in the grammar.
Instead, one can approach the approximation by
doing a decomposition for each slice of T separately
using singular value decomposition. This will lead
to a more accurate approximation, but will also lead
to an extra factor of m during parsing. This factor
is added because now there is not a single U , V and
W , but instead there are such matrices for each non-
terminal in the grammar.
8 Conclusion
We described an approximation algorithm for prob-
abilistic context-free parsing. The approximation al-
gorithm is based on tensor decomposition performed
on the underlying rule table of the CFG grammar.
The approximation algorithm leads to significant
speed-up in PCFG parsing, with minimal loss in per-
formance.
References
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of DARPA Work-
shop on Speech and Natural Language.
P. Boullier. 2003. Guided earley parsing. In 8th In-
ternational Workshop on Parsing Technologies, pages
43?54.
J. D. Carroll and J. J. Chang. 1970. Analysis of indi-
vidual differences in multidimensional scaling via an
N-way generalization of Eckart-Young decomposition.
Psychometrika, 35:283?319.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil,
D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore,
M. Pozar, and T. Vu. 2006. Multilevel coarse-to-fine
pcfg parsing. In Proceedings of HLT-NAACL.
E. C. Chi and T. G. Kolda. 2011. On tensors, spar-
sity, and nonnegative factorizations. arXiv:1112.2414
[math.NA], December.
S. B. Cohen and M. Collins. 2012. Tensor decomposi-
tion for fast latent-variable PCFG parsing. In Proceed-
ings of NIPS.
S. B. Cohen, R. J. Simmons, and N. A. Smith. 2011.
Products of weighted logic programs. Theory and
Practice of Logic Programming, 11(2?3):263?296.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft and
hard constraints on dependency length. In Proceed-
ings of IWPT, Parsing ?05.
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A
declarative language for implementing dynamic pro-
grams. In Proc. of ACL (companion volume).
N. M. Faber, R. Bro, and P. Hopke. 2003. Recent devel-
opments in CANDECOMP/PARAFAC algorithms: a
critical review. Chemometrics and Intelligent Labora-
tory Systems, 65(1):119?137.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of ACL.
R. A. Harshman. 1970. Foundations of the PARAFAC
procedure: Models and conditions for an ?explana-
tory? multi-modal factor analysis. UCLA working pa-
pers in phoentics, 16:1?84.
J. H?astad. 1990. Tensor rank is NP-complete. Algo-
rithms, 11:644?654.
H. Jaeger. 2000. Observable operator models for dis-
crete stochastic time series. Neural Computation,
12(6):1371?1398.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of ACL.
T. G. Kolda and B. W. Bader. 2009. Tensor decomposi-
tions and applications. SIAM Rev., 51:455?500.
J. B. Kruskal. 1989. Rank, decomposition, and unique-
ness for 3-way and N-way arrays. In R. Coppi and
S. Bolasco, editors, Multiway Data Analysis, pages 7?
18.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a large-
scale annotated Arabic corpus. In Proceedings NEM-
LAR.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19(2):313?330.
495
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of ACL.
M.-J. Nederhof. 2000. Practical experiments with regu-
lar approximation of context-free languages. Compu-
tational Linguistics, 26(1):17?44.
496
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 525?533,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Optimal rank reduction
for Linear Context-Free Rewriting Systems with Fan-Out Two
Benot Sagot
INRIA & Universite? Paris 7
Le Chesnay, France
benoit.sagot@inria.fr
Giorgio Satta
Department of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
Linear Context-Free Rewriting Systems
(LCFRSs) are a grammar formalism ca-
pable of modeling discontinuous phrases.
Many parsing applications use LCFRSs
where the fan-out (a measure of the dis-
continuity of phrases) does not exceed 2.
We present an efficient algorithm for opti-
mal reduction of the length of production
right-hand side in LCFRSs with fan-out at
most 2. This results in asymptotical run-
ning time improvement for known parsing
algorithms for this class.
1 Introduction
Linear Context-Free Rewriting Systems
(LCFRSs) have been introduced by Vijay-
Shanker et al (1987) for modeling the syntax
of natural language. The formalism extends the
generative capacity of context-free grammars, still
remaining far below the class of context-sensitive
grammars. An important feature of LCFRSs is
their ability to generate discontinuous phrases.
This has been recently exploited for modeling
phrase structure treebanks with discontinuous
constituents (Maier and S?gaard, 2008), as well as
non-projective dependency treebanks (Kuhlmann
and Satta, 2009).
The maximum number f of tuple components
that can be generated by an LCFRS G is called
the fan-out of G, and the maximum number r of
nonterminals in the right-hand side of a production
is called the rank of G. As an example, context-
free grammars are LCFRSs with f = 1 and r
given by the maximum length of a production
right-hand side. Tree adjoining grammars (Joshi
and Levy, 1977) can also be viewed as a special
kind of LCFRS with f = 2, since each auxil-
iary tree generates two strings, and with r given
by the maximum number of adjunction and sub-
stitution sites in an elementary tree. Beyond tree
adjoining languages, LCFRSs with f = 2 can
also generate languages in which pair of strings
derived from different nonterminals appear in so-
called crossing configurations. It has recently been
observed that, in this way, LCFRSs with f = 2
can model the vast majority of data in discontinu-
ous phrase structure treebanks and non-projective
dependency treebanks (Maier and Lichte, 2009;
Kuhlmann and Satta, 2009).
Under a theoretical perspective, the parsing
problem for LCFRSs with f = 2 is NP-complete
(Satta, 1992), and in known parsing algorithms
the running time is exponentially affected by the
rank r of the grammar. Nonetheless, in natu-
ral language parsing applications, it is possible to
achieve efficient, polynomial parsing if we suc-
ceed in reducing the rank r (number of nontermi-
nals in the right-hand side) of individual LCFRSs?
productions (Kuhlmann and Satta, 2009). This
process is called production factorization. Pro-
duction factorization is very similar to the reduc-
tion of a context-free grammar production into
Chomsky normal form. However, in the LCFRS
case some productions might not be reducible to
r = 2, and the process stops at some larger value
for r, which in the worst case might as well be the
rank of the source production (Rambow and Satta,
1999).
Motivated by parsing efficiency, the factoriza-
tion problem for LCFRSs with f = 2 has at-
tracted the attention of many researchers in recent
years. Most of the literature has been focusing on
binarization algorithms, which attempt to find a re-
duction to r = 2 and return a failure if this is not
possible. Go?mez-Rodr??guez et al (2009) report a
general binarization algorithm for LCFRS which,
in the case of f = 2, works in time O(|p|7), where
|p| is the size of the input production. A more ef-
ficient binarization algorithm for the case f = 2 is
presented in (Go?mez-Rodr??guez and Satta, 2009),
working in time O(|p|).
525
In this paper we are interested in general factor-
ization algorithms, i.e., algorithms that find factor-
izations with the smallest possible rank (not nec-
essarily r = 2). We present a novel technique that
solves the general factorization problem in time
O(|p|2) for LCFRSs with f = 2.
Strong generative equivalence results between
LCFRS and other finite copying parallel rewrit-
ing systems have been discussed in (Weir, 1992)
and in (Rambow and Satta, 1999). Through these
equivalence results, we can transfer the factoriza-
tion techniques presented in this article to other
finite copying parallel rewriting systems.
2 LCFRSs
In this section we introduce the basic notation for
LCFRS and the notion of production factoriza-
tion.
2.1 Definitions
Let ?T be a finite alphabet of terminal symbols.
As usual, ? ?T denotes the set of all finite strings
over ?T , including the empty string ?. For in-
teger k ? 1, (? ?T )k denotes the set of all tuples
(w1, . . . , wk) of strings wi ? ? ?T . In what follows
we are interested in functions mapping several tu-
ples of strings in ? ?T into tuples of strings in ? ?T .
Let r and f be two integers, r ? 0 and f ? 1.
We say that a function g has rank r if there exist
integers fi ? 1, 1 ? i ? r, such that g is defined
on (? ?T )f1 ? (? ?T )f2 ? ? ? ? ? (? ?T )fr . We also say
that g has fan-out f if the range of g is a subset of
(? ?T )f . Let yh, xij , 1 ? h ? f , 1 ? i ? r and
1 ? j ? fi, be string-valued variables. A func-
tion g as above is said to be linear regular if it is
defined by an equation of the form
g(?x11, . . . , x1f1?, . . . , ?xr1, . . . , xrfr?) =
= ?y1, . . . , yf ?, (1)
where ?y1, . . . , yf ? represents some grouping into
f sequences of all and only the variables appear-
ing in the left-hand side of (1) (without repeti-
tions) along with some additional terminal sym-
bols (with possible repetitions).
For a mathematical definition of LCFRS we re-
fer the reader to (Weir, 1992, p. 137). Informally,
in a LCFRS every nonterminal symbol A is asso-
ciated with an integer ?(A) ? 1, called its fan-out,
and it generates tuples in (? ?T )?(A). Productions
in a LCFRS have the form
p : A ? g(B1, B2, . . . , B?(p)),
where ?(p) ? 0, A and Bi, 1 ? i ? ?(p), are non-
terminal symbols, and g is a linear regular func-
tion having rank ?(p) and fan-out ?(A), defined
on (? ?T )?(B1) ?? ? ?? (? ?T )?(B?(p)) and taking val-
ues in (? ?T )?(A). The basic idea underlying the
rewriting relation associated with LCFRS is that
production p applies to any sequence of string tu-
ples generated by the Bi?s, and provides a new
string tuple in (? ?T )?(A) obtained through function
g. We say that ?(p) = ?(A) is the fan-out of p,
and ?(p) is the rank of p.
Example 1 Let L be the language L =
{anbnambmanbnambm |n,m ? 1}. A LCFRS
generating L is defined by means of the nonter-
minals S, ?(S) = 1, and A, ?(A) = 2, and the
productions in figure 1. Observe that nonterminal
A generates all tuples of the form ?anbn, anbn?. 2
Recognition and parsing for a given LCFRS
can be carried out in polynomial time on the length
of the input string. This is usually done by exploit-
ing standard dynamic programming techniques;
see for instance (Seki et al, 1991).1 However, the
polynomial degree in the running time is a mono-
tonically strictly increasing function that depends
on both the rank and the fan-out of the productions
in the grammar. To optimize running time, one can
then recast the source grammar in such a way that
the value of the above function is kept to a min-
imum. One way to achieve this is by factorizing
the productions of a LCFRS, as we now explain.
2.2 Factorization
Consider a LCFRS production of the form
p : A ? g(B1, B2, . . . , B?(p)), where g is
specified as in (1). Let alo C be a subset of
{B1, B2, . . . , B?(p)} such that |C| 6= 0 and |C| 6=
?(p). We let ?C be the alphabet of all variables
xij defined as in (1), for all values of i and j such
that Bi ? C and 1 ? j ? fi. For each i with
1 ? i ? f , we rewrite each string yi in (1) in a
form yi = y?i0zi1y?i1 ? ? ? y?idi?1zidiy
?
idi , with di ? 0,
such that the following conditions are all met:
? each zij , 1 ? j ? di, is a string with one or
more occurrences of variables, all in ?C ;
? each y?ij , 1 ? j ? di ? 1, is a non-empty
string with no occurrences of symbols in ?C ;
? y?0j and y?0di are (possibly empty) strings with
no occurrences of symbols in ?C .
1In (Seki et al, 1991) a syntactic variant of LCFRS is
used, called multiple context-free grammars.
526
S ? gS(A,A), gS(?x11, x12?, ?x21, x22?) = ?x11x21x12x22?;
A ? gA(A), gA(?x11, x12?) = ?ax11b, ax12b?;
A ? g?A(), g?A() = ?ab, ab?.
Figure 1: A LCFRS for language L = {anbnambmanbnambm |n,m ? 1}.
Let c = |C| and c = ?(p) ? |C|. Assume that
C = {Bh1 , . . . , Bhc}, and {B1, . . . , B?(p)} ? C =
{Bh?1 , . . . , Bh?c}. We introduce a fresh nontermi-
nal C with ?(C) = ?fi=1 di and replace pro-
duction p in our grammar by means of the two
new productions p1 : C ? g1(Bh1 , . . . , Bhc) and
p2 : A ? g2(C,Bh?1 , . . . , Bh?c). Functions g1 and
g2 are defined as:
g1(?xh11, . . . , xh1fh1 ?, . . . , ?xhc1, . . . , xhcfhc ?)
= ?z11, ? ? ? , z1d1 , z21, ? ? ? , zfdf ?;
g2(?xh?11, . . . , xh?1fh?1 ?, . . . , ?xh?c1, . . . , xh?cfh?c ?)
= ?y?10, . . . , y?1d1 , y?20, . . . , y?fdf ?.
Note that productions p1 and p2 have rank strictly
smaller than the source production p. Further-
more, if it is possible to choose set C in such a
way that
?f
i=0 di ? f , then the fan-out of p1 and
p2 will be no greater than the fan-out of p.
We can iterate the procedure above as many
times as possible, under the condition that the fan-
out of the productions does not increase.
Example 2 Let us consider the following produc-
tion with rank 4:
A ? gS(B,C,D,E),
gA(?x11, x12?, ?x21, x22?, ?x31, x32?, ?x41, x42?)
= ?x11x21x31x41x12x42, x22x32?.
Applyng the above procedure twice, we obtain a
factorization consisting of three productions with
rank 2 (variables have been renamed to reflect our
conventions):
A ? gA(A1, A2),
gA(?x11, x12?, ?x21, x22?)
= ?x11x21x12, x22?;
A1 ? gA1(B,E),
gA1(?x11, x12?, ?x21, x22?) = ?x11, x21x12x22?;
A2 ? gA2(C,D),
gA2(?x11, x12?, ?x21, x22?) = ?x11x21, x12x22?.
2
The factorization procedure above should be ap-
plied to all productions of a LCFRS with rank
larger than two. This might result in an asymptotic
improvement of the running time of existing dy-
namic programming algorithms for parsing based
on LCFRS.
The factorization technique we have discussed
can also be viewed as a generalization of well-
known techniques for casting context-free gram-
mars into binary forms. These are forms where no
more than two nonterminal symbols are found in
the right-hand side of productions of the grammar;
see for instance (Harrison, 1978). One important
difference is that, while production factorization
into binary form is always possible in the context-
free case, for LCFRS there are worst case gram-
mars in which rank reduction is not possible at all,
as shown in (Rambow and Satta, 1999).
3 A graph-based representation for
LCFRS productions
Rather than factorizing LCFRS productions di-
rectly, in this article we work with a more abstract
representation of productions based on graphs.
From now on we focus on LCFRS whose non-
terminals and productions all have fan-out smaller
than or equal to 2. Consider then a production p :
A ? g(B1, B2, . . . , B?(p)), with ?(A), ?(Bi) ?
2, 1 ? i ? ?(p), and with g defined as
g(?x11, . . . , x1?(B1)?, . . .
. . . , ?x?(p)1, . . . , x?(p)?(B?(p))?)
= ?y1, . . . , y?(A)?.
In what follows, if ?(A) = 1 then ?y1, . . . , y?(A)?
should be read as ?y1? and y1 ? ? ? y?(A) should be
read as y1. The same convention applies to all
other nonterminals and tuples.
We now introduce a special kind of undirected
graph that is associated with a linear order defined
over the set of its vertices. The p-graph associated
with production p is a triple (Vp, Ep,?p) such that
? Vp = {xij | 1 ? i ? ?(p), ?(Bi) = 2, 1 ?
j ? ?(Bi)} is a set of vertices;2
2Here we are overloading symbols xij . It will always be
clear from the context whether xij is a string-valued variable
or a vertex in a p-graph.
527
? Ep = {(xi1, xi2) |xi1, xi2 ? Vp} is a set of
undirected edges;
? for x, x? ? Vp, x ?p x? if x 6= x? and the
(unique) occurrence of x in y1 ? ? ? y?(A) pre-
cedes the (unique) occurrence of x?.
Note that in the above definition we are ignor-
ing all string-valued variables xij associated with
nonterminals Bi with ?(Bi) = 1. This is be-
cause nonterminals with fan-out one can always
be treated as in the context-free grammar case, as
it will be explained later.
Example 3 The p-graph associated with the
LCFRS production in Example 2 is shown in Fig-
ure 2. Circled sets of edges indicate the factoriza-
tion in that example. 2
x21 x31 x41x11
B
CD
E
A1
A2
x42x12 x22 x32
Figure 2: The p-graph associated with the LCFRS
production in Example 2.
We close this section by introducing some ad-
ditional notation related to p-graphs that will be
used throughout this paper. Let E ? Ep be some
set of edges. The cover set for E is defined as
V (E) = {x | (x, x?) ? E} (recall that our edges
are unordered pairs, so (x, x?) and (x?, x) denote
the same edge). Conversely, let V ? Vp be some
set of vertices. The incident set for V is defined
as E(V ) = {(x, x?) | (x, x?) ? Ep, x ? V }.
Assume ?(p) = 2, and let x1, x2 ? Vp. If x1
and x2 do not occur both in the same string y1 or
y2, then we say that there is a gap between x1 and
x2. If x1 ?p x2 and there is no gap between x1
and x2, then we write [x1, x2] to denote the set
{x1, x2} ? {x |x ? Vp, x1 ?p x ?p x2}. For x ?
Vp we also let [x, x] = {x}. A set [x, x?] is called a
range. Let r and r? be two ranges. The pair (r, r?)
is called a tandem if the following conditions are
both satisfied: (i) r?r? is not a range, and (ii) there
exists some edge (x, x?) ? Ep with x ? r and
x? ? r?. Note that the first condition means that r
and r? are disjoint sets and, for any pair of vertices
x ? r and x? ? r?, either there is a gap between x
and x? or else there exists some xg ? Vp such that
x ?p xg ?p x? and xg 6? r ? r?.
A set of edges E ? Ep is called a bundle with
fan-out one if V (E) = [x1, x2] for some x1, x2 ?
Vp, i.e., V (E) is a range. Set E is called a bundle
with fan-out two if V (E) = [x1, x2] ? [x3, x4] for
some x1, x2, x3, x4 ? Vp, and ([x1, x2], [x3, x4])
is a tandem. Note that if E is a bundle with fan-out
two with V (E) = [x1, x2] ? [x3, x4], then neither
E([x1, x2]) nor E([x3, x4]) are bundles with fan-
out one, since there is at least one edge incident
upon a vertex in [x1, x2] and a vertex in [x3, x4].
We also use the term bundle to denote a bundle
with fan-out either one or two.
Intuitively, in a p-graph associated with a
LCFRS production p, a bundle E with fan-out f
and with |E| > 1 identifies a set of nonterminals
C in the right-hand side of p that can be factorized
into a new production. The nonterminals in C are
then replaced in p by a fresh nonterminal C with
fan-out f , as already explained. Our factorization
algorithm is based on efficient methods for the de-
tection of bundles with fan-out one and two.
4 The algorithm
In this section we provide an efficient, recursive
algorithm for the decomposition of a p-graph into
bundles, which corresponds to factorizing the rep-
resented LCFRS production.
4.1 Overview of the algorithm
The basic idea underlying our graph-based algo-
rithm can be described as follows. We want to
compute an optimal hierarchical decomposition of
an input bundle with fan-out 1 or 2. This decom-
position can be represented by a tree, in which
each node N corresponds to a bundle (the root
node corresponds to the input bundle) and the
daughters of N represent the bundles in which N
is immediately decomposed. The decomposition
is optimal in so far as the maximum arity of the
decomposition tree is as small as possible. As
already explained above, this decomposition rep-
resents a factorization of some production p of a
LCFRS, resulting in optimal rank reduction. All
the internal nodes in the decomposition represent
fresh nonterminals that will be created during the
factorization process.
The construction of the decomposition tree is
carried out recursively. For a given bundle with
fan-out 1 or 2, we apply a procedure for decom-
posing this bundle in its immediate sub-bundles
with fan-out 1 or 2, in an optimal way. Then,
528
we recursively apply our procedure to the obtained
sub-bundles. Recursion stops when we reach bun-
dles containing only one edge (which correspond
to the nonterminals in the right-hand side of the
input production). We shall prove that the result is
an optimal decomposition.
The procedure for computing an optimal de-
composition of a bundle F into its immediate sub-
bundles, which we describe in the first part of this
section, can be sketched as follows. First, we iden-
tify and temporarily remove all maximal bundles
with fan-out 1 (Section 4.3). The result is a new
bundle F ? which is a subset of the original bundle,
and has the same fan-out. Next, we identify all
sub-bundles with fan-out 2 in F ? (Section 4.4). We
compute the optimal decomposition of F ?, rest-
ing on the hypothesis that there are no sub-bundles
with fan-out 1. Each resulting sub-bundle is later
expanded with the maximal sub-bundles with fan-
out 1 that have been previously removed. This re-
sults in a ?first level? decomposition of the original
bundle F . We then recursively decompose all in-
dividual sub-bundles of F , including the bundles
with fan-out 1 that have been later attached.
4.2 Backward and forward quantities
For a set V ? Vp of vertices, we write max(V )
(resp. min(V )) the maximum (resp. minimum)
vertex in V w.r.t. the ?p total order.
Let r = [x1, x2] be a range. We write r.left =
x1 and r.right = x2. The set of backward edges
for r is defined as Br = {(x, x?) | (x, x?) ?
Er, x ?p r.left , x? ? r}. The set of for-
ward edges for r is defined symmetrically as Fr =
{(x, x?) | (x, x?) ? Er, x ? r, r.right ?p
x?}. For E ? {Br, Fr} we also define L(E) =
{x | (x, x?) ? E, x ?p x?} and R(E) =
{x? | (x, x?) ? E, x ?p x?}.
Let us assume Br 6= ?. We write r.b.left =
min(L(Br)). Intuitively, r.b.left is the leftmost
vertex of the p-graph that is located at the left
of range r and that is connected to some ver-
tex in r through some edge. Similarly, we write
r.b.right = max(L(Br)). If Br = ?, then we set
r.b.left = r.b.right = ?. Quantities r.b.left and
r.b.right are called backward quantities.
We also introduce local backward quanti-
ties, defined as follows. We write r.lb.left =
min(R(Br)). Intuitively, r.lb.left is the leftmost
vertex among all those vertices in r that are con-
nected to some vertex to the left of r. Similarly,
we write r.lb.right = max(R(Br)). If Br = ?,
then we set r.lb.left = r.lb.right = ?.
We define forward and local forward quanti-
ties in a symmetrical way.
The backward quantities r.b.left and r.b.right
and the local backward quantities r.lb.left and
r.lb.right for all ranges r in the p-graph can
be computed efficiently as follows. We process
ranges in increasing order of size, expanding each
range r by one unit at a time by adding a new
vertex at its right. Backward and local backward
quantities for the expanded range can be expressed
as a function of the same quantities for r. There-
fore if we store our quantities for previously pro-
cessed ranges, each new range can be annotated
with the desired quantities in constant time. This
algorithm runs in time O(n2), where n is the num-
ber of vertices in Vp. This is an optimal result,
since O(n2) is also the size of the output.
We compute in a similar way the forward quan-
tities r.f .left and r.f .right and the local forward
quantities r.lf .left and r.lf .right , this time ex-
panding each range by one unit at its left.
4.3 Bundles with fan-out one
The detection of bundles with fan-out 1 within the
p-graph can be easily performed in O(n2), where
n is the number of its vertices. Indeed, the incident
set E(r) of a range r is a bundle with fan-out one
if and only if r.b.left = r.f .left = ?. This imme-
diately follows from the definitions given in Sec-
tion 4.2. It is therefore possible to check all ranges
the one after the other, once the backward and
forward properties have been computed. These
checks take constant time for each of the ?(n2)
ranges, hence the quadratic complexity.
We now remove from F all bundles with fan-out
1 from the original bundle F . The result is the new
bundle F ?, that has no sub-bundles with fan-out 1.
4.4 Bundles with fan-out two
Efficient detection of bundles with fan-out two in
F ? is considerably more challenging. A direct gen-
eralization of the technique proposed for detecting
bundles with fan-out 1 would use the following
property, that is also a direct corollary of the def-
initions in Section 4.2: the incident set E(r ? r?)
of a tandem (r, r?) is a bundle with fan-out two if
and only if all of the following conditions hold:
(i) r.b.left = r?.f .left = ?, (ii) r.f .left ? r?,
r.f .right ? r?, (iii) r?.b.left ? r, r?.b.right ? r.
529
However, checking all O(n4) tandems the one af-
ter the other would require time O(n4). Therefore,
preserving the quadratic complexity of the overall
algorithm requires a more complex representation.
From now on, we assume that Vp =
{x1, . . . , xn}, and we write [i, j] as a shorthand
for the range [xi, xj].
First, we need to compute an additional data
structure that will store local backward figures in
a convenient way. Let us define the expansion ta-
ble T as follows: for a given range r? = [i?, j?],
T (r?) is the set of all ranges r = [i, j] such that
r.lb.left = i? and r.lb.right = j?, ordered by in-
creasing left boundary i. It turns out that the con-
struction of such a table can be achieved in time
O(n2). Moreover, it is possible to compute in
O(n2) an auxiliary table T ? that associates with r
the first range r?? in T ([r.f.left, r.f.right]) such
that r??.b.right ? r. Therefore, either (r, T ?(r))
anchors a valid bundle, or there is no bundle E
such that the first component of V (E) is r.
We now have all the pieces to extract bundles
with fan-out 2 in time O(n2). We proceed as fol-
lows. For each range r = [i, j]:
? We first retrieve r? = [r.f.left, r.f.right] in
constant time.
? Then, we check in constant time whether
r?.b.left lies within r. If it doesn?t, r is not
the first part of a valid bundle with fan-out 2,
and we move on to the next range r.
? Finally, for each r?? in the ordered set
T (r?), starting with T ?(r), we check whether
r??.b.right is inside r. If it is not, we stop and
move on to the next range r. If it is, we out-
put the valid bundle (r, r??) and move on to
the next element in T (r?). Indeed, in case of
a failure, the backward edge that relates a ver-
tex in r?? with a vertex outside r will still be
included in all further elements in T (r?) since
T (r?) is ordered by increasing left boundary.
This step costs a constant time for each suc-
cess, and a constant time for the unique fail-
ure, if any.
This algorithm spends a constant time on each
range plus a constant time on each bundle with
fan-out 2. We shall prove in Section 5 that there
are O(n2) bundles with fan-out 2. Therefore, this
algorithm runs in time O(n2).
Now that we have extracted all bundles, we
need to extract an optimal decomposition of the in-
put bundle F ?, i.e., a minimal size partition of all
n elements (edges) in the input bundle such that
each of these partition is a bundle (with fan-out 2,
since bundles with fan-out 1 are excluded, except
for the input bundle). By definition, a partition has
minimal size if there is no other partition it is a
refinment of.3
4.5 Extracting an optimal decomposition
We have constructed the set of all (fan-out 2) sub-
bundles of F ?. We now need to build one optimal
decomposition of F ? into sub-bundles. We need
some more theoretical results on the properties of
bundles.
Lemma 1 Let E1 and E2 be two sub-bundles of
F ? (with fan-out 2) that have non-empty intersec-
tion, but that are not included the one in the other.
Then E1 ? E2 is a bundle (with fan-out 2).
PROOF This lemma can be proved by considering
all possible respective positions of the covers of
E1 and E2, and discarding all situations that would
lead to the existence of a fan-out 1 sub-bundle. 
Theorem 1 For any bundle E, either it has at
least one binary decomposition, or all its decom-
positions are refinements of a unique optimal one.
PROOF Let us suppose that E has no bi-
nary decomposition. Its cover corresponds to
the tandem (r, r?) = ([i, j], [i?, j?]). Let
us consider two different decompositions of
E, that correspond respectively to decomposi-
tions of the range r in two sets of sub-ranges
of the form [i, k1], [k1 + 1, k2], . . . , [km, j] and
[i, k?1], [k?1 + 1, k?2], . . . , [k?m? , j]. For simplifying
the notations, we write k0 = k?0 = i and km+1 =
km?+1 = j. Since k0 = k?0, there exist an in-
dex p > 0 such that for any l < p, kl = k?l, but
kp 6= k?p: p is the index that identifies the first
discrepancy between both decomposition. Since
km+1 = km?+1, there must exist q ? m and
q? ? m? such that q and q? are strictly greater
than p and that are the minimal indexes such that
kq = k?q? . By definition, all bundles of the form
E[kl?1,kl] (p ? l ? q) have a non-empty intersec-
tion with at least one bundle of the form E[k?l?1,k?l]
3The term ?refinement? is used in the usual way concern-
ing partitions, i.e., a partition P1 is a refinement of another
one P2 if all constituents in P1 are constituents of P2, or be-
longs to a subset of the partition P1 that is a partition of one
element of P2.
530
(p ? l ? q?). The reverse is true as well. Ap-
plying Lemma 1, this shows that E([kp+1, kq]) is
a bundle with fan-out 2. Therefore, by replacing
all ranges involved in this union in one decom-
position or the other, we get a third decomposi-
tion for which the two initial ones are strict refine-
ments. This is a contradiction, which concludes
the proof. 
Lemma 2 Let E = V (r ? r?) be a bundle, with
r = [i, j]. We suppose it has a unique (non-binary)
optimal decomposition, which decomposes [i, j]
into [i, k1], [k1 + 1, k2], . . . , [km, j]. There exist
no range r?? ? r such that (i) Er?? is a bundle and
(ii) ?l, 1 ? l ? m such that [kl, kl+1] ? r??.
PROOF Let us consider a range r?? that would con-
tradict the lemma. The union of r?? and of the
ranges in the optimal decomposition that have a
non-empty intersection with r?? is a fan-out 2 bun-
dle that includes at least two elements of the opti-
mal decomposition, but that is strictly included in
E because the decomposition is not binary. This
is a contradiction. 
Lemma 3 Let E = V (r, r?) be a bundle, with r =
[i, j]. We suppose it has a binary (optimal) decom-
position (not necessarily unique). Let r?? = [i, k]
be the largest range starting in i such that k < j
and such that it anchors a bundle, namely E(r??).
Then E(r??) and E([k + 1, j]) form a binary de-
composition of E.
PROOF We need to prove that E([k + 1, j]) is a
bundle. Each (optimal) binary decomposition of
E decomposes r in 1, 2 or 3 sub-ranges. If no opti-
mal decomposition decomposes r in at least 2 sub-
ranges, then the proof given here can be adapted
by reasoning on r? instead of r. We now sup-
pose that at least one of them decomposes r in at
least 2 sub-ranges. Therefore, it decomposes r in
[i, k1] and [k1 + 1, j] or in [i, k1], [k1 + 1, k2] and
[k2 + 1, j]. We select one of these optimal decom-
position by taking one such that k1 is maximal.
We shall now distinguish between two cases.
First, let us suppose that r is decomposed
into two sub-ranges [i, k1] and [k1 + 1, j] by
the selected optimal decomposition. Obviously,
E([i, k1]) is a ?crossing? bundle, i.e., the right
component of its cover is is a sub-range of r?.
Since r is decomposed in two sub-ranges, it is
necessarily the same for r?. Therefore, E([i, k1])
has a cover of the form [i, k1] ? [i?, k?1] or [i, k1] ?
[k?1 + 1, j]. Since r?? includes [i, k1], E(r??) has a
cover of the form [i, k]?[i?, k?] or [i, k]?[k? + 1, j].
This means that r? is decomposed by E(r??) in
only 2 ranges, namely the right component of
E(r??)?s cover and another range, that we can call
r???. Since r \ r?? = [k + 1, j] may not anchor
a bundle with fan-out 1, it must contain at least
one crossing edge. All such edges necessarily fall
within r???. Conversely, any crossing edge that
falls inside r??? necessarily has its other end inside
[k + 1, j]. Which means that E(r??) and E(r???)
form a binary decomposition of E. Therefore, by
definition of k1, k = k1.
Second, let us suppose that r is decomposed
into 3 sub-ranges by the selected original decom-
position (therefore, r? is not decomposed by this
decomposition). This means that this decompo-
sition involves a bundle with a cover of the form
[i, k1]?[k2 + 1, j] and another bundle with a cover
of the form [k1 + 1, k2] ? r? (this bundle is in fact
E(r?)). If k ? k2, then the left range of both mem-
bers of the original decomposition are included in
r??, which means that E(r??) = E, and therefore
r?? = r which is excluded. Note that k is at least
as large as k1 (since [i, k1] is a valid ?range start-
ing in i such that k < j and such that it anchors
a bundle?). Therefore, we have k1 ? k < k2.
Therefore, E([i, k1]) ? E(r??), which means that
all edges anchored inside [k2 + 1, j]) are included
in E(r??). Hence, E(r??) can not be a crossing bun-
dle without having a left component that is [i, j],
which is excluded (it would mean E(r??) = E).
This means that E(r??) is a bundle with a cover
of the form [i, k] ? [k? + 1, j]. Which means
that E(r?) is in fact the bundle whose cover is
[k + 1, k? + 1]? r?. Hence, E(r??) and E(r?) form
a binary decomposition of E. Hence, by definition
of k1, k = k1. 
As an immediate consequence of Lemmas 2
and 3, our algorithm for extracting the optimal de-
composition for F ? consists in applying the fol-
lowing procedure recursively, starting with F ?,
and repeating it on each constructed sub-bundle E,
until sub-bundles with only one edge are reached.
Let E = E(r, r?) be a bundle, with r = [i, j].
One optimal decomposition of E can be obtained
as follows. One selects the bundle with a left com-
ponent starting in i and with the maximum length,
and iterating this selection process until r is cov-
ered. The same is done with r?. We retain the opti-
mal among both resulting decompositions (or one
of them if they are both optimal). Note that this
531
decomposition is unique if and only if it has four
components or more; it can not be ternary; it may
be binary, and in this case it may be non-unique.
This algorithm gives us a way to extract an op-
timal decomposition of F ? in linear time w.r.t. the
number of sub-bundles in this optimal decomposi-
tion. The only required data structure is, for each
i (resp. k), the list of bundles with a cover of the
form [i, j]? [k, l] ordered by decreasing j (resp. l).
This can trivially be constructed in time O(n2)
from the list of all bundles we built in time O(n2)
in the previous section. Since the number of bun-
dles is bounded by O(n2) (as mentioned above
and proved in Section 5), this means we can ex-
tract an optimal decomposition for F ? in O(n2).
Similar ideas apply to the simpler case of the
decomposition of bundles with fan-out 1.
4.6 The main decomposition algorithm
We now have to generalize our algorithm in or-
der to handle the possible existence of fan-out 1
bundles. We achieve this by using the fan-out 2
algorithm recursively. First, we extract and re-
move (maximal) bundles with fan-out 1 from F ,
and recursively apply to each of them the com-
plete algorithm. What remains is F ?, which is a
set of bundles with no sub-bundles with fan-out 1.
This means we can apply the algorithm presented
above. Then, for each bundle with fan-out 1, we
group it with a randomly chosen adjacent bundle
with fan-out 2, which builds an expanded bundle
with fan-out 2, which has a binary decomposition
into the original bundle with fan-out 2 and the bun-
dle with fan-out 1.
5 Time complexity analysis
In Section 4, we claimed that there are no more
than O(n2) bundles. In this section we sketch the
proof of this result, which will prove the quadratic
time complexity of our algorithm.
Let us compute an upper bound on the num-
ber of bundles with fan-out two that can be found
within the p-graph processed in Section 4.5, i.e., a
p-graph with no fan-out 1 sub-bundle.
Let E,E? ? Ep be bundles with fan-out two. If
E ? E?, then we say that E? expands E. E? is
said to immediately expand E, written E ? E?,
if E? expands E and there is no bundle E?? such
that E?? expands E and E? expands E??.
Let us represent bundles and the associated im-
mediate expansion relation by means of a graph.
Let E denote the set of all bundles (with fan-out
two) in our p-graph. The e-graph associated with
our LCFRS production p is the directed graph
with vertices E and edges defined by the relation
?. For E ? E , we let out(E) = {E? |E ? E?}
and in(E) = {E? |E? ? E}.
Lack of space prevents us from providing the
proof of the following property. For any E ? E
that contains more than one edge, |out(E)| ? 2
and |in(E)| ? 2. This allows us to prove our up-
per bound on the size of E .
Theorem 2 The e-graph associated with an
LCFRS production p has at most n2 vertices,
where n is the rank of p.
PROOF Consider the e-graph associated with pro-
duction p, with set of vertices E . For a vertex
E ? E , we define the level of E as the number
|E| of edges in the corresponding bundle from the
p-graph associated with p. Let d be the maximum
level of a vertex in E . We thus have 1 ? d ? n.
We now prove the following claim. For any inte-
ger k with 1 ? k ? d, the set of vertices in E with
level k has no more than n elements.
For k = 1, since there are no more than n edges
in such a p-graph, the statement holds.
We can now consider all vertices in E with level
k > 1 (k ? d). Let E(k?1) be the set of all ver-
tices in E with level smaller than or equal to k?1,
and let us call T (k?1) the set of all edges in the e-
graph that are leaving from some vertex in E(k?1).
Since for each bundle E in E(k?1) we know that
|out(E)| ? 2, we have |T (k?1)| ? 2|E(k?1)|.
The number of vertices in E(k) with level larger
than one is at least |E(k?1)| ? n. Since for each
E ? E(k?1) we know that |in(E)| ? 2, we con-
clude that at least 2(|E(k?1)| ? n) edges in T (k?1)
must end up at some vertex in E(k). Let T be the
set of edges in T (k?1) that impinge on some ver-
tex in E \ E(k). Thus we have |T | ? 2|E(k?1)| ?
2(|E(k?1)|?n) = 2n. Since the vertices of level k
in E must have incoming edges from set T , and be-
cause each of them have at least 2 incoming edges,
there cannot be more than n such vertices. This
concludes the proof of our claim.
Since the the level of a vertex in E is necessarily
lower than n, this completes the proof. 
The overall complexity of the complete algo-
rithm can be computed by induction. Our in-
duction hypothesis is that for m < n, the time
complexity is in O(m2). This is obviously true
for n = 1 and n = 2. Extracting the bundles
532
with fan-out 1 costs O(n2). These bundles are of
length n1 . . . nm. Extracting bundles with fan-out
2 costs O((n? n1 ? . . .? nm)2). Applying re-
cursively the algorithm to bundles with fan-out 1
costs O(n21) + . . . +O(n2m). Therefore, the com-
plexity is in O(n2)+O((n ? n1 ? . . .? nm)2)+
?n
i=1 O(ni) = O(n2) +O(
?n
i=1 ni) = O(n2).
6 Conclusion
We have introduced an efficient algorithm for opti-
mal reduction of the rank of LCFRSs with fan-out
at most 2, that runs in quadratic time w.r.t. the rank
of the input grammar. Given the fact that fan-out 1
bundles can be attached to any adjacent bundle in
our factorization, we can show that our algorithm
also optimizes time complexity for known tabular
parsing algorithms for LCFRSs with fan-out 2.
As for general LCFRS, it has been shown by
Gildea (2010) that rank optimization and time
complexity optimization are not equivalent. Fur-
thermore, all known algorithms for rank or time
complexity optimization have an exponential time
complexity (Go?mez-Rodr??guez et al, 2009).
Acknowledgments
Part of this work was done while the second author
was a visiting scientist at Alpage (INRIA Paris-
Rocquencourt and Universite? Paris 7), and was fi-
nancially supported by the hosting institutions.
References
Daniel Gildea. 2010. Optimal parsing strategies for
linear context-free rewriting systems. In Human
Language Technologies: The 11th Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics; Proceedings of
the Main Conference, Los Angeles, California. To
appear.
Carlos Go?mez-Rodr??guez and Giorgio Satta. 2009.
An optimal-time binarization algorithm for linear
context-free rewriting systems with fan-out two. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 985?993, Suntec, Singapore,
August. Association for Computational Linguistics.
Carlos Go?mez-Rodr??guez, Marco Kuhlmann, Giorgio
Satta, and David J. Weir. 2009. Optimal reduc-
tion of rule length in linear context-free rewriting
systems. In Proceedings of the North American
Chapter of the Association for Computational Lin-
guistics - Human Language Technologies Confer-
ence (NAACL?09:HLT), Boulder, Colorado. To ap-
pear.
Michael A. Harrison. 1978. Introduction to Formal
Language Theory. Addison-Wesley, Reading, MA.
Aravind K. Joshi and Leon S. Levy. 1977. Constraints
on local descriptions: Local transformations. SIAM
Journal of Computing
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of the 12th Meeting of the
European Chapter of the Association for Computa-
tional Linguistics (EACL 2009), Athens, Greece. To
appear.
Wolfgang Maier and Timm Lichte. 2009. Character-
izing discontinuity in constituent treebanks. In Pro-
ceedings of the 14th Conference on Formal Gram-
mar (FG 2009), Bordeaux, France.
Wolfgang Maier and Anders S?gaard. 2008. Tree-
banks and mild context-sensitivity. In Philippe
de Groote, editor, Proceedings of the 13th Confer-
ence on Formal Grammar (FG 2008), pages 61?76,
Hamburg, Germany. CSLI Publications.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theoretical Computer Science, 223:87?120.
Giorgio Satta. 1992. Recognition of linear context-free
rewriting systems. In Proceedings of the 30th Meet-
ing of the Association for Computational Linguistics
(ACL?92), Newark, Delaware.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science, 88:191?
229.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In Pro-
ceedings of the 25th Meeting of the Association for
Computational Linguistics (ACL?87).
David J. Weir. 1992. Linear context-free rewriting
systems and deterministic tree-walk transducers. In
Proceedings of the 30th Meeting of the Association
for Computational Linguistics (ACL?92), Newark,
Delaware.
533
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 534?543,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The Importance of Rule Restrictions in CCG
Marco Kuhlmann
Dept. of Linguistics and Philology
Uppsala University
Uppsala, Sweden
Alexander Koller
Cluster of Excellence
Saarland University
Saarbr?cken, Germany
Giorgio Satta
Dept. of Information Engineering
University of Padua
Padua, Italy
Abstract
Combinatory Categorial Grammar (CCG)
is generally construed as a fully lexicalized
formalism, where all grammars use one and
the same universal set of rules, and cross-
linguistic variation is isolated in the lexicon.
In this paper, we show that the weak gener-
ative capacity of this ?pure? form of CCG is
strictly smaller than that of CCG with gram-
mar-specific rules, and of other mildly con-
text-sensitive grammar formalisms, includ-
ing Tree Adjoining Grammar (TAG). Our
result also carries over to a multi-modal
extension of CCG.
1 Introduction
Combinatory Categorial Grammar (CCG) (Steed-
man, 2001; Steedman and Baldridge, 2010) is an
expressive grammar formalism with formal roots
in combinatory logic (Curry et al, 1958) and links
to the type-logical tradition of categorial grammar
(Moortgat, 1997). It has been successfully used for
a wide range of practical tasks, such as data-driven
parsing (Hockenmaier and Steedman, 2002; Clark
and Curran, 2007), wide-coverage semantic con-
struction (Bos et al, 2004), and the modelling of
syntactic priming (Reitter et al, 2006).
It is well-known that CCG can generate lan-
guages that are not context-free (which is neces-
sary to capture natural languages), but can still
be parsed in polynomial time. Specifically, Vijay-
Shanker and Weir (1994) identified a version of
CCG that is weakly equivalent to Tree Adjoining
Grammar (TAG) (Joshi and Schabes, 1997) and
other mildly context-sensitive grammar formalisms,
and can generate non-context-free languages such
as anbncn. The generative capacity of CCG is com-
monly attributed to its flexible composition rules,
which allow it to model more complex word orders
that context-free grammar can.
The discussion of the (weak and strong) gener-
ative capacity of CCG and TAG has recently been
revived (Hockenmaier and Young, 2008; Koller and
Kuhlmann, 2009). In particular, Koller and Kuhl-
mann (2009) have shown that CCGs that are pure
(i.e., they can only use generalized composition
rules, and there is no way to restrict the instances
of these rules that may be used) and first-order
(i.e., all argument categories are atomic) can not
generate anbncn. This shows that the generative
capacity of at least first-order CCG crucially relies
on its ability to restrict rule instantiations, and is at
odds with the general conception of CCG as a fully
lexicalized formalism, in which all grammars use
one and the same set of universal rules. A question
then is whether the result carries over to pure CCG
with higher-order categories.
In this paper, we answer this question to the pos-
itive: We show that the weak generative capacity of
general pure CCG is still strictly smaller than that
of the formalism considered by Vijay-Shanker and
Weir (1994); composition rules can only achieve
their full expressive potential if their use can be
restricted. Our technical result is that every lan-
guage L that can be generated by a pure CCG has
a context-free sublanguage L0  L such that every
string in L is a permutation of a string in L0, and
vice versa. This means that anbncn, for instance,
cannot be generated by pure CCG, as it does not
have any (non-trivial) permutation-equivalent sub-
languages. Conversely, we show that there are still
languages that can be generated by pure CCG but
not by context-free grammar.
We then show that our permutation language
lemma also holds for pure multi-modal CCG as
defined by Baldridge and Kruijff (2003), in which
the use of rules can be controlled through the lex-
icon entries by assigning types to slashes. Since
this extension was intended to do away with
the need for grammar-specific rule restrictions, it
comes as quite a surprise that pure multi-modal
534
CCG in the style of Baldridge and Kruijff (2003) is
still less expressive than the CCG formalism used
by Vijay-Shanker and Weir (1994). This means that
word order in CCG cannot be fully lexicalized with
the current formal tools; some ordering constraints
must be specified via language-specific combina-
tion rules and not in lexicon entries. On the other
hand, as pure multi-modal CCG has been success-
fully applied to model the syntax of a variety of
natural languages, another way to read our results
is as contributions to a discussion about the exact
expressiveness needed to model natural language.
The remainder of this paper is structured as fol-
lows. In Section 2, we introduce the formalism
of pure CCG that we consider in this paper, and
illustrate the relevance of rule restrictions. We then
study the generative capacity of pure CCG in Sec-
tion 3; this section also presents our main result. In
Section 4, we show that this result still holds for
multi-modal CCG. Section 5 concludes the paper
with a discussion of the relevance of our findings.
2 Combinatory Categorial Grammar
We start by providing formal definitions for cat-
egories, syntactic rules, and grammars, and then
discuss the relevance of rule restrictions for CCG.
2.1 Categories
Given a finite set A of atomic categories, the set of
categories over A is the smallest set C such that
A  C , and .x=y/; .xny/ 2 C whenever x; y 2 C .
A category x=y represents a function that seeks a
string with category y to the right (indicated by the
forward slash) and returns a new string with cat-
egory x; a category xny instead seeks its argument
to the left (indicated by the backward slash). In
the remainder of this paper, we use lowercase sans-
serif letters such as x; y; z as variables for categor-
ies, and the vertical bar j as a variable for slashes.
In order to save some parentheses, we understand
slashes as left-associative operators, and write a
category such as .x=y/nz as x=ynz.
The list of arguments of a category c is defined
recursively as follows: If c is atomic, then it has no
arguments. If c D xjy for some categories x and y,
then the arguments of c are the slashed category jy,
plus the arguments of x. We number the arguments
of a category from outermost to innermost. The
arity of a category is the number of its arguments.
The target of a category c is the atomic category
that remains when stripping c of its arguments.
x=y y ) x forward application >
y xny ) x backward application <
x=y y=z ) x=z forward harmonic composition >B
ynz xny ) xnz backward harmonic composition <B
x=y ynz ) xnz forward crossed composition >B
y=z xny ) x=z backward crossed composition <B
Figure 1: The core set of rules of CCG.
2.2 Rules
The syntactic rules of CCG are directed versions
of combinators in the sense of combinatory logic
(Curry et al, 1958). Figure 1 lists a core set of
commonly assumed rules, derived from functional
application and the B combinator, which models
functional composition. When talking about these
rules, we refer to the premise containing the argu-
ment jy as the primary premise, and to the other
premise as the secondary premise of the rule.
The rules in Figure 1 can be generalized into
composition rules of higher degrees. These are
defined as follows, where n  0 and ? is a variable
for a sequence of n arguments.
x=y y? ) x? generalized forward composition >n
y? xny ) x? generalized backward composition <n
We call the value n the degree of the composition
rule. Note that the rules in Figure 1 are the special
cases for n D 0 and n D 1.
Apart from the core rules given in Figure 1, some
versions of CCG also use rules derived from the S
and T combinators of combinatory logic, called
substitution and type-raising, the latter restricted
to the lexicon. However, since our main point of
reference in this paper, the CCG formalism defined
by Vijay-Shanker and Weir (1994), does not use
such rules, we will not consider them here, either.
2.3 Grammars and Derivations
With the set of rules in place, we can define a
pure combinatory categorial grammar (PCCG) as
a construct G D .A;?;L; s/, where A is an alpha-
bet of atomic categories, s 2 A is a distinguished
atomic category called the final category, ? is a
finite set of terminal symbols, and L is a finite rela-
tion between symbols in ? and categories over A,
called the lexicon. The elements of the lexicon L
are called lexicon entries, and we represent them
using the notation  ` x, where  2 ? and x
is a category over A. A category that occurs in a
lexicon entry is called a lexical category.
535
A derivation in a grammar G can be represen-
ted as a derivation tree as follows. Given a string
w 2 ?, we choose a lexicon entry for each oc-
currence of a symbol in w, line up the respective
lexical categories from left to right, and apply ad-
missible rules to adjacent pairs of categories. After
the application of a rule, only the conclusion is
available for future applications. We iterate this
process until we end up with a single category. The
string w is called the yield of the resulting deriva-
tion tree. A derivation tree is complete, if the last
category is the final category of G. The language
generated by G, denoted by L.G/, is formed by
the yields of all complete derivation trees.
2.4 Degree Restrictions
Work on CCG generally assumes an upper bound
on the degree of composition rules that can be used
in derivations. We also employ this restriction, and
only consider grammars with compositions of some
bounded (but arbitrary) degree n  0.1 CCG with
unbounded-degree compositions is more express-
ive than bounded-degree CCG or TAG (Weir and
Joshi, 1988).
Bounded-degree grammars have a number of
useful properties, one of which we mention here.
The following lemma rephrases Lemma 3.1 in
Vijay-Shanker and Weir (1994).
Lemma 1 For every grammar G, every argument
in a derivation ofG is the argument of some lexical
category of G.
As a consequence, there is only a finite number
of categories that can occur as arguments in some
derivation. In the presence of a bound on the degree
of composition rules, this implies the following:
Lemma 2 For every grammar G, there is a finite
number of categories that can occur as secondary
premises in derivations of G.
Proof. The arity of a secondary premise c can be
written as mC n, where m is the arity of the first
argument of the corresponding primary premise,
and n is the degree of the rule applied. Since each
argument is an argument of some lexical category
of G (Lemma 1), and since n is assumed to be
bounded, both m and n are bounded. Hence, there
is a bound on the number of choices for c. 
Note that the number of categories that can occur
as primary premises is generally unbounded even
in a grammar with bounded degree.
1For practical grammars, n  4.
2.5 Rule Restrictions
The rule set of pure CCG is universal: the differ-
ence between the grammars of different languages
should be restricted to different choices of categor-
ies in the lexicon. This is what makes pure CCG
a lexicalized grammar formalism (Steedman and
Baldridge, 2010). However, most practical CCG
grammars rely on the possibility to exclude or re-
strict certain rules. For example, Steedman (2001)
bans the rule of forward crossed composition from
his grammar of English, and stipulates that the rule
of backward crossed composition may be applied
only if both of its premises share the common tar-
get category s, representing sentences. Exclusions
and restrictions of rules are also assumed in much
of the language-theoretic work on CCG. In partic-
ular, they are essential for the formalism used in
the aforementioned equivalence proof for CCG and
TAG (Vijay-Shanker and Weir, 1994).
To illustrate the formal relevance of rule restric-
tions, suppose that we wanted to write a pure CCG
that generates the language
L3 D f anbncn j n  1 g ,
which is not context-free. An attempt could be
G1 D .f s; a; b; c g; f a; b; c g; L; s/ ,
where the lexicon L is given as follows:
a ` a , b ` s=cna , b ` b=cna ,
b ` s=c=bna , b ` s=c=bna , c ` c .
From a few sample derivations like the one given
in Figure 2a, we can convince ourselves that G1
generates all strings of the form anbncn, for any
n  1. However, a closer inspection reveals that it
also generates other, unwanted strings?in partic-
ular, strings of the form .ab/ncn, as witnessed by
the derivation given in Figure 2b.
Now suppose that we would have a way to only
allow those instances of generalized composition in
which the secondary premise has the form b=c=bna
or b=cna. Then the compositions
b=c=b b=c
b=c=c >
1 and s=c=b b=c
s=c=c >
1
would be disallowed, and it is not hard to see
that G1 would generate exactly anbncn.
As we will show in this paper, our attempt to
capture L3 with a pure CCG grammar failed not
only because we could not think of one: L3 cannot
be generated by any pure CCG.
536
a...................
a
a...........
a
a...
a
b...
s=c=bna
b.......
b=c=bna
b...............
b=cna
c.......................
c
c...........................
c
c...............................
c
<0
s=c=b
>3
s=c=c=bna
<0
s=c=c=b
>2
s=c=c=cna
<0
s=c=c=c
>0
s=c=c
>0
s=c
>0
s
(a) Derivation of the string aaabbbccc.
a...........
a
b...........
s=c=bna
a...
a
b...
b=c=bna
a...
a
b...
b=cna
c...........
c
c...................
c
c.......................
c
<0
s=c=b
<0
b=c=b
<0
b=c
>1
b=c=c
>0
b=c
>1
s=c=c
>0
s=c
>0
s
(b) Derivation of the string abababccc.
Figure 2: Two derivations of the grammar G1.
3 The Generative Capacity of Pure CCG
We will now develop a formal argument showing
that rule restrictions increase the weak generative
capacity of CCG. We will first prove that pure CCG
is still more expressive than context-free grammar.
We will then spend the rest of this section working
towards the result that pure CCG is strictly less
expressive than CCG with rule restrictions. Our
main technical result will be the following:
Theorem 1 Every language that can be generated
by a pure CCG has a Parikh-equivalent context-free
sublanguage.
Here, two languages L and L0 are called Parikh-
equivalent if every string in L is the permutation
of a string in L0 and vice versa.
3.1 CFG ? PCCG
Proposition 1 The class of languages generated
by pure CCG properly includes the class of context-
free languages.
Proof. To see the inclusion, it suffices to note that
pure CCG when restricted to application rules is
the same as AB-grammar, the classical categorial
formalism investigated by Ajdukiewicz and Bar-
Hillel (Bar-Hillel et al, 1964). This formalism is
weakly equivalent to context-free grammar.
To see that the inclusion is proper, we can go
back to the grammarG1 that we gave in Section 2.5.
We have already discussed that the language L3 is
included inL.G1/. We can also convince ourselves
that all strings generated by the grammar G1 have
an equal number of as, bs and cs. Consider now
the regular language R D abc. From our ob-
servations, it follows that L.G1/\R D L3. Since
context-free languages are closed under intersec-
tion with regular languages, we find that L.G1/
can be context-free only if L3 is. Since L3 is not
context-free, we therefore conclude that L.G1/ is
not context-free, either. 
Two things are worth noting. First, our result shows
that the ability of CCG to generate non-context-free
languages does not hinge on the availability of sub-
stitution and type-raising rules: The derivations
of G1 only use generalized compositions. Neither
does it require the use of functional argument cat-
egories: The grammarG1 is first-order in the sense
of Koller and Kuhlmann (2009).
Second, it is important to note that if the com-
position degree n is restricted to 0 or 1, pure CCG
actually collapses to context-free expressive power.
This is clear for n D 0 because of the equivalence
to AB grammar. For n D 1, observe that the arity
of the result of a composition is at most as high as
537
that of each premise. This means that the arity of
any derived category is bounded by the maximal
arity of lexical categories in the grammar, which
together with Lemma 1 implies that there is only
a finite set of derivable categories. The set of all
valid derivations can then be simulated by a con-
text-free grammar. In the presence of rules with
n  2, the arities of derived categories can grow
unboundedly.
3.2 Active and Inactive Arguments
In the remainder of this section, we will develop
the proof of Theorem 1, and use it to show that the
generative capacity of PCCG is strictly smaller than
that of CCG with rule restrictions. For the proof,
we adopt a certain way to view the information
flow in CCG derivations. Consider the following
instance of forward harmonic composition:
a=b b=c ) a=c
This rule should be understood as obtaining its con-
clusion a=c from the primary premise a=b by the
removal of the argument =b and the subsequent
transfer of the argument =c from the secondary
premise. With this picture in mind, we will view
the two occurrences of =c in the secondary premise
and in the conclusion as two occurrences of one
and the same argument. Under this perspective,
in a given derivation, an argument has a lifespan
that starts in a lexical category and ends in one
of two ways: either in the primary or in the sec-
ondary premise of a composition rule. If it ends
in a primary premise, it is because it is matched
against a subcategory of the corresponding second-
ary premise; this is the case for the argument =b
in the example above. We will refer to such argu-
ments as active. If an argument ends its life in a
secondary premise, it is because it is consumed as
part of a higher-order argument. This is the case
for the argument =c in the secondary premise of
the following rule instance:
a=.b=c/ b=c=d ) a=d
(Recall that we assume that slashes are left-associ-
ative.) We will refer to such arguments as inactive.
Note that the status of an argument as either active
or inactive is not determined by the grammar, but
depends on a concrete derivation.
The following lemma states an elementary prop-
erty in connection with active and inactive argu-
ments, which we will refer to as segmentation:
Lemma 3 Every category that occurs in a CCG
derivation has the general form a??, where a is an
atomic category, ? is a sequence of inactive argu-
ments, and ? is a sequence of active arguments.
Proof. The proof is by induction on the depth of a
node in the derivation. The property holds for the
root (which is labeled with the final category), and
is transferred from conclusions to premises. 
3.3 Transformation
The fundamental reason for why the example gram-
mar G1 from Section 2.5 overgenerates is that in
the absence of rule restrictions, we have no means
to control the point in a derivation at which a cat-
egory combines with its arguments. Consider the
examples in Figure 2: It is because we cannot en-
sure that the bs finish combining with the other bs
before combining with the cs that the undesirable
word order in Figure 2b has a derivation. To put
it as a slogan: Permuting the words allows us to
saturate arguments prematurely.
In this section, we show that this property applies
to all pure CCGs. More specifically, we show that,
in a derivation of a pure CCG, almost all active
arguments of a category can be saturated before
that category is used as a secondary premise; at
most one active argument must be transferred to
the conclusion of that premise. Conversely, any
derivation that still contains a category with at least
two active arguments can be transformed into a
new derivation that brings us closer to the special
property just characterized.
We formalize this transformation by means of a
system of rewriting rules in the sense of Baader and
Nipkow (1998). The rules are given in Figure 3. To
see how they work, let us consider the first rule, R1;
the other ones are symmetric. This rules states that,
whenever we see a derivation in which a category
of the form x=y (here marked as A) is combined
with a category of the form y?=z (marked as B),
and the result of this combination is combined with
a category of the form z (C), then the resulting
category can also be obtained by ?rotating? the de-
rivation to first saturate =z by combining B with C,
and only then do the combination with A. When ap-
plying these rotations exhaustively, we end up with
a derivation in which almost all active arguments of
a category are saturated before that category is used
as a secondary premise. Applying the transform-
ation to the derivation in Figure 2a, for instance,
yields the derivation in Figure 2b.
We need the following result for some of the
lemmas we prove below. We call a node in a deriv-
538
A x=y B y?=z
x?=z C z
x?
R1
H) x=y
y?=z z
y?
x?
B y?=z A xny
x?=z C z
x?
R2
H)
y?=z z
y? xny
x?
C z
A x=y B y?nz
x?nz
x?
R3
H) x=y
z y?nz
y?
x?
C z
B y?nz A xny
x?nz
x?
R4
H)
z y?nz
y? xny
x?
Figure 3: Rewriting rules used in the transformation. Here,  represents a (possibly empty) sequence of
arguments, and ? represents a sequence of arguments in which the first (outermost) argument is active.
ation critical if its corresponding category contains
more than one active argument and it is the second-
ary premise of a rule. We say that u is a highest
critical node if there is no other critical node whose
distance to the root is shorter.
Lemma 4 If u is a highest critical node, then we
can apply one of the transformation rules to the
grandparent of u.
Proof. Suppose that the category at u has the form
y?=z, where =z is an active argument, and the first
argument in ? is active as well. (The other possible
case, in which the relevant occurrence has the form
y?nz, can be treated symmetrically.) Since u is a
secondary premise, it is involved in an inference of
one of the following two forms:
x=y y?=z
x?=z
y?=z xny
x?=z
Since u is a highest critical node, the conclusion
of this inference is not a critical node itself; in
particular, it is not a secondary premise. Therefore,
the above inferences can be extended as follows:
x=y y?=z
x?=z z
x?
y?=z xny
x?=z z
x?
These partial derivations match the left-hand side of
the rewriting rules R1 and R2, respectively. Hence,
we can apply a rewriting rule to the derivation. 
We now show that the transformation is well-
defined, in the sense that it terminates and trans-
forms derivations of a grammar G into new deriva-
tions of G.
Lemma 5 The rewriting of a derivation tree ends
after a finite number of steps.
Proof. We assign natural numbers to the nodes
of a derivation tree as follows. Each leaf node
is assigned the number 0. For an inner node u,
which corresponds to the conclusion of a composi-
tion rule, let m; n be the numbers assigned to the
nodes corresponding to the primary and second-
ary premise, respectively. Then u is assigned the
number 1C 2mCn. Suppose now that we have as-
sociated premise A with the number x, premise B
with the number y, and premise C with the num-
ber z. It is then easy to verify that the conclusion
of the partial derivation on the left-hand side of
each rule has the value 3 C 4x C 2y C z, while
the conclusion of the right-hand side has the value
2C 2x C 2y C z. Thus, each step decreases the
value of a derivation tree under our assignment by
the amount 1C 2x. Since this value is positive for
all choices of x, the rewriting ends after a finite
number of steps. 
To convince ourselves that our transformation does
not create ill-formed derivations, we need to show
that none of the rewriting rules necessitates the use
of composition operations whose degree is higher
than the degree of the operations used in the ori-
ginal derivation.
Lemma 6 Applying the rewriting rules from the
top down does not increase the degree of the com-
position operations.
Proof. The first composition rule used in the left-
hand side of each rewriting rule has degree j?j C 1,
the second rule has degree j j; the first rule used in
the right-hand side has degree j j, the second rule
has degree j?jC j j. To prove the claim, it suffices
to show that j j  1. This is a consequence of the
following two observations.
1. In the category x? , the arguments in  occur
on top of the arguments in ?, the first of which is
active. Using the segmentation property stated in
Lemma 3, we can therefore infer that  does not
contain any inactive arguments.
539
2. Because we apply rules top-down, premise B
is a highest critical node in the derivation (by
Lemma 4). This means that the category at
premise C contains at most one active argument;
otherwise, premise C would be a critical node
closer to the root than premise B. 
We conclude that, if we rewrite a derivation d of G
top-down until exhaustion, then we obtain a new
valid derivation d 0. We call all derivations d 0 that
we can build in this way transformed. It is easy to
see that a derivation is transformed if and only if it
contains no critical nodes.
3.4 Properties of Transformed Derivations
The special property established by our transform-
ation has consequences for the generative capacity
of pure CCG. In particular, we will now show that
the set of all transformed derivations of a given
grammar yields a context-free language. The cru-
cial lemma is the following:
Lemma 7 For every grammar G, there is some
k  0 such that no category in a transformed
derivation of G has arity greater than k.
Proof. The number of inactive arguments in the
primary premise of a rule does not exceed the num-
ber of inactive arguments in the conclusion. In
a transformed derivation, a symmetric property
holds for active arguments: Since each second-
ary premise contains at most one active argument,
the number of active arguments in the conclusion
of a rule is not greater than the number of act-
ive arguments in its primary premise. Taken to-
gether, this implies that the arity of a category that
occurs in a transformed derivation is bounded by
the sum of the maximal arity of a lexical category
(which bounds the number of active arguments),
and the maximal arity of a secondary premise
(which bounds the number of inactive arguments).
Both of these values are bounded in G. 
Lemma 8 The yields corresponding to the set of
all transformed derivations of a pure CCG form a
context-free language.
Proof. Let G be a pure CCG. We construct a con-
text-free grammar GT that generates the yields of
the set of all transformed derivations of G.
As the set of terminals of GT , we use the set of
terminals ofG. To form the set of nonterminals, we
take all categories that can occur in a transformed
derivation of G, and mark each argument as either
?active? (C) or ?inactive? ( ), in all possible ways
that respect the segmentation property stated in
Lemma 3. Note that, because of Lemma 7 and
Lemma 1, the set of nonterminals is finite. As the
start symbol, we use s, the final category of G.
The set of productions of GT is constructed as
follows. For each lexicon entry  ` c of G, we in-
clude all productions of the form x !  , where x
is some marked version of c. These productions
represent all valid guesses about the activity of the
arguments of c during a derivation of G. The re-
maining productions encode all valid instantiations
of composition rules, keeping track of active and
inactive arguments to prevent derivations with crit-
ical nodes. More specifically, they have the form
x? ! x=yC y? or x? ! y? xnyC ,
where the arguments in the y-part of the secondary
premise are all marked as inactive, the sequence ?
contains at most one argument marked as active,
and the annotations of the left-hand side nonter-
minal are copied over from the corresponding an-
notations on the right-hand side.
The correctness of the construction ofGT can be
proved by induction on the length of a transformed
derivation of G on the one hand, and the length of
a derivation of GT on the other hand. 
3.5 PCCG ? CCG
We are now ready to prove our main result, repeated
here for convenience.
Theorem 1 Every language that can be generated
by a pure CCG grammar has a Parikh-equivalent
context-free sublanguage.
Proof. Let G be a pure CCG, and let LT be the
set of yields of the transformed derivations of G.
Inspecting the rewriting rules, it is clear that every
string of L.G/ is the permutation of a string in LT :
the transformation only rearranges the yields. By
Lemma 8, we also know that LT is context-free.
Since every transformed derivation is a valid deriv-
ation of G, we have LT  L.G/. 
As an immediate consequence, we find:
Proposition 2 The class of languages generated
by pure CCG cannot generate all languages that
can be generated by CCG with rule restrictions.
Proof. The CCG formalism considered by Vijay-
Shanker and Weir (1994) can generate the non-con-
text-free language L3. However, the only Parikh-
equivalent sublanguage of that language isL3 itself.
From Theorem 1, we therefore conclude that L3
cannot be generated by pure CCG. 
540
In the light of the equivalence result established
by Vijay-Shanker and Weir (1994), this means that
pure CCG cannot generate all languages that can
be generated by TAG.
4 Multi-Modal CCG
We now extend Theorem 1 to multi-modal CCG.
We will see that at least for a popular version
of multi-modal CCG, the B&K-CCG formalism
presented by Baldridge and Kruijff (2003), the
proof can be adapted quite straightforwardly. This
means that even B&K-CCG becomes less express-
ive when rule restrictions are disallowed.
4.1 Multi-Modal CCG
The term ?multi-modal CCG? (MM-CCG) refers to
a family of extensions to CCG which attempt to
bring some of the expressive power of Categorial
Type Logic (Moortgat, 1997) into CCG. Slashes in
MM-CCG have slash types, and rules can be restric-
ted to only apply to arguments that have slashes
of the correct type. The idea behind this extension
is that many constraints that in ordinary CCG can
only be expressed in terms of rule restrictions can
now be specified in the lexicon entries by giving
the slashes the appropriate types.
The most widely-known version of multi-modal
CCG is the formalism defined by Baldridge and
Kruijff (2003) and used by Steedman and Baldridge
(2010); we refer to it as B&K-CCG. This formalism
uses an inventory of four slash types, f?;;?;  g,
arranged in a simple type hierarchy: ? is the most
general type,  the most specific, and  and ? are
in between. Every slash in a B&K-CCG lexicon is
annotated with one of these slash types.
The combinatory rules in B&K-CCG, given in
Figure 4, are defined to be sensitive to the slash
types. In particular, slashes with the types ? and 
can only be eliminated by harmonic and crossed
compositions, respectively.2 Thus, a grammar
writer can constrain the application of harmonic
and crossed composition rules to certain categor-
ies by assigning appropriate types to the slashes
of this category in the lexicon. Application rules
apply to slashes of any type. As before, we call
an MM-CCG grammar pure if it only uses applic-
ation and generalized compositions, and does not
provide means to restrict rule applications.
2Our definitions of generalized harmonic and crossed com-
position are the same as the ones used by Hockenmaier and
Young (2008), but see the discussion in Section 4.3.
x=?y y ) x forward application
y xn?y ) x backward application
x=?y y=?z? ) x=?z? forward harmonic composition
x=y ynz? ) xnz? forward crossed composition
yn?z? xn?y ) xn?z? backward harmonic composition
y=z? xny ) x=z? backward crossed composition
Figure 4: Rules in B&K-CCG.
4.2 Rule Restrictions in B&K-CCG
We will now see what happens to the proof of The-
orem 1 in the context of pure B&K-CCG. There
is only one point in the entire proof that could be
damaged by the introduction of slash types, and
that is the result that if a transformation rule from
Figure 3 is applied to a correct derivation, then the
result is also grammatical. For this, it must not
only be the case that the degree on the composition
operations is preserved (Lemma 6), but also that
the transformed derivation remains consistent with
the slash types. Slash types make the derivation
process sensitive to word order by restricting the
use of compositions to categories with the appropri-
ate type, and the transformation rules permute the
order of the words in the string. There is a chance
therefore that a transformed derivation might not
be grammatical in B&K-CCG.
We now show that this does not actually happen,
for rule R3; the other three rules are analogous.
Using s1; s2; s3 as variables for the relevant slash
types, rule R3 appears in B&K-CCG as follows:
z
x=s1y yjs2w?ns3z
xjs2w?ns3z
xjs2w?
R3
H) x=s1y
z yjs2w?ns3z
yjs2w?
xjs2w?
Because the original derivation is correct, we know
that, if the slash of w is forward, then s1 and s2 are
subtypes of ?; if the slash is backward, they are
subtypes of . A similar condition holds for s3 and
the first slash in  ; if  is empty, then s3 can be
anything because the second rule is an application.
After the transformation, the argument =s1y is
used to compose with yjs2w? . The direction of
the slash in front of the w is the same as before,
so the (harmonic or crossed) composition is still
compatible with the slash types s1 and s2. An
analogous argument shows that the correctness of
combining ns3z with  carries over from the left to
the right-hand side. Thus the transformation maps
grammatical derivations into grammatical deriva-
tions. The rest of the proof in Section 3 continues
to work literally, so we have the following result:
541
Theorem 2 Every language that can be generated
by a pure B&K-CCG grammar contains a Parikh-
equivalent context-free sublanguage.
This means that pure B&K-CCG is just as unable
to generate L3 as pure CCG is. In other words,
the weak generative capacity of CCG with rule
restrictions, and in particular that of the formalism
considered by Vijay-Shanker and Weir (1994), is
strictly greater than the generative capacity of pure
B&K-CCG?although we conjecture (but cannot
prove) that pure B&K-CCG is still more expressive
than pure non-modal CCG.
4.3 Towards More Expressive MM-CCGs
To put the result of Theorem 2 into perspective, we
will now briefly consider ways in which B&K-CCG
might be modified in order to obtain a pure multi-
modal CCG that is weakly equivalent to CCG in
the style of Vijay-Shanker and Weir (1994). Such
a modification would have to break the proof in
Section 4.2, which is harder than it may seem at
first glance. For instance, simply assuming a more
complex type system will not do it, because the
arguments ns3z and =s1y are eliminated using the
same rules in the original and the transformed deriv-
ations, so if the derivation step was valid before, it
will still be valid after the transformation. Instead,
we believe that it is necessary to make the composi-
tion rules sensitive to the categories inside ? and 
instead of only the arguments ns3z and =s1y, and
we can see two ways how to do this.
First, one could imagine a version of multi-
modal CCG with unary modalities that can be used
to mark certain category occurrences. In such an
MM-CCG, the composition rules for a certain slash
type could be made sensitive to the presence or
absence of unary modalities in ?. Say for instance
that the slash type s1 in the modalized version of
R3 in Section 4.2 would require that no category in
the secondary argument is marked with the unary
modality ??, but ? contains a category marked
with ??. Then the transformed derivation would
be ungrammatical.
A second approach concerns the precise defin-
ition of the generalized composition rules, about
which there is a surprising degree of disagreement.
We have followed Hockenmaier and Young (2008)
in classifying instances of generalized forward
composition as harmonic if the innermost slash of
the secondary argument is forward and crossed if
it is backward. However, generalized forward com-
position is sometimes only accepted as harmonic
if all slashes of the secondary argument are for-
ward (see e.g. Baldridge (2002) (40, 41), Steedman
(2001) (19)). At the same time, based on the prin-
ciple that CCG rules should be derived from proofs
of Categorial Type Logic as Baldridge (2002) does,
it can be argued that generalized composition rules
of the form x=y y=znw ) x=znw, which we
have considered as harmonic, should actually be
classified as crossed, due to the presence of a slash
of opposite directionality in front of the w. This
definition would break our proof. Thus our res-
ult might motivate further research on the ?correct?
definition of generalized composition rules, which
might then strengthen the generative capacity of
pure MM-CCG.
5 Conclusion
In this paper, we have shown that the weak generat-
ive capacity of pure CCG and even pure B&K-CCG
crucially depends on the ability to restrict the ap-
plication of individual rules. This means that these
formalisms cannot be fully lexicalized, in the sense
that certain languages can only be described by
selecting language-specific rules.
Our result generalizes Koller and Kuhlmann?s
(2009) result for pure first-order CCG. Our proof
is not as different as it looks at first glance, as
their construction of mapping a CCG derivation to
a valency tree and back to a derivation provides a
different transformation on derivation trees. Our
transformation is also technically related to the nor-
mal form construction for CCG parsing presented
by Eisner (1996).
Of course, at the end of the day, the issue that is
more relevant to computational linguistics than a
formalism?s ability to generate artificial languages
such as L3 is how useful it is for modeling natural
languages. CCG, and multi-modal CCG in partic-
ular, has a very good track record for this. In this
sense, our formal result can also be understood as
a contribution to a discussion about the expressive
power that is needed to model natural languages.
Acknowledgments
We have profited enormously from discussions with
Jason Baldridge and Mark Steedman, and would
also like to thank the anonymous reviewers for their
detailed comments.
542
References
Franz Baader and Tobias Nipkow. 1998. Term Rewrit-
ing and All That. Cambridge University Press.
Jason Baldridge and Geert-Jan M. Kruijff. 2003.
Multi-modal Combinatory Categorial Grammar.
In Proceedings of the Tenth Conference of the
European Chapter of the Association for Compu-
tational Linguistics (EACL), pages 211?218, Bud-
apest, Hungary.
Jason Baldridge. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Yehoshua Bar-Hillel, Haim Gaifman, and Eli Shamir.
1964. On categorial and phrase structure gram-
mars. In Language and Information: Selected Es-
says on their Theory and Application, pages 99?115.
Addison-Wesley.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of the 20th International
Conference on Computational Linguistics (COL-
ING), pages 176?182, Geneva, Switzerland.
Stephen Clark and James Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4).
Haskell B. Curry, Robert Feys, and William Craig.
1958. Combinatory Logic. Volume 1. Studies in
Logic and the Foundations of Mathematics. North-
Holland.
Jason Eisner. 1996. Efficient normal-form parsing
for combinatory categorial grammar. In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 79?86,
Santa Cruz, CA, USA.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combin-
atory Categorial Grammar. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 335?342, Phil-
adelphia, USA.
Julia Hockenmaier and Peter Young. 2008. Non-local
scrambling: the equivalence of TAG and CCG revis-
ited. In Proceedings of the 9th Internal Workshop on
Tree Adjoining Grammars and Related Formalisms
(TAG+9), T?bingen, Germany.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages, volume 3, pages 69?123. Springer.
Alexander Koller and Marco Kuhlmann. 2009. De-
pendency trees and the strong generative capacity of
CCG. In Proceedings of the Twelfth Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL), pages 460?468, Athens,
Greece.
Michael Moortgat. 1997. Categorial type logics. In
Handbook of Logic and Language, chapter 2, pages
93?177. Elsevier.
David Reitter, Julia Hockenmaier, and Frank Keller.
2006. Priming effects in combinatory categorial
grammar. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 308?316, Sydney, Australia.
Mark Steedman and Jason Baldridge. 2010. Combin-
atory categorial grammar. In R. Borsley and K. Bor-
jars, editors, Non-Transformational Syntax. Black-
well. Draft 7.0, to appear.
Mark Steedman. 2001. The Syntactic Process. MIT
Press.
K. Vijay-Shanker and David J. Weir. 1994. The equi-
valence of four extensions of context-free grammars.
Mathematical Systems Theory, 27(6):511?546.
David J. Weir and Aravind K. Joshi. 1988. Combinat-
ory categorial grammars: Generative power and rela-
tionship to linear context-free rewriting systems. In
Proceedings of the 26th Annual Meeting of the As-
sociation for Computational Linguistics, pages 278?
285, Buffalo, NY, USA.
543
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 450?459,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Optimal Head-Driven Parsing Complexity
for Linear Context-Free Rewriting Systems
Pierluigi Crescenzi
Dip. di Sistemi e Informatica
Universita` di Firenze
Daniel Gildea
Computer Science Dept.
University of Rochester
Andrea Marino
Dip. di Sistemi e Informatica
Universita` di Firenze
Gianluca Rossi
Dip. di Matematica
Universita` di Roma Tor Vergata
Giorgio Satta
Dip. di Ingegneria dell?Informazione
Universita` di Padova
Abstract
We study the problem of finding the best head-
driven parsing strategy for Linear Context-
Free Rewriting System productions. A head-
driven strategy must begin with a specified
righthand-side nonterminal (the head) and add
the remaining nonterminals one at a time in
any order. We show that it is NP-hard to find
the best head-driven strategy in terms of either
the time or space complexity of parsing.
1 Introduction
Linear Context-Free Rewriting Systems (LCFRSs)
(Vijay-Shankar et al, 1987) constitute a very general
grammatical formalism which subsumes context-
free grammars (CFGs) and tree adjoining grammars
(TAGs), as well as the synchronous context-free
grammars (SCFGs) and synchronous tree adjoin-
ing grammars (STAGs) used as models in machine
translation.1 LCFRSs retain the fundamental prop-
erty of CFGs that grammar nonterminals rewrite
independently, but allow nonterminals to generate
discontinuous phrases, that is, to generate more
than one span in the string being produced. This
important feature has been recently exploited by
Maier and S?gaard (2008) and Kallmeyer and Maier
(2010) for modeling phrase structure treebanks with
discontinuous constituents, and by Kuhlmann and
Satta (2009) for modeling non-projective depen-
dency treebanks.
The rules of a LCFRS can be analyzed in terms
of the properties of rank and fan-out. Rank is the
1To be more precise, SCFGs and STAGs generate languages
composed by pair of strings, while LCFRSs generate string lan-
guages. We can abstract away from this difference by assuming
concatenation of components in a string pair.
number of nonterminals on the right-hand side (rhs)
of a rule, while fan-out is the number of spans of
the string generated by the nonterminal in the left-
hand side (lhs) of the rule. CFGs are equivalent to
LCFRSs with fan-out one, while TAGs are one type
of LCFRSs with fan-out two. Rambow and Satta
(1999) show that rank and fan-out induce an infi-
nite, two-dimensional hierarchy in terms of gener-
ative power; while CFGs can always be reduced to
rank two (Chomsky Normal Form), this is not the
case for LCFRSs with any fan-out greater than one.
General algorithms for parsing LCFRSs build a
dynamic programming chart of recognized nonter-
minals bottom-up, in a manner analogous to the
CKY algorithm for CFGs (Hopcroft and Ullman,
1979), but with time and space complexity that are
dependent on the rank and fan-out of the gram-
mar rules. Whenever it is possible, binarization of
LCFRS rules, or reduction of rank to two, is there-
fore important for parsing, as it reduces the time
complexity needed for dynamic programming. This
has lead to a number of binarization algorithms for
LCFRSs, as well as factorization algorithms that
factor rules into new rules with smaller rank, with-
out necessarily reducing rank all the way to two.
Kuhlmann and Satta (2009) present an algorithm
for binarizing certain LCFRS rules without increas-
ing their fan-out, and Sagot and Satta (2010) show
how to reduce rank to the lowest value possible for
LCFRS rules of fan-out two, again without increas-
ing fan-out. Go?mez-Rodr??guez et al (2010) show
how to factorize well-nested LCFRS rules of arbi-
trary fan-out for efficient parsing.
In general there may be a trade-off required
between rank and fan-out, and a few recent pa-
pers have investigated this trade-off taking gen-
450
eral LCFRS rules as input. Go?mez-Rodr??guez et
al. (2009) present an algorithm for binarization of
LCFRSs while keeping fan-out as small as possi-
ble. The algorithm is exponential in the resulting
fan-out, and Go?mez-Rodr??guez et al (2009) mention
as an important open question whether polynomial-
time algorithms to minimize fan-out are possible.
Gildea (2010) presents a related method for bina-
rizing rules while keeping the time complexity of
parsing as small as possible. Binarization turns out
to be possible with no penalty in time complexity,
but, again, the factorization algorithm is exponen-
tial in the resulting time complexity. Gildea (2011)
shows that a polynomial time algorithm for factor-
izing LCFRSs in order to minimize time complexity
would imply an improved approximation algorithm
for the well-studied graph-theoretic property known
as treewidth. However, whether the problem of fac-
torizing LCFRSs in order to minimize time com-
plexity is NP-hard is still an open question in the
above works.
Similar questions have arisen in the context of
machine translation, as the SCFGs used to model
translation are also instances of LCFRSs, as already
mentioned. For SCFG, Satta and Peserico (2005)
showed that the exponent in the time complexity
of parsing algorithms must grow at least as fast as
the square root of the rule rank, and Gildea and
?Stefankovic? (2007) tightened this bound to be lin-
ear in the rank. However, neither paper provides an
algorithm for finding the best parsing strategy, and
Huang et al (2009) mention that whether finding the
optimal parsing strategy for an SCFG rule is NP-
hard is an important problem for future work.
In this paper, we investigate the problem of rule
binarization for LCFRSs in the context of head-
driven parsing strategies. Head-driven strategies be-
gin with one rhs symbol, and add one nontermi-
nal at a time. This rules out any factorization in
which two subsets of nonterminals of size greater
than one are combined in a single step. Head-driven
strategies allow for the techniques of lexicalization
and Markovization that are widely used in (projec-
tive) statistical parsing (Collins, 1997). The statis-
tical LCFRS parser of Kallmeyer and Maier (2010)
binarizes rules head-outward, and therefore adopts
what we refer to as a head-driven strategy. How-
ever, the binarization used by Kallmeyer and Maier
(2010) simply proceeds left to right through the rule,
without considering the impact of the parsing strat-
egy on either time or space complexity. We examine
the question of whether we can efficiently find the
strategy that minimizes either the time complexity
or the space complexity of parsing. While a naive
algorithm can evaluate all r! head-driven strategies
in time O(n ? r!), where r is the rule?s rank and n
is the total length of the rule?s description, we wish
to determine whether a polynomial-time algorithm
is possible.
Since parsing problems can be cast in terms of
logic programming (Shieber et al, 1995), we note
that our problem can be thought of as a type of
query optimization for logic programming. Query
optimization for logic programming is NP-complete
since query optimization for even simple conjunc-
tive database queries is NP-complete (Chandra and
Merlin, 1977). However, the fact that variables in
queries arising from LCFRS rules correspond to the
endpoints of spans in the string to be parsed means
that these queries have certain structural properties
(Gildea, 2011). We wish to determine whether the
structure of LCFRS rules makes efficient factoriza-
tion algorithms possible.
In the following, we show both the the time- and
space-complexity problems to be NP-hard for head-
driven strategies. We provide what is to our knowl-
edge the first NP-hardness result for a grammar fac-
torization problem, which we hope will aid in under-
standing parsing algorithms in general.
2 LCFRSs and parsing complexity
In this section we briefly introduce LCFRSs and de-
fine the problem of optimizing head-driven parsing
complexity for these formalisms. For a positive in-
teger n, we write [n] to denote the set {1, . . . , n}.
As already mentioned in the introduction,
LCFRSs generate tuples of strings over some finite
alphabet. This is done by associating each produc-
tion p of a grammar with a function g that takes as
input the tuples generated by the nonterminals in p?s
rhs, and rearranges their string components into a
new tuple, possibly adding some alphabet symbols.
Let V be some finite alphabet. We write V ? for
the set of all (finite) strings over V . For natural num-
bers r ? 0 and f, f1, . . . , fr ? 1, consider a func-
451
tion g : (V ?)f1 ? ? ? ? ? (V ?)fr ? (V ?)f defined by
an equation of the form
g(?x1,1, . . . , x1,f1?, . . . , ?xr,1, . . . , xr,fr?) = ~? .
Here the xi,j?s denote variables over strings in V ?,
and ~? = ??1, . . . , ?f ? is an f -tuple of strings over
g?s argument variables and symbols in V . We say
that g is linear, non-erasing if ~? contains exactly
one occurrence of each argument variable. We call r
and f the rank and the fan-out of g, respectively,
and write r(g) and f(g) to denote these quantities.
Example 1 g1(?x1,1, x1,2?) = ?x1,1x1,2? takes as
input a tuple with two strings and returns a tuple
with a single string, obtained by concatenating the
components in the input tuple. g2(?x1,1, x1,2?) =
?ax1,1b, cx1,2d? takes as input a tuple with two
strings and wraps around these strings with sym-
bols a, b, c, d ? V . Both functions are linear, non-
erasing, and we have r(g1) = r(g2) = 1, f(g1) = 1
and f(g2) = 2. 2
A linear context-free rewriting system is a tuple
G = (VN , VT , P, S), where VN and VT are finite,
disjoint alphabets of nonterminal and terminal sym-
bols, respectively. Each A ? VN is associated with
a value f(A), called its fan-out. The nonterminal S
is the start symbol, with f(S) = 1. Finally, P is a
set of productions of the form
p : A ? g(A1, A2, . . . , Ar(g)) , (1)
where A,A1, . . . , Ar(g) ? VN , and g : (V ?T )f(A1)
? ? ? ?? (V ?T )f(Ar(g)) ? (V ?T )f(A) is a linear, non-
erasing function.
Production (1) can be used to transform the
r(g) string tuples generated by the nonterminals
A1, . . . , Ar(g) into a tuple of f(A) strings gener-
ated by A. The values r(g) and f(g) are called the
rank and fan-out of p, respectively, written r(p) and
f(p). Given that f(S) = 1, S generates a set of
strings, defining the language L(G).
Example 2 Let g1 and g2 be as in Example 1, and
let g3() = ??, ??. Consider the LCFRS G defined by
the productions p1 : S ? g1(A), p2 : A ? g2(A)
and p3 : A ? g3(). We have f(S) = 1, f(A) =
f(G) = 2, r(p3) = 0 and r(p1) = r(p2) = r(G) =
1. We have L(G) = {anbncndn |n ? 1}. For in-
stance, the string a3b3c3d3 is generated by means
fan-out strategy
4 ((A1 ?A4) ?A3)? ?A2
3 (A1 ?A4)? ? (A2 ?A3)
3 ((A1 ?A2)? ?A4) ?A3
2 ((A?2 ?A3) ?A4) ?A1
Figure 1: Some parsing strategies for production p in Ex-
ample 3, and the associated maximum value for fan-out.
Symbol ? denotes the merging operation, and superscript
? marks the first step in the strategy in which the highest
fan-out is realized.
of the following bottom-up process. First, the tuple
??, ?? is generated by A through p3. We then iterate
three times the application of p2 to ??, ??, resulting
in the tuple ?a3b3, c3d3?. Finally, the tuple (string)
?a3b3c3d3? is generated by S through application of
p1. 2
Existing parsing algorithms for LCFRSs exploit
dynamic programming. These algorithms compute
partial parses of the input string w, represented by
means of specialized data structures called items.
Each item indexes the boundaries of the segments
of w that are spanned by the partial parse. In the
special case of parsing based on CFGs, an item con-
sists of two indices, while for TAGs four indices are
required.
In the general case of LCFRSs, parsing of a pro-
duction p as in (1) can be carried out in r(g) ? 1
steps, collecting already available parses for nonter-
minals A1, . . . , Ar(g) one at a time, and ?merging?
these into intermediate partial parses. We refer to the
order in which nonterminals are merged as a pars-
ing strategy, or, equivalently, a factorization of the
original grammar rule. Any parsing strategy results
in a complete parse of p, spanning f(p) = f(A)
segments of w and represented by some item with
2f(A) indices. However, intermediate items ob-
tained in the process might span more than f(A)
segments. We illustrate this through an example.
Example 3 Consider a linear non-erasing function
g(?x1,1, x1,2?, ?x2,1, x2,2?, ?x3,1, x3,2?, ?x4,1, x4,2?)
= ?x1,1x2,1x3,1x4,1, x3,2x2,2x4,2x1,2?, and a pro-
duction p : A ? g(A1, A2, A3, A4), where all the
nonterminals involved have fan-out 2. We could
parse p starting from A1, and then merging with A4,
452
v1
v2
v3 v4e1
e3
e2
e4
Figure 2: Example input graph for our construction of an
LCFRS production.
A3, and A2. In this case, after we have collected the
first three nonterminals, we have obtained a partial
parse having fan-out 4, that is, an item spanning 4
segments of the input string. Alternatively, we could
first merge A1 and A4, then merge A2 and A3, and
finally merge the two obtained partial parses. This
strategy is slightly better, resulting in a maximum
fan-out of 3. Other possible strategies can be ex-
plored, displayed in Figure 1. It turns out that the
best parsing strategy leads to fan-out 2. 2
The maximum fan-out f realized by a parsing
strategy determines the space complexity of the
parsing algorithm. For an input string w, items will
require (in the worst-case) 2f indices, each taking
O(|w|) possible values. This results in space com-
plexity of O(|w|2f ). In the special cases of parsing
based on CFGs and TAGs, this provides the well-
known space complexity of O(|w|2) and O(|w|4),
respectively.
It can also be shown that, if a partial parse hav-
ing fan-out f is obtained by means of the combi-
nation of two partial parses with fan-out f1 and f2,
respectively, the resulting time complexity will be
O(|w|f+f1+f2) (Seki et al, 1991; Gildea, 2010). As
an example, in the case of parsing based on CFGs,
nonterminals as well as partial parses all have fan-
out one, resulting in the standard time complexity of
O(|w|3) of dynamic programming methods. When
parsing with TAGs, we have to manipulate objects
with fan-out two (in the worst case), resulting in time
complexity of O(|w|6).
We investigate here the case of general LCFRS
productions, whose internal structure is consider-
ably more complex than the context-free or the tree
adjoining case. Optimizing the parsing complexity
for a production means finding a parsing strategy
that results in minimum space or time complexity.
We now turn the above optimization problems
into decision problems. In the MIN SPACE STRAT-
EGY problem one takes as input an LCFRS produc-
tion p and an integer k, and must decide whether
there exists a parsing strategy for p with maximum
fan-out not larger than k. In the MIN TIME STRAT-
EGY problem one is given p and k as above and must
decide whether there exists a parsing strategy for
p such that, in any of its steps merging two partial
parses with fan-out f1 and f2 and resulting in a par-
tial parse with fan-out f , the relation f+f1+f2 ? k
holds.
In this paper we investigate the above problems in
the context of a specific family of linguistically mo-
tivated parsing strategies for LCFRSs, called head-
driven. In a head-driven strategy, one always starts
parsing a production p from a fixed nonterminal in
its rhs, called the head of p, and merges the remain-
ing nonterminals one at a time with the partial parse
containing the head. Thus, under these strategies,
the construction of partial parses that do not include
the head is forbidden, and each parsing step involves
at most one partial parse. In Figure 1, all of the dis-
played strategies but the one in the second line are
head-driven (for different choices of the head).
3 NP-completeness results
For an LCFRS production p, let H be its head non-
terminal, and let A1, . . . , An be all the non-head
nonterminals in p?s rhs, with n + 1 = r(p). A head-
driven parsing strategy can be represented as a per-
mutation pi over the set [n], prescribing that the non-
head nonterminals in p?s rhs should be merged with
H in the order Api(1), Api(2), . . . , Api(n). Note that
there are n! possible head-driven parsing strategies.
To show that MIN SPACE STRATEGY is NP-
hard under head-driven parsing strategies, we reduce
from the MIN CUT LINEAR ARRANGEMENT prob-
lem, which is a decision problem over (undirected)
graphs. Given a graph M = (V,E) with set of ver-
tices V and set of edges E, a linear arrangement
of M is a bijective function h from V to [n], where
|V | = n. The cutwidth of M at gap i ? [n? 1] and
with respect to a linear arrangement h is the number
of edges crossing the gap between the i-th vertex and
its successor:
cw(M,h, i) = |{(u, v) ? E |h(u) ? i < h(v)}| .
453
p : A ? g(H,A1, A2, A3, A4)
g(?xH,e1 , xH,e2 , xH,e3 , xH,e4?, ?xA1,e1,l, xA1,e1,r, xA1,e3,l, xA1,e3,r?, ?xA2,e1,l, xA2,e1,r, xA2,e2,l, xA2,e2,r?,
?xA3,e2,l, xA3,e2,r, xA3,e3,l, xA3,e3,r, xA3,e4,l, xA3,e4,r?, ?xA4,e4,l, xA4,e4,r?) =
? xA1,e1,lxA2,e1,lxH,e1xA1,e1,rxA2,e1,r, xA2,e2,lxA3,e2,lxH,e2xA2,e2,rxA3,e2,r,
xA1,e3,lxA3,e3,lxH,e3xA1,e3,rxA3,e3,r, xA3,e4,lxA4,e4,lxH,e4xA3,e4,rxA4,e4,r ?
Figure 3: The construction used to prove Theorem 1 builds the LCFRS production p shown, when given as input the
graph of Figure 2.
The cutwidth of M is then defined as
cw(M) = min
h
max
i?[n?1]
cw(M,h, i) .
In the MIN CUT LINEAR ARRANGEMENT problem,
one is given as input a graph M and an integer k, and
must decide whether cw(M) ? k. This problem has
been shown to be NP-complete (Gavril, 1977).
Theorem 1 The MIN SPACE STRATEGY problem
restricted to head-driven parsing strategies is NP-
complete.
PROOF We start with the NP-hardness part. Let
M = (V,E) and k be an input instance for
MIN CUT LINEAR ARRANGEMENT, and let V =
{v1, . . . , vn} and E = {e1, . . . , eq}. We assume
there are no self loops in M , since these loops do not
affect the value of the cutwidth and can therefore be
removed. We construct an LCFRS production p and
an integer k? as follows.
Production p has a head nonterminal H and a non-
head nonterminal Ai for each vertex vi ? V . We let
H generate tuples with a string component for each
edge ei ? E. Thus, we have f(H) = q. Accord-
ingly, we use variables xH,ei , for each ei ? E, to
denote the string components in tuples generated by
H .
For each vi ? V , let E(vi) ? E be the set of
edges impinging on vi; thus |E(vi)| is the degree
of vi. We let Ai generate a tuple with two string
components for each ej ? E(vi). Thus, we have
f(Ai) = 2 ? |E(vi)|. Accordingly, we use variables
xAi,ej ,l and xAi,ej ,r , for each ej ? E(vi), to de-
note the string components in tuples generated by
Ai (here subscripts l and r indicate left and right
positions, respectively; see below).
We set r(p) = n + 1 and f(p) = q, and
define p by A ? g(H,A1, A2, . . . , An), with
g(tH , tA1 , . . . , tAn) = ??1, . . . , ?q?. Here tH is the
tuple of variables for H and each tAi , i ? [n], is the
tuple of variables for Ai. Each string ?i, i ? [q], is
specified as follows. Let vs and vt be the endpoints
of ei, with vs, vt ? V and s < t. We define
?i = xAs,ei,lxAt,ei,lxH,eixAs,ei,rxAt,ei,r .
Observe that whenever edge ei impinges on vertex
vj , then the left and right strings generated by Aj
and associated with ei wrap around the string gen-
erated by H and associated with the same edge. Fi-
nally, we set k? = q + k.
Example 4 Given the input graph of Figure 2, our
reduction constructs the LCFRS production shown
in Figure 3. Figure 4 gives a visualization of how the
spans in this production fit together. For each edge
in the graph of Figure 2, we have a group of five
spans in the production: one for the head nontermi-
nal, and two spans for each of the two nonterminals
corresponding to the edge?s endpoints. 2
Assume now some head-driven parsing strategy
pi for p. For each i ? [n], we define Dpii to be the
partial parse obtained after step i in pi, consisting
of the merge of nonterminals H,Api(1), . . . , Api(i).
Consider some edge ej = (vs, vt). We observe that
for any Dpii that includes or excludes both nontermi-
nals As and At, the ?j component in the definition
of p is associated with a single string, and therefore
contributes with a single unit to the fan-out of the
partial parse. On the other hand, if Dpii includes only
one nonterminal between As and At, the ?j compo-
nent is associated with two strings and contributes
with two units to the fan-out of the partial parse.
We can associate with pi a linear arrangement hpi
of M by letting hpi(vpi(i)) = i, for each vi ? V .
From the above observation on the fan-out of Dpii ,
454
xA1,e1,lxA2,e1,l xH,e1 xA1,e1,rxA2,e1,r xA2,e2,lxA3,e2,l xH,e2 xA2,e2,rxA3,e2,r xA1,e3,lxA3,e3,l xH,e3 xA1,e3,rxA3,e3,r xA3,e4,lxA4,e4,l xH,e4 xA3,e4,rxA4,e4,r
H
A1
A2
A3
A4
Figure 4: A visualization of how the spans for each nonterminal fit together in the left-to-right order defined by the
production of Figure 3.
we have the following relation, for every i ? [n?1]:
f(Dpii ) = q + cw(M,hpi, i) .
We can then conclude that M,k is a positive instance
of MIN CUT LINEAR ARRANGEMENT if and only
if p, k? is a positive instance of MIN SPACE STRAT-
EGY. This proves that MIN SPACE STRATEGY is
NP-hard.
To show that MIN SPACE STRATEGY is in NP,
consider a nondeterministic algorithm that, given an
LCFRS production p and an integer k, guesses a
parsing strategy pi for p, and tests whether f(Dpii ) ?
k for each i ? [n]. The algorithm accepts or rejects
accordingly. Such an algorithm can clearly be im-
plemented to run in polynomial time. 
We now turn to the MIN TIME STRATEGY prob-
lem, restricted to head-driven parsing strategies. Re-
call that we are now concerned with the quantity
f1 + f2 + f , where f1 is the fan-out of some partial
parse D, f2 is the fan-out of a nonterminal A, and f
is the fan out of the partial parse resulting from the
merge of the two previous analyses.
We need to introduce the MODIFIED CUTWIDTH
problem, which is a variant of the MIN CUT LIN-
EAR ARRANGEMENT problem. Let M = (V,E) be
some graph with |V | = n, and let h be a linear ar-
rangement for M . The modified cutwidth of M at
position i ? [n] and with respect to h is the number
of edges crossing over the i-th vertex:
mcw(M,h, i) = |{(u, v) ? E |h(u) < i < h(v)}| .
The modified cutwidth of M is defined as
mcw(M) = min
h
max
i?[n]
mcw(M,h, i) .
In the MODIFIED CUTWIDTH problem one is given
as input a graph M and an integer k, and must
decide whether mcw(M) ? k. The MODIFIED
CUTWIDTH problem has been shown to be NP-
complete by Lengauer (1981). We strengthen this
result below; recall that a cubic graph is a graph
without self loops where each vertex has degree
three.
Lemma 1 The MODIFIED CUTWIDTH problem re-
stricted to cubic graphs is NP-complete.
PROOF The MODIFIED CUTWIDTH problem has
been shown to be NP-complete when restricted to
graphs of maximum degree three by Makedon et al
(1985), reducing from a graph problem known as
bisection width (see also Monien and Sudborough
(1988)). Specifically, the authors construct a graph
G? of maximum degree three and an integer k? from
an input graph G = (V,E) with an even number n
of vertices and an integer k, such that mcw(G?) ? k?
if and only if the bisection width bw(G) of G is not
greater than k, where
bw(G) = min
A,B?V
|{(u, v) ? E |u ? A ? v ? B}|
with A ?B = ?, A ?B = V , and |A| = |B|.
The graph G? has vertices of degree two and three
only, and it is based on a grid-like gadget R(r, c); see
Figure 5. For each vertex of G, G? includes a com-
ponent R(2n4, 8n4+8). Moreover, G? has a compo-
nent called an H-shaped graph, containing left and
right columns R(3n4, 12n4 + 12) connected by a
middle bar R(2n4, 12n4 + 9); see Figure 6. From
each of the n vertex components there is a sheaf of
2n2 edges connecting distinct degree 2 vertices in
the component to 2n2 distinct degree 2 vertices in
455
x
x x1
x2
x3
x4
x5
x x1 x2
x5
x3
x4
Figure 5: The R(5, 10) component (left), the modification of its degree 2 vertex x (middle), and the corresponding
arrangement (right).
the middle bar of the H-shaped graph. Finally, for
each edge (vi, vj) of G there is an edge in G? con-
necting a degree 2 vertex in the component corre-
sponding to the vertex vi with a degree 2 vertex in
the component corresponding to the vertex vj . The
integer k? is set to 3n4 + n3 + k ? 1.
Makedon et al (1985) show that the modified
cutwidth of R(r, c) is r ? 1 whenever r ? 3 and
c ? 4r + 8. They also show that an optimal lin-
ear arrangement for G? has the form depicted in Fig-
ure 6, where half of the vertex components are to
the left of the H-shaped graph and all the other ver-
tex components are to the right. In this arrangement,
the modified cutwidth is attested by the number of
edges crossing over the vertices in the left and right
columns of the H-shaped graph, which is equal to
3n4 ? 1 + n
2
2n2 + ? = 3n4 + n3 + ? ? 1 (2)
where ? denotes the number of edges connecting
vertices to the left with vertices to the right of the
H-shaped graph. Thus, bw(G) ? k if and only if
mcw(G?) ? k?.
All we need to show now is how to modify the
components of G? in order to make it cubic.
Modifying the vertex components All vertices
x of degree 2 of the components corresponding to
a vertex in G can be transformed into a vertex of
degree 3 by adding five vertices x1, . . . , x5 con-
nected as shown in the middle bar of Figure 5. Ob-
serve that these five vertices can be positioned in
the arrangement immediately after x in the order
x1, x2, x5, x3, x4 (see the right part of the figure).
The resulting maximum modified cutwidth can in-
crease by 2 in correspondence of vertex x5. Since
the vertices of these components, in the optimal
arrangement, have modified cutwidth smaller than
2n4 + n3 + n2, an increase by 2 is still smaller than
the maximum modified cutwidth of the entire graph,
which is 3n4 + O(n3).
Modifying the middle bar of the H-shaped graph
The vertices of degree 2 of this part of the graph can
be modified as in the previous paragraph. Indeed, in
the optimal arrangement, these vertices have mod-
ified cutwidth smaller than 2n4 + 2n3 + n2, and
an increase by 2 is still smaller than the maximum
cutwidth of the entire graph.
Modifying the left/right columns of the H-shaped
graph We replace the two copies of component
R(3n4, 12n4 + 12) with two copies of the new
component D(3n4, 24n4 + 16) shown in Figure 7,
which is a cubic graph. In order to prove that rela-
tion (2) still holds, it suffices to show that the modi-
fied cutwidth of the component D(r, c) is still r ? 1
whenever r ? 3 and c = 8r + 16.
We first observe that the linear arrangement ob-
tained by visiting the vertices of D(r, c) from top to
bottom and from left to right has modified cutwidth
r? 1. Let us now prove that, for any partition of the
vertices into two subsets V1 and V2 with |V1|, |V2| ?
4r2, there exist at least r disjoint paths between ver-
tices of V1 and vertices of V2. To this aim, we dis-
tinguish the following three cases.
? Any row has (at least) one vertex in V1 and one
vertex in V2: in this case, it is easy to see there
exist at least r disjoint paths between vertices
of V1 and vertices of V2.
? There exist at least 3r ?mixed? columns, that is,
columns with (at least) one vertex in V1 and one
vertex in V2. Again, it is easy to see that there
exist at least r disjoint paths between vertices
456
Figure 6: The optimal arrangement of G?.
of V1 and vertices of V2 (at least one path every
three columns).
? The previous two cases do not apply. Hence,
there exists a row entirely formed by vertices
of V1 (or, equivalently, of V2). The worst case
is when this row is the smallest one, that is, the
one with (c?3?1)2 + 1 = 4r + 7 vertices. Since
at most 3r ? 1 columns are mixed, we have
that at most (3r ? 1)(r ? 2) = 3r2 ? 7r +
2 vertices of V2 are on these mixed columns.
Since |V2| ? 4r2, this implies that at least r
columns are fully contained in V2. On the other
hand, at least 4r+7?(3r?1) = r+8 columns
are fully contained in V1. If the V1-columns
interleave with the V2-columns, then there exist
at least 2(r?1) disjoint paths between vertices
of V1 and vertices of V2. Otherwise, all the V1-
columns precede or follow all the V2-columns
(this corresponds to the optimal arrangement):
in this case, there are r disjoint paths between
vertices of V1 and vertices of V2.
Observe now that any linear arrangement partitions
the set of vertices in D(r, c) into the sets V1, consist-
ing of the first 4r2 vertices in the arrangement, and
V2, consisting of all the remaining vertices. Since
there are r disjoint paths connecting V1 and V2, there
must be at least r?1 edges passing over every vertex
in the arrangement which is assigned to a position
between the (4r2 + 1)-th and the position 4r2 + 1
from the right end of the arrangement: thus, the
modified cutwidth of any linear arrangement of the
vertices of D(r, c) is at least r ? 1.
We can then conclude that the original proof
of Makedon et al (1985) still applies, according to
relation (2). 
Figure 7: The D(5, 10) component.
We can now reduce from the MODIFIED
CUTWIDTH problem for cubic graphs to the MIN
TIME STRATEGY problem restricted to head-driven
parsing strategies.
Theorem 2 The MIN TIME STRATEGY problem re-
stricted to head-driven parsing strategies is NP-
complete.
PROOF We consider hardness first. Let M and k
be an input instance of the MODIFIED CUTWIDTH
problem restricted to cubic graphs, where M =
(V,E) and V = {v1, . . . , vn}. We construct an
LCFRS production p exactly as in the proof of The-
orem 1, with rhs nonterminals H,A1, . . . , An. We
also set k? = 2 ? k + 2 ? |E| + 9.
Assume now some head-driven parsing strategy pi
for p. After parsing step i ? [n], we have a partial
parse Dpii consisting of the merge of nonterminals
H,Api(1), . . . , Api(i). We write tc(p, pi, i) to denote
the exponent of the time complexity due to step i.
As already mentioned, this quantity is defined as the
sum of the fan-out of the two antecedents involved
in the parsing step and the fan-out of its result:
tc(p, pi, i) = f(Dpii?1) + f(Api(i)) + f(Dpii ) .
Again, we associate with pi a linear arrangement
hpi of M by letting hpi(vpi(i)) = i, for each vi ? V .
As in the proof of Theorem 1, the fan-out of Dpii
is then related to the cutwidth of the linear arrange-
457
ment hpi of M at position i by
f(Dpii ) = |E| + cw(M,hpi, i) .
From the proof of Theorem 1, the fan-out of nonter-
minal Api(i) is twice the degree of vertex vpi(i), de-
noted by |E(vpi(i))|. We can then rewrite the above
equation in terms of our graph M :
tc(p, pi, i) = 2 ? |E| + cw(M,hpi, i? 1) +
+ 2 ? |E(vpi(i))| + cw(M,hpi, i) .
The following general relation between cutwidth
and modified cutwidth is rather intuitive:
mcw(M,hpi, i) =
1
2
? [cw(M,hpi, i? 1) +
? |E(vpi(i))| + cw(M,hpi, i)] .
Combining the two equations above we obtain:
tc(p, pi, i) = 2 ? |E| + 3 ? |E(vpi(i))| +
+ 2 ?mcw(M,hpi, i) .
Because we are restricting M to the class of cubic
graphs, we can write:
tc(p, pi, i) = 2 ? |E| + 9 + 2 ?mcw(M,hpi, i) .
We can thus conclude that there exists a head-driven
parsing strategy for p with time complexity not
greater than 2 ? |E| + 9 + 2 ? k = k? if and only
if mcw(M) ? k.
The membership of MODIFIED CUTWIDTH in NP
follows from an argument similar to the one in the
proof of Theorem 1. 
We have established the NP-completeness of both
the MIN SPACE STRATEGY and the MIN TIME
STRATEGY decision problems. It is now easy to see
that the problem of finding a space- or time-optimal
parsing strategy for a LCFRS production is NP-hard
as well, and thus cannot be solved in polynomial (de-
terministic) time unless P = NP.
4 Concluding remarks
Head-driven strategies are important in parsing
based on LCFRSs, both in order to allow statistical
modeling of head-modifier dependencies and in or-
der to generalize the Markovization of CFG parsers
to parsers with discontinuous spans. However, there
are n! possible head-driven strategies for an LCFRS
production with a head and n modifiers. Choosing
among these possible strategies affects both the time
and the space complexity of parsing. In this paper
we have shown that optimizing the choice according
to either metric is NP-hard. To our knowledge, our
results are the first NP-hardness results for a gram-
mar factorization problem.
SCFGs and STAGs are specific instances of
LCFRSs. Grammar factorization for synchronous
models is an important component of current ma-
chine translation systems (Zhang et al, 2006), and
algorithms for factorization have been studied by
Gildea et al (2006) for SCFGs and by Nesson et al
(2008) for STAGs. These algorithms do not result
in what we refer as head-driven strategies, although,
as machine translation systems improve, lexicalized
rules may become important in this setting as well.
However, the results we have presented in this pa-
per do not carry over to the above mentioned syn-
chronous models, since the fan-out of these models
is bounded by two, while in our reductions in Sec-
tion 3 we freely use unbounded values for this pa-
rameter. Thus the computational complexity of opti-
mizing the choice of the parsing strategy for SCFGs
is still an open problem.
Finally, our results for LCFRSs only apply when
we restrict ourselves to head-driven strategies. This
is in contrast to the findings of Gildea (2011), which
show that, for unrestricted parsing strategies, a poly-
nomial time algorithm for minimizing parsing com-
plexity would imply an improved approximation al-
gorithm for finding the treewidth of general graphs.
Our result is stronger, in that it shows strict NP-
hardness, but also weaker, in that it applies only to
head-driven strategies. Whether NP-hardness can be
shown for unrestricted parsing strategies is an im-
portant question for future work.
Acknowledgments
The first and third authors are partially supported
from the Italian PRIN project DISCO. The sec-
ond author is partially supported by NSF grants IIS-
0546554 and IIS-0910611.
458
References
Ashok K. Chandra and Philip M. Merlin. 1977. Op-
timal implementation of conjunctive queries in rela-
tional data bases. In Proc. ninth annual ACM sympo-
sium on Theory of computing, STOC ?77, pages 77?90.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proc. 35th Annual
Conference of the Association for Computational Lin-
guistics (ACL-97), pages 16?23.
F. Gavril. 1977. Some NP-complete problems on graphs.
In Proc. 11th Conf. on Information Sciences and Sys-
tems, pages 91?95.
Daniel Gildea and Daniel ?Stefankovic?. 2007. Worst-case
synchronous grammar rules. In Proc. 2007 Meeting
of the North American chapter of the Association for
Computational Linguistics (NAACL-07), pages 147?
154, Rochester, NY.
Daniel Gildea, Giorgio Satta, and Hao Zhang. 2006.
Factoring synchronous grammars by sorting. In
Proc. International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06) Poster Session, pages 279?286.
Daniel Gildea. 2010. Optimal parsing strategies for Lin-
ear Context-Free Rewriting Systems. In Proc. 2010
Meeting of the North American chapter of the Associa-
tion for Computational Linguistics (NAACL-10), pages
769?776.
Daniel Gildea. 2011. Grammar factorization by tree de-
composition. Computational Linguistics, 37(1):231?
248.
Carlos Go?mez-Rodr??guez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in Linear Context-Free Rewriting Systems.
In Proc. 2009 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-09), pages 539?547.
Carlos Go?mez-Rodr??guez, Marco Kuhlmann, and Gior-
gio Satta. 2010. Efficient parsing of well-nested linear
context-free rewriting systems. In Proc. 2010 Meeting
of the North American chapter of the Association for
Computational Linguistics (NAACL-10), pages 276?
284, Los Angeles, California.
John E. Hopcroft and Jeffrey D. Ullman. 1979. Intro-
duction to Automata Theory, Languages, and Compu-
tation. Addison-Wesley, Reading, MA.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009. Binarization of synchronous
context-free grammars. Computational Linguistics,
35(4):559?595.
Laura Kallmeyer and Wolfgang Maier. 2010. Data-
driven parsing with probabilistic linear context-free
rewriting systems. In Proc. 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 537?545.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proc. 12th Conference of the European
Chapter of the ACL (EACL-09), pages 478?486.
Thomas Lengauer. 1981. Black-white pebbles and graph
separation. Acta Informatica, 16:465?475.
Wolfgang Maier and Anders S?gaard. 2008. Treebanks
and mild context-sensitivity. In Philippe de Groote,
editor, Proc. 13th Conference on Formal Grammar
(FG-2008), pages 61?76, Hamburg, Germany. CSLI
Publications.
F. S. Makedon, C. H. Papadimitriou, and I. H. Sudbor-
ough. 1985. Topological bandwidth. SIAM J. Alg.
Disc. Meth., 6(3):418?444.
B. Monien and I.H. Sudborough. 1988. Min cut is NP-
complete for edge weighted trees. Theor. Comput.
Sci., 58:209?229.
Rebecca Nesson, Giorgio Satta, and Stuart M. Shieber.
2008. Optimal k-arization of synchronous tree adjoin-
ing grammar. In Proc. 46th Annual Meeting of the
Association for Computational Linguistics (ACL-08),
pages 604?612.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theor. Comput. Sci., 223(1-2):87?120.
Beno??t Sagot and Giorgio Satta. 2010. Optimal rank re-
duction for linear context-free rewriting systems with
fan-out two. In Proc. 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 525?533,
Uppsala, Sweden.
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous context-
free grammars. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 803?810, Vancouver, Canada.
H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991.
On multiple context-free grammars. Theoretical Com-
puter Science, 88:191?229.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. The Journal of Logic Programming,
24(1-2):3?36.
K. Vijay-Shankar, D. L. Weir, and A. K. Joshi. 1987.
Characterizing structural descriptions produced by
various grammatical formalisms. In Proc. 25th An-
nual Conference of the Association for Computational
Linguistics (ACL-87), pages 104?111.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. 2006 Meeting of the North Ameri-
can chapter of the Association for Computational Lin-
guistics (NAACL-06), pages 256?263.
459
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 460?469,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Prefix Probability
for Probabilistic Synchronous Context-Free Grammars
Mark-Jan Nederhof
School of Computer Science
University of St Andrews
North Haugh, St Andrews, Fife
KY16 9SX
United Kingdom
markjan.nederhof@googlemail.com
Giorgio Satta
Dept. of Information Engineering
University of Padua
via Gradenigo, 6/A
I-35131 Padova
Italy
satta@dei.unipd.it
Abstract
We present a method for the computation of
prefix probabilities for synchronous context-
free grammars. Our framework is fairly gen-
eral and relies on the combination of a sim-
ple, novel grammar transformation and stan-
dard techniques to bring grammars into nor-
mal forms.
1 Introduction
Within the area of statistical machine translation,
there has been a growing interest in so-called syntax-
based translation models, that is, models that de-
fine mappings between languages through hierar-
chical sentence structures. Several such statistical
models that have been investigated in the literature
are based on synchronous rewriting or tree transduc-
tion. Probabilistic synchronous context-free gram-
mars (PSCFGs) are one among the most popular ex-
amples of such models. PSCFGs subsume several
syntax-based statistical translation models, as for in-
stance the stochastic inversion transduction gram-
mars of Wu (1997), the statistical model used by the
Hiero system of Chiang (2007), and systems which
extract rules from parsed text, as in Galley et al
(2004).
Despite the widespread usage of models related to
PSCFGs, our theoretical understanding of this class
is quite limited. In contrast to the closely related
class of probabilistic context-free grammars, a syn-
tax model for which several interesting mathemati-
cal and statistical properties have been investigated,
as for instance by Chi (1999), many theoretical prob-
lems are still unsolved for the class of PSCFGs.
This paper considers a parsing problem that is
well understood for probabilistic context-free gram-
mars but that has never been investigated in the con-
text of PSCFGs, viz. the computation of prefix prob-
abilities. In the case of a probabilistic context-free
grammar, this problem is defined as follows. We
are asked to compute the probability that a sentence
generated by our model starts with a prefix string v
given as input. This quantity is defined as the (pos-
sibly infinite) sum of the probabilities of all strings
of the form vw, for any string w over the alphabet
of the model. This problem has been studied by
Jelinek and Lafferty (1991) and by Stolcke (1995).
Prefix probabilities can be used to compute probabil-
ity distributions for the next word or part-of-speech.
This has applications in incremental processing of
text or speech from left to right; see again (Jelinek
and Lafferty, 1991). Prefix probabilities can also be
exploited in speech understanding systems to score
partial hypotheses in beam search (Corazza et al,
1991).
This paper investigates the problem of computing
prefix probabilities for PSCFGs. In this context, a
pair of strings v1 and v2 is given as input, and we are
asked to compute the probability that any string in
the source language starting with prefix v1 is trans-
lated into any string in the target language starting
with prefix v2. This probability is more precisely
defined as the sum of the probabilities of translation
pairs of the form [v1w1, v2w2], for any strings w1
and w2.
A special case of prefix probability for PSCFGs
is the right prefix probability. This is defined as the
probability that some (complete) input string w in
the source language is translated into a string in the
target language starting with an input prefix v.
460
Prefix probabilities and right prefix probabilities
for PSCFGs can be exploited to compute probabil-
ity distributions for the next word or part-of-speech
in left-to-right incremental translation, essentially in
the same way as described by Jelinek and Lafferty
(1991) for probabilistic context-free grammars, as
discussed later in this paper.
Our solution to the problem of computing prefix
probabilities is formulated in quite different terms
from the solutions by Jelinek and Lafferty (1991)
and by Stolcke (1995) for probabilistic context-free
grammars. In this paper we reduce the computation
of prefix probabilities for PSCFGs to the computa-
tion of inside probabilities under the same model.
Computation of inside probabilities for PSCFGs is
a well-known problem that can be solved using off-
the-shelf algorithms that extend basic parsing algo-
rithms. Our reduction is a novel grammar trans-
formation, and the proof of correctness proceeds
by fairly conventional techniques from formal lan-
guage theory, relying on the correctness of standard
methods for the computation of inside probabilities
for PSCFG. This contrasts with the techniques pro-
posed by Jelinek and Lafferty (1991) and by Stolcke
(1995), which are extensions of parsing algorithms
for probabilistic context-free grammars, and require
considerably more involved proofs of correctness.
Our method for computing the prefix probabili-
ties for PSCFGs runs in exponential time, since that
is the running time of existing methods for comput-
ing the inside probabilities for PSCFGs. It is un-
likely this can be improved, because the recogni-
tion problem for PSCFG is NP-complete, as estab-
lished by Satta and Peserico (2005), and there is a
straightforward reduction from the recognition prob-
lem for PSCFGs to the problem of computing the
prefix probabilities for PSCFGs.
2 Definitions
In this section we introduce basic definitions re-
lated to synchronous context-free grammars and
their probabilistic extension; our notation follows
Satta and Peserico (2005).
Let N and ? be sets of nonterminal and terminal
symbols, respectively. In what follows we need to
represent bijections between the occurrences of non-
terminals in two strings over N ??. This is realized
by annotating nonterminals with indices from an in-
finite set. We define I(N) = {A t | A ? N, t ?
N} and VI = I(N) ? ?. For a string ? ? V ?I , we
write index(?) to denote the set of all indices that
appear in symbols in ?.
Two strings ?1, ?2 ? V ?I are synchronous if each
index from N occurs at most once in ?1 and at most
once in ?2, and index(?1) = index(?2). Therefore
?1, ?2 have the general form:
?1 = u10A
t1
11 u11A
t2
12 u12 ? ? ?u1r?1A
tr
1r u1r
?2 = u20A
tpi(1)
21 u21A
tpi(2)
22 u22 ? ? ?u2r?1A
tpi(r)
2r u2r
where r ? 0, u1i, u2i ? ??, A
ti
1i , A
tpi(i)
2i ? I(N),
ti 6= tj for i 6= j, and pi is a permutation of the set
{1, . . . , r}.
A synchronous context-free grammar (SCFG)
is a tuple G = (N,?,P, S), where N and ? are fi-
nite, disjoint sets of nonterminal and terminal sym-
bols, respectively, S ? N is the start symbol and
P is a finite set of synchronous rules. Each syn-
chronous rule has the form s : [A1 ? ?1, A2 ?
?2], where A1, A2 ? N and where ?1, ?2 ? V ?I are
synchronous strings. The symbol s is the label of
the rule, and each rule is uniquely identified by its
label. For technical reasons, we allow the existence
of multiple rules that are identical apart from their
labels. We refer to A1 ? ?1 and A2 ? ?2, respec-
tively, as the left and right components of rule s.
Example 1 The following synchronous rules im-
plicitly define a SCFG:
s1 : [S ? A
1 B 2 , S ? B 2 A 1 ]
s2 : [A ? aA
1 b, A ? bA 1 a]
s3 : [A ? ab, A ? ba]
s4 : [B ? cB
1 d, B ? dB 1 c]
s5 : [B ? cd, B ? dc] 2
In each step of the derivation process of a SCFG
G, two nonterminals with the same index in a pair of
synchronous strings are rewritten by a synchronous
rule. This is done in such a way that the result is once
more a pair of synchronous strings. An auxiliary
notion is that of reindexing, which is an injective
function f fromN toN. We extend f to VI by letting
f(A t ) = A f(t) for A t ? I(N) and f(a) = a
for a ? ?. We also extend f to strings in V ?I by
461
letting f(?) = ? and f(X?) = f(X)f(?), for each
X ? VI and ? ? V ?I .
Let ?1, ?2 be synchronous strings in V ?I . The de-
rive relation [?1, ?2] ?G [?1, ?2] holds whenever
there exist an index t in index(?1) = index(?2), a
synchronous rule s : [A1 ? ?1, A2 ? ?2] in P
and some reindexing f such that:
(i) index(f(?1)) ? (index(?1) \ {t}) = ?;
(ii) ?1 = ??1A
t
1 ?
??
1 , ?2 = ?
?
2A
t
2 ?
??
2 ; and
(iii) ?1 = ??1f(?1)?
??
1 , ?2 = ?
?
2f(?2)?
??
2 .
We also write [?1, ?2] ?sG [?1, ?2] to explicitly
indicate that the derive relation holds through rule s.
Note that ?1, ?2 above are guaranteed to be syn-
chronous strings, because ?1 and ?2 are syn-
chronous strings and because of (i) above. Note
also that, for a given pair [?1, ?2] of synchronous
strings, an index t and a rule s, there may be in-
finitely many choices of reindexing f such that the
above constraints are satisfied. In this paper we will
not further specify the choice of f .
We say the pair [A1, A2] of nonterminals is linked
(in G) if there is a rule of the form s : [A1 ?
?1, A2 ? ?2]. The set of linked nonterminal pairs
is denoted by N [2].
A derivation is a sequence ? = s1s2 ? ? ? sd of syn-
chronous rules si ? P with d ? 0 (? = ? for
d = 0) such that [?1i?1, ?2i?1] ?
si
G [?1i, ?2i] for
every i with 1 ? i ? d and synchronous strings
[?1i, ?2i] with 0 ? i ? d. Throughout this paper,
we always implicitly assume some canonical form
for derivations in G, by demanding for instance that
each step rewrites a pair of nonterminal occurrences
of which the first is leftmost in the left component.
When we want to focus on the specific synchronous
strings being derived, we also write derivations in
the form [?10, ?20] ??G [?1d, ?2d], and we write
[?10, ?20] ??G [?1d, ?2d] when ? is not further
specified. The translation generated by a SCFG G
is defined as:
T (G) = {[w1, w2] | [S
1 , S 1 ] ??G [w1, w2],
w1, w2 ? ?
?}
For w1, w2 ? ??, we write D(G, [w1, w2]) to de-
note the set of all (canonical) derivations ? such that
[S 1 , S 1 ] ??G [w1, w2].
Analogously to standard terminology for context-
free grammars, we call a SCFG reduced if ev-
ery rule occurs in at least one derivation ? ?
D(G, [w1, w2]), for some w1, w2 ? ??. We as-
sume without loss of generality that the start sym-
bol S does not occur in the right-hand side of either
component of any rule.
Example 2 Consider the SCFG G from example 1.
The following is a canonical derivation in G, since it
is always the leftmost nonterminal occurrence in the
left component that is involved in a derivation step:
[S 1 , S 1 ] ?G [A
1 B 2 , B 2 A 1 ]
?G [aA
3 bB 2 , B 2 bA 3 a]
?G [aaA
4 bbB 2 , B 2 bbA 4 aa]
?G [aaabbbB
2 , B 2 bbbaaa]
?G [aaabbbcB
5 d, dB 5 cbbbaaa]
?G [aaabbbccdd, ddccbbbaaa]
It is not difficult to see that the generated translation
is T (G) = {[apbpcqdq, dqcqbpap] | p, q ? 1}. 2
The size of a synchronous rule s : [A1 ? ?1,
A2 ? ?2], is defined as |s| = |A1?1A2?2|. The
size of G is defined as |G| =
?
s?P |s|.
A probabilistic SCFG (PSCFG) is a pair G =
(G, pG) where G = (N,?,P, S) is a SCFG and pG
is a function from P to real numbers in [0, 1]. We
say that G is proper if for each pair [A1, A2] ? N [2]
we have:
?
s:[A1??1, A2??2]
pG(s) = 1
Intuitively, properness ensures that where a pair
of nonterminals in two synchronous strings can be
rewritten, there is a probability distribution over the
applicable rules.
For a (canonical) derivation ? = s1s2 ? ? ? sd, we
define pG(?) =
?d
i=1 pG(si). For w1, w2 ? ?
?,
we also define:
pG([w1, w2]) =
?
??D(G,[w1,w2])
pG(?) (1)
We say a PSCFG is consistent if pG defines a prob-
ability distribution over the translation, or formally:
?
w1,w2
pG([w1, w2]) = 1
462
If the grammar is reduced, proper and consistent,
then also:
?
w1,w2???, ??P ?
s.t. [A 11 , A
1
2 ]?
?
G[w1, w2]
pG(?) = 1
for every pair [A1, A2] ? N [2]. The proof is identi-
cal to that of the corresponding fact for probabilistic
context-free grammars.
3 Effective PSCFG parsing
If w = a1 ? ? ? an then the expression w[i, j], with
0 ? i ? j ? n, denotes the substring ai+1 ? ? ? aj (if
i = j then w[i, j] = ?). In this section, we assume
the input is the pair [w1, w2] of terminal strings.
The task of a recognizer for SCFG G is to decide
whether [w1, w2] ? T (G).
We present a general algorithm for solving the
above problem in terms of the specification of a de-
duction system, following Shieber et al (1995). The
items that are constructed by the system have the
form [m1, A1,m?1; m2, A2,m
?
2], where [A1, A2] ?
N [2] and where m1, m?1, m2, m
?
2 are non-negative
integers such that 0 ? m1 ? m?1 ? |w1| and
0 ? m2 ? m?2 ? |w2|. Such an item can be de-
rived by the deduction system if and only if:
[A 11 , A
1
2 ] ?
?
G [w1[m1,m
?
1], w2[m2,m
?
2]]
The deduction system has one inference rule,
shown in figure 1. One of its side conditions has
a synchronous rule in P of the form:
s : [A1 ? u10A
t1
11 u11 ? ? ?u1r?1A
tr
1r u1r,
A2 ? u20A
tpi(1)
21 u21 ? ? ?u2r?1A
tpi(r)
2r u2r] (2)
Observe that, in the right-hand side of the two rule
components above, nonterminals A1i and A2pi?1(i),
1 ? i ? r, have both the same index. More pre-
cisely, A1i has index ti and A2pi?1(i) has index ti?
with i? = pi(pi?1(i)) = i. Thus the nonterminals in
each antecedent item in figure 1 form a linked pair.
We now turn to a computational analysis of the
above algorithm. In the inference rule in figure 1
there are 2(r + 1) variables that can be bound to
positions in w1, and as many that can be bound to
positions in w2. However, the side conditions imply
m?ij = mij + |uij |, for i ? {1, 2} and 0 ? j ? r,
and therefore the number of free variables is only
r + 1 for each component. By standard complex-
ity analysis of deduction systems, for example fol-
lowing McAllester (2002), the time complexity of
a straightforward implementation of the recogni-
tion algorithm is O(|P | ? |w1|
rmax+1 ? |w2|
rmax+1),
where rmax is the maximum number of right-hand
side nonterminals in either component of a syn-
chronous rule. The algorithm therefore runs in ex-
ponential time, when the grammar G is considered
as part of the input. Such computational behavior
seems unavoidable, since the recognition problem
for SCFG is NP-complete, as reported by Satta and
Peserico (2005). See also Gildea and Stefankovic
(2007) and Hopkins and Langmead (2010) for fur-
ther analysis of the upper bound above.
The recognition algorithm above can easily be
turned into a parsing algorithm by letting an imple-
mentation keep track of which items were derived
from which other items, as instantiations of the con-
sequent and the antecedents, respectively, of the in-
ference rule in figure 1.
A probabilistic parsing algorithm that computes
pG([w1, w2]), defined in (1), can also be obtained
from the recognition algorithm above, by associat-
ing each item with a probability. To explain the ba-
sic idea, let us first assume that each item can be
inferred in finitely many ways by the inference rule
in figure 1. Each instantiation of the inference rule
should be associated with a term that is computed
by multiplying the probability of the involved rule
s and the product of all probabilities previously as-
sociated with the instantiations of the antecedents.
The probability associated with an item is then
computed as the sum of each term resulting from
some instantiation of an inference rule deriving that
item. This is a generalization to PSCFG of the in-
side algorithm defined for probabilistic context-free
grammars (Manning and Schu?tze, 1999), and we
can show that the probability associated with item
[0, S, |w1| ; 0, S, |w2|] provides the desired value
pG([w1, w2]). We refer to the procedure sketched
above as the inside algorithm for PSCFGs.
However, this simple procedure fails if there are
cyclic dependencies, whereby the derivation of an
item involves a proper subderivation of the same
item. Cyclic dependencies can be excluded if it can
463
[m?10, A11,m11; m
?
2pi?1(1)?1, A2pi?1(1),m2pi?1(1)]
...
[m?1r?1, A1r,m1r; m
?
2pi?1(r)?1, A2pi?1(r),m2pi?1(r)]
[m10, A1,m?1r; m20, A2,m
?
2r]
?
???????
???????
s:[A1 ? u10A
t1
11 u11 ? ? ?u1r?1A
tr
1r u1r,
A2 ? u20A
tpi(1)
21 u21 ? ? ?u2r?1A
tpi(r)
2r u2r] ? P,
w1[m10,m?10] = u10,
...
w1[m1r,m?1r] = u1r,
w2[m20,m?20] = u20,
...
w2[m2r,m?2r] = u2r
Figure 1: SCFG recognition, by a deduction system consisting of a single inference rule.
be guaranteed that, in figure 1, m?1r ?m10 is greater
than m1j ? m?1j?1 for each j (1 ? j ? r), or
m?2r ? m20 is greater than m2j ? m
?
2j?1 for each
j (1 ? j ? r).
Consider again a synchronous rule s of the form
in (2). We say s is an epsilon rule if r = 0 and
u10 = u20 = . We say s is a unit rule if r = 1
and u10 = u11 = u20 = u21 = . Similarly to
context-free grammars, absence of epsilon rules and
unit rules guarantees that there are no cyclic depen-
dencies between items and in this case the inside al-
gorithm correctly computes pG([w1, w2]).
Epsilon rules can be eliminated from PSCFGs
by a grammar transformation that is very similar
to the transformation eliminating epsilon rules from
a probabilistic context-free grammar (Abney et al,
1999). This is sketched in what follows. We first
compute the set of all nullable linked pairs of non-
terminals of the underlying SCFG, that is, the set of
all [A1, A2] ? N [2] such that [A
1
1 , A
1
2 ] ?
?
G [?, ?].
This can be done in linear time O(|G|) using essen-
tially the same algorithm that identifies nullable non-
terminals in a context-free grammar, as presented for
instance by Sippu and Soisalon-Soininen (1988).
Next, we identify all occurrences of nullable pairs
[A1, A2] in the right-hand side components of a rule
s, such that A1 and A2 have the same index. For
every possible choice of a subset U of these occur-
rences, we add to our grammar a new rule sU con-
structed by omitting all of the nullable occurrences
in U . The probability of sU is computed as the prob-
ability of s multiplied by terms of the form:
?
? s.t. [A 11 ,A
1
2 ]?
?
G[?, ?]
pG(?) (3)
for every pair [A1, A2] in U . After adding these extra
rules, which in effect circumvents the use of epsilon-
generating subderivations, we can safely remove all
epsilon rules, with the only exception of a possible
rule of the form [S ? , S ? ]. The translation and
the associated probability distribution in the result-
ing grammar will be the same as those in the source
grammar.
One problem with the above construction is that
we have to create new synchronous rules sU for each
possible choice of subset U . In the worst case, this
may result in an exponential blow-up of the source
grammar. In the case of context-free grammars, this
is usually circumvented by casting the rules in bi-
nary form prior to epsilon rule elimination. How-
ever, this is not possible in our case, since SCFGs
do not allow normal forms with a constant bound
on the length of the right-hand side of each compo-
nent. This follows from a result due to Aho and Ull-
man (1969) for a formalism called syntax directed
translation schemata, which is a syntactic variant of
SCFGs.
An additional complication with our construction
is that finding any of the values in (3) may involve
solving a system of non-linear equations, similarly
to the case of probabilistic context-free grammars;
see again Abney et al (1999), and Stolcke (1995).
Approximate solution of such systems might take
exponential time, as pointed out by Kiefer et al
(2007).
Notwithstanding the worst cases mentioned
above, there is a special case that can be easily dealt
with. Assume that, for each nullable pair [A1, A2] in
G we have that [A 11 , A
1
2 ] ?
?
G [w1, w2] does not
hold for any w1 and w2 with w1 6= ? or w2 6= ?.
Then each of the values in (3) is guaranteed to be 1,
and furthermore we can remove the instances of the
nullable pairs in the source rule s all at the same
time. This means that the overall construction of
464
elimination of nullable rules from G can be imple-
mented in linear time |G|. It is this special case that
we will encounter in section 4.
After elimination of epsilon rules, one can elimi-
nate unit rules. We define Cunit([A1, A2], [B1, B2])
as the sum of the probabilities of all derivations de-
riving [B1, B2] from [A1, A2] with arbitrary indices,
or more precisely:
?
??P ? s.t. ?t?N,
[A 11 , A
1
2 ]?
?
G[B
t
1 , B
t
2 ]
pG(?)
Note that [A1, A2] may be equal to [B1, B2] and ?
may be ?, in which case Cunit([A1, A2], [B1, B2]) is
at least 1, but it may be larger if there are unit rules.
Therefore Cunit([A1, A2], [B1, B2]) should not be
seen as a probability.
Consider a pair [A1, A2] ? N [2] and let al unit
rules with left-hand sides A1 and A2 be:
s1 : [A1, A2] ? [A
t1
11 , A
t1
21 ]
...
sm : [A1, A2] ? [A
tm
1m , A
tm
2m ]
The values ofCunit(?, ?) are related by the following:
Cunit([A1, A2], [B1, B2]) =
?([A1, A2] = [B1, B2]) +
?
i
pG(si) ? C
unit([A1i, A2i], [B1, B2])
where ?([A1, A2] = [B1, B2]) is defined to be 1 if
[A1, A2] = [B1, B2] and 0 otherwise. This forms a
system of linear equations in the unknown variables
Cunit(?, ?). Such a system can be solved in polyno-
mial time in the number of variables, for example
using Gaussian elimination.
The elimination of unit rules starts with adding
a rule s? : [A1 ? ?1, A2 ? ?2] for each non-
unit rule s : [B1 ? ?1, B2 ? ?2] and pair
[A1, A2] such that Cunit([A1, A2], [B1, B2]) > 0.
We assign to the new rule s? the probability pG(s) ?
Cunit([A1, A2], [B1, B2]). The unit rules can now
be removed from the grammar. Again, in the re-
sulting grammar the translation and the associated
probability distribution will be the same as those in
the source grammar. The new grammar has size
O(|G|2), where G is the input grammar. The time
complexity is dominated by the computation of the
solution of the linear system of equations. This com-
putation takes cubic time in the number of variables.
The number of variables in this case is O(|G|2),
which makes the running time O(|G|6).
4 Prefix probabilities
The joint prefix probability pprefixG ([v1, v2]) of a
pair [v1, v2] of terminal strings is the sum of the
probabilities of all pairs of strings that have v1 and
v2, respectively, as their prefixes. Formally:
pprefixG ([v1, v2]) =
?
w1,w2???
pG([v1w1, v2w2])
At first sight, it is not clear this quantity can be ef-
fectively computed, as it involves a sum over in-
finitely many choices of w1 and w2. However, anal-
ogously to the case of context-free prefix probabili-
ties (Jelinek and Lafferty, 1991), we can isolate two
parts in the computation. One part involves infinite
sums, which are independent of the input strings v1
and v2, and can be precomputed by solving a sys-
tem of linear equations. The second part does rely
on v1 and v2, and involves the actual evaluation of
pprefixG ([v1, v2]). This second part can be realized
effectively, on the basis of the precomputed values
from the first part.
In order to keep the presentation simple, and
to allow for simple proofs of correctness, we
solve the problem in a modular fashion. First,
we present a transformation from a PSCFG
G = (G, pG), with G = (N,?,P, S), to a
PSCFG Gprefix = (Gprefix, pGprefix), with Gprefix =
(Nprefix, ?, Pprefix, S?). The latter grammar derives
all possible pairs [v1, v2] such that [v1w1, v2w2] can
be derived from G, for some w1 and w2. Moreover,
pGprefix([v1, v2]) = p
prefix
G ([v1, v2]), as will be veri-
fied later.
Computing pGprefix([v1, v2]) directly using a
generic probabilistic parsing algorithm for PSCFGs
is difficult, due to the presence of epsilon rules and
unit rules. The next step will be to transform Gprefix
into a third grammar G?prefix by eliminating epsilon
rules and unit rules from the underlying SCFG,
and preserving the probability distribution over pairs
of strings. Using G?prefix one can then effectively
465
apply generic probabilistic parsing algorithms for
PSCFGs, such as the inside algorithm discussed in
section 3, in order to compute the desired prefix
probabilities for the source PSCFG G.
For each nonterminal A in the source SCFG G,
the grammar Gprefix contains three nonterminals,
namely A itself, A? and A?. The meaning of A re-
mains unchanged, whereas A? is intended to gen-
erate a string that is a suffix of a known prefix v1 or
v2. Nonterminals A? generate only the empty string,
and are used to simulate the generation by G of in-
fixes of the unknown suffix w1 or w2. The two left-
hand sides of a synchronous rule in Gprefix can con-
tain different combinations of nonterminals of the
forms A, A?, or A?. The start symbol of Gprefix is
S?. The structure of the rules from the source gram-
mar is largely retained, except that some terminal
symbols are omitted in order to obtain the intended
interpretation of A? and A?.
In more detail, let us consider a synchronous rule
s : [A1 ? ?1, A2 ? ?2] from the source gram-
mar, where for i ? {1, 2} we have:
?i = ui0A
ti1
i1 ui1 ? ? ?uir?1A
tir
ir uir
The transformed grammar then contains a large
number of rules, each of which is of the form s? :
[B1 ? ?1, B2 ? ?2], where Bi ? ?i is of
one of three forms, namely Ai ? ?i, A
?
i ? ?
?
i
or A?i ? ?
?
i , where ?
?
i and ?
?
i are explained below.
The choices for i = 1 and for i = 2 are independent,
so that we can have 3 ? 3 = 9 kinds of synchronous
rules, to be further subdivided in what follows. A
unique label s? is produced for each new rule, and
the probability of each new rule equals that of s.
The right-hand side ??i is constructed by omitting
all terminals and propagating downwards the ? su-
perscript, resulting in:
??i = A
? ti1
i1 ? ? ?A
? tir
ir
It is more difficult to define ??i . In fact, there can
be a number of choices for ??i and, for each choice,
the transformed grammar contains an instance of the
synchronous rule s? : [B1 ? ?1, B2 ? ?2] as de-
fined above. The reason why different choices need
to be considered is because the boundary between
the known prefix vi and the unknown suffix wi can
occur at different positions, either within a terminal
string uij or else further down in a subderivation in-
volving Aij . In the first case, we have for some j
(0 ? j ? r):
??i = ui0A
ti1
i1 ui1A
ti2
i2 ? ? ?
uij?1A
tij
ij u
?
ijA
? tij+1
ij+1 A
? tij+2
ij+2 ? ? ?A
? tir
ir
where u?ij is a choice of a prefix of uij . In words,
the known prefix ends after u?ij and, thereafter, no
more terminals are generated. We demand that u?ij
must not be the empty string, unless Ai = S and
j = 0. The reason for this restriction is that we want
to avoid an overlap with the second case. In this
second case, we have for some j (1 ? j ? r):
??i = ui0A
ti1
i1 ui1A
ti2
i2 ? ? ?
uij?1A
? tij
ij A
? tij+1
ij+1 A
? tij+2
ij+2 ? ? ?A
? tir
ir
Here the known prefix of the input ends within a sub-
derivation involving Aij , and further to the right no
more terminals are generated.
Example 3 Consider the synchronous rule s :
[A ? aB 1 bc C 2 d,D ? ef E 2 F 1 ]. The first
component of a synchronous rule derived from this
can be one of the following eight:
A? ? B? 1 C? 2
A? ? aB? 1 C? 2
A? ? aB? 1 C? 2
A? ? aB 1 b C? 2
A? ? aB 1 bc C? 2
A? ? aB 1 bc C? 2
A? ? aB 1 bc C 2 d
A ? aB 1 bc C 2 d
The second component can be one of the following
six:
D? ? E? 2 F ? 1
D? ? eE? 2 F ? 1
D? ? ef E? 2 F ? 1
D? ? ef E? 2 F ? 1
D? ? ef E 2 F ? 1
D ? ef E 2 F 1
466
In total, the transformed grammar will contain 8 ?
6 = 48 synchronous rules derived from s. 2
For each synchronous rule s, the above gram-
mar transformation produces O(|s|) left rule com-
ponents and as many right rule components. This
means the number of new synchronous rules is
O(|s|2), and the size of each such rule is O(|s|). If
we sum O(|s|3) for every rule s we obtain a time
and space complexity of O(|G|3).
We now investigate formal properties of our
grammar transformation, in order to relate it to pre-
fix probabilities. We define the relation ` between P
and Pprefix such that s ` s? if and only if s? was ob-
tained from s by the transformation described above.
This is extended in a natural way to derivations, such
that s1 ? ? ? sd ` s?1 ? ? ? s
?
d? if and only if d = d
? and
si ` s?i for each i (1 ? i ? d).
The formal relation between G and Gprefix is re-
vealed by the following two lemmas.
Lemma 1 For each v1, v2, w1, w2 ? ?? and
? ? P ? such that [S, S] ??G [v1w1, v2w2], there
is a unique ?? ? P ?prefix such that [S
?, S?] ??
?
Gprefix
[v1, v2] and ? ` ??. 2
Lemma 2 For each v1, v2 ? ?? and derivation
?? ? P ?prefix such that [S
?, S?] ??
?
Gprefix
[v1, v2],
there is a unique ? ? P ? and unique w1, w2 ? ??
such that [S, S] ??G [v1w1, v2w2] and ? ` ?
?. 2
The only non-trivial issue in the proof of Lemma 1
is the uniqueness of ??. This follows from the obser-
vation that the length of v1 in v1w1 uniquely deter-
mines how occurrences of left components of rules
in P found in ? are mapped to occurrences of left
components of rules in Pprefix found in ??. The same
applies to the length of v2 in v2w2 and the right com-
ponents.
Lemma 2 is easy to prove as the structure of the
transformation ensures that the terminals that are in
rules from P but not in the corresponding rules from
Pprefix occur at the end of a string v1 (and v2) to form
the longer string v1w1 (and v2w2, respectively).
The transformation also ensures that s ` s? im-
plies pG(s) = pGprefix(s
?). Therefore ? ` ?? implies
pG(?) = pGprefix(?
?). By this and Lemmas 1 and 2
we may conclude:
Theorem 1 pGprefix([v1, v2]) = p
prefix
G ([v1, v2]). 2
Because of the introduction of rules with left-hand
sides of the formA? in both the left and right compo-
nents of synchronous rules, it is not straightforward
to do effective probabilistic parsing with the gram-
mar Gprefix. We can however apply the transforma-
tions from section 3 to eliminate epsilon rules and
thereafter eliminate unit rules, in a way that leaves
the derived string pairs and their probabilities un-
changed.
The simplest case is when the source grammar G
is reduced, proper and consistent, and has no epsilon
rules. The only nullable pairs of nonterminals in
Gprefix will then be of the form [A?1, A
?
2]. Consider
such a pair [A?1, A
?
2]. Because of reduction, proper-
ness and consistency of G we have:
?
w1,w2???, ??P ? s.t.
[A 11 , A
1
2 ]?
?
G[w1, w2]
pG(?) = 1
Because of the structure of the grammar transforma-
tion by which Gprefix was obtained from G, we also
have:
?
??P ? s.t.
[A? 11 , A
? 1
2 ]?
?
Gprefix
[?, ?]
pGprefix(?) = 1
Therefore pairs of occurrences of A?1 and A
?
2 with
the same index in synchronous rules of Gprefix
can be systematically removed without affecting the
probability of the resulting rule, as outlined in sec-
tion 3. Thereafter, unit rules can be removed to allow
parsing by the inside algorithm for PSCFGs.
Following the computational analyses for all of
the constructions presented in section 3, and for the
grammar transformation discussed in this section,
we can conclude that the running time of the pro-
posed algorithm for the computation of prefix prob-
abilities is dominated by the running time of the in-
side algorithm, which in the worst case is exponen-
tial in |G|. This result is not unexpected, as already
pointed out in the introduction, since the recogni-
tion problem for PSCFGs is NP-complete, as estab-
lished by Satta and Peserico (2005), and there is a
straightforward reduction from the recognition prob-
lem for PSCFGs to the problem of computing the
prefix probabilities for PSCFGs.
467
One should add that, in real world machine trans-
lation applications, it has been observed that recog-
nition (and computation of inside probabilities) for
SCFGs can typically be carried out in low-degree
polynomial time, and the worst cases mentioned
above are not observed with real data. Further dis-
cussion on this issue is due to Zhang et al (2006).
5 Discussion
We have shown that the computation of joint prefix
probabilities for PSCFGs can be reduced to the com-
putation of inside probabilities for the same model.
Our reduction relies on a novel grammar transfor-
mation, followed by elimination of epsilon rules and
unit rules.
Next to the joint prefix probability, we can also
consider the right prefix probability, which is de-
fined by:
pr?prefixG ([v1, v2]) =
?
w
pG([v1, v2w])
In words, the entire left string is given, along with a
prefix of the right string, and the task is to sum the
probabilities of all string pairs for different suffixes
following the given right prefix. This can be com-
puted as a special case of the joint prefix probability.
Concretely, one can extend the input and the gram-
mar by introducing an end-of-sentence marker $.
Let G? be the underlying SCFG grammar after the
extension. Then:
pr?prefixG ([v1, v2]) = p
prefix
G? ([v1$, v2])
Prefix probabilities and right prefix probabilities
for PSCFGs can be exploited to compute probability
distributions for the next word or part-of-speech in
left-to-right incremental translation of speech, or al-
ternatively as a predictive tool in applications of in-
teractive machine translation, of the kind described
by Foster et al (2002). We provide some technical
details here, generalizing to PSCFGs the approach
by Jelinek and Lafferty (1991).
Let G = (G, pG) be a PSCFG, with ? the alpha-
bet of terminal symbols. We are interested in the
probability that the next terminal in the target trans-
lation is a ? ?, after having processed a prefix v1 of
the source sentence and having produced a prefix v2
of the target translation. This can be computed as:
pr?wordG (a | [v1, v2]) =
pprefixG ([v1, v2a])
pprefixG ([v1, v2])
Two considerations are relevant when applying
the above formula in practice. First, the computa-
tion of pprefixG ([v1, v2a]) need not be computed from
scratch if pprefixG ([v1, v2]) has been computed al-
ready. Because of the tabular nature of the inside al-
gorithm, one can extend the table for pprefixG ([v1, v2])
by adding new entries to obtain the table for
pprefixG ([v1, v2a]). The same holds for the compu-
tation of pprefixG ([v1b, v2]).
Secondly, the computation of pprefixG ([v1, v2a]) for
all possible a ? ? may be impractical. However,
one may also compute the probability that the next
part-of-speech in the target translation isA. This can
be realised by adding a rule s? : [B ? b, A ? cA]
for each rule s : [B ? b, A ? a] from the source
grammar, where A is a nonterminal representing a
part-of-speech and cA is a (pre-)terminal specific to
A. The probability of s? is the same as that of s. If
G? is the underlying SCFG after adding such rules,
then the required value is pprefixG? ([v1, v2 cA]).
One variant of the definitions presented in this pa-
per is the notion of infix probability, which is use-
ful in island-driven speech translation. Here we are
interested in the probability that any string in the
source language with infix v1 is translated into any
string in the target language with infix v2. However,
just as infix probabilities are difficult to compute
for probabilistic context-free grammars (Corazza et
al., 1991; Nederhof and Satta, 2008) so (joint) infix
probabilities are difficult to compute for PSCFGs.
The problem lies in the possibility that a given in-
fix may occur more than once in a string in the lan-
guage. The computation of infix probabilities can
be reduced to that of solving non-linear systems of
equations, which can be approximated using for in-
stance Newton?s algorithm. However, such a system
of equations is built from the input strings, which en-
tails that the computational effort of solving the sys-
tem primarily affects parse time rather than parser-
generation time.
468
References
S. Abney, D. McAllester, and F. Pereira. 1999. Relating
probabilistic grammars and automata. In 37th Annual
Meeting of the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 542?549,
Maryland, USA, June.
A.V. Aho and J.D. Ullman. 1969. Syntax directed trans-
lations and the pushdown assembler. Journal of Com-
puter and System Sciences, 3:37?56.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131?160.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
A. Corazza, R. De Mori, R. Gretter, and G. Satta.
1991. Computation of probabilities for an island-
driven parser. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 13(9):936?950.
G. Foster, P. Langlais, and G. Lapalme. 2002. User-
friendly text prediction for translators. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 148?155, University of Pennsylvania,
Philadelphia, PA, USA, July.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In HLT-NAACL 2004,
Proceedings of the Main Conference, Boston, Mas-
sachusetts, USA, May.
D. Gildea and D. Stefankovic. 2007. Worst-case syn-
chronous grammar rules. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Proceedings of the Main Conference, pages 147?
154, Rochester, New York, USA, April.
M. Hopkins and G. Langmead. 2010. SCFG decod-
ing without binarization. In Conference on Empirical
Methods in Natural Language Processing, Proceed-
ings of the Conference, pages 646?655, October.
F. Jelinek and J.D. Lafferty. 1991. Computation of the
probability of initial substring generation by stochas-
tic context-free grammars. Computational Linguistics,
17(3):315?323.
S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On the
convergence of Newton?s method for monotone sys-
tems of polynomial equations. In Proceedings of the
39th ACM Symposium on Theory of Computing, pages
217?266.
C.D. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press.
D. McAllester. 2002. On the complexity analysis of
static analyses. Journal of the ACM, 49(4):512?537.
M.-J. Nederhof and G. Satta. 2008. Computing parti-
tion functions of PCFGs. Research on Language and
Computation, 6(2):139?162.
G. Satta and E. Peserico. 2005. Some computational
complexity results for synchronous context-free gram-
mars. In Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing, pages 803?810.
S.M. Shieber, Y. Schabes, and F.C.N. Pereira. 1995.
Principles and implementation of deductive parsing.
Journal of Logic Programming, 24:3?36.
S. Sippu and E. Soisalon-Soininen. 1988. Parsing
Theory, Vol. I: Languages and Parsing, volume 15
of EATCS Monographs on Theoretical Computer Sci-
ence. Springer-Verlag.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):167?201.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Confer-
ence, pages 256?263, New York, USA, June.
469
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 673?682,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Dynamic Programming Algorithms
for Transition-Based Dependency Parsers
Marco Kuhlmann
Dept. of Linguistics and Philology
Uppsala University, Sweden
marco.kuhlmann@lingfil.uu.se
Carlos G?mez-Rodr?guez
Departamento de Computaci?n
Universidade da Coru?a, Spain
cgomezr@udc.es
Giorgio Satta
Dept. of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
We develop a general dynamic programming
technique for the tabulation of transition-based
dependency parsers, and apply it to obtain
novel, polynomial-time algorithms for parsing
with the arc-standard and arc-eager models. We
also show how to reverse our technique to ob-
tain new transition-based dependency parsers
from existing tabular methods. Additionally,
we provide a detailed discussion of the con-
ditions under which the feature models com-
monly used in transition-based parsing can be
integrated into our algorithms.
1 Introduction
Dynamic programming algorithms, also known as
tabular or chart-based algorithms, are at the core of
many applications in natural language processing.
When applied to formalisms such as context-free
grammar, they provide polynomial-time parsing al-
gorithms and polynomial-space representations of
the resulting parse forests, even in cases where the
size of the search space is exponential in the length
of the input string. In combination with appropri-
ate semirings, these packed representations can be
exploited to compute many values of interest for ma-
chine learning, such as best parses and feature expec-
tations (Goodman, 1999; Li and Eisner, 2009).
In this paper, we follow the line of investigation
started by Huang and Sagae (2010) and apply dy-
namic programming to (projective) transition-based
dependency parsing (Nivre, 2008). The basic idea,
originally developed in the context of push-down
automata (Lang, 1974; Tomita, 1986; Billot and
Lang, 1989), is that while the number of computa-
tions of a transition-based parser may be exponential
in the length of the input string, several portions of
these computations, when appropriately represented,
can be shared. This can be effectively implemented
through dynamic programming, resulting in a packed
representation of the set of all computations.
The contributions of this paper can be summarized
as follows. We provide (declarative specifications of)
novel, polynomial-time algorithms for two widely-
used transition-based parsing models: arc-standard
(Nivre, 2004; Huang and Sagae, 2010) and arc-eager
(Nivre, 2003; Zhang and Clark, 2008). Our algorithm
for the arc-eager model is the first tabular algorithm
for this model that runs in polynomial time. Both
algorithms are derived using the same general tech-
nique; in fact, we show that this technique is applica-
ble to all transition-parsing models whose transitions
can be classified into ?shift? and ?reduce? transitions.
We also show how to reverse the tabulation to de-
rive a new transition system from an existing tabular
algorithm for dependency parsing, originally devel-
oped by G?mez-Rodr?guez et al (2008). Finally, we
discuss in detail the role of feature information in
our algorithms, and in particular the conditions under
which the feature models traditionally used in transi-
tion-based dependency parsing can be integrated into
our framework.
While our general approach is the same as the one
of Huang and Sagae (2010), we depart from their
framework by not representing the computations of
a parser as a graph-structured stack in the sense of
Tomita (1986). We instead simulate computations
as in Lang (1974), which results in simpler algo-
rithm specifications, and also reveals deep similari-
ties between transition-based systems for dependency
parsing and existing tabular methods for lexicalized
context-free grammars.
673
2 Transition-Based Dependency Parsing
We start by briefly introducing the framework of
transition-based dependency parsing; for details, we
refer to Nivre (2008).
2.1 Dependency Graphs
Let w D w0   wn 1 be a string over some fixed
alphabet, where n  1 and w0 is the special token
root. A dependency graph for w is a directed graph
G D .Vw ; A/, where Vw D f0; : : : ; n   1g is the set
of nodes, and A  Vw  Vw is the set of arcs. Each
node in Vw encodes the position of a token in w, and
each arc in A encodes a dependency relation between
two tokens. To denote an arc .i; j / 2 A, we write
i ! j ; here, the node i is the head, and the node j is
the dependent. A sample dependency graph is given
in the left part of Figure 2.
2.2 Transition Systems
A transition system is a structure S D .C; T; I; Ct /,
where C is a set of configurations, T is a finite set
of transitions, which are partial functions t WC * C ,
I is a total initialization function mapping each input
string to a unique initial configuration, and Ct  C
is a set of terminal configurations.
The transition systems that we investigate in this
paper differ from each other only with respect to
their sets of transitions, and are identical in all other
aspects. In each of them, a configuration is de-
fined relative to a string w as above, and is a triple
c D .; ?; A/, where  and ? are disjoint lists of
nodes from Vw , called stack and buffer, respectively,
and A  Vw  Vw is a set of arcs. We denote the
stack, buffer and arc set associated with c by .c/,
?.c/, and A.c/, respectively. We follow a standard
convention and write the stack with its topmost ele-
ment to the right, and the buffer with its first element
to the left; furthermore, we indicate concatenation
in the stack and in the buffer by a vertical bar. The
initialization function maps each string w to the ini-
tial configuration .??; ?0; : : : ; jwj   1?;;/. The set of
terminal configurations contains all configurations of
the form .?0?; ??; A/, where A is some set of arcs.
Given an input string w, a parser based on S pro-
cesses w from left to right, starting in the initial con-
figuration I.w/. At each point, it applies one of
the transitions, until at the end it reaches a terminal
.; i j?;A/ ` . ji; ?; A/ .sh/
. ji jj; ?;A/ ` . jj; ?;A [ fj ! ig/ .la/
. ji jj; ?;A/ ` . ji; ?; A [ fi ! j g/ .ra/
Figure 1: Transitions in the arc-standard model.
configuration; the dependency graph defined by the
arc set associated with that configuration is then re-
turned as the analysis for w. Formally, a computation
of S on w is a sequence  D c0; : : : ; cm, m  0, of
configurations (defined relative to w) in which each
configuration is obtained as the value of the preced-
ing one under some transition. It is called complete
whenever c0 D I.w/, and cm 2 Ct . We note that a
computation can be uniquely specified by its initial
configuration c0 and the sequence of its transitions,
understood as a string over T . Complete computa-
tions, where c0 is fixed, can be specified by their
transition sequences alone.
3 Arc-Standard Model
To introduce the core concepts of the paper, we first
look at a particularly simple model for transition-
based dependency parsing, known as the arc-stan-
dard model. This model has been used, in slightly
different variants, by a number of parsers (Nivre,
2004; Attardi, 2006; Huang and Sagae, 2010).
3.1 Transition System
The arc-standard model uses three types of transi-
tions: Shift (sh) removes the first node in the buffer
and pushes it to the stack. Left-Arc (la) creates a
new arc with the topmost node on the stack as the
head and the second-topmost node as the dependent,
and removes the second-topmost node from the stack.
Right-Arc (ra) is symmetric to Left-Arc in that it
creates an arc with the second-topmost node as the
head and the topmost node as the dependent, and
removes the topmost node.
The three transitions can be formally specified as
in Figure 1. The right half of Figure 2 shows a com-
plete computation of the arc-standard transition sys-
tem, specified by its transition sequence. The picture
also shows the contents of the stack over the course of
the computation; more specifically, column i shows
the stack .ci / associated with the configuration ci .
674
root This news had little effect on the markets
0
1
0 0
1
2
0
2
0
2
3
0
3
0
3
0
3
0
3
54 4
5
0
3
5
0
3
5
0
3
5
6 6 6
77
8
0
3
5
6
8
0
3
5
6
0
3
5
0
3
0
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
sh sh sh la sh la sh sh la sh sh sh la ra ra ra ra
1 2
0
Figure 2: A dependency tree (left) and a computation generating this tree in the arc-standard system (right).
3.2 Push Computations
The key to the tabulation of transition-based depen-
dency parsers is to find a way to decompose com-
putations into smaller, shareable parts. For the arc-
standard model, as well as for the other transition
systems that we consider in this paper, we base our
decomposition on the concept of push computations.
By this, we mean computations
 D c0; : : : ; cm ; m  1 ;
on some input string w with the following properties:
(P1) The initial stack .c0/ is not modified during
the computation, and is not even exposed after the
first transition: For every 1  i  m, there exists a
non-empty stack i such that .ci / D .c0/ji .
(P2) The overall effect of the computation is to
push a single node to the stack: The stack .cm/ can
be written as .cm/ D .c0/jh, for some h 2 Vw .
We can verify that the computation in Figure 2 is
a push computation. We can also see that it contains
shorter computations that are push computations; one
example is the computation 0 D c1; : : : ; c16, whose
overall effect is to push the node 3. In Figure 2, this
computation is marked by the zig-zag path traced
in bold. The dashed line delineates the stack .c1/,
which is not modified during 0.
Every computation that consists of a single sh tran-
sition is a push computation. Starting from these
atoms, we can build larger push computations by
means of two (partial) binary operations fla and fra,
defined as follows. Let 1 D c10; : : : ; c1m1 and
2 D c20; : : : ; c2m2 be push computations on the
same input string w such that c1m1 D c20. Then
fra.1; 2/ D c10; : : : ; c1m1 ; c21; : : : ; c2m2 ; c ;
where c is obtained from c2m2 by applying the ra
transition. (The operation fla is defined analogously.)
We can verify that fra.1; 2/ is another push com-
putation. For instance, with respect to Figure 2,
fra.1; 2/ D 0. Conversely, we say that the push
computation 0 can be decomposed into the subcom-
putations 1 and 2, and the operation fra.
3.3 Deduction System
Building on the compositional structure of push com-
putations, we now construct a deduction system (in
the sense of Shieber et al (1995)) that tabulates the
computations of the arc-standard model for a given
input string w D w0   wn 1. For 0  i  n, we
shall write ?i to denote the buffer ?i; : : : ; n 1?. Thus,
?0 denotes the full buffer, associated with the initial
configuration I.w/, and ?n denotes the empty buffer,
associated with a terminal configuration c 2 Ct .
Item form. The items of our deduction system
take the form ?i; h; j ?, where 0  i  h < j  n.
The intended interpretation of an item ?i; h; j ? is:
For every configuration c0 with ?.c0/ D ?i , there
exists a push computation  D c0; : : : ; cm such that
?.cm/ D j? , and .cm/ D .c0/jh.
Goal. The only goal item is ?0; 0; n?, asserting
that there exists a complete computation for w.
Axioms. For every stack  , position i < n and
arc set A, by a single sh transition we obtain the
push computation .; ?i ; A/; . ji; ?iC1; A/. There-
fore we can take the set of all items of the form
?i; i; i C 1? as the axioms of our system.
Inference rules. The inference rules parallel the
composition operations fla and fra. Suppose that
we have deduced the items ?i; h1; k? and ?k; h2; j ?,
where 0  i  h1 < k  h2 < j  n. The
item ?i; h1; k? asserts that for every configuration c10
675
Item form: ?i; h; j ? , 0  i  h < j  jwj Goal: ?0; 0; jwj? Axioms: ?i; i; i C 1?
Inference rules:
?i; h1; k? ?k; h2; j ?
?i; h2; j ?
.laI h2 ! h1/
?i; h1; k? ?k; h2; j ?
?i; h1; j ?
.raI h1 ! h2/
Figure 3: Deduction system for the arc-standard model.
with ?.c10/ D ?i , there exists a push computation
1 D c10; : : : ; c1m1 such that ?.c1m1/ D ?k , and
.c1m1/ D .c10/jh1. Using the item ?k; h2; j ?,
we deduce the existence of a second push compu-
tation 2 D c20; : : : ; c2m2 such that c20 D c1m1 ,
?.c2m2/ D j? , and .c2m2/ D .c10/jh1jh2. By
means of fra, we can then compose 1 and 2 into a
new push computation
fra.1; 2/ D c10; : : : ; c1m1 ; c21; : : : ; c2m2 ; c :
Here, ?.c/ D j? , and .c/ D .c10/jh1. Therefore,
we may generate the item ?i; h1; j ?. The inference
rule for la can be derived analogously.
Figure 3 shows the complete deduction system.
3.4 Completeness and Non-Ambiguity
We have informally argued that our deduction sys-
tem is sound. To show completeness, we prove the
following lemma: For all 0  i  h < j  jwj and
every push computation  D c0; : : : ; cm on w with
?.c0/ D ?i , ?.cm/ D j? and .cm/ D .c0/jh, the
item ?i; h; j ? is generated. The proof is by induction
on m, and there are two cases:
m D 1. In this case,  consists of a single sh transi-
tion, h D i , j D i C 1, and we need to show that the
item ?i; i; i C 1? is generated. This holds because this
item is an axiom.
m  2. In this case,  ends with either a la or a ra
transition. Let c be the rightmost configuration in 
that is different from cm and whose stack size is one
larger than the size of .c0/. The computations
1 D c0; : : : ; c and 2 D c; : : : ; cm 1
are both push computations with strictly fewer tran-
sitions than  . Suppose that the last transition in 
is ra. In this case, ?.c/ D ?k for some i < k < j ,
.c/ D .c0/jh with h < k, ?.cm 1/ D j? , and
.cm 1/ D .c0/jhjh0 for some k  h0 < j . By
induction, we may assume that we have generated
items ?i; h; k? and ?k; h0; j ?. Applying the inference
rule for ra, we deduce the item ?i; h; j ?. An analo-
gous argument can be made for fla.
Apart from being sound and complete, our deduc-
tion system also has the property that it assigns at
most one derivation to a given item. To see this,
note that in the proof of the lemma, the choice of c
is uniquely determined: If we take any other con-
figuration c0 that meets the selection criteria, then
the computation  02 D c
0; : : : ; cm 1 is not a push
computation, as it contains c as an intermediate con-
figuration, and thereby violates property P1.
3.5 Discussion
Let us briefly take stock of what we have achieved
so far. We have provided a deduction system capable
of tabulating the set of all computations of an arc-
standard parser on a given input string, and proved
the correctness of this system relative to an interpre-
tation based on push computations. Inspecting the
system, we can see that its generic implementation
takes space in O.jwj3/ and time in O.jwj5/.
Our deduction system is essentially the same as the
one for the CKY algorithm for bilexicalized context-
free grammar (Collins, 1996; G?mez-Rodr?guez et
al., 2008). This equivalence reveals a deep correspon-
dence between the arc-standard model and bilexical-
ized context-free grammar, and, via results by Eisner
and Satta (1999), to head automata. In particular,
Eisner?s and Satta?s ?hook trick? can be applied to
our tabulation to reduce its runtime to O.jwj4/.
4 Adding Features
The main goal with the tabulation of transition-based
dependency parsers is to obtain a representation
based on which semiring values such as the high-
est-scoring computation for a given input (and with
it, a dependency tree) can be calculated. Such com-
putations involve the use of feature information. In
this section, we discuss how our tabulation of the arc-
standard system can be extended for this purpose.
676
?i; h1; kI hx2; x1i; hx1; x3i? W v1 ?k; h2; j I hx1; x3i; hx3; x4i? W v2
?i; h1; j I hx2; x1i; hx1; x3i? W v1 C v2 C hx3; x4i  E?ra
.ra/
?i; h; j I hx2; x1i; hx1; x3i? W v
?j; j; j C 1I hx1; x3i; hx3; wj i? W hx1; x3i  E?sh
.sh/
Figure 4: Extended inference rules under the feature model ? D hs1:w; s0:wi. The annotations indicate how to calculate
a candidate for an update of the Viterbi score of the conclusion using the Viterbi scores of the premises.
4.1 Scoring Computations
For the sake of concreteness, suppose that we want
to score computations based on the following model,
taken from Zhang and Clark (2008). The score of a
computation  is broken down into a sum of scores
score.t; ct / for combinations of a transition t in the
transition sequence associated with  and the config-
uration ct in which t was taken:
score./ D
X
t2
score.t; ct / (1)
The score score.t; ct / is defined as the dot product of
the feature representation of ct relative to a feature
model ? and a transition-specific weight vector E?t :
score.t; ct / D ?.ct /  E?t
The feature model ? is a vector h1; : : : ; ni of
elementary feature functions, and the feature rep-
resentation ?.c/ of a configuration c is a vector
Ex D h1.c/; : : : ; n.c/i of atomic values. Two ex-
amples of feature functions are the word form associ-
ated with the topmost and second-topmost node on
the stack; adopting the notation of Huang and Sagae
(2010), we will write these functions as s0:w and
s1:w, respectively. Feature functions like these have
been used in several parsers (Nivre, 2006; Zhang and
Clark, 2008; Huang et al, 2009).
4.2 Integration of Feature Models
To integrate feature models into our tabulation of
the arc-standard system, we can use extended items
of the form ?i; h; j I ExL; ExR? with the same intended
interpretation as the old items ?i; h; j ?, except that
the initial configuration of the asserted computations
 D c0; : : : ; cm now is required to have the feature
representation ExL, and the final configuration is re-
quired to have the representation ExR:
?.c0/ D ExL and ?.cm/ D ExR
We shall refer to the vectors ExL and ExR as the left-
context vector and the right-context vector of the
computation  , respectively.
We now need to change the deduction rules so that
they become faithful to the extended interpretation.
Intuitively speaking, we must ensure that the feature
values can be computed along the inference rules.
As a concrete example, consider the feature model
? D hs1:w; s0:wi. In order to integrate this model
into our tabulation, we change the rule for ra as in
Figure 4, where x1; : : : ; x4 range over possible word
forms. The shared variable occurrences in this rule
capture the constraints that hold between the feature
values of the subcomputations 1 and 2 asserted
by the premises, and the computations fra.1; 2/
asserted by the conclusion. To illustrate this, suppose
that 1 and 2 are as in Figure 2. Then the three
occurrences of x3 for instance encode that
?s0:w?.c6/ D ?s1:w?.c15/ D ?s0:w?.c16/ D w3 :
We also need to extend the axioms, which cor-
respond to computations consisting of a single sh
transition. The most conservative way to do this is
to use a generate-and-test technique: Extend the ex-
isting axioms by all valid choices of left-context and
right-context vectors, that is, by all pairs ExL; ExR such
that there exists a configuration c with ?.c/ D ExL
and ?.sh.c// D ExR. The task of filtering out use-
less guesses can then be delegated to the deduction
system.
A more efficient way is to only have one axiom, for
the case where c D I.w/, and to add to the deduction
system a new, unary inference rule for sh as in Fig-
ure 4. This rule only creates items whose left-context
vector is the right-context vector of some other item,
which prevents the generation of useless items. In
the following, we take this second approach, which
is also the approach of Huang and Sagae (2010).
677
?i; h; j I hx2; x1i; hx1; x3i? W .p; v/
?j; j; j C 1I hx1; x3i; hx3; wj i? W .p C ; /
.sh/ , where  D hx1; x3i  E?sh
?i; h1; kI hx2; x1i; hx1; x3i? W .p1; v1/ ?k; h2; j I hx1; x3i; hx3; x4i? W .p2; v2/
?i; h1; j I hx2; x1i; hx1; x3i? W .p1 C v2 C Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 296?300,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Heuristic Cube Pruning in Linear Time
Andrea Gesmundo
Department of
Computer Science
University of Geneva
andrea.gesmundo@unige.ch
Giorgio Satta
Department of
Information Engineering
University of Padua
satta@dei.unipd.it
James Henderson
Department of
Computer Science
University of Geneva
james.henderson@unige.ch
Abstract
We propose a novel heuristic algorithm for
Cube Pruning running in linear time in the
beam size. Empirically, we show a gain in
running time of a standard machine translation
system, at a small loss in accuracy.
1 Introduction
Since its first appearance in (Huang and Chiang,
2005), the Cube Pruning (CP) algorithm has quickly
gained popularity in statistical natural language pro-
cessing. Informally, this algorithm applies to sce-
narios in which we have the k-best solutions for two
input sub-problems, and we need to compute the k-
best solutions for the new problem representing the
combination of the two sub-problems.
CP has applications in tree and phrase based ma-
chine translation (Chiang, 2007; Huang and Chi-
ang, 2007; Pust and Knight, 2009), parsing (Huang
and Chiang, 2005), sentence alignment (Riesa and
Marcu, 2010), and in general in all systems combin-
ing inexact beam decoding with dynamic program-
ming under certain monotonic conditions on the def-
inition of the scores in the search space.
Standard implementations of CP run in time
O(k log(k)), with k being the size of the in-
put/output beams (Huang and Chiang, 2005). Ges-
mundo and Henderson (2010) propose Faster CP
(FCP) which optimizes the algorithm but keeps the
O(k log(k)) time complexity. Here, we propose a
novel heuristic algorithm for CP running in time
O(k) and evaluate its impact on the efficiency and
performance of a real-world machine translation
system.
2 Preliminaries
Let L = ?x0, . . . , xk?1? be a list over R, that is,
an ordered sequence of real numbers, possibly with
repetitions. We write |L| = k to denote the length of
L. We say that L is descending if xi ? xj for every
i, j with 0 ? i < j < k. Let L1 = ?x0, . . . , xk?1?
and L2 = ?y0, . . . , yk??1? be two descending lists
over R. We write L1 ? L2 to denote the descending
list with elements xi+yj for every i, j with 0 ? i <
k and 0 ? j < k?.
In cube pruning (CP) we are given as input two
descending lists L1, L2 over R with |L1| = |L2| =
k, and we are asked to compute the descending list
consisting of the first k elements of L1 ?L2.
A problem related to CP is the k-way merge
problem (Horowitz and Sahni, 1983). Given de-
scending lists Li for every i with 0 ? i < k, we
write mergek?1i=0 Li to denote the ?merge? of all the
lists Li, that is, the descending list with all elements
from the lists Li, including repetitions.
For ? ? R we define shift(L,?) = L ? ???. In
words, shift(L,?) is the descending list whose ele-
ments are obtained by ?shifting? the elements of L
by ?, preserving the order. Let L1,L2 be descend-
ing lists of length k, with L2 = ?y0, . . . , yk?1?.
Then we can express the output of CP on L1,L2 as
the list
mergek?1i=0 shift(L1, yi) (1)
truncated after the first k elements. This shows that
the CP problem is a particular instance of the k-way
merge problem, in which all input lists are related by
k independent shifts.
296
Computation of the solution of the k-way merge
problem takes time O(q log(k)), where q is the
size of the output list. In case each input list has
length k this becomes O(k2 log(k)), and by restrict-
ing the computation to the first k elements, as re-
quired by the CP problem, we can further reduce to
O(k log(k)). This is the already known upper bound
on the CP problem (Huang and Chiang, 2005; Ges-
mundo and Henderson, 2010). Unfortunately, there
seems to be no way to achieve an asymptotically
faster algorithm by exploiting the restriction that the
input lists are all related by some shifts. Nonethe-
less, in the next sections we use the above ideas to
develop a heuristic algorithm running in time linear
in k.
3 Cube Pruning With Constant Slope
Consider lists L1,L2 defined as in section 2. We say
that L2 has constant slope if yi?1? yi = ? > 0 for
every i with 0 < i < k. Throughout this section we
assume that L2 has constant slope, and we develop
an (exact) linear time algorithm for solving the CP
problem under this assumption.
For each i ? 0, let Ii be the left-open interval
(x0 ? (i + 1) ? ?, x0 ? i ? ?] of R. Let alo s =
?(x0 ? xk?1)/?? + 1. We split L1 into (possibly
empty) sublists ?i, 0 ? i < s, called segments, such
that each ?i is the descending sublist consisting of
all elements fromL1 that belong to Ii. Thus, moving
down one segment in L1 is the closest equivalent to
moving down one element in L2.
Let t = min{k, s}; we define descending lists
Mi, 0 ? i < t, as follows. We set M0 =
shift(?0, y0), and for 1 ? i < t we let
Mi = merge{shift(?i, y0), shift(Mi?1,??)} (2)
We claim that the ordered concatenation of M0,
M1, . . . , Mt?1 truncated after the first k elements
is exactly the output of CP on input L1,L2.
To prove our claim, it helps to visualize the de-
scending list L1 ? L2 (of size k2) as a k ? k matrix
L whose j-th column is shift(L1, yj), 0 ? j < k.
For an interval I = (x, x?], we define shift(I, y) =
(x+ y, x?+ y]. Similarly to what we have done with
L1, we can split each column of L into s segments.
For each i, j with 0 ? i < s and 0 ? j < k, we de-
fine the i-th segment of the j-th column, written ?i,j ,
as the descending sublist consisting of all elements
of that column that belong to shift(Ii, yj). Then we
have ?i,j = shift(?i, yj).
For any d with 0 ? d < t, consider now all
segments ?i,j with i + j = d, forming a sub-
antidiagonal in L. We observe that these segments
contain all and only those elements of L that belong
to the interval Id. It is not difficult to show by in-
duction that these elements are exactly the elements
that appear in descending order in the list Mi defined
in (2).
We can then directly use relation (2) to iteratively
compute CP on two lists of length k, under our as-
sumption that one of the two lists has constant slope.
Using the fact that the merge of two lists as in (2) can
be computed in time linear in the size of the output
list, it is not difficult to implement the above algo-
rithm to run in time O(k).
4 Linear Time Heuristic Solution
In this section we further elaborate on the exact al-
gorithm of section 3 for the constant slope case, and
develop a heuristic solution for the general CP prob-
lem. Let L1,L2, L and k be defined as in sections 2
and 3. Despite the fact that L2 does not have a con-
stant slope, we can still split each column of L into
segments, as follows.
Let I?i, 0 ? i < k ? 1, be the left-open interval
(x0 + yi+1, x0+ yi] of R. Note that, unlike the case
of section 3, intervals I?i?s are not all of the same size
now. Let alo I?k?1 = [xk?1 + yk?1, x0 + yk?1].
For each i, j with 0 ? j < k and 0 ? i < k ?
j, we define segment ??i,j as the descending sublist
consisting of all elements of the j-th column of L
that belong to I?i+j . In this way, the j-th column
of L is split into segments I?j , I?j+1, . . . , I?k?1, and
we have a variable number of segments per column.
Note that segments ??i,j with a constant value of i+j
contain all and only those elements of L that belong
to the left-open interval I?i+j .
Similarly to section 3, we define descending lists
M?i, 0 ? i < k, by setting M?0 = ??0,0 and, for
1 ? i < k, by letting
M?i = merge{??i,0 , path(M?i?1, L)} (3)
Note that the function path(M?i?1, L) should not re-
turn shift(M?i?1,??), for some value ?, as in the
297
1: Algorithm 1 (L1, L2) : L??
2: L??.insert(L[0, 0]);
3: referColumn? 0;
4: xfollow ? L[0, 1];
5: xdeviate ? L[1, 0];
6: C ? CircularList([0, 1]);
7: C-iterator? C.begin();
8: while |L??| < k do
9: if xfollow > xdeviate then
10: L??.insert(xfollow );
11: if C-iterator.current()=[0, 1] then
12: referColumn++;
13: [i, j]? C-iterator.next();
14: xfollow ? L[i,referColumn+j];
15: else
16: L??.insert(xdeviate );
17: i? xdeviate .row();
18: C-iterator.insert([i,?referColumn]);
19: xdeviate ? L[i + 1, 0];
case of (2). This is because input list L2 does not
have constant slope in general. In an exact algo-
rithm, path(M?i?1, L) should return the descending
list L?i?1 = mergeij=1 ??i?j,j: Unfortunately, we do
not know how to compute such a i-way merge with-
out introducing a logarithmic factor.
Our solution is to define path(M?i?1, L) in such a
way that it computes a list L?i?1 which is a permu-
tation of the correct solution L?i?1. To do this, we
consider the ?relative? path starting at x0+yi?1 that
we need to follow in L in order to collect all the el-
ements of M?i?1 in the given order. We then apply
such a path starting at x0 + yi and return the list of
collected elements. Finally, we compute the output
list L?? as the concatenation of all lists M?i up to the
first k elements.
It is not difficult to see that when L2 has constant
slope we have M?i = Mi for all i with 0 ? i < k,
and list L?? is the exact solution to the CP prob-
lem. When L2 does not have a constant slope, list
L?? might depart from the exact solution in two re-
spects: it might not be a descending list, because
of local variations in the ordering of the elements;
and it might not be a permutation of the exact so-
lution, because of local variations at the end of the
list. In the next section we evaluate the impact that
 
 
	

 


 


 
 


 

 


 

  

  


  
 

  
 

Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 135?144,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Transition-Based Dependency Parser
Using a Dynamic Parsing Strategy
Francesco Sartorio
Department of
Information Engineering
University of Padua, Italy
sartorio@dei.unipd.it
Giorgio Satta
Department of
Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Joakim Nivre
Department of
Linguistics and Philology
Uppsala University, Sweden
joakim.nivre@lingfil.uu.se
Abstract
We present a novel transition-based, greedy
dependency parser which implements a
flexible mix of bottom-up and top-down
strategies. The new strategy allows the
parser to postpone difficult decisions until
the relevant information becomes available.
The novel parser has a ?12% error reduc-
tion in unlabeled attachment score over an
arc-eager parser, with a slow-down factor
of 2.8.
1 Introduction
Dependency-based methods for syntactic parsing
have become increasingly popular during the last
decade or so. This development is probably due
to many factors, such as the increased availability
of dependency treebanks and the perceived use-
fulness of dependency structures as an interface
to downstream applications, but a very important
reason is also the high efficiency offered by de-
pendency parsers, enabling web-scale parsing with
high throughput. The most efficient parsers are
greedy transition-based parsers, which only explore
a single derivation for each input and relies on
a locally trained classifier for predicting the next
parser action given a compact representation of the
derivation history, as pioneered by Yamada and
Matsumoto (2003), Nivre (2003), Attardi (2006),
and others. However, while these parsers are cap-
able of processing tens of thousands of tokens per
second with the right choice of classifiers, they are
also known to perform slightly below the state-of-
the-art because of search errors and subsequent
error propagation (McDonald and Nivre, 2007),
and recent research on transition-based depend-
ency parsing has therefore explored different ways
of improving their accuracy.
The most common approach is to use beam
search instead of greedy decoding, in combination
with a globally trained model that tries to minim-
ize the loss over the entire sentence instead of a
locally trained classifier that tries to maximize the
accuracy of single decisions (given no previous er-
rors), as first proposed by Zhang and Clark (2008).
With these methods, transition-based parsers have
reached state-of-the-art accuracy for a number of
languages (Zhang and Nivre, 2011; Bohnet and
Nivre, 2012). However, the drawback with this ap-
proach is that parsing speed is proportional to the
size of the beam, which means that the most accur-
ate transition-based parsers are not nearly as fast
as the original greedy transition-based parsers. An-
other line of research tries to retain the efficiency of
greedy classifier-based parsing by instead improv-
ing the way in which classifiers are learned from
data. While the classical approach limits training
data to parser states that result from oracle predic-
tions (derived from a treebank), these novel ap-
proaches allow the classifier to explore states that
result from its own (sometimes erroneous) predic-
tions (Choi and Palmer, 2011; Goldberg and Nivre,
2012).
In this paper, we explore an orthogonal approach
to improving the accuracy of transition-based pars-
ers, without sacrificing their advantage in efficiency,
by introducing a new type of transition system.
While all previous transition systems assume a
static parsing strategy with respect to top-down
and bottom-up processing, our new system allows
a dynamic strategy for ordering parsing decisions.
This has the advantage that the parser can postpone
difficult decisions until the relevant information be-
comes available, in a way that is not possible in
existing transition systems. A second advantage of
dynamic parsing is that we can extend the feature
inventory of previous systems. Our experiments
show that these advantages lead to significant im-
provements in parsing accuracy, compared to a
baseline parser that uses the arc-eager transition
system of Nivre (2003), which is one of the most
135
widely used static transition systems.
2 Static vs. Dynamic Parsing
The notions of bottom-up and top-down parsing
strategies do not have a general mathematical defin-
ition; they are instead specified, often only inform-
ally, for individual families of grammar formal-
isms. In the context of dependency parsing, a pars-
ing strategy is called purely bottom-up if every
dependency h ? d is constructed only after all
dependencies of the form d ? i have been con-
structed. Here h? d denotes a dependency with
h the head node and d the dependent node. In con-
trast, a parsing strategy is called purely top-down
if h? d is constructed before any dependency of
the form d? i.
If we consider transition-based dependency pars-
ing (Nivre, 2008), the purely bottom-up strategy is
implemented by the arc-standard model of Nivre
(2004). After building a dependency h ? d, this
model immediately removes from its stack node d,
preventing further attachment of dependents to this
node. A second popular parser, the arc-eager model
of Nivre (2003), instead adopts a mixed strategy.
In this model, a dependency h? d is constructed
using a purely bottom-up strategy if it represents a
left-arc, that is, if the dependent d is placed to the
left of the head h in the input string. In contrast, if
h ? d represents a right-arc (defined symmetric-
ally), then this dependency is constructed before
any right-arc d ? i (top-down) but after any left-
arc d? i (bottom-up).
What is important to notice about the above
transition-based parsers is that the adopted pars-
ing strategies are static. By this we mean that each
dependency is constructed according to some fixed
criterion, depending on structural conditions such
as the fact that the dependency represents a left or a
right arc. This should be contrasted with dynamic
parsing strategies in which several parsing options
are simultaneously available for the dependencies
being constructed.
In the context of left-to-right, transition-based
parsers, dynamic strategies are attractive for sev-
eral reasons. One argument is related to the well-
known PP-attachment problem, illustrated in Fig-
ure 1. Here we have to choose whether to attach
node P as a dependent of V (arc ?2) or else as
a dependent of N1 (arc ?3). The purely bottom-
up arc-standard model has to take a decision as
soon as N1 is placed into the stack. This is so
V N1 P N2
?1
?2
?3 ?4
Figure 1: PP-attachment example, with dashed arcs
identifying two alternative choices.
because the construction of ?1 excludes ?3 from
the search space, while the alternative decision of
shifting P into the stack excludes ?2. This is bad,
because the information about the correct attach-
ment could come from the lexical content of node P.
The arc-eager model performs slightly better, since
it can delay the decision up to the point in which ?1
has been constructed and P is read from the buffer.
However, at this point it must make a commitment
and either construct ?3 or pop N1 from the stack
(implicitly committing to ?2) before N2 is read
from the buffer. In contrast with this scenario, in
the next sections we implement a dynamic parsing
strategy that allows a transition system to decide
between the attachments ?2 and ?3 after it has seen
all of the four nodes V, N1, P and N2.
Other additional advantages of dynamic parsing
strategies with respect to static strategies are re-
lated to the increase in the feature inventory that
we apply to parser states, and to the increase of
spurious ambiguity. However, these arguments are
more technical than the PP-attachment argument
above, and will be discussed later.
3 Dependency Parser
In this section we present a novel transition-based
parser for projective dependency trees, implement-
ing a dynamic parsing strategy.
3.1 Preliminaries
For non-negative integers i and j with i ? j, we
write [i, j] to denote the set {i, i+1, . . . , j}. When
i > j, [i, j] is the empty set.
We represent an input sentence as a string w =
w0 ? ? ?wn, n ? 1, where token w0 is a special
root symbol and, for each i ? [1, n], token wi =
(i, ai, ti) encodes a lexical element ai and a part-of-
speech tag ti associated with the i-th word in the
sentence.
A dependency tree for w is a directed, ordered
tree Tw = (Vw, Aw), where Vw = {wi | i ?
136
w4
w2 w5 w7
w1 w3 w6
Figure 2: A dependency tree with left spine
?w4, w2, w1? and right spine ?w4, w7?.
[0, n]} is the set of nodes, and Aw ? Vw ? Vw is
the set of arcs. Arc (wi, wj) encodes a dependency
wi ? wj . A sample dependency tree (excluding
w0) is displayed in Figure 2. If (wi, wj) ? Aw for
j < i, we say that wj is a left child of wi; a right
child is defined in a symmetrical way.
The left spine of Tw is an ordered sequence
?u1, . . . , up? with p ? 1 and ui ? Vw for i ? [1, p],
consisting of all nodes in a descending path from
the root of Tw taking the leftmost child node at
each step. More formally, u1 is the root node of Tw
and ui is the leftmost child of ui?1, for i ? [2, p].
The right spine of Tw is defined symmetrically;
see again Figure 2. Note that the left and the right
spines share the root node and no other node.
3.2 Basic Idea
Transition-based dependency parsers use a stack
data structure, where each stack element is associ-
ated with a tree spanning some (contiguous) sub-
string of the input w. The parser can combine
two trees T and T ? through attachment operations,
called left-arc or right-arc, under the condition that
T and T ? appear at the two topmost positions in
the stack. Crucially, only the roots of T and T ? are
available for attachment; see Figure 3(a).
In contrast, a stack element in our parser records
the entire left spine and right spine of the associated
tree. This allows us to extend the inventory of the
attachment operations of the parser by including
the attachment of tree T as a dependent of any node
in the left or in the right spine of a second tree T ?,
provided that this does not violate projectivity.1
See Figure 3(b) for an example.
The new parser implements a mix of bottom-up
and top-down strategies, since after any of the at-
tachments in Figure 3(b) is performed, additional
dependencies can still be created for the root of T .
Furthermore, the new parsing strategy is clearly dy-
1A dependency tree for w is projective if every subtree has
a contiguous yield in w.
T
T ?
T
T ?
(a) (b)
Figure 3: Left-arc attachment of T to T ? in case
of (a) standard transition-based parsers and (b) our
parser.
namic, due to the free choice in the timing for these
attachments. The new strategy is more powerful
than the strategy of the arc-eager model, since we
can use top-down parsing at left arcs, which is not
allowed in arc-eager parsing, and we do not have
the restrictions of parsing right arcs (h? d) before
the attachment of right dependents at node d.
To conclude this section, let us resume our dis-
cussion of the PP-attachment example in Figure 1.
We observe that the new parsing strategy allows the
construction of a tree T ? consisting of the only de-
pendency V? N1 and a tree T , placed at the right
of T ?, consisting of the only dependency P? N2.
Since the right spine of T ? consists of nodes V
and N1, we can freely choose between attachment
V? P and attachment N1? P. Note that this is
done after we have seen node N2, as desired.
3.3 Transition-based Parser
We assume the reader is familiar with the formal
framework of transition-based dependency parsing
originally introduced by Nivre (2003); see Nivre
(2008) for an introduction. To keep the notation at
a simple level, we only discuss here the unlabeled
version of our parser; however, a labeled extension
is used in ?5 for our experiments.
Our transition-based parser uses a stack data
structure to store partial parses for the input string
w. We represent the stack as an ordered sequence
? = [?d, . . . , ?1], d ? 0, of stack elements, with
the topmost element placed at the right. When d =
0, we have the empty stack ? = []. Sometimes we
use the vertical bar to denote the append operator
for ?, and write ? = ??|?1 to indicate that ?1 is the
topmost element of ?.
A stack element is a pair
?k = (?uk,1, . . . , uk,p?, ?vk,1, . . . , vk,q?)
where the ordered sequences ?uk,1, . . . , uk,p? and
137
?vk,1, . . . , vk,q? are the left and the right spines, re-
spectively, of the tree associated with ?k. Recall
that uk,1 = vk,1, since the root node of the associ-
ated tree is shared by the two spines.
The parser also uses a buffer to store the por-
tion of the input string still to be processed. We
represent the buffer as an ordered sequence ? =
[wi, . . . , wn], i ? 0, of tokens from w, with the
first element placed at the left. Note that ? always
represents a (non-necessarily proper) suffix of w.
When i > n, we have the empty buffer ? = [].
Sometimes we use the vertical bar to denote the
append operator for ?, and write ? = wi|?? to in-
dicate that wi is the first token of ?; consequently,
we have ?? = [wi+1, . . . , wn].
When processing w, the parser reaches several
states, technically called configurations. A con-
figuration of the parser relative to w is a triple
c = (?, ?,A), where ? and ? are a stack and
a buffer, respectively, and A ? Vw ? Vw is a
set of arcs. The initial configuration for w is
([], [w0, . . . , wn], ?). The set of terminal config-
urations consists of all configurations of the form
([?1], [], A), where ?1 is associated with a tree hav-
ing root w0, that is, u1,1 = v1,1 = w0, and A is any
set of arcs.
The core of a transition-based parser is the set
of its transitions. Each transition is a binary rela-
tion defined over the set of configurations of the
parser. Since the set of configurations is infinite,
a transition is infinite as well, when viewed as a
set. However, transitions can always be specified
by some finite means. Our parser uses three types
of transitions, defined in what follows.
? SHIFT, or sh for short. This transition re-
moves the first node from the buffer and
pushes into the stack a new element, consist-
ing of the left and right spines of the associ-
ated tree. More formally
(?,wi|?,A) `sh (?|(?wi?, ?wi?), ?, A)
? LEFT-ARCk, k ? 1, or lak for short. Let h
be the k-th node in the left spine of the top-
most tree in the stack, and let d be the root
node of the second topmost tree in the stack.
This transition creates a new arc h? d. Fur-
thermore, the two topmost stack elements are
replaced by a new element associated with the
tree resulting from the h? d attachment. The
transition does not advance with the reading
of the buffer. More formally
(??|?2|?1, ?, A) `lak (??|?la, ?, A ? {h? d})
where
?1 = (?u1,1, . . . , u1,p?, ?v1,1, . . . , v1,q?) ,
?2 = (?u2,1, . . . , u2,r?, ?v2,1, . . . , v2,s?) ,
?la = (?u1,1, . . . , u1,k, u2,1, . . . , u2,r?,
?v1,1, . . . , v1,q?) ,
and where we have set h = u1,k and d = u2,1.
? RIGHT-ARCk, k ? 1, or rak for short. This
transition is defined symmetrically with re-
spect to lak. We have
(??|?2|?1, ?, A) `rak (??|?ra, ?, A ? {h? d})
where ?1 and ?2 are as in the lak case,
?ra = (?u2,1, . . . , u2,r?,
?v2,1, . . . , v2,k, v1,1, . . . , v1,q?) ,
and we have set h = v2,k and d = v1,1.
Transitions lak and rak are parametric in k,
where k is bounded by the length of the input string
and not by a fixed constant (but see also the experi-
mental findings in ?5). Thus our system uses an un-
bounded number of transition relations, which has
an apparent disadvantage for learning algorithms.
We will get back to this problem in ?4.3.
A complete computation relative to w is a se-
quence of configurations c1, c2, . . . , ct, t ? 1, such
that c1 and ct are initial and final configurations,
respectively, and for each i ? [2, t], ci is produced
by the application of some transition to ci?1. It is
not difficult to see that the transition-based parser
specified above is sound, meaning that the set of
arcs constructed in any complete computation on
w is always a dependency tree for w. The parser
is also complete, meaning that every (projective)
dependency tree for w is constructed by some com-
plete computation on w. A mathematical proof of
this statement is beyond the scope of this paper,
and will not be provided here.
3.4 Deterministic Parsing Algorithm
The transition-based parser of the previous sec-
tion is a nondeterministic device, since several
transitions can be applied to a given configuration.
This might result in several complete computations
138
Algorithm 1 Parsing Algorithm
Input: string w = w0 ? ? ?wn, function score()
Output: dependency tree Tw
c = (?, ?,A)? ([], [w0, . . . , wn], ?)
while |?| > 1 ? |?| > 0 do
while |?| < 2 do
update c with sh
p? length of left spine of ?1
s? length of right spine of ?2
T ? {lak | k ? [1, p]} ?
{rak | k ? [1, s]} ? {sh}
bestT ? argmaxt?T score(t , c)
update c with bestT
return Tw = (Vw, A)
for w. We present here an algorithm that runs
the parser in pseudo-deterministic mode, greed-
ily choosing at each configuration the transition
that maximizes some score function. Algorithm 1
takes as input a string w and a scoring function
score() defined over parser transitions and parser
configurations. The scoring function will be the
subject of ?4 and is not discussed here. The output
of the parser is a dependency tree for w.
At each iteration the algorithm checks whether
there are at least two elements in the stack and, if
this is not the case, it shifts elements from the buffer
to the stack. Then the algorithm uses the function
score() to evaluate all transitions that can be ap-
plied under the current configuration c = (?, ?,A),
and it applies the transition with the highest score,
updating the current configuration.
To parse a sentence of length n (excluding the
root token w0) the algorithm applies exactly 2n+1
transitions. In the worst case, each transition ap-
plication involves 1 + p+ s transition evaluations.
We therefore conclude that the algorithm always
reaches a configuration with an empty buffer and a
stack which contains only one element. Then the al-
gorithm stops, returning the dependency tree whose
arc set is defined as in the current configuration.
4 Model and Training
In this section we introduce the adopted learning
algorithm and discuss the model parameters.
4.1 Learning Algorithm
We use a linear model for the score function in
Algorithm 1, and define score(t , c) = ~? ? ?(t , c).
Here ~? is a weight vector and function ? provides
Algorithm 2 Learning Algorithm
Input: pair (w = w0 ? ? ?wn, Ag), vector ~?
Output: vector ~?
c = (?, ?,A)? ([], [w0, . . . , wn], ?)
while |?| > 1 ? |?| > 0 do
while |?| < 2 do
update c with SHIFT
p? length of left spine of ?1
s? length of right spine of ?2
T ? {lak | k ? [1, p]} ?
{rak | k ? [1, s]} ? {sh}
bestT ? argmaxt?T score(t , c)
bestCorrectT ?
argmaxt?T ?isCorrect(t) score(t , c)
if bestT 6= bestCorrectT then
~? ? ~? ? ?(bestT , c)
+?(bestCorrectT , c)
update c with bestCorrectT
a feature vector representation for a transition t ap-
plying to a configuration c. The function ? will be
discussed at length in ?4.3. The vector ~? is trained
using the perceptron algorithm in combination with
the averaging method to avoid overfitting; see Fre-
und and Schapire (1999) and Collins and Duffy
(2002) for details.
The training data set consists of pairs (w,Ag),
where w is a sentence and Ag is the set of arcs
of the gold (desired) dependency tree for w. At
training time, each pair (w,Ag) is processed using
the learning algorithm described as Algorithm 2.
The algorithm is based on the notions of correct and
incorrect transitions, discussed at length in ?4.2.
Algorithm 2 parsesw following Algorithm 1 and
using the current ~?, until the highest score selec-
ted transition bestT is incorrect according to Ag .
When this happens, ~? is updated by decreasing the
weights of the features associated with the incorrect
bestT and by increasing the weights of the features
associated with the transition bestCorrectT having
the highest score among all possible correct trans-
itions. After each update, the learning algorithm
resumes parsing from the current configuration by
applying bestCorrectT , and moves on using the
updated weights.
4.2 Correct and Incorrect Transitions
Standard transition-based dependency parsers are
trained by associating each gold tree with a canon-
ical complete computation. This means that, for
each configuration of interest, only one transition
139
?2 ?1 b1
(a)
?2 ?1 b1
(b)
?2 ?1
? ? ?
bi
(c)
?2 ?1
? ? ?
bi
(d)
Figure 4: Graphical representation of configura-
tions; drawn arcs are in Ag but have not yet been
added to the configuration. Transition sh is incor-
rect for configuration (a) and (b); sh and ra1 are
correct for (c); sh and la1 are correct for (d).
leading to the gold tree is considered as correct. In
this paper we depart from such a methodology, and
follow Goldberg and Nivre (2012) in allowing more
than one correct transition for each configuration,
as explained in detail below.
Let (w,Ag) be a pair in the training set. In ?3.3
we have mentioned that there is always a complete
computation on w that results in the construction
of the set Ag . In general, there might be more than
one computation forAg . This means that the parser
shows spurious ambiguity.
Observe that all complete computations for Ag
share the same initial configuration cI,w and final
configuration cF,Ag . Consider now the set C(w) of
all configurations c that are reachable from cI,w,
meaning that there exists a sequence of transitions
that takes the parser from cI,w to c. A configuration
c ? C(w) is correct for Ag if cF,Ag is reachable
from c; otherwise, c is incorrect for Ag .
Let c ? C(w) be a correct configuration for Ag .
A transition t is correct for c and Ag if c `t c?
and c? is correct for Ag ; otherwise, t is incorrect
for c and Ag . The next lemma provides a charac-
terization of correct and incorrect transitions; see
Figure 4 for examples. We use this characterization
in the implementation of predicate isCorrect() in
Algorithm 2.
Lemma 1 Let (w,Ag) be a pair in the training set
and let c ? C(w) with c = (?, ?,A) be a correct
configuration for Ag . Let alo v1,k, k ? [1, q], be
the nodes in the right spine of ?1.
(i) lak and rak are incorrect for c and Ag if and
only if they create a new arc (h? d) 6? Ag ;
(ii) sh is incorrect for c and Ag if and only if the
following conditions are both satisfied:
(a) there exists an arc (h ? d) in Ag such
that h is in ? and d = v1,1;
(b) there is no arc (h? ? d?) in Ag with
h? = v1,k, k ? [1, q], and d? in ?. 2
PROOF (SKETCH) To prove part (i) we focus on
transition rak; a similar argument applies to lak.
The ?if? statement in part (i) is self-evident.
?Only if?. Assuming that transition rak creates
a new arc (h? d) ? Ag , we argue that from con-
figuration c? with c `rak c? we can still reach the
final configuration associated with Ag . We have
h = v2,k and d = u1,1. The tree fragments in ?
with roots v2,k+1 and u1,1 must be adjacent siblings
in the tree associated with Ag , since c is a correct
configuration for Ag and (v2,k ? u1,1) ? Ag .
This means that each of the nodes v2,k+1, . . . , v2,s
in the right spine in ?2 in c must have already ac-
quired all of its right dependents, since the tree is
projective. Therefore it is safe for transition rak to
eliminate the nodes v2,k+1, . . . , v2,s from the right
spine in ?2.
We now deal with part (ii). Let c `sh c?, c? =
(??, ??, A).
?If?. Assuming (ii)a and (ii)b, we argue that c? is
incorrect. Node d is the head of ??2. Arc (h? d) is
not inA, and the only way we could create (h? d)
from c? is by reaching a new configuration with d
in the topmost stack symbol, which amounts to say
that ??1 can be reduced by a correct transition. Node
h is in some ??i, i > 2, by (ii)a. Then reduction of
??1 implies that the root of ??1 is reachable from the
root of ??2, which contradicts (ii)b.
?Only if?. Assuming (ii)a is not satisfied, we
argue that sh is correct for c and Ag . There must
be an arc (h? d) not in A with d = v1,1 and h is
some token wi in ?. From stack ?? = ???|??2|??1 it
is always possible to construct (h? d) consuming
the substring of ? up to wi and ending up with
stack ???|?red , where ?red is a stack element with
root wi. From there, the parser can move on to
the final configuration cF,Ag . A similar argument
applies if we assume that (ii)b is not satisfied. 
From condition (i) in Lemma 1 and from the fact
that there are no cycles in Ag , it follows that there
is at most one correct transition among the trans-
itions of type lak or rak. From condition (ii) in the
lemma we can also see that the existence of a cor-
rect transition of type lak or rak for some configura-
tion does not imply that the sh transition is incorrect
140
for the same configuration; see Figures 4(c,d) for
examples. It follows that for a correct configuration
there might be at most 2 correct transitions. In our
training experiments for English in ?5 we observe 2
correct transitions for 42% of the reached configur-
ations. This nondeterminism is a byproduct of the
adopted dynamic parsing strategy, and eventually
leads to the spurious ambiguity of the parser.
As already mentioned, we do not impose any ca-
nonical form on complete computations that would
hardwire a preference for some correct transition
and get rid of spurious ambiguity. Following Gold-
berg and Nivre (2012), we instead regard spurious
ambiguity as an additional resource of our pars-
ing strategy. Our main goal is that the training
algorithm learns to prefer a sh transition in a con-
figuration that does not provide enough information
for the choice of the correct arc. In the context of
dependency parsing, the strategy of delaying arc
construction when the current configuration is not
informative is called the easy-first strategy, and
has been first explored by Goldberg and Elhadad
(2010).
4.3 Feature Extraction
In existing transition-based parsers a set of atomic
features is statically defined and extracted from
each configuration. These features are then com-
bined together into complex features, according to
some feature template, and joined with the avail-
able transition types. This is not possible in our
system, since the number of transitions lak and rak
is not bounded by a constant. Furthermore, it is not
meaningful to associate transitions lak and rak, for
any k ? 1, always with the same features, since
the constructed arcs impinge on nodes at differ-
ent depths in the involved spines. It seems indeed
more significant to extract information that is local
to the arc h? d being constructed by each trans-
ition, such as for instance the grandparent and the
great grandparent nodes of d. This is possible if
we introduce a higher level of abstraction than in
existing transition-based parsers. We remark here
that this abstraction also makes the feature repres-
entation more similar to the ones typically found
in graph-based parsers, which are centered on arcs
or subgraphs of the dependency tree.
We index the nodes in the stack ? relative to
the head node of the arc being constructed, in
case of the transitions lak or rak, or else relative
to the root node of ?1, in case of the transition
sh. More precisely, let c = (?, ?,A) be a con-
figuration and let t be a transition. We define
the context of c and t as the tuple C(c, t) =
(s3, s2, s1, q1, q2, gp, gg), whose components are
placeholders for word tokens in ? or in ?. All these
placeholders are specified in Table 1, for each c and
t . Figure 5 shows an example of feature extraction
for the displayed configuration c = (?, ?,A) and
the transition la2. In this case we have s3 = u3,1,
s2 = u2,1, s1 = u1,2, q1 = gp = u1,1, q2 = b1;
gg = none because the head of gp is not available
in c.
Note that in Table 1 placeholders are dynamic-
ally assigned in such a way that s1 and s2 refer to
the nodes in the constructed arc h? d, and gp, gg
refer to the grandparent and the great grandparent
nodes, respectively, of d. Furthermore, the node
assigned to s3 is the parent node of s2, if such a
node is defined; otherwise, the node assigned to
s3 is the root of the tree fragment in the stack un-
derneath ?2. Symmetrically, placeholders q1 and
q2 refer to the parent and grandparent nodes of s1,
respectively, when these nodes are defined; other-
wise, these placeholders get assigned tokens from
the buffer. See again Figure 5.
Finally, from the placeholders in C(c, t) we ex-
tract a standard set of atomic features and their
complex combinations, to define the function ?.
Our feature template is an extended version of the
feature template of Zhang and Nivre (2011), ori-
ginally developed for the arc-eager model. The
extension is obtained by adding top-down features
for left-arcs (based on placeholders gp and gg),
and by adding right child features for the first stack
element. The latter group of features is usually ex-
ploited for the arc-standard model, but is undefined
for the arc-eager model.
5 Experimental Assessment
Performance evaluation is carried out on the Penn
Treebank (Marcus et al, 1993) converted to Stan-
ford basic dependencies (De Marneffe et al, 2006).
We use sections 2-21 for training, 22 as develop-
ment set, and 23 as test set. The part-of-speech
tags are assigned by an automatic tagger with ac-
curacy 97.1%. The tagger used on the training set
is trained on the same data set by using four-way
jackknifing, while the tagger used on the develop-
ment and test sets is trained on all the training set.
We train an arc-labeled version of our parser.
In the first three lines of Table 2 we compare
141
context sh lak rak
placeholder k = 1 k = 2 k > 2 k = 1 k = 2 k > 2
s1 u1,1 = v1,1 u1,k u1,1 = v1,1
s2 u2,1 = v2,1 u2,1 = v2,1 v2,k
s3 u3,1 = v3,1 u3,1 = v3,1 u3,1 = v3,1 v2,k?1
q1 b1 b1 u1,k?1 b1
q2 b2 b2 b1 u1,k?2 b2
gp none none u1,k?1 none v2,k?1
gg none none none u1,k?2 none none v2,k?2
Table 1: Definition ofC(c, t) = (s3, s2, s1, q1, q2, gp, gg), for c = (??|?3|?2|?1, b1|b2|?,A) and t of type
sh or lak, rak, k ? 1. Symbols uj,k and vj,k are the k-th nodes in the left and right spines, respectively, of
stack element ?j , with uj,1 = vj,1 being the shared root of ?j ; none is an artificial element used when
some context?s placeholder is not available.
? ? ?
stack ?
u3,1 = v3,1
v3,2
u2,1 = v2,1
u2,2 v2,2
v2,3
u1,1 = v1,1
u1,2 v1,2
u1,3 v1,3
la2
buffer ?
b1 b2 b3 ? ? ?
context extracted for la2
s3 s2 s1 q1=gp q2
Figure 5: Extraction of atomic features for context C(c, la2) = (s3, s2, s1, q1, q2, gp, gg), c = (?, ?,A).
parser iter UAS LAS UEM
arc-standard 23 90.02 87.69 38.33
arc-eager 12 90.18 87.83 40.02
this work 30 91.33 89.16 42.38
arc-standard + easy-first 21 90.49 88.22 39.61
arc-standard + spine 27 90.44 88.23 40.27
Table 2: Accuracy on test set, excluding punc-
tuation, for unlabeled attachment score (UAS),
labeled attachment score (LAS), unlabeled exact
match (UEM).
the accuracy of our parser against our implementa-
tion of the arc-eager and arc-standard parsers. For
the arc-eager parser, we use the feature template
of Zhang and Nivre (2011). The same template is
adapted to the arc-standard parser, by removing the
top-down parent features and by adding the right
child features for the first stack element. It turns out
that our feature template, described in ?4.3, is the
exact merge of the templates used for the arc-eager
and the arc-standard parsers.
We train all parsers up to 30 iterations, and for
each parser we select the weight vector ~? from the
iteration with the best accuracy on the development
set. All our parsers attach the root node at the end
of the parsing process, following the ?None? ap-
proach discussed by Ballesteros and Nivre (2013).
Punctuation is excluded in all evaluation metrics.
Considering UAS, our parser provides an improve-
ment of 1.15 over the arc-eager parser and an im-
provement of 1.31 over the arc-standard parser, that
is an error reduction of ?12% and ?13%, respect-
ively. Considering LAS, we achieve improvements
of 1.33 and 1.47, with an error reduction of ?11%
and ?12%, over the arc-eager and the arc-standard
parsers, respectively.
We speculate that the observed improvement of
our parser can be ascribed to two distinct com-
ponents. The first component is the left-/right-
spine representation for stack elements, introduced
in ?3.3. The second component is the easy-first
strategy, implemented on the basis of the spurious
ambiguity of our parser and the definition of cor-
rect/incorrect transitions in ?4.2. In this perspective,
we observe that our parser can indeed be viewed as
an arc-standard model augmented with (i) the spine
representation, and (ii) the easy-first strategy. More
specifically, (i) generalizes the la/ra transitions to
the lak/rak transitions, introducing a top-down com-
ponent into the purely bottom-up arc-standard. On
the other hand, (ii) drops the limitation of canonical
computations for the arc-standard, and leverages
142
on the spurious ambiguity of the parser to enlarge
the search space.
The two components above are mutually inde-
pendent, meaning that we can individually imple-
ment each component on top of an arc-standard
model. More precisely, the arc-standard + spine
model uses the transitions lak/rak but retains the
definition of canonical computation, defined by ap-
plying each lak/rak transition as soon as possible.
On the other hand, the arc-standard + easy-first
model retains the original la/ra transitions but is
trained allowing any correct transition at each con-
figuration. In this case the characterization of cor-
rect and incorrect configurations in Lemma 1 has
been adapted to transitions la/ra, taking into ac-
count the bottom-up constraint.
With the purpose of incremental comparison, we
report accuracy results for the two ?incremental?
models in the last two lines of Table 2. Analyzing
these results, and comparing with the plain arc-
standard, we see that the spine representation and
the easy-first strategy individually improve accur-
acy. Moreover, their combination into our model
(third line of Table 2) works very well, with an
overall improvement larger than the sum of the
individual contributions.
We now turn to a computational analysis. At
each iteration our parser evaluates a number of
transitions bounded by ?+1, with ? the maximum
value of the sum of the lengths of the left spine in ?1
and of the right spine in ?2. Quantity ? is bounded
by the length n of the input sentence. Since the
parser applies exactly 2n + 1 transitions, worst
case running time is O(n2). We have computed
the average value of ? on our English data set,
resulting in 2.98 (variance 2.15) for training set,
and 2.95 (variance 1.96) for development set. We
conclude that, in the expected case, running time is
O(n), with a slow down constant which is rather
small, in comparison to standard transition-based
parsers. Accordingly, when running our parser
against our implementation of the arc-eager and
arc-standard models, we measured a slow-down of
2.8 and 2.2, respectively. Besides the change in
representation, this slow-down is also due to the
increase in the number of features in our system.
We have also checked the worst case value of ? in
our data set. Interestingly, we have seen that for
strings of length smaller than 40 this value linearly
grows with n, and for longer strings the growth
stops, with a maximum worst case observed value
of 22.
6 Concluding Remarks
We have presented a novel transition-based parser
using a dynamic parsing strategy, which achieves
a ?12% error reduction in unlabeled attachment
score over the static arc-eager strategy and even
more over the (equally static) arc-standard strategy,
when evaluated on English.
The idea of representing the right spine of a
tree within the stack elements of a shift-reduce
device is quite old in parsing, predating empirical
approaches. It has been mainly exploited to solve
the PP-attachment problem, motivated by psycho-
linguistic models. The same representation is also
adopted in applications of discourse parsing, where
right spines are usually called right frontiers; see
for instance Subba and Di Eugenio (2009). In
the context of transition-based dependency parsers,
right spines have also been exploited by Kitagawa
and Tanaka-Ishii (2010) to decide where to attach
the next word from the buffer. In this paper we
have generalized their approach by introducing the
symmetrical notion of left spine, and by allowing
attachment of full trees rather than attachment of a
single word.2
Since one can regard a spine as a stack in it-
self, whose elements are tree nodes, our model is
reminiscent of the embedded pushdown automata
of Schabes and Vijay-Shanker (1990), used to parse
tree adjoining grammars (Joshi and Schabes, 1997)
and exploiting a stack of stacks. However, by im-
posing projectivity, we do not use the extra-power
of the latter class.
An interesting line of future research is to com-
bine our dynamic parsing strategy with a training
method that allows the parser to explore transitions
that apply to incorrect configurations, as in Gold-
berg and Nivre (2012).
Acknowledgments
We wish to thank Liang Huang and Marco Kuhl-
mann for discussion related to the ideas reported in
this paper, and the anonymous reviewers for their
useful suggestions. The second author has been
partially supported by MIUR under project PRIN
No. 2010LYA9RH 006.
2Accuracy comparison of our work with Kitagawa and
Tanaka-Ishii (2010) is not meaningful, since these authors
have evaluated their system on the same data set but based on
gold part-of-speech tags (personal communication).
143
References
Giuseppe Attardi. 2006. Experiments with a multil-
anguage non-projective dependency parser. In Pro-
ceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL), pages 166?
170.
Miguel Ballesteros and Joakim Nivre. 2013. Going
to the roots of dependency parsing. Computational
Linguistics, 39(1):5?13.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1455?
1465.
Jinho D. Choi and Martha Palmer. 2011. Getting the
most out of transition-based dependency parsing. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
687?692.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
263?270, Philadephia, Pennsylvania.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC),
volume 6, pages 449?454.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277?296, December.
Yoav Goldberg and Michael Elhadad. 2010. An ef-
ficient algorithm for easy-first non-directional de-
pendency parsing. In Proceedings of Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL), pages 742?
750, Los Angeles, USA.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic or-
acle for arc-eager dependency parsing. In Proceed-
ings of the 24th International Conference on Com-
putational Linguistics (COLING), pages 959?976.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages, volume 3, pages 69?123. Springer.
Kotaro Kitagawa and Kumiko Tanaka-Ishii. 2010.
Tree-based deterministic dependency parsing ? an
application to Nivre?s method ?. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL) Short Papers, pages
189?193.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
Eighth International Workshop on Parsing Techno-
logies (IWPT), pages 149?160, Nancy, France.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Workshop on Incremental
Parsing: Bringing Engineering and Cognition To-
gether, pages 50?57, Barcelona, Spain.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Yves Schabes and K. Vijay-Shanker. 1990. Determ-
inistic left to right parsing of tree adjoining lan-
guages. In Proceedings of the 28th annual meet-
ing of the Association for Computational Linguistics
(ACL), pages 276?283, Pittsburgh, Pennsylvania.
Rajen Subba and Barbara Di Eugenio. 2009. An effect-
ive discourse parser that uses rich linguistic inform-
ation. In Proceedings of Human Language Techno-
logies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 566?574.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Stat-
istical dependency analysis with support vector ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT), pages
195?206.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 562?
571.
Yue Zhang and Joakim Nivre. 2011. Transition-based
parsing with rich non-local features. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 188?193.
144
Transactions of the Association for Computational Linguistics, 1 (2013) 267?278. Action Editor: Brian Roark.
Submitted 3/2013; Published 7/2013. c?2013 Association for Computational Linguistics.
Efficient Parsing for Head-Split Dependency Trees
Giorgio Satta
Dept. of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Marco Kuhlmann
Dept. of Linguistics and Philology
Uppsala University, Sweden
marco.kuhlmann@lingfil.uu.se
Abstract
Head splitting techniques have been success-
fully exploited to improve the asymptotic
runtime of parsing algorithms for project-
ive dependency trees, under the arc-factored
model. In this article we extend these tech-
niques to a class of non-projective dependency
trees, called well-nested dependency trees with
block-degree at most 2, which has been previ-
ously investigated in the literature. We define a
structural property that allows head splitting for
these trees, and present two algorithms that im-
prove over the runtime of existing algorithms
at no significant loss in coverage.
1 Introduction
Much of the recent work on dependency parsing has
been aimed at finding a good balance between ac-
curacy and efficiency. For one end of the spectrum,
Eisner (1997) showed that the highest-scoring pro-
jective dependency tree under an arc-factored model
can be computed in timeO.n3/, where n is the length
of the input string. Later work has focused on mak-
ing projective parsing viable under more expressive
models (Carreras, 2007; Koo and Collins, 2010).
At the same time, it has been observed that for
many standard data sets, the coverage of projective
trees is far from complete (Kuhlmann and Nivre,
2006), which has led to an interest in parsing al-
gorithms for non-projective trees. While non-project-
ive parsing under an arc-factored model can be done
in time O.n2/ (McDonald et al, 2005), parsing with
more informed models is intractable (McDonald and
Satta, 2007). This has led several authors to investig-
ate ?mildly non-projective? classes of trees, with the
goal of achieving a balance between expressiveness
and complexity (Kuhlmann and Nivre, 2006).
In this article we focus on a class of mildly non-
projective dependency structures called well-nested
dependency trees with block-degree at most 2. This
class was first introduced by Bodirsky et al (2005),
who showed that it corresponds, in a natural way, to
the class of derivation trees of lexicalized tree-adjoin-
ing grammars (Joshi and Schabes, 1997). While there
are linguistic arguments against the restriction to this
class (Maier and Lichte, 2011; Chen-Main and Joshi,
2010), Kuhlmann and Nivre (2006) found that it has
excellent coverage on standard data sets. Assum-
ing an arc-factored model, well-nested dependency
trees with block-degree  2 can be parsed in time
O.n7/ using the algorithm of Go?mez-Rodr??guez et
al. (2011). Recently, Pitler et al (2012) have shown
that if an additional restriction called 1-inherit is im-
posed, parsing can be done in time O.n6/, without
any additional loss in coverage on standard data sets.
Standard context-free parsing methods, when adap-
ted to the parsing of projective trees, provide O.n5/
time complexity. The O.n3/ time result reported by
Eisner (1997) has been obtained by exploiting more
sophisticated dynamic programming techniques that
?split? dependency trees at the position of their heads,
in order to save bookkeeping. Splitting techniques
have also been exploited to speed up parsing time
for other lexicalized formalisms, such as bilexical
context-free grammars and head automata (Eisner
and Satta, 1999). However, to our knowledge no at-
tempt has been made in the literature to extend these
techniques to non-projective dependency parsing.
In this article we leverage the central idea from
Eisner?s algorithm and extend it to the class of well-
nested dependency trees with block-degree at most 2.
267
We introduce a structural property, called head-split,
that allows us to split these trees at the positions of
their heads. The property is restrictive, meaning that
it reduces the class of trees that can be generated.
However, we show that the restriction to head-split
trees comes at no significant loss in coverage, and it
allows parsing in timeO.n6/, an asymptotic improve-
ment of one order of magnitude over the algorithm
by Go?mez-Rodr??guez et al (2011) for the unrestric-
ted class. We also show that restricting the class of
head-split trees by imposing the already mentioned
1-inherit property does not cause any additional loss
in coverage, and that parsing for the combined class
is possible in time O.n5/, one order of magnitude
faster than the algorithm by Pitler et al (2012) for
the 1-inherit class without the head-split condition.
The above results have consequences also for the
parsing of other related formalisms, such as the
already mentioned lexicalized tree-adjoining gram-
mars. This will be discussed in the final section.
2 Head Splitting
To introduce the basic idea of this article, we briefly
discuss in this section two well-known algorithms for
computing the set of all projective dependency trees
for a given input sentence: the na??ve, CKY-style
algorithm, and the improved algorithm with head
splitting, in the version of Eisner and Satta (1999).1
CKY parsing The CKY-style algorithm works in
a pure bottom-up way, building dependency trees
by combining subtrees. Assuming an input string
w D a1    an, n  1, each subtree t is represented
by means of a finite signature ?i; j; h?, called item,
where i; j are the boundary positions of t ?s span over
w and h is the position of t?s root. This is the only
information we need in order to combine subtrees
under the arc-factored model. Note that the number
of possible signatures is O.n3/.
The main step of the algorithm is displayed in
Figure 1(a). Here we introduce the graphical conven-
tion, used throughout this article, of representing a
subtree by a shaded area, with an horizontal line in-
dicating the spanned fragment of the input string, and
of marking the position of the head by a bullet. The
illustrated step attaches a tree with signature ?k; j; d ?
1Eisner (1997) describes a slightly different algorithm.
(a)
ah ad
i k j
)
ah ad
i j
(b)
ah ad
k
)
ah ad
(c)
ah ad
j
)
ah ad
j
Figure 1: Basic steps for (a) the CKY-style algorithm
and (b, c) the head splitting algorithm.
as a dependent of a tree with signature ?i; k; h?. There
can be O.n5/ instantiations of this step, and this is
also the running time of the algorithm.
Eisner?s algorithm Eisner and Satta (1999) im-
prove over the CKY algorithm by reducing the num-
ber of position records in an item. They do this by
?splitting? each tree into a left and a right fragment,
so that the head is always placed at one of the two
boundary positions of a fragment, as opposed to be-
ing placed at an internal position. In this way items
need only two indices. Left and right fragments can
be processed independently, and merged afterwards.
Let us consider a right fragment t with head ah.
Attachment at t of a right dependent tree with head
ad is now performed in two steps. The first step at-
taches a left fragment with head ad , as in Figure 1(b).
This results in a new type of fragment/item that has
both heads ah and ad placed at its boundaries. The
second step attaches a right fragment with head ad ,
as in Figure 1(c). The number of possible instanti-
ations of these steps, and the asymptotic runtime of
the algorithm, is O.n3/.
In this article we extend the splitting technique to
the class of well-nested dependency trees with block-
degree at most 2. This amounts to defining a fac-
torization for these trees into fragments, each with
its own head at one of its boundary positions, along
with some unfolding of the attachment operation into
intermediate steps. While for projective trees head
splitting can be done without any loss in coverage,
for the extended class head splitting turns out to be
a proper restriction. The empirical relevance of this
will be discussed in ?7.
268
3 Head-Split Trees
In this section we introduce the class of well-nested
dependency trees with block-degree at most 2, and
define the subclass of head-split dependency trees.
3.1 Preliminaries
For non-negative integers i; j we write ?i; j ? to de-
note the set fi; iC1; : : : ; j g; when i > j , ?i; j ? is the
empty set. For a string w D a1    an, where n  1
and each ai is a lexical token, and for i; j 2 ?0; n?
with i  j , we write wi;j to denote the substring
aiC1    aj of w; wi;i is the empty string.
A dependency tree t over w is a directed tree
whose nodes are a subset of the tokens ai in w and
whose arcs encode a dependency relation between
two nodes. We write ai ! aj to denote the arc
.ai ; aj / in t ; here, the node ai is the head, and the
node aj is the dependent. If each token ai , i 2 ?1; n?,
is a node of t , then t is called complete. Sometimes
we write tai to emphasize that tree t is rooted in node
ai . If ai is a node of t , we also write t ?ai ? to denote
the subtree of t composed by node ai as its root and
all of its descendant nodes.
The nodes of t uniquely identify a set of max-
imal substrings of w, that is, substrings separated
by tokens not in t . The sequence of such substrings,
ordered from left to right, is the yield of t , written
yd.t/. Let ai be some node of t . The block-degree
of ai in t , written bd.ai ; t /, is defined as the number
of string components of yd.t ?ai ?/. The block-degree
of t , written bd.t/, is the maximal block-degree of
its nodes. Tree t is non-projective if bd.t/ > 1.
Tree t is well-nested if, for each node ai of t and for
every pair of outgoing dependencies ai ! ad1 and
ai ! ad2 , the string components of yd.t ?ad1 ?/ and
yd.t ?ad2 ?/ do not ?interleave? in w. More precisely,
it is required that, if some component of yd.t ?adi ?/,
i 2 ?1; 2?, occurs in w in between two components
s1; s2 of yd.t ?adj ?/, j 2 ?1; 2? and j ? i , then allcomponents of yd.t ?adi ?/ occur in between s1; s2.
Throughout this article, whenever we consider a
dependency tree t we always implicitly assume that
t is over w, that t has block-degree at most 2, and
that t is well-nested. Let tai be such a tree, with
bd.ai ; tai / D 2. We call the portion of w in between
the two substrings of yd.tai / the gap of tai , denoted
by gap.tai /.
ah ad4ad3ad2ad1
m.tah/
Figure 2: Example of a node ah with block-degree 2 in a
non-projective, well-nested dependency tree tah . Integerm.tah/, defined in ?3.2, is also marked.
Example 1 Figure 2 schematically depicts a well-
nested tree tah with block-degree 2; we have marked
the root node ah and its dependent nodes adi . Foreach node adi , a shaded area highlights t ?adi ?. Wehave bd.ah; tah/ D bd.ad1 ; tah/ D bd.ad4 ; tah/ D
2 and bd.ad2 ; tah/ D bd.ad3 ; tah/ D 1. 
3.2 The Head-Split Property
We say that a dependency tree t has the head-split
property if it satisfies the following condition. Let
ah ! ad be any dependency in t with bd.ah; t / D
bd.ad ; t / D 2. Whenever gap.t ?ad ?/ contains ah, it
must also contain gap.t ?ah?/. Intuitively, this means
that if yd.t ?ad ?/ ?crosses over? the lexical token ah in
w, then yd.t ?ad ?/ must also ?cross over? gap.t ?ah?/.
Example 2 Dependency ah ! ad1 in Figure 3 viol-
ates the head-split condition, since yd.t ?ad1 ?/ crosses
over the lexical token ah inw, but does not cross over
gap.t ?ah?/. The remaining outgoing dependencies of
ah trivially satisfy the head-split condition, since the
child nodes have block-degree 1. 
Let tah be a dependency tree satisfying the head-
split property and with bd.ah; tah/ D 2. We specify
below a construction that ?splits? tah with respect to
the position of the head ah in yd.tah/, resulting in
two dependency trees sharing the root ah and having
all of the remaining nodes forming two disjoint sets.
Furthermore, the resulting trees have block-degree at
most 2.
ahad1 ad2 ad3
Figure 3: Arc ah ! ad1 violates the head-split condition.
269
(a)
ah ad4ad3
(b)
ahad2ad1 m.tah/
Figure 4: Lower tree (a) and upper tree (b) fragments for
the dependency tree in Figure 2.
Let yd.tah/ D hwi;j ; wp;qi and assume that ah
is placed within wi;j . (A symmetric construction
should be used in case ah is placed withinwp;q .) The
mirror image of ah with respect to gap.tah/, written
m.tah/, is the largest integer in ?p; q? such that there
are no dependencies linking nodes in wi;h 1 and
nodes in wp;m.tah / and there are no dependencieslinking nodes in wh;j and nodes in wm.tah /;q . It isnot hard to see that such an integer always exists,
since tah is well-nested.
We classify every dependent ad of ah as being
an ?upper? dependent or a ?lower? dependent of
ah, according to the following conditions: (i) If
d 2 ?i; h   1? [ ?m.tah/C 1; q?, then ad is an upper
dependent of ah. (ii) If d 2 ?hC 1; j ? [ ?p;m.tah/?,
then ad is a lower dependent of ah.
The upper tree of tah is the dependency tree
rooted in ah and composed of all dependencies
ah ! ad in tah with ad an upper dependent of
ah, along with all subtrees tah ?ad ? rooted in those
dependents. Similarly, the lower tree of tah is the
dependency tree rooted in ah and composed of all
dependencies ah ! ad in tah with ad a lower de-
pendent of ah, along with all subtrees tah ?ad ? rooted
in those dependents. As a general convention, in this
article we write tU;ah and tL;ah to denote the upper
and the lower trees of tah , respectively. Note that, in
some degenerate cases, the set of lower or upper de-
pendents may be empty; then tU;ah or tL;ah consists
of the root node ah only.
Example 3 Consider the tree tah displayed in Fig-
ure 2. Integer m.tah/ denotes the boundary between
the right component of yd.tah ?ad4 ?/ and the right
component of yd.tah ?ad1 ?/. Nodes ad3 and ad4 are
lower dependents, and nodes ad1 and ad2 are upper
dependents. Trees tL;ah and tU;ah are displayed in
Figure 4 (a) and (b), respectively. 
The importance of the head-split property can be
informally explained as follows. Let ah ! ad be a
dependency in tah . When we take apart the upper and
the lower trees of tah , the entire subtree tah ?ad ? ends
up in either of these two fragments. This allows us to
represent upper and lower fragments for some head
independently of the other, and to freely recombine
them. More formally, our algorithms will make use
of the following three properties, stated here without
any formal proof:
P1 Trees tU;ah and tL;ah are well-nested, have block-
degree  2, and satisfy the head-split property.
P2 Trees tU;ah and tL;ah have their head ah always
placed at one of the boundaries in their yields.
P3 Let t 0U;ah and t 00L;ah be the upper and lower treesof distinct trees t 0ah and t 00ah , respectively. If m.t 0ah/ Dm.t 00ah/, then there exists a tree tah such that tU;ah D
t 0U;ah and tL;ah D t 00L;ah .
4 Parsing Items
Let w D a1    an, n  1, be the input string. We
need to compactly represent trees that span substrings
of w by recording only the information that is needed
to combine these trees into larger trees during the
parsing process. We do this by associating each
tree with a signature, called item, which is a tuple
?i; j ; p; q; h?X , where h 2 ?1; n? identifies the token
ah, i; j with 0  i  j  n identify a substringwi;j ,
and p; q with j < p  q  n identify a substring
wp;q . We also use the special setting p D q D  .
The intended meaning is that each item repres-
ents some tree tah . If p; q ?   then yd.tah/ D
hwi;j ; wp;qi. If p; q D   then
yd.tah/ D
8
<?
:?
hwi;j i if h 2 ?i C 1; j ?
hwh;h; wi;j i if h < i
hwi;j ; wh;hi if h > j C 1
The two cases h < i and h > j C 1 above will
be used when the root node ah of tah has not yet
collected all of its dependents.
Note that h 2 fi; j C 1g is not used in the
definition of item. This is meant to avoid differ-
ent items representing the same dependency tree,
270
which is undesired for the specification of our al-
gorithm. As an example, items ?i; j ; ; ; i C 1?X
and ?i C 1; j ; ; ; i C 1?X both represent a depend-
ency tree taiC1 with yd.taiC1/ D hwi;j i. This and
other similar cases are avoided by the ban against
h 2 fi; j C 1g, which amounts to imposing some
normal form for items. In our example, only item
?i; j ; ; ; i C 1?X is a valid signature.
Finally, we distinguish among several item types,
indicated by the value of subscript X . These types
are specific to each parsing algorithm, and will be
defined in later sections.
5 Parsing of Head-Split Trees
We present in this section our first tabular algorithm
for computing the set of all dependency trees for an
input sentence w that have the head-split property,
under the arc-factored model. Recall that tai denotes
a tree with root ai , and tL;ai and tU;ai are the lower
and upper trees of tai . The steps of the algorithm
are specified by means of deduction rules over items,
following the approach of Shieber et al (1995).
5.1 Basic Idea
Our algorithm builds trees step by step, by attaching
a tree tah0 as a dependent of a tree tah and creatingthe new dependency ah ! ah0 . Computationally,
the worst case for this operation is when both tah
and tah0 have a gap; then, for each tree we need tokeep a record of the four boundaries, along with the
position of the head, as done by Go?mez-Rodr??guez et
al. (2011). However, if we are interested in parsing
trees that satisfy the head-split property, we can avoid
representing a tree with a gap by means of a single
item. We instead follow the general idea of ?2 for
projective parsing, and use different items for the
upper and the lower trees of the source tree.
When we need to attach tah0 as an upper dependentof tah , defined as in ?3.2, we perform two consecutive
steps. First, we attach tL;ah0 to tU;ah , resulting in anew intermediate tree t1. As a second step, we attach
tU;ah0 to t1, resulting in a new tree t2 which is tU;ahwith tah0 attached as an upper dependent, as desired.Both steps are depicted in Figure 5; here we introduce
the convention of indicating tree grouping through
a dashed line. A symmetric procedure can be used
to attach tah0 as a lower dependent to tL;ah . The
ah
tU;ah
ah0
tL;ah0
+
t1
(a)
t1
ah0 ah
ah0
tU;ah0
+
t2
(b)
Figure 5: Two step attachment of tah0 at tU;ah : (a) attach-ment of tL;ah0 ; (b) attachment of tU;ah0 .
correctness of the two step approach follows from
properties P1 and P3 in ?3.2.
By property P2 in ?3.2, in both steps above the
lexical heads ah and ah0 can be read from the bound-
aries of the involved trees. Then these steps can be
implemented more efficiently than the na??ve method
of attaching tah0 to tah in a single step. A more de-tailed computational analysis will be provided in ?5.7.
To simplify the presentation, we restrict the use of
head splitting to trees with a gap and parse trees with
no gap with the na??ve method; this does not affect
the computational complexity.
5.2 Item Types
We distinguish five different types of items, indicated
by the subscriptX 2 f0;L;U; =L; =U g, as described
in what follows.
 If X D 0, we have p D q D   and yd.ah/ is
specified as in ?4.
 If X D L, we use the item to represent some
lower tree. We have therefore p; q ?   and
h 2 fi C 1; qg.
 If X D U , we use the item to represent some
upper tree. We have therefore p; q ?   and
h 2 fj; p C 1g.
 If X D =L or X D =U , we use the item to
represent some intermediate step in the parsing
process, in which only the lower or upper tree of
some dependent has been collected by the head
ah, and we are still missing the upper (=U ) or
the lower (=L) tree.
271
We further specialize symbol =U by writing =U<
(=U>) to indicate that the missing upper tree should
have its head to the left (right) of its gap. We also use
=L< and =L> with a similar meaning.
5.3 Item Normal Form
It could happen that our algorithm produces items of
type 0 that do not satisfy the normal form condition
discussed in ?4. To avoid this problem, we assume
that every item of type 0 that is produced by the
algorithm is converted into an equivalent normal form
item, by means of the following rules:
?i; j ; ; ; i ?0
?i   1; j ; ; ; i ?0 (1)
?i; j ; ; ; j C 1?0
?i; j C 1; ; ; j C 1?0 (2)
5.4 Items of Type 0
We start with deduction rules that produce items of
type 0. As already mentioned, we do not apply the
head splitting technique in this case.
The next rule creates trees with a single node, rep-
resenting the head, and no dependents. The rule is
actually an axiom (there is no antecedent) and the
statement i 2 ?1; n? is a side condition.
?i   1; i ; ; ; i ?0
?
i 2 ?1; n? (3)
The next rule takes a tree headed in ah0 and makes
it a dependent of a new head ah. This rule imple-
ments what has been called the ?hook trick?. The first
side condition enforces that the tree headed in ah0
has collected all of its dependents, as discussed in ?4.
The second side condition enforces that no cycle is
created. We also write ah ! ah0 to indicate that a
new dependency is created in the parse forest.
?i; j ; ; ; h0?0
?i; j ; ; ; h?0
8
<
:
h0 2 ?i C 1; j ?
h 62 ?i C 1; j ?
ah ! ah0
(4)
The next two rules combine gap-free dependents
of the same head ah.
?i; k; ; ; h?0 ?k; j ; ; ; h?0
?i; j ; ; ; h?0 (5)
?i; h; ; ; h?0 ?h   1; j ; ; ; h?0
?i; j ; ; ; h?0 (6)
We need the special case in (6) to deal with the con-
catenation of two items that share the head ah at the
concatenation point. Observe the apparent mismatch
in step (6) between index h in the first antecedent
and index h   1 in the second antecedent. This is
because in our normal form, both the first and the
second antecedent have already incorporated a copy
of the shared head ah.
The next two rules collect a dependent of ah that
wraps around the dependents that have already been
collected. As already discussed, this operation is
performed by two successive steps: We first collect
the lower tree and then the upper tree. We present
the case in which the shared head of the two trees is
placed at the left of the gap. The case in which the
head is placed at the right of the gap is symmetric.
?i 0; j 0; ; ; h?0
?i; i 0; j 0; j ; i C 1?L
?i; j ; ; ; h?=U<
A Tabular Method for Dynamic Oracles in Transition-Based Parsing
Yoav Goldberg
Department of
Computer Science
Bar Ilan University, Israel
yoav.goldberg@gmail.com
Francesco Sartorio
Department of
Information Engineering
University of Padua, Italy
sartorio@dei.unipd.it
Giorgio Satta
Department of
Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
We develop parsing oracles for two trans-
ition-based dependency parsers, including the
arc-standard parser, solving a problem that
was left open in (Goldberg and Nivre, 2013).
We experimentally show that using these or-
acles during training yields superior parsing
accuracies on many languages.
1 Introduction
Greedy transition-based dependency parsers (Nivre,
2008) incrementally process an input sentence from
left to right. These parsers are very fast and
provide competitive parsing accuracies (Nivre et al.,
2007). However, greedy transition-based parsers
still fall behind search-based parsers (Zhang and
Clark, 2008; Huang and Sagae, 2010) with respect
to accuracy.
The training of transition-based parsers relies on
a component called the parsing oracle, which maps
parser configurations to optimal transitions with re-
spect to a gold tree. A discriminative model is then
trained to simulate the oracle?s behavior. A parsing
oracle is deterministic if it returns a single canon-
ical transition. Furthermore, an oracle is partial if it
is defined only for configurations that can reach the
gold tree, that is, configurations representing pars-
ing histories with no mistake. Oracles that are both
deterministic and partial are called static. Tradition-
ally, only static oracles have been exploited in train-
ing of transition-based parsers.
Recently, Goldberg and Nivre (2012; 2013)
showed that the accuracy of greedy parsers can be
substantially improved without affecting their pars-
ing speed. This improvement relies on the intro-
duction of novel oracles that are nondeterministic
and complete. An oracle is nondeterministic if it re-
turns the set of all transitions that are optimal with
respect to the gold tree, and it is complete if it is
well-defined and correct for every configuration that
is reachable by the parser. Oracles that are both non-
deterministic and complete are called dynamic.
Goldberg and Nivre (2013) develop dynamic or-
acles for several transition-based parsers. The con-
struction of these oracles is based on a property of
transition-based parsers that they call arc decompos-
ition. They also prove that the popular arc-standard
system (Nivre, 2004) is not arc-decomposable, and
they leave as an open research question the construc-
tion of a dynamic oracle for the arc-standard system.
In this article, we develop one such oracle (?4) and
prove its correctness (?5).
An extension to the arc-standard parser was
presented by Sartorio et al. (2013), which relaxes
the bottom-up construction order and allows mixing
of bottom-up and top-down strategies. This parser,
called here the LR-spine parser, achieves state-of-
the-art results for greedy parsing. Like the arc-stand-
ard system, the LR-spine parser is not arc-decom-
posable, and a dynamic oracle for this system was
not known. We extend our oracle for the arc-stand-
ard system to work for the LR-spine system as well
(?6).
The dynamic oracles developed by Goldberg and
Nivre (2013) for arc-decomposable systems are
based on local properties of computations. In con-
trast, our novel dynamic oracle algorithms rely on
arguably more complex structural properties of com-
putations, which are computed through dynamic
programming. This leaves open the question of
whether a machine-learning model can learn to ef-
fectively simulate such complex processes: will the
119
Transactions of the Association for Computational Linguistics, 2 (2014) 119?130. Action Editor: Ryan McDonald.
Submitted 11/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
benefit of training with the dynamic oracle carry
over to the arc-standard and LR-spine systems? We
show experimentally that this is indeed the case (?8),
and that using the training-with-exploration method
of (Goldberg and Nivre, 2013) with our dynamic
programming based oracles yields superior parsing
accuracies on many languages.
2 Arc-Standard Parser
In this section we introduce the arc-standard parser
of Nivre (2004), which is the model that we use in
this article. To keep the notation at a simple level,
we only discuss the unlabeled version of the parser;
however, a labeled extension is used in ?8 for our
experiments.
2.1 Preliminaries and Notation
The set of non-negative integers is denoted as N0.
For i, j ? N0 with i ? j, we write [i, j] to denote
the set {i, i + 1, . . . , j}. When i > j, [i, j] denotes
the empty set.
We represent an input sentence as a string w =
w0 ? ? ?wn, n ? N0, where token w0 is a special
root symbol, and each wi with i ? [1, n] is a lex-
ical token. For i, j ? [0, n] with i ? j, we write
w[i, j] to denote the substring wiwi+1 ? ? ?wj of w.
We write i ? j to denote a grammatical de-
pendency of some unspecified type between lexical
tokens wi and wj , where wi is the head and wj is the
dependent. A dependency tree for w is a directed,
ordered tree t = (Vw, A), such that Vw = [0, n] is
the set of nodes, A ? Vw?Vw is the set of arcs, and
node 0 is the root. Arc (i, j) encodes a dependency
i ? j, and we will often use the latter notation to
denote arcs.
2.2 Transition-Based Dependency Parsing
We assume the reader is familiar with the formal
framework of transition-based dependency parsing
originally introduced by Nivre (2003); see Nivre
(2008) for an introduction. We only summarize here
our notation.
Transition-based dependency parsers use a stack
data structure, where each stack element is associ-
ated with a tree spanning (generating) some sub-
string of the input w. The parser processes the in-
put string incrementally, from left to right, applying
at each step a transition that updates the stack and/or
consumes one token from the input. Transitions may
also construct new dependencies, which are added to
the current configuration of the parser.
We represent the stack data structure as an
ordered sequence ? = [?d, . . . , ?1], d ? N0, of
nodes ?i ? Vw, with the topmost element placed
at the right. When d = 0, we have the empty stack
? = []. Sometimes we use the vertical bar to denote
the append operator for ?, and write ? = ??|?1 to
indicate that ?1 is the topmost element of ?.
The parser also uses a buffer to store the portion
of the input string still to be processed. We represent
the buffer as an ordered sequence ? = [i, . . . , n] of
nodes from Vw, with i the first element of the buf-
fer. In this way ? always encodes a (non-necessarily
proper) suffix of w. We denote the empty buffer as
? = []. Sometimes we use the vertical bar to denote
the append operator for ?, and write ? = i|?? to in-
dicate that i is the first token of ?; consequently, we
have ?? = [i+ 1, . . . , n].
When processing w, the parser reaches several
states, technically called configurations. A config-
uration of the parser relative to w is a triple c =
(?, ?,A), where ? and ? are a stack and a buffer,
respectively, and A ? Vw ? Vw is a set of arcs. The
initial configuration for w is ([], [0, . . . , n], ?). For
the purpose of this article, a configuration is final
if it has the form ([0], [], A), and in a final config-
uration arc set A always defines a dependency tree
for w.
The core of a transition-based parser is the set of
its transitions, which are specific to each family of
parsers. A transition is a binary relation defined
over the set of configurations of the parser. We use
symbol ` to denote the union of all transition rela-
tions of a parser.
A computation of the parser on w is a sequence
c0, . . . , cm, m ? N0, of configurations (defined rel-
ative to w) such that ci?1 ` ci for each i ? [1,m].
We also use the reflexive and transitive closure rela-
tion `? to represent computations. A computation is
called complete whenever c0 is initial and cm is fi-
nal. In this way, a complete computation is uniquely
associated with a dependency tree for w.
2.3 Arc-Standard Parser
The arc-standard model uses the three types of trans-
itions formally specified in Figure 1
120
(?, i|?,A) `sh (?|i, ?, A)
(?|i|j, ?,A) `la (?|j, ?,A ? {j ? i})
(?|i|j, ?,A) `ra (?|i, ?, A ? {i? j})
Figure 1: Transitions in the arc-standard model.
? Shift (sh) removes the first node in the buffer
and pushes it into the stack;
? Left-Arc (la) creates a new arc with the topmost
node on the stack as the head and the second-
topmost node as the dependent, and removes
the second-topmost node from the stack;
? Right-Arc (ra) is symmetric to la in that it cre-
ates an arc with the second-topmost node as the
head and the topmost node as the dependent,
and removes the topmost node.
Notation We sometimes use the functional nota-
tion for a transition ? ? {sh, la, ra}, and write
?(c) = c? in place of c `? c?. Naturally, sh applies
only when the buffer is not empty, and la,ra require
two elements on the stack. We denote by valid(c)
the set of valid transitions in a given configuration.
2.4 Arc Decomposition
Goldberg and Nivre (2013) show how to derive dy-
namic oracles for any transition-based parser which
has the arc decomposition property, defined below.
They also show that the arc-standard parser is not
arc-decomposable.
For a configuration c, we write Ac to denote the
associated set of arcs. A transition-based parser is
arc-decomposable if, for every configuration c and
for every set of arcs A that can be extended to a pro-
jective tree, we have
?(i? j) ? A,?c?[c `? c? ? (i? j) ? Ac? ]
? ?c??[c `? c?? ?A ? Ac?? ] .
In words, if each arc in A is individually derivable
from c, then the set A in its entirety can be derived
from c as well. The arc decomposition property
is useful for deriving dynamic oracles because it is
relatively easy to investigate derivability for single
arcs and then, using this property, draw conclusions
about the number of gold-arcs that are simultan-
eously derivable from the given configuration.
Unfortunately, the arc-standard parser is not arc-
decomposable. To see why, consider a configura-
tion with stack ? = [i, j, k]. Consider also arc set
A = {(i, j), (i, k)}. The arc (i, j) can be derived
through the transition sequence ra, ra, and the arc
(i, k) can be derived through the alternative trans-
ition sequence la, ra. Yet, it is easy to see that a con-
figuration containing both arcs cannot be reached.
As we cannot rely on the arc decomposition prop-
erty, in order to derive a dynamic oracle for the arc-
standard model we need to develop more sophistic-
ated techniques which take into account the interac-
tion among the applied transitions.
3 Configuration Loss and Dynamic Oracles
We aim to derive a dynamic oracle for the arc-stand-
ard (and related) system. This is a function that takes
a configuration c and a gold tree tG and returns a set
of transitions that are ?optimal? for c with respect
to tG. As already mentioned in the introduction, a
dynamic oracle can be used to improve training of
greedy transition-based parsers. In this section we
provide a formal definition for a dynamic oracle.
Let t1 and t2 be two dependency trees over the
same stringw, with arc setsA1 andA2, respectively.
We define the loss of t1 with respect to t2 as
L(t1, t2) = |A1 \A2| . (1)
Note that L(t1, t2) = L(t2, t1), since |A1| =
|A2|. Furthermore L(t1, t2) = 0 if and only if t1
and t2 are the same tree.
Let c be a configuration of our parser relative to
input string w. We write D(c) to denote the set of
all dependency trees that can be obtained in a com-
putation of the form c `? cf , where cf is some final
configuration. We extend the loss function in (1) to
configurations by letting
L(c, t2) = min
t1?D(c)
L(t1, t2) . (2)
Assume some reference (desired) dependency
tree tG for w, which we call the gold tree. Quantity
L(c, tG) can be used to compute a dynamic oracle
relating a parser configuration c to a set of optimal
actions by setting
oracle(c, tG) =
{? | L(?(c), tG)? L(c, tG) = 0} . (3)
121
We therefore need to develop an algorithm for com-
puting (2). We will do this first for the arc-standard
parser, and then for an extension of this model.
Notation We also apply the loss function L(t, tG)
in (1) when t is a dependency tree for a substring
of w. In this case the nodes of t are a subset of
the nodes of tG, and L(t, tG) provides a count of
the nodes of t that are assigned a wrong head node,
when tG is considered as the reference tree.
4 Main Algorithm
Throughout this section we assume an arc-standard
parser. Our algorithm takes as input a projective
gold tree tG and a configuration c = (?L, ?, A). We
call ?L the left stack, in contrast with a right stack
whose construction is specified below.
4.1 Basic Idea
The algorithm consists of two steps. Informally, in
the first step we compute the largest subtrees, called
here tree fragments, of the gold tree tG that have
their span entirely included in the buffer ?. The
root nodes of these tree fragments are then arranged
into a stack data structure, according to the order in
which they appear in ? and with the leftmost root in
? being the topmost element of the stack. We call
this structure the right stack ?R. Intuitively, ?R can
be viewed as the result of pre-computing ? by ap-
plying all sequences of transitions that match tG and
that can be performed independently of the stack in
the input configuration c, that is, ?L.
In the second step of the algorithm we use dy-
namic programming techniques to simulate all com-
putations of the arc-standard parser starting in a con-
figuration with stack ?L and with a buffer consisting
of ?R, with the topmost token of ?R being the first
token of the buffer. As we will see later, the search
space defined by these computations includes the de-
pendency trees for w that are reachable from the in-
put configuration c and that have minimum loss. We
then perform a Viterbi search to pick up such value.
The second step is very similar to standard imple-
mentations of the CKY parser for context-free gram-
mars (Hopcroft and Ullman, 1979), running on an
input string obtained as the concatenation of ?L and
?R. The main difference is that we restrict ourselves
to parse only those constituents in ?L?R that dom-
inate the topmost element of ?L (the rightmost ele-
ment, if ?L is viewed as a string). In this way, we ac-
count for the additional constraint that we visit only
those configurations of the arc-standard parser that
can be reached from the input configuration c. For
instance, this excludes the reduction of two nodes in
?L that are not at the two topmost positions. This
would also exclude the reduction of two nodes in
?R: this is correct, since the associated tree frag-
ments have been chosen as the largest such frag-
ments in ?.
The above intuitive explanation will be made
mathematically precise in ?5, where the notion of
linear dependency tree is introduced.
4.2 Construction of the Right Stack
In the first step we process ? and construct a stack
?R, which we call the right stack associated with c
and tG. Each node of ?R is the root of a tree t which
satisfies the following properties
? t is a tree fragment of the gold tree tG having
span entirely included in the buffer ?;
? t is bottom-up complete for tG, meaning that
for each node i of t different from t?s root, the
dependents of i in tG cannot be in ?L;
? t is maximal for tG, meaning that every super-
tree of t in tG violates the above conditions.
The stack ?R is incrementally constructed by pro-
cessig ? from left to right. Each node i is copied into
?R if it satisfies any of the following conditions
? the parent node of i in tG is not in ?;
? some dependent of i in tG is in ?L or has
already been inserted in ?R.
It is not difficult to see that the nodes in ?R are the
roots of tree fragments of tG that satisfy the condi-
tion of bottom-up completeness and the condition of
maximality defined above.
4.3 Computation of Configuration Loss
We start with some notation. Let `L = |?L| and
`R = |?R|. We write ?L[i] to denote the i-th ele-
ment of ?L and t(?L[i]) to denote the correspond-
ing tree fragment; ?R[i] and t(?R[i]) have a similar
meaning. In order to simplify the specification of
the algorithm, we assume below that ?L[1] = ?R[1].
122
Algorithm 1 Computation of the loss function for the arc-standard parser
1: T [1, 1](?L[1])? L(t(?L[1]), tG)
2: for d? 1 to `L + `R ? 1 do . d is the index of a sub-anti-diagonal
3: for j ? max{1, d? `L + 1} to min{d, `R} do . j is the column index
4: i? d? j + 1 . i is the row index
5: if i < `L then . expand to the left
6: for each h ? ?i,j do
7: T [i+ 1, j](h)? min{T [i+ 1, j](h), T [i, j](h) + ?G(h? ?L[i+ 1])}
8: T [i+ 1, j](?L[i+ 1])? min{T [i+ 1, j](?L[i+ 1]), T [i, j](h) + ?G(?L[i+ 1]? h)}
9: if j < `R then . expand to the right
10: for each h ? ?i,j do
11: T [i, j + 1](h)? min{T [i, j + 1](h), T [i, j](h) + ?G(h? ?R[j + 1])}
12: T [i, j+1](?R[j + 1])? min{T [i, j+1](?R[j + 1]), T [i, j](h)+?G(?R[j + 1]? h)}
13: return T [`L, `R](0) +?i?[1,`L] L(t(?L[i]), tG)
Therefore the elements of ?R which have been con-
structed in ?4.2 are ?R[i], i ? [2, `R].
Algorithm 1 uses a two-dimensional array T of
size `L ? `R, where each entry T [i, j] is an as-
sociation list from integers to integers. An entry
T [i, j](h) stores the minimum loss among depend-
ency trees rooted at h that can be obtained by run-
ning the parser on the first i elements of stack ?L and
the first j elements of buffer ?R. More precisely, let
?i,j = {?L[k] | k ? [1, i]} ?
{?R[k] | k ? [1, j]} . (4)
For each h ? ?i,j , the entry T [i, j](h) is the
minimum loss among all dependency trees defined
as above and with root h. We also assume that
T [i, j](h) is initialized to +? (not reported in the
algorithm).
Algorithm 1 starts at the top-left corner of T , vis-
iting each individual sub-anti-diagonal of T in as-
cending order, and eventually reaching the bottom-
right corner of the array. For each entry T [i, j], the
left expansion is considered (lines 5 to 8) by com-
bining with tree fragment ?L[i+ 1], through a left
or a right arc reduction. This results in the update
of T [i + 1, j](h), for each h ? ?i+1,j , whenever a
smaller value of the loss is achieved for a tree with
root h. The Kronecker-like function used at line 8
provides the contribution of each single arc to the
loss of the current tree. Denoting with AG the set of
arcs of tG, such a function is defined as
?G(i? j) =
{
0, if (i? j) ? AG;
1, otherwise. (5)
A symmetrical process is implemented for the
right expansion of T [i, j] through tree fragment
?R[j + 1] (lines 9 to 12).
As we will see in the next section, quantity
T [`L, `R](0) is the minimal loss of a tree composed
only by arcs that connect nodes in ?L and ?R. By
summing the loss of all tree fragments t(?L[i]) to
the loss in T [`L, `R](0), at line 13, we obtain the
desired result, since the loss of each tree fragment
t(?R[j]) is zero.
5 Formal Properties
Throughout this section we let w, tG, ?L, ?R and
c = (?L, ?, A) be defined as in ?4, but we no longer
assume that ?L[1] = ?R[1]. To simplify the present-
ation, we sometimes identify the tokens in w with
the associated nodes in a dependency tree for w.
5.1 Linear Trees
Algorithm 1 explores all dependency trees that can
be reached by an arc-standard parser from configur-
ation c, under the condition that (i) the nodes in the
buffer ? are pre-computed into tree fragments and
collapsed into their root nodes in the right stack ?R,
and (ii) nodes in ?R cannot be combined together
prior to their combination with other nodes in the
left stack ?L. This set of dependency trees is char-
123
j4
i6 i5 i3 j5
i4 i1 j3
i2 j1 j2
?R?L
Figure 2: A possible linear tree for string pair (?L, ?R),
where ?L = i6i5i4i3i2i1 and ?R = j1j2j3j4j5. The
spine of the tree consists of nodes j4, i3 and i1.
acterized here using the notion of linear tree, to be
used later in the correctness proof.
Consider two nodes ?L[i] and ?L[j] with j >
i > 1. An arc-standard parser can construct an arc
between ?L[i] and ?L[j], in any direction, only after
reaching a configuration in which ?L[i] is at the top
of the stack and ?L[j] is at the second topmost posi-
tion. In such configuration we have that ?L[i] dom-
inates ?L[1]. Furthermore, consider nodes ?R[i] and
?R[j] with j > i ? 1. Since we are assuming that
tree fragments t(?R[i]) and t(?R[j]) are bottom-up
complete and maximal, as defined in ?4.2, we allow
the construction of an arc between ?R[i] and ?R[j],
in any direction, only after reaching a configuration
in which ?R[i] dominates node ?L[1].
The dependency trees satisfying the restrictions
above are captured by the following definition. A
linear tree over (?L, ?R) is a projective dependency
tree t for string ?L?R satisfying both of the addi-
tional conditions reported below. The path from t?s
root to node ?L[1] is called the spine of t.
? Every node of t not in the spine is a dependent
of some node in the spine.
? For each arc i ? j in t with j in the spine, no
dependent of i can be placed in between i and
j within string ?L?R.
An example of a linear tree is depicted in Figure 2.
Observe that the second condition above forbids the
reduction of two nodes i and j, in case none of these
dominates node ?L[1]. For instance, the ra reduc-
tion of nodes i3 and i2 would result in arc i3 ? i2
replacing arc i1 ? i2 in Figure 2. The new depend-
ency tree is not linear, because of a violation of the
second condition above. Similarly, the la reduction
of nodes j3 and j4 would result in arc j4 ? j3 re-
placing arc i3 ? j3 in Figure 2, again a violation of
the second condition above.
Lemma 1 Any tree t ? D(c) can be decomposed
into trees t(?L[i]), i ? [1, `L], trees tj , j ? [1, q] and
q ? 1, and a linear tree tl over (?L, ?R,t), where
?R,t = r1 ? ? ? rq and each rj is the root node of tj . 2
PROOF (SKETCH) Trees t(?L[i]) are common to
every tree in D(c), since the arc-standard model can
not undo the arcs already built in the current con-
figuration c. Similar to the construction in ?4.2 of
the right stack ?R from tG, we let tj , j ? [1, q], be
tree fragments of t that cover only nodes associated
with the tokens in the buffer ? and that are bottom-
up complete and maximal for t. These trees are in-
dexed according to their left to right order in ?. Fi-
nally, tl is implicitly defined by all arcs of t that are
not in trees t(?L[i]) and tj . It is not difficult to see
that tl has a spine ending with node ?L[1] and is a
linear tree over (?L, ?R,t). 
5.2 Correctness
Our proof of correctness for Algorithm 1 is based on
a specific dependency tree t? for w, which we define
below. Let SL = {?L[i] | i ? [1, `L]} and letDL be
the set of nodes that are descendants of some node
in SL. Similarly, let SR = {?R[i] | i ? [1, `R]}
and let DR be the set of descendants of nodes in
SR. Note that sets SL, SR, DL and DR provide a
partition of Vw.
We choose any linear tree t?l over (?L, ?R) having
root 0, such that L(t?l , tG) = mint L(t, tG), where
t ranges over all possible linear trees over (?L, ?R)
with root 0. Tree t? consists of the set of nodes Vw
and the set of arcs obtained as the union of the set
of arcs of t?l and the set of arcs of all trees t(?L[i]),
i ? [1, `L], and t(?R[j]), j ? [1, `R].
Lemma 2 t? ? D(c). 2
PROOF (SKETCH) All tree fragments t(?L[i]) have
already been parsed and are available in the stack
associated with c. Each tree fragment t(?R[j]) can
later be constructed in the computation, when a con-
figuration c? is reached with the relevant segment of
w at the start of the buffer. Note also that parsing of
t(?R[j]) can be done in a way that does not depend
on the content of the stack in c?.
124
Finally, the parsing of the tree fragments t(?R[j])
is interleaved with the construction of the arcs from
the linear tree t?l , which are all of the form (i ? j)
with i, j ? (SL ? SR). More precisely, if (i ? j)
is an arc from t?l , at some point in the computation
nodes i and j will become available at the two top-
most positions in the stack. This follows from the
second condition in the definition of linear tree. 
We now show that tree t? is ?optimal? within the
set D(c) and with respect to tG.
Lemma 3 L(t?, tG) = L(c, tG). 2
PROOF Consider an arbitrary tree t ? D(c). As-
sume the decomposition of t defined in the proof of
Lemma 1, through trees t(?L[i]), i ? [1, `L], trees
tj , j ? [1, q], and linear tree tl over (?L, ?R,t).
Recall that an arc i ? j denotes an ordered pair
(i, j). Let us consider the following partition for the
set of arcs of any dependency tree for w
A1 = (SL ?DL)?DL ,
A2 = (SR ?DR)?DR ,
A3 = (Vw ? Vw) \ (A1 ?A2) .
In what follows, we compare the losses L(t, tG) and
L(t?, tG) by separately looking into the contribution
to such quantities due to the arcs in A1, A2 and A3.
Note that the arcs of trees t(?L[i]) are all in A1,
the arcs of trees t(?R[j]) are all in A2, and the arcs
of tree t?l are all in A3. Since t and t? share trees
t(?L[i]), when restricted to arcs in A1 quantities
L(t, tG) and L(t?, tG) are the same. When restric-
ted to arcs in A2, quantity L(t?, tG) is zero, by con-
struction of the trees t(?R[j]). Thus L(t, tG) can not
be smaller thanL(t?, tG) for these arcs. The difficult
part is the comparison of the contribution to L(t, tG)
and L(t?, tG) due to the arcs in A3. We deal with
this below.
LetAS,G be the set of all arcs from tG that are also
in set (SL ? SR) ? (SR ? SL). In words, AS,G rep-
resents gold arcs connecting nodes in SL and nodes
in SR, in any direction. Within tree t, these arcs can
only be found in the tl component, since nodes in
SL are all placed within the spine of tl, or else at the
left of that spine.
Let us consider an arc (j ? i) ? AS,G with j ?
SL and i ? SR, and let us assume that (j ? i) is in
t?l . If token ai does not occur in ?R,t, node i is not
in tl and (j ? i) can not be an arc of t. We then
have that (j ? i) contributes one unit to L(t, tG)
but does not contribute to L(t?, tG). Similarly, let
(i ? j) ? AS,G be such that i ? SR and j ? SL,
and assume that (i? j) is in t?l . If token ai does not
occur in ?R,t, arc (i ? j) can not be in t. We then
have that (i ? j) contributes one unit to L(t, tG)
but does not contribute to L(t?, tG).
Intuitively, the above observations mean that the
winning strategy for trees in D(c) is to move nodes
from SR as much as possible into the linear tree
component tl, in order to make it possible for these
nodes to connect to nodes in SL, in any direction. In
this case, arcs fromA3 will also move into the linear
tree component of a tree inD(c), as it happens in the
case of t?. We thus conclude that, when restricted to
the set of arcs in A3, quantity L(t, tG) is not smal-
ler than L(t?, tG), because stack ?R has at least as
many tokens corresponding to nodes in SR as stack
?R,t, and because t?l has the minimum loss among
all the linear trees over (?L, ?R).
Putting all of the above observations together,
we conclude that L(t, tG) can not be smaller than
L(t?, tG). This concludes the proof, since t has been
arbitrarily chosen in D(c). 
Theorem 1 Algorithm 1 computes L(c, tG). 2
PROOF (SKETCH) Algorithm 1 implements a Vi-
terbi search for trees with smallest loss among all
linear trees over (?L, ?R). Thus T [`L, `R](0) =
L(t?l , tG). The loss of the tree fragments t(?R[j])
is 0 and the loss of the tree fragments t(?L[i]) is ad-
ded at line 13 in the algorithm. Thus the algorithm
returns L(t?, tG), and the statement follows from
Lemma 2 and Lemma 3. 
5.3 Computational Analysis
Following ?4.2, the right stack ?R can be easily
constructed in time O(n), n the length of the in-
put string. We now analyze Algorithm 1. For each
entry T [i, j] and for each h ? ?i,j , we update
T [i, j](h) a number of times bounded by a con-
stant which does not depend on the input. Each up-
dating can be computed in constant time as well.
We thus conclude that Algorithm 1 runs in time
O(`L ? `R ? (`L + `R)). Quantity `L+`R is bounded
by n, but in practice the former is significantly smal-
ler. When measured over the sentences in the Penn
125
Treebank, the average value of `L+`Rn is 0.29. Interms of runtime, training is 2.3 times slower when
using our oracle instead of a static oracle.
6 Extension to the LR-Spine Parser
In this section we consider the transition-based
parser proposed by Sartorio et al. (2013), called
here the LR-spine parser. This parser is not arc-
decomposable: the same example reported in ?2.4
can be used to show this fact. We therefore extend to
the LR-spine parser the algorithm developed in ?4.
6.1 The LR-Spine Parser
Let t be a dependency tree. The left spine of t is
an ordered sequence ?i1, . . . , ip?, p ? 1, consisting
of all nodes in a descending path from the root of
t taking the leftmost child node at each step. The
right spine of t is defined symmetrically. We use ?
to denote sequence concatenation.
In the LR-spine parser each stack element ?[i] de-
notes a partially built subtree t(?[i]) and is represen-
ted by a pair (lsi, rsi), with lsi and rsi the left and the
right spine, respectively, of t(?[i]). We write lsi[k]
(rsi[k]) to represent the k-th element of lsi (rsi, re-
spectively). We also write r(?[i]) to denote the root
of t(?[i]), so that r(?[i]) = lsi[1] = rsi[1].
Informally, the LR-spine parser uses the same
transition typologies as the arc-standard parser.
However, an arc (h ? d) can now be created with
the head node h chosen from any node in the spine
of the involved tree. The transition types of the LR-
spine parser are defined as follows.
? Shift (sh) removes the first node from the buf-
fer and pushes into the stack a new element,
consisting of the left and right spines of the as-
sociated tree
(?, i|?,A) `sh (?|(?i?, ?i?), ?, A) .
? Left-Arc k (lak) creates a new arc h ? d from
the k-th node in the left spine of the topmost
tree in the stack to the head of the second ele-
ment in the stack. Furthermore, the two top-
most stack elements are replaced by a new ele-
ment associated with the resulting tree
(??|?[2]|?[1], ?, A) `lak (??|?lak , ?, A ? {h? d})
where we have set h = ls1[k], d = r(?[2]) and
?lak = (?ls1[1], . . . , ls1[k]? ? ls2, rs1).
? Right-Arc k (rak for short) is defined symmet-
rically with respect to lak
(??|?[2]|?[1], ?, A) `rak (??|?rak , ?, A ? {h? d})
where we have set h = rs2[k], d = r(?[1]) and
?rak = (ls2, ?rs2[1], . . . , rs2[k]? ? rs1).
Note that, at each configuration in the LR-spine
parser, there are |ls1| possible lak transitions, one for
each choice of a node in the left spine of t(?[1]);
similarly, there are |rs2| possible rak transitions,
one for each choice of a node in the right spine of
t(?[2]).
6.2 Configuration Loss
We only provide an informal description of the ex-
tended algorithm here, since it is very similar to the
algorithm in ?4.
In the first phase we use the procedure of ?4.2 for
the construction of the right stack ?R, considering
only the roots of elements in ?L and ignoring the
rest of the spines. The only difference is that each
element ?R[j] is now a pair of spines (lsR,j , rsR,j).
Since tree fragment t(?R[j]) is bottom-up complete
(see ?4.1), we now restrict the search space in such
a way that only the root node r(?R[j]) can take de-
pendents. This is done by setting lsR,j = rsR,j =
?r(?R[j])? for each j ? [1, `R]. In order to simplify
the presentation we also assume ?R[1] = ?L[1], as
done in ?4.3.
In the second phase we compute the loss of an in-
put configuration using a two-dimensional array T ,
defined as in ?4.3. However, because of the way
transitions are defined in the LR-spine parser, we
now need to distinguish tree fragments not only on
the basis of their roots, but also on the basis of their
left and right spines. Accordingly, we define each
entry T [i, j] as an association list with keys of the
form (ls, rs). More specifically, T [i, j](ls, rs) is the
minimum loss of a tree with left and right spines ls
and rs, respectively, that can be obtained by running
the parser on the first i elements of stack ?L and the
first j elements of buffer ?R.
We follow the main idea of Algorithm 1 and ex-
pand each tree in T [i, j] at its left side, by combin-
ing with tree fragment t(?L[i+ 1]), and at its right
side, by combining with tree fragment t(?R[j + 1]).
126
Tree combination deserves some more detailed dis-
cussion, reported below.
We consider the combination of a tree ta from
T [i, j] and tree t(?L[i+ 1]) by means of a left-arc
transition. All other cases are treated symmetric-
ally. Let (lsa, rsa) be the spine pair of ta, so that
the loss of ta is stored in T [i, j](lsa, rsa). Let also
(lsb, rsb) be the spine pair of t(?L[i+ 1]). In case
there exists a gold arc in tG connecting a node from
lsa to r(?L[i+ 1]), we choose the transition lak,
k ? [1, |lsa|], that creates such arc. In case such gold
arc does not exists, we choose the transition lak with
the maximum possible value of k, that is, k = |lsa|.
We therefore explore only one of the several pos-
sible ways of combining these two trees by means
of a left-arc transition.
We remark that the above strategy is safe. In fact,
in case the gold arc exists, no other gold arc can ever
involve the nodes of lsa eliminated by lak (see the
definition in ?6.1), because arcs can not cross each
other. In case the gold arc does not exist, our choice
of k = |lsa| guarantees that we do not eliminate any
element from lsa.
Once a transition lak is chosen, as described
above, the reduction is performed and the spine
pair (ls, rs) for the resulting tree is computed from
(lsa, rsa) and (lsb, rsb), as defined in ?6.1. At the
same time, the loss of the resulting tree is com-
puted, on the basis of the loss T [i, j](lsa, rsa), the
loss of tree t(?L[i+ 1]), and a Kronecker-like func-
tion defined below. This loss is then used to update
T [i+ 1, j](ls, rs).
Let ta and tb be two trees that must be combined
in such a way that tb becomes the dependent of
some node in one of the two spines of ta. Let also
pa = (lsa, rsa) and pb = (lsb, rsb) be spine pairs for
ta and tb, respectively. Recall that AG is the set of
arcs of tG. The new Kronecker-like function for the
computation of the loss is defined as
?G(pa, pb) =
?
?????
?????
0, if r(ta) < r(tb)?
?k[(rska ? r(tb)) ? AG];
0, if r(ta) > r(tb)?
?k[(lska ? r(tb)) ? AG];
1, otherwise.
6.3 Efficiency Improvement
The algorithm in ?6.2 has an exponential behaviour.
To see why, consider trees in T [i, j]. These trees are
produced by the combination of trees in T [i ? 1, j]
with tree t(?L[i]), or by the combination of trees in
T [i, j ? 1] with tree t(?R[j]). Since each combin-
ation involves either a left-arc or a right-arc trans-
ition, we obtain a recursive relation that resolves into
a number of trees in T [i, j] bounded by 4i+j?2.
We introduce now two restrictions to the search
space of our extended algorithm that result in a huge
computational saving. For a spine s, we write N (s)
to denote the set of all nodes in s. We also let ?i,j be
the set of all pairs (ls, rs) such that T [i, j](ls, rs) 6=
+?.
? Every time a new pair (ls, rs) is created in
?[i, j], we remove from ls all nodes different
from the root that do not have gold dependents
in {r(?L[k]) | k < i}, and we remove from
rs all nodes different from the root that do not
have gold dependents in {r(?R[k]) | k > j}.
? A pair pa = (lsa, rsa) is removed from
?[i, j] if there exists a pair pb = (lsb, rsb)
in ?[i, j] with the same root node as pa and
with (lsa, rsa) 6= (lsb, rsb), such that N (lsa) ?
N (lsb), N (rsa) ? N (rsb), and T [i, j](pa) ?
T [i, j](pb).
The first restriction above reduces the size of a spine
by eliminating a node if it is irrelevant for the com-
putation of the loss of the associated tree. The
second restriction eliminates a tree ta if there is a
tree tb with smaller loss than ta, such that in the
computations of the parser tb provides exactly the
same context as ta. It is not difficult to see that
the above restrictions do not affect the correctness of
the algorithm, since they always leave in our search
space some tree that has optimal loss.
A mathematical analysis of the computational
complexity of the extended algorithm is quite in-
volved. In Figure 3, we plot the worst case size
of T [i, j] for each value of j + i ? 1, computed
over all configurations visited in the training phase
(see ?7). We see that |T [i, j]| grows linearly with
j + i? 1, leading to the same space requirements of
Algorithm 1. Empirically, training with the dynamic
127
0 10 20 30 40 500
10
20
30
40
50
i+ j ? 1
ma
xn
um
be
ro
fe
lem
en
ts
Figure 3: Empirical worst case size of T [i, j] for each
value of i + j ? 1 as measured on the Penn Treebank
corpus.
Algorithm 2 Online training for greedy transition-
based parsers
1: w? 0
2: for k iterations do
3: shuffle(corpus)
4: for sentencew and gold tree tG in corpus do
5: c? INITIAL(w)
6: while not FINAL(c) do
7: ?p ? argmax??valid(c)w ? ?(c, ?)
8: ?o ? argmax??oracle(c,tG)w??(c, ?)
9: if ?p 6? oracle(c, tG) then
10: w? w + ?(c, ?o)? ?(c, ?p)
11: ? ?
{
?p if EXPLORE
?o otherwise
12: c? ?(c)
return averaged(w)
oracle is only about 8 times slower than training with
the oracle of Sartorio et al. (2013) without exploring
incorrect configurations.
7 Training
We follow the training procedure suggested by
Goldberg and Nivre (2013), as described in Al-
gorithm 2. The algorithm performs online learning
using the averaged perceptron algorithm. A weight
vector w (initialized to 0) is used to score the valid
transitions in each configuration based on a feature
representation ?, and the highest scoring transition
?p is predicted. If the predicted transition is not
optimal according to the oracle, the weights w are
updated away from the predicted transition and to-
wards the highest scoring oracle transition ?o. The
parser then moves to the next configuration, by tak-
ing either the predicted or the oracle transition. In
the ?error exploration? mode (EXPLORE is true), the
parser follows the predicted transition, and other-
wise the parser follows the oracle transition. Note
that the error exploration mode requires the com-
pleteness property of a dynamic oracle.
We consider three training conditions: static, in
which the oracle is deterministic (returning a single
canonical transition for each configuration) and no
error exploration is performed; nondet, in which we
use a nondeterministic partial oracle (Sartorio et al.,
2013), but do not perform error exploration; and ex-
plore in which we use the dynamic oracle and per-
form error exploration. The static setup mirrors the
way greedy parsers are traditionally trained. The
nondet setup allows the training procedure to choose
which transition to take in case of spurious ambigu-
ities. The explore setup increases the configuration
space explored by the parser during training, by ex-
posing the training procedure to non-optimal con-
figurations that are likely to occur during parsing,
together with the optimal transitions to take in these
configurations. It was shown by Goldberg and Nivre
(2012; 2013) that the nondet setup outperforms the
static setup, and that the explore setup outperforms
the nondet setup.
8 Experimental Evaluation
Datasets Performance evaluation is carried out on
CoNLL 2007 multilingual dataset, as well as on the
Penn Treebank (PTB) (Marcus et al., 1993) conver-
ted to Stanford basic dependencies (De Marneffe
et al., 2006). For the CoNLL datasets we use gold
part-of-speech tags, while for the PTB we use auto-
matically assigned tags. As usual, the PTB parser is
trained on sections 2-21 and tested on section 23.
Setup We train labeled versions of the arc-stand-
ard (std) and LR-spine (lrs) parsers under the static,
nondet and explore setups, as defined in ?7. In
the nondet setup we use a nondeterministic partial
oracle and in the explore setup we use the non-
deterministic complete oracles we present in this pa-
per. In the static setup we resolve oracle ambiguities
and choose a canonic transition sequence by attach-
ing arcs as soon as possible. In the explore setup,
128
parser:train Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish PTB
UAS
std:static 81.39 75.37 90.32 85.17 78.90 85.69 79.90 77.67 82.98 77.04 89.86
std:nondet 81.33 74.82 90.75 84.80 79.92 86.89 81.19 77.51 84.15 76.85 90.56
std:explore 82.56 74.39 90.95 85.65 81.01 87.70 81.85 78.72 84.37 77.21 90.92
lrs:static 81.67 76.07 91.47 84.24 77.93 86.36 79.43 76.56 84.64 77.00 90.33
lrs:nondet 83.14 75.53 91.31 84.98 80.03 88.38 81.12 76.98 85.29 77.63 91.18
lrs:explore 84.54 75.82 91.92 86.72 81.19 89.37 81.78 77.48 85.38 78.61 91.77
LAS
std:static 71.93 65.64 84.90 80.35 71.39 84.60 72.25 67.66 78.77 65.90 87.56
std:nondet 71.09 65.28 85.36 80.06 71.47 85.91 73.40 67.77 80.06 65.81 88.30
std:explore 72.89 65.27 85.82 81.28 72.92 86.79 74.22 69.57 80.25 66.71 88.72
lrs:static 72.24 66.21 86.02 79.36 70.48 85.38 72.36 66.79 80.38 66.02 88.07
lrs:nondet 72.94 65.66 86.03 80.47 71.32 87.45 73.09 67.70 81.32 67.02 88.96
lrs:explore 74.54 66.91 86.83 82.38 72.72 88.44 74.04 68.76 81.50 68.06 89.53
Table 1: Scores on the CoNLL 2007 dataset (including punctuation, gold parts of speech) and on Penn Tree Bank
(excluding punctuation, predicted parts of speech). Label ?std? refers to the arc-standard parser, and ?lrs? refers to the
LR-spine parser. Each number is an average over 5 runs with different randomization seeds.
from the first round of training onward, we always
follow the predicted transition (EXPLORE is true).
For all languages, we deal with non-projectivity by
skipping the non-projective sentences during train-
ing but not during test. For each parsing system,
we use the same feature templates across all lan-
guages.1 The arc-standard models are trained for 15
iterations and the LR-spine models for 30 iterations,
after which all the models seem to have converged.
Results In Table 1 we report the labeled (LAS)
and unlabeled (UAS) attachment scores. As expec-
ted, the LR-spine parsers outperform the arc-stand-
ard parsers trained under the same setup. Training
with the dynamic oracles is also beneficial: despite
the arguable complexity of our proposed oracles, the
trends are consistent with those reported by Gold-
berg and Nivre (2012; 2013). For the arc-standard
model we observe that the move from a static to
a nondeterministic oracle during training improves
the accuracy for most of languages. Making use of
the completeness of the dynamic oracle and moving
to the error exploring setup further improve results.
The only exceptions are Basque, that has a small
dataset with more than 20% of non-projective sen-
tences, and Chinese. For Chinese we observe a re-
duction of accuracy in the nondet setup, but an in-
crease in the explore setup.
For the LR-spine parser we observe a practically
constant increase of performance by moving from
1Our complete code, together with the description of the fea-
ture templates, is available on the second author?s homepage.
the static to the nondeterministic and then to the er-
ror exploring setups.
9 Conclusions
We presented dynamic oracles, based on dynamic
programming, for the arc-standard and the LR-
spine parsers. Empirical evaluation on 10 languages
showed that, despite the apparent complexity of the
oracle calculation procedure, the oracles are still
learnable, in the sense that using these oracles in
the error exploration training algorithm presented in
(Goldberg and Nivre, 2012) considerably improves
the accuracy of the trained parsers.
Our algorithm computes a dynamic oracle using
dynamic programming to explore a forest of depend-
ency trees that can be reached from a given parser
configuration. For the arc-standard parser, the com-
putation takes cubic time in the size of the largest of
the left and right input stacks. Dynamic program-
ming for the simulation of arc-standard parsers have
been proposed by Kuhlmann et al. (2011). That al-
gorithm could be adapted to compute minimum loss
for a given configuration, but the running time is
O(n4), n the size of the input string: besides being
asymptotically slower by one order of magnitude, in
practice n is also larger than the stack size above.
Acknowledgments We wish to thank the anonym-
ous reviewers. In particular, we are indebted to one
of them for two important technical remarks. The
third author has been partially supported by MIUR
under project PRIN No. 2010LYA9RH 006.
129
References
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC), volume 6,
pages 449?454.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic or-
acle for arc-eager dependency parsing. In Proc. of the
24th COLING, Mumbai, India.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the association for Computational
Linguistics, 1.
John E. Hopcroft and Jeffrey D. Ullman. 1979. Intro-
duction to Automata Theory, Languages and Compu-
tation. Addison-Wesley, Reading, MA.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, July.
Marco Kuhlmann, Carlos Go?mez-Rodr??guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Techno-
logies, pages 673?682, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
Eighth International Workshop on Parsing Technolo-
gies (IWPT), pages 149?160, Nancy, France.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In Workshop on Incremental Pars-
ing: Bringing Engineering and Cognition Together,
pages 50?57, Barcelona, Spain.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguist-
ics, 34(4):513?553.
Francesco Sartorio, Giorgio Satta, and Joakim Nivre.
2013. A transition-based dependency parser using a
dynamic parsing strategy. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 135?144,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of EMNLP.
130
Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 19?27,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Parsing and Translation Algorithms
Based on Weighted Extended Tree Transducers
Andreas Maletti?
Departament de Filologies Roma`niques
Universitat Rovira i Virgili
Tarragona, Spain
Giorgio Satta
Department of Information Engineering
University of Padua
Padua, Italy
Abstract
This paper proposes a uniform frame-
work for the development of parsing and
translation algorithms for weighted ex-
tended (top-down) tree transducers and in-
put strings. The asymptotic time complex-
ity of these algorithms can be improved
in practice by exploiting an algorithm for
rule factorization in the above transducers.
1 Introduction
In the field of statistical machine translation, con-
siderable interest has recently been shown for
translation models based on weighted tree trans-
ducers. In this paper we consider the so-called
weighted extended (top-down) tree transducers
(WXTTs for short). WXTTs have been proposed
by Graehl and Knight (2004) and Knight (2007)
and are rooted in similar devices introduced ear-
lier in the formal language literature (Arnold and
Dauchet, 1982).
WXTTs have enough expressivity to represent
hierarchical syntactic analyses for natural lan-
guage sentences and can directly model most of
the elementary operations that rule the process
of translation between natural languages (Knight,
2007). Furthermore, the use of weights and in-
ternal states allows the encoding of statistical pa-
rameters that have recently been shown to be ex-
tremely useful in discriminating likely translations
from less plausible ones.
For an WXTT M , the parsing problem is tradi-
tionally defined for a pair of trees t and u and re-
quires as output some representation of the set of
all computations ofM that map t into u. Similarly,
the translation problem forM is defined for an in-
put tree t and requires as output some representa-
tion of the set of all computations of M mapping t
?Financially supported by the Ministerio de Educacio?n y
Ciencia (MEC) grant JDCI-2007-760.
into any other tree. When we deal with natural
language processing applications, however, pars-
ing and translation are most often represented on
the basis of input strings rather than trees. Some
tricks are then applied to map the problem back
to the case of input trees. As an example in the
context of machine translation, let w be some in-
put string to be translated. One can intermediately
construct a tree automaton Mw that recognizes the
set of all possible trees that have w as yield with
internal nodes from the input alphabet of M . This
automaton Mw is further transformed into a tree
transducer implementing a partial identity trans-
lation. This transducer is then composed with M
(relational composition) to obtain a transducer that
represents all translations of w. This is usually
called the ?cascaded? approach.
In contrast with the cascaded approach above,
which may be rather inefficient, we investigate a
more direct technique for both parsing and transla-
tion of strings based on WXTTs. We do this by ex-
tending to WXTTs the well-known BAR-HILLEL
construction defined for context-free grammars
(Bar-Hillel et al, 1964) and for weighted context-
free grammars (Nederhof and Satta, 2003). We
then derive computational complexity results for
parsing and translation of input strings on the ba-
sis of WXTTs. Finally, we develop a novel fac-
torization algorithm for WXTTs that, in practical
applications, can reduce the asymptotic complex-
ity for such problems.
2 Preliminary definitions
Let ? be an associative binary operation on a set S.
If S contains an element 1 such that 1?s = s = s?1
for every s ? S, then (S, ?, 1) is a monoid. Such
a monoid (S, ?, 1) is commutative if the identity
s1 ?s2 = s2 ?s1 holds for all s1, s2 ? S. A commu-
tative semiring (S,+, ?, 0, 1) is an algebraic struc-
ture such that:
? (S,+, 0) and (S, ?, 1) are commutative
19
monoids,
? ? distributes over + (from both sides), and
? s ? 0 = 0 = 0 ? s for every s ? S.
From now on, let (S,+, ?, 0, 1) be a com-
mutative semiring. An alphabet is a finite
set of symbols. A weighted string automa-
ton [WSA] (Schu?tzenberger, 1961; Eilenberg,
1974) is a system N = (P,?, J, ?, F ) where
? P and ? are alphabets of states and input
symbols, respectively,
? J, F : P ? S assign initial and final weights,
respectively, and
? ? : P ? ?? P ? S assigns a weight to each
transition.
The transition weight mapping ? can be under-
stood as square matrices ?(?, ?, ?) ? SP?P for ev-
ery ? ? ?. The WSA N is deterministic if
? J(p) 6= 0 for at most one p ? P and
? for every p ? P and ? ? ? there exists at
most one p? ? P such that ?(p, ?, p?) 6= 0.
We now proceed with the semantics of N . We
will define the initial algebra semantics here; al-
ternative, equivalent definitions of the semantics
exist (Sakarovitch, 2009). Let w ? ?? be an in-
put string, ? ? ?, and p, p? ? P be two states.
We extend ? to a mapping h? : P ? ?? ? P ? S
recursively as follows:
h?(p, ?, p
?) =
{
1 if p = p?
0 otherwise
h?(p, ?w, p
?) =
?
p???P
?(p, ?, p??) ? h?(p
??, w, p?) .
Consequently,
h?(p, uw, p
?) =
?
p???P
h?(p, u, p
??) ? h?(p
??, w, p?)
for all p, p? ? P and u,w ? ??. Then the matrix
h?(?, ?1 ? ? ? ?k, ?) equals ?(?, ?1, ?) ? . . . ? ?(?, ?k, ?).
Thus, if the semiring operations can be performed
in constant time and access to ?(p, ?, q) is in con-
stant time for every p, q ? P , then for every
w ? ?? we can compute the matrix h?(?, w, ?) in
time O(|w| ? |P |3) because it can be computed by
|w| ? 1 matrix multiplications.
The WSA N computes the map N : ?? ? S,
which is defined for every w ? ?? by1
N(w) =
?
p,p??P
J(p) ? h?(p, w, p
?) ? F (p?) .
1We overload the symbol N to denote both the WSA and
its recognized mapping. However, the intended meaning will
always be clear from the context.
Since we will also consider individual runs,
let us recall the run semantics as well. Let
w = ?1 ? ? ? ?k ? ?? be an input string of length k.
Then any mapping r : [0, k] ? P is a run of N
on w, where [0, k] denotes the set of integers be-
tween (inclusive) 0 and k. A run can be under-
stood as a vector of states and thus we some-
times write ri instead of r(i). The weight of
such a run r, denoted by wtN (r), is defined by
wtN (r) =
?k
i=1 ?(ri?1, ?i, ri). Then
h?(p, w, p
?) =
?
r : [0,k]?P
r0=p,rk=p?
wtN (r)
for every p, p? ? P and w ? ??.
3 Weighted extended tree transducers
Next, we move to tree languages, for which we
need to introduce some additional notation. Let
? be a ranked alphabet, that is, an alphabet
whose symbols have a unique associated arity. We
write ?k to denote the set of all k-ary symbols
in ?. We use the special nullary symbol e ? ?0 to
syntactically represent the empty string ?. The set
of ?-trees indexed by a set V , denoted by T?(V ),
is the smallest set satisfying both of the following
conditions:
? for every v ? V , the single node labeled v,
written v, is a tree of T?(V ),
? for every ? ? ?k and t1, . . . , tk ? T?(V ),
the tree with a root node labeled ? and
trees t1, . . . , tk as its k children, written
?(t1, . . . , tk), belongs to T?(V ).
Throughout this paper we sometimes write ?() as
just ?. In the following, let t ? T?(V ). The set
of positions Pos(t) ? N? of a tree t ? T?(V ) is
recursively defined as follows:
Pos(v) = {?}
Pos(t) = {?} ? {iw | 1 ? i ? k,w ? Pos(ti)}
for every v ? V , ? ? ?k, and t1, . . . , tk ? T?(V )
where t = ?(t1, . . . , tk). The label of t at posi-
tion w ? Pos(t) is denoted by t(w). The size of
the tree t ? T? is defined as |t| = |Pos(t)|. For
every w ? Pos(t) the subtree of t that is rooted
at w is denoted by subt(w); i.e.,
subt(?) = t
sub?(t1,...,tk)(iw) = subti(w)
20
for every ? ? ?k, t1, . . . , tk ? T?(V ), 1 ? i ? k,
and w ? Pos(ti). Finally, the set of vari-
ables var(t) is given by
var(t) = {v ? V | ?w ? Pos(t) : t(w) = v} .
If for every v ? var(t) there exists exactly one
w ? Pos(t) such that t(w) = v, then t is linear.
We use the fixed sets X = {xi | i ? 1} and
Y = {yi,j | 1 ? i < j} of formal variables
and the subsets Xk = {xi | 1 ? i ? k} and
Yk = {yi,j | 1 ? i < j ? k} for every k ? 0.
Note thatX0 = ?. For everyH ? ?0?X?Y , the
H-yield of t is recursively defined by ydH(t) = t
if t ? H \ {e}, ydH(t) = ydH(t1) ? ? ? ydH(tk) if
t = ?(t1, . . . , tk) with ? ? ?k and k ? 1, and
ydH(t) = ? otherwise. If H = ?0 ?X ? Y , then
we also omit the index and just write yd(t).
Let l ? T?(V ) and ? : V ? T?(V ). Then
l? denotes the result obtained from l by replacing
every occurrence of v ? V by ?(v). The k-fold
application is denoted by l?k. If l?k = l?k+1 for
some k ? 0, then we denote l?k by l??. In addi-
tion, if V = Xk, then we write l[?(x1), . . . , ?(xk)]
instead of l?. We write C?(Xk) for the subset
of those trees of T?(Xk) such that every vari-
able of x ? Xk occurs exactly once in it. Given
t ? T?(X), we write dec(t) for the set
{
(l, t1, . . . , tk)
?
?
?
l ? C?(Xk), l[t1, . . . , tk] = t,
t1, . . . , tk ? T?(X)
}
A (linear and nondeleting) weighted extended
(top-down) tree transducer [WXTT] (Arnold and
Dauchet, 1975; Arnold and Dauchet, 1976; Lilin,
1981; Arnold and Dauchet, 1982; Maletti et al,
2009) is a system M = (Q,?,?, I, R) where
? Q is an alphabet of states,
? ? and ? are ranked alphabets of input and
output symbols, respectively,
? I : Q? S assigns initial weights, and
? R is a finite set of rules of the form
(q, l)
s
? (q1 ? ? ? qk, r) with q, q1, . . . , qk ? Q,
l ? C?(Xk) and r ? C?(Xk), and s ? S
such that {l, r} 6? X .
Let us discuss the final restriction imposed on
the rules of a WXTT. Essentially, it disallows rules
of the form (q, x1)
s
? (q?, x1) with q, q? ? Q and
s ? S. Such pure epsilon rules only change the
state and charge a cost. However, they can yield
infinite derivations (and with it infinite products
and sums) and are not needed in our applications.
The WXTT M is standard if ydX(r) = x1 ? ? ?xk
for every (q, l)
s
? (q1 ? ? ? qk, r) ? R. This restric-
tion enforces that the order of the variables is fixed
on the right-hand side r, but since the order is ar-
bitrary in the left-hand side l (and the names of the
variables are inconsequential), it can be achieved
easily without loss of generality. If there are sev-
eral rules that differ only in the naming of the vari-
ables, then their weights should be added to obtain
a single standard rule. To keep the presentation
simple, we also construct nonstandard WXTTs in
the sequel. However, we implicitly assume that
those are converted into standard WXTTs.
The semantics of a standard WXTT is in-
spired by the initial-algebra semantics for classi-
cal weighted top-down and bottom-up tree trans-
ducers (Fu?lo?p and Vogler, 2009) [also called top-
down and bottom-up tree series transducers by En-
gelfriet et al (2002)]. Note that our semantics
is equivalent to the classical term rewriting se-
mantics, which is presented by Graehl and Knight
(2004) and Graehl et al (2008), for example. In
fact, we will present an equivalent semantics based
on runs later. Let M = (Q,?,?, I, R) be a
WXTT. We present a definition that is more gen-
eral than immediately necessary, but the general-
ization will be useful later on. For every n ? N,
p1, . . . , pn ? Q, and L ? R, we define the
mapping hp1???pnL : T?(Xn) ? T?(Xn) ? S
Q by
hp1???pnL (xi, xi)pi = 1 for every 1 ? i ? n and
hp1???pnL (t, u)q
=
?
(l,t1,...,tk)?dec(t)
(r,u1,...,uk)?dec(u)
(q,l)
s
?(q1???qk,r)?L
s ?
k?
i=1
hp1???pnL (ti, ui)qi (1)
for all remaining t ? T?(Xn), u ? T?(Xn), and
q ? Q. Note that for each nonzero summand in (1)
one of the decompositions dec(t) and dec(u) must
be proper (i.e., either l /? X or r /? X). This
immediately yields that the sum is finite and the
recursion well-defined. The transformation com-
puted by M , also denoted by M , is the map-
ping M : T? ? T? ? S, which is defined by
M(t, u) =
?
q?Q I(q)?hR(t, u)q for every t ? T?
and u ? T?.
Let us also introduce a run semantics for the
WXTT (Q,?,?, I, R). The rank of a rule
? = (q, l)
s
? (q1 ? ? ? qk, r) ? R, denoted by rk(?),
is rk(?) = k. This turns R into a ranked alphabet.
The input state of ? is in(?) = q, the ith output
state is outi(?) = qi for every 1 ? i ? k, and
21
the weight of ? is wt(?) = s. A tree r ? TR(X)
is called run if in(r(wi)) = outi(r(w)) for every
wi ? Pos(r) and 1 ? i ? rk(r(w)) such that
r(wi) ? R. The weight of a run r ? TR(X) is
wt(r) =
?
w?Pos(r),r(w)?R
wt(r(w)) .
The evaluation mappings pi1 : TR(X) ? T?(X)
and pi2 : TR(X) ? T?(X) are defined for every
x ? X , ? = (q, l)
s
? (q1 ? ? ? qk, r) ? R, and
r1, . . . , rk ? TR(X) by pi1(x) = x, pi2(x) = x,
and
pi1(?(r1, . . . , rk)) = l[pi1(r1), . . . , pi1(rk)]
pi2(?(r1, . . . , rk)) = r[pi2(r1), . . . , pi2(rk)] .
We obtain the weighted tree transformation for ev-
ery t ? T? and u ? T? as follows2
M(t, u) =
?
run r?TR
t=pi1(r),u=pi2(r)
I(in(r(?))) ? wt(r) .
This approach is also called the bimorphism ap-
proach (Arnold and Dauchet, 1982) to tree trans-
formations.
4 Input and output restrictions of WXTT
In this section we will discuss the BAR-HILLEL
construction for the input and the output part of a
WXTT M . This construction essentially restricts
the input or output of the WXTT M to the string
language recognized by a WSA N . Contrary to
(direct or inverse) application, this construction
is supposed to yield another WXTT. More pre-
cisely, the constructed WXTT should assign to
each translation (t, u) the weight assigned to it
by M multiplied by the weight assigned by N
to the yield of t (or u if the output is restricted).
Since our WXTTs are symmetric, we will actu-
ally only need one construction. Let us quickly
establish the mentioned symmetry statement. Es-
sentially we just have to exchange left- and right-
hand sides and redistribute the states in those left-
and right-hand sides accordingly.
From now on, let M = (Q,?,?, I, R) be a
WXTT.
Theorem 1. There exists a WXTT M ? such that
M ?(u, t) = M(t, u) for every t ? T? and u ? T?.
2We immediately also use M for the run semantics be-
cause the two semantics trivially coincide.
Proof. Let M ? = (Q,?,?, I, R?) be the WXTT
such that
R? = {(q, r)
s
? (w, l) | (q, l)
s
? (w, r) ? R} .
It should be clear that M ?(u, t) = M(t, u) for ev-
ery t ? T? and u ? T?.
With the symmetry established, we now only
need to present the BAR-HILLEL construction for
either the input or output side. Without loss of
generality, let us assume that M is standard. We
then choose the output side here because the order
of variables is fixed in it. Note that we sometimes
use the angled parentheses ??? and ??? instead of
parentheses for clarity.
Definition 2. Let N = (P,?, J, ?, F ) be a WSA
with ? = ?0 \ {e}. We construct the output prod-
uct Prod(M,N) = (P?Q?P,?,?, I ?, R?) such
that
? I ?(?p, q, p??) = J(p) ? I(q) ? F (p?) for every
p, p? ? P and q ? Q,
? for every rule (q, l)
s
? (q1 ? ? ? qk, r) ? R and
every p0, . . . , pk, p?0, . . . , p
?
k ? P , let
(q?, l)
s?s0?...?sk?????? (q?1 ? ? ? q
?
k, r) ? R
?
where
? q? = ?p0, q, p?k?,
? q?i = ?p
?
i?1, qi, pi? for every 1 ? i ? k,
? yd(r) = w0x1w1 ? ? ?wk?1xkwk with
w0, . . . , wk ? ??, and
? si = h?(pi, wi, p?i) for every 0 ? i ? k.
Let ? = (q, l)
s
? (q1 ? ? ? qk, r) ? R. The
size of ? is |?| = |l| + |r|. The size and
rank of the WXTT M are |M | =
?
??R|?|
and rk(M) = max??R rk(?), respectively. Fi-
nally, the maximal output yield length of M , de-
noted by len(M), is the maximal length of yd(r)
for all rules (q, l)
s
? (q1 ? ? ? qk, r) ? R.
The size and rank of Prod(M,N) are in
O(|M | ? |P |2 rk(M)+2) and rk(M), respec-
tively. We can compute Prod(M,N) in time
O(|R| ? len(M) ? |P |2 rk(M)+5). If N is de-
terministic, then the size of Prod(M,N) is
in O(|M | ? |P |rk(M)+1) and the required time is
inO(|R| ? len(M) ? |P |rk(M)+1). Next, let us prove
that our BAR-HILLEL construction is actually cor-
rect.
Theorem 3. Let M and N be as in Defini-
tion 2, and let M ? = Prod(M,N). Then
M ?(t, u) = M(t, u) ?N(yd(u)) for every t ? T?
and u ? T?.
22
Proof. Let M ? = (Q?,?,?, I ?, R?). First, a sim-
ple proof shows that
hR?(t, u)?p,q,p?? = hR(t, u)q ? h?(p, yd(u), p
?)
for every t ? T?, u ? T?, q ? Q, and p, p? ? P .
Now we can prove the main statement as follows:
M ?(t, u)
=
?
q??Q?
I ?(q?) ? hR?(t, u)q?
=
?
p,p??P
q?Q
I ?(?p, q, p??) ? hR(t, u)q ? h?(p, yd(u), p
?)
= M(t, u) ?N(yd(u))
for every t ? T? and u ? T?.
Note that the typical property of many BAR-
HILLEL constructions, namely that a run of M
and a run of N uniquely determine a run
of Prod(M,N) and vice versa, does not hold for
our construction. In fact, a run of M and a run
of N uniquely determine a run of Prod(M,N),
but the converse does not hold. We could modify
the construction to enable this property at the ex-
pense of an exponential increase in the number of
states of Prod(M,N). However, since those re-
lations are important for our applications, we ex-
plore the relation between runs in some detail here.
To simplify the discussion, we assume, without
loss of generality, that M is standard and s = s?
for every two rules (q, l)
s
? (w, r) ? R and
(q, l)
s?
? (w, r) ? R. Moreover, we assume the
symbols of Definition 2. For every r? ? TR?(X),
we let base(r?) denote the run obtained from r? by
replacing each symbol
(q?, l)
s?s0?...?sk?????? (q?1 ? ? ? q
?
k, r)
by just (q, l)
s
? (q1 ? ? ? qk, r) ? R. Thus, we re-
place a rule (which is a symbol) of R? by the un-
derlying rule of R. We start with a general lemma,
which we believe to be self-evident.
Lemma 4. Let r? ? TR? and n = |yd(pi2(r?))|.
Then wtM ?(r?) = wtM (base(r?))?
?
r?R?? wtN (r)
where R?? is a nonempty subset of
{r : [0, n]? P | in(r?(?)) = ?r0, q, rn?}.
Let us assume that N is trim (i.e., all states are
reachable and co-reachable) and unambiguous. In
this case, for every ?1 ? ? ? ?k ? ?? and p, p? ? P
there is at most one successful run r : [0, k] ? P
such that
? ?(ri?1, ?i, ri) 6= 0 for every 1 ? i ? k, and
? r0 = p and rk = p?.
This immediately yields the following corollary.
Corollary 5 (of Lemma 4). Let N be trim and
unambiguous. For every r? ? TR? we have
wtM ?(r
?) = wtM (base(r
?)) ? wtN (r)
for some r : [0, n]? P with n = |yd(pi2(r?))|.
We now turn to applications of the product con-
struction. We first consider the translation prob-
lem for an input string w and a WXTTM . We can
represent w as a trim and unambiguous WSA Nw
that recognizes the language {w} with weight
of 1 on each transition (which amounts to ignor-
ing the weight contribution of Nw). Then the in-
put product transducer Mw = Prod(Nw,M) pro-
vides a compact representation of the set of all
computations of M that translate the string w.
From Corollary 5 we have that the weights of
these computations are also preserved. Thus,
Mw(T? ? T?) =
?
(t,u)?T??T?
Mw(t, u) is the
weight of the set of string translations of w.
As usual in natural language processing ap-
plications, we can exploit appropriate semirings
and compute several useful statistical parameters
through Mw(T? ? T?), as for instance the high-
est weight of a computation, the inside probabil-
ity and the rule expectations; see (Li and Eisner,
2009) for further discussion.
One could also construct in linear time the range
tree automaton for Mw, which can be interpreted
as a parsing forest with all the weighted trees as-
signed to translations of w under M . If we fur-
ther assume thatM is unambiguous, thenMw will
also have this property, and we can apply standard
techniques to extract from Mw the highest score
computation. In machine translation applications,
the unambiguity assumption is usually met, and
avoids the so-called ?spurious? ambiguity, that is,
having several computations for an individual pair
of trees.
The parsing problem for input strings w and u
can be treated in a similar way, by restricting M
both to the left and to the right.
5 Rule factorization
As already discussed, the time complexity of the
product construction is an exponential function
of the rank of the transducer. Unfortunately,
it is not possible in the general case to cast a
23
WXTT into a normal form such that the rank is
bounded by some constant. This is also expected
from the fact that the translation problem for sub-
classes of WXTTs such as synchronous context-
free grammars is NP-hard (Satta and Peserico,
2005). Nonetheless, there are cases in which a
rank reduction is possible, which might result in
an improvement of the asymptotical run-time of
our construction.
Following the above line, we present here a
linear time algorithm for reducing the rank of a
WXTT under certain conditions. Similar algo-
rithms for tree-based transformation devices have
been discussed in the literature. Nesson et al
(2008) consider synchronous tree adjoining gram-
mars; their algorithm is conceptually very sim-
ilar to ours, but computationally more demand-
ing due to the treatment of adjunction. Follow-
ing that work, we also demand here that the new
WXTT ?preserves? the recursive structure of the
input WXTT, as formalized below. Galley et al
(2004) algorithm also behaves in linear time, but
deals with the different problem of tree to string
translation. Rank reduction algorithms for string-
based translation devices have also been discussed
by Zhang et al (2006) and Gildea et al (2006).
Recall that M = (Q,?,?, I, R) is a standard
WXTT. Let M ? = (Q?,?,?, I ?, R?) be a WXTT
with Q ? Q?.3 Then M ? is a structure-preserving
factorization of M if
? I ?(q) = I(q) for every q ? Q and I ?(q) = 0
otherwise, and
? hp1???pnR? (t, u)q = h
p1???pn
R (t, u)q for every
q, p1, . . . , pn ? Q, t ? T?(Xn), and
u ? T?(Xn).
In particular, we have hR?(t, u)q = hR(t, u)q for
n = 0. Consequently, M ? and M are equivalent
because
M ?(t, u) =
?
q?Q?
I ?(q) ? hR?(t, u)q
=
?
q?Q
I(q) ? hR(t, u)q = M(t, u) .
Note that the relation ?is structure-preserving fac-
torization of? is reflexive and transitive, and thus, a
pre-order. Moreover, in a ring (actually, additively
cancellative semirings are sufficient) it is also anti-
symmetric, and consequently, a partial order.
3Actually, an injective mapping Q ? Q? would be suffi-
cient, but since the naming of the states is arbitrary, we im-
mediately identify according to the injective mapping.
Informally, a structure-preserving factorization
ofM consists in a set of new rules that can be com-
posed to provide the original rules and preserve
their weights. We develop an algorithm for finding
a structure-preserving factorization by decompos-
ing each rule as much as possible. The algorithm
can then be iterated for all the rules in the WXTT.
The idea underlying our algorithm is very simple.
Let ? = (q, l)
s
? (q1 ? ? ? qk, r) ? R be an origi-
nal rule. We look for subtrees l? and r? of l and r,
respectively, such that var(l?) = var(r?). The con-
dition that var(l?) = var(r?) is derived from the
fact that hq1???qkR (l
?, r?)q = 0 if var(l?) 6= var(r?).
We then split ? into two new rules by ?excis-
ing? subtrees l? and r? from l and r, respectively.
In the remaining trees the ?excised? trees are re-
placed with some fresh variable. The tricky part
is the efficient computation of the pairs (wl, wr),
since in the worst case the number of such pairs
is in ?(|l| ? |r|), and na??ve testing of the condition
var(l?) = var(r?) takes time O(rk(?)).
Let us start with the formal development. Recall
the doubly-indexed set Y = {yi,j | 1 ? i < j}.
Intuitively speaking, the variable yi,j will
represent the set {xi, . . . , xj}. With this
intuition in mind, we define the mapping
vars : T?(X ? Y )? N3? as follows:
vars(xi) = (i, i, 1)
vars(yi,j) = (i, j, j ? i+ 1)
and vars(?(t1, . . . , tk)) is
(
k
min
`=1
vars(t`)1,
k
max
`=1
vars(t`)2,
k?
`=1
vars(t`)3)
for every i, j ? N with i < j, ? ? ?k, and
t1, . . . , tk ? T?(X ? Y ). Clearly, vars(t) can
be computed in time O(|t|), which also in-
cludes the computation of vars(u) for every sub-
tree u of t. In addition, vars(t)3 = |var(t)|
for all linear t ? T?(X). Finally, if
t ? T?(X), then vars(t)1 and vars(t)2 are the
minimal and maximal index i ? N such that
xi ? var(t), respectively (they are ? and 0,
respectively, if var(t) = ?). For better read-
ability, we use minvar(t) and maxvar(t) for
vars(t)1 and vars(t)2, respectively.
Let ? = (q, l)
s
? (q1 ? ? ? qk, r) ? R be an origi-
nal rule. In the following, we will use minvar(t),
maxvar(t), and |var(t)| freely for all subtrees t
of l and r and assume that they are precomputed,
24
which can be done in time O(|?|). Moreover, we
will freely use the test ?var(t) = var(u)? for sub-
trees t and u of l and r, respectively. This test can
be performed in constant time [disregarding the
time needed to precompute vars(t) and vars(u)]
by the equivalent test
? minvar(t) = minvar(u),
? maxvar(t) = maxvar(u),
? |var(t)| = maxvar(t)?minvar(t) + 1, and
? |var(u)| = maxvar(u)?minvar(u) + 1.
Our factorization algorithm is presented in Al-
gorithm 1. Its first two parameters hold the left-
and right-hand side (l, r), which are to be decom-
posed. The third and fourth parameter should ini-
tially be x1. To simplify the algorithm, we assume
that it is only called with left- and right-hand sides
that (i) contain the same variables and (ii) contain
at least two variables. These conditions are en-
sured by the algorithm for the recursive calls. The
algorithm returns a decomposition of (l, r) in the
form of a set D ? T?(X ? Y ) ? T?(X ? Y )
such that var(l?) = var(r?) for every (l?, r?) ? D.
Moreover, all such l? and r? are linear. Finally, the
pairs in D can be composed (by means of point-
wise substitution at the variables of Y ) to form the
original pair (l, r).
Before we move on to formal properties of Al-
gorithm 1, let us illustrate its execution on an ex-
ample.
Example 6. We work with the left-hand side
l = ?(x1, ?(x3, x2)) and the right-hand side
r = ?(?(x1, ?(?(x2, x3)))). Then |var(l)| ? 2
and var(l) = var(r). Let us trace the call
DECOMPOSE(l, r, x1, x1). The condition in line 1
is clearly false, so we proceed with line 3. The
condition is true for i = 1, so we continue with
DECOMPOSE(l, ?(x1, ?(?(x2, x3))), x1, ?(x1)).
This time neither the condition in line 1 nor the
condition in line 3 are true. In line 6, j is set to 1
and we initialize r?1 = x1 and r
?
2 = ?(?(x2, x3)).
Moreover, the array h is initialized to h(1) = 1,
h(2) = 2, and h(3) = 2. Now let us discuss the
main loop starting in line 12 in more detail. First,
we consider i = 1. Since l1 = x1, the condition in
line 13 is fulfilled and we set l?1 = x1 and proceed
with the next iteration (i = 2). This time the condi-
tion of line 13 is false because l2 = ?(x3, x2) and
var(l2) = var(rh(2)) = var(r2) = {x2, x3}. Con-
sequently, j is set to 2 and l?2 = r
?
2 = y2,3. Next,
DECOMPOSE(?(x3, x2), ?(?(x2, x3)), x1, x1) is
processed. Let us suppose that it generates the
set D. Then we return
D ? {(?(x1, y2,3), ?(?(x1, y2,3)))} .
Finally, let us quickly discuss how the set D
is obtained. Since the condition in line 3 is
true, we have to evaluate the recursive call
DECOMPOSE(?(x3, x2), ?(x2, x3), x1, ?(x1)).
Now, j = 2, h(2) = 1, and h(3) = 2.
Moreover, r?1 = x2 and r
?
2 = x3. In the
main loop starting in line 12, the condition of
line 13 is always fulfilled, which yields that
l?1 = x3 and l
?
2 = x2. Thus, we return
{(?(x3, x2), ?(?(x2, x3)))}, which is exactly the
input because decomposition completely failed.
Thus, the overall decomposition of l and r is
{(?(x1, y2,3), ?(?(x1, y2,3))),
(?(x3, x2), ?(?(x2, x3)))} ,
which, when the second pair is substituted (point-
wise) for y2,3 in the first pair, yields exactly (l, r).
Informally, the rules are obtained as follows
fromD. If all variables occur in a pair (l?, r?) ? D,
then the left-hand side is assigned to the original
input state. Furthermore, for every variable yi,j we
introduce a new fresh state qi,j whereas the vari-
able xi is associated to qi. In this way, we deter-
mine the states in the right-hand side.
Formally, let ? = (q, l)
s
? (q1 ? ? ? qk, r)
be the original rule and D be the result of
DECOMPOSE(l, r, x1, x1) of Algorithm 1. In ad-
dition, for every 1 ? i < j ? k, let q?,i,j be a new
state such that q?,1,k = q. Let
Q?? = {q, q1, . . . , qk} ? {q?,i,j | 1 ? i < j ? k} .
Then for every (l?, r?) ? D we obtain the rule
(q?,minvar(r?),maxvar(r?), l
?)
s?
? (p1 ? ? ? pn, r
?)
where ydX?Y (r
?) = z1 ? ? ? zn,
s? =
{
s if vars(r?)3 = k
1 otherwise
q?` =
{
qj if z` = xj
q?,i,j if z` = yi,j
for every 1 ? ` ? n. The rules obtained in this
fashion are collected in R??.
4 The WXTT dec(M)
is dec(M) = (Q?,?,?, I ?, R?) where
4Those rules need to be normalized to obtain a standard
WXTT.
25
Algorithm 1 DECOMPOSE(l, r, l?, r?) computing the decomposition of linear l ? T?(Xk) and
r ? T?(Xk) with var(l) = var(r) and |var(l)| ? 2.
if l = ?(l1, . . . , lm) and there exists i ? N is such that var(li) = var(l) then
2: return DECOMPOSE(li, r, l?[?(l1, . . . , li?1, x1, li+1, . . . , lm)], r?[x1])
if r = ?(r1, . . . , rn) and there exists i ? N is such that var(ri) = var(r) then
4: return DECOMPOSE(l, ri, l?[x1], r?[?(r1, . . . , ri?1, x1, ri+1, . . . , rn)])
let l = ?(l1, . . . , lm) and r = ?(r1, . . . , rn)
6: j = minvar(r)
for all 1 ? i ? n do
8: r?i = ri
while j ? maxvar(ri) do
10: h(j) = i; j = j + 1
D = ?
12: for all 1 ? i ? m do
if |var(li)| ? 1 or var(li) 6= var(rh(minvar(li))) then
14: l?i = li
else
16: j = h(minvar(li))
l?i = r
?
j = yminvar(li),maxvar(li)
18: D = D ? DECOMPOSE(li, rj , x1, x1)
return D ? {(l?[?(l?1, . . . , l
?
m)], r
?[?(r?1, . . . , r
?
n)])}
? Q? = Q ?
?
??R,rk(?)?2Q
?
?,
? I ?(q) = I(q) for every q ? Q and I ?(q) = 0
otherwise, and
? R? is
{? ? R | rk(?) < 2} ?
?
??R,rk(?)?2
R?? .
To measure the success of the factorization, we
introduce the following notion. The degree of M ,
denoted by deg(M), is the minimal rank of all
structure-preserving factorizations M ? of M ; i.e.,
deg(M) = min
M ? a structure-preserving
factorization of M
rk(M ?) .
Then the goal of this section is the efficient com-
putation of a structure-preserving factorizationM ?
of M such that rk(M ?) = deg(M).
Theorem 7. The WXTT dec(M) is a structure-
preserving factorization of M such that
rk(dec(M)) = deg(M). Moreover, dec(M) can
be computed in time O(|M |).
Proof. Let us only discuss the run-time complex-
ity shortly. Clearly, DECOMPOSE(l, r, x1, x1)
should be called once for each rule
(q, l)
s
? (q1 ? ? ? qk, r) ? R. In lines 1?4 the
structure of l and r is inspected and the prop-
erties var(li) = var(l) and var(ri) = var(r)
are tested in constant time. Mind that we pre-
computed vars(l) and vars(r), which can be
done in linear time in the size of the rule. Then
each subtree ri is considered in lines 7?10 in
constant time. Finally, we consider all direct input
subtrees li in lines 12?18. The tests involving
the variables are all performed in constant time
due to the preprocessing step that computes
vars(l) and vars(r). Moreover, at most one
recursive call to DECOMPOSE is generated for
each input subtree ti. So if we implement the
union in lines 18 and 19 by a constant-time
operation (such as list concatenation, which can
be done since it is trivially a disjoint union), then
we obtain the linear time-complexity.
6 Concluding remarks
In this paper we have shown how to restrict com-
putations of WXTTs to given input and output
WSA, and have discussed the relevance of this
technique for parsing and translation applications
over input strings, resulting in the computation of
translation forests and other statistical parameters
of interest. We have also shown how to factorize
transducer rules, resulting in an asymptotic reduc-
tion in the complexity for these algorithms.
In machine translation applications transduc-
ers usually have very large sets of rules. One
should then specialize the restriction construction
in such a way that the number of useless rules
for Prod(Nw,M) is considerably reduced, result-
ing in a more efficient construction. This can be
achieved by grounding the construction of the new
rules by means of specialized strategies, as usually
done for parsing based on context-free grammars;
see for instance the parsing algorithms by Younger
(1967) or by Earley (1970).
26
References
Andre? Arnold and Max Dauchet. 1975. Transductions
inversibles de fore?ts. The`se 3e`me cycle M. Dauchet,
Universite? de Lille.
Andre? Arnold and Max Dauchet. 1976. Bi-
transductions de fore?ts. In ICALP, pages 74?86. Ed-
inburgh University Press.
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Yehoshua Bar-Hillel, Micha Perles, and Eliyahu
Shamir. 1964. On formal properties of simple
phrase structure grammars. In Yehoshua Bar-Hillel,
editor, Language and Information: Selected Essays
on their Theory and Application, chapter 9, pages
116?150. Addison Wesley.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. Commun. ACM, 13(2):94?102.
Samuel Eilenberg. 1974. Automata, Languages, and
Machines, volume 59 of Pure and Applied Math.
Academic Press.
Joost Engelfriet, Zolta?n Fu?lo?p, and Heiko Vogler.
2002. Bottom-up and top-down tree series transfor-
mations. J. Autom. Lang. Combin., 7(1):11?70.
Zolta?n Fu?lo?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Hand-
book of Weighted Automata, EATCS Monographs on
Theoret. Comput. Sci., chapter IX, pages 313?403.
Springer.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL, pages 273?280. Association
for Computational Linguistics.
Daniel Gildea, Giorgio Satta, and Hao Zhang. 2006.
Factoring synchronous grammars by sorting. In
Proc. CoLing/ACL, pages 279?286. Association for
Computational Linguistics.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In HLT-NAACL, pages 105?112.
Association for Computational Linguistics. See
also (Graehl et al, 2008).
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391?427.
Kevin Knight. 2007. Capturing practical natural
language transformations. Machine Translation,
21(2):121?133.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proc. EMNLP, pages 40?51. Association for Com-
putational Linguistics.
Eric Lilin. 1981. Proprie?te?s de clo?ture d?une extension
de transducteurs d?arbres de?terministes. In CAAP,
volume 112 of LNCS, pages 280?289. Springer.
Andreas Maletti, Jonathan Graehl, Mark Hopkins,
and Kevin Knight. 2009. The power of ex-
tended top-down tree transducers. SIAM J. Comput.,
39(2):410?430.
Mark-Jan Nederhof and Giorgio Satta. 2003. Prob-
abilistic parsing as intersection. In Proc. IWPT,
pages 137?148. Association for Computational Lin-
guistics.
Rebecca Nesson, Giorgio Satta, and Stuart M. Shieber.
2008. Optimal k-arization of synchronous tree-
adjoining grammar. In Proc. ACL, pages 604?612.
Association for Computational Linguistics.
Jacques Sakarovitch. 2009. Rational and recognisable
power series. In Manfred Droste, Werner Kuich, and
Heiko Vogler, editors, Handbook of Weighted Au-
tomata, EATCS Monographs on Theoret. Comput.
Sci., chapter IV, pages 105?174. Springer.
Giorgio Satta and Enoch Peserico. 2005. Some
computational complexity results for synchronous
context-free grammars. In Proc. HLT-EMNLP,
pages 803?810. Association for Computational Lin-
guistics.
Marcel Paul Schu?tzenberger. 1961. On the definition
of a family of automata. Information and Control,
4(2?3):245?270.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Inform. Control,
10(2):189?208.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proc. HLT-NAACL, pages 256?
263. Association for Computational Linguistics.
27
