Structural variation in generated health reports
Catalina Hallett and Donia Scott
Centre for Research in Computing
The Open University
Walton Hall
Milton Keynes MK7 6AA
{c.hallett,d.scott}@open.ac.uk
Abstract
We present a natural language gener-
ator that produces a range of medi-
cal reports on the clinical histories of
cancer patients, and discuss the prob-
lem of conceptual restatement in gen-
erating various textual views of the
same conceptual content. We focus on
two features of our system: the de-
mand for ?loose paraphrases? between
the various reports on a given patient,
with a high degree of semantic overlap
but some necessary amount of distinc-
tive content; and the requirement for
paraphrasing at primarily the discourse
level.
1 Introduction
Patient records are typically large collections of
documents that reflect the medical history of
a patient over a period of time. On average,
the electronic patient record of a cancer patient
contains information from over 150 documents,
representing consult notes, referral letters, letters
to and from the patient?s GP, hospital admission
and discharge notes, laboratory test results,
surgery and other treatment descriptions, and
drug dispensing notes. Although each document
in this collection will have a specified purpose,
there tends to be a high degree of redundancy
between documents, but the sheer volume of
information makes access extremely difficult.
The work presented in this paper is part of the
Clinical E-Science Framework project (CLEF),
which aims at providing tools to facilitate easy
access to a patient?s medical history. In particular,
we describe a natural language generation system
that produces a range of summarised reports
of patient records from data-encoded views
of patient histories which we call chronicles.
Although we are concentrating on cancer patients,
we aim to produce good quality reports without
the need to construct extensive domain models.
Our typical user is a GP or clinician who
uses electronic patient records at the point of
care to familiarise themselves with a patient?s
medical history and current situation. A number
of specific requirements arise from this particular
setting:
 Reports that provide a quick potted overview
of the patient?s history are essential; this type
of report should not be too long (ideally they
should fit entirely on a computer screen) and
should take less than a minute to read;
 At the same time, a complete view of the
medical history must always be available on
demand;
 Clinicians often need to examine a patient?s
history from a particular perspective (e.g.,
tests administered, treatments undertaken,
drugs prescribed), and having focussed
reports is also a requirement;
 Reports should be formatted to enhance
readability;
 The selection of events for inclusion in a
report should follow some basic rules:
33
? Events that deviate from what is
considered to be normal are more
important than normal events
(for example, an examination
of the lymphnodes that reveals
lymphadenopathy is more important
than an examination that doesn?t).
? Some events are more important than
others and should not only be included
in the report but also highlighted
(e.g., through colour coding, graphical
timelines or similar display features).
? Less important events should be
available on a need-to-know basis
These requirements impose important
restrictions on the content of the reports and
implicitly on the variety of lexical and syntactical
devices we can employ:
(a) the veracity of the report is essential, therefore
we are not at liberty to employ synonymy or
lexical paraphrasing that may alter (however
slightly) the meaning of the original input,
(b) we are required to maintain a certain
syntactical ordering throughout a report in order
to allow the user to quickly scan through the
report with ease, and
(c) we have to produce several types of reports
from the same input data.
In this paper, we focus on this last requirement,
describing the methods we employ for
reformulating content according to the type
and focus of the generated report.
2 Types of report
In the current implementation, the generator
produces two main types of report. The first is a
longitudinal report, which is intended to provide
a quick historical overview of the patient?s
illness, whilst preserving the main events (such
as diagnoses, investigations and interventions).
It presents the events in the patient?s history
ordered chronologically and grouped according
to type. In this type of report, events are
fully described (i.e., an event description includes
all the attributes of the event) and aggregation
is minimal (events with common attributes are
aggregated, but there is no aggregation through
generalization, for example). The following
example displays a fragment of a generated
longitudinal report1:
Example 1
The patient is diagnosed with grade
9 invasive medullary carcinoma of the
breast. She was 39 years old when
the first cell became malignant. The
history covers 1517 weeks, from week
180 to week 1697. During this time, the
patient attended 38 consults.
YEAR 3:
Week 183
 Radical mastectomy on the breast was
performed to treat primary cancer of the
left breast.
 Histopathology revealed primary cancer
of the left breast.
Week 191
 Examination of the abdomen revealed
no enlargement of the liver or of the
spleen.
 Examination of the axillary lymphnodes
revealed no lymphadenopathy of the left
axillary lymphnodes.
 Examination of the breast revealed no
recurrent cancer of the left breast.
 Testing of the blood revealed
no abnormality of the haemoglobin
concentration or of the leucocyte count.
 Radiotherapy was initiated to treat
primary cancer of the left breast.
Week 192
 First radiotherapy cycle was
performed.
 ...
The second type of report focusses on a given
type of event in a patient?s history, such as the
history of diagnoses, interventions, investigations
or drug prescription. Under this category fall
user-defined reports as well, where the user
selects classes of interesting events (for example,
Investigations of type CT scan and Interventions
of type surgery).
A report of the diagnoses, for example,
will focus on the Problem events that are
recorded in the chronicle (e.g., cancer, anaemia,
lymphadenopathy); other event types will only
1All the examples presented in this paper are extracted
from summaries produced by our Report generator.
34
appear if they are directly related to a Problem.
As it can be seen in Example 2, this type of report
is necessarily more condensed, since the events
do not have to appear chronologically and can be
grouped in larger clusters. Secondary events are
also more highly aggregated.
Example 2
 In week 483, primary cancer of the
right breast was revealed by the
histopathology report. The cancer was
treated with radical mastectomy on the
breast.
 In week 491, no abnormality of the
leucocyte count or of the haemoglobin
concentration, no lymphadenopathy of
the right axillary lymphnodes, no
enlargement of the spleen or of the
liver and no recurrent cancer of the
right breast were found. Radiotherapy
was initiated to treat primary cancer of
the right breast.
 In the weeks 492 to 496, 5
radiotherapy cycles were performed.
If the focus is on Interventions, the same
information in the previous example will be
presented as:
Example 3
 In week 483, histopathology revealed
primary cancer of the right breast.
Radical mastectomy on the breast
was performed to treat the cancer.
Radiotherapy was initiated to treat
primary cancer of the right breast.
 In the weeks 492 to 496, 5
radiotherapy cycles were performed.
In an Investigation-focussed report, the
intervention will be omitted, since they are
not directly relevant:
Example 4
 In week 483, histopathology revealed
primary cancer of the right breast
 In week 491, examination revealed no
abnormality of the leucocyte count or
of the haemoglobin concentration, no
lymphadenopathy of the right axillary
lymphnodes, no enlargement of the spleen
or of the liver and no recurrent cancer
of the right breast.
It is important to note that although the reports
are generated from the same input content, they
are not exact reformulations of each other, but
rather different views of the same content with a
large degree of overlap. This feature is a direct
result of the report requirements.
3 Input
As mentioned earlier, the input to our Report
Generator is a data-encoded chronicle of the
patient?s medical history. Technically, the
chronicle is the partial result of information
extraction applied on clinical narratives,
combined with structured data (such as radiology
results or demographic data), and supplemented
with inferences. However, in developing our
report generator, we are currently using a
Chronicle Simulator, which constructs invented
chronicles, allowing us to ignore for the time
being some problems that can appear when
using an information extraction system (being
developed in parallel). Firstly, the resulting
data is complete and correct, thus allowing us
to concentrate on the design and testing of the
generation and summarisation system without
having to take into account at this point errors in
the Information Extraction. Secondly, our data
on cancer patients is highly confidential, which
makes presentation of the output of the report
generator (e.g., for evaluation with real subjects,
or dissemination purposes) very difficult. Using
a simulator also means that we can have instant
access to a large number of randomly generated
chronicles, which at this stage of the project are
not yet available.
The Chronicle Simulator simulates the history
of a patient?s illness, and links the events in
the history in a manner that closely resembles
the expected output of the real Automatic
Chronicler. The current output format of the
simulator is a relational database that stores
six types of event2 (interventions, investigations,
consults, drugs, problems and loci) and 14
types of relation between events (e.g., Problem
2The term event is loosely used to denote dynamic
(such as interventions) as well as static concepts (such as
problems).
35
HAS-LOCUS Locus, Intervention CAUSED-BY
Problem, Intervention SUBPART-OF Intervention,
Investigation HAS-INDICATION Problem). Each
event has a variable number of attributes, and each
dynamic event is time-stamped with a start date
and an end date3. A typical chronicle contains
around 350 events and about 600 relations.
4 Architecture
The design of the Report Generator follows a
classical pipeline architecture, with a content
selector, content planner and syntactic realiser.
The Content Planner is tightly coupled to the
Content Selector, since part of the discourse
structure is already determined in the event
selection phase. Aggregation is mostly
conceptual rather than syntactic, and thus it
is performed in the content planning stage as well
as during realisation (Reape and Mellish, 1999).
4.1 Content selection
The Content selection process represents the most
important component of the Report Generator.
Although in some contexts it may be useful to
generate reports containing all the events in a
chronicle, the most useful types of report are
the focused, summarised ones, for which good
selection of important events is essential.
The process of content selection is currently
driven by two parameters of a report: type and
length. We define the concept of report spine to
represent a list of concepts that are essential to
the construction of a given type of report. For
example, in a report of the diagnoses, all events
of type Problem will be part of the spine. Events
linked to the spine through some kind of relation
may or may not be included in the summary,
depending on the type and length of the summary
(see Figure 1). The design of the system does
not restrict the spine to containing only events of
the same type. In future extensions to the system
where the user will be able to select facts they
want in the summary, a spine could contain, for
example, problems of type cancer, investigations
of type x-ray and interventions of type surgery.
3In the current implementation of the chronicle, time
stamps are week numbers starting with the date of the first
diagnosis.
Investigation
Intervention
Drug
Problem
Problem
Problem
Problem
Locus
Investigation
Locus
Locus
Intervention
Figure 1: Example of a generated spine structure
Spines are not predefined templates, but
structures that are constructed dynamically with
each request and they depend on the type of
request and on the length of the summary.
Important events are selected according to
semantic relations. The first step in the selection
process is to cluster related events based on the
relations stored in the chronicle. A cluster of
events may tell us, for example, that a patient
was diagnosed with cancer following a clinical
examination, for which she had a mastectomy to
remove the tumour, was given a histopathological
test of the removed tumour, which confirmed the
cancer, and had a complete radiotherapy course
to treat the cancer; the radiotherapy caused an
ulcer, which in turn was treated with some drug.
A typical chronicle contains a small number of
clusters, typically one or two large clusters and
several small ones. Smaller clusters are generally
not related to the main thread of events.
The summarisation process starts with the
removal of small clusters, which in the current
implementation are defined as clusters containing
at most three events4. This excludes some
specified types of information that will be
included in the report even when they only appear
in short clusters; for example, all reports will
contain essential information such as the initial
diagnosis and the cause of death (if available).
The next step is the selection of important
events, as defined by the type of report. Each
cluster of events is a graph, with some nodes
representing spine events. For each cluster, the
spine events are selected, as well as all nodes that
are at a distance of less than n from spine events,
4This threshold was set following a series of experiments.
36
where the depth n is a user-defined parameter
used to adjust the size of the report. For example,
in the cluster presented in Fig. 2, assuming
a depth value of 1, the content selector will
choose cancer, left breast and radiotherapy but
not radiotherapy cycle or ulcer.
Has_Locus
Caused_ByIs_Subpart_Of
Indic
ated
_By Has_Locus
cycle
radiotherapy ulcer
left breastradiotherapy
cancer
Figure 2: Example of a cluster
A document plan is typically a hierarchical
structure that contains and combines the
messages to be conveyed by the report generator.
Technically, a document plan is an ordered
collection of message clusters, where messages
within a cluster are combined using rhetorical
relations, while individual clusters are ordered
and linked according to the type of report.
The construction of document plans is partly
performed in the content selection phase,
since the content is selected according to the
relations between events, which in turn provide
information about the structure of the target
text. The actual document planner component
is concerned with the construction of complete
document plans, according to the type of report
and cohesive relations identified in the previous
stage. A report typically consists of three parts:
(a) a schematic description of the patient?s
demographic information (name, age, gender),
(b) a two sentence summary of the patient?s
record (presenting the time span of the illness,
the number of consults the patient attended, and
the number of investigations and interventions
performed) and
(c) the actual report of the record produced from
the events selected to be part of the content.
We focus here on this last part.
4.2 Document planning
The first stage in structuring the body of the
report is to combine messages linked through
attributive relations (e.g., combining messages of
type Problem with messages of type Locus if
the Problem has a HAS-LOCUS relation pointing
to a Locus). In the second stage, messages are
grouped according to specific rules, depending on
the type of report. For longitudinal reports, the
rules stipulate that events occurring in the same
week should be grouped together, and further
grouped into years. In event-specific reports,
patterns of similar events are first identified and
then grouped according to the week(s) they occur
in. For example, if in week 1 the patient was
examined for enlargement of the liver and of the
spleen with negative results and in week 2 the
patient was again examined with the same results
and had a mastectomy, two groups of events will
be constructed:
Example 5
 In weeks 1 and 2, examination of the
abdomen revealed no enlargement of the
liver or of the spleen.
 In week 2, the patient underwent a
mastectomy.
Within groups, messages are structured
according to discourse relations that are either
deduced from the input database or automatically
inferred by applying domain specific rules. At
the moment, the input provides three types of
rhetorical relation: Cause, Result and Sequence.
The domain specific rules specify the ordering
of messages, and always introduce a Sequence
relation. An example of such a rule is that a
histopathology event has to follow a biopsy
event, if both of them are present and they start
and end at the same time. These rules facilitate
the construction of a partial rhetorical structure
tree. Messages that are not connected in the tree
are by default assumed to be in a List relation to
other messages in the group, and their position is
set arbitrarily.
The document planner also applies aggregation
rules between similar messages and employs
ellipsis and conjunction in order to create a
more fluent text. Simple aggregation rules
state, for example, that two investigations with
37
Investigation
[id:03,
name:examination,
HAS?TARGET: {abdomen, breast}]
Investigation
HAS?TARGET: abdomen]
[id: 01,
name: examination,
Investigation
[id:02,
name:examination,
HAS?TARGET: breast]
Figure 3: Aggregation of Investigation messages on the HAS-TARGET field
the same name and two different target loci
can be collapsed into one investigation with
two target loci (Fig.3). Aggregation rules of
this type are designed to make the resulting
text more fluent, however they do not always
provide the degree of condensation required
by the summary. For example, each clinical
examination consists of examinations of the
abdomen for enlargement of internal organs (liver
and spleen) and examination of the lymphnodes.
Thus, each clinical examination will typically
consist of three independent Investigation events.
When fully aggregated according to conceptual
and syntactical rules, the three Investigation
messages are collapsed into one structure such
as:
Example 6
Examination revealed no enlargement
of the spleen or of the liver and no
lymphadenopathy of the axillary nodes.
However, this level of aggregation that only
takes into account the semantics of individual
messages may be not enough, since clinical
examinations are performed repeatedly and
consist of the same types of investigation. Two
approaches have been implemented in the Report
Generator, both of which make use of domain
specific rules. The first is to report only
events that deviate from the norm. In the case
of investigations, for example, this equates to
reporting only those that have abnormal results.
The second, which produces larger reports, is to
produce synthesised descriptions of events. In
the case of clinical examination for example, we
could describe a sequence of investigations such
as the one in example (5) as Clinical examination
was normal. If the examination deviates from the
norm on a restricted numbers of parameters only,
this can be described as Clinical examination was
normal, apart from an enlargement of the spleen.
4.3 Maintaining the thread of discourse
In producing multiple reports on the same patient
from different perspectives, or of different types,
we operate under the strong assumption that
event-focussed reports should be organised in a
way that emphasises the importance of the event
in focus. From a document structure viewpoint,
this equates to constructing rhetorical structures
where the focus event (i.e., the spine event) is
expressed in a nuclear unit, and skeleton events
are preferably in sattelite units.
Within sentences, spine events are assigned
salient syntactical roles that allows them to be
kept in focus. For example, a relation such as
Problem CAUSED-BY Intervention
is more likely to be expressed as :
The patient developed a Problem as a result of an
Intervention.
when the focus is on Problem events, and, when
the focus is on Interventions as:
An Intervention caused a Problem.
This kind of variation reflects the different
emphasis that is placed on spine events, although
the wording in the actual report may be different.
Rhetorical relations holding between simple event
descriptions are most often realised as a single
sentence (as in the examples above). Complex
individual events are realised in individual clauses
or sentences which are connected to other
accompanying events through the appropriate
rhetorical relation.
38
For example, a Problem event has a large
number of attributes, consisting of name, status,
existence, number of nodes counted, number
of nodes involved, clinical course, tumour size,
genotype, grade, tumour marker and histology,
as well as the usual time stamp. The selection
of attributes that are going to be included in a
Problem description depends on a number of
factors, including whether the Problem is a spine
or a skeleton event, and whether the event is
mentioned for the first time or is a subsequent
mention. Aditionally, the number of attributes
included in the description of a Problem is a
decisive factor in realising the Problem as a
phrase, a sentence or a group of sentences. In the
following two examples, there are two Problem
events (cancer and lymphnode count) linked
through an Investigation event (excision biopsy,
which is indicated by the first problem and has
as a finding the second problem. In Example 7,
the problems are first mentioned spine events,
while in Example 8, the problems are skeleton
events (the cancer is a subsequent mention and
the lymphnode count is a first mention), with the
Investigation being the spine event.
Example 7
A 10mm, EGFR +ve, HER-2/neu +ve,
oestrogen receptor positive cancer was
found in the left breast (histology:
invasive tubular adenocarcinoma).
Consequently, an excision biopsy was
performed which revealed no metastatic
involvement in the 5 nodes sampled.
Example 8
An excision biopsy on the left breast
was performed because of cancer. It
revealed no metastatic involvement in
the 5 nodes sampled.
As can be seen from the examples above,
the same basic rhetorical structure consisting
of three nodes and two relations (causality and
consequence) is realised differently in a Problem-
focussed report compared to an Investigation-
based report. The conceptual reformulation is
guided by the type of report, which in turn has
consequences at syntactical level.
5 Evaluation
Automatic evaluation of the generated reports
is not possible, as there is no gold standard
for such documents. Additionally, a full-blown
quantitative evaluation is not yet feasible, since
our users are cancer specialists who cannot
easily dedicate time to evaluating large numbers
of reports. However, we have conducted an
informal survey with two cancer clinicians to gain
feedback on the quality of the current output of
the Report Generator. To do this, we showed them
three patient records encoded as chronicles, and,
for each patient, two types of report produced
from that record: a longitudinal report, and
a summarised report of diagnoses. The three
patient records were selected to display a variety
of events and sizes (a 6-year history containing
621 events, a 12-year history with 1418 events,
and a 9-year history with 717 events).
Although they were (unusually) familiar with
the coding scheme of the chronicles, the
clinicians found it very difficult to extract a useful
overview of the patients? histories from the three
chronicles we showed them. In contrast, they
found the generated reports to be much more
useful and the quality of the text to be very good.
The clinicians commended the reports for their
ability to provide a quick and clear view of data
that would be otherwise difficult to access and
process. Most importantly, the various report
types were judged to be highly appropriate for use
in clinical care.
Whilst this preliminary evaluation was
conducted with the aim of finding early
shortcomings of the Report Generator and
receiving feedback from potential users, we
are now embarking on a more extensive formal
evaluation with cancer clinicians and medical
researchers with specialist knowledge in the area
of cancer. We believe, however, that the true test
of utility will be the actual use of the Report
Generator in practice.
6 Conclusions
We have described a system that generates a range
of health reports on individual cancer patients.
At present, our intended readership is composed
of clinicians and medical researchers, and the
39
type of report will depend on his or her stated
needs. Reports that are required at the point
of care (e.g., for a doctor interviewing a newly
referred patient, or a team of medics on ward
rounds) are likely to be short ?30-second? potted
histories. At other times longer, more detailed
reports will be required, as will reports that focus
on particular aspects of the patient?s ?journey?
through their disease (e.g., from the perspective
of the diagnoses that have been made, the drugs
they have been prescribed, or surgery they have
undergone). The system is fully implemented
in Java and currently generates this full range of
reports on-the-fly. A summarised report based on
about 1000 input events is constructed in less than
2 seconds, a speed which is highly appropriate to
the demands of clinical practice.
While the various types of generated report all
share the same input (i.e., the patient?s chronicle),
and thus will have a large degree of conceptual
overlap, clearly there will be occassions when
information that is included in some reports will
not be in others.
The range of reports for any given patient at any
given point in their illness thus present a special
class of paraphrase, with a looser adherance to
semantic equivalence between versions than is
typically found in other paraphrase generators, for
example Kozlowski et al(2003), McKeown et al
(1994), Power, Scott and Bouyaad-Agha (2003),
Rosner and Stede (1994),(1996), and Scott and
Souza (1990). In this sense, our Report Generator
is rather closer in spirit to Hovy?s PAULINE
system, which generates descriptions of given
news events from different perspectives and with
different stylistic goals (Hovy, 1988). However,
we achieve our goal with less reliance on
terminological variation and more on structural
variation at the discourse level. Syntactic
variation, where it does occur, is almost always
simply a side-effect of an earlier discourse choice.
Terminological variation is deliberately avoided
to prevent false implicatures; however, we are
about to introduce a further class of readership,
namely patients, at which stage we will make
fuller use of our lexical resources.
7 Acknowledgments
CLEF is supported in part by grant G0100852
under the E-Science Initiative. Thanks are due its
clinical collaborators at the Royal Marsden and
Royal Free hospitals, to colleagues at the National
Cancer Research Institute (NCRI) and NTRAC
and to its industrial collaborators. Special thanks
to Dr. Jeremy Rogers who provided us with the
automated Chronicle Simulator that we have used
in all our experiments.
References
Eduard H. Hovy. 1988. Generating natural language
under pragmatic constraints. Lawrence Erlbaum,
Hillsdale, New Jersey.
Raymond Kozlowski, Kathleen F. McCoy, and
K. Vijay-Shanker. 2003. Generation of single-
sentence paraphrases from predicate/argument
structure using lexico-grammatical resources.
In Kentaro Inui and Ulf Hermjakob, editors,
Proceedings of the Second International Workshop
on Paraphrasing, pages 1?8.
Kathleen McKeown, Karen Kukich, and James
Shaw. 1994. Practical issues in automatic
document generation. In Proceedings of the
Fourth Conference on Applied Natural-Language
Processing (ANLP-1994), pages 7?14, Stuttgart,
Germany.
Richard Power, Donia Scott, and Nadjet Bouayad-
Agha. 2003. Document structure. Computational
Linguistics, 29(2):211?260.
Mike Reape and Chris Mellish. 1999. Just what
is aggregation anyway? In Proceedings of
the 7th European Workshop on Natural Language
Generation (EWNLG?99), pages 20?29, Toulouse,
France.
Dietmar Ro?sner and Manfred Stede. 1994.
Generating multilingual documents from
a knowledge base: the TECHDOC project.
In Proceedings of the 15th conference on
Computational Linguistics (Coling?94), pages
339?343, Kyoto, Japan.
Donia Scott and Clarisse de Souza. 1990. Getting
the message across in RST-based text generation. In
R. Dale C. Mellish and M. Zock, editors, Current
Research in Natural Language Generation, pages
31 ? 56. Academic Press.
Manfred Stede. 1996. Lexical paraphrases
in multilingual sentence generation. Machine
Translation, 11:75?107.
40
Proceedings of the Fourth International Natural Language Generation Conference, pages 95?102,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Generic Querying of Relational Databases using Natural Language
Generation Techniques
Catalina Hallett
Center for Research in Computing
The Open University
Walton Hall, Milton Keynes
United Kingdom
c.hallett@open.ac.uk
Abstract
This paper presents a method of querying
databases by means of a natural language-
like interface which offers the advantage
of minimal configuration necessary for
porting the system. The method allows
us to first automatically infer the set of
possible queries that can apply to a given
database, automatically generate a lexicon
and grammar rules for expressing these
queries, and then provide users with an
interface that allows them to pose these
queries in natural language without the
well-known limitations of most natural
language interfaces to databases. The way
the queries are inferred and constructed
means that semantic translation is per-
formed with perfect reliability.
1 Introduction
Natural Language interfaces to databases (here-
after NLIDBs ) have long held an appeal to both
the databases and NLP communities. However,
difficulties associated with text processing, seman-
tic encoding, translation to database querying lan-
guages and, above all, portability, have meant that,
despite recent advances in the field, NLIDBs are
still more a research topic than a commercial solu-
tion.
Broadly, research in NLIDBs has focused
on addressing the following fundamental, inter-
dependent issues1:
? domain knowledge aquisition (Frank et al,
2005);
1The extent of NLIDB research is such that it is beyond
the scope of this paper to reference a comprehensive list of
projects in this area. For reviews on various NLIDBs , the
reader is referred to (Androutsopoulos et al, 1995).
? interpretation of the input query, including
parsing and semantic disambiguation, se-
mantic interpretation and transformation of
the query to an intermediary logical form
(Hendrix et al, 1978; Zhang et al, 1999;
Tang and Mooney, 2001; Popescu et al,
2003; Kate et al, 2005);
? translation to a database query language
(Lowden et al, 1991; Androutsopoulos,
1992);
? portability (Templeton and Burger, 1983; Ka-
plan, 1984; Hafner and Godden, 1985; An-
droutsopoulos et al, 1993; Popescu et al,
2003)
In order to recover from errors in any either of
these steps, most advanced NLIDB systems will
also incorporate some sort of cooperative user
feedback module that will inform the user of the
inability of the system to construct their query and
ask for clarification.
We report here on a generic method we have
developed to automatically infer the set of possi-
ble queries that can apply to a given database, and
an interface that allows users to pose these ques-
tions in natural language but without the previ-
ously mentioned drawbacks of most NLIDBs . Our
work is substantially different from previous re-
search in that it does not require the user to input
free text queries, but it assists the user in com-
posing query through a natural language-like in-
terface. Consequently, the necessity for syntactic
parsing and semantic interpretation is eliminated.
Also, since users are in control of the meaning of
the query they compose, ambiguity is not an issue.
Our work builds primarily on two directions
of research: conceptual authoring of queries via
95
WYSIWYM interfaces, as described in section 2,
and NLIDB portability research. From the perspec-
tive of the query composing technique, our sys-
tem resembles early menu-based techniques, such
as Mueckstein (1985), NL-Menu (Tennant et al,
1983) and its more recent re-development Lingo-
Logic (Thompson et al, 2005). This resemblance
is however only superficial. Our query editing in-
terface employs natural language generation tech-
niques for rendering queries in fluent language; it
also allows the editing of the semantic content of
a query rather than its surface form, which allows
seamless translation to SQL .
As in (Zhang et al, 1999), our system makes
use of a semantic graph as a mean of representing
the database model. However, whilst Zhang et al
(1999) use the Semantic Graph as a resource for
providing and interpreting keywords in the input
query, we use this information as the main means
of automatically generating query frames.
2 WYSIWYM interfaces for database
querying
Conceptual authoring through WYSIWYM editing
alleviates the need for expensive syntactic and se-
mantic processing of the queries by providing the
users with an interface for editing the conceptual
meaning of a query instead of the surface text
(Power and Scott, 1998).
The WYSIWYM interface presents the contents
of a knowledge base to the user in the form of
a natural language feedback text. In the case of
query editing, the content of the knowledge base is
a yet to be completed formal representation of the
users query. The interface presents the user with
a natural language text that corresponds to the in-
complete query and guides them towards editing
a semantically consistent and complete query. In
this way, the users are able to control the interpre-
tation that the system gives to their queries. The
user starts by editing a basic query frame, where
concepts to be instantiated (anchors) are clickable
spans of text with associated pop-up menus con-
taining options for expanding the query.
Previously, WYSIWYM interfaces have proved
valid solutions to querying databases of legal doc-
uments and medical records (Piwek et al, 2000),
(Piwek, 2002), (Hallett et al, 2005).
As a query-formulation method, WYSIWYM
provides most of the advantages of NLIDBs , but
overcomes the problems associated with natural
language interpretation and of users attempting to
pose questions that are beyond the capability of
the system or, conversely, refraining from asking
useful questions that are in fact within the sys-
tem?s capability. However, one of the disadvan-
tages of the WYSIWYM method is the fact that do-
main knowledge has to be manually encoded. In
order to construct a querying interface for a new
database, one has to analyse the database and man-
ually model the queries that can be posed, then
implement grammar rules for the construction of
these queries. Also, the process of transforming
WYSIWYM queries into SQL or another database
querying language has previously been database-
specific. These issues have made it expensive to
port the interface to new databases and new do-
mains.
The research reported here addresses both these
shortcomings by providing a way of automatically
inferring the type of possible queries from a graph
representation of the database model and by devel-
oping a generic way of translating internal repre-
sentations of WYSIWYM constructed queries into
SQL .
3 Current approach
In the rest of the paper, we will use the following
terms: a query frame refers to a system-generated
query that has not been yet edited by the user,
therefore containing only unfilled WYSIWYM an-
chors. An anchor is part of the WYSIWYM ter-
minology and means a span of text in a partially
formulated query, that can be edited by the user to
expand a concept. Anchors are displayed in square
brackets (see examples in section 3.3).
To exemplify the system behaviour, we will use
as a case study the MEDIGAP database, which
is a freely downloadable repository of informa-
tion concerning medical insurance companies in
the United States. We have chosen this particu-
lar database because it contains a relatively wide
range of entity and relation types and can yield a
large number of types of queries. In practice we
have often noticed that large databases tend to be
far less complex.
3.1 System architecture
Figure 1 shows the architecture of the query-
ing system. It receives as input a model of
the database semantics (the semantic graph) and
it automatically generates some of the compo-
96
Semantic graph
User
interface
Wysiwym
components
T-box
Grammar
rules
Text
generator
Query SQLQuery
LexiconDatabase
Figure 1: System architecture
nents and resources (highlighted in grey) that in
previous WYSIWYM querying systems were con-
structed manually. Finally, it implements a mod-
ule that translates the user-composed query into
SQL .
The components highlighted in grey are those
that are constructed by the current system.
The T-box describes the high-level components
of the queries. It is represented in Profit notation
(Erbach, 1995) and describes the composition of
the query frames (the elements that contribute to a
query and their type) . A fragment of the semantic
graph displayed in 2 will generate the following
fragment of t-box:
query > [about_company, about_state,
about_phone, about_ext].
about_company > [company_state, company_phone,
company_ext].
company_state intro [company:company_desc].
company_desc intro [comp:comp_desc, phone:phone_desc,
ext:ext_desc].
state_desc > external(?dbo_vwOrgsByState_StateName?).
comp_desc > external(?dbo_vwOrgsByState_org_name?).
phone_desc > external(?dbo_vwOrgsByState_org_phone?).
ext_desc > external(?dbo_vwOrgsByState_org_ext?).
The grammar rules are also expressed in
Profit, and they describe the query formulation
procedure. For example, the following rule will be
generated automatically to represent the construc-
tion procedure for the query in Example (1.1):
rule(english, company_state,
meaning!(<description &
predicate!company_state &
properties![attribute!comp & value!Comp]) &
layout!level!question &
cset![meaning!in &
syntax!category!prep &
layout!level!word,
meaning!which &
syntax!category!int &
layout!level!word,
meaning!state &
syntax!category!np &
layout!level!word,
meaning!be &
syntax!(category!vb & form!pres),
layout!level!word,
meaning!Comp &
syntax!category!np &
layout!level!phrase,
meaning!locate &
syntax!(category!vb & form!part),
layout!level!word]).
In addition to the grammar rules automatically
generated by the system, the WYSIWYM pack-
age also contains a set of English grammar rules
(for example, rules for the construction of defi-
nite noun phrases or attachment of prepositional
phrases). These rules are domain independent, and
therefore a constant resource for the system.
The lexicon consists of a list of concepts to-
gether with their lexical form and syntactic cate-
gory. For example, the lexicon entry for insurance
company will look like:
word(english, meaning!company &
syntax!(category!noun & form!name) &
cset!?insurance company?)).
3.2 Semantic graph
The semantics of a relational database is specified
as a directed graph where the nodes represent el-
ements and the edges represent relations between
elements. Each table in the database can be seen
as a subgraph, with edges between subgraphs rep-
resenting a special type of join relation.
Each node has to be described in terms of its se-
mantics and, at least for the present, in terms of its
linguistic realisation. The semantic type of a node
is restricted by the data type of the correspond-
ing entity in the database. A database entity of
type String can belong to one of the following se-
mantic categories: person, organization, location
(town, country), address (street or complete ad-
dress), telephone number, other name, other ob-
ject. Similarly, numerical entities can have the se-
mantic type: age, time (year, month, hour), length,
97
OrgName
Sem: organisation
Lex: insurance company
Morph: proper noun
Phone#
?
Extension
?
StateName
Sem: location
Lex: state
Morph: proper noun
Type: descriptive
Arity: n to 1
Sem: location
Lex: be located in
Frame: NP-VB-NP
Table 1
Type: attributive
Sem: possession
Lex: have
Frame: NP-VB-NP
Type: attributive
Sem: possession
Lex: have
Frame: NP-VB-NP
Table 2
Figure 2: Example of a semantic graph
weight, value, height. The data type date has only
one possible semantic type, which is date. These
semantic types have proved sufficient in our exper-
iments, however this list can be expanded if nec-
essary.
Apart from the semantic type, each node must
specify the linguistic form used to express that
node in a query. For example, in our case study,
the field StateName will be realised as state, with
the semantic category location. Additionally, each
node will contain the name of the table it belongs
to and the name of the column it describes.
Relations in the semantic graph are also de-
scribed in terms of their semantic type. Since re-
lations are always realised as verbs, their seman-
tic type also defines the subcategorisation frame
associated with that verb. For the moment, sub-
categorisation frames are imported from a locally
compiled dictionary of 50 frequent verbs. The user
only needs to specify the semantic type of the verb
and, optionally, the verb to use. The system au-
tomatically retrieves from the dictionary the ap-
propriate subcategorisation frame. The dictionary
has the disadvantage of being rather restricted in
coverage, however it alleviates the need for the
user to enter subcategorisation frames manually,
a task which may prove tedious for a user with-
out the necessary linguistic knowledge. However,
we allow users to enter new frames in the dictio-
nary, should their verb or category of choice not
be present. A relation must also specify its arity.
This model of the database semantics is par-
tially constructed automatically by extracting
database metadata information such as data types
and value ranges and foreign keys. The manual
effort involved in creating the semantic graph is
reduced to the input of semantic and linguistic in-
formation.
3.3 Constructing queries
We focus our attention in this paper to the con-
struction of the most difficult type of queries:
complex wh-queries over multiple database tables
and containing logical operators. The only restric-
tion on the range of wh-queries we currently con-
struct is that we omit queries that require infer-
ences over numerical and date types.
Each node in the semantic graph can be used
to generate a number of query frames equal to the
number of nodes it is connected to in the graph.
Each query frame is constructed by pairing the
current node with each other of the nodes it is
linked to. By generation of query frames we desig-
nate the process of automatically generating Profit
code for the grammar rule or set of rules used by
WYSIWYM , together with the T-box entries re-
quired by that particular rule.
If we consider the graph presented in Fig.2,
and focus on the node orgName, the system will
construct the query frames:
Example (1):
1. In which state is [some insurance
company] located?
2. What phone number does [some
insurance company] have?
3. What extension does [some insurance
company] have?
If we consider the first query in the example
above, the user can further specify details about
98
the company by selecting the [some insurance
company] anchor and choosing one of the options
available (which themselves are automatically
generated from the database in question). This
information may come from one or more tables.
For example, one table in our database contains
information about the insurance companies con-
tact details, whilst another describes the services
provided by the insurance companies. Therefore,
the user can choose between three options:
contact details, services and all. Each selection
triggers a text regeneration process, where the
feedback text is transformed to reflect the user
selection, as in the example below:
Example (2):
1. In which state is [some insurance
company] that has [some phone number]
and [some extension] located?
2. In which state is [some insurance
company] that offers [some medical
insurance plan] and [is available] to
people over 65 located?
3. In which state is the insurance
company with the following features
located:
? It has [some phone number] and [some
extension]
and
? It offers [some medical insurance
plan] and [is available] to people over
65
Figure 3 shows a snapshot of the query editing
interface where query (2.1) is being composed.
Each query frame is syntactically realised by
using specially designed grammar rules. The
generation of high level queries such as those
in Example (1.1) relies on basic query syntax
rules. The semantic type of each linked element
determines the type of wh-question that will be
constructed. For example, if the element has the
semantic type location, we will construct where
questions, whilst a node with the semantic type
PERSON will give rise to a who-question. In
order to avoid ambiguities, we impose further
restrictions on the realisation of the query frames.
If there is more than one location-type element
linked to a node, the system will not generate two
where query frames, which would be ambiguous,
but more specific which queries. For example,
our database contains two nodes of semantic type
location linked to the node OrgName. The first
describes the state where an insurance company is
located, the second its address. The query frames
generated will be:
Example (3):
1. In which states is some insurance
company located?
2. At what addresses is some insurance
company located?
The basic grammar rule pattern for queries
based on one table only states that elements linked
to a particular node will be realised in relative
clauses modifying that node. For example, in Ex-
ample (2.1), the nodes phones and ext are accessi-
ble from the node orgName, therefore will be re-
alised in a relative clause that modifies insurance
company.
In the case where the information comes from
more than one table, it is necessary to introduce
more complex layout features in order to make the
query readable. For each table that provides in-
formation about the focused element we generate
bulleted lines as in Example (2.3).
Each question frame consists of a bound ele-
ment2, i.e., the user cannot edit any values for that
particular element. This corresponds to the infor-
mation that represents the answer to the questions.
In example (2), the bound element is state. All
other nodes will be realised in the feedback text
as anchors, that are editable by users. One ex-
ception is represented by nodes that correspond to
database elements of boolean type. In this case,
the anchor will not be associated to a node, but to
a relation, as in Example (2.3) (the availability of
an insurance plan is a boolean value). This is to
allow the verb to take negative form - in our ex-
ample, one can have is available to people over 65
or is not available to people over 65.
Since not all anchors have to be filled in, one
query frame can in fact represent more than
one real-life question. In example (4), one can
edit the query to compose any of the following
corresponding natural language questions:
Example (4):
1. In which state is the insurance
company with the phone number 8008474836
located?
2. In which state is the insurance
2In fact, a single element can be replaced of any number
of elements of the same type linked by conjunctions or dis-
junctions. However, we will refer to a single element by way
of simplification. The process of inferring queries remains
esentially the same.
99
Figure 3: Query editing interface snapshot
company Thrivent Financial for Lutherans
with the phone number 8008474836
located?
3. In which state is the insurance
company Thrivent Financial for Lutherans
with the phone number 8008474836 and
extension 8469 located?
The actual values of anchors are extracted from
the database and transformed into correct lexicon
entries on a per-need basis. The strings associated
with a value (e.g. Thrivent Financial for Luther-
ans) are retrieved from the database table and col-
umn indicated in the description of the node that
was used for generating the anchor (e.g. orgName)
and the syntactic role (e.g. proper noun) is given
by the syntactic information associated with the
node.
3.4 Query translation module
Once a query has been constructed, it is rep-
resented internally as a directed acyclic graph.
Moreover, each node in the graph can be mapped
into a node in the semantic graph of the database.
The translation module transforms a contructed
query to an SQL statement by parsing the query
graph and combining it with the corresponding el-
ements in the semantic graph.
The SELECT portion of the statement contains
the focused element. The WHERE portion con-
tains those nodes in the question graph that cor-
respond to edited anchors. For constructing the
FROM portion of the statement, we extract, from
the semantic graph, for each SELECTED element
information about their corresponding database ta-
ble.
For example, if we assume that in Example (2.1)
the user has specified the name of the company
and its phone number, the SQL statement gener-
ated will be:
SELECT dbo_vwOrgsByState.StateName
FROM dbo_vwOrgsByState
WHERE org_name="Thrivent Financial for
Lutherans"
And org_phone="8008474836";
4 Evaluation
4.1 Usability
A recent study of the usability of a WYSIWYM type
of interface for querying databases (Hallett et al,
2006) has shown that users can learn how to use
the interface after a very brief training and suc-
ceed in composing queries of quite a high level of
complexity. They achieve near-perfect query con-
struction after the first query they compose. The
study also showed that the queries as they appear
in the WYSIWYM feedback text are unambiguous
? not only to the back-end system ? but also to
the user, i.e., users are not misled into constructing
queries that may have a different meaning than the
one intended. Additionally, it appears that expert
users of SQL , with expert knowledge of the un-
derlying database, find the query interface easier
to use than querying the database directly in SQL
. We consider that most of the conclusions drawn
in (Hallett et al, 2006) apply to the current sys-
tem. The only difference may appear in assess-
ing the ambiguity of the feedback text. Since the
query construction rules used for our system are
generated automatically, it is likely that the feed-
back text may be less fluent and, potentially, more
ambiguous than a feedback text generated using
manually constructed rules, as in (Hallett et al,
2006). We have not yet addressed this issue in a
formal evaluation of the current system.
100
4.2 Coverage
We have assessed the coverage of the system us-
ing as our test set a set of English questions
posed over a database of geographical information
GEOBASE, as in (Tang and Mooney, 2001) and
(Popescu et al, 2003). Our first step was to convert
the original Prolog database (containing about 800
facts) into a relational database. Then we tested
how many of the 250 human produced questions
in the test set can be constructed using our system.
There are several issues in using this particu-
lar dataset for testing. Since we do not provide
a pure natural language interface, the queries our
system can construct are not necessarily expressed
in the same way or using the same words as the
questions in the test set. For example, the ques-
tion ?How high is Mount McKinley?? in the test
set is equivalent to ?What is the height of Mount
McKinley?? produced by our system. Similarly,
?Name all the rivers in Colorado.? is equivalent
to ?Which rivers flow through Colorado??. Also,
since the above test set was designed for testing
and evaluating natural language interfaces, many
of the questions have equivalent semantic content.
For example, ?How many people live in Califor-
nia?? is semantically equivalent to ?What is the
population of California??. Similarly, there is no
difference in composing and analysing ?What is
the population of Utah?? and ?What is the popu-
lation of New York City??.
Out of 250 test questions, 100 had duplicate se-
mantic content and the remaining 150 had original
content. On the whole test set of 250 questions,
our system was able to generate query frames that
allow the construction of 145 questions, therefore
58%. The remaining 42% of questions belong to
a single type of questions that our current imple-
mentation cannot handle, which is questions that
require inferences over numerical types, such as
Which is the highest point in Alaska? or What is
the combined area of all 50 states?.
Similar results are achieved when testing the
system on the 150 relevant questions only: 60%
of the questions can be formulated, while the re-
maining 40% cannot.
4.3 Correctness
The correctness of the SQL generated queries was
assessed on the subset of queries that our system
can formulate out of the total number of queries
in the test set. We found that the correct SQL was
produced for all the generated WYSIWYM queries
produced.
5 Conclusions & Further work
Our method presents three main advantages over
other natural language interfaces to databases:
1. It is easily customizable for new domains and
databases.
2. It eliminates errors in parsing and query-to-SQL
translation.
3. It makes clear to the user the full range of
possible queries that can be posed to any given
database.
From a user?s point of view, one could argue
that our method is less natural to use than one that
allows unconstrained (or less constrained) natural
language input. It could also be said that while
syntactically correct, the queries as presented to
the user may not be as fluent as human-authored
questions. These possible disadvantages are, in
our opinion, outweighed by the clarity of the query
composition process, since the user is fully in con-
trol of the semantic content of the query she com-
poses; they are unambiguous to both the user and
the back-end system.
We are currently extending this work to cover
more complex queries that require inferences and
ones that contain elements linked through tempo-
ral relations. We will also refine the query layout
procedures to allow complex queries to be pre-
sented in a more intuitive way. Additionally, we
are about to begin work on automating the the con-
struction of the semantic graph. We expect that
some of the semantic and syntactic information
that, at the moment, has to be manually entered
in the description of the semantic graph can be in-
ferred automatically from the database content.
Acknowledgement
The work described in this paper is part of the
Clinical E-Science Framework (CLEF) project,
funded by the Medical Research Council grant
G0100852 under the E-Science Initiative. We
gratefully acknowledge the contribution of our
clinical collaborators at the Royal Marsden and
Royal Free hospitals, colleagues at the National
Cancer Research Institute (NCRI) and NTRAC
and to the CLEF industrial collaborators.
101
References
I. Androutsopoulos, G.Ritchie, and P.Thanitsch. 1993.
An effcient and portable natural language query in-
terface for relational databases. In Proceedings of
the 6th International Conference on Industrial En-
gineering Applications of Artificial Intelligence and
Expert Systems Edinburgh, pages 327?330.
I. Androutsopoulos, G.D. Ritchie, and P.Thanisch.
1995. Natural language interfaces to databases -
an introduction. Natural Language Engineering,
2(1):29?81.
I. Androutsopoulos. 1992. Interfacing a natural lan-
guage front end to a relational database. Master?s
thesis, Department of Artificial Intelligence Univer-
sity of Edinburgh.
Gregor Erbach. 1995. ProFIT ? prolog with features,
inheritance, and templates. In Proceedings of the
7th Conference of the European Chapter of the Ass-
cociation for Computational Linguistics, EACL-95,
Dublin, Ireland.
A. Frank, Hans-Ulrich Krieger, Feiyu Xu, Hans Uszko-
reit, Berthold Crysmann, Brigitte Jorg, and Ulrich
Schafer. 2005. Querying structured knowledge
sources. In AAAI-05Workshop onQuestion Answer-
ing in Restricted Domains, Pittsburgh, Pennsylva-
nia.
Carole D. Hafner and Kurt Godden. 1985. Portability
of syntax and semantics in datalog. ACM Trans. Inf.
Syst., 3(2):141?164.
C. Hallett, D. Scott, and R.Power. 2005. Intuitive
querying of ehealth data repositories. In Proceed-
ings of the UK E-Science All-Hands Meeting, Not-
tingham, UK.
C. Hallett, D. Scott, and R.Power. 2006. Evaluation of
the clef query interface. Technical Report 2006/01,
Department of Computing, The Open University.
Gary G. Hendrix, Earl D. Sacerdoti, Daniel Sagalow-
icz, and Jonathan Slocum. 1978. Developing a natu-
ral language interface to complex data. ACM Trans.
Database Syst., 3(2):105?147.
S. Jerrold Kaplan. 1984. Designing a portable nat-
ural language database query system. ACM Trans.
Database Syst., 9(1):1?19.
R.J. Kate, Y.W. Wong, and R.J. Mooney. 2005. Learn-
ing to transform natural to formal languages. InPro-
ceedings of the Twentieth National Conference on
Artificial Intelligence (AAAI-05), pages 1062?1068,
Pittsburgh, PA.
B.G.T. Lowden, B.R. Walls, A. De Roeck, C.J. Fox,
and R. Turner. 1991. A formal approach to translat-
ing english into sql. In Jackson and Robinson, edi-
tors, Proceedings of the 9th British National Confer-
ence on Databases.
Eva-Martin Mueckstein. 1985. Controlled natural
language interfaces (extended abstract): the best of
three worlds. In CSC ?85: Proceedings of the 1985
ACM thirteenth annual conference on Computer Sci-
ence, pages 176?178, New York, NY, USA. ACM
Press.
P. Piwek, R. Evans, L. Cahill, and N. Tipper. 2000.
Natural language generation in the mile system. In
Proceedings of the IMPACTS in NLG Workshop,
Schloss Dagstuhl, Germany.
P. Piwek. 2002. Requirements definition, validation,
verification and evaluation of the clime interface and
language processing technology. Technical Report
ITRI-02-03, ITRI, University of Brighton.
Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.
2003. Towards a theory of natural language in-
terfaces to databases. In IUI ?03: Proceedings of
the 8th international conference on Intelligent user
interfaces, pages 149?157, New York, NY, USA.
ACM Press.
Richard Power and Donia Scott. 1998. Multilin-
gual authoring using feedback texts. In Proceedings
of 17th International Conference on Computational
Linguistics and 36th Annual Meeting of the Associ-
ation for Computational Linguistics (COLING-ACL
98), pages 1053?1059, Montreal, Canada.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In EMCL ?01:
Proceedings of the 12th European Conference on
Machine Learning, pages 466?477, London, UK.
Springer-Verlag.
Marjorie Templeton and John Burger. 1983. Problems
in natural-language interface to dbms with examples
from eufid. In Proceedings of the first conference
on Applied natural language processing, pages 3?
16, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Harry R. Tennant, Kenneth M. Ross, and Craig W.
Thompson. 1983. Usable natural language inter-
faces through menu-based natural language under-
standing. In CHI ?83: Proceedings of the SIGCHI
conference on Human Factors in Computing Sys-
tems, pages 154?160, New York, NY, USA. ACM
Press.
C. Thompson, P. Pazandak, and H. Tennant. 2005.
Talk to your semantic web. IEEE Internet Comput-
ing, 9:75?78.
Guogen Zhang, Wesley W. Chu, Frank Meng, and
Gladys Kong. 1999. Query formulation from high-
level concepts for relational databases. In UIDIS
?99: Proceedings of the 1999 User Interfaces to
Data Intensive Systems, page 64, Washington, DC,
USA. IEEE Computer Society.
102
BioNLP 2007: Biological, translational, and clinical language processing, pages 161?162,
Prague, June 2007. c?2007 Association for Computational Linguistics
Exploring the Use of NLP in the Disclosure of Electronic Patient Records
David Hardcastle
Faculty of Mathematics and Computing
The Open University
d.w.hardcastle@open.ac.uk
Catalina Hallett
Faculty of Mathematics and Computing
The Open University
c.hallett@open.ac.uk
Abstract
This paper describes a preliminary analysis
of issues involved in the production of re-
ports aimed at patients from Electronic Pa-
tient Records. We present a system proto-
type and discuss the problems encountered.
1 Introduction
Allowing patient access to Electronic Patient
Records (EPR) in a comprehensive format is a le-
gal requirement in most European countries. Apart
from this legal aspect, research shows that the provi-
sion of clear information to patients is instrumental
in improving the quality of care (Detmer and Sin-
gleton, 2004). Current work on generating expla-
nations of EPRs to patients suffer from two major
drawbacks. Firstly, existing report generation sys-
tems have taken an intuitive approach to the gener-
ation of explanation: there is no principled way of
selecting the information that requires further expla-
nation. Secondly, most work on medical report gen-
eration systems has concentrated on explaining the
structured part of an EPR; there has been very lit-
tle work on providing automatic explanations of the
narratives (such as letters between health practition-
ers) which represent a considerable part of an EPR.
Attempting to rewrite narratives in a patient-friendly
way is in many ways more difficult than providing
suggestions for natural language generation systems
that take as input data records. In narratives, ambi-
guity can arise from a combination of aspects over
which NLG systems have full control, such as syn-
tax, discourse structure, sentence length, formatting
and readability.
This paper introduces a pilot project that attempts
to address this gap by addressing the following re-
search questions:
1. Given the text-based part of a patient record,
which segments require explanation before being re-
leased to patients?
2. Which types of explanation are appropriate for
various types of segment?
3. Which subparts of a segment require explanation?
The prototype system correctly selects the seg-
ments that require explanation, but we have yet to
solve the problem of accurately identifiying the fea-
tures that contribute to the ?expertness? of a doc-
ument. We discuss the underlying issues in more
detail in section 3 below.
2 Feature identification method
To identify a set of features that differentiate med-
ical expert and lay language, we compared a cor-
pus of expert text with a corpus of lay texts. We
then used the selected features on a corpus of nar-
ratives extracted from a repository of Electronic Pa-
tient Records to attempt to answer the three ques-
tions posed above. First, paragraphs that contain
features characteristic to expert documents are high-
lighted using a corpus of patient information leaflets
as a background reference. Second, we prioritise the
explanations required by decomposing the classifi-
cation data. Finally, we identify within those sec-
tions the features that contribute to the classification
of the section as belonging to the expert register, and
provide suggestions for text simplification.
2.1 Features
The feature identification was performed on two cor-
pora of about 200000 words each: (a) an expert
corpus, containing clinical case studies and med-
ical manuals produced for doctors and (b) a lay
corpus, containing patient testimonials and infor-
mational materials for patients. Both corpora were
161
sourced from a variety of online sources. In com-
paring the corpora we considered a variety of fea-
tures in the following categories: medical content,
syntactic structure, discourse structure, readability
and layout. The features that proved to be best dis-
criminators were the frequency of medical terms,
readability indices, average NP length and the rela-
tive frequency of loan words against English equiva-
lents1. The medical content analysis is based on the
MeSH terminology (Canese, 2003) and consists of
assessing: (a) the frequency of MeSH primary con-
cepts and alternative descriptions, (b) the frequency
of medical terms types and occurences and (c) the
frequency of MeSH terms in various top-level cate-
gories. The readability features consist of two stan-
dard readability indices (FOG and Flesch-Kincaid).
Although some discourse and layout features also
proved to have a high discriminatory power, they
are strongly dependent on the distribution medium
of the analysed materials, hence not suitable for our
analysis of EPR narratives.
2.2 Analysing EPR narratives
We performed our analysis on a corpus of 11000
narratives extracted from a large repository of Elec-
tronic Patient Records, totalling almost 2 million
words. Each segment of each narrative was then as-
sessed on the basis of the features described above,
such as Fog, sentence length, MeSH primary con-
cepts etc. We then smoothed all of the scores for
all segments for each feature forcing the minimum
to 0.0, the maximum to 1.0 and the reference corpus
score for that feature to 0.5. This made it possible to
compare scores with different gradients and scales
against a common baseline in a consistent way.
3 Evaluation and discussion
We evaluated our segment identification method on
a set of 10 narratives containing 27 paragraphs, ex-
tracted from the same repository of EPRs . The seg-
ment identification method proved succesful, with
26/27 (96.3%) segments marked correctly are re-
quiring/not requiring explanation. However, this
only addresses the first of the three questions set
out above, leaving the following research questions
1An in-depth analysis of unfamiliar terms in medical docu-
ments can be found in (Elhadad, 2006)
open to further analysis.
Quantitative vs qualitative analysis
Many of the measures that discriminate expert from
lay texts are based on indicative features; for exam-
ple complex words are indicative of text that is dif-
ficult to read. However, there is no guarantee that
individual words or phrases that are indicative are
also representative - in other words a given complex
word or long sentence will contribute to the readabil-
ity score of the segment, but may not itself be prob-
lematic. Similarly, frequency based measures, such
as a count of medical terminology, discriminate at a
segment level but do not entail that each occurrence
requires attention.
Terminology
We used the MeSH terminology to analyse med-
ical terms in patient records, however (as with prac-
tically all medical terminologies) it contains many
non-expert medical terms. We are currently investi-
gating the possibility of mining a list of expert terms
from MeSH or of making use of medical-lay aligned
ontologies.
Classification
Narratives in the EPR are written in a completely dif-
ferent style from both our training expert corpus and
the reference patient information leaflets corpus. It
is therefore very difficult to use the reference corpus
as a threshold for feature values which can produce
good results on the corpus of narratives, suggest-
ing that a statistical thresholding technique might be
more effective.
Feature dependencies
Most document features are not independent. There-
fore, the rewriting suggestions the system provides
may themselves have an unwanted impact on the
rewritten text, leading to a circular process for the
end-user.
References
Kathi Canese. 2003. New Entrez Database: MeSH.
NLM Technical Bulletin, March-April.
D. Detmer and P. Singleton. 2004. The informed pa-
tient. Technical Report TIP-2, Judge Institute of Man-
agement, University of Cambridge, Cambridge.
Noemi Elhadad. 2006. Comprehending technical texts:
Predicting and defining unfamiliar terms. In Proceed-
ing of AMIA?06, pages 239?243.
162
Composing Questions through
Conceptual Authoring
Catalina Hallett?
The Open University
Richard Power?
The Open University
Donia Scott?
The Open University
This article describes a method for composing fluent and complex natural language ques-
tions, while avoiding the standard pitfalls of free text queries. The method, based on Conceptual
Authoring, is targeted at question-answering systems where reliability and transparency
are critical, and where users cannot be expected to undergo extensive training in question
composition. This scenario is found in most corporate domains, especially in applications that
are risk-averse. We present a proof-of-concept system we have developed: a question-answering
interface to a large repository of medical histories in the area of cancer. We show that the method
allows users to successfully and reliably compose complex queries with minimal training.
1. Introduction
Where early attempts to build natural language question-answering systems focused on
accessing and presenting information held in (closed domain) databases (e.g., Hendrix
et al 1978; Templeton and Burger 1983; Kaplan 1984; Hafner and Godden 1985), the
advent of the World Wide Web has led to a shift towards (open domain) collections
of texts. However, despite significant advances in open domain question answering
since the simple pattern-matching systems of the first TREC competition in 1999,
current systems are still largely restricted to simple questions. They can, for example,
successfully find answers to questions like Which is the highest peak in Africa? or Who first
climbed Kilimanjaro? but they cannot correctly answer more complex questions like:
What is the median height of the top twelve highest peaks in Africa?
Which explorer who climbed Kilimanjaro but not Everest between 1960 and 1995 died in the last
three years before the age of 55?
How many of the explorers who climbed Kilimanjaro but not Everest between 1960 and 1995
did so more than three times during that period?
? Department of Computing, The Open University, Walton Hall, Milton Keynes, Buckinghamshire,
MK7 6AA, UK. E-mail: D.Scott@open.ac.uk; C.Hallett@open.ac.uk; R.Power@open.ac.uk.
The research presented here was supported in part by Medical Research Council grant G0100852
under the e-Science GRID Initiative. Special thanks are due our colleagues on CLEF (Alan Rector,
Jeremy Rogers, and James McKay) and to the CLEF clinical collaborators at the Royal Marsden and
Royal Free hospitals?see www.clinical-escience.org.
Submission received: 17 July 2005; revised submission received: 3 May 2006; accepted for publication:
28 July 2006.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 1
There are many reasons why such queries are unlikely to be successful. For example,
although the first question is very simple to interpret, a correct answer is unlikely to be
available (in a retrievable form) in any individual document in the target collection.
A question-answering system would thus have to first retrieve the heights of each
of the top twelve highest peaks, probably from different documents, and apply some
calculations to obtain their median height, and then generate a response that aggregates
answers from multiple documents. The answer to the second question, on the other
hand, is very simple and likely to be found in a small number of documents, but
the question itself is not trivial to interpret and would require (among other things)
resolving the temporal information, correctly assuming that 55 refers to age at the time
of death, and interpreting the negation but not as referring to the climbing of Everest
only within the specified time span. For the third question, the difficulty comes from
a combination of complex question and complex answer. Retrieving aggregated results
from the World Wide Web also introduces issues of reliability because the sources may
not all be trusted, and there is no guarantee that a different selection of sources would
not yield a contrary result.
For many applications of question answering, the need for complex questions
and trusted answers is paramount?for example, in the medical, legal, and financial
domains, or indeed in any research area?and it is to this scenario that the work we
present here applies. Our goal is to develop a general and intuitive method by which
users can pose complex queries to data repositories; we are particularly concerned
with scenarios where the users are domain experts (i.e., clinicians, lawyers, financiers,
etc.) rather than database experts, where reliability of the answer is critical, where
the method of posing questions should be easy to learn, and where the questions
themselves should be transparent (i.e., clear and unambiguous) to both user and
system.
Current methods for querying databases typically make use of formal query
languages such as SQL. These languages are highly technical and require a great deal
of training to achieve the level of proficiency required to pose the kinds of complex
queries shown in the previous example. Successful query composition requires the user
to be proficient in the query language and have detailed knowledge of the structure
of the database to which the queries are being addressed. Users also need to be
fluent in any formal codes employed to refer to entities in the domain (e.g., disease
classifications, laws, bank codes). For example, in the medical domain alone there are a
large number of clinical terminologies and classifications, used for different purposes:
Some classifications, such as ICD-9, ICD-10, and OPCS-4, are employed in summarizing
the incidence of diseases and operations on a national or worldwide level; others,
such as CPT4 or ICD-9CM, manage the process of billing patients. Each covers a large
number of terms and associated codes: SNOMED-CT alone, to name the most widely
used medical terminology, currently contains some 365,000 individual concepts, and is
being updated continuously (College of American Pathologists 2004). Finally, because
database languages are not transparent, mistakes in query formulation can be difficult
to spot; so even where the system itself may be highly reliable, there is a reasonable
chance that?except for very highly experienced database programmers?the returned
answer may not be an accurate response to the intended question.
A well-known alternative to formal database languages is available in visual query
systems, which make use of graphical devices such as forms, diagrams, menus, and
pointers to communicate the content of a database to the user. They are also widely
used in commercial applications, and research shows that they are much preferred over
textual query languages like SQL, especially by casual and non-expert users (Capindale
106
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
and Crawford 1990; Bell and Rowe 1992; Catarci and Santucci 1995). However, visual
interfaces are also problematic: empirical studies report high error rates by domain
experts using visual object-oriented modeling tools (Kim 1990), and a clear advantage
of text over graphics for understanding nested conditional structures (Petre 1995).
Natural language clearly provides a more intuitive means for users to pose their
questions, but this is also highly problematic because queries expressed in free natural
language are obviously very sensitive to errors of composition (e.g., misspellings,
ungrammaticalities) or processing (at the lexical, syntactic, or semantic level).
2. Natural Language Interfaces
In a typical natural language interface to a database (henceforth NLIDB), the user
requests database records through a query expressed in natural language. The ques-
tion is first parsed and analyzed semantically by a linguistic front-end, which trans-
lates it into an intermediate meaning representation language (typically, some form
of logic). The intermediate language expression is then translated into a database
language (usually SQL) that is supported by the underlying database management
system.
A large number of NLIDBs have been developed in the past 30 years, featuring a
wide range of techniques. The general drawback of these systems1 is that they normally
understand only a subset of natural language. Casual users cannot always discern
which constructions are valid or whether the lack of response from the system is due to
the unavailability of an answer or to an unaccepted input construction. On the positive
side, natural language is far more expressive than formal database query languages
such as SQL, so it is generally easier to ask complex questions using natural language
(NL) than a database language (a single natural language query will have to be trans-
lated into multiple SQL statements). Natural language queries are not only more user-
friendly for the non-expert user, but they also allow easier manipulation of temporal
constructions.
Broadly, research in NLIDBs has addressed the following issues:2
 domain knowledge acquisition (Frank et al 2005)
 interpretation of the NL input query, including parsing and semantic
disambiguation, semantic interpretation, and transformation of the query
to an intermediate logical form (Hendrix et al 1978; Zhang et al 1999;
Tang and Mooney 2001; Popescu, Etzioni, and Kautz 2003; Kate, Wong,
and Mooney 2005)
 translation to a database query language (Lowden et al 1991;
Androutsopoulos 1992)
 portability (Templeton and Burger 1983; Kaplan 1984; Hafner and Godden
1985; Androutsopoulos, Ritchie, and Thanitsch 1993; Popescu, Etzioni, and
Kautz 2003)
1 Leaving aside here the possibility of errors in parsing and interpretation.
2 The extent of NLIDBs research is such that it is beyond the scope of this article to reference a
comprehensive list of projects in this area. For a critical review of various NLIDBs, the reader is
referred to Androutsopoulos, Ritchie, and Thanisch (1995).
107
Computational Linguistics Volume 33, Number 1
In order to recover from errors in any of these steps, most advanced NLIDB systems also
incorporate some sort of cooperative user feedback module that will inform users when
the system cannot construct their query, and ask for clarification.
2.1 Our Solution: A Quasi-NL Interface
The solution that we propose partially overlaps with previous research in NLIDBs, in
that a logical representation is constructed using a NL interface, and then mapped
into the database query language. The difference lies in the nature of the NL interface,
which in our case uses a method that we call Conceptual Authoring; this replaces the
traditional method of free text entry followed by automatic interpretation.
There are two key ideas to Conceptual Authoring. The first (captured by ?Con-
ceptual?) is that all editing operations are defined directly on an underlying logical
representation, governed by a predefined ontology. Instead of typing in text, the user
builds the logical representation directly, so no problem of interpretation arises. The
second key idea (captured by ?Authoring?) is that the user interface presents the
developing logical representation, and the options for editing it, in a way that is
transparent to users?namely, natural language text, possibly supplemented by other
familiar media; users therefore feel that they are performing a familiar activity, a kind
of guided writing, rather than an unfamiliar activity akin to programming.
In general, then, Conceptual Authoring requires that some kind of formal knowl-
edge encoding is edited by direct manipulation of a familiar presentation, the presen-
tation being generated automatically from the underlying knowledge encoding, and
updated every time knowledge is added (or removed) through an editing operation.
The user need not be aware of the underlying formalism any more than a person using
a text editor need be aware of ASCII codes. Conceptual Authoring therefore depends
entirely on language generation technology; it does not use language interpretation
at all.
Various applications of Conceptual Authoring are possible, depending on the
nature of the underlying knowledge and the presentational medium. In the query
editor described in this article, the underlying knowledge is a set of assertions (i.e., an
A-box), and the presentational medium is natural language text. Elsewhere, we have
used the term WYSIWYM (What You See Is What You Meant) for various systems of this
kind that we have developed (Power and Scott 1998): as well as query interfaces they
include programs that generate technical documentation in multiple languages. We use
?Conceptual Authoring? as a more general term that would also cover applications in
which the underlying knowledge included conceptual definitions and rules as well as
assertions, and the presentation medium included diagrams as well as text?provided,
of course, that the diagrams were familiar to the relevant subject-matter experts (e.g., a
molecular structure diagram if the user were an organic chemist).
The basic idea of Conceptual Authoring is that a special kind of natural language
text is generated in order to present successive states of the underlying logical
representation. This text includes generic phrases, called ?place-holders?, which mark
attributes that currently have no value. Place-holders serve as the locations where new
objects may be added. By opening a pop-up menu on a place-holder, the user obtains
a list of short (generated) phrases describing the types of objects that are permissible
values of the attribute; when one of these options is selected, a new object of the
specified type is added. New text is then generated to present the modified logical
representation, including the attributes of the new object. As more information is added
108
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
about an object, it will be presented by longer spans of text, comprising sentences or
perhaps even paragraphs. These spans of text are also mouse-sensitive, so that the
associated semantic material can be cut or copied. The cutting operation removes the
logical fragment that was previously the value of an attribute, and stores it in a buffer,
where it remains available for pasting into another suitable location. The text associated
with the fragment may or may not remain the same, depending on the context of the
new location.
As an illustration, suppose that the user wishes to define an event that might
naturally be expressed by the sentence The doctor examined the patient with a stethoscope.
The underlying logical structure could be an event object of type examined, with
attributes for actor, actee, and instrument; this will of course be only one among
many events allowed by the ontology. To define this content using a Conceptual
Authoring interface, the user begins from a text containing a place-holder for any kind
of event. By clicking on this place-holder, the user obtains a list of event patterns, each
shown as a short phrase corresponding to a specific event type from the ontology:
FEEDBACK TEXT
[Some event]
MENU OF OPTIONS
.....
consulted
examined
treated
visited
.....
When the user selects examined from this list, an event object of type examined is added
to the underlying semantic model, and the feedback text is regenerated to express the
new event and its attributes (as yet unspecified), which are shown by short phrases in
square brackets (the place-holders). A color code on place-holders indicates whether
an attribute is obligatory (it must be specified) or optional (it can be left unspecified);
here for convenience we use boldface for obligatory, and italics for optional. By clicking
on a place-holder, say the first, the user can now obtain options for specifying the
corresponding attribute (in this case the actor).
FEEDBACK TEXT
[Some person] examined [some person] [in some way]
MENU OF OPTIONS
.....
doctor
nurse
patient
.....
109
Computational Linguistics Volume 33, Number 1
By making successive choices in this way, the user will complete the desired proposi-
tion, perhaps through the following sequence:
[Some event].
[Some person] examined [some person] [in some way].
The doctor examined [some person] [in some way].
The doctor examined the patient [in some way].
The doctor examined the patient by using a stethoscope.
Note that because the feedback text is always generated by the system, we can try to
design feedback texts in a way that minimizes ambiguity. We might, for instance, prefer
to avoid the more natural phrase ?with a stethoscope?, which introduces the well-known
PP-attachment ambiguity, in favor of the slightly clumsy but unambiguous alternative
employed herein.
As well as introducing new objects on place-holders, the user can select a span
representing a filled slot and perform Cut or Copy. For instance, from the feedback
sentence reached in the last example, the user could select the span ?the patient? and
choose Cut, thus emptying the slot and reinstalling the place-holder:
The doctor examined [some person] by using a stethoscope.
Having freed up the slot in this way, the user might next select ?The doctor?, choose
Copy, select the place-holder ?[some person]?, and choose Paste. The Copy operation
here applies to the actual instance selected: It does not create a new instance of the same
type. Therefore, after the Paste operation, the doctor instance fills two slots, both the
actor and the actee of the event. The resulting coreference is shown by the wording of
the feedback text:
The doctor examined himself by using a stethoscope.
Conceptual Authoring (or WYSIWYM) has been applied as a tool for creating
knowledge content for multilingual generation of instruction manuals (Power and Scott
1998; Power, Scott, and Evans 1998; Scott, Power, and Evans 1998) and pharmaceutical
leaflets (Bouayad-Agha et al 2002). It has also been applied in a tool for posing queries
to a knowledge base of legal and regulatory information about maritime shipping
(Piwek et al 2000; Piwek 2002; Evans et al in press). In some ways the interface
resembles early menu-based techniques like Tennant, Ross, and Thompson (1983) and
Mueckstein (1985); however, this resemblance is only superficial, because in these
techniques the user edits a linguistic structure, whereas in Conceptual Authoring all
editing operations are defined on an underlying logical structure.
3. A Test Application: Electronic Health Records
Typically, an individual?s medical record is a collection of documents held in his or her
doctor?s office; most people will also have other records held at other sites, such as
110
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
hospitals or clinics they have attended, or specialists they have seen. These records are
primarily textual, and the record of the average hospital patient will consist of a large
number of documents?around 100 narratives, plus hundreds of items of structured
data derived from laboratory, pharmacy, or other hospital subsystems. There is a
significant move?not just by medical providers, but by governments (e.g., the National
Programme for Information Technology in Medicine [NPfIT] in the UK, and various
e-Health initiatives in other countries)?to replace or supplement the current form of
patient records with electronic records; these are intended to be not simply electronic
text files of the existing records, but collections of ?messages? held in databases and
accessible at the point of care. One of the disadvantages of text-only health records is
that the information contained within them, because it is ?locked into? the text, is not
available for statistical manipulation and cannot be easily interrogated.
Previous studies (Gorman and Helfand 1995; Ely et al 2000; Jerome et al 2001;
Estrella et al 2004; Koonce, Giuse, and Todd 2004), as well as our own preliminary
analysis, show that free text queries written by medical professionals are mostly
complex and often highly ambiguous. From this we conclude that when querying
medical databases, such users need to be able to construct queries that are complex,
both in volume of material and in the organization of this material (e.g., into temporal
or conditional constructions). Traditionally, user interfaces to medical databases have
been complex visual interfaces that are unsuitable for use by a casual user (Nadkarni
and Brandt 1998; Shahar and Cheng 1999).
Electronic health records provide a good example of the kind of application
for which question-answering systems are required for accessing large collections of
trusted closed-domain data. Not only is there a requirement for complex queries of
the sort that are extremely difficult to achieve in current natural language question-
answering systems but, for obvious reasons, the veracity of the results of any query is
critical, making it doubly important that queries put to the system, and their resulting
responses, are unambiguous and clearly understandable to the user. Because the users
will be medical professionals, with great demands on their time, the ease of use
of the question-answering system is also extremely important. We have applied our
Conceptual Authoring question-answering method to one such application: the Clinical
E-Science Framework (CLEF).
3.1 The Clinical E-Science Framework
The Clinical E-Science Framework (CLEF) aims at providing a repository of well-
organized clinical histories that can be queried and summarized both for biomedical
research and clinical care (Rector et al 2003). In this context, the purpose of the query
interface is to provide efficient access to aggregated data for performing a variety of
tasks: assisting in diagnosis or treatment, identifying patterns in treatment, selecting
subjects for clinical trials, and monitoring the participants in clinical trials. Although
the CLEF architecture is largely independent of any particular area of medicine, it is
currently being applied to cancer, in collaboration with the Royal Marsden Hospital in
London, one of the primary centers for the treatment of cancer in Britain.
The current CLEF database repository contains 22,500 patient records,3 containing
a total of more than 400,000 database entries, some 3.5 million record components
3 At present, the repository contains records of deceased patients only. In the near future, it will grow
significantly with the addition of live patient records.
111
Computational Linguistics Volume 33, Number 1
and more than 5 gigabytes of data, implemented as a relational database that stores
patient records modeled on an archetype for cancer developed by Kalra et al (2001).
The information on each patient comes from hundreds of documents, and a single care
episode or clinical problem is likely to be mentioned repeatedly in several documents.
Within CLEF, a patient record is organized as a collection of individual entries, each
entry representing an instance of the cancer archetype.
3.2 Users and Extent
The CLEF query system is designed to answer questions relating to patterns in medical
histories over sets of patients in the repository. At this point, the system supports
attribute-centric queries asking for aggregated results, such as:
{Absolute count/percentage/statistical measure} of patients with certain characteristics.
The answers to such queries can be produced by simple interrogation of the
database, because they do not require inferences over the repository of patient records.
However, the query interface is also coupled with a data-mining module to provide
answers to more complex queries, such as
Given certain conditions, what is the treatment with the highest chance of success for a patient
with certain characteristics?
The query interface can also be used for accessing information about individual
patients.
The interface is designed for casual and moderate users who are familiar with the
semantic domain of the repository (but not with its actual structure or encoding) and
who require queries of little variance but with relatively high structural complexity.
Under this description come three primary types of users, each having a different goal
in interrogating the repository:
 clinicians, who use it for assisting in diagnosis or treatment
 medical researchers, who use it for identifying patterns in treatment,
selecting subjects for clinical trials, or monitoring the participants in
clinical trials
 hospital administrators, who use it for collecting information about
patterns of treatment, frequency of tests, hospital admissions, and so on
Among these, we expect those users with little or no knowledge of formal database
languages (e.g., SQL) to be the main beneficiaries of the query interface, although in the
Evaluation (Section 6) we will show that even for SQL-aware users, the query interface
represents an improved alternative to standard SQL. We also target the interface at users
who are unfamiliar with medical encoding schemes, such as SNOMED or ICD, or who
prefer to use natural language expressions instead of medical codes.
3.3 Previous Work on Querying Clinical Databases
There are a number of query systems for clinical databases, mostly designed for
formulating patient-centric queries and typically using visual interfaces. For example:
112
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
Figure 1
Architecture of the CLEF query interface.
The Columbia-Presbyterian Medical Center Query Builder works off the medical
center?s data repository and generates both HL7 and Arden Syntax4 for several
categories of data; for example, patient demographics, laboratory values, medi-
cations, and diagnoses (Wilcox, Hripcsak, and Chen 1997). The user interface is a
simple HTML-based application that allows users to select the type of data they
want to query and to specify constraints on it.
TrialDB (formerly ACT/DB) is a clinical study data management system that provides
a complex visual interface for formulating attribute-centric temporal queries
(Nadkarni and Brandt 1998; Deshpande, Brandt, and Nadkarni 2001, 2003). The
interface allows for attributes to be searched and selected by specifying key words
or part of the attribute name/caption. Once an attribute is selected, the user may
optionally specify that an aggregate value for that attribute be returned. Searching
criteria can be combined using boolean operators.
KNAVE is a visualization and navigation model that enables clinicians to query a specific
patient record for time-oriented raw data, external interventions, abstractions, and
temporal patterns, and to visualize the results of the temporal query (Shahar and
Cheng 1999).
Natural language query interfaces have been used far less extensively and for more
restricted tasks. For example, the HERMES system (Rivera and Cercone 1998) allows
the formulation and interpretation of ad hoc queries relating to doctor and patient
demographic information, patients? personal details, visit information, and insurance
coverage information. It is designed as an aid to hospital administration, and not to
clinical care.
4. The CLEF Query Interface
The CLEF Query Tool has four components which are invoked in sequence whenever a
query is posed by the user (Figure 1). The first is the Query Editor, a natural language
interface which guides the user in building a clear and valid query. The output of
4 HL7 is an internationally adopted communication language used for healthcare data. It covers the whole
scope of healthcare communication (http://www.hl7.org). Arden Syntax is a standard specification of
defining and sharing modular health knowledge bases, providing procedural representations of medical
knowledge and explicit definitions.
113
Computational Linguistics Volume 33, Number 1
this component is a logical representation underlying the query text that the user has
created. The second component, the Query Transcoder, converts this logical represen-
tation to a Java encoding accepted by the CLEF database management system (DBMS).
In this form, the query is sent to the DBMS, which recodes it again into SQL and
submits it to the database. The result of the query, usually a list of records for relevant
patients, returns to the third component of the Query Tool, the Result Processor, which
transforms the raw data into an aggregated representation defining the content of the
answer. This representation then passes to the fourth and final component, the Answer
Renderer, which configures a convenient display for the user by combining fluent text
with diagrams (tables and charts).
We now describe these components in more detail.
4.1 Query Editor
The Query Editor allows the user to create a logical representation of the query by means
of Conceptual Authoring. When beginning a new query, the user is shown a minimally
specified feedback text based on a model of query structure in this domain; this model
is described in Section 5. By inserting content in the initial place-holders, the user can
build up the full text of a query in a few dozen choices, a process that takes a few
minutes once the user has become accustomed to the editing process (for details, see
Section 6). A query is potentially complete when all obligatory slots have been filled.
This is easy for the user to verify because obligatory place-holders are shown in red:
When no red text remains, the query is complete. At this point, the user can hit the
Submit button, whereupon the current A-box is passed to the Query Transcoder.
4.2 Query Transcoder
The Query Transcoder takes as input an A-box from the Query Editor, and recodes it
in the format expected by the DBMS. This conversion depends on a mapping between
the ontology (or T-box) employed by the Query Editor, and the concepts of the database
archetype. The T-box cannot be exactly the same as the archetype, because it has to serve
a different purpose?that of providing logical representations suitable for generating
linguistic structures like clauses and nominals.
4.3 Result Processor
The Result Processor receives the data returned by the DBMS, normally a set of records
for relevant patients, and constructs the logical representation of an answer for the user.
A typical result set received from the DBMS would list the patients that fulfilled the
requirements of the query, and specify, for each patient, the features AGE and GENDER
along with values for each of the query elements. For example, the query
How many patients between 30 and 70 years of age, who had a clinical diagnosis of malignant
neoplasm of breast and underwent surgery, had a haematoma after surgery?
may yield the result set shown in Figure 2.
From such data, the Result Processor plans aggregate presentations in which
patients are grouped according to the age/gender breakdown and the individual query
terms. For each query term, the data are split into a dynamically determined number of
age groups, and for each age group the patients are further subdivided by gender.
114
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
Figure 2
Example of a result set.
4.4 Answer Renderer
The data thus organized are presented to the user in three types of formats: tables, charts
and text. Each individual chart is accompanied by an automatically generated caption
that explains its content.
Captions are generated using template-based techniques, where fillers are provided
by the same data that were used for generating the chart. For example, the results we
saw in Figure 2 are presented as an answer that includes the bar chart in Figure 3.
This is accompanied by a textual explanation in the form of a caption, a fragment of
which reads:
Your query has returned 965 patients between 30 and 70 years of age who had a clinical
diagnosis of malignant neoplasm of breast and underwent surgery. This chart displays
the distribution of patients in five age groups according to their gender and time of
haematoma after surgery.
? In the 30?39 years age group there were 163 patients (2 men and 161 women):
151 patients did not have haematoma after surgery, 12 patients had haematoma
after surgery.
? In the 40?49 years age group there were 326 patients (no men and 326 women):
304 patients did not have haematoma after surgery, 22 patients had haematoma
after surgery.
? In the 50?59 years age group there were 363 patients (8 men and 355 women):
337 patients did not have haematoma after surgery, 26 patients had haematoma
after surgery.
? In the 60?69 years age group there were 110 patients (2 men and 108 women):
97 patients did not have haematoma after surgery, 13 patients had haematoma
after surgery.
? In the 70?79 years age group there were 3 patients (one man and two women):
two patients did not have haematoma after surgery, one patient had haematoma
after surgery.
5. Query Model
A controlled editing environment is most effective when based on a model of the kinds
of queries that users will wish to make. There is a trade-off here between flexibility and
ease of use. If we have no preconceptions about the general nature of queries, we have
to provide users with a wide set of possible patterns, leaving them to search for the
particular pattern they happen to want. If instead we can assume that the query will
belong to a known set of patterns, the editor can help the user to get started by offering
a manageable list of alternatives, so avoiding the ?blank page? problem.
Investigations on a taxonomy of queries posed by general practitioners in an
outpatient setting has shown that in primary care, queries are relatively simple and
115
Computational Linguistics Volume 33, Number 1
Figure 3
Chart generated as part of a response displaying the distribution of patients who developed and
did not develop haematoma according to their age and gender.
generally ask for evidence-based advice for treatment decisions (Ely et al 2000). For
example, of 64 generic question types, the three most common are:
What is the drug of choice for condition X?
What is the cause of symptom X?
What test is indicated in situation X?
In contrast to these findings, our consultation with cancer clinicians revealed that
questions posed in a clinical research setting tend to have a more complex nature
and to be directed at groups of patients, searching for relationships rather than simple
values:
What is the average time of relapse in Acute Myeloid Leukaemia for patients with a complete
response after two cycles of treatment?
Can this time be linked to the cytogenetic findings in the original blood sample?
What is the median time between first drug treatment for metastatic breast cancer and death?
116
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
Our policy in CLEF has been to aim first at a relatively specific model, under the
guidance of the relevant experts?clinicians and medical researchers in the area of
cancer.
5.1 Elements of a Basic Query
The structure of a typical query (according to our experts) is shown by the following
relatively simple example:
For all patients with cancer of the pancreas, what is the percentage alive at five years for those
who had a course of gemcitabine?
As can be seen, this query breaks down into three elements: the set of relevant patients,
defined by a problem; the partition of this set according to treatment; and the further
partition according to outcome, from which the percentage can be calculated. For
maximum clarity, the Query Editor can format the query so that these three elements
are marked explicitly and presented separately:
Relevant subjects: Patients with cancer of the pancreas.
Treatment profile: Patients who received a course of gemcitabine.
Outcome measure: Percentage of patients alive after five years.
Generalizing from this example, we can identify the following basic query pattern:
Relevant subjects: Patients with [some diagnosis].
Treatment profile: Patients who received [some treatment].
Outcome measure: [Measure] of patients [with some status] [at some point in time].
An important requirement on this formulation of the query is that it should
be unambiguous?namely, that users should understand correctly how the outcome
measure will be calculated. We assume that the calculation will proceed through the
following steps. First, retrieve all the patients in the database who satisfy the conditions
in the first two paragraphs (Relevant subjects and Treatment profile)?in this example,
all patients with cancer of the pancreas who received a course of gemcitabine. Call this
set S and let its cardinality (i.e., the number of patients in the set) be C(S). Next, find
the subset of S also satisfying the outcome condition?in this example, the patients still
alive after five years. Call this set M and its cardinality C(M). Finally, divide C(M) by
C(S) and express the result as a percentage.
With a slight elaboration of this basic pattern, we can obtain a second kind of query,
which requests a comparison rather than a single value:
For all patients with cancer of the pancreas, compare the percentage alive at five years for those
who had a course of gemcitabine with those who didn?t.
Again this can be presented to the user using a separate paragraph for each element:
Relevant subjects: Patients with cancer of the pancreas.
Treatment profile: Patients who received a course of gemcitabine, compared with
patients who did not.
Outcome measure: Percentage of patients alive after five years.
117
Computational Linguistics Volume 33, Number 1
For a comparison question we need to compute two outcome measures, so the steps in
the calculation have to be elaborated as follows. First, retrieve two sets of patients, S1
and S2, satisfying the conditions that we want to compare. In the example, S1 will be
the set of patients with cancer of the pancreas who had a course of gemcitabine; S2 will
be the set of patients with the same type of cancer but no gemcitabine treatment.5 Next,
for each set, find the subset of patients still alive after five years: Call these subsets M1
and M2. Finally, compute the measures to be compared by dividing C(M1) by C(S1), and
C(M2) by C(S2), and expressing the resulting ratios as percentages.
5.2 Complex Queries
Each element of a query can be made more complex in two ways. First, it can be replaced
by a conjunction or disjunction, so that the query in a sense becomes several queries
requiring several answers. Second, the content of the description can be elaborated, for
example by adding more qualifications. Here is an example of the first kind:
For all patients with a brain glioma, what percentages are still alive at 1, 2, and 5 years if they
take Imatinib Mesylate every day?
This can be analyzed as a single relevance group, single treatment, and multiple
outcome measures (survival at 1, 2, and 5 years). Separate answers for these measures
will be needed. An example of the second kind is the following:
For all patients with cancer of the vulva that is locally advanced and/or metastatic or recurrent,
and where this cannot be treated with either surgery or radiotherapy of any kind, what is the
survival rate for those given Taxol only?
We assume this is a single rather than a multiple query, and that separate answers are
not needed for the various conjunctions and disjunctions. The treatment profile (Taxol)
and the outcome measure (survival rate) have a content that can be easily specified?a
single choice from a menu would suffice. However, the set of relevant patients requires
a very elaborate description because there are so many qualifications.
5.3 Multiple Relevance Sets
When the phrase describing a relevance set includes a conjunction or disjunction, there
may be ambiguity over whether the intended query is single or multiple. Compare these
three patterns:
(1) For all patients with lung cancer, and for all patients with breast cancer . . .
(2) For all patients with lung cancer and breast cancer . . .
(3) For all patients with lung cancer or breast cancer . . .
Here (1) seems a clear case of a multiple query, whereas the others are ambiguous
but tending to a single-query interpetation. It is hard to eliminate such ambiguities
altogether while wording the query in a way that is reasonably natural, but at least we
can impose consistency by using different realization devices for the two cases?for
5 These sets are obviously disjoint.
118
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
example, bulleted lists for conjunctions (or disjunctions) that imply multiple queries,
and discourse connectives (and, or) for ones that imply single queries. For example:
Relevant subjects:
? Patients younger than 60 years of age who have had bad prognosis
myelodysplastic syndrome only for at least six months
? Patients younger than 60 years of age who have had acute
myelogenous leukaemia caused by bad prognosis myelodysplastic
syndrome for at least six months
versus
Relevant subjects:
? Patients younger than 60 years of age who have either had bad
prognosis myelodysplastic syndrome only for at least six months or
acute myelogenous leukaemia caused by bad prognosis myelodysplastic
syndrome for at least six months
In the first we have two relevance sets; in the second we have only one.
5.4 Multiple Treatment Profiles
A similar ambiguity is found when several treatment profiles are mentioned. Either
there are several queries, or there is a single query concerning a logical combination of
the treatments. The following query text (written as an example by a medical researcher)
could be interpreted either way:
For all patients younger than 60 years of age who have either had bad prognosis myelodysplastic
syndrome only for at least six months or acute myelogenous leukaemia caused by bad prognosis
myelodysplastic syndrome for at least six months, what is the survival rate if you give them
intensified remission induction chemotherapy followed by either an autologous or allogeneic
bone marrow transplant?
Perhaps the researcher?s aim is to compare autologous bone marrow transplants
with allogeneic ones. Alternatively, it might not matter whether the transplant is
autologous or allogeneic provided that it is one or the other, as suggested by the singular
verb (?what is the survival?). In the query interface, the ambiguity can be avoided in the
same way as before, by using bullets to mark separate queries. For example:
Treatment profiles:
? Intensified remission induction chemotherapy followed by an
autologous bone marrow transplant
? Intensified remission induction chemotherapy followed by an
allogeneic bone marrow transplant
versus
Treatment profiles:
? Intensified remission induction chemotherapy followed by an
autologous or allogeneic bone marrow transplant
In the first we have two treatment profiles and hence separate queries; in the second we
have only one.
119
Computational Linguistics Volume 33, Number 1
5.5 Multiple Outcome Measures
There are several examples in which survival rates are requested at, say, one year,
two years, and five years. It makes no sense to combine these into a single query, so
they are always interpreted as separate queries.
5.6 Comparison Queries
Comparison queries are those that ask for certain outcomes of separate groups of
patients that do not share common diagnosis or treatment profiles.
Compare survival rates at 5 years after diagnosis for patients with adenocarcinoma who received
chemotherapy and patients with invasive ductal carcinoma who received radiotherapy.
We represent such queries as two independent queries, with separate profiles.
5.7 Elaborate Descriptions
Descriptions are boolean combinations of properties. A description can be elaborate
either because it contains many boolean operators, or because the properties are
themselves complicated. The following description of a reference set is elaborate in
both ways:
For all patients younger than 60 years of age who have either had bad prognosis myelodysplastic
syndrome only for at least six months or acute myelogenous leukaemia caused by bad prognosis
myelodysplastic syndrome for at least six months, what is the survival rate. . . ?
Complex boolean combinations of this kind often cannot be presented in running
prose without the scopes of the boolean operators becoming unclear. To avoid this
problem, the feedback text generator formats complex boolean combinations using
hierarchical layout:
Relevant subjects:
? Patients with the following properties:
a. They are younger than 60 years of age
AND
b. They have one of these properties:
b1. They have had bad prognosis myelodysplastic syndrome only
for at least six months
OR
b2. They have had acute myelogenous leukaemia caused by
bad prognosis myelodysplastic syndrome for at least six
months
Descriptions of treatment profiles can be elaborate in the same ways. The following
excerpt has two treatment profiles, the first using a combination of AND and NOT, the
second using AND combined with temporal sequence (marked by THEN).
. . . compare the survival rates over time for those who had no surgery but did have mitomycin C
injected into the bladder once a month, with those who had transurethral resection of the tumor
and then a single one-time injection of mitomycin C into the bladder.
120
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
The corresponding part of the feedback text is laid out as follows:
Treatment profiles:
? a. NO surgery
AND
b. Mitomycin C injected into the bladder once a month
? a. Transurethral resection of the tumor
THEN
b. A single one-time injection of mitomycin C into the bladder
5.8 Representing Time
Queries about patient records contain many references to events occurring at particular
times: diagnoses, tests, treatments, deaths, and so forth. These time specifications are
crucial. To deal with them effectively, the query tool must meet two requirements.
First, the feedback text should express temporal relations naturally and unambiguously,
using familiar devices like tenses and adverbials. Second, the resulting conceptual
representation (in the A-box) must be aligned with the fields through which time is
represented in the database.
Like most databases, the CLEF medical records employ several types of time stamp.
First of all, an event has a valid time, the moment when it actually took place. For
example, if a mastectomy was performed on 29th January 2000, the valid time would be
some representation of this day, perhaps ?29-01-2000?, assuming a granularity calibrated
in days (rather than hours, weeks, etc.). Next, an event has a recording time, the
moment when it was written down. Obviously this might differ from the valid time,
although they would be the same if the doctor kept prompt records. We also use a
concept of query time, the moment when a query was formulated by the CLEF user:
this is needed in order to interpret deictic time references in the feedback text, based
for example on tenses or on phrases like ?after 1995?, which can be interpreted to mean
?from 1995 until now?.
For some events the valid time can be a single moment, specified for example by a
date. For events that last for longer intervals, like a whole course of treatment, two valid
time stamps have to be given, one for the start time and one for the end time. Of course
this distinction is related to granularity. With a granularity based on days, a week has
to be treated as an interval with a start date and an end date; with a granularity based
on weeks, the same week could be identified by a single time stamp (e.g., ?W40-2000?,
meaning the 40th week of the year 2000).
To model time effectively in queries, we need to provide a range of natural and
clear options, and map them to the time stamps used in the database. At present, the
temporal modifiers offered during query editing are as follows:
 between [date 1] and [date 2]: interpreted as a closed interval [date 1, date 2]
 after [some date]:interpreted as a closed interval [this date, query time]
 before [some date]
 in [this year]: interpreted as [01/01/this year, 31/12/this year]
 any of the above, where instead of a specific date the user enters an index
event after the surgery; in this case, the implied time will be computed by
the DBMS instead of explicitly entered by the user
121
Computational Linguistics Volume 33, Number 1
 event1 {while/at the same time as/during event2} : will be interpreted as two
overlapping time intervals corresponding to the two events
For example, such time expressions cover queries like: patients diagnosed with cancer
before 1999, or patients who received chemotherapy within 5 months of surgery. The interface
allows Allen?s 13 basic interval relationships to be expressed in natural language (Allen
1984).
In principle we could require users to associate a time stamp with every event
mentioned in a query; however, by imposing this further requirement on users we
would pay a high price in usability, virtually doubling the number of operations needed
in order to complete the query, and damaging the transparency of the resulting text.
In the CLEF query interface we have decided instead to associate default values to
time descriptions and to make the time stamp anchors visible only on demand in the
feedback text. The output text will contain all the time stamps, with the values either
entered by the user or defaulted, so allowing the user to review the query and amend it
where necessary.
6. Evaluation
The best evaluation of any question-answering system is one which looks at real users
making information-seeking requests in real-life contexts. Because the complete CLEF
system is not yet ready for deployment, this is impractical at this stage. However, we
have been able to perform usability tests on the query interface in isolation from the
full system, and this is what we report on here. Our current study does not cover
the Query to SQL Translation and the Answer Retrieval components, which are part of
the server components side of the query interface. This separation is not always possible
in practice. For example, we cannot at this stage test the full range of queries that can
be constructed in the interface, because some are not yet supported by the back-end.
Similarly, we can only assess the time necessary for editing queries, not for retrieving
answers, because this is almost entirely dependent on the communication procedure
and on the speed of the SQL translator.
We have thus far conducted two formal experiments, to address the following
questions:
 Are users able to successfully compose complex queries using the system?
 Can the system be used with minimal training?
 Are the queries, as presented in the interface, easily understandable?
6.1 Experiment 1: Query Composition
As mentioned earlier (Section 1), one of the main desiderata behind the design of our
querying method is that it should be intuitive. With respect to the system we have
implemented for CLEF, what this means is that medics and bio-informaticians should
be able to pose the kind of complex queries that they require, without the need for
extensive training, or for knowledge of the structure or language of the underlying
repository. This experiment tests the extent to which our querying method fulfills these
requirements.
122
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
Subjects. Fifteen medics and bio-informaticians participated in the experiment. All had
previously been granted clearance6 to see the information in the confidential repository
of patient records. All subjects were knowledgeable in the domain of cancer, and all
but two had no knowledge of the representation language of the repository (SQL), or
of how the data contained therein were structured; none had any prior experience with
the query-formulation interface.
Methodology. Each subject was given a short (5?10 minute) introduction to the interface,
which included a demonstration of the construction of a fairly simple query. Subjects
were then given a set of four queries, which they were asked to compose using the
interface. To increase the difficulty of the task, the questions presented to the subjects
avoided, where possible, the wording required by the user interface, so that users were
obliged to think about the meaning rather than to aim for particular target phrases. To
avoid effects of practice, we varied randomly the order in which the questions in the
set were presented to subjects. Subjects were allowed as much time as they needed to
compose each query.
For each subject, we measured the time taken to build each query, and recorded the
number of operations used for constructing it.
Materials. The materials for the experiment consisted of the following set of four queries:
How many patients who received surgical treatment for malignant neoplasm of the central
portion of the breast had no curative radiotherapy?
How many patients between the ages of 40 and 60 when they were first diagnosed with lung
cancer (malignant neoplasm of bronchus or lung, unspec) received radiotherapy and had
a platelet count higher than 300 and a leukocytes count lower than 3?
What percentage of patients under the age of 60 treated for breast cancer (malignant neoplasm
of breast, unspec) died within 5 years of a mastectomy?
How many patients with acute lymphoid leukaemia have been given chemotherapy?
These are representative of the query types that emerged from an earlier requirements
analysis with oncologists and cancer bio-informaticians. They also vary in their levels of
structural complexity and in the number of interface operations required to successfully
complete them.
As can be seen, these questions are far more complex than the queries standardly
posed to search engines or to most other interactive query engines (as described,
for example in [Hovy, Hermjakob, and Ravichandran 2002]; [Soricut and Brill 2004];
[TREC 2005]).
Results. The main finding of this experiment is the achievement of 100% success in
subjects? ability to use the interface for the purpose for which it was intended: All
subjects successfully composed all queries. The mean completion time per query was
3.9 minutes (noting that subjects were under no time pressure to complete the individual
6 By the UK Medical Research Ethics Committee (MREC).
123
Computational Linguistics Volume 33, Number 1
Figure 4
Mean completion time for queries in order of occurrence.
queries).7 Figure 4, which gives the average time to completion across all subjects, shows
that subjects learned to use the interface quickly: they take much longer on their first
query, and their performance asymptotes by the time they get to the second query.
This effect is confirmed by an analysis of variance (ANOVA)8, which shows a highly
significant effect of order of presentation (F = 9.8427; p< .0001). Furthermore, significant
differences were found between subjects? performance on the first query they composed
compared to the second, third, and the fourth (each at p < .01 on the Tukey HSD
test). However, application of the same test showed no significant difference in subjects?
performance on the second versus third, second versus fourth, or the third versus fourth
composed query.
Because the queries vary in structural complexity, some will require the user to
perform more interface actions than others, and so one would predict a difference in
subjects? performance (i.e., time to completion) on the individual queries; this was borne
out by the analysis (ANOVA, F = 5.5015; p < .0028).
If the method is easy to learn, one would predict that subjects? proficiency with the
interface will increase fairly quickly as they move from the first query they encounter
to the last, irrespective of complexity. This can be tested by measuring subjects?
performance on the interface in terms of the number of interface operations (mouse
clicks and selections) they perform, normalized for complexity: a value of 1 would
mean that subjects perform twice as many operations as are required; a value of 0 would
mean that subjects perform the minimal number of required operations (i.e., perfect
performance). The result of such an analysis is shown in Figure 5. The picture that
emerges from this is one where, overall, subjects are very efficient, achieving an average
score of 0.19 over their first four encounters with the method. They make a fair number
of false starts when composing their first query, but become extremely proficient by
the time they get to their second query, and near perfect by the time they get to the
fourth. Analysis of variance9 shows a highly significant effect of order of presentation
(F = 7.4993; p < .0004). Once again, the Tukey HSD Test shows a significant difference
between the first query encountered and each of the subsequent ones (p <. 01), and that
the differences between the second and third, the second and fourth, and the third and
fourth, were nonsignificant.
7 For the last 5 subjects, all of whom used a version of the interface that had been improved to respond
faster to interface actions, this average went down to 2.7 minutes.
8 One-way ANOVA for correlated samples.
9 One-way ANOVA for correlated samples.
124
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
Figure 5
Proficiency with the interface as a function of experience.
6.2 Experiment 2: Clarity of the Queries
Interfaces to databases based on natural language interpretation inevitably suffer from
the ambiguity and imprecision of the input texts, unless users can be trained in a
controlled language. Our method of composing queries avoids this problem altogether:
because the natural language feedback text is generated by the system rather than the
user, there is no need for the system to choose among alternative interpretations. Of
course, this does not guarantee that the query text is equally transparent to the user: this
will depend on the efficacy of our feedback text design?the point we wish to evaluate
in the present experiment, which explores the extent to which composed queries, as
presented in the feedback texts, can be clearly understood.
Subjects. Fifteen subjects participated in the experiment. Of these, ten had previously
participated in Experiment 1; the new subjects had the same profile as those previously
seen.
Methodology. Subjects were given a paper-based questionnaire containing 24 trials, each
showing a completed complex query as presented in the interface (i.e., as a ?feedback
text?). Each query was associated with three alternative interpretations, presented as full
natural language questions: only one of these represented the correct meaning; the other
two represented plausible but incorrect meanings. Subjects were given a forced-choice
task to identify which of the three alternatives corresponded to the meaning of the given
feedback text.
The queries were presented to all subjects in the same (random) order. We devised
five presentation sets, each containing a different ordering of the options for each query,
and these were randomly assigned to subjects. We suggested to subjects that a useful
strategy might be to read the alternatives before looking at the associated feedback text.
There was no time limit.
Materials. The materials comprised four examples, each of six patterns of ambiguity:
Type 1: Attachment of temporal expression. Most events can have a temporal expression
associated. When there is more than one event that could be subsumed by a temporal
expression, the text may become ambiguous. For example:
Relevant subjects: patients with a clinical diagnosis of breast cancer
Treatment: patients who did not receive adjuvant chemotherapy in the past year
Tests: [ ]
Outcome: absolute number of patients
125
Computational Linguistics Volume 33, Number 1
Options:10
 How many patients diagnosed with breast cancer had no adjuvant chemotherapy
in the past year?
 How many patients treated for breast cancer in the past year had no
adjuvant chemotherapy?
 How many patients diagnosed with breast cancer in the past year had no
adjuvant chemotherapy?
Type 2: Scope of conjunctions. Whenever a complex expression contains a combination
of conjunctions and disjunctions, potential ambiguities may occur, especially when
combined with negations or prepositional phrases. For example:
Relevant subjects: patients with a clinical diagnosis of invasive ductal carcinoma
Treatment: patients who received breast conservation surgery, no auxillary surgery,
and radiotherapy
Tests:[ ]
Outcome: absolute number of patients
Options:
 How many patients diagnosed with invasive ductal carcinoma underwent breast
conservation surgery, did not undergo auxillary surgery, and received
radiotherapy?
 How many patients diagnosed with invasive ductal carcinoma underwent
breast conservation surgery, did not undergo auxillary surgery, and did
not receive radiotherapy?
 How many patients diagnosed with invasive ductal carcinoma did not
undergo breast conservation surgery, did not undergo auxillary surgery,
and received radiotherapy?
Type 3: Scope of conjunctions plus attachment of temporal expression. This is an extension of
the first two cases, where a temporal expression post-modifies an expression that is part
of a conjunction of events. For example:
Relevant subjects: patients with a clinical diagnosis of malignant neoplasm,
unspecified
Treatment: patients who received radiotherapy and chemotherapy within 1 year
of the diagnosis
Tests:[ ]
Outcome:absolute number of patients
Options:
 How many patients diagnosed with cancer had radiotherapy and chemotherapy
both within 1 year of diagnosis?
 How many patients diagnosed with cancer had radiotherapy within
1 year of diagnosis and also had chemotherapy at any time?
10 In the examples that follow, the correct interpretations are indicated with italics.
126
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
 How many patients diagnosed with cancer had radiotherapy and
chemotherapy and received any kind of treatment within 1 year of
diagnosis?
Type 4: Combination of various query components. Events in a query can be linked to each
other by various means, including temporal expressions, conjunctions, and disjunc-
tions. Complex combinations may render the feedback text ambiguous. For example:
Relevant subjects: patients with a clinical diagnosis of breast cancer and who had
nausea within 1 year of the chemotherapy
Treatment: patients who received [some surgical procedure] [at some point in time]
and chemotherapy but no radiotherapy within 1 year of the diagnosis
Tests:[ ]
Outcome: percentage of patients who were alive after 5 years of the diagnosis
Options:
 What percentage of patients diagnosed with breast cancer who underwent a
surgical procedure at any time, received chemotherapy within 1 year of the
diagnosis, had nausea within 1 year of the chemotherapy, and received no
radiotherapy within 1 year of the diagnosis, survived more than 5 years
after diagnosis?
 What percentage of patients diagnosed with breast cancer who underwent
a surgical procedure at any time, received chemotherapy at any time, had
nausea at any time after chemotherapy, and received no radiotherapy
within 1 year of the diagnosis, survived more than 5 years after diagnosis?
 What percentage of patients diagnosed with breast cancer who underwent
a surgical procedure at any time, received chemotherapy within 1 year of
the diagnosis, had nausea after chemotherapy but within 1 year of the
diagnosis, and received no radiotherapy within 1 year of the diagnosis,
survived more than 5 years after diagnosis?
Type 5: Complex queries, non-ambiguous components. We introduced this category in order
to test the readability of complex queries that do not necessarily contain ambiguous
components. Because most queries in the medical domain are likely to be very complex,
can the sheer number of query components render the query ambiguous to the users?
For example:
Relevant subjects: patients under the age of 50 at the time of diagnosis, with a
clinical diagnosis of breast cancer
Treatment: patients who received [some surgical procedure] [at some point in time]
and no chemotherapy within 1 year of the diagnosis
Tests: [ ]
Outcome: absolute number of patients
Options
 How many patients with breast cancer, under the age of 50, had a surgical
procedure at any time and did not have chemotherapy within 1 year of the
diagnosis?
127
Computational Linguistics Volume 33, Number 1
 How many patients with breast cancer, under the age of 50, had a surgical
procedure within one year of the diagnosis and did not have
chemotherapy within one year of the diagnosis?
 How many patients with breast cancer, below the age of 50, had a surgical
procedure within one year of the diagnosis and had chemotherapy after
one year of the diagnosis?
Type 6: Attachment/interpretation of outcome. The outcome section generally describes a
condition holding between a reference and a target set of patients. If the query contains
multiple features describing the patient set, it may be difficult to differentiate between
features that contribute to the reference set and features that contribute to the target set.
For example:
Relevant subjects: patients with a clinical diagnosis of breast cancer and who had
anaemia after chemotherapy
Treatment: patients who received chemotherapy
Tests:[ ]
Outcome: percentage of patients who were alive after 5 years from the diagnosis
Options:
 Of the patients diagnosed with breast cancer who developed anaemia after
chemotherapy, what percentage survived 5 years after diagnosis?
 Of the patients in the database, what percentage were diagnosed with
breast cancer, developed anaemia after chemotherapy, and survived
5 years after diagnosis?
 Of the patients diagnosed with breast cancer, what percentage developed
anaemia after chemotherapy and survived 5 years after diagnosis?
Results. If the presented feedback text is incomprehensible, the probability that subjects
will select the correct interpretation will be 0.33 (i.e., they will get the right answer only
a third of the time). Our results show that subjects? precision is 0.84; that is, on average,
they select the intended interpretation 84% of the time, rather than 33% as would be predicted if
their selections were random. Statistical analysis of these results, using a one-sample t-test,
shows this effect to be highly significant (mean = 0.8361, d = 0.5028, t = 16.76, p < .0001).
The breakdown by type of ambiguity is shown in Table 1.
Table 1
Interpretation of feedback texts.
Ambiguity Total correct Percent
Type 1 51 85
Type 2 54 90
Type 3 50 83
Type 4 48 80
Type 5 47 78
Type 6 51 85
128
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
6.3 Summarizing the Evaluation
The true test of any system comes with its use in situ by the users for which it was
designed. Normally, this would be preceded by a bank of formal empirical studies
under more controlled conditions. For a question-answering system like the one we are
addressing in this article (which is but a small part of a much larger system), a formal
controlled evaluation would ideally cover a large number of exemplars of each type of
query supported by the system, and a large number of subjects. Given our constraints
on the number of available subjects (and the concomitant effect this has on the possible
design of any experiments), the evaluation reported here is necessarily more limited in
scope. This is not an unusual situation in system development, where evaluation must
proceed by gradual refinement through the application of rigor, wherever possible,
but also applying along the way intuition, common sense, and past experience. The
evaluation we have presented here shows what can be done during the early phases
of the development of a large and complex system whose components are in different
stages of completion, and where access to representative users is limited.
Given these caveats, the picture that emerges from this study is nonetheless very
encouraging. Our results suggest that our target users (medical researchers) can quickly
learn to construct queries of the type and complexity that they have identified as
relevant. Specifically:
 They are able to use the Conceptual Authoring method successfully to
compose complex queries, with no prior exposure to the method and
with the benefit of only minimal training.
 They become quickly proficient with the system, achieving near perfect
performance by their fourth attempt at query composition.
The study also indicates that the feedback texts employed to construct queries of a
high degree of structural complexity are not difficult to understand. This is extremely
important, as it means that users can be confident that they are obtaining an answer
that pertains to the question that they think they are asking, as opposed to an answer to
some other similar question.
Additionally, in a separate, informal study, we have found suggestive evidence
that the Conceptual Authoring method of query composition may be much more
user-friendly than the traditional method of direct SQL editing, even for extremely
skilled SQL coders with a high level of familiarity with the database and the domain
(Hallett, Scott, and Power 2006). Our tests showed that (an albeit small sample of) such
experts, even in a situation that is heavily biased towards optimal performance of
SQL codes, found it much easier to compose queries with the Conceptual Authoring
interface than in SQL. Not only did it take them more than three times longer, on
average, to compose the query in SQL, but they were not able to produce the complete
SQL in that time.
7. Conclusion
Most question-answering systems make use of natural language understanding and
allow users to pose simple questions to textual repositories. We have presented
here a generic method for composing natural language questions within a question-
answering system that avoids the well-know pitfalls of natural language understanding
129
Computational Linguistics Volume 33, Number 1
while allowing users to pose complex questions to data repositories. The method,
Conceptual Authoring, involves no natural language interpretation?only generation?
and is particularly well-suited to query interfaces to closed-domain systems. We have
elucidated the method through its use in the CLEF query tool, which has been designed
to meet the requirements of a particular context: the querying of large repositories
of electronic health records by doctors and medical researchers. Similar requirements
almost certainly apply in other fields of expertise (e.g., engineering, genomics, law,
finance), as data are increasingly available in machine-usable electronic form; they can
be summarized as follows:
 Users: The query tool must be usable by the relevant domain
experts?doctors, lawyers, or whomever?with no training in database
query languages.
 Training: The users must be able to use the query tool after minimal
learning time (minutes rather than hours).
 Time: After training, users must be able to construct complex queries in
a time comparable to writing the query down on paper?that is, a few
minutes.
 Reliability: The query tool must be close to 100% reliable, in the sense that
any query correctly formed by the user will be correctly transcoded into
the database query language and therefore answered by the system.
 Transparency: Queries must be presented to users in a form that is clear
and unambiguous, so that they know exactly what question they have
asked.
Although not exactly a requirement, a practical consideration is that the queries should
be frequent and important enough to justify the effort needed to meet the very stringent
requirements on usability and transparency. There is no point investing in a natural
language interface like the CLEF query tool except in contexts where the query results
are highly valuable.
In our view, transparent communication with expert users depends first and
foremost on using a familiar medium?the medium the experts use in their
normal work, which in this case means natural language. However, as argued in
Section 2, traditional natural language interfaces to database systems cannot meet the
requirements on reliability and training, because reliable interpretation of an input
text can be achieved only if the text conforms strictly to a controlled language (which
our users would not have time to learn). We therefore proposed a modification of
the traditional approach, in which the semantic representation of the query is edited
directly, through an interactive feedback text generated by the system. Otherwise the
approaches are the same: once obtained, the semantic representation is transcoded to
the database query language and passed to the database management system; when
the answer is returned, it is organized to suit the purposes of users, and presented in a
familiar display such as a text or diagram.
Ultimately, the value of such a tool must be proved in everyday use, but our
evaluation study provides some evidence that our approach can meet the requirements.
First, our studies were performed with the relevant users, in this case medical experts.
The training required for reaching a reliable level of performance was a matter of
minutes?usually a single demonstration followed by a single trial. Thereafter, most
130
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
users could formulate fairly complex queries in a reasonably short time (3?4 minutes); in
contrast, we have found in informal tests that expert SQL coders take at least three times
as long, while often failing to achieve a complete query (Hallett, Scott, and Power 2006).
The reliability achieved over 60 queries was 100%, in the sense that all users managed to
formulate all their targets. Finally, a further experiment showed that the formulations
of the queries in the feedback texts were transparent, with accuracy rates of 80?90% on a
multiple-choice comprehension task with a random baseline of 33%.
These evaluation results offer strong support for Conceptual Authoring as an
approach to this class of problems: we know of no alternative approach that can
achieve similar success in meeting these requirements. However, there are several areas
where improvement should be possible. First, we need to find ways of facilitating
the development process: building and maintaining a system like the CLEF query tool
requires, at present, the hand-coding of linguistic and conceptual resources; as a step in
this direction, we have developed a method of automatically inferring relevant types
of queries for any database, and automatically constructing resources that inform a
Conceptual Authoring interface (Hallett 2006). Second, we cannot be sure yet that the
wording of feedback texts is optimal?perhaps with more research the comprehension
rates can be pushed higher. Finally, we have only begun to explore the possibilities for
improving the GUI for Conceptual Authoring.
References
Allen, James. 1984. Towards a general theory
of action and time. Artificial Intelligence,
23(2):123?154.
Androutsopoulos, I. 1992. Interfacing a
natural language front end to a relational
database. Masters thesis, Department of
Artificial Intelligence, University of
Edinburgh.
Androutsopoulos, I., G. Ritchie, and P.
Thanitsch. 1993. An efficient and portable
natural language query interface for
relational databases. In Proceedings of the
6th International Conference on Industrial
Engineering Applications of Artificial
Intelligence and Expert Systems Edinburgh,
pages 327?330, Edinburgh.
Androutsopoulos, I., G. D. Ritchie, and
P. Thanisch. 1995. Natural language
interfaces to databases?an introduction.
Natural Language Engineering, 2(1):29?81.
Bell, J. E. and L. A. Rowe. 1992. An
exploratory study of ad hoc query
languages to databases. In Proceedings of
the 8th International Conference on Data
Engineering, pages 606?613, Tempe, AZ.
Bouayad-Agha, Nadjet, Richard Power,
Donia Scott, and Anja Belz. 2002.
PILLS: Multilingual generation of
medical information documents with
overlapping content. In Proceedings
of the Third International Conference on
Language Resources and Evaluation,
pages 2111?2114, Las Palmas, Spain.
Capindale, R. A. and R. G. Crawford. 1990.
Using a natural language interface with
casual users. International Journal of
Man?Machine Studies, 32(3):341?361.
Catarci, T. and G. Santucci. 1995. Are visual
query languages easier to use than
traditional ones? An experimental proof.
In Proceedings of the International Conference
on Human-Computer Interaction (HCI95).
College of American Pathologists, 2004.
SNOMED Clinical Terms User Guide.
July 2004 release.
Deshpande, A., C. Brandt, and P. Nadkarni.
2001. Ad hoc query of patient data:
Meeting the needs of clinical studies.
Journal of the American Medical Informatics
Association, 9(4):369?382.
Deshpande, A., C. Brandt, and P. Nadkarni.
2003. Temporal query of attribute-value
patient data: Utilizing the constraints of
clinical studies. International Journal of
Medical Informatics, 70:59?77.
Ely, John W., Jerome A. Osheroff, Paul N.
Gorman, Mark H. Ebell, M. Lee Chambliss,
Eric A. Pifer, and P. Zoe Stavri. 2000. A
taxonomy of generic clinical questions:
Classification study. British Medical Journal,
321:429?432.
Estrella, Florida, Chiara del Frate, Tamas
Hauer, Richard McClatchey, Mohammed
Odeh, Dmitry Rogulin, Salvator Roberto
Amendolia, David Schottlander,
Tony Solomonides, and Ruth Warren.
2004. Resolving clinicians queries
131
Computational Linguistics Volume 33, Number 1
across a grids infrastructure. In
Proceedings of the 2nd International
HealthGRID Conference.
Evans, Roger, Paul Piwek, Lynne Cahill, and
Neil Tipper. In press. Natural language
processing in CLIME, a multilingual legal
advisory system. Natural Language
Engineering.
Frank, Annette, Hans-Ulrich Krieger, Feiyu
Xu, Hans Uszkoreit, Berthold Crysmann,
Brigitte Jorg, and Ulrich Schafer. 2005.
Querying structured knowledge sources.
In AAAI-05 Workshop on Question
Answering in Restricted Domains, pages
10?19, Pittsburgh, Pennsylvania.
Gorman, P. N. and M. Helfand. 1995.
Information seeking in primary care:
How physicians choose which clinical
questions to pursue and which to leave
unanswered. Medical Decision Making,
(15):113?119.
Hafner, Carole D. and Kurt Godden. 1985.
Portability of syntax and semantics in
datalog. ACM Transactions on Information
Systems, 3(2):141?164.
Hallett, Catalina. 2006. Generic querying of
relational databases using natural
language generation techniques. In
Proceedings of the 4th International Natural
Language Generation Conference (INLG?06),
pages 95?102, Sydney, Australia.
Hallett, Catalina, Donia Scott, and Richard
Power. 2006. Evaluation of the CLEF
query interface. Technical Report 2006/01,
Centre for Research in Computing, The
Open University.
Hendrix, Gary G., Earl D. Sacerdoti, Daniel
Sagalowicz, and Jonathan Slocum. 1978.
Developing a natural language interface to
complex data. ACM Transactions on
Database Systems, 3(2):105?147.
Hovy, E. H., U. Hermjakob, and
D. Ravichandran. 2002. A question/
answer typology with surface text
patterns. In Proceedings of the DARPA
Human Language Technology Conference,
pages 247?250, San Diego, CA.
Jerome, R. N., N. B. Giuse, K. W. Gish,
N. A. Sathe, and M. S. Dietrich. 2001.
Information needs of clinical teams:
Analysis of questions received by the
clinical informatics consult service.
Bulletin of the Medical Library Association,
89(2):177?184.
Kalra, Dipak, Anthony Austin, A. O?Connor,
D. Patterson, David Lloyd, and David
Ingram. 2001. Design and Implementation
of a Federated Health Record Server,
pages 1?13. Medical Records Institute
for the Centre for Advancement of
Electronic Records Ltd.
Kaplan, S. Jerrold. 1984. Designing a portable
natural language database query system.
ACM Transactions on Database Systems,
9(1):1?19.
Kate, R. J., Y. W. Wong, and R. J. Mooney.
2005. Learning to transform natural to
formal languages. In Proceedings of the
Twentieth National Conference on Artificial
Intelligence (AAAI-05), pages 1062?1068,
Pittsburgh, PA.
Kim, Y. 1990. Effects of conceptual data
modelling formalisms on user validation and
analyst modelling of information requirements.
Ph.D. thesis, University of Minnesota.
Koonce, Taneya Y., Nunzia Bettinsoli Giuse,
and Pauline Todd. 2004. Evidence-based
databases versus primary medical
literature: An in-house investigation on
their optimal use. Bulletin of the Medical
Library Association, 92(4):407?411.
Lowden, B. G. T., B. R. Walls, A. De Roeck,
C. J. Fox, and R. Turner. 1991. A formal
approach to translating English into SQL.
In Proceedings of the 9th British National
Conference on Databases, pages 110?127,
Wolverhampton, UK.
Mueckstein, Eva-Martin. 1985. Controlled
natural language interfaces (extended
abstract): The best of three worlds. In CSC
?85: Proceedings of the 1985 ACM thirteenth
annual conference on Computer Science,
pages 176?178, New York, NY.
Nadkarni, P. and C. Brandt. 1998. Data
extraction and ad hoc query of an
entity-attribute-value database. Journal
of the American Medical Informatics
Association, 5(6):511?527.
Petre, Marian. 1995. Why looking isn?t
always seeing: Readership skills and
graphical programming. Communications
of the ACM, 38(6):33?44.
Piwek, Paul. 2002. Requirements definition,
validation, verification and evaluation
of the clime interface and language
processing technology. Technical Report
ITRI-02-03, ITRI, University of Brighton.
Piwek, Paul, Roger Evans, Lynne Cahill,
and Neil Tipper. 2000. Natural language
generation in the MILE system. In
Proceedings of the IMPACTS in NLG
Workshop, pages 33?42, Schloss Dagstuhl,
Germany.
Popescu, Ana-Maria, Oren Etzioni, and
Henry Kautz. 2003. Towards a theory
of natural language interfaces to
databases. In IUI ?03: Proceedings of
the 8th international conference on
132
Hallett, Scott, and Power Composing Questions through Conceptual Authoring
Intelligent user interfaces, pages 149?157,
New York, NY.
Power, Richard and Donia Scott. 1998.
Multilingual authoring using feedback
texts. In Proceedings of 17th International
Conference on Computational Linguistics
and 36th Annual Meeting of the
Association for Computational Linguistics
(COLING-ACL 98), pages 1053?1059,
Montreal, Canada.
Power, Richard, Donia Scott, and Roger
Evans. 1998. What you see is what you
meant: direct knowledge editing with
natural language feedback. In Proceedings
of the 13th Biennial European Conference on
Artificial Intelligence, pages 675?681,
Brighton, UK.
Rector, Alan, Jeremy Rogers, Adel Taweel,
David Ingram, Dipak Kalra, Jo Milan,
Robert Gaizauskas, Mark Hepple,
Donia Scott, and Richard Power. 2003.
CLEF?joining up healthcare with
clinical and post-genomic research. In
Second UK E-Science ?All Hands Meeting?,
Nottingham, UK.
Rivera, Carlos and Nick Cercone. 1998.
Hermes: Natural language access to a
medical database. Technical report
CS-98-03, University of Regina, Canada.
Scott, Donia, Richard Power, and Roger
Evans. 1998. Generation as a solution
to its own problem. In Proceedings
of the 9th International Workshop on
Natural Language Generation,
pages 256?265, Niagara-on-the-Lake,
Canada.
Shahar, Yuval and Cleve Cheng. 1999.
Intelligent visualization and exploration of
time-oriented clinical data. In Proceedings
of HICSS, pages 4019?4030, Maui, HI.
Soricut, R. and E. Brill. 2004. Automatic
question answering: Beyond the factoid.
In Proceedings of the HLT/NAACL 2004,
pages 57?64, Boston, MA.
Tang, Lappoon R. and Raymond J. Mooney.
2001. Using multiple clause constructors
in inductive logic programming
for semantic parsing. In EMCL ?01:
Proceedings of the 12th European Conference
on Machine Learning, pages 466?477,
London, UK.
Templeton, Marjorie and John Burger. 1983.
Problems in natural-language interface to
DBMs with examples from EUFID. In
Proceedings of the First Conference on Applied
Natural Language Processing, pages 3?16,
Morristown, NJ.
Tennant, Harry R., Kenneth M. Ross, and
Craig W. Thompson. 1983. Usable natural
language interfaces through menu-based
natural language understanding. In CHI
?83: Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems,
pages 154?160, New York, NY.
TREC. 2005. Question answering data.
http://trec.nist.gov/data/qa/t2005
qadata.html.
Wilcox, Adam, George Hripcsak, and
Cynthia Chen. 1997. Creating an
environment for linking knowledge-based
systems to a clinical database: A suite of
tools. In Proceedings of AMIA Annual Fall
Symposium, pages 303?307, Nashville, TN.
Zhang, Guogen, Wesley W. Chu, Frank
Meng, and Gladys Kong. 1999. Query
formulation from high-level concepts
for relational databases. In UIDIS ?99:
Proceedings of the 1999 User Interfaces
to Data Intensive Systems, page 64,
Washington, DC.
133

