Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 423?431,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Who, What, When, Where, Why?  
Comparing Multiple Approaches to the Cross-Lingual 5W Task 
Kristen Parton*, Kathleen R. McKeown*, Bob Coyne*, Mona T. Diab*,  
Ralph Grishman?, Dilek Hakkani-T?r?, Mary Harper?, Heng Ji?, Wei Yun Ma*,  
Adam Meyers?, Sara Stolbach*, Ang Sun?, Gokhan Tur?, Wei Xu? and Sibel Yaman? 
 
*Columbia University 
New York, NY, USA 
{kristen, kathy, 
coyne, mdiab, ma, 
sara}@cs.columbia.edu 
 
?New York University 
New York, NY, USA 
{grishman, meyers, 
asun, xuwei} 
@cs.nyu.edu 
?International Computer 
Science Institute 
Berkeley, CA, USA 
{dilek, sibel} 
@icsi.berkeley.edu 
 
?Human Lang. Tech. Ctr. of 
Excellence, Johns Hopkins 
and U. of Maryland, 
College Park  
mharper@umd.edu 
?City University of  
New York 
New York, NY, USA 
hengji@cs.qc.cuny.edu 
 
 
?SRI International 
Palo Alto, CA, USA 
gokhan@speech.sri.com 
 
 
  
Abstract 
Cross-lingual tasks are especially difficult 
due to the compounding effect of errors in 
language processing and errors in machine 
translation (MT). In this paper, we present an 
error analysis of a new cross-lingual task: the 
5W task, a sentence-level understanding task 
which seeks to return the English 5W's (Who, 
What, When, Where and Why) corresponding 
to a Chinese sentence. We analyze systems 
that we developed, identifying specific prob-
lems in language processing and MT that 
cause errors. The best cross-lingual 5W sys-
tem was still 19% worse than the best mono-
lingual 5W system, which shows that MT 
significantly degrades sentence-level under-
standing. Neither source-language nor target-
language analysis was able to circumvent 
problems in MT, although each approach had 
advantages relative to the other. A detailed 
error analysis across multiple systems sug-
gests directions for future research on the 
problem. 
1 Introduction 
In our increasingly global world, it is ever more 
likely for a mono-lingual speaker to require in-
formation that is only available in a foreign lan-
guage document. Cross-lingual applications ad-
dress this need by presenting information in the 
speaker?s language even when it originally ap-
peared in some other language, using machine 
translation (MT) in the process. In this paper, we 
present an evaluation and error analysis of a 
cross-lingual application that we developed for a 
government-sponsored evaluation, the 5W task. 
The 5W task seeks to summarize the informa-
tion in a natural language sentence by distilling it 
into the answers to the 5W questions: Who, 
What, When, Where and Why. To solve this 
problem, a number of different problems in NLP 
must be addressed: predicate identification, ar-
gument extraction, attachment disambiguation, 
location and time expression recognition, and 
(partial) semantic role labeling. In this paper, we 
address the cross-lingual 5W task: given a 
source-language sentence, return the 5W?s trans-
lated (comprehensibly) into the target language. 
Success in this task requires a synergy of suc-
cessful MT and answer selection.  
The questions we address in this paper are: 
? How much does machine translation (MT) 
degrade the performance of cross-lingual 
5W systems, as compared to monolingual 
performance? 
? Is it better to do source-language analysis 
and then translate, or do target-language 
analysis on MT? 
? Which specific problems in language 
processing and/or MT cause errors in 5W 
answers?  
In this evaluation, we compare several differ-
ent approaches to the cross-lingual 5W task, two 
that work on the target language (English) and 
one that works in the source language (Chinese). 
423
A central question for many cross-lingual appli-
cations is whether to process in the source lan-
guage and then translate the result, or translate 
documents first and then process the translation. 
Depending on how errorful the translation is, 
results may be more accurate if models are de-
veloped for the source language. However, if 
there are more resources in the target language, 
then the translate-then-process approach may be 
more appropriate. We present a detailed analysis, 
both quantitative and qualitative, of how the ap-
proaches differ in performance.  
We also compare system performance on hu-
man translation (which we term reference trans-
lations) and MT of the same data in order to de-
termine how much MT degrades system per-
formance. Finally, we do an in-depth analysis of 
the errors in our 5W approaches, both on the 
NLP side and the MT side. Our results provide 
explanations for why different approaches suc-
ceed, along with indications of where future ef-
fort should be spent. 
2 Prior Work 
The cross-lingual 5W task is closely related to 
cross-lingual information retrieval and cross-
lingual question answering (Wang and Oard 
2006; Mitamura et al 2008). In these tasks, a 
system is presented a query or question in the 
target language and asked to return documents or 
answers from a corpus in the source language. 
Although MT may be used in solving this task, it 
is only used by the algorithms ? the final evalua-
tion is done in the source language. However, in 
many real-life situations, such as global business, 
international tourism, or intelligence work, users 
may not be able to read the source language. In 
these cases, users must rely on MT to understand 
the system response. (Parton et al 2008) exam-
ine the case of ?translingual? information re-
trieval, where evaluation is done on translated 
results in the target language. In cross-lingual 
information extraction (Sudo et al 2004) the 
evaluation is also done on MT, but the goal is to 
learn knowledge from a large corpus, rather than 
analyzing individual sentences.  
The 5W task is also closely related to Seman-
tic Role Labeling (SRL), which aims to effi-
ciently and effectively derive semantic informa-
tion from text. SRL identifies predicates and 
their arguments in a sentence, and assigns roles 
to each argument. For example, in the sentence 
?I baked a cake yesterday.?, the predicate 
?baked? has three arguments. ?I? is the subject of 
the predicate, ?a cake? is the object and ?yester-
day? is a temporal argument.  
Since the release of large data resources anno-
tated with relevant levels of semantic informa-
tion, such as the FrameNet (Baker et al, 1998) 
and PropBank corpora (Kingsbury and Palmer, 
2003), efficient approaches to SRL have been 
developed (Carreras and Marquez, 2005). Most 
approaches to the problem of SRL follow the 
Gildea and Jurafsky (2002) model. First, for a 
given predicate, the SRL system identifies its 
arguments' boundaries. Second, the Argument 
types are classified depending on an adopted 
lexical resource such as PropBank or FrameNet. 
Both steps are based on supervised learning over 
labeled gold standard data. A final step uses heu-
ristics to resolve inconsistencies when applying 
both steps simultaneously to the test data.  
Since many of the SRL resources are English, 
most of the SRL systems to date have been for 
English. There has been work in other languages 
such as German and Chinese (Erk 2006; Sun 
2004; Xue and Palmer 2005). The systems for 
the other languages follow the successful models 
devised for English, e.g. (Gildea and Palmer, 
2002; Chen and Rambow, 2003; Moschitti, 2004; 
Xue and Palmer, 2004; Haghighi et al, 2005). 
3 The Chinese-English 5W Task 
3.1 5W Task Description 
We participated in the 5W task as part of the 
DARPA GALE (Global Autonomous Language 
Exploitation) project. The goal is to identify the 
5W?s (Who, What, When, Where and Why) for a 
complete sentence. The motivation for the 5W 
task is that, as their origin in journalism suggests, 
the 5W?s cover the key information nuggets in a 
sentence. If a system can isolate these pieces of 
information successfully, then it can produce a 
pr?cis of the basic meaning of the sentence. Note 
that this task differs from QA tasks, where 
?Who? and ?What? usually refer to definition 
type questions. In this task, the 5W?s refer to se-
mantic roles within a sentence, as defined in Ta-
ble 1.  
In order to get al 5W?s for a sentence correct, 
a system must identify a top-level predicate, ex-
tract the correct arguments, and resolve attach-
ment ambiguity. In the case of multiple top-level 
predicates, any of the top-level predicates may be 
chosen. In the case of passive verbs, the Who is 
the agent (often expressed as a ?by clause?, or 
not stated), and the What should include the syn-
tactic subject.  
424
Answers are judged Correct1 if they identify a 
correct null argument or correctly extract an ar-
gument that is present in the sentence. Answers 
are not penalized for including extra text, such as 
prepositional phrases or subordinate clauses, 
unless the extra text includes text from another 
answer or text from another top-level predicate. 
In sentence 2a in Table 2, returning ?bought and 
cooked? for the What would be Incorrect. Simi-
larly, returning ?bought the fish at the market? 
for the What would also be Incorrect, since it 
contains the Where. Answers may also be judged 
Partial, meaning that only part of the answer was 
returned. For example, if the What contains the 
predicate but not the logical object, it is Partial.  
Since each sentence may have multiple correct 
sets of 5W?s, it is not straightforward to produce 
a gold-standard corpus for automatic evaluation. 
One would have to specify answers for each pos-
sible top-level predicate, as well as which parts 
of the sentence are optional and which are not 
allowed. This also makes creating training data 
for system development problematic. For exam-
ple, in Table 2, the sentence in 2a and 2b is the 
same, but there are two possible sets of correct 
answers. Since we could not rely on a gold-
standard corpus, we used manual annotation to 
judge our 5W system, described in section 5. 
3.2 The Cross-Lingual 5W Task 
In the cross-lingual 5W task, a system is given a 
sentence in the source language and asked to 
produce the 5W?s in the target language. In this 
task, both machine translation (MT) and 5W ex-
traction must succeed in order to produce correct 
answers. One motivation behind the cross-lingual 
5W task is MT evaluation. Unlike word- or 
phrase-overlap measures such as BLEU, the 5W 
evaluation takes into account ?concept? or ?nug-
get? translation. Of course, only the top-level 
predicate and arguments are evaluated, so it is 
not a complete evaluation. But it seeks to get at 
the understandability of the MT output, rather 
than just n-gram overlap. 
Translation exacerbates the problem of auto-
matically evaluating 5W systems. Since transla-
tion introduces paraphrase, rewording and sen-
tence restructuring, the 5W?s may change from 
one translation of a sentence to another transla-
tion of the same sentence. In some cases, roles 
may swap. For example, in Table 2, sentences 1a 
and 1b could be valid translations of the same 
                                                 
1
 The specific guidelines for determining correctness 
were formulated by BAE.  
Chinese sentence. They contain the same infor-
mation, but the 5W answers are different. Also, 
translations may produce answers that are textu-
ally similar to correct answers, but actually differ 
in meaning. These differences complicate proc-
essing in the source followed by translation. 
 
Example: On Tuesday, President Obama met with 
French President Sarkozy in Paris to discuss the 
economic crisis. 
W Definition Example  
answer 
WHO Logical subject of the 
top-level predicate in 
WHAT, or null. 
President 
Obama 
WHAT One of the top-level 
predicates in the sen-
tence, and the predi-
cate?s logical object. 
met with 
French Presi-
dent Sarkozy 
WHEN ARGM-TMP of the 
top-level predicate in 
WHAT, or null. 
On Tuesday 
WHERE ARGM-LOC of the 
top-level predicate in 
WHAT, or null. 
in Paris 
WHY ARGM-CAU of the 
top-level predicate in 
WHAT, or null. 
to discuss the 
economic crisis 
Table 1. Definition of the 5W task, and 5W answers 
from the example sentence above. 
4 5W System 
We developed a 5W combination system that 
was based on five other 5W systems. We se-
lected four of these different systems for evalua-
tion: the final combined system (which was our 
submission for the official evaluation), two sys-
tems that did analysis in the target-language 
(English), and one system that did analysis in the 
source language (Chinese). In this section, we 
describe the individual systems that we evalu-
ated, the combination strategy, the parsers that 
we tuned for the task, and the MT systems.  
 Sentence WHO WHAT 
1a Mary bought a cake 
from Peter. 
Mary bought a 
cake 
1b Peter sold Mary a 
cake. 
Peter sold Mary 
2a I bought the fish at 
the market yesterday 
and cooked it today. 
I bought the 
fish 
[WHEN: 
yesterday] 
2b I bought the fish at 
the market yesterday 
and cooked it today. 
I cooked it 
[WHEN: 
today] 
Table 2. Example 5W answers. 
425
4.1 Latent Annotation Parser 
For this work, we have re-implemented and en-
hanced the Berkeley parser (Petrov and Klein 
2007) in several ways: (1) developed a new 
method to handle rare words in English and Chi-
nese; (2) developed a new model of unknown 
Chinese words based on characters in the word; 
(3) increased robustness by adding adaptive 
modification of pruning thresholds and smooth-
ing of word emission probabilities. While the 
enhancements to the parser are important for ro-
bustness and accuracy, it is even more important 
to train grammars matched to the conditions of 
use. For example, parsing a Chinese sentence 
containing full-width punctuation with a parser 
trained on half-width punctuation reduces accu-
racy by over 9% absolute F. In English, parsing 
accuracy is seriously compromised by training a 
grammar with punctuation and case to process 
sentences without them.  
We developed grammars for English and Chi-
nese trained specifically for each genre by sub-
sampling from available treebanks (for English, 
WSJ, BN, Brown, Fisher, and Switchboard; for 
Chinese, CTB5) and transforming them for a 
particular genre (e.g., for informal speech, we 
replaced symbolic expressions with verbal forms 
and remove punctuation and case) and by utiliz-
ing a large amount of genre-matched self-labeled 
training parses. Given these genre-specific 
parses, we extracted chunks and POS tags by 
script. We also trained grammars with a subset of 
function tags annotated in the treebank that indi-
cate case role information (e.g., SBJ, OBJ, LOC, 
MNR) in order to produce function tags.   
4.2 Individual 5W Systems 
The English systems were developed for the 
monolingual 5W task and not modified to handle 
MT. They used hand-crafted rules on the output 
of the latent annotation parser to extract the 5Ws.  
English-function used the function tags from 
the parser to map parser constituents to the 5Ws. 
First the Who, When, Where and Why were ex-
tracted, and then the remaining pieces of the sen-
tence were returned as the What. The goal was to 
make sure to return a complete What answer and 
avoid missing the object. 
English-LF, on the other hand, used a system 
developed over a period of eight years (Meyers 
et al 2001) to map from the parser?s syntactic 
constituents into logical grammatical relations 
(GLARF), and then extracted the 5Ws from the 
logical form. As a back-up, it also extracted 
GLARF relations from another English-treebank 
trained parser, the Charniak parser (Charniak 
2001). After the parses were both converted to 
the 5Ws, they were then merged, favoring the 
system that: recognized the passive, filled more 
5W slots or produced shorter 5W slots (provid-
ing that the WHAT slot consisted of more than 
just the verb). A third back-up method extracted 
5Ws from part-of-speech tag patterns. Unlike 
English-function, English-LF explicitly tried to 
extract the shortest What possible, provided there 
was a verb and a possible object, in order to 
avoid multiple predicates or other 5W answers.  
Chinese-align uses the latent annotation 
parser (trained for Chinese) to parse the Chinese 
sentences. A dependency tree converter (Johans-
son and Nuges 2007) was applied to the constitu-
ent-based parse trees to obtain the dependency 
relations and determine top-level predicates. A 
set of hand-crafted dependency rules based on 
observation of Chinese OntoNotes were used to 
map from the Chinese function tags into Chinese 
5Ws.  Finally, Chinese-align used the alignments 
of three separate MT systems to translate the 
5Ws: a phrase-based system, a hierarchical 
phrase-based system, and a syntax augmented 
hierarchical phrase-based system. Chinese-align 
faced a number of problems in using the align-
ments, including the fact that the best MT did not 
always have the best alignment. Since the predi-
cate is essential, it tried to detect when verbs 
were deleted in MT, and back-off to a different 
MT system. It also used strategies for finding 
and correcting noisy alignments, and for filtering 
When/Where answers from Who and What.  
4.3 Hybrid System 
A merging algorithm was learned based on a de-
velopment test set. The algorithm selected all 
5W?s from a single system, rather than trying to 
merge W?s from different systems, since the 
predicates may vary across systems. For each 
document genre (described in section 5.4), we 
ranked the systems by performance on the devel-
opment data. We also experimented with a vari-
ety of features (for instance, does ?What? include 
a verb). The best-performing features were used 
in combination with the ranked list of priority 
systems to create a rule-based merger. 
4.4 MT Systems 
The MT Combination system used by both of the 
English 5W systems combined up to nine sepa-
rate MT systems. System weights for combina-
tion were optimized together with the language 
426
model score and word penalty for a combination 
of BLEU and TER (2*(1-BLEU) + TER). Res-
coring was applied after system combination us-
ing large language models and lexical trigger 
models. Of the nine systems, six were phrased-
based systems (one of these used chunk-level 
reordering of the Chinese, one used word sense 
disambiguation, and one used unsupervised Chi-
nese word segmentation), two were hierarchical 
phrase-based systems, one was a string-to-
dependency system, one was syntax-augmented, 
and one was a combination of two other systems. 
Bleu scores on the government supplied test set 
in December 2008 were 35.2 for formal text, 
29.2 for informal text, 33.2 for formal speech, 
and 27.6 for informal speech. More details may 
be found in (Matusov et al 2009). 
5 Methods 
5.1 5W Systems 
For the purposes of this evaluation2, we com-
pared the output of 4 systems: English-Function, 
English-LF, Chinese-align, and the combined 
system. Each English system was also run on 
reference translations of the Chinese sentence. 
So for each sentence in the evaluation corpus, 
there were 6 systems that each provided 5Ws. 
5.2 5W Answer Annotation 
For each 5W output, annotators were presented 
with the reference translation, the MT version, 
and the 5W answers. The 5W system names 
were hidden from the annotators. Annotators had 
to select ?Correct?, ?Partial? or ?Incorrect? for 
each W. For answers that were Partial or Incor-
rect, annotators had to further specify the source 
of the error based on several categories (de-
scribed in section 6). All three annotators were 
native English speakers who were not system 
developers for any of the 5W systems that were 
being evaluated (to avoid biased grading, or as-
signing more blame to the MT system). None of 
the annotators knew Chinese, so all of the judg-
ments were based on the reference translations. 
After one round of annotation, we measured 
inter-annotator agreement on the Correct, Partial, 
or Incorrect judgment only. The kappa value was 
0.42, which was lower than we expected. An-
other surprise was that the agreement was lower 
                                                 
2
 Note that an official evaluation was also performed by 
DARPA and BAE. This evaluation provides more fine-
grained detail on error types and gives results for the differ-
ent approaches. 
for When, Where and Why (?=0.31) than for 
Who or What (?=0.48). We found that, in cases 
where a system would get both Who and What 
wrong, it was often ambiguous how the remain-
ing W?s should be graded. Consider the sentence: 
?He went to the store yesterday and cooked lasa-
gna today.? A system might return erroneous 
Who and What answers, and return Where as ?to 
the store? and When as ?today.? Since Where 
and When apply to different predicates, they 
cannot both be correct. In order to be consistent, 
if a system returned erroneous Who and What 
answers, we decided to mark the When, Where 
and Why answers Incorrect by default. We added 
clarifications to the guidelines and discussed ar-
eas of confusion, and then the annotators re-
viewed and updated their judgments.  
After this round of annotating, ?=0.83 on the 
Correct, Partial, Incorrect judgments. The re-
maining disagreements were genuinely ambigu-
ous cases, where a sentence could be interpreted 
multiple ways, or the MT could be understood in 
various ways. There was higher agreement on 
5W?s answers from the reference text compared 
to MT text, since MT is inherently harder to 
judge and some annotators were more flexible 
than others in grading garbled MT. 
5.3 5W Error Annotation 
In addition to judging the system answers by the 
task guidelines, annotators were asked to provide 
reason(s) an answer was wrong by selecting from 
a list of predefined errors. Annotators were asked 
to use their best judgment to ?assign blame? to 
the 5W system, the MT, or both. There were six 
types of system errors and four types of MT er-
rors, and the annotator could select any number 
of errors. (Errors are described further in section 
6.) For instance, if the translation was correct, 
but the 5W system still failed, the blame would 
be assigned to the system. If the 5W system 
picked an incorrectly translated argument (e.g., 
?baked a moon? instead of ?baked a cake?), then 
the error would be assigned to the MT system. 
Annotators could also assign blame to both sys-
tems, to indicate that they both made mistakes.  
Since this annotation task was a 10-way selec-
tion, with multiple selections possible, there were 
some disagreements. However, if categorized 
broadly into 5W System errors only, MT errors 
only, and both 5W System and MT errors, then 
the annotators had a substantial level of agree-
ment (?=0.75 for error type, on sentences where 
both annotators indicated an error).  
427
5.4 5 W Corpus 
The full evaluation corpus is 350 documents, 
roughly evenly divided between four genres: 
formal text (newswire), informal text (blogs and 
newsgroups), formal speech (broadcast news) 
and informal speech (broadcast conversation). 
For this analysis, we randomly sampled docu-
ments to judge from each of the genres. There 
were 50 documents (249 sentences) that were 
judged by a single annotator. A subset of that set, 
with 22 documents and 103 sentences, was 
judged by two annotators. In comparing the re-
sults from one annotator to the results from both 
annotators, we found substantial agreement. 
Therefore, we present results from the single an-
notator so we can do a more in-depth analysis. 
Since each sentence had 5W?s, and there were 6 
systems that were compared, there were 7,500 
single-annotator judgments over 249 sentences. 
6 Results 
Figure 1 shows the cross-lingual performance 
(on MT) of all the systems for each 5W. The best 
monolingual performance (on human transla-
tions) is shown as a dashed line (% Correct 
only). If a system returned Incorrect answers for 
Who and What, then the other answers were 
marked Incorrect (as explained in section 5.2). 
For the last 3W?s, the majority of errors were due 
to this (details in Figure 1), so our error analysis 
focuses on the Who and What questions. 
6.1 Monolingual 5W Performance 
To establish a monolingual baseline, the Eng-
lish 5W system was run on reference (human) 
translations of the Chinese text. For each partial 
or incorrect answer, annotators could select one 
or more of these reasons: 
? Wrong predicate or multiple predicates. 
? Answer contained another 5W answer. 
? Passive handled wrong (WHO/WHAT). 
? Answer missed. 
? Argument attached to wrong predicate. 
Figure 1 shows the performance of the best 
monolingual system for each 5W as a dashed 
line. The What question was the hardest, since it 
requires two pieces of information (the predicate 
and object). The When, Where and Why ques-
tions were easier, since they were null most of 
the time. (In English OntoNotes 2.0, 38% of sen-
tences have a When, 15% of sentences have a 
Where, and only 2.6% of sentences have a Why.) 
The most common monolingual system error on 
these three questions was a missed answer, ac-
counting for all of the Where errors, all but one 
Why error and 71% of the When errors. The re-
maining When errors usually occurred when the 
system assumed the wrong sense for adverbs 
(such as ?then? or ?just?). 
 Missing Other 
5W 
Wrong/Multiple 
Predicates 
Wrong 
REF-func 37 29 22 7 
REF-LF 54 20 17 13 
MT-func 18 18 18 8 
MT-LF 26 19 10 11 
Chinese 23 17 14 8 
Hybrid 13 17 15 12 
Table 3. Percentages of Who/What errors attributed to 
each system error type. 
The top half of Table 3 shows the reasons at-
tributed to the Who/What errors for the reference 
corpus. Since English-LF preferred shorter an-
swers, it frequently missed answers or parts of 
Figure 1. System performance on each 5W. ?Partial? indicates that part of the answer was missing. Dashed lines 
show the performance of the best monolingual system (% Correct on human translations). For the last 3W?s, the 
percent of answers that were Incorrect ?by default? were: 30%, 24%, 27% and 22%, respectively, and 8% for the 
best monolingual system 
60 60 56 66
36 40 38 42
56 59 59 64 63
70 66 73 68 75 71 78
19201914
0
10
20
30
40
50
60
70
80
90
100
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
WHO WHAT WHEN WHERE WHY
Partia l
Correct
90
75 81
83 90
Best 
mono-
lingual
428
answers. English-LF also had more Partial an-
swers on the What question: 66% Correct and 
12% Partial, versus 75% Correct and 1% Partial 
for English-function. On the other hand, English-
function was more likely to return answers that 
contained incorrect extra information, such as 
another 5W or a second predicate. 
6.2 Effect of MT on 5W Performance 
The cross-lingual 5W task requires that systems 
return intelligible responses that are semantically 
equivalent to the source sentence (or, in the case 
of this evaluation, equivalent to the reference).  
As can be seen in Figure 1, MT degrades the 
performance of the 5W systems significantly, for 
all question types, and for all systems. Averaged 
over all questions, the best monolingual system 
does 19% better than the best cross-lingual sys-
tem. Surprisingly, even though English-function 
outperformed English-LF on the reference data, 
English-LF does consistently better on MT. This 
is likely due to its use of multiple back-off meth-
ods when the parser failed.  
6.3 Source-Language vs. Target-Language 
The Chinese system did slightly worse than ei-
ther English system overall, but in the formal 
text genre, it outperformed both English systems.  
Although the accuracies for the Chinese and 
English systems are similar, the answers vary a 
lot. Nearly half (48%) of the answers can be an-
swered correctly by both the English system and 
the Chinese system. But 22% of the time, the 
English system returned the correct answer when 
the Chinese system did not. Conversely, 10% of 
the answers were returned correctly by the Chi-
nese system and not the English systems. The 
hybrid system described in section 4.2 attempts 
to exploit these complementary advantages. 
After running the hybrid system, 61% of the 
answers were from English-LF, 25% from Eng-
lish-function, 7% from Chinese-align, and the 
remaining 7% were from the other Chinese 
methods (not evaluated here). The hybrid did 
better than its parent systems on all 5Ws, and the 
numbers above indicate that further improvement 
is possible with a better combination strategy.  
6.4 Cross-Lingual 5W Error Analysis 
For each Partial or Incorrect answer, annotators 
were asked to select system errors, translation 
errors, or both. (Further analysis is necessary to 
distinguish between ASR errors and MT errors.) 
The translation errors considered were: 
? Word/phrase deleted. 
? Word/phrase mistranslated. 
? Word order mixed up. 
? MT unreadable. 
Table 4 shows the translation reasons attrib-
uted to the Who/What errors. For all systems, the 
errors were almost evenly divided between sys-
tem-only, MT-only and both, although the Chi-
nese system had a higher percentage of system-
only errors. The hybrid system was able to over-
come many system errors (for example, in Table 
2, only 13% of the errors are due to missing an-
swers), but still suffered from MT errors. 
Table 4. Percentages of Who/What errors by each 
system attributed to each translation error type. 
Mistranslation was the biggest translation 
problem for all the systems. Consider the first 
example in Figure 3. Both English systems cor-
rectly extracted the Who and the When, but for 
Mistrans-
lation 
Deletion Word 
Order 
Unreadable 
MT-func 34 18 24 18 
MT-LF 29 22 21 14 
Chinese 32 17 9 13 
Hybrid 35 19 27 18 
MT: After several rounds of reminded, I was a little bit 
Ref: After several hints, it began to come back to me. 
 ??????,?????????? 
MT: The Guizhou province, within a certain bank robber, under the watchful eyes of a weak woman, and, with a 
knife stabbed the woman. 
Ref: I saw that in a bank in Guizhou Province, robbers seized a vulnerable young woman in front of a group of 
onlookers and stabbed the woman with a knife. 
 ?????????,?????????,???????,??,???????? 
MT: Woke up after it was discovered that the property is not more than eleven people do not even said that the 
memory of the receipt of the country into the country. 
Ref: Well, after waking up, he found everything was completely changed. Apart from having additional eleven 
grandchildren, even the motherland as he recalled has changed from a socialist country to a capitalist country. 
 ?????????????,?????????,????????????????????????? 
Figure 3 Example sentences that presented problems for the 5W systems. 
 
429
What they returned ?was a little bit.? This is the 
correct predicate for the sentence, but it does not 
match the meaning of the reference. The Chinese 
5W system was able to select a better translation, 
and instead returned ?remember a little bit.? 
Garbled word order was chosen for 21-24% of 
the target-language system Who/What errors, but 
only 9% of the source-language system 
Who/What errors. The source-language word 
order problems tended to be local, within-phrase 
errors (e.g., ?the dispute over frozen funds? was 
translated as ?the freezing of disputes?). The tar-
get-language system word order problems were 
often long-distance problems. For example, the 
second sentence in Figure 3 has many phrases in 
common with the reference translation, but the 
overall sentence makes no sense. The watchful 
eyes actually belong to a ?group of onlookers? 
(deleted). Ideally, the robber would have 
?stabbed the woman? ?with a knife,? rather than 
vice versa. Long-distance phrase movement is a 
common problem in Chinese-English MT, and 
many MT systems try to handle it (e.g., Wang et 
al. 2007). By doing analysis in the source lan-
guage, the Chinese 5W system is often able to 
avoid this problem ? for example, it successfully 
returned ?robbers? ?grabbed a weak woman? for 
the Who/What of this sentence. 
Although we expected that the Chinese system 
would have fewer problems with MT deletion, 
since it could choose from three different MT 
versions, MT deletion was a problem for all sys-
tems. In looking more closely at the deletions, 
we noticed that over half of deletions were verbs 
that were completely missing from the translated 
sentence. Since MT systems are tuned for word-
based overlap measures (such as BLEU), verb 
deletion is penalized equally as, for example, 
determiner deletion. Intuitively, a verb deletion 
destroys the central meaning of a sentence, while 
a determiner is rarely necessary for comprehen-
sion. Other kinds of deletions included noun 
phrases, pronouns, named entities, negations and 
longer connecting phrases.  
Deletion also affected When and Where. De-
leting particles such as ?in? and ?when? that in-
dicate a location or temporal argument caused 
the English systems to miss the argument. Word 
order problems in MT also caused attachment 
ambiguity in When and Where. 
The ?unreadable? category was an option of 
last resort for very difficult MT sentences. The 
third sentence in Figure 3 is an example where 
ASR and MT errors compounded to create an 
unparseable sentence.  
7 Conclusions 
In our evaluation of various 5W systems, we dis-
covered several characteristics of the task. The 
What answer was the hardest for all systems, 
since it is difficult to include enough information 
to cover the top-level predicate and object, with-
out getting penalized for including too much. 
The challenge in the When, Where and Why 
questions is due to sparsity ? these responses 
occur in much fewer sentences than Who and 
What, so systems most often missed these an-
swers. Since this was a new task, this first 
evaluation showed clear issues on the language 
analysis side that can be improved in the future. 
The best cross-lingual 5W system was still 
19% worse than the best monolingual 5W sys-
tem, which shows that MT significantly degrades 
sentence-level understanding. A serious problem 
in MT for systems was deletion. Chinese con-
stituents that were never translated caused seri-
ous problems, even when individual systems had 
strategies to recover. When the verb was deleted, 
no top level predicate could be found and then all 
5Ws were wrong.  
One of our main research questions was 
whether to extract or translate first. We hypothe-
sized that doing source-language analysis would 
be more accurate, given the noise in Chinese 
MT, but the systems performed about the same. 
This is probably because the English tools (logi-
cal form extraction and parser) were more ma-
ture and accurate than the Chinese tools.  
Although neither source-language nor target-
language analysis was able to circumvent prob-
lems in MT, each approach had advantages rela-
tive to the other, since they did well on different 
sets of sentences. For example, Chinese-align 
had fewer problems with word order, and most 
of those were due to local word-order problems.  
Since the source-language and target-language 
systems made different kinds of mistakes, we 
were able to build a hybrid system that used the 
relative advantages of each system to outperform 
all systems. The different types of mistakes made 
by each system suggest features that can be used 
to improve the combination system in the future. 
Acknowledgments 
This work was supported in part by the Defense 
Advanced Research Projects Agency (DARPA) 
under contract number HR0011-06-C-0023. Any 
opinions, findings and conclusions or recom-
mendations expressed in this material are the 
authors' and do not necessarily reflect those of 
the sponsors. 
430
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet project. In 
COLING-ACL '98: Proceedings of the Conference, 
held at the University of Montr?al, pages 86?90. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role 
labeling. In Proceedings of the Ninth Conference 
on Computational Natural Language Learning 
(CoNLL-2005), pages 152?164. 
Eugene Charniak. 2001. Immediate-head parsing for 
language models. In Proceedings of the 39th An-
nual Meeting on Association For Computational 
Linguistics (Toulouse, France, July 06 - 11, 2001).   
John Chen and Owen Rambow. 2003. Use of deep 
linguistic features for the recognition and labeling 
of semantic arguments. In Proceedings of the 2003 
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan. 
Katrin Erk and Sebastian Pado. 2006. Shalmaneser ? 
a toolchain for shallow semantic parsing. Proceed-
ings of LREC. 
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. 
In Proceedings of the 40th Annual Conference of 
the Association for Computational Linguistics 
(ACL-02), Philadelphia, PA, USA. 
Mary Harper and Zhongqiang Huang. 2009. Chinese 
Statistical Parsing, chapter to appear. 
Aria Haghighi, Kristina Toutanova, and Christopher 
Manning. 2005. A joint model for semantic role la-
beling. In Proceedings of the Ninth Conference on 
Computational Natural Language Learning 
(CoNLL-2005), pages 173?176.  
Paul Kingsbury and Martha Palmer. 2003. Propbank: 
the next level of treebank. In Proceedings of Tree-
banks and Lexical Theories. 
Evgeny Matusov, Gregor Leusch, & Hermann Ney: 
Learning to combine machine translation systems.  
In: Cyril Goutte, Nicola Cancedda, Marc Dymet-
man, & George Foster (eds.) Learning machine 
translation. (Cambridge, Mass.: The MIT Press, 
2009); pp.257-276. 
Adam Meyers, Ralph Grishman, Michiko Kosaka and 
Shubin Zhao.  2001. Covering Treebanks with 
GLARF. In Proceedings of the ACL 2001 Work-
shop on Sharing Tools and Resources. Annual 
Meeting of the ACL. Association for Computa-
tional Linguistics, Morristown, NJ, 51-58. 
Teruko Mitamura, Eric Nyberg, Hideki Shima, 
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, 
Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, 
Donghong Ji, and Noriko Kando. 2008. Overview 
of the NTCIR-7 ACLIA Tasks: Advanced Cross-
Lingual Information Access. In Proceedings of the 
Seventh NTCIR Workshop Meeting. 
Alessandro Moschitti, Silvia Quarteroni, Roberto 
Basili, and Suresh Manandhar. 2007. Exploiting 
syntactic and shallow semantic kernels for question 
answer classification. In Proceedings of the 45th 
Annual Meeting of the Association of Computa-
tional Linguistics, pages 776?783.  
Kristen Parton, Kathleen R. McKeown, James Allan, 
and Enrique Henestroza. Simultaneous multilingual 
search for translingual information retrieval. In 
Proceedings of ACM 17th Conference on Informa-
tion and Knowledge Management (CIKM), Napa 
Valley, CA, 2008. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. North American Chapter 
of the Association for Computational Linguistics 
(HLT-NAACL 2007). 
Sudo, K., Sekine, S., and Grishman, R. 2004. Cross-
lingual information extraction system evaluation. 
In Proceedings of the 20th international Confer-
ence on Computational Linguistics. 
Honglin Sun and Daniel Jurafsky. 2004. Shallow Se-
mantic Parsing of Chinese. In Proceedings of 
NAACL-HLT. 
Cynthia A. Thompson, Roger Levy, and Christopher 
Manning. 2003. A generative model for semantic 
role labeling. In 14th European Conference on Ma-
chine Learning. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
and Dekai Wu, editors, Proceedings of EMNLP 
2004, pages 88?94, Barcelona, Spain, July. Asso-
ciation for Computational Linguistics. 
Xue, Nianwen and Martha Palmer. 2005. Automatic 
semantic role labeling for Chinese verbs. InPro-
ceedings of the Nineteenth International Joint Con-
ference on Artificial Intelligence, pages 1160-1165.  
Chao Wang, Michael Collins, and Philipp Koehn. 
2007. Chinese Syntactic Reordering for Statistical 
Machine Translation. Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), 737-745. 
Jianqiang Wang and Douglas W. Oard, 2006. "Com-
bining Bidirectional Translation and Synonymy for 
Cross-Language Information Retrieval," in 29th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Re-
trieval, pp. 202-209. 
431
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 146?154,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Recognition of Logical Relations for English, Chinese and
Japanese in the GLARF Framework
Adam Meyers?, Michiko Kosaka?, Nianwen Xue?, Heng Ji?, Ang Sun?, Shasha Liao? and Wei Xu?
? New York University, ?Monmouth University, ?Brandeis University, ? City University of New York
Abstract
We present GLARF, a framework for repre-
senting three linguistic levels and systems for
generating this representation. We focus on a
logical level, like LFG?s F-structure, but com-
patible with Penn Treebanks. While less fine-
grained than typical semantic role labeling ap-
proaches, our logical structure has several ad-
vantages: (1) it includes all words in all sen-
tences, regardless of part of speech or seman-
tic domain; and (2) it is easier to produce ac-
curately. Our systems achieve 90% for En-
glish/Japanese News and 74.5% for Chinese
News ? these F-scores are nearly the same as
those achieved for treebank-based parsing.
1 Introduction
For decades, computational linguists have paired a
surface syntactic analysis with an analysis represent-
ing something ?deeper?. The work of Harris (1968),
Chomsky (1957) and many others showed that one
could use these deeper analyses to regularize differ-
ences between ways of expressing the same idea.
For statistical methods, these regularizations, in ef-
fect, reduce the number of significant differences be-
tween observable patterns in data and raise the fre-
quency of each difference. Patterns are thus easier
to learn from training data and easier to recognize in
test data, thus somewhat compensating for the spare-
ness of data. In addition, deeper analyses are often
considered semantic in nature because conceptually,
two expressions that share the same regularized form
also share some aspects of meaning. The specific de-
tails of this ?deep? analysis have varied quite a bit,
perhaps more than surface syntax.
In the 1970s and 1980s, Lexical Function Gram-
mar?s (LFG) way of dividing C-structure (surface)
and F-structure (deep) led to parsers such as (Hobbs
and Grishman, 1976) which produced these two lev-
els, typically in two stages. However, enthusiasm
for these two-stage parsers was eclipsed by the ad-
vent of one stage parsers with much higher accu-
racy (about 90% vs about 60%), the now-popular
treebank-based parsers including (Charniak, 2001;
Collins, 1999) and many others. Currently, many
different ?deeper? levels are being manually anno-
tated and automatically transduced, typically using
surface parsing and other processors as input. One
of the most popular, semantic role labels (annota-
tion and transducers based on the annotation) char-
acterize relations anchored by select predicate types
like verbs (Palmer et al, 2005), nouns (Meyers et
al., 2004a), discourse connectives (Miltsakaki et al,
2004) or those predicates that are part of particular
semantic frames (Baker et al, 1998). The CONLL
tasks for 2008 and 2009 (Surdeanu et al, 2008;
Hajic? et al, 2009) has focused on unifying many of
these individual efforts to produce a logical structure
for multiple parts of speech and multiple languages.
Like the CONLL shared task, we link surface lev-
els to logical levels for multiple languages. How-
ever, there are several differences: (1) The logical
structures produced automatically by our system can
be expected to be more accurate than the compara-
ble CONLL systems because our task involves pre-
dicting semantic roles with less fine-grained distinc-
tions. Our English and Japanese results were higher
than the CONLL 2009 SRL systems. Our English F-
scores range from 76.3% (spoken) to 89.9% (News):
146
the best CONLL 2009 English scores were 73.31%
(Brown) and 85.63% (WSJ). Our Japanese system
scored 90.6%: the best CONLL 2009 Japanese score
was 78.35%. Our Chinese system 74.5%, 4 points
lower than the best CONLL 2009 system (78.6%),
probably due to our system?s failings, rather than the
complexity of the task; (2) Each of the languages
in our system uses the same linguistic framework,
using the same types of relations, same analyses of
comparable constructions, etc. In one case, this re-
quired a conversion from a different framework to
our own. In contrast, the 2009 CONLL task puts
several different frameworks into one compatible in-
put format. (3) The logical structures produced by
our system typically connect all the words in the sen-
tence. While this is true for some of the CONLL
2009 languages, e.g., Czech, it is not true about
all the languages. In particular, the CONLL 2009
English and Chinese logical structures only include
noun and verb predicates.
In this paper, we will describe the GLARF frame-
work (Grammatical and Logical Representation
Framework) and a system for producing GLARF
output (Meyers et al, 2001; Meyers, 2008). GLARF
provides a logical structure for English, Chinese and
Japanese with an F-score that is within a few per-
centage points of the best parsing results for that
language. Like LFG?s (LFG) F-structure, our log-
ical structure is less fine-grained than many of the
popular semantic role labeling schemes, but also has
two main advantages over these schemes: it is more
reliable and it is more comprehensive in the sense
that it covers all parts of speech and the resulting
logical structure is a connected graph. Our approach
has proved adequate for three genetically unrelated
natural languages: English, Chinese and Japanese.
It is thus a good candidate for additional languages
with accurate parsers.
2 The GLARF framework
Our system creates a multi-tiered representation in
the GLARF framework, combining the theory un-
derlying the Penn Treebank for English (Marcus et
al., 1994) and Chinese (Xue et al, 2005) (Chom-
skian linguistics of the 1970s and 1980s) with: (2)
Relational Grammar?s graph-based way of repre-
senting ?levels? as sequences of relations; (2) Fea-
ture structures in the style of Head-Driven Phrase
Structure Grammar; and (3) The Z. Harris style goal
of attempting to regularize multiple ways of saying
the same thing into a single representation. Our
approach differs from LFG F-structure in several
ways: we have more than two levels; we have a
different set of relational labels; and finally, our ap-
proach is designed to be compatible with the Penn
Treebank framework and therefore, Penn-Treebank-
based parsers. In addition, the expansion of our the-
ory is governed more by available resources than by
the underlying theory. As our main goal is to use
our system to regularize data, we freely incorporate
any analysis that fits this goal. Over time, we have
found ways of incorporating Named Entities, Prop-
Bank, NomBank and the Penn Discourse Treebank.
Our agenda also includes incorporating the results of
other research efforts (Pustejovsky et al, 2005).
For each sentence, we generate a feature structure
(FS) representing our most complete analysis. We
distill a subset of this information into a dependency
structure governed by theoretical assumptions, e.g.,
about identifying functors of phrases. Each GLARF
dependency is between a functor and an argument,
where the functor is the head of a phrase, conjunc-
tion, complementizer, or other function word. We
have built applications that use each of these two
representations, e.g., the dependency representation
is used in (Shinyama, 2007) and the FS represen-
tation is used in (K. Parton and K. R. McKeown
and R. Coyne and M. Diab and R. Grishman and
D. Hakkani-Tu?r and M. Harper and H. Ji and W. Y.
Ma and A. Meyers and S. Stolbach and A. Sun and
G. Tu?r and W. Xu and S. Yarman, 2009).
In the dependency representation, each sentence
is a set of 23 tuples, each 23-tuple characterizing up
to three relations between two words: (1) a SUR-
FACE relation, the relation between a functor and an
argument in the parse of a sentence; (2) a LOGIC1
relation which regularizes for lexical and syntac-
tic phenomena like passive, relative clauses, deleted
subjects; and (3) a LOGIC2 relation corresponding
to relations in PropBank, NomBank, and the Penn
Discourse Treebank (PDTB). While the full output
has all this information, we will limit this paper to
a discussion of the LOGIC1 relations. Figure 1 is
a 5 tuple subset of the 23 tuple GLARF analysis of
the sentence Who was eaten by Grendel? (The full
147
L1 Surf L2 Func Arg
NIL SENT NIL Who was
PRD PRD NIL was eaten
COMP COMP ARG0 eaten by
OBJ NIL ARG1 eaten Who
NIL OBJ NIL by Grendel
SBJ NIL NIL eaten Grendel
Figure 1: 5-tuples: Who was eaten by Grendel
Who
eaten
was
by
PRD
S?OBJ
L?OBJ
ARG1
COMP
ARG0
S?SENT
L?SBJ
Grendel
Figure 2: Graph of Who was eaten by Grendel
23 tuples include unique ids and fine-grained lin-
guistic features). The fields listed are: logic1 label
(L1), surface label (Surf), logic2 label (L2), func-
tor (Func) and argument (Arg). NIL indicates that
there is no relation of that type. Figure 2 repre-
sents this as a graph. For edges with two labels,
the ARG0 or ARG1 label indicates a LOGIC2 re-
lation. Edges with an L- prefix are LOGIC1 la-
bels (the edges are curved); edges with S-prefixes
are SURFACE relations (the edges are dashed); and
other (thick) edges bear unprefixed labels represent-
ing combined SURFACE/LOGIC1 relations. Delet-
ing the dashed edges yields a LOGIC1 representa-
tion; deleting the curved edges yields a SURFACE
representation; and a LOGIC2 consists of the edges
labeled ARGO and ARG1 relations, plus the sur-
face subtrees rooted where the LOGIC2 edges ter-
minate. Taken together, a sentence?s SURFACE re-
lations form a tree; the LOGIC1 relations form a
directed acyclic graph; and the LOGIC2 relations
form directed graphs with some cycles and, due to
PDTB relations, may connect sentences to previous
ones, e.g., adverbs like however, take the previous
sentence as one of their arguments.
LOGIC1 relations (based on Relational Gram-
mar) regularize across grammatical and lexical al-
ternations. For example, subcategorized verbal ar-
guments include: SBJect, OBJect and IND-OBJ (in-
direct Object), COMPlement, PRT (Particle), PRD
(predicative complement). Other verbal modifiers
include AUXilliary, PARENthetical, ADVerbial. In
contrast, FrameNet and PropBank make finer dis-
tinctions. Both PP arguments of consulted in John
consulted with Mary about the project bear COMP
relations with the verb in GLARF, but would have
distinct labels in both PropBank and FrameNet.
Thus Semantic Role Labeling (SRL) should be more
difficult than recognizing LOGIC1 relations.
Beginning with Penn Treebank II, Penn Treebank
annotation includes Function tags, hyphenated addi-
tions to phrasal categories which indicate their func-
tion. There are several types of function tags:
? Argument Tags such as SBJ, OBJ, IO (IND-
OBJ), CLR (COMP) and PRD?These are lim-
ited to verbal relations and not all are used in
all treebanks. For example, OBJ and IO are
used in the Chinese, but not the English tree-
bank. These labels can often be directly trans-
lated into GLARF LOGIC1 relations.
? Adjunct Tags such as ADV, TMP, DIR, LOC,
MNR, PRP?These tags often translate into a
single LOGIC1 tag (ADV). However, some of
these also correspond to LOGIC1 arguments.
In particular, some DIR and MNR tags are re-
alized as LOGIC1 COMP relations (based on
dictionary entries). The fine grained seman-
tic distinctions are maintained in other features
that are part of the GLARF description.
In addition, GLARF treats Penn?s PRN phrasal
category as a relation rather than a phrasal category.
For example, given a sentence like, Banana ketchup,
the agency claims, is very nutritious, the phrase
the agency claims is analyzed as an S(entence) in
GLARF bearing a (surface) PAREN relation to the
main clause. Furthermore, the whole sentence is a
COMP of the verb claims. Since PAREN is a SUR-
FACE relation, not a LOGIC1 relation, there is no
LOGIC1 cycle as shown by the set of 5-tuples in
Figure 3? a cycle only exists if you include both
SURFACE and LOGIC1 relations in a single graph.
Another important feature of the GLARF frame-
work is transparency, a term originating from N.
148
L1 Surf L2 Func Arg
NIL SBJ ARG1 is ketchup
PRD PRD ARG2 is nutritious
SBJ NIL NIL nutritious Ketchup
ADV ADV NIL nutritious very
N-POS N-POS NIL ketchup Banana
NIL PAREN NIL is claims
SBJ SBJ ARG0 claims agency
Q-POS Q-POS NIL agency the
COMP NIL ARG1 claims is
Figure 3: 5-tuples: Banana Ketchup, the agency claims,
is very nutritious
L1 Surf L2 Func Arg
SBJ SBJ ARG0 ate and
OBJ OBJ ARG1 ate box
CONJ CONJ NIL and John
CONJ CONJ NIL and Mary
COMP COMP NIL box of
Q-POS Q-POS NIL box the
OBJ OBJ NIL of cookies
Figure 4: 5-tuples: John and Mary ate the box of cookies
Sager?s unpublished work. A relation between two
words is transparent if: the functor fails to character-
ize the selectional properties of the phrase (or sub-
graph in a Dependency Analysis), but its argument
does. For example, relations between conjunctions
(e.g., and, or, but) and their conjuncts are transparent
CONJ relations. Thus although and links together
John and Mary, it is these dependents that deter-
mine that the resulting phrase is noun-like (an NP
in phrase structure terminology) and sentient (and
thus can occur as the subject of verbs like ate). An-
other common example of transparent relations are
the relations connecting certain nouns and the prepo-
sitional objects under them, e.g., the box of cookies
is edible, because cookies are edible even though
boxes are not. These features are marked in the
NOMLEX-PLUS dictionary (Meyers et al, 2004b).
In Figure 4, we represent transparent relations, by
prefixing the LOGIC1 label with asterisks.
The above description most accurately describes
English GLARF. However, Chinese GLARF has
most of the same properties, the main exception be-
ing that PDTB arguments are not currently marked.
For Japanese, we have only a preliminary represen-
tation of LOGIC2 relations and they are not derived
from PropBank/NomBank/PDTB.
2.1 Scoring the LOGIC1 Structure
For purposes of scoring, we chose to focus on
LOGIC1 relations, our proposed high-performance
level of semantics. We scored with respect to: the
LOGIC1 relational label, the identity of the functor
and the argument, and whether the relation is trans-
parent or not. If the system output differs in any of
these respects, the relation is marked wrong. The
following sections will briefly describe each system
and present an evaluation of its results.
The answer keys for each language were created
by native speakers editing system output, as repre-
sented similarly to the examples in this paper, al-
though part of speech is included for added clar-
ity. In addition, as we attempted to evaluate logi-
cal relation (or dependency) accuracy independent
of sentence splitting. We obtained sentence divi-
sions from data providers and treebank annotation
for all the Japanese and most of the English data, but
used automatic sentence divisions for the English
BLOG data. For the Chinese, we omitted several
sentences from our evaluation set due to incorrect
sentence splits. The English and Japanese answer
keys were annotated by single native speakers ex-
pert in GLARF. The Chinese data was annotated by
several native speakers and may have been subject
to some interannotator agreement difficulties, which
we intend to resolve in future work. Currently, cor-
recting system output is the best way to create an-
swer keys due to certain ambiguities in the frame-
work, some of which we hope to incorporate into fu-
ture scoring procedures. For example, consider the
interpretation of the phrase five acres of land in Eng-
land with respect to PP attachment. The difference
in meaning between attaching the PP in England
to acres or to land is too subtle for these authors?
we have difficulty imagining situations where one
statement would be accurate and the other would
not. This ambiguity is completely predictable be-
cause acres is a transparent noun and similar ambi-
guities hold for all such cases where a transparent
noun takes a complement and is followed by a PP
attachment. We believe that a more complex scor-
ing program could account for most of these cases.
149
Similar complexities arise for coordination and sev-
eral other phenomena.
3 English GLARF
We generate English GLARF output by applying a
procedure that combines:
1. The output of the 2005 version of the Charniak
parser described in (Charniak, 2001), which
label precision and recall scores in the 85%
range. The updated version of the parser seems
to perform closer to 90% on News data and per-
form lower on other genres. That performance
would reflect reports on other versions of the
Charniak parser for which statistics are avail-
able (Foster and van Genabith, 2008).
2. Named entity (NE) tags from the JET NE sys-
tem (Ji and Grishman, 2006), which achieves
F-scores ranging 86%-91% on newswire for
both English and Chinese (depending on
Epoch). The JET system identifies seven
classes of NEs: Person, GPE, Location, Orga-
nization, Facility, Weapon and Vehicle.
3. Machine Readable dictionaries: COMLEX
(Macleod et al, 1998), NOMBANK dictio-
naries (from http://nlp.cs.nyu.edu/
meyers/nombank/) and others.
4. A sequence of hand-written rules (citations
omitted) such that: (1) the first set of rules con-
vert the Penn Treebank into a Feature Structure
representation; and (2) each rule N after the
first rule is applied to an entire Feature Struc-
ture that is the output of rule N ? 1.
For this paper, we evaluated the English output for
several different genres, all of which approximately
track parsing results for that genre. For written
genres, we chose between 40 and 50 sentences.
For speech transcripts, we chose 100 sentences?we
chose this larger number because a lot of so-called
sentences contained text with empty logical de-
scriptions, e.g., single word utterances contain no
relations between pairs of words. Each text comes
from a different genre. For NEWS text, we used 50
sentences from the aligned Japanese-English data
created as part of the JENAAD corpus (Utiyama
Genre Prec Rec F
NEWS 731815 = 89.7%
715
812 = 90.0% 89.9%
BLOG 704844 = 83.4%
704
899 = 78.3% 80.8%
LETT 392434 = 90.3%
392
449 = 87.3% 88.8%
TELE 472604 = 78.1%
472
610 = 77.4% 77.8%
NARR 732959 = 76.3%
732
964 = 75.9% 76.1%
Table 1: English Aggregate Scores
Corpus Prec Rec F Sents
NEWS 90.5% 90.8% 90.6% 50
BLOG 84.1% 79.6% 81.7% 46
LETT 93.9% 89.2% 91.4% 46
TELE 81.4% 83.2% 84.9% 103
NARR 77.1% 78.1% 79.5% 100
Table 2: English Score per Sentence
and Isahara, 2003); the web text (BLOGs) was
taken from some corpora provided by the Linguistic
Data Consortium through the GALE (http:
//projects.ldc.upenn.edu/gale/) pro-
gram; the LETTer genre (a letter from Good Will)
was taken from the ICIC Corpus of Fundraising
Texts (Indiana Center for Intercultural Communi-
cation); Finally, we chose two spoken language
transcripts: a TELEphone conversation from
the Switchboard Corpus (http://www.ldc.
upenn.edu/Catalog/readme_files/
switchboard.readme.html) and one NAR-
Rative from the Charlotte Narrative and Conversa-
tion Collection (http://newsouthvoices.
uncc.edu/cncc.php). In both cases, we
assumed perfect sentence splitting (based on Penn
Treebank annotation). The ICIC, Switchboard
and Charlotte texts that we used are part of the
Open American National Corpus (OANC), in
particular, the SIGANN shared subcorpus of the
OANC (http://nlp.cs.nyu.edu/wiki/
corpuswg/ULA-OANC-1) (Meyers et al, 2007).
Comparable work for English includes: (1) (Gab-
bard et al, 2006), a system which reproduces the
function tags of the Penn Treebank with 89% accu-
racy and empty categories (and their antecedents)
with varying accuracies ranging from 82.2% to
96.3%, excluding null complementizers, as these are
theory-internal and have no value for filling gaps.
(2) Current systems that generate LFG F-structure
150
such as (Wagner et al, 2007) which achieve an F
score of 91.1 on the F-structure PRED relations,
which are similar to our LOGIC1 relations.
4 Chinese GLARF
The Chinese GLARF program takes a Chinese
Treebank-style syntactic parse and the output of a
Chinese PropBanker (Xue, 2008) as input, and at-
tempts to determine the relations between the head
and its dependents within each constituent. It does
this by first exploiting the structural information
and detecting six broad categories of syntactic rela-
tions that hold between the head and its dependents.
These are predication, modification, complementa-
tion, coordination, auxiliary, and flat. Predication
holds at the clause level between the subject and the
predicate, where the predicate is considered to be
the head and the subject is considered to the depen-
dent. Modification can also hold mainly within NPs
and VPs, where the dependents are modifiers of the
NP head or adjuncts to the head verb. Coordination
holds almost for all phrasal categories where each
non-punctuation child within this constituent is ei-
ther conjunction or a conjunct. The head in a co-
ordination structure is underspecified and can be ei-
ther a conjunct or a conjunction depending on the
grammatical framework. Complementation holds
between a head and its complement, with the com-
plement usually being a core argument of the head.
For example, inside a PP, the preposition is the head
and the phrase or clause it takes is the dependent. An
auxiliary structure is one where the auxiliary takes
a VP as its complement. This structure is identi-
fied so that the auxiliary and the verb it modifies can
form a verb group in the GLARF framework. Flat
structures are structures where a constituent has no
meaningful internal structure, which is possible in a
small number of cases. After these six broad cate-
gories of relations are identified, more fine-grained
relation can be detected with additional information.
Figure 5 is a sample 4-tuple for a Chinese translation
of the sentence in figure 3.
For the results reported in Table 3, we used the
Harper and Huang parser described in (Harper and
Huang, Forthcoming) which can achieve F-scores
as high as 85.2%, in combination with informa-
tion about named entities from the output of the
Figure 5: Agency claims, Banana Ketchup is very have
nutrition DE.
JET Named Entity tagger for Chinese (86%-91% F-
measure as per section 3). We used the NE tags to
adjust the parts of speech and the phrasal boundaries
of named entities (we do the same with English).
As shown in Table 3, we tried two versions of the
Harper and Huang parser, one which adds function
tags to the output and one that does not. The Chinese
GLARF system scores significantly (13.9% F-score)
higher given function tagged input, than parser out-
put without function tags. Our current score is about
10 points lower than the parser score. Our initial er-
ror analysis suggests that the most common forms
of errors involve: (1) the processing of long NPs;
(2) segmentation and POS errors; (3) conjunction
scope; and (4) modifier attachment.
5 Japanese GLARF
For Japanese, we process text with the KNP parser
(Kurohashi and Nagao, 1998) and convert the output
into the GLARF framework. The KNP/Kyoto Cor-
pus framework is a Japanese-specific Dependency
framework, very different from the Penn Treebank
framework used for the other systems. Process-
ing in Japanese proceeds as follows: (1) we pro-
cess the Japanese with the Juman segmenter (Kuro-
151
Type Prec Rec F
No Function Tags Version
Aggr 8431374 = 61.4%
843
1352 = 62.4% 61.8%
Aver 62.3% 63.5% 63.6%
Function Tags Version
Aggr 10311415 = 72.9%
1031
1352 = 76.3% 74.5%
Aver 73.0% 75.3% 74.9%
Table 3: 53 Chinese Newswire Sentences: Aggregate and
Average Sentence Scores
hashi et al, 1994) and KNP parser 2.0 (Kurohashi
and Nagao, 1998), which has reported accuracy of
91.32% F score for dependency accuracy, as re-
ported in (Noro et al, 2005). As is standard in
Japanese linguistics, the KNP/Kyoto Corpus (K)
framework uses a dependency analysis that has some
features of a phrase structure analysis. In partic-
ular, the dependency relations are between bun-
setsu, small constituents which include a head word
and some number of modifiers which are typically
function words (particles, auxiliaries, etc.), but can
also be prenominal noun modifiers. Bunsetsu can
also include multiple words in the case of names.
The K framework differentiates types of dependen-
cies into: the normal head-argument variety, coor-
dination (or parallel) and apposition. We convert
the head-argument variety of dependency straight-
forwardly into a phrase consisting of the head and
all the arguments. In a similar way, appositive re-
lations could be represented using an APPOSITIVE
relation (as is currently done with English). In the
case of bunsetsu, the task is to choose a head and
label the other constituents?This is very similar to
our task of labeling and subdividing the flat noun
phrases of the English Penn Treebank. Conjunction
is a little different because the K analysis assumes
that the final conjunct is the functor, rather than a
conjunction. We automatically changed this analy-
sis to be the same as it is for English and Chinese.
When there was no actual conjunction, we created a
theory-internal NULL conjunction. The final stages
include: (1) processing conjunction and apposition,
including recognizing cases that the parser does not
recognize; (2) correcting parts of speech; (3) label-
ing all relations between arguments and heads; (4)
recognizing and labeling special constituent types
Figure 6: It is the state?s duty to protect lives and assets.
Type Prec Rec F
Aggr 764843 = 91.0%
764
840 = 90.6% 90.8%
Aver 90.7% 90.6% 90.6%
Table 4: 40 Japanese Sentences from JENAA Corpus:
Aggregate and Average Sentence Scores
such as Named Entities, double quote constituents
and number phrases (twenty one); (5) handling com-
mon idioms; and (6) processing light verb and cop-
ula constructions.
Figure 6 is a sample 4-tuple for a Japanese
sentence meaning It is the state?s duty to protect
lives and assets. Conjunction is handled as dis-
cussed above, using an invisible NULL conjunction
and transparent (asterisked) logical CONJ relations.
Copulas in all three languages take surface subjects,
which are the LOGIC1 subjects of the PRD argu-
ment of the copula. We have left out glosses for the
particles, which act solely as case markers and help
us identify the grammatical relation.
We scored Japanese GLARF on forty sentences of
the Japanese side of the JENAA data (25 of which
are parallel with the English sentences scored). Like
the English, the F score is very close to the parsing
scores achieved by the parser.
152
6 Concluding Remarks and Future Work
In this paper, we have described three systems
for generating GLARF representations automati-
cally from text, each system combines the out-
put of a parser and possibly some other processor
(segmenter, Named Entity Recognizer, PropBanker,
etc.) and creates a logical representation of the sen-
tence. Dictionaries, word lists, and various other
resources are used, in conjunction with hand writ-
ten rules. In each case, the results are very close to
parsing accuracy. These logical structures are in the
same annotation framework, using the same labeling
scheme and the same analysis for key types of con-
structions. There are several advantages to our ap-
proach over other characterizations of logical struc-
ture: (1) our representation is among the most accu-
rate and reliable; (2) our representation connects all
the words in the sentence; and (3) having the same
representation for multiple languages facilitates run-
ning the same procedures in multiple languages and
creating multilingual applications.
The English system was developed for the News
genre, specifically the Penn Treebank Wall Street
Journal Corpus. We are therefore considering
adding rules to better handle constructions that ap-
pear in other genres, but not news. The experi-
ments describe here should go a long way towards
achieving this goal. We are also considering ex-
periments with parsers tailored to particular genres
and/or parsers that add function tags (Harper et al,
2005). In addition, our current GLARF system uses
internal Propbank/NomBank rules, which have good
precision, but low recall. We expect that we achieve
better results if we incorporate the output of state
of the art SRL systems, although we would have to
conduct experiments as to whether or not we can im-
prove such results with additional rules.
We developed the English system over the course
of eight years or so. In contrast, the Chinese and
Japanese systems are newer and considerably less
time was spent developing them. Thus they cur-
rently do not represent as many regularizations. One
obstacle is that we do not currently use subcate-
gorization dictionaries for either language, while
we have several for English. In particular, these
would be helpful in predicting and filling relative
clause and others gaps. We are considering auto-
matically acquiring simple dictionaries by recording
frequently occurring argument types of verbs over
a larger corpus, e.g., along the lines of (Kawahara
and Kurohashi, 2002). In addition, existing Japanese
dictionaries such as the IPAL (monolingual) dictio-
nary (technology Promotion Agency, 1987) or previ-
ously acquired case information reported in (Kawa-
hara and Kurohashi, 2002).
Finally, we are investigating several avenues for
using this system output for Machine Translation
(MT) including: (1) aiding word alignment for other
MT system (Wang et al, 2007); and (2) aiding the
creation various MT models involving analyzed text,
e.g., (Gildea, 2004; Shen et al, 2008).
Acknowledgments
This work was supported by NSF Grant IIS-
0534700 Structure Alignment-based MT.
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet Project. In Coling-ACL98, pages
86?90.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL 2001, pages 116?123.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Foster and J. van Genabith. 2008. Parser Evaluation
and the BNC: 4 Parsers and 3 Evaluation Metrics. In
LREC 2008, Marrakech, Morocco.
R. Gabbard, M. Marcus, and S. Kulick. 2006. Fully pars-
ing the penn treebank. In NAACL/HLT, pages 184?
191.
D. Gildea. 2004. Dependencies vs. Constituents for
Tree-Based Alignment. In EMNLP, Barcelona.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multiple lan-
guages. In CoNLL-2009, Boulder, Colorado, USA.
M. Harper and Z. Huang. Forthcoming. Chinese Statis-
tical Parsing. In J. Olive, editor, Global Autonomous
Language Exploitation. Publisher to be Announced.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, L. Yung, A. Krasnyan-
skaya, and R. Stewart. 2005. Parsing and Spoken
153
Structural Event. Technical Report, The John-Hopkins
University, 2005 Summer Research Workshop.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley-Interscience, New York.
J. R. Hobbs and R. Grishman. 1976. The Automatic
Transformational Analysis of English Sentences: An
Implementation. International Journal of Computer
Mathematics, 5:267?283.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Sydney,
Australia.
K. Parton and K. R. McKeown and R. Coyne and M. Diab
and R. Grishman and D. Hakkani-Tu?r and M. Harper
and H. Ji and W. Y. Ma and A. Meyers and S. Stol-
bach and A. Sun and G. Tu?r and W. Xu and S. Yarman.
2009. Who, What, When, Where, Why? Comparing
Multiple Approaches to the Cross-Lingual 5W Task.
In ACL 2009.
D. Kawahara and S. Kurohashi. 2002. Fertilization
of Case Frame Dictionary for Robust Japanese Case
Analysis. In Proc. of COLING 2002.
S. Kurohashi and M. Nagao. 1998. Building a Japanese
parsed corpus while improving the parsing system. In
Proceedings of The 1st International Conference on
Language Resources & Evaluation, pages 719?724.
S. Kurohashi, T. Nakamura, Y. Matsumoto, and M. Na-
gao. 1994. Improvements of Japanese Morpholog-
ical Analyzer JUMAN. In Proc. of International
Workshop on Sharable Natural Language Resources
(SNLR), pages 22?28.
C. Macleod, R. Grishman, and A. Meyers. 1998. COM-
LEX Syntax. Computers and the Humanities, 31:459?
481.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Proceed-
ings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004a. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
A. Meyers, R. Reeves, Catherine Macleod, Rachel Szeke-
ley, Veronkia Zielinska, and Brian Young. 2004b. The
Cross-Breeding of Dictionaries. In Proceedings of
LREC-2004, Lisbon, Portugal.
A. Meyers, N. Ide, L. Denoyer, and Y. Shinyama. 2007.
The shared corpora working group report. In Pro-
ceedings of The Linguistic Annotation Workshop, ACL
2007, pages 184?190, Prague, Czech Republic.
A. Meyers. 2008. Using treebank, dictionaries and
glarf to improve nombank annotation. In Proceedings
of The Linguistic Annotation Workshop, LREC 2008,
Marrakesh, Morocco.
E. Miltsakaki, A. Joshi, R. Prasad, and B. Webber. 2004.
Annotating discourse connectives and their arguments.
In A. Meyers, editor, NAACL/HLT 2004 Workshop:
Frontiers in Corpus Annotation, pages 9?16, Boston,
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
T. Noro, C. Koike, T. Hashimoto, T. Tokunaga, and
Hozumi Tanaka. 2005. Evaluation of a Japanese CFG
Derived from a Syntactically Annotated corpus with
Respect to Dependency Measures. In 2005 Workshop
on Treebanks and Linguistic theories, pages 115?126.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
J. Pustejovsky, A. Meyers, M. Palmer, and M. Poe-
sio. 2005. Merging PropBank, NomBank, TimeBank,
Penn Discourse Treebank and Coreference. In ACL
2005 Workshop: Frontiers in Corpus Annotation II:
Pie in the Sky.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In ACL 2008.
Y. Shinyama. 2007. Being Lazy and Preemptive at
Learning toward Information Extraction. Ph.D. the-
sis, NYU.
M. Surdeanu, R. Johansson, A. Meyers, Ll. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Dependen-
cies. In Proceedings of the CoNLL-2008 Shared Task,
Manchester, GB.
Information technology Promotion Agency. 1987. IPA
Lexicon of the Japanese Language for Computers
IPAL (Basic Verbs). (in Japanese).
M. Utiyama and H. Isahara. 2003. Reliable Measures
for Aligning Japanese-English News Articles and Sen-
tences. In ACL-2003, pages 72?79.
J. Wagner, D. Seddah, J. Foster, and J. van Genabith.
2007. C-Structures and F-Structures for the British
National Corpus. In Proceedings of the Twelfth In-
ternational Lexical Functional Grammar Conference,
Stanford. CSLI Publications.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
EMNLP-CoNLL 2007, pages 737?745.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese TreeBank: Phrase Structure Annotation
of a Large Corpus. Natural Language Engineering,
11:207?238.
N. Xue. 2008. Labeling Chinese Predicates with Seman-
tic roles. Computational Linguistics, 34:225?255.
154
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 116?120,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Transducing Logical Relations from Automatic and Manual GLARF
Adam Meyers?, Michiko Kosaka?, Heng Ji?, Nianwen Xue?,
Mary Harper?, Ang Sun?, Wei Xu? and Shasha Liao?
? New York Univ., ?Monmouth Univ., ?Brandeis Univ,, ?City Univ. of New York, ?Johns
Hopkins Human Lang. Tech. Ctr. of Excellence & U. of Maryland, College Park
Abstract
GLARF relations are generated from tree-
bank and parses for English, Chinese and
Japanese. Our evaluation of system out-
put for these input types requires consid-
eration of multiple correct answers.1
1 Introduction
Systems, such as treebank-based parsers (Char-
niak, 2001; Collins, 1999) and semantic role la-
belers (Gildea and Jurafsky, 2002; Xue, 2008), are
trained and tested on hand-annotated data. Evalu-
ation is based on differences between system out-
put and test data. Other systems use these pro-
grams to perform tasks unrelated to the original
annotation. For example, participating systems in
CONLL (Surdeanu et al, 2008; Hajic? et al, 2009),
ACE and GALE tasks merged the results of sev-
eral processors (parsers, named entity recognizers,
etc.) not initially designed for the task at hand.
This paper discusses differences between hand-
annotated data and automatically generated data
with respect to our GLARFers, systems for gen-
erating Grammatical and Logical Representation
Framework (GLARF) for English, Chinese and
Japanese sentences. The paper describes GLARF
(Meyers et al, 2001; Meyers et al, 2009) and
GLARFers and compares GLARF produced from
treebank and parses.
2 GLARF
Figure 1 includes simplified GLARF analyses for
English, Chinese and Japanese sentences. For
each sentence, a GLARFer constructs both a Fea-
ture Structure (FS) representing a constituency
analysis and a set of 31-tuples, each representing
1Support includes: NSF IIS-0534700 & IIS-0534325
Structure Alignment-based MT; DARPA HR0011-06-C-
0023 & HR0011-06-C-0023; CUNY REP & GRTI Program.
This work does not necessarily reflect views of sponsors.
up to three dependency relations between pairs of
words. Due to space limitations, we will focus on
the 6 fields of the 31-tuple represented in Figure 1.
These include: (1) a functor (func); (2) the de-
pending argument (Arg); (3) a surface (Surf) la-
bel based on the position in the parse tree with no
regularizations; (4) a logic1 label (L
?
1) for a re-
lation that reflects grammar-based regularizations
of the surface level. This marks relations for fill-
ing gaps in relative clauses or missing infinitival
subjects, represents passives as paraphrases as ac-
tives, etc. While the general framework supports
many regularizations, the relations actually repre-
sented depends on the implemented grammar, e.g.,
our current grammar of English regularizes across
passives and relative clauses, but our grammars
of Japanese and Chinese do not currently.; (5) a
logic2 label (L2) for Chinese and English, which
represents PropBank, NomBank and Penn Dis-
course Treebank relations; and (6) Asterisks (*)
indicate transparent relations, relations where the
functor inherits semantic properties of certain spe-
cial arguments (*CONJ, *OBJ, *PRD, *COMP).
Figure 1 contains several transparent relations.
The interpretation of the *CONJ relations in the
Japanese example, include not only that the nouns
[zaisan] (assets) and [seimei] (lives) are con-
joined, but also that these two nouns, together
form the object of the Japanese verb [mamoru]
(protect). Thus, for example, semantic selection
patterns should treat these nouns as possible ob-
jects for this verb. Transparent relations may serve
to neutralize some of the problematic cases of at-
tachment ambiguity. For example, in the English
sentence A number of phrases with modifiers are
not ambiguous, there is a transparent *COMP re-
lation between numbers and of and a transpar-
ent *OBJ relation between of and phrases. Thus,
high attachment of the PP with modifiers, would
have the same interpretation as low attachment
since phrases is the underlying head of number of
116
Figure 1: GLARF 5-tuples for 3 languages
phrases. In this same example, the adverb not can
be attached to either the copula are or the pred-
icative adjective, with no discernible difference in
meaning?this factor is indicated by the transparent
designation of the relations where the copula is a
functor. Transparent features also provide us with
a simple way of handling certain function words,
such as the Chinese word De which inherits the
function of its underlying head, connecting a vari-
ety of such modifiers to head nouns (an adjective
in the Chinese example.). For conjunction cases,
the number of underlying relations would multi-
ply, e.g., Mary and John bought and sold stock
would (underlyingly) have four subject relations
derived by pairing each of the underlying subject
nouns Mary and John with each of the underlying
main predicate verbs bought and sold.
3 Automatic vs. Manual Annotation
Apart from accuracy, there are several other ways
that automatic and manual annotation differs. For
Penn-treebank (PTB) parsing, for example, most
parsers (not all) leave out function tags and empty
categories. Consistency is an important goal for
manual annotation for many reasons including: (1)
in the absence of a clear correct answer, consis-
tency helps clarify measures of annotation quality
(inter-annotator agreement scores); and (2) consis-
tent annotation is better training data for machine
learning. Thus, annotation specifications use de-
faults to ensure the consistent handling of spurious
ambiguity. For example, given a sentence like I
bought three acres of land in California, the PP in
California can be attached to either acres or land
with no difference in meaning. While annotation
guidelines may direct a human annotator to prefer,
for example, high attachment, systems output may
have other preferences, e.g., the probability that
land is modified by a PP (headed by in) versus the
probability that acres can be so modified.
Even if the manual annotation for a particular
corpus is consistent when it comes to other factors
such as tokenization or part of speech, developers
of parsers sometimes change these guidelines to
suit their needs. For example, users of the Char-
niak parser (Charniak, 2001) should add the AUX
category to the PTB parts of speech and adjust
their systems to account for the conversion of the
word ain?t into the tokens IS and n?t. Similarly, to-
kenization decisions with respect to hyphens vary
among different versions of the Penn Treebank, as
well as different parsers based on these treebanks.
Thus if a system uses multiple parsers, such differ-
ences must be accounted for. Differences that are
not important for a particular application should
be ignored (e.g., by merging alternative analyses).
For example, in the case of spurious attachment
ambiguity, a system may need to either accept both
as right answers or derive a common representa-
tion for both. Of course, many of the particular
problems that result from spurious ambiguity can
be accounted for in hind sight. Nevertheless, it
is precisely this lack of a controlled environment
which adds elements of spurious ambiguity. Us-
ing new processors or training on new treebanks
can bring new instances of spurious ambiguity.
4 Experiments and Evaluation
We ran GLARFers on both manually created tree-
banks and automatically produced parses for En-
glish, Chinese and Japanese. For each corpus, we
created one or more answer keys by correcting
117
system output. For this paper, we evaluate solely
on the logic1 relations (the second column in fig-
ure 1.) Figure 2 lists our results for all three lan-
guages, based on treebank and parser input.
As in (Meyers et al, 2009), we generated 4-
tuples consisting of the following for each depen-
dency: (A) the logic1 label (SBJ, OBJ, etc.), (B)
its transparency (True or False), (C) The functor (a
single word or a named entity); and (D) the argu-
ment (a single word or a named entity). In the case
of conjunction where there was no lexical con-
junction word, we used either punctuation (com-
mas or semi-colons) or the placeholder *NULL*.
We then corrected these results by hand to produce
the answer key?an answer was correct if all four
members of the tuple were correct and incorrect
otherwise. Table 2 provides the Precision, Recall
and F-scores for our output. The F-T columns
indicates a modified F-score derived by ignoring
the +/-Transparent distinction (resulting changes
in precision, recall and F-score are the same).
For English and Japanese, an expert native
speaking linguist corrected the output. For Chi-
nese, several native speaking computational lin-
guists shared the task. By checking compatibil-
ity of the answer keys with outputs derived from
different sources (parser, treebank), we could de-
tect errors and inconsistencies. We processed the
following corpora. English: 86 sentence article
(wsj 2300) from the Wall Street Journal PTB test
corpus (WSJ); 46 sentence letter from Good Will
(LET), the first 100 sentences of a switchboard
telephone transcript (TEL) and the first 100 sen-
tences of a narrative from the Charlotte Narra-
tive and Conversation (NAR). These samples are
taken from the PTB WSJ Corpus and the SIGANN
shared subcorpus of the OANC. The filenames are:
110CYL067, NapierDianne and sw2014. Chi-
nese: a 20 sentence sample of text from the
Penn Chinese Treebank (CTB) (Xue et al, 2005).
Japanese: 20 sentences from the Kyoto Corpus
(KYO) (Kurohashi and Nagao, 1998)
5 Running the GLARFer Programs
We use Charniak, UMD and KNP parsers (Char-
niak, 2001; Huang and Harper, 2009; Kurohashi
and Nagao, 1998), JET Named Entity tagger (Gr-
ishman et al, 2005; Ji and Grishman, 2006)
and other resources in conjunction with language-
specific GLARFers that incorporate hand-written
rules to convert output of these processors into
a final representation, including logic1 struc-
ture, the focus of this paper. English GLAR-
Fer rules use Comlex (Macleod et al, 1998a)
and the various NomBank lexicons (http://
nlp.cs.nyu.edu/meyers/nombank/) for
lexical lookup. The GLARF rules implemented
vary by language as follows. English: cor-
recting/standardizing phrase boundaries and part
of speech (POS); recognizing multiword expres-
sions; marking subconstituents; labeling rela-
tions; incorporating NEs; regularizing infiniti-
val, passives, relatives, VP deletion, predica-
tive and numerous other constructions. Chi-
nese: correcting/standardizing phrase boundaries
and POS, marking subconstituents, labeling rela-
tions; regularizing copula constructions; incorpo-
rating NEs; recognizing dates and number expres-
sions. Japanese: converting to PTB format; cor-
recting/standardizing phrase boundaries and POS;
labeling relations; processing NEs, double quote
constructions, number phrases, common idioms,
light verbs and copula constructions.
6 Discussion
Naturally, the treebank-based system out-
performed parse-based system. The Charniak
parser for English was trained on the Wall Street
Journal corpus and can achieve about 90% accu-
racy on similar corpora, but lower accuracy on
other genres. Differences between treebank and
parser results for English were higher for LET and
NAR genres than for the TEL because the system
is not currently designed to handle TEL-specific
features like disfluencies. All processors were
trained on or initially designed for news corpora.
Thus corpora out of this domain usually produce
lower results. LET was easier as it consisted
mainly of short simple sentences. In (Meyers et
al., 2009), we evaluated our results on 40 Japanese
sentences from the JENAAD corpus (Utiyama
and Isahara, 2003) and achieved a higher F-score
(90.6%) relative to the Kyoto corpus, as JENAAD
tends to have fewer long complex sentences.
By using our answer key for multiple inputs, we
discovered errors and consequently improved the
quality of the answer keys. However, at times we
were also compelled to fork the answer keys?given
multiple correct answers, we needed to allow dif-
ferent answer keys corresponding to different in-
puts. For English, these items represent approxi-
mately 2% of the answer keys (there were a total
118
Treebank Parser
ID % Prec % Rec F F-T % Prec % Rec F F-T
WSJ 12381491 = 83.0
1238
1471 = 84.2 83.6 87.1
1164
1452 = 80.2
1164
1475 = 78.9 79.5 81.8
LET 419451 = 92.9
419
454 = 92.3 92.6 93.3
390
434 = 89.9
390
454 = 85.9 87.8 87.8
TEL 478627 = 76.2
478
589 = 81.2 78.6 82.2
439
587 = 74.8
439
589 = 74.5 74.7 77.4
NAR 8171013 = 80.7
817
973 =84.0 82.3 84.1
724
957 = 75.7
724
969 = 74.7 75.2 76.1
CTB 351400 = 87.8
351
394 = 89.1 88.4 88.7
352
403 = 87.3
352
438 = 80.4 83.7 83.7
KYO 525575 = 91.3
525
577 = 91.0 91.1 91.1
493
581 = 84.9
493
572 = 86.2 85.5 87.8
Figure 2: Logic1 Scores
Figure 3: Examples of Answer Key Divergences
of 74 4-tuples out of a total of 3487). Figure 3 lists
examples of answer key divergences that we have
found: (1) alternative tokenizations; (2) spurious
differences in attachment and conjunction scope;
and (3) ambiguities specific to our framework.
Examples 1 and 2 reflect different treatments of
hyphenation and contractions in treebank specifi-
cations over time. Parsers trained on different tree-
banks will either keep hyphenated words together
or separate more words at hyphens. The Treebank
treatment of can?t regularizes so that (can need
not be differentiated from ca), whereas the parser
treatment makes maintaining character offsets eas-
ier. In example 3, the Japanese parser recognizes
a single word whereas the treebank divides it into
a prefix plus stem. Example 4 is a case of differ-
ences in character encoding (zero).
Example 5 is a common case of spurious attach-
ment ambiguity for English, where a transparent
noun takes an of PP complement?nouns such as
form, variety and thousands bear the feature trans-
parent in the NOMLEX-PLUS dictionary (a Nom-
Bank dictionary based on NOMLEX (Macleod et
al., 1998b)). The relative clause attaches either
to the noun thousands or people and, therefore,
the subject gap of the relative is filled by either
thousands or people. This ambiguity is spurious
since there is no meaningful distinction between
these two attachments. Example 6 is a case of
attachment ambiguity due to a support construc-
tion (Meyers et al, 2004). The recipient of the
gift will be Goodwill regardless of whether the
PP is attached to give or gift. Thus there is not
much sense in marking one attachment more cor-
rect than the other. Example 7 is a case of conjunc-
tion ambiguity?the context does not make it clear
whether or not the pearls are part of a necklace or
just the beads are. The distinction is of little con-
sequence to the understanding of the narrative.
Example 8 is a case in which our grammar han-
dles a case ambiguously: the prenominal adjective
can be analyzed either as a simple noun plus ad-
jective phrase meaning various businesses or as a
noun plus relative clause meaning businesses that
are varied. Example 9 is a common case in Chi-
nese where the verb/noun distinction, while un-
clear, is not crucial to the meaning of the phrase ?
under either interpretation, 5 billion was exported.
7 Concluding Remarks
We have discussed challenges of automatic an-
notation when transducers of other annotation
schemata are used as input. Models underly-
ing different transducers approximate the origi-
nal annotation in different ways, as do transduc-
ers trained on different corpora. We have found
it necessary to allow for multiple correct answers,
due to such differences, as well as, genuine and
spurious ambiguities. In the future, we intend to
investigate automatic ways of identifying and han-
dling spurious ambiguities which are predictable,
including examples like 5,6 and 7 in figure 3 in-
volving transparent functors.
119
References
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In ACL 2001, pages 116?123.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28:245?288.
R. Grishman, D. Westbrook, and A. Meyers. 2005.
Nyu?s english ace 2005 system description. In ACE
2005 Evaluation Workshop.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In CoNLL-2009, Boulder, Col-
orado, USA.
Z. Huang and M. Harper. 2009. Self-training PCFG
Grammars with Latent Annotations across Lan-
guages. In EMNLP 2009.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Syd-
ney, Australia.
S. Kurohashi and M. Nagao. 1998. Building a
Japanese parsed corpus while improving the pars-
ing system. In Proceedings of The 1st International
Conference on Language Resources & Evaluation,
pages 719?724.
C. Macleod, R. Grishman, and A. Meyers. 1998a.
COMLEX Syntax. Computers and the Humanities,
31:459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998b. Nomlex: A lexicon of nominal-
izations. In Proceedings of Euralex98.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Pro-
ceedings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, and Catherine Macleod. 2004.
NP-External Arguments: A Study of Argument
Sharing in English. In The ACL 2004 Workshop
on Multiword Expressions: Integrating Processing,
Barcelona, Spain.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao,
and W. Xu. 2009. Automatic Recognition of Log-
ical Relations for English, Chinese and Japanese in
the GLARF Framework. In SEW-2009 at NAACL-
HLT-2009.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Depen-
dencies. In Proceedings of the CoNLL-2008 Shared
Task, Manchester, GB.
M. Utiyama and H. Isahara. 2003. Reliable Mea-
sures for Aligning Japanese-English News Articles
and Sentences. In ACL-2003, pages 72?79.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase Structure Annota-
tion of a Large Corpus. Natural Language Engi-
neering.
N. Xue. 2008. Labeling Chinese Predicates with Se-
mantic roles. Computational Linguistics, 34:225?
255.
120
Coling 2010: Poster Volume, pages 1194?1202,
Beijing, August 2010
Semi-supervised Semantic Pattern Discovery with Guidance 
from Unsupervised Pattern Clusters 
Ang Sun 
Computer Science Department 
New York University 
asun@cs.nyu.edu 
Ralph Grishman 
Computer Science Department 
New York University 
grishman@cs.nyu.edu 
 
Abstract 
We present a simple algorithm for 
clustering semantic patterns based on 
distributional similarity and use cluster 
memberships to guide semi-supervised 
pattern discovery. We apply this 
approach to the task of relation 
extraction. The evaluation results 
demonstrate that our novel 
bootstrapping procedure significantly 
outperforms a standard bootstrapping. 
Most importantly, our algorithm can 
effectively prevent semantic drift and 
provide semi-supervised learning with a 
natural stopping criterion. 
1 Introduction 
The Natural Language Processing (NLP) 
community faces new tasks and new domains 
all the time. Without enough labeled data of a 
new task or a new domain to conduct supervised 
learning, semi-supervised learning (SSL) is 
particularly attractive to NLP researchers since 
it only requires a handful of labeled examples, 
known as seeds.  SSL starts with these seeds to 
train an initial model; it then applies this model 
to a large volume of unlabeled data to get more 
labeled examples and adds the most confident 
ones as new seeds to re-train the model. This 
iterative procedure has been successfully 
applied to a variety of NLP tasks, such as 
hypernym/hyponym extraction (Hearst, 1992), 
word sense disambiguation (Yarowsky, 1995), 
question answering (Ravichandran and Hovy, 
2002), and information extraction (Brin, 1998; 
Collins and Singer, 1999; Riloff and Jones, 
1999; Agichtein and Gravano, 2000; Yangarber 
et al, 2000; Chen and Ji, 2009).  
While SSL can give good performance for 
many tasks, it is a procedure born with two 
defects. One is semantic drift.  When SSL is 
under-constrained, the semantics of newly 
promoted examples might stray away from the 
original meaning of seed examples as discussed 
in (Brin, 1998; Curran et al, 2007; Carlson et 
al., 2010). For example, a SSL procedure to 
learn semantic patterns for the LocatedIn 
relation (PERSON in LOCATION/GPE1) might 
accept patterns for the Employment relation 
(employee of GPE / ORGANIZATION) 
because many unlabeled pairs of names are 
connected by patterns belonging to multiple 
relations. Patterns connecting <Bill Clinton, 
Arkansas> include LocatedIn patterns such as 
?visit?, ?arrive in? and ?fly to?, but also patterns 
indicating other relations such as ?governor of?, 
?born in?, and ?campaign in?. Similar analyses 
can be applied to many other examples such as 
<Bush, Texas> and <Schwarzenegger, 
California>. Without careful design, SSL 
procedures usually accept bogus examples 
during certain iterations and hence the learning 
quality degrades.  
The other shortcoming of SSL is its lack of 
natural stopping criteria. Most SSL algorithms 
either run a fixed number of iterations 
(Agichtein and Gravano, 2000) or run against a 
separate labeled test set to find the best stopping 
criterion (Abney, 2008). The former solution 
needs a human to keep eyeballing the learning 
quality of different iterations and set ad-hoc 
thresholds accordingly. The latter requires a 
                                                 
1 These are the types of relations and names used in 
the NIST-sponsored ACE evaluation. 
http://www.itl.nist.gov/iad/mig//tests/ace/. GPE 
represents a Geo-Political Entity ? an entity with 
land and a government. 
1194
separate labeled test set for each new task or 
domain. They make SSL less appealing than it 
could be since the intention of using SSL is to 
minimize supervision.  
In this paper, we propose a novel learning 
framework which can automatically monitor the 
semantic drift and find a natural stopping 
criterion for SSL. Central to our idea is that 
instead of using unlabeled data directly in SSL, 
we first cluster the seeds and unlabeled data in 
an unsupervised way before conducting SSL. 
The semantics of unsupervised clusters are 
usually unknown. However, the cluster to which 
the seeds belong can serve as the target cluster. 
Then we guide the SSL procedure using the 
target cluster. Under such learning settings, 
semantic drift can be automatically detected and 
a stopping criterion can be found:  stopping the 
SSL procedure when it tends to accept examples 
belonging to clusters other than the target 
cluster.  
We demonstrate in this paper the above 
general idea by considering a bootstrapping 
procedure to discover semantic patterns for 
extracting relations between named entities 
(NE). Standard bootstrapping usually starts with 
some high-precision and high frequency seed 
patterns for a specific relation to match named 
entities, then it uses newly promoted entities to 
search for additional confident patterns 
connecting them. It is a procedure driven by the 
duality between patterns and entities: a good 
pattern can connect more than one pair of 
named entities and a pair of named entities is 
usually connected by more than one good 
pattern.  
We present a new bootstrapping procedure in 
which we first cluster the seed and other 
patterns in a large corpus based on distributional 
similarity. We then guide the bootstrapping 
using the target cluster.  
The next section describes our unsupervised 
pattern clusters. Section 3 presents the details of 
our novel bootstrapping procedure with 
guidance from pattern clusters. We evaluate our 
algorithms in Section 4 and present related work 
in Section 5. We draw conclusions and point to 
future work in Section 6. 
2 Pattern Clusters 
2.1 Distributional Hypothesis 
The Distributional Hypothesis (Harris, 1954) 
states that words that tend to occur in similar 
contexts tend to have similar meanings. Lin and 
Pantel (2001) extended this hypothesis to cover 
patterns (dependency paths in their case). The 
idea of the extension is that if two patterns tend 
to occur in similar contexts then the meanings 
of the patterns tend to be similar. For example, 
in ?X solves Y? and ?X finds a solution to Y?, 
?solves? and ?finds a solution to? share many 
common Xs and Ys and hence are similar to 
each other. This extended distributional 
hypothesis serves as the basis on which we 
compute similarities for each pair of patterns. 
2.2 Pattern Representation ? Shortest 
Dependency Path 
We adopt a shortest dependency path (SDP) 
representation of relation patterns. SDP has 
demonstrated its power in kernel methods for 
relation extraction (Bunescu and Mooney, 2005). 
Its capability in capturing most of the 
information of interest is also evidenced by a 
systematic comparison of effectiveness of 
different information extraction (IE) patterns in 
(Stevenson and Greenwood, 2006) 2 . For 
example, ?nsubj ? met ? prep_in? is able to 
represent LocatedIn between ?Gates? and 
?Seattle? while a token-based pattern would be 
much less general because it would have to 
specify all the intervening tokens. 
 
Figure 1.  Stanford dependency tree for sentence 
?Gates, Microsoft?s chairman, met with President 
Clinton in Seattle?.  
 
                                                 
2 SDP is equivalent to the linked chains described in 
Stevenson and Greenwood (2006) when the 
dependency of a sentence is represented as a tree not 
a graph. 
1195
2.3 Pre-processing 
We tag and parse each sentence in our corpus 
with the NYU named entity tagger 3  and the 
Stanford dependency parser. Then for each pair 
of names in the dependency tree, we extract the 
SDP connecting them. Names in the path are 
replaced by their types. We require SDP to 
contain at least one verb or noun. We use the 
base form of words in SDP. We also require the 
length of the path (defined as the number of 
dependency relations and words in it) to be 
between 3 and 7. Short paths are more likely to 
be generic patterns such as ?of? and can be 
handled separately as in (Pantel and 
Pennacchiotti, 2006). Very long paths are more 
likely to be non-relation patterns and too sparse 
to be useful even if they are relation patterns. 
2.4 Clustering Algorithm 
The basic idea of our clustering algorithm is to 
group all the paths (including the seed paths 
used later for SSL) in our corpus into different 
clusters based on distributional similarities. We 
first extract a variety of features from the named 
entities X and Y connected by a path P as shown 
in Table 1. We then compute an analogue of tf-
idf for each feature f of P as follows: tf as the 
number of corpus instances of P having feature f 
divided by the number of instances of P; idf as 
the total number of paths in the corpus divided 
by the number of paths with at least one 
instance with feature f. Then we adopt a vector 
space model, i.e., we construct a tf-idf feature 
vector for each P.  Now we compute the 
similarity between two vectors/paths using 
Cosine similarity and cluster all the paths using 
Complete Linkage. 
Some technical details deserve more attention 
here.  
Feature extraction: We extract more types 
of features than the DIRT paraphrase discovery 
procedure used in (Lin and Pantel, 2001). Lin 
and Pantel (2001) considered X and Y separately 
while we also use the conjunction of X and Y. 
We also extract named entity types as features 
since we are interested in discovering relations 
among different types of names. Some names 
are ambiguous such as Jordan. We hope 
                                                 
3 Please refer to Grishman et al (2005) and 
http://cs.nyu.edu/grishman/jet/license.html 
coupling the type with the string of the name 
may alleviate the ambiguity. 
 
Table 1. Sample features for ?X visited Y? as in ?Jordan 
visited China? 
Feature Type Example 
Name Type of X LEFT_PERSON 
Name Type of Y RIGHT_GPE 
Combination of 
Types of X and Y 
PERSON_GPE 
Conjunction of String 
and Type of X 
LEFT_Jordan_PERSON 
Conjunction of String 
and Type of Y 
RIGHT_China_GPE 
Conjunction of 
Strings and Types of 
X and Y 
Jordan_PERSON_China_GPE 
 
Similarity measure and clustering method: 
There are many ways to compute the 
similarity/distance between two feature vectors, 
such as Cosine, Euclidean, Hamming, and 
Jaccard coefficient. There are also many 
standard clustering algorithms. A systematic 
comparison of the performance of different 
distance measures and clustering algorithms is 
beyond the scope of this paper.  
3 Semi-supervised Relation Pattern 
Discovery 
We first present a standard bootstrapping 
algorithm coupled with analyses of some of its 
shortcomings. Then we describe our new 
bootstrapping procedure which is guided by 
pattern clusters.   
3.1 Bootstrapping without Guidance  
The procedure associates a precision between 0 
and 1 with each pattern, and a confidence 
between 0 and 1 with each name pair. Initially 
the seed patterns for a specific relation R have 
precision 1 and all other patterns 0. It consists of 
the following steps: 
Step1: Use seed patterns to match new NE 
pairs and evaluate NE pairs. 
Intuitively, for a newly matched NE pair iN , 
if many of the k patterns connecting the two 
names are high-precision patterns then the name 
pair has a high confidence. The confidence is 
computed by the following formula. 
1
( ) 1 (1 Pr ( ))
k
i j
j
Conf N ec p
=
= ? ??  (1) 
1196
Problem: While the intuition is correct, in 
practice this will over-rank NE pairs which are 
not only matched by patterns belonging to the 
target relation R but are also connected by 
patterns of many other relations. This is because 
of the initial settings used in many SSL systems: 
seeds are assigned high confidence. Thus all NE 
pairs matched by initial seed patterns will have 
very high confidence.  
Suppose the target relation is LocatedIn, and 
?visited? is a seed pattern; then the <Clinton, 
Arkansas> example will be over-rated because 
we cannot take into account that it would also 
match patterns of other relations such as 
PersonGovernorOfLocation and 
PersonBornInLocation in a real corpus. This 
will cause a vicious circle, i.e., bogus NE pairs 
extract more bogus patterns which further 
extract more bogus NE pairs. We believe this 
flaw of the initial settings partially results in the 
semantic drift problem.  
One can imagine that this is not a problem 
that can be solved by using a different formula 
to replace the one presented here. A possible 
solution is to study the structure of unlabeled 
data (NE pairs in our case) and integrate this 
structure information into the initial settings. 
Indeed, this is where pattern clusters come into 
play. We will demonstrate this in Section 3.2. 
Step 2: Use NE pairs to search for new 
patterns and rank patterns. 
Similar to the intuition in Step 1, for a pattern 
p, if many of the NE pairs it matches are very 
confident then p has many supporters and 
should have a high ranking. We can use formula 
(2) to estimate the confidence of patterns and 
rank them. 
( )
( ) log ( )
| |
Sup p
Conf p Sup p
H
= ?   (2) 
Here |H| is the number of unique NE pairs 
matched by p and Sup(p) is the sum of the 
support it can get from the |H| pairs: 
| |
1
( ) ( )
H
j
j
Sup p Conf N
=
= ?   (3) 
The precision of p is given by the average 
confidence of the NE pairs matched by p. 
( )
Pr ( )
| |
Sup p
ec p
H
=     (4) 
Formula (4) normalizes the precision to range 
from 0 to 1. As a result the confidence of each 
NE pair is also normalized to between 0 and 1. 
Step 3: Accept patterns 
Most systems accept the K top ranked 
patterns in Step 2 as new seeds, subject to some 
restrictions such as requiring the differences of 
confidence of the K patterns to be within a small 
range. 
 Step 4: Loop or stop 
The procedure now decides whether to repeat 
from Step 1 or to terminate. 
Most systems simply do not know when to 
stop. They either run a fixed number of 
iterations or use some held-out data to find one 
criterion that works the best for the held-out 
data. 
3.2 Bootstrapping Guided by Clusters 
Recall that our clustering algorithm in Section 2 
provides us with K clusters, each of which 
contains n (n differs in different clusters) 
patterns. Every pattern in our corpus now has a 
cluster membership (the seed patterns have the 
same membership).  
The most important benefit from our pattern 
clusters is that now we can measure how 
strongly a NE pair iN  is associated with our 
target cluster tC  (the one to which the seed 
patterns belong).  
( , )
Pr ( ) t
i
p C
i t
freq N p
ob N C
m
?? =
?
      (5) 
Here ( , )ifreq N p  is the number of times p 
matches iN  and m is the total number of pattern 
instances matching iN . 
We integrate this prior cluster distribution of 
each NE pair into the initial settings of our new 
bootstrapping procedure.  
Step1: Use seed patterns to match new NE 
pairs and evaluate NE pairs. 
 Assumption: A good NE pair must be 
strongly associated with the target cluster and 
can be matched by multiple high-precision 
patterns.  
So we evaluate a NE pair by the harmonic 
mean of two confidence scores, namely the 
confidence as its association with the target 
cluster and the confidence given by the patterns 
matching it. 
1197
_ ( ) _ ( )
( ) 2
_ ( ) _ ( )
i i
i
i i
Semi Conf N Cluster Conf N
Conf N
Semi Conf N Cluster Conf N
?= ? +
     (6) 
1
_ ( ) 1 (1 ( ))
k
i j
j
Semi Conf N Prec p
=
= ? ??  (7) 
_ ( ) Pr ( )i i tCluster Conf N ob N C= ?      (8) 
Under such settings, <Clinton, Arkansas> 
will be assigned a lower confidence score for 
the LocatedIn relation than it is in the standard 
bootstrapping. Even if we assign high precision 
to our seed patterns such as ?visited? and 
consequently the Semi_Conf is very high, it can 
still be discounted by the Cluster_Conf4.   
Step 2: Use NE pairs to search for new 
patterns and rank patterns. 
All the measurement functions are the same 
as those used in the standard bootstrapping. 
However, with better ranking of NE pairs in 
Step 1, the patterns are also ranked better than 
they are in the standard bootstrapping. 
Step 3: Accept patterns 
We also accept the K top ranked patterns.  
Step 4: Loop or stop 
Since each pattern in our corpus has a cluster 
membership, we can monitor the semantic drift 
easily and naturally stop: it drifts when the 
procedure tries to accept patterns which do not 
belong to the target cluster; we can stop when 
the procedure tends to accept more patterns 
outside of the target cluster. 
If our clustering algorithm can give us perfect 
pattern clusters, we can stop bootstrapping 
immediately after it accepts the first pattern not 
belonging to the target cluster. Then the 
bootstrapping becomes redundant since all it 
does is to consume the patterns of the target 
cluster.  
Facing the reality of the behavior of many 
clustering algorithms, we allow the procedure to 
occasionally accept patterns outside of the target 
cluster but we are not tolerant when it tries to 
accept more patterns outside of the target cluster 
than patterns in it. Note that when such patterns 
are accepted they will be moved to the target 
cluster and invoke the recomputation of 
Cluster_Conf of NE pairs connected by these 
patterns. The ranking functions in step 1 and 2 
                                                 
4 The Cluster_Conf of <Clinton, Arkansas> related 
to the LocatedIn relation is indeed very low (less 
than 0.1) in our experiments. 
insure that the procedure will only accept 
patterns which can gain strong support from NE 
pairs that are strongly associated with the target 
cluster and are connected by many confident 
patterns.  
4 Experiments 
4.1 Corpus 
Our corpora contain 37 years of news articles: 
TDT5, NYT(94-00), APW(98-00), 
XINHUA(96-00), WSJ(94-96), LATWP(94-97), 
REUFF(94-96), REUTE(94-96), and 
WSJSF(87-94). It contains roughly 65 million 
sentences and 1.3 billion tokens.  
4.2 Seeds 
Seeds of the 3 relations we are going to test are 
given in table 2. LocatedIn detects relation 
between PERSON and LOCATION/GPE; 
Social (SOC) detects social relations (either 
business or family) between PERSON and 
PERSON; Employment (EMP) detects 
employment relations between PERSON and 
ORGANIZATION.  
 
Table 2.  Seed Patterns 
Relation Seeds 
Located-
in 
nsubj' visit dobj 
nsubj' travel prep_to 
poss' trip prep_to 
SOC appos friend/lawyer poss 
appos son/spokesman prep_of/prep_for 
nsubj' fire dobj 
nsubjpass' fire agent 
 EMP5 appos chairman/executive/founder prep_of 
appos editor prep_of  
appos director/head/officer/analyst prep_at 
appos manager prep_with 
 
(nsubj, dobj, prep, appos, poss, nsubjpass, agent 
stand for subject, direct object, preposition, 
apposition, possessive, passive nominal subject 
and complement of passive verb. The quote 
marks in Table 2 and Table 3 denote inverse 
dependencies in the dependency path.) 
We work on these three relations mainly 
because of the availability of benchmark 
evaluation data. These are the most frequent 
relations in our evaluation data.  
                                                 
5 We provide more seeds (executives and staff) for 
EMP because it has been pointed out in (Sun, 2009) 
that EMP contains a lot of job titles.  
1198
4.3 Unsupervised Experiments 
We run the clustering algorithm described in 
Section 2 using all the 37 years? data. We 
require that a pattern match at least 7 distinct 
NE pairs and that an NE pair must be connected 
by at least 7 unique patterns. As a result, there 
are 635,128 patterns (22,225 unique ones) used 
in experiments. We use 0.005 as the cutoff 
threshold of complete linkage. The threshold is 
decided by trying a series of thresholds and 
searching for the maximal6 one that is capable 
of placing the seed patterns for each relation 
into a single cluster. Table 3 shows the top 15 
patterns (ranked by their corpus frequency) of 
the cluster into which our LocatedIn seeds fall.  
 
Table 3.  Top 15 patterns in the LocatedIn Cluster 
Index Pattern Frequency 
1 nsubj' said prep_in 2203 
2 nsubj' visit dobj 1831 
3 poss' visit prep_to 1522 
4 nsubj' return prep_to 1394 
5 nsubj' tell prep_in 1363 
6 nsubj' be prep_in 1283 
7 nsubj' arrive prep_in 1113 
8 nsubj' leave dobj 1106 
9 nsubj' go prep_to 926 
10 nsubj' fly prep_to 700 
11 nsubj' come prep_to 658 
12 appos leader poss 454 
13 poss' trip prep_to 442 
14 rcmod be prep_in 419 
15 nsubj' make prep_in 418 
4.4 Semi-supervised Experiments 
To provide strong statistical evidence, we divide 
our data into 10 folds (combinations of news 
articles from different years and different news 
resources). We then run both the standard and 
our new bootstrapping on the 10 folds. For both 
procedures, we accept n patterns in a single 
iteration (n is initialized to 2 and set to n + 1 
after each iteration). We run 50 iterations in the 
standard bootstrapping and 1,325 patterns are 
accepted for each fold and each relation. Our 
new bootstrapping procedure stops when there 
are two consecutive iterations in which more 
than half of the newly accepted patterns do not 
belong to the target cluster. Thus the number of 
                                                 
6  We choose the maximal value because many 
clusters will be merged to a single one when the 
threshold is close to 0, making the clusters too 
general to be useful. 
patterns accepted for each fold and each relation 
differs as the last iteration differs. 
4.5 Evaluation 
The output of our bootstrapping procedures is 
60 sets of patterns (3 relations ?  2 methods ?  
10 folds). We need a data set and evaluation 
method which can compare their effectiveness 
equally and consistently.  
Evaluation data: ACE 2004 training data. 
ACE does not provide relation annotation 
between each pair of names. For example, in 
?US President Clinton said that the United 
States ?? ACE annotates an EMP relation 
between the name ?US? and nominal 
?President?. There is no annotation between 
?US? and ?Clinton?. However, it provides entity 
co-reference information which connects 
?President? to ?Clinton?. So we take advantage 
of this entity co-reference information to 
automatically re-annotate the relations where 
possible to link a pair of names within a single 
sentence. The re-annotation yields an EMP 
relation between ?US? and ?Clinton?. The re-
annotation is reviewed by hand to avoid adding 
a relation linking ?Clinton? and the more distant 
co-referent ?United States?, even though ?US? 
and ?the United States? refer to the same entity. 
This data set provides us with 412/3492 
positive/negative relation instances between 
names. Among the 412 positive instances, there 
are 188/117/35 instances for 
EMP/LocatedIn/SOC relations.  
Evaluation method: We adopt a direct 
evaluation method, i.e., use our sets of patterns 
to extract relations between names on ACE data. 
Applying patterns to a benchmark data set can 
provide us with better precision/recall analyses. 
We use a strict pattern match strategy. We can 
certainly take advantage of loose match or add 
patterns as additional features to feature-based 
relation extraction systems to boost our 
performance but we do not want these to 
complicate the comparison of the standard and 
our new bootstrapping procedures.  
4.6 Results and Analyses  
We average our results on the 10 folds. We plot 
precision against recall and semantic drift rate 
against iterations (Drift). We compute the 
semantic drift rate as the percentage of false 
1199
Figure 2.  Performance for EMP/LocatedIn/SOC 
EMP  Precision vs. Recall
Recall
0.0 .1 .2 .3 .4 .5 .6
P
re
ci
si
on
.5
.6
.7
.8
.9
1.0
LocatedIn  Precision vs. Recall
Recall
0.00 .05 .10 .15 .20 .25 .30 .35
P
re
ci
si
on
.3
.4
.5
.6
.7
.8
.9
1.0
1.1
 
SOC  Precision vs. Recall
Recall
.05 .10 .15 .20 .25 .30 .35 .40 .45
P
re
ci
si
on
0.0
.2
.4
.6
.8
1.0
1.2
 
positive instances belonging to ACE relations 
other than the target relation. Take EMP for 
example, we compute how many of the false 
positive instances belonging to other relations 
such as LocatedIn, SOC and other ACE 
relations. In all plots, red solid lines represent 
bootstrapping with guidance from clusters and  
blue dotted lines standard bootstrapping. 
  There are a number of conclusions that can be  
Figure 3.  Drift for EMP/LocatedIn/SOC 
EMP  Drift
Iteration
0 10 20 30 40 50 60
D
rif
t
0.00
.05
.10
.15
.20
.25
.30
 
LocatedIn  Drift
Iteration
0 10 20 30 40 50 60
D
rif
t
0.00
.05
.10
.15
.20
.25
.30
.35
SOC  Drift
Iteration
0 10 20 30 40 50 60
D
rif
t
0.00
.01
.02
.03
.04
.05
.06
.07
   
 
drawn from these results. We are particularly 
interested in the following two questions: To 
what extent did we prevent semantic drift by the 
guidance of pattern clusters? Did we stop at the 
right point, i.e., can we keep high precision 
while maintaining near maximal recall? 
1) It is obvious from the drift curves that our 
bootstrapping effectively prevents semantic drift. 
Indeed, there is no drift at all when LocatedIn 
1200
and SOC learners terminate. Although drift 
indeed occurs in the EMP relation, its curve is 
much lower than that of the standard 
bootstrapping.  
2) Our new procedure terminates when the 
precision is still high while maintaining a 
reasonable recall. Our bootstrapping for 
EMP/SOC/LocatedIn terminates at F-measures 
of 60/37/28 (in percentage). We conducted the 
Wilcoxon Matched-Pairs Signed-Ranks Test on 
the 10 folds, comparing the F-measures of the 
last iteration of our bootstrapping guided by 
clusters and the iteration which provides the 
best average F-measure over the 3 relations of 
the standard bootstrapping. The results show 
that the improvement of using clusters to guide 
bootstrapping is significant at a 97% confidence 
level. 
We hypothesize that when working on 
dozens or hundreds of relations the gain of our 
procedure will be even bigger since we can 
effectively prevent inter-class errors.  
5 Related Work 
Recent research starts exploring unlabeled data 
for discriminative learning. Miller et al, (2004) 
augmented name tagging training data with 
hierarchical word clusters and encoded cluster 
membership in features for improving name 
tagging. Lin and Wu (2009) further explored a 
two-stage cluster-based approach: first 
clustering phrases and then relying on a 
supervised learner to identify useful clusters and 
assign proper weights to cluster features. Other 
similar work includes (Wong and Ng, 2007) for 
name tagging, and (Koo et. al., 2008) for 
dependency parsing.  
While similar in spirit, our supervision is 
minimal, i.e., we only use a few seeds while the 
above approaches rely on a large amount of 
labeled data. To the best of our knowledge, the 
theme explored in this paper is the first study of 
using pattern clusters for preventing semantic 
drift in semi-supervised pattern discovery.  
Recent research also explored the idea of 
driving SSL with explicit constraints 
constructed by hand such as identifying mutual 
exclusion of different categories (i.e., people 
and sport are mutually exclusive). This is 
termed constraint-driven learning in (Chang et 
al., 2007), coupled learning in (Carlson et al, 
2010) and counter-training in (Yangarber, 2003). 
The learning quality largely depends on the 
completeness of explicit constraints. While we 
share the same goal, i.e., to prevent semantic 
drift, we rely on unsupervised clusters to 
discover implicit constraints for us instead of 
generating constraints by hand. 
Our research is also close to semi-supervised 
IE pattern learners including (Riloff and Jones, 
1999), (Agichtein and Gravano, 2000), 
(Yangarber et al, 2000), and many others. 
While they conduct bootstrapping on unlabeled 
data directly, we first cluster unlabeled data and 
then bootstrap with help from clusters. 
There are also clear connections to work on 
unsupervised relation discovery (Hasegawa et 
al., 2004; Zhang et al, 2005; Rosenfeld and 
Feldman, 2007). They group pairs of names into 
relation clusters based on the contexts between 
names while we group the contexts/patterns into 
clusters based on features extracted from names. 
6 Conclusions and Future Work 
We presented a simple algorithm for clustering 
patterns and used pattern clusters to guide semi-
supervised semantic pattern discovery. The 
novel bootstrapping procedure can achieve the 
best F-1 score while maintaining a good trade-
off between precision and recall. We also 
demonstrated that it can effectively prevent 
semantic drift and naturally terminate.  
We plan to extend this idea to improve 
relation extraction performance with a richer 
model as used in (Zhang et al, 2004; Zhou et al, 
2008) than a simple pattern learner. The feature 
space will be much larger than the one adopted 
in this paper. We will investigate how to 
overcome the memory bottleneck when we 
apply rich models to millions of instances.  
7 Acknowledgements 
We would like to thank Prof. Satoshi Sekine for 
his useful suggestions. 
References 
Steven Abney. 2008. Semisupervised Learning for 
Computational Linguistics, Chapman and Hall. 
Eugene Agichtein and Luis Gravano. 2000. Snowball: 
Extracting relations from large plain-text 
1201
collections. In Proc. of the Fifth ACM 
International Conference on Digital Libraries. 
Sergey Brin. Extracting patterns and relations from 
the World-Wide Web. 1998. In Proc. of the 1998 
Intl. Workshop on the Web and Databases. 
Razvan C. Bunescu and Raymond J. Mooney. 2005. 
A Shortest Path Dependency Kernel for Relation 
Extraction. In Proc. of HLT/EMNLP. 
Andrew Carlson, Justin Betteridge, Richard C. Wang, 
Estevam Rafael Hruschka Junior and Tom M. 
Mitchell. 2010. Coupled Semi-Supervised 
Learning for Information Extraction. In WSDM. 
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007. 
Guiding semisupervision with constraint-driven 
learning. In Proc. of ACL-2007, Prague.  
Zheng Chen and Heng Ji. 2009. Can One Language 
Bootstrap the Other: A Case Study on Event 
Extraction. In NAACL HLT Workshop on Semi-
supervised Learning for NLP. 
Michael Collins and Yoram Singer. 1999. 
Unsupervised models for named entity 
classication. In Proc. of EMNLP-99. 
James R. Curran, Tara Murphy, and Bernhard Scholz. 
2007. Minimising semantic drift with Mutual 
Exclusion Bootstrapping. In Proc. of PACLING. 
Ralph Grishman, David Westbrook and Adam 
Meyers. 2005. NYU?s English ACE 2005 System 
Description. ACE 2005 Evaluation Workshop.  
Zellig S. Harris. 1954. Distributional Structure. Word. 
Vol 10,1954, 146-162. 
Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman. 
2004. Discovering Relations among Named 
Entities from Large Corpora. In Proc. of ACL-04.  
Marti Hearst. 1992. Automatic acquisition of 
hyponyms from large text corpora. In Proc. of the 
14th Intl. Conf. on Computational Linguistics. 
Terry Koo, Xavier Carreras, and Michael Collins. 
2008. Simple Semi-supervised Dependency 
Parsing. In Proceedings of ACL-08: HLT. 
Dekang Lin and Patrick Pantel. 2001. Discovery of 
inference rules for question-answering. Natural 
Language Engineering, 7(4):343?360. 
Dekang Lin and Xiaoyun Wu. 2009. Phrase 
Clustering for Discriminative Learning. In 
Proceedings of the ACL and IJCNLP 2009. 
Marie-Catherine de Marneffe and Christopher D. 
Manning. 2008. The Stanford typed dependencies 
representation. In COLING Workshop on Cross-
framework and Cross-domain Parser Evaluation. 
Scott Miller, Jethran Guinness and Alex Zamanian. 
2004. Name Tagging with Word Clusters and 
Discriminative Training. In Proc. of HLT-NAACL. 
Patrick Pantel and Marco Pennacchiotti. 2006. 
Espresso: Leveraging Generic Patterns for 
Automatically Harvesting Semantic Relations. In 
Proc. of COLING-06 and ACL-06. 
Deepak Ravichandran and Eduard Hovy. 2002. 
Learning Surface Text Patterns for a Question 
Answering System. In Proc. of ACL-2002. 
Ellen Riloff and Rosie Jones. 1999. Learning 
dictionaries for information extraction by multi-
level bootstrapping. In Proc. of AAAI-99. 
Benjamin Rosenfeld, Ronen Feldman. 2007. 
Clustering for Unsupervised Relation 
Identification. In Proc. of CIKM ?07. 
Mark Stevenson and Mark A. Greenwood. 2006. 
Comparing Information Extraction Pattern 
Models. In Proceedings of the Workshop on 
Information Extraction Beyond The Document. 
Mark Stevenson and Mark A. Greenwood. 2005. A 
Semantic Approach to IE Pattern Induction. In 
Proc. of the 43rd Annual Meeting of the ACL. 
Ang Sun. 2009. A Two-stage Bootstrapping 
Algorithm for Relation Extraction. In RANLP-09. 
Yingchuan Wong and Hwee Tou Ng. 2007. One 
Class per Named Entity: Exploiting Unlabeled 
Text for Named Entity Recognition. In Proc. of 
IJCAI-07. 
Roman Yangarber. 2003. Counter-training in the 
discovery of semantic patterns. In Proc. of ACL.  
Roman Yangarber, Ralph Grishman, Pasi 
Tapanainen and Silja Huttunen. 2000. Automatic 
acquisition of domain knowledge for information 
extraction. In Proc. of COLING-2000. 
David Yarowsky. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proc. of ACL-95. 
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, 
and Chew Lim Tan. 2005. Discovering Relations 
Between Named Entities from a Large Raw 
Corpus Using Tree Similarity-Based Clustering. 
In IJCNLP 2005, LNAI 3651, pp. 378 ? 389. 
Zhu Zhang. (2004). Weakly supervised relation 
classification for information extraction. In Proc. 
of CIKM?2004.  
GuoDong Zhou, JunHui Li, LongHua Qian and 
QiaoMing Zhu. 2008.  Semi-supervised learning 
for relation extraction.  IJCNLP?2008:32-39. 
1202
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 521?529,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Semi-supervised Relation Extraction with Large-scale Word Clustering 
 
 
Ang Sun             Ralph Grishman             Satoshi Sekine 
  
Computer Science Department 
New York University 
{asun,grishman,sekine}@cs.nyu.edu 
 
 
 
 
 
 
Abstract 
We present a simple semi-supervised 
relation extraction system with large-scale 
word clustering. We focus on 
systematically exploring the effectiveness 
of different cluster-based features. We also 
propose several statistical methods for 
selecting clusters at an appropriate level of 
granularity. When training on different 
sizes of data, our semi-supervised approach 
consistently outperformed a state-of-the-art 
supervised baseline system. 
1 Introduction 
Relation extraction is an important information 
extraction task in natural language processing 
(NLP), with many practical applications. The goal 
of relation extraction is to detect and characterize 
semantic relations between pairs of entities in text. 
For example, a relation extraction system needs to 
be able to extract an Employment relation between 
the entities US soldier and US in the phrase US 
soldier.  
Current supervised approaches for tackling this 
problem, in general, fall into two categories: 
feature based and kernel based. Given an entity 
pair and a sentence containing the pair, both 
approaches usually start with multiple level 
analyses of the sentence such as tokenization, 
partial or full syntactic parsing, and dependency 
parsing. Then the feature based method explicitly 
extracts a variety of lexical, syntactic and semantic 
features for statistical learning, either generative or 
discriminative (Miller et al, 2000; Kambhatla, 
2004; Boschee et al, 2005; Grishman et al, 2005; 
Zhou et al, 2005; Jiang and Zhai, 2007). In 
contrast, the kernel based method does not 
explicitly extract features; it designs kernel 
functions over the structured sentence 
representations (sequence, dependency or parse 
tree) to capture the similarities between different 
relation instances (Zelenko et al, 2003; Bunescu 
and Mooney, 2005a; Bunescu and Mooney, 2005b; 
Zhao and Grishman, 2005; Zhang et al, 2006; 
Zhou et al, 2007; Qian et al, 2008). Both lines of 
work depend on effective features, either explicitly 
or implicitly.  
The performance of a supervised relation 
extraction system is usually degraded by the 
sparsity of lexical features. For example, unless the 
example US soldier has previously been seen in the 
training data, it would be difficult for both the 
feature based and the kernel based systems to 
detect whether there is an Employment relation or 
not. Because the syntactic feature of the phrase US 
soldier is simply a noun-noun compound which is 
quite general, the words in it are crucial for 
extracting the relation. 
This motivates our work to use word clusters as 
additional features for relation extraction. The 
assumption is that even if the word soldier may 
never have been seen in the annotated Employment 
relation instances, other words which share the 
same cluster membership with soldier such as 
president and ambassador may have been 
observed in the Employment instances. The 
absence of lexical features can be compensated by 
521
the cluster features. Moreover, word clusters may 
implicitly correspond to different relation classes. 
For example, the cluster of president may be 
related to the Employment relation as in US 
president while the cluster of businessman may be 
related to the Affiliation relation as in US 
businessman.   
The main contributions of this paper are: we 
explore the cluster-based features in a systematic 
way and propose several statistical methods for 
selecting effective clusters.  We study the impact 
of the size of training data on cluster features and 
analyze the performance improvements through an 
extensive experimental study. 
The rest of this paper is organized as follows: 
Section 2 presents related work and Section 3 
provides the background of the relation extraction 
task and the word clustering algorithm. Section 4 
describes in detail a state-of-the-art supervised 
baseline system. Section 5 describes the cluster-
based features and the cluster selection methods. 
We present experimental results in Section 6 and 
conclude in Section 7.  
2 Related Work 
The idea of using word clusters as features in 
discriminative learning was pioneered by Miller et 
al. (2004), who augmented name tagging training 
data with hierarchical word clusters generated by 
the Brown clustering algorithm (Brown et al, 1992) 
from a large unlabeled corpus. They used different 
thresholds to cut the word hierarchy to obtain 
clusters of various granularities for feature 
decoding. Ratinov and Roth (2009) and Turian et 
al. (2010) also explored this approach for name 
tagging. Though all of them used the same 
hierarchical word clustering algorithm for the task 
of name tagging and reported improvements, we 
noticed that the clusters used by Miller et al (2004) 
were quite different from that of Ratinov and Roth 
(2009) and Turian et al (2010). To our knowledge, 
there has not been work on selecting clusters in a 
principled way. We move a step further to explore 
several methods in choosing effective clusters. A 
second difference between this work and the above 
ones is that we utilize word clusters in the task of 
relation extraction which is very different from 
sequence labeling tasks such as name tagging and 
chunking. 
Though Boschee et al (2005) and Chan and 
Roth (2010) used word clusters in relation 
extraction, they shared the same limitation as the 
above approaches in choosing clusters. For 
example, Boschee et al (2005) chose clusters of 
different granularities and Chan and Roth (2010) 
simply used a single threshold for cutting the word 
hierarchy.  Moreover, Boschee et al (2005) only 
augmented the predicate (typically a verb or a 
noun of the most importance in a relation in their 
definition) with word clusters while Chan and Roth 
(2010) performed this for any lexical feature 
consisting of a single word. In this paper, we 
systematically explore the effectiveness of adding 
word clusters to different lexical features.  
3 Background  
3.1 Relation Extraction 
One of the well defined relation extraction tasks is 
the Automatic Content Extraction1 (ACE) program 
sponsored by the U.S. government. ACE 2004 
defined 7 major entity types: PER (Person), ORG 
(Organization), FAC (Facility), GPE (Geo-Political 
Entity: countries, cities, etc.), LOC (Location), 
WEA (Weapon) and VEH (Vehicle). An entity has 
three types of mention: NAM (proper name), NOM 
(nominal) or PRO (pronoun). A relation was 
defined over a pair of entity mentions within a 
single sentence. The 7 major relation types with 
examples are shown in Table 1. ACE 2004 also 
defined 23 relation subtypes. Following most of 
the previous work, this paper only focuses on 
relation extraction of major types. 
Given a relation instance ( , , )i jx s m m?
, where 
im  and jm
 are a pair of mentions and s  is the 
sentence containing the pair, the goal is to learn a 
function which maps the instance x to a type c, 
where c is one of the 7 defined relation types or the 
type Nil (no relation exists). There are two 
commonly used learning paradigms for relation 
extraction: 
Flat: This strategy performs relation detection 
and classification at the same time. One multi-class 
classifier is trained to discriminate among the 7 
relation types plus the Nil type. 
Hierarchical: This one separates relation 
detection from relation classification. One binary 
                                                        
1 Task definition: http://www.itl.nist.gov/iad/894.01/tests/ace/ 
ACE guidelines: http://projects.ldc.upenn.edu/ace/ 
522
classifier is trained first to distinguish between 
relation instances and non-relation instances. This 
can be done by grouping all the instances of the 7 
relation types into a positive class and the instances 
of Nil into a negative class. Then the thresholded 
output of this binary classifier is used as training 
data for learning a multi-class classifier for the 7 
relation types (Bunescu and Mooney, 2005b). 
 
Type Example 
EMP-ORG US president 
PHYS a military base in Germany 
GPE-AFF U.S. businessman 
PER-SOC a spokesman for the senator 
DISC each of whom 
ART US helicopters 
OTHER-AFF Cuban-American people 
 
Table 1:  ACE relation types and examples from the 
annotation guideline 2 . The heads of the two entity 
mentions are marked. Types are listed in decreasing 
order of frequency of occurrence in the ACE corpus. 
3.2 Brown Word Clustering 
The Brown algorithm is a hierarchical clustering 
algorithm which initially assigns each word to its 
own cluster and then repeatedly merges the two 
clusters which cause the least loss in average 
mutual information between adjacent clusters 
based on bigram statistics.  By tracing the pairwise 
merging steps, one can obtain a word hierarchy 
which can be represented as a binary tree. A word 
can be compactly represented as a bit string by 
following the path from the root to itself in the tree, 
assigning a 0 for each left branch, and a 1 for each 
right branch. A cluster is just a branch of that tree. 
A high branch may correspond to more general 
concepts while the lower branches it includes 
might correspond to more specific ones.  
Brown et al (1992) described an efficient 
implementation based on a greedy algorithm which 
initially assigned only the most frequent words into 
distinct clusters. It is worth pointing out that in this 
implementation each word occupies a leaf in the 
hierarchy, but each leaf might contain more than 
one word as can be seen from Table 2. The lengths 
of the bit strings also vary among different words. 
 
 
                                                        
2 http://projects.ldc.upenn.edu/ace/docs/EnglishRDCV4-3-
2.PDF 
Bit string Examples 
111011011100 US ? 
1110110111011 U.S. ? 
1110110110000 American ? 
1110110111110110 Cuban, Pakistani, Russian ?  
11111110010111 Germany, Poland, Greece ?  
110111110100 businessman, journalist, reporter 
1101111101111 president, governor, premier?  
1101111101100    senator, soldier, ambassador ? 
11011101110 spokesman, spokeswoman, ? 
11001100 people, persons, miners, Haitians 
110110111011111 base, compound, camps, camp ? 
110010111 helicopters, tanks, Marines ? 
 
Table 2: An example of words and their bit string 
representations obtained in this paper. Words in bold are 
head words that appeared in Table 1. 
4 Feature Based Relation Extraction 
Given a pair of entity mentions ,i jm m? ?
and the 
sentence containing the pair, a feature based 
system extracts a feature vector v  which contains 
diverse lexical, syntactic and semantic features. 
The goal is to learn a function which can estimate 
the conditional probability ( | )p c v , the probability 
of a relation type c given the feature vector v . The 
type with the highest probability will be output as 
the class label for the mention pair.  
We now describe a supervised baseline system 
with a very large set of features and its learning 
strategy.  
4.1 Baseline Feature Set 
We first adopted the full feature set from Zhou et 
al. (2005), a state-of-the-art feature based relation 
extraction system. For space reasons, we only 
show the lexical features as in Table 3 and refer the 
reader to the paper for the rest of the features.  
At the lexical level, a relation instance can be 
seen as a sequence of tokens which form a five 
tuple <Before, M1, Between, M2, After>. Tokens 
of the five members and the interaction between 
the heads of the two mentions can be extracted as 
features as shown in Table 3. 
In addition, we cherry-picked the following 
features which were not included in Zhou et al 
(2005) but were shown to be quite effective for 
relation extraction. 
Bigram of the words between the two mentions: 
This was extracted by both Zhao and Grishman 
(2005) and Jiang and Zhai (2007), aiming to 
523
provide more order information of the tokens 
between the two mentions. 
Patterns:  There are three types of patterns: 1) 
the sequence of the tokens between the two 
mentions as used in Boschee et al (2005); 2) the 
sequence of the heads of the constituents between 
the two mentions as used by Grishman et al (2005); 
3) the shortest dependency path between the two 
mentions in a dependency tree as adopted by 
Bunescu and Mooney (2005a). These patterns can 
provide more structured information of how the 
two mentions are connected.  
Title list: This is tailored for the EMP-ORG type 
of relations as the head of one of the mentions is 
usually a title. The features are decoded in a way 
similar to that of Sun (2009).  
 
Position Feature Description 
Before BM1F first word before M1 
BM1L second word before M1 
M1 WM1 bag-of-words in M1 
HM1 head3 word of M1 
Between WBNULL when no word in between 
WBFL the only word in between when 
only one word in between 
WBF first word in between when at 
least two words in between 
WBL last word in between when at 
least two words in between 
WBO other words in between except 
first and last words when at 
least three words in between 
M2 WM2 bag-of-words in M2 
HM2 head word of M2 
M12 HM12 combination of HM1 and HM2 
After AM2F  first word after M2 
AM2L  second word after M2 
 
Table 3: Lexical features for relation extraction. 
4.2 Baseline Learning Strategy 
We employ a simple learning framework that is 
similar to the hierarchical learning strategy as 
described in Section 3.1. Specifically, we first train 
a binary classifier to distinguish between relation 
instances and non-relation instances. Then rather 
than using the thresholded output of this binary 
classifier as training data, we use only the 
annotated relation instances to train a multi-class 
classifier for the 7 relation types. In the test phase, 
                                                        
3 The head word of a mention is normally set as the last word 
of the mention as in Zhou et al (2005). 
given a test instance x , we first apply the binary 
classifier to it for relation detection; if it is detected 
as a relation instance we then apply the multi-class 
relation classifier to classify it4. 
5 Cluster Feature Selection 
The selection of cluster features aims to answer the 
following two questions: which lexical features 
should be augmented with word clusters to 
improve generalization accuracy? How to select 
clusters at an appropriate level of granularity? We 
will describe our solutions in Section 5.1 and 5.2. 
5.1 Cluster Feature Decoding 
While each one of the lexical features in Table 3 
used by the baseline can potentially be augmented 
with word clusters, we believe the effectiveness of 
a lexical feature with augmentation of word 
clusters should be tested either individually or 
incrementally according to a rank of its importance 
as shown in Table 4. We will show the 
effectiveness of each cluster feature in the 
experiment section. 
 
Impor- 
tance 
Lexical 
Feature 
Description of 
lexical feature 
Cluster Feature 
1 HM HM1, HM2 and 
HM12 
HM1_WC, 
HM2_WC, 
HM12_WC 
2 BagWM WM1 and WM2 BagWM_WC 
3 HC a head5 of a chunk 
in context 
HC_WC 
4 BagWC word of context BagWC_WC 
 
Table 4: Cluster features ordered by importance. 
 
The importance is based on linguistic intuitions 
and observations of the contributions of different 
lexical features from various feature based systems. 
Table 4 simplifies a relation instance as a three 
tuple <Context, M1, M2> where the Context 
includes the Before, Between and After from the 
                                                        
4 Both the binary and multi-class classifiers output normalized 
probabilities in the range [0,1]. When the binary classifier?s 
prediction probability is greater than 0.5, we take the 
prediction with the highest probability of the multi-class 
classifier as the final class label. When it is in the range 
[0.3,0.5], we only consider as the final class label the 
prediction of the multi-class classifier with a probability which 
is greater than 0.9. All other cases are taken as non-relation 
instances. 
5 The head of a chunk is defined as the last word in the chunk. 
524
five tuple representation. As a relation in ACE is 
usually short, the words of the two entity mentions 
can provide more critical indications for relation 
classification than the words from the context. 
Within the two entity mentions, the head word of 
each mention is usually more important than other 
words of the mention; the conjunction of the two 
heads can provide an additional clue. And in 
general words other than the chunk head in the 
context do not contribute to establishing a 
relationship between the two entity mentions. 
The cluster based semi-supervised system works 
by adding an additional layer of lexical features 
that incorporate word clusters as shown in column 
4 of Table 4. Take the US soldier as an example, if 
we decide to use a length of 10 as a threshold to 
cut the Brown word hierarchy to generate word 
clusters, we will extract a cluster feature 
HM1_WC10=1101111101 in addition to the 
lexical feature HM1=soldier given that the full bit 
string of soldier is  1101111101100 in Table 2. 
(Note that the cluster feature is a nominal feature, 
not to be confused with an integer feature.) 
5.2 Selection of Clusters 
Given the bit string representations of all the words 
in a vocabulary, researchers usually use prefixes of 
different lengths of the bit strings to produce word 
clusters of various granularities. However, how to 
choose the set of prefix lengths in a principled way? 
This has not been answered by prior work. 
Our main idea is to learn the best set of prefix 
lengths, perhaps through the validation of their 
effectiveness on a development set of data. To our 
knowledge, previous research simply uses ad-hoc 
prefix lengths and lacks this training procedure. 
The training procedure can be extremely slow for 
reasons to be explained below. 
Formally, let l  be the set of available prefix 
lengths ranging from 1 bit to the length of the 
longest bit string in the Brown word hierarchy and 
let m  be the set of prefix lengths we want to use in 
decoding cluster features, then the problem of 
selecting effective clusters transforms to finding a 
| |m -combination of the set l which maximizes 
system performance. The training procedure can be 
extremely time consuming if we enumerate every 
possible | |m -combination of l , given that | |m  
can range from 1 to the size of l and the size of 
l equals the length of the longest bit string which is 
usually 20 when inducing 1,000 clusters using the 
Brown algorithm.                                  
One way to achieve better efficiency is to 
consider only a subset of l instead of the full set. In 
addition, we limit ourselves to use sizes 3 and 4 for 
m  for matching prior work. This keeps the cluster 
features to a manageable size considering that 
every word in your vocabulary could contribute to 
a lexical feature. For picking a subset of l , we 
propose below two statistical measures for 
computing the importance of a certain prefix 
length. 
Information Gain (IG): IG measures the 
quality or importance of a feature f by computing 
the difference between the prior entropy of classes 
C and the posterior entropy, given values V of the 
feature f (Hunt et al, 1966; Quinlan, 1986). For 
our purpose, C is the set of relation types, f is a 
cluster-based feature with a certain prefix length 
such as HM1_WC* where * means the prefix 
length and a value v is the prefix of the bit string 
representation of HM1. More formally, the IG of f 
is computed as follows: 
( ) ( ) log ( )
( ( ) ( | ) log ( | ))
c C
v V c C
IG f p c p c
p v p c v p c v
?
? ?
? ? ?
? ?
?
? ?
        (1) 
where the first and second terms refer to the prior 
and posterior entropies respectively. 
For each prefix length in the set l , we can 
compute its IG for a type of cluster feature and 
then rank the prefix lengths based on their IGs for 
that cluster feature. For simplicity, we rank the 
prefix lengths for a group of cluster features (a 
group is a row from column 4 in Table 4) by 
collapsing the individual cluster features into a 
single cluster feature. For example, we collapse the 
3 types: HM1_WC, HM2_WC and HM12_WC into 
a single type HM_WC for computing the IG.  
Prefix Coverage (PC): If we use a short prefix 
then the clusters produced correspond to the high 
branches in the word hierarchy and would be very 
general. The cluster features may not provide more 
informative information than the words themselves. 
Similarly, if we use a long prefix such as the length 
of the longest bit string, then maybe only a few of 
the lexical features can be covered by clusters. To 
capture this intuition, we define the PC of a prefix 
length i as below: 
525
( )( ) ( )
ic
l
count fPC i count f?
                        (2) 
where 
lf  stands for a lexical feature such as HM1 
and
icf
 a cluster feature with prefix length i such as 
HM1_WCi, (*)count  is the number of 
occurrences of that feature in training data. 
Similar to IG, we compute PC for a group of 
cluster features, not for each individual feature. 
In our experiments, the top 10 ranked prefix 
lengths based on IG and prefix lengths with PC 
values in the range [0.4, 0.9] were used. 
In addition to the above two statistical measures, 
for comparison, we introduce another two simple 
but extreme measures for the selection of clusters. 
Use All Prefixes (UA): UA produces a cluster 
feature at every available bit length with the hope 
that the underlying supervised system can learn 
proper weights of different cluster features during 
training. For example, if the full bit representation 
of ?Apple? is ?000?, UA would produce three 
cluster features: prefix1=0, prefix2=00 and 
prefix3=000. Because this method does not need 
validation on the development set, it is the laziest 
but the fastest method for selecting clusters.  
Exhaustive Search (ES): ES works by trying 
every possible combination of the set l and picking 
the one that works the best for the development set. 
This is the most cautious and the slowest method 
for selecting clusters. 
6 Experiments 
In this section, we first present details of our 
unsupervised word clusters, the relation extraction 
data set and its preprocessing. We then present a 
series of experiments coupled with result analyses. 
We used the English portion of the TDT5 
corpora (LDC2006T18) as our unlabeled data for 
inducing word clusters. It contains roughly 83 
million words in 3.4 million sentences with a 
vocabulary size of 450K. We left case intact in the 
corpora. Following previous work, we used 
Liang?s implementation of the Brown clustering 
algorithm (Liang, 2005).  We induced 1,000 word 
clusters for words that appeared at least twice in 
the corpora. The reduced vocabulary contains 
255K unique words. The clusters are available at 
http://www.cs.nyu.edu/~asun/data/TDT5_BrownW
C.tar.gz. 
For relation extraction, we used the benchmark 
ACE 2004 training data. Following most of the 
previous research, we used in experiments the 
nwire (newswire) and bnews (broadcast news) 
genres of the data containing 348 documents and 
4374 relation instances. We extracted an instance 
for every pair of mentions in the same sentence 
which were separated by no more than two other 
mentions. The non-relation instances generated 
were about 8 times more than the relation instances.  
Preprocessing of the ACE documents: We used 
the Stanford parser6 for syntactic and dependency 
parsing. We used chunklink7  to derive chunking 
information from the Stanford parsing. Because 
some bnews documents are in lower case, we 
recover the case for the head of a mention if its 
type is NAM by making the first character into its 
upper case. This is for better matching between the 
words in ACE and the words in the unsupervised 
word clusters. 
We used the OpenNLP 8  maximum entropy 
(maxent) package as our machine learning tool. 
We choose to work with maxent because the 
training is fast and it has a good support for multi-
class classification. 
6.1 Baseline Performance 
Following previous work, we did 5-fold cross-
validation on the 348 documents with hand-
annotated entity mentions. Our results are shown in 
Table 5 which also lists the results of another three 
state-of-the-art feature based systems. For this and 
the following experiments, all the results were 
computed at the relation mention level. 
 
System P(%) R(%) F(%) 
Zhou et al (2007)9 78.2 63.4 70.1 
Zhao and Grishman (2005)10 69.2 71.5 70.4 
Our Baseline 73.4 67.7 70.4 
Jiang and Zhai (2007) 11 72.4 70.2 71.3 
 
Table 5: Performance comparison on the ACE 2004 
data over the 7 relation types. 
                                                        
6 http://nlp.stanford.edu/software/lex-parser.shtml 
7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 
8 http://opennlp.sourceforge.net/ 
9 Zhou et al (2005) tested their system on the ACE 2003 data; 
Zhou et al (2007) tested their system on the ACE 2004 data. 
10  The paper gives a recall value of 70.5, which is not 
consistent with the given values of P and F. An examination of 
the correspondence in preparing this paper indicates that the 
correct recall value is 71.5. 
11 The result is from using the All features in Jiang and Zhai 
(2007). It is not quite clear from the paper that whether they 
used the 348 documents or the whole 2004 training data. 
526
Note that although all the 4 systems did 5-fold 
cross-validation on the ACE 2004 data, the 
detailed data partition might be different. Also, we 
were doing cross-validation at the document level 
which we believe was more natural than the 
instance level. Nonetheless, we believe our 
baseline system has achieved very competitive 
performance. 
6.2 The Effectiveness of Cluster Selection 
Methods 
We investigated the tradeoff between performance 
and training time of each proposed method in 
selecting clusters. In this experiment, we randomly 
selected 70 documents from the 348 documents as 
test data which roughly equaled the size of 1 fold 
in the baseline in Section 6.1. For the baseline in 
this section, all the rest of the documents were used 
as training data. For the semi-supervised system, 
70 percent of the rest of the documents were 
randomly selected as training data and 30 percent 
as development data. The set of prefix lengths that 
worked the best for the development set was 
chosen to select clusters. We only used the cluster 
feature HM_WC in this experiment.  
 
System F ? Training  Time (in minute) 
Baseline 70.70  1 
UA 71.19 +0.49 1.5 
PC3 71.65 +0.95 30 
PC4 71.72 +1.02 46 
IG3 71.65 +0.95 45 
IG4 71.68 +0.98 78 
ES3 71.66 +0.96 465 
ES4 71.60 +0.90 1678 
 
Table 6: The tradeoff between performance and training 
time of each method in selecting clusters. PC3 means 
using 3 prefixes with the PC method. ? in this paper 
means the difference between a system and the baseline. 
 
Table 6 shows that all the 4 proposed methods 
improved baseline performance, with UA as the 
fastest and ES as the slowest. It was interesting that 
ES did not always outperform the two statistical 
methods which might be because of its overfitting 
to the development set. In general, both PC and IG 
had good balances between performance and 
training time. There was no dramatic difference in 
performance between using 3 and 4 prefix lengths.  
For the rest of this paper, we will only use PC4 
as our method in selecting clusters. 
6.3 The Effectiveness of Cluster Features 
The baseline here is the same one used in Section 
6.1. For the semi-supervised system, each test fold 
was the same one used in the baseline and the other 
4 folds were further split into a training set and a 
development set in a ratio of 7:3 for selecting 
clusters. We first added the cluster features 
individually into the baseline and then added them 
incrementally according to the order specified in 
Table 4. 
 
System F ? 
1 Baseline 70.4  
2 1 + HM_WC 71.5 + 1.1 
3 1 + BagWM_WC 71.0 + 0.6 
4 1 + HC_WC 69.6 - 0.8 
5 1 + BagWC_WC 46.1 - 24.3 
6 2 + BagWM_WC 71.0 + 0.6 
7 6 + HC_WC 70.6 + 0.2 
8 7+ BagWC_WC 50.3 - 20.1 
 
Table 7: Performance 12  of the baseline and using 
different cluster features with PC4 over the 7 types.  
 
We found that adding clusters to the heads of the 
two mentions was the most effective way of 
introducing cluster features. Adding clusters to the 
words of the mentions can also help, though not as 
good as the heads. We were surprised that the 
heads of chunks in context did not help. This might 
be because ACE relations are usually short and the 
limited number of long relations is not sufficient in 
generalizing cluster features. Adding clusters to 
every word in context hurt the performance a lot. 
Because of the behavior of each individual feature, 
it was not surprising that adding them 
incrementally did not give more performance gain.  
For the rest of this paper, we will only use 
HM_WC as cluster features. 
6.4 The Impact of Training Size 
We studied the impact of training data size on 
cluster features as shown in Table 8. The test data 
was always the same as the 5-fold used in the 
baseline in Section 6.1. no matter the size of the 
training data. The training documents for the  
                                                        
12  All the improvements of F in Table 7, 8 and 9 were 
significant at confidence levels >= 95%. 
527
# docs F of Relation Classification F of Relation Detection 
Baseline PC4 (?) Prefix10(?) Baseline PC4(?) Prefix10(?) 
50 62.9 63.8(+ 0.9) 63.7(+0.8) 71.4 71.9(+ 0.5) 71.6(+0.2) 
75 62.8 64.6(+ 1.8) 63.9(+1.1) 71.5 72.3(+ 0.8) 72.5(+1.0) 
125 66.1 68.1(+ 2.0) 67.5(+1.4) 74.5 74.8(+ 0.3) 74.3(-0.2) 
175 67.8 69.7(+ 1.9) 69.5(+1.7) 75.2 75.5(+ 0.3) 75.2(0.0) 
225 68.9 70.1(+ 1.2) 69.6(+0.7) 75.6 75.9(+ 0.3) 75.3(-0.3) 
?280 70.4 71.5(+ 1.1) 70.7(+0.3) 76.4 76.9(+ 0.5) 76.3(-0.1) 
 
Table 8: Performance over the 7 relation types with different sizes of training data. Prefix10 uses the single prefix 
length 10 to generate word clusters as used by Chan and Roth (2010). 
 
Type P R F 
Baseline PC4 (?) Baseline PC4 (?) Baseline PC4 (?) 
EMP-ORG 75.4 77.2(+1.8) 79.8 81.5(+1.7) 77.6 79.3(+1.7) 
PHYS 73.2 71.2(-2.0) 61.6 60.2(-1.4) 66.9 65.3(-1.7) 
GPE-AFF 67.1 69.0(+1.9) 60.0 63.2(+3.2) 63.3 65.9(+2.6) 
PER-SOC 88.2 83.9(-4.3) 58.4 61.0(+2.6) 70.3 70.7(+0.4) 
DISC 79.4 80.6(+1.2) 42.9 46.0(+3.2) 55.7 58.6(+2.9) 
ART 87.9 96.9(+9.0) 63.0 67.4(+4.4) 73.4 79.3(+5.9) 
OTHER-AFF 70.6 80.0(+9.4) 41.4 41.4(0.0) 52.2 54.6(+2.4) 
 
Table 9: Performance of each individual relation type based on 5-fold cross-validation. 
 
current size setup were randomly selected and 
added to the previous size setup (if applicable). For 
example, we randomly selected another 25 
documents and added them to the previous 50 
documents to get 75 documents. We made sure 
that every document participated in this experiment. 
The training documents for each size setup were 
split into a real training set and a development set 
in a ratio of 7:3 for selecting clusters.  
There are some clear trends in Table 8. Under 
each training size, PC4 consistently outperformed 
the baseline and the system Prefix10 for relation 
classification. For PC4, the gain for classification 
was more pronounced than detection. The mixed 
detection results of Prefix10 indicated that only 
using a single prefix may not be stable.   
We did not observe the same trend in the 
reduction of annotation need with cluster-based 
features as in Koo et al (2008) for dependency 
parsing. PC4 with sizes 50, 125, 175 outperformed 
the baseline with sizes 75, 175, 225 respectively. 
But this was not the case when PC4 was tested 
with sizes 75 and 225.  This might due to the 
complexity of the relation extraction task. 
6.5 Analysis 
There were on average 69 cross-type errors in the 
baseline in Section 6.1 which were reduced to 56 
by using PC4. Table 9 showed that most of the 
improvements involved EMP-ORG, GPE-AFF, 
DISC, ART and OTHER-AFF. The performance 
gain for PER-SOC was not as pronounced as the 
other five types. The five types of relations are 
ambiguous as they share the same entity type GPE 
while the PER-SOC relation only holds between 
PER and PER. This reflects that word clusters can 
help to distinguish between ambiguous relation 
types. 
As mentioned earlier the gain of relation 
detection was not as pronounced as classification 
as shown in Table 8. The unbalanced distribution 
of relation instances and non-relation instances 
remains as an obstacle for pushing the performance 
of relation extraction to the next level. 
7 Conclusion and Future Work 
We have described a semi-supervised relation 
extraction system with large-scale word clustering. 
We have systematically explored the effectiveness 
of different cluster-based features. We have also 
demonstrated that the two proposed statistical 
methods are both effective and efficient in 
selecting clusters at an appropriate level of 
granularity through an extensive experimental 
study. 
528
Based on the experimental results, we plan to 
investigate additional ways to improve the 
performance of relation detection. Moreover, 
extending word clustering to phrase clustering (Lin 
and Wu, 2009) and pattern clustering (Sun and 
Grishman, 2010) is worth future investigation for 
relation extraction. 
References  
Rie K. Ando and Tong Zhang. 2005 A Framework for 
Learning Predictive Structures from Multiple Tasks 
and Unlabeled Data. Journal of Machine Learning 
Research, Vol 6:1817-1853. 
Elizabeth Boschee, Ralph Weischedel, and Alex 
Zamanian. 2005. Automatic information extraction. 
In Proceedings of the International Conference on 
Intelligence Analysis. 
Peter F. Brown, Vincent J. Della Pietra, Peter V. 
deSouza,  Jenifer C. Lai, and Robert L. Mercer. 1992. 
Class-based n-gram models of natural language. 
Computational Linguistics, 18(4):467?479. 
Razvan C. Bunescu and Raymond J. Mooney. 2005a. A 
shortest path dependency kenrel for relation 
extraction. In Proceedings of HLT/EMNLP. 
Razvan C. Bunescu and Raymond J. Mooney. 2005b. 
Subsequence kernels for relation extraction. In 
Proceedings of NIPS. 
Yee Seng Chan and Dan Roth. 2010. Exploiting 
background knowledge for relation extraction. In 
Proc. of COLING. 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System Description. 
ACE 2005 Evaluation Workshop.  
Earl B. Hunt, Philip J. Stone and Janet Marin. 1966. 
Experiments in Induction. New York: Academic 
Press, 1966. 
Jing Jiang and ChengXiang Zhai. 2007. A systematic 
exploration of the feature space for relation 
extraction. In Proceedings of HLT-NAACL-07.  
Nanda Kambhatla. 2004. Combining lexical, syntactic, 
and semantic features with maximum entropy models 
for information extraction. In Proceedings of ACL-04. 
Terry Koo, Xavier Carreras, and Michael Collins. 2008. 
Simple Semi-supervised Dependency Parsing. In 
Proceedings of ACL-08: HLT. 
Percy Liang. 2005. Semi-Supervised Learning for 
Natural Language. Master?s thesis, Massachusetts 
Institute of Technology. 
Dekang Lin and Xiaoyun Wu. 2009. Phrase Clustering 
for Discriminative Learning. In Proc. of ACL-09. 
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph 
Weischedel. 2000. A novel use of statistical parsing 
to extract information from text. In Proc. of NAACL. 
Scott Miller, Jethran Guinness and Alex Zamanian. 
2004. Name Tagging with Word Clusters and 
Discriminative Training. In Proc. of HLT-NAACL. 
Longhua Qian, Guodong Zhou, Qiaoming Zhu and 
Peide Qian. 2008. Exploiting constituent 
dependencies for tree kernel-based semantic relation 
extraction . In Proc. of COLING. 
John Ross Quinlan. 1986. Induction of decision trees. 
Machine Learning, 1(1), 81-106. 
Lev Ratinov and Dan Roth. 2009. Design challenges 
and misconceptions in named entity recognition. In 
Proceedings of CoNLL-09. 
Ang Sun. 2009. A Two-stage Bootstrapping Algorithm 
for Relation Extraction. In RANLP-09. 
Ang Sun and Ralph Grishman. 2010. Semi-supervised 
Semantic Pattern Discovery with Guidance from 
Unsupervised Pattern Clusters. In Proc. of COLING. 
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 
2010. Word representations: A simple and general 
method for semi-supervised learning. In Proceedings 
of ACL. 
Dmitry Zelenko, Chinatsu Aone, and Anthony 
Richardella. 2003. Kernel methods for relation 
extraction. Journal of Machine Learning Research, 
3:1083?1106. 
Zhu Zhang. 2004. Weakly supervised relation 
classification for information extraction. In Proc. of 
CIKM?2004. 
Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou. 
2006. A composite kernel to extract relations 
between entities with both flat and structured features. 
In Proceedings of COLING-ACL-06. 
Shubin Zhao and Ralph Grishman. 2005. Extracting 
relations with integrated information using kernel 
methods. In Proceedings of ACL. 
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang. 
2005. Exploring various knowledge in relation 
extraction. In Proceedings of ACL-05. 
Guodong Zhou, Min Zhang, DongHong Ji, and 
QiaoMing Zhu. 2007. Tree kernel-based relation 
extraction with context-sensitive structured parse tree 
information. In Proceedings of EMNLPCoNLL-07. 
529
Proceedings of the TextGraphs-8 Workshop, pages 70?78,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Graph-based Approaches for Organization Entity Resolution in MapReduce
Hakan Kardes, Deepak Konidena, Siddharth Agrawal, Micah Huff and Ang Sun
inome Inc.
Bellevue, WA, USA
{hkardes,dkonidena,sagrawal,mhuff,asun}@inome.com
Abstract
Entity Resolution is the task of identifying which
records in a database refer to the same entity. A
standard machine learning pipeline for the entity res-
olution problem consists of three major components:
blocking, pairwise linkage, and clustering. The
blocking step groups records by shared properties to
determine which pairs of records should be exam-
ined by the pairwise linker as potential duplicates.
Next, the linkage step assigns a probability score to
pairs of records inside each block. If a pair scores
above a user-defined threshold, the records are pre-
sumed to represent the same entity. Finally, the clus-
tering step turns the input records into clusters of
records (or profiles), where each cluster is uniquely
associated with a single real-world entity. This paper
describes the blocking and clustering strategies used
to deploy a massive database of organization entities
to power a major commercial People Search Engine.
We demonstrate the viability of these algorithms for
large data sets on a 50-node hadoop cluster.
1 Introduction
A challenge for builders of databases whose information
is culled from multiple sources is the detection of dupli-
cates, where a single real-world entity gives rise to mul-
tiple records (see (Elmagarmid, 2007) for an overview).
Entity Resolution is the task of identifying which records
in a database refer to the same entity. Online citation
indexes need to be able to navigate through the differ-
ent capitalization and abbreviation conventions that ap-
pear in bibliographic entries. Government agencies need
to know whether a record for ?Robert Smith? living on
?Northwest First Street? refers to the same person as one
for a ?Bob Smith? living on ?1st St. NW?. In a standard
machine learning approach to this problem all records
first go through a cleaning process that starts with the re-
moval of bogus, junk and spam records. Then all records
are normalized to an approximately common representa-
tion. Finally, all major noise types and inconsistencies
are addressed, such as empty/bogus fields, field duplica-
tion, outlier values and encoding issues. At this point, all
records are ready for the major stages of the entity resolu-
tion, namely blocking, pairwise linkage, and clustering.
Since comparing all pairs of records is quadratic in the
number of records and hence is intractable for large data
sets, the blocking step groups records by shared proper-
ties to determine which pairs of records should be exam-
ined by the pairwise linker as potential duplicates. Next,
the linkage step assigns a score to pairs of records inside
each block. If a pair scores above a user-defined thresh-
old, the records are presumed to represent the same entity.
The clustering step partitions the input records into sets
of records called profiles, where each profile corresponds
to a single entity.
In this paper, we focus on entity resolution for the or-
ganization entity domain where all we have are the orga-
nization names and their relations with individuals. Let?s
first describe the entity resolution for organization names,
and discuss its significance and the challenges in more de-
tail. Our process starts by collecting billions of personal
records from three sources of U.S. records to power a ma-
jor commercial People Search Engine. Example fields
on these records might include name, address, birthday,
phone number, (encrypted) social security number, rel-
atives, friends, job title, universities attended, and orga-
nizations worked for. Since the data sources are hetero-
geneous, each data source provides different aliases of
an organization including abbreviations, preferred names,
legal names, etc. For example, Person A might have
both ?Microsoft?, ?Microsoft Corp?, ?Microsoft Corpo-
ration?, and ?Microsoft Research? in his/her profile?s or-
ganization field. Person B might have ?University of
Washington?, while Person C has ?UW? as the organi-
zation listed in his/her profile. Moreover, some organi-
zations change their names, or are acquired by other in-
70
stitutions and become subdivisions. There are also many
organizations that share the same name or abbreviation.
For instance, both ?University of Washington?, ?Univer-
sity of Wisconsin Madison?, ?University of Wyoming?
share the same abbreviation, ?UW?. Additionally, some
of the data sources might be noisier than the others and
there might be different kind of typos that needs to be
addressed.
Addressing the above issues in organization fields is
crucial for data quality as graphical representations of the
data become more popular. If we show different represen-
tations of the same organization as separate institutions in
a single person?s profile, it will decrease the confidence of
a customer about our data quality. Moreover, we should
have a unique representation of organizations in order to
properly answer more complicated graph-based queries
such as ?how am I connected to company X??, or ?who
are my friends that has a friend that works at organization
X, and graduated from school Y??.
We have developed novel and highly scalable com-
ponents for our entity resolution pipeline which is cus-
tomized for organizations. The focus of this paper is the
graph-based blocking and clustering components. In the
remainder of the paper, we first describe these compo-
nents in Section 2. Then, we evaluate the performance of
our entity resolution framework using several real-world
datasets in Section 3. Finally, we conclude in Section 4.
2 Methodology
In this section, we will mainly describe the blocking and
clustering strategies as they are more graph related. We
will also briefly mention our pairwise linkage model.
The processing of large data volumes requires highly
scalable parallelized algorithms, and this is only possible
with distributed computing. To this end, we make heavy
use of the hadoop implementation of the MapReduce
computing framework, and both the blocking and cluster-
ing procedures described here are implemented as a series
of hadoop jobs written in Java. It is beyond the scope of
this paper to fully describe the MapReduce framework
(see (Lin, 2010) for an overview), but we do discuss the
ways its constraints inform our design. MapReduce di-
vides computing tasks into a map phase in which the in-
put, which is given as (key,value) pairs, is split up among
multiple machines to be worked on in parallel and a re-
duce phase in which the output of the map phase is put
back together for each key to independently process the
values for each key in parallel. Moreover, in a MapRe-
duce context, recursion becomes iteration.
2.1 Blocking
How might we subdivide a huge number of organiza-
tions based on similarity or probability scores when all
we have is their names and their relation with people? We
could start by grouping them into sets according to the
words they contain. This would go a long way towards
putting together records that represent the same organiza-
tion, but it would still be imperfect because organizations
may have nicknames, abbreviations, previous names, or
misspelled names. To enhance this grouping we could
consider a different kind of information like soundex or a
similar phonetic algorithm for indexing words to address
some of the limitations of above grouping due to typos.
We can also group together the organizations which ap-
pear in the same person?s profile. This way, we will be
able to block the different representations of the same or-
ganization to some extent. With a handful of keys like this
we can build redundancy into our system to accommodate
different types of error, omission, and natural variability.
The blocks of records they produce may overlap, but this
is desirable because it gives the clustering a chance to join
records that blocking did not put together.
The above blocks will vary widely in size. For exam-
ple, we may have a small set of records containing the
word ?Netflix? which can then be passed along immedi-
ately to the linkage component. However, we may have
a set of millions of records containing the word ?State?
which still needs to be cut down to subsets with manage-
able sizes, otherwise it will be again impractical to do
all pairwise computations in this block. One way to do
this is to find other common properties to further subdi-
vide this set. The set of all records containing not only
?State? but also a specific state name like ?Washington?
is smaller than the set of all records containing the word
?State?, and intuitively records in this set will be more
likely to represent the same organization. Additionally
we could block together all the ?State? records with the
same number of words, or combination of the initials of
each word. As with the original blocks, overlap between
these sub-blocks is desirable. We do not have to be par-
ticularly artful in our choice of sub-blocking criteria: any
property that seems like it might be individuating will
do. As long as we have an efficient way to search the
space, we can let the data dynamically choose different
sub-blocking strategies for each oversize block. To this
end, we use the ordering on block keys to define a bino-
mial tree where each node contains a list of block keys
and is the parent of nodes that have keys that come later
in the ordering appended to the list. Figure 1 shows a
tree for the oversize top-level set tTkn1 with three sub-
blocking tokens sTkn1 < sTkn2 < sTkn3. With each
node of the tree we can associate a block whose key is
the list of blocks keys in that node and whose records are
the intersection of the records in those blocks, e.g. the
tTkn1 ? sTkn1 ? sTkn2 node represents all the records
for organizations containing all these tokens. Because the
cardinality of an intersected set is less than or equal to the
cardinalities of the sets that were intersected, every block
71
tTkn1
tTkn1 ? sTkn3tTkn1 ? sTkn2
tTkn1 ? sTkn2 ? sTkn3
tTkn1 ? sTkn1
tTkn1 ? sTkn1 ? sTkn3tTkn1 ? sTkn1 ? sTkn2
tTkn1 ? sTkn1 ? sTkn2 ? sTkn3
Figure 1: The root node of this tree represents an oversized block for the name Smith and the other nodes represent possible
sub-blocks. The sub-blocking algorithm enumerates the tree breadth-first, stopping when it finds a correctly-sized sub-block.
in the tree is larger than or equal to any of its children. We
traverse the tree breadth-first and only recurse into nodes
above the maximum block size. This allows us to explore
the space of possible sub-blocks in cardinality order for a
given branch, stopping as soon as we have a small enough
sub-block.
The algorithm that creates the blocks and sub-blocks
takes as input a set of records and a maximum block size
M . All the input records are grouped into blocks defined
by the top-level properties. Those top-level blocks that
are not above the maximum size are set aside. The re-
maining oversized blocks are partitioned into sub-blocks
by sub-blocking properties that the records they con-
tain share, and those properties are appended to the key.
The process is continued recursively until all sub-blocks
have been whittled down to an acceptable size. The
pseudo code of the blocking algorithm is presented in
Figure 2. We will represent the key and value pairs
in the MapReduce framework as < key; value >.
The input organization records are represented as <
INPUT FLAG,ORG NAME >. For the first iter-
ation, this job takes the organization list as input. In
later iterations, the input is the output of the previous
blocking iteration. In the first iteration, the mapper
function extracts the top-level and sub-level tokens from
the input records. It combines the organization name
and all the sub-level tokens in a temp variable called
newV alue. Next, for each top-level token, it emits this
top-level token and the newValue in the following for-
mat: < topToken, newV alue >. For the later itera-
tions, it combines each sub level token with the current
blocking key, and emits them to the reducer. Also note
that the lexicographic ordering of the block keys allows
separate mapper processes to work on different nodes in a
level of the binomial tree without creating redundant sub-
blocks (e.g. if one mapper creates a International ? Busi-
ness ? Machines block another mapper will not create a
International ? Machines ? Business one). This is nec-
essary because individual MapReduce jobs run indepen-
dently without shared memory or other runtime commu-
nication mechanisms. In the reduce phase, all the records
will be grouped together for each block key. The reducer
function iterates over all the records in a newly-created
sub-block, counting them to determine whether or not the
block is small enough or needs to be further subdivided.
The blocks that the reducer deems oversized become in-
puts to the next iteration. Care is taken that the memory
requirements of the reducer function are constant in the
size of a fixed buffer because otherwise the reducer runs
out of memory on large blocks. Note that we create a
black list from the high frequency words in organization
names, and we don?t use these as top-level properties as
such words do not help us with individuating the records.
More formally, this process can be understood in terms
of operations on sets. In a set of N records there are
1
2
N(N ? 1) unique pairs, so an enumeration over all
of them is O(N2). The process of blocking divides this
original set into k blocks, each of which contains at most
a fixed maximum of M records. The exhaustive compar-
ison of pairs from these sets is O(k), and the constant
factors are tractable if we choose a small enough M . In
the worst case, all the sub-blocks except the ones with the
very longest keys are oversize. Then the sub-blocking al-
gorithm will explore the powerset of all possible block-
ing keys and thus have exponential runtime. However, as
the blocking keys get longer, the sets they represent get
smaller and eventually fall beneath the maximum size. In
practice these two countervailing motions work to keep
this strategy tractable.
2.2 Pairwise Linkage Model
In this section, we give just a brief overview of our pair-
wise linkage system as a detailed description and evalua-
tion of that system is beyond the scope of this paper.
We take a feature-based classification approach to pre-
dict the likelihood of two organization names < o
1
, o
2
>
referring to the same organization entity. Specifically, we
use the OpenNLP1 maximum entropy (maxent) package
as our machine learning tool. We choose to work with
maxent because the training is fast and it has a good sup-
port for classification. Regarding the features, we mainly
have two types: surface string features and context fea-
tures. Examples of surface string features are edit dis-
1http://opennlp.apache.org/
72
Blocking Iterations
map(key, value)
if(key.equals(EntityName)
String[] tokens ? value.split(? ?)
sublevelTokenSet ? ?
toplevelTokenSet ? ?
for each (token ? tokens)
sublevelTokenSet.add(token.hashCode())
if(notExist(blackList, token))
toplevelTokenSet.add(token.hashCode())
String newValue ? value
for each (sToken ? sublevelTokenSet)
newValue ? newV alue.append(STR + sToken)
for each (tToken ? toplevelTokenSet)
emit(tToken, newV alue)
else
String[] keyTokens ? key.split(STR)
String[] valueTokens ? value.split(STR)
for each (token ? valueTokens)
if(token > keyTokens[keyTokens.length? 1])
emit(key.append(STR + token), value)
reduce(key,< iterable > values)
buffer ? ?
for each (value ? values)
buffer.add(value)
if(buffer.length ? MAXBLOCKSIZE)
break
if(buffer.length ? MAXBLOCKSIZE)
for each (ele ? buffer)
emit(key, ele)
for each (value ? values)
emit(key, value)
elseif(buffer.length ? 1)
blocks.append(key, buffer)
Figure 2: Alg.1 - Blocking
tance of the two names, whether one name is an abbre-
viation of the other name, and the longest common sub-
string of the two names. Examples of context features are
whether the two names share the same url and the number
of times that the two names co-occur with each other in a
single person record.
2.3 Clustering
In this section, we present our clustering approach. Let?s,
first clarify a set of terms/conditions that will help us de-
scribe the algorithms.
Definition (Connected Component): Let G = (V,E)
be an undirected graph where V is the set of vertices and
E is the set of edges. C = (C
1
, C
2
, ..., Cn) is the set of
disjoint connected components in this graph where (C
1
?
C
2
? ... ? Cn) = V and (C1 ? C2 ? ... ? Cn) = ?. For
each connected component Ci ? C, there exists a path in
G between any two vertices vk and vl where (vk, vl) ?
Ci. Additionally, for any distinct connected component
sClust 
Transitive 
Closure Edge List 
Node - 
ClusterID 
mapping 
anyClus
ter > 
maxSize 
no 
yes 
Extract Pairs 
Figure 3: Clustering Component
(Ci, Cj) ? C, there is no path between any pair vk and
vl where vk ? Ci, vl ? Cj . Moreover, the problem of
finding all connected components in a graph is finding
the C satisfying the above conditions. 
Definition (Component ID): A component id is a
unique identifier assigned to each connected component.
Definition (Max Component Size): This is the maxi-
mum allowed size for a connected component. 
Definition (Cluster Set): A cluster set is a set of
records that belong to the same real world entity. 
Definition (Max Cluster Size): This is the maximum
allowed size for a cluster. 
Definition (Match Threshold): Match threshold is a
score where pairs scoring above this score are said to rep-
resent the same entity. 
Definition (No-Match Threshold): No-Match thresh-
old is a score where pairs scoring below this score are said
to represent different entities. 
Definition (Conflict Set): Each record has a conflict
set which is the set of records that shouldn?t appear with
this record in any of the clusters. 
The naive approach to clustering for entity resolu-
tion is transitive closure by using only the pairs having
scores above the match threshold. However, in practice
we might see many examples of conflicting scores. For
example, (a,b) and (b,c) pairs might have scores above
match threshold while (a,c) pair has a score below no-
match threshold. If we just use transitive closure, we
will end up with a single cluster with these three records
(a,b,c). Another weakness of the regular transitive clo-
sure is that it creates disjoint sets. However, organiza-
tions might share name, or abbreviation. So, we need a
soft clustering approach where a record might be in dif-
ferent clusters.
On the other hand, the large volume of our data re-
quires highly scalable and efficient parallelized algo-
rithms. However, it is very hard to implement par-
73
TC-Iterate TC-Dedup Edge List 
Node - 
ComponentID 
mapping 
newPair 
> 0 
no 
yes 
iterationID=1 
iterationID>1 
Figure 4: Transitive Closure Component
allelized clustering approaches with high precision for
large scale graphs due to high time and space complexi-
ties (Bansal, 2003). So, we propose a two-step approach
in order to build both a parallel and an accurate clustering
framework. The high-level architecture of our cluster-
ing framework is illustrated in Figure 3. We first find the
connected components in the graph with our MapReduce
based transitive closure approach, then further, partition
each connected component in parallel with our novel soft
clustering algorithm, sClust. This way, we first combine
similar record pairs into connected components in an effi-
cient and scalable manner, and then further partition each
connected component into smaller clusters for better pre-
cision. Note that there is a dangerous phenomenon, black
hole entities, in transitive closure of the pairwise scores
(Michelson, 2009). A black hole entity begins to pull
an inordinate amount of records from an increasing num-
ber of different true entities into it as it is formed. This
is dangerous, because it will then erroneously match on
more and more records, escalating the problem. Thus, by
the end of the transitive closure, one might end up with
black hole entities with millions of records belonging to
multiple different entities. In order to avoid this problem,
we define a black hole threshold, and if we end up with
a connected component above the size of the black hole
threshold, we increment the match threshold by a delta
and further partition this black hole with one more tran-
sitive closure job. We repeat this process until the sizes
of all the connected components are below the black hole
threshold, and then apply sClust on each connected com-
ponent. Hence at the end of the entire entity resolution
process, the system has partitioned all the input records
into cluster sets called profiles, where each profile corre-
sponds to a single entity.
2.4 Transitive Closure
In order to find the connected components in a graph, we
developed the Transitive Closure (TC) module shown in
Figure 4. The input to the module is the list of all pairs
having scores above the match threshold. As an output
from the module, what we want to obtain is the mapping
from each node in the graph to its corresponding com-
ponentID. For simplicity, we use the smallest node id in
each connected component as the identifier of that com-
ponent. Thus, the module should output a mapping table
from each node in the graph to the smallest node id in
its corresponding connected component. To this end, we
designed a chain of two MapReduce jobs, namely, TC-
Transitive Closure Iterate
map(key, value)
emit(key, value)
emit(value, key)
reduce(key,< iterable > values)
minV alue ? values.next()
if(minV alue < key)
emit(key,minV alue)
for each (value ? values)
Counter.NewPair.increment(1)
emit(value,minV alue)
(a) Transitive Closure - Iterate
Transitive Closure Dedup
map(key, value)
emit(key.append(STR + value), null)
reduce(key,< iterable > values)
String[] keyTokens ? key.split(STR)
emit(keyTokens[0], keyTokens[1])
(b) Transitive Closure - Dedup
Figure 5: Alg.3 - Transitive Closure
Iterate, and TC-Dedup, that will run iteratively till we
find the corresponding componentIDs for all the nodes
in the graph.
TC-Iterate job generates adjacency lists AL =
(a
1
, a
2
, ..., an) for each node v, and if the node id of this
node vid is larger than the min node id amin in the adja-
cency list, it first creates a pair (vid, amin) and then a pair
for each (ai, amin) where ai ? AL, and ai = amin. If
there is only one node in AL, it means we will generate
the pair that we have in previous iteration. However, if
there is more than one node in AL, it means we might
generate a pair that we didn?t have in the previous itera-
tion, and one more iteration is needed. Please note that,
if vid is smaller than amin, we don?t emit any pair.
The pseudo code of TC-Iterate is given in Figure 5-
(a). For the first iteration, this job takes the pairs having
scores above the match threshold from the initial edge list
as input. In later iterations, the input is the output of TC-
Dedup from the previous iteration. We first start with the
initial edge list to construct the first degree neighborhood
of each node. To this end, for each edge < a; b >, the
mapper emits both < a; b >, and < b; a > pairs so that
a should be in the adjacency list of b and vice versa. In
the reduce phase, all the adjacent nodes will be grouped
together for each node. Reducers don?t receive the values
in a sorted order. So, we use a secondary sort approach
to pass the values to the reducer in a sorted way with cus-
tom partitioning (see (Lin, 2010) for details). This way,
the first value becomes the minValue. If the minValue is
larger than the key, we don?t emit anything. Otherwise,
74
Bring together all edges for each partition
Phase-1
map(key, value)
if(key.equals(ConflationOutput))
if ((value.score ? NO MATCH THR)||
(value.score ? MATCH THR))
emit(value.entity1, value)
else //TCDedupOutput
temp.entity1 ? value
temp.entity2 ? null
temp.score ? null
emit(key, temp)
emit(value, temp)
reduce(key,< iterable > values)
valueList ? ?
for each (value ? values)
if(value.entity2 = null)
clusID ? value.entity1
else
valueList.add(value)
for each (value ? valueList)
emit(clusID, value)
Phase-2
map(key, value)
emit(key, value)
reduce(key,< iterable > values)
valueList ? ?
for each (value ? values)
valueList.add(value)
emit(key, valueList)
Figure 6: Alg.3 - Bring together all edges for each partition
we first emit the < key;minV alue > pair. Next, we
emit a pair for all other values as < value;minV alue >,
and increase the global NewPair counter by 1. If the
counter is 0 at the end of the job, it means that we found
all the components and there is no need for further itera-
tions.
During the TC-Iterate job, the same pair might be emit-
ted multiple times. The second job, TC-Dedup, just dedu-
plicates the output of the CCF-Iterate job. This job in-
creases the efficiency of TC-Iterate job in terms of both
speed and I/O overhead. The pseudo code for this job is
given in Figure 5-(b).
The worst case scenario for the number of necessary
iterations is d+1 where d is the diameter of the net-
work. The worst case happens when the min node in
the largest connected component is an end-point of the
largest shortest-path. The best case scenario takes d/2+1
iterations. For the best case, the min node should be at
the center of the largest shortest-path.
2.5 sClust: A Soft Agglomerative Clustering
Approach
After partitioning the records into disjoint connected
components, we further partition each connected compo-
nent into smaller clusters with sClust approach. sClust
is a soft agglomerative clustering approach, and its main
difference from any other hierarchical clustering method
is the ?conflict set? term that we described above. Any of
the conflicting nodes cannot appear in a cluster with this
approach. Additionally, the maximum size of the clusters
can be controlled by an input parameter.
First as a preprocessing step, we have a two-step
MapReduce job (see Figure 6) which puts together and
sorts all the pairwise scores for each connected compo-
nent discovered by transitive closure. Next, sClust job
takes the sorted edge lists for each connected component
as input, and partitions each connected component in par-
allel. The pseudo-code for sClust job is given in Figure 7.
sClust iterates over the pairwise scores twice. During the
first iteration, it generates the node structures, and conflict
sets for each of these structures. For example, if the pair-
wise score for (a, b) pair is below the no-match threshold,
node a is added to node b?s conflict set, and vice versa. By
the end of the first iteration, all the conflict sets are gen-
erated. Now, one more pass is needed to build the final
clusters. Since the scores are sorted, we start from the
highest score to agglomeratively construct the clusters by
going over all the scores above the match threshold. Let?s
assume we have a pair (a, b) with a score above the match
threshold. There might be 4 different conditions. First,
both node a and node b are not in any of the clusters yet.
In this case, we generate a cluster with these two records
and the conflict set of this cluster becomes the union of
conflict sets of these two records. Second, node a might
already be assigned to a set of clusters C? while node b is
not in any of the clusters. In these case, we add node b to
each cluster in C? if it doesn?t conflict with b. If there is
no such cluster, we build a new cluster with nodes a and
b. Third is the opposite version of the second condition,
and the procedure is the same. Finally, both node a and
node b might be in some set of clusters. If they already
appear in the same cluster, no further action needed. If
they just appear in different clusters, these clusters will
be merged as long as there is no conflict between these
clusters. If there are no such unconflicting clusters, we
again build a new cluster with nodes a and b. This way,
we go over all the scores above the match threshold and
build the cluster sets. Note that if the clusters are merged,
their conflict sets are also merged. Additionally, if the
max cluster size parameter is defined, this condition is
also checked before merging any two clusters, or adding
a new node to an existing cluster.
75
Clustering
map(key, valueList)
for each (value ? valueList)
if(value.score ? MATCH THR)
nodes.insert(value.entity1)
nodes.insert(value.entity2)
else
node1Index ? find(value.entity1, nodes)
node2Index ? find(value.entity2, nodes)
nodes[node1Index].conflictSet.insert(node2Index)
nodes[node2Index].conflictSet.insert(node1Index)
for each (value ? valueList)
if(value.score ? MATCH THR)
node1Index ? find(value.entity1, nodes)
node2Index ? find(value.entity2, nodes)
node1ClusIDLength ? nodes[node1Index].clusIDs.length
node2ClusIDLength ? nodes[node2Index].clusIDs.length
if((node1ClusIDLength = 0) && (node2ClusIDLength = 0))
clusters[numClusters].nodes[0] ? node1Index
clusters[numClusters].nodes[1] ? node2Index
clusters[numClusters].confSet ?
mergeSortedLists(nodes[node1Index].confSet, nodes[node2Index].confSet)
nodes[node1Index].clusIDs.insert(numClusters)
nodes[node2Index].clusIDs.insert(numClusters)
numClusters++
elseif(node1ClusIDLength = 0)
for each (node2ClusID ? nodes[node2Index].clusIDs)
if(notContain(clusters[node2ClusID].confSet, node1Index))
insertToSortedList(clusters[node2ClusID].nodes, node1Index)
clusters[node2ClusID].confSet ?
mergeSortedLists(clusters[node2ClusID].confSet, nodes[node1Index].confSet)
nodes[node1Index].clusIDs.insert(node2ClusID)
elseif(node2ClusIDLength = 0)
for each (node1ClusID ? nodes[node1Index].clusIDs)
if(notContain(clusters[node1ClusID].confSet, node2Index))
insertToSortedList(clusters[node1ClusID].nodes, node2Index)
clusters[node1ClusID].confSet ?
mergeSortedLists(clusters[node1ClusID].confSet, nodes[node2Index].confSet)
nodes[node2Index].clusIDs.insert(node1ClusID)
elseif(notIntersect(clusters[node1ClusID].clusIDs, clusters[node2ClusID].clusIDs))
for each (node1ClusID ? nodes[node1Index].clusIDs)
for each (node2ClusID ? nodes[node2Index].clusIDs)
if( notIntersect(clusters[node1ClusID].confSet, clusters[node2ClusID].nodes) &&
notIntersect(clusters[node2ClusID].confSet, clusters[node1ClusID].nodes) )
clusters[node1ClusID].nodes ?
mergeSortedList(clusters[node1ClusID].nodes, clusters[node2ClusID].nodes)
clusters[node1ClusID].confSet ?
mergeSortedLists(clusters[node1ClusID].confSet, clusters[node2ClusID].confSet)
for each (nodeIndex ? clusters[node2ClusID].nodes)
nodes[nodeIndex].clusIDs.insert(node1ClusID)
clusters[node2ClusID].isRemoved ? true
clusters[node2ClusID].nodes ? null
clusters[node2ClusID].confSet ? null
Figure 7: Alg.4 - Clustering
76
(a) Block Dist. (iterations)
(b) Block Dist. (overall)
(c) Component & Cluster Size Dist.
Figure 8: Size Distributions
3 Evaluation
In this section, we present the experimental results for
our entity resolution framework. We ran the experiments
on a hadoop cluster consisting of 50 nodes, each with 8
cores. There are 10 mappers, and 6 reducers available
at each node. We also allocated 3 Gb memory for each
map/reduce task.
We used two different real-world datasets for our ex-
periments. The first one is a list of 150K organizations
along with their aliases provided by freebase2. By using
this dataset, we both trained our pairwise linkage model
and measured the precision and recall of our system. We
randomly selected 135K organizations from this list for
the training. We used the rest of the organizations to mea-
2http://www.freebase.com/
precision recall f-measure
Pairwise Classifier 97 63 76
Transitive Closure 64 98 77
sClust 95 76 84
Table 1: Performance Comparison
sure the performance of our system. Next, we generated
positive examples by exhaustively generating a pair be-
tween all the aliases. We also randomly generated equal
number of negative examples among pairs of different
organization alias sets. We trained our pairwise classi-
fier with the training set, then ran it on the test set and
measured its performance. Next, we extracted all the or-
ganization names from this set, and ran our entire entity
resolution pipeline on top of this set. Table 1 presents
the performance results. Our pairwise classifier has 97%
precision and 63% recall when we use a match threshold
of 0.65. Using same match threshold, we then performed
transitive closure. We also measured the precision and
recall numbers for transitive closure as it is the naive ap-
proach for the entity resolution problem. Since transitive
closure merges records transitively, it has very high recall
but the precision is just 64%. Finally, we performed our
sClust approach with the same match threshold. We set
the no-match threshold to 0.3. The pairwise classifier has
slightly better precision than sClust but sClust has much
better recall. Overall, sClust has a much better f-measure
than both the pairwise classifier and transitive closure.
Second, we used our production set to show the viabil-
ity of our framework. In this set, we have 68M organiza-
tion names. We ran our framework on this dataset. Block-
ing generated 14M unique blocks, and there are 842M
unique comparisons in these blocks. The distribution of
the block sizes presented in Figure 8-(a) and (b). Block-
ing finished in 42 minutes. Next, we ran our pairwise
classifier on these 842M pairs and it finished in 220 min-
utes. Finally, we ended up with 10M clusters at the end
of the clustering stage which took 3 hours. The distribu-
tion of the connected components and final clusters are
presented in Figure 8-(c).
4 Conclusion
In this paper, we presented a novel entity resolution ap-
proach for the organization entity domain. We have im-
plemented this in the MapReduce framework with low
memory requirements so that it may scale to large scale
datasets. We used two different real-world datasets in our
experiments. We first evaluated the performance of our
approach on truth data provided by freebase. Our cluster-
ing approach, sClust, significantly improved the recall of
the pairwise classifier. Next, we demonstrated the viabil-
ity of our framework on a large scale dataset on a 50-node
hadoop cluster.
77
References
A. K. Elmagarmid, P. G. Iperirotis, and V. S. Verykios. Dupli-
cate record detection: A survey. In IEEE Transactions on
Knowledge and Data Engineering, pages 1?16, 2007.
J. Lin and C. Dyer. Data-Intensive Text Processing with
MapReduce. Synthesis Lectures on Human Langugage
Technologies. Morgan & Claypool, 2010.
N. Bansal, A. Blum and S. Chawla Correlation Clustering. Ma-
chine Learning, 2003.
M. Michelson and S.A. Macskassy Record Linkage Measures
in an Entity Centric World. In Proceedings of the 4th work-
shop on Evaluation Methods for Machine Learning, Mon-
treal, Canada, 2009.
N. Adly. Efficient record linkage using a double embedding
scheme. In R. Stahlbock, S. F. Crone, and S. Lessmann, edi-
tors, DMIN, pages 274?281. CSREA Press, 2009.
A. N. Aizawa and K. Oyama. A fast linkage detection scheme
for multi-source information integration. In WIRI, pages 30?
39. IEEE Computer Society, 2005.
R. Baxter, P. Christen, and T. Churches. A comparison of fast
blocking methods for record linkage, 2003.
M. Bilenko and B. Kamath. Adaptive blocking: Learning to
scale up record linkage. In Data Mining, 2006. ICDM?, num-
ber December, 2006.
P. Christen. A survey of indexing techniques for scalable record
linkage and deduplication. IEEE Transactions on Knowl-
edge and Data Engineering, 99(PrePrints), 2011.
T. de Vries, H. Ke, S. Chawla, and P. Christen. Robust record
linkage blocking using suffix arrays. In Proceedings of the
18th ACM conference on Information and knowledge man-
agement, CIKM ?09, pages 305?314, New York, NY, USA,
2009. ACM.
T. de Vries, H. Ke, S. Chawla, and P. Christen. Robust record
linkage blocking using suffix arrays and bloom filters. ACM
Trans. Knowl. Discov. Data, 5(2):9:1?9:27, Feb. 2011.
M. A. Hernandez and S. J. Stolfo. Real-world data is dirty. data
cleansing and the merge/purge problem. Journal of Data
Mining and Knowledge Discovery, pages 1?39, 1998.
L. Jin, C. Li, and S. Mehrotra. Efficient record linkage in large
data sets. In Proceedings of the Eighth International Confer-
ence on Database Systems for Advanced Applications, DAS-
FAA ?03, pages 137?, Washington, DC, USA, 2003. IEEE
Computer Society.
A. McCallum, K. Nigam, and L. H. Ungar. Efficient clustering
of high-dimensional data sets with application to reference
matching. In Proceedings of the ACM International Confer-
ence on Knowledge Discover and Data Mining, pages 169?
178, 2000.
J. Nin, V. Muntes-Mulero, N. Martinez-Bazan, and J.-L.
Larriba-Pey. On the use of semantic blocking techniques for
data cleansing and integration. In Proceedings of the 11th
International Database Engineering and Applications Sym-
posium, IDEAS ?07, pages 190?198, Washington, DC, USA,
2007. IEEE Computer Society.
A. D. Sarma, A. Jain, and A. Machanavajjhala. CBLOCK:
An Automatic Blocking Mechanism for Large-Scale De-
duplication Tasks. Technical report, 2011.
M. Weis, F. Naumann, U. Jehle, J. Lufter, and H. Schuster.
Industry-scale duplicate detection. Proc. VLDB Endow.,
1(2):1253?1264, Aug. 2008.
S. E. Whang, D. Menestrina, G. Koutrika, M. Theobald, and
H. Garcia-Molina. Entity resolution with iterative blocking.
In Proceedings of the 35th SIGMOD international confer-
ence on Management of data, SIGMOD ?09, pages 219?232,
New York, NY, USA, 2009. ACM.
W. Winkler. Overview of record linkage and current research
directions. Technical report, U.S. Bureau of the Census,
2006.
S. Yan, D. Lee, M.-Y. Kan, and L. C. Giles. Adaptive sorted
neighborhood methods for efficient record linkage. In Pro-
ceedings of the 7th ACM/IEEE-CS joint conference on Dig-
ital libraries, JCDL ?07, pages 185?194, New York, NY,
USA, 2007. ACM.
L. Kolb, and E. Rahm. Parallel entity resolution with Dedoop.
Datenbank-Spektrum (2013): 1-10.
B. McNeill, H. Kardes, and A. Borthwick. Dynamic
Record Blocking: Efficient Linking of Massive Databases in
MapReduce. In QDB (2012).
78
