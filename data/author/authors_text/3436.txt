Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 829?838,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
N -gram Weighting: Reducing Training Data Mismatch
in Cross-Domain Language Model Estimation
Bo-June (Paul) Hsu, James Glass
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar Street, Cambridge, MA, 02139 USA
{bohsu,glass}@csail.mit.edu
Abstract
In domains with insufficient matched training
data, language models are often constructed
by interpolating component models trained
from partially matched corpora. Since the n-
grams from such corpora may not be of equal
relevance to the target domain, we propose
an n-gram weighting technique to adjust the
component n-gram probabilities based on fea-
tures derived from readily available segmen-
tation and metadata information for each cor-
pus. Using a log-linear combination of such
features, the resulting model achieves up to a
1.2% absolute word error rate reduction over a
linearly interpolated baseline language model
on a lecture transcription task.
1 Introduction
Many application domains in machine learning suf-
fer from a dearth of matched training data. However,
partially matched data sets are often available in
abundance. Past attempts to utilize the mismatched
data for training often result in models that exhibit
biases not observed in the target domain. In this
work, we will investigate the use of the often readily
available data segmentation and metadata attributes
associated with each corpus to reduce the effect of
such bias. We will examine this approach in the con-
text of language modeling for lecture transcription.
Compared with other types of audio data, lecture
speech often exhibits a high degree of spontaneity
and focuses on narrow topics with special termi-
nologies (Glass et al, 2004). While we may have
existing transcripts from general lectures or written
text on the precise topic, training data that matches
both the style and topic of the target lecture is often
scarce. Thus, past research has investigated various
adaptation and interpolation techniques to make use
of partially matched corpora (Bellegarda, 2004).
Training corpora are often segmented into docu-
ments with associated metadata, such as title, date,
and speaker. For lectures, if the data contains even
a few lectures on linear algebra, conventional lan-
guage modeling methods that lump the documents
together will tend to assign disproportionately high
probability to frequent terms like vector and matrix.
Can we utilize the segmentation and metadata infor-
mation to reduce the biases resulting from training
data mismatch?
In this work, we present such a technique where
we weight each n-gram count in a standard n-gram
language model (LM) estimation procedure by a rel-
evance factor computed via a log-linear combina-
tion of n-gram features. Utilizing features that cor-
relate with the specificity of n-grams to subsets of
the training documents, we effectively de-emphasize
out-of-domain n-grams. By interpolating models,
such as general lectures and course textbook, that
match the target domain in complementary ways,
and optimizing the weighting and interpolation pa-
rameters jointly, we allow each n-gram probabil-
ity to be modeled by the most relevant interpolation
component. Using a combination of features derived
from multiple partitions of the training documents,
the resulting weighted n-gram model achieves up
to a 1.2% absolute word error rate (WER) reduc-
tion over a linearly interpolated baseline on a lecture
transcription task.
829
2 Related Work
To reduce topic mismatch in LM estimation, we
(2006) have previously assigned topic labels to each
word by applying HMM-LDA (Griffiths et al, 2005)
to the training documents. Using an ad hoc method
to reduce the effective counts of n-grams ending
on topic words, we achieved better perplexity and
WER than standard trigram LMs. Intuitively, de-
emphasizing such n-grams will lower the transition
probability to out-of-domain topic words from the
training data. In this work, we further explore this
intuition with a principled feature-based model, in-
tegrated with LM smoothing and estimation to allow
simultaneous optimization of all model parameters.
As Gao and Lee (2000) observed, even purported
matched training data may exhibit topic, style, or
temporal biases not present in the test set. To ad-
dress the mismatch, they partition the training doc-
uments by their metadata attributes and compute a
measure of the likelihood that an n-gram will appear
in a new partitioned segment. By pruning n-grams
with generality probability below a given threshold,
the resulting model achieves lower perplexity than a
count-cutoff model of equal size. Complementary to
our work, this technique also utilizes segmentation
and metadata information. However, our model en-
ables the simultaneous use of all metadata attributes
by combining features derived from different parti-
tions of the training documents.
3 N -gram Weighting
Given a limited amount of training data, an n-gram
appearing frequently in a single document may be
assigned a disproportionately high probability. For
example, an LM trained from lecture transcripts
tends to assign excessive probability to words from
observed lecture topics due to insufficient coverage
of the underlying document topics. On the other
hand, excessive probabilities may also be assigned
to n-grams appearing consistently across documents
with mismatched style, such as the course textbook
in the written style. Traditional n-gram smoothing
techniques do not address such issues of insufficient
topic coverage and style mismatch.
One approach to addressing the above issues is
to weight the counts of the n-grams according to
the concentration of their document distributions.
Assigning higher weights to n-grams with evenly
spread distributions captures the style of a data set,
as reflected across all documents. On the other hand,
emphasizing the n-grams concentrated within a few
documents focuses the model on the topics of the
individual documents.
In theory, n-gram weighting can be applied to any
smoothing algorithm based on counts. However,
because many of these algorithms assume integer
counts, we will apply the weighting factors to the
smoothed counts, instead. For modified Kneser-Ney
smoothing (Chen and Goodman, 1998), applying n-
gram weighting yields:
p(w|h) = ?(hw)c?
?(hw)?
w ?(hw)c?(hw)
+ ?(h)p(w|h?)
where p(w|h) is the probability of word w given his-
tory h, c? is the adjusted Kneser-Ney count, c?? is the
discounted count, ? is the n-gram weighting factor,
? is the normalizing backoff weight, and h? is the
backoff history.
Although the weighting factor ? can in general be
any function of the n-gram, in this work, we will
consider a log-linear combination of n-gram fea-
tures, or ?(hw) = exp(?(hw) ? ?), where ?(hw)
is the feature vector for n-gram hw and ? specifies
the parameter vector to be learned. To better fit the
data, we allow independent parameter vectors ?o for
each n-gram order o. Note that with ?(hw) = 1, the
model degenerates to the original modified Kneser-
Ney formulation. Furthermore, ? only specifies the
relative weighting among n-grams with a common
history h. Thus, scaling ?(hw) by an arbitrary func-
tion g(h) has no effect on the model.
In isolation, n-gram weighting shifts probability
mass from out-of-domain n-grams via backoff to
the uniform distribution to improve the generality
of the resulting model. However, in combination
with LM interpolation, it can also distribute prob-
abilities to LM components that better model spe-
cific n-grams. For example, n-gram weighting can
de-emphasize off-topic and off-style n-grams from
general lectures and course textbook, respectively.
Tuning the weighting and interpolation parameters
jointly further allows the estimation of the n-gram
probabilities to utilize the best matching LM com-
ponents.
830
3.1 Features
To address the issue of sparsity in the document
topic distribution, we can apply n-gram weight-
ing with features that measure the concentration of
the n-gram distribution across documents. Simi-
lar features can also be computed from documents
partitioned by their categorical metadata attributes,
such as course and speaker for lecture transcripts.
Whereas the features derived from the corpus docu-
ments should correlate with the topic specificity of
the n-grams, the same features computed from the
speaker partitions might correspond to the speaker
specificity. By combining features from multiple
partitions of the training data to compute the weight-
ing factors, n-gram weighting allows the resulting
model to better generalize across categories.
To guide the presentation of the n-gram features
below, we will consider the following example parti-
tion of the training corpus. Words tagged by HMM-
LDA as topic words appear in bold.
A B A A C C A B A B
B A A C C B A A B A
A C B A A C A B B A
One way to estimate the specificity of an n-gram
across partitions is to measure the n-gram frequency
f , or the fraction of partitions containing an n-gram.
For instance, f(A) = 3/3, f(C) = 2/3. However,
as the size of each partition increases, this ratio in-
creases to 1, since most n-grams have a non-zero
probability of appearing in each partition. Thus,
an alternative is to compute the normalized entropy
of the n-gram distribution across the S partitions,
or h = ?1logS
?S
s=1 p(s) log p(s), where p(s) is the
fraction of an n-gram appearing in partition s. For
example, the normalized entropy of the unigram C is
h(C) = ?1log 3 [26 log 26 + 46 log 46 +0] = .58. N -grams
clustered in fewer partitions have lower entropy than
ones that are more evenly spread out.
Following (Hsu and Glass, 2006), we also con-
sider features derived from the HMM-LDA word
topic labels.1 Specifically, we compute the empir-
ical probability t that the target word of the n-gram
1HMM-LDA is performed using 20 states and 50 topics with
a 3rd-order HMM. Hyperparameters are sampled with a log-
normal Metropolis proposal. The model with the highest likeli-
hood from among 10,000 iterations of Gibbs sampling is used.
Feature of
the
ith
ink
km
ea
ns
the
su
n
thi
si
sa
al
ot
of
big
oo
f
em
f
Random 0.03 0.32 0.33 0.19 0.53 0.24 0.37 0.80
log(c) 9.29 8.09 3.47 5.86 6.82 7.16 3.09 4.92
fdoc 1.00 0.93 0.00 0.18 0.92 0.76 0.00 0.04
fcourse 1.00 1.00 0.06 0.56 0.94 0.94 0.06 0.06
f speaker 0.83 0.70 0.00 0.06 0.41 0.55 0.01 0.00
hdoc 0.96 0.84 0.00 0.56 0.93 0.85 0.00 0.34
hcourse 0.75 0.61 0.00 0.55 0.78 0.65 0.00 0.00
hspeaker 0.76 0.81 0.00 0.09 0.65 0.80 0.12 0.00
tdoc 0.00 0.00 0.91 1.00 0.01 0.00 0.00 0.04
tcourse 0.00 0.00 0.88 0.28 0.01 0.00 0.00 1.00
tspeaker 0.00 0.00 0.94 0.92 0.01 0.00 0.09 0.99
Table 1: A list of n-gram weighting features. f : n-gram
frequency, h: normalized entropy, t: topic probability.
is labeled as a topic word. In the example corpus,
t(C) = 3/6, t(A C) = 2/4.
All of the above features can be computed for any
partitioning of the training data. To better illustrate
the differences, we compute the features on a set of
lecture transcripts (see Section 4.1) partitioned by
lecture (doc), course, and speaker. Furthermore, we
include the log of the n-gram counts c and random
values between 0 and 1 as baseline features. Table 1
lists all the features examined in this work and their
values on a select subset of n-grams.
3.2 Training
To tune the n-gram weighting parameters ?, we ap-
ply Powell?s method (Press et al, 2007) to numeri-
cally minimize the development set perplexity (Hsu
and Glass, 2008). Although there is no guarantee
against converging to a local minimum when jointly
tuning both the n-gram weighting and interpolation
parameters, we have found that initializing the pa-
rameters to zero generally yields good performance.
4 Experiments
4.1 Setup
In this work, we evaluate the perplexity and WER of
various trigram LMs trained with n-gram weighting
on a lecture transcription task (Glass et al, 2007).
The target data consists of 20 lectures from an in-
troductory computer science course, from which we
withhold the first 10 lectures for the development
831
Dataset # Words # Sents # Docs
Textbook 131,280 6,762 271
Lectures 1,994,225 128,895 230
Switchboard 3,162,544 262,744 4,876
CS Dev 93,353 4,126 10
CS Test 87,527 3,611 10
Table 2: Summary of evaluation corpora.
Perplexity WER
Model Dev Test Dev Test
FixKN(1) 174.7 196.7 34.9% 36.8%
+ W(hdoc) 172.9 194.8 34.7% 36.7%
FixKN(3) 168.6 189.3 34.9% 36.9%
+ W(hdoc) 166.8 187.8 34.6% 36.6%
FixKN(10) 167.5 187.6 35.0% 37.2%
+ W(hdoc) 165.3 185.8 34.7% 36.8%
KN(1) 169.7 190.4 35.0% 37.0%
+ W(hdoc) 167.3 188.2 34.8% 36.7%
KN(3) 163.4 183.1 35.0% 37.1%
+ W(hdoc) 161.1 181.2 34.7% 36.8%
KN(10) 162.3 181.8 35.1% 37.1%
+ W(hdoc) 160.1 180.0 34.8% 36.8%
Table 3: Performance of n-gram weighting with a variety
of Kneser-Ney settings. FixKN(d): Kneser-Ney with d
fixed discount parameters. KN(d): FixKN(d) with tuned
values. W(feat): n-gram weighting with feat feature.
set (CS Dev) and use the last 10 for the test set
(CS Test). For training, we will consider the course
textbook with topic-specific vocabulary (Textbook),
numerous high-fidelity transcripts from a variety of
general seminars and lectures (Lectures), and the
out-of-domain LDC Switchboard corpus of spon-
taneous conversational speech (Switchboard) (God-
frey and Holliman, 1993). Table 2 summarizes all
the evaluation data.
To compute the word error rate, we use a speaker-
independent speech recognizer (Glass, 2003) with a
large-margin discriminative acoustic model (Chang,
2008). The lectures are pre-segmented into utter-
ances via forced alignment against the reference
transcripts (Hazen, 2006). Since all the models con-
sidered in this work can be encoded as n-gram back-
off models, they are applied directly during the first
recognition pass instead of through a subsequent n-
best rescoring step.
Model Perplexity WER
Lectures 189.3 36.9%
+ W(hdoc) 187.8 (-0.8%) 36.6%
Textbook 326.1 43.1%
+ W(hdoc) 317.5 (-2.6%) 43.1%
LI(Lectures + Textbook) 141.6 33.7%
+ W(hdoc) 136.6 (-3.5%) 32.7%
Table 4: N -gram weighting with linear interpolation.
4.2 Smoothing
In Table 3, we compare the performance of n-gram
weighting with the hdoc document entropy feature
for various modified Kneser-Ney smoothing config-
urations (Chen and Goodman, 1998) on the Lec-
tures dataset. Specifically, we considered varying
the number of discount parameters per n-gram order
from 1 to 10. The original and modified Kneser-Ney
smoothing algorithms correspond to a setting of 1
and 3, respectively. Furthermore, we explored using
both fixed parameter values estimated from n-gram
count statistics and tuned values that minimize the
development set perplexity.
In this task, while the test set perplexity tracks
the development set perplexity well, the WER corre-
lates surprisingly poorly with the perplexity on both
the development and test sets. Nevertheless, n-gram
weighting consistently reduces the absolute test set
WER by a statistically significant average of 0.3%,
according to the Matched Pairs Sentence Segment
Word Error test (Pallet et al, 1990). Given that we
obtained the lowest development set WER with the
fixed 3-parameter modified Kneser-Ney smoothing,
all subsequent experiments are conducted using this
smoothing configuration.
4.3 Linear Interpolation
Applied to the Lectures dataset in isolation, n-gram
weighting with the hdoc feature reduces the test set
WER by 0.3% by de-emphasizing the probability
contributions from off-topic n-grams and shifting
their weights to the backoff distributions. Ideally
though, such weights should be distributed to on-
topic n-grams, perhaps from other LM components.
In Table 4, we present the performance of apply-
ing n-gram weighting to the Lectures and Textbook
models individually versus in combination via linear
interpolation (LI), where we optimize the n-gram
832
Model Perplexity WER
LI(Lectures + Textbook) 141.6 33.7%
+ W(Random) 141.5 (-0.0%) 33.7%
+ W(log(c)) 137.5 (-2.9%) 32.8%
+ W(fdoc) 136.3 (-3.7%) 32.8%
+ W(fcourse) 136.5 (-3.6%) 32.7%
+ W(f speaker) 138.1 (-2.5%) 33.0%
+ W(hdoc) 136.6 (-3.5%) 32.7%
+ W(hcourse) F 136.1 (-3.9%) 32.7%
+ W(hspeaker) 138.6 (-2.1%) 33.1%
+ W(tdoc) 134.8 (-4.8%) 33.2%
+ W(tcourse) 136.4 (-3.6%) 33.1%
+ W(tspeaker) 136.4 (-3.7%) 33.2%
Table 5: N -gram weighting with various features.
weighting and interpolation parameters jointly. The
interpolated model with n-gram weighting achieves
perplexity improvements roughly additive of the re-
ductions obtained with the individual models. How-
ever, the 1.0% WER drop for the interpolated model
significantly exceeds the sum of the individual re-
ductions. Thus, as we will examine in more detail
in Section 5.1, n-gram weighting allows probabili-
ties to be shifted from less relevant n-grams in one
component to more specific n-grams in another.
4.4 Features
With n-gram weighting, we can model the weight-
ing function ?(hw) as a log-linear combination of
any n-gram features. In Table 5, we show the effect
various features have on the performance of linearly
interpolating Lectures and Textbook. As the docu-
ments from the Lectures dataset is annotated with
course and speaker metadata attributes, we include
the n-gram frequency f , entropy h, and topic proba-
bility t features computed from the lectures grouped
by the 16 unique courses and 299 unique speakers.2
In terms of perplexity, the use of the Random
feature has negligible impact on the test set per-
formance, as expected. On the other hand, the
log(c) count feature reduces the perplexity by nearly
3%, as it correlates with the generality of the n-
grams. By using features that leverage the infor-
mation from document segmentation and associated
2Features that are not applicable to a particular corpus (e.g.
hcourse for Textbook) are removed from the n-gram weighting
computation for that component. Thus, models with course and
speaker features have fewer tunable parameters than the others.
metadata, we are generally able to achieve further
perplexity reductions. Overall, the frequency and
entropy features perform roughly equally. However,
by considering information from the more sophisti-
cated HMM-LDA topic model, the topic probability
feature tdoc achieves significantly lower perplexity
than any other feature in isolation.
In terms of WER, the Random feature again
shows no effect on the baseline WER of 33.7%.
However, to our surprise, the use of the simple
log(c) feature achieves nearly the same WER im-
provement as the best segmentation-based feature,
whereas the more sophisticated features computed
from HMM-LDA labels only obtain half of the re-
duction even though they have the best perplexities.
When comparing the performance of different n-
gram weighting features on this data set, the per-
plexity correlates poorly with the WER, on both the
development and test sets. Fortunately, the features
that yield the lowest perplexity and WER on the de-
velopment set alo yield one of the lowest perplex-
ities and WERs, respectively, on the test set. Thus,
during feature selection for speech recognition ap-
plications, we should consider the development set
WER. Specifically, since the differences in WER
are often statistically insignificant, we will select the
feature that minimizes the sum of the development
set WER and log perplexity, or cross-entropy.3
In Tables 5 and 6, we have underlined the per-
plexities and WERs of the features with the lowest
corresponding development set values (not shown)
and bolded the lowest test set values. The features
that achieve the lowest combined cross-entropy and
WER on the development set are starred.
4.5 Feature Combination
Unlike most previous work, n-gram weighting en-
ables a systematic integration of features computed
from multiple document partitions. In Table 6, we
compare the performance of various feature combi-
nations. We experiment with incrementally adding
features that yield the lowest combined development
set cross-entropy and WER. Overall, this metric ap-
pears to better predict the test set WER than either
the development set perplexity or WER alone.
3The choice of cross-entropy instead of perplexity is par-
tially motivated by the linear correlation reported by (Chen and
Goodman, 1998) between cross-entropy and WER.
833
Features Perplexity WER
hcourse 136.1 32.7%
+ log(c) 135.4 (-0.5%) 32.6%
+ fdoc 135.1 (-0.7%) 32.6%
+ hdoc 135.6 (-0.5%) 32.6%
+ tdoc F 133.2 (-2.1%) 32.6%
+ fcourse 136.0 (-0.1%) 32.6%
+ tcourse 134.8 (-1.0%) 32.9%
+ f speaker 136.0 (-0.1%) 32.6%
+ hspeaker 136.1 (-0.0%) 32.8%
+ tspeaker 134.7 (-1.0%) 32.7%
hcourse + tdoc 133.2 32.6%
+ log(c) 132.8 (-0.3%) 32.5%
+ fdoc F 132.8 (-0.4%) 32.5%
+ hdoc 133.0 (-0.2%) 32.5%
+ fcourse 133.1 (-0.1%) 32.5%
+ tcourse 133.0 (-0.1%) 32.6%
+ f speaker 133.1 (-0.1%) 32.5%
+ hspeaker 133.2 (-0.0%) 32.6%
+ tspeaker 133.1 (-0.1%) 32.7%
Table 6: N -gram weighting with feature combinations.
Using the combined feature selection technique,
we notice that the greedily selected features tend to
differ in the choice of document segmentation and
feature type, suggesting that n-gram weighting can
effectively integrate the information provided by the
document metadata. By combining features, we are
able to further reduce the test set WER by a statis-
tically significant (p < 0.001) 0.2% over the best
single feature model.
4.6 Advanced Interpolation
While n-gram weighting with all three features is
able to reduce the test set WER by 1.2% over the
linear interpolation baseline, linear interpolation is
not a particularly effective interpolation technique.
In Table 7, we compare the effectiveness of n-gram
weighting in combination with better interpolation
techniques, such as count merging (CM) (Bacchi-
ani et al, 2006) and generalized linear interpolation
(GLI) (Hsu, 2007). As expected, the use of more
sophisticated interpolation techniques decreases the
perplexity and WER reductions achieved by n-gram
weighting by roughly half for a variety of feature
combinations. However, all improvements remain
statistically significant.
Model Perplexity WER
Linear(L + T) 141.6 33.7%
+ W(hcourse) 136.1 (-3.9%) 32.7%
+ W(tdoc) 133.2 (-5.9%) 32.6%
+ W(fdoc) 132.8 (-6.2%) 32.5%
CM(L + T) 137.9 33.0%
+ W(hcourse) 135.5 (-1.8%) 32.4%
+ W(tdoc) 133.4 (-3.3%) 32.4%
+ W(fdoc) 133.2 (-3.5%) 32.4%
GLIlog(1+c?)(L + T) 135.9 33.0%
+ W(hcourse) 133.0 (-2.2%) 32.4%
+ W(tdoc) 130.6 (-3.9%) 32.4%
+ W(fdoc) 130.5 (-4.2%) 32.4%
Table 7: Effect of interpolation technique. L: Lectures, T:
Textbook.
Feature Parameter Values
hdoc ?L = [3.42, 1.46, 0.12]
?T = [?0.45,?0.35,?0.73]
[?L, ?T] = [0.67, 0.33]
tdoc ?L = [?2.33,?1.63,?1.19]
?T = [1.05, 0.46, 0.12]
[?L, ?T] = [0.68, 0.32]
Table 8: N -gram weighting parameter values. ?L, ?T:
parameters for each order of the Lectures and Textbook
trigram models, ?L,?T: linear interpolation weights.
Although the WER reductions from better inter-
polation techniques are initially statistically signif-
icant, as we add features to n-gram weighting, the
differences among the interpolation methods shrink
significantly. With all three features combined, the
test set WER difference between linear interpolation
and generalized linear interpolation loses its statisti-
cal significance. In fact, we can obtain statistically
the same WER of 32.4% using the simpler model of
count merging and n-gram weighting with hcourse.
5 Analysis
5.1 Weighting Parameters
To obtain further insight into how n-gram weighting
improves the resulting n-gram model, we present in
Table 8 the optimized parameter values for the linear
interpolation model between Lectures and Textbook
using n-gram weighting with hdoc and tdoc features.
Using ?(hw) = exp(?(hw) ? ?) to model the n-
gram weights, a positive value of ?i corresponds to
834
100 300 1000 3000 10000
Development Set Size (Words)
132
134
136
138
140
142
144
P
e
r
p
l
e
x
i
t
y
LI
CM
GLI
LI+W
CM+W
GLI+W
Figure 1: Test set perplexity vs. development set size.
increasing the weights of the ith order n-grams with
positive feature values.
For the hdoc normalized entropy feature, values
close to 1 correspond to n-grams that are evenly dis-
tributed across the documents. When interpolating
Lectures and Textbook, we obtain consistently pos-
itive values for the Lectures component, indicating
a de-emphasis on document-specific terms that are
unlikely to be found in the target computer science
domain. On the other hand, the values correspond-
ing to the Textbook component are consistently neg-
ative, suggesting a reduced weight for mismatched
style terms that appear uniformly across textbook
sections.
For tdoc, values close to 1 correspond to n-grams
ending frequently on topic words with uneven dis-
tribution across documents. Thus, as expected, the
signs of the optimized parameter values are flipped.
By de-emphasizing topic n-grams from off-topic
components and style n-grams from off-style com-
ponents, n-gram weighting effectively improves the
performance of the resulting language model.
5.2 Development Set Size
So far, we have assumed the availability of a large
development set for parameter tuning. To obtain
a sense of how n-gram weighting performs with
smaller development sets, we randomly select utter-
ances from the full development set and plot the test
set perplexity in Figure 1 as a function of the devel-
opment set size for various modeling techniques.
As expected, GLI outperforms both LI and CM.
However, whereas LI and CM essentially converge
in test set perplexity with only 100 words of devel-
100 300 1000 3000 10000
Development Set Size (Words)
32.0
32.5
33.0
33.5
34.0
34.5
35.0
W
E
R
LI
CM
GLI
LI+W
CM+W
GLI+W
Figure 2: Test set WER vs. development set size.
opment data, it takes about 500 words before GLI
converges due to the increased number of parame-
ters. By adding n-gram weighting with the hcourse
feature, we see a significant drop in perplexity for
all models at all development set sizes. However,
the performance does not fully converge until 3,000
words of development set data.
As shown in Figure 2, the test set WER behaves
more erratically, as the parameters are tuned to min-
imize the development set perplexity. Overall, n-
gram weighting decreases the WER significantly,
except when applied to GLI with less than 1000
words of development data when the perplexity of
GLI has not itself converged. In that range, CM with
n-gram weighting performs the best. However, with
more development data, GLI with n-gram weight-
ing generally performs slightly better. From these
results, we conclude that although n-gram weight-
ing increases the number of tuning parameters, they
are effective in improving the test set performance
even with only 100 words of development set data.
5.3 Training Set Size
To characterize the effectiveness of n-gram weight-
ing as a function of the training set size, we evalu-
ate the performance of various interpolated models
with increasing subsets of the Lectures corpus and
the full Textbook corpus. Overall, every doubling of
the number of training set documents decreases both
the test set perplexity and WER by approximately 7
points and 0.8%, respectively. To better compare re-
sults, we plot the performance difference between
various models and linear interpolation in Figures 3
and 4.
835
2 4 8 16 32 64 128 230
Training Set Size (Documents)
-12
-10
-8
-6
-4
-2
0
2
4
P
e
r
p
l
e
x
i
t
y
 
D
i
f
f
e
r
e
n
c
e
LI
CM
GLI
LI+W
CM+W
GLI+W
Figure 3: Test set perplexity vs. training set size.
Interestingly, the peak gain obtained from n-gram
weighting with the hdoc feature appears at around
16 documents for all interpolation techniques. We
suspect that as the number of documents initially
increases, the estimation of the hdoc features im-
proves, resulting in larger perplexity reduction from
n-gram weighting. However, as the diversity of the
training set documents increases beyond a certain
threshold, we experience less document-level spar-
sity. Thus, we see decreasing gain from n-gram
weighting beyond 16 documents.
For all interpolation techniques, even though the
perplexity improvements from n-gram weighting
decrease with more documents, the WER reductions
actually increase. N -gram weighting showed sta-
tistically significant reductions for all configurations
except generalized linear interpolation with less than
8 documents. Although count merging with n-gram
weighting has the lowest WER for most training set
sizes, GLI ultimately achieves the best test set WER
with the full training set.
5.4 Training Corpora
In Table 9, we compare the performance of n-gram
weighting with different combination of training
corpora and interpolation techniques to determine
its effectiveness across different training conditions.
With the exception of interpolating Lectures and
Switchboard using count merging, all other model
combinations yield statistically significant improve-
ments with n-gram weighting using hcourse, tdoc,
and fdoc features.
The results suggest that n-gram weighting with
these features is most effective when interpolating
2 4 8 16 32 64 128 230
Training Set Size (Documents)
-1.4
-1.2
-1.0
-0.8
-0.6
-0.4
-0.2
0.0
0.2
W
E
R
 
D
i
f
f
e
r
e
n
c
e
LI
CM
GLI
LI+W
CM+W
GLI+W
Figure 4: Test set WER vs. training set size.
Model L + T L + S T + S L + T + S
LI 33.7% 36.7% 36.4% 33.6%
LI + W 32.5% 36.4% 35.7% 32.5%
CM 33.0% 36.6% 35.5% 32.9%
CM + W 32.4% 36.5% 35.4% 32.3%
GLI 33.0% 36.6% 35.7% 32.8%
GLI + W 32.4% 36.4% 35.3% 32.2%
Table 9: Test set WER with various training corpus com-
binations. L: Lectures, T: Textbook, S: Switchboard, W:
n-gram weighting.
corpora that differ in how they match the target do-
main. Whereas the Textbook corpus is the only cor-
pus with matching topic, both Lectures and Switch-
board have a similar matching spoken conversa-
tional style. Thus, we see the least benefit from
n-gram weighting when interpolating Lectures and
Switchboard. By combining Lectures, Textbook,
and Switchboard using generalized linear interpola-
tion with n-gram weighting using hcourse, tdoc, and
fdoc features, we achieve our best test set WER of
32.2% on the lecture transcription task, a full 1.5%
over the initial linear interpolation baseline.
6 Conclusion & Future Work
In this work, we presented the n-gram weighting
technique for adjusting the probabilities of n-grams
according to a set of features. By utilizing features
derived from the document segmentation and asso-
ciated metadata inherent in many training corpora,
we achieved up to a 1.2% and 0.6% WER reduc-
tion over the linear interpolation and count merging
baselines, respectively, using n-gram weighting on
a lecture transcription task.
836
We examined the performance of various n-gram
weighting features and generally found entropy-
based features to offer the best predictive perfor-
mance. Although the topic probability features
derived from HMM-LDA labels yield additional
improvements when applied in combination with
the normalized entropy features, the computational
cost of performing HMM-LDA may not justify the
marginal benefit in all scenarios.
In situations where the document boundaries are
unavailable or when finer segmentation is desired,
automatic techniques for document segmentation
may be applied (Malioutov and Barzilay, 2006).
Synthetic metadata information may also be ob-
tained via clustering techniques (Steinbach et al,
2000). Although we have primarily focused on n-
gram weighting features derived from segmentation
information, it is also possible to consider other fea-
tures that correlate with n-gram relevance.
N -gram weighting and other approaches to cross-
domain language modeling require a matched devel-
opment set for model parameter tuning. Thus, for
future work, we plan to investigate the use of the ini-
tial recognition hypotheses as the development set,
as well as manually transcribing a subset of the test
set utterances.
As speech and natural language applications shift
towards novel domains with limited matched train-
ing data, better techniques are needed to maximally
utilize the often abundant partially matched data. In
this work, we examined the effectiveness of the n-
gram weighting technique for estimating language
models in these situations. With similar investments
in acoustic modeling and other areas of natural lan-
guage processing, we look forward to an ever in-
creasing diversity of practical speech and natural
language applications.
Availability An implementation of the n-gram
weighting algorithm is available in the MIT Lan-
guage Modeling (MITLM) toolkit (Hsu and Glass,
2008): http://www.sls.csail.mit.edu/mitlm/.
Acknowledgments
We would like to thank the anonymous reviewers for
their constructive feedback. This research is sup-
ported in part by the T-Party Project, a joint research
program between MIT and Quanta Computer Inc.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochastic
grammars. Computer Speech & Language, 20(1):41?
68.
Jerome R. Bellegarda. 2004. Statistical language model
adaptation: Review and perspectives. Speech Commu-
nication, 42(1):93?108.
Hung-An Chang. 2008. Large margin Gaussian mix-
ture modeling for automatic speech recognition. Mas-
sachusetts Institute of Technology. Masters Thesis.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. In Technical Report TR-10-98. Computer Science
Group, Harvard University.
Jianfeng Gao and Kai-Fu Lee. 2000. Distribution-based
pruning of backoff language models. In Proc. Asso-
ciation of Computational Linguistics, pages 579?588,
Hong Kong, China.
James Glass, Timothy J. Hazen, Lee Hetherington, and
Chao Wang. 2004. Analysis and processing of lecture
audio data: Preliminary investigations. In Proc. HLT-
NAACL Workshop on Interdisciplinary Approaches to
Speech Indexing and Retrieval, pages 9?12, Boston,
MA, USA.
James Glass, Timothy J. Hazen, Scott Cyphers, Igor
Malioutov, David Huynh, and Regina Barzilay. 2007.
Recent progress in the MIT spoken lecture process-
ing project. In Proc. Interspeech, pages 2553?2556,
Antwerp, Belgium.
James Glass. 2003. A probabilistic framework for
segment-based speech recognition. Computer Speech
& Language, 17(2-3):137?152.
John J. Godfrey and Ed Holliman. 1993. Switchboard-1
transcripts. Linguistic Data Consortium, Philadelphia,
PA, USA.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In Advances in Neural Information Processing
Systems 17, pages 537?544. MIT Press, Cambridge,
MA, USA.
T.J. Hazen. 2006. Automatic alignment and error cor-
rection of human generated transcripts for long speech
recordings. In Proc. Interspeech, Pittsburgh, PA,
USA.
Bo-June (Paul) Hsu and James Glass. 2006. Style &
topic language model adaptation using HMM-LDA.
In Proc. Empirical Methods in Natural Language Pro-
cessing, pages 373?381, Sydney, Australia.
Bo-June (Paul) Hsu and James Glass. 2008. Iterative
language model estimation: Efficient data structure &
algorithms. In Proc. Interspeech, Brisbane, Australia.
837
Bo-June (Paul) Hsu. 2007. Generalized linear interpola-
tion of language models. In Proc. Automatic Speech
Recognition and Understanding, pages 136?140, Ky-
oto, Japan.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Proc.
Association for Computational Linguistics, pages 25?
32, Sydney, Australia.
D. Pallet, W. Fisher, and Fiscus. 1990. Tools for the anal-
ysis of benchmark speech recognition tests. In Proc.
ICASSP, Albuquerque, NM, USA.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical Recipes.
Cambridge University Press, 3rd edition.
Michael Steinbach, George Karypis, and Vipin Kumar.
2000. A comparison of document clustering tech-
niques. Technical Report #00-034, University of Min-
nesota.
838
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 373?381,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Style & Topic Language Model Adaptation Using HMM-LDA 
 
 
Bo-June (Paul) Hsu, James Glass 
MIT Computer Science and Artificial Intelligence Laboratory 
32 Vassar Street, Cambridge, MA 02139, USA 
{bohsu,glass}@mit.edu 
 
 
  
 
Abstract 
Adapting language models across styles 
and topics, such as for lecture transcrip-
tion, involves combining generic style 
models with topic-specific content rele-
vant to the target document.  In this 
work, we investigate the use of the Hid-
den Markov Model with Latent Dirichlet 
Allocation (HMM-LDA) to obtain syn-
tactic state and semantic topic assign-
ments to word instances in the training 
corpus.  From these context-dependent 
labels, we construct style and topic mod-
els that better model the target document, 
and extend the traditional bag-of-words 
topic models to n-grams.  Experiments 
with static model interpolation yielded a 
perplexity and relative word error rate 
(WER) reduction of 7.1% and 2.1%, re-
spectively, over an adapted trigram base-
line.  Adaptive interpolation of mixture 
components further reduced perplexity 
by 9.5% and WER by a modest 0.3%. 
1 Introduction 
With the rapid growth of audio-visual materials 
available over the web, effective language mod-
eling of the diverse content, both in style and 
topic, becomes essential for efficient access and 
management of this information.  As a prime 
example, successful language modeling for aca-
demic lectures not only enables the initial tran-
scription via automatic speech recognition, but 
also assists educators and students in the creation 
and navigation of these materials through annota-
tion, retrieval, summarization, and even transla-
tion of the embedded content. 
Compared with other types of audio content, 
lecture speech often exhibits a high degree of 
spontaneity and focuses on narrow topics with 
specific terminology (Furui, 2003; Glass et al 
2004).  Unfortunately, training corpora available 
for language modeling rarely match the target 
lecture in both style and topic.  While transcripts 
from other lectures better match the style of the 
target lecture than written text, it is often difficult 
to find transcripts on the target topic.  On the 
other hand, although topic-specific vocabulary 
can be gleaned from related text materials, such 
as the textbook and lecture slides, written lan-
guage is a poor predictor of how words are actu-
ally spoken.  Furthermore, given that the precise 
topic of a target lecture is often unknown a priori 
and may even shift over time, it is generally dif-
ficult to identify topically related documents.  
Thus, an effective language model (LM) need to 
not only account for the casual speaking style of 
lectures, but also accommodate the topic-specific 
vocabulary of the subject matter. Moreover, the 
ability of the language model to dynamically 
adapt over the course of the lecture could prove 
extremely useful for both increasing transcription 
accuracy, as well as providing evidence for lec-
ture segmentation and information retrieval. 
In this paper, we investigate the application of 
the syntactic state and semantic topic assign-
ments from the Hidden Markov Model with La-
tent Dirichlet Allocation model to the problem of 
language modeling.  We explore the use of these 
context-dependent labels to identify style and 
learn topics from both a large number of spoken 
lectures as well as written text.  By dynamically 
interpolating lecture style models with topic-
specific models, we obtain language models that 
better describe the subtopic structure within a 
lecture.  Initial experiments demonstrate a 16.1% 
perplexity reduction and a 2.4% WER reduction 
over an adapted trigram baseline. 
373
In the following sections, we first summarize 
related research on adaptive and topic-mixture 
language models, and describe previous work on 
the HMM-LDA model.  We then examine the 
ability of the model to learn syntactic classes as 
well as topics from textbook materials and lec-
ture transcripts.  Next, we describe a variety of 
language model experiments we performed to 
combine style and topic models constructed from 
the state and topic labels with conventional tri-
gram models trained from both spoken and writ-
ten materials.  We also demonstrate the use of 
the combined model in an on-line adaptive mode.  
Finally, we summarize the results of this research 
and suggest future opportunities for related mod-
eling techniques in spoken lecture and other con-
tent processing research. 
2 Adaptive and Topic-Mixture LMs 
The concept of adaptive and topic-mixture lan-
guage models has been previously explored by 
many researchers.  Adaptive language modeling 
exploits the property that words appearing earlier 
in a document are likely to appear again. Cache 
language models (Kuhn and De Mori, 1990; 
Clarkson and Robinson, 1997) leverage this ob-
servation and increase the probability of previ-
ously observed words in a document when pre-
dicting the next word. By interpolating with a 
conditional trigram cache model, Goodman 
(2001) demonstrated up to 34% decrease in per-
plexity over a trigram baseline for small training 
sets. 
The cache intuition has been extended by at-
tempting to increase the probability of unob-
served but topically related words.  Specifically, 
given a mixture model with topic-specific com-
ponents, we can increase the mixture weights of 
the topics corresponding to previously observed 
words to better predict the next word.  Some of 
the early work in this area used a maximum en-
tropy language model framework to trigger in-
creases in likelihood of related words (Lau et al, 
1993; Rosenfeld, 1996). 
A variety of methods has been used to explore 
topic-mixture models.  To model a mixture of 
topics within a document, the sentence mixture 
model (Iyer and Ostendorf, 1999) builds multiple 
topic models from clusters of training sentences 
and defines the probability of a target sentence as 
a weighted combination of its probability under 
each topic model.  Latent Semantic Analysis 
(LSA) has been used to cluster topically related 
words and has demonstrated significant reduc-
tion in perplexity and word error rate (Belle-
garda, 2000).  Probabilistic LSA (PLSA) has 
been used to decompose documents into compo-
nent word distributions and create unigram topic 
models from these distributions.  Gildea and 
Hofmann (1999) demonstrated noticeable per-
plexity reduction via dynamic combination of 
these unigram topic models with a generic tri-
gram model. 
To identify topics from an unlabeled corpus, 
(Blei et al, 2003) extends PLSA with the Latent 
Dirichlet Allocation (LDA) model that describes 
each document in a corpus as generated from a 
mixture of topics, each characterized by a word 
unigram distribution. Hidden Markov Model 
with LDA (HMM-LDA) (Griffiths et al, 2004) 
further extends this topic mixture model to sepa-
rate syntactic words from content words whose 
distributions depend primarily on local context 
and document topic, respectively. 
In the specific area of lecture processing, pre-
vious work in language model adaptation has 
primarily focused on customizing a fixed n-gram 
language model for each lecture by combining n-
gram statistics from general conversational 
speech, other lectures, textbooks, and other re-
sources related to the target lecture (Nanjo and 
Kawahara, 2002, 2004; Leeuwis et al, 2003; 
Park et al, 2005). 
Most of the previous work on topic-mixture 
models focuses on in-domain adaptation using 
large amounts of matched training data.  How-
ever, most, if not all, of the data available to train 
a lecture language model are either cross-domain 
or cross-style.  Furthermore, although adaptive 
models have been shown to yield significant per-
plexity reduction on clean transcripts, the im-
provements tend to diminish when working with 
speech recognizer hypotheses with high WER. 
In this work, we apply the concept of dynamic 
topic adaptation to the lecture transcription task.  
Unlike previous work, we first construct a style 
model and a topic-domain model using the clas-
sification of word instances into syntactic states 
and topics provided by HMM-LDA.  Further-
more, we leverage the context-dependent labels 
to extend topic models from unigrams to n-
grams, allowing for better prediction of transi-
tions involving topic words.  Note that although 
this work focuses on the use of HMM-LDA to 
generate the state and topic labels, any method 
that yields such labels suffices for the purpose of 
the language modeling experiments.  The follow-
ing section describes the HMM-LDA framework 
in more detail. 
374
3 HMM-LDA 
3.1 Latent Dirichlet Allocation 
Discrete Principal Component Analysis describes 
a family of models that decompose a set of fea-
ture vectors into its principal components (Bun-
tine and Jakulin, 2005).  Describing feature vec-
tors via their components reduces the number of 
parameters required to model the data, hence im-
proving the quality of the estimated parameters 
when given limited training data.  LSA, PLSA, 
and LDA are all examples from this family. 
Given a predefined number of desired compo-
nents, LSA models feature vectors by finding a 
set of orthonormal components that maximize 
the variance using singular value decomposition 
(Deerwester et al, 1990).  Unfortunately, the 
component vectors may contain non-interpret-
able negative values when working with word 
occurrence counts as feature vectors.  PLSA 
eliminates this problem by using non-negative 
matrix factorization to model each document as a 
weighted combination of a set of non-negative 
feature vectors (Hofmann, 1999).  However, be-
cause the number of parameters grows linearly 
with the number of documents, the model is 
prone to overfitting.  Furthermore, because each 
training document has its own set of topic weight 
parameters, PLSA does not provide a generative 
framework for describing the probability of an 
unseen document (Blei et al, 2003). 
To address the shortcomings of PLSA, Blei et 
al. (2003) introduced the LDA model, which fur-
ther imposes a Dirichlet distribution on the topic 
mixture weights corresponding to the documents 
in the corpus.  With the number of model pa-
rameters dependent only on the number of topic 
mixtures and vocabulary size, LDA is less prone 
to overfitting and is capable of estimating the 
probability of unobserved test documents. 
Empirically, LDA has been shown to outper-
form PLSA in corpus perplexity, collaborative 
filtering, and text classification experiments (Blei 
et al, 2003).  Various extensions to the basic 
LDA model have since been proposed.  The Au-
thor Topic model adds an additional dependency 
on the author(s) to the topic mixture weights of 
each document (Rosen-Zvi et al, 2005).  The 
Hierarchical Dirichlet Process is a nonparametric 
model that generalizes distribution parameter 
modeling to multiple levels.  Without having to 
estimate the number of mixture components, this 
model has been shown to match the best result 
from LDA on a document modeling task (Teh et 
al., 2004). 
3.2 Hidden Markov Model with LDA 
HMM-LDA model proposed by Griffiths et al 
(2004) combines the HMM and LDA models to 
separate syntactic words with local dependencies 
from topic-dependent content words without re-
quiring any labeled data.  Similar to HMM-based 
part-of-speech taggers, HMM-LDA maps each 
word in the document to a hidden syntactic state.  
Each state generates words according to a uni-
gram distribution except the special topic state, 
where words are modeled by document-specific 
mixtures of topic distributions, as in LDA.  
Figure 1 describes this generative process in 
more detail. 
Figure 1: Generative framework and graphical 
model representation of HMM-LDA.  The num-
ber of states and topics are pre-specified.  The 
topic mixture for each document is modeled with 
a Dirichlet distribution.  Each word wi in the n-
word document is generated from its hidden state 
si or hidden topic zi if si is the special topic state. 
 
Unlike vocabulary selection techniques that 
separate domain-independent words from topic-
specific keywords using word collocation statis-
tics, HMM-LDA classifies each word instance 
according to its context.  Thus, an instance of the 
word ?return? may be assigned to a syntactic 
state in ?to return a?, but classified as a topic 
keyword in ?expected return for?.  By labeling 
each word in the training set with its syntactic 
state and mixture topic, HMM-LDA not only 
separates stylistic words from content words in a 
context-dependent manner, but also decomposes 
the corpus into a set of topic word distributions.  
This form of soft, context-dependent classifica-
For each document d in the corpus: 
1. Draw topic weights d?  from )(Dirichlet ?  
2. For each word wi in document d: 
a. Draw topic zi from )l(Multinomia d?  
b. Draw state si from )(ultinomialM 1?ispi  
c. Draw word wi from: 



=
otherwise
s
i
i
s
i
z
)(lMultinomia
)(lMultinomia
?
? topics
 
 
 
D
  d
 
z1 w1 
z2 w2 
zn wn 
s1 
s2 
sn 
? ? ? 
 
375
tion has many potential uses for language model-
ing, topic segmentation, and indexing. 
3.3 Training 
To train an HMM-LDA model, we employ the 
MATLAB Topic Modeling Toolbox 1.3 (Grif-
fiths and Steyvers, 2004; Griffiths et al, 2004).  
This particular implementation performs Gibbs 
sampling, a form of Markov chain Monte Carlo 
(MCMC), to estimate the optimal model parame-
ters fitted to the training data.  Specifically, the 
algorithm creates a Markov chain whose station-
ary distribution matches the expected distribution 
of the state and topic labels for each word in the 
training corpus.  Starting from random labels, 
Gibbs sampling sequentially samples the label 
for each hidden variable conditioned on the cur-
rent value of all other variables.  After a suffi-
cient number of iterations, the Markov chain 
converges to the stationary distribution.  We can 
easily compute the posterior word distribution 
for each state and topic from a single sample by 
averaging over the label counts and prior pa-
rameters.  With a sufficiently large training set, 
we will have enough words assigned to each 
state and topic to yield a reasonable approxima-
tion to the underlying distribution. 
In the following sections, we examine the ap-
plication of models derived from the HMM-LDA 
labels to the task of spoken lecture transcription 
and explore techniques on adaptive topic model-
ing to construct a better lecture language model. 
4 HMM-LDA Analysis 
Our language modeling experiments have been 
conducted on high-fidelity transcripts of ap-
proximately 168 hours of lectures from three un-
dergraduate subjects in math, physics, and com-
puter science (CS), as well as 79 seminars cover-
ing a wide range of topics (Glass et al, 2004).  
For evaluation, we withheld the set of 20 CS lec-
tures and used the first 10 lectures as a develop-
ment set and the last 10 lectures for the test set.  
The remainder of these data was used for training 
and will be referred to as the Lectures dataset. 
To supplement the out-of-domain lecture tran-
scripts with topic-specific textual resources, we 
added the CS course textbook (Textbook) as ad-
ditional training data for learning the target top-
ics.  To create topic-cohesive documents, the 
textbook is divided at every section heading to 
form 271 documents.  Next, the text is heuristi-
cally segmented at sentence-like boundaries and 
normalized into the words corresponding to the 
spoken form of the text.  Table 1 summarizes the 
data used in this evaluation. 
 
Dataset Documents Sentences Vocabulary Words 
Lectures 150 58,626 25,654 1,390,039 
Textbook 271 6,762 4,686 131,280 
CS Dev 10 4,102 3,285 93,348 
CS Test 10 3,595 3,357 87,518 
Table 1: Summary of evaluation datasets. 
 
In the following analysis, we ran the Gibbs 
sampler against the Lectures dataset for a total of 
2800 iterations, computing a model every 10 it-
erations, and took the model with the lowest per-
plexity as the final model.  We built the model 
with 20 states and 100 topics based on prelimi-
nary experiments.  We also trained an HMM-
LDA model on the Textbook dataset using the 
same model parameters.  We ran the sampler for 
a total of 2000 iterations, computing the perplex-
ity every 100 iterations.  Again, we selected the 
lowest perplexity model as the final model. 
4.1 Semantic Topics 
HMM-LDA extracts words whose distributions 
vary across documents and clusters them into a 
set of components.  In Figure 2, we list the top 
10 words from a random selection of 10 topics 
computed from the Lectures dataset.  As shown, 
the words assigned to the LDA topic state are 
representative of content words and are grouped 
into broad semantic topics.  For example, topic 4, 
8, and 9 correspond to machine learning, linear 
algebra, and magnetism, respectively. 
Since the Lectures dataset consists of speech 
transcripts with disfluencies, it is interesting to 
1 2 3 4 5 6 7 8 9 10 
center 
world 
and 
ideas 
new 
technology 
innovation 
community 
place 
building 
work 
research 
right 
people 
computing 
network 
system 
information 
software 
computers 
rights 
human 
U. 
S. 
government 
international 
countries 
president 
world 
support 
system 
things 
robot 
systems 
work 
example 
person 
robots 
learning 
machine 
<laugh> 
her 
children 
book 
Cambridge 
books 
street 
city 
library 
brother 
<partial> 
memory 
ah 
brain 
animal 
okay 
eye 
synaptic 
receptors 
mouse 
class 
people 
tax 
wealth 
social 
American 
power 
world 
<unintelligible> 
society 
basis 
v 
<eh> 
vector 
matrix 
transformation 
linear 
eight 
output 
t 
magnetic 
current 
field 
loop 
surface 
direction 
e 
law 
flux 
m 
light 
red 
water 
colors 
white 
angle 
blue 
here 
rainbow 
sun 
 
Figure 2: The top 10 words from 10 randomly selected topics computed from the Lectures dataset. 
376
observe that ?<laugh>? is the top word in a 
topic corresponding to childhood memories.  
Cursory examination of the data suggests that the 
speakers talking about children tend to laugh 
more during the lecture.  Although it may not be 
desirable to capture speaker idiosyncrasies in the 
topic mixtures, HMM-LDA has clearly demon-
strated its ability to capture distinctive semantic 
topics in a corpus.  By leveraging all documents 
in the corpus, the model yields smoother topic 
word distributions that are less vulnerable to 
overfitting. 
Since HMM-LDA labels the state and topic of 
each word in the training corpus, we can also 
visualize the results by color-coding the words 
by their topic assignments.  Figure 3 shows a 
color-coded excerpt from a topically coherent 
paragraph in the Textbook dataset.  Notice how 
most of the content words (uppercase) are as-
signed to the same topic/color.  Furthermore, of 
the 7 instances of the words ?and? and ?or? 
(underlined), 6 are correctly classified as syntac-
tic or topic words, demonstrating the context-
dependent labeling capabilities of the HMM-
LDA model.  Moreover, from these labels, we 
can identify multi-word topic key phrases (e.g. 
output signals, input signal, ?and? gate) in addi-
tion to standalone keywords, an observation we 
will leverage later on with n-gram topic models. 
 
 
Figure 3: Color-coded excerpt from the Textbook 
dataset showing the context-dependent topic la-
bels.  Syntactic words appear black in lowercase.  
Topic words are shown in uppercase with their 
respective topic colors.  All instances of the 
words ?and? and ?or? are underlined. 
4.2 Syntactic States 
Since the syntactic states are shared across all 
documents, we expect words associated with the 
syntactic states when applying HMM-LDA to the 
Lectures dataset to reflect the lecture style vo-
cabulary.   
In Figure 4, we list the top 10 words from each 
of the 19 syntactic states (state 20 is the topic 
state).  Note that each state plays a clear syntactic 
role.  For example, state 2 contains prepositions 
while state 7 contains verbs.  Since the model is 
trained on transcriptions of spontaneous speech, 
hesitation disfluencies (<uh>, <um>, <partial>) 
are all grouped in state 3 along with other words 
(so, if, okay) that frequently indicate hesitation.  
While many of these hesitation words are con-
junctions, the words in state 6 show that most 
conjunctions are actually assigned to a different 
state representing different syntactic behavior 
from hesitations.  As demonstrated with sponta-
neous speech, HMM-LDA yields syntactic states 
that have a good correspondence to part-of-
speech labels, without requiring any labeled 
training data. 
4.3 Discussions 
Although MCMC techniques converge to the 
global stationary distribution, we cannot guaran-
tee convergence from observation of the perplex-
ity alone.  Unlike EM algorithms, random sam-
pling may actually temporarily decrease the 
model likelihood.  Thus, in the above analysis, 
the number of iterations was chosen to be at least 
double the point at which the perplexity first ap-
peared to converge. 
In addition to the number of iterations, the 
choice of the number of states and topics, as well 
as the values of the hyper-parameters on the 
Dirichlet prior, also impact the quality and effec-
tiveness of the resulting model.  Ideally, we run 
the algorithm with different combinations of the 
parameter values and perform model selection to 
choose the model with the best complexity-
penalized likelihood.  However, given finite 
computing resources, this approach is often im-
We draw an INVERTER SYMBOLICALLY as in Figure 3.24.  
An AND GATE, also shown in Figure 3.24, is a PRIMITIVE 
FUNCTION box with two INPUTS and ONE OUTPUT.  It 
drives its OUTPUT SIGNAL to a value that is the LOGICAL 
AND of the INPUTS.  That is, if both of its INPUT SIGNALS 
BECOME 1.  Then ONE and GATE DELAY time later the AND 
GATE will force its OUTPUT SIGNAL TO be 1; otherwise the 
OUTPUT will be 0.  An OR GATE is a SIMILAR two INPUT 
PRIMITIVE FUNCTION box that drives its OUTPUT SIGNAL 
to a value that is the LOGICAL OR of the INPUTS.  That is, the 
OUTPUT will BECOME 1 if at least ONE of the INPUT 
SIGNALS is 1; otherwise the OUTPUT will BECOME 0. 
 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 
the 
this 
a 
that 
these 
my 
our 
your 
those 
their 
of 
in 
for 
on 
with 
at 
from 
by 
about 
as 
so 
<uh> 
if 
<um> 
<partial> 
now 
then 
okay 
well 
but 
know 
see 
do 
think 
go 
get 
say 
make 
look 
take 
I 
you 
we 
they 
let 
let's 
he 
I'll 
people 
I'd 
and 
but 
or 
because 
as 
that 
where 
thank 
which 
is 
is 
are 
was 
has 
were 
goes 
had 
comes 
means 
says 
it's 
not 
that's 
I'm 
just 
there's 
<uh> 
we're 
also 
you're 
it 
you 
out 
up 
them 
that 
me 
about 
here 
all 
a 
an 
some 
one 
no 
in 
two 
any 
this 
another 
way 
time 
thing 
lot 
question 
kind 
point 
case 
idea 
problem 
it 
this 
that 
there 
which 
he 
here 
course 
who 
they 
two 
one 
three 
hundred 
m 
t 
five 
d 
years 
four 
going 
doing 
one 
looking 
sort 
done 
able 
coming 
talking 
trying 
that 
what 
how 
where 
when 
if 
why 
which 
as 
because 
can 
will 
would 
don't 
could 
do 
just 
me 
should 
may 
very 
more 
little 
much 
good 
different 
than 
important 
long 
as 
to 
just 
longer 
doesn't 
never 
go 
physically 
that'll 
anybody's 
with 
have 
be 
want 
had 
get 
like 
got 
need 
try 
take 
Figure 4: The top 10 words from the 19 syntactic states computed from the Lectures dataset. 
377
practical.  As an alternative for future work, we 
would like to perform Gibbs sampling on the 
hyper-parameters (Griffiths et al, 2004) and ap-
ply the Dirichlet process to estimate the number 
of states and topics (Teh et al, 2004). 
Despite the suboptimal choice of parameters 
and potential lack of convergence, the labels de-
rived from HMM-LDA are still effective for lan-
guage modeling applications, as described next. 
5 Language Modeling Experiments 
To evaluate the effectiveness of models derived 
from the separation of syntax from content, we 
performed experiments that compare the per-
plexities and WERs of various model combina-
tions.  For a baseline, we used an adapted model 
(L+T) that linearly interpolates trigram models 
trained on the Lectures (L) and Textbook (T) 
datasets.  In all models, all interpolation weights 
and additional parameters are tuned on a devel-
opment set consisting of the first half of the CS 
lectures and tested on the second half.  Unless 
otherwise noted, modified Kneser-Ney discount-
ing (Chen and Goodman, 1998) is applied with 
the respective training set vocabulary using the 
SRILM Toolkit (Stolcke, 2002). 
To compute the word error rates associated 
with a specific language model, we used a 
speaker-independent speech recognizer (Glass, 
2003).  The lectures were pre-segmented into 
utterances by forced alignment of the reference 
transcription. 
5.1 Lecture Style 
In general, an n-gram model trained on a limited 
set of topic-specific documents tends to overem-
phasize words from the observed topics instead 
of evenly distributing weights over all potential 
topics.  Specifically, given the list of words fol-
lowing an n-gram context, we would like to 
deemphasize the observed occurrences of topic 
words and ideally redistribute these counts to all 
potential topic words.  As an approximation, we 
can build such a topic-deemphasized style tri-
gram model (S) by using counts of only n-gram 
sequences that do not end on a topic word, 
smoothed over the Lectures vocabulary.  Figure 
5 shows the n-grams corresponding to an utter-
ance used to build the style trigram model.  Note 
that the counts of topic to style word transitions 
are not altered as these probabilities are mostly 
independent of the observed topic distribution. 
By interpolating the style model (S) from 
above with the smoothed trigram model based on 
the Lectures dataset (L), the combined model 
(L+S) achieves a 3.6% perplexity reduction and 
1.0% WER reduction over (L), as shown in Table 
2.  Without introducing topic-specific training 
data, we can already improve the generic lecture 
LM performance using the HMM-LDA labels. 
 
<s> for the SPATIAL MEMORY </s> 
unigrams: for, the, spatial, memory, </s> 
bigrams: <s> for, for the, the spatial, spatial memory, memory </s> 
trigrams: <s> <s> for, <s> for the, for the spatial, 
 the spatial memory, spatial memory </s> 
Figure 5: Style model n-grams.  Topic words in 
the utterance are in uppercase.   
5.2 Topic Domain 
Unlike Lectures, the Textbook dataset contains 
content words relevant to the target lectures, but 
in a mismatched style.  Commonly, the Textbook 
trigram model is interpolated with the generic 
model to improve the probability estimates of the 
transitions involving topic words.  The interpola-
tion weight is chosen to best fit the probabilities 
of these n-gram sequences while minimizing the 
mismatch in style.  However, with only one pa-
rameter, all n-gram contexts must share the same 
mixture weight.  Because transitions from con-
texts containing topic words are rarely observed 
in the off-topic Lectures, the Textbook model (T) 
should ideally have higher weight in these con-
texts than contexts that are more equally ob-
served in both datasets. 
One heuristic approach for adjusting the 
weight in these contexts is to build a topic-
domain trigram model (D) from the Textbook n-
gram counts with Witten-Bell smoothing (Chen 
and Goodman, 1998) where we emphasize the 
sequences containing a topic word in the context 
by doubling their counts.  In effect, this reduces 
the smoothing on words following topic contexts 
with respect to lower-order models without sig-
nificantly affecting the transitions from non-topic 
words.  Figure 6 shows the adjusted counts for an 
utterance used to build the domain trigram 
model.   
 
<s> HUFFMAN CODE can be represented as a BINARY TREE ? 
unigrams: huffman, code, can, be, represented, as, binary, tree, ? 
bigrams: <s> huffman, huffman code (2?), code can (2?),  
 can be, be represented, represented as, a binary,  
 binary tree (2?), ? 
trigrams: <s> <s> hufmann, <s> hufmann code (2?),  
 hufmann code can (2?), code can be (2?),  
 can be represented, be represented as,  
 represented as a, as a binary, a binary tree (2?), ... 
Figure 6: Domain model n-grams.  Topic words 
in the utterance are in uppercase. 
378
Empirically, interpolating the lectures, text-
book, and style models with the domain model 
(L+T+S+D) further decreases the perplexity by 
1.4% and WER by 0.3% over (L+T+S), validat-
ing our intuition.  Overall, the addition of the 
style and domain models reduces perplexity and 
WER by a noticeable 7.1% and 2.1%, respec-
tively, as shown in Table 2. 
 
 Perplexity 
Model Development Test 
L: Lectures Trigram 180.2 (0.0%) 199.6 (0.0%) 
T: Textbook Trigram 291.7 (+61.8%) 331.7 (+66.2%) 
S: Style Trigram 207.0 (+14.9%) 224.6 (+12.5%) 
D: Domain Trigram 354.1 (+96.5%) 411.6 (+106.3%) 
L+S 174.2 (?3.3%) 192.4 (?3.6%) 
L+T: Baseline 138.3 (0.0%) 154.4 (0.0%) 
L+T+S 131.0 (?5.3%) 145.6 (?5.7%) 
L+T+S+D 128.8 (?6.9%) 143.6 (?7.1%) 
L+T+S+D+Topic100 
? Static Mixture (cheat) 
? Dynamic Mixture 
 
118.1 (?14.6%) 
115.7 (?16.4%) 
 
131.3 (?15.0%) 
129.5 (?16.1%) 
 
 Word Error Rate 
Model Development Test 
L: Lectures Trigram 49.5% (0.0%) 50.2% (0.0%) 
L+S 49.2% (?0.7%) 49.7% (?1.0%) 
L+T: Baseline 46.6% (0.0%) 46.7% (0.0%) 
L+T+S 46.0% (?1.2%) 45.8% (?1.8%) 
L+T+S+D 45.8% (?1.8%) 45.7% (?2.1%) 
L+T+S+D+Topic100 
? Static Mixture (cheat) 
? Dynamic Mixture 
 
45.5% (?2.4%) 
45.4% (?2.6%) 
 
45.4% (?2.8%) 
45.6% (?2.4%) 
 
Table 2: Perplexity (top) and WER (bottom) per-
formance of various model combinations.  Rela-
tive reduction is shown in parentheses. 
5.3 Textbook Topics 
In addition to identifying content words, HMM-
LDA also assigns words to a topic based on their 
distribution across documents.  Thus, we can 
apply HMM-LDA with 100 topics to the Text-
book dataset to identify representative words and 
their associated contexts for each topic.  From 
these labels, we can build unsmoothed trigram 
language models (Topic100) for each topic from 
the counts of observed n-gram sequences that 
end in a word assigned to the respective topic. 
Figure 7 shows a sample of the word n-grams 
identified via this approach for a few topics.  
Note that some of the n-grams are key phrases 
for the topic while others contain a mixture of 
syntactic and topic words.  Unlike bag-of-words 
models that only identify the unigram distribu-
tion for each topic, the use of context-dependent 
labels enables the construction of n-gram topic 
models that not only characterize the frequencies 
of topic words, but also describe the transition 
contexts leading up to these words. 
 
Huffman tree 
relative frequency 
relative frequencies 
the tree 
one hundred 
Monte Carlo 
rand update 
random numbers 
trials remaining 
trials passed 
time segment 
the agenda 
segment time 
current time 
first agenda 
assoc key 
the table 
local table 
a table 
of records 
Figure 7: Sample of n-grams from select topics. 
5.4 Topic Mixtures 
Since each target lecture generally only covers a 
subset of the available topics, it will be ideal to 
identify the specific topics corresponding to a 
target lecture and assign those topic models more 
weight in a linearly interpolated mixture model.  
As an ideal case, we performed a cheating ex-
periment to measure the best performance of a 
statically interpolated topic mixture model 
(L+T+S+D+Topic100) where we tuned the 
mixture weights of all mixture components, in-
cluding the lectures, textbook, style, domain, and 
the 100 individual topic trigram models on indi-
vidual target lectures.   
Table 2 shows that by weighting the compo-
nent models appropriately, we can reduce the 
perplexity and WER by an additional 7.9% and 
0.7%, respectively, over the (L+T+S+D) model 
even with simple linear interpolation for model 
combination. 
To gain further insight into the topic mixture 
model, we examine the breakdown of the nor-
malized topic weights for a specific lecture.  As 
shown in Figure 8, of the 100 topic models, 15 of 
them account for over 90% of the total weight.  
Thus, lectures tend to show a significant topic 
skew which topic adaptation approaches can 
model effectively. 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
 
Figure 8: Topic mixture weight breakdown. 
5.5 Topic Adaptation 
Unfortunately, since different lectures cover dif-
ferent topics, we generally cannot tune the topic 
mixture weights ahead of time.  One approach, 
without any a priori knowledge of the target lec-
ture, is to adaptively estimate the optimal mix-
ture weights as we process the lecture (Gildea 
and Hofmann, 1999).  However, since the topic 
distribution shifts over a long lecture, modeling a 
lecture as an interpolation of components with 
fixed weights may not be the most optimal.  In-
stead, we employ an exponential decay strategy 
where we update the current mixture distribution 
by linearly interpolating it with the posterior 
topic distribution given the current word.  Spe-
cifically, applying Bayes? rule, the probability of 
topic t generating the current word w is given by: 
379
( ) ( ) ( )( ) ( ) ???= t tPtwP
tPtwP
wtP |
||  
To achieve the exponential decay, we update the 
topic distribution after each word according to 
Pi+1(t) = (1 ?   )Pi(t) +   P(t | wi), where    is the 
adaptation rate. 
We evaluated this approach of dynamic mix-
ture weight adaptation on the (L+T+S+D+Topic 
100) model, with the same set of components as 
the cheating experiment with static weights.  As 
shown in Table 2, the dynamic model actually 
outperforms the static model by more than 1% in 
perplexity, by better modeling the dynamic topic 
substructure within the lecture. 
To run the recognizer with a dynamic LM, we 
rescored the top 100 hypotheses generated with 
the (L+T+S+D) model using the dynamic LM.  
The WER obtained through such n-best rescoring 
yielded noticeable improvements over the 
(L+T+S+D) model without a priori knowledge 
of the topic distribution, but did not beat the op-
timal static model on the test set.   
To further gain an intuition for mixture weight 
adaptation, we plotted the normalized adapted 
weights of the topic models across the first lec-
ture of the test set in Figure 9.  Note that the 
topic mixture varies greatly across the lecture.  In 
this particular lecture, the lecturer starts out with 
a review of the previous lecture.  Subsequently, 
he shows an example of computation using ac-
cumulators.  Finally, he focuses the lecture on 
stream as a data structure, with an intervening 
example that finds pairs of i and j that sum up to 
a prime.  By comparing the topic labels in Figure 
9 with the top words from the corresponding top-
ics in Figure 10, we observe that the topic 
weights obtained via dynamic adaptation match 
the subject matter of the lecture fairly closely. 
Finally, to assess the effect that word error rate 
has on adaptation performance, we applied the 
adaptation algorithm to the corresponding tran-
script from the automatic speech recognizer 
(ASR).  Traditional cache language models tend 
to be vulnerable to recognition errors since incor-
rect words in the history negatively bias the pre-
diction of the current word.  However, by adapt-
ing at a topic level, which reduces the number of 
dynamic parameters, the dynamic topic model is 
less sensitive to recognition errors.  As seen in 
Figure 9, even with a word error rate around 
40%, the normalized topic mixture weights from 
the ASR transcript still show a strong resem-
blance to the original weights from the manual 
reference transcript.  
 
Figure 9: Adaptation of topic model weights on 
manual and ASR transcription of a single lecture. 
 
T12 T35 T98 T99 
stream 
s 
streams 
integers 
series 
prime 
filter 
delayed 
interleave 
infinite 
pairs 
i 
j 
k 
pair 
s 
integers 
sum 
queens 
t 
sequence 
enumerate 
accumulate 
map 
interval 
filter 
sequences 
operations 
odd 
nil 
of 
see 
and 
in 
for 
vs 
register 
data 
as 
make 
Figure 10: Top 10 words from select Textbook 
topics appearing in Figure 9. 
6 Summary and Conclusions 
In this paper, we have shown how to leverage 
context-dependent state and topic labels, such as 
the ones generated by the HMM-LDA model, to 
construct better language models for lecture tran-
scription and extend topic models beyond tradi-
tional unigrams.  Although the WER of the top 
recognizer hypotheses exceeds 45%, by dynami-
cally updating the mixture weights to model the 
topic substructure within individual lectures, we 
are able to reduce the test set perplexity and 
WER by over 16% and 2.4%, respectively, rela-
tive to the combined Lectures and Textbook 
(L+T) baseline. 
Although we primarily focused on lecture 
transcription in this work, the techniques extend 
to language modeling scenarios where exactly 
matched training data are often limited or non-
existent.  Instead, we have to rely on appropriate 
combination of models derived from partially 
matched data.  HMM-LDA and related tech-
niques show great promise for finding structure 
in unlabeled data, from which we can build more 
sophisticated models. 
The experiments in this paper combine models 
primarily through simple linear interpolation.  As 
motivated in section 5.2, allowing for context-
dependent interpolation weights based on topic 
380
labels may yield significant improvement for 
both perplexity and WER.  Thus, in future work, 
we would like to study algorithms for automati-
cally learning appropriate context-dependent in-
terpolation weights.  Furthermore, we hope to 
improve the convergence properties of the dy-
namic adaptation scheme at the start of lectures 
and across topic transitions.  Lastly, we would 
like to extend the LDA framework to support 
speaker-specific adaptation and apply the result-
ing topic distributions to lecture segmentation. 
Acknowledgements 
We would like to thank the anonymous review-
ers for their useful comments and feedback.  
Support for this research was provided in part by 
the National Science Foundation under grant 
#IIS-0415865.   Any opinions, findings, and con-
clusions, or recommendations expressed in this 
material are those of the authors and do not nec-
essarily reflect the views of the NSF. 
Reference 
Y. Akita and T. Kawahara.  2004.  Language Model 
Adaptation Based on PLSA of Topics and Speak-
ers.  In Proc. ICSLP. 
J. Bellegarda.  2000.  Exploiting Latent Semantic In-
formation in Statistical Language Modeling.  In 
Proc. IEEE, 88(8):1279-1296. 
D. Blei, A. Ng, and M. Jordan.  1993.  Latent 
Dirichlet Allocation.  Journal of Machine Learning 
Research, 3:993-1022. 
W. Buntine and A. Jakulin.  2005.  Discrete Principal 
Component Analysis.  Technical Report, Helsinki 
Institute for Information Technology. 
S. Chen and J. Goodman.  1996.  An Empirical Study 
of Smoothing Techniques for Language Modeling.  
In Proc. ACL, 310-318. 
P. Clarkson and A. Robinson.  1997.  Language 
Model Adaptation Using Mixtures and an Expo-
nentially Decaying Cache.  In Proc. ICASSP.   
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, R. 
Harshman.  1990.  Indexing by Latent Semantic 
Analysis.  Journal of the American Society for In-
formation Science, 41(6):391-407. 
S. Furui.  2003.  Recent Advances in Spontaneous 
Speech Recognition and Understanding.  In Proc. 
IEEE Workshop on Spontaneous Speech Proc. and 
Rec, 1-6. 
D. Gildea and T. Hofmann.  1999.  Topic-Based Lan-
guage Models Using EM.  In Proc. Eurospeech. 
J. Glass.  2003.  A Probabilistic Framework for Seg-
ment-based Speech Recognition.  Computer, Speech 
and Language, 17:137-152. 
J. Glass, T.J. Hazen, L. Hetherington, and C. Wang.  
2004.  Analysis and Processing of Lecture Audio 
Data: Preliminary Investigations.  In Proc. HLT-
NAACL Workshop on Interdisciplinary Approaches 
to Speech Indexing and Retrieval, 9-12. 
J. Goodman.  2001.  A Bit of Progress in Language 
Modeling (Extended Version).  Technical Report, 
Microsoft Research. 
T. Griffiths and M. Steyvers.  2004.  Finding Scien-
tific Topics.  In Proc. National Academy of Sci-
ence, 101(Suppl. 1):5228-5235. 
T. Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum.  
2004.  Integrating Topics and Syntax.  Adv. in Neu-
ral Information Processing Systems, 17:537-544. 
R. Iyer and M. Ostendorf.  1999.  Modeling Long 
Distance Dependence in Language: Topic Mixtures 
Versus Dynamic Cache.  In IEEE Transactions on 
Speech and Audio Processing, 7:30-39. 
R. Kuhn and R. De Mori.  1990.  A Cache-Based 
Natural Language Model for Speech Recognition.  
In IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 12:570-583. 
R. Lau, R. Rosenfeld, S. Roukos.  1993.  Trigger-
Based Language Models: a Maximum Entropy 
Approach.  In Proc. ICASSP. 
E. Leeuwis, M. Federico, and M. Cettolo.  2003.  Lan-
guage Modeling and Transcription of the TED 
Corpus Lectures.  In Proc. ICASSP. 
H. Nanjo and T. Kawahara.  2002.  Unsupervised 
Language Model Adaptation for Lecture Speech 
Recognition.  In Proc. ICSLP. 
H. Nanjo and T. Kawahara.  2004.  Language Model 
and Speaking Rate Adaptation for Spontaneous 
Presentation Speech Recognition.  In IEEE Trans. 
SAP, 12(4):391-400. 
A. Park, T. Hazen, and J. Glass.  2005.  Automatic 
Processing of Audio Lectures for Information Re-
trieval: Vocabulary Selection and Language Mod-
eling.  In Proc. ICASSP. 
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. 
Smyth.  2004.  The Author-Topic Model for Au-
thors and Documents.  20th Conference on Uncer-
tainty in Artificial Intelligence. 
R. Rosenfeld.  1996.  A Maximum Entropy Approach 
to Adaptive Statistical Language Modeling.  Com-
puter, Speech and Language, 10:187-228. 
A. Stolcke.  2002.  SRILM ? An Extensible Language 
Modeling Toolkit.  In Proc. ICSLP. 
Y. Teh, M. Jordan, M. Beal, and D.  Blei.  2006.  Hi-
erarchical Dirichlet Processes.  To appear in Jour-
nal of the American Statistical Association. 
381
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 1?9,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Multimodal Home Entertainment Interface via a Mobile Device
Alexander Gruenstein Bo-June (Paul) Hsu James Glass Stephanie Seneff
Lee Hetherington Scott Cyphers Ibrahim Badr Chao Wang Sean Liu
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar St, Cambridge, MA 02139 USA
http://www.sls.csail.mit.edu/
Abstract
We describe a multimodal dialogue system for
interacting with a home entertainment center
via a mobile device. In our working proto-
type, users may utilize both a graphical and
speech user interface to search TV listings,
record and play television programs, and listen
to music. The developed framework is quite
generic, potentially supporting a wide variety
of applications, as we demonstrate by integrat-
ing a weather forecast application. In the pro-
totype, the mobile device serves as the locus
of interaction, providing both a small touch-
screen display, and speech input and output;
while the TV screen features a larger, richer
GUI. The system architecture is agnostic to
the location of the natural language process-
ing components: a consistent user experience
is maintained regardless of whether they run
on a remote server or on the device itself.
1 Introduction
People have access to large libraries of digital con-
tent both in their living rooms and on their mobile
devices. Digital video recorders (DVRs) allow peo-
ple to record TV programs from hundreds of chan-
nels for subsequent viewing at home?or, increas-
ingly, on their mobile devices. Similarly, having
accumulated vast libraries of digital music, people
yearn for an easy way to sift through them from the
comfort of their couches, in their cars, and on the go.
Mobile devices are already central to accessing
digital media libraries while users are away from
home: people listen to music or watch video record-
ings. Mobile devices also play an increasingly im-
portant role in managing digital media libraries. For
instance, a web-enabled mobile phone can be used to
remotely schedule TV recordings through a web site
or via a custom application. Such management tasks
often prove cumbersome, however, as it is challeng-
ing to browse through listings for hundreds of TV
channels on a small display. Indeed, even on a large
screen in the living room, browsing alphabetically,
or by time and channel, for a particular show using
the remote control quickly becomes unwieldy.
Speech and multimodal interfaces provide a nat-
ural means of addressing many of these challenges.
It is effortless for people to say the name of a pro-
gram, for instance, in order to search for existing
recordings. Moreover, such a speech browsing ca-
pability is useful both in the living room and away
from home. Thus, a natural way to provide speech-
based control of a media library is through the user?s
mobile device itself.
In this paper we describe just such a prototype
system. A mobile phone plays a central role in pro-
viding a multimodal, natural language interface to
both a digital video recorder and a music library.
Users can interact with the system?presented as a
dynamic web page on the mobile browser?using
the navigation keys, the stylus, or spoken natural
language. In front of the TV, a much richer GUI is
also available, along with support for playing video
recordings and music.
In the prototype described herein, the mobile de-
vice serves as the locus of natural language in-
teraction, whether a user is in the living room or
walking down the street. Since these environments
may be very different in terms of computational re-
1
sources and network bandwidth, it is important that
the architecture allows for multiple configurations in
terms of the location of the natural language pro-
cessing components. For instance, when a device
is connected to a Wi-Fi network at home, recogni-
tion latency may be reduced by performing speech
and natural language processing on the home me-
dia server. Moreover, a powerful server may enable
more sophisticated processing techniques, such as
multipass speech recognition (Hetherington, 2005;
Chung et al, 2004), for improved accuracy. In sit-
uations with reduced network connectivity, latency
may be improved by performing speech recognition
and natural language processing tasks on the mobile
device itself. Given resource constraints, however,
less detailed acoustic and language models may be
required. We have developed just such a flexible ar-
chitecture, with many of the natural language pro-
cessing components able to run on either a server or
the mobile device itself. Regardless of the configu-
ration, a consistent user experience is maintained.
2 Related Work
Various academic researchers and commercial busi-
nesses have demonstrated speech-enabled interfaces
to entertainment centers. A good deal of the work
focuses on adding a microphone to a remote con-
trol, so that speech input may be used in addition
to a traditional remote control. Much commercial
work, for example (Fujita et al, 2003), tends to fo-
cus on constrained grammar systems, where speech
input is limited to a small set of templates corre-
sponding to menu choices. (Berglund and Johans-
son, 2004) present a remote-control based speech
interface for navigating an existing interactive tele-
vision on-screen menu, though experimenters man-
ually transcribed user utterances as they spoke in-
stead of using a speech recognizer. (Oh et al, 2007)
present a dialogue system for TV control that makes
use of concept spotting and statistical dialogue man-
agement to understand queries. A version of their
system can run independently on low-resource de-
vices such as PDAs; however, it has a smaller vo-
cabulary and supports a limited set of user utterance
templates. Finally, (Wittenburg et al, 2006) look
mainly at the problem of searching for television
programs using speech, an on-screen display, and a
remote control. They explore a Speech-In List-Out
interface to searching for episodes of television pro-
grams.
(Portele et al, 2003) depart from the model of
adding a speech interface component to an exist-
ing on-screen menu. Instead, they a create a tablet
PC interface to an electronic program guide, though
they do not use the television display as well. Users
may search an electronic program guide using con-
straints such as date, time, and genre; however, they
can?t search by title. Users can also perform typi-
cal remote-control tasks like turning the television
on and off, and changing the channel. (Johnston et
al., 2007) also use a tablet PC to provide an inter-
face to television content?in this case a database of
movies. The search can be constrained by attributes
such as title, director, or starring actors. The tablet
PC pen can be used to handwrite queries and to point
at items (such as actor names) while the user speaks.
We were also inspired by previous prototypes in
which mobile devices have been used in conjunc-
tion with larger, shared displays. For instance, (Paek
et al, 2004) demonstrate a framework for building
such applications. The prototype we demonstrate
here fits into their ?Jukebox? model of interaction.
Interactive workspaces, such as the one described in
(Johanson et al, 2002), also demonstrate the utility
of integrating mobile and large screen displays. Our
prototype is a departure from these systems, how-
ever, in that it provides for spoken interactions.
Finally, there is related work in the use of mobile
devices for various kinds of search. For instance, of-
ferings from Microsoft (Acero et al, 2008), Vlingo,1
and Promptu2 allow users to search for items like
businesses and songs using their mobile phones.
These applications differ from ours in that speech
is used only for search, without any accompanying
command and control capabilities. Also, these ser-
vices do not allow interaction with your own de-
vices at home. Efforts have been made to use mo-
bile devices for control of devices in the home, such
as in (Nichols and Myers, 2006), however these ef-
forts have not involved the use of speech as an input
modality.
1http://www.vlingo.com
2http://www.promptu.com
2
Na
vig
ati
on
 pa
d o
r s
tyl
us 
can
 
be
 us
ed
 to
 na
vig
ate
 G
UI
Sta
tus
 ic
on
 in
dic
ate
s c
on
ne
cti
on
 to
 se
rve
r, 
mi
cro
ph
on
e i
co
n w
he
n l
ist
en
ing
Ho
ld-
to-
tal
k 
bu
tto
n
Op
era we
b 
bro
ws
er
Sy
nc
hro
niz
ed
 se
lec
tio
n o
n m
ob
ile
 an
d 
tel
ev
isi
on
 di
spl
ay
s
Fir
efo
x
we
b b
row
ser
Ta
bs 
all
ow
 ea
sy 
sw
itc
hin
g 
am
on
g d
om
ain
s
Me
dia
 pl
ay
ba
ck
 
co
ntr
ols
Figure 1: User interface overview.
(a) Program Guide
(b) Scheduled Recordings
(c) Music Library
U: What?s on tonight at seven o?clock?
S: I found seventy entries. [Displays program guide.]
U: Are there any action movies on HBO tomorrow?
S: I found two entries. [Displays results.]
U: When is American Idol on?
S: I found three entries. [Displays results.]
U: Record the first one.
S: I have added American Idol to the list of programs to be
recorded. [Displays recording settings.]
U: Please record Nova on Wednesday night.
S: I have added Nova to the list of programs to be recorded.
[Displays recording settings.]
U: Show all my scheduled recordings.
S: There are seven scheduled recordings. [Shows list.]
U: Delete BBC World News.
S: I have deleted the scheduled recording: BBC World
News. [Updates List.]
U: Show my videos.
S: [Displays list of videos.]
U: Delete Frasier.
S: I have deleted the recording Frasier. [Updates list.]
U: Play Grey?s Anatomy.
S: Playing Grey?s Anatomy. [Plays recording.]
U: Stop Playing.
S: Stopping. [Stops playback.]
U: [Clicks on Music tab.]
S: [Displays artist list.]
U: Show albums by Billie Holiday.
S: I found three albums by Billie Holiday. [Shows albums.]
U: Please play A Hard Rain?s A-Gonna Fall by Bob Dylan.
S: Playing A Hard Rain?s A-Gonna Fall. [Plays song.]
Figure 2: Screenshots and an example interaction. Utterances are labeled with U for user and S for system.
3
3 User Experience
Our current prototype system implements the basic
functionalities that one expects from a home enter-
tainment center. Users can navigate through and
record programs from the television?s electronic pro-
gram guide, manage recording settings, and play
recorded videos. They can also browse and listen to
selections from their music libraries. However, un-
like existing prototypes, ours employs a smartphone
with a navigation pad, touch-sensitive screen, and
built-in microphone as the remote control. Figure 1
provides an overview of the graphical user interface
on both the TV and mobile device.
Mirroring the TV?s on-screen display, the proto-
type system presents a reduced view on the mobile
device with synchronized cursors. Users can navi-
gate the hierarchical menu structure using the arrow
keys or directly click on the target item with the sty-
lus. While away from the living room, or when a
recording is playing full screen, users can browse
and manage their media libraries using only the mo-
bile device.
While the navigation pad and stylus are great for
basic navigation and control, searching for media
with specific attributes, such as title, remains cum-
bersome. To facilitate such interactions, the cur-
rent system supports spoken natural language inter-
actions. For example, the user can press the hold-to-
talk button located on the side of the mobile device
and ask ?What?s on the National Geographic Chan-
nel this afternoon?? to retrieve a list of shows with
the specified channel and time. The system responds
with a short verbal summary ?I found six entries on
January seventh? and presents the resulting list on
both the TV and mobile displays. The user can then
browse the list using the navigation pad or press the
hold-to-talk button to barge in with another com-
mand, e.g. ?Please record the second one.? Depress-
ing the hold-to-talk button not only terminates any
current spoken response, but also mutes the TV to
minimize interference with speech recognition. As
the previous example demonstrates, contextual in-
formation is used to resolve list position references
and disambiguate commands.
The speech interface to the user?s music library
works in a similar fashion. Users can search by
artist, album, and song name, and then play the
songs found. To demonstrate the extensibility of
the architecture, we have also integrated an exist-
ing weather information system (Zue et al, 2000),
which has been previously deployed as a telephony
application. Users simply click on the Weather tab
to switch to this domain, allowing them to ask a wide
range of weather queries. The system responds ver-
bally and with a simple graphical forecast.
To create a natural user experience, we designed
the multimodal interface to allow users to seam-
lessly switch among the different input modalities
available on the mobile device. Figure 2 demon-
strates an example interaction with the prototype, as
well as several screenshots of the user interface.
4 System Architecture
The system architecture is quite flexible with re-
gards to the placement of the natural language pro-
cessing components. Figure 3 presents two possible
configurations of the system components distributed
across the mobile device, home media server, and
TV display. In 3(a), all speech recognition and nat-
ural language processing components reside on the
server, with the mobile device acting as the micro-
phone, speaker, display, and remote control. In 3(b),
the speech recognizer, language understanding com-
ponent, language generation component, and text-
to-speech (TTS) synthesizer run on the mobile de-
vice. Depending on the capabilities of the mobile
device and network connection, different configu-
rations may be optimal. For instance, on a power-
ful device with slow network connection, recogni-
tion latency may be reduced by performing speech
recognition and natural language processing on the
device. On the other hand, streaming audio via a fast
wireless network to the server for processing may
result in improved accuracy.
In the prototype system, flexible and reusable
speech recognition and natural language processing
capabilities are provided via generic components de-
veloped and deployed in numerous spoken dialogue
systems by our group, with the exception of an off-
the-shelf speech synthesizer. Speech input from the
mobile device is recognized using the landmark-
based SUMMIT system (Glass, 2003). The result-
ing N-best hypotheses are processed by the TINA
language understanding component (Seneff, 1992).
4
Gala
xy
Spee
ch?R
ecog
nize
r
Lang
uage
?Und
ersta
ndin
g
Dialo
gue?
Man
ager
Lang
uage
?Gen
erat
ion
Text
?To?S
peec
h
Web
?
Serv
er
Tele
visio
n Web
?Bro
wse
r
Med
ia?Pl
ayer
TV?G
uide
?
Med
ia
Mob
ile?D
evic
e
Web
?Bro
wse
r
Hom
e?M
edia
?Serv
er
Wea
ther
Mob
ile?
Man
ager
Aud
io?In
put?
/?Ou
tput
Gala
xy
Dialo
gue?
Man
ager
Web
?
Serv
er Tele
visio
n Web
?Bro
wse
r
Med
ia?Pl
ayer
TV?G
uide
?
Med
ia
Mob
ile?D
evic
e
Web
?Bro
wse
r
Hom
e?M
edia
?Serv
er
Wea
ther
Mob
ile?
Man
ager
Aud
io?In
put?
/?Ou
tput
Spee
ch?R
ecog
nize
r
Lang
uage
?Gen
erat
ion
Text
?To?S
peec
h
Lang
uage
?Und
ersta
ndin
g
(a) (b)
Figure 3: Two architecture diagrams. In (a) speech recognition and natural language processing occur on the server,
while in (b) processing is primarily performed on the device.
Based on the resulting meaning representation, the
dialogue manager (Polifroni et al, 2003) incorpo-
rates contextual information (Filisko and Seneff,
2003), and then determines an appropriate response.
The response consists of an update to the graph-
ical display, and a spoken system response which
is realized via the GENESIS (Baptist and Seneff,
2000) language generation module. To support on-
device processing, all the components are linked via
the GALAXY framework (Seneff et al, 1998) with
an additional Mobile Manager component responsi-
ble for coordinating the communication between the
mobile device and the home media server.
In the currently deployed system, we use a mo-
bile phone with a 624 MHz ARM processor run-
ning the Windows Mobile operating system and
Opera Mobile web browser. The TV program and
music databases reside on the home media server
running GNU/Linux. The TV program guide data
and recording capabilities are provided via MythTV,
a full-featured, open-source digital video recorder
software package.3 Daily updates to the program
guide information typically contain hundreds of
unique channel names and thousands of unique pro-
gram names. The music library is comprised of
5,000 songs from over 80 artists and 13 major gen-
res, indexed using the open-source text search en-
gine Lucene.4 Lastly, the TV display can be driven
by a web browser on either the home media server or
a separate computer connected to the server via a fast
Ethernet connection, for high quality video stream-
ing.
3http://www.mythtv.org/
4http://lucene.apache.org/
While the focus of this paper is on the natural lan-
guage processing and user interface aspects of the
system, our work is actually situated within a larger
collaborative project at MIT that also includes sim-
plified device configuration (Mazzola Paluska et al,
2008; Mazzola Paluska et al, 2006), transparent ac-
cess to remote servers (Ford et al, 2006), and im-
proved security.
5 Mobile Natural Language Components
Porting the implementation of the various speech
recognizer and natural language processing com-
ponents to mobile devices with limited computa-
tion and memory presents both a research and en-
gineering challenge. Instead of creating a small vo-
cabulary, fixed phrase dialogue system, we aim to
support?on the mobile device?the same flexible
and natural language interactions currently available
on our desktop, tablet, and telephony systems; see
e.g., (Gruenstein et al, 2006; Seneff, 2002; Zue et
al., 2000). In this section, we summarize our ef-
forts thus far in implementing the SUMMIT speech
recognizer and TINA natural language parser. Ports
of the GENESIS language generation system and of
our dialogue manager are well underway, and we ex-
pect to have these components working on the mo-
bile device in the near future.
5.1 PocketSUMMIT
To significantly reduce the memory footprint and
overall computation, we chose to reimplement our
segment-based speech recognizer from scratch, uti-
lizing fixed-point arithmetic, parameter quantiza-
tion, and bit-packing in the binary model files.
The resulting PocketSUMMIT recognizer (Hether-
5
ington, 2007) utilizes only the landmark features,
initially forgoing segment features such as phonetic
duration, as they introduce algorithmic complexities
for relatively small word error rate (WER) improve-
ments.
In the current system, we quantize the mean and
variance of each Gaussian mixture model dimension
to 5 and 3 bits, respectively. Such quantization not
only results in an 8-fold reduction in model size, but
also yields about a 50% speedup by enabling table
lookups for Gaussian evaluations. Likewise, in the
finite-state transducers (FSTs) used to represent the
language model, lexical, phonological, and class di-
phone constraints, quantizing the FST weights and
bit-packing not only compress the resulting binary
model files, but also reduce the processing time with
improved processor cache locality.
In the aforementioned TV, music, and weather do-
mains with a moderate vocabulary of a few thou-
sand words, the resulting PocketSUMMIT recog-
nizer performs in approximately real-time on 400-
600 MHz ARM processors, using a total of 2-4
MB of memory, including 1-2 MB for memory-
mapped model files. Compared with equivalent non-
quantized models, PocketSUMMIT achieves dra-
matic improvements in speed and memory while
maintaining comparable WER performance.
5.2 PocketTINA
Porting the TINA natural language parser to mobile
devices involved significant software engineering to
reduce the memory and computational requirements
of the core data structures and algorithms. TINA
utilizes a best-first search that explores thousands of
partial parses when processing an input utterance.
To efficiently manage memory allocation given the
unpredictability of pruning invalid parses (e.g. due
to subject-verb agreement), we implemented a mark
and sweep garbage collection mechanism. Com-
bined with a more efficient implementation of the
priority queue and the use of aggressive ?beam?
pruning, the resulting PocketTINA system provides
identical output as server-side TINA, but can parse
a 10-best recognition hypothesis list into the corre-
sponding meaning representation in under 0.1 sec-
onds, using about 2 MB of memory.
6 Rapid Dialogue System Development
Over the course of developing dialogue systems for
many domains, we have built generic natural lan-
guage understanding components that enable the
rapid development of flexible and natural spoken di-
alogue systems for novel domains. Creating such
prototype systems typically involves customizing
the following to the target domain: recognizer lan-
guage model, language understanding parser gram-
mar, context resolution rules, dialogue management
control script, and language generation rules.
Recognizer Language Model Given a new do-
main, we first identify a set of semantic classes
which correspond to the back-end application?s
database, such as artist, album, and genre. Ideally,
we would have a corpus of tagged utterances col-
lected from real users. However, when building pro-
totypes such as the one described here, little or no
training data is usually available. Thus, we create
a domain-specific context-free grammar to generate
a supplemental corpus of synthetic utterances. The
corpus is used to train probabilities for the natural
language parsing grammar (described immediately
below), which in turn is used to derive a class n-
gram language model (Seneff et al, 2003).
Classes in the language model which corre-
spond to contents of the database are marked as
dynamic, and are populated at runtime from the
database (Chung et al, 2004; Hetherington, 2005).
Database entries are heuristically normalized into
spoken forms. Pronunciations not in our 150,000
word lexicon are automatically generated (Seneff,
2007).
Parser Grammar The TINA parser uses a prob-
abilistic context-free grammar enhanced with sup-
port for wh-movement and grammatical agreement
constraints. We have developed a generic syntac-
tic grammar by examining hundreds of thousands
of utterances collected from real user interactions
with various existing dialogue systems. In addition,
we have developed libraries which parse and inter-
pret common semantic classes like dates, times, and
numbers. The grammar and semantic libraries pro-
vide good coverage for spoken dialogue systems in
database-query domains.
6
To build a grammar for a new domain, a devel-
oper extends the generic syntactic grammar by aug-
menting it with domain-specific semantic categories
and their lexical entries. A probability model which
conditions each node category on its left sibling and
parent is then estimated from a training corpus of
utterances (Seneff et al, 2003).
At runtime, the recognizer tags the hypothesized
dynamic class expansions with their class names,
allowing the parser grammar to be independent of
the database contents. Furthermore, each semantic
class is designated either as a semantic entity, or as
an attribute associated with a particular entity. This
enables the generation of a semantic representation
from the parse tree.
Dialogue Management & Language Generation
Once an utterance is recognized and parsed, the
meaning representation is passed to the context res-
olution and dialogue manager component. The con-
text resolution module (Filisko and Seneff, 2003)
applies generic and domain-specific rules to re-
solve anaphora and deixis, and to interpret frag-
ments and ellipsis in context. The dialogue man-
ager then interacts with the application back-end
and database, controlled by a script customized for
the domain (Polifroni et al, 2003). Finally, the
GENESIS module (Baptist and Seneff, 2000) ap-
plies domain-specific rules to generate a natural lan-
guage representation of the dialogue manager?s re-
sponse, which is sent to a speech synthesizer. The
dialogue manager also sends an update to the GUI,
so that, for example, the appropriate database search
results are displayed.
7 Mobile Design Challenges
Dialogue systems for mobile devices present a
unique set of design challenges not found in tele-
phony and desktop applications. Here we describe
some of the design choices made while developing
this prototype, and discuss their tradeoffs.
7.1 Client/Server Tradeoffs
Towards supporting network-less scenarios, we have
begun porting various natural language processing
components to mobile platforms, as discussed in
Section 5. Having efficient mobile implementations
further allows the natural language processing tasks
to be performed on either the mobile device or the
server. While building the prototype, we observed
that the Wi-Fi network performance can often be un-
predictable, resulting in erratic recognition latency
that occasionally exceeds on-device recognition la-
tency. However, utilizing the mobile processor for
computationally intensive tasks rapidly drains the
battery. Currently, the component architecture in the
prototype system is pre-configured. A more robust
implementation would dynamically adjust the con-
figuration to optimize the tradeoffs among network
use, CPU utilization, power consumption, and user-
perceived latency/accuracy.
7.2 Speech User Interface
As neither open-mic nor push-to-talk with automatic
endpoint detection is practical on mobile devices
with limited battery life, our prototype system em-
ploys a hold-to-talk hardware button for microphone
control. To guide users to speak commands only
while the button is depressed, a short beep is played
as an earcon both when the button is pushed and
released. Since users are less likely to talk over
short audio clips, the use of earcons mitigates the
tendency for users to start speaking before pushing
down the microphone button.
In the current system, media audio is played over
the TV speakers, whereas TTS output is sent to
the mobile device speakers. To reduce background
noise captured from the mobile device?s far-field mi-
crophone, the TV is muted while the microphone
button is depressed. Unlike telephony spoken di-
alogue systems where the recognizer has to con-
stantly monitor for barge-in, the use of a hold-to-
talk button significantly simplifies barge-in support,
while reducing power consumption.
7.3 Graphical User Interface
In addition to supporting interactive natural lan-
guage dialogues via the spoken user interface, the
prototype system implements a graphical user in-
terface (GUI) on the mobile device to supplement
the TV?s on-screen interface. To faciliate rapid pro-
totyping, we chose to implement both the mobile
and TV GUI using web pages with AJAX (Asyn-
chronous Javascript and XML) techniques, an ap-
proach we have leveraged in several existing mul-
timodal dialogue systems, e.g. (Gruenstein et al,
7
2006; McGraw and Seneff, 2007). The resulting in-
terface is largely platform-independent and allows
display updates to be ?pushed? to the client browser.
As many users are already familiar with the TV?s
on-screen interface, we chose to mirror the same in-
terface on the mobile device and synchronize the
selection cursor. However, unlike desktop GUIs,
mobile devices are constrained by a small display,
limited computational power, and reduced network
bandwidth. Thus, both the page layout and infor-
mation detail were adjusted for the mobile browser.
Although AJAX is more responsive than traditional
web technology, rendering large formatted pages?
such as the program guide grid?is often still un-
acceptably slow. In the current implementation, we
addressed this problem by displaying only the first
section of the content and providing a ?Show More?
button that downloads and renders the full content.
While browser-based GUIs expedite rapid prototyp-
ing, deployed systems may want to take advantage
of native interfaces specific to the device for more
responsive user interactions. Instead of limiting the
mobile interface to reflect the TV GUI, improved us-
ability may be obtained by designing the interface
for the mobile device first and then expanding the
visual content to the TV display.
7.4 Client/Server Communication
In the current prototype, communication between
the mobile device and the media server consists of
AJAX HTTP and XML-RPC requests. To enable
server-side ?push? updates, the client periodically
pings the server for messages. While such an im-
plementation provides a responsive user interface, it
quickly drains the battery and is not robust to net-
work outages resulting from the device being moved
or switching to power-saving mode. Reestablish-
ing connection with the server further introduces la-
tency. In future implementations, we would like to
examine the use of Bluetooth for lower power con-
sumption, and infrared for immediate response to
common controls and basic navigation.
8 Conclusions & Future Work
We have presented a prototype system that demon-
strates the feasibility of deploying a multimodal,
natural language interface on a mobile device for
browsing and managing one?s home media library.
In developing the prototype, we have experimented
with a novel role for a mobile device?that of a
speech-enabled remote control. We have demon-
strated a flexible natural language understanding ar-
chitecture, in which various processing stages may
be performed on either the server or mobile device,
as networking and processing power considerations
require.
While the mobile platform presents many chal-
lenges, it also provides unique opportunities.
Whereas desktop computers and TV remote controls
tend to be shared by multiple users, a mobile device
is typically used by a single individual. By collect-
ing and adapting to the usage data, the system can
personalize the recognition and understanding mod-
els to improve the system accuracy. In future sys-
tems, we hope to not only explore such adaptation
possibilities, but also study how real users interact
with the system to further improve the user interface.
Acknowledgments
This research is sponsored by the TParty Project,
a joint research program between MIT and Quanta
Computer, Inc.; and by Nokia, as part of a joint MIT-
Nokia collaboration. We are also thankful to three
anonymous reviewers for their constructive feed-
back.
References
A. Acero, N. Bernstein, R. Chambers, Y. C. Jui, X. Li,
J. Odell, P. Nguyen, O. Scholz, and G. Zweig. 2008.
Live search for mobile: Web services by voice on the
cellphone. In Proc. of ICASSP.
L. Baptist and S. Seneff. 2000. Genesis-II: A versatile
system for language generation in conversational sys-
tem applications. In Proc. of ICSLP.
A. Berglund and P. Johansson. 2004. Using speech and
dialogue for interactive TV navigation. Universal Ac-
cess in the Information Society, 3(3-4):224?238.
G. Chung, S. Seneff, C. Wang, and L. Hetherington.
2004. A dynamic vocabulary spoken dialogue inter-
face. In Proc. of INTERSPEECH, pages 327?330.
E. Filisko and S. Seneff. 2003. A context resolution
server for the GALAXY conversational systems. In
Proc. of EUROSPEECH.
B. Ford, J. Strauss, C. Lesniewski-Laas, S. Rhea,
F. Kaashoek, and R. Morris. 2006. Persistent personal
names for globally connected mobile devices. In Pro-
ceedings of the 7th USENIX Symposium on Operating
Systems Design and Implementation (OSDI ?06).
8
K. Fujita, H. Kuwano, T. Tsuzuki, and Y. Ono. 2003.
A new digital TV interface employing speech recog-
nition. IEEE Transactions on Consumer Electronics,
49(3):765?769.
J. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer Speech and Lan-
guage, 17:137?152.
A. Gruenstein, S. Seneff, and C. Wang. 2006. Scalable
and portable web-based multimodal dialogue interac-
tion with geographical databases. In Proc. of INTER-
SPEECH.
I. L. Hetherington. 2005. A multi-pass, dynamic-
vocabulary approach to real-time, large-vocabulary
speech recognition. In Proc. of INTERSPEECH.
I. L. Hetherington. 2007. PocketSUMMIT: Small-
footprint continuous speech recognition. In Proc. of
INTERSPEECH, pages 1465?1468.
B. Johanson, A. Fox, and T. Winograd. 2002. The in-
teractive workspaces project: Experiences with ubiq-
uitous computing rooms. IEEE Pervasive Computing,
1(2):67?74.
M. Johnston, L. F. D?Haro, M. Levine, and B. Renger.
2007. A multimodal interface for access to content in
the home. In Proc. of ACL, pages 376?383.
J. Mazzola Paluska, H. Pham, U. Saif, C. Terman, and
S. Ward. 2006. Reducing configuration overhead with
goal-oriented programming. In PerCom Workshops,
pages 596?599. IEEE Computer Society.
J. Mazzola Paluska, H. Pham, U. Saif, G. Chau, C. Ter-
man, and S. Ward. 2008. Structured decomposition of
adapative applications. In Proc. of 6th IEEE Confer-
ence on Pervasive Computing and Communications.
I. McGraw and S. Seneff. 2007. Immersive second lan-
guage acquisition in narrow domains: A prototype IS-
LAND dialogue system. In Proc. of the Speech and
Language Technology in Education Workshop.
J. Nichols and B. A. Myers. 2006. Controlling home and
office appliances with smartphones. IEEE Pervasive
Computing, special issue on SmartPhones, 5(3):60?
67, July-Sept.
H.-J. Oh, C.-H. Lee, M.-G. Jang, and Y. K. Lee. 2007.
An intelligent TV interface based on statistical dia-
logue management. IEEE Transactions on Consumer
Electronics, 53(4).
T. Paek, M. Agrawala, S. Basu, S. Drucker, T. Kristjans-
son, R. Logan, K. Toyama, and A. Wilson. 2004. To-
ward universal mobile interaction for shared displays.
In Proc. of Computer Supported Cooperative Work.
J. Polifroni, G. Chung, and S. Seneff. 2003. Towards
the automatic generation of mixed-initiative dialogue
systems from web content. In Proc. EUROSPEECH,
pages 193?196.
T. Portele, S. Goronzy, M. Emele, A. Kellner, S. Torge,
and J. te Vrugt. 2003. SmartKom-Home - an ad-
vanced multi-modal interface to home entertainment.
In Proc. of INTERSPEECH.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. GALAXY-II: A reference architecture
for conversational system development. In Proc. IC-
SLP.
S. Seneff, C. Wang, and T. J. Hazen. 2003. Automatic in-
duction of n-gram language models from a natural lan-
guage grammar. In Proceedings of EUROSPEECH.
S. Seneff. 1992. TINA: A natural language system
for spoken language applications. Computational Lin-
guistics, 18(1):61?86.
S. Seneff. 2002. Response planning and generation in
the MERCURY flight reservation system. Computer
Speech and Language, 16:283?312.
S. Seneff. 2007. Reversible sound-to-letter/letter-to-
sound modeling based on syllable structure. In Proc.
of HLT-NAACL.
K. Wittenburg, T. Lanning, D. Schwenke, H. Shubin, and
A. Vetro. 2006. The prospects for unrestricted speech
input for TV content search. In Proc. of AVI?06.
V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao, T. J.
Hazen, and L. Hetherington. 2000. JUPITER: A
telephone-based conversational interface for weather
information. IEEE Transactions on Speech and Audio
Processing, 8(1), January.
9
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 45?48,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An Overview of Microsoft Web N-gram Corpus and Applications

Kuansan Wang        Christopher Thrasher       Evelyne Viegas 
Xiaolong Li        Bo-june (Paul) Hsu         
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052, USA 
webngram@microsoft.com 
  
 
 
Abstract 
This document describes the properties and 
some applications of the Microsoft Web N-
gram corpus. The corpus is designed to have 
the following characteristics. First, in contrast 
to static data distribution of previous corpus 
releases, this N-gram corpus is made publicly 
available as an XML Web Service so that it 
can be updated as deemed necessary by the 
user community to include new words and 
phrases constantly being added to the Web. 
Secondly, the corpus makes available various 
sections of a Web document, specifically, the 
body, title, and anchor text, as separates mod-
els as text contents in these sections are found 
to possess significantly different statistical 
properties and therefore are treated as distinct 
languages from the language modeling point 
of view. The usages of the corpus are demon-
strated here in two NLP tasks: phrase segmen-
tation and word breaking. 
1 Introduction 
Since Banko and Brill?s pioneering work almost a 
decade ago (Banko and Brill 2001), it has been 
widely observed that the effectiveness of statistical 
natural language processing (NLP) techniques is 
highly susceptible to the data size used to develop 
them. As empirical studies have repeatedly shown 
that simple algorithms can often outperform their 
more complicated counterparts in wide varieties of 
NLP applications with large datasets, many have 
come to believe that it is the size of data, not the 
sophistication of the algorithms that ultimately 
play the central role in modern NLP (Norvig, 
2008). Towards this end, there have been consider-
able efforts in the NLP community to gather ever 
larger datasets, culminating the release of the Eng-
lish Giga-word corpus (Graff and Cieri, 2003) and 
the 1 Tera-word Google N-gram (Thorsten and 
Franz, 2006) created from arguably the largest text 
source available, the World Wide Web. 
Recent research, however, suggests that studies 
on the document body alone may no longer be suf-
ficient in understanding the language usages in our 
daily lives. A document, for example, is typically 
associated with multiple text streams. In addition 
to the document body that contains the bulk of the 
contents, there are also the title and the file-
name/URL the authors choose to name the docu-
ment. On the web, a document is often linked with 
anchor text or short messages from social network 
applications that other authors use to summarize 
the document, and from the search logs we learn 
the text queries formulated by the general public to 
specify the document. A large scale studies reveal 
that these text streams have significantly different 
properties and lead to varying degrees of perfor-
mance in many NLP applications (Wang et al 
2010, Huang et al 2010). Consequently from the 
statistical modeling point of view, these streams 
are better regarded as composed in distinctive lan-
guages and treated as such. 
This observation motivates the creation of Mi-
crosoft Web N-gram corpus in which the materials 
from the body, title and anchor text are made 
available separately. Another notable feature of the 
corpus is that Microsoft Web N-gram is available 
as a cross-platform XML Web service1 that can be 
freely and readily accessible by users through the 
Internet anytime and anywhere. The service archi-
tecture also makes it straightforward to perform on 
                                                          
1 Please visit http://research.microsoft.com/web-ngram for 
more information. 
45
demand updates of the corpus with the new con-
tents that can facilitate the research on the dynam-
ics of the Web.2 
2 General Model Information  
Like the Google N-gram, Microsoft Web N-gram 
corpus is based on the web documents indexed by 
a commercial web search engine in the EN-US 
market, which, in this case, is the Bing service 
from Microsoft. The URLs in this market visited 
by Bing are at the order of hundreds of billion, 
though the spam and other low quality web pages 
are actively excluded using Bing?s proprietary al-
gorithms. The various streams of the web docu-
ments are then downloaded, parsed and tokenized 
by Bing, in which process the text is lowercased 
with the punctuation marks removed. However, no 
stemming, spelling corrections or inflections are 
performed.  
Unlike the Google N-gram release which con-
tains raw N-gram counts, Microsoft Web N-gram 
provides open-vocabulary, smoothed back-off N-
gram models for the three text streams using the 
CALM algorithm (Wang and Li, 2009) that dy-
namically adapts the N-gram models as web doc-
uments are crawled. The design of CALM ensures 
that new N-grams are incorporated into the models 
as soon as they are encountered in the crawling and 
become statistically significant. The models are 
therefore kept up-to-date with the web contents. 
CALM is also designed to make sure that dupli-
cated contents will not have outsized impacts in 
biasing the N-gram statistics. This property is use-
ful as Bing?s crawler visits URLs in parallel and on 
the web many URLs are pointing to the same con-
tents. Currently, the maximum order of the N-gram 
available is 5, and the numbers of N-grams are 
shown in Table 1. 
 
Table 1: Numbers of N-grams for various streams 
 Body Title Anchor 
1-gram 1.2B 60M 150M 
2-gram 11.7B 464M 1.1B 
3-gram 60.1B 1.4B 3.2B 
4-gram 148.5B 2.3B 5.1B 
5-gram 237B 3.8B 8.9B 
                                                          
2 The WSDL for the web service is located at http://web-
ngram.research.microsoft.com/Lookup.svc/mex?wsdl. 
CALM algorithm adapts the model from a seed 
model based on the June 30, 2009 snapshot of the 
Web with the algorithm described and imple-
mented in the MSRLM toolkit (Nguyen et al 
2007). The numbers of tokens in the body, title, 
and anchor text in the snapshot are of the order of 
1.4 trillion, 12.5 billion, and 357 billion, respec-
tively. 
3 Search Query Segmentation 
In this demonstration, we implement a straightfor-
ward algorithm that generates hypotheses of the 
segment boundaries at all possible placements in a 
query and rank their likelihoods using the N-gram 
service. In other words, a query of T terms will 
have 2T-1 segmentation hypotheses. Using the fam-
ous query ?mike siwek lawyer mi? described in 
(Levy, 2010) as an example, the likelihoods and 
the segmented queries for the top 5 hypotheses are 
shown in Figure 1. 
Body: 
  
Title: 
 
Anchor: 
 
 
Figure 1: Top 5 segmentation hypotheses under 
body, title, and anchor language models. 
 
As can be seen, the distinctive styles of the lan-
guages used to compose the body, title, and the 
anchor text contribute to their respective models 
producing different outcomes on the segmentation 
46
task, many of which research issues have been ex-
plored in (Huang et al 2010). It is hopeful that the 
release of Microsoft Web N-gram service can ena-
ble the community in general to accelerate the re-
search on this and related areas. 
4 Word Breaking Demonstration 
Word breaking is a challenging NLP task, yet the 
effectiveness of employing large amount of data to 
tackle word breaking problems has been demon-
strated in (Norvig, 2008). To demonstrate the ap-
plicability of the web N-gram service for the work 
breaking problem, we implement the rudimentary 
algorithm described in (Norvig, 2008) and extend 
it to use body N-gram for ranking the hypotheses. 
In essence, the word breaking task can be regarded 
as a segmentation task at the character level where 
the segment boundaries are delimitated by white 
spaces. By using a larger N-gram model, the demo 
can successfully tackle the challenging word 
breaking examples as mentioned in (Norvig, 2008). 
Figure 2 shows the top 5 hypotheses of the simple 
algorithm. We note that the word breaking algo-
rithm can fail to insert desired spaces into strings 
that are URL fragments and occurred in the docu-
ment body frequently enough. 
 
 
 
Figure 2: Norvig's word breaking examples (Norvig, 
2008) re-examined with Microsoft Web N-gram 
 
 
 
 
 
47
Two surprising side effects of creating the N-
gram models from the web in general are worth 
noting. First, as more and more documents contain 
multi-lingual contents, the Microsoft Web N-gram 
corpus inevitably include languages other than EN-
US, the intended language. Figure 3 shows exam-
ples in German, French and Chinese (Romanized) 
each.  
 
 
 
 
 
Figure 3: Word breaking examples for foreign lan-
guages: German (top), French and Romanized Chi-
nese 
 
Secondly, since the web documents contain many 
abbreviations that are popular in short messaging, 
the consequent N-gram model lends the simple 
word breaking algorithm to cope with the common 
short hands surprisingly well. An example that de-
codes the short hand for ?wait for you? is shown in 
Figure 4. 
 
 
Figure 4: A word breaking example on SMS-style 
message. 
References  
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram 
Version 1. Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia. 
Michel Banko and Eric Brill. 2001. Mitigating the pauc-
ity-of-data problem: exploring the effect of training 
corpus size on classifier performance for natural lan-
guage processing. Proc. 1st Internal Conference on 
human language technology research, 1-5, San Di-
ego, CA.  
David Graff and Christopher Cieri. 2003. English Gi-
gaword. Linguistic Data Consortium, ISBN: 1-
58563-260-0, Philadelphia. 
Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li, 
Kuansan Wang, and Fritz Behr. 2010. Exploring web 
scale language models for search query processing. 
In Proc. 19th International World Wide Web Confe-
rence (WWW-2010), Raleigh, NC. 
Steven Levy, 2010. How Google?s algorithm rules the 
web. Wired Magazine, February.  
Patrick Nguyen, Jianfeng Gao, and Milind Mahajan. 
2007. MSRLM: a scalable language modeling tool-
kit. Microsoft Research Technical Report MSR-TR-
2007-144. 
Peter Norvig. 2008. Statistical learning as the ultimate 
agile development tool. ACM 17th Conference on In-
formation and Knowledge Management Industry 
Event (CIKM-2008), Napa Valley, CA. 
Kuansan Wang, Jianfeng Gao, and Xiaolong Li. 2010. 
The multi-style language usages on the Web and 
their implications on information retrieval. In sub-
mission. 
Kuansan Wang, Xiaolong Li and Jianfeng Gao, 2010. 
Multi-style language model for web scale informa-
tion retrieval. In Proc. ACM 33rd Conference on Re-
search and Development in Information Retrieval 
(SIGIR-2010), Geneva, Switzerland. 
Kuansan Wang and Xiaolong Li, 2009. Efficacy of a 
constantly adaptive language modeling technique for 
web scale application. In Proc. IEEE International 
Conference on Acoustics, Speech, and Signal 
Processing (ICASSP-2009), Taipei, Taiwan. 
48
