Two-Phase Biomedical NE Recognition based on SVMs
Ki-Joong Lee Young-Sook Hwang and Hae-Chang Rim
Department of Computer Science & Engineering
Korea University
1, 5-ka, Anam-dong, SEOUL, 136-701, KOREA
{kjlee, yshwang, rim}@nlp.korea.ac.kr
Abstract
Using SVMs for named entity recogni-
tion, we are often confronted with the
multi-class problem. Larger as the num-
ber of classes is, more severe the multi-
class problem is. Especially, one-vs-rest
method is apt to drop the performance by
generating severe unbalanced class distri-
bution. In this study, to tackle the prob-
lem, we take a two-phase named entity
recognition method based on SVMs and
dictionary; at the first phase, we try to
identify each entity by a SVM classifier
and post-process the identified entities by
a simple dictionary look-up; at the sec-
ond phase, we try to classify the seman-
tic class of the identified entity by SVMs.
By dividing the task into two subtasks, i.e.
the entity identification and the semantic
classification, the unbalanced class distri-
bution problem can be alleviated. Further-
more, we can select the features relevant
to each task and take an alternative classi-
fication method according to the task. The
experimental results on the GENIA cor-
pus show that the proposed method is ef-
fective not only in the reduction of train-
ing cost but also in performance improve-
ment: the identification performance is
about 79.9(F? = 1), the semantic clas-
sification accuracy is about 66.5(F? = 1).
1 Introduction
Knowledge discovery in the rapidly growing area of
biomedicine is very important. While most knowl-
edge are provided in a vast amount of texts, it is im-
possible to grasp all of the huge amount of knowl-
edge provided in the form of natural language. Re-
cently, computational text analysis techniques based
on NLP have received a spotlight in bioinformat-
ics. Recognizing the named entities such as proteins,
DNAs, RNAs, cells etc. has become one of the most
fundamental tasks in the biomedical knowledge dis-
covery.
Conceptually, named entity recognition consists
of two tasks: identification, which finds the bound-
aries of a named entity in a text, and classifi-
cation, which determines the semantic class of
that named entity. Many machine learning ap-
proaches have been applied to biomedical named
entity recognition(Nobata, 1999)(Hatzivalssiloglou,
2001)(Kazama, 2002). However, no work has
achieved sufficient recognition accuracy. One rea-
son is the lack of annotated corpora. This is some-
what appeased with announcement of the GENIA
corpus v3.0(GENIA, 2003). Another reason is that
it is difficult to recognize biomedical named entities
by using general features compared with the named
entities in newswire articles. In addition, since non-
entity words are much more than entity words in
biomedical documents, class distribution in the class
representation combining a B/I/O tag with a seman-
tic class C is so severely unbalanced that it costs too
much time and huge resources, especially in SVMs
training(Hsu, 2001).
Therefore, Kazama and his colleagues tackled the
problems by tuning SVMs(Kazama, 2002). They
splitted the class with unbalanced class distribution
into several subclasses to reduce the training cost.
In order to solve the data sparseness problem, they
explored various features such as word cache fea-
tures and HMM state features. According to their re-
port, the word cache and HMM state features made
a positive effect on the performance improvement.
But, not separating the identification task from the
semantic classification, they tried to classify the
named entities in the integrated process.
By the way, the features for identifying the
biomedical entity are different from those for se-
mantically classifying the entity. For example, while
orthographical characteristics and a part-of-speech
tag sequence of an entity are strongly related to the
identification, those are weakly related to the seman-
tic classification. On the other hand, context words
seem to provide useful clues to the semantic classifi-
cation of a given entity. Therefore, we will separate
the identification task from the semantic classifica-
tion task. We try to select different features accord-
ing to the task. This approach enables us to solve the
unbalanced class distribution problem which often
occurs in a single complicated approach. Besides, to
improve the performance, we will post-process the
results of SVM classifiers by utilizing the dictionary.
That is, we adopt a simple dictionary lookup method
to correct the errors by SVMs in the identification
phase.
Through some experiments, we will show how
separating the entity recognition task into two sub-
tasks contributes to improving the performance of
biomedical named entity recognition. And we will
show the effect the hybrid approach of the SVMs
and the dictionary-lookup.
2 Definition of Named Entity
Classification Problem
We divide the named entity recognition into two
subtasks, the identification task which finds the re-
gions of the named entities in a text and the semantic
classification which determines the semantic classes
of them. Figure 1 illustrates the proposed method,
which is called two-phase named entity recognition
method.
Figure 1: Examples of Biomedical Named Entity
Recognition
The identification task is formulated as classifica-
tion of each word into one of two classes, T or O
that represent region information. The region infor-
mation is encoded by using simple T/O representa-
tion: T means that current word is a part of a named
entity, and O means that the word is not in a named
entity. With the representation, we need only one
binary SVM classifier of two classes, T, O.
The semantic classification task is to assign one
of semantic classes to the identified entity. At the
semantic classification phase, we need to classify
only the identified entities into one of the N seman-
tic classes because the entities were already identi-
fied. Non-entity words are ignored at this phase. The
classes needed to be classified are just only the N se-
mantic classes. Note that the number of total classes,
N + 1 is remarkably small compared with the num-
ber, 2N + 1 required in the complicated recognition
approaches in which a class is represented by com-
bining a region information B/I/O with a semantic
class C. It can considerably reduce workload in the
named entity recognition.
Especially when using SVMs, the number of
classes is very critical to the training in the as-
pect of training time and required resources. Let
L be the number of training samples and let N be
the number of classes. Then one-vs-rest method
takes N ? O(L) in the training step. The com-
plicated approach with the B/I/O notation requires
(2N + 1)?O(Lwords) (L is number of total words
in a training corpus). In contrast, the proposed ap-
proach requires (N ? O(Lentities)) + O(Lwords).
Here, O(Lwords) stands for the number of words in
a training corpus and O(Lentities) for the number of
entities. It is a considerable reduction in the training
cost. Ultimately, it affects the performance of the
entity recognizer.
To achieve a high performance of the defined
tasks, we use SVM(Joachims, 2002) as a machine
learning approach which has showed the best perfor-
mance in various NLP tasks. And we post-process
the classification results of SVMs by utilizing a dic-
tionary. Figure 2 outlines the proposed two-phase
named entity recognition system. At each phase,
each classifier with SVMs outputs the class of the
best score. For classifying multi-classes based on a
binary classifier SVM, we use the one-vs-rest clas-
sification method and the linear kernel in both tasks.
Furthermore, for correcting the errors by SVMs,
the entity-word dictionary constructed from a train-
ing corpus is utilized in the identification phase. The
dictionary is searched to check whether the bound-
ary words of an identified entity were excluded or
not because the boundary words of an entity might
be excluded during the entity identification. If a
boundary word was excluded, then we concatenate
the left or the right side word adjacent to the iden-
tified entity. This post-processing may enhance the
capability of the entity identifier.
3 Biomedical Named Entity Identification
The named entity identification is defined as the
classification of each word to one of the classes that
represent the region information. The region infor-
mation is encoded by using simple T/O representa-
tion: T means that the current word is a part of a
named entity, and O means that the current word is
not in a named entity.
The above representation yields two classes of the
task and we build just one binary SVM classifiers for
them. By accepting the results of the SVM classifier,
we determine the boundaries of an entity. To correct
boundary errors, we post-process the identified enti-
ties with the entity-word dictionary.
3.1 Features for Entity Identification
An input x to a SVM classifier is a feature represen-
tation of a target word to be classified and its context.
We use a bit-vector representation. The features of
the designated word are composed of orthographi-
cal characteristic features, prefix, suffix, and lexical
of the word.
Table 1 shows all of the 24 orthographical fea-
tures. Each feature may be a discriminative fea-
ture appeared in biomedical named entites such as
protein, DNA and RNA etc. Actually, the name of
protein, DNA or RNA is composed by combining
alpha-numeric string with several characters such as
Greek or special symbols and so on.
Table 1: Orthographical characteristic features of
the designated word
Orthographic Feature examples
DIGITS 1 , 39
SINGLE CAP A , M
COMMA ,
PERIOD .
HYPHON -
SLASH /
QUESTION MARK ?
OPEN SQUARE [
CLOSE SQUARE ]
OPEN PAREN (
CLOSE PAREN )
COLON :
SEMICOLON ;
PERCENT %
APOSTROPHE ?
ETC SYMBOL +, *, etc.
TWO CAPS alphaCD28
ALL UPPER AIDS
INCLUDE CAPS c-Jun
GREEK LETTER NF-kappa
ALPHA NUMERIC p65
ALL LOWER motif
CAPS DIGIT CD40
INIT CAP Rel
And the suffix/prefix, the designated word and the
context word features are as follows:
wi =
?
??
??
1 if the word is the ith word
in the vocabulary V
0 otherwise
Figure 2: System Configuration of Two Phase Biomedical NE Recognition System
posi =
?
??
??
1 if the word is assigned the ith
POS tag in the POS tag list
0 otherwise
sufi =
?
??
??
1 if the word contains the
ith suffix in the suffix list
0 otherwise
prei =
?
??
??
1 if the word contains the
ith prefix in the prefix list
0 otherwise
wki =
?
??
??
1 if a word at k is the ith word
in the vocabulary V
0 otherwise
poski =
?
??
??
1 if a word at k is assigned the
ith POS tag in the POS tag list
0 otherwise
In the definition, k is the relative word position
from the target word. A negative value represents
a preceeding word and a positive value represents
a following word. Among them, the part-of-speech
tag sequence of the word and the context words is a
kind of a syntactic rule to compose an entity. And
lexical information is a sort of filter to identify an
entity which is as possible as semantically cohesive.
3.2 Post-Processing by Dictionary Look-Up
After classifying the given instances, we do post-
processing of the identified entities. During the post-
processing, we scan the identified entities and exam-
ine the adjacent words to those. If the part-of-speech
of an adjacent word belongs to one of the group, ad-
jective, noun, or cardinal, then we look up the dic-
tionary to check whether the word is in it or not. If it
exists in the dictionary, we include the word into the
entity region. The dictionary is constructed of words
consisting of the named entities in a training corpora
and stopwords are ignored.
Figure 3 illustrates the post-processing algorithm.
In Figure 3, the word cell adjacent to the left of the
identified entity cycle-dependent transcription, has
the part-of-speech NN and exists in the dictionary.
The word factor adjacent to the right of the entity
has the part-of-speech NN. It exists in the dictionary,
too. Therefore, we include the words cell and factor
into the entity region and change the position tags of
the words in the entity.
By taking the post-processing method, we can
correct the errors by a SVM classifier. It also gives
us a great effect of overcoming the low coverage
problem of the small-sized entity dictionary.
4 Semantic Classification of Biomedical
Named Entity
The objects of the semantic tagging are the entities
identified in the identification phase. Each entity is
assigned to a proper semantic class by voting the
SVM classifiers.
Figure 3: An example of the post-processing of an entity identification
4.1 Features for Semantic Classification
For semantically tagging an entity, an input x to a
SVM classifier is represented by a feature vector.
The vector is composed of following features:
fwi =
?
??
??
1 if a given entity contains one
of the functional words
0 otherwise
inwi =
?
??
??
1 if one of the words in the
entity is in the inside word list
0 otherwise
lcwi =
?
????
????
1 if noun or verb word in the
left context is the ith word
in the left context word list
0 otherwise
rcwi =
?
????
????
1 if noun or verb word in the
right context is the ith word
in the right context word list
0 otherwise
Of the above features, fwi checks whether the
entity contains one of functional words. The func-
tional words are similar to the feature terms used by
(Fukuda, 1998). For example, the functional words
such as factor, receptor and protein are very help-
ful to classifying named entities into protein and the
functional words such as gene, promoter and motif
are very useful for classifying DNA.
In case of the context features of a given entity, we
divide them into two kinds of context features, inside
context features and outside context features. As in-
side context features, we take at most three words
from the backend of the entity 1. We make a list of
the inside context words by collecting words in the
1The average length of entities is about 2.2 in GENIA cor-
pus.
range of the inside context. If one of the three words
is the ith word in the inside context word list, we set
the inwi bit to 1. The outside context features are
grouped in the left ones and the right ones. For the
left and the right context features, we restrict them
to noun or verb words in a sentence, whose position
is not specified. This grouping make an effect of al-
leviating the data sparseness problem when using a
word as a feature.
For example, given a sentence with the entity,
RNA polymerase II as follows:
General transcription factor are required
for accurate initiation of transcription by
RNA polymerase II PROTEIN .
The nouns transcription, factor, initiation and the
verbs are, required are selected as left context fea-
tures, and the words RNA, polymerase, II are se-
lected as inside context features. The bit field cor-
responding to each of the selected word is set to 1.
In this case, there is no right context features. And
since the entity contains the functional word RNA,
the bit field of RNA is set to 1.
For classifying a given entity, we build SVM clas-
sifiers as many as the number of semantic classes.
We take linear kernel and one-vs-rest classification
method.
5 Experiments
5.1 Experimental Environments
Experiments have been conducted on the GENIA
corpus(v3.0p)(GENIA, 2003), which consists of
2000 MEDLINE abstracts annotated with Penn
Treebank (PTB) POS tags. There exist 36 distinct
semantic classes in the corpus. However, we used
22 semantic classes which are all but protein, DNA
and RNA?s subclasses on the GENIA ontology 2.
The corpus was transformed into a B/I/O annotated
corpus to represent entity boundaries and a semantic
class.
We divided 2000 abstracts into 10 collections for
10-fold cross validation. Each collection contains
not only abstracts but also paper titles. The vo-
cabularies for lexical features and prefix/suffix lists
were constructed by taking the most frequent 10,000
words from the training part only.
Also, we made another experimental environ-
ment to compare with the previous work by
(Kazama, 2002). From the GENIA corpus, 590
abstracts(4,808 sentences; 20,203 entities; 128,463
words) were taken as a training part and 80 ab-
stracts(761 sentences; 3,327 entities; 19,622 words)
were selected as a test part. Because we couldn?t
make the experimental environment such as the
same as that of Kazama?s, we tried to make a com-
parable environment.
We implemented our method using the SVM-light
package(Joachims, 2002). Though various learning
parameters can significantly affect the performance
of the resulting classifiers, we used the SVM system
with linear kernel and default options.
The performance was evaluated by precision, re-
call and F?=1. The overall F?=1 for two models and
ten collections, were calculated using 10-fold cross
validation on total test collection.
5.2 Effect of Training Data Size
In this experiment, varying the size of training set,
we observed the change of F?=1 in the entity identi-
fication and the semantic classification. We fixed the
test data with 200 abstracts(1,921 sentences; 50,568
words). Figure 4 shows that the performance was
improved by increasing the training set size. As the
performance of the identification increases, the gap
between the performance of the identification and
that of the semantic classification is gradually de-
creased.
5.3 Computational Efficiency
When using one-vs-rest method, the number of
negative samples is very critical to the training in
2That is, All of the protein?s subclass such as pro-
tein molecule, protein family or group were regarded as pro-
tein.
Figure 4: Perfomance shift according to the increase
of training data size w/o post-processing
the aspect of training time and required resources.
The SVM classifier for entity identifiation deter-
mines whether each word is included in an entity
or not. Figure 5 shows there are much more nega-
tive samples than positive samples in the identifica-
tion phase. Once entities are identified, non-entity
words are not considered in next semantic classifi-
cation phase. Therefore, the proposed method can
effectively remove the unnecessary samples. It en-
ables us effectively save the training costs.
Furthermore, the proposed method could effec-
tively decrease the degree of the unbalance among
classes by simplifying the classes. Figure 6 shows
how much the proposed method can alleviate the un-
balanced class distribution problem compared with
1-phase complicated classification model. However,
even though the unbalanced class distribution prob-
lem could be alleviated in the identification phase,
we are still suffering from the problem in the seman-
tic classification as long as we take the one-vs-rest
method. It indicates that we need to take another
classification method such as a pairwise method in
the semantic classification(Krebel, 1999).
5.4 Discriminative Feature Selection
We subsequently examined several alternatives for
the feature sets described in section 3.1 and section
4.1.
The column (A) in Table 2 shows the identifica-
tion cases. The base feature set consisted of only the
designated word and the context words in the range
from the left 2 to the right 2. Several alternatives for
feature sets were constructed by adding a different
combination of features to the base feature set. From
Figure 5: training size vs. positive and negative sam-
ple size in identification phase and semantic classi-
fication phase
Figure 6: 2-phase model vs. 1-phase model : change
of the negative and the positive sample size accord-
ing to the training data size
( A ) ( B )
FeatSet F-score FeatSet F-score
base 74.6 base(inw) 65.8
pos 77.4 (+2.8) fw 67.9 (+2.1)
pre 75.0 (+0.4) lcw 67.9 (+2.1)
suf 75.2 (+0.6) rcw 67.0 (+1.2)
pre+suf 75.6 (+1.0) lcw+rcw 66.4 (+0.6)
pos+pre 77.9 (+3.3) fw+lcw 68.1(+2.3)
pos+suf 77.9 (+3.3) fw+rcw 67.1 (+1.3)
all 77.9 (+3.3) all 66.9 (+1.1)
Table 2: Effect of each feature set(training with 900
abstracts, test with 100 abstracts): (A) identification
phase, (B) semantic classification phase
Table 2, we can see that part-of-speech information
certainly improves the identification accuracy(about
+2.8). Prefix and suffix features made a positive ef-
fect, but only modestly(about +1.2 on average).
The column (B) in Table 2 shows semantic clas-
sification cases with the identification phase of the
best performance. We took the feature set composed
of the inside words of an entity as a base feature set.
And we made several alternatives by adding another
features. The experimental results show that func-
tional words and left context features are useful, but
right context features are not. Furthermore, part-of-
speech information was not effective in the seman-
tic classification while it was useful for the entity
identification. That is, when we took the part-of-
speech tags of inside context words instead of the
inside context words, the performance of the seman-
tic classification was very low(F?=1.0 was 25.1).
5.5 Effect of PostProcessing by Dictionary
Lookup
Our two-phase model has the problem that identifi-
cation errors are propagated to the semantic classi-
fication. For this reason, it is necessary to ensure
a high accuracy of the boundary identification by
adopting a method such as post processing of the
identified entities. Table 3 shows that the post pro-
cessing by dictionary lookup is effective to improv-
ing the performance of not only the boundary identi-
fication accurary(79.2 vs. 79.9) but also the seman-
tic classification accuracy(66.1 vs. 66.5).
When comparing with the (Kazama, 2002) even
though the environments is not the same, the pro-
posed two-phase model showed much better per-
formance in both the entity identification (73.6 vs.
81.4) and the entity classification (54.4 vs. 68.0).
One of the reason of the performance improvement
is that we could take discriminative features for each
subtask by separating the task into two subtasks.
6 Conclusion
In this paper, we proposed a new method of two-
phase biomedical named entity recognition based on
SVMs and dictionary-lookup. At the first phase, we
tried to identify each entity with one SVM classifier
and to post-process with a simple dictionary look-up
for correcting the errors by the SVM. At the second
Table 3: Performance comparison with or w/o post-processing(F?=1): (A)10-fold cross validation(1800
abstracts, test with 200 abstracts), (B)training with 590 abstracts, test with 80 abstracts
A B (Kazama, 2002)
No. of W/O PostProc with PostProc No. of W/O PostProc with PostProc No. of
Inst Inst Inst
Identification 76.2/82.4/79.2 76.8/83.1/79.9 78.4/80.8/79.6 80.2/82.6/81.4 75.9/71.4/73.6
Classification 63.6/68.8/66.1 64.0/69.2/66.5 65.8/67.9/66.8 67.0/69.0/68.0 56.2/52.8/54.4
protein 25,276 60.9/79.8/69.1 61.7/78.8/69.2 1,056 61.3/81.3/69.9 62.8/80.7/70.6 709 49.2/66.4/56.5
DNA 8,858 65.1/63.9/64.5 65.0/63.8/64.4 474 71.4/61.0/65.8 72.1/61.6/66.4 460 49.6/37.0/42.3
RNA 683 72.2/71.7/72.0 73.8/72.5/73.1 36 74.4/88.9/81.0 75.6/86.1/80.5
cell line 3,783 71.6/54.2/61.7 72.3/72.3/72.3 201 73.2/44.8/55.6 73.2/44.8/55.6 121 60.2/46.3/52.3
cell type 6,423 67.2/77.5/72.0 67.5/67.5/67.5 252 64.9/82.1/72.5 65.4/81.7/72.7 199 70.0/75.4/72.6
phase, we tried to classify the identified entity into
its semantic class by voting the SVMs. By dividing
the task into two subtasks, the identification and the
semantic classification task, we could select more
relevant features for each task and take an alternative
classification method according to the task. This is
resulted into the mitigation effect of the unbalanced
class distribution problem but also improvement of
the performance of the overall tasks.
References
N. Collier, C. Nobata, and J. Tsujii 2000. Extracting
the Names of Genes and Gene Products with a Hidden
Markov Model. In Proc. of Coling2000, pages 201-
207.
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi. 1998.
Information extraction: identifying protein nmes from
biological papers. In Proc. of the Pacific Symposium
on Biocomputing ?98(PSB?98).
GENIA Corpus 3.0p. 2003. available
at http://www-tsujii.is.s.u-tokyo.ac.jp/ ge-
nia/topics/Corpus/3.0/GENIA3.0p.intro.html
V. Hatzivalssiloglou, P. A. Duboue, and A. Rzhetsky.
2001. Disambiguating proteins, genes, and RNA in
text: a machine learning approach. Bioinformatics. 17
Supple 1.
C. Hsu and C. Lin. 2001. A comparison on methods for
multi-class support vector machines. Technical report,
National Taiwan University, Taiwan.
T. Joachims. 1998. Making Large-Scale SVM Learning
Practical. LS8-Report, 24, Universitat Dortmund, LS
VIII-Report.
T. Joachims. 2000. Estimating the generalization per-
formance of a SVM efficiently. In Proc. of the Seven-
teenth International Conference on Machine Learning.
Morgan Kaufmann, pages 431-438.
SVM Light. 2002. available at
http://svmlight.joachims.org/
Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta and
Jun?ichi Tsujii. 2002. Tuning support vector machines
for biomedical named entity recognition. In Proc. of
ACL-02 Workshop on Natural Language Processing in
the Biomedical Domain, pages 1-8.
U. H.-G Krebel 1999. Pairwise Classification and Sup-
port Vector machines. In B. Scholkopf, C.J.C. Burges,
Advances in Kernel Methods: Support Vector Learn-
ing, pp. 255-268, The MIT Press, Cambridge, MA.
C. Nobata, N. Collier, and J. Tsujii. 1999. Automatic
term identification and classification in biology texts.
In Proc. of the 5th NLPRS, pages 369-374.
B.J. Stapley, L.A. Kelley, and M.J.E. Sternberg. 2002.
Predicting the Sub-Cellular Location of Proteins from
Text Using Support Vector Machines. In Proc. of Pa-
cific Symposium on Biocomputing 7, pages 374-385.
Vladimir Vapnik. 1998. Statistical Learning Theory Wi-
ley, New York.
Semantic Role Labeling using Maximum Entropy Model
Joon-Ho Lim, Young-Sook Hwang, So-Young Park, Hae-Chang Rim
Department of Computer Science & Engineering Korea University
5-ka, Anam-dong, SEOUL, 136-701, KOREA
{jhlim, yshwang, ssoya, rim}@nlp.korea.ac.kr
Abstract
In this paper, we propose a semantic role label-
ing method using a maximum entropy model,
which enables not only to exploit rich features
but also to alleviate the data sparseness prob-
lem in a well-founded model. For applying the
maximum entropy model to semantic role la-
beling, we take a incremental approach as fol-
lows: firstly, the semantic roles are assigned to
the arguments in the immediate clause includ-
ing a predicate, and then, the semantic roles are
assigned to the arguments in the upper clauses
by using previously assigned labels. The exper-
imental result shows that the proposed method
has about 64.76% (F1-measure) on the test set.
1 Introduction
The semantic role represents the relationship between a
predicate and an argument. It provides a general semantic
interpretation of the sentence, and it can play a key role
in NLP. The shared task of CoNLL-2004 concerns the
automatic semantic role labeling (Carreras, 2004). The
challenge for this task is to come forward with machine
learning approaches which based on only partial syntactic
information such as words, POS tags, chunks, clauses,
and named entities.
Some machine learning approaches for semantic role
labeling have been previously developed (Gildea, 2002;
Pradhan, 2003; Thompson, 2003). Gildea (2002) pro-
posed a probabilistic discriminative model to assign a
semantic roles to the constituent. However, it needs a
complex interpolation for smoothing because of the data
sparseness problem. Pradhan (2003) applied a support
vector machine to semantic role labeling, but if it use
a polynomial kernel function for the dependencies be-
tween features, it requires high computational complex-
ity. Futhermore, becuase the SVM is a binary classifier,
one-vs-rest or pairwise method is required for multi-class
classification. Thompson (2003) proposed a probabilis-
tic generative model which the constituents is generated
by the semantic roles. In this model, because a con-
stituent depends only on the role that generated it, and
constituents are independent of each other, so this model
can not utilize contextual information or a relational in-
formation between the constituent and the predicate.
In this paper, we propose a semantic role labeling
method using a maximum entropy model. It is moti-
vated by the thought of that for building a successful
model, some knowledge of the task are reflected into
the model based on the machine learning technique. In
this method, we try to combine the structural linguistic
knowledge linking syntax to semantics into the machine
learning technique. It is realized in terms of two aspects:
one is the model framework, the other is the design of
feature sets. First of all, for the model framework, we uti-
lize the syntactic knowledge of representing the semantic
roles in a clause: the arguments of a predicate are located
in the immediate clause or the upper clauses. Secondly,
for the feature sets, we consider the relation between syn-
tactic and semantic characteristics of a given context. For
implementing the method with a machine learning algo-
rithm, we take a maximum entropy model, which enables
not only to exploit rich features but also to alleviate the
data sparseness problem in a well-founded model.
The remaining of the paper is organized as follows:
section 2 describes the proposed semantic role labeling
method using a maximum entropy model. Section 3
presents feature sets for semantic role labeling. Section 4
shows some experimental results of the proposed method.
Finally, section 5 concludes with some directions of fu-
ture works.
2 Semantic Role Labeling using ME
In the maximum entropy framework (Berger, 1996), the
conditional probability of predicting an outcome y given
Figure 1: An example of the semantic role labels and an incremental approach.
a history x is defined as follows :
P (y|x) =
1
Z(x)
exp
(
k
?
i=1
?
i
f
i
(x, y)
)
where f
i
(x, y) is the feature function, ?
i
is the weighting
parameter of f
i
(x, u), k is the number of features, and
Z(x) is the normalization factor for
?
y
p(y|x) = 1.
Given a predicate and its partial parse tree represented
by constituents such as chunks and clauses, the proba-
bilistic model for semantic role labeling assigns the se-
mantic role labels to the constituents as described in the
equation (1).
R
best
= argmax
R
P (R|c
1n
, pred)
= argmax
R
?
n
i=1
P (r
i
|c
1n
, pred, r
1...i?1
) (1)
where R is a sequence of the semantic roles, c
1n
is a se-
quence of constituents, pred is the given predicate, r
i
is
the i-th semantic role, n is the number of constituents.
In order to apply the equation (1) to an incremental
approach, we classify clauses into the immediate clause
and the upper clause. The immediate clause is the clause
which contains the target predicate, and the upper clause
is the clause which includes the immediate clause. Gen-
erally, most of the arguments of the predicate are located
in the immediate clause while some of them are located
in the upper clauses, especially the first or second up-
per clauses. Since it is much easier and more reliable to
identify the arguments in the immediate clause, the pro-
posed method first assigns the semantic role labels to the
constituents 1 in the immediate clause. Then, it assigns
the semantic role labels to the constituents in the upper
clauses by using previously assigned labels. This incre-
mental approach is described in the equation (2) derived
from the equation (1).
R
best
= argmax
R
?
n
i=1
P (r
i
|c
1n
, pred, r
1...i?1
)
? argmax
R
?
m
i=1
P (r
i
|?
1
(c
1n
, pred, r
1...i?1
))
?
?
n
i=m+1
P (r
i
|?
2
(c
1n
, pred, r
1...i?1
)) (2)
1Here, we regard a chunk or a clause as a constituent.
where m is the number of constituents covered by the im-
mediate clause, ?
1
is a feature set for immediate clause,
and ?
2
is a feature set for upper clauses.
A semantic role label(r
i
) is represented by using a BIO
notation such as B-A*, I-A*, etc. However, O is too fre-
quently occurred than other semantic role labels, it can
have a somewhat high probability than others. Therefore,
to degrade its probability, we divide the single O into O-,
O+, O0with respect to the position of a constituent which
is relative to the predicate. Therefore, B-A*, I-A*, O-,
O+, and O0 are used as semantic roles as shown in Fig-
ure 1.
After processing the equation (2), we use some heuris-
tic to attach the some semantic roles and to adjust the
boundary of semantic arguments in the post-processing
step. More specifically, we use some rules to attach the
V, AM-MOD, and AM-NEG, and extend the boundary
of core roles to include to infinitive of the VP chunk like
?expect/B-VP (A1 to/I-VP take/I-VP dive/B-NP)?.
3 Feature Sets for Semantic Role Labeling
For accurate semantic role labeling, we regard that the
following information is important: the contextual infor-
mation of the constituent, the syntactic information of the
predicate, and the relation between the constituent and
the predicate. Therefore, we use the features presented in
Table 1 for semantic role labeling. For example, Figure
previous-label(pl)
predicate-POS(predpos), predicate-lex(predlex)
predicate-type(predtype)
tag(ctag), voice(v), position(p), path(path)
head-lex(hl), head-POS(hp), content-head(chl)
prev-tag(ptag), prev-head-lex(phl)
next-tag(ntag), next-head-lex(nhl)
path-immediate-clause(path-im-cl)
path-begin-end(path-beg-end)
level-of-clause(l-cl), is-clause-boundary(cl-bn)
immediate-clause-roles(im-cl-roles)
Table 1: Features for semantic role labeling.
Figure 2: Some instances extracted from example of Figure 1.
feature set ?
1
for immediate clause feature set ?
2
for upper clause
pl, ctag, ctag+v+p, ctag+v+p+pl pl, ctag, ctag+v+p
ptag+ctag, ctag+ntag ptag+ctag, ctag+ntag
hp+p, hp+p+ntag hp+p, hl+ctag,
predtype+ctag predtype+ctag
predlex+hl, predlex+ctag+v+p, predlex+ctag+pl predlex+hl, predlex+ctag+v+p, predlex+ctag+pl
predpos+p, predpos+hp+pl, predpos+ctag
path, path+hp+v, path+nhl, path+predlex path-im-cl, path-im-cl+ctag+v, path-beg-end
hl+p, hl+ctag, hl+ctag+predlex ctag+l-cl, ptag+ctag+l-cl, ctag+ntag+l-cl
chl+pl, chl+pl+predlex ctag+cl-bn, ptag+ctag+cl-bn, ctag+ntag+cl-bn
chl+phl, chl+phl+predlex im-cl-roles
Table 2: Conjoined Feature Sets
2 shows how the features in Table 1 are used for labeling
semantic roles to the proposition in Figure 1.
Because the maximum entropy model assumes the in-
dependence of features, we should conjoin the coherent
features. As presented in Table 2, we use the conjoined
feature sets to assign semantic roles to the constituents of
the immediate clause and the upper clauses.
The predicate-type feature represents the predicate us-
age such as to-infinitive form (TO), the beginning of the
immediate clause (BEG), and otherwise (SEN). The tag
feature represents the tag of the current constituent. If it
is a clause, it is subdivided into a relative pronoun, a in-
finitival relative clause, etc according to its represented
form.
The path feature indicates the sequence of constituent
tags between the current constituent and the predicate.
The voice feature is determined to be an active or pas-
sive voice of the predicate, and the position feature is as-
signed by the constituent position with respect to pred-
icate. These features implicitly represent the predicate-
argument relation such as predicate-subject or predicate-
object.
For the headword feature, we use the Collins? head-
word rules, and as a complementary feature to the head
word feature, a content word feature2 is used to represent
the content of the PP, VP, or CONJP chunk.
The path-immediate-clause feature is the sequence of
constituent tags between the current constituent and the
immediate clause, and the path-begin-end feature is the
sequence between current constituent and beginning/end
of clause. The level-of-clause feature indicates whether
the current constituent is located in the first upper clause
or in the second upper clause, and the is-clause-boundary
feature is the binary value which indicates the existence
of the starting clause. The immediate-clause-roles fea-
tures are the binary indicators to represent whether the
core arguments exist in the immediate clause or not.
The path-immediate-clause, path-begin-end, level-of-
clause, is-clause-boundary, and immediate-clause-roles
features are used only in the second phase, and the others
except the path feature and the content-head feature are
used in common.
2For example, if the PP-chunk is because of, the headword
feature is of, and the content word feature is because.
Precision Recall F
?=1
Overall 68.42% 61.47% 64.76
A0 79.20% 75.73% 77.42
A1 67.41% 64.65% 66.00
A2 52.65% 45.94% 49.07
A3 52.53% 34.67% 41.77
A4 63.16% 48.00% 54.55
A5 0.00% 0.00% 0.00
AM-ADV 46.75% 35.18% 40.15
AM-CAU 57.69% 30.61% 40.00
AM-DIR 48.28% 28.00% 35.44
AM-DIS 60.11% 50.23% 54.73
AM-EXT 58.33% 50.00% 53.85
AM-LOC 35.56% 35.09% 35.32
AM-MNR 51.26% 23.92% 32.62
AM-MOD 89.77% 91.10% 90.43
AM-NEG 86.15% 88.19% 87.16
AM-PNC 48.98% 28.24% 35.82
AM-PRD 100.00% 33.33% 50.00
AM-TMP 59.51% 42.70% 49.73
R-A0 86.96% 75.47% 80.81
R-A1 57.89% 62.86% 60.27
R-A2 50.00% 33.33% 40.00
R-A3 0.00% 0.00% 0.00
R-AM-LOC 33.33% 25.00% 28.57
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 42.86% 21.43% 28.57
V 97.99% 97.99% 97.99
Table 3: Experimental results on the test set.
4 Experiments
To test the proposed method, we have experimented on
CoNLL-2004 datasets. For our experiments, we use the
Zhang le?s MaxEnt toolkit 3, and the L-BFGS parame-
ter estimation algorithm with Gaussian Prior smoothing
(Chen, 1999). The results on the test set are shown in
Table 3, and Table 4 shows the overall results when the
model is tested on the training set, the development set,
and the test set.
From these experimental results, we can find that the
proposed model has relatively high performance on the
labels related to A0 and A1, while it has relatively low
performance on the other labels. This may be caused by
following two reasons. Firstly, the instances of A0 or A1
are provided enough for accurate semantic role labeling.
Secondly, the thematic roles of A0 and A1 are more clear
than other core semantic roles. For example, agent is la-
beled as mainly A0 while benefactive can be labeled as
A2 or A3. Therefore, the maximum entropy model can
3http://www.nlplab.cn/zhangle/maxent toolkit.html
Precision Recall F
?=1
Overall(training) 96.40% 92.28% 94.29
Overall(dev) 69.78% 62.56% 65.97
Overall(test) 68.42% 61.47% 64.76
Table 4: The results when the model is tested on the train-
ing set, the development set, and the test set
get a good generalize performance in case of A0 or A1,
but can?t generalize well in other cases.
5 Conclusion
In this paper, we propose a semantic role labeling method
using a maximum entropy model. Because the maximum
entropy model enables not only to exploit rich features
but also to alleviate the data sparseness problem, we use
it to model the probability of a semantic role label se-
quence. The proposed method has following characteris-
tics: firstly, it assigns the semantic role labels to the con-
stituents in the immediate clause, and then assigns role
labels to the constituents in the upper clauses, and it uti-
lizes the relation between syntactic and semantic charac-
teristics of a given context.
For the future work, we will device a method of clus-
tering for the path and predicate features, and include the
clustering results as additional features.
References
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to natu-
ral language processing . Computational Linguistics,
22(1):39?71.
Xavier Carreras, and Lluis Marquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic Role
Labeling . Proceedings of CoNLL-2004.
S. Chen and R. Rosenfeld. 1999. A Gaussian prior for
smoothing maximum entropy models . Technical Re-
port CMUCS-99-108, Carnegie Mellon University.
Daniel Gildea, and Daniel Jurafsky. 2002. Automatic La-
beling of Semantic Roles . Computational Linguistics,
28(3):245-288.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin and Daniel Jurafsky.
Oct 2003. Support Vector Learning for Semantic Ar-
gument Classification . Technical Report, TR-CSLR-
2003-03.
Cynthia A. Thompson, Roger Levy and Christopher D.
Manning. A Generative Model for Semantic Role La-
beling . ECML 2003, 397-408.
Two-Phase Semantic Role Labeling based on Support Vector Machines
Kyung-Mi Park and Young-Sook Hwang and Hae-Chang Rim
Department of Computer Science & Engineering, Korea University
5-ka, Anam-dong, SEOUL, 136-701, KOREA
{kmpark, yshwang, rim}@nlp.korea.ac.kr
Abstract
In this study, we try to apply SVMs to the se-
mantic role labeling task, which is one of the
multiclass problems. As a result, we propose a
two-phase semantic role labeling model which
consists of the identification phase and the clas-
sification phase. We first identify semantic ar-
guments, and then assign semantic roles to the
identified semantic arguments. By taking the
two-phase approach, we can alleviate the un-
balanced class distribution problem, and select
the features appropriate for each task.
1 Introduction
A semantic role in a language is a semantic relation-
ship between a syntactic constituent and a predicate. The
shared task of CoNLL-2004 relates to recognize seman-
tic roles in English (X. Carreras, 2004). Given a sentence,
the task is to analyze a proposition expressed by a target
verb of a sentence. Especially, for each target verb, all
constituents in a sentence which fill semantic roles of the
verb have to be recognized. This task is based only on
partial parsing information, avoiding use of a full parser
and external lexico-semantic knowledge base. According
to previous results of the CoNLL shared task, the POS
tagged, chunked, clause identified, and named-entity rec-
ognized sentences are given as an input (Figure 1).
SVM is a well-known machine learning algorithm with
high generalization performance in high dimensional fea-
ture spaces (H. Yamada, 2003). Also, learning with com-
bination of multiple features is possible by virtue of poly-
nomial kernel functions. However, since it is a binary
classifier, we are often confronted with the unbalanced
class distribution problem in a multiclass classification
task. The larger the number of classes, the more severe
the problem is. The semantic role labeling can be formu-
lated as a multiclass classification problem. If we try to
apply SVMs in the semantic role labeling problem, we
have to find a method of resolving the unbalanced class
distribution problem.
Conceptually, semantic role labeling can be divided
into two subtasks: the identification task which finds
the boundary of semantic arguments in a given sentence,
and the classificiation task which determines the seman-
tic role of the argument. This provides us a hint of using
SVMs with less severe unbalanced class distribution. In
this paper, we present a two-phase semantic role label-
ing method which consists of an identification phase and
a classification phase. By taking two phase model based
on SVMs, we can alleviate the unbalanced class distri-
bution problem. That is, since we find only the bound-
ary of an argument in the identification phase, the num-
ber of classes is decreased into two (ARG, NON-ARG)
or three (B-ARG, I-ARG, O). Therefore, we have to build
only one or three SVM classifiers. We can alleviate the
unbalanced class distribution problem by decreasing the
number of negative examples, which is much larger than
the number of positive exampels without two-phase mod-
eling. In the classification phase, we classify only the
identified argument into a proper semantic role. This en-
ables us to reduce the computational cost by ignoring the
non-argument constitutents.
Since features for identifying arguments are different
from features for classifying a role, we need to determine
different feature sets appropriate for the tasks. For iden-
tification, we focus on the features to detect the depen-
dency between a constituent and a predicate because the
arguments are dependent on the predicate. For seman-
tic role labeling, we consider both the syntactic and the
semantic information such as the sentential form of the
target predicate, the head of a constituent, and so on. In
the following sections, we will explain the two phase se-
mantic role labeling method in detail and show some ex-
perimental results.
Figure 1: An example of semantic role labeling. The columns contain: the word, its POS, its chunk type, clause
boundary, its named-entity tag, the target predicate, the result of semantic role labeling in the target predicate exist,
and deliver.
2 Two Phase Semantic Role Labeling
based on SVMs
We regard the semantic role labeling as a classification
problem of a syntactic constituent. However, a syntac-
tic constituent can be a chunk, or a clause. Therefore, we
have to identify the boundaries of semantic arguments be-
fore we assign roles to the arguments.
2.1 Semantic Argument Identification
This phase is the step of finding the boundary of seman-
tic arguments. A sequence of chunks or a subclause in
the immediate clause of a predicate can be a semantic ar-
gument of the predicate. A chunk or a subclause of the
predicate becomes a unit of the constituent of an argu-
ment. The chunks within the subclause are ignored.
For identifying the semantic arguments of a target
predicate, it is necessary to find the dependency rela-
tion between each constituent and a predicate. Identify-
ing a dependency relation is important for identifying a
subject/object relation (S. Buchholz, 2002) and also for
identifying the semantic arguments of a target predicate.
Therefore, the features for finding dependency relations
are implicitly represented in the feature set for the identi-
fication task.
For implementing the method based on the SVMs, we
represent a constituent of an argument with B/I/O nota-
tion, and assign one of the following classes to each con-
stituent: B-ARG class representing the beginning of se-
mantic argument, I-ARG class representing a part of a
semantic argument, or O class indicating that the con-
stituent does not belong to the semantic arguments.
Because we decide the unit of a constituent as a chunk
or a subclause, words except the predicate in the target
phrase 1 do not belong to constituent. Therefore, these
words have to be handled independently. In the training
data, we often observed that the beginning of semantic
arguments starts from the word right after the predicate.
For the agreement with the chunk boundary, we regard
the word following a predicate as the beginning word of
a new chunk. Namely, when the beginning of chunk tag
is I, we change I to B. Also, the words located in front of
the predicate in the target phrase are post-processed by 4
hand-crafted rules 2 and 211 automated rules 3 based on
frequency in the training data.
In order to restrict the search space in terms of the con-
stituents, we use the clause boundaries. The left search
boundary for identifying the semantic argument is set to
the left boundary of the second upper clause, and the right
search boundary is set to the right boundary of the imme-
diate clause.
2.1.1 Features for Identifying Semantic Argument
For this phase, we use 29 features for representing syn-
tactic and semantic information related to constituent and
predicate. Table1 shows a set of features employed. The
features can be described as follows:
? position: This is a binary feature identifying
whether the constituent is before (-1) or after (1) the
predicate in the immediate clause. The feature value
1The chunk containing a predicate is referred to as target
phrase.
2For example, if a word in target phrase is n?t, not or Not,
and POS tag of the word is RB and the distance between the
word and the predicate is less than 4, then the semantic role is
AM-NEG.
3For example, if a word in target phrase is already, and POS
tag of the word is RB, then the semantic role is AM-TMP.
Features examples
position -2, -1, 1
distance 0, 1, 2, . . .
predicate-candidate # of VP, NP, SBAR 0, 1, 2, . . .
(intervening features) # of POS [CC], [,], [:] 0, 1, 2, . . .
POS [?] & POS [?] -1, 0, 1
path VP-PP-NP, . . .
headword, headword?s POS, chunk type
predicate itself & context beginning word?s POS MD, TO, VBZ, . . .
context-1: headword, headword?s POS, chunk type
headword, headword?s POS, chunk type
candidate itself & context context-2: headword, headword?s POS, chunk type
context-1: headword, headword?s POS, chunk type
context+1: headword, headword?s POS, chunk type
Table 1: Features for Identifying a semantic argument
Figure 2: Two-phase semantic role labeling procedure using the example sentence presented in Figure 1. (P means the
target phrase containing the predicate deliver, and C means the constituent such as a chunk (e.g. Under) or a subclause
(e.g. Rockwell said))
(-2) means that the constituent is out of the immedi-
ate clause.
? distance: The distance is measured by the number
of chunks between the predicate and the constituent.
? # of VP, NP, SBAR: These are numeric features rep-
resenting the number of the specific chunk types be-
tween the predicate and the constituent.
? # of POS [CC], [,], [:]: These are numeric features
representing the number of the specific POS types
between the predicate and the constituent.
? POS [?] & POS [?]: This is used as a feature rep-
resenting the difference between # of POS[?] and #
of POS[?] counted in the range from the predicate
to the constituent. In Table 1, the feature value (-1)
means that # of POS[?] is larger than # of POS[?].
The feature value (1) conversly means that # of
POS[?] is larger than # of POS[?]. The featue value
(0) means that # of POS[?] is equal to # of POS[?].
? path: This is the syntactic path from the predicate to
the constituent, and is a symbolic feature comprising
all the elements (chunk or subclause) between the
predicate and the constituent.
? beginning word?s POS: In the target phrase, these
values appear only with VPs and represent the POS
of the syntactic head (MD, TO, VB, VBD, VBG,
VBN, VBP, VBZ). This represents the property of the
target phrase, for example, the feature value TO in-
dicates that the target phrase is to-infinitive.
? context: These are information for the predicate it-
self, the left context of the predicate, the constituent
itself, and the left and right context of the con-
stituent. In Table 1, - means the left context, and +
means the right context. In case that the constituent
is the subclause, the chunk type of the constituent is
set to the first chunk type of the subclause.
2.2 Semantic Role Assignment
In this phase, we assign appropriate semantic roles to the
identified semantic arguments. For learning SVM classi-
fiers, we consider not all semantic roles, but only 18 se-
mantic roles based on frequency in the training data (Ta-
ble 2). The (AM-MOD, AM-NEG) are post-processed by
hand-crafted rules. As we decrease the number of SVM
classifiers to be learned in the training data, the training
cost of classifiers can be reduced. Furthermore, we can
alleviate the unbalanced class distribution problem by ex-
semantic role
A0, A1, A2, A3, A4, R-A0, R-A1, R-A2, C-A1
AM-TMP, AM-ADV, AM-MNR, AM-LOC, AM-DIS
AM-PNC, AM-CAU, AM-DIR, AM-EXT
Table 2: 18 semantic roles
cluding the infrequent classes.
2.2.1 Features for Assigning Semantic Role
This phase also uses all features applied in the seman-
tic argument identification phase, except for # of POS [:]
and POS[?] & POS[?]. In addition, we use the following
feature.
? voice: This is a binary feature identifying whether
the target phrase is active or passive.
In Figure 2, we show two-phase semantic role labeling
procedure using the example sentence in Figure 1.
3 Experiments
For experiments, we utilized the SVM light package (T.
Joachims, 2002). In both the semantic argument identifi-
cation and the semantic role assignment phase, we used a
polynomial kernel (degree 2) with the one-vs-rest classi-
fication method. Table 3 shows the experimental results
on the test set and Table 4 shows the experimental results
on the development set. Table 4 also shows the perfor-
mance of each phase.
For improving the performance, we try to select the
discrminative features for each subtask. Especially, since
the performance of the identification phase is critical
to the total performance, we concentrate on improving
the identification performance. Our system obtains a F-
measure of 74.08 in the identification phase, as present-
eded in Table 4. For the argument classification task, the
our system obtains a classification accuracy (A) of 85.45.
4 Conclusion
In this paper, we present a method of two phase seman-
tic role labeling based on the support vector machines.
We found that SVM is useful to incorporate the hetero-
geneous features for the semantic role labeling. Also, by
applying the two phase model, we can alleviate the unbal-
anced class distribution problem caused by the the nega-
tive examples. Experimental results show that our system
obtains a F-measure of 63.99 on the test set and 65.78 on
the development set.
Precision Recall F?=1
Overall 65.63% 62.43% 63.99
A0 78.24% 74.60% 76.38
A1 65.83% 66.46% 66.14
A2 49.84% 43.70% 46.57
A3 56.04% 34.00% 42.32
A4 62.86% 44.00% 51.76
A5 0.00% 0.00% 0.00
AM-ADV 45.18% 44.30% 44.74
AM-CAU 36.67% 22.45% 27.85
AM-DIR 20.00% 20.00% 20.00
AM-DIS 56.62% 58.22% 57.41
AM-EXT 61.54% 57.14% 59.26
AM-LOC 26.01% 31.14% 28.34
AM-MNR 43.54% 35.69% 39.22
AM-MOD 97.46% 91.10% 94.17
AM-NEG 94.92% 88.19% 91.43
AM-PNC 40.00% 28.24% 33.10
AM-PRD 0.00% 0.00% 0.00
AM-TMP 51.83% 45.38% 48.39
R-A0 80.49% 83.02% 81.73
R-A1 75.00% 51.43% 61.02
R-A2 100.00% 33.33% 50.00
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 0.00% 0.00% 0.00
V 96.66% 96.66% 96.66
Table 3: Results on the test set: closed challenge
Precision Recall F?=1 A
Overall 67.27% 64.36% 65.78 -
identification 75.96% 72.30% 74.08 -
classification - - - 85.45
Table 4: Results on the development set: closed chal-
lenge. (A means accuracy.)
References
X. Carreras and L. Marquez. 2004. Introduction to the
CoNLL-2004 Shared Task: Semantic Role Labeling.
CoNLL.
H. Yamada and Y. Matsumoto. 2003. Statistical
Dependency Analysis with Support Vector Machines.
IWPT03.
S. Buchholz 2002. Memory-Based Grammatical Rela-
tion Finding. PhD. thesis, Tilburg University.
T. Joachims 2002. SVM Light available at http:// svm-
light.joachims.org/.
Paraphrasing depending on Bilingual Context Toward Generalization of
Translation Knowledge
Young-Sook Hwang
ETRI
161, Yuseong-gu, Daejeon
305-700, KOREA
yshwang7@etri.re.kr
YoungKil Kim
ETRI
161, Yuseong-gu, Daejeon
305-700, KOREA
kimyk@etri.re.kr
SangKyu Park
ETRI
161, Yuseong-gu, Daejeon
305-700, KOREA
parksk@etri.re.kr
Abstract
This study presents a method to automat-
ically acquire paraphrases using bilingual
corpora, which utilizes the bilingual de-
pendency relations obtained by projecting
a monolingual dependency parse onto the
other language sentence based on statisti-
cal alignment techniques. Since the para-
phrasing method is capable of clearly disam-
biguating the sense of an original phrase us-
ing the bilingual context of dependency re-
lation, it would be possible to obtain inter-
changeable paraphrases under a given con-
text. Also, we provide an advanced method
to acquire generalized translation knowl-
edge using the extracted paraphrases. We
applied the method to acquire the gener-
alized translation knowledge for Korean-
English translation. Through experiments
with parallel corpora of a Korean and En-
glish language pairs, we show that our para-
phrasing method effectively extracts para-
phrases with high precision, 94.3% and
84.6% respectively for Korean and English,
and the translation knowledge extracted
from the bilingual corpora could be general-
ized successfully using the paraphrases with
the 12.5% compression ratio.
1 Introduction
Approaches based on bilingual corpora are promis-
ing for the automatic acquisition of translation
knowledge. Phrase-based SMT(Statistical Machine
Translation) models have advanced the state of the
art in machine translation by expanding the basic
unit of translation from words to phrases, which al-
lows the local reordering of words and translation
of multi-word expressions(Chiang, 2007) (Koehn
et al, 2003) (Och and Ney, 2004).
However phrase-based SMT techniques suffer
from data sparseness problems, that is; unreliable
translation probabilities of low frequency phrases
and low coverage in that many phrases encountered
at run-time are not observed in the training data.
An alternative for these problems is to utilize para-
phrases. An unknown phrase can be replaced with
its paraphrase that is already known. Moreover, we
can smooth the phrase translation probability using
the class of paraphrases.
On the other hand, EBMT or PBMT systems
might translate a given sentence fast and robustly
geared by sentence translation patterns or general-
ized transfer rules. Since it costs too much to con-
struct the translation knowledge, they suffer from
the problem of knowledge acquisition bottleneck.
In this study, we present a method of automat-
ically extracting paraphrases from bilingual cor-
pora. Furthermore, we introduce a new method
for acquiring the generalized translation knowledge.
The translation knowledge is a kind of verb sub-
categorization pattern composed of bilingual depen-
dency relations. We obtain the generalized trans-
lation knowledge by grouping the equivalent con-
stituent phrases. The task of identifying the phrases
equivalent to each other is defined as paraphrasing.
Our paraphrasing method utilizes bilingual cor-
pora and alignment techniques in SMT. Unlike pre-
327
vious approaches which identify paraphrases using
a phrase in another language as a pivot without con-
text information (Bannard et al, 2005), or apply
the distributional hypothesis to paths in dependency
trees for inferring paraphrasing rules from monolin-
gual corpora(Lin et al, 2001), we take the bilingual
context of a bilingual dependency relation into ac-
count for disambiguating the sense of paraphrases.
First, we create a large inventory of bilingual de-
pendency relations and equate the pairs of depen-
dency relations that are aligned with a single depen-
dency relation in the other language as paraphrased
dependency relations. Then, we extract the phrases
sharing the same head (or modifier) phrase among
the paraphrased dependency relations aligned with a
unique dependency relation in the other language.
We regard them as conceptually equivalent para-
phrases. This work is based on the assumption of
similar meaning when multiple phrases map onto a
single foreign language phrase that is the converse of
the assumption made in the word sense disambigua-
tion work(Diab and Resnik, 2002). The two-step
paraphrasing method allows us to increase the pre-
cision of the paraphrases by constraining the para-
phrase candidates under the bilingual contexts of de-
pendency relations.
In order to systematically acquire the generalized
translation knowledge, our method includes follow-
ing steps:
? Derive a bilingually parsed sentence through
projecting the source language parse onto the
word/phrase aligned target sentence.
? Extract bilingual dependency relations from the
bilingual dependency parses.
? Acquire paraphrases by exploiting the ex-
tracted bilingual dependency relations.
? Generalize the bilingual dependency relations
by substituting the phrases with their para-
phrase class.
2 Extracting Translation Patterns
In this section, we introduce a method to acquire
translation knowledge like a bilingual dependency
pattern using bilingual corpus. The bilingual depen-
dency pattern is defined as an asymmetric binary re-
lationship between a phrase called head and another
phrase called modifier which are paired with their
corresponding translations in the other language. In
order to acquire the bilingual dependency relations,
we do bilingual dependency parsing based on the
word/phrase alignments and extract bilingual depen-
dency relations by navigating the dependency parse
tree.
2.1 Bilingual Dependency Parsing based on
Word/Phrase Alignment
Given an input sentence pair, a source language sen-
tence is dependency parsed in a base phrase level
and a target language sentence is chunked by a shal-
low parser. During the dependency parsing and the
chunking, each sentence is also segmented into mor-
phemes and we regard a morpheme as a word.
We make word alignments through the learning
of IBM models by using the GIZA++ toolkit(Och
and Ney, 2000): we learn the translation model
toward IBM model 4, initiating translation itera-
tions from IBM model 1 with intermediate HMM
model iterations. For improving the word align-
ment, we use the word-classes that are trained from a
monolingual corpus using the srilm toolkit(Stolcke,
2002). Then, we do phrase alignments based on the
word alignments, which are consistent with the base
phrase boundaries as well as the word alignments as
(Hwang et al, 2007) did. A phrase is defined as a
word sequence that is covered by a base phrase se-
quence, not by a single sub-tree in a syntactic parse
tree.
After the word and the phrase alignments, we
obtain bilingual dependency parses by sharing the
dependency relations of a monolingual dependency
parser among the aligned phrases. The bilingual de-
pendency parsing is similar to the technique of bilin-
gual parsing in a word level described in (Hwa et al,
2005)(Quirk et al, 2005). Our bilingual parsing in a
phrase level has an advantage of being capable of re-
ducing not only the parsing complexity but also the
errors caused by structural differences between two
languages, such like a Korean and English pairs1.
For bilingual parsing between Korean and En-
glish, we use a Korean dependency parse on the
1Since we regard that a phrase in a source language sentence
is aligned with a target phrase if at least one word in a source
phrase is aligned with the words in a target phrase, we robustly
project the source phrases onto the target phrases.
328
Figure 1: Illustration of Acquiring Bilingual Dependency Relations
source language side as a pivot. Figure 1 shows an
illustration of bilingual dependency parsing between
Korean and English based on the word/phrase align-
ments. The dependency structure induced on the tar-
get language side is in some sense isomorphic to the
structure of the source language.
2.2 Extracting Bilingual Dependency Patterns
Starting from the head phrase of a given source lan-
guage sentence, we extract bilingual dependency re-
lations by traversing a bilingual dependency parse
tree. A dependency relation is a binary relation be-
tween a head and modifier phrases. Each phrase is
paired with its corresponding translation. For effec-
tively using them during the decoding or the sen-
tence generation, we attach an additional tag for in-
dicating the order(e.g. Reverse or Forward) of target
language phrases to the bilingual dependency rela-
tion. A dependency pattern refers to the bilingual
dependency relation with the phrase order tag.
Figure 1(c) shows some examples of bilingual de-
pendency patterns extracted from the bilingual de-
pendency parse tree in Figure 1(b). In the exam-
ple, Korean phrase ?sinae ga neun? aligned with
the English phrase ?for downtown? modifies the
phrase ?bus siganpyo? aligned with the English ?the
bus timetable?. Through traversing the dependency
parse trees, we acquire the bilingual dependency
pattern <sinae ga neun:for downtown, bus sigan-
pyo:the bus timetable;Reverse>.
If we apply the bilingual dependency pattern
<sinae ga neun:for downtown, bus siganpyo:the
bus timetable;Reverse> for machine translation of
a given Korean expression ?sinae ga neun bus sigan-
pyo?, we might generate an English phrase ?the bus
timetable for downtown? by reversing the order of
English head and modifier phrase corresponding to
the Korean phrase ?sinae ga neun bus siganpyo?.
3 Acquisition of Paraphrases
Paraphrasing is based on the assumption that if
multiple Korean phrases are equivalent to each
other, they can be translated into a single English
phrase. But, the reverse is not always true. That
is, even though a single phrase in a source lan-
guage sentence maps onto multiple phrases in a
foreign language sentence, the phrases might not
be paraphrases. For example, two different Ko-
rean phrases, ?gyedan/{stairs,steps}? and ?baldong-
jak/steps?, might be translated into a single English
phrase ?the steps?. But since the meaning of two
Korean phrases is not equivalent to each other, the
Korean phrases cannot be paraphrases. This implies
that the sense of candidate paraphrases should be
disambiguated depending on a given context.
For extracting the paraphrases of which sense is
disambiguated under a given context, we give a
strong constraint on paraphrases with bilingual con-
text evidence of dependency relation denoted as R(x,
y) :
329
Figure 2: Illustration of Paraphrasing based on Bilingual Dependency Relations
R(ei, ej) ? R(kai , kaj ) and R(ei?, ej?) ? R(kai? , kaj? ) (1)
? R(kai , kaj ) ? R(kai? , kaj? )
where the relation of R(ei, ej) = R(ei?, ej?) with the
condition of ei = ei? and ej = ej? .
R(ei, ej) ? R(kai , kaj ) and R(kai , kaj ) ? R(kai? , kaj? ) (2)
? kai ? kai? iff kaj ? kaj?
For the identification of paraphrases, we equate the
different dependency relations aligned with a unique
dependency relation in the other language and regard
them as a set of paraphrased dependency relations
(see eq.(1)). Under the constraint of the paraphrased
dependency relations, we again try to acquire para-
phrases at a phrase level. That is, we extract the
phrases sharing the same head/modifier phrase in
paraphrased dependency relations as a phrase para-
phrase under a given bilingual dependency context
(see eq.(2)).
Figure 2 shows some examples of paraphrased de-
pendency relations and paraphrases. In Figure 2 (a),
the Korean dependency relations <bus siganpyo,
sinae ga neun>,<bus seukejul, sinae ga neun> and
<bus seukejul, sinae banghyang> mapped onto the
English relation<the bus timetable, for downtown>
are the paraphrases. Under the condition of para-
phrased dependency relations, the phrases, ?bus
seukejul? and ?bus siganpyo? modified by the same
phrase ?sinae ga neun? are extracted as paraphrases.
In the same way, the set of modifier phrases,
p1={?sinae banghyang?, ?sinae ga neun?} is ac-
quired as a paraphrase set. For English, we obtain
the set of paraphrases, p3={?the bus timetable?, ?the
bus schedule?} as we did for Korean.
The induced set of paraphrases can be applied
to dependency relations to extend the set through
higher inference as in Figure 2(b). We replace a
phrase, which is a part of a bilingual dependency
relation and a member of a paraphrase set with the
representative phrase of the paraphrase set. And we
repeatedly apply the paraphrase extraction algorithm
to the bilingual dependency relations of which a part
is replaced with the previously acquired paraphrase
set. Finally, we can acquire new paraphrase sets
such as p4 and p5.
4 Generalizing Translation Patterns
The acquired paraphrases can be utilized for various
NLP applications. In this work, we focus on mak-
ing use of the paraphrases to generalize the trans-
lation knowledge of bilingual dependency patterns.
By generalizing the bilingual dependency patterns,
we aim at increasing the coverage of them without
any over-generation.
The algorithm for generalizing bilingual depen-
dency patterns is very simple. The main idea
is to replace the constituent phrases of a given
bilingual dependency pattern with their paraphrase
classes. The paraphrase classes are extracted un-
der the condition of a given bilingual context as
follows: < PP (km, dpi) : PP (em, dpi), PP (kh, dpi) :
PP (eh, dpi);Order := Reverse|Forward > where the
330
Figure 3: Illustration of Generalizing Bilingual Dependency Patterns
function, PP (x, y) returns the identifier of the para-
phrase set of a given phrase x, which is constrained
on a given context y = dpi; km and kh denote a
modifier and a head in Korean, respectively and em
and eh denote the English phrases.
Figure 3 shows an illustration of generalizing the
translation patterns using the previously acquired
paraphrase classes. In the pattern dpi, the English
modifier ?for downtown? uses the phrase itself be-
cause there is no paraphrase class. But, the others
are generalized by using their paraphrase classes.
5 Experiments
We used the Basic Travel Expression Corpus
(BTEC)(Takezawa et al, 2002), a collection of con-
versational travel phrases for Korean and English.
We used 152,175 sentences in parallel corpus for
training and 10,146 sentences for test. The Korean
sentences were automatically dependency parsed by
in-house dependency parser and the English sen-
tences were chunked by in-house shallow parser.
Through experiments, we investigated the accu-
racy of the acquired paraphrases, and the compres-
sion ratio of the generalized translation patterns
compared to the raw translation patterns. Moreover,
we show the strength of utilizing bilingual context
information in the acquisition of paraphrases with
the comparison to the previous approach.
5.1 Accuracy of the Acquired Paraphrases
Through the alignments and bilingual dependency
parsing, we extracted 66,664 bilingual dependency
relations. 24.15% of Korean phrases and 21.8% of
English phrases are paraphrased with more than two
phrases under a given bilingual dependency context.
The statistics of Korean and English paraphrases
based on bilingual dependency relations is shown in
Table 1.
Especially, the paraphrasing ratio of the Korean
head phrases, 28.63% is higher than that of the
English heads,22.6%. Many of the Korean head
phrases are verb phrases that reflects the honorific
and inflectional characteristics of Korean language.
We might expect that the problems caused by vari-
ous honorific expressions can be resolved with the
paraphrases such like {?ga r geoyeyo?, ?ga gess-
seupnida?}.
For evaluating the accuracy of the acquired para-
phrases, we randomly selected 100 sets of para-
phrases for Korean and English phrase respectively.
Because the accuracy of paraphrases can vary de-
pending on context, we selected the dependency re-
lations that contain a phrase in a paraphrase set from
the test set. And we generated the dependency re-
lations by substituting the phrase by the other para-
phrases. Accuracy was judged by two native speak-
ers for each language. We measured the percentage
of completely interchangeable paraphrases under a
given bilingual dependency context.
Table 1 shows the performance of the paraphrases
depending on their bilingual context. The accuracy
of Korean and English paraphrases are 94.6% and
84.6% respectively. Korean paraphrases are more
accurate than English paraphrases. Especially the
quality of Korean head paraphrases(97.5%) is very
high.
Since we used a simple base-phrase chunker for
English, where most base phrases except for noun
phrases are composed of single words, most of En-
glish phrases aligned to Korean phrases were depen-
dent on the word alignments. Big structural differ-
ence between Korean and English made the word
alignments more difficult. These alignment results
might influence not only the paraphrasing ratio but
331
Korean Relation English Relation
Kor-head Kor-mod Eng-head Eng-mod
# of relations 66,664 66,664
# of uniq relations 59,633 58,187
36,157 33,088
# of uniq phrases 17,867 22,699 13,623 24,000
6,156 5,390
# of paraphrase set 4,474 2,890 3,425 3,169
24.15 21.8
Paraphrasing Ratio(%) 28.63 17.7 22.6 19.4
94.6 84.6
Accuracy(%) 97.5 91.2 86 82.3
Paraphrasing ratio(%) (Bannard et al, 2005) 44.4 37.4
accuracy (%) (Bannard et al, 2005) 71.4 76.2
Table 1: Statistics of the extracted bilingual dependency relations and paraphrases
also the performance of the paraphrases.
Nevertheless, our paraphrasing method outper-
formed previous approaches which do not use bilin-
gual dependency context. Because the paraphrasing
methods are different, we could not compare them
directly. But, we tried to make similar experimental
condition on the same BTEC corpus by implement-
ing the previous approach(Bannard et al, 2005).
When evaluating the previous approach, the accu-
racy of (Bannard et al, 2005) was 71.4% and 76.2%
for Korean and English paraphrases, respectively.
The results show that our paraphrasing method can
acquire the paraphrases of higher quality than (Ban-
nard et al, 2005) while the paraphrasing ratio is
lower than (Bannard et al, 2005).
5.2 Power of Generalization by Paraphrases
Finally, we investigated how many the ex-
tracted bilingual dependency patterns are general-
ized. Among 66,664 bilingual dependency patterns,
20,968 patterns were generalized into 12,631 unique
generalized patterns by applying the extracted para-
phrases2. As a result, the 66,664 bilingual depen-
dency patterns were compressed into 58,324 gener-
alized patterns with 12.5% compression ratio.
Furthermore, we examined how many bilingual
dependency patterns can be generated by the gener-
alized patterns in reverse. When replacing the gen-
eralized phrases with all of their paraphrases in both
English and Korean sides, 235,640 bilingual transla-
tion patterns are generated. These are 3.53 times of
the amount of the original translation patterns.
Even we have some errors in the paraphrase
2A paraphrase set is composed of more than two paraphrases
sets, these results might contribute to increasing the
coverage of the translation knowledge for machine
translation.
6 Related Work and Discussion
The proposed paraphrasing method can be an exten-
sion of the work done by (Bannard et al, 2005).
They introduced the method for extracting para-
phrases: Using the automatic alignment method
from phrase-based SMT, they showed that para-
phrases in one language can be identified using a
phrase in another language as a pivot. Furthermore,
they defined a paraphrase probability to rank the ex-
tracted paraphrases and suggested a method to refine
it by taking contextual information into account i.e.
including simple language model.
Our study for paraphrasing is similar to their work
but we take the bilingual dependency context into
account for disambiguating the sense of a phrase.
Limiting the candidate paraphrases to be the same
sense as the original phrase is critical to the per-
formance of paraphrases. Our approach provides
the solution to clearly disambiguate the sense of a
phrase using bilingual context information. This is
the strong point of our approach different from the
previous approaches.
Furthermore, in this work, we presented a method
to acquire somewhat generalized machine transla-
tion knowledge of bilingual dependency patterns.
There are few research of the acquisition of trans-
lation knowledge such like verb sub-categorization
patterns (Fung et al, 2004). (Fung et al, 2004)
tried to construct a bilingual semantic network,
BiFrameNet to enhance statistical and transfer-
332
based machine translation systems. They induced
the mapping between the English lexical entries in
FrameNet to Chinese word senses in HowNet. It
takes such an advantage of generalized bilingual
frame semantics. But, they have problems of appro-
priate mapping from lexical entries to word senses
and obtaining correct example sentences.
In our approach to acquire the generalized bilin-
gual translation patterns, a bilingual dependency
pattern is one of the decomposed bilingual verb sub-
categorization patterns. It is possible to construct
more complicated bilingual verb sub-categorization
pattern by applying a kind of unification operation.
In that case, we have the advantage of automati-
cally disambiguating the word/phrase senses via the
alignment techniques contrary to (Fung et al, 2004).
7 Conclusion
In this paper,we proposed a method to extract para-
phrases using bilingual corpora, which utilizes the
bilingual dependency relations obtained by project-
ing a monolingual dependency parse onto the other
language sentence based on statistical alignment
techniques. The advantage of our paraphrasing
method is that it can produce paraphrases of high
quality by clearly disambiguating the sense of an
original phrase.
Furthermore, we suggested an advanced method
to acquire generalized translation knowledge using
the extracted paraphrases. With the bilingual depen-
dency patterns generalized by the paraphrases, we
aim at reducing the translation ambiguity, but also
increasing the coverage of the translation knowl-
edge. The experimental results showed that our gen-
eralization method is effective to achieve the goals.
In future, we will utilize the paraphrases based
on bilingual dependency relations for increasing the
amount of bilingual corpus and for smoothing the
phrase probability table in statistical machine trans-
lation. Moreover, we plan to apply the acquired
translation patterns, which are generalized by para-
phrases, to various machine translation systems.
Acknowledgements
This work was supported by the IT R&D program of
MIC/IITA, Domain Customization Machine Trans-
lation Technology Development for Korean, Chi-
nese, and English.
References
Colin Bannard and Chris Callison Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora, Proc. of the
43rd Annual Meeting of the Association for Computa-
tional Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation,
Computational Linguistics, 19(2):263-311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation, Computational Linguistics, 33(2).
M. Diab and P. Resnik. An Unsupervised Method for
Word Sense Tagging Using Parallel Corpora, Proc. of
the 40th Annual Meeting of the Association for Com-
putational Linguistics.
Atsushi Fujita, Kentaro Inui, and Yuji Matsumoto. 2005.
Exploiting Lexical Conceptual Structure for Para-
phrase Generation, Proc. of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP).
Pascale Fung and Benfeng Chen 2004 BiFrameNet:
Bilingual Frame Semantics Resource Construction by
Cross-lingual Inductio, Proc. of the 20th International
Conference on Computational Linguistics,(COLING
2004),Geneva, Switzerland
Rebeca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts
, Natural Language Engineering , Vol 11(3), Pages:
311 - 325
Young-Sook Hwang, Andrew Finch and Yutaka Sasaki.
2007. Improving statistical machine translation using
shallow linguistic knowledge, Computer Speech and
Language , Vol. 21(2).
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003 Statistical Phrase-Based Translation, Proc.
of the Human Language Technology Confer-
ence(HLT/NAACL)
D. Lin and P. Pantel 2001. DIRT-Discovery of Infer-
ence Rules from Text, ACM SIGKDD Conference on
Knowledge Discovery and Data Mining, pages 323-
328.
Franz Josef Och and Hermann Ney. 2000. Improved Sta-
tistical Alignment Models , Proc. of the 38th Annual
Meeting of the Association for Computational Lin-
guistics, pp. 440-447, Hongkong, China.
333
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation ,
Computational Linguistics, Vol. 30(4), Pages 417-449.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT, Proc. of the 43rd Annual Meeting of the
Association for Computational Linguistics, pp. 271-
279.
S. Stolcke 2002 SRILM - an extensible language model-
ing toolkit, Proc. of International Conference of Spo-
ken Language Processing.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world,
Proc. of LREC 2002, pp. 147-152, Spain.
334
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215?222,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Lexical Dependency and Ontological Knowledge to Improve a
Detailed Syntactic and Semantic Tagger of English
Andrew Finch
NiCT?-ATR?
Kyoto, Japan
andrew.finch
@atr.jp
Ezra Black
Epimenides Corp.
New York, USA
ezra.black
@epimenides.com
Young-Sook Hwang
ETRI
Seoul, Korea
yshwang7
@etri.re.kr
Eiichiro Sumita
NiCT-ATR
Kyoto, Japan
eiichiro.sumita
@atr.jp
Abstract
This paper presents a detailed study of
the integration of knowledge from both
dependency parses and hierarchical word
ontologies into a maximum-entropy-based
tagging model that simultaneously labels
words with both syntax and semantics.
Our findings show that information from
both these sources can lead to strong im-
provements in overall system accuracy:
dependency knowledge improved perfor-
mance over all classes of word, and knowl-
edge of the position of a word in an on-
tological hierarchy increased accuracy for
words not seen in the training data. The
resulting tagger offers the highest reported
tagging accuracy on this tagset to date.
1 Introduction
Part-of-speech (POS) tagging has been one of the
fundamental areas of research in natural language
processing for many years. Most of the prior re-
search has focussed on the task of labeling text
with tags that reflect the words? syntactic role in
the sentence. In parallel to this, the task of word
sense disambiguation (WSD), the process of de-
ciding in which semantic sense the word is being
used, has been actively researched. This paper ad-
dresses a combination of these two fields, that is:
labeling running words with tags that comprise, in
addition to their syntactic function, a broad seman-
tic class that signifies the semantics of the word in
the context of the sentence, but does not neces-
sarily provide information that is sufficiently fine-
grained as to disambiguate its sense. This differs
?National Institute of Information and Communications
Technology
?ATR Spoken Language Communication Research Labs
from what is commonly meant by WSD in that al-
though each word may have many ?senses? (by
senses here, we mean the set of semantic labels
the word may take), these senses are not specific
to the word itself but are drawn from a vocabulary
applicable to the subset of all types in the corpus
that may have the same semantics.
In order to perform this task, we draw on re-
search from several related fields, and exploit pub-
licly available linguistic resources, namely the
WordNet database (Fellbaum, 1998). Our aim is
to simultaneously disambiguate the semantics of
the words being tagged while tagging their POS
syntax. We treat the task as fundamentally a POS
tagging task, with a larger, more ambiguous tag
set. However, as we will show later, the ?n-gram?
feature set traditionally employed to perform POS
tagging, while basically competent, is not up to
this challenge, and needs to be augmented by fea-
tures specifically targeted at semantic disambigua-
tion.
2 Related Work
Our work is a synthesis of POS tagging and WSD,
and as such, research from both these fields is di-
rectly relevant here.
The basic engine used to perform the tagging
in these experiments is a direct descendent of the
maximum entropy (ME) tagger of (Ratnaparkhi,
1996) which in turn is related to the taggers of
(Kupiec, 1992) and (Merialdo, 1994). The ME
approach is well-suited to this kind of labeling be-
cause it allows the use of a wide variety of features
without the necessity to explicitly model the inter-
actions between them.
The literature on WSD is extensive. For a good
overview we direct the reader to (Nancy and Jean,
1998). Typically, the local context around the
215
word to be sense-tagged is used to disambiguate
the sense (Yarowsky, 1993), and it is common for
linguistic resources such as WordNet (Li et al,
1995; Mihalcea and Moldovan, 1998; Ramakrish-
nan and Prithviraj, 2004), or bilingual data (Li and
Li, 2002) to be employed as well as more long-
range context. An ME-system for WSD that op-
erates on similar principles to our system (Suarez,
2002) was based on an array of local features that
included the words/POS tags/lemmas occurring in
a window of +/-3 words of the word being dis-
ambiguated. (Lamjiri et al, 2004) also developed
an ME-based system that used a very simple set
of features: the article before; the POS before
and after; the preposition before and after, and the
syntactic category before and after the word be-
ing labeled. The features used in both of these
approaches resemble those present in the feature
set of a standard n-gram tagger, such as the one
used as the baseline for the experiments in this pa-
per. The semantic tags we use can be seen as a
form of semantic categorization acting in a similar
manner to the semantic class of a word in the sys-
tem of (Lamjiri et al, 2004). The major difference
is that with a left-to-right beam-search tagger, la-
beled context to the right of the word being labeled
is not available for use in the feature set.
Although POS tag information has been utilized
in WSD techniques (e.g. (Suarez, 2002)), there
has been relatively little work addressing the prob-
lem of assigning a part-of-speech tag to a word
together with its semantics, despite the fact that
the tasks involve a similar process of label disam-
biguation for a word in running text.
3 Experimental Data
The primary corpus used for the experiments pre-
sented in this paper is the ATR General English
Treebank. This consists of 518,080 words (ap-
proximately 20 words per sentence, on average) of
text annotated with a detailed semantic and syntac-
tic tagset.
To understand the nature of the task involved
in the experiments presented in this paper, one
needs some familiarity with the ATR General
English Tagset. For detailed presentations,
see (Black et al, 1996b; Black et al, 1996a;
Black and Finch, 2001). An apercu can be
gained, however, from Figure 1, which shows
two sample sentences from the ATR Treebank
(and originally from a Chinese take?out food
flier), tagged with respect to the ATR General
English Tagset. Each verb, noun, adjective and
adverb in the ATR tagset includes a semantic
label, chosen from 42 noun/adjective/adverb
categories and 29 verb/verbal categories, some
overlap existing between these category sets.
Proper nouns, plus certain adjectives and
certain numerical expressions, are further cat-
egorized via an additional 35 ?proper?noun?
categories. These semantic categories are in-
tended for any ?Standard?American?English?
text, in any domain. Sample categories include:
?physical.attribute? (nouns/adjectives/adverbs),
?alter? (verbs/verbals), ?interpersonal.act?
(nouns/adjectives/adverbs/verbs/verbals),
?orgname? (proper nouns), and ?zipcode?
(numericals). They were developed by the ATR
grammarian and then proven and refined via
day?in?day?out tagging for six months at ATR by
two human ?treebankers?, then via four months of
tagset?testing?only work at Lancaster University
(UK) by five treebankers, with daily interactions
among treebankers, and between the treebankers
and the ATR grammarian. The semantic catego-
rization is, of course, in addition to an extensive
syntactic classification, involving some 165 basic
syntactic tags.
The test corpus has been designed specifically
to cope with the ambiguity of the tagset. It is pos-
sible to correctly assign any one of a number of
?allowable? tags to a word in context. For exam-
ple, the tag of the word battle in the phrase ?a
legal battle? could be either NN1PROBLEM or
NN1INTER-ACT, indicating that the semantics is
either a problem, or an inter-personal action. The
test corpus consists of 53,367 words sampled from
the same domains as, and in approximately the
same proportions as the training data, and labeled
with a set of up to 6 allowable tags for each word.
During testing, only if the predicted tag fails to
match any of the allowed tags is it considered an
error.
4 Tagging Model
4.1 ME Model
Our tagging framework is based on a maximum
entropy model of the following form:
p(t, c) = ?
K?
k=0
?fk(c,t)k p0 (1)
where:
216
(_( Please_RRCONCESSIVE Mention_VVIVERBAL-ACT this_DD1 coupon_NN1DOCUMENT
when_CSWHEN ordering_VVGINTER-ACT
OR_CCOR ONE_MC1WORD FREE_JJMONEY FANTAIL_NN1ANIMAL SHRIMPS_NN1FOOD
Figure 1: Two ATR Treebank Sentences from a Take?Out Food Flier
- t is tag being predicted;
- c is the context of t;
- ? is a normalization coefficient that ensures:
?Lt=0?
?K
k=0 ?
fk(c,t)
k p0 = 1;
- K is the number of features in the model;
- L is the number of tags in our tag set;
- ?k is the weight of feature fk;
- fk are feature functions and fk{0, 1};
- p0 is the default tagging model (in our case,
the uniform distribution, since all of the in-
formation in the model is specified using ME
constraints).
Our baseline model contains the following fea-
ture predecate set:
w0 t?1 pos0 pref1(w0)
w?1 t?2 pos?1 pref2(w0)
w?2 pos?2 pref3(w0)
w+1 pos+1 suff1(w0)
w+2 pos+2 suff2(w0)
suff3(w0)
where:
- wn is the word at offset n relative to the word
whose tag is being predicted;
- tn is the tag at offset n;
- posn is the syntax-only tag at offset n as-
signed by a syntax-only tagger;
- prefn(w0) is the first n characters of w0;
- suffn(w0) is the last n characters of w0;
This feature set contains a typical selection of
n-gram and basic morphological features. When
the tagger is trained in tested on the UPENN tree-
bank (Marcus et al, 1994), its accuracy (excluding
the posn features) is over 96%, close to the state of
the art on this task. (Black et al, 1996b) adopted
a two-stage approach to prediction, first predicting
syntax, then semantics given the syntax, whereas
in (Black et al, 1998) both syntax and semantics
were predicted together in one step. In using syn-
tactic tags as features, we take a softer approach
to the two-stage process. The tagger has access
to accurate syntactic information; however, it is
not necessarily constrained to accept this choice
of syntax. Rather, it is able to decide both syn-
tax and semantics while taking semantic context
into account. In order to find the most probable
sequence of tags, we tag in a left-to-right manner
using a beam-search algorithm.
4.2 Feature selection
For reasons of practicability, it is not always pos-
sible to use the full set of features in a model: of-
ten it is necessary to control the number of fea-
tures to reduce resource requirements during train-
ing. We use mutual information (MI) to select
the most useful feature predicates (for more de-
tails, see (Rosenfeld, 1996)). It can be viewed as
a means of determining how much information a
given predicate provides when used to predict an
outcome.
That is, we use the following formula to gauge
a feature?s usefulness to the model:
I(f ;T ) =
?
f?{0,1}
?
t?T
p(f, t)log
p(f, t)
p(f)p(t)
(2)
where:
- t ? T is a tag in the tagset;
- f ? {0, 1} is the value of any kind of predi-
cate feature.
Using mutual information is not without its
shortcomings. It does not take into account any
of the interactions between features. It is possi-
ble for a feature to be pronounced useful by this
procedure, whereas in fact it is merely giving the
same information as another feature but in differ-
ent form. Nonetheless this technique is invaluable
in practice. It is possible to eliminate features
217
which provide little or no benefit to the model,
thus speeding up the training. In some cases it
even allows a model to be trained where it would
not otherwise be possible to train one. For the pur-
poses of our experiments, we use the top 50,000
predicates for each model to form the feature set.
5 External Knowledge Sources
5.1 Lexical Dependencies
Features derived from n-grams of words and tags
in the immediate vicinity of the word being tagged
have underpinned the world of POS tagging for
many years (Kupiec, 1992; Merialdo, 1994; Rat-
naparkhi, 1996), and have proven to be useful fea-
tures in WSD (Yarowsky, 1993). Lower-order
n-grams which are closer to word being tagged
offer the greatest predictive power (Black et al,
1998). However, in the field of WSD, relational
information extracted from grammatical analysis
of the sentence has been employed to good effect,
and in particular, subject-object relationships be-
tween verbs and nouns have been shown be effec-
tive in disambiguating semantics (Nancy and Jean,
1998). We take the broader view that dependency
relationships in general between any classes of
words may help, and use the ME training process
to weed out the irrelevant relationships. The prin-
ciple is exactly the same as when using a word in
the local context as a feature, except that the word
in this case has a grammatical relationship with the
word being tagged, and can be outside the local
neighborhood of the word being tagged. For both
types of dependency, we encoded the model con-
straints fstl(d) as boolean functions of the form:
fstl(d) =
{
1 if d.s = s ? d.t = t ? d.l = l
0 otherwise
(3)
where:
- d is a lexical dependency, consisting of a
source word (the word being tagged) d.s, a
target word d.t and a label d.l
- s and t (words), and l (link label) are specific
to the feature
We generated two distinct features for each de-
pendency. The source and target were exchanged
to create these features. This was to allow the
models to capture the bidirectional nature of the
dependencies. For example, when tagging a verb,
the model should be aware of the dependent ob-
ject, and conversely when tagging that object, the
model should have a feature imposing a constraint
arising from the identity of the dependent verb.
5.1.1 Dependencies from the CMU Link
Grammar
We parsed our corpus using the parser detailed
in (Grinberg et al, 1995). The dependencies out-
put by this parser are labeled with the type of de-
pendency (connector) involved. For example, sub-
jects (connector type S) and direct objects of verbs
(O) are explicitly marked by the process (a full list
of connectors is provided in the paper). We used
all of the dependencies output by the parser as fea-
tures in the models.
5.1.2 Dependencies from Phrasal Structure
It is possible to extract lexical dependencies
from a phrase-structure parse. The procedure is
explained in detail in (Collins, 1996). In essence,
each non-terminal node in the parse tree is as-
signed a head word, which is the head of one of
its children denoted the ?head child?. Dependen-
cies are established between this headword and
the heads of each of the children (except for the
head child). In these experiments we used the
MXPOST tagger (Ratnaparkhi, 1996) combined
with Collins? parser (Collins, 1996) to assign parse
trees to the corpus. The parser had a 98.9% cover-
age of the sentences in our corpora. Again, all of
the dependencies output by the parser were used
as features in the models.
5.2 Hierarchical Word Ontologies
In this section we consider the effect of features
derived from hierarchical sets of words. The pri-
mary advantage is that we are able to construct
these hierarchies using knowledge from outside
the training corpus of the tagger itself, and thereby
glean knowledge about rare words. In these exper-
iments we use the human annotated word taxon-
omy of hypernyms (IS-A relations) in the Word-
Net database, and an automatically acquired on-
tology made by clustering words in a large corpus
of unannotated text.
We have chosen to use hierarchical schemes for
both the automatic and manually acquired ontolo-
gies because this offers the opportunity to com-
bat data-sparseness issues by allowing features de-
rived from all levels of the hierarchy to be used.
The process of training the model is able to de-
218
Top-level category
appleedible fruit apple treefruit
reproductivestructure fruit treeplant organ
plant partnatural objectobject angiospermoustreetreewoody plant
vascular plantplant
peargrape crab applewild appleHierarchy 
for sense 1 Hierarchy for sense 2
Figure 2: The WordNet taxonomy for both (WordNet) senses of the word apple
cide the levels of granularity that are most useful
for disambiguation. For the purposes of generat-
ing features for the ME tagger we treat both types
of hierarchy in the same fashion. One of these fea-
tures is illustrated in Figure 5.3. Each predicate
is effectively a question which asks whether the
word (or word being used in a particular sense in
the case of the WordNet hierarchy) is a descendent
of the node to which the predicate applies. These
predicates become more and more general as one
moves up the hierarchy. For example in the hierar-
chy shown in Figure 5.2, looking at the nodes on
the right hand branch, the lowest node represents
the class of apple trees whereas the top node rep-
resents the class of all plants.
We expect these hierarchies to be particularly
useful when tagging out of vocabulary words
(OOV?s). The identity of the word being tagged
is by far the most important feature in our baseline
model. When tagging an OOV this information is
not available to the tagger. The automatic cluster-
ing has been trained on 100 times as much data
as our tagger, and therefore will have information
about words that tagger has not seen during train-
ing. To illustrate this point, suppose that we are
tagging the OOV pomegranate. This word is in the
WordNet database, and is in the same synset as the
?fruit? sense of the word apple. It is reasonable to
assume that the model will have learned (from the
many examples of all fruit words) that the predi-
cate representing membership of this fruit synset
should, if true, favor the selection of the correct tag
for fruit words: NN1FOOD. The predicate will be
true for the word pomegranate which will thereby
benefit from the model?s knowledge of how to tag
the other words in its class. Even if this is not so
at this level in the hierarchy, it is likely to be so at
some level of granularity. Precisely which levels
of detail are useful will be learned by the model
during training.
5.2.1 Automatic Clustering of Text
We used the automatic agglomerative mutual-
information-based clustering method of (Ushioda,
1996) to form hierarchical clusters from approx-
imately 50 million words of tokenized, unanno-
tated text drawn from similar domains as the tree-
bank used to train the tagger. Figure 5.2 shows
the position of the word apple within the hierar-
chy of clusters. This example highlights both the
strengths and weaknesses of this approach. One
strength is that the process of clustering proceeds
in a purely objective fashion and associations be-
tween words that may not have been considered
by a human annotator are present. Moreover, the
clustering process considers all types that actually
occur in the corpus, and not just those words that
might appear in a dictionary (we will return to this
later). A major problem with this approach is that
219
eggapplecoca PREDICATE:Is the word in thesubtree below thisnode?coffee chicken diamond tin newsstandwellhead calf after-market palm-oilwinter-wheat meat milk timber ?
Figure 3: The dendrogram for the automatically acquired ontology, showing the word apple
the clusters tend to contain a lot of noise. Rare
words can easily find themselves members of clus-
ters to which they do not seem to belong, by virtue
of the fact that there are too few examples of the
word to allow the clustering to work well for these
words. This problem can be mitigated somewhat
by simply increasing the size of the text that is
clustered. However the clustering process is com-
putationally expensive. Another problem is that a
word may only be a member of a single cluster;
thus typically the cluster set assigned to a word
will only be appropriate for that word when used
in its most common sense.
Approximately 93% of running words in the test
corpus, and 95% in the training corpus were cov-
ered by the words in the clusters (when restricted
to verbs, nouns, adjectives and adverbs, these fig-
ures were 94.5% and 95.2% respectively). Ap-
proximately 81% of the words in the vocabulary
from the test corpus were covered, and 71% of the
training corpus vocabulary was covered.
5.2.2 WordNet Taxonomy
For this class of features, we used the hypernym
taxonomy of WordNet (Fellbaum, 1998). Fig-
ure 5.2 shows the WordNet hypernym taxonomy
for the two senses of the word apple that are in
the database. The set of predicates query member-
ship of all levels of the taxonomy for all WordNet
senses of the word being tagged. An example of
one such predicate is shown in the figure.
Only 63% of running words in both the train-
ing and the test corpus were covered by the words
in the clusters. Although this figure appears low,
it can be explained by the fact that WordNet only
contains entries for words that have senses in cer-
tain parts of speech. Some very frequent classes of
words, for example determiners, are not in Word-
Net. The coverage of only nouns, verbs, adjectives
and adverbs in running text is 94.5% for both train-
ing and test sets. Moreover, approximately 84%
of the words in the vocabulary from the test cor-
pus were covered, and 79% on the training cor-
pus. Thus, the effective coverage of WordNet on
the important classes of words is similar to that of
the automatic clustering method.
6 Experimental Results
The results of our experiments are shown in Ta-
ble 1. The task of assigning semantic and syntac-
tic tags is considerably more difficult than simply
assigning syntactic tags due to the inherent ambi-
guity of the tagset. To gauge the level of human
performance on this task, experiments were con-
ducted to determine inter-annotator consistency;
in addition, annotator accuracy was measured on
5,000 words of data. Both the agreement and ac-
curacy were found to be approximately 97%, with
all of the inconsistencies and tagging errors aris-
ing from the semantic component of the tags. 97%
accuracy is therefore an approximate upper bound
for the performance one would expect from an au-
tomatic tagger. As a point of reference for a lower
bound, the overall accuracy of a tagger which uses
only a single feature representing the identity of
the word being tagged is approximately 73%.
The overall baseline accuracy was 82.58% with
only 30.58% of OOV?s being tagged correctly.
Of the two lexical dependency-based approaches,
220
the features derived from Collins? parser were the
most effective, improving accuracy by 0.8% over-
all. To put the magnitude of this gain into perspec-
tive, dropping the features for the identity of the
previous word from the baseline model, only de-
graded performance by 0.2%. The features from
the link grammar parser were handicapped due to
the fact that only 31% of the sentences were able
to be parsed. When the model (Model 3 in Ta-
ble 1) was evaluated on only the parsable portion
on the test set, the accuracy obtained was roughly
comparable to that using the dependencies from
Collins? parses. To control for the differences be-
tween these parseable sentences and the full test
set, Model 4 was tested on the same 31% of sen-
tence that parsed. Its accuracy was within 0.2% of
the accuracy on the whole test set in all cases. Nei-
ther of the lexical dependency-based approaches
had a particularly strong effect on the performance
on OOV?s. This is in line with our intuition, since
these features rely on the identity of the word be-
ing tagged, and the performance gain we see is
due to the improvement in labeling accuracy of the
context around the OOV.
In contrast to this, for the word-ontology-based
feature sets, one would hope to see a marked im-
provement on OOV?s, since these features were
designed specifically to address this issue. We do
see a strong response to these features in the ac-
curacy of the models. The overall accuracy when
using the automatically acquired ontology is only
0.1% higher than the accuracy using dependencies
from Collins? parser. However the accuracy on
OOV?s jumps 3.5% to 35.08% compared to just
0.7% for Model 4. Performance for both cluster-
ing techniques was quite similar, with the Word-
Net taxonomical features being slightly more use-
ful, especially for OOV?s. One possible explana-
tion for this is that overall, the coverage of both
techniques is similar, but for rarer words, the MI
clustering can be inconsistent due to lack of data
(for an example, see Figure 5.2: the word news-
stand is a member of a cluster of words that appear
to be commodities), whereas the WordNet clus-
tering remains consistent even for rare words. It
seems reasonable to expect, however, that the au-
tomatic method would do better if trained on more
data. Furthermore, all uses of words can be cov-
ered by automatic clustering, whereas for exam-
ple, the common use of the word apple as a com-
pany name is beyond the scope of WordNet.
In Model 7 we combined the best lexical depen-
dency feature set (Model 4) with the best cluster-
ing feature set (Model 6) to investigate the amount
of information overlap existing between the fea-
ture sets. Models 4 and 6 improved the base-
line performance by 0.8% and 1.3% respectively.
In combination, accuracy was increased by 2.3%,
0.2% more than the sum of the component mod-
els? gains. This is very encouraging and indicates
that these models provide independent informa-
tion, with virtually all of the benefit from both
models manifesting itself in the combined model.
7 Conclusion
We have described a method for simultaneously
labeling the syntax and semantics of words in run-
ning text. We develop this method starting from
a state-of-the-art maximum entropy POS tagger
which itself outperforms previous attempts to tag
this data (Black et al, 1996b). We augment this
tagging model with two distinct types of knowl-
edge: the identity of dependent words in the sen-
tence, and word class membership information of
the word being tagged. We define the features in
such a manner that the useful lexical dependen-
cies are selected by the model, as is the granu-
larity of the word classes used. Our experimental
results show that large gains in performance are
obtained using each of the techniques. The de-
pendent words boosted overall performance, es-
pecially when tagging verbs. The hierarchical
ontology-based approaches also increased over-
all performance, but with particular emphasis on
OOV?s, the intended target for this feature set.
Moreover, when features from both knowledge
sources were applied in combination, the gains
were cumulative, indicating little overlap.
Visual inspection the output of the tagger on
held-out data suggests there are many remaining
errors arising from special cases that might be bet-
ter handled by models separate from the main tag-
ging model. In particular, numerical expressions
and named entities cause OOV errors that the tech-
niques presented in this paper are unable to handle.
In future work we would like to address these is-
sues, and also evaluate our system when used as a
component of a WSD system, and when integrated
within a machine translation system.
221
# Model Accuracy (? c.i.) OOV?s Nouns Verbs Adj/Adv
1 Baseline 82.58? 0.32 30.58 68.47 74.32 70.99
2 + Dependencies (link grammar) 82.74? 0.32 30.92 68.18 74.96 73.02
3 As above (only parsed sentences) 83.59? 0.53 30.92 69.16 77.21 73.52
4 + Dependencies (Collins? parser) 83.37? 0.31 31.24 69.36 75.78 72.62
5 + Automatically acquired ontology 83.71? 0.31 35.08 71.89 75.83 75.34
6 + WordNet ontology 83.90? 0.31 36.18 72.28 76.29 74.47
7 + Model 4 + Model 6 84.90? 0.31 37.02 72.80 78.36 76.16
Table 1: Tagging accuracy (%), ?+? being shorthand for ?Baseline +?, ?c.i.? denotes the confidence
interval of the mean at a 95% significance level, calculated using bootstrap resampling.
References
E. Black and A. Finch. 2001. Developing and prov-
ing effective broad-coverage semantic-and-syntactic
tagsets for natural language: The atr approach. In
Proceedings of ICCPOL-2001.
E. Black, S. Eubank, H. Kashioka, R. Garside,
G. Leech, and D. Magerman. 1996a. Beyond
skeleton parsing: producing a comprehensive large?
scale general?english treebank with full grammati-
cal analysis. In Proceedings of the 16th Annual Con-
ference on Computational Linguistics, pages 107?
112, Copenhagen.
E. Black, S. Eubank, H. Kashioka, and J. Saia. 1996b.
Reinventing part-of-speech tagging. Journal of Nat-
ural Language Processing (Japan), 5:1.
Ezra Black, Andrew Finch, and Hideki Kashioka.
1998. Trigger-pair predictors in parsing and tag-
ging. In Proceedings, 36th Annual Meeting of
the Association for Computational Linguistics, 17th
Annual Conference on Computational Linguistics,
Montreal, Canada.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Arivind
Joshi and Martha Palmer, editors, Proceedings of
the Thirty-Fourth Annual Meeting of the Association
for Computational Linguistics, pages 184?191, San
Francisco. Morgan Kaufmann Publishers.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Dennis Grinberg, John Lafferty, and Daniel Sleator.
1995. A robust parsing algorithm for LINK
grammars. Technical Report CMU-CS-TR-95-125,
CMU, Pittsburgh, PA.
J. Kupiec. 1992. Robust part-of-speech tagging using
a hidden markov model. Computer Speech and Lan-
guage, 6:225?242.
A. K. Lamjiri, O. El Demerdash, and L.Kosseim. 2004.
Simple features for statistical word sense disam-
biguation. In Proc. ACL 2004 ? Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text (Senseval-3), Barcelona,
Spain, July. ACL-2004.
C. Li and H. Li. 2002. Word translation disambigua-
tion using bilingual bootstrapping.
Xiaobin Li, Stan Szpakowicz, and Stan Matwin. 1995.
A wordnet-based algorithm for word sense disam-
biguation. In IJCAI, pages 1368?1374.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
B. Merialdo. 1994. Tagging english text with a
probabilistic model. Computational Linguistics,
20(2):155?172.
Rada Mihalcea and Dan I. Moldovan. 1998. Word
sense disambiguation based on semantic density. In
Sanda Harabagiu, editor, Use of WordNet in Natural
Language Processing Systems: Proceedings of the
Conference, pages 16?22. Association for Compu-
tational Linguistics, Somerset, New Jersey.
I. Nancy and V. Jean. 1998. Word sense disambigua-
tion: The state of the art. Computational Linguis-
tics, 24:1:1?40.
G. Ramakrishnan and B. Prithviraj. 2004. Soft word
sense disambiguation. In International Conference
on Global Wordnet (GWC 04), Brno, Czeck Repub-
lic.
A. Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the Empirical
Methods in Natural Language Processing Confer-
ence.
R. Rosenfeld. 1996. A maximum entropy approach to
adaptive statistical language modelling. Computer
Speech and Language, 10:187?228.
A. Suarez. 2002. A maximum entropy-based word
sense disambiguation system. In Proc. International
Conference on Computational Linguistics.
A. Ushioda. 1996. Hierarchical clustering of words.
In In Proceedings of COLING 96, pages 1159?1162.
D. Yarowsky. 1993. One sense per collocation. In
In the Proceedings of ARPA Human Language Tech-
nology Workshop.
222
Empirical Study of Utilizing Morph-Syntactic
Information in SMT
Young-Sook Hwang, Taro Watanabe, and Yutaka Sasaki
ATR SLT Research Labs, 2-2-2 Hikaridai Seika-cho,
Soraku-gun Kyoto, 619-0288, Japan
{youngsook.hwang, taro.watanabe, yutaka.sasaki}@atr.jp
Abstract. In this paper, we present an empirical study that utilizes
morph-syntactical information to improve translation quality. With three
kinds of language pairs matched according to morph-syntactical similar-
ity or difference, we investigate the effects of various morpho-syntactical
information, such as base form, part-of-speech, and the relative positional
information of a word in a statistical machine translation framework.
We learn not only translation models but also word-based/class-based
language models by manipulating morphological and relative positional
information. And we integrate the models into a log-linear model. Ex-
periments on multilingual translations showed that such morphological
information as part-of-speech and base form are effective for improving
performance in morphologically rich language pairs and that the relative
positional features in a word group are useful for reordering the local
word orders. Moreover, the use of a class-based n-gram language model
improves performance by alleviating the data sparseness problem in a
word-based language model.
1 Introduction
For decades, many research efforts have contributed to the advance of statisti-
cal machine translation. Such an approach to machine translation has proven
successful in various comparative evaluations. Recently, various works have im-
proved the quality of statistical machine translation systems by using phrase
translation [1,2,3,4] or using morpho-syntactic information [6,8]. But most sta-
tistical machine translation systems still consider surface forms and rarely use
linguistic knowledge about the structure of the languages involved[8]. In this
paper, we address the question of the effectiveness of morpho-syntactic features
such as parts-of-speech, base forms, and relative positions in a chunk or an ag-
glutinated word for improving the quality of statistical machine translations.
Basically, we take a statistical machine translation model based on an IBM
model that consists of a language model and a separate translation model [5]:
eI1 = argmaxeI1Pr(f
J
1 |eI1)Pr(eI1) (1)
The translation model links the source language sentence to the target language
sentence. The target language model describes the well-formedness of the target
language sentence.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 474?485, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Empirical Study of Utilizing Morph-Syntactic Information in SMT 475
One of the main problems in statistical machine translation is to learn the
less ambiguous correspondences between the words in the source and target
languages from the bilingual training data. When translating one source lan-
guage(which may be inflectional or non-inflectional) into the morphologically
rich language such like Japanese or Korean, the bilingual training data can be
exploited better by explicitly taking into account the interdependencies of re-
lated inflected or agglutinated forms. In this study, we represent a word with
its morphological features in both sides of the source and the target language
to learn less ambiguous correspondences between the source and the target lan-
guage words or phrases. In addition, we utilize the relative positional information
of a word in its word group to consider the word order in an agglutinated word
or a chunk.
Another problem is to produce a correct target sentence. To produce more
correct target sentence, we should consider the following problems: word re-
ordering in a language pair with different word order, production of correct
inflected and agglutinated words in an inflectional or agglutinative target lan-
guage. In this study, we tackle the problem with language models. For learning
language model that can treat morphological and word-order problem, we rep-
resent a word with its morphological and positional information. However, a
word-based language model with enriched word is likely to suffer from a severe
data sparseness problem. To alleviate the problem, we interpolate the word-based
language model with a class-based n-gram model.
In the next section, we briefly discuss related works. Then, we describe the
method that utilizes morpho-syntactic information under consideration for im-
proving the quality of translations. Then we report the experimental results with
some analysis and conclude our study.
2 Related Work
Few papers deal with the integration of linguistic information into the process of
statistical machine translation. [8] introduced hierarchical lexicon models includ-
ing base-form and POS information for translation from German into English.
Irrelevant information contained in the German entries for the generation of the
English translation were omitted. They trained the lexicon model using maxi-
mum entropy. [6] enriched English with knowledge to help select the correct full-
form from morphologically richer languages such as Spanish and Catalan. In other
words, they introduced a splicing operation that merged the pronouns/modals
and verbs for treating differences in verbal expressions. To treat the unknown en-
tries in the lexicon resulting from the splicing operation, they trained the lexicon
model using maximum entropy and used linguistic knowledge just in the source
language part and not in the target language. They don?t use any linguistic knowl-
edge in the target language and use full-form words during training.
In addition, [6] and [8] proposed re-ordering operations to make similar word
orders in the source and target language sentences. In other words, for the in-
terrogative phrases with different word order from the declarative sentences,
476 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
they introduced techniques of question inversion and removed unnecessary aux-
iliary verbs. But, such inversion techniques require additional preprocessing with
heuristics.
Unlike them, we investigate methods for utilizing linguistic knowledge in
both of the source and the target language at the morpheme level. To generate
a correct full-form word in a target language, we consider not only both the
surface and base form of a morpheme but also the relative positional informa-
tion in a full-form word. We strongly utilize the combined features in language
modeling. By training alignments and language models with morphological and
positional features at the morpheme-level, the severe data sparseness problem
can be alleviated with the combined linguistic features. And the correspondence
ambiguities between the source and target words can be decreased.
3 Utilization of Morpho-Syntactic Information in SMT
Generally, the probabilistic lexicon resulting from training a translation model
contains all word forms occurring in the training corpus as separate entries,
not taking into account whether they are inflected forms. A language model
is also composed of the words in the training corpus. However, the use of a
full-form word itself may cause severe data sparseness problem, especially rel-
evant for more inflectional/agglutinative languages like Japanese and Korean.
One alternative is to utilize the results of morphological analysis such as base
form, part-of-speech and other information at the morpheme level. We address
the usefulness of morphological information to improve the quality of statistical
machine translation.
3.1 Available Morpho-Syntactic Information
A prerequisite for methods that improve the quality of statistical machine trans-
lation is the availability of various kinds of morphological and syntactic infor-
mation. In this section, we examine the morpho-syntactic information available
from the morphological analyzers of Korean, Japanese, English and Chinese and
describe a method of utilizing the information.
Japanese and Korean are highly inflectional and agglutinative languages, and
in English inflection has only a marginal role; whereas Chinese usually is regarded
as an isolating language since it has almost no inflectional morphology. As the
syntactic role of each word within Japanese and Korean sentences are often
marked, word order in a sentence plays a relatively small role in characterizing
the syntactic function of each word than in English or Chinese sentences. Thus,
Korean and Japanese sentences have a relatively free word order; whereas words
within Chinese and English sentences adhere to a rigid order. The treatment
of inflection, and not word order, plays the most important role in processing
Japanese and Korean, while word order has a central role in Chinese and English.
Figure 1 shows some examples of morphological information by Chinese,
Japanese, English and Korean morphological analyzers and Figure 2 the corre-
spondences among the words. Note that Korean and Japanese are very similar:
Empirical Study of Utilizing Morph-Syntactic Information in SMT 477
Fig. 1. Examples of linguistic information from Chinese, Japanese, English, and Korean
morphological analyzers
Fig. 2. Correspondences among the words in parallel sentences
highly inflected and agglutinated. One difference in Korean from Japanese is
that a Korean sentence consists of spacing units, eojeols,1 while there are no
space in a Japanese sentence. Especially, a spacing unit(i.e., eojeol) in Korean
often becomes a base phrase that contains such syntactic information as subject,
object, and the mood/tense of a verb in a given sentence. The treatment of such
a Korean spacing unit may contribute to the improvement of translation quality
because a morpheme can be represented with its relative positional information
within an eojeol. The relative positional information is obtained by calculating
the distance between the beginning syllable of a given eojeol and the beginning
of each morpheme within the eojeol. The relative positional information is rep-
resented with indexes of the beginning and the ending syllables (See Figure 1).
3.2 Word Representation
A word(i.e. morpheme) is represented by the combination of the information pro-
vided by a morphological analyzer including the surface form, base form, part-of-
speech or other information such as relative position within an eojeol. The word
1 An eojeol is composed of no less than one morpheme by agglutination principle.
478 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
Table 1. Word Representation According to Morpho-Syntactic Characteristics (S: sur-
face form, B:base form, P:part-of-speech, L:RelativePosition)
Chinese English Japanese Korean
Morph-Syntactic no inflection Inflectional Inflectional, Inflectional
Characteristics Agglutinative Agglutinative
Spacing Unit
(Word-Order) Rigid Rigid Partial Free Partial Free
Word Representation S?P S?B?P S?B?P S?B?P?L
S?B, S?P S?B, S?P S?B?P, S?B?L, S?P?L
S?B, S?P, S?L
enriched by the combination of morph-syntactic information must alway include
the surface form of a given word for the direct generation of target sentence
without any post-processing. Other different morphological information is com-
bined according to representation models such as surface plus base form (SB),
surface plus part-of-speech (SP), surface plus relative position (SL), and so on.
Table 1 shows the word representation of each language with every possible
morphological information. Yet, we are not limited to only this word represen-
tation, but we have many possibilities of word representation by removing some
morphological information or inserting additional morpho-syntactic information
as mentioned previously. In order to develop the best translation systems, we
select the best word representation models of the source and the target language
through empirical experiments.
The inherent in the original word forms is augmented by a morphological
analyzer. Of course, this results in an enlarged vocabulary while it may provide
useful disambiguation clues. However, since we regard a morpheme as a word
in a corpus(henceforth, we call a morpheme a word), the enlarged vocabulary
does not make more severe data sparseness problem than using the inflected or
agglutinated word. By taking the approch of morpheme-level alignment, we may
obtain more accurate correspondences among words as illustrated in Figure 2.
Moreover, by learning the language model with rich morph-syntactic informa-
tion, we can generate more syntactically fluent and correct sentence.
3.3 Log-Linear Model for Statistical Machine Translation
In order to improve translation quality, we evaluate the translation candidates
by using the relevant features in a log-linear model framework[11]. The log-linear
model used in our statistical translation process, Pr(eI1|fJ1 ), is:
Pr(eI1|f I1 ) =
exp(
?
m ?mhm(e
I
1, f
J
1 , a
J
1 ))
?
e?I1 ,f
I
1 ,a
I
1
exp(
?
m ?mhm(e
?I
1 , f
J
1 , a
J
1 ))
(2)
where hm(eI1, f
J
1 , a
J
1 ) is the logarithm value of the m-th feature; ?m is the weight
of the m-th feature. Integrating different features in the equation results in dif-
ferent models.
Empirical Study of Utilizing Morph-Syntactic Information in SMT 479
The statistical machine translation process in IBM models is as follows; a
given source string fJ1 = f1 ? ? ? fJ is to be translated into eI1 = e1 ? ? ? eI . Accord-
ing to the Bayes? decision rule, we choose the optimal translation for given string
fJ1 that maximizes the product of target language model Pr(e
I
1) and translation
model Pr(fJ1 |eI1)
eI1 = argmaxeI1Pr(f
J
1 |eI1)Pr(eI1) (3)
In IBM model 4, translation model P (fJ1 |eI1) is further decomposed into four
submodels:
? Lexicon Model, t(f |e): probability of word f in the source language being
translated into word e in the target language.
? Fertility model, n(?|e): probability of target language word e generating ?
words.
? Distortion model d: probability of distortion, which is decomposed into the
distortion probabilities of head words and non-head words.
? NULL translation model p1: a fixed probability of inserting a NULL word
after determining each target word.
In addition to the five features (Pr(eI1), t(f |e), n(?|e), d, p1) from IBM model
4, we incorporate the following features into the log-linear translation model:
? Class-based n-gram model Pr(eI1) =
?
i Pr(ei|ci)Pr(ci|c
i?1
1 ): Grouping of
words into C classes is done according to the statistical similarity of their
surroundings. Target word ei is mapped into its class, ci, which is one of C
classes[13].
? Length model Pr(l|eI1, fJi ): l is the length (number of words) of a translated
target sentence.
? Example matching score: The translated target sentence is matched with
phrase translation examples. A score is derived based on the number of
matches [10]. To extract phrase translation examples, we compute the inter-
section of word alignment of both directions and derive the union. Then we
grab the phrase translation pairs that contain at least one intersected word
alignment and some unioned word alignments[1].
Under the framework of log-linear models, we investigate the effects of morpho-
syntactic information with word representation. The overall training and testing
process with morphological and positional information is depicted in Figure 3. In
the training step, we train the word- and class-based language models with var-
ious word representation methods[12]. Also, we make word alignments through
the learning of IBM models by using GIZA++ toolkit[3]: we learn the translation
model toward IBM model 4, initiating translation iterations from IBM model
1 with intermediate HMM model iterations. Then, we extract example phrases
and translation model features from the alignment results.
Then in the test step, we perform morphological anlysis of a given sentence for
word representation corresponding to training corpus representation. We decode
the best translation of a given test sentence by generating word graphs and
searching for the best hypothesis in a log-linear model[7].
480 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
Fig. 3. Overview of training and test of statistical machine translation system with
linguistic information
4 Experiments
4.1 Experimental Environments
The corpus for the experiment was extracted from the Basic Travel Expression
Corpus (BTEC), a collection of conversational travel phrases for Chinese, En-
glish, Japanese and Korean[15]. The entire corpus was split into three parts:
152,169 sentences in parallel for training, 10,150 sentences for testing and the
remaining 10,148 sentences for parameter tuning, such as termination criteria
for training iteration and parameter tuning for decoders. For the reconstruction
of each corpus with morphological information, we used in-house morphological
Table 2. Statistics of Basic Travel Expression Corpus
Chinese English Japanese Korean
# of sentences 167,163
# of words(morph) 1,006,838 1,128,151 1,226,774 1,313,407
Vocabulary size(S) 17,472 11,737 19,485 17,600
Vocabulary size(B) 17,472 9172 15,939 15,410
Vocabulary size(SB) 17,472 13,385 20,197 18,259
Vocabulary size(SP) 18,505 13,467 20,118 20,249
Vocabulary size(SBP(L)) 18,505 14,408 20,444 20,369(26,668)
# of singletons(S) 7,137 4,046 8,107 7,045
# of singletons(B) 7,137 3,025 6,497 6,303
# of singletons(SB) 7,137 4,802 9,453 7,262
# of singletons(SP) 7,601 4,693 8,343 7,921
# of singletons(SBP(L)) 7,601 5,140 8,525 7,983(11,319)
Empirical Study of Utilizing Morph-Syntactic Information in SMT 481
Table 3. Perplexities of tri-gram language model trained on the training corpora
with S, SB, SP SBP, SBL, and SBPL morpho-syntactic representation: word-based
3-gram/class-based 5-gram
S SB SP SBP SBL SBPL
Chinese 31.57/24.09 N/S 35.83/26.28 N/A N/A N/A
English 22.35/18.82 22.19/18.54 22.24/18.12 22.08/18.03 N/A N/A
Japanese 17.89/ 13.44 17.92/13.29 17.82/13.13 17.83/13.06 N/A N/A
Korean 15.54/12.42 15.41/12.09 16.04/11.89 16.03/11.88 16.48/12.24 17.13/11.99
analyzers for four languages: Chinese morphological analyzer with 31 parts-of-
speech tags, English morphological analyzer with 34 tags, Japanese morphologi-
cal analyzer with 34 tags, and Korean morphological analyzer with 49 tags. The
accuracies of Chinese, English, Japanese and Korean morphological analyzers in-
cluding segmentation and POS tagging are 95.82% , 99.25%, 98.95%, and 98.5%
respectively. Table 2 summarizes the morph-syntactic statistics of the Chinese,
English, Japanese, and Korean.
For the four languages, word-based and class-based n-gram language models
were trained on the training set by using SRILM toolkit[12]. The perplexity of
each language model is shown in Table 3.
For the four languages, we chose three kinds of language pairs according to
the linguistic characteristics of morphology and word order, Chinese-Korean,
Japanese-Korean, and English-Korean. 42 translation models based on word
representation methods(S, SB, SP, SBP, SBL, SPL,SBPL) were trained by using
GIZA++[3].
4.2 Evaluation
Translation evaluations were carried out on 510 sentences selected randomly
from the test set. The metrics for the evaluations are as follows:
mWER(multi-reference Word Error Rate), which is based on the minimum
edit distance between the target sentence and the sentences in the reference
set [9].
BLEU, which is the ratio of the n-gram for the translation results found in the
reference translations with a penalty for too short sentences [14].
NIST which is a weighted n-gram precision in combination with a penalty for
too short sentences.
For this evaluation, we made 16 multiple references available. We computed all
of the above criteria with respect to these multiple references.
Table 4, 5 and 6 show the evaluation results on three kinds of language pairs.
The effects of morpho-syntactic information and class-based n-gram language
models on multi-lingual machine translation are shown: The combined morpho-
logical information was useful for improving the translation quality in the NIST,
BLEU and mWER evaluations. Moreover, the class-based n-gram language mod-
els were effective in the BLEU and the mWER scores.
482 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
Table 4. Evaluation results of Japanese to Korean and Korean to Japaneses transla-
tions(with class-based n-gram/word-based n-gram language model)
J to K K to J
NIST BLEU WER NIST BLEU WER
S 8.46/8.64 0.694/0.682 26.33/26.73 8.21/8.39 0.666/0.649 25.00/25.81
SB 8.05/8.32 0.705/0.695 26.82/26.97 7.67/8.17 0.690/0.672 23.77/24.68
SP 9.15/9.25 0.755/0.747 21.71/22.22 9.02/9.13 0.720/0.703 21.94/23.50
SL 8.37/8.47 0.699/0.667 25.49/27.76 8.48/8.74 0.671/0.629 25.14/27.88
SBL 8.92/9.12 0.748/0.730 22.66/23.36 8.85/8.92 0.712/0.691 21.88/23.37
SBP 8.19/8.57 0.713/0.696 26.17/27.09 8.21/8.39 0.698/0.669 22.94/24.88
SBPL 8.41/8.85 0.772/0.757 22.30/21.74 7.77/7.83 0.626/0.619 25.19/25.57
Table 5. Evaluation results of English to Korean and Korean to English transla-
tions(with class-based n-gram/word-based n-gram language model)
E to K K to E
NIST BLEU WER NIST BLEU WER
S 5.12/5.79 0.353/0.301 51.12/58.52 5.76/6.05 0.300/0.255 52.54/61.23
SB 6.71/6.87 0.533/0.474 39.10/47.18 7.72/8.15 0.482/0.446 37.86/42.71
SP 6.88/7.19 0.552/0.502 37.63/42.34 8.01/8.46 0.512/0.460 35.13/40.91
SL 6.66/6.96 0.546/0.516 38.20/40.67 7.71/8.02 0.484/0.436 36.79/42.88
SPL 6.16/7.01 0.542/0.519 38.21/39.85 7.83/8.22 0.482/0.443 37.52/41.63
SBL 6.52/6.93 0.547/0.504 37.76/42.23 7.64/8.08 0.479/0.439 37.10/42.30
SBP 7.42/7.60 0.612/0.573 32.17/35.96 8.86/9.05 0.551/0.523 33.13/37.07
SBPL 6.29/6.59 0.580/0.561 36.73/38.36 8.08/8.36 0.528/0.515 36.46/38.21
Table 6. Evaluation results of Chinese to Korean and Korean to Chinese transla-
tions(with class-based n-gram/word-based n-gram language model)
C to K K to C
NIST BLEU WER NIST BLEU WER
S 7.62/7.82 0.640/0.606 30.01/32.79 7.85/7.69 0.380/0.365 53.65/58.46
SB 7.73/7.98 0.643/0.632 29.26/30.08 7.68/7.50 0.366/0.349 54.48/60.49
SP 7.71/7.98 0.651/0.643 28.26/28.60 8.00/7.77 0.383/0.362 54.15/58.30
SL 7.64/7.97 0.656/0.635 28.94/30.33 7.84/7.65 0.373/0.350 54.53/58.38
SPL 7.69/7.93 0.665/0.659 28.43/28.88 7.78/7.62 0.373/0.351 56.14/59.54
SBL 7.65/7.94 0.659/0.635 28.76/30.87 7.85/7.64 0.377/0.354 55.01/58.39
SBP 7.81/7.98 0.660/0.643 28.85/29.61 7.94/7.68 0.386/0.360 53.99/58.94
SBPL 7.64/7.90 0.652/0.634 29.54/30.46 7.82/7.66 0.376/0.358 55.64/58.79
In detail, Table 4 shows the effects of the morphological and relative posi-
tional information on Japanese-to-Korean and Korean-to-Japanese translation.
In almost of the evaluation metrics, the SP model in which a word is repre-
sented by a combination of its surface form and part-of-speech showed the best
performance. The SBL model utilizing the base form and relative positional
information only in Korean showed the second best performance. In Korean-
Empirical Study of Utilizing Morph-Syntactic Information in SMT 483
to-Japanese translation, the SBPL model showed the best score in BLEU and
mWER. In this language pair of highly inflectional and agglutinative languages,
the part-of-speech information combined with surface form was the most ef-
fective in improving the performance. The base form and relative positional
information were less effective than part-of-speech. It could be explained in sev-
eral points: Japanese and Korean are very similar languages in the word order
of SOVs and the ambiguities of translation correspondences in both directions
were converged into 1.0 by combining the distinctive morphological information
with the surface form. When refering to the vocabulay size of SP model in Table
2, it makes it more clear. The Japanese-to-Korean translation outperforms the
Korean-to-Japanese. It might be closely related to the language model: the per-
plexity of the Korean language model is lower than Japanese according to our
corpus statistics.
Table 5 shows the performance of the English-to-Korean and Korean-to-
English translation: a pair of highly inflectional and agglutinative language with
partially free word-order and an inflectional language with rigid word order. In
this language pair, the combined word representation models improved the trans-
lation performance into significantly higher BLEU and mWER scores in both
directions. The part-of-speech and the base form information were distinctive
features. When comparing the performance of SP, SB and SL models, part-of-
speech might be more effective than base form or relative positional information,
and the relative positional information in Korean might play a role not only in
controlling word order in the language models but also in discriminating word
correspondences during alignment.
When the target language was Korean, we had higher BLEU scores in all the
morpho-syntactic models but lower NIST scores. In other words, we took advan-
tage of generating more accurate full-form eojeol with positional information,
i.e. local word ordering.
Table 6 shows the performance of the Chinese-to-Korean and Korean-to-
Chinese translation: a pair of a highly inflectional and agglutinative language
with partially free word order and a non-inflectional language with rigid word
order. This language pair is a quite morpho-syntactically different. When a non-
inflectional language is a target language(i.e. Korean-to-Chinese translation), the
performance was the worst compared with other language pairs and directions in
BLEU and mWER. On the other hand, the performance of Chinese-to-Korean
was much better than Korean-to-Chinese, meaning that it is easier to generate
Korean sentence from Chinese the same as in Japanese-to-Korean and English-
to-Korean. In this language pair, we had gradual improvements according to
the use of combined morpho-syntactic information, but there was no significant
difference from the use of only the surface form. There was scant contribution of
Chinese morphological information such as part-of-speech. On the other hand,
we could get some advantageous Korean morpho-syntactic information in the
Chinese-to-Korean translation, i.e., the advantage of language and translation
models using morpho-syntactic information.
484 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
5 Conclusion and Future Works
In this paper, we described an empirical study of utilizing morpho-syntactic
information in a statistical machine translation framework. We empirically in-
vestigated the effects of morphological information with several language pairs:
Japanese and Korean with the same word order and high inflection/
agglutination, English and Korean, a pair of a highly inflecting and agglutinat-
ing language with partial free word order and an inflecting language with rigid
word order, and Chinese-Korean, a pair of a highly inflecting and agglutinating
language with partially free word order and a non-inflectional language with
rigid word order. As the results of experiments, we found that combined mor-
phological information is useful for improving the translation quality in BLEU
and mWER evaluations. According to the language pair and the direction, we
had different combinations of morpho-syntactic information that are the best
for improving the translation quality: SP(surface form and part-of-speech) for
translating J-to-K or K-to-J, SBP(surface form, base form and part-of-speech)
for E-to-K or K-to-E, SPL(surface form, part-of-speech and relative position) for
C-to-K. The utilization of morpho-syntactic information in the target language
was the most effective. Language models based on morpho-syntactic informa-
tion were very effective for performance improvement. The class-based n-gram
models improved the performance with smoothing effects in the statistical lan-
guage model. However, when translating an inflectional language, Korean into
a non-inflectional language, Chinese with quite different word order, we found
very few advantages using morphological information. One of the main reasons
might be the relatively low performance of the Chinese morphological analyzer.
The other might come from the linguistic difference. For the latter case, we need
to adopt approaches to reflect the structural characteristics such like using a
chunker/parser, context-dependent translation modeling.
Acknowledgments
The research reported here was supported in part by a contract with the National
Institute of Information and Communications Technology entitled ?A study of
speech dialogue translation technology based on a large corpus?.
References
1. Koehn P., Och F.J., and Marcu D.: Statistical Phrase-Based Translation, Proc. of
the Human Language Technology Conference(HLT/NAACL) (2003)
2. Och F. J., Tillmann C., Ney H.: Improved alignment models for statistical machine
translation, Proc. of EMNLP/WVLC (1999).
3. Och F.J. and Ney H. Improved Statistical Alignment Models, Proc. of the 38th
Annual Meeting of the Association for Computational Linguistics (2000) pp. 440-
447.
4. Zens R. and Ney H.: Improvements in Phrase-Based Statistical Machine Transla-
tion, Proc. of the Human Language Technology Conference (HLT-NAACL) (2004)
pp. 257-264
Empirical Study of Utilizing Morph-Syntactic Information in SMT 485
5. Brown P. F., Della Pietra S. A., Della Pietra V. J., and Mercer R. L.: The math-
ematics of statistical machine translation: Parameter estimation, Computational
Linguistics, (1993) 19(2):263-311
6. Ueffing N., Ney H.: Using POS Information for Statistical Machine Translation
into Morphologically Rich Languages, In Proc. 10th Conference of the European
Chapter of the Association for Computational Linguistics (EACL), (2003) pp. 347-
354
7. Ueffing N., Och F.J., Ney H.: Generation of Word Graphs in Statistical Machine
Translation In Proc. Conference on Empirical Methods for Natural Language Pro-
cessing, (2002) pp. 156-163
8. Niesen S., Ney H.: Statistical Machine Translation with Scarce Resources using
Morpho-syntactic Information, Computational Linguistics, (2004) 30(2):181-204
9. Niesen S., Och F.J., Leusch G., Ney H: An Evaluation Tool for Machine Transla-
tion: Fast Evaluation for MT Research, Proc. of the 2nd International Conference
on Language Resources and Evaluation, (2000) pp. 39-45
10. Watanabe T. and Sumita E.: Example-based Decoding for Statistical Machine
Translation, Proc. of MT Summit IX (2003) pp. 410?417
11. Och F. J. Och and Ney H.: Discriminative Training and Maximum Entropy Models
for Statistical Machine Translation, Proc. of ACL (2002)
12. Stolcke, A.: SRILM - an extensible language modeling toolkit. In Proc. Intl. Conf.
Spoken Language Processing, (2002) Denver.
13. Brown P. F., Della Pietra V. J. and deSouza P. V. and Lai J. C. and Mercer
R.L.: Class-Based n-gram Models of Natural Language, Computational Linguistics
(1992) 18(4) pp. 467-479
14. Papineni K., Roukos S., Ward T., and Zhu W.-J.: Bleu: a method for automatic
evaluation of machine translation, IBM Research Report,(2001) RC22176.
15. Takezawa T., Sumita E., Sugaya F., Yamamoto H., and Yamamoto S.: Toward a
broad-coverage bilingual corpus for speech translation of travel conversations in
the real world, Proc. of LREC (2002), pp. 147-152.
Using Machine Translation Evaluation Techniques to Determine
Sentence-level Semantic Equivalence
Andrew Finch
ATR Research Institute
2-2-2 Hikaridai
?Keihanna Science City?
Kyoto 619-0288
JAPAN
andrew.finch@atr.jp
Young-Sook Hwang
ATR Research Institute
2-2-2 Hikaridai
?Keihanna Science City?
Kyoto 619-0288
JAPAN
youngsook.hwang@atr.jp
Eiichiro Sumita
ATR Research Institute
2-2-2 Hikaridai
?Keihanna Science City?
Kyoto 619-0288
JAPAN
eiichiro.sumita@atr.jp
Abstract
The task of machine translation (MT)
evaluation is closely related to the
task of sentence-level semantic equiv-
alence classification. This paper in-
vestigates the utility of applying stan-
dard MT evaluation methods (BLEU,
NIST, WER and PER) to building clas-
sifiers to predict semantic equivalence
and entailment. We also introduce a
novel classification method based on
PER which leverages part of speech
information of the words contributing
to the word matches and non-matches
in the sentence. Our results show
that MT evaluation techniques are able
to produce useful features for para-
phrase classification and to a lesser ex-
tent entailment. Our technique gives a
substantial improvement in paraphrase
classification accuracy over all of the
other models used in the experiments.
1 Introduction
Automatic machine translation evaluation is a
means of scoring the output from a machine trans-
lation system with respect to a small corpus of
reference translations. The basic principle being
that an output is a good translation if it is ?close?
in some way to a member of a set of perfect trans-
lations for the input sentence. The closeness that
these techniques are trying to capture is in essence
the notion of semantic equivalence. Two sen-
tences being semantically equivalent if they con-
vey the same meaning.
MT evaluation techniques have found appli-
cation in the field of entailment recognition, a
close relative of semantic equivalence determina-
tion that seeks methods for deciding whether the
information provided by one sentence is included
in an another. (Perez and Alfonseca, 2005) di-
rectly applied the BLEU score to this task and
(Kouylekov and Magnini, 2005) applied both a
word and tree edit distance algorithm. In this pa-
per we evaluate these techniques or variants of
them and other MT evaluation techniques on both
entailment and semantic equivalence determina-
tion, to allow direct comparison to our results.
When using a single reference sentence for
each candidate the task of deciding whether a
pair of sentences are paraphrases and the task of
MT evaluation are very similar. Differences arise
from the nature of the sentences being compared,
that is MT output might not consist of grammat-
ically correct sentences. Moreover, MT evalu-
ation scoring need not necessarily be computed
on a sentence-by-sentence basis, but can be based
on statistics derived at the corpus level. Finally,
the process of MT evaluation is asymmetrical.
That is, there is a distinction between the ref-
erences and the candidate machine translations.
Fortunately, the automatic MT evaluation tech-
niques commonly in use do not make any ex-
plicit attempt to score grammaticality, and (ex-
cept BLEU) decompose naturally into their com-
ponent scores at the sentence level. (Blatz et al,
2004) used a variant of the WER score and the
NIST score at the sentence level to assign correct-
17
ness to translation candidates, by scoring them
with respect to a reference set. These correctness
labels were used as the ?ground truth? for classi-
fiers for the correctness of translation candidates
for candidate sentence confidence estimation. We
too adopt sentence level versions of these scores
and use them to classify paraphrase candidates.
The motivation for these experiments is two-
fold: firstly to determine how useful the features
used by these MT evaluation techniques to se-
mantic equivalence classifiers. One would ex-
pect that systems that perform well in one domain
should also perform well in the other. After all,
determining sentence level semantic equivalence
is ?part of the job? of an MT evaluator. Our sec-
ond motivation is the conjecture that successful
techniques and strategies will be transferable be-
tween the two tasks.
2 MT Evaluation Methods
MT evaluation schemes score a set of MT sys-
tem output segments (sentences in our case) S =
{s1, s2, ..., sI} with respect to a set of references
R corresponding to correct translations for their
respective segments. Since we classify sentence
pairs, we only consider the case of using a single
reference for evaluation. Thus the set of refer-
ences is given by: R = {r1, r2, ..., rI}.
2.1 WER
Word error rate (WER) (Su et al, 1992) is a mea-
sure of the number of edit operations required to
transform one sentence into another, defined as:
WER(si, ri) =
I(si, ri) + D(si, ri) + S(si, ri)
|ri|
where I(si, ri), D(si, ri) and S(si, ri) are the
number of insertions, deletions and substitutions
respectively.
2.2 PER
Position-independent word error rate (PER) (Till-
mann et al, 1997) is similar to WER except that
word order is not taken into account, both sen-
tences are treated as bags of words:
PER(si, ri) =
max[diff(si, ri), diff(ri, si)]
|ri|
where diff(si, ri) is the number of words ob-
served only in si.
2.3 BLEU
The BLEU score (Papineni et al, 2001) is based
on the geometric mean of n-gram precision. The
score is given by:
BLEU = BP ? exp
[ N
?
n=1
1
N ? log(pn)
]
where N is the maximum n-gram size.
The n-gram precision pn is given by:
pn =
? ? count(ngram)
i=1..I ngram?si
? ? countsys(ngram)
i=1..I ngram?si
where count(ngram) is the count of ngram
found in both si and ri and countsys(ngram) is
the count of ngram in si.
The brevity penalty BP penalizes MT output
for being shorter than the corresponding refer-
ences and is given by:
BP = exp
[
min
[
1? LrefLsys
, 1
]]
where Lsys is the number of words in the MT
output sentences and Lref is the number of words
in the corresponding references.
The BLEU brevity penalty is a single value
computed over the whole corpus rather than an
average of sentence level penalties which would
have made its effect too severe. For this reason,
in our experiments we omit the brevity penalty
from the BLEU score. Its effect is small since the
reference sentences and system outputs are drawn
from the same sample and have approximately the
same average length.
We ran experiments for N = 1...4, these are
referred to as BLEU1 to BLEU4 respectively.
2.4 NIST
The NIST score (Doddington, 2002) also uses
n-gram precision, differing in that an arithmetic
mean is used, weights are used to emphasize in-
formative word sequences and a different brevity
penalty is used:
NIST =
N
?
n=1
BP ?
? info(ngram)
all ngram
that co?occur
? 1
ngram?si
18
Sentence pair 1 (semantically equivalent):
1. Amrozi accused his brother, whom he called ?the witness?, of deliberately distorting his evidence.
2. Referring to him as only ?the witness?, Amrozi accused his brother of deliberately distorting his evidence.
Sentence pair 2 (not semantically equivalent):
1. Yucaipa owned Dominick?s before selling the chain to Safeway in 1998 for $2.5 billion.
2. Yucaipa bought Dominick?s in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.
Sentence pair 3 (semantically equivalent):
1. The stock rose $2.11, or about 11 percent, to close Friday at $21.51 on the New York Stock Exchange.
2. PG&E Corp. shares jumped $1.63 or 8 percent to $21.03 on the New York Stock Exchange on Friday.
Figure 1: Example sentences from the Microsoft Research Paraphrase Corpus (MSRP)
info is defined to be:
info(ngram) = log2
[count((n? 1)gram)
count(ngram)
]
where count(ngram) is the count of ngram =
w1w2 . . . wn in all the reference translations, and
(n? 1)gram is w1w2 . . . wn?1.
For NIST the brevity penalty is computed on a
segment-by-segment basis and is given by:
BP = exp
[
? log2min
[
Lsys
Lref
, 1
]]
where Lsys is the length of the MT system
output, Lref is the average number of words in
a reference translation and ? is chosen to make
BP = 0.5 when LsysLref =
2
3 .
We ran experiments for N = 1...5, these are
referred to as NIST1 to NIST5 respectively. We
include the brevity penalty in the scores used for
our experiments.
2.5 Introducing Part of Speech Information
Early experiments based on the PER score re-
vealed that removing certain classes of function
words from the edit distance calculation had a
positive impact on classification performance. In-
stead of simply removing these words, we cre-
ated a mechanism that would allow the classifier
to learn for itself the usefulness of various classes
of word. For example, one would expect edits in-
volving nouns or verbs to cost more than edits in-
volving interjections or punctuation. We used a
POS tagger for the UPENN tag set (Marcus et al,
1994) to label all the data. We then divided the
total edit distance, into components, one for each
POS tag which hold the amount of edit distance
that words bearing this POS tag contributed to the
total edit distance. The feature vector therefore
having one element for each UPENN POS tag.
Let W? be the bag of words from si that have
no matches in ri and let W+ be the bag of words
from si that have matches in ri. The value of the
feature vector ~f? corresponding to the contribu-
tion to the PER from POS tag t is given by:
f?t =
?
w?W? count?t (w)
|si|
where count?t (w) is the number of times word
w occurs in W? with tag t.
The feature vector defined above characterizes
the nature of the words in the sentences that do
not match. However it might also be important to
include information on the words in the sentence
that match. To investigate this, we augment the
feature vector ~f? with an analogous set of fea-
tures ~f+ (again one for each UPENN POS tag)
that represent the distribution over the tag set of
word unigram precision, given by:
f+t =
?
w?W+ count+t (w)
|si|
where count+t (w) is the number of times word
w occurs in W+ with tag t.
This technique is analogous to the NIST score
in that it allows the classifier to weight the impor-
tance of matches, but differs in that this weight is
learned rather than defined, and is with respect to
the word?s grammatical/semantic role rather than
as a function of rarity. When both ~f+ and ~f? are
19
MSRP PASCAL CD IE MT QA RC PP IR
Sentence1 length 21.6 27.8 24.0 27.4 36.7 31.5 27.9 24.0 24.6
Sentence2 length 21.6 11.6 16.1 8.4 19.2 8.7 10.2 11.2 7.2
Length difference ratio 0.14 0.54 0.32 0.66 0.46 0.68 0.60 0.46 0.66
Edit distance 11.3 22.0 18.2 22.2 28.1 26.8 21.8 17.3 21.0
Table 1: Corpus statistics (columns CD-IR are sub-tasks of PASCAL), ?length difference ratio? is
explained in Section 3, ?edit distance? is the average Levenstein distance between the sentences of the
pairs
used in combination the method differs again by
utilizing information about the nature of both the
matching words and the non-matching words.
We will refer to the system based only on the
feature vector ~f? as POS- , that based only on
~f+ as POS+ and that based on both as POS.
2.6 Dealing with Synonyms
Often in paraphrases the semantic information
carried by a word in one sentence is conveyed by
a synonymous word in its paraphrase. To cover
these cases we investigated the effect of allow-
ing words to match with synonyms in the edit
distance calculations. Another pilot experiment
was run with a modified edit distance that al-
lowed words in the sentences to match if their
semantic distance was less than a specific thresh-
old (chosen by visual inspection of the output of
the system). The semantic distance measure we
used was that of (Jiang and Conrath, 1997) de-
fined using the relationships between words in the
WordNet database (Fellbaum, 1998). A perfor-
mance improvement of approximately 0.6% was
achieved on the semantic equivalence task using
the strategy.
3 Experimental Data
Two corpora were used for the experiments in this
paper: the Microsoft Research Paraphrase Corpus
(MSRP) and the PASCAL Challenge?s entailment
recognition corpus (PASCAL). Corpus statistics
for these corpora (after pre-processing) are pre-
sented in Table 1.
The MSRP corpus consists of 5801 sentence
pairs drawn from a corpus of news articles from
the internet. The sentences were annotated by hu-
man annotators with labels indicating whether or
not the two sentences are close enough in mean-
ing to be close paraphrases. Multiple annotators
were used to annotate each sentence: two anno-
tators labeled the data and a third resolved the
cases where they disagreed. The average inter-
annotator agreement on this task was 83%, indi-
cating the difficulty in defining the task and the
ambiguity of the labeling. Approximately 67% of
the sentences were judged to be paraphrases. The
data was divided randomly into 4076 training sen-
tences and 1725 test sentences. For full details of
how the corpus was collected we refer the reader
to the corpus documentation. To give an idea of
the nature of the data and the difficulty of the task,
three sentences from the corpus are shown in Fig-
ure 1. The example sentences show the ambigu-
ity inherent in this task. The first sentence pair
is clearly a pair of paraphrases. The second pair
of sentences share semantic information, but were
judged to be not semantically equivalent. The
third pair are not paraphrases, they are clearly de-
scribing the movements of totally different stocks,
but the sentences share sufficient semantic con-
tent to be labeled equivalent.
For the MSRP corpus we present results using
the provided training and test sets to allow com-
parison with our results. To obtain more accurate
figures and to get an estimate of the confidence
intervals we also conducted experiments by 10-
fold jackknifing over all the data. The results from
each fold were then averaged and 95% confidence
intervals were estimated for the means.
The PASCAL data consists of 567 development
sentences and 800 test sentences drawn from 7
domains: comparable document (CD), informa-
tion extraction (IE), machine translation (MT),
question answering (QA), reading comprehension
(RC), paraphrasing (PP) and information retrieval
(IR). A full description of this corpus is given in
20
the/DT cat/NN sat/VBD on/IN the/DT mat/NN
the/DT dog/NN sat/VBD on/IN the/DT mat/NN
DT
2/6
NN
1/6
VBD
1/6
IN
1/6
DT
0/6
NN
1/6
VBD
0/6
IN
0/6
Matches Non-matches
Sentence 1:
Sentence 2:
Feature Vector = (0.33, 0.16, 0.16, 0.16, 0, 0.16, 0, 0):
Figure 2: Example of a POS feature vector. The sentences are presented in word/TAG format, and the
feature vector is labeled with these POS tags (in the upper part of the squares)
the corpus documentation 1. The data differs from
the MSRP corpus in that it is annotated for en-
tailment rather than semantic equivalence. This
explains the asymmetry in the sentence lengths,
which is apparent even in the PP component of
the corpus. We do not present results for 10-fold
jackknifing on the PASCAL data since the data
were too small in number for this type of analy-
sis.
In Table 1 ?Sentence 1? refers to the first sen-
tence of a sentence pair in the corpus, and ?Sen-
tence 2? the second. The length distance ratio
(LDR) is defined to be the average over the corpus
of:
LDR(si, ri) =
||si| ? |ri||
max(|si|, |ri|)
This measures the similarity of the lengths of
the sentences in the pairs, it has the property of
being 0 when all sentence pairs have sentences of
the same length and 1 when all sentence pairs dif-
fer maximally in length. For the PASCAL corpus
the LDR is around 0.5 for the corpus as a whole,
corresponding to a large difference in the sentence
lengths. The CD component of the corpus being
considerably more consistent in terms of sentence
length. The differences among the tasks in terms
of edit distance are less clear-cut, with the PP task
having the lowest average edit distance despite its
higher LDR. The MSRP corpus has an LDR of
only 0.14. The sentences pairs are more similar in
terms of their length and edit distance than those
in the PASCAL corpus. We will argue later that
this length similarity has a significant effect on
the performance and applicability of these tech-
niques.
1http://www.pascal-network.org/Challenges/RTE/
4 Experimental Methodology
4.1 Tokenization
In order that the sentences could be tagged with
UPENN tags (Marcus et al, 1994), they were pre-
processed by a tokenizer. After tokenization the
average MSRP sentence length was 21 words.
4.2 Stemming
Stemming conflates morphologically related
words to the same root and has been shown to
have a beneficial effect on IR tasks (Krovetz,
1993). A pilot experiment showed that the
performance of a PER-based system degraded if
the stemmed form of the word was used in place
of the surface form. However, if the stemmer was
applied only to words labeled by a POS tagger
as verbs and nouns, a performance improvement
of around 0.8% was observed on the semantic
equivalence task. Therefore, for the purposes
of the experiments, the nouns and verbs in the
sentences were all pre-processed by a stemmer.
4.3 Classification
We used a support vector machine (SVM) clas-
sifier (Vapnik, 1995) with radial basis function
kernels to classify the data. The training sets for
the respective corpora were used for training, ex-
cept in the jackknifing experiments. Feature vec-
tors (an example is given in Figure 2) were con-
structed directly from the output of the MT evalu-
ation systems, when used. The vector has 2 parts,
one due to matches and one due to non-matches.
The sum of the elements corresponding to non-
matches is equal to the PER. We calculated the
vectors for each sentence in the pair as both ref-
erence and system output and averaged to get the
vector for the pair.
21
5 Results
5.1 MSRP Corpus
The results for the jackknifing experiments are
shown in Table 2 and the results using the pro-
vided training and test sets are shown in Table 3.
In the tables the rows labeled ?PER POS+?, re-
fer to models built using feature vectors made by
combining both the PER and POS+ feature vec-
tors. The rows labeled POS refer to models built
from the combination of features from the POS+
and POS- models. The rows labeled ALL refer
to models built from combining all of the features
used in these experiments.
The results show that decomposing the PER
edit distance score into components for each POS
tag is not able to better the classification perfor-
mance of PER. The accuracy (jackknifing) for
PER alone was 71.25% and the accuracy for the
analogous technique which divides this informa-
tion in contributions for each POS tag (POS-) was
70.99%. However, when the features from PER
and POS- are combined there is an improvement
in performance (to 72.71%) indicating that the
components for each POS tag are useful, but only
in addition to the more primitive feature encod-
ing the total edit distance. Moreover, comparing
the results from POS-, POS+ and POS it is clear
that there lot to be gained by considering the con-
tributions from both the matching words and the
non-matching words. Using both together gives a
classification performance of 74.2% whereas us-
ing either component in isolation can give a per-
formance no better than 71.5%.
The one of the worst performing systems was
that based on the WER score. However, it is
possible that the way the sentences were selected
handicapped this system, since only sentences
pairs with a word-based Levenshtein distance of 8
or higher were included in the corpus. Choosing
sentence pairs with larger edit distances makes
large structural differences more likely, and the
editing effort needed to correct such structural dif-
ferences may obscure the lexical comparison that
this score relies upon.
The results for the BLEU score were unex-
pected because the performance degrades as the
order of n-gram considered increases. This effect
is much less apparent in the NIST scores where
the performance degrades but to a lesser extent.
Paraphrases exhibit variety in their grammatical
structure and perhaps changes in word ordering
can explain this effect. If so, the geometric mean
employed in the BLEU score would make the ef-
fect of higher order n-grams considerably more
detrimental than with the arithmetic mean used in
the NIST score.
5.2 PASCAL Challenge Corpus
The results for the PASCAL corpus are given in
Table 4. As expected our results are consistent
with those of (Perez and Alfonseca, 2005). The
5% overall gain in accuracy may be accounted
for by the stemming and synonym extensions to
our technique and the fact that we used BLEU1.
Our approach also differs by being symmetrical
over source and reference sentences, however it
is not clear whether this would improve perfor-
mance. The number of test examples for the
sub-experiments for each task is low (50 to 150),
therefore the results here are likely to be noisy,
but it is apparent from our results that the CD
task is the most suitable for approaches based on
word/n-gram matching. Our POS technique per-
formed well on overall and particularly well on
the CD andMT tasks, but the overall performance
improvement relative to the other techniques is
not as clear-cut. We believe this is due to difficul-
ties arising from the asymmetrical nature of the
data, and we explore this in the next section.
5.3 Sentence length similarity
In this experiment we investigate whether there is
any advantage to be gained by using these tech-
niques on corpora consisting of sentence pairs of
similar length. Both the BLEU and NIST scores
use some form of count of the total number of
n-grams in the denominator of their n-gram pre-
cision formulae. When the sentences differ in
length, the total number of n-grams is likely to
be large in relation to the number of matching n-
grams since this is bounded by the number of n-
grams in the shorter sentence. This may result in
an increase in the ?noise? in the score due to vari-
ations in sentence length similarity, degrading its
effectiveness. To address the more general issue
of whether sentence length similarity has an im-
pact on the effectiveness of these techniques we
22
Accuracy Precision Recall F-measure
?95% conf. ?95% conf. ?95% conf. ?95% conf.
WER 68.80?0.90 69.89?1.08 94.20?0.99 80.22?0.69
PER 71.25?1.03 72.05?1.23 93.58?0.59 81.39?0.72
POS- 70.99?1.16 72.07?1.43 92.99?1.52 81.15?0.79
PER POS- 72.71?1.34 73.99?1.47 91.67?0.53 81.86?0.97
POS+ 71.56?0.99 72.51?1.20 93.02?1.50 81.46?0.74
POS 74.18?0.94 75.52?1.16 91.13?0.59 82.58?0.76
BLEU1 72.30?1.10 73.71?1.30 91.41?0.70 81.59?0.83
BLEU2 70.26?1.37 71.55?1.46 92.65?0.66 80.72?0.95
BLEU3 68.30?1.42 69.40?1.25 94.54?0.87 80.03?0.97
BLEU4 67.64?1.22 68.46?1.13 96.18?0.67 79.97?0.86
NIST1 71.78?1.44 73.95?1.55 89.65?1.06 81.02?1.04
NIST2 71.64?1.12 73.64?1.43 90.13?0.25 81.03?0.81
NIST3 71.59?1.17 72.94?1.36 91.82?0.39 81.28?0.87
NIST4 71.56?1.17 72.82?1.35 92.08?0.38 81.30?0.87
NIST5 71.52?1.14 72.75?1.33 92.18?0.45 81.30?0.85
ALL 75.35?1.13 77.35?1.10 89.54?0.90 82.99?0.89
Table 2: Experimental Results (10-fold Jackknifing)
Accuracy Precision Recall F-measure
WER 68.29 69.35 93.72 79.71
PER 71.88 72.30 93.55 81.56
POS- 70.96 72.09 91.89 80.79
PER POS- 73.33 74.14 91.98 82.10
POS+ 70.96 72.09 91.89 80.79
POS 74.20 75.29 91.11 82.45
BLEU1 73.22 74.17 91.63 81.98
BLEU2 70.96 71.62 93.29 81.03
BLEU3 68.93 69.45 95.12 80.28
BLEU4 67.88 68.13 97.12 80.08
NIST1 72.35 73.83 90.50 81.32
NIST2 71.59 73.09 90.67 80.94
NIST3 71.01 72.17 91.80 80.81
NIST4 70.96 72.09 91.89 80.79
NIST5 70.75 71.89 91.67 80.58
ALL 74.96 76.58 89.80 82.66
Table 3: Experimental Results (Microsoft?s Provided Train and Test Set)
sorted the sentences pairs of the MSRP corpus
according to the length difference ratio (LDR) de-
fined in Section 3, and partitioned the sorted cor-
pus into two: low and high LDR.We then selected
as many sentences as possible from the corpus
such that the training and test sets for each data
set (high and low LDR) contained the same num-
ber positive and negative examples. This gave two
sets (high and low LDR) of 1008 training exam-
ples and 438 test examples, all training and test
data consisiting of 50% positive and 50% nega-
tive examples. The results are shown in Table 5.
The experimental results validate our concerns. In
all of the cases the performance was higher on
the data with low LDR. Moreover, the effect was
most for the BLEU and NIST scores for which we
have an explanation of the cause.
6 Conclusion
We have shown that it is possible to derive fea-
tures that can be used to determine whether sim-
ilar sentences are paraphrases of each other from
methods currently being used to automatically
evaluate machine translation systems. The ex-
periments also show that using features that en-
code the distribution over the POS tag set of both
matching words and non-matching words can sig-
nificantly enhance the performance of a PER-
based system on this task.
23
Task BLEU1 NIST1 PER POS ALL
CD 74.67 76.67 73.33 79.33 82.00
IE 49.17 50.00 48.33 42.50 44.17
IR 47.78 45.56 41.11 37.78 40.00
MT 39.17 52.50 69.17 65.83 61.67
PP 56.00 44.00 58.00 44.00 38.00
QA 56.15 53.08 56.92 53.08 55.38
RC 52.86 53.57 48.57 57.14 55.00
ALL 54.50 55.63 57.37 56.75 56.75
Table 4: Accurracy Results (PASCAL Train and PASCAL Test Set)
BLEU1 NIST1 PER POS ALL
Low LDR 76.71 77.85 72.15 75.80 76.48
High LDR 68.49 70.09 69.63 72.83 73.52
Table 5: Accuracy Results Length Similarity (MSRP)
This research begs the important question ?Is
there any correlation between performance on the
semantic equivalence classification task and per-
formance of the underlying evaluation technique
on the task of MT evaluation??. Intuitively at
least, there certainly should be. If there is, it may
be possible to use the task of classifying sentences
for semantic equivalence as a proxy for the com-
plex and time-consuming task of evaluating eval-
uation schemes by correlating automatic scores
with human scores during the development pro-
cess of MT evaluation techniques. In future work
we look forward to addressing this question, as
well as incorporating new features into the mod-
els to increase their potency.
7 Acknowledgments
The research reported here was supported in part
by a contract with the National Institute of Infor-
mation and Communications Technology entitled
?A study of speech dialogue translation technol-
ogy based on a large corpus?.
References
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2004. Confidence estimation for machine transla-
tion. Technical report, Final report JHU / CLSP
2003 Summer Workshop, Baltimore.
G. Doddington. 2002. Automatic Evaluation of
Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the HLT
Conference, San Diego, California.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Jay J. Jiang and David W. Conrath. 1997. Seman-
tic similarity based on corpus statistics and lexical
taxonomy. CoRR, 9709008.
Milen Kouylekov and Bernardo Magnini. 2005.
Recognizing textual entailment with tree edit dis-
tance algorithms. In Proceedings PASCAL Chal-
lenges Worshop on Recognising Textual Entailment,
Southampton, UK.
Robert Krovetz. 1993. Viewing morphology as an
inference process. Technical Report UM-CS-1993-
036, University of Mass-Amherst, April.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Ma-
chine Translation. IBM Research Report rc22176
(w0109022), Thomas J. Watson Research Center.
Diana Perez and Enrique Alfonseca. 2005. Appli-
cation of the bleu algroithm for recognising tex-
tual entailments. In Proceedings PASCAL Chal-
lenges Worshop on Recognising Textual Entailment,
Southampton, UK.
K.Y. Su, M.W. Wu, and J.S. Chang. 1992. A new
quantitative quality measure for machine transla-
tion systems. In Proceedings of COLING-92, pages
433?439, Nantes, France.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H. Sawaf. 1997. Accelerated dp based search
for statistical translation. In Proceedings of
Eurospeech-97, pages 2667?2670, Rhodes, Greece.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
24
Proceedings of the 43rd Annual Meeting of the ACL, pages 549?556,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Context-dependent SMT Model using Bilingual Verb-Noun Collocation
Young-Sook Hwang
ATR SLT Research Labs
2-2-2 Hikaridai Seika-cho
Soraku-gun Kyoto, 619-0288, JAPAN
youngsook.hwang@atr.jp
Yutaka Sasaki
ATR SLT Research Labs
2-2-2 Hikaridai Seika-cho
Soraku-gun Kyoto, 619-0288, JAPAN
yutaka.sasaki@atr.jp
Abstract
In this paper, we propose a new context-
dependent SMT model that is tightly cou-
pled with a language model. It is de-
signed to decrease the translation ambi-
guities and efficiently search for an opti-
mal hypothesis by reducing the hypothe-
sis search space. It works through recipro-
cal incorporation between source and tar-
get context: a source word is determined
by the context of previous and correspond-
ing target words and the next target word
is predicted by the pair consisting of the
previous target word and its correspond-
ing source word. In order to alleviate
the data sparseness in chunk-based trans-
lation, we take a stepwise back-off trans-
lation strategy. Moreover, in order to ob-
tain more semantically plausible transla-
tion results, we use bilingual verb-noun
collocations; these are automatically ex-
tracted by using chunk alignment and a
monolingual dependency parser. As a case
study, we experimented on the language
pair of Japanese and Korean. As a result,
we could not only reduce the search space
but also improve the performance.
1 Introduction
For decades, many research efforts have contributed
to the advance of statistical machine translation.
Recently, various works have improved the quality
of statistical machine translation systems by using
phrase translation (Koehn et al, 2003; Marcu et al,
2002; Och et al, 1999; Och and Ney, 2000; Zens
et al, 2004). Most of the phrase-based translation
models have adopted the noisy-channel based IBM
style models (Brown et al, 1993):



 












 (1)
In these model, we have two types of knowledge:
translation model, 




 and language model,



. The translation model links the source lan-
guage sentence to the target language sentence. The
language model describes the well-formedness of
the target language sentence and might play a role
in restricting hypothesis expansion during decoding.
To recover the word order difference between two
languages, it also allows modeling the reordering by
introducing a relative distortion probability distribu-
tion. However, in spite of using such a language
model and a distortion model, the translation outputs
may not be fluent or in fact may produce nonsense.
To make things worse, the huge hypothesis search
space is much too large for an exhaustive search. If
arbitrary reorderings are allowed, the search prob-
lem is NP-complete (Knight, 1999). According
to a previous analysis (Koehn et al, 2004) of how
many hypotheses are generated during an exhaustive
search using the IBM models, the upper bound for
the number of states is estimated by 	   




 ,
where  is the number of source words and 


 is
the size of the target vocabulary. Even though the
number of possible translations of the last two words
is much smaller than 




, we still need to make
further improvement. The main concern is the ex-
549
ponential explosion from the possible configurations
of source words covered by a hypothesis. In order
to reduce the number of possible configurations of
source words, decoding algorithms based on  as
well as the beam search algorithm have been pro-
posed (Koehn et al, 2004; Och et al, 2001). (Koehn
et al, 2004; Och et al, 2001) used heuristics for
pruning implausible hypotheses.
Our approach to this problem examines the pos-
sibility of utilizing context information in a given
language pair. Under a given target context, the cor-
responding source word of a given target word is al-
most deterministic. Conversely, if a translation pair
is given, then the related target or source context is
predictable. This implies that if we considered bilin-
gual context information in a given language pair
during decoding, we can reduce the computational
complexity of the hypothesis search; specifically, we
could reduce the possible configurations of source
words as well as the number of possible target trans-
lations.
In this study, we present a statistical machine
translation model as an alternative to the classical
IBM-style model. This model is tightly coupled
with target language model and utilizes bilingual
context information. It is designed to not only re-
duce the hypothesis search space by decreasing the
translation ambiguities but also improve translation
performance. It works through reciprocal incorpo-
ration between source and target context: source
words are determined by the context of previous
and corresponding target words, and the next target
words are predicted by the current translation pair.
Accordingly, we do not need to consider any dis-
tortion model or language model as is the case with
IBM-style models.
Under this framework, we propose a chunk-based
translation model for more grammatical, fluent and
accurate output. In order to alleviate the data sparse-
ness problem in chunk-based translation, we use a
stepwise back-off method in the order of a chunk,
sub-parts of the chunk, and word level. Moreover,
we utilize verb-noun collocations in dealing with
long-distance dependency which are automatically
extracted by using chunk alignment and a monolin-
gual dependency parser.
As a case study, we developed a Japanese-to-
Korean translation model and performed some ex-
periments on the BTEC corpus.
2 Overview of Translation Model
The goal of machine translation is to transfer the
meaning of a source language sentence, 




   

, into a target language sentence, 




   

. In most types of statistical machine trans-
lation, conditional probability 




 is used to
describe the correspondence between two sentences.
This model is used directly for translation by solving
the following maximization problem:



 









 (2)
 














(3)
 






 


 (4)
Since a source language sentence is given and the



 probability is applied to all possible corre-
sponding target sentences, we can ignore the denom-
inator in equation (3). As a result, the joint proba-
bility model can be used to describe the correspon-
dence between two sentences. We apply Markov
chain rules to the joint probability model and obtain
the following decomposed model:



 


 



	



 






	


(5)
where 

is the index of the source word that is
aligned to the word 

under the assumption of the
fixed one-to-one alignment. In this model, we have
two probabilities:
 source word prediction probability under a
given target language context, 
	



 


 target word prediction probability under the
preceding translation pair, 



 
	


The probability of target word prediction is used for
selecting the target word that follows the previous
target words. In order to make this more determin-
istic, we use bilingual context, i.e. the translation
pair of the preceding target word. For a given target
word, the corresponding source word is predicted by
source word prediction probability based on the cur-
rent and preceding target words.
550
Since a target and a source word are predicted
through reciprocal incorporation between source
and target context from the beginning of a target
sentence, the word order in the target sentence is
automatically determined and the number of pos-
sible configurations of source words is decreased.
Thus, we do not need to perform any computation
for word re-ordering. Moreover, since correspon-
dences are provided based on bilingual contextual
evidence, translation ambiguities can be decreased.
As a result, the proposed model is expected to re-
duce computational complexity during the decoding
as well as improve performance.
Furthermore, since a word-based translation ap-
proach is often incapable of handling complicated
expressions such as an idiomatic expressions or
complicated verb phrases, it often outputs nonsense
translations. To avoid nonsense translations and to
increase explanatory power, we incorporate struc-
tural aspects of the language into the chunk-based
translation model. In our model, one source chunk
is translated by exactly one target chunk, i.e., one-
to-one chunk alignment. Thus we obtain:




 













 (6)










 







	



 








	


(7)
where  is the number of chunks in a source and a
target sentence.
3 Chunk-based J/K Translation Model
with Back-Off
With the translation framework described above, we
built a chunk-based J/K translation model as a case
study. Since a chunk-based translation model causes
severe data sparseness, it is often impossible to ob-
tain any translation of a given source chunk. In order
to alleviate this problem, we apply back-off trans-
lation models while giving the consideration to lin-
guistic characteristics.
Japanese and Korean is a very close language pair.
Both are agglutinative and inflected languages in the
word formation of a bunsetsu and an eojeol. A bun-
setsu/eojeol consists of two sub parts: the head part
composed of content words and the tail part com-
posed of functional words agglutinated at the end of
the head part. The head part is related to the mean-
ing of a given segment, while the tail part indicates
a grammatical role of the head in a given sentence.
By putting this linguistic knowledge to practical
use, we build a head-tail based translation model
as a back-off version of the chunk-based translation
model. We place several constraints on this head-tail
based translation model as follows:
 The head of a given source chunk corresponds
to the head of a target chunk. The tail of the
source chunk corresponds to the tail of a target
chunk. If a chunk does not have a tail part, we
assign NUL to the tail of the chunk.
 The head of a given chunk follows the tail of the
preceding chunk and the tail follows the head of
the given chunk.
The constraints are designed to maintain the struc-
tural consistency of a chunk. Under these con-
straints, the head-tail based translation can be for-
mulated as the following equation:



	



 








	

  (8)




	




 











	






	




 











	


where 

denotes the head of the  chunk and 

means the tail of the chunk.
In the worst case, even the head-tail based model
may fail to obtain translations. In this case, we
back it off into a word-based translation model. In
the word-based translation model, the constraints
on the head-tail based translation model are not ap-
plied. The concept of the chunk-based J/K transla-
tion framework with back-off scheme can be sum-
marized as follows:
1. Input a dependency-parsed sentence at the
chunk level,
2. Apply the chunk-based translation model to the
given sentence,
3. If one of chunks does not have any correspond-
ing translation:
 divide the failed chunk into a head and a
tail part,
551
Figure 1: An example of (a) chunk alignment for chunk-based, head-tail based translation and (b) bilingual
verb-noun collocation by using the chunk alignment and a monolingual dependency parser
 back-off the translation into the head-tail
based translation model,
 if the head or tail does not have any corre-
sponding translation, apply a word-based
translation model to the chunk.
Here, the back-off model is applied only to the part
that failed to get translation candidates.
3.1 Learning Chunk-based Translation
We learn chunk alignments from a corpus that has
been word-aligned by a training toolkit for word-
based translation models: the Giza++ (Och and
Ney, 2000) toolkit for the IBM models (Brown
et al, 1993). For aligning chunk pairs, we con-
sider word(bunsetsu/eojeol) sequences to be chunks
if they are in an immediate dependency relationship
in a dependency tree. To identify chunks, we use
a word-aligned corpus, in which source language
sentences are annotated with dependency parse trees
by a dependency parser (Kudo et al, 2002) and tar-
get language sentences are annotated with POS tags
by a part-of-speech tagger (Rim, 2003). If a se-
quence of target words is aligned with the words in
a single source chunk, the target word sequence is
regarded as one chunk corresponding to the given
source chunk. By applying this method to the cor-
pus, we obtain a word- and chunk-aligned corpus
(see Figure 1).
From the aligned corpus, we directly estimate
the phrase translation probabilities,  ,
and the model parameters,  
	



 

,







	

. These estimation are made
based on relative frequencies.
3.2 Decoding
For efficient decoding, we implement a multi-stack
decoder and a beam search with  algorithm. At
each search level, the beam search moves through at
most -best translation candidates, and a multi-stack
is used for partial translations according to the trans-
lation cardinality. The output sentence is generated
from left to right in the form of partial translations.
Initially, we get  translation candidates for each
source chunk with the beam size . Every possible
translation is sorted according to its translation prob-
ability. We start the decoding with the initialized
beams and initial stack 

, the top of which has the
information of the initial hypothesis, 

 




. The decoding algorithm is described in Table 1.
In the decoding algorithm, estimating the back-
ward score is so complicated that the computational
complexity becomes too high because of the context
consideration. Thus, in order to simplify this prob-
lem, we assume the context-independence of only
the backward score estimation. The backward score
is estimated by the translation probability and lan-
guage model score of the uncovered segments. For
each uncovered segment, we select the best transla-
tion with the highest score by multiplying the trans-
lation probability of the segment by its language
model score. The translation probability and lan-
guage model score are computed without giving
consideration to context.
After estimating the forward and backward score
of each partial translation on stack 

, we try to
552
1. Push the initial hypothesis 

 



  on the initial
stack 

2. for i=1 to K
 Pop the previous state information of 







from stack 

 Get next target 

and corresponding source 


 for all pairs of 







? Check the head-tail consistency
? Mark the source segment as a covered one
? Estimate forward and backward score
? Push the state of pair 






 onto stack 

 Sort all translations on stack 

by the scores
 Prune the hypotheses
3. while (stack 

is not empty)
 Pop the state of the pair 







 Compose translation output, 

   


4. Output the best  translations
Table 1:  multi-stack decoding algorithm
prune the hypotheses. In pruning, we first sort the
partial translations on stack 

according to their
scores. If the gradient of scores steeply decreases
over the given threshold at the  translation, we
prune the translations of lower scores than the 
one. Moreover, if the number of filtered translations
is larger than 	 , we only take the top 	 transla-
tions. As a final translation, we output the single
best translation.
4 Resolving Long-distance Dependency
Since most of the current translation models take
only the local context into account, they cannot
account for long-distance dependency. This often
causes syntactically or semantically incorrect trans-
lation to be output. In this section, we describe
how this problem can be solved. For handling the
long-distance dependency problem, we utilize bilin-
gual verb-noun collocations that are automatically
acquired from the chunk-aligned bilingual corpora.
4.1 Automatic Extraction of Bilingual
Verb-Noun Collocation(BiVN)
To automatically extract the bilingual verb-noun
collocations, we utilize a monolingual dependency
parser and the chunk alignment result. The basic
concept is the same as that used in (Hwang et al,
2004): bilingual dependency parses are obtained by
sharing the dependency relations of a monolingual
dependency parser among the aligned chunks. Then
bilingual verb sub-categorization patterns are ac-
quired by navigating the bilingual dependency trees.
A verb sub-categorization is the collocation of a verb
and all of its argument/adjunct nouns, i.e. verb-noun
collocation(see Figure 1).
To acquire more reliable and general knowledge,
we apply the following filtering method with statis-
tical  test and unification operation:
 step 1. Filter out the reliable translation corre-
spondences from all of the alignment pairs by

 test at a probability level of 

 step 2. Filter out reliable bilingual verb-noun
collocations BiVN by a unification and  test
at a probability level of 

: Here, we assume
that two bilingual pairs, 

 

 and 

 


are unifiable into a frame 

 

 

 

 iff
both of them are reliable pairs filtered in step 1.
and they share the same verb pair 

 

.
4.2 Application of BiVN
The acquired BiVN is used to evaluate the bilingual
correspondence of a verb-noun pair dependent on
each other and to select the correct translation. It
can be applied to any verb-noun pair regardless of
the distance between them in a sentence. Moreover,
since the verb-noun relation in BiVN is bilingual
knowledge, the sense of each corresponding verb
and noun can be almost completely disambiguated
by each other.
In our translation system, we apply this BiVN
during decoding as follows:
1. Pivot verbs and their dependents in a given
dependency-parsed source sentence
2. When extending a hypothesis, if one of the piv-
oted verb and noun pairs is covered and its cor-
responding translation pair is in BiVN, we give
positive weight   	 to the hypothesis.
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1092?1100,
Beijing, August 2010
Dependency Forest for Statistical Machine Translation
Zhaopeng Tu ? Yang Liu ? Young-Sook Hwang ? Qun Liu ? Shouxun Lin ?
?Key Lab. of Intelligent Info. Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
{tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn yshwang@sktelecom.com
Abstract
We propose a structure called dependency
forest for statistical machine translation.
A dependency forest compactly represents
multiple dependency trees. We develop
new algorithms for extracting string-to-
dependency rules and training depen-
dency language models. Our forest-based
string-to-dependency system obtains sig-
nificant improvements ranging from 1.36
to 1.46 BLEU points over the tree-based
baseline on the NIST 2004/2005/2006
Chinese-English test sets.
1 Introduction
Dependency grammars have become increasingly
popular in syntax-based statistical machine trans-
lation (SMT). One important advantage of depen-
dency grammars is that they directly capture the
dependencies between words, which are key to re-
solving most parsing ambiguities. As a result, in-
corporating dependency trees proves to be effec-
tive in improving statistical machine translation
(Quirk et al, 2005; Ding and Palmer, 2005; Shen
et al, 2008).
However, most dependency-based translation
systems suffer from a major drawback: they only
use 1-best dependency trees for rule extraction,
dependency language model training, and decod-
ing, which potentially introduces translation mis-
takes due to the propagation of parsing errors
(Quirk and Corston-Oliver, 2006). While the
treelet system (Quirk et al, 2005) takes a de-
pendency tree as input, the string-to-dependency
system (Shen et al, 2008) decodes on a source-
language string. However, as we will show, the
string-to-dependency system still commits to us-
ing degenerate rules and dependency language
models learned from noisy 1-best trees.
To alleviate this problem, an obvious solu-
tion is to offer more alternatives. Recent studies
have shown that SMT systems can benefit from
widening the annotation pipeline: using packed
forests instead of 1-best trees (Mi and Huang,
2008), word lattices instead of 1-best segmenta-
tions (Dyer et al, 2008), and weighted alignment
matrices instead of 1-best alignments (Liu et al,
2009).
Along the same direction, we propose a struc-
ture called dependency forest, which encodes ex-
ponentially many dependency trees compactly, for
dependency-based translation systems. In this pa-
per, we develop two new algorithms for extracting
string-to-dependency rules and for training depen-
dency language models, respectively. We show
that using the rules and dependency language
models learned from dependency forests leads to
consistent and significant improvements over that
of using 1-best trees on the NIST 2004/2005/2006
Chinese-English test sets.
2 Background
Figure 1 shows a dependency tree of an English
sentence he saw a boy with a telescope. Arrows
point from the child to the parent, which is often
referred to as the head of the child. For example,
in Figure 1, saw is the head of he. A dependency
tree is more compact than its constituent counter-
part because there is no need to build a large su-
perstructure over a sentence.
Shen et al (2008) propose a novel string-to-
dependency translation model that features two
important advantages. First, they define that
a string-to-dependency rule must have a well-
formed dependency structure on the target side,
which makes efficient dynamic programming pos-
sible and manages to retain most useful non-
constituent rules. A well-formed structure can be
either fixed or floating . A fixed structure is a
1092
saw
he boy with
a telescope
a
he saw a boy with a telescope
ta kandao yige dai wangyuanjing de nanhai
Figure 1: A training example for tree-based rule
extraction.
dependency tree with all the children complete.
Floating structures consist of sibling nodes of a
common head, but the head itself is unspecified
or floating. For example, Figure 2(a) and Figure
2(b) are two fixed structures while Figure 2(c) is a
floating one.
Formally, for a given sentence w1:l = w1 . . . wl,
d1 . . . dl represent the parent word IDs for each
word. If wi is a root, we define di = 0.
Definition 1. A dependency structure di..j is fixed
on head h, where h /? [i, j], or fixed for short, if
and only if it meets the following conditions
? dh /? [i, j]
? ?k ? [i, j] and k 6= h, dk ? [i, j]
? ?k /? [i, j], dk = h or dk /? [i, j]
Definition 2. A dependency structure di..j is
floating with children C, for a non-empty set C
? {i, ..., j}, or floating for short, if and only if it
meets the following conditions
? ?h /? [i, j], s.t.?k ? C, dk = h
? ?k ? [i, j] and k /? C, dk ? [i, j]
? ?k /? [i, j], dk /? [i, j]
A dependency structure is well-formed if and
only if it is either fixed or floating.
2.1 Tree-based Rule Extraction
Figure 1 shows a training example consisting of an
English dependency tree, its Chinese translation,
boy
a
(a)
with
telescope
a
(b)
boy with
a telescope
a
(c)
Figure 2: Well-formed dependency structures cor-
responding to Figure 1. (a) and (b) are fixed and
(c) is floating.
and the word alignments between them. To facil-
itate identifying the correspondence between the
English and Chinese words, we also gives the En-
glish sentence. Extracting string-to-dependency
rules from aligned string-dependency pairs is sim-
ilar to extracting SCFG (Chiang, 2007) except that
the target side of a rule is a well-formed struc-
ture. For example, we can first extract a string-to-
dependency rule that is consistent with the word
alignment (Och and Ney, 2004):
with ((a) telescope) ? dai wangyuanjing de
Then a smaller rule
(a) telescope ? wangyuanjing
can be subtracted to obtain a rule with one non-
terminal:
with (X1) ? dai X1 de
where X is a non-terminal and the subscript indi-
cates the correspondence between non-terminals
on the source and target sides.
2.2 Tree-based Dependency Language Model
As dependency relations directly model the se-
mantics structure of a sentence, Shen et al (2008)
introduce dependency language model to better
account for the generation of target sentences.
Compared with the conventional n-gram language
models, dependency language model excels at
capturing non-local dependencies between words
(e.g., saw ... with in Figure 1). Given a depen-
dency tree, its dependency language model prob-
ability is a product of three sub-models defined
between headwords and their dependants. For ex-
ample, the probability of the tree in Figure 1 can
1093
saw0,7
he0,1 boy2,4 with4,7
a2,3 telescope5,7
a5,6
(a)
saw0,7
he0,1 boy2,7
a2,3 with4,7
telescope5,7
a5,6
(b)
saw0,7
he0,1 boy2,4 boy2,7
with4,7
e1 e2
a2,3
e3 e4
telescope5,7
e5
a5,6
e6
(c)
Figure 3: (a) the dependency tree in Figure 1, (b) another dependency tree for the same sentence, and
(c) a dependency forest compactly represents the two trees.
be calculated as:
Prob = PT (saw)
?PL(he|saw-as-head)
?PR(boy|saw-as-head)
?PR(with|boy, saw-as-head)
?PL(a|boy-as-head)
?PR(telescope|with-as-head)
?PL(a|telescope-as-head)
where PT (x) is the probability of word x being
the root of a dependency tree. PL and PR are the
generative probabilities of left and right sides re-
spectively.
As the string-to-tree system relies on 1-best
trees for parameter estimation, the quality of rule
table and dependency language model might be
affected by parsing errors and therefore ultimately
results in translation mistakes.
3 Dependency Forest
We propose to encode multiple dependency trees
in a compact representation called dependency
forest, which offers an elegant solution to the
problem of parsing error propagation.
Figures 3(a) and 3(b) show two dependency
trees for the example English sentence in Figure
1. The prepositional phrase with a telescope could
either depend on saw or boy. Figure 3(c) is a
dependency forest compactly represents the two
trees by sharing common nodes and edges.
Each node in a dependency forest is a word.
To distinguish among nodes, we attach a span to
each node. For example, in Figure 1, the span of
the first a is (2, 3) because it is the third word in
the sentence. As the fourth word boy dominates
the node a2,3, it can be referred to as boy2,4. Note
that the position of boy itself is taken into consid-
eration. Similarly, the word boy in Figure 3(b) can
be represented as boy2,7.
The nodes in a dependency forest are connected
by hyperedges. While an edge in a dependency
tree only points from a dependent to its head, a
hyperedge groups all the dependants that have a
common head. For example, in Figure 3(c), the
hyperedge
e1: ?(he0,1, boy2,4,with4,7), saw0,7?
denotes that he0,1, boy2,4, and with4,7 are depen-
dants (from left to right) of saw0,7.
More formally, a dependency forest is a pair
?V,E?, where V is a set of nodes, and E
is a set of hyperedges. For a given sentence
w1:l = w1 . . . wl, each node v ? V is in the
form of wi,j , which denotes that w dominates
the substring from positions i through j (i.e.,
wi+1 . . . wj). Each hyperedge e ? E is a pair
?tails(e), head(e)?, where head(e) ? V is the
head and tails(e) ? V are its dependants.
A dependency forest has a structure of a hy-
pergraph such as packed forest (Klein and Man-
ning, 2001; Huang and Chiang, 2005). However,
while each hyperedge in a packed forest naturally
treats the corresponding PCFG rule probability as
its weight, it is challenging to make dependency
forest to be a weighted hypergraph because depen-
dency parsers usually only output a score, which
can be either positive or negative, for each edge
in a dependency tree rather than a hyperedge in a
1094
saw0,7
he0,1 boy2,4 boy2,7
with4,7
e1 e2
a2,3
e3 e4
telescope5,7
e5
a5,6
e6
he saw a boy with a telescope
ta kandao yige dai wangyuanjing de nanhai
Figure 4: A training example for forest-based rule
extraction.
dependency forest. For example, in Figure 3(a),
the scores for the edges he ? saw, boy ? saw,
and with ? saw could be 13, 22, and -12, respec-
tively.
To assign a probability to each hyperedge, we
can first obtain a positive number for a hyperedge
using the scores of the corresponding edges:1
c(e) = exp
(?
v?tails(e) s
(
v, head(e)
)
|tails(e)|
)
(1)
where c(e) is the count of a hyperedge e, head(e)
is a head, tails(e) is a set of dependants of the
head, v is one dependant, and s(v, head(e)) is the
score of an edge from v to head(e). For example,
the count of the hyperedge e1 in Figure 3(c) is
c(e1) = exp
(
13 + 22 ? 12
3
)
(2)
Then, the probability of a hyperedge can be ob-
tained by normalizing the count among all hyper-
edges with the same head collected from a training
corpus:
p(e) = c(e)?
e?:head(e?)=head(e) c(e?)
(3)
Therefore, we obtain a weighted dependency
forest in which each hyperedge has a probability.
1It is difficult to assign a probability to each hyperedge.
The current method is arbitrary, and we will improve it in the
future.
Algorithm 1 Forest-based Initial Phrase Extrac-
tion
Input: a source sentence ?, a forest F , an alignment a,
and k
Output: minimal initial phrase setR
1: for each node v ? V in a bottom-up order do
2: for each hyperedge e ? E and head(e) = v do
3: W ? ?
4: fixs? EnumFixed(v,modifiers(e))
5: floatings? EnumFloating(modifiers(e))
6: add structures fixs, floatings to W
7: for each ? ?W do
8: if ? is consistent with a then
9: generate a rule r
10: R.append(r)
11: keep k-best dependency structures for v
4 Forest-based Rule Extraction
In tree-based rule extraction, one just needs to first
enumerate all bilingual phrases that are consis-
tent with word alignment and then check whether
the dependency structures over the target phrases
are well-formed. However, this algorithm fails to
work in the forest scenario because there are usu-
ally exponentially many well-formed structures
over a target phrase.
The GHKM algorithm (Galley et al, 2004),
which is originally developed for extracting tree-
to-string rules from 1-best trees, has been suc-
cessfully extended to packed forests recently (Mi
and Huang, 2008). The algorithm distinguishes
between minimal and composed rules. Although
there are exponentially many composed rules, the
number of minimal rules extracted from each node
is rather limited (e.g., one or zero). Therefore, one
can obtain promising composed rules by combin-
ing minimal rules.
Unfortunately, the GHKM algorithm cannot be
applied to extracting string-to-dependency rules
from dependency forests. This is because the
GHKM algorithm requires a complete subtree to
exist in a rule while neither fixed nor floating de-
pendency structures ensure that all dependants of
a head are included. For example, the floating
structure shown in Figure 2(c) actually contains
two trees.
Alternatively, our algorithm searches for well-
formed structures for each node in a bottom-up
style. Algorithm 1 shows the algorithm for ex-
tracting initial phrases, that is, rules without non-
1095
terminals from dependency forests. The algorithm
maintains k-best well-formed structures for each
node (line 11). The well-formed structures of a
head can be constructed from those of its depen-
dants. For example, in Figure 4, as the fixed struc-
ture rooted at telescope5,7 is
(a) telescope
we can obtain a fixed structure rooted for the node
with4,7 by attaching the fixed structure of its de-
pendant to the node (EnumFixed in line 4). Figure
2(b) shows the resulting fixed structure.
Similarly, the floating structure for the node
saw0,7 can be obtained by concatenating the fixed
structures of its dependants boy2,4 and with4,7
(EnumFloating in line 5). Figure 2(c) shows the
resulting fixed structure. The algorithm is similar
to Wang et al (2007), which binarize each con-
stituent node to create some intermediate nodes
that correspond to the floating structures.
Therefore, we can find k-best fixed and float-
ing structures for a node in a dependency forest
by manipulating the fixed structures of its depen-
dants. Then we can extract string-to-dependency
rules if the dependency structures are consistent
with the word alignment.
How to judge a well-formed structure extracted
from a node is better than others? We follow Mi
and Huang (2008) to assign a fractional count to
each well-formed structure. Given a tree fragment
t, we use the inside-outside algorithm to compute
its posterior probability:
??(t) = ?(root(t)) ?
?
e?t
p(e)
?
?
v?leaves(t)
?(v) (4)
where root(t) is the root of the tree, e is an edge,
leaves(t) is a set of leaves of the tree, ?(?) is out-
side probability, and ?(?) is inside probability.
For example, the subtree rooted at boy2,7 in Fig-
ure 4 has the following posterior probability:
?(boy2,7) ? p(e4) ? p(e5)
?p(e6) ? ?(a2,3) ? ?(a5,6) (5)
Now the fractional count of the subtree t is
c(t) = ??(t)??(TOP ) (6)
where TOP denotes the root node of the forest.
As a well-formed structure might be non-
constituent, we approximate the fractional count
by taking that of the minimal constituent tree frag-
ment that contains the well-formed structure. Fi-
nally, the fractional counts of well-formed struc-
tures can be used to compute the relative frequen-
cies of the rules having them on the target side (Mi
and Huang, 2008):
?(r|lhs(r)) = c(r)?
r?:lhs(r?)=lhs(r) c(r?)
(7)
?(r|rhs(r)) = c(r)?
r?:rhs(r?)=rhs(r) c(r?)
(8)
Often, our approach extracts a large amount of
rules from training corpus as we usually retain ex-
ponentially many well-formed structures over a
target phrase. To maintain a reasonable rule ta-
ble size, we discard any rule that has a fractional
count lower that a threshold t.
5 Forest-based Dependency Language
Model Training
Dependency language model plays an important
role in string-to-dependency system. Shen et
al. (2008) show that string-to-dependency system
achieves 1.48 point improvement in BLEU along
with dependency language model, while no im-
provement without it. However, the string-to-
dependency system still commits to using depen-
dency language model from noisy 1-best trees.
We now turn to dependency forest for it encodes
multiple dependency trees.
To train a dependency language model from a
dependency forest, we need to collect all heads
and their dependants. This can be easily done by
enumerating all hyperedges. Similarly, we use the
inside-outside algorithm to compute the posterior
probability of each hyperedge e,
??(e) = ?(head(e)) ? p(e)
?
?
v?tailes(e)
?(v) (9)
For example, the posterior probability of the hy-
peredge e2 in Figure 4 is calculated as
??(e2) = ?(saw0,7) ? p(e2)
??(he0,1) ? ?(boy2,7) (10)
1096
Rule DepLM NIST 2004 NIST 2005 NIST 2006 time
tree tree 33.97 30.21 30.73 19.6
tree forest 34.42? 31.06? 31.37? 24.1
forest tree 34.60? 31.16? 31.45? 21.7
forest forest 35.33?? 31.57?? 32.19?? 28.5
Table 1: BLEU scores and average decoding time (second/sentence) on the Chinese-English test sets.
The baseline system (row 2) used the rule table and dependency language model learned both from
1-best dependency trees. We use ? *? and ?**? to denote a result is better than baseline significantly at
p < 0.05 and p < 0.01, respectively.
Then, we can obtain the fractional count of a
hyperedge e,
c(e) = ??(e)??(TOP ) (11)
Each n-gram (e.g., ?boy-as-head a?) is assigned
the same fractional count of the hyperedge it be-
longs to.
We also tried training dependency language
model as in (Shen et al, 2008), which means
all hyperedges were on equal footing without re-
garding probabilities. However, the performance
is about 0.8 point lower in BLEU. One possbile
reason is that hyperedges with probabilities could
distinguish high quality structures better.
6 Experiments
6.1 Results on the Chinese-English Task
We used the FBIS corpus (6.9M Chinese words
+ 8.9M English words) as our bilingual train-
ing corpus. We ran GIZA++ (Och and Ney,
2000) to obtain word alignments. We trained a
4-gram language model on the Xinhua portion
of GIGAWORD corpus using the SRI Language
Modeling Toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing (Kneser and Ney,
1995). We optimized feature weights using the
minimum error rate training algorithm (Och and
Ney, 2002) on the NIST 2002 test set. We evalu-
ated the translation quality using case-insensitive
BLEU metric (Papineni et al, 2002) on the NIST
2004/2005/2006 test sets.
To obtain dependency trees and forests, we
parsed the English sentences of the FBIS corpus
using a shift-reduce dependency parser that en-
ables beam search (Huang et al, 2009). We only
Rules Size New Rules
tree 7.2M -
forest 7.6M 16.86%
Table 2: Statistics of rules. The last column shows
the ratio of rules extracted from non 1-best parses
being used in 1-best derivations.
retained the best well-formed structure for each
node when extracting string-to-tree rules from de-
pendency forests (i.e., k = 1). We trained two
3-gram depLMs (one from trees and another from
forests) on English side of FBIS corpus plus 2M
sentence pairs from other LDC corpus.
After extracting rules and training depLMs, we
ran our replication of string-to-dependency sys-
tem (Shen et al, 2008) to translate the develop-
ment and test sets.
Table 1 shows the BLEU scores on the test
sets. The first column ?Rule? indicates where
the string-to-dependency rules are learned from:
1-best dependency trees or dependency forests.
Similarly, the second column ?DepLM? also dis-
tinguish between the two sources for training de-
pendency language models. The baseline sys-
tem used the rule table and dependency lan-
guage model both learned from 1-best depen-
dency trees. We find that adding the rule table and
dependency language models obtained from de-
pendency forests improves string-to-dependency
translation consistently and significantly, ranging
from +1.3 to +1.4 BLEU points. In addition, us-
ing the rule table and dependency language model
trained from forest only increases decoding time
insignificantly.
How many rules extracted from non 1-best
1097
Rule DepLM BLEU
tree tree 22.31
tree forest 22.73?
forest tree 22.80?
forest forest 23.12??
Table 3: BLEU scores on the Korean-Chinese test
set.
parses are used by the decoder? Table 2 shows the
number of rules filtered on the test set. We observe
that the rule table size hardly increases. One pos-
sible reason is that we only keep the best depen-
dency structure for each node. The last row shows
that 16.86% of the rules used in 1-best deriva-
tions are extracted from non 1-best parses in the
forests, indicating that some useful rules cannot
be extracted from 1-best parses.
6.2 Results on the Korean-Chinese Task
To examine the efficacy of our approach on differ-
ent language pairs, we carried out an experiment
on Korean-Chinese translation. The training cor-
pus contains about 8.2M Korean words and 7.3M
Chinese words. The Chinese sentences were used
to train a 5-gram language model as well as a 3-
gram dependency language model. Both the de-
velopment and test sets consist of 1,006 sentences
with single reference. Table 3 shows the BLEU
scores on the test set. Again, our forest-based ap-
proach achieves significant improvement over the
baseline (p < 0.01).
6.3 Effect of K-best
We investigated the effect of different k-best
structures for each node on translation quality
(BLEU scores on the NIST 2005 set) and the rule
table size (filtered for the tuning and test sets), as
shown in Figure 5. To save time, we extracted
rules just from the first 30K sentence pairs of the
FBIS corpus. We trained a language model and
depLMs on the English sentences. We used 10
different k: 1, 2, 3, 4, 5, 6, 7, 8, 9 and 10. Ob-
viously, the higher the k is, the more rules are
extracted. When k=10, the number of rules used
on the tuning and test sets was 1,299,290 and the
BLEU score was 20.88. Generally, both the num-
ber of rules and the BLEU score went up with
20.4
20.5
20.6
20.7
20.8
20.9
21.0
21.1
21.2
21.3
21.4
21.5
21.6
21.7
21.8
0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35
BL
EU
 s
co
re
rule table size(M)
k=1,2,...,10
Figure 5: Effect of k-best on rule table size and
translation quality.
20.4
20.5
20.6
20.7
20.8
20.9
21.0
21.1
21.2
21.3
21.4
21.5
21.6
21.7
21.8
0.98 1.00 1.02 1.04 1.06 1.08 1.10
BL
EU
 s
co
re
rule table size(M)
t=1.0,0.9,...,0.1
Figure 6: Effect of pruning threshold on rule table
size and translation quality.
the increase of k. However, this trend did not
hold within the range [4,10]. We conjecture that
when retaining more dependency structures for
each node, low quality structures would be intro-
duced, resulting in much rules of low quality.
An interesting finding is that the rule table grew
rapidly when k is in range [1,4], while gradually
within the range [4,10]. One possible reason is
that there are limited different dependency struc-
tures in the spans with a maximal length of 10,
which the target side of rules cover.
6.4 Effect of Pruning Threshold
Figure 6 shows the effect of pruning threshold on
translation quality and the rule table size. We
retained 10-best dependency structures for each
node in dependency forests. We used 10 different
1098
pruning thresholds: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,
0.8, 0.9 and 1.0. Intuitively, the higher the prun-
ing threshold is, the less rules are extracted. When
t=0.1, the number of rules used on the tuning and
test sets was 1,081,841 and the BLEU score was
20.68.
Lots of rules are pruned when the pruning
threshold increases from 0.0 to 0.3 (around 20%).
After pruning away these rules, we achieved 0.6
point improvement in BLEU. However, when we
filtered more rules, the BLEU score went down.
Figures 5 and 6 show that using two parame-
ters that have to be hand-tuned achieves a small
improvement at the expense of an additional com-
plexity. To simplify the approach, we only keep
the best dependency structure for each node with-
out pruning any rule.
7 Related Works
While Mi and Huang (2008) and we both use
forests for rule extraction, there remain two ma-
jor differences. Firstly, Mi and Huang (2008) use
a packed forest, while we use a dependency forest.
Packed forest is a natural weighted hypergraph
(Klein and Manning, 2001; Huang and Chiang,
2005), for each hyperedge treats the correspond-
ing PCFG rule probability as its weight. However,
it is challenging to make dependency forest to be a
weighted hypergraph because dependency parsers
usually only output a score for each edge in a de-
pendency tree rather than a hyperedge in a depen-
dency forest. Secondly, The GHKM algorithm
(Galley et al, 2004), which is originally devel-
oped for extracting tree-to-string rules from 1-best
trees, has been successfully extended to packed
forests recently (Mi and Huang, 2008). Unfor-
tunately, the GHKM algorithm cannot be applied
to extracting string-to-dependency rules from de-
pendency forests, because the GHKM algorithm
requires a complete subtree to exist in a rule while
neither fixed nor floating dependency structures
ensure that all dependants of a head are included.
8 Conclusion and Future Work
In this paper, we have proposed to use dependency
forests instead of 1-best parses to extract string-to-
dependency tree rules and train dependency lan-
guage models. Our experiments show that our ap-
proach improves translation quality significantly
over a state-of-the-art string-to-dependency sys-
tem on various language pairs and test sets. We
believe that dependency forest can also be used to
improve the dependency treelet system (Quirk et
al., 2005) that takes 1-best trees as input.
Acknowledgement
The authors were supported by SK Telecom C&I
Business, and National Natural Science Founda-
tion of China, Contracts 60736014 and 60903138.
We thank the anonymous reviewers for their in-
sightful comments. We are also grateful to Wen-
bin Jiang for his invaluable help in dependency
forest.
References
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, pages 201?
228.
Ding, Yuan and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of ACL.
Dyer, Christopher, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL.
Galley, Michel, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Huang, Liang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Klein, Dan and Christopher D. Manning. 2001. Pars-
ing and hypergraphs. In Proceedings of IWPT.
Kneser, R. and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
Acoustics, Speech, and Signal.
Liu, Yang, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of EMNLP.
Mi, Haitao and Liang Huang. 2008. Forest-based
translation rule extraction. In Proceedings of
EMNLP.
1099
Och, Franz J. and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL.
Och, Franz J. and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL.
Och, Franz J. and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL.
Quirk, Chris and Simon Corston-Oliver. 2006. The
impact of parsing quality on syntactically-informed
statistical machine translation. In Proceedings of
EMNLP.
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal smt. In Proceedings of ACL.
Shen, Libin, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL.
Stolcke, Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP.
Wang, Wei, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based
machine translation accuracy. In Proceedings of
EMNLP.
1100
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1200?1208,
Beijing, August 2010
Joint Tokenization and Translation
Xinyan Xiao ? Yang Liu ? Young-Sook Hwang ? Qun Liu ? Shouxun Lin ?
?Key Lab. of Intelligent Info. Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
{xiaoxinyan,yliu,liuqun,sxlin}@ict.ac.cn yshwang@sktelecom.com
Abstract
As tokenization is usually ambiguous for
many natural languages such as Chinese
and Korean, tokenization errors might po-
tentially introduce translation mistakes for
translation systems that rely on 1-best to-
kenizations. While using lattices to of-
fer more alternatives to translation sys-
tems have elegantly alleviated this prob-
lem, we take a further step to tokenize
and translate jointly. Taking a sequence
of atomic units that can be combined to
form words in different ways as input, our
joint decoder produces a tokenization on
the source side and a translation on the
target side simultaneously. By integrat-
ing tokenization and translation features
in a discriminative framework, our joint
decoder outperforms the baseline trans-
lation systems using 1-best tokenizations
and lattices significantly on both Chinese-
English and Korean-Chinese tasks. In-
terestingly, as a tokenizer, our joint de-
coder achieves significant improvements
over monolingual Chinese tokenizers.
1 Introduction
Tokenization plays an important role in statistical
machine translation (SMT) because tokenizing a
source-language sentence is always the first step
in SMT systems. Based on the type of input, Mi
and Huang (2008) distinguish between two cat-
egories of SMT systems : string-based systems
(Koehn et al, 2003; Chiang, 2007; Galley et al,
source
target
tokenize+translate
string tokenization
translation
source
target
string
tokenize
tokenization
translate
translation
(a)
(b)
Figure 1: (a) Separate tokenization and translation and (b)
joint tokenization and translation.
2006; Shen et al, 2008) that take a string as input
and tree-based systems (Liu et al, 2006; Mi et al,
2008) that take a tree as input. Note that a tree-
based system still needs to first tokenize the input
sentence and then obtain a parse tree or forest of
the sentence. As shown in Figure 1(a), we refer to
this pipeline as separate tokenization and transla-
tion because they are divided into single steps.
As tokenization for many languages is usually
ambiguous, SMT systems that separate tokeniza-
tion and translation suffer from a major drawback:
tokenization errors potentially introduce transla-
tion mistakes. As some languages such as Chi-
nese have no spaces in their writing systems, how
to segment sentences into appropriate words has
a direct impact on translation performance (Xu et
al., 2005; Chang et al, 2008; Zhang et al, 2008).
In addition, although agglutinative languages such
as Korean incorporate spaces between ?words?,
which consist of multiple morphemes, the gran-
ularity is too coarse and makes the training data
1200
considerably sparse. Studies reveal that seg-
menting ?words? into morphemes effectively im-
proves translating morphologically rich languages
(Oflazer, 2008). More importantly, a tokenization
close to a gold standard does not necessarily leads
to better translation quality (Chang et al, 2008;
Zhang et al, 2008). Therefore, it is necessary
to offer more tokenizations to SMT systems to
alleviate the tokenization error propagation prob-
lem. Recently, many researchers have shown that
replacing 1-best tokenizations with lattices im-
proves translation performance significantly (Xu
et al, 2005; Dyer et al, 2008; Dyer, 2009).
We take a next step towards the direction of
offering more tokenizations to SMT systems by
proposing joint tokenization and translation. As
shown in Figure 1(b), our approach tokenizes
and translates jointly to find a tokenization and
a translation for a source-language string simul-
taneously. We integrate translation and tokeniza-
tion models into a discriminative framework (Och
and Ney, 2002), within which tokenization and
translation models interact with each other. Ex-
periments show that joint tokenization and trans-
lation outperforms its separate counterparts (1-
best tokenizations and lattices) significantly on
the NIST 2004 and 2005 Chinese-English test
sets. Our joint decoder also reports positive results
on Korean-Chinese translation. As a tokenizer,
our joint decoder achieves significantly better to-
kenization accuracy than three monolingual Chi-
nese tokenizers.
2 Separate Tokenization and Translation
Tokenization is to split a string of characters into
meaningful elements, which are often referred to
as words. Typically, machine translation sepa-
rates tokenization from decoding as a preprocess-
ing step. An input string is first preprocessed by a
tokenizer, and then is translated based on the tok-
enized result. Take the SCFG-based model (Chi-
ang, 2007) as an example. Given the character
sequence of Figure 2(a), a tokenizer first splits it
into the word sequence as shown in Figure 2(b),
then the decoder translates the word sequence us-
ing the rules in Table 1.
This approach makes the translation process
simple and efficient. However, it may not be
? ? ? ? ? ? 0? 1 2 3 4 5 6 7
Figure 2: Chinese tokenization: (a) character sequence; (b)
and (c) tokenization instances; (d) lattice created from (b)
and (c). We insert ?-? between characters in a word just for
clarity.
r1 tao-fei-ke ?Taufik
r2 duo fen ? gain a point
r3 x1 you-wang x2 ? x1 will have the chance to x2
Table 1: An SCFG derivation given the tokenization of Fig-
ure 2(b).
optimal for machine translation. Firstly, optimal
granularity is unclear for machine translation. We
might face severe data sparseness problem by us-
ing large granularity, while losing much useful in-
formation with small one. Consider the example
in Figure 2. It is reasonable to split duo fen into
two words as duo and fen, since they have one-
to-one alignments to the target side. Nevertheless,
while you and wang also have one-to-one align-
ments, it is risky to segment them into two words.
Because the decoder is prone to translate wang as
a verb look without the context you. Secondly,
there may be tokenization errors. In Figure2(c),
tao fei ke is recognized as a Chinese person name
with the second name tao and the first name fei-ke,
but the whole string tao fei ke should be a name of
the Indonesian badminton player.
Therefore, it is necessary to offer more tok-
enizations to SMT systems to alleviate the tok-
enization error propagation problem. Recently,
many researchers have shown that replacing 1-
best tokenizations with lattices improves transla-
tion performance significantly. In this approach, a
lattice compactly encodes many tokenizations and
is fixed before decoding.
1201
0 1 2 3 4 5 6 7
1 2
3
Figure 3: A derivation of the joint model for the tokenization
in Figure 2(b) and the translation in Figure 2 by using the
rules in Table 1. N means tokenization while  represents
translation.
3 Joint Tokenization and Translation
3.1 Model
We take a next step towards the direction of of-
fering more tokenizations to SMT systems by
proposing joint tokenization and translation. As
shown in Figure 1(b), the decoder takes an un-
tokenized string as input, and then tokenizes the
source side string while building the correspond-
ing translation of the target side. Since the tradi-
tional rules like those in Table 1 natively include
tokenization information, we can directly apply
them for simultaneous construction of tokeniza-
tion and translation by the source side and target
side of rules respectively. In Figure 3, our joint
model takes the character sequence in Figure 2(a)
as input, and synchronously conducts both trans-
lation and tokenization using the rules in Table 1.
As our model conducts tokenization during de-
coding, we can integrate tokenization models as
features together with translation features under
the discriminative framework. We expect tok-
enization and translation could collaborate with
each other. Tokenization offers translation with
good tokenized results, while translation helps to-
kenization to eliminate ambiguity. Formally, the
probability of a derivation D is represented as
P (D) ?
?
i
?i(D)?i (1)
where ?i are features defined on derivations in-
cluding translation and tokenization, and ?i are
feature weights. We totally use 16 features:
? 8 traditional translation features (Chiang,
2007): 4 rule scores (direct and reverse trans-
lation scores; direct and reverse lexical trans-
lation scores); language model of the target
side; 3 penalties for word count, extracted
rule and glue rule.
? 8 tokenization features: maximum entropy
model, language model and word count of
the source side (Section 3.2). To handle
the Out Of Vocabulary (OOV) problem (Sec-
tion 3.3), we also introduce 5 OOV features:
OOV character count and 4 OOV discount
features.
Since our model is still a string-based model, the
CKY algorithm and cube pruning are still applica-
ble for our model to find the derivation with max
score.
3.2 Adding Tokenization Features
Maximum Entropy model (ME). We first intro-
duce ME model feature for tokenization by cast-
ing it as a labeling problem (Xue and Shen, 2003;
Ng and Low, 2004). We label a character with the
following 4 types:
? b: the begin of a word
? m: the middle of a word
? e: the end of a word
? s: a single-character word
Taking the tokenization you-wang of the string
you wang for example, we first create a label se-
quence b e for the tokenization you-wang and then
calculate the probability of tokenization by
P (you-wang | you wang)
= P (b e | you wang)
= P (b | you, you wang)
? P (e | wang, you wang)
Given a tokenization wL1 with L words for a
character sequence cn1 , we firstly create labels ln1
for every characters and then calculate the proba-
bility by
P (wL1 |cn1 ) = P (ln1 |cn1 ) =
n?
i=1
P (li|ci, cn1 ) (2)
1202
Under the ME framework, the probability of as-
signing the character c with the label l is repre-
sented as:
P (l|c, cn1 ) =
exp[?i ?ihi(l, c, cn1 )]?
l? exp[
?
i ?ihi(l?, c, cn1 )]
(3)
where hi is feature function, ?i is the feature
weight of hi. We use the feature templates the
same as Jiang et al, (2008) to extract features for
ME model. Since we directly construct tokeniza-
tion when decoding, it is straight to calculate the
ME model score of a tokenization according to
formula (2) and (3).
Language Model (LM). We also use the n-
gram language model to calculate the probability
of a tokenization wL1 :
P (wL1 ) =
L?
i=1
P (wi|wi?1i?n+1) (4)
For instance, we compute the probability of the
tokenization shown in Figure 2(b) under a 3-gram
model by
P (tao-fei-ke)
?P (you-wang | tao-fei-ke)
?P (duo | tao-fei-ke, you-wang)
?P (fen | you-wang, duo)
Word Count (WC). This feature counts the
number of words in a tokenization. Language
model is prone to assign higher probabilities to
short sentences in a biased way. This feature can
compensate this bias by encouraging long sen-
tences. Furthermore, using this feature, we can
optimize the granularity of tokenization for trans-
lation. If larger granularity is preferable for trans-
lation, then we can use this feature to punish the
tokenization containing more words.
3.3 Considering All Tokenizations
Obviously, we can construct the potential tok-
enizations and translations by only using the ex-
tracted rules, in line with traditional translation
decoding. However, it may limits the potential to-
kenization space. Consider a string you wang. If
you-wang is not reachable by the extracted rules,
the tokenization you-wang will never be consid-
ered under this way. However, the decoder may
still create a derivation by splitting the string as
small as possible with tokenization you wang and
translating you with a and wang with look, which
may hurt the translation performance. This case
happens frequently for named entity especially.
Overall, it is necessary to assure that the de-
coder can derive all potential tokenizations (Sec-
tion 4.1.3).
To assure that, when a span is not tokenized into
a single word by the extracted rules, we will add
an operation, which is considering the entire span
as an OOV. That is, we tokenize the entire span
into a single word with a translation that is the
copy of source side. We can define the set of all
potential tokenizations ?(cn1 ) for the character se-
quence cn1 in a recursive way by
?(cn1 ) =
n?1?
i
{?(ci1)
?
{w(cni+1)}} (5)
here w(cni+1) means a word contains characters
cni+1 and
?
means the times of two sets. Ac-
cording to this recursive definition, it is easy to
prove that all tokenizations is reachable by using
the glue rule (S ? SX,SX) and the added op-
eration. Here, glue rule is used to concatenate the
translation and tokenization of the two variables S
and X, which acts the role of the operator ? in
equation (5).
Consequently, this introduces a large number
of OOVs. In order to control the generation of
OOVs, we introduce the following OOV features:
OOV Character Count (OCC). This feature
counts the number of characters covered by OOV.
We can control the number of OOV characters by
this feature. It counts 3 when tao-fei-ke is an OOV,
since tao-fei-ke has 3 characters.
OOV Discount (OD). The chances to be OOVs
vary for words with different counts of characters.
We can directly attack this problem by adding
features ODi that reward or punish OOV words
which contains with i characters, or ODi,j for
OOVs contains with i to j characters. 4 OD fea-
tures are used in this paper: 1, 2, 3 and 4+. For
example, OD3 counts 1 when the word tao-fei-ke
is an OOV.
1203
Method Train #Rule Test TFs MT04 MT05 Speed
Separate
ICT 151M ICT ? 34.82 33.06 2.48
SF 148M SF ? 35.29 33.22 2.55
ME 141M ME ? 33.71 30.91 2.34
All 219M Lattice ? 35.79 33.95 3.83? 35.85 33.76 6.79
Joint
ICT 151M
Character
?
36.92 34.69 17.66
SF 148M 37.02 34.56 17.37
ME 141M 36.78 34.17 17.23
All 219M 37.25** 34.88** 17.52
Table 2: Comparison of Separate and Joint methods in terms of BLEU and speed (second per sentence). Columns Train
and Test represents the tokenization methods for training and testing respectively. Column TFs stands for whether the 8
tokenization features is used (?) or not (?). ICT, SF and ME are segmenter names for preprocessing. All means combined
corpus processed by the three segmenters. Lattice represent the system implemented as Dyer et al, (2008). ** means
significantly (Koehn, 2004) better than Lattice (p < 0.01).
4 Experiments
In this section, we try to answer the following
questions:
1. Does the joint method outperform conven-
tional methods that separate tokenization
from decoding. (Section 4.1)
2. How about the tokenization performance of
the joint decoder? (Section 4.2)
4.1 Translation Evaluation
We use the SCFG model (Chiang, 2007) for our
experiments. We firstly work on the Chinese-
English translation task. The bilingual training
data contains 1.5M sentence pairs coming from
LDC data.1 The monolingual data for training
English language model includes Xinhua portion
of the GIGAWORD corpus, which contains 238M
English words. We use the NIST evaluation sets
of 2002 (MT02) as our development data set, and
sets of 2004(MT04) and 2005(MT05) as test sets.
We use the corpus derived from the People?s Daily
(Renmin Ribao) in Feb. to Jun. 1998 containing
6M words for training LM and ME tokenization
models.
Translation Part. We used GIZA++ (Och and
Ney, 2003) to perform word alignment in both di-
rections, and grow-diag-final-and (Koehn et al,
2003) to generate symmetric word alignment. We
extracted the SCFG rules as describing in Chiang
(2007). The language model were trained by the
1including LDC2002E18, LDC2003E07, LDC2003E14,
Hansards portion of LDC2004T07, LDC2004T08 and
LDC2005T06
SRILM toolkit (Stolcke, 2002).2 Case insensitive
NIST BLEU (Papineni et al, 2002) was used to
measure translation performance.
Tokenization Part. We used the toolkit imple-
mented by Zhang (2004) to train the ME model.
Three Chinese word segmenters were used for
comparing: ICTCLAS (ICT) developed by insti-
tute of Computing Technology Chinese Academy
of Sciences (Zhang et al, 2003); SF developed at
Stanford University (Huihsin et al, 2005) and ME
which exploits the ME model described in section
(3.2).
4.1.1 Joint Vs. Separate
We compared our joint tokenization and trans-
lation with the conventional separate methods.
The input of separate tokenization and translation
can either be a single segmentation or a lattice.
The lattice combines the 1-best segmentations of
segmenters. Same as Dyer et al, (2008), we also
extracted rules from a combined bilingual corpus
which contains three copies from different seg-
menters. We refer to this version of rules as All.
Table 2 shows the result.3 Using all rule ta-
ble, our joint method significantly outperforms the
best single system SF by +1.96 and +1.66 points
on MT04 and MT05 respectively, and also out-
performs the lattice-based system by +1.46 and
+0.93 points. However, the 8 tokenization fea-
tures have small impact on the lattice system,
probably because the tokenization space limited
2The calculation of LM probabilities for OOVs is done
by the SRILM without special treatment by ourself.
3The weights are retrained for different test conditions, so
do the experiments in other sections.
1204
ME LM WC OCC OD MT05
? ? ? ? ? 24.97? ? ? ? ? 25.30
? ? ? ? ? 24.70
? ? ? ? ? 24.84
? ? ? ? ? 25.51
? ? ? ? ? 25.34
? ? ? ? ? 25.74? ? ? ? ?
26.37
Table 3: Effect of tokenization features on Chinese-English
translation task. ?
?
? denotes using a tokenization feature
while ??? denotes that it is inactive.
by lattice has been created from good tokeniza-
tion. Not surprisingly, our decoding method is
about 2.6 times slower than lattice method with
tokenization features, since the joint decoder takes
character sequences as input, which is about 1.7
times longer than the corresponding word se-
quences tokenized by segmenters. (Section 4.1.4).
The number of extracted rules with different
segment methods are quite close, while the All
version contains about 45% more rules than the
single systems. With the same rule table, our joint
method improves the performance over separate
method up to +3.03 and +3.26 points (ME). In-
terestingly, comparing with the separate method,
the tokenization of training data has smaller effect
on joint method. The BLEU scores of MT04 and
MT05 fluctuate about 0.5 and 0.7 points when ap-
plying the joint method, while the difference of
separate method is up to 2 and 3 points respec-
tively. It shows that the joint method is more ro-
bust to segmentation performance.
4.1.2 Effect of Tokenization Model
We also investigated the effect of tokenization
features on translation. In order to reduce the time
for tuning weights and decoding, we extracted
rules from the FBIS part of the bilingual corpus,
and trained a 4-gram English language model on
the English side of FBIS.
Table 3 shows the result. Only using the 8 trans-
lation features, our system achieves a BLEU score
of 24.97. By activating all tokenization features,
the joint decoder obtains an absolute improve-
ment by 1.4 BLEU points. When only adding
one single tokenization feature, the LM and WC
fail to show improvement, which may result from
their bias to short or long tokenizations. How-
Method BLEU #Word Grau #OOV
ICT 33.06 30,602 1.65 644
SF 33.22 30,119 1.68 882
ME 30.91 29,717 1.70 1,614
Lattice 33.95 30,315 1.66 494
JointICT 34.69 29,723 1.70 996
JointSF 34.56 29,839 1.69 972
JointME 34.17 29,771 1.70 1,062
JointAll 34.88 29,644 1.70 883
Table 4: Granularity (Grau, counts of character per word)
and counts of OOV words of different methods on MT05.
The subscript of joint means the type of rule table.
ever, these two features have complementary ad-
vantages and collaborate well when using them to-
gether (line 8). The OCC and OD features also
contribute improvements which reflects the fact
that handling the generation of OOV is important
for the joint model.
4.1.3 Considering All Tokenizations?
In order to explain the necessary of considering
all potential tokenizations, we compare the perfor-
mances of whether to tokenize a span as a single
word or not as illustrated in section 3.3. When
only tokenizing by the extracted rules, we obtain
34.37 BLEU on MT05, which is about 0.5 points
lower than considering all tokenizations shown in
Table 2. This indicates that spuriously limitation
of the tokenization space may degenerate transla-
tion performance.
4.1.4 Results Analysis
To better understand why the joint method can
improve the translation quality, this section shows
some details of the results on the MT05 data set.
Table 4 shows the granularity and OOV word
counts of different configurations. The lattice
method reduces the OOV words quite a lot which
is 23% and 70% comparing with ICT and ME. In
contrast, the joint method gain an absolute im-
provement even thought the OOV count do not
decrease. It seems the lattice method prefers to
translate more characters (since smaller granular-
ity and less OOVs), while our method is inclined
to maintain integrity of words (since larger granu-
larity and more OOVs). This also explains the dif-
ficulty of deciding optimal tokenization for trans-
lation before decoding.
There are some named entities or idioms that
1205
Method Type F1 Time
Monolingual
ICT 97.47 0.010
SF 97.48 0.007
ME 95.53 0.008
Joint
ICT 97.68 9.382
SF 97.68 10.454
ME 97.60 10.451
All 97.70 9.248
Table 5: Comparison of segmentation performance in terms
of F1 score and speed (second per sentence). Type column
means the segmenter for monolingual method, while repre-
sents the rule tables used by joint method.
are split into smaller granularity by the seg-
menters. For example:???? which is an English
name ?Stone? or ??-g -u? which means
?teenage?. Although the separate method is possi-
ble to translate them using smaller granularity, the
translation results are in fact wrong. In contrast,
the joint method tokenizes them as entire OOV
words, however, it may result a better translation
for the whole sentence.
We also count the overlap of the segments
used by the JointAll system towards the single
segmentation systems. The tokenization result
of JointAll contains 29, 644 words, and shares
28, 159 , 27, 772 and 27, 407 words with ICT ,
SF and ME respectively. And 46 unique words
appear only in the joint method, where most of
them are named entity.
4.2 Chinese Word Segmentation Evaluation
We also test the tokenization performance of our
model on Chinese word segmentation task. We
randomly selected 3k sentences from the corpus
of People?s Daily in Jan. 1998. 1k sentences
were used for tuning weights, while the other 2k
sentences were for testing. We use MERT (Och,
2003) to tune the weights by minimizing the error
measured by F1 score.
As shown in Table 5, with all features activated,
our joint decoder achieves an F1 score of 97.70
which reduces the tokenization error comparing
with the best single segmenter ICT by 8.7%. Sim-
ilar to the translation performance evaluation, our
joint decoder outperforms the best segmenter with
any version of rule tables.
Feature F1
TFs 97.37
TFs + RS 97.65
TFs + LM 97.67
TFs + RS + LM 97.62
All 97.70
Table 6: Effect of the target side information on Chinese
word segmentation. TFs stands for the 8 tokenization fea-
tures. All represents all the 16 features.
4.2.1 Effect of Target Side Information
We compared the effect of the 4 Rule Scores
(RS), target side Language Model (LM) on tok-
enization. Table 6 shows the effect on Chinese
word segmentation. When only use tokenization
features, our joint decoder achieves an F1 score
of 97.37. Only integrating language model or rule
scores, the joint decoder achieves an absolute im-
provement of 0.3 point in F1 score, which reduces
the error rate by 11.4%. However, when combin-
ing them together, the F1 score deduces slightly,
which may result from the weight tuning. Us-
ing all feature, the performance comes to 97.70.
Overall, our experiment shows that the target side
information can improve the source side tokeniza-
tion under a supervised way, and outperform state-
of-the-art systems.
4.2.2 Best Tokenization = Best Translation?
Previous works (Zhang et al, 2008; Chang et
al., 2008) have shown that preprocessing the in-
put string for decoder by better segmenters do
not always improve the translation quality, we re-
verify this by testing whether the joint decoder
produces good tokenization and good translation
at the same time. To answer the question, we
used the feature weights optimized by maximiz-
ing BLEU for tokenization and used the weights
optimized by maximizing F1 for translation. We
test BLEU on MT05 and F1 score on the test data
used in segmentation evaluation experiments. By
tuning weights regarding to BLEU (the configura-
tion for JointAll in table 2), our decoder achieves
a BLEU score of 34.88 and an F1 score of 92.49.
Similarly, maximizing F1 (the configuration for
the last line in table 6) leads to a much lower
BLEU of 27.43, although the F1 is up to 97.70.
This suggests that better tokenization may not al-
ways lead to better translations and vice versa
1206
Rule #Rule Method Test Time
Morph 46M Separate 21.61 4.12Refined 55M 21.21 4.63
All 74M Joint 21.93* 5.10
Table 7: Comparison of Separate and Joint method in terms
of BLEU score and decoding speed (second per sentence) on
Korean-Chinese translation task.
even by the joint decoding. This also indicates the
hard of artificially defining the best tokenization
for translation.
4.3 Korean-Chinese Translation
We also test our model on a quite different task:
Korean-Chinese. Korean is an agglutinative lan-
guage, which comes from different language fam-
ily comparing with Chinese.
We used a newswire corpus containing 256k
sentence pairs as training data. The development
and test data set contain 1K sentence each with
one single reference. We used the target side of
training set for language model training. The Ko-
rean part of these data were tokenized into mor-
pheme sequence as atomic unit for our experi-
ments.
We compared three methods. First is directly
use morpheme sequence (Morph). The second
one is refined data (Refined), where we use selec-
tive morphological segmentation (Oflazer, 2008)
for combining morpheme together on the training
data. Since the selective method needs alignment
information which is unavailable in the decod-
ing, the test data is still of morpheme sequence.
These two methods still used traditional decoding
method. The third one extracting rules from com-
bined (All) data of methods 1 and 2, and using
joint decoder to exploit the different granularity
of rules.
Table 7 shows the result. Since there is no gold
standard data for tokenization, we do not use ME
and LM tokenization features here. However, our
joint method can still significantly (p < 0.05) im-
prove the performance by about +0.3 points. This
also reflects the importance of optimizing granu-
larity for morphological complex languages.
5 Related Work
Methods have been proposed to optimize tok-
enization for word alignment. For example, word
alignment can be simplified by packing (Ma et al,
2007) several consecutive words together. Word
alignment and tokenization can also be optimized
by maximizing the likelihood of bilingual corpus
(Chung and Gildea, 2009; Xu et al, 2008). In fact,
these work are orthogonal to our joint method,
since they focus on training step while we are con-
cerned of decoding. We believe we can further
the performance by combining these two kinds of
work.
Our work also has connections to multilingual
tokenization (Snyder and Barzilay, 2008). While
they have verified that tokenization can be im-
proved by multilingual learning, our work shows
that we can also improve tokenization by collabo-
rating with translation task in a supervised way.
More recently, Liu and Liu (2010) also shows
the effect of joint method. They integrate parsing
and translation into a single step and improve the
performance of translation significantly.
6 Conclusion
We have presented a novel method for joint tok-
enization and translation which directly combines
the tokenization model into the decoding phase.
Allowing tokenization and translation to collab-
orate with each other, tokenization can be opti-
mized for translation, while translation also makes
contribution to tokenization performance under a
supervised way. We believe that our approach can
be applied to other string-based model such as
phrase-based model (Koehn et al, 2003), string-
to-tree model (Galley et al, 2006) and string-to-
dependency model (Shen et al, 2008).
Acknowledgement
The authors were supported by SK Telecom C&I
Business, and National Natural Science Founda-
tion of China, Contracts 60736014 and 60903138.
We thank the anonymous reviewers for their in-
sightful comments. We are also grateful to Wen-
bin Jiang, Zhiyang Wang and Zongcheng Ji for
their helpful feedback.
1207
References
Chang, Pi-Chuan, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In the
Third Workshop on SMT.
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):201?
228.
Chung, Tagyoung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Proc.
EMNLP 2009.
Dyer, Christopher, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proc. ACL 2008.
Dyer, Chris. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proc.
NAACL 2009.
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL 2006.
Huihsin, Tseng, Pichuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning. 2005.
A conditional random field word segmenter. In
Fourth SIGHAN Workshop.
Jiang, Wenbin, Liang Huang, Qun Liu, and Yajuan Lu?.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proc. ACL 2008.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.
Koehn, Philipp. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
Liu, Yang and Qun Liu. 2010. Joint parsing and trans-
lation. In Proc. Coling 2010.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. ACL 2006.
Ma, Yanjun, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proc. ACL 2007.
Mi, Haitao, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL 2008.
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proc. EMNLP
2004.
Och, Franz J. and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. ACL 2002.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Och, Franz Josef. 2003. Minimum error rate train-
ing in statistical machine translation. In Proc. ACL
2003.
Oflazer, Kemal. 2008. Statistical machine translation
into a morphologically complex language. In Proc.
CICL 2008.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Proc.
ACL 2002.
Shen, Libin, Xu Jinxi, and Weischedel Ralph. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. ACL 2008.
Snyder, Benjamin and Regina Barzilay. 2008. Un-
supervised multilingual learning for morphological
segmentation. In Proc. ACL 2008.
Stolcke, Andreas. 2002. Srilm ? an extensible lan-
guage modeling toolkit.
Xu, Jia, Evgeny Matusov, Richard Zens, and Her-
mann Ney. 2005. Integrated chinese word segmen-
tation in statistical machine translation. In Proc.
IWSLT2005.
Xu, Jia, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised
chinese word segmentation for statistical machine
translation. In Proc. Coling 2008.
Xue, Nianwen and Libin Shen. 2003. Chinese word
segmentation as LMR tagging. In SIGHAN Work-
shop.
Zhang, Hua-Ping, Hong-Kui Yu, De-Yi Xiong, and
Qun Liu. 2003. Hhmm-based chinese lexical an-
alyzer ictclas. In the Second SIGHAN Workshop.
Zhang, Ruiqiang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In the Third
Workshop on SMT.
Zhang, Le. 2004. Maximum entropy modeling toolkit
for python and c++.
1208
Coling 2010: Poster Volume, pages 623?629,
Beijing, August 2010
A Post-processing Approach to Statistical Word Alignment  
Reflecting Alignment Tendency between Part-of-speeches 
Jae-Hee Lee1, Seung-Wook Lee1, Gumwon Hong1, 
Young-Sook Hwang2, Sang-Bum Kim2,  Hae-Chang Rim1 
1Dept. of Computer and Radio Communications Engineering, Korea University 
2Institute of Future Technology, SK Telecom 
1{jlee,swlee,gwhong,rim}@nlp.korea.ac.kr, 
2{yshwang,sangbum.kim}@sktelecom.com 
 
Abstract 
Statistical word alignment often suffers 
from data sparseness. Part-of-speeches 
are often incorporated in NLP tasks to 
reduce data sparseness. In this paper, 
we attempt to mitigate such problem by 
reflecting alignment tendency between 
part-of-speeches to statistical word 
alignment. Because our approach does 
not rely on any language-dependent 
knowledge, it is very simple and purely 
statistic to be applied to any language 
pairs. End-to-end evaluation shows that 
the proposed method can improve not 
only the quality of statistical word 
alignment but the performance of sta-
tistical machine translation. 
1 Introduction 
Word alignment is defined as mapping corre-
sponding words in parallel text. A word 
aligned parallel corpora are very valuable re-
sources in NLP. They can be used in various 
applications such as word sense disambigua-
tion, automatic construction of bilingual lexi-
con, and statistical machine translation (SMT). 
In particular, the initial quality of statistical 
word alignment dominates the quality of SMT 
(Och and Ney 2000; Ganchev et al, 2008); 
almost all current SMT systems basically refer 
to the information inferred from word align-
ment result. 
One of the widely used approaches to statis-
tical word alignment is based on the IBM 
models (Brown et al, 1993). IBM models are 
constructed based on words? co-occurrence 
and positional information. If sufficient train-
ing data are given, IBM models can be suc-
cessfully applied to any language pairs. How-
ever, for minority language pairs such as Eng-
lish-Korean and Swedish-Japanese, it is very 
difficult to obtain large amounts of parallel 
corpora. Without sufficient amount of parallel 
corpus, it is very difficult to learn the correct 
correspondences between words that infre-
quently occur in the training data. 
Part-of-speeches (POS), which represent 
morphological classes of words, can give valu-
able information about individual words and 
their neighbors. Identifying whether a word is 
a noun or a verb can let us predict which words 
are likely to be mapped in word alignment and 
which words are likely to occur in its vicinity 
in target sentence generation. 
Many studies incorporate POS information 
in SMT. Some researchers perform POS tag-
ging on their bilingual training data (Lee et al, 
2006; Sanchis and S?nchez, 2008). Some of 
them replace individual words as new words, 
such as in ?word/POS? form, producing new, 
extended vocabulary. The advantage of this 
approach is that POS information can help to 
resolve lexical ambiguity and thus improve 
translation quality. 
On the other hand, Koehn et al (2007) pro-
pose a factored translation model that can in-
corporate any linguistic factors including POS 
information in phrase-based SMT. The model 
provides a generalized representation of a 
translation model, because it can map multiple 
source and target factors. 
Although all of these approaches are shown 
to improve SMT performance by utilizing POS 
information, we observe that the influence is 
virtually marginal in two ways: 
623
 
 
 
 
1) The POS information tagged to each word 
may help to disambiguate in selecting 
word correspondences, but the increased 
vocabulary can also make the training data 
more sparse. 
2) The factored translation model may help to 
effectively handle out-of-vocabulary 
(OOV) by incorporating many linguistic 
factors, but it still crucially relies on the in-
itial quality of word alignment that will 
dominate the translation probabilities. 
This paper focuses on devising a better 
method for incorporating POS information in 
word alignment. It attempts to answer the fol-
lowing questions: 
1) Can the information regarding POS align-
ment tendency affect the post-processing 
of word alignment? 
2) Can the result of word alignment affected 
by such information help improving the 
quality of SMT? 
2 POS Alignment Tendency 
Despite the language pairs, words with similar 
POSs often correspond to each other in statisti-
cal word alignment. Similarly, words with dif-
ferent POSs are seldom aligned. For example, 
Korean proper nouns very often align with 
English proper nouns very often but seldom 
align with English adverbs. We believe that 
this phenomenon occurs not only on English-
Korean pairs but also on most of other lan-
guage pairs.  
Thus, in this study we hypothesize that all 
source language (SL) POSs have some rela-
tionship with target language (TL) POSs. Fig-
ure 1 exemplifies some results of using the 
IBM Models in English-Korean word align-
ment. As can be seen in the figure, the English 
word ?visiting? is incorrectly and excessively 
aligned to four Korean morphemes ?maejang?, 
?chat?, ?yeoseong?, and ?gogaek?. One reason 
for this is the sparseness of the training data; 
the only correct Korean morpheme ?chat? does 
not sufficiently co-occur with ?visiting? in the 
training data. However, it is generally believed 
that an English verb is more likely aligned to a 
Korean verb rather than a Korean noun. Like-
wise, we suppose that among many POSs, 
there are strong relationships between similar 
POSs and relatively weak relationships be-
tween different POSs. We hypothesize that the 
discovery of such relationships in advance can 
lead to better word alignment results. 
 In this paper, we propose a new method to 
obtain the relationship from word alignment 
results. The relationships among POSs, hence-
forth the POS alignment tendency, can be 
identified by the probability of the given POS 
pairs? alignment result where the source lan-
guage POS and the target language POS co-
occur in bilingual sentences. We formulate this 
idea using the maximum likelihood estimation 
as follows: 
     (          |   ( )    ( ))   
 
    
     (              ( )    ( ))
?      (           ( )    ( ))  *          +
 
 
where f and e denote source word and target 
word respectively. count() is a function that 
returns the number of co-occurrence of f and e 
when they are aligned (or not aligned). Then, 
we adjust the formula with the existing align-
ment score between f and e. 
     (   )        (   )   
             (   ) (          |   ( )    ( )) 
 
where )|( efPIBM  indicates the alignment prob-
ability estimated by the IBM models.   is a 
weighting parameter to interpolate the reliabili-
ties of both alignment factors. In the expe-
 
Figure 1. An example of inaccurate word alignment 
624
 
 
 
 
riment,   is empirically set to improve the 
word alignment performance ( =0.5).  
3 Modifying Alignment 
Based on the new scoring scheme as intro-
duced in the previous section, we modify the 
result of the initial word alignment. The modi-
fication is performed in the following proce-
dure: 
1. For each source word f that has out-bound 
alignment link other than null, 
2. Find the target word e that has the maxi-
mum alignment score according to the 
proposed alignment adjustment measure, 
and change the alignment result by map-
ping f to e. 
This modification guarantees that the number 
of alignment does not change; the algorithm is 
designed to minimize the risk by maintaining 
the fertility of a word estimated by the IBM 
Model. Figure 2 illustrates the result before 
and after the alignment modification. Incor-
rectly links from e1 and e3 are deleted and 
missing links from e2 and e4 are generated dur-
ing this alignment modification. 
The alignment modification through the re-
flection of POS alignment tendency is per-
formed on both e-to-f and f-to-e bidirectional 
word alignments. The bidirectional word 
alignment results are then symmetrized. 
4 Experiments 
In this paper, we attempt to reflect the POS 
alignment tendency in improving the word 
alignment performance. This section provides 
the experimental setup and the results that 
demonstrate whether the proposed approach 
can improve the statistical word alignment per-
formance. 
We collected bilingual texts from major bi-
lingual news broadcasting sites. 500K sentence 
pairs are collected and refined manually to 
construct correct parallel sentences pairs. The 
same number of monolingual sentences is also 
used from the same sites to train Korean lan-
guage. We also prepared a subset of the bilin-
gual text with the size of 50K to show that the 
proposed model is very effective when the 
training set is small. 
In order to evaluate the performance of 
word alignment, we additionally constructed a 
reference set with 400 sentence pairs. The 
evaluation is performed using precision, recall, 
and F-score. We use the GIZA++ toolkit for 
word alignment as well as four heuristic sym-
metrizations: intersection, union, grow-diag-
final, and grow-diag (Och, 2000).  
4.1 Word Alignment 
We now evaluate the effectiveness of the pro-
posed word alignment method. Table 1 and 2 
report the experimental results by adding POS 
information to the parallel corpus. ?Lexical? 
denotes the result of conventional word align-
ment produced by GIZA++. No pre-processing 
or post-processing is applied in this result. 
?Lemma/POS? is the result of word alignment 
with the pre-processing introduced Lee et al 
(2006). Compared to the result, lemmatized 
lexical and POS tags are proven to be useful 
information for word alignment. ?Lemma/POS? 
consistently outperforms ?Lexical? despite the 
symmetrization heuristics in terms of precision, 
recall and F-score. We expect this improve-
ment is benefited from the alleviated data 
sparseness by using lemmatized lexical and 
POS tags rather than using the lexical itself. 
 
 
Figure 2. An example of word alignment modification 
625
 
 
 
 
  
 Alignment heuristic Precision Recall F-score 
Lexical 
Intersection 94.0% 50.8% 66.0% 
Union 53.2% 81.2% 64.3% 
Grow-diag-final 54.6% 80.9% 65.2% 
Grow-diag 60.9% 67.2% 63.9% 
Lemma/POS 
Intersection 95.8% 55.3% 70.1% 
Union 58.1% 83.3% 68.4% 
Grow-diag-final 59.7% 83.0% 69.5% 
Grow-diag 67.0% 71.6% 69.2% 
Lemma/POS 
+ POS alignment 
tendency 
Intersection 96.1% 63.5% 76.5% 
Union 67.4% 85.1% 75.2% 
Grow-diag-final 69.8% 84.9% 76.6% 
Grow-diag 80.0% 77.0% 78.5% 
Table 1. The performance of word alignment using small training set (50k pairs) 
 
Experimental Setup Alignment heuristic Precision Recall F-score 
Lexical 
Intersection 96.8% 64.9% 77.7% 
Union 66.6% 87.4% 75.6% 
Grow-diag-final 67.8% 87.1% 76.2% 
Grow-diag 74.4% 79.2% 76.7% 
Lemma/POS 
Intersection 97.3% 66.2% 78.8% 
Union 70.7% 89.0% 78.8% 
Grow-diag-final 72.1% 88.8% 79.6% 
Grow-diag 78.8% 80.5% 79.7% 
Lemma/POS 
+ POS alignment 
tendency 
Intersection 97.2% 69.3% 80.9% 
Union 73.9% 86.7% 79.8% 
Grow-diag-final 75.6% 86.4% 80.7% 
Grow-diag 85.2% 81.5% 83.4% 
Table 2. The performance of word alignment using a large training set (500k pairs) 
 
Experimental Setup Symmetrization Heuristic BLEU(50k) BLEU (500k) 
Lexical 
Intersection 20.1% 29.2% 
Union 18.6% 27.2% 
Grow-diag-final 19.9% 27.7% 
Grow-diag 20.2% 29.4% 
Lemma/POS 
Intersection 20.3% 26.4% 
Union 18.5% 27.8% 
Grow-diag-final 20.1% 29.2% 
Grow-diag 20.4% 30.8% 
Factored Model 
(Lemma, POS) 
Intersection 20.5% 30.0% 
Union 18.1% 27.5% 
Grow-diag-final 20.3% 28.2% 
Grow-diag 20.9% 31.1% 
Lemma/POS 
+ POS alignment 
tendency 
Intersection 21.8% 29.3% 
Union 19.5% 27.2% 
Grow-diag-final 21.3% 28.4% 
Grow-diag 20.8% 29.1% 
Table  3. The performance of translation 
626
 
 
 
 
 
Since lemmatized lexical and POS tags are 
shown to be useful, our post-processing meth-
od is applied to ?Lemma/POS?. 
The experimental results show that the pro-
posed method consistently improves word 
alignment in terms of F-score. It is interesting 
that the proposed method improves the recall 
of the intersection result and the precision of 
the union result. Thus, the proposed method 
achieves the best alignment performance. 
As can be seen in Table 1 and 2, our method 
consistently improves the performance of word 
alignment despite the size of training data. In a 
small data set, the improvement of our method 
is much higher than that in a large set. This 
implies that our method is more helpful when 
the training data set is insufficient.  
We investigate whether the proposed meth-
od actually alleviates the data sparseness prob-
lem by analyzing the aligned word pairs of low 
co-occurrence frequency. There are multiple 
word pairs that share the same number of co-
occurrence in the corpus. For example, let us 
assume that ?report-bogoha?, ?newspaper-
sinmun? and ?China-jungguk? pairs are co-
occurred 1,000 times. We can calculate the 
mean of their individual recalls. We refer to 
this new measurement as average recall. The 
average recalls of these pairs are relatively 
higher than those of pairs with low co-
occurrence frequency such as ?food-jinji? and 
?center-chojeom? pairs. These pairs are diffi-
cult to be linked, because the word alignment 
model suffers from data sparseness when esti-
mating their translation probability.  
Figure 3 shows the average recall according 
to the number of co-occurrence. We can ob-
serve that the word alignment model tends to 
link word pairs more correctly if they are more 
frequently co-occurred. Both ?Lemma/POS? 
and our method consistently show higher aver-
age recall throughout all frequencies, and the 
proposed method shows the best performance. 
It is also notable that the both ?Lemma/POS? 
and our method achieve much more improve-
ment for low co-occurrence frequencies (e.g., 
11~40). This implies that the proposed method 
incorporates POS information more effectively 
than the previous method, since the proposed 
method achieves much higher average recall. 
4.2 Statistical Machine Translation 
Next, we examine the effect of the improve-
ment of the word alignment on the translation 
quality. For this, we built some SMT systems 
with the word alignment results. We use the 
Moses toolkit for translation (Koehn et al, 
2007). Moses is an implementation of phrase-
based statistical machine translation model that 
has shown a state-of-the-art performance in 
various evaluation sets. We also perform the 
evaluation of the Factored model (Koehn et al, 
2007) using Moses.  
To investigate how the improved word 
alignment affect the quality of machine trans-
lation, we calculate the BLEU score for trans-
lation results with different word alignment 
settings as shown in Table 3. First of all, we 
can easily conclude that the quality of the 
translation is strongly dominated by the size of 
the training data. We can also find that the 
quality of the translation is correlated to the 
performance of the word alignment. 
For a small test set, the proposed method 
achieved the best performance in terms of 
BLEU (21.8%). For a larger test set, however, 
the proposed method could not improve the 
performance of the translation with better word 
alignment. It is not feasible to investigate the 
factors that affect this deterioration, since Mo-
ses is a black box module to our system. The 
training of the phrase-based SMT model in-
volves the extraction of phrases, and the result 
of word alignment is reflected within this pro-
cess. When the training data is small, the num-
ber of extracted phrases is also apparently 
small. However, abundant phrases are extract-
ed from a large amount of training data. In this 
case, we hypothesize that the most plausible 
 
Figure 3. Average recall of word alignment pairs 
according to the number of their co-occurrence 
627
 
 
 
 
phrases are already obtained, and the effect of 
more accurate word alignment seems insignifi-
cant. More thorough analysis of this is re-
mained as future work. 
4.3 Acquisition of Bilingual Dictionary 
One of the most applications of word align-
ment is the construction of bilingual dictionar-
ies. By using word alignment, we can collect a 
(ranked) list of bilingual word pairs. Table 4 
reports the top 10 translations (the most ac-
ceptable target words to align) for Korean 
word ?bap? (food). The table contains the 
probabilities estimated by the IBM Models, the 
adjusted scores, and the number of co-
occurrence, respectively. Italicized translations 
are in fact incorrect translations. Highlighted 
ones are new translation candidates that are 
correct. As can be seen in the table, the pro-
posed approach shows a positive effect of rais-
ing new and better candidates for translation. 
For example, ?bread? and ?breakfast? have 
come up to the top 10 translations. This 
demonstrates that the low co-occurrences of 
?bap? with ?bread? and ?breakfast? are not 
suitably handled by alignments solely based on 
lexicals. However, the proposed approach 
ranks them at higher positions by reflecting the 
alignment tendency of POSs. 
5 Conclusion 
In this paper, we propose a new method for 
incorporating the POS alignment tendency to 
improve traditional word alignment model in 
post processing step. Experimental results 
show that the proposed method helps to allevi-
ate the data sparseness problem especially 
when the training data is insufficient. 
It is still difficult to conclude that better 
word alignment always leads to better transla-
tion. We plan on investigating the effective-
ness of the proposed method using other trans-
lation system, such as Hiero (Chiang et al, 
2005). We also plan to incorporate our method 
into other effective models, such as Factored 
translation model. 
References 
David Chiang et al, 2005. The Hiero machine 
translation system: Extensions, evaluation, and 
analysis. In Proc. of HLT-EMLP:779?786, Oct. 
 
Franz Josef Och. 2000. Giza++: Training of statis-
tical translation models. Available at http://www-
i6.informatik.rwthaachen.de/?och/software/GIZA+
+.html. 
 
Franz Josef Och & Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment 
Models. Computational Linguistics 29 (1):19-51. 
 
G. Sanchis and J.A. S?nchez. Vocabulary exten- 
sion via POS information for SMT. In Mixing 
Approaches to Machine Translation, 2008. 
 
Jonghoon Lee, Donghyeon Lee and Gary Geunbae 
Lee. Improving Phrase-based Korean-English Sta-
tistical Machine Translation. INTERSPEECH 2006. 
 
Kuzman Ganchev, Joao V. Graca and Ben Taskar. 
2008. Better Alignments = Better Translations? 
Proceedings of ACL-08: HLT: 986?993. 
 
Peter F. Brown et al,1993. The Mathematics of 
Statistical Machine Translation: Parameter Esti-
mation. Computational Linguistics 9(2): 263-311 
 
Rank 
IBM Model POS Alignment Tendency 
translation     (   ) #co-occur translation      (   ) #co-occur 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
bob/NNP 
rice/NN 
eat/VB 
meal/NN 
food/NN 
bob/NN 
feed/VB 
cook/VB 
living/NN 
dinner/NN 
0.348 
0.192 
0.107 
0.075 
0.043 
0.038 
0.010 
0.010 
0.008 
0.008 
83 
73 
57 
43 
29 
10 
7 
9 
4 
10 
bob/NNP 
rice/NN 
meal/NN 
food/NN 
eat/VB 
bob/NN 
living/NN 
dinner/NN 
bread/NN 
breakfast/NN 
0.214 
0.136 
0.078 
0.062 
0.061 
0.059 
0.045 
0.044 
0.044 
0.043 
83 
73 
43 
29 
57 
10 
4 
10 
9 
6 
Table 4. Top 10 translations for Korean word ?bap? (food). 
628
 
 
 
 
Philipp Koehn and Hieu Hoang. Factored Transla-
tion Models. EMNLP 2007. 
 
Phillipp Koehn et al, 2007. Moses: Open source 
toolkit for statistical machine translation.In Pro-
ceedings of the Annual Meeting of the Association 
for Computational Linguistics, demonstation ses-
sion. 
629
Proceedings of the ACL 2010 Conference Short Papers, pages 142?146,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Better Filtration and Augmentation for Hierarchical Phrase-Based
Translation Rules
Zhiyang Wang ? Yajuan Lu? ? Qun Liu ? Young-Sook Hwang ?
?Key Lab. of Intelligent Information Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
P.O. Box 2704, Beijing 100190, China 11, Euljiro2-ga, Jung-gu, Seoul 100-999, Korea
wangzhiyang@ict.ac.cn yshwang@sktelecom.com
Abstract
This paper presents a novel filtration cri-
terion to restrict the rule extraction for
the hierarchical phrase-based translation
model, where a bilingual but relaxed well-
formed dependency restriction is used to
filter out bad rules. Furthermore, a new
feature which describes the regularity that
the source/target dependency edge trig-
gers the target/source word is also pro-
posed. Experimental results show that, the
new criteria weeds out about 40% rules
while with translation performance im-
provement, and the new feature brings an-
other improvement to the baseline system,
especially on larger corpus.
1 Introduction
Hierarchical phrase-based (HPB) model (Chiang,
2005) is the state-of-the-art statistical machine
translation (SMT) model. By looking for phrases
that contain other phrases and replacing the sub-
phrases with nonterminal symbols, it gets hierar-
chical rules. Hierarchical rules are more powerful
than conventional phrases since they have better
generalization capability and could capture long
distance reordering. However, when the train-
ing corpus becomes larger, the number of rules
will grow exponentially, which inevitably results
in slow and memory-consuming decoding.
In this paper, we address the problem of reduc-
ing the hierarchical translation rule table resorting
to the dependency information of bilingual lan-
guages. We only keep rules that both sides are
relaxed-well-formed (RWF) dependency structure
(see the definition in Section 3), and discard others
which do not satisfy this constraint. In this way,
about 40% bad rules are weeded out from the orig-
inal rule table. However, the performance is even
better than the traditional HPB translation system.
Source
Target 
f f? 
e
Figure 1: Solid wire reveals the dependency rela-
tion pointing from the child to the parent. Target
word e is triggered by the source word f and it?s
head word f ?, p(e|f ? f ?).
Based on the relaxed-well-formed dependency
structure, we also introduce a new linguistic fea-
ture to enhance translation performance. In the
traditional phrase-based SMT model, there are
always lexical translation probabilities based on
IBM model 1 (Brown et al, 1993), i.e. p(e|f),
namely, the target word e is triggered by the source
word f . Intuitively, however, the generation of e
is not only involved with f , sometimes may also
be triggered by other context words in the source
side. Here we assume that the dependency edge
(f ? f ?) of word f generates target word e (we
call it head word trigger in Section 4). Therefore,
two words in one language trigger one word in
another, which provides a more sophisticated and
better choice for the target word, i.e. Figure 1.
Similarly, the dependency feature works well in
Chinese-to-English translation task, especially on
large corpus.
2 Related Work
In the past, a significant number of techniques
have been presented to reduce the hierarchical rule
table. He et al (2009) just used the key phrases
of source side to filter the rule table without taking
advantage of any linguistic information. Iglesias
et al (2009) put rules into syntactic classes based
on the number of non-terminals and patterns, and
applied various filtration strategies to improve the
rule table quality. Shen et al (2008) discarded
142
found
The
girl 
lovely 
house
a beautiful
Figure 2: An example of dependency tree. The
corresponding plain sentence is The lovely girl
found a beautiful house.
most entries of the rule table by using the con-
straint that rules of the target-side are well-formed
(WF) dependency structure, but this filtering led to
degradation in translation performance. They ob-
tained improvements by adding an additional de-
pendency language model. The basic difference
of our method from (Shen et al, 2008) is that we
keep rules that both sides should be relaxed-well-
formed dependency structure, not just the target
side. Besides, our system complexity is not in-
creased because no additional language model is
introduced.
The feature of head word trigger which we ap-
ply to the log-linear model is motivated by the
trigger-based approach (Hasan and Ney, 2009).
Hasan and Ney (2009) introduced a second word
to trigger the target word without considering any
linguistic information. Furthermore, since the sec-
ond word can come from any part of the sentence,
there may be a prohibitively large number of pa-
rameters involved. Besides, He et al (2008) built
a maximum entropy model which combines rich
context information for selecting translation rules
during decoding. However, as the size of the cor-
pus increases, the maximum entropy model will
become larger. Similarly, In (Shen et al, 2009),
context language model is proposed for better rule
selection. Taking the dependency edge as condi-
tion, our approach is very different from previous
approaches of exploring context information.
3 Relaxed-well-formed Dependency
Structure
Dependency models have recently gained consid-
erable interest in SMT (Ding and Palmer, 2005;
Quirk et al, 2005; Shen et al, 2008). Depen-
dency tree can represent richer structural infor-
mation. It reveals long-distance relation between
words and directly models the semantic structure
of a sentence without any constituent labels. Fig-
ure 2 shows an example of a dependency tree. In
this example, the word found is the root of the tree.
Shen et al (2008) propose the well-formed de-
pendency structure to filter the hierarchical rule ta-
ble. A well-formed dependency structure could be
either a single-rooted dependency tree or a set of
sibling trees. Although most rules are discarded
with the constraint that the target side should be
well-formed, this filtration leads to degradation in
translation performance.
As an extension of the work of (Shen et
al., 2008), we introduce the so-called relaxed-
well-formed dependency structure to filter the hi-
erarchical rule table. Given a sentence S =
w1w2...wn. Let d1d2...dn represent the position of
parent word for each word. For example, d3 = 4
means that w3 depends on w4. If wi is a root, we
define di = ?1.
Definition A dependency structure wi...wj is
a relaxed-well-formed structure, where there is
h /? [i, j], all the words wi...wj are directly or
indirectly depended on wh or -1 (here we define
h = ?1). If and only if it satisfies the following
conditions
? dh /? [i, j]
? ?k ? [i, j], dk ? [i, j] or dk = h
From the definition above, we can see that
the relaxed-well-formed structure obviously cov-
ers the well-formed one. In this structure, we
don?t constrain that all the children of the sub-root
should be complete. Let?s review the dependency
tree in Figure 2 as an example. Except for the well-
formed structure, we could also extract girl found
a beautiful house. Therefore, if the modifier The
lovely changes to The cute, this rule also works.
4 Head Word Trigger
(Koehn et al, 2003) introduced the concept of
lexical weighting to check how well words of
the phrase translate to each other. Source word
f aligns with target word e, according to the
IBM model 1, the lexical translation probability
is p(e|f). However, in the sense of dependency
relationship, we believe that the generation of the
target word e, is not only triggered by the aligned
source word f , but also associated with f ?s head
word f ?. Therefore, the lexical translation prob-
ability becomes p(e|f ? f ?), which of course
allows for a more fine-grained lexical choice of
143
the target word. More specifically, the probabil-
ity could be estimated by the maximum likelihood
(MLE) approach,
p(e|f ? f ?) = count(e, f ? f
?)
?
e? count(e?, f ? f ?)
(1)
Given a phrase pair f , e and word alignment
a, and the dependent relation of the source sen-
tence dJ1 (J is the length of the source sentence,
I is the length of the target sentence). Therefore,
given the lexical translation probability distribu-
tion p(e|f ? f ?), we compute the feature score of
a phrase pair (f , e) as
p(e|f, dJ1 , a)
= ?|e|i=1
1
|{j|(j, i) ? a}|
?
?(j,i)?a
p(ei|fj ? fdj) (2)
Now we get p(e|f, dJ1 , a), we could obtain
p(f |e, dI1, a) (dI1 represents dependent relation of
the target side) in the similar way. This new fea-
ture can be easily integrated into the log-linear
model as lexical weighting does.
5 Experiments
In this section, we describe the experimental set-
ting used in this work, and verify the effect of
the relaxed-well-formed structure filtering and the
new feature, head word trigger.
5.1 Experimental Setup
Experiments are carried out on the NIST1
Chinese-English translation task with two differ-
ent size of training corpora.
? FBIS: We use the FBIS corpus as the first
training corpus, which contains 239K sen-
tence pairs with 6.9M Chinese words and
8.9M English words.
? GQ: This is manually selected from the
LDC2 corpora. GQ contains 1.5M sentence
pairs with 41M Chinese words and 48M En-
glish words. In fact, FBIS is the subset of
GQ.
1www.nist.gov/speech/tests/mt
2It consists of six LDC corpora:
LDC2002E18, LDC2003E07, LDC2003E14, Hansards part
of LDC2004T07, LDC2004T08, LDC2005T06.
For language model, we use the SRI Language
Modeling Toolkit (Stolcke, 2002) to train a 4-
gram model on the first 1/3 of the Xinhua portion
of GIGAWORD corpus. And we use the NIST
2002 MT evaluation test set as our development
set, and NIST 2004, 2005 test sets as our blind
test sets. We evaluate the translation quality us-
ing case-insensitive BLEU metric (Papineni et
al., 2002) without dropping OOV words, and the
feature weights are tuned by minimum error rate
training (Och, 2003).
In order to get the dependency relation of the
training corpus, we re-implement a beam-search
style monolingual dependency parser according
to (Nivre and Scholz, 2004). Then we use the
same method suggested in (Chiang, 2005) to
extract SCFG grammar rules within dependency
constraint on both sides except that unaligned
words are allowed at the edge of phrases. Pa-
rameters of head word trigger are estimated as de-
scribed in Section 4. As a default, the maximum
initial phrase length is set to 10 and the maximum
rule length of the source side is set to 5. Besides,
we also re-implement the decoder of Hiero (Chi-
ang, 2007) as our baseline. In fact, we just exploit
the dependency structure during the rule extrac-
tion phase. Therefore, we don?t need to change
the main decoding algorithm of the SMT system.
5.2 Results on FBIS Corpus
A series of experiments was done on the FBIS cor-
pus. We first parse the bilingual languages with
monolingual dependency parser respectively, and
then only retain the rules that both sides are in line
with the constraint of dependency structure. In
Table 1, the relaxed-well-formed structure filtered
out 35% of the rule table and the well-formed dis-
carded 74%. RWF extracts additional 39% com-
pared to WF, which can be seen as some kind
of evidence that the rules we additional get seem
common in the sense of linguistics. Compared to
(Shen et al, 2008), we just use the dependency
structure to constrain rules, not to maintain the tree
structures to guide decoding.
Table 2 shows the translation result on FBIS.
We can see that the RWF structure constraint can
improve translation quality substantially both at
development set and different test sets. On the
Test04 task, it gains +0.86% BLEU, and +0.84%
on Test05. Besides, we also used Shen et al
(2008)?s WF structure to filter both sides. Al-
though it discard about 74% of the rule table, the
144
System Rule table size
HPB 30,152,090
RWF 19,610,255
WF 7,742,031
Table 1: Rule table size with different con-
straint on FBIS. Here HPB refers to the base-
line hierarchal phrase-based system, RWF means
relaxed-well-formed constraint and WF represents
the well-formed structure.
System Dev02 Test04 Test05
HPB 0.3285 0.3284 0.2965
WF 0.3125 0.3218 0.2887
RWF 0.3326 0.3370** 0.3050
RWF+Tri 0.3281 / 0.2965
Table 2: Results of FBIS corpus. Here Tri means
the feature of head word trigger on both sides. And
we don?t test the new feature on Test04 because of
the bad performance on development set. * or **
= significantly better than baseline (p < 0.05 or
0.01, respectively).
over-all BLEU is decreased by 0.66%-0.78% on
the test sets.
As for the feature of head word trigger, it seems
not work on the FBIS corpus. On Test05, it gets
the same score with the baseline, but lower than
RWF filtering. This may be caused by the data
sparseness problem, which results in inaccurate
parameter estimation of the new feature.
5.3 Result on GQ Corpus
In this part, we increased the size of the training
corpus to check whether the feature of head word
trigger works on large corpus.
We get 152M rule entries from the GQ corpus
according to (Chiang, 2007)?s extraction method.
If we use the RWF structure to constrain both
sides, the number of rules is 87M, about 43% of
rule entries are discarded. From Table 3, the new
System Dev02 Test04 Test05
HPB 0.3473 0.3386 0.3206
RWF 0.3539 0.3485** 0.3228
RWF+Tri 0.3540 0.3607** 0.3339*
Table 3: Results of GQ corpus. * or ** = sig-
nificantly better than baseline (p < 0.05 or 0.01,
respectively).
feature works well on two different test sets. The
gain is +2.21% BLEU on Test04, and +1.33% on
Test05. Compared to the result of the baseline,
only using the RWF structure to filter performs the
same as the baseline on Test05, and +0.99% gains
on Test04.
6 Conclusions
This paper proposes a simple strategy to filter the
hierarchal rule table, and introduces a new feature
to enhance the translation performance. We em-
ploy the relaxed-well-formed dependency struc-
ture to constrain both sides of the rule, and about
40% of rules are discarded with improvement of
the translation performance. In order to make full
use of the dependency information, we assume
that the target word e is triggered by dependency
edge of the corresponding source word f . And
this feature works well on large parallel training
corpus.
How to estimate the probability of head word
trigger is very important. Here we only get the pa-
rameters in a generative way. In the future, we we
are plan to exploit some discriminative approach
to train parameters of this feature, such as EM al-
gorithm (Hasan et al, 2008) or maximum entropy
(He et al, 2008).
Besides, the quality of the parser is another ef-
fect for this method. As the next step, we will
try to exploit bilingual knowledge to improve the
monolingual parser, i.e. (Huang et al, 2009).
Acknowledgments
This work was partly supported by National
Natural Science Foundation of China Contract
60873167. It was also funded by SK Telecom,
Korea under the contract 4360002953. We show
our special thanks to Wenbin Jiang and Shu Cai
for their valuable suggestions. We also thank
the anonymous reviewers for their insightful com-
ments.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263?
311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL
145
?05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 263?
270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In ACL ?05: Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 541?548.
Sas?a Hasan and Hermann Ney. 2009. Comparison of
extended lexicon models in search and rescoring for
smt. In NAACL ?09: Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 17?20.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, and
Jesu?s Andre?s-Ferrer. 2008. Triplet lexicon models
for statistical machine translation. In EMNLP ?08:
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 372?
381.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In COLING ?08: Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 321?328.
Zhongjun He, Yao Meng, Yajuan Lu?, Hao Yu, and Qun
Liu. 2009. Reducing smt rule table with monolin-
gual key phrase. In ACL-IJCNLP ?09: Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 121?124.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In EMNLP ?09: Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1222?1231.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In EACL ?09:
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 380?388.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54.
Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In COLING
?04: Proceedings of the 20th international confer-
ence on Computational Linguistics, pages 64?70.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL ?02: Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal smt. In ACL ?05: Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 271?279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of lin-
guistic and contextual information for statistical ma-
chine translation. In EMNLP ?09: Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 72?80.
Andreas Stolcke. 2002. Srilman extensible language
modeling toolkit. In In Proceedings of the 7th Inter-
national Conference on Spoken Language Process-
ing (ICSLP 2002), pages 901?904.
146
