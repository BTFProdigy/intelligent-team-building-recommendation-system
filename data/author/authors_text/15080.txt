Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 110?114,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Easy-First POS Tagging and Dependency Parsing with Beam Search 
Ji Ma?   JingboZhu?  Tong Xiao?   Nan Yang? 
?Natrual Language Processing Lab., Northeastern University, Shenyang, China 
?MOE-MS Key Lab of MCC, University of Science and Technology of China, 
Hefei, China 
majineu@outlook.com 
{zhujingbo, xiaotong}@mail.neu.edu.cn 
nyang.ustc@gmail.com 
 
Abstract 
In this paper, we combine easy-first de-
pendency parsing and POS tagging algo-
rithms with beam search and structured 
perceptron. We propose a simple variant 
of ?early-update? to ensure valid update 
in the training process. The proposed so-
lution can also be applied to combine 
beam search and structured perceptron 
with other systems that exhibit spurious 
ambiguity. On CTB, we achieve 94.01% 
tagging accuracy and 86.33% unlabeled 
attachment score with a relatively small 
beam width. On PTB, we also achieve 
state-of-the-art performance. 
1 Introduction 
The easy-first dependency parsing algorithm 
(Goldberg and Elhadad, 2010) is attractive due to 
its good accuracy, fast speed and simplicity. The 
easy-first parser has been applied to many appli-
cations (Seeker et al, 2012; S?ggard and Wulff, 
2012). By processing the input tokens in an easy-
to-hard order, the algorithm could make use of 
structured information on both sides of the hard 
token thus making more indicative predictions. 
However, rich structured information also causes 
exhaustive inference intractable. As an alterna-
tive, greedy search which only explores a tiny 
fraction of the search space is adopted (Goldberg 
and Elhadad, 2010). 
 To enlarge the search space, a natural exten-
sion to greedy search is beam search. Recent 
work also shows that beam search together with 
perceptron-based global learning (Collins, 2002) 
enable the use of non-local features that are help-
ful to improve parsing performance without 
overfitting (Zhang and Nivre, 2012). Due to the-
se advantages, beam search and global learning 
has been applied to many NLP tasks (Collins and 
Roark 2004; Zhang and Clark, 2007). However, 
to the best of our knowledge, no work in the lit-
erature has ever applied the two techniques to 
easy-first dependency parsing.  
While applying beam-search is relatively 
straightforward, the main difficulty comes from 
combining easy-first dependency parsing with 
perceptron-based global learning. In particular, 
one needs to guarantee that each parameter up-
date is valid, i.e., the correct action sequence has 
lower model score than the predicted one1. The 
difficulty in ensuring validity of parameter up-
date for the easy-first algorithm is caused by its 
spurious ambiguity, i.e., the same result might be 
derived by more than one action sequences.  
For algorithms which do not exhibit spurious 
ambiguity, ?early update? (Collins and Roark 
2004) is always valid: at the k-th step when the 
single correct action sequence falls off the beam, 
                                                 
1 As shown by (Huang et al, 2012), only valid update guar-
antees the convergence of any perceptron-based training. 
Invalid update may lead to bad learning or even make the 
learning not converge at all. 
Figure 1: Example of cases without/with spurious 
ambiguity. The 3 ? 1 table denotes a beam. ?C/P? 
denotes correct/predicted action sequence. The 
numbers following C/P are model scores. 
 
110
its model score must be lower than those still in 
the beam (as illustrated in figure 1, also see the 
proof in (Huang et al, 2012)). While for easy-
first dependency parsing, there could be multiple 
action sequences that yield the gold result (C1 and 
C2 in figure 1). When all correct sequences fall 
off the beam, some may indeed have higher 
model score than those still in the beam (C2 in 
figure 1), causing invalid update. 
For the purpose of valid update, we present a 
simple solution which is based on early update. 
The basic idea is to use one of the correct action 
sequences that were pruned right at the k-th step 
(C1 in figure 1) for parameter update.  
The proposed solution is general and can also 
be applied to other algorithms that exhibit spuri-
ous ambiguity, such as easy-first POS tagging 
(Ma et al, 2012) and transition-based dependen-
cy parsing with dynamic oracle (Goldberg and 
Nivre, 2012). In this paper, we report experi-
mental results on both easy-first dependency 
parsing and POS tagging (Ma et al, 2012). We 
show that both easy-first POS tagging and de-
pendency parsing can be improved significantly 
from beam search and global learning. Specifi-
cally, on CTB we achieve 94.01% tagging accu-
racy which is the best result to date2 for a single 
tagging model. With a relatively small beam, we 
achieve 86.33% unlabeled score (assume gold 
tags), better than state-of-the-art transition-based 
parsers (Huang and Sagae, 2010; Zhang and 
Nivre, 2011). On PTB, we also achieve good 
results that are comparable to the state-of-the-art. 
2 Easy-first dependency parsing 
The easy-first dependency parsing algorithm 
(Goldberg and Elhadad, 2010) builds a depend-
ency tree by performing two types of actions 
LEFT(i) and RIGHT(i) to a list of sub-tree struc-
tures p1,?, pr. pi is initialized with the i-th word  
                                                 
2 Joint tagging-parsing models achieve higher accuracy, but 
those models are not directly comparable to ours.  
Algorithm 1: Easy-first with beam search 
Input:     sentence   of n words,  beam width s 
Output:  one best dependency tree 
     (     )        
         ( )   
    (  ) 
            // top s extensions from the beam 
1                     // initially, empty beam 
2 for    1   1 do 
3             (        ) 
4 return        ( )   // tree built by the best sequence  
 
of the input sentence. Action LEFT(i)/RIGHT(i) 
attaches pi to its left/right neighbor and then re-
moves pi from the sub-tree list. The algorithm 
proceeds until only one sub-tree left which is the 
dependency tree of the input sentence (see the 
example in figure 2). Each step, the algorithm 
chooses the highest score action to perform ac-
cording to the linear model: 
     ( )     ( ) 
Here,  is the weight vector and  is the feature 
representation. In particular,  (    ( ) 
     ( )) denotes features extracted from pi. 
The parsing algorithm is greedy which ex-
plores a tiny fraction of the search space. Once 
an incorrect action is selected, it can never yield 
the correct dependency tree. To enlarge the 
search space, we introduce the beam-search ex-
tension in the next section. 
3 Easy-first with beam search  
In this section, we introduce easy-first with beam 
search in our own notations that will be used 
throughout the rest of this paper.  
For a sentence x of n words, let   be the action 
(sub-)sequence that can be applied, in sequence, 
to x and the result sub-tree list is denoted by 
 ( )  For example, suppose x is ?I am valid? and 
y is [RIGHT(1)], then y(x) yields figure 2(b). Let 
   to be LEFT(i)/RIGHT(i) actions where    1   . 
Thus, the set of all possible one-action extension 
of   is: 
     ( )            ( )   
Here, ? ? means insert   to the end of  . Follow-
ing (Huang et al, 2012), in order to formalize 
beam search, we also use the          
    ( ) 
operation which returns the top s action sequenc-
es in   according to   ( ). Here,  denotes a 
set of action sequences,   ( ) denotes the sum of 
feature vectors of each action in    
Pseudo-code of easy-first with beam search is 
shown in algorithm 1. Beam search grows s 
(beam width) action sequences in parallel using a  
Figure 2: An example of parsing ?I am valid?. Spu-
rious ambiguity: (d) can be derived by both 
[RIGHT(1), LEFT(2)] and [LEFT(3), RIGHT(1)]. 
111
Algorithm 2: Perceptron-based training over one 
training sample (   ) 
Input:    (   ), s, parameter   
Output: new parameter    
    (       )        
     (      ( ))   
   (  ) 
 // top correct extension from the beam 
1         
2 for    1   1 do 
3     ?      (          ) 
4            (        ) 
5    if           // all correct seq. falls off the beam 
6             ( ?)   (     ) 
7         break 
8 if        ( )      // full update 
9         ( ?)   (       )  
10 return  
 
beam  , (sequences in   are sorted in terms of 
model score, i.e.,   (    )     (  1 ) ). 
At each step, the sequences in   are expanded in 
all possible ways and then   is filled up with the 
top s newly expanded sequences (line 2 ~ line 3). 
Finally, it returns the dependency tree built by 
the top action sequence in      . 
4 Training  
To learn the weight vector , we use the percep-
tron-based global learning3 (Collins, 2002) which 
updates  by rewarding the feature weights fired 
in the correct action sequence and punish those 
fired in the predicted incorrect action sequence. 
Current work (Huang et al, 2012) rigorously 
explained that only valid update ensures conver-
gence of any perceptron variants. They also justi-
fied that the popular ?early update? (Collins and  
Roark, 2004) is valid for the systems that do not 
exhibit spurious ambiguity4.  
However, for the easy-first algorithm or more 
generally, systems that exhibit spurious ambigui-
ty, even ?early update? could fail to ensure valid-
ity of update (see the example in figure 1). For 
validity of update, we propose a simple solution 
which is based on ?early update? and which can 
accommodate spurious ambiguity. The basic idea 
is to use the correct action sequence which was  
                                                 
3 Following (Zhang and Nivre, 2012), we say the training 
algorithm is global if it optimizes the score of an entire ac-
tion sequence. A local learner trains a classifier which dis-
tinguishes between single actions. 
4 As shown in (Goldberg and Nivre 2012), most transition-
based dependency parsers (Nivre et al, 2003; Huang and 
Sagae 2010;Zhang and Clark 2008) ignores spurious ambi-
guity by using a static oracle which maps a dependency tree 
to a single action sequence.  
Features of (Goldberg and Elhadad, 2010) 
for p in pi-1, pi, pi+1 wp-vlp, wp-vrp, tp-vlp,  
tp-vrp, tlcp, trcp, wlcp, wlcp 
for p in pi-2, pi-1, pi, pi+1, pi+2 tp-tlcp,  tp-trcp, tp-tlcp-trcp 
for p, q, r in (pi-2, pi-1, pi), (pi-
1, pi+1, pi), (pi+1, pi+2 ,pi) 
tp-tq-tr, tp-tq-wr 
for p, q in (pi-1, pi) tp-tlcp-tq,   tp-trcp-tq,   ,tp-tlcp-wq,, 
 tp-trcp-wq,   tp-wq-tlcq,  tp-wq-trcq 
 
Table 1: Feature templates for English dependency 
parsing. wp denotes the head word of p, tp denotes the 
POS tag of wp. vlp/vrp denotes the number p?s of 
left/right child. lcp/rcp denotes p?s leftmost/rightmost 
child. pi denotes partial tree being considered. 
 
pruned right at the step when all correct sequence 
falls off the beam (as C1 in figure 1).  
Algorithm 2 shows the pseudo-code of the 
training procedure over one training sample 
(   ), a sentence-tree pair. Here we assume   to 
be the set of all correct action sequences/sub-
sequences. At step k, the algorithm constructs a 
correct action sequence  ? of length k by extend-
ing those in      (line 3). It also checks whether 
   no longer contains any correct sequence. If so, 
 ? together with       are used for parameter up-
date (line 5 ~ line 6). It can be easily verified that 
each update in line 6 is valid. Note that both 
?TOPC? and the operation in line 5 use   to check 
whether an action sequence y is correct or not. 
This  can  be  efficiently  implemented   (without 
explicitly enumerating  ) by checking if each 
LEFT(i)/RIGHT(i) in y are compatible with (   ): 
pi already collected all its dependents according 
to t; pi is attached to the correct neighbor sug-
gested by t.  
5 Experiments 
For English, we use PTB as our data set. We use 
the standard split for dependency parsing and the 
split used by (Ratnaparkhi, 1996) for POS tag-
ging. Penn2Malt5 is used to convert the bracket-
ed structure into dependencies. For dependency 
parsing, POS tags of the training set are generat-
ed using 10-fold jack-knifing.  
For Chinese, we use CTB 5.1 and the split 
suggested by (Duan et al, 2007) for both tagging 
and dependency parsing. We also use Penn2Malt 
and the head-finding rules of (Zhang and Clark 
2008) to convert constituency trees into depend-
encies. For dependency parsing, we assume gold 
segmentation and POS tags for the input.  
                                                 
5 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html 
112
Features used in English dependency parsing 
are listed in table 1. Besides the features in 
(Goldberg and Elhadad, 2010), we also include 
some trigram features and valency features 
which are useful for transition-based dependency 
parsing (Zhang and Nivre, 2011). For English 
POS tagging, we use the same features as in 
(Shen et al, 2007). For Chinese POS tagging and 
dependency parsing, we use the same features as 
(Ma et al, 2012). All of our experiments are 
conducted on a Core i7 (2.93GHz) machine, both 
the tagger and parser are implemented using C++.  
5.1 Effect of beam width 
Tagging/parsing performances with different 
beam widths on the development set are listed in 
table 2 and table 3. We can see that Chinese POS  
tagging, dependency parsing as well as English 
dependency parsing greatly benefit from beam 
search. While tagging accuracy on English only 
slightly improved. This may because that the 
accuracy of the greedy baseline tagger is already 
very high and it is hard to get further improve-
ment. Table 2 and table 3 also show that the 
speed of both tagging and dependency parsing 
drops linearly with the growth of beam width. 
5.2 Final results 
Tagging results on the test set together with some 
previous results are listed in table 4. Dependency 
parsing results on CTB and PTB are listed in ta-
ble 5 and table 6, respectively. 
On CTB, tagging accuracy of our greedy base-
line is already comparable to the state-of-the-art. 
As the beam size grows to 5, tagging accuracy 
increases to 94.01% which is 2.3% error reduc-
tion. This is also the best tagging accuracy com-
paring with previous single tagging models (For 
limited space, we do not list the performance of 
joint tagging-parsing models).  
Parsing performances on both PTB and CTB 
are significantly improved with a relatively small 
beam width (s = 8). In particular, we achieve 
86.33% uas on CTB which is 1.54% uas im-
provement over the greedy baseline parser. 
Moreover, the performance is better than the best 
transition-based parser (Zhang and Nivre, 2011) 
which adopts a much larger beam width (s = 64).  
6 Conclusion and related work 
This work directly extends (Goldberg and El-
hadad, 2010) with beam search and global learn-
ing. We show that both the easy-first POS tagger 
and dependency parser can be significantly impr- 
s PTB CTB speed  
1 97.17 93.91 1350 
3 97.20 94.15 560 
5 97.22 94.17 385 
 
Table 2: Tagging accuracy vs beam width vs. Speed is 
evaluated using the number of sentences that can be 
processed in one second 
 
s 
PTB CTB 
speed 
uas compl uas compl 
1 91.77 45.29 84.54 33.75 221 
2 92.29 46.28 85.11 34.62 124 
4 92.50 46.82 85.62 37.11 71 
8 92.74 48.12 86.00 35.87 39 
 
Table 3: Parsing accuracy vs beam width. ?uas? and 
?compl? denote unlabeled score and complete match 
rate respectively (all excluding punctuations). 
 
PTB CTB 
(Collins, 2002) 97.11 (Hatori et al, 2012) 93.82 
(Shen et al, 2007) 97.33 (Li et al, 2012) 93.88 
(Huang et al, 2012) 97.35 (Ma et al, 2012) 93.84 
this work   1 97.22 this work   1 93.87 
this work     97.28 this work     94.01? 
 
Table 4: Tagging results on the test set. ??? denotes 
statistically significant over the greedy baseline by 
McNemar?s test (      ) 
 
Systems s uas compl 
(Huang and Sagae, 2010) 8 85.20 33.72 
(Zhang and Nivre, 2011) 64 86.00 36.90 
(Li et al, 2012) ? 86.55 ? 
this work 1 84.79 32.98 
this work 8 86.33
?
 36.13 
 
Table 5: Parsing results on CTB test set. 
  
Systems s uas compl 
(Huang and Sagae, 2010) 8 92.10 ? 
(Zhang and Nivre, 2011) 64 92.90 48.50 
(Koo and Collins, 2010) ? 93.04 ? 
this work 1 91.72 44.04 
this work 8 92.47
?
 46.07 
 
Table 6: Parsing results on PTB test set.  
 
oved using beam search and global learning. 
This work can also be considered as applying 
(Huang et al, 2012) to the systems that exhibit 
spurious ambiguity. One future direction might 
be to apply the training method to transition-
based parsers with dynamic oracle (Goldberg and 
Nivre, 2012) and potentially further advance per-
formances of state-of-the-art transition-based 
parsers. 
113
Shen et al, (2007) and (Shen and Joshi, 2008) 
also proposed bi-directional sequential classifica-
tion with beam search for POS tagging and 
LTAG dependency parsing, respectively. The 
main difference is that their training method aims 
to learn a classifier which distinguishes between 
each local action while our training method aims 
to distinguish between action sequences. Our 
method can also be applied to their framework. 
Acknowledgments 
We would like to thank Yue Zhang, Yoav Gold-
berg and Zhenghua Li for discussions and sug-
gestions on earlier drift of this paper. We would 
also like to thank the three anonymous reviewers 
for their suggestions. This work was supported in 
part by the National Science Foundation of Chi-
na (61073140; 61272376), Specialized Research 
Fund for the Doctoral Program of Higher Educa-
tion (20100042110031) and the Fundamental 
Research Funds for the Central Universities 
(N100204002). 
References  
Collins, M. 2002. Discriminative training methods for 
hidden markov models: Theory and experiments 
with perceptron algorithms. In Proceedings of 
EMNLP. 
Duan, X., Zhao, J., , and Xu, B. 2007. Probabilistic 
models for action-based Chinese dependency pars-
ing. In Proceedings of ECML/ECPPKDD. 
Goldberg, Y. and Elhadad, M. 2010 An Efficient Al-
gorithm for Eash-First Non-Directional Dependen-
cy Parsing. In Proceedings of NAACL 
Huang, L. and Sagae, K. 2010. Dynamic program-
ming for linear-time incremental parsing. In Pro-
ceedings of ACL. 
Huang, L. Fayong, S. and Guo, Y. 2012. Structured 
Perceptron with Inexact Search. In Proceedings of 
NAACL. 
Koo, T. and Collins, M. 2010. Efficient third-order 
dependency parsers. In Proceedings of ACL. 
Li, Z., Zhang, M., Che, W., Liu, T. and Chen, W. 
2012. A Separately Passive-Aggressive Training 
Algorithm for Joint POS Tagging and Dependency 
Parsing. In Proceedings of COLING 
Ma, J., Xiao, T., Zhu, J. and Ren, F. 2012. Easy-First 
Chinese POS Tagging and Dependency Parsing. In 
Proceedings of COLING 
Rataparkhi, A. (1996) A Maximum Entropy Part-Of-
Speech Tagger. In Proceedings of EMNLP 
Shen, L., Satt, G. and Joshi, A. K. (2007) Guided 
Learning for Bidirectional Sequence Classification. 
In Proceedings of ACL. 
Shen, L. and  Josh, A. K. 2008. LTAG Dependency 
Parsing with Bidirectional Incremental Construc-
tion. In Proceedings of  EMNLP. 
Seeker, W., Farkas, R. and Bohnet, B. 2012 Data-
driven Dependency Parsing With Empty Heads. In 
Proceedings of COLING 
S?ggard, A. and Wulff, J. 2012. An Empirical Study 
of Non-lexical Extensions to Delexicalized Trans-
fer. In Proceedings of COLING 
Yue Zhang and Stephen Clark. 2007 Chinese Seg-
mentation Using a Word-based Perceptron Algo-
rithm. In Proceedings of ACL.  
Zhang, Y. and Clark, S. 2008. Joint word segmenta-
tion and POS tagging using a single perceptron. In 
Proceedings of ACL. 
Zhang, Y. and Nivre, J. 2011. Transition-based de-
pendency parsing with rich non-local features. In 
Proceedings of ACL. 
Zhang, Y. and Nivre, J. 2012. Analyzing the Effect of 
Global Learning and Beam-Search for Transition-
Based Dependency Parsing. In Proceedings of 
COLING. 
114
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 144?154,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Tagging The Web: Building A Robust Web Tagger with Neural Network
Ji Ma
?
, Yue Zhang
?
and Jingbo Zhu
?
?
Northeastern University, China
?
Singapore University of Technology and Design
majineu@gmail.com
yue zhang@sutd.edu.sg
zhujingbo@mail.neu.edu.cn
Abstract
In this paper, we address the problem of
web-domain POS tagging using a two-
phase approach. The first phase learns rep-
resentations that capture regularities un-
derlying web text. The representation is
integrated as features into a neural network
that serves as a scorer for an easy-first POS
tagger. Parameters of the neural network
are trained using guided learning in the
second phase. Experiment on the SANCL
2012 shared task show that our approach
achieves 93.15% average tagging accu-
racy, which is the best accuracy reported
so far on this data set, higher than those
given by ensembled syntactic parsers.
1 Introduction
Analysing and extracting useful information from
the web has become an increasingly important re-
search direction for the NLP community, where
many tasks require part-of-speech (POS) tag-
ging as a fundamental preprocessing step. How-
ever, state-of-the-art POS taggers in the literature
(Collins, 2002; Shen et al, 2007) are mainly opti-
mized on the the Penn Treebank (PTB), and when
shifted to web data, tagging accuracies drop sig-
nificantly (Petrov and McDonald, 2012).
The problem we face here can be considered
as a special case of domain adaptation, where we
have access to labelled data on the source domain
(PTB) and unlabelled data on the target domain
(web data). Exploiting useful information from
the web data can be the key to improving web
domain tagging. Towards this end, we adopt the
idea of learning representations which has been
demonstrated useful in capturing hidden regular-
ities underlying the raw input data (web text, in
our case).
Our approach consists of two phrases. In the
pre-training phase, we learn an encoder that con-
verts the web text into an intermediate represen-
tation, which acts as useful features for prediction
tasks. We integrate the learned encoder with a set
of well-established features for POS tagging (Rat-
naparkhi, 1996; Collins, 2002) in a single neural
network, which is applied as a scorer to an easy-
first POS tagger. We choose the easy-first tagging
approach since it has been demonstrated to give
higher accuracies than the standard left-to-right
POS tagger (Shen et al, 2007; Ma et al, 2013).
In the fine-tuning phase, the parameters of the
network are optimized on a set of labelled train-
ing data using guided learning. The learned model
preserves the property of preferring to tag easy
words first. To our knowledge, we are the first to
investigate guided learning for neural networks.
The idea of learning representations from un-
labelled data and then fine-tuning a model with
such representations according to some supervised
criterion has been studied before (Turian et al,
2010; Collobert et al, 2011; Glorot et al, 2011).
While most previous work focus on in-domain se-
quential labelling or cross-domain classification
tasks, we are the first to learn representations for
web-domain structured prediction. Previous work
treats the learned representations either as model
parameters that are further optimized in super-
vised fine-tuning (Collobert et al, 2011) or as
fixed features that are kept unchanged (Turian et
al., 2010; Glorot et al, 2011). In this work,
we investigate both strategies and give empirical
comparisons in the cross-domain setting. Our re-
sults suggest that while both strategies improve
in-domain tagging accuracies, keeping the learned
representation unchanged consistently results in
better cross-domain accuracies.
We conduct experiments on the official data set
provided by the SANCL 2012 shared task (Petrov
and McDonald, 2012). Our method achieves a
93.15% average accuracy across the web-domain,
which is the best result reported so far on this data
144
set, higher than those given by ensembled syntac-
tic parsers. Our code will be publicly available at
https://github.com/majineu/TWeb.
2 Learning from Web Text
Unsupervised learning is often used for training
encoders that convert the input data to abstract rep-
resentations (i.e. encoding vectors). Such repre-
sentations capture hidden properties of the input,
and can be used as features for supervised tasks
(Bengio, 2009; Ranzato et al, 2007). Among the
many proposed encoders, we choose the restricted
Boltzmann machine (RBM), which has been suc-
cessfully used in many tasks (Lee et al, 2009b;
Hinton et al, 2006). In this section, we give some
background on RBMs and then show how they can
be used to learn representations of the web text.
2.1 Restricted Boltzmann Machine
The RBM is a type of graphical model that con-
tains two layers of binary stochastic units v ?
{0, 1}
V
and h ? {0, 1}
H
, corresponding to a set
of visible and hidden variables, respectively. The
RBM defines the joint probability distribution over
v and h by an energy function
E(v,h) = ?c
?
h? b
?
v ? h
?
Wv, (1)
which is factorized by a visible bias b ? R
V
, a
hidden bias c ? R
H
and a weight matrix W ?
R
H?V
. The joint distribution P (v,h) is given by
P (v,h) =
1
Z
exp(E(v,h)), (2)
where Z is the partition function.
The affine form of E with respect to v and h
implies that the visible variables are conditionally
independent with each other given the hidden layer
units, and vice versa. This yields the conditional
distribution:
P (v|h) =
V
?
j=1
P (v
j
|h) P (h|v) =
H
?
i=1
P (h
i
|v)
P (v
j
= 1|h) = ?(b
j
+W
?j
h) (3)
P (h
i
= 1|v) = ?(c
j
+W
i?
v) (4)
Here ? denotes the sigmoid function. Parameters
of RBMs ? = {b, c,W} can be trained efficiently
using contrastive divergence learning (CD), see
(Hinton, 2002) for detailed descriptions of CD.
2.2 Encoding Web Text with RBM
Most of the indicative features for POS disam-
biguation can be found from the words and word
combinations within a local context (Ratnaparkhi,
1996; Collins, 2002). Inspired by this observa-
tion, we apply the RBM to learn feature repre-
sentations from word n-grams. More specifically,
given the i
th
word w
i
of a sentence, we apply
RBMs to model the joint distribution of the n-gram
(w
i?l
, ? ? ? , w
i+r
), where l and r denote the left
and right window, respectively. Note that the vis-
ible units of RBMs are binary. While in our case,
each visible variable corresponds to a word, which
may take on tens-of-thousands of different values.
Therefore, the RBM need to be re-factorized to
make inference tractable.
We utilize the Word Representation RBM (WR-
RBM) factorization proposed by Dahl et al
(2012). The basic idea is to share word representa-
tions across different positions in the input n-gram
while using position-dependent weights to distin-
guish between different word orders.
Let w
k
be the k-th entry of lexicon L, and w
k
be its one-hot representation (i.e., only the k-th
component of w
k
is 1, and all the others are 0).
Let v
(j)
represents the j-th visible variable of the
WRRBM, which is a vector of length |L|. Then
v
(j)
= w
k
means that the j-th word in the n-gram
is w
k
. Let D ? R
D?|L|
be a projection matrix,
then Dw
k
projects w
k
into a D-dimensional real
value vector (embedding). For each position j,
there is a weight matrix W
(j)
? R
H?D
, which
is used to model the interaction between the hid-
den layer and the word projection in position j.
The visible biases are also shared across different
positions (b
(j)
= b ?j) and the energy function is:
E(v,h) = ?c
?
h?
n
?
j=1
(b
?
v
(j)
+ h
?
W
(j)
Dv
(j)
),
(5)
which yields the conditional distributions:
P (v|h) =
n
?
j=1
P (v
(j)
|h) P (h|v) =
?
i=1
P (h
i
|v)
P (h
i
= 1|v) = ?(c
i
+
n
?
j=1
W
(j)
i?
Dv
(j)
) (6)
P (v
(j)
= w
k
|h) =
1
Z
exp(b
?
w
k
+ h
?
W
(j)
Dw
k
)
(7)
145
Again Z is the partition function.
The parameters {b, c,D,W
(1)
, . . . ,W
(n)
}
can be trained using a Metropolis-Hastings-based
CD variant and the learned word representations
also capture certain syntactic information; see
Dahl et al (2012) for more details.
Note that one can stack standard RBMs on top
of a WRRBM to construct a Deep Belief Network
(DBN). By adopting greedy layer-wise training
(Hinton et al, 2006; Bengio et al, 2007), DBNs
are capable of modelling higher order non-linear
relations between the input, and has been demon-
strated to improve performance for many com-
puter vision tasks (Hinton et al, 2006; Bengio et
al., 2007; Lee et al, 2009a). However, in this work
we do not observe further improvement by em-
ploying DBNs. This may partly be due to the fact
that unlike computer vision tasks, the input struc-
ture of POS tagging or other sequential labelling
tasks is relatively simple, and a single non-linear
layer is enough to model the interactions within
the input (Wang and Manning, 2013).
3 Neural Network for POS
Disambiguation
We integrate the learned WRRBM into a neural
network, which serves as a scorer for POS dis-
ambiguation. The main challenge to designing
the neural network structure is: on the one hand,
we hope that the model can take the advantage
of information provided by the learned WRRBM,
which reflects general properties of web texts, so
that the model generalizes well in the web domain;
on the other hand, we also hope to improve the
model?s discriminative power by utilizing well-
established POS tagging features, such as those of
Ratnaparkhi (1996).
Our approach is to leverage the two sources of
information in one neural network by combining
them though a shared output layer, as shown in
Figure 1. Under the output layer, the network
consists of two modules: the web-feature mod-
ule, which incorporates knowledge from the pre-
trained WRRBM, and the sparse-feature module,
which makes use of other POS tagging features.
3.1 The Web-Feature Module
The web-feature module, shown in the lower left
part of Figure 1, consists of a input layer and two
hidden layers. The input for the this module is the
word n-gram (w
i?l
, . . . , w
i+r
), the form of which
Figure 1: The proposed neural network. The web-
feature module (lower left) and sparse-feature
module (lower right) are combined by a shared
output layer (upper).
is identical to the training data of the pre-trained
WRRBM.
The first layer is a linear projection layer, where
each word in the input is projected into a D-
dimensional real value vector using the projection
operation described in Section 2.2. The output of
this layer o
1
w
is the concatenation of the projec-
tions of w
i?l
, . . . , w
i+r
:
o
1
w
=
?
?
?
M
1
w
w
i?l
.
.
.
M
1
w
w
i+r
?
?
?
(8)
Here M
1
w
denotes the parameters of the first layer
of the web-feature module, which is a D ? |L|
projection matrix.
The second layer is a sigmoid layer to model
non-linear relations between the word projections:
o
2
w
= ?(M
2
w
o
1
w
+ b
2
w
) (9)
Parameters of this layer include: a bias vector
b
2
w
? R
H
and a weight matrix M
2
w
? R
H?nD
.
The web-feature module enables us to explore
the learned WRRBM in various ways. First, it al-
lows us to investigate knowledge from the WR-
RBM incrementally. We can choose to use only
the word representations of the learned WRRBM.
This can be achieved by initializing only the first
layer of the web module with the projection matrix
D of the learned WRRBM:
M
1
w
? D. (10)
Alternatively, we can choose to use the hidden
states of the WRRBM, which can be treated as the
146
representations of the input n-gram. This can be
achieved by also initializing the parameters of the
second layer of the web-feature module using the
position-dependent weight matrix and hidden bias
of the learned WRRBM:
b
2
w
? c (11)
M
2
w
? (W
(1)
, . . . ,W
(n)
) (12)
Second, the web-feature module also allows us
to make a comparison between whether or not to
further adjust the pre-trained representation in the
supervised fine-tuning phase, which corresponds
to the supervised learning strategies of Turian et al
(2010) and Collobert et al (2011), respectively. To
our knowledge, no investigations have been pre-
sented in the literature on this issue.
3.2 The Sparse-Feature Module
The sparse-feature module, as shown in the lower
right part of Figure 1, is designed to incorporate
commonly-used tagging features. The input for
this module is a vector of boolean values ?(x) =
(f
1
(x), . . . , f
k
(x)), where x denotes the partially
tagged input sentence and f
i
(x) denotes a fea-
ture function, which returns 1 if the correspond-
ing feature fires and 0 otherwise. The first layer of
this module is a linear transformation layer, which
converts the high dimensional sparse vector into a
fixed-dimensional real value vector:
o
s
= M
s
?(x) + b
s
(13)
Depending on the specific task being considered,
the output of this layer can be further fed to other
non-linear layers, such as a sigmoid or hyperbolic
tangent layer, to model more complex relations.
For POS tagging, we found that a simple linear
layer yields satisfactory accuracies.
The web-feature and sparse-feature modules are
combined by a linear output layer, as shown in the
upper part of Figure 1. The value of each unit in
this layer denotes the score of the corresponding
POS tag.
o
o
= M
o
(
o
w
o
s
)
+ b
o
(14)
In some circumstances, probability distribution
over POS tags might be a more preferable form
of output. Such distribution can be easily obtained
by adding a soft-max layer on top of the output
layer to perform a local normalization, as done by
Collobert et al (2011).
Algorithm 1 Easy-first POS tagging
Input: x a sentence of m words w
1
, . . . , w
m
Output: tag sequence of x
1: U? [w
1
, . . . , w
m
] // untagged words
2: while U 6= [] do
3: (w?,
?
t)? arg max
(w,t)?U?T
S(w, t)
4: w?.t?
?
t
5: U? U/[w?] // remove w? from U
6: end while
7: return [w
1
.t, . . . , w
m
.t]
4 Easy-first POS tagging with Neural
Network
The neural network proposed in Section 3 is used
for POS disambiguation by the easy-first POS tag-
ger. Parameters of the network are trained using
guided learning, where learning and search inter-
act with each other.
4.1 Easy-first POS tagging
Pseudo-code of easy-first tagging is shown in Al-
gorithm 1. Rather than tagging a sentence from
left to right, easy-first tagging is based on a deter-
ministic process, repeatedly selecting the easiest
word to tag. Here ?easiness? is evaluated based
on a statistical model. At each step, the algorithm
adopts a scorer, the neural network in our case,
to assign a score to each possible word-tag pair
(w, t), and then selects the highest score one (w?,
?
t)
to tag (i.e., tag w? with
?
t). The algorithm repeats
until all words are tagged.
4.2 Training
The training algorithm repeats for several itera-
tions over the training data, which is a set of sen-
tences labelled with gold standard POS tags. In
each iteration, the procedure shown in Algorithm
2 is applied to each sentence in the training set.
At each step during the processing of a training
example, the algorithm calculates a margin loss
based on two word-tag pairs (w, t) and (w?,
?
t) (line
4 ? line 6). (w, t) denotes the word-tag pair that
has the highest model score among those that are
inconsistent with the gold standard, while (w?,
?
t)
denotes the one that has the highest model score
among those that are consistent with the gold stan-
dard. If the loss is zero, the algorithm continues to
process the next untagged word. Otherwise, pa-
rameters are updated using back-propagation.
The standard back-propagation algorithm
147
(Rumelhart et al, 1988) cannot be applied
directly. This is because the standard loss is
calculated based on a unique input vector. This
condition does not hold in our case, because w?
and w may refer to different words, which means
that the margin loss in line 6 of Algorithm 2 is
calculated based on two different input vectors,
denoted by ?w?? and ?w?, respectively.
We solve this problem by decomposing the mar-
gin loss in line 6 into two parts:
? 1 + nn(w, t), which is associated with ?w?;
? ?nn(w?,
?
t), which is associated with ?w??.
In this way, two separate back-propagation up-
dates can be used to update the model?s parameters
(line 8 ? line 11). For the special case where w?
and w do refer to the same word w, it can be easily
verified that the two separate back-propagation up-
dates equal to the standard back-propagation with
a loss 1 + nn(w, t)? nn(w,
?
t) on the input ?w?.
The algorithm proposed here belongs to a gen-
eral framework named guided learning, where
search and learning interact with each other. The
algorithm learns not only a local classifier, but also
the inference order. While previous work (Shen et
al., 2007; Zhang and Clark, 2011; Goldberg and
Elhadad, 2010) apply guided learning to train a
linear classifier by using variants of the percep-
tron algorithm, we are the first to combine guided
learning with a neural network, by using a margin
loss and a modified back-propagation algorithm.
5 Experiments
5.1 Setup
Our experiments are conducted on the data set
provided by the SANCL 2012 shared task, which
aims at building a single robust syntactic anal-
ysis system across the web-domain. The data
set consists of labelled data for both the source
(Wall Street Journal portion of the Penn Treebank)
and target (web) domains. The web domain data
can be further classified into five sub-domains, in-
cluding emails, weblogs, business reviews, news
groups and Yahoo!Answers. While emails and
weblogs are used as the development sets, reviews,
news groups and Yahoo!Answers are used as the
final test sets. Participants are not allowed to use
web-domain labelled data for training. In addi-
tion to labelled data, a large amount of unlabelled
data on the web domain is also provided. Statistics
Algorithm 2 Training over one sentence
Input: (x, t) a tagged sentence, neural net nn
Output: updated neural net nn
?
1: U? [w
1
, . . . , w
m
] // untagged words
2: R? [(w
1
, t
1
), . . . , (w
m
, t
m
)] // reference
3: while U 6= [] do
4: (w, t)? arg max
(w,t)?(U?T/R)
nn(w, t)
5: (w?,
?
t)? arg max
(w,t)?R
nn(w, t)
6: loss? max(0, 1 + nn(w, t)? nn(w?,
?
t))
7: if loss > 0 then
8: e?? nn.BackPropErr(?w??,?nn(w?,
?
t))
9: e? nn.BackPropErr(?w?, 1+nn(w, t))
10: nn.Update(?w??, e?)
11: nn.Update(?w?, e)
12: else
13: U? U/{w?}, R? R/(w?,
?
t)
14: end if
15: end while
16: return nn
about labelled and unlabelled data are summarized
in Table 1 and Table 2, respectively.
The raw web domain data contains much noise,
including spelling error, emotions and inconsis-
tent capitalization. Following some participants
(Le Roux et al, 2012), we conduct simple prepro-
cessing steps to the input of the development and
the test sets
1
? Neutral quotes are transformed to opening or
closing quotes.
? Tokens starting with ?www.?, ?http.? or end-
ing with ?.org?, ?.com? are converted to a
?#URL? symbol
? Repeated punctuations such as ?!!!!? are col-
lapsed into one.
? Left brackets such as ?<?,?{? and ?[? are
converted to ?-LRB-?. Similarly, right brack-
ets are converted to ?-RRB-?
? Upper cased words that contain more than 4
letters are lowercased.
? Consecutive occurrences of one or more dig-
its within a word are replaced with ?#DIG?
We apply the same preprocessing steps to all the
unlabelled data. In addition, following Dahl et
1
The preprocessing steps make use of no POS knowledge,
and does not bring any unfair advantages to the participants.
148
Training set Dev set Test set
WSJ-Train Emails Weblogs WSJ-dev Answers Newsgroups Reviews WSJ-test
#Sen 30060 2,450 1,016 1,336 1,744 1,195 1,906 1,640
#Words 731,678 29,131 24,025 32,092 28,823 20,651 28,086 35,590
#Types 35,933 5,478 4,747 5,889 4,370 4,924 4,797 6,685
Table 1: Statistics of the labelled data. #Sen denotes number of sentences. #Words and #Types denote
number of words and unique word types, respectively.
Emails Weblogs Answers Newsgroups Reviews
#Sen 1,194,173 524,834 27,274 1,000,000 1,965,350
#Words 17,047,731 10,365,284 424,299 18,424,657 29,289,169
#Types 221,576 166,515 33,325 357,090 287,575
Table 2: Statistics of the raw unlabelled data.
features templates
unigram H(w
i
), C(w
i
), L(w
i
), L(w
i?1
), L(w
i+1
), t
i?2
, t
i?1
, t
i+1
, t
i+2
bigram L(w
i
) L(w
i?1
), L(w
i
) L(w
i+1
), t
i?2
 t
i?1
, t
i?1
 t
i+1
, t
i+1
 t
i+2
,
L(w
i
) t
i?2
, L(w
i
) t
i?1
, L(w
i
) t
i+1
, L(w
i
) t
i+2
trigram L(w
i
) t
i?2
 t
i?1
, L(w
i
) t
i?1
 t
i+1
, L(w
i
) t
i+1
 t
i+2
Table 3: Feature templates, where w
i
denotes the current word. H(w) and C(w) indicates whether w
contains hyphen and upper case letters, respectively. L(w) denotes a lowercased w.
al. (2012) and Turian et al (2010), we also low-
ercased all the unlabelled data and removed those
sentences that contain less than 90% a-z letters.
The tagging performance is evaluated accord-
ing to the official evaluation metrics of SANCL
2012. The tagging accuracy is defined as the per-
centage of words (punctuations included) that are
correctly tagged. The averaged accuracies are cal-
culated across the web domain data.
We trained the WRRBM on web-domain data
of different sizes (number of sentences). The data
sets are generated by first concatenating all the
cleaned unlabelled data, then selecting sentences
evenly across the concatenated file.
For each data set, we investigate an extensive set
of combinations of hyper-parameters: the n-gram
window (l, r) in {(1, 1), (2, 1), (1, 2), (2, 2)}; the
hidden layer size in {200, 300, 400}; the learning
rate in {0.1, 0.01, 0.001}. All these parameters are
selected according to the averaged accuracy on the
development set.
5.2 Baseline
We reimplemented the greedy easy-first POS tag-
ger of Ma et al (2013), which is used for all the
experiments. While the tagger of Ma et al (2013)
utilizes a linear scorer, our tagger adopts the neural
network as its scorer. The neural network of our
baseline tagger only contains the sparse-feature
module. We use this baseline to examine the per-
formance of a tagger trained purely on the source
domain. Feature templates are shown in Table 3,
which are based on those of Ratnaparkhi (1996)
and Shen et al (2007).
Accuracies of the baseline tagger are shown in
the upper part of Table 6. Compared with the
performance of the official baseline (row 4 of Ta-
ble 6), which is evaluated based on the output of
BerkeleyParser (Petrov et al, 2006; Petrov and
Klein, 2007), our baseline tagger achieves com-
parable accuracies on both the source and target
domain data. With data preprocessing, the aver-
age accuracy boosts to about 92.02 on the test set
of the target domain. This is consistent with pre-
vious work (Le Roux et al, 2011), which found
that for noisy data such as web domain text, data
cleaning is a effective and necessary step.
5.3 Exploring the Learned Knowledge
As mentioned in Section 3.1, the knowledge
learned from the WRRBM can be investigated
incrementally, using word representation, which
corresponds to initializing only the projection
layer of web-feature module with the projection
matrix of the learned WRRBM, or ngram-level
representation, which corresponds to initializing
both the projection and sigmoid layers of the web-
feature module by the learned WRRBM. In each
case, there can be two different training strate-
gies depending on whether the learned representa-
tions are further adjusted or kept unchanged dur-
ing the fine-turning phrase. Experimental results
under the 4 combined settings on the development
sets are illustrated in Figure 2, 3 and 4, where the
149
96.5
96.6
96.7
96.8
96.9
200 400 600 800 1000
Acc
ura
cy
Number of unlabelled sentences (k)
WSJ
word-fixedword-adjustngram-fixedngram-adjust
Figure 2: Tagging accuracies on the source-
domain data. ?word? and ?ngram? denote using
word representations and n-gram representations,
respectively. ?fixed? and ?adjust? denote that the
learned representation are kept unchanged or fur-
ther adjusted in supervised learning, respectively.
89.8
90
90.2
90.4
90.6
90.8
91
200 400 600 800 1000
Acc
ura
cy
Number of unlabelled sentences (k)
Email
word-fixedword-adjustngram-fixedngram-adjust
Figure 3: Accuracies on the email domain.
94.8
95
95.2
95.4
95.6
95.8
200 400 600 800 1000
Acc
ura
cy
Number of unlabelled sentences (k)
Weblog
word-fixedword-adjustngram-fixedngram-adjust
Figure 4: Accuracies on the weblog domain.
x-axis denotes the size of the training data and y-
axis denotes tagging accuracy.
5.3.1 Effect of the Training Strategy
From Figure 2 we can see that when knowl-
edge from the pre-trained WRRBM is incorpo-
method all non-oov oov
baseline 89.81 92.42 65.64
word-adjust +0.09 ?0.05 +1.38
word-fix +0.11 +0.13 +1.73
ngram-adjust +0.53 +0.52 +0.53
ngram-fix +0.69 +0.60 +2.30
Table 4: Performance on the email domain.
rated, both the training strategies (?word-fixed?
vs ?word-adjusted?, ?ngram-fixed? vs ?ngram-
adjusted?) improve accuracies on the source do-
main, which is consistent with previous findings
(Turian et al, 2010; Collobert et al, 2011). In
addition, adjusting the learned representation or
keeping them fixed does not result in too much dif-
ference in tagging accuracies.
On the web-domain data, shown in Figure 3 and
4, we found that leaving the learned representation
unchanged (?word-fixed?, ?ngram-fixed?) yields
consistently higher performance gains. This re-
sult is to some degree expected. Intuitively, unsu-
pervised pre-training moves the parameters of the
WRRBM towards the region where properties of
the web domain data are properly modelled. How-
ever, since fine-tuning is conducted with respect
to the source domain, adjusting the parameters
of the pre-trained representation towards optimiz-
ing source domain tagging accuracies would dis-
rupt its ability in modelling the web domain data.
Therefore, a better idea is to keep the representa-
tion unchanged so that we can learn a function that
maps the general web-text properties to its syntac-
tic categories.
5.3.2 Word and N-gram Representation
From Figures 2, 3 and 4, we can see that
adopting the ngram-level representation consis-
tently achieves better performance compared with
using word representations only (?word-fixed?
vs ?ngram-fixed?, ?word-adjusted? vs ?ngram-
adjusted?). This result illustrates that the ngram-
level knowledge captures more complex interac-
tions of the web text, which cannot be recovered
by using only word embeddings. Similar result
was reported by Dahl et al (2012), who found
that using both the word embeddings and the hid-
den units of a tri-gram WRRBM as additional fea-
tures for a CRF chunker yields larger improve-
ments than using word embeddings only.
Finally, more detailed accuracies under the 4
settings on the email domain are shown in Table
4. We can see that the improvement of using word
150
RBM-E RBM-W RBM-M
+acc%
Emails +0.73 +0.37 +0.69
Weblog +0.31 +0.52 +0.54
cov%
Emails 95.24 92.79 93.88
Weblog 90.21 97.74 94.77
Table 5: Effect of unlabelled data. ?+acc? denotes
improvement in tagging accuracy and ?cov? de-
notes the lexicon coverages.
representations mainly comes from better accu-
racy of out-of-vocabulary (oov) words. By con-
trast, using n-gram representations improves the
performance on both oov and non-oov.
5.4 Effect of Unlabelled Domain Data
In some circumstances, we may know beforehand
that the target domain data belongs to a certain
sub-domain, such as the email domain. In such
cases, it might be desirable to train WRRBM using
data only on that domain. We conduct experiments
to test whether using the target domain data to
train the WRRBM yields better performance com-
pared with using mixed data from all sub-domains.
We trained 3 WRRBMs using the email do-
main data (RBM-E), weblog domain data (RBM-
W) and mixed domain data (RBM-M), respec-
tively, with each data set consisting of 300k sen-
tences. Tagging performance and lexicon cover-
ages of each data set on the development sets are
shown in Table 5. We can see that using the target
domain data achieves similar improvements com-
pared with using the mixed data. However, for the
email domain, RBM-W yields much smaller im-
provement compared with RBM-E, and vice versa.
From the lexicon coverages, we can see that the
sub-domains varies significantly. The results sug-
gest that using mixed data can achieve almost as
good performance as using the target sub-domain
data, while using mixed data yields a much more
robust tagger across all sub-domains.
5.5 Final Results
The best result achieved by using a 4-gram WR-
RBM, (w
i?2
, . . . , w
i+1
), with 300 hidden units
learned on 1,000k web domain sentences are
shown in row 3 of Table 6. Performance of the
top 2 systems of the SANCL 2012 task are also
shown in Table 6. Our greedy tagger achieves 93%
tagging accuracy, which is significantly better than
the baseline?s 92.02% accuracy (p < 0.05 by Mc-
Nemar?s test). Moreover, we achieve the high-
est tagging accuracy reported so far on this data
set, surpassing those achieved using parser combi-
nations based on self-training (Tang et al, 2012;
Le Roux et al, 2012). In addition, different from
Le Roux et al (2012), we do not use any external
resources in data cleaning.
6 Related Work
Learning representations has been intensively
studied in computer vision tasks (Bengio et al,
2007; Lee et al, 2009a). In NLP, there is also
much work along this line. In particular, Col-
lobert et al (2011) and Turian et al (2010) learn
word embeddings to improve the performance of
in-domain POS tagging, named entity recogni-
tion, chunking and semantic role labelling. Yang
et al (2013) induce bi-lingual word embeddings
for word alignment. Zheng et al (2013) investi-
gate Chinese character embeddings for joint word
segmentation and POS tagging. While those ap-
proaches mainly explore token-level representa-
tions (word or character embeddings), using WR-
RBM is able to utilize both word and n-gram rep-
resentations.
Titov (2011) and Glorot et al (2011) propose
to learn representations from the mixture of both
source and target domain unlabelled data to im-
prove cross-domain sentiment classification. Titov
(2011) also propose a regularizer to constrain the
inter-domain variability. In particular, their reg-
ularizer aims to minimize the Kullback-Leibler
(KL) distance between the marginal distributions
of the learned representations on the source and
target domains.
Their work differs from ours in that their ap-
proaches learn representations from the feature
vectors for sentiment classification, which might
be of thousands of dimensions. Such high di-
mensional input gives rise to high computational
cost and it is not clear whether those approaches
can be applied to large scale unlabelled data, with
hundreds of millions of training examples. Our
method learns representations from only word n-
grams with n ranging from 3 to 5, which can
be easily applied to large scale-data. In addition,
while Titov (2011) and Glorot et al (2011) use the
learned representation to improve cross-domain
classification tasks, we are the first to apply it to
cross-domain structured prediction.
Blitzer et al (2006) propose to induce shared
representations for domain adaptation, which is
based on the alternating structure optimization
151
System Answer Newsgroup Review WSJ-t Avg
baseline-raw 89.79 91.36 89.96 97.09 90.31
baseline-clean 91.35 92.06 92.92 97.09 92.02
best-clean 92.37 93.59 93.62 97.44 93.15
baseline-offical 90.20 91.24 89.33 97.08 90.26
Le Roux et al(2011) 91.79 93.81 93.11 97.29 92.90
Tang et al (2012) 91.76 92.91 91.94 97.49 92.20
Table 6: Main results. ?baseline-raw? and ?baseline-clean? denote performance of our baseline tagger
on the raw and cleaned data, respectively. ?best-clean? is best performance achieved using a 4-gram
WRRBM. The lower part shows accuracies of the official baseline and that of the top 2 participants.
(ASO) method of Ando and Zhang (2005). The
idea is to project the original feature representa-
tions into low dimensional representations, which
yields a high-accuracy classifier on the target do-
main. The new representations are induced based
on the auxiliary tasks defined on unlabelled data
together with a dimensionality reduction tech-
nique. Such auxiliary tasks can be specific to the
supervised task. As pointed out by Plank (2009),
for many NLP tasks, defining the auxiliary tasks is
a non-trivial engineering problem. Compared with
Blitzer et al (2006), the advantage of using RBMs
is that it learns representations in a pure unsuper-
vised manner, which is much simpler.
Besides learning representations, another line
of research addresses domain-adaptation by in-
stance re-weighting (Bickel et al, 2007; Jiang
and Zhai, 2007) or feature re-weighting (Satpal
and Sarawagi, 2007). Those methods assume that
each example x that has a non-zero probability on
the source domain must have a non-zero proba-
bility on the target domain, and vice-versa. As
pointed out by Titov (2011), such an assumption
is likely to be too restrictive since most NLP tasks
adopt word-based or lexicon-based features that
vary significantly across different domains.
Regarding using neural networks for sequential
labelling, our approach shares similarity with that
of Collobert et al (2011). In particular, we both
use a non-linear layer to model complex relations
underling word embeddings. However, our net-
work differs from theirs in the following aspects.
Collobert et al (2011) model the dependency be-
tween neighbouring tags in a generative manner,
by employing a transition score A
ij
. Training the
score involves a forward process of complexity
O(nT
2
), where T denotes the number of tags. Our
model captures such a dependency in a discrimina-
tive manner, by just adding tag-related features to
the sparse-feature module. In addition, Collobert
et al (2011) train their network by maximizing the
training set likelihood, while our approach is to
minimize the margin loss using guided learning.
7 Conclusion
We built a web-domain POS tagger using a
two-phase approach. We used a WRRBM to
learn the representation of the web text and
incorporate the representation in a neural net-
work, which is trained using guided learning
for easy-first POS tagging. Experiment showed
that our approach achieved significant improve-
ment in tagging the web domain text. In ad-
dition, we found that keeping the learned repre-
sentations unchanged yields better performance
compared with further optimizing them on the
source domain data. We release our tools at
https://github.com/majineu/TWeb.
For future work, we would like to investigate
the two-phase approach to more challenging tasks,
such as web domain syntactic parsing. We be-
lieve that high-accuracy web domain taggers and
parsers would benefit a wide range of downstream
tasks such as machine translation
2
.
8 Acknowledgements
We would like to thank Hugo Larochelle for his
advices on re-implementing WRRBM. We also
thank Nan Yang, Shujie Liu and Tong Xiao for
the fruitful discussions, and three anonymous re-
viewers for their insightful suggestions. This re-
search was supported by the National Science
Foundation of China (61272376; 61300097), the
research grant T2MOE1301 from Singapore Min-
istry of Education (MOE) and the start-up grant
SRG ISTD2012038 from SUTD.
References
Rie Ando and Tong Zhang. 2005. A high-performance
semi-supervised learning method for text chunk-
2
This work is done while the first author is visiting SUTD.
152
ing. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 1?9, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2007. Greedy layer-wise train-
ing of deep networks. In B. Sch?olkopf, J. Platt, and
T. Hoffman, editors, Advances in Neural Informa-
tion Processing Systems 19, pages 153?160. MIT
Press, Cambridge, MA.
Yoshua Bengio. 2009. Learning deep architectures for
AI. Foundations and Trends in Machine Learning,
2(1):1?127. Also published as a book. Now Pub-
lishers, 2009.
Steffen Bickel, Michael Brckner, and Tobias Scheffer.
2007. Discriminative learning for differing training
and test distributions. In Proc of ICML 2007, pages
81?88. ACM Press.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120?128, Sydney, Australia, July.
Association for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493?2537.
George E. Dahl, Ryan P. Adams, and Hugo Larochelle.
2012. Training restricted boltzmann machines on
word observations. In John Langford and Joelle
Pineau, editors, Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12),
ICML ?12, pages 679?686, New York, NY, USA,
July. Omnipress.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proc of
ICML 2011, pages 513?520.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 742?750, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep belief
nets. Neural Comput., 18(7):1527?1554, July.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Comput., 14(8):1771?1800, August.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in nlp. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 264?271,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-
sul Samad Zadeh Kaljahi, and Anton Bryl. 2012.
DCU-Paris13 Systems for the SANCL 2012 Shared
Task. In Proceedings of the NAACL 2012 First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL), pages 1?4, Montr?eal, Canada,
June.
Honglak Lee, Roger Grosse, Rajesh Ranganath, and
Andrew Y. Ng. 2009a. Convolutional deep belief
networks for scalable unsupervised learning of hi-
erarchical representations. In Proc of ICML 2009,
pages 609?616.
Honglak Lee, Peter Pham, Yan Largman, and Andrew
Ng. 2009b. Unsupervised feature learning for audio
classification using convolutional deep belief net-
works. In Y. Bengio, D. Schuurmans, J. Lafferty,
C. K. I. Williams, and A. Culotta, editors, Advances
in Neural Information Processing Systems 22, pages
1096?1104.
Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang. 2013.
Easy-first pos tagging and dependency parsing with
beam search. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 110?114,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Association for Computational Linguistics.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. Notes of
the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Barbara Plank. 2009. Structural correspondence learn-
ing for parse disambiguation. In Alex Lascarides,
153
Claire Gardent, and Joakim Nivre, editors, EACL
(Student Research Workshop), pages 37?45. The As-
sociation for Computer Linguistics.
Marc?Aurelio Ranzato, Christopher Poultney, Sumit
Chopra, and Yann LeCun. 2007. Efficient learn-
ing of sparse representations with an energy-based
model. In B. Sch?olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Process-
ing Systems 19, pages 1137?1144. MIT Press, Cam-
bridge, MA.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1988. Neurocomputing: Foundations
of research. chapter Learning Representations
by Back-propagating Errors, pages 696?699. MIT
Press, Cambridge, MA, USA.
Sandeepkumar Satpal and Sunita Sarawagi. 2007. Do-
main adaptation of conditional probability models
via feature subsetting. In PKDD, volume 4702 of
Lecture Notes in Computer Science, pages 224?235.
Springer.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760?767, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Buzhou Tang, Min Jiang, and Hua Xu. 2012.
Varderlibt?s systems for sancl2012 shared task. In
Proceedings of the NAACL 2012 First Workshop
on Syntactic Analysis of Non-Canonical Language
(SANCL), Montr?eal, Canada, June.
Ivan Titov. 2011. Domain adaptation by constraining
inter-domain variability of latent feature representa-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 62?71, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384?394, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Mengqiu Wang and Christopher D. Manning. 2013.
Effect of non-linear deep architecture in sequence la-
beling. In Proceedings of the 6th International Joint
Conference on Natural Language Processing (IJC-
NLP).
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-
hai Yu. 2013. Word alignment modeling with con-
text dependent deep neural network. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 166?175, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Yue Zhang and Stephen Clark. 2011. Syntax-based
grammaticality improvement using ccg and guided
search. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1147?1157, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for Chinese word segmenta-
tion and POS tagging. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 647?657, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
154
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 791?796,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Punctuation Processing for Projective Dependency Parsing
?
Ji Ma
?
, Yue Zhang
?
and Jingbo Zhu
?
?
?
Northeastern University, Shenyang, China
?
Singapore University of Technology and Design, Singapore
?
Hangzhou YaTuo Company, 358 Wener Rd., Hangzhou, China, 310012
majineu@gmail.com
yue zhang@sutd.edu.sg
zhujingbo@mail.neu.edu.cn
Abstract
Modern statistical dependency parsers as-
sign lexical heads to punctuations as well
as words. Punctuation parsing errors lead
to low parsing accuracy on words. In this
work, we propose an alternative approach
to addressing punctuation in dependency
parsing. Rather than assigning lexical
heads to punctuations, we treat punctu-
ations as properties of their neighbour-
ing words, used as features to guide the
parser to build the dependency graph. In-
tegrating our method with an arc-standard
parser yields a 93.06% unlabelled attach-
ment score, which is the best accuracy by
a single-model transition-based parser re-
ported so far.
1 Introduction
The task of dependency parsing is to identify the
lexical head of each of the tokens in a string.
Modern statistical parsers (McDonald et al, 2005;
Nivre et al, 2007; Huang and Sagae, 2010; Zhang
and Nivre, 2011) treat all the tokens equally, as-
signing lexical heads to punctuations as well as
words. Punctuations arguably play an important
role in syntactic analysis. However, there are a
number of reasons that it is not necessary to parse
punctuations:
First, the lexical heads of punctuations are not
as well defined as those of words. Consequently,
punctuations are not as consistently annotated in
treebanks as words, making it harder to parse
punctuations. For example, modern statistical
parsers achieve above 90% unlabelled attachment
score (UAS) on words. However, the UAS on
punctuations are generally below 85%.
?
This work was done while the first author was visiting
SUTD
Moreover, experimental results showed that
parsing accuracy of content words drops on sen-
tences which contain higher ratios of punctuations.
One reason for this result is that projective de-
pendency parsers satisfy the ?no crossing links?
constraint, and errors in punctuations may pre-
vent correct word-word dependencies from being
created (see section 2). In addition, punctuations
cause certain type of features inaccurate. Take va-
lency features for example, previous work (Zhang
and Nivre, 2011) has shown that such features are
important to parsing accuracy, e.g., it may inform
the parser that a verb already has two objects at-
tached to it. However, such information might
be inaccurate when the verb?s modifiers contain
punctuations.
Ultimately, it is the dependencies between
words that provide useful information for real
world applications. Take machine translation or
information extraction for example, most systems
take advantage of the head-modifier relationships
between word pairs rather than word-punctuation
pairs to make better predictions. The fact that most
previous work evaluates parsing accuracies with-
out taking punctuations into account is also largely
due to this reason.
Given the above reasons, we propose an alterna-
tive approach to punctuation processing for depen-
dency parsing. In this method, punctuations are
not associated with lexical heads, but are treated
as properties of their neighbouring words.
Our method is simple and can be easily incor-
porated into state-of-the-art parsers. In this work,
we report results on an arc-standard transition-
based parser. Experiments show that our method
achieves about 0.90% UAS improvement over the
greedy baseline parser on the standard Penn Tree-
bank test set. Although the improvement becomes
smaller as the beam width grows larger, we still
achieved 93.06% UAS with a beam of width 64,
which is the best result for transition-based parsers
791
Length 1 ? 20 21? 40 41? 60
Punc % 0 ? 15 15 ? 30 > 30 0 ? 15 15 ? 30 > 30 0 ? 15 15 ? 30 > 30
E-F 94.56 92.88 87.67 91.84 91.82 83.87 89.83 88.01 ?
A-S 93.87 92.00 90.05 90.81 90.15 75.00 88.06 88.89 ?
A-S-64 95.28 94.43 88.15 92.96 92.63 76.61 90.78 88.76 ?
MST 94.90 93.55 88.15 92.45 93.11 77.42 90.89 89.77 ?
Table 2: Parsing accuracies vs punctuation ratios, on the development set
System E-F A-S A-S-64 MST
Dev UAS 91.83 90.71 93.02 92.56
Test UAS 91.75 90.34 92.84 92.10
Dev UAS-p 83.20 79.69 84.80 84.42
Test UAS-p 84.67 79.64 87.80 85.67
Dev
?
UAS 90.64 89.55 91.87 90.11
Test
?
UAS 90.40 89.33 91.75 89.82
Table 1: Parsing accuracies. ?E-F? and ?MST? de-
note easy-first parser and MSTparser, respectively.
?A-S? and ?A-S 64? denote our arc-standard parser
with beam width 1 and 64, respectively. ?UAS?
and ?UAS-p? denote word and punctuation unla-
belled attachment score, respectively. ?
?
? denotes
the data set with punctuations removed.
reported so far. Our code will be available at
https://github.com/majineu/Parser/Punc/A-STD.
2 Influence of Punctuations on Parsing
In this section, we conduct a set of experiments to
show the influence of punctuations on dependency
parsing accuracies.
2.1 Setup
We use the Wall Street Journal portion of the Penn
Treebank with the standard splits: sections 02-21
are used as the training set; section 22 and sec-
tion 23 are used as the development and test set,
respectively. Penn2Malt is used to convert brack-
eted structures into dependencies. We use our own
implementation of the Part-Of-Speech (POS) tag-
ger proposed by Collins (2002) to tag the devel-
opment and test sets. Training set POS tags are
generated using 10-fold jack-knifing. Parsing ac-
curacy is evaluated using unlabelled attachment
score (UAS), which is the percentage of words that
are assigned the correct lexical heads.
To show that the influence of punctuations
on parsing is independent of specific pars-
ing algorithms, we conduct experiments us-
ing three parsers, each representing a different
parsing methodology: the open source MST-
Parser
1
(McDonald and Pereira, 2006), our own
re-implementation of an arc-standard transition-
based parser (Nivre, 2008), which is trained us-
ing global learning and beam-search (Zhang and
Clark, 2008) with a rich feature set (Zhang and
Nivre, 2011)
2
, and our own re-implementation of
the easy-first parser (Goldberg and Elhadad, 2010)
with an extended feature set (Ma et al, 2013).
2.2 Punctuations and Parsing Accuracy
Our first experiment is to show that, compared
with words, punctuations are more difficult to
parse and to learn. To see this, we evaluate the
parsing accuracies of the selected parsers on words
and punctuations, separately. Results are listed
in Table 1, where row 2 and row 3 list the UAS
of words (all excluding punctuations) on the de-
velopment and test set, respectively. Row 4 and
row 5 list accuracies of punctuations (all excluding
words) on the development and test set, respec-
tively. We can see that although all the parsers
achieve above 90% UAS on words, the UAS on
punctuations are mostly below 85%.
As for learning, we calculate the percentage of
parameter updates that are caused by associating
punctuations with incorrect heads during training
of the easy-first parser
3
. The result is that more
than 31% of the parameter updates are caused due
to punctuations, though punctuations account for
only 11.6% of the total tokens in the training set.
The fact that parsers achieve low accuracies on
punctuations is to some degree expected, because
the head of a punctuation mark is linguistically
less well-defined. However, a related problem is
1
We trained a second order labelled parser with all the
configurations set to the default value. The code is publicly
available at http://sourceforge.net/projects/mstparser/
2
Some feature templates in Zhang and Nivre (2011) in-
volve head word and head POS tags which are not avail-
able for an arc-standard parser. Interestingly, without those
features our arc-standard parser still achieves 92.84% UAS
which is comparable to the 92.90% UAS obtained by the arc-
eager parser of Zhang and Nivre (2011)
3
For the greedy easy-first parser, whether a parameter up-
date is caused by punctuation error can be determined with
no ambiguity.
792
Figure 1: Illustration of processing paired punctuation. The property of a word is denoted by the punc-
tuation below that word.
that parsing accuracy on words tends to drop on
the sentences which contain high ratio of punc-
tuations. To see this, we divide the sentences in
the development set into sub-sets according the
punctuation ratio (percentage of punctuations that
a sentence contains), and then evaluate parsing ac-
curacies on the sub-sets separately.
The results are listed in Table 2. Since long
sentences are inherently more difficult to parse,
to make a fair comparison, we further divide the
development set according to sentence lengths as
shown in the first row
4
. We can see that most of the
cases, parsing accuracies drop on sentences with
higher punctuation ratios. Note that this negative
effect on parsing accuracy might be overlooked
since most previous work evaluates parsing accu-
racy without taking punctuations into account.
By inspecting the parser outputs, we found that
error propagation caused by assigning incorrect
head to punctuations is one of the main reason that
leads to this result. Take the sentence shown in
Figure 1 (a) for example, the word Mechanisms
is a modifier of entitled according to the gold ref-
erence. However, if the quotation mark, ?, is in-
correctly recognized as a modifier of was, due to
the ?no crossing links? constraint, the arc between
Mechanisms and entitled can never be created.
A natural question is whether it is possible to
reduce such error propagation by simply remov-
ing all punctuations from parsing. Our next ex-
periment aims at answering this question. In this
experiment, we first remove all punctuations from
the original data and then modify the dependency
arcs accordingly in order to maintain word-word
dependencies in the original data. We re-train the
parsers on the modified training set and evaluate
4
1694 out of 1700 sentences on the development set with
length no larger than 60 tokens
parsing accuracies on the modified data.
Results are listed in row 6 and row 7 of Table 1.
We can see that parsing accuracies on the modified
data drop significantly compared with that on the
original data. The result indicates that by remov-
ing punctuations, we lose some information that is
important for dependency parsing.
3 Punctuation as Properties
In our method, punctuations are treated as prop-
erties of its neighbouring words. Such properties
are used as additional features to guide the parser
to construct the dependency graph.
3.1 Paired Punctuation
Our method distinguishes paired punctuations
from other punctuations. Here paired punctuations
include brackets and quotations marks, whose
Penn Treebank POS tags are the following four:
-LRB- -RRB- ? ?
The characteristics of paired punctuations include:
(1) they typically exist in pairs; (2) they serve as
boundaries that there is only one dependency arc
between the words inside the boundaries and the
words outside. Take the sentence in Figure 1 (a)
for example, the only arc cross the boundary is
(Mechanisms, entitled) where entitled is the head.
To utilize such boundary information, we fur-
ther classify paired punctuations into two cate-
gories: those that serve as the beginning of the
boundary, whose POS tags are either -LRB- or ?,
denoted by BPUNC; and those that serve as the end
of the boundary, denoted by EPUNC.
Before parsing starts, a preprocessing step is
used to first attach the paired punctuations as
properties of their neighbouring words, and then
remove them from the sentence. In particular,
793
unigram for p in ?
0
, ?
1
, ?
2
, ?
3
, ?
0
, ?
1
, ?
2
p
punc
for p in ?
0
, ?
1
, ?
2
, ?
0
, ?
1
p
punc
 p
w
, p
punc
 p
t
bigram for p, q in (?
0
, ?
0
), (?
0
, ?
1
), (?
0
, ?
2
), (?
0
, ?
1
), (?
0
, ?
2
) p
punc
 q
punc
, p
punc
 q
t
, p
punc
 q
w
for p, q in (?
2
, ?
0
), (?
1
, ?
0
), (?
2
, ?
0
) p
punc
 q
t
, p
punc
 p
t
 q
t
for p, q in (?
2
, ?
0
), (?
1
, ?
0
), (?
0
, ?
0
) d
pq
 p
punc
 p
t
 q
t
Table 3: Feature templates. For an element p either on ? or ? of an arc-standard parser, we use p
punc
,
p
w
and p
t
to denote the punctuation property, head word and head tag of p, respectively. d
pq
denotes the
distance between the two elements p and q.
we attach BPUNCs to their right neighbours and
EPUNCs to their left neighbours, as shown in Fig-
ure 1 (b). Note that in Figure 1 (a), the left neigh-
bour of ? is also a punctuation. In such cases, we
simply remove these punctuations since the exis-
tence of paired punctuations already indicates that
there should be a boundary.
During parsing, when a dependency arc with
lexical head w
h
is created, the property of w
h
is
updated by the property of its left (or right) most
child to keep track whether there is a BPUNC (or
EPUNC) to the left (or right) side of the sub-tree
rooted at w
h
, as shown in Figure 1 (c). When
BPUNCs and EPUNCs meet each other at w
h
, a
PAIRED property is assigned to w
h
to capture that
the words within the paired punctuations form a
sub-tree, rooted at w
h
. See Figure 1 (d).
3.2 Practical Issues
It is not uncommon that two BPUNCS appear ad-
jacent to each other. For example,
(?Congress?s Environmental Buccaneers,?
Sept. 18).
In our implementation, BPUNC or EPUNC prop-
erties are implemented using flags. In the exam-
ple, we set two flags ? and ( on the word Con-
grees?s. When BPUNC and EPUNC meet each
other, the corresponding flags are turned off. In
the example, when Congrees?s is identified as a
modifier of Buccaneers, the ? flag of Buccaneers
is turned off. However, we do not assign a PAIRED
property to Buccaneers since its ( flag is still on.
The PAIRED property is assigned only when all
the flags are turned off.
3.3 Non-Paired Punctuations
Though some types of non-paired punctuations
may capture certain syntactic patterns, we do not
make further distinctions between them, and treat
these punctuations uniformly for simplicity.
Before parsing starts and after the preprocessing
step for paired punctuations, our method employs
a second preprocessing step to attach non-paired
punctuations to their left neighbouring words. It
is guaranteed that the property of the left neigh-
bouring words of non-paired punctuations must be
empty. Otherwise, it means the non-paired punc-
tuation is adjacent to a paired punctuation. In
such cases, the non-paired punctuation would be
removed in the first processing step.
During parsing, non-paired punctuations are
also passed bottom-up: the property of w
h
is up-
dated by its right-most dependent to keep track
whether there is a punctuation to the right side
of the tree rooted at w
h
. The only special case is
that ifw
h
already contains a BPUNC property, then
our method simply ignores the non-paired prop-
erty since we maintain the boundary information
with the highest priority.
3.4 Features
We incorporate our method into the arc-standard
transition-based parser, which uses a stack ? to
maintain partially constructed trees and a buffer ?
for the incoming words (Nivre, 2008). We design
a set of features to exploit the potential of using
punctuation properties for the arc-standard parser.
The feature templates are listed in Table 3.
In addition to the features designed for paired
punctuations, such as bigram punctuation features
listed in line 3 of Table 3, we also design features
for non-paired punctuations. For example, the dis-
tance features in line 5 of Table 3 is used to capture
the pattern that if a word w with comma property
is the left modifier of a noun or a verb, the distance
between w and its lexical head is often larger than
1. In other words, they are not adjacent.
4 Results
Our first experiment is to investigate the effect of
processing paired punctuations on parsing accu-
racy. In this experiment, the method introduced
in Section 3.1 is used to process paired punctua-
tions, and the non-paired punctuations are left un-
794
s Baseline Paired All
1 90.76 91.25 91.47
2 91.88 92.06 92.34
4 92.50 92.61 92.70
8 92.73 92.76 92.82
16 92.90 92.94 92.99
64 92.99 93.04 93.10
Table 4: Parsing accuracies on the development
set. s denotes the beam width.
touched. Feature templates used in this experi-
ment are those listed in the top three rows of Ta-
ble 3 together with those used for the baseline arc-
standard parser.
Results on the development set are shown in the
second column of Table 4. We can see that when
the beam width is set to 1, our method achieves an
0.49 UAS improvement. By comparing the out-
puts of the two parsers, two types of errors made
by the baseline parser are effectively corrected.
The first is that our method is able to cap-
ture the pattern that there is only one depen-
dency arc between the words within the paired-
punctuations and the words outside, while the
baseline parser sometimes creates more depen-
dency arcs that cross the boundary.
The second is more interesting. Our method is
able to capture that the root, w
h
, of the sub-tree
within the paired-punctuation, such as ?Mecha-
nisms? in Figure 1, generally serves as a modifier
of the words outside, while the baseline parser oc-
casionally make w
h
as the head of the sentence.
As we increase the beam width, the improve-
ment of our method over the baseline becomes
smaller. This is as expected, since beam search
also has the effect of reducing error propagation
(Zhang and Nivre, 2012), thereby alleviating the
errors caused by punctuations.
In the last experiment, we examine the effect
of incorporating all punctuations using the method
introduced in Section 2. In this experiment, we
use all the feature templates in Table 3 and those
in the baseline parser. Results are listed in the
fourth column of Table 4, which shows that pars-
ing accuracies can be further improved by also
processing non-paired punctuations. The overall
accuracy improvement when the beam width is 1
reaches 0.91%. The extra improvements mainly
come from better accuracies on the sentences with
comma. However, the exact type of errors that
are corrected by using non-paired punctuations is
more difficult to summarize.
system UAS Comp Root
Baseline 90.38 37.71 89.45
All-Punc 91.32 41.35 92.43
Baseline-64 92.84 46.90 95.57
All-Punc-64 93.06 48.55 95.53
Huang 10 92.10 ? ?
Zhang 11 92.90 48.00 91.80
Choi 13 92.96 ? ?
Bohnet 12 93.03 ? ?
Table 5: Final result on the test set.
The final results on the test set are listed in Ta-
ble 5
5
. Table 5 also lists the accuracies of state-
of-the-art transition-based parsers. In particular,
?Huang 10? and ?Zhang 11? denote Huang and
Sagae (2010) and Zhang and Nivre (2011), re-
spectively. ?Bohnet 12? and ?Choi 13? denote
Bohnet and Nivre (2012) and Choi and Mccal-
lum (2013), respectively. We can see that our
method achieves the best accuracy for single-
model transition-based parsers.
5 Conclusion and Related Work
In this work, we proposed to treat punctuations
as properties of context words for dependency
parsing. Experiments with an arc-standard parser
showed that our method effectively improves pars-
ing performance and we achieved the best accu-
racy for single-model transition-based parser.
Regarding punctuation processing for depen-
dency parsing, Li et al (2010) proposed to uti-
lize punctuations to segment sentences into small
fragments and then parse the fragments separately.
A similar approach is proposed by Spitkovsky et
al. (2011) which also designed a set of constraints
on the fragments to improve unsupervised depen-
dency parsing.
Acknowledgements
We highly appreciate the anonymous reviewers
for their insightful suggestions. This research
was supported by the National Science Founda-
tion of China (61272376; 61300097; 61100089),
the Fundamental Research Funds for the Cen-
tral Universities (N110404012), the research grant
T2MOE1301 from Singapore Ministry of Ed-
ucation (MOE) and the start-up grant SRG
ISTD2012038 from SUTD.
5
The number of training iteration is determined using the
development set.
795
References
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ?12, pages 1455?1465, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jinho D. Choi and Andrew Mccallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 742?750, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Jan Hajic, Sandra Carberry, and Stephen Clark, ed-
itors, ACL, pages 1077?1086. The Association for
Computer Linguistics.
Zhenghua Li, Wanxiang Che, and Ting Liu. 2010. Im-
proving dependency parsing using punctuation. In
Minghui Dong, Guodong Zhou, Haoliang Qi, and
Min Zhang, editors, IALP, pages 53?56. IEEE Com-
puter Society.
Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang. 2013.
Easy-first pos tagging and dependency parsing with
beam search. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 110?114,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2006)), volume 6, pages
81?88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?05, pages 91?98, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2011. Punctuation: Making a point in un-
supervised dependency parsing. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning (CoNLL-2011).
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Yue Zhang and Joakim Nivre. 2012. Analyzing
the effect of global learning and beam-search on
transition-based dependency parsing. In Proceed-
ings of COLING 2012: Posters, pages 1391?1400,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
796
A Multi-stage Clustering Framework for Chinese Personal 
Name Disambiguation 
    Huizhen Wang, Haibo Ding, Yingchao Shi, Ji Ma,  Xiao Zhou, Jingbo Zhu 
Natural Language Processing Laboratory, 
Northeatern University 
 Shenyang, Liaoning, China 
{wanghuizhen|zhujingbo@mail.neu.edu.cn 
{dinghb|shiyc|maji}@mail.neu.edu.cn 
 
Abstract 
This paper presents our systems for the 
participation of Chinese Personal Name 
Disambiguation task in the CIPS-
SIGHAN 2010. We submitted two dif-
ferent systems for this task, and both of 
them all achieve the best performance. 
This paper introduces the multi-stage 
clustering framework and some key 
techniques used in our systems, and 
demonstrates experimental results on 
evaluation data. Finally, we further dis-
cuss some interesting issues found dur-
ing the development of the system. 
1 Introduction 
Personal name disambiguation (PND) is very 
important for web search and potentially other 
natural language applications such as question 
answering. CIPS-SIGHAN bakeoffs provide a 
platform to evaluate the effectiveness of various 
methods on Chinese PND task.  
Different from English PND, word segmenta-
tion techniques are needed for Chinese PND 
tasks. In practice, person names are highly am-
biguous because different people may have the 
same name, and the same name can be written 
in different ways. It?s an n-to-n mapping of per-
son names to the specific people. There are two 
main challenges on Chinese PND: the first one 
is how to correctly recognize personal names in 
the text, and the other is how to distinguish dif-
ferent persons who have the same name. For 
address these challenges, we designed a rule-
based combination technique to improve NER 
performance and propose a multi-stage cluster-
ing framework for Chinese PND. We partici-
pated in the bakeoff of the Chinese PND task, 
on the test set and the diagnosis test set, our two 
systems are ranked at the 1st and 2nd position. 
The rest of this paper is organized as follows. 
In Section 2, we first give the key features and 
techniques used in our two systems. In Section 
3, experimental results on the evaluation test 
data demonstrated that our methods are effec-
tive to disambiguate the personal name, and 
discussions on some issues we found during the 
development of the system are given. In Section 
4, we conclude our work. 
2 System Description 
In this section, we describe the framework of 
our systems in more detail, involving data pre-
processing, discard-class document identifica-
tion, feature definition, clustering algorithms, 
and sub-system combination. 
2.1 Data Preprocessing 
There are around 100-300 news articles per per-
sonal name in the evaluation corpus. Each arti-
cle is stored in the form of XML and encoded in 
UTF-8. At first, each news article should be 
preprocessed as follows: 
 Use a publicly available Chinese encoding 
Converter tool to convert each news article 
from UTF-8 coding into GB1; 
 Remove all XML tags; 
 Process Chinese word segmentation, part-
of-speech (POS) tagging and name entity 
recognition (NER); 
The performance of word segmentation and 
NER tools generally affect the effectiveness of 
our Chinese PND systems. During system de-
                                                 
1
 http://www.mandarintools.com/ 
veloping process, we found that the publicly 
available NER systems obtain unsatisfactory 
performance on evaluation data. To address this 
challenge, we propose a new rule-based combi-
nation technique to improve NER performance. 
In our combination framework, two different 
NER systems are utilized, including a CRF-
based NER system and our laboratory?s NER 
system (Yao et al,2002). The latter was imple-
mented based on the maximum matching prin-
ciple and some linguistic post-preprocessing 
rules. Since both two NER systems adopt dif-
ferent technical frameworks, it is possible to 
achieve a better performance by means of sys-
tem combination techniques.  
The basic idea of our combination method is 
to first simply combine the results produced by 
both NER systems, and further utilize some 
heuristic post-processing rules to refine NE 
identification results. To achieve this goal, we 
first investigate error types caused by both NER 
systems, and design some post-preprocessing 
rules to correct errors or select the appropriate 
NER results from disagreements. Notice that 
such rules are learned from sample data (i.e., 
training set), not from test set. Experimental 
results demonstrate satisfactory NER perform-
ance by introducing these heuristic refinement 
rules as follows:  
 Conjunction Rules. Two NEs separated 
by a conjunction (such as ???,???,???, 
??? ) belong to the same type, e.g., ???
/adj.?/???/person?. Such a conjunc-
tion rule can help NER systems make a 
consistent prediction on both NEs, e.g., ??
?/person? and ???/person?.  
 Professional Title Rules. Professional title 
words such as ???? are strong indicators 
of person names, e.g., ???/???. Such a 
rule can be written in the form of ?profes-
sional_title+person_name?.  
 Suffix Rules. If an identified person name 
is followed by a suffix of another type of 
named entities such as location, it is not a 
true person name, for example, ?????
???/person ?/?/???. Since ??? is 
a suffix of a location name. ??????
??/person ?/location-suffix? should be 
revised to be a new location name, namely 
?????????/location?. 
 Foreign Person Name Rules. Two identi-
fied person names connected by a dot are 
merged into a single foreign person name, 
e.g., ??/./???? => ??.???? 
 Chinese Surname Rules. Surnames are 
very important for Chinese person name 
identification. However, some common 
surnames can be single words depending 
upon the context, for example, the Chinese 
word ??? can be either a surname or a 
quantifier. To tackle this problem, some 
post-processing rules for ??, ?, ?, ?, 
?? are designed in our system. 
 Query-Dependent Rules. Given a query 
person name A, if the string AB occurring 
in the current document has been identified 
as a single person name many times in 
other documents, our system would tend to 
segment AB as a single person name rather 
than as A/B. For example, if ????? was 
identified as a true person name more than 
one time in other documents, in such a case, 
???/??/?/??/?=> ???/???
/person??/? 
Incorporating these above post-processing 
rules, our NER system based on heuristic post-
processing rules shows 98.89% precision of 
NER on training set.  
2.2 Discard-Class Document Identification 
Seen from evaluation data, there are a lot of 
documents belonging to a specific class, re-
ferred to as discard-class. In the discard-class, 
the query person name occurring in the docu-
ment is not a true person name. For example, a 
query word ???? is a famous ocean name not 
a person name in the sentence ???????
???????????????. In such a 
case, the corresponding document is considered 
as discard-class. Along this line, actually the 
discard-class document identification is very 
simple task. If a document does not contain a 
true person name that is the same as the query 
or contains the query, it is a discard-class 
document.  
2.3 Feature Definition 
To identify different types of person name and 
for the PND purpose, some effective binary fea-
tures are defined to construct the document rep-
resentation as feature vectors as follows: 
 Personal attributes: involving profes-
sional title, affiliation, location, co-
occurrence person name and organization 
related to the given query.   
 NE-type Features: collecting all NEs oc-
curring in the context of the given query. 
There are two kinds of NE-type features 
used in our systems, local features and 
global features. The global features are de-
fined with respect to the whole document 
while the local features are extracted only 
from the two or three adjacent sentences 
for the given query.  
 BOW-type features: constructing the con-
text feature vector based on bag-of-word 
model. Similarly, there are local and global 
BOW-type features with respect to the con-
text considered.  
2.4 A Multi-stage Clustering Framework  
Seen from the training set, 36% of person 
names indicate journalists, 10% are sportsmen, 
and the remaining are common person names. 
Based on such observations, it is necessary to 
utilize different methodology to PND on differ-
ent types of person names, for example, because 
the most effective information to distinguish 
different journalists are the reports? location and 
colleagues, instead of the whole document con-
tent. To achieve a satisfactory PND perform-
ance, in our system we design three different 
modules for analyzing journalist, sportsman and 
common person name, respectively.  
2.4.1 PND on the Journalist Class 
In our system, some regular expressions are 
designed to determine whether a person name is 
a journalist or not. For example: 
 ??? /ni */ns */t */t ?? |? /n (/w .* 
[?/w */ni ?/w ]* query name/nh .*)/w 
 (/w .*query name/nh .*)/w 
 [*/nh]* query name/nh [*/nh] 
 ? ? | ? ? /n [*/nh]* query name/nh 
[*/nh]* 
To disambiguate on the journalist class, our 
system utilizes a rule-based clustering technique 
distinguish different journalists. For each 
document containing the query person name as 
journalists, we first extract the organization and 
the location occurring in the local context of the 
query. Two such documents can be put into the 
same cluster if they contain the same organiza-
tion or location names, otherwise not. In our 
system, a location dictionary containing prov-
ince-city information extracted from Wikipedia 
is used to identify location name. For example: 
??? (?? ?? ?? ?? ?), ??(??
? ?? ??? ?? ???). Based on this 
dictionary, it is very easy to map a city to its 
corresponding province.  
2.4.2 PND on the Sportsman Class 
Like done in PND on the journalist class, we 
also use rule-based clustering techniques for 
disambiguating sportsman class. The major dif-
ference is to utilize topic features for PND on 
the sportsman class. If the topic of the given 
document is sports, this document can be con-
sidered as sportsman class. The key is to how to 
automatically identify the topic of the document 
containing the query. To address this challenge, 
we adopt a domain knowledge based technique 
for document topic identification. The basic 
idea is to utilize a domain knowledge dictionary 
NEUKD developed by our lab, which contains 
more than 600,000 domain associated terms and 
the corresponding domain features. Some do-
main associated terms defined in NEUKD are 
shown in Table 1.  
 
Domain associated term Domain feature concept 
???(football team) Football, Sports 
???? 
(cycling team) Traffic, Sports, cycling 
???? 
(Chinese chess) Sports, Chinese chess 
??(white side) Sports, the game of go 
????? 
(Chicago bulls) Sports, basketball 
 
Table 1: Six examples defined in the NEUKD 
 
In the domain knowledge based topic identi-
fication algorithm, all domain associated terms 
occurring in the given document are first 
mapped into domain features such as football, 
basketball or cycling. The most frequent do-
main feature is considered as the most likely 
topic. See Zhu and Chen (2005) for details. 
Two documents with the same topic can be 
grouped into the same cluster.  
 
 
Table 2: Examples of PND on Sportsman Class 
 
2.4.3 Multi-Stage Clustering Framework 
We proposed a multi-stage clustering frame-
work for PND on common person name class, 
as shown In Figure 1.  
In the multi-stage clustering framework, the 
first-stage is to adopt strict rule-based hard clus-
tering algorithm using the feature set of per-
sonal attributes. The second-stage is to imple-
ment constrained hierarchical agglomerative 
clustering using NE-type local features. The 
third-stage is to design hierarchical agglomera-
tive clustering using BOW-type global features. 
By combining those above techniques, we sub-
mitted the first system named NEU_1. 
2.4.4 The second system 
Besides, we also submitted another PND system 
named NEU_2 by using the single-link hierar-
chical agglomerative clustering algorithm in 
which the distance of two clusters is the cosine 
similarity of their most similar members (Ma-
saki et al, 2009, Duda et al, 2004). The differ-
ence between our two submission systems 
NEU_1 and NEU_2 is the feature weighting 
method. The motivation of feature weighting 
method used in NEU_2 is to assume that words 
surrounding the query person name in the given 
document are more important features than 
those far away from it, and person name and 
location names occurring in the context are 
more discriminative features than common 
words for PND purpose. Along this line, in the 
feature weighting scheme used in NEU_2, for 
each feature extracted from the sentence con-
taining the query person name, the weight of a 
word-type feature with the POS of ?ns?, ?ni? 
or ?nh ? is assigned  as 3, Otherwise 1.5; For 
the features extracted from other sentences, the 
weight of a word with the POS of ?ns?or ?nh ? 
is set to be 2, the ones of ?ni? POS is set to 1.5, 
otherwise 1.0. 
 
Algorithm 1: Multi-stage Clustering Framework 
Input: a person name pn, and its related document 
set D={d1, d2, ?, dm} in which each document di 
contains the person name pn; 
Output: clustering results C={C1,C2, ?,Cn}, where 
CCi =?i
 and ?=? ji CC  
For each di?D do 
 Si = {s|pn?s, s?di}; 
ORGi={t|t?s, s?Si, POS(t)= ni}; 
PERi={t|t?s, s?Si, POS(t)=nh} ; 
Ldi = {t|t?s, s?Si }; //local feature set 
Gdi = {t|t?di}; //global feature set 
Ci = {di} ; 
End for 
Stage 1: Strict rules-based clustering 
 Begin 
 For each Ci ? C do 
If ??? ji ORGORG or 
2?? ji PERPER  
Then Ci = Ci ?Cj;  
ORGi = ORGi?ORGj ; 
PERi = PERi?PERj ; 
Remove Cj from C ; 
End for 
End  
Stage 2: Constrained hierarchical agglomerative 
clustering algorithm using local features 
Begin  
         Set each c ?C as an initial cluster; 
 do  
),(maxarg],[
,
ji
CCC
ji CCsimCC
ji ?
=  
),cos(max
),(max),(
,
,
yxjyix
jyix
ddCdCd
yxCdCdji
LL
ddsimCCsim
??
??
=
=
 
Ci = Ci ?Cj; 
Remove Cj from C ; 
        until  sim(Ci,Cj) < ?. 
End 
Stage 3: Constrained hierarchical agglomerative 
clustering algorithm using global features, i.e., util-
ize the same algorithm used in stage 2 by consider-
ing the global feature set G for cosine-based similar-
ity calculation instead of the local feature set L. 
 
Figure 1: Multi-stage Clustering Framework 
Person name Document no. sports 
?? 081 ?? 
?? 094 ?? 
?? 098 ?? 
?? 100 ?? 
2.5 Final Result Generation 
As discussed above, there are many modules for 
PND on Chinese person name. In our NEU_1, 
the final results are produced by combining 
outputs of discard-class document clustering, 
journalist-class clustering, sportsman-class 
clustering and multi-stage clustering modules. 
In NEU-2 system, the outputs of discard-class 
document clustering, journalist-class clustering, 
sportsman-class clustering and single-link 
clustering modules are combined to generate 
the final results.  
3 Evaluation 
3.1 Experimental Settings 
 Training data: containing about 30 Chinese 
person names, and a set of about 100-300 
news articles are provided for each person 
name.  
 Test data: similar to the training data, and 
containing 26 unseen Chinese personal 
names, provided by the SIGHAN organizer.  
 Performance evaluation metrics (Artiles et 
al., 2009): B_Cubed and P_IP metrics. 
3.2 Results 
Table 3 shows the performance of our two 
submission systems NEU_1 and NEU_2 on the 
test set of Sighan2010 Chinese personal name 
disambiguation task. 
  
B_Cubed P_IP System 
No. P R F P IP F 
NEU_1 95.76 88.37 91.47 96.99 92.58 94.56 
NEU_2 95.08 88.62 91.15 96.73 92.73 94.46 
 
Table 3: Results on the test data 
 
NEU-1 system was implemented by the 
multi-stage clustering framework that uses sin-
gle-link clustering method. In this framework, 
there are two threshold parameters ? and ?. 
Both threshold parameters are tuned from train-
ing data sets.  
After the formal evaluation, the organizer 
provided a diagnosis test designed to explore 
the relationship between Chinese word segmen-
tation and personal name disambiguation. In the 
diagnosis test, the personal name disambigua-
tion task was simplified and limited to the 
documents in which the personal name is 
tagged correctly. The performance of our two 
systems on the diagnosis test set of Sighan2010 
Chinese personal name disambiguation task are 
shown in Table 4. 
 
B_Cubed P_IP System 
no. P R F  P IP F  
NEU_1 95.6 89.74 92.14 96.83 93.62 95.03 
NEU_2 94.53 89.99 91.66 96.41 93.8 94.9 
 
Table 4: Results of the diagnosis test on test 
data 
 
As shown in the Table 3 and Table 4, NEU-1 
system achieves the highest precision and F 
values on the test data and the diagnosis test 
data. 
3.3 Discussion 
We propose a multi-stage clustering framework 
for Chinese personal name disambiguation. The 
evaluation results demonstrate that the features 
and key techniques our systems adopt are effec-
tive. Our systems achieve the best performance 
in this competition. However, our recall values 
are not unsatisfactory. In such a case, there is 
still much room for improvement. Observed 
from experimental results, some interesting is-
sues are worth being discussed and addressed in 
our future work as follows: 
(1) For PND on some personal names, the 
document topic information seems not effective. 
For example, the personal name "?? (Guo 
Hua)" in training set represent one shooter and 
one billiards player. The PND system based on 
traditional clustering method can not effectively 
work in such a case due to the same sports topic. 
To solve this problem, one solution is to suffi-
ciently combine the personal attributes and 
document topic information for PND on this 
person name. 
(2) For the journalist-class personal names, 
global BOW-type features are not effective in 
this case as different persons can report on the 
same or similar events. For example, there are 
four different journalists named ????(Zhu 
Jianjun)? in the training set, involving different 
locations such as Beijing, Zhengzhou, Xining or 
Guangzhou. We can distinguish them in terms 
of the location they are working in.  
(3) We found that some documents in the 
training set only contain lists of news title and 
the news reporter. In this case, we can not dis-
criminate the persons with respect to the loca-
tion of entire news. It?s worth studying some 
effective solution to address this challenge in 
our future work.  
(4) Seen from the experimental results, some 
personal names such as ???(Li gang)? are 
wrong identified because this person is associ-
ated with multiple professional titles and affili-
ates. In this case, the use of exact matching 
methods can not yield satisfactory results. For 
example, the query name ???(Li gang)? in 
the documents 274 and 275 is the president of  
???????????(China International 
Culture Association)? while in the documents 
202, 225 and 228, he is the director of ????
???????(Bureau of External Cultural 
Relations of Chinese Ministry of Culture)?. To 
group both cases into the same cluster, it?s 
worth mining the relations and underlying se-
mantic relations between entities to achieve this 
goal.  
 
4 Conclusion 
This paper presents our two Chinese personal 
name disambiguation systems in which various 
constrained hierarchical agglomerative cluster-
ing algorithms using local or global features are 
adopted. The bakeoff results show that our sys-
tems achieve the best performance. In the future, 
we will pay more attention on the personal at-
tribute extraction and unsupervised learning 
approaches for Chinese personal name disam-
biguation.  
5 Acknowledgements 
This work was supported in part by the National 
Science Foundation of China (60873091) and 
the Fundamental Research Funds for the Cen-
tral Universities. 
 
References 
Artiles, Javier, Julio Gonzalo and Satoshi Sekine. 
2009. ?WePS 2 Evaluation Campaign: overview of 
the Web People Search Clustering Task,? In 2nd 
Web People Search Evaluation Workshop (WePS 
2009), 18th WWW Conference. 
Duda, Richard O., Peter E.Hart, and David G.Stork. 
2004. Pattern Classification. China Machine Press. 
Masaki, Ikeda, Shingo Ono, Issei Sato, Minoru Yo-
shida, and Hiroshi Nakagawa. 2009. Person Name 
Disambiguation on the Web by TwoStage Clustering. 
In 2nd Web People Search Evaluation Workshop 
(WePS 2009), 18th WWW Conference. 
Yao, Tianshun, Zhu Jingbo , Zhang Li, Yang Ying. 
Nov. 2002. Natural Language Processing , Second 
Edition, Tsinghua press. 
Zhu, Jingbo and Wenliang Chen. 2005. Some Stud-
ies on Chinese Domain Knowledge Dictionary and 
Its Application to Text Classification. In Proc. of 
SIGHAN4. 
 
 
 
