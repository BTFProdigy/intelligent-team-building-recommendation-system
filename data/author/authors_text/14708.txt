Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 697?704
Manchester, August 2008
Exploiting Constituent Dependencies for Tree Kernel-based Semantic 
Relation Extraction 
Longhua Qian   Guodong Zhou   Fang Kong   Qiaoming Zhu   Peide Qian 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology, Soochow University 
1 Shizi Street, Suzhou, China 215006 
{qianlonghua,gdzhou,kongfang,qmzhu,pdqian}@suda.edu.cn
Abstract
This paper proposes a new approach to 
dynamically determine the tree span for 
tree kernel-based semantic relation ex-
traction. It exploits constituent dependen-
cies to keep the nodes and their head 
children along the path connecting the 
two entities, while removing the noisy in-
formation from the syntactic parse tree, 
eventually leading to a dynamic syntactic 
parse tree. This paper also explores entity 
features and their combined features in a 
unified parse and semantic tree, which in-
tegrates both structured syntactic parse 
information and entity-related semantic 
information. Evaluation on the ACE 
RDC 2004 corpus shows that our dy-
namic syntactic parse tree outperforms all 
previous tree spans, and the composite 
kernel combining this tree kernel with a 
linear state-of-the-art feature-based ker-
nel, achieves the so far best performance. 
1 Introduction 
Information extraction is one of the key tasks in 
natural language processing. It attempts to iden-
tify relevant information from a large amount of 
natural language text documents. Of three sub-
tasks defined by the ACE program1, this paper 
focuses exclusively on Relation Detection and 
Characterization (RDC) task, which detects and 
classifies semantic relationships between prede-
fined types of entities in the ACE corpus. For 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
1 http://www.ldc.upenn.edu/Projects/ACE/ 
example, the sentence ?Microsoft Corp. is based 
in Redmond, WA? conveys the relation ?GPE-
AFF.Based? between ?Microsoft Corp.? [ORG] 
and ?Redmond? [GPE]. Due to limited accuracy 
in state-of-the-art syntactic and semantic parsing, 
reliably extracting semantic relationships be-
tween named entities in natural language docu-
ments is still a difficult, unresolved problem. 
In the literature, feature-based methods have 
dominated the research in semantic relation ex-
traction. Featured-based methods achieve prom-
ising performance and competitive efficiency by 
transforming a relation example into a set of syn-
tactic and semantic features, such as lexical 
knowledge, entity-related information, syntactic 
parse trees and deep semantic information. How-
ever, detailed research (Zhou et al, 2005) shows 
that it?s difficult to extract new effective features 
to further improve the extraction accuracy. 
Therefore, researchers turn to kernel-based 
methods, which avoids the burden of feature en-
gineering through computing the similarity of 
two discrete objects (e.g. parse trees) directly. 
From prior work (Zelenko et al, 2003; Culotta 
and Sorensen, 2004; Bunescu and Mooney, 2005) 
to current research (Zhang et al, 2006; Zhou et 
al., 2007), kernel methods have been showing 
more and more potential in relation extraction. 
The key problem for kernel methods on rela-
tion extraction is how to represent and capture 
the structured syntactic information inherent in 
relation instances. While kernel methods using 
the dependency tree (Culotta and Sorensen, 2004) 
and the shortest dependency path (Bunescu and 
Mooney, 2005) suffer from low recall perform-
ance, convolution tree kernels (Zhang et al, 2006; 
Zhou et al, 2007) over syntactic parse trees 
achieve comparable or even better performance 
than feature-based methods. 
However, there still exist two problems re-
garding currently widely used tree spans. Zhang 
et al (2006) discover that the Shortest Path-
697
enclosed Tree (SPT) achieves the best perform-
ance. Zhou et al (2007) further extend it to Con-
text-Sensitive Shortest Path-enclosed Tree (CS-
SPT), which dynamically includes necessary 
predicate-linked path information. One problem 
with both SPT and CS-SPT is that they may still 
contain unnecessary information. The other prob-
lem is that a considerable number of useful con-
text-sensitive information is also missing from 
SPT/CS-SPT, although CS-SPT includes some 
contextual information relating to predicate-
linked path. 
This paper proposes a new approach to dy-
namically determine the tree span for relation 
extraction by exploiting constituent dependencies 
to remove the noisy information, as well as keep 
the necessary information in the parse tree. Our 
motivation is to integrate dependency informa-
tion, which has been proven very useful to rela-
tion extraction, with the structured syntactic in-
formation to construct a concise and effective 
tree span specifically targeted for relation extrac-
tion. Moreover, we also explore interesting com-
bined entity features for relation extraction via a 
unified parse and semantic tree. 
The other sections in this paper are organized 
as follows. Previous work is first reviewed in 
Section 2. Then, Section 3 proposes a dynamic 
syntactic parse tree while the entity-related se-
mantic tree is described in Section 4. Evaluation 
on the ACE RDC corpus is given in Section 5. 
Finally, we conclude our work in Section 6. 
2 Related Work 
Due to space limitation, here we only review 
kernel-based methods used in relation extraction. 
For those interested in feature-based methods, 
please refer to Zhou et al (2005) for more details. 
Zelenko et al (2003) described a kernel be-
tween shallow parse trees to extract semantic 
relations, where a relation instance is trans-
formed into the least common sub-tree connect-
ing the two entity nodes. The kernel matches the 
nodes of two corresponding sub-trees from roots 
to leaf nodes recursively layer by layer in a top-
down manner. Their method shows successful 
results on two simple extraction tasks. Culotta 
and Sorensen (2004) proposed a slightly general-
ized version of this kernel between dependency 
trees, in which a successful match of two relation 
instances requires the nodes to be at the same 
layer and in the identical path starting from the 
roots to the current nodes. These strong con-
straints make their kernel yield high precision but 
very low recall on the ACE RDC 2003 corpus. 
Bunescu and Mooney (2005) develop a shortest 
path dependency tree kernel, which simply 
counts the number of common word classes at 
each node in the shortest paths between two enti-
ties in dependency trees. Similar to Culotta and 
Sorensen (2004), this method also suffers from 
high precision but low recall.
Zhang et al (2006) describe a convolution tree 
kernel (CTK, Collins and Duffy, 2001) to inves-
tigate various structured information for relation 
extraction and find that the Shortest Path-
enclosed Tree (SPT) achieves the F-measure of 
67.7 on the 7 relation types of the ACE RDC 
2004 corpus. One problem with SPT is that it 
loses the contextual information outside SPT, 
which is usually critical for relation extraction. 
Zhou et al (2007) point out that both SPT and 
the convolution tree kernel are context-free. They 
expand SPT to CS-SPT by dynamically includ-
ing necessary predicate-linked path information 
and extending the standard CTK to context-
sensitive CTK, obtaining the F-measure of 73.2 
on the 7 relation types of the ACE RDC 2004 
corpus. However, the CS-SPT only recovers part 
of contextual information and may contain noisy 
information as much as SPT. 
In order to fully utilize the advantages of fea-
ture-based methods and kernel-based methods, 
researchers turn to composite kernel methods. 
Zhao and Grishman (2005) define several fea-
ture-based composite kernels to capture diverse 
linguistic knowledge and achieve the F-measure 
of 70.4 on the 7 relation types in the ACE RDC 
2004 corpus. Zhang et al (2006) design a com-
posite kernel consisting of an entity linear kernel 
and a standard CTK, obtaining the F-measure of 
72.1 on the 7 relation types in the ACE RDC 
2004 corpus. Zhou et al (2007) describe a com-
posite kernel to integrate a context-sensitive 
CTK and a state-of-the-art linear kernel. It 
achieves the so far best F-measure of 75.8 on the 
7 relation types in the ACE RDC 2004 corpus. 
In this paper, we will further study how to dy-
namically determine a concise and effective tree 
span for a relation instance by exploiting con-
stituent dependencies inherent in the parse tree 
derivation. We also attempt to fully capture both 
the structured syntactic parse information and 
entity-related semantic information, especially 
combined entity features, via a unified parse and 
semantic tree. Finally, we validate the effective-
ness of a composite kernel for relation extraction, 
which combines a tree kernel and a linear kernel. 
698
3 Dynamic Syntactic Parse Tree 
This section discusses how to generate dynamic 
syntactic parse tree by employing constituent 
dependencies to overcome the problems existing 
in currently used tree spans. 
3.1 Constituent Dependencies in Parse Tree 
Zhang et al (2006) explore five kinds of tree 
spans and find that the Shortest Path-enclosed 
Tree (SPT) achieves the best performance. Zhou 
et al (2007) further propose Context-Sensitive 
SPT (CS-SPT), which can dynamically deter-
mine the tree span by extending the necessary 
predicate-linked path information outside SPT. 
However, the key problem of how to represent 
the structured syntactic parse tree is still partially 
resolved. As we indicate as follows, current tree 
spans suffer from two problems: 
(1) Both SPT and CS-SPT still contain unnec-
essary information. For example, in the sentence 
??bought one of town?s two meat-packing
plants?, the condensed information ?one of 
plants? is sufficient to determine ?DISC? rela-
tionship between the entities ?one? [FAC] and 
?plants? [FAC], while SPT/CS-SPT include the 
redundant underlined part. Therefore more un-
necessary information can be safely removed 
from SPT/CS-SPT. 
(2) CS-SPT only captures part of context-
sensitive information relating to predicate-linked 
structure (Zhou et al, 2007) and still loses much 
context-sensitive information. Let?s take the 
same example sentence ??bought one of town?s
two meat-packing plants?, where indeed there is 
no relationship between the entities ?one? [FAC] 
and ?town? [GPE]. Nevertheless, the information 
contained in SPT/CS-SPT (?one of town?) may 
easily lead to their relationship being misclassi-
fied as ?DISC?, which is beyond our expectation. 
Therefore the underlined part outside SPT/CS-
SPT should be recovered so as to differentiate it 
from positive instances. 
Since dependency plays a key role in many 
NLP problems such as syntactic parsing, seman-
tic role labeling as well as semantic relation ex-
traction, our motivation is to exploit dependency 
knowledge to distinguish the necessary evidence 
from the unnecessary information in the struc-
tured syntactic parse tree.  
On one hand, lexical or word-word depend-
ency indicates the relationship among words 
occurring in the same sentence, e.g. predicate-
argument dependency means that arguments are 
dependent on their target predicates, modifier-
head dependency means that modifiers are de-
pendent on their head words. This dependency 
relationship offers a very condensed representa-
tion of the information needed to assess the rela-
tionship in the forms of the dependency tree (Cu-
lotta and Sorensen, 2004) or the shortest depend-
ency path (Bunescu and Mooney, 2005) that in-
cludes both entities.
On the other hand, when the parse tree corre-
sponding to the sentence is derived using deriva-
tion rules from the bottom to the top, the word-
word dependencies extend upward, making a 
unique head child containing the head word for 
every non-terminal constituent. As indicated as 
follows, each CFG rule has the form: 
P? Ln?L1H R1?Rm
Here, P is the parent node, H is the head child of 
the rule, Ln?L1 and R1?Rm are left and right 
modifiers of H respectively, and both n and m
may be zero. In other words, the parent node P
depends on the head child H, this is what we call 
constituent dependency. Vice versa, we can also 
determine the head child of a constituent in terms 
of constituent dependency. Our hypothesis stipu-
lates that the contribution of the parse tree to es-
tablishing a relationship is almost exclusively 
concentrated in the path connecting the two enti-
ties, as well as the head children of constituent 
nodes along this path. 
3.2 Generation of Dynamic Syntactic Parse 
Tree
Starting from the Minimum Complete Tree 
(MCT, the complete sub-tree rooted by the near-
est common ancestor of the two entities under 
consideration) as the representation of each rela-
tion instance, along the path connecting two enti-
ties, the head child of every node is found ac-
cording to various constituent dependencies. 
Then the path nodes and their head children are 
kept while any other nodes are removed from the 
tree. Eventually we arrive at a tree called Dy-
namic Syntactic Parse Tree (DSPT), which is 
dynamically determined by constituent depend-
encies and only contains necessary information 
as expected. 
There exist a considerable number of constitu-
ent dependencies in CFG as described by Collins 
(2003). However, since our task is to extract the 
relationship between two named entities, our fo-
cus is on how to condense Noun-Phrases (NPs) 
and other useful constituents for relation extrac-
tion. Therefore constituent dependencies can be 
classified according to constituent types of the 
CFG rules: 
699
(1) Modification within base-NPs: base-NPs 
mean that they do not directly dominate an NP
themselves, unless the dominated NP is a posses-
sive NP. The noun phrase right above the entity
headword, whose mention type is nominal or 
name, can be categorized into this type. In this 
case, the entity headword is also the headword of 
the noun phrase, thus all the constituents before 
the headword are dependent on the headword,
and may be removed from the parse tree, while 
the headword and the constituents right after the 
headword remain unchanged. For example, in the 
sentence ??bought one of town?s two meat-
packing plants? as illustrated in Figure 1(a), the
constituents before the headword  ?plants? can 
be removed from the parse tree. In this way the
parse tree ?one of plants? could capture the
?DISC? relationship more concisely and pre-
cisely. Another interesting example is shown in 
Figure 1(b), where the base-NP of the second
entity ?town? is a possessive NP and there is no 
relationship between the entities ?one? and
?town? defined in the ACE corpus. For both SPT
and CS-SPT, this example would be condensed 
to ?one of town? and therefore easily misclassi-
fied as the ?DISC? relationship between the two 
entities. In the contrast, our DSPT can avoid this 
problem by keeping the constituent ??s? and the 
headword ?plants?.
(2) Modification to NPs: except base-NPs,
other modification to NPs can be classified into 
this type. Usually these NPs are recursive, mean-
ing that they contain another NP as their child. 
The CFG rules corresponding to these modifica-
tions may have the following forms:
NP? NP SBAR [relative clause]
NP? NP VP [reduced relative]
NP? NP PP [PP attachment]
Here, the NPs in bold mean that the path con-
necting the two entities passes through them. For
every right hand side, the NP in bold is modified
by the constituent following them. That is, the 
latter is dependent on the former, and may be 
reduced to a single NP. In Figure 1(c) we show a
sentence ?one of about 500 people nominated
for ??, where there exists a ?DISC? relationship
between the entities ?one? and ?people?. Since 
the reduced relative ?nominated for ?? modifies
and is therefore dependent on the ?people?, they 
can be removed from the parse tree, that is, the 
right side (?NP VP?) can be reduced to the left 
hand side, which is exactly a single NP. 
(a) Removal of constituents before the headword in base-NP
(b) Keeping of constituents after the headword in base-NP
NN
one
IN
of
DT
the
NN
town
POS
's
E-FAC
NN
plantstwo
CD NN
one
IN
of
NN
town
POS
's
E-FAC
NN
plantsmeat-packing
JJ
NN
one
PP
IN
of
NP
DT
the
NN
town
POS
's
NN
plantstwo
CD NN
one
IN
of
NN
plantsmeat-packing
JJ NN
one
IN
of
RB
about
QP
CD
500
NNS
people
...
nominated
VBN
for
IN
VP
PP
...
E2-PER
NN
one
IN
of
NNS
people
NN
property
PRP
he
VP
VBZ IN
in
NP
PP
state
NNS
the
NP
JJ
rental
S
owns
DT NN
property
PRP
he
VP
VBZ
owns
governors from connecticut
NNS IN
NP
E-GPE
NNP
,
,
south
NP
E-GPE
NNP
dakota
NNP
,
,
and
CC
montana
NNP
governors from
NNS IN
montana
NNP
(c) Reduction of modification to NP
(d) Removal of arguments to verb
(e) Reduction of conjuncts for NP coordination
E-GPE
NPPP
E1-FAC
NP
E2-FAC
NP
E1-FAC
NP
NP
NP
NP
E2-FAC E1-PER
NP
NP
PP
NP
NP
NP
E1-PER
PP
NP
E2-PER
NP
SBAR
E2-PER
S
NPNP
E1-FAC
PP
NP
NP
E1-PER
NP
E2-GPE
NP
E1-PER
PP
NP
NP
NP
E2-GPE
NP
E1-FAC E2-PER
NP
NP
SBAR
NP
NP
E1-FAC
PP
NP
NP
E2-GPE
NP
NP
E1-PER
PP
NP
NP
E2-GPE
Figure 1. Removal and reduction of constituents using dependencies 
700
(3) Arguments/adjuncts to verbs: this type 
includes the CFG rules in which the left side in-
cludes S, SBAR or VP. An argument represents
the subject or object of a verb, while an adjunct
indicates the location, date/time or way of the
action corresponding to the verb. They depend
on the verb and can be removed if they are not
included in the path connecting the two entities.
However, when the parent tag is S or SBAR, and
its child VP is not included in the path, this VP
should be recovered to indicate the predicate
verb. Figure 1(d) shows a sentence ?? maintain
rental property he owns in the state?, where the
?ART.User-or-Owner? relation holds between 
the entities ?property? and ?he?. While PP can be
removed from the rule  (?VP? VBZ PP?), the 
VP should be kept in the rule (?S? NP VP?).
Consequently, the tree span looks more concise 
and precise for relation extraction. 
(4) Coordination conjunctions: In coordina-
tion constructions, several peer conjuncts may be 
reduced into a single constituent. Although the
first conjunct is always considered as the head-
word (Collins, 2003), actually all the conjuncts
play an equal role in relation extraction. As illus-
trated in Figure 1(e), the NP coordination in the 
sentence (?governors from connecticut, south
dakota, and montana?) can be reduced to a single 
NP (?governors from montana?) by keeping the
conjunct in the path while removing the other 
conjuncts.
(5) Modification to other constituents: ex-
cept for the above four types, other CFG rules 
fall into this type, such as modification to PP,
ADVP and PRN etc. These cases are similar to 
arguments/adjuncts to verbs, but less frequent 
than them, so we will not detail this scenario. 
In fact, SPT (Zhang et al, 2006) can be ar-
rived at by carrying out part of the above re-
moval operations using a single rule (i.e. all the 
constituents outside the linking path should be
removed) and CS-CSPT (Zhou et al, 2007) fur-
ther recovers part of necessary context-sensitive 
information outside SPT, this justifies that SPT
performs well, while CS-SPT outperforms SPT. 
4 Entity-related Semantic Tree 
Entity semantic features, such as entity headword, 
entity type and subtype etc., impose a strong
constraint on relation types in terms of relation
definition by the ACE RDC task. Experiments by
Zhang et al (2006) show that linear kernel using 
only entity features contributes much when com-
bined with the convolution parse tree kernel. 
Qian et al (2007) further indicates that among
these entity features, entity type, subtype, and 
mention type, as well as the base form of predi-
cate verb, contribute most while the contribution
of other features, such as entity class, headword 
and GPE role, can be ignored. 
In order to effectively capture entity-related
semantic features, and their combined features as
well, especially bi-gram or tri-gram features, we 
build an Entity-related Semantic Tree (EST) in 
three ways as illustrated in Figure 2. In the ex-
ample sentence ?they ?re here?, which is ex-
cerpted from the ACE RDC 2004 corpus, there 
exists a relationship ?Physical.Located? between
the entities ?they? [PER] and ?here?
[GPE.Population-Center]. The features are en-
coded as ?TP?, ?ST?, ?MT? and ?PVB?, which
denote type, subtype, mention-type of the two 
entities, and the base form of predicate verb if 
existing (nearest to the 2nd entity along the path 
connecting the two entities) respectively. For 
example, the tag ?TP1? represents the type of the 
1st entity, and the tag ?ST2? represents the sub-
type of the 2nd entity. The three entity-related
semantic tree setups are depicted as follows: 
TP2TP1
(a) Bag Of Features(BOF)
ENT
ST2ST1 MT2MT1 PVB
(c) Entity-Paired Tree(EPT)
ENT
E1 E2
(b) Feature Paired Tree(FPT)
ENT
TP ST MT
ST1TP1 MT1 TP2 ST2 MT2
PVB
TP1 TP2 ST1 ST2 MT1 MT2
PVB
PER null PRO GPE Pop. PRO be
PER null PRO GPE Pop. PRO
be
PER GPE null Pop. PRO PRO
be
Figure 2. Different setups for entity-related se-
mantic tree (EST) 
(a) Bag of Features (BOF, e.g. Fig. 2(a)): all 
feature nodes uniformly hang under the root node,
so the tree kernel simply counts the number of 
common features between two relation instances.
This tree setup is similar to linear entity kernel
explored by Zhang et al (2006). 
(b) Feature-Paired Tree (FPT, e.g. Fig. 2(b)): 
the features of two entities are grouped into dif-
ferent types according to their feature names, e.g.
?TP1? and ?TP2? are grouped to ?TP?. This tree 
setup is aimed to capture the additional similarity
701
of the single feature combined from different 
entities, i.e., the first and the second entities. 
(c) Entity-Paired Tree (EPT, e.g. Fig. 2(c)): all 
the features relating to an entity are grouped to 
nodes ?E1? or ?E2?, thus this tree kernel can fur-
ther explore the equivalence of combined entity 
features only relating to one of the entities be-
tween two relation instances. 
In fact, the BOF only captures the individual 
entity features, while the FPT/EPT can addition-
ally capture the bi-gram/tri-gram features respec-
tively. 
Rather than constructing a composite kernel, 
we incorporate the EST into the DSPT to pro-
duce a Unified Parse and Semantic Tree (UPST) 
to investigate the contribution of the EST to rela-
tion extraction. The entity features can be at-
tached under the top node, the entity nodes, or 
directly combined with the entity nodes as in 
Figure 1. However, detailed evaluation (Qian et 
al., 2007) indicates that the UPST achieves the 
best performance when the feature nodes are at-
tached under the top node. Hence, we also attach 
three kinds of entity-related semantic trees (i.e. 
BOF, FPT and EPT) under the top node of the 
DSPT right after its original children. Thereafter, 
we employ the standard CTK (Collins and Duffy, 
2001) to compute the similarity between two 
UPSTs, since this CTK and its variations are 
successfully applied in syntactic parsing, seman-
tic role labeling (Moschitti, 2004) and relation 
extraction (Zhang et al, 2006; Zhou et al, 2007) 
as well. 
5 Experimentation 
This section will evaluate the effectiveness of the 
DSPT and the contribution of entity-related se-
mantic information through experiments. 
5.1 Experimental Setting  
For evaluation, we use the ACE RDC 2004 cor-
pus as the benchmark data. This data set contains 
451 documents and 5702 relation instances. It 
defines 7 entity types, 7 major relation types and 
23 subtypes. For comparison with previous work, 
evaluation is done on 347 (nwire/bnews) docu-
ments and 4307 relation instances using 5-fold 
cross-validation. Here, the corpus is parsed using 
Charniak?s parser (Charniak, 2001) and relation 
instances are generated by iterating over all pairs 
of entity mentions occurring in the same sentence 
with given ?true? mentions and coreferential in-
formation. In our experimentations, SVMlight
(Joachims, 1998) with the tree kernel function
(Moschitti, 2004) 2  is selected as our classifier. 
For efficiency, we apply the one vs. others
strategy, which builds K classifiers so as to 
separate one class from all others. For 
comparison purposes, the training parameters C 
(SVM) and ? (tree kernel) are also set to 2.4 and 
0.4 respectively. 
5.2 Experimental Results 
Table 1 evaluates the contributions of different 
kinds of constituent dependencies to extraction 
performance on the 7 relation types of the ACE 
RDC 2004 corpus using the convolution parse 
tree kernel as depicted in Figure 1. The MCT 
with only entity-type information is first used as 
the baseline, and various constituent dependen-
cies are then applied sequentially to dynamically 
reshaping the tree in two different modes: 
--[M1] Respective:  every constituent depend-
ency is individually applied on MCT. 
--[M2] Accumulative: every constituent de-
pendency is incrementally applied on the previ-
ously derived tree span, which begins with the 
MCT and eventually gives rise to a Dynamic 
Syntactic Parse Tree (DSPT).  
Dependency types P(%) R(%) F
MCT (baseline) 75.1 53.8 62.7
Modification within 
base-NPs
76.5
(59.8)
59.8
(59.8)
67.1
(67.1)
Modification to NPs 
77.0
(76.2)
63.2
(56.9)
69.4
(65.1)
Arguments/adjuncts to verb 
77.1
(76.1)
63.9
(57.5)
69.9
(65.5)
Coordination conjunctions 
77.3
(77.3)
65.2
(55.1)
70.8
(63.8)
Other modifications 
77.4
(75.0)
65.4
(53.7)
70.9
(62.6)
Table 1. Contribution of constituent dependen-
cies in respective mode (inside parentheses) and 
accumulative mode (outside parentheses) 
The table shows that the final DSPT achieves 
the best performance of 77.4%/65.4%/70.9 in
precision/recall/F-measure respectively after ap-
plying all the dependencies, with the increase of 
F-measure by 8.2 units compared to the baseline 
MCT. This indicates that reshaping the tree by 
exploiting constituent dependencies may signifi-
cantly improve extraction accuracy largely due to 
the increase in recall. It further suggests that con-
stituent dependencies knowledge is very effec-
2 http://ai-nlp.info.uniroma2.it/moschitti/ 
702
tive and can be fully utilized in tree kernel-based 
relation extraction. This table also shows that: 
(1) Both modification within base-NPs and 
modification to NPs contribute much to perform-
ance improvement, acquiring the increase of F-
measure by 4.4/2.4 units in mode M1 and 4.4/2.3 
units in mode M2 respectively. This indicates the 
local characteristic of semantic relations, which 
can be effectively captured by NPs near the two 
involved entities in the DSPT. 
(2) All the other three dependencies show mi-
nor contribution to performance enhancement, 
they improve the F-measure only by 2.8/0.9/-0.1 
units in mode M1 and 0.5/0.9/0.1 units in mode 
M2. This may be due to the reason that these de-
pendencies only remove the nodes far from the 
two entities. 
We compare in Table 2 the performance of 
Unified Parse and Semantic Trees with different 
kinds of Entity Semantic Tree setups using stan-
dard convolution tree kernel, while the SPT and 
DSPT with only entity-type information are 
listed for reference. It shows that: 
(1) All the three unified parse and semantic 
tree kernels significantly outperform the DSPT 
kernel, obtaining an average increase of ~4 units 
in F-measure. This means that they can effec-
tively capture both the structured syntactic in-
formation and the entity-related semantic fea-
tures.
(2) The Unified Parse and Semantic Tree with 
Feature-Paired Tree achieves the best perform-
ance of 80.1/70.7/75.1 in P/R/F respectively, 
with an increase of F-measure by 0.4/0.3 units 
over BOF and EPT respectively. This suggests 
that additional bi-gram entity features captured 
by FPT are more useful than tri-gram entity fea-
tures captured by EPT. 
Tree setups P(%) R(%) F
SPT 76.3 59.8 67.1
DSPT 77.4 65.4 70.9
UPST (BOF) 80.4 69.7 74.7
UPST (FPT) 80.1 70.7 75.1
UPST (EPT) 79.9 70.2 74.8
Table 2. Performance of Unified Parse and 
Semantic Trees (UPSTs) on the 7 relation types 
of the ACE RDC 2004 corpus 
In Table 3 we summarize the improvements of 
different tree setups over SPT. It shows that in a 
similar setting, our DSPT outperforms SPT by 
3.8 units in F-measure, while CS-SPT outper-
forms SPT by 1.3 units in F-measure. This sug-
gests that the DSPT performs best among these 
tree spans. It also shows that the Unified Parse 
and Semantic Tree with Feature-Paired Tree per-
form significantly better than the other two tree 
setups (i.e., CS-SPT and DSPT) by 6.7/4.2 units 
in F-measure respectively. This implies that the 
entity-related semantic information is very useful 
and contributes much when they are incorporated 
into the parse tree for relation extraction. 
Tree setups P(%) R(%) F
CS-SPT over SPT3 1.5   1.1 1.3
DSPT over SPT 1.1   5.6 3.8
UPST (FPT) over SPT 3.8 10.9 8.0
Table 3. Improvements of different tree setups 
over SPT on the ACE RDC 2004 corpus 
Finally, Table 4 compares our system with 
other state-of-the-art kernel-based systems on the 
7 relation types of the ACE RDC 2004 corpus. It 
shows that our UPST outperforms all previous 
tree setups using one single kernel, and even bet-
ter than two previous composite kernels (Zhang 
et al, 2006; Zhao and Grishman, 2005). Fur-
thermore, when the UPST (FPT) kernel is com-
bined with a linear state-of-the-state feature-
based kernel (Zhou et al, 2005) into a composite 
one via polynomial interpolation in a setting 
similar to Zhou et al (2007) (i.e. polynomial de-
gree d=2 and coefficient ?=0.3), we get the so far 
best performance of 77.1 in F-measure for 7 rela-
tion types on the ACE RDC 2004 data set. 
Systems P(%) R(%) F
Ours:
composite kernel 
83.0 72.0 77.1
Zhou et al, (2007):
composite kernel 
82.2 70.2 75.8
Zhang et al, (2006):
composite kernel 
76.1 68.4 72.1
Zhao and Grishman, (2005):4
composite kernel 
69.2 70.5 70.4
Ours:
CTK with UPST 
80.1 70.7 75.1
Zhou et al, (2007): context-
sensitive CTK with CS-SPT 
81.1 66.7 73.2
Zhang et al, (2006):
CTK with SPT 
74.1 62.4 67.7
Table 4. Comparison of different systems on 
the ACE RDC 2004 corpus 
3  We arrive at these values by subtracting P/R/F 
(79.6/5.6/71.9) of Shortest-enclosed Path Tree from P/R/F  
(81.1/6.7/73.2) of Dynamic Context-Sensitive Shortest-
enclosed Path Tree according to Table 2 (Zhou et al, 2007) 
4 There might be some typing errors for the performance 
reported in Zhao and Grishman (2005) since P, R and F do 
not match. 
703
6 Conclusion
This paper further explores the potential of struc-
tured syntactic information for tree kernel-based 
relation extraction, and proposes a new approach 
to dynamically determine the tree span (DSPT) 
for relation instances by exploiting constituent 
dependencies. We also investigate different ways 
of how entity-related semantic features and their 
combined features can be effectively captured in 
a Unified Parse and Semantic Tree (UPST). 
Evaluation on the ACE RDC 2004 corpus shows 
that our DSPT is appropriate for structured repre-
sentation of relation instances. We also find that, 
in addition to individual entity features, com-
bined entity features (especially bi-gram) con-
tribute much when they are combined with a 
DPST into a UPST. And the composite kernel, 
combining the UPST kernel and a linear state-of-
the-art kernel, yields the so far best performance. 
For the future work, we will focus on improv-
ing performance of complex structured parse 
trees, where the path connecting the two entities 
involved in a relationship is too long for current 
kernel methods to take effect. Our preliminary 
experiment of applying certain discourse theory 
exhibits certain positive results.
Acknowledgements 
This research is supported by Project 60673041 
under the National Natural Science Foundation 
of China, Project 2006AA01Z147 under the 
?863? National High-Tech Research and Devel-
opment of China, and the National Research 
Foundation for the Doctoral Program of Higher 
Education of China under Grant No. 
20060285008. We would also like to thank the 
excellent and insightful comments from the three 
anonymous reviewers. 
References
Bunescu, Razvan C. and Raymond J. Mooney. 2005. 
A Shortest Path Dependency Kernel for Relation 
Extraction. In Proceedings of the Human Language 
Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP-2005), pages 724-731. Vancover, B.C. 
Charniak, Eugene. 2001. Intermediate-head Parsing 
for Language Models. In Proceedings of the 39th 
Annual Meeting of the Association of Computa-
tional Linguistics (ACL-2001), pages 116-123. 
Collins, Michael. 2003. Head-Driven Statistics Mod-
els for Natural Language Parsing. Computational 
linguistics, 29(4): 589-617. 
Collins, Michael and Nigel Duffy. 2001. Convolution 
Kernels for Natural Language. In Proceedings of 
Neural Information Processing Systems (NIPS-
2001), pages 625-632. Cambridge, MA. 
Culotta, Aron and Jeffrey Sorensen. 2004. Depend-
ency tree kernels for relation extraction. In Pro-
ceedings of the 42nd Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL-2004),
pages 423-439. Barcelona, Spain. 
Joachims, Thorsten. 1998. Text Categorization with 
Support Vector Machine: learning with many rele-
vant features. In Proceedings of the 10th European 
Conference on Machine Learning (ECML-1998),
pages 137-142. Chemnitz, Germany. 
Moschitti, Alessandro. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. In Proceed-
ings of the 42nd Annual Meeting of the Association 
of Computational Linguistics (ACL-2004). Barce-
lona, Spain. 
Qian, Longhua, Guodong Zhou, Qiaoming Zhu and 
Peide Qian. 2007. Relation Extraction using Con-
volution Tree Kernel Expanded with Entity Fea-
tures. In Proceedings of the 21st Pacific Asian 
Conference on Language, Information and Compu-
tation (PACLIC-21), pages 415-421. Seoul, Korea. 
Zelenko, Dmitry, Chinatsu Aone and Anthony Rich-
ardella. 2003. Kernel Methods for Relation Extrac-
tion. Journal of Machine Learning Research, 
3(2003): 1083-1106. 
Zhang, Min, Jie Zhang, Jian Su and Guodong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of the 21st International 
Conference on Computational Linguistics and the 
44th Annual Meeting of the Association of Compu-
tational Linguistics (COLING/ACL-2006), pages 
825-832. Sydney, Australia. 
Zhao, Shubin and Ralph Grishman. 2005. Extracting 
relations with integrated information using kernel 
methods. In Proceedings of the 43rd Annual Meet-
ing of the Association of Computational Linguistics 
(ACL-2005), pages 419-426. Ann Arbor, USA. 
Zhou, Guodong, Jian Su, Jie Zhang and Min Zhang. 
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meet-
ing of the Association of Computational Linguistics 
(ACL-2005), pages 427-434. Ann Arbor, USA. 
Zhou, Guodong, Min Zhang, Donghong Ji and 
Qiaoming Zhu. 2007. Tree Kernel-based Relation 
Extraction with Context-Sensitive Structured Parse 
Tree Information. In Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural 
Language Learning (EMNLP/CoNLL-2007), pages 
728-736. Prague, Czech. 
704
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 728?736, Prague, June 2007. c?2007 Association for Computational Linguistics
Tree Kernel-based Relation Extraction  
with Context-Sensitive Structured Parse Tree Information 
  
    GuoDong ZHOU12     Min ZHANG 2     Dong Hong JI 2     QiaoMing ZHU 1 
         1School of Computer Science & Technology         2  Institute for Infocomm Research 
                      Soochow Univ.                                              Heng Mui Keng Terrace 
         Suzhou, China 215006                                           Singapore 119613 
                 Email: {gdzhou,qmzhu}@suda.edu.cn      Email: {zhougd, mzhang, dhji}@i2r.a-star.edu.sg 
  
Abstract 
This paper proposes a tree kernel with context-
sensitive structured parse tree information for re-
lation extraction. It resolves two critical problems 
in previous tree kernels for relation extraction in 
two ways. First, it automatically determines a dy-
namic context-sensitive tree span for relation ex-
traction by extending the widely-used Shortest 
Path-enclosed Tree (SPT) to include necessary 
context information outside SPT. Second, it pro-
poses a context-sensitive convolution tree kernel, 
which enumerates both context-free and context-
sensitive sub-trees by considering their  ancestor 
node paths as their contexts. Moreover, this paper 
evaluates the complementary nature between our 
tree kernel and a state-of-the-art linear kernel. 
Evaluation on the ACE RDC corpora shows that 
our dynamic context-sensitive tree span is much 
more suitable for relation extraction than SPT and 
our tree kernel outperforms the state-of-the-art 
Collins and Duffy?s convolution tree kernel. It 
also shows that our tree kernel achieves much bet-
ter performance than the state-of-the-art linear 
kernels . Finally, it shows that feature-based and 
tree kernel-based methods much complement each 
other and the composite kernel can well integrate 
both flat and structured features.  
1 Introduction 
Relation extraction is to find various predefined se-
mantic relations between pairs of entities in text. The 
research in relation extraction has been promoted by 
the Message Understanding Conferences (MUCs) 
(MUC, 1987-1998) and the NIST Automatic Content 
Extraction (ACE) program (ACE, 2002-2005). Ac-
cording to the ACE Program, an entity is an object or 
a set of objects in the world and a relation is an ex-
plicitly or implicitly stated relationship among enti-
ties. For example, the sentence ?Bill Gates is the 
chairman and chief software architect of Microsoft 
Corporation.? conveys the ACE-style relation 
?EMPLOYMENT.exec? between the entities ?Bill 
Gates? (person name) and ?Microsoft Corporation? 
(organization name). Extraction of semantic relations 
between entities can be very useful in many applica-
tions such as question answering, e.g. to answer the 
query ?Who is the president of the United States??, 
and information  retrieval, e.g. to expand the query 
?George W. Bush? with ?the president of the United 
States? via his relationship with ?the United States?. 
Many researches have been done in relation extrac-
tion. Among them, feature-based methods (Kamb-
hatla 2004; Zhou et al, 2005) achieve certain success 
by employing a large amount of diverse linguistic 
features, varying from lexical knowledge, entity-
related information to syntactic parse trees, depend-
ency trees and semantic information. However, it is 
difficult for them to effectively capture structured 
parse tree information (Zhou et al2005), which is 
critical for further performance improvement in rela-
tion extraction.  
As an alternative to feature-based methods, tree 
kernel-based methods provide an elegant solution to 
explore implicitly structured features by directly 
computing the similarity between two trees. Although 
earlier researches (Zelenko et al2003; Culotta and 
Sorensen 2004; Bunescu and Mooney 2005a) only 
achieve success on simple tasks and fail on complex 
tasks, such as the ACE RDC task, tree kernel-based 
methods achieve much progress recently. As the 
state-of-the-art, Zhang et al(2006) applied the convo-
lution tree kernel (Collins and Duffy 2001) and 
achieved comparable performance with a state-of-the-
art linear kernel (Zhou et al2005) on the 5 relation  
types in the ACE RDC 2003 corpus.  
However, there are two problems in Collins and 
Duffy?s convolution tree kernel for relation extraction.  
The first is that the sub-trees enumerated in the tree 
kernel computation are context-free. That is, each 
sub-tree enumerated in the tree kernel computation 
728
does not consider the context information outside the 
sub-tree. The second is to decide a proper tree span in 
relation extraction. Zhang et al(2006) explored five 
tree spans in relation extraction and it was  a bit sur-
prising to find that the Shortest Path-enclosed Tree 
(SPT, i.e. the sub-tree enclosed by the shortest path 
linking two involved entities in the parse tree) per-
formed best. This is contrast to our intuition. For ex-
ample, ?got married? is critical to determine the 
relationship between ?John? and ?Mary? in the sen-
tence ?John and Mary got married? ? as shown in 
Figure 1(e). It is obvious that the information con-
tained in SPT (?John and Marry?) is not enough to 
determine their relationship. 
This paper proposes a context-sensitive convolu-
tion tree kernel for relation extraction to resolve the 
above two problems. It first automatically determines 
a dynamic context-sensitive tree span for relation ex-
traction by extending the Shortest Path-enclosed Tree 
(SPT) to include necessary context information out-
side SPT. Then it proposes a context-sensitive convo-
lution tree kernel, whic h not only enumerates context-
free sub-trees but also context-sensitive sub-trees by 
considering their ancestor node paths as their contexts. 
Moreover, this paper evaluates the complementary 
nature of different linear kernels and tree kernels via a 
composite kernel.  
The layout of this paper is as follows. In Section 2, 
we review related work in more details. Then, the 
dynamic context-sensitive tree span and the context-
sensitive convolution tree kernel are proposed in Sec-
tion 3 while Section 4 shows the experimental results. 
Finally, we conclude our work in Sec tion 5.  
2 Related Work 
The relation extraction task was first introduced as 
part of the Template Element task in MUC6 and then 
formulated as the Template Relation task in MUC7. 
Since then, many methods, such as feature-based 
(Kambhatla 2004; Zhou et al2005, 2006), tree ker-
nel-based (Zelenko et al2003; Culotta and Sorensen 
2004; Bunescu and Mooney 2005a; Zhang et al2006) 
and composite kernel-based (Zhao and Gris hman 
2005; Zhang et al2006), have been proposed in lit-
erature. 
For the feature-based methods, Kambhatla (2004) 
employed Maximum Entropy models to combine di-
verse lexical, syntactic and semantic features in rela-
tion extraction, and achieved the F-measure of 52.8 
on the 24 relation subtypes in the ACE RDC 2003 
corpus. Zhou et al(2005) further systematically ex-
plored diverse features through a linear kernel and 
Support Vector Machines, and achieved the F-
measures of 68.0 and 55.5 on the 5 relation types and 
the 24 relation subtypes in the ACE RDC 2003 cor-
pus respectively. One problem with the feature-based 
methods is that they need extensive feature engineer-
ing. Another problem is that, although they can ex-
plore some structured information in the parse tree 
(e.g. Kambhatla (2004) used the non-terminal path 
connecting the given two entities in a parse tree while 
Zhou et al (2005) introduced additional chunking 
features to enhance the performance), it is found dif-
ficult to well preserve structured information in the 
parse trees using the feature-based methods. Zhou et 
al (2006) further improved the performance by ex-
ploring the commonality among related classes in a 
class hierarchy using hierarchical learning strategy. 
As an alternative to the feature-based methods, the 
kernel-based methods (Haussler, 1999) have been 
proposed to implicitly explore various features in a 
high dimensional space by employing a kernel to cal-
culate the similarity between two objects directly. In 
particular, the kernel-based methods could be very 
effective at reducing the burden of feature engineer-
ing for structured objects in NLP researches, e.g. the 
tree structure in relation extraction.   
Zelenko et al (2003) proposed a kernel between 
two parse trees, which recursively matches nodes 
from roots to leaves in a top-down manner. For each 
pair of matched nodes, a subsequence kernel on their 
child nodes is invoked. They achieved quite success 
on two simple relation extraction tasks. Culotta and 
Sorensen (2004) extended this work to estimate simi-
larity between augmented dependency trees and 
achieved the F-measure of 45.8 on the 5 relation 
types in the ACE RDC 2003 corpus. One problem 
with the above two tree kernels is that matched nodes 
must be at the same height and have the same path to 
the root node. Bunescu and Mooney (2005a) pro-
posed a shortest path dependency tree kernel, which 
just sums up the number of common word classes 
at each position in the two paths, and achieved the 
F-measure of 52.5 on the 5 relation types in the ACE 
RDC 2003 corpus. They argued that the information 
to model a relationship between two entities can be 
typically captured by the shortest path between them 
in the dependency graph. While the shortest path 
may not be able to well preserve structured de-
pendency tree information, another problem with 
their kernel is that the two paths should have same 
length. This makes it suffer from the similar behavior 
with that of Culotta and Sorensen (2004): high preci-
sion but very low recall.  
As the state-of-the-art tree kernel-based method, 
Zhang et al(2006) explored various structured feature 
729
spaces and used the convolution tree kernel over 
parse trees (Collins and Duffy 2001) to model syntac-
tic structured information for relation extraction. 
They achieved the F-measures of 61.9 and 63.6 on the 
5 relation types of the ACE RDC 2003 corpus and the 
7 relation types of the ACE RDC 2004 corpus respec-
tively without entity-related information while the F-
measure on the 5 relation types in the ACE RDC 
2003 corpus reached 68.7 when entity-related infor-
mation was included in the parse tree. One problem 
with Collins and Duffy?s convolution tree kernel is 
that the sub-trees involved in the tree kernel computa-
tion are context-free, that is, they do not consider the 
information outside the sub-trees. This is different 
from the tree kernel in Culota and Sorensen (2004), 
where the sub-trees involved in the tree kernel com-
putation are context-sensitive (that is, with the path 
from the tree root node to the sub-tree root node in 
consideration). Zhang et al(2006) also showed that 
the widely-used Shortest Path-enclosed Tree (SPT) 
performed best. One problem with SPT is that it fails 
to capture the contextual information outside the 
shortest path, which is important for relation extrac-
tion in many cases. Our random selection of 100 pos i-
tive training instances from the ACE RDC 2003 
training corpus shows that ~25% of the cases need 
contextual information outside the shortest path. 
Among other kernels, Bunescu and Mooney (2005b) 
proposed a subsequence kernel and applied it in pro-
tein interaction and ACE relation extraction tasks. 
In order to integrate the advantages of feature-
based and tree kernel-based methods, some research-
ers have turned to composite kernel-based methods. 
Zhao and Grishman (2005) defined several feature-
based composite kernels to integrate diverse features 
for relation extraction and achieved the F-measure of 
70.4 on the 7 relation types of the ACE RDC 2004 
corpus. Zhang et al(2006) proposed two composite 
kernels to integrate a linear kernel and Collins and 
Duffy?s convolution tree kernel. It achieved the F-
measure of 70.9/57.2 on the 5 relation types/24 rela-
tion subtypes in the ACE RDC 2003 corpus and the 
F-measure of 72.1/63.6 on the 7 relation types/23 
relation subtypes in the ACE RDC 2004 corpus. 
The above discussion suggests that structured in-
formation in the parse tree may not be fully utilized in 
the previous works, regardless of feature-based, tree 
kernel-based or composite kernel-based methods. 
Compared with the previous works, this paper pro-
poses a dynamic context-sensitive tree span trying to 
cover necessary structured information and a context-
sensitive convolution tree kernel considering both 
context-free and context-sensitive sub-trees. Further-
more, a composite kernel is applied to combine our 
tree kernel and a state-of-the-art linear kernel for in-
tegrating both flat and structured features in relation 
extraction as well as validating their complementary 
nature. 
3 Context Sensitive Convolution Tree 
Kernel for Relation Extraction 
In this section, we first propose an algorithm to dy-
namically determine a proper context-sensitive tree 
span and then a context-sensitive convolution tree 
kernel for relation extraction.  
3.1 Dynamic Context-Sensitive Tree Span in 
Relation Extraction 
A relation instance between two entities is encaps u-
lated by a parse tree. Thus, it is critical to understand 
which portion of a parse tree is important in the tree 
kernel calculation. Zhang et al(2006) systematically 
explored seven different tree spans, including the 
Shortest Path-enclosed Tree (SPT) and a Context-
Sensitive Path-enclosed Tree1 (CSPT), and found that 
SPT per formed best. That is, SPT even outperforms 
CSPT. This is contrary to our intuition. For example, 
?got married? is critical to determine the relationship 
between ?John? and ?Mary? in the sentence ?John 
and Mary got married? ? as shown in Figure 1(e), 
and the information contained in SPT (?John and 
Mary?) is not enough to determine their relationship. 
Obviously, context-sensitive tree spans should have 
the potential for better performance. One problem 
with the context-sensitive tree span explored in Zhang 
et al(2006) is that it only considers the availability of 
entities? siblings and fails to consider following two 
factors: 
1) Whether is the information contained in SPT 
enough to determine the relationship between 
two entities? It depends. In the embedded cases, 
SPT is enough. For example, ?John?s wife? is 
enough to determine the relationship between 
?John? and ?John?s wife? in the sentence ?John?s 
wife got a good job? ? as shown in Figure 1(a) . 
However, SPT is not enough in the coordinated 
cases, e.g. to determine the relationship between 
?John? and ?Mary? in the sentence ?John and 
Mary got married? ? as shown in Figure 1(e). 
                                                               
1 CSPT means SPT extending with the 1st left sibling of 
the node of entity 1 and the 1st right sibling of the node 
of entity 2.  In the case of no available  sibling, it moves 
to the parent of current node and repeat the same proc-
ess until a sibling is available or the root is reached. 
730
2) How can we extend SPT to include necessary 
context information if there is no enough infor-
mation in SPT for relation extraction?  
To answer the above two questions, we randomly 
chose 100 positive instances from the ACE RDC 
2003 training data and studied their necessary tree 
spans. It was observed that we can classify them into 
5 categories: 1) embedded (37 instances), where one 
entity is embedded in another entity, e.g. ?John? and 
?John?s wife? as shown in Figure 1(a); 2) PP-linked 
(21 instances), where one entity is linked to another 
entity via PP attachment, e.g. ?CEO? and ?Microsoft? 
in the sentence ?CEO of Microsoft announced ? ? as 
shown in Figure 1(b); 3) semi-structured (15 in-
stances), where the sentence consists of a sequence of 
noun phrases (including the two given entities), e.g. 
?Jane? and ?ABC news? in the sentence ?Jane, ABC 
news, California.? as shown in Figure 1(c); 4) de-
scriptive (7 instances), e.g. the citizenship between 
?his mother? and ?Lebanese? in the sentence ?his 
mother Lebanese landed at ?? as shown in Figure 
1(d); 5) predicate-linked and others (19 instances, 
including coordinated cases), where the predicate 
information is necessary to determine the relationship 
between two entities, e.g.  ?John? and ?Mary? in the 
sentence ?John and Mary got married?? as shown in 
Figure 1(e); 
Based on the above observations, we implement an 
algorithm to determine the necessary tree span for the 
relation extract task. The idea behind the algorithm is 
that the necessary tree span for a relation should be 
determined dynamically according to its tree span 
category and context. Given a parsed tree and two 
entities in consideration, it first determin es the tree 
span category and then extends the tree span accord-
ingly. By default, we adopt the Shortest Path-
enclosed Tree (SPT) as our tree span. We only ex-
pand the tree span when the tree span belongs to the 
?predicate-linked? category. This is based on our ob-
servation that the tree spans belonging to the ?predi-
cate-linked? category vary much syntactically and 
majority (~70%) of them need information outside 
SPT while it is quite safe (>90%) to use SPT as the 
tree span for the remaining categories. In our algo-
rithm, the expansion is done by first moving up until 
a predicate-headed phrase is found and then moving 
down along the predicated-headed path to the predi-
cate terminal node. Figure 1(e) shows an example for 
the ?predicate-linked? category where the lines with 
arrows indicate the expansion path.  
 
   
 
e) predicate-linked: SPT and the dynamic context-sensitive tree span  
Figure 1: Different tree span categories with SPT (dotted circle) and an ex-
ample of the dynamic context-sensitive tree span (solid circle) 
  
 
Figure 2: Examples of context-
free and context-sensitive sub-
trees related with Figure 1(b). 
Note: the bold node is the root 
for a sub-tree. 
A problem with our algorithm is how to deter-
mine whether an entity pair belongs to the ?predi-
cate-linked? category. In this paper, a simple 
method is applied by regarding the ?predicate-
linked? category as the default category. That is, 
those entity pairs, which do not belong to the four 
well defined and easily detected categories (i.e. 
embedded, PP-liked, semi-structured and descrip-
tive), are classified into the ?predicate-linked? cate-
gory. 
His mother Lebanese  landed 
PRP$ NNP VBD IN 
NP-E1-PER NP-E2-GPE PP 
S 
d)  descriptive 
NP 
NN 
at 
?  
VP 
Jane ABC news ,  
NNP , NNP NNS , NNP . 
NP NP-E1-PER NP-E2-ORG
NP 
c) semi-structured  
California . . 
, 
, 
, 
NP(NN) 
of Microsoft 
IN NNP 
NP-E2-ORG 
PP(IN)-subroot 
b) context -sensitive 
NP(NN) 
of Microsoft 
IN NNP 
NP-E2-ORG 
S(VBD) 
PP(IN)-subroot 
c) context -sensitive 
PP(IN)-subtoot 
NP-E2-ORG 
of Microsoft 
IN NNP 
a) context -free 
?  
NP 
John and Mary  got 
NNP CC NNP VBD 
married  
NP-E1-PER NP-E2-PER VP 
S 
VP 
VBN ?  
John and Mary  got 
NNP CC NNP VBD  
married 
NP-E1-PER NP-E2-PER VP 
 
NP VP 
 
?  
NP 
CEO of Microsoft announced 
NN IN NNP VBD ?  
NP-E1-PER NP-E2-ORG 
VP 
S 
b)  PP -linked  
PP 
?  
John ?s wife found a  job 
NNP POS NN VBD DT JJ NN 
NP NP-E1-PER 
NP-E2-PER VP 
S 
a) embedded  
good 
731
Since ?predicate -linked? instances only occupy 
~20% of cases, this explains why SPT performs 
better than the Context-Sensitive Path-enclosed 
Tree (CSPT) as described in Zhang et al(2006): 
consistently adopting CSPT may introduce too 
much noise/unnecessary information in the tree 
kernel. 
3.2 Context-Sensitive Convolution Tree Kernel 
Given any tree span, e.g. the dynamic context-
sensitive tree span in the last subsection, we now 
study how to measure the similarity between two 
trees, using a convolution tree kernel.A convolution 
kernel (Haussler D., 1999) aims to capture structured 
information in terms of substructures . As a special-
ized convolution kernel, Collins and Duffy?s convolu-
tion tree kernel ),( 21 TTKC  (?C? for convolution) 
counts the number of common sub-trees (sub-
structures) as the syntactic structure similarity be-
tween two parse trees T1 and T2 (Collins and Duffy 
2001): 
?
??
D=
2211 ,
2121 ),(),(
NnNn
C nnTTK    (1) 
where Nj is the set of nodes in tree Tj , and 1 2( , )n nD  
evaluates the common sub-trees rooted at n1 and n2 2 
and is computed recursively as follows:  
1) If the context-free productions (Context-Free 
Grammar(CFG) rules) at 1n  and 2n  are different, 
1 2( , ) 0n nD = ; Otherwise go to 2. 
2) If both 1n  and 2n  are POS tags, 1 2( , ) 1n n lD = ? ; 
Otherwise go to 3. 
3)  Calculate 1 2( , )n nD recursively as: 
?
=
D+=D
)(#
1
2121
1
)),(),,((1(),(
nch
k
knchknchnn l  (2) 
where )(# nch is the number of children of node n , 
),( knch  is the k th child of node n  andl (0< l <1) is 
the decay factor in order to make the kernel value less 
variable with respect to different sub-tree sizes.  
This convolution tree kernel has been successfully 
applied by Zhang et al(2006) in relation extraction. 
However, there is one problem with this tree kernel: 
the sub-trees involved in the tree kernel computation 
are context-free (That is, they do not consider the 
information outside the sub-trees). This is contrast to 
                                                               
2 That is, each node n encodes the identity of a sub-
tree rooted at n and, if there are two nodes in the 
tree with the same label, the summation will go over 
both of them. 
the tree kernel proposed in Culota and Sorensen 
(2004) which is context-sensitive, that is, it considers 
the path from the tree root node to the sub-tree root 
node. In order to integrate the advantages of both tree 
kernels and resolve the problem in Collins and 
Duffy?s convolution tree kernel, this paper proposes a 
context-sensitive convolution tree kernel. It works by 
taking ancestral information (i.e. the root node path) 
of sub-trees into consideration: 
? ?
= ??
D=
m
i NnNn
ii
C
iiii
nnTTK
1 ]2[]2[],1[]1[
11
1111
])2[],1[(])2[],1[(  (3) 
Where 
? ][1 jN i is the set of root node paths with length i 
in tree T[j] while the maximal length of a root 
node path is defined by m.  
? ])[...(][ 211 jnnnjn ii = is a root node path with 
length i in tree T[j] , which takes into account the 
i-1 ancestral nodes in2 [j] of 1n [j] in T[j]. Here, 
][1 jn k+  is the parent of ][ jn k and ][1 jn  is the 
root node of a context-free sub-tree in T[j]. For 
better differentiation, the label of each ancestral 
node in in1 [j] is augmented with the POS tag of 
its head word.  
? ])2[],1[( 11 ii nnD  measures the common context-
sensitive sub-trees rooted at root node paths 
]1[1in  and ]2[1in
3. In our tree kernel, a sub-tree 
becomes context-sensitive with its dependence on 
the root node path instead of the root node itself. 
Figure 2 shows a few examples of context-
sensitive sub-trees with comparison to context-
free sub-trees. 
Similar to Collins and Duffy (2001),   our tree ker-
nel computes ])2[],1[( 11 ii nnD recursively as follows:  
1) If the context-sensitive productions (Context-
Sensitive Grammar (CSG) rules with root node 
paths as their left hand sides) rooted at ]1[1in  and 
]2[1
in  are different, return ])2[],1[( 11
ii nnD =0; 
Otherwise go to Step 2. 
2) If both ]1[1n  and ]2[1n  are POS tags, 
l=D ])2[],1[( 11 ii nn ; Otherwise go to Step 3. 
                                                               
3 That is, each root node path in1  encodes the identity 
of a context-sensitive sub-tree rooted at in1  and, if 
there are two root node paths in the tree with the 
same label sequence, the summation will go over 
both of them.  
732
3) Calculate ])2[],1[( 11 ii nnD  recursively as: 
?
=
D+=
D
])1[(#
1
11
11
1
))],2[(),],1[((1(
])2[],1[(
inch
k
ii
ii
knchknch
nn
l
 (4) 
where ])],[( 1 kjnch i  is the k
th context-sensitive 
child of the context-sensitive sub-tree rooted at 
][1 jn i  with ])[(# 1 jnch i the number of the con-
text-sensitive children. Here, l (0< l <1) is the 
decay factor in order to make the kernel value 
less variable with respect to different sizes of the 
context-sensitive sub-trees. 
It is worth comparing our tree kernel with previous 
tree kernels. Obviously, our tree kernel is an exten-
sion of Collins and Duffy?s convolution tree kernel, 
which is a special case of our tree kernel (if m=1 in 
Equation (3)). Our tree kernel not only counts the 
occurrence of each context-free sub-tree, which does 
not consider its ancestors, but also counts the occur-
rence of each context-sensitive sub-tree, which con-
siders its ancestors. As a result, our tree kernel is not 
limited by the constraints in previous tree kernels (as 
discussed in Section 2), such as Collins and Duffy 
(2001), Zhang et al(2006), Culotta and Sorensen 
(2004) and Bunescu and Mooney (2005a). Finally, 
let?s study the computational issue with our tree ker-
nel. Although our tree kernel takes the context-
sensitive sub-trees into consideration, it only slightly 
increases the computational burden, compared with 
Collins and Duffy?s convolution tree kernel. This is 
due to that 0])2[],1[( 11 =D nn  holds for the major-
ity of context-free sub-tree pairs (Collins and Duffy 
2001) and that computation for context-sensitive sub-
tree pairs is necessary only when 
0])2[],1[( 11 ?D nn  and the context-sensitive sub-
tree pairs have the same root node path(i.e. 
]2[]1[ 11 ii nn =  in Equation (3)). 
4 Experimentation 
This paper uses the ACE RDC 2003 and 2004 cor-
pora provided by LDC in all our experiments. 
4.1 Experimental Setting  
The ACE RDC corpora are gathered from various 
newspapers, newswire and broadcasts. In the 2003 
corpus , the training set consists of 674 documents and 
9683 positive relation instances w hile the test set con-
sists of 97 documents and 1386 positive relation in-
stances. The 2003 corpus defines 5 entity types, 5 
major relation types and 24 relation subtypes. All the 
reported performances in this paper on the ACE RDC 
2003 corpus are evaluated on the test data. The 2004 
corpus  contains 451 documents and 5702 positive 
relation instances. It redefines 7 entity types, 7 major 
relation types and 23 relation subtypes. For compari-
son, we use the same setting as Zhang et al(2006) by 
applying a 5-fold cross-validation on a subset of the 
2004 data, containing 348 documents and 4400 rela-
tion instances. That is, all the reported performances 
in this paper on the ACE RDC 2004 corpus are evalu-
ated using 5-fold cross validation on the entire corpus . 
Both corpora are parsed using Charniak?s parser 
(Charniak, 2001) with the boundaries of all the entity 
mentions kept 4 . We iterate over all pairs of entity 
mentions occurring in the same sentence to generate 
potential relation instances5. In our experimentation, 
SVM (SVMLight, Joachims(1998)) is selected as our 
classifier. For efficiency, we apply the one vs. others 
strategy, which builds K classifiers so as to separate 
one class from all others. The training parameters are 
chosen using cross-validation on the ACE RDC 2003 
training data.  In particular, l  in our tree kernel is 
fine-tuned to 0.5. This suggests that about 50% dis-
count is done as our tree kernel moves down one 
level in computing ])2[],1[( 11 ii nnD .  
4.2 Experimental Results  
First, we systematically evaluate the context-sensitive 
convolution tree kernel and the dynamic context-
sensitive tree span proposed in this paper. 
Then, we evaluate the complementary nature be-
tween our tree kernel and a state-of-the-art linear ker-
nel via a composite kernel. Generally different 
feature-based methods and tree kernel-based methods 
have their own merits. It is usually easy to build a 
system using a feature-based method and achieve the 
state-of-the-art performance, while tree kernel-based 
methods  hold the potential for further performance 
improvement. Therefore, it is always a good idea to 
integrate them via a composite kernel.  
                                                               
4 This can be done by first representing all entity men-
tions with their head words and then restoring all the 
entity mentions after parsing. Moreover, please note 
that the final performance of relation extraction may 
change much with different range of parsing errors. 
We will study this issue in the near future. 
5 In this paper, we only measure the performance of rela-
tion extraction on ?true? mentions with ?true? chain-
ing of co-reference (i.e. as annotated by LDC 
annotators ). Moreover, we only model explicit relations and 
explicitly model the argument order of the two mentions in-
volved. 
733
Finally, we compare our system with the state-of-
the-art systems in the literature.  
Context-Sensitive Convolution Tree Kernel 
In this paper, the m parameter of our context-sensitive 
convolution tree kernel as shown in Equation (3) 
indicates the maximal length of root node paths and is 
optimized to 3 using 5-fold cross validation on the 
ACE RDC 2003 training data. Table 1 compares the 
impact of different m in context-sensitive convolution 
tree kernels using the Shortest Path-enclosed Tree 
(SPT) (as described in Zhang et al(2006)) on the 
major relation types of the ACE RDC 2003 and 2004 
corpora, in details. It also shows that our tree kernel 
achieves best performance on the test data using SPT 
with m = 3, which outperforms the one with m = 1 by 
~2.3 in F-measure. This suggests the parent and 
grandparent nodes of a sub-tree  contains much 
information for relation extraction while considering 
more ancestral nodes may not help. This may be due 
to that, although our experimentation on the 
training data indicates that  more than 80% (on 
average) of subtrees has a root node path longer 
than 3 (since most of the subtrees are deep from the 
root node and more than 90% of the parsed trees in 
the training data are deeper than 6 levels), 
including a root node path longer than 3 may be 
vulnerable to the full parsing errors and have 
negative impact. Table 1 also evaluates the impact of 
entity-related information in our tree kernel by 
attaching entity type information (e.g. ?PER? in the 
entity node 1 of Figure 1(b)) into both entity nodes. 
It shows that such information can significantly 
improve the performance by ~6.0 in F-measure. In all 
the following experiments, we will apply our tree 
kernel with m=3 and entity-related information by 
default. 
Table 2 compares the dynamic context-sensitive 
tree span with SPT using our tree kernel. It shows that 
the dynamic tree span can futher improve the 
performance by ~1.2 in F-measure6. This suggests the 
usefulness of extending the tree span beyond SPT for 
the ?predicate-linked? tree span category. In the 
future work, we will further explore expanding the 
dynamic tree span beyond SPT for the remaining tree 
span categories. 
  
  
  
                                                               
6 Significance test shows that the dynamic tree span per-
forms s tatistically significantly better than SPT with p-
values smaller than 0.05. 
m P(%) R(%) F 
1 72.3(72.7)  56.6(53.8) 63.5(61.8)  
2 74.9(75.2)  57.9(54.7) 65.3(63.5)  
3 75.7(76.1)  58.3(55.1) 65.9(64.0)  
4 76.0(75.9)  58.3(55.3) 66.0(63.9)  
a) without entity-related information 
m P(%) R(%) F 
1 77.2(76.9)  63.5(60.8) 69.7(67.9)  
2 79.1(78.6)  65.0(62.2) 71.3(69.4)  
3 79.6(79.4)  65.6(62.5) 71.9(69.9)  
4 79.4(79.1)  65.6(62.3) 71.8(69.7)  
b) with entity-related information 
Table 1: Evaluation of context-sensitive convolution 
tree kernels using SPT on the major relation types of 
the ACE RDC 2003 (inside the parentheses) and 2004 
(outside the parentheses) corpora. 
Tree Span P(%) R(%) F 
Shortest Path-  
enclosed Tree 
79.6 
(79.4) 
65.6 
(62.5) 
71.9 
(69.9) 
Dynamic Context- 
Sensitive Tee 
81.1 
(80.1) 
66.7 
(63.8) 
73.2 
(71.0) 
Table 2: Comparison of dynamic context-sensitive 
tree span with SPT using our context-sensitive 
convolution tree kernel on the major relation types of 
the ACE RDC 2003 (inside the parentheses) and 2004 
(outside the parentheses) corpora. 18% of positive 
instances in the ACE RDC 2003 test data belong to 
the predicate-linked category. 
  
Composite Kernel 
In this paper, a composite kernel via polynomial in-
terpolation, as described Zhang et al(2006), is ap-
plied to integrate the proposed context-sensitive 
convolution tree kernel with a state-of-the-art linear 
kernel (Zhou et al2005) 7: 
),()1(),(),(1 ???-+???=?? CPL KKK aa  (5) 
Here, ),( ??LK  and ),( ??CK  indicates the normal-
ized linear kernel and context-sensitive convolution 
tree kernel respectively while  ( , )pK ? ?  is the poly-
nomial expansion of ( , )K ? ?  with degree d=2, i.e. 
2( , ) ( ( , ) 1)pK K? ? ? ?= +  and a  is the coefficient (a  is 
set to 0.3 using cross-validation). 
                                                               
7 Here, we use the same set of flat features (i.e. word, 
entity type, mention level, overlap, base phrase chunk-
ing, dependency tree, parse tree and semantic informa-
tion) as Zhou et al(2005). 
734
Table 3 evaluates the performance of the 
composite kernel. It shows that the composite kernel 
much further improves the performance beyond that 
of either the state-of-the-art linear kernel or our tree 
kernel and achieves the F-measures of 74.1 and 75.8 
on the major relation types of the ACE RDC 2003 
and 2004 corpora respectively. This suggests that our 
tree kernel and the state-of-the-art linear kernel are 
quite complementary, and that our composite kernel 
can effectively integrate both flat and structured 
features. 
System P(%) R(%) F 
Linear Kernel 78.2 (77.2) 
63.4 
(60.7) 
70.1 
(68.0) 
Context-Sensitive Con-
volution Tree Kernel 
81.1 
(80.1) 
66.7 
(63.8) 
73.2 
(71.0) 
Composite Kernel 82.2 (80.8) 
70.2 
(68.4) 
75.8 
(74.1) 
Table 3: Performance of the compos ite kernel via 
polynomial interpolation on the major relation types 
of the ACE RDC 2003 (inside the parentheses) and 
2004 (outside the parentheses) corpora 
  
Comparison with Other Systems  
ACE RDC 2003 P(%) R(%) F 
Ours:  
composite kernel 
80.8 
(65.2) 
68.4 
(54.9) 
74.1 
(59.6) 
Zhang et al(2006):  
composite kernel 
77.3 
(64.9) 
65.6 
(51.2) 
70.9 
(57.2) 
Ours: context-sensitive  
convolution tree kernel 
80.1 
(63.4) 
63.8 
(51.9) 
71.0 
(57.1) 
Zhang et al(2006):  
convolution tree kernel 
76.1 
(62.4) 
62.6 
(48.5) 
68.7 
(54.6) 
Bunescu et al(2005):  
shortest path  
dependency kernel 
65.5 
(-) 
43.8 
(-) 
52.5 
(-) 
Culotta et al(2004):  
dependency kernel 
67.1 
(-) 
35.0 
(-) 
45.8 
(-) 
Zhou et al (2005):  
feature-based 
77.2 
(63.1) 
60.7 
(49.5) 
68.0 
(55.5) 
Kambhatla (2004):  
feature-based 
- 
(63.5) 
- 
(45.2) 
- 
(52.8) 
Table 4: Comparison of difference systems on the 
ACE RDC 2003 corpus over both 5 types (outside the 
parentheses) and 24 subtypes (inside the parentheses) 
  
  
  
ACE RDC 2004 P(%) R(%) F 
Ours:  
composite kernel 
82.2 
(70.3) 
70.2 
(62.2) 
75.8 
(66.0) 
Zhang et al(2006):  
composite kernel 
76.1 
(68.6) 
68.4 
(59.3) 
72.1 
(63.6) 
Zhao et al(2005):8  
composite kernel 
69.2 
(-) 
70.5 
(-) 
70.4 
(-) 
Ours: context-sensitive  
convolution tree kernel 
81.1 
(68.8) 
66.7 
(60.3) 
73.2 
(64.3) 
Zhang et al(2006):  
convolution tree kernel 
72.5 
(-) 
56.7 
(-) 
63.6 
(-) 
Table 5: Comparison of difference systems on the 
ACE RDC 2004 corpus over both 7 types (outside the 
parentheses) and 23 subtypes (inside the parentheses) 
  
Finally, Tables 4 and 5 compare our system with 
other state-of-the-art systems9 on the ACE RDC 2003 
and 2004 corpora, respectively. They show that our 
tree kernel-based system outperforms previous tree 
kernel-based systems. This is largely due to the con-
text-sensitive nature of our tree kernel which resolves 
the limitations of the previous tree kernels. They also 
show that our tree kernel-based system outperforms 
the state-of-the-art feature-based system. This proves 
the great potential inherent in the parse tree structure 
for relation extraction and our tree kernel takes a big 
stride towards the right direction. Finally, they also 
show that our composite kernel-based system outper-
forms other composite kernel-based systems. 
5 Conclusion 
Structured parse tree information holds great potential 
for relation extraction. This paper proposes a context-
sensitive convolution tree kernel to resolve two criti-
cal problems in previous tree kernels for relation ex-
traction by first automatically determining a dynamic 
context-sensitive tree span and then applying a con-
text-sensitive convolution tree kernel. Moreover, this 
paper evaluates the complementary nature between 
our tree kernel and a state-of-the-art linear kernel. 
Evaluation on the ACE RDC corpora shows that our 
dynamic context-sensitive tree span is much more 
suitable for relation extraction than the widely -used 
Shortest Path-enclosed Tree and our tree kernel out-
performs the state-of-the-art Collins and Duffy?s con-
volution tree kernel. It also shows that feature-based 
                                                               
8 There might be some typing errors for the performance 
reported in Zhao and Grishman(2005) since P, R and F 
do not match. 
9 All the state-of-the-art systems apply the entity-related 
information. It is not supervising: our experiments 
show that using the entity-related information gives a 
large performance improvement.  
735
and tree kernel-based methods well complement each 
other and the composite kernel can effectively inte-
grate both flat and structured features.  
To our knowledge, this is the first research to dem-
onstrate that, without extensive feature engineer ing, 
an individual tree kernel can achieve much better per-
formance than the state-of-the-art linear kernel in re-
lation extraction. This shows the great potential of 
structured parse tree information for relation extrac-
tion and our tree kernel takes a big stride towards the 
right direction.  
For the future work, we will focus on improving 
the context-sensitive convolution tree kernel by ex-
ploring more useful context information. Moreover, 
we will explore more entity-related information in the 
parse tree. Our preliminary work of including the en-
tity type information significantly improves the per-
formance. Finally, we will study how to resolve the 
data imbalance and sparseness issues from the learn-
ing algorithm viewpoint.  
Acknowledgement 
This research is supported by Project 60673041 under 
the National Natural Science Foundation of China 
and Project 2006AA01Z147 under the ?863? National 
High-Tech Research and Development of China. We 
would also like to thank the critical and insightful 
comments from the four anonymous reviewers. 
References  
ACE. (2000-2005). Automatic Content Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/  
Bunescu R. & Mooney R.J. (2005a). A shortest path 
dependency kernel for relation extraction. 
HLT/EMNLP?2005 : 724-731. 6-8 Oct 2005. Van-
cover, B.C. 
Bunescu R. & Mooney R.J. (2005b). Subsequence Ker-
nels for Relation Extraction  NIPS?2005. Vancouver, 
BC, December 2005  
Charniak E. (2001). Immediate-head Parsing for Lan-
guage Models. ACL?2001: 129-137. Toulouse, France 
Collins M. and Duffy N. (2001). Convolution Ke rnels 
for Natural Language. NIPS?2001: 625-632. Ca m-
bridge, MA 
Culotta A. and Sorensen J. (2004). Dependency tree 
kernels for relation extraction. ACL?2004 . 423-429. 
21-26 July 2004. Ba rcelona, Spain. 
Haussler D. (1999). Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, Uni-
versity of California, Santa Cruz 
Joachims T. (1998). Text Categorization with Su pport 
Vector Machine: learning with many relevant fea-
tures. ECML-1998 : 137-142.  Chemnitz, Germany 
Kambhatla N. (2004). Combining lexical, syntactic and 
semantic features with Maximum Entropy models for 
extracting relations. ACL?2004(Poster). 178-181. 21-
26 July 2004. Barcelona, Spain. 
MUC. (1987-1998). The NIST MUC website: http: 
//www.itl.nist.gov/iaui/894.02/related_projects/muc/ 
Zelenko D., Aone C. and Richardella. (2003). Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. 3(Feb):1083-1106. 
Zhang M., Zhang J., Su J. and Zhou G.D. (2006). A 
Composite Kernel to Extract Relations between Enti-
ties with both Flat and Structured Features . COLING-
ACL-2006: 825-832. Sydney, Australia 
Zhao S.B. and Grishman R. (2005). Extracting relations 
with integrated information using kernel methods. 
ACL?2005: 419-426. Univ of Michigan-Ann Arbor,  
USA,  25-30 June 2005. 
Zhou G.D., Su J. Zhang J. and Zhang M. (2005). Ex-
ploring various knowledge in relation extraction. 
ACL?2005. 427-434. 25-30 June, Ann Arbor, Mich-
gan, USA.  
Zhou G.D., Su J. and Zhang M. (2006). Modeling com-
monality among related classes in relation extraction, 
COLING-ACL?2006: 121-128. Sydney, Australia. 
736
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 987?996,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
 
Employing the Centering Theory in Pronoun Resolution from the Se-
mantic Perspective 
 
KONG Fang     ZHOU GuoDong*    ZHU Qiaoming 
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow University. Suzhou, China 215006 
Email: {kongfang, gdzhou, qmzhu}@suda.edu.cn 
 
  
 
                                                          
* Corresponding author 
Abstract 
In this paper, we employ the centering the-
ory in pronoun resolution from the seman-
tic perspective. First, diverse semantic role 
features with regard to different predicates 
in a sentence are explored. Moreover, given 
a pronominal anaphor, its relative ranking 
among all the pronouns in a sentence, ac-
cording to relevant semantic role informa-
tion and its surface position, is incorporated. 
In particular, the use of both the semantic 
role features and the relative pronominal 
ranking feature in pronoun resolution is 
guided by extending the centering theory 
from the grammatical level to the semantic 
level in tracking the local discourse focus. 
Finally, detailed pronominal subcategory 
features are incorporated to enhance the 
discriminative power of both the semantic 
role features and the relative pronominal 
ranking feature. Experimental results on the 
ACE 2003 corpus show that the centering-
motivated features contribute much to pro-
noun resolution.  
1 Introduction 
Coreference accounts for cohesion in a text and 
is, in a sense, the hyperlink for a natural lan-
guage. Especially, a coreference instance de-
notes an identity of reference and holds between 
two referring expressions, which can be named 
entities, definite noun phrases, pronouns and so 
on. Coreference resolution is the process of link-
ing together multiple referring expressions of a 
given entity in the world. The key in coreference 
resolution is to determine the antecedent for 
each referring expression in a text. The ability of 
linking referring expressions both within a sen-
tence and across the sentences in a text is critical 
to discourse and language understanding in gen-
eral. For example, coreference resolution is a 
key task in information extraction, machine 
translation, text summarization, and question 
answering. 
There is a long tradition of research on 
coreference resolution within computational lin-
guistics. While earlier knowledge-lean ap-
proaches heavily depend on domain and 
linguistic knowledge (Carter 1987; Carbonell 
and Brown 1988) and have significantly influ-
enced the research, the later approaches usually 
rely on diverse lexical, syntactic and semantic 
properties of referring expressions (Soon et al, 
2001;Ng and Cardie, 2002; Zhou et al, 2004). 
Current research has been focusing on exploiting 
semantic information in coreference resolution. 
For example, Yang et al(2005) proposed a tem-
plate-based statistical approach to compute the 
semantic compatibility between a pronominal 
anaphor and an antecedent candidate, and Yang 
and Su (2007) explored semantic relatedness 
information from automatically discovered pat-
terns, while Ng (2007) automatically induced 
semantic class knowledge from a treebank and 
explored its application in coreference resolution. 
Particularly, this paper focuses on the center-
ing theory (Sidner,1981;Grosz et al,1995; 
Tetreault,2001), which reveals the significant 
impact of the local focus on referring expres-
sions in that the antecedent of a referring expres-
sion usually depends on the center of attention 
throughout the local discourse segment (Mit-
kov,1998). Although the centering theory has 
been considered as a critical theory and the driv-
ing force behind the coreferential phenomena 
since its proposal, its application in coreference 
resolution (in particular pronoun resolution) has 
been somewhat disappointing: it fails to improve 
or even harms the performance of the state-of-
987
 the-art coreference resolution systems in previ-
ous research (e.g. Yang et al 2004). This may be 
due to that centering was originally proposed as 
a model of discourse coherence instead of 
coreference. 
The purpose of this paper is to employ the 
centering theory in pronoun resolution by ex-
tending it from the grammatical level to the se-
mantic level. The intuition behind our approach 
is that, via determining the semantic roles of 
referring expressions in a sentence, such as 
agent and patient, we can derive various center-
ing theory-motivated features in tracking the 
continuity or shift of the local discourse focus, 
thus allowing us to include document-level 
event descriptive information in resolving the 
coreferential relations between referring expres-
sions.  
To the best of our knowledge, this is the first 
research, which successfully applies the center-
ing theory in pronoun resolution from the se-
mantic perspective.  
The rest of this paper is organized as follows. 
Section 2 briefly describes related work in em-
ploying the centering theory and semantic in-
formation in coreference resolution. Then, the 
centering theory is introduced in Section 3 while 
Section 4 details how to employ the centering 
theory from the semantic perspective. Section 5 
reports and discusses the experimental results. 
Finally, we conclude our work in Section 6. 
2 Related Work 
This section briefly overviews the related work 
in coreference resolution from both the centering 
theory and semantic perspectives. 
2.1 Centering Theory 
In the literature, there has been much research in 
the centering theory and its application to 
coreference resolution. 
In the centering theory itself, since the origi-
nal work of Sidner (1979) on immediate focus-
ing of pronouns and the subsequent work of 
Joshi and Weinstein (1981) on centering and 
inferences, much research has been done, in-
cluding centering and linguistic realizations 
(Cote 1993; Prince and Walker 1995), empirical 
and psycholinguistic evaluation of centering 
predictions (Gordon et al 1993,1995; Brennan 
1995; Walker et al1998; Kibble 2001), and the 
cross-linguistic work on centering (Ziv and 
Crosz1994). 
In applications of the centering theory to 
coreference resolution, representative work in-
cludes Brennan et al (1987), Strube (1998), 
Tetreault (1999) and Yang et al (2004). Brennan 
et al (1987) presented a centering theory-based 
formalism in modeling the local focus structure 
in discourse and used it to track the discourse 
context in binding occurring pronouns to corre-
sponding entities. In particular, a BFP (Brennan, 
Friedman and Pollard) algorithm is proposed to 
extend the original centering model to include 
two additional transitions called smooth shift 
and rough shift. Strube (1998) proposed an S-list 
model, assuming that a referring expression pre-
fers a hearer-old discourse entity to other hearer-
new candidates. Tetreault (1999) further ad-
vanced the BFP algorithm by adopting a left-to-
right breadth first walk of the syntactic parse 
trees to rank the antecedent candidates. However, 
the above methods have not been systematically 
evaluated on large annotated corpora, such as 
MUC and ACE. Thus their effects are still un-
clear in real coreference resolution tasks. Yang 
et al(2004) presented a learning-based approach 
by incorporating several S-list model-based fea-
tures to improve the performance in pronoun 
resolution. It shows that, although including S-
list model-based features can slightly boost the 
performance in the ideal case (i.e. given the cor-
rect antecedents of anaphor?s candidates), it de-
teriorates the overall performance in F-measure 
with slightly higher precision but much lower 
recall, in real cases, where the antecedents of 
anaphor?s candidates are determined automati-
cally by a separate coreference resolution mod-
ule.  
2.2 Semantic Information 
It is well known that semantic information plays 
a critical role in coreference resolution. Besides 
the common practice of employing a thesaurus 
(e.g. WordNet) in semantic consistency check-
ing, much research has been done to explore 
various kinds of semantic information, such as 
semantic similarity (Harabagiu et al2000), se-
mantic compatibility (Yang et al2005, 2007), 
and semantic class information (Soon et al2001; 
Ng 2007). Although these methods have been 
proven useful in coreference resolution, their 
contributions are much limited. For example, Ng 
(2007) showed that semantic similarity informa-
tion and semantic agreement information could 
only improve the performance of coreference 
resolution by 0.6 and 0.5 in F-measure respec-
tively, on the ACE 2003 NWIRE corpus.  
988
 3 Centering Theory 
The centering theory is a theory about the local 
discourse structure that models the interaction of 
referential continuity and the salience of dis-
course entities in the internal organization of a 
text. In natural languages, a given entity may be 
referred by different expressions and act as dif-
ferent grammatical roles throughout a text. For 
example, people often use pronouns to refer to 
the main subject of the discourse in focus, which 
can change over different portions of the dis-
course. One main goal of the centering theory is 
to track the focus entities throughout a text.  
The main claims of the centering theory can 
be formalized in terms of Cb (the backward-
looking center), Cf (a list of forward-looking 
centers for each utterance Un) and Cp (the pre-
ferred center, i.e. the most salient candidate for 
subsequent utterances). Given following two 
sentences: 1) Susani gave Betsyj a pet hamsterk; 
2) Shei reminded herj that such hamstersk were 
quite shy. We can have Ub, Uf and Up as follows: 
Ub= ?Susan?; Uf={?Susan?, ?Betsy?, ?a pet 
hamster?}; Up= ?Susan?. 
 Cb(Un)=Cb(Un-1)  or Cb(Un-1) undefined
Cb(Un)?Cb(Un-1)
Cb(Un)=Cp(Un) Continue Smooth Shift
Cb(Un)?Cp(Un) Retain Rough Shift
Table 1: Transitions in the centering theory 
Constraints 
C1. There is precisely one Cb. 
C2. Every element of Cf(Un) must be realized in Un. 
C3. Cb(Un) is the highest-ranked element of Cf(Un-1) 
that is realized in Un. 
Rules 
R1. If some element of Cf(Un-1) is realized as a pro-
noun in Un, then so is Cb(Un). 
R2.Transitions have the descending preference order 
of ?Continue > Retain > Smooth Shift > Rough 
Shift?. 
Table 2: Constraints and rules in the centering theory 
Furthermore, several kinds of focus transi-
tions are defined in terms of two tests: whether 
Cb stays the same (i.e. Cb(Un+1)=Cb(Un)), and 
whether Cb is realized as the most prominent 
referring expression (i.e. Cb(Un=Cp(Un)). We 
refer to the first test as cohesion, and the second 
test as salience. Therefore, there are four possi-
ble combinations, which are displayed in Table 
1 and can result in four kinds of transitions, 
namely Continue, Retain, Smooth Shift, and 
Rough Shift. Obviously, salience, which chooses 
a proper verb form to make Cb prominent within 
a clause or sentence, is an important matter for 
sentence planning, while cohesion, which orders 
propositions in a text to maintain referential con-
tinuity, is an important matter for text planning.  
Finally, the centering theory imposes several 
constraints and rules over Cb/Cf and above tran-
sitions, as shown in Table 2. 
Given the centering theory as described above, 
we can draw the following conclusions: 
1) The centering theory is discourse-related and 
centers are discourse constructs.   
2) The backward-looking center Cb of Un de-
pends only on the expressions that constitute 
the utterance. That is, it is independent of its 
surface position and grammatical roles. 
Moreover, it is not constrained by any previ-
ous utterance in the segment. While the ele-
ments of Cf(Un) are partially ordered to 
reflect relative prominence in Un, grammati-
cal role information is often a major determi-
nant in ranking Cf, e.g. in the descending 
priority order of ?Subject > Object > Others? 
in English (Grosz and Joshi, 2001).  
3) Psychological research (Gordon et al 1993) 
and cross-linguistic research (Kameyama 
1986, 1988; Walker et al 1990,1994) have 
validated that Cb is preferentially realized by 
a pronoun in English.  
4) Frequent rough shifts would lead to a lack of 
local cohesion. To keep local cohesion, peo-
ple tend to plan ahead and minimize the 
number of focus shifts. 
In this paper, we extend the centering theory 
from the grammatical level to the semantic level 
in attempt to better model the continuity or shift 
in the local discourse focus and improve the per-
formance of pronoun resolution via centering-
motivated semantic role features. 
4 Employing Centering Theory from  
Semantic Perspective 
In this section, we discuss how to employ the 
centering theory in pronoun resolution from the 
semantic perspective. In Subsection 4.1, we in-
troduce the semantic roles. In Subsection 4.2, we 
introduce how to employ the centering theory in 
pronoun resolution via semantic role features. 
Finally we compare our method with the previ-
ous work in Subsection 4.3. 
4.1 Semantic Role 
A semantic role is the underlying relationship 
that a participant has with a given predicate in a 
clause, i.e. the actual role a participant plays in 
989
 an event, apart from linguistic encoding of the 
situation. If, in some situation, someone named 
?John? purposely hits someone named ?Bill?, 
then ?John? is the agent and ?Bill? is the patient 
of the hitting event. Therefore, given the predi-
cate ?hit? in both of the following sentences, 
?John? has the same semantic role of agent and 
?Bill? has the same semantic role of patient: 1) 
John hit Bill. 2) Bill was hit by John.  
In the literature, labeling of such semantic 
roles has been well defined by the SRL (Seman-
tic Role Labeling) task, which first identifies the 
arguments of a given predicate and then assigns 
them appropriate semantic roles. During the last 
few years, there has been growing interest in 
SRL. For example, CoNLL 2004 and 2005 have 
made this problem a well-known shared task. 
However, there is still little consensus in the lin-
guistic and NLP communities about what set of 
semantic role labels are most appropriate. Typi-
cal semantic roles include core roles, such as 
agent, patient, instrument, and adjunct roles 
(such as locative, temporal, manner, and cause). 
For core roles, only agent and patient are consis-
tently defined across different predicates, e.g. in 
the popular PropBank (Palmer et al 2005) and 
the derived version evaluated in the CoNLL 
2004 and 2005 shared tasks, as ARG0 and 
ARG1.  
In this paper, we extend the centering theory 
from the grammatical level to the semantic level 
for its better application in pronoun resolution 
via proper semantic role features due to three 
reasons:  
Sentence Grammatical Role Semantic Role
Bob opened the 
door with a key. 
Bob:  
SUBJECT 
Bob:  
AGENT 
The key opened 
the door. 
The key: 
SUBJECT 
The key : 
INSTRUMENT
The door opened. The door: 
SUBJECT 
The door: 
PATIENT 
Table 3: Relationship between grammatical roles and 
semantic roles: an example 
1) Semantic roles are conceptual notions, 
whereas grammatical roles are morph-
syntactic. While the original centering theory 
mainly builds from the grammatical perspec-
tive and grammatical roles do not always cor-
respond directly to semantic roles (Table 3 
shows an example of various semantic roles 
which a subject can play), there is a close re-
lationship between semantic roles and gram-
matical roles. The statistics in the CoNLL 
2004 and 2005 shared tasks (Shen and Lapata, 
2007) shows that the semantic roles of 
ARG0/agent and ARG1/patient account for 
85% of all arguments and most likely act as 
the centers of the local focus structure in dis-
course due to the close relationship between 
subject/object and agent/patient. Therefore, it 
is appropriate to model the centers of an ut-
terance from the semantic perspective via 
semantic roles. 
2) In a sense, semantic roles imply the informa-
tion of grammatical roles, especially for sub-
ject/object. For example, the position of an 
argument and the voice of the predicate verb 
play a central role in SRL. In intuition, an ar-
gument, which occurs before an active verb 
and has the semantic role of Arg0/agent, 
tends to be a subject. That is to say, semantic 
roles (e.g. Arg0/agent and Arg1/patient) can 
be mapped into their corresponding gram-
matical roles (e.g. subject and object), using 
some heuristic rules. Therefore, it would be 
interesting to represent the centers of the ut-
terances and employ the centering theory 
from the semantic perspective. 
3) Semantic role labeling has been well studied 
in the literature and there are good ready-to-
use toolkits available. For example, Pradhan 
(2005) achieved 82.2 in F-measure on the 
CoNLL 2005 version of the Propbank. In 
contrast, the research on grammatical role la-
beling is much less with the much lower 
state-of-the-art performance of 71.2 in F-
measure (Buchholz, 1999). Therefore, it may 
be better to explore the centering theory from 
the semantic perspective. 
4.2 Designing Centering-motivated Fea-
tures from  Semantic Perspective 
In this paper, the centering theory is employed in 
pronoun resolution via three kinds of centering-
motivated features: 
1) Semantic role features. They are achieved by 
checking possible semantic roles of referring 
expressions with regard to various predicates 
in a sentence. Due to the close relationship 
between subject/object and agent/patient, se-
mantic role information should be also a ma-
jor determinant in deciding the center of an 
utterance, which is likely to be the antecedent 
of a referring expression in the descending 
priority order of ?Agent > Patient > Others? 
with regard to their semantic roles, corre-
sponding to the descending priority order of 
?Subject > Object > Others? with regard to 
their grammatical roles. 
990
 2) Relative pronominal ranking feature. Due to 
the predominance of pronouns in tracking the 
local discourse structure1, the relative rank-
ing of a pronoun among all the pronouns in a 
sentence should be useful in pronoun resolu-
tion. This is realized in this paper according 
to its semantic roles (with regard to various 
predicates in a sentence) and surface position 
(in a left-to-right order) by mapping each 
pronoun into 5 levels: a) rank 1 for pronouns 
with semantic role ARG0/agent of the main 
predicate; b) rank 2 for pronouns with seman-
tic role ARG1/patient of the main predicate; c) 
rank 3 for pronouns with semantic role 
ARG0/agent of other predicates; d) rank 4 for 
pronouns with semantic role ARG1/patient of 
other predicates; e) rank 5 for remaining pro-
nouns. Furthermore, for those pronouns with 
the same ranking level, they are ordered ac-
cording to their surface positions in a left-to-
right order, motivated by previous research 
on the centering theory (Grosz et al 1995). 
3) Detailed pronominal subcategory features. 
Given a pronominal expression, its detailed 
pronominal subcategory features, such as 
whether it is a first person pronoun, second 
person pronoun, third person pronoun, neuter 
pronoun or others, are explored to enhance 
the discriminative power of both the semantic 
role features and the relative pronominal 
ranking feature, considering the predominant 
importance of pronouns in tracking the local 
focus structure in discourse.  
4.3 Comparison with Previous Work 
As a representative in explicitly employing se-
mantic role labeling in coreference resolution, 
Ponzetto and Strube (2006) explored two seman-
tic role features to capture the predicate-
argument structure information to benefit 
coreference resolution: I_SEMROLE, the predi-
cate-argument pairs of one referring expression, 
and J_SEMROLE, the predicate-argument pairs 
of another referring expression. Their experi-
ments on the ACE 2003 corpus shows that, 
while the two semantic role features much im-
prove the performance of common noun resolu-
tion by 3.8 and 2.7 in F-measure on the BNEWS 
and NWIRE domains respectively, they only 
                                                          
1 According to the centering theory, the backward-looking 
center Cb is preferentially realized by a pronoun in the sub-
ject position in natural languages, such as English, and 
people tend to plan ahead and minimize the number of 
focus shifts. 
slightly improve the performance of pronoun 
resolution by 0.4 and 0.3 in F-measure on the 
BNEWS and NWIRE domains respectively.  
In comparison, this paper proposes various 
kinds of centering-motivated semantic role fea-
tures in attempt to better model the continuity or 
shift in the local discourse focus by extending 
the centering theory from the grammatical level 
to the semantic level. For example, the 
CAARG0MainVerb feature (as shown in Table 
5) is designed to capture the semantic role of the 
antecedent candidate in the main predicate in 
modeling the discourse center, while, the AN-
PronounRanking feature (as shown in Table 5) is 
designed to determinate the relative priority of 
the pronominal anaphor in retaining the dis-
course center.  
Although both this paper and Ponzetto and 
Strube (2006) employs semantic role features, 
their ways of deriving such features are much 
different due to different driving 
forces/motivations behind. As a result, their con-
tributions on coreference resolution are different: 
while the semantic role features in Ponzette and 
Strube (2006) captures the predicate-argument 
structure information and contributes much to 
common noun resolution and their contribution 
on pronoun resolution can be ignored, the cen-
tering-motivated semantic role features in this 
paper contribute much in pronoun resolution. 
This justifies our attempt to better model the 
continuity or shift of the discourse focus in pro-
noun resolution by extending the centering the-
ory from the grammatical level to the semantic 
level and employing the centering-motivated 
features in pronoun resolution.. 
5 Experimentation and Discussion 
We have evaluated our approach of employing 
the centering theory in pronoun resolution from 
the semantic perspective on the ACE 2003 cor-
pus. 
5.1 Experimental Setting 
The ACE 2003 corpus contains three domains: 
newswire (NWIRE), newspaper (NPAPER), and 
broadcast news (BNEWS). For each domain, 
there exist two data sets, training and devtest, 
which are used for training and testing respec-
tively. Table 4 lists the pronoun distributions 
with coreferential relationships in the training 
data and the test data over pronominal subcate-
gories and sentence distances. Table 4(a) shows 
that third person pronouns occupy most and neu-
991
 tral pronouns occupy second while Table 4(b) 
shows that the antecedents of most pronouns 
occur within the current sentence and the previ-
ous sentence, with a little exception in the test 
data set of BNEWS.  
NWIRE NPAPER BNEWSPronoun  
Subcategory Train Test Train Test Train Test
First Person 263 103 283 120 455 258
Second Person 61 16 29 36 203 68
Third Person 618 179 919 263 736 158
Neuter 395 151 577 190 482 137
Reflexive 23 6 42 12 26 6
Other 0 0 2 0 2 3
(a) Distribution over pronominal subcategories 
NWIRE NPAPER BNEWSDistance 
Train Test Train Test Train Test
0 890 254 1281 347 1149 295
1 447 149 529 197 729 188
2 0 27 0 24 0 41
>2 0 19 0 41 0 100
Total 1337 449 1810 609 1878 624
(b) Distribution over sentence distances 
Table 4: Pronoun statistics on the ACE 2003 corpus 
For preparation, all the documents in the cor-
pus are preprocessed automatically using a pipe-
line of NLP components, including tokenization 
and sentence segmentation, named entity recog-
nition, part-of-speech tagging and noun phrase 
chunking. Among them, named entity recogni-
tion, part-of-speech tagging and noun phrase 
chunking apply the same Hidden Markov Model 
(HMM)-based engine with error-driven learning 
capability (Zhou and Su, 2000 & 2002). In par-
ticular for SRL, we use a state-of-the-art in-
house toolkit, which achieved the precision of 
87.07% for ARG0 identification and the preci-
sion of 78.97% for ARG1 identification, for easy 
integration. In addition, we use the SVM-light2 
toolkit with the radial basis kernel and default 
learning parameters. Finally, we report the per-
formance in terms of recall, precision, and F-
measure, where precision measures the percent-
age of correctly-resolved pronouns (i.e. correctly 
linked with any referring expression in the 
coreferential chain), recall measures the cover-
age of correctly-resolved pronouns, and F-
measure gives an overall figure on equal har-
mony between precision and recall. To see 
whether an improvement is significant, we also 
conduct significance testing using paired t-test. 
In this paper, ?>>>?, ?>>? and ?>? denote p-
values of an improvement smaller than 0.01, in-
between (0.01, 0,05] and bigger than 0.05, 
                                                          
2 http://svmlight.joachims.org/ 
which mean significantly better, moderately 
better and slightly better, respectively. 
5.2 Experimental Results 
Table 5 details various centering-motivated fea-
tures from the semantic perspective, which are 
incorporated in our final system. For example, 
the CAARG0MainVerb feature is designed to 
capture the semantic role of the antecedent can-
didate in the main predicate in modeling the dis-
course center, while the ANPronounRanking 
feature is designed to determinate the relative 
priority of the pronominal anaphor in retaining 
the discourse center. As the baseline, we dupli-
cated the representative system with the same set 
of 12 basic features, as described in Soon et al
(2001). Table 6 shows that our baseline system 
achieves the state-of-the-art performance of 62.3, 
65.3 and 59.0 in F-measure on the NWIRE, 
NPAPER and BNEWS domains, respectively. It 
also shows that the centering-motivated features 
(from the semantic perspective) significantly 
improve the F-measure by 3.6(>>>), 4.5(>>>) 
and 7.7(>>>) on the NWIRE, NPAPER and 
BNEWS domains, respectively. This justifies 
our attempt to model the continuity or shift of 
the discourse focus in pronoun resolution via 
centering-motivated features from the semantic 
perspective. For comparison, we also evaluate 
the performance of our final system from the 
grammatical perspective. This is done by replac-
ing semantic roles with grammatical roles in 
deriving centering-motivated features. Here, la-
beling of grammatical roles is achieved using a 
state-of-the-art toolkit, as described in Buchholz 
(1999). Table 6 shows that properly employing 
the centering theory in pronoun resolution from 
the grammatical perspective can also improve 
the performance. However, the performance im-
provement of employing the centering theory 
from the grammatical perspective is much lower, 
compared with that from the semantic perspec-
tive. This validates our attempt of employing the 
centering theory in pronoun resolution from the 
semantic perspective instead of from the gram-
matical perspective. This also suggests the great 
potential of applying the centering theory in 
pronoun resolution since the centering theory is 
a local coherence theory, which tells how subse-
quent utterances in a text link together.  
Table 7 shows the contribution of the seman-
tic role features and the relative pronominal 
ranking feature in pronoun resolution when the 
detailed pronominal subcategory features are 
included: 
992
  
Feature category Feature Remarks 
CAARG0 1 if the semantic role of the antecedent candidate is ARG0/agent; else 0 
CAARG0MainVerb 1 if the antecedent candidate has the semantic role of ARG0/agent for the main predicate of the sentence; else 0 
Semantic Role-based  Fea-
tures 
ANCASameTarget 1 if the anaphor and the antecedent candidate share the same predicate with regard to their semantic roles; else 0 
Relative Pronominal Rank-
ing Feature ANPronounRanking
Whether the pronominal anaphor is ranked highest among all 
the pronouns in the sentence 
ANPronounType Whether the anaphor is a first person, second person, third person, neuter pronoun or others Detailed Pronominal Sub-
category Features 
CAPronounType Whether the antecedent candidate is a first person, second person, third person, neuter pronoun or others 
Table 5: Centering-motivated features incorporated in our final system  
(with AN indicating the anaphor and CA indicating the antecedent candidate) 
NWIRE NPAPER BNEWS System Variation 
R% P% F R% P% F R% P% F 
Baseline System 57.0 68.6 62.3 61.1 70.1 65.3 49.0 73.9 59.0
Final System 
(from the semantic perspective) 
64.1 67.8 65.9 67.5 72.4 69.8 59.9 75.3 66.7
Final System  
(from the grammatical perspective, for comparison)
63.3 64 63.6 64.7 68.8 66.7 57.1 70.1 63.1
Table 6: Contributions of centering-motivated features in pronoun resolution 
NWIRE NPAPER BNEWS System Variation 
R% P% F R% P% F R% P% F 
Baseline System 57.0 68.6 62.3 61.1 70.1 65.3 49.0 73.9 59.0 
+SR and DC 64.8 67.8 66.3 67.2 72.9 69.9 59.1 75.3 66.3 
+PR and DC 61.5 65.4 63.4 64.9 72.1 68.3 57.4 73.5 64.5 
+SR, PR and DC (Final System) 64.1 67.8 65.9 67.5 72.4 69.8 59.9 75.3 66.7 
Table 7: Contribution of the semantic role features (SR) and the relative pronominal ranking feature (PR) in pro-
noun resolution when the detailed pronominal subcategory features are included 
1) The inclusion of the semantic role features 
improve the performance by 4.0(>>>), 
4.6(>>>) and 7.3(>>>) in F-measure on the 
NWIRE, NPAPER and BNEWS domains, re-
spectively. This suggests the impact of se-
mantic role information in determining the 
local discourse focus.  Since pronouns prefer-
entially occur in the subject position and tend 
to refer to the main subject (Ehrlich 1980; 
Brennan 1995; Walker et al 1998; Cahn 
1995; Gordon and Searce 1995; Kibble et al 
2001), this paper only applies semantic fea-
tures related with the semantic role of 
ARG0/agent, which is closely related with 
the grammatical role of subject, with regard 
to various predicates in a sentence. We have 
also explored features related with other se-
mantic roles. However, our preliminary ex-
perimentation shows that they do not improve 
the performance, even for ARG1/patient, and 
thus are not included in the final system. This 
may be due to that other semantic roles are 
not discriminative enough to make a differ-
ence in deciding the local discourse structure. 
2) It is surprising to notice that further inclusion 
of the relative pronominal ranking feature has 
only slight impact (slight positive impact on 
the BNEWS domain and slight negative im-
pact on the NWIRE and NPAPER domains) 
on the ACE 2003 corpus. This suggests that 
most of information in the relative pronomi-
nal ranking feature has been covered by the 
semantic role features. This is not surprising 
since the semantic role of ARG0/agent, 
which is explored to derive the semantic role 
features, is also applied to decide the relative 
pronominal ranking feature.  
The inclusion of the relative pronominal 
ranking feature improve the performance by 
1.1(>>>), 3.0(>>>) and 5.5(>>>) in F-measure. 
Our further evaluation reveals that the perform-
ance improvement difference among different 
domains of the ACE 2003 corpus is due to the 
distribution of pronouns? antecedents occurring 
over different sentence distances, as shown in 
993
 Table 4. This suggests the usefulness of the rela-
tive pronominal ranking feature in resolving 
pronominal anaphors over longer distance. This 
is consistent with our observation that, as the 
percentage of pronominal anaphors referring to 
more distant antecedents increase, its impact 
turns gradually from negative to positive, when 
further including the relative pronominal ranking 
feature after the semantic role features. The rea-
son that we include the detailed pronominal sub-
category information is due to predominant 
importance of pronouns in tracking the local 
focus structure in discourse and that such de-
tailed pronominal subcategory information is 
discriminative in tracking different subcatego-
ries of pronouns. This suggests the usefulness of 
considering the distribution of the local dis-
course focus over detailed pronominal subcate-
gories. One interesting finding in our 
preliminary experimentation is that the inclusion 
of the detailed pronominal subcategory features 
alone even harms the performance. This may be 
due to the reason that the detailed pronominal 
subcategory features do not have the discrimina-
tive power themselves and that the semantic role 
features and the relative pronominal ranking fea-
ture provide an effective mechanism to explore 
the role of such detailed pronominal subcategory 
features in helping determine the local discourse 
focus. 
 Pronoun  
Subcategory 
NWIRE NPAPER BNEWS
First Person 55.7 55.9 56.6 
Second Person 54.6 60.4 44.0 
Third Person 72.6 80.9 75.7 
Neuter 41.5 50.4 50.2 
Reflexive 85.7 70.0 60.0 
B
as
el
in
e 
Sy
st
em
 
Total 62.3 65.3 59.0 
First Person 64.7 67.0 65.6 
Second Person 78.6 70.0 51.9 
Third Person 80.9 81.8 80.4 
Neuter 48.3 53.0 58.3 
Reflexive 71.4 66.7 80.0 Fi
na
l S
ys
te
m
 
Total 65.9 69.8 66.7 
Table 8: Performance comparison of pronoun resolu-
tion in F-measure over pronoun subcategories 
Table 8 shows the contribution of the center-
ing-motivated features over different pronoun 
subcategories. It shows that the centering-
motivated features contribute much to the reso-
lution of the four major pronoun subcategories 
(i.e. first person, second person, third person and 
neuter) while its negative impact on the minor 
pronoun subcategories (e.g. reflexive) can be 
ignored due to their much less frequent occur-
rence in the corpus.  In particular, the centering-
motivated features improve the performance on 
the major three pronoun subcategories of third 
person / neuter / first person, by 
8.3(>>>)/6.8(>>>)/9.0(>>>), 0.9(>>)/ 2.6 
(>>>)/11.1(>>>) and 4.7(>>>)/8.1(>>>)/9.0 
(>>>), on the NWIRE, NPAPER and BNEWS 
domains of the ACE 2003 corpus, respectively. 
 Distance NWIRE NPAPER BNEWS
<=0 61.6 64.5 68.7 
<=1 60.4 67.5 60.0 
<=2 62.9 67.4 63.7 
B
as
el
in
e 
Sy
st
em
 
Total 62.3 65.3 59.0 
<=0 64.3 70.3 78.7 
<=1 66.8 72.3 72.5 
<=2 66.6 71.8 71.8 
Fi
na
l S
ys
-
te
m
 
Total 65.9 69.8 66.7 
Table 9: Performance comparison of pronoun resolu-
tion in F-measure over sentence distances 
Table 9 shows the contribution of the center-
ing-motivated features over different sentence 
distances. It shows that the centering-motivated 
features improve the performance of pronoun 
resolution on different sentence distances of 
0/1/2, by 2.7(>>>) / 5.8(>>>) / 10.0 (>>>), 
6.4(>>>) / 4.8(>>>) / 12.5(>>>) and 3.7 
(>>>)/4.4(>>>)/8.1(>>>), on the NWIRE, 
NPAPER and BNEWS domains of the ACE 
2003 corpus, respectively. This suggests that the 
centering-motivated features are helpful for both 
intra-sentential and inter-sentential pronoun 
resolution. 
6 Conclusion and Further Work 
This paper extends the centering theory from the 
grammatical level to the semantic level and 
much improves the performance of pronoun 
resolution via centering-motivated features from 
the semantic perspective. This is mainly realized 
by employing various semantic role features 
with regard to various predicates in a sentence, 
in attempt to model the continuity or shift of the 
local discourse focus. Moreover, the relative 
ranking feature of a pronoun among all the pro-
nouns is explored to help determine the relative 
priority of the pronominal anaphor in retaining 
the local discourse focus. Evaluation on the 
ACE 2003 corpus shows that both the centering-
motivated semantic role features and pronominal 
ranking feature much improve the performance 
of pronoun resolution, especially when the de-
tailed pronominal subcategory features of both 
the anaphor and the antecedent candidate are 
included. It is not surprising due to the predomi-
994
 nance of pronouns in tracking the local discourse 
structure in a text.   
To our best knowledge, this paper is the first 
research which successfully applies the center-
ing-motivated features in pronoun resolution 
from the semantic perspective. 
For future work, we will explore more kinds 
of semantic information and structured syntactic 
information in pronoun resolution. In particular, 
we will further employ the centering theory in 
pronoun resolution from both grammatical and 
semantic perspectives on more corpora. 
Acknowledgement  
This research is supported by Project 60673041 
under the National Natural Science Foundation 
of  China, project 2006AA01Z147 under the  
?863? National High-Tech Research and Devel-
opment of China, project 200802850006 under 
the National Research Foundation for the Doc-
toral Program of Higher Education of China, 
project 08KJD520010 under the Natural Science 
Foundation of the Jiangsu Higher Education In-
stitutions of China. 
References 
C. Aone and W.W. Bennett. 1995. Evaluating auto-
mated and manual acquisition of anaphora resolu-
tion strategies. ACL?1995:122-129. 
S.E. Brennan, M.W. Friedman, and C.J. Pollard. 1987. 
A centering approach to pronoun. ACL?1987: 290-
297. 
S.E. Brennan 1995. Centering attention in discourse. 
Language and Cognitive Process, 10/2: 137-67. 
S. Buchholz, J. Veenstra and W. Daelemans. 1999. 
Cascaded Grammatical Relation Assignment. 
EMNLP-VLC?1999: 239-246 
S. Cote. 1983. Ranking and forward-looking centers. 
In Proceedings of the Workshop on the Centering 
Theory in Naturally-Occurring Discourse. 1993. 
D.M. Carter. 1987. Interpreting Anaphors in Natural 
Language Texts. Ellis Horwood, Chichester, UK. 
J. Carbonell and R. Brown. 1988. Anaphora resolu-
tion: a multi-strategy approach. COLING?1988: 
96-101. 
B.J. Grosz, A.K. JoShi and S. Weinstein. 1995. Cen-
tering: a framework for modeling the local coher-
ence of discourse. Computational Linguistics, 
21(2):203-225. 
P.C. Gordon, B.J. Grosz and L.A. Gilliom. 1993. 
Pronouns, names and the centering of attention in 
discourse. Cognitive Science.1993.17(3):311-348 
P.C. Gordon and K. A. Searce. 1995. Pronominaliza-
tion and discourse coherence, discourse structure 
and pronoun interpretation. Memory and Cogni-
tion. 1995. 
S. Harabagiu and S. Maiorano. 2000. Multiligual 
coreference resolution. ANLP-NAACL 2000:142-
149. 
A.K. Joshi and S. Weinstein. 1981. Control of infer-
ence: Role of some aspects of discourse structure-
centering. IJCAI?1981:385-387 
R. Kibble. 2001. A Reformulation of Rule 2 of Cen-
tering. Computational Linguistics, 2001,27(4): 
579-587 
M. Kameyama. 1986. Aproperty-sharing constraint in 
centering. ACL 1986:200-206 
M. Kameyama. 1988. Japanese zero pronominal 
binding, where syntax and discourse meet. In Pro-
ceeding of the Second International Workshop on 
Japanese Syntax. 1988.  
R. Mitkov. 1998. Robust pronoun resolution with 
limited knowledge. COLING-ACL?1998:869-875.  
A. Moschitti and S. Quarteroni. 2008. Kernels on 
linguistics structures for answer entraction. 
ACL?08:113-116 
V. Ng and C. Cardie. 2002. Improving machine 
learning approaches to coreference resolution. 
ACL?2002: 104-111 
V. Ng. 2007. Semantic Class Induction and Corefer-
ence Resolution.  ACL?2007 536-543. 
M. Palmer, D. Gildea and P. Kingsbury. 2005. The 
proposition bank: A corpus annotated with seman-
tic roles. Computational Linguistics, 31(1):71-106. 
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J.H. 
Martin, and D. Jurafsky. 2005. Support vector 
learning for semantic argument classification. Ma-
chine Learning, 2005.60:11-39.  
S. P. Ponzetto and M. Strube. 2006. Semantic Role 
Labeling for Coreference Resolution. 
EMNLP?2006 143-146. 
E. F. Prince and M. A. Walker. 1995. A bilateral ap-
porach to givenness: a hearer-status algorithm and 
a centering algorithm. In Proceedings of 4th Inter-
national Pragmatics Conference. 
E. Rich and S. LuperFoy. 1988. An architecture for 
anaphora resolution. In Proceedings of the 2nd 
Conference on Applied Natural Language Proc-
essing. ANLP?1988: 18-24. 
W.M. Soon, H.T. Ng and D. Lim. 2001. A machine 
learning approach to coreference resolution of 
noun phrase. Computational Linguistics, 2001, 
27(4):521-544. 
D. Shen and M. Lapata. 2007. Using semantic roles 
to improve question answering. EMNLP-CoNIL 
2007:12-21 
C. Sidner. 1979. Toward a computation of intrasen-
tential coreference. Technical Report TR-537,MIT. 
Artificial Intelligence Laboratory. 
C. Sidner. 1981. Focusing for interpretation of pro-
nouns. Computational Linguistics,1981.7:217-231 
J. Tetreault. 1999. Analysis of syntax-based pronoun 
resolution methods. ACL 1999:602-605 
J. Tetreault. 2001. A corpus-based evaluation of cen-
tering and pronoun resolution. Computational Lin-
guistics. 2001. 27(4):507-520. 
995
 M. Walker, A. K. Joshi and E. Prince. 1998. Center-
ing in naturally occurring discourse: An overview. 
Clarendon Press:1-28 
X.F. Yang?J. Su?G.D. Zhou and C.L. Tan. 2004. 
Improving pronoun resolution by incorporating 
coreferential information of candidates. 
ACL?2004:127-134. 
X.F. Yang? J. Su and C.L. Tan. 2005. Improving 
Pronoun Resolution Using Statistics - Based Se-
mantic Compatibility Information. ACL?2005:165 
-172. 
X.F. Yang and J. Su. 2007. Coreference Resolution 
Using Semantic Relatedness Information from 
Automatically Discovered Patterns. ACL?2007: 
528-535. 
G.D. Zhou and J. Su. 2004. A high- performance 
coreference resolution system using a multi- agent 
strategy. COLING? 2004:522- 528. 
Y. Ziv and B.J. Grosz. 1994. Right dislocation and 
attentional state. Israel Association of Theoretical 
Linguistics Meetings?1994. 184-199. 
996
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280?1288,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
 
Improving Nominal SRL in Chinese Language with Verbal SRL In-
formation and Automatic Predicate Recognition 
 
Junhui Li?   Guodong Zhou??   Hai Zhao??  Qiaoming Zhu?  Peide Qian? 
? Jiangsu Provincial Key Lab for Computer Information Processing Technologies
School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
? Department of Chinese, Translation and Linguistics 
City University of HongKong, China 
Email: {lijunhui,gdzhou,hzhao,qmzhu,pdqian}@suda.edu.cn
 
                                                          
? Corresponding author 
Abstract 
This paper explores Chinese semantic role la-
beling (SRL) for nominal predicates. Besides 
those widely used features in verbal SRL, 
various nominal SRL-specific features are 
first included. Then, we improve the perform-
ance of nominal SRL by integrating useful 
features derived from a state-of-the-art verbal 
SRL system. Finally, we address the issue of 
automatic predicate recognition, which is es-
sential for a nominal SRL system. Evaluation 
on Chinese NomBank shows that our research 
in integrating various features derived from 
verbal SRL significantly improves the per-
formance. It also shows that our nominal SRL 
system much outperforms the state-of-the-art 
ones. 
1. Introduction 
Semantic parsing maps a natural language sen-
tence into a formal representation of its meaning. 
Due to the difficulty in deep semantic parsing, 
most of previous work focuses on shallow se-
mantic parsing, which assigns a simple structure 
(such as WHO did WHAT to WHOM, WHEN, 
WHERE, WHY, HOW) to each predicate in a 
sentence. In particular, the well-defined seman-
tic role labeling (SRL) task has been drawing 
more and more attention in recent years due to 
its importance in deep NLP applications, such as 
question answering (Narayanan and Harabagiu, 
2004), information extraction (Surdeanu et al, 
2003), and co-reference resolution (Ponzetto and 
Strube, 2006). Given a sentence and a predicate 
(either a verb or a noun) in it, SRL recognizes 
and maps all the constituents in the sentence into 
their corresponding semantic arguments (roles) 
of the predicate. According to the predicate 
types, SRL could be divided into SRL for verbal 
predicates (verbal SRL, in short) and SRL for 
nominal predicates (nominal SRL, in short). 
During the past few years, verbal SRL has 
dominated the research on SRL with the avail-
ability of FrameNet (Baker et al, 1998), Prop-
Bank (Palmer et al, 2005), and the consecutive 
CoNLL shared tasks (Carreras and M?rquez, 
2004 & 2005) in English language. As a com-
plement to PropBank on verbal predicates, 
NomBank (Meyers et al, 2004) annotates nomi-
nal predicates and their corresponding semantic 
roles using similar semantic framework as 
PropBank. As a representative, Jiang and Ng 
(2006) pioneered the exploration of various 
nominal SRL-specific features besides the tradi-
tional verbal SRL-related features on NomBank. 
They achieved the performance of 72.73 and 
69.14 in F1-measure on golden and automatic 
syntactic parse trees, respectively, given golden 
nominal predicates. 
For SRL in Chinese, Sun and Jurafsky (2004) 
and Pradhan et al (2004) pioneered the research 
on Chinese verbal and nominal SRLs, respec-
tively, on small private datasets. Taking the ad-
vantage of recent release of Chinese PropBank 
(Xue and Palmer, 2003) and Chinese NomBank 
(Xue, 2006a), Xue and his colleagues (Xue and 
Palmer 2005; Xue 2006b; Xue, 2008) pioneered 
the exploration of Chinese verbal and nominal 
SRLs, given golden predicates. Among them, 
Xue and Palmer (2005) studied Chinese verbal 
SRL using Chinese PropBank and achieved the 
performance of 91.3 and 61.3 in F1-measure on 
golden and automatic syntactic parse trees, re-
spectively. Xue (2006b) extended their study on 
Chinese nominal SRL and attempted to improve 
the performance of nominal SRL by simply in-
1280
 cluding the Chinese PropBank training instances 
into the training data for nominal SRL on Chi-
nese NomBank. However, such integration was 
empirically proven unsuccessful due to the dif-
ferent nature of certain features for verbal and 
nominal SRLs. Xue (2008) further improved the 
performance on both verbal and nominal SRLs 
with a better syntactic parser and new features. 
Ding and Chang (2008) focused on argument 
classification for Chinese verbal predicates with 
hierarchical feature selection strategy. They 
achieved the classification precision of 94.68% 
on golden parse trees on Chinese PropBank. 
This paper focuses on Chinese nominal SRL. 
This is done by adopting a traditional verbal 
SRL architecture to handle Chinese nominal 
predicates with additional nominal SRL-specific 
features. Moreover, we significantly enhance the 
performance of nominal SRL by properly inte-
grating various features derived from verbal 
SRL. Finally, this paper investigates the effect of 
automatic nominal predicate recognition on the 
performance of Chinese nominal SRL. Although 
previous research (e.g. CoNLL?2008) in English 
nominal SRL reveals the importance of auto-
matic predicate recognition, there has no re-
ported research on automatic predicate 
recognition in Chinese nominal SRL. 
The rest of this paper is organized as follows: 
Section 2 introduces Chinese NomBank while 
the baseline nominal SRL system is described in 
Section 3 with traditional and nominal SRL-
specific features. Then, the baseline nominal 
SRL system is improved by integrating useful 
features derived from verbal SRL (Section 4) 
and extended with automatic recognition of 
nominal predicates (Section 5). Section 6 gives 
experimental results and discussion. Finally, 
Section 7 concludes the paper.    
2. Chinese NomBank 
Chinese NomBank (Xue, 2006a) adopts similar 
semantic framework as NomBank, and focuses 
on Chinese nominal predicates with their argu-
ments in Chinese TreeBank. The semantic ar-
guments include:  
1) Core arguments: Arg0 to Arg5. Generally, 
Arg0 and Arg1 denotes the agent and the 
patient, respectively, while arguments from 
Arg2 to Arg5 are predicate-specific.  
2) Adjunct arguments, which are universal to 
all predicates, e.g. ArgM-LOC for locative, 
and ArgM-TMP for temporal. 
 
All the arguments are annotated on parse tree 
nodes with their boundaries aligning with the 
spans of tree nodes. Figure 1 gives an example 
with two nominal predicates and their respective 
arguments, while the nominal predicate ???
/investment? has two core arguments, ?NN(??
/foreign businessman)? as Arg0 and ?NN(??
/bank)? as Arg1, and the other nominal predicate 
??? /loan? also has two core arguments, 
?NP(???? /Bank of China)? as Arg1 and 
Figure 1: Two nominal predicates and their arguments in the style of NomBank. 
? 
?? ?? ??
??
???
P 
NN NN NN
VV
NN NN 
Arg0/Rel1 Rel1 Arg1/Rel1
NP 
PP 
Arg0/Rel2 
ArgM-MNR/Rel2 Rel2 
NP 
CD
QP
NP
VP
VP
??? ?? 
? 
NN NN 
PU 
NP 
Arg1/Rel2 
IP
?? ?? 
Sup/Rel2
Bank of China 
to 
Foreign  Investment  Bank 
provide
4 billion
RMB loan 
. 
Bank of China provides 4 billion RMB loan to Foreign Investment Bank. 
1281
 ?PP(??????? /to Foreign Investment 
Bank)? as Arg0,  and 1 adjunct argument, 
?NN(???/RMB)? as ArgM-MNR, denoting 
the manner of loan. It is worth noticing that 
there is a (Chinese) NomBank-specific label in 
Figure 1, Sup (support verb) (Xue, 2006a), in 
helping introduce the arguments, which occur 
outside the nominal predicate-headed noun 
phrase. This is illustrated by the nominal predi-
cate ???/loan?, whose Arg0 and Arg1 are both 
realized outside the nominal predicate-headed 
noun phrase, NP(????????/4 billion 
RMB loan). Normally, a verb is marked as a 
support verb only when it shares some argu-
ments with the nominal predicate. 
3. Baseline: Chinese Nominal SRL 
Popular SRL systems usually formulate SRL as 
a classification problem, which annotates each 
constituent in a parse tree with a semantic role 
label or with the non-argument label NULL. Be-
sides, we divide the system into three consecu-
tive phases so as to overcome the imbalance 
between the training instances of the NULL 
class and those of any other argument classes.  
Argument pruning. Here, several heuristic 
rules are adopted to filter out constituents, which 
are most likely non-arguments. According to the 
argument structures of nominal predicates, we 
categorize arguments into two types: arguments 
inside NP (called inside arguments) and argu-
ments introduced via a support verb (called out-
side arguments), and handle them separately. 
For the inside arguments, the following three 
heuristic rules are applied to find inside argu-
ment candidates: 
z All the sisters of the predicate are candi-
dates. 
z If a CP or DNP node is a candidate, its chil-
dren are candidates too. 
z For any node X, if its parent is an ancestral 
node of the predicate, and the internal 
nodes along the path between X and the 
predicate are all NPs, then X is a candidate. 
For outside arguments, we look for the sup-
port verb of the focus nominal predicate, and 
then adopt the rules as proposed in Xue and 
Palmer (2005) to find the candidates for the sup-
port verb, since outside argument candidates are 
introduced via this support verb. That to say, the 
argument candidates of the support verb are re-
garded as outside argument candidates of the 
nominal predicate. However, as support verbs 
are not annotated explicitly in the testing phase, 
we identify intervening verbs as alternatives to 
support verbs in both training and testing phases 
with the path between the nominal predicate and 
intervening verb in the form of 
?VV<VP>[NP>]+NN?, where ?[NP>]+? denotes 
one or more NPs.  Our statistics on Chinese 
NomBank shows that 51.96% of nominal predi-
cates have no intervening verb while 48.04% of 
nominal predicates have only one intervening 
verb. 
Taken the nominal predicate ???/loan? in 
Figure 1 as an example, NN(???/RMB) and 
QP(??? /4 billion) are identified as inside 
argument candidates, while PP(??????
?/to Foreign Investment Bank) and NP(???
?/Bank of China) are identified as outside ar-
gument candidates via the support verb VV(?
?/provide). 
Argument identification. A binary classifier 
is applied to determine the candidates as either 
valid arguments or non-arguments. It is worth 
pointing out that we only mark those candidates 
that are most likely to be NULL (with probabil-
ity > 0.90) as non-arguments. Our empirical 
study shows that this little trick much benefits 
nominal SRL, since argument identification for 
nominal predicates is much more difficult than 
that for verbal predicates and thus many argu-
ments would have been falsely marked as non-
arguments if the threshold is set as 0.5. 
Argument classification. A multi-class classi-
fier is employed to label identified arguments 
with specific argument labels (including the 
NULL class for non-argument). 
In the following, we first adapt some tradi-
tional features, which have been proven effec-
tive in verbal SRL, to nominal SRL, and then 
introduce several nominal SRL-specific features. 
3.1. Traditional Features 
Using the feature naming convention as adopted 
in Jiang and Ng (2006), Table 1 lists the tradi-
tional features, where ?I? and ?C? indicate the 
features for argument identification and classifi-
cation, respectively. Among them, the predicate 
class (b2) feature was first introduced in Xue 
and Palmer (2005) to overcome the imbalance of 
the predicate distribution in that some predicates 
can be only found in the training data while 
some predicates in the testing data are absent 
from the training data. In particular, the verb 
class is classified along three dimensions: the 
number of arguments, the number of framesets 
and selected syntactic alternations. For example, 
1282
 the verb class of ?C1C2a? means that it has two 
framesets, with the first frameset having one 
argument and the second having two arguments. 
The symbol ?a? in the second frameset repre-
sents a type of syntactic alternation. 
 
Feature Remarks: b1-b5(C, I), b6-b7(C) 
b1 Predicate: the nominal predicate itself. (??
/loan) 
b2 Predicate class: the verb class that the predi-
cate belongs to. (C4a) 
b3 Head word (b3H) and its POS (b3P).  (??
/bank, NN) 
b4 Phrase type: the syntactic category of the 
constituent. (NP) 
b5 Path: the path from the constituent to the 
nominal predicate. 
 (NP<IP>VP>VP>NP>NP>NN) 
b6 Position: the positional relationship of the 
constituent with the predicate. ?left? or 
?right?. (left) 
b7 First word (b7F) and last word (b7L) of the 
focus constituent. (??/China, ??/bank) 
Combined features: b11-b14(C, I), b15(C) 
b11: b1&b4;       b12: b1&b3H;       b13: b2&b4;  
b14: b2&b3H;    b15: b5&b6 
Table 1: Traditional features and their instantiations 
for argument identification and classification, with 
NP(????/Bank of China)  as the focus constitu-
ent and NN(??/loan) as the nominal predicate, re-
garding Figure 1. 
3.2. Nominal SRL-specific Features 
To capture more useful information in the predi-
cate-argument structure, we also study addi-
tional features which provide extra information. 
Statistics on Chinese NomBank show that about 
40% of pruned inside candidates are arguments. 
Since inside arguments usually locate near to the 
nominal predicate, its surroundings are expected 
to be helpful in SRL. Table 2 shows the features 
in better capturing the details between inside 
arguments and nominal predicates. Specially, 
features ai6 and ai7 are sister-related features, 
inspired by the features related with the 
neighboring arguments in Jiang and Ng (2006). 
Statistics on NomBank and Chinese Nom-
Bank show that about 20% and 22% of argu-
ments are introduced via a support verb, 
respectively. Since a support verb pivots outside 
arguments and the nominal predicate on its two 
sides, support verbs play an important role in 
labeling these arguments. Here, we also identify 
intervening verbs as alternatives to support verbs 
since support verbs are not explicitly in the test-
ing phase. Table 3 lists the intervening verb-
related features (ao1-ao4, ao11-ao14) employed 
in this paper. 
 
Feature Remarks 
ai1 Whether the focus constituent is adjacent to 
the predicate. Yes or No. (Yes) 
ai2 The headword (ai2H) and pos (ai2P) of the 
predicate?s nearest right sister. (??/bank, 
NN) 
ai3 Whether the predicate has right sisters. Yes 
or No. (Yes) 
ai4 Compressed path of b5: compressing se-
quences of identical labels into one. 
(NN<NP>NN) 
ai5 Whether the predicate has sisters. Yes or 
No. (Yes) 
ai6 For each sister of the focus constituent, 
combine b3H&b4&b5&b6. ( ? ?
/bank&NN & NN<NP>NN&right) 
ai7 Coarse version of ai6, b4&b6. (NN&right) 
Table 2: Additional features and their instantiations  
for inside argument candidates, with ?NN(??
/foreign businessman)? as the focus constituent and 
?NN(?? /investment)? as the nominal predicate, 
regarding Figure1. 
 
Feature Remarks 
ao1 Intervening verb itself. (??/provide) 
ao2 The verb class that the intervening verb 
belongs to. (C3b) 
ao3 The path from the focus constituent to the 
intervening verb. (NP<IP>VP>VP>VV) 
ao4 The compressed path of ao3: compressing 
sequences of identical labels into one. 
(NP<IP>VP>VV) 
Combined features: ao11-ao14 
ao11: ao1&ao3;      ao12: ao1&ao4;    
ao13: ao2&ao3;      ao14: ao2&ao4. 
Table 3: Additional features and their instantiations 
for outside argument candidates, with ?NP(????
/Bank of China)? as the focus constituent and ???
/loan? as the nominal predicate, regarding Figure1. 
Feature selection. Some Features proposed 
above may not be effective in tasks of identifica-
tion and classification. We adopt the greedy fea-
ture selection algorithm as described in Jiang 
and Ng (2006) to pick up positive features em-
pirically and incrementally according to their 
contributions on the development data. The al-
gorithm repeatedly selects one feature each time 
which contributes most, and stops when adding 
any of the remaining features fails to improve 
the performance. As far as the SRL task con-
cerned, the whole feature selection process could 
be done as follows: 1). Feature selection for ar-
gument identification: run the selection algo-
1283
 rithm with the basic set of features (b1-b5, b11-
b14) to pick up effective features from (ai1-ai7, 
ao1-ao4, ao11-ao14); 2). Feature selection for 
argument classification: fix the output returned 
in step1 as the feature set of argument identifica-
tion, and run the selection algorithm with the 
basic set of features (b1-b7, b11-b15) to select 
positive features from (ai1-ai7, ao1-ao4, ao11-
ao14) for argument classification. 
4. Integrating Features derived from 
Verbal SRL 
Since Chinese PropBank and NomBank are an-
notated on the same data set with the same lexi-
cal guidelines (e.g. frame files), it may be 
interesting to investigate the contribution of 
Chinese verbal SRL on the performance of Chi-
nese nominal SRL. In the frame files, argument 
labels are defined with regard to their semantic 
roles to the predicate, either a verbal or nominal 
predicate. For example, in the frame file of 
predicate ???/loan?, the borrower is always 
labeled with Arg0 and the lender labeled with 
Arg1. This can be demonstrated by the follow-
ing two sentences: ???/loan? is annotated as a 
nominal and a verbal predicate in S1 and S2, 
respectively. 
S1 [Arg1 ????/Bank of China] [Arg0 ???
????/to Foreign Investment Bank] ??
/provide [Rel??/loan] 
S2  [Arg0 ????/Bank of China] [Arg1 ???
????/from Foreign Investment Bank] [Rel 
??/loan] 
Therefore, it is straightforward to augment 
nominal training instances with verbal ones. 
However, Xue (2006b) found that simply adding 
the training instances for verbal SRL to the 
training data for nominal SRL and indiscrimi-
nately extracting the same features in both ver-
bal and nominal SRLs hurt the performance. 
This may be due to that certain features (e.g. the 
path feature) are much different for verbal and 
nominal SRLs. This can be illustrated in sen-
tences S1 and S2: the verbal instances in S2 are 
negative for semantic role labeling of the nomi-
nal predicate ???/loan? in S1, since ????
?/Bank of China? takes opposite roles in S1 
and S2. So does ????????/(from/to) 
Foreign Investment Bank?. 
Although several support verb-related features 
(ao1-ao4, ao11-ao14) have been proposed, one 
may still ask how large the role support verbs 
can play in nominal SRL. It is interesting to note 
that outside arguments and the highest NP 
phrase headed by the nominal predicate are also 
annotated as arguments of the support verb in 
Chinese PropBank. For example, Chinese Prop-
Bank marks ?????/Bank of China? as Arg0 
and ?????????/4 billion RMB loan? 
as Arg1 for verb ???/provide? in Figure1. Let 
OA be the outside argument, VV be the support 
verb, and NP be the highest NP phrase headed 
by the nominal predicate NN, then there exists a 
pattern ?OA VV NN? in the sentence, where the 
support verb VV plays a certain role in trans-
ferring roles between OA and NN. For example, 
if OA is the agent of VV, then OA is also the 
agent of phrase VP(VV NN). Like the example 
in Figure1, supposing a NP is the agent of sup-
port verb ???/provide? as well as VP phrase 
(??????????? /provide 4 billion 
RMB loan?), we can infer that the NP is the 
lender of the nominal predicate ???/loan? in-
dependently on any other information, such as 
the NP content and the path from the NP to the 
nominal predicate ???/loan?.  
Let C be the focus constituent, V be the inter-
vening verb, and NP be the highest NP headed 
by the nominal predicate. Table 4 shows the fea-
tures (ao5-ao8, p1-p7) derived from verbal SRL. 
In this paper, we develop a state-of-the-art Chi-
nese verbal SRL system, similar to the one as 
shown in Xue (2008), to achieve the goal. Based 
on golden parse trees on Chinese PropBank, our 
Chinese verbal SRL system achieves the per-
formance of 92.38 in F1-measure, comparable to 
Xue (2008) which achieved the performance of 
92.0 in F1-measure. 
 
Feature Remarks 
ao5 Whether C is an argument for V. Yes or No
ao6 The semantic role of C for V. 
ao7 Whether NP is an argument for V. Yes or No
ao8 The semantic role of NP for V. 
Combined features: p1-p7 
p1: ao1&ao5;         p2: ao1&ao6;    p3: ao1&ao5&b1; 
p4: ao1&ao6&b1;  p5: ao1&apo7;  p6: ao1&ao8;  
p7: ao5&ao7. 
Table 4: Features derived from verbal SRL. 
5. Automatic Predicate Recognition 
Unlike Chinese PropBank where almost all the 
verbs are annotated as predicates, Chinese Nom-
Bank only marks those nouns having arguments 
as predicates. Statistics on Chinese NomBank 
show that only 17.5% of nouns are marked as 
predicates. It is possible that a noun is a predi-
1284
 cate in some cases but not in others. Previous 
Chinese nominal SRL systems (Xue, 2006b; 
Xue, 2008) assume that nominal predicates have 
already been manually annotated and thus are 
available. To our best knowledge, there is no 
report on addressing automatic recognition of 
nominal predicates on Chinese nominal SRL. 
Automatic recognition of nominal predicates 
can be cast as a binary classification (e.g., Predi-
cate vs. Non-Predicate) problem. This paper 
employs the convolution tree kernel, as proposed 
in Collins and Duffy (2001), on automatic rec-
ognition of nominal predicates. 
Given the convolution tree kernel, the key 
problem is how to extract a parse tree structure 
from the parse tree for a nominal predicate can-
didate. In this paper, the parse tree structure is 
constructed as follows: 1) starting from the 
predicate candidate?s POS node, collect all of its 
sister nodes (with their headwords); 2). recur-
sively move one level up and collect all of its 
sister nodes (with their headwords) till reaching 
a non-NP node. Specially, in order to explicitly 
mark the positional relation between a node and 
the predicate candidate, all nodes on the left side 
of the candidate are augmented with tags 1 and 2 
for nodes on the right side. Figure 2 shows an 
example of the parse tree structure with regard 
to the predicate candidate ???/loan? as shown 
in Figure 1. 
In our extra experiments we found global sta-
tistic features (e.g. g1-g5) about the predicate 
candidate are helpful in a feature vector-based 
method for predicate recognition. Figure 2 
makes an attempt to utilize those features in ker-
nel-based method. We have explored other ways 
to include those global features. However, the 
way in Figure 2 works best.  
 
 
Let the predicate candidate be w0, and its left 
and right neighbor words be w-1 and w1, respec-
tively. The five global features are defined as 
follows. 
g1 Whether w0 is ever tagged as a verb in the 
training data? Yes or No. 
g2 Whether w0 is ever annotated as a nominal 
predicate in the training data? Yes or No. 
g3 The most likely label for w0 when it occurs 
together with w-1 and w1. 
g4 The most likely label for w0 when it occurs 
together with w-1. 
g5 The most likely label for w0 when it occurs 
together with w1. 
6. Experiment Results and Discussion 
We have evaluated our Chinese nominal SRL 
system on Chinese NomBank with Chinese 
PropBank 2.0 as its counterpart. 
6.1. Experimental Settings 
This version of Chinese NomBank consists of 
standoff annotations on the files (chtb_001 to 
1151.fid) of Chinese Penn TreeBank 5.1. Fol-
lowing the experimental setting in Xue (2008), 
648 files (chtb_081 to 899.fid) are selected as 
the training data, 72 files (chtb_001 to 040.fid 
and chtb_900 to 931.fid) are held out as the test 
data, and 40 files (chtb_041 to 080.fid) as the 
development data, with 8642, 1124, and 731 
propositions, respectively. 
As Chinese words are not naturally segmented 
in raw sentences, two Chinese automatic parsers 
are constructed: word-based parser (assuming 
golden word segmentation) and character-based 
parser (with automatic word segmentation). 
Here, Berkeley parser (Petrov and Klein, 2007)1 
is chosen as the Chinese automatic parser. With 
regard to character-based parsing, we employ a 
Chinese word segmenter, similar to Ng and Low 
(2004), to obtain the best automatic segmenta-
tion result for a given sentence, which is then 
fed into Berkeley parser for further syntactic 
parsing. Both the word segmenter and Berkeley 
parser are developed with the same training and 
development datasets as our SRL experiments. 
The word segmenter achieves the performance 
of 96.1 in F1-measure while the Berkeley parser 
gives a performance of 82.5 and 85.5 in F1-
measure on golden and automatic word segmen-
tation, respectively2.  
??? 1 In addition, SVMLight with the tree kernel 
function (Moschitti, 2004) 3  is selected as our 
classifier. In order to handle multi-classification 
                                                          
1 Berkeley Parser. http://code.google.com/p/berkeleyparser/ 
2 POSs are not counted in evaluating the performance of 
word-based syntactic parser, but they are counted in evalu-
ating the performance of character-based parser. Therefore 
the F1-measure for the later is higher than that for the for-
mer. 
3 SVM-LIGHT-TK. http://dit.unitn.it/~moschitt/ 
Figure 2: Semantic sub-tree for nominal predicate
RMB 
?? 
loan 
?? 1 
provide 
??? 1 
4 billion 
VV1 
NN1 NN 
NPQP1 
NP
VP 
g1 ?. g5
1285
 problem in argument classification, we apply the 
one vs. others strategy, which builds K classifi-
ers so as to separate one class from all others. 
For argument identification and classification, 
we adopt the linear kernel and the training pa-
rameter C is fine-tuned to 0.220. For automatic 
recognition of nominal predicates, the training 
parameter C and the decay factor ?  in the con-
volution tree kernel are fine-tuned to 2.0 and 0.2, 
respectively. 
6.2. Results with Golden Parse Trees and 
Golden Nominal Predicates 
Effect of nominal SRL-specific features 
 
 Rec.(%) Pre.(%) F1 
traditional features 62.83 73.58 67.78 
+nominal SRL-specific  
features 
69.90 75.11 72.55 
Table 5: The performance of nominal SRL on the 
development data with golden parse trees and golden 
nominal predicates 
After performing the greedy feature selection 
algorithm on the development data, features 
{ao1, ai6, ai2P, ai5, ao2, ao12, ao14}, as pro-
posed in Section 3.2, are selected consecutively 
for argument identification, while features {ai7, 
ao1, ai1, ao2, ai5, ao4} are selected for argument 
classification. Table 5 presents the SRL results 
on the development data. It shows that nominal 
SRL-specific features significantly improve the 
performance from 67.78 to 72.55 ( ) 
in F1-measure. 
05.0;2 <p?
Effect of features derived from verbal SRL 
 
Features Rec.(%) Pre.(%) F1 
baseline 67.86 73.63 70.63  
+ao5 68.15 73.60 70.77 (+0.14)
+ao6 67.66 72.80 70.14 (-0.49)
+ao7 68.20 75.41 71.62 (+0.99)
+ao8 68.30 75.39 71.67 (+1.04)
+p1 67.91 74.40 71.00 (+0.37)
+p2 67.76 74.20 70.83 (+0.20)
+p3 67.96 74.69 71.16 (+0.53)
+p4 68.01 74.18 70.96 (+0.33)
+p5 68.01 75.01 71.39 (+0.76)
+p6 68.20 75.12 71.49 (+0.86)
+p7 68.40 75.70 71.87 (+1.24)
Table 6: Effect of features derived from verbal SRL 
on the performance of nominal SRL on the test data 
with golden parse trees and golden nominal predi-
cates. The first row presents the performance using 
traditional and nominal SRL-specific features. 
 
 
 Rec.(%) Pre.(%) F1 
baseline  67.86 73.63 70.63 
+features derived 
from verbal SRL
68.40 77.51 72.67 
Xue (2008) 66.1 73.4 69.6 
Table 7: The performance of nominal SRL on the test 
data with golden parse trees and golden nominal 
predicates 
 
Table 6 shows the effect of features derived 
from verbal SRL in an incremental way. It 
shows that only the feature ao6 has negative ef-
fect due to its strong relevance with intervening 
verbs and thus not included thereafter. Table 7 
shows the performance on the test data with or 
without using the features derived from the ver-
bal SRL system. It shows these features signifi-
cantly improve the performance ( ) 
on nominal SRL. Table 7 also shows our system 
outperforms Xue (2008) by 3.1 in F1-measure. 
05.0;2 <p?
6.3. Results with Automatic Parse Trees 
and Golden Nominal Predicates 
In previous section we have assumed the avail-
ability of golden parse trees during the testing 
process. Here we conduct experiments on auto-
matic parse trees, using the Berkeley parser. 
Since arguments come from constituents in 
parse trees, those arguments, which do not align 
with any syntactic constituents, are simply dis-
carded. Moreover, for any nominal predicate 
segmented incorrectly by the word segmenter, 
all its arguments are unable to be labeled neither. 
Table 8 presents the SRL performance on the 
test data by using automatic parse trees. It shows 
that the performance drops from 72.67 to 60.87 
in F1-measure when replacing golden parse trees 
with word-based automatic ones, partly due to 
the absence of 6.9% arguments in automatic 
trees, and wrong POS tagging of nominal predi-
cates. Table 8 also compares our system with 
Xue (2008). It shows that our system also out-
performs Xue (2008) on Chinese NomBank. 
 Rec. (%) Pre. (%) F1 
This paper 56.95(53.55) 66.74(66.69) 60.87(59.40)
Xue (2008) 53.1 (52.9) 62.9 (62.3) 57.6 (57.3) 
Table 8: The performance of nominal SRL on the test 
data with automatic parse trees and golden predicates. 
Here, the numbers outside the parentheses indicate 
the performance using a word-based parser, while the 
numbers inside indicate the performance using a 
character-based parser4. 
                                                          
4 About 1.6% nominal predicates are mistakenly segmented 
by the character-based parser, thus their arguments are 
missed directly. 
1286
 6.4. Results with Automatic Nominal Predi-
cates 
So far nominal predicates are assumed to be 
manually annotated and available. Here we turn 
to a more realistic scenario in which both the 
parse tree and nominal predicates are automati-
cally obtained. In the following, we first report 
the results of automatic nominal predicate rec-
ognition and then the results of nominal SRL on 
automatic recognition of nominal predicates. 
Results of nominal predicate recognition 
Parses g1-g5 Rec.(%) Pre.(%) F1 
no 91.46 88.93 90.18 golden 
yes 92.62 89.36 90.96 
word-based yes 86.39 81.80 84.03 
character-based yes 84.79 81.94 83.34 
Table 9: The performance of automatic nominal 
predicate recognition on the test data 
 
Table 9 lists the predicate recognition results, 
using the parse tree structure, as shown in Sec-
tion 5, and the convolution tree kernel, as pro-
posed in Collins and Duffy (2001). The second 
column (g1-g5) indicates whether the global fea-
tures (g1-g5) are included in the parse tree struc-
ture. We have also defined a simple rule that 
treats a noun which is ever a verb or a nominal 
predicate in the training data as a nominal predi-
cate. Based on golden parse trees, the rule re-
ceives the performance of 81.40 in F1-measure. 
This suggests that our method significantly out-
performs the simple rule-based one. Table 9 also 
shows that: 
z As a complement to local structural informa-
tion, global features improve the performance 
of automatic nominal predicate recognition 
by 0.78 in F1-measure. 
z The word-based syntactic parser decreases 
the F1-measure from 90.96 to 84.03, mostly 
due to the POSTagging errors between NN 
and VV, while the character-based syntactic 
parser further drops the F1-measure by 0.69, 
due to automatic word segmentation. 
Results with automatic predicates 
 
Parses Predicates Rec.(%) Pre.(%) F1 
golden 68.40 77.51 72.67 golden 
automatic 65.07 74.65 69.53 
golden 55.95 66.74 60.87 word-
based automatic 52.67 59.56 55.90 
golden 53.55 66.69 59.40 character-
based automatic 50.66 59.60 54.77 
Table 10: The performance of nominal SRL on the 
test data with the choices of golden/automatic parse 
trees and golden/automatic predicates 
In order to have a clear performance comparison 
among nominal SRL on golden/automatic parse 
trees and golden/automatic predicates, Table 10 
lists all the results in those scenarios. 
6.5. Comparison 
Chinese nominal SRL vs. Chinese verbal SRL 
Comparison with Xue (2008) shows that the per-
formance of Chinese nominal SRL is about 20 
lower (e.g. 72.67 vs. 92.38 in F1-measure) than 
that of Chinese verbal SRL, partly due to the 
smaller amount of annotated data (about 1/5) in 
Chinese NomBank than that in Chinese Prop-
Bank. Moreover, according to Chinese Nom-
Bank annotation criteria (Xue 2006a), even 
when a noun is a true deverbal noun, not all of 
its modifiers are legitimate arguments or ad-
juncts of this predicate. Only arguments that can 
co-occur with both the nominal and verbal forms 
of the predicate are considered in the NomBank 
annotation. This means that the judgment of ar-
guments is semantic rather than syntactic. These 
facts may also partly explain the lower nominal 
SRL performance, especially the performance of 
argument identification. This can be illustrated 
by the statistics on the development data that 
96% (40%) of verbal (nominal) predicates? sis-
ters are annotated as arguments. Finally, the 
predicate-argument structure of nominal predi-
cates is more flexible and complicated than that 
of verbal predicates as illustrated in Xue (2006a). 
Chinese nominal SRL vs. English nominal 
SRL 
Liu and Ng (2007) reported the performance of 
77.04 and 72.83 in F1-measure on English Nom-
Bank when golden and automatic parse trees are 
used, respectively. Taking into account that Chi-
nese verbal SRL achieves comparable perform-
ance with English verbal SRL on golden parse 
trees, the performance gap between Chinese and 
English nominal SRL (e.g. 72.67 vs. 77.04 in 
F1-measure) presents great challenge for Chi-
nese nominal SRL. Moreover, while automatic 
parse trees only decrease the performance of 
English nominal SRL by about 4.2 in F1-
measure, automatic parse trees significantly de-
crease the performance of Chinese nominal SRL 
by more than 12 in F1-measure due to the much 
lower performance of Chinese syntactic parsing. 
7. Conclusion 
In this paper we investigate nominal SRL in 
Chinese language. In particular, some nominal 
SRL-specific features are included to improve 
1287
 the performance. Moreover, various features 
derived from verbal SRL are properly integrated 
into nominal SRL. Finally, a convolution tree 
kernel is adopted to address the issue of auto-
matic nominal predicates recognition, which is 
essential in a nominal SRL system.  
To our best knowledge, this is the first re-
search on 
1) Exploring Chinese nominal SRL on auto-
matic parse trees with automatic predicate 
recognition; 
2) Successfully integrating features derived 
from Chinese verbal SRL into Chinese nomi-
nal SRL with much performance improve-
ment. 
Acknowledgement  
This research was supported by Project 
60673041 and 60873150 under the National 
Natural Science Foundation of  China, Project 
2006AA01Z147 under the  ?863? National 
High-Tech Research and Development of China, 
and Project BK2008160 under the Natural Sci-
ence Foundation of the Jiangsu province of 
China. We also want to thank Dr. Nianwen Xue 
for share of the verb class file. We also want to 
thank the reviewers for insightful comments. 
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of COLING-ACL 1998. 
Xavier Carreras and Lluis M?rquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2004.  
Xavier Carreras and Lluis M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2005.  
Michael Collins and Nigel Duffy. 2001. Convolution 
Kernels for Natural Language. In Proceedings of 
NIPS 2001.  
Weiwei Ding and Baobao Chang. 2008. Improving 
Chinese Semantic Role Classification with Hierar-
chical Feature Selection Strategy. In Proceedings 
of EMNLP 2008. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic 
role Labeling of NomBank: a Maximum Entropy 
Approach. In Proceedings of EMNLP 2006.  
Chang Liu and Hwee Tou Ng. 2007. Learning Predic-
tive Structures for Semantic Role Labeling of 
NomBank. In Proceedings of ACL 2007. 
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Yong, and R. Grishman. 2004. Anno-
tating Noun Argument Structure for NomBank. In 
Proceedings of LREC 2004.  
Alessandro Moschitti. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. In Pro-
ceedings of ACL 2004. 
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion Answering based on Semantic Structures. In 
Proceedings of COLING 2004.  
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-speech Tagging: One-at-a-time or All-at-once? 
Word-based or Character-based? In Proceedings 
of EMNLP 2004. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics. 
Slav Petrov. and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceesings of 
NAACL 2007.  
Simone Paolo Ponzetto and Michael Strube. 2006. 
Semantic Role Labeling for Coreference Resolu-
tion. In Proceedings of EACL 2006. 
Sameer Pradhan, Honglin Sun, Wayne Ward, James 
H. Martin, and Dan Jurafsky. 2004. Parsing Ar-
guments of Nominalizations in English and Chi-
nese. In Proceedings of NAACL-HLT 2004.  
Honglin Sun and Daniel Jurafsky. 2004. Shallow 
Semantic Parsing of Chinese. In Proceedings of 
NAACL 2004.  
Mihai Surdeanu, Sanda Harabagiu, John Williams 
and Paul Aarseth. 2003. Using Predicate-argument 
Structures for Information Extraction. In Proceed-
ings of ACL 2003. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluis M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of 
Syntactic and Semantic Dependencies. In Pro-
ceedings of CoNLL 2008. 
Nianwen Xue and Martha Palmer. 2003. Annotating 
the Propositions in the Penn Chinese TreeBank. In 
Proceedings of 2nd SIGHAN Workshop on Chinese 
Language Processing.  
Nianwen Xue and Martha Palmer. 2005. Automatic 
Semantic Role Labeling for Chinese verbs. In 
Proceedings of IJCAI 2005.  
Nianwen Xue. 2006a. Annotating the Predicate-
Argument Structure of Chinese Nominalizations. 
In Proceedings of the LREC 2006. 
Nianwen Xue. 2006b. Semantic Role Labeling of 
Nominalized Predicates in Chinese. In Proceed-
ings of HLT-NAACL 2006. 
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
1288
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1437?1445,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Semi-Supervised Learning for Semantic Relation Classification using 
Stratified Sampling Strategy 
 
Longhua Qian   Guodong Zhou   Fang Kong   Qiaoming Zhu 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology, Soochow University 
1 Shizi Street, Suzhou, China 215006 
{qianlonghua,gdzhou,kongfang,qmzhu}@suda.edu.cn 
 
 
 
 
Abstract 
 
This paper presents a new approach to 
selecting the initial seed set using stratified 
sampling strategy in bootstrapping-based 
semi-supervised learning for semantic relation 
classification.  First, the training data is 
partitioned into several strata according to 
relation types/subtypes, then relation instances 
are randomly sampled from each stratum to 
form the initial seed set. We also investigate 
different augmentation strategies in iteratively 
adding reliable instances to the labeled set, and 
find that the bootstrapping procedure may stop 
at a reasonable point to significantly decrease 
the training time without degrading too much 
in performance. Experiments on the ACE 
RDC 2003 and 2004 corpora show the 
stratified sampling strategy contributes more 
than the bootstrapping procedure itself. This 
suggests that a proper sampling strategy is 
critical in semi-supervised learning. 
1 Introduction 
With the dramatic increase in the amount of 
textual information available in digital archives 
and the WWW, there has been growing interest 
in techniques for automatically extracting 
information from text documents. Information 
Extraction (IE) is such a technology that IE 
systems are expected to identify relevant 
information (usually of pre-defined types) from 
text documents in a certain domain and put them 
in a structured format. 
According to the scope of the NIST Automatic 
Content Extraction (ACE) program (ACE, 2000-
2007), current research in IE has three main 
objectives: Entity Detection and Tracking (EDT), 
Relation Detection and Characterization (RDC), 
and Event Detection and Characterization (EDC). 
This paper focuses on the ACE RDC subtask, 
where many machine learning methods have 
been proposed, including supervised methods 
(Miller et al, 2000; Zelenko et al, 2002; Culotta 
and Soresen, 2004; Kambhatla, 2004; Zhou et al, 
2005; Zhang et al, 2006; Qian et al, 2008), 
semi-supervised methods (Brin, 1998; Agichtein 
and Gravano, 2000; Zhang, 2004; Chen et al, 
2006; Zhou et al, 2008), and unsupervised 
methods (Hasegawa et al, 2004; Zhang et al, 
2005).  
Current work on semantic relation extraction 
task mainly uses supervised learning methods, 
since it achieves relatively better performance. 
However this method requires a large amount of 
manually labeled relation instances, which is 
both time-consuming and laborious. In the 
contrast, unsupervised methods do not need 
definitions of relation types and hand-tagged data, 
but it is difficult to evaluate their performance 
since there are no criteria for evaluation. 
Therefore, semi-supervised learning has received 
more and more attention, as it can balance the 
advantages and disadvantages between 
supervised and unsupervised methods. With the 
plenitude of unlabeled natural language data at 
hand, semi-supervised learning can significantly 
reduce the need for labeled data with only 
limited sacrifice in performance. Specifically, a 
bootstrapping algorithm chooses the unlabeled 
instances with the highest probability of being 
correctly labeled and use them to augment 
labeled training data iteratively.  
Although previous work (Yarowsky, 1995; 
Blum and Mitchell, 1998; Abney, 2000; Zhang, 
2004) has tackled the bootstrapping approach 
from both the theoretical and practical point of 
view, many key problems still remain unresolved, 
such as the selection of initial seed set. Since the 
size of the initial seed set is usually small (e.g. 
1437
100 instances), the imbalance of relation types or 
manifold structure (cluster structure) in it will 
severely weaken the strength of bootstrapping. 
Therefore, it is critical for a bootstrapping 
approach to select the most appropriate initial 
seed set. However, current systems (Zhang, 2004; 
Chen et al, 2006) use a randomly sampling 
strategy, which fails to explore the affinity nature 
among the training instances. Alternatively, 
Zhou et al (2008) bootstrap a set of weighted 
support vectors from both labeled and unlabeled 
data using SVM. Nevertheless, the initial labeled 
data is still randomly generated only to ensure 
that there are at least 5 instances for every 
relation subtype. 
This paper presents a new approach to 
selecting the initial seed set based on stratified 
sampling strategy in the bootstrapping procedure 
for semi-supervised semantic relation 
classification. The motivation behind the 
stratified sampling is that every relation type 
should be as much as possible represented in the 
initial seed set, thus leading to more instances 
with diverse structures being added to the labeled 
set. In addition, we also explore different 
strategies to augment reliably classified instances 
to the labeled data iteratively, and attempt to find 
a stoppage criterion for the iteration procedure to 
greatly decrease the training time, other than 
using up all the unlabeled set. 
The rest of this paper is organized as follows. 
First, Section 2 reviews related work on semi-
supervised relation extraction. Then we present 
an underlying supervised learner in Section 3. 
Section 4 details various key aspects of the 
bootstrapping procedure, including the stratified 
sampling strategy. Experimental results are 
reported in Section 5. Finally we conclude our 
work in Section 6. 
2 Related Work 
Within the realm of information extraction, 
currently there are several representative semi-
supervised learning systems for extracting 
relations between named entities. 
DIPRE (Dual Iterative Pattern Relation 
Expansion) (Brin, 1998) is a system based on 
bootstrapping that exploits the duality between 
patterns and relations to augment the target 
relation starting from a small sample. However, 
it only extracts simple relations such as (author, 
title) pairs from the WWW. Snowball (Agichtein 
and Gravano, 2000) is another bootstrapping-
based system that extracts relations from 
unstructured text. Snowball shares much in 
common with DIPRE, including the use of both 
the bootstrapping framework and the pattern 
matching approach to extract new unlabeled 
instances. Due to pattern matching techniques, 
their systems are hard to be adapted to the 
general problem of relation extraction. 
Zhang (2004) approaches the relation 
classification problem with bootstrapping on top 
of SVM. He uses various lexical and syntactic 
features in the BootProject algorithm based on 
random feature projection to extract top-level 
relation types in the ACE corpus. Evaluation 
shows that bootstrapping can alleviate the burden 
of hand annotations for supervised learning 
methods to a certain extent.  
Chen et al (2006) investigate a semi-
supervised learning algorithm based on label 
propagation for relation extraction, where labeled 
and unlabeled examples and their distances are 
represented as the nodes and the weights of 
edges respectively in a connected graph, then the 
label information is propagated from any vertex 
to nearby vertices through weighted edges 
iteratively, finally the labels of unlabeled 
examples are inferred after the propagation 
process converges.  
Zhou et al (2008) integrate the advantages of 
SVM bootstrapping in learning critical instances 
and label propagation in capturing the manifold 
structure in both the labeled and unlabeled data, 
by first bootstrapping a moderate number of 
weighted support vectors through a co-training 
procedure from all the available data, and then 
applying label propagation algorithm via the 
bootstrapped support vectors. 
However, in most current systems, the initial 
seed set is selected randomly such that they may 
not adequately represent the inherent structure of 
unseen examples, hence the power of 
bootstrapping may be severely weakened. 
This paper presents a simple yet effective 
approach to generate the initial seed set by 
applying the stratified sampling strategy, 
originated from statistics theory. Furthermore, 
we try to employ the same stratified strategy to 
augment the labeled set. Finally, we attempt to 
find a reasonable criterion to terminate the 
iteration process. 
3 Underlying Supervised Learning 
A semi-supervised learning system usually 
consists of two relevant components: an 
underlying supervised learner and a 
1438
bootstrapping algorithm on top of it. In this 
section we discuss the former, while the latter 
will be described in the following section.  
In this paper, we select Support Vector 
Machines (SVMs) as the underlying supervised 
classifier since it represents the state-of-the-art in 
the machine learning research community, and 
there are good implementations of the algorithm 
available. Specifically, we use LIBSVM (Chang 
et al, 2001), an effective tool for support vector 
classification, since it supports multi-class 
classification and provides probability estimation 
as well. 
For each pair of entity mentions, we extract 
and compute various lexical and syntactic 
features, as employed in a state-of-the-art 
relation extraction system (Zhou et al, 2005). 
(1) Words: According to their positions, four 
categories of words are considered: a) the words 
of both the mentions; b) the words between the 
two mentions; c) the words before M1; and d) 
the words after M2.  
(2) Entity type: This category of features 
concerns about the entity types of both the 
mentions. 
(3) Mention Level: This category of features 
considers the entity level of both the mentions. 
(4) Overlap: This category of features includes 
the number of other mentions and words between 
two mentions. Typically, the overlap features are 
usually combined with other features such as 
entity type and mention level. 
(5) Base phrase chunking: The base phrase 
chunking is proved to play an important role in 
semantic relation extraction. Most of the 
chunking features concern about the headwords 
of the phrases between the two mentions.  
In this paper, we do not employ any deep 
syntactic or semantic features (such as 
dependency tree, full parse tree etc.), since they 
contribute quite limited in relation extraction. 
4 Bootstrapping & Stratified Sampling 
We first present the self-bootstrapping algorithm, 
and then discuss several key problems on 
bootstrapping in the order of initial seed 
selection, augmentation of labeled data and 
stoppage criterion for iteration. 
4.1 Bootstrapping Algorithm 
Following Zhang (2004), we define a basic self-
bootstrapping strategy, which keeps augmenting 
the labeled data set with the models 
straightforwardly trained from previously 
available labeled data as follows: 
Require: labeled seed set L
Require: unlabeled data set U
Require: batch size S
Repeat
    Train a single classifier on L
    Run the classifier on U
    Find at most S instances in U that the classifier has
the highest prediction confidence
    Add them into L
Until: no data points available or the stoppage
condition is reached
Algorithm self-bootstrapping
Figure 1. Self-bootstrapping algorithm 
In order to measure the confidence of the 
classifier?s prediction, we compute the entropy 
of the label probability distribution that the 
classifier assigns to the class label on an example 
(the lower the entropy, the higher the confidence): 
log
n
i i
i
H p p= ??      (1) 
Where n denotes the total number of relation 
classes, and pi denotes the probability of current 
example being classified as the ith class.  
4.2 Stratified Sampling for Initial Seeds  
Normally, the number of available labeled 
instances is quite limited (usually less than 100 
instances) when the iterative bootstrapping 
procedure begins. If the distribution of the initial 
seed set fails to approximate the distribution of 
the test data, the augmented data generated from 
bootstrapping would not capture the essence of 
relation types, and the performance on the test 
set will significantly decrease even only after one 
or two rounds of iterations. Therefore, the 
selection of initial seed set plays an important 
role in bootstrapping-based semantic relation 
extraction. 
Sampling is a part of statistical practice 
concerned with the selection of individual 
observations, which is intended to yield some 
knowledge about a population of interest. When 
dealing with the task of semi-supervised 
semantic relation classification, the population is 
the training set of relation instances from the 
ACE RDC corpora. We compare two practical 
sampling strategies as follows: 
(1) Randomly sampling, which picks the initial 
seeds from the training data using a random 
scheme. Each element thus has an equal 
probability of selection, and the population is not 
1439
subdivided or partitioned. Currently, most work 
on semi-supervised relation extraction employs 
this method. However, since the size of the initial 
seed set is very small, they are not guaranteed to 
capture the statistical properties of the whole 
training data, let alne of the test data. 
(2) Stratified sampling. When the population 
embraces a number of distinct categories, 
stratified sampling (Neyman, 1934) can be 
applied to this case. First, the population can be 
organized by these categories into separate 
"strata", then a sample is selected within each 
"stratum" separately, and randomly. Generally, 
the sample size is normally proportional to the 
relative size of the strata. The main motivation 
for using a stratified sampling design is to ensure 
that particular groups within a population are 
adequately represented in the sample. 
It is well known that the number of the 
instances for each relation type in the ACE RDC 
corpora is greatly unbalanced  (Zhou et al, 2005) 
as shown in Table 1 for the ACE RDC 2004 
corpus. When the relation instances for a specific 
relation type occurs frequently in the initial seed 
set, the classifier will achieve good performance 
on this type, otherwise the classifier can hardly 
recognize them from the test set. In order for 
every type of relations to be properly represented, 
the stratified sampling strategy is applied to the 
seed selection procedure. 
Types Subtypes Train Test
Located 593 145
Near 70 17
PHYS 
Part-Whole 299 79
Business 134 39
Family 101 20
PER-SOC 
Other 44 11
Employ-Executive 388 101
Employ-Staff 427 112
Employ-Undetermined 66 12
Member-of-Group 152 39
Subsidiary 169 37
Partner 10 2
EMP-ORG 
Other 64 16
User-or-Owner 160 40
Inventor-or-Man. 8 1
ART 
Other 1 1
Ethnic 31 8
Ideology 39 9
OTHER-
AFF 
Other 43 11
Citizen-or-Resid. 226 47
Based-In 165 50
GPE-AFF 
Other 31 8
DISC  224 55
Total  3445 860
Table 1. Numbers of relations on the ACE RDC 
2004: break down by relation types and subtypes 
Figure 2 illustrates the stratified sampling 
strategy we use in bootstrapping, where RSET 
denotes the training set, V is the stratification 
variable, and SeedSET denotes the initial seed set. 
First, we divide the relation instances into 
different strata according to available properties, 
such as major relation type (considering reverse 
relations or not) and relation subtype 
(considering reverse relations or not). Then 
within every stratum, a certain number of 
instances are sampled randomly, and this number 
is normally proportional to the size of that 
stratum in the whole population. However, when 
this number is 0 due to the rounding of real 
numbers, it is set to 1. Also it must be ensured 
that the total number of instances being sampled 
is NS. Finally, these instances form the initial 
seed set and can be used as the input to the 
underlying supervised learning for the 
bootstrapping procedure. 
 
Require: RSET ={R1,R2,?,RN} 
Require: V = {v1, v2,?,vK} 
Require: SeedSET with the size of NS (100) 
Initialization: 
SeedSET = NULL 
Steps: 
z Group RSET into K strata according to the 
stratified variable V, i.e.:  
RSET={RSET1,RSET2,?,RSETK} 
z Calculate the class prior probability for each 
stratum i={1,2,?,K} 
)(/)( RSETNUMRSETNUMP ii =  
z Caculate the number of intances being sampled 
for each stratum 
NPN ii ?=  
If Ni =0 then Ni=1 
z Calculate the difference of numbers as follows: 
?
=
? ?=
K
i
iS NNN
1
 
z If N?>0 then add Ni (i=1,2,?,|N?|) by 1 
If N?<0 then subtract 1 from Ni (i=1,2,...,|N?|) 
z For each i from 1 to K 
Select Ni instances from RESTi randomly 
Add them into SeedSET 
 
Figure 2. Stratefied Sampling for initial seeds 
4.3 Augmentation of labeled data 
After each round of iteration, some newly 
classified instances with the highest confidence 
can be augmented to the labeled training data. 
Nevertheless, just like the selection of initial seed 
set, we still wish that every stratum would be 
represented as appropriately as possible in the 
1440
instances added to the labeled set. In this paper, 
we compare two kinds of augmentation strategies 
available: 
(1) Top n method: the classified instances are 
first sorted in the ascending order by their 
entropies (i.e. decreasing confidence), and then 
the top n (usually 100) instances are chosen to be 
added.  
(2) Stratified method: in order to make the 
added instances representative for their stratum, 
we first select m (usually greater than n) 
instances with the highest confidence, then we 
choose n instances from them using the stratified 
strategy. 
4.4 Stoppage of Iterations 
In a self-bootstrapping procedure, as the 
iterations go on, both the reliable and unreliable 
instances are added to the labeled data 
continuously, hence the performance will 
fluctuate in a relatively small range. The key 
question here is how we can know when the 
bootstrapping procedure reaches its best 
performance on the test data. The bootstrapping 
algorithm by Zhang (2004) stops after it runs out 
of all the training instances, which may take a 
relatively long time. In this paper, we present a 
method to determine the stoppage criterion based 
on the mean entropy as follows: 
Hi <= p    (2) 
Where Hi denotes the mean entropy of the 
confidently classified instances being augmented 
to the labeled data in each iteration, and p 
denotes a threshold for the mean entropy, which 
will be fixed through empirical experiments. 
This criterion is based on the assumption that 
when the mean entropy becomes less than or 
equal to a certain threshold, the classifier would 
achieve the most reliable confidence on the 
instances being added to the labeled set, and it 
may be impossible to yield better performance 
since then. Therefore, the iteration may stop at 
that reasonable point.  
5 Experimentation 
This section aims to empirically investigate the 
effectiveness of the bootstrapping-based semi-
supervised learning we discussed above for 
semantic relation classification. In particular, 
different methods for selecting the initial seed set 
and augmenting the labeled data are evaluated. 
5.1 Experimental Setting 
We use the ACE corpora as the benchmark data, 
which are gathered from various newspapers, 
newswire and broadcasts. The ACE 2004 corpus 
contains 451 documents and 5702 positive 
relation instances. It defines 7 relation types and 
23 subtypes between 7 entity types. For easy 
reference with related work in the literature, 
evaluation is also done on 347 documents 
(including nwire and bnews domains) and 4305 
relation instances using 5-fold cross-validation. 
That is, these relation instances are first divided 
into 5 sets, then, one of them (about 860 
instances) is used as the test data set, while the 
others are regarded as the training data set, from 
which the initial seed set is sampled. In the ACE 
2003 corpus, the training set consists of 674 
documents and 9683 positive relation instances 
while the test data consists of 97 documents and 
1386 positive relation instances. The ACE RDC 
2003 task defines 5 relation types and 24 
subtypes between 5 entity types. 
The corpora are first parsed using Collins?s 
parser (Collins, 2003) with the boundaries of all 
the entity mentions kept. Then, the parse trees 
are converted into chunklink format using 
chunklink.pl 1. Finally, various useful lexical and 
syntactic features, as described in Subsection 3.1, 
are extracted and computed accordingly. For the 
purpose of comparison, we define our task as the 
classification of the 5 or 7 major relation types in 
the ACE RDC 2003 and 2004 corpora. 
For LIBSVM parameters, we adopted the 
polynomial kernel, and c is set to 10, g is set to 
0.15. Under this setting, we achieved the best 
classification performance. 
5.2 Experimental Results 
In this subsection, we compare and discuss the 
experimental results using various sampling 
strategies, different augmentation methods, and 
iteration stoppage criterion. 
 
Comparison of sampling strategies in selecting 
the initial seed set 
Table 2 and Table 3 show the initial and the 
highest classification performance of 
Precision/Recall/F-measure for various sampling 
strategies of the initial seed set on 7 major 
relation types of the ACE RDC 2004 corpus 
respectively when the size of initial seed set L is 
100, the batch size S is 100, and the top 100 
                                                 
1 http://ilk.kub.nl/~sabine/chunklink/ 
1441
instances with the highest confidence are added 
at each iteration. Table 2 also lists the number of 
strata for stratified sampling methods from which 
the initial seeds are randomly chosen 
respectively. Table 3 additionally lists the time 
needed to complete the bootstrapping process (on 
a PC with a Pentium IV 3.0G CPU and 1G 
memory). In this paper, we consider the 
following five experimental settings when 
sampling the initial seeds: 
z Randomly Sampling: as described in 
Subsection 4.2. 
z Stratified-M Sampling: the strata are 
grouped in terms of major relation types 
without considering reverse relations. 
z Stratified-MR Sampling: the strata are 
grouped in terms of major relation types, 
including reverse relations. 
z Stratified-S Sampling: the strata are 
grouped in terms of relation subtypes 
without considering reverse subtypes. 
z Stratified-SR Sampling: the strata are 
grouped in terms of relation subtypes, 
including reverse subtypes. 
For each sampling strategies, we performed 20 
trials and computed average scores and the total 
time on the test set over these 20 trials. 
Sampling strategies 
for initial seeds 
# of 
strat. P(%) R(%) F 
Randomly 1 66.1 65.9 65.9
Stratified-M 7 69.1 66.5 67.7
Stratified-MR 13 69.3 67.3 68.2
Stratified-S 30 69.8 67.7 68.7
Stratified-SR 39 69.9 68.5 69.2
Table 2. The initial performance of applying 
various sampling strategies to selecting the initial 
seed set on the ACE RDC 2004 corpus 
Sampling strategies 
for initial seeds 
Time 
(min) P(%) R(%) F 
Randomly 52 68.6 66.2 67.3
Stratified-M 65 71.0 66.9 68.8
Stratified-MR 65 71.6 67.0 69.2
Stratified-S 71 72.7 67.8 70.1
Stratified-SR 77 72.9 68.4 70.6
Table 3. The highest performance of applying 
various sampling strategies in selecting the initial 
seed set on the ACE RDC 2004 corpus 
 
These two tables jointly indicate that the self-
bootstrapping procedure for all sampling 
strategies can moderately improve the 
classification performance by ~1.2 units in F-
score, which is also verified by Zhang (2004). 
Furthermore, they show that: 
z The most improvements in performance 
come from improvements in precision. Actually, 
for some settings the recalls even decrease 
slightly. The reason may be that due to the nature 
of self-bootstrapping, the instances augmented at 
each iteration are always those which are the 
most similar to the initial seed instances, 
therefore the models trained from them would 
exhibit higher precision on the test set, while it 
virtually does no help for recall. 
z All of the four stratified sampling methods 
outperform the randomly sampling method to 
various degrees, both in the initial performance 
and the highest performance. This means that 
sampling of the initial seed set based on 
stratification by major/sub relation types can be 
helpful to relation classification, largely due to 
the performance improvement of the initial seed 
set, which is caused by adequate representation 
of instances for every relation type. 
z Of all the four stratified sampling methods, 
the Stratified-SR sampling achieves the best 
performance of 72.9/68.4/70.6 in P/R/F. 
Moreover, the more the number of strata 
generated by the sampling strategy, the more 
appropriately they would be represented in the 
initial seed set, and the better performance it will 
yield. This also implies that the hierarchy of 
relation types/subtypes in the ACE RDC 2004 
corpus is fairly reasonably defined. 
z An important conclusion, which can be 
draw accordingly, is that the F-score 
improvement of Stratified-SR sampling over 
Randomly sampling in initial performance (3.3 
units) is significantly greater than the F-score 
improvement gained by bootstrapping itself 
using Randomly sampling (1.4 units). This means 
that the sampling strategy of the initial seed set is 
even more important than the bootstrapping 
algorithm itself for relation classification. 
z It is interesting to note that the time needed 
to bootstrap increases with the number of strata. 
The reason may be that due to more diverse 
structures in the labeled data for stratified 
sampling, the SVM needs more time to 
differentiate between instances, i.e. more time to 
learn the models. 
 
Comparison of different augmentation 
strategies of training data 
Figure 3 compares the performance of F-score 
for two augmentation strategies: the Top n 
method and the stratified method, over various 
initial seed sampling strategies on the ACE RDC 
2004 corpus. For each iteration, a variable 
1442
number (m is ranged from 100 to 500) of 
classified instances in the decreasing order of 
confidence are first chosen as the base examples, 
then at most 100 examples are selected from the 
base examples to be augmented to the labeled set. 
Specifically, when m is equal to 100, the whole 
set of the base example is added to the labeled 
data, i.e. degenerated to the Top n augmentation 
strategy. On the other hand, when m is greater 
than 100, we wish we would select examples of 
different major relation types from the base 
examples according to their distribution in the 
training set, in order to achieve the performance 
improvement as much as the stratified sampling 
does in the selection of the initial seed set. 
64
65
66
67
68
69
70
71
72
100 200 300 400 500
# Base examples
F-
sc
or
e
Randomly
Stratified-M 
Stratified-MR
Stratified-S
Stratified-SR
Figure 3. Comparison of two augmentation 
strategies over different sampling strategies in 
selecting the initial seed set. 
This figure shows that, except for randomly 
sampling strategy, the stratified augmentation 
strategies improve the performance. Nevertheless, 
this result is far from our expectation in two 
ways: 
z The performance improvement in F-score is 
trivial, at most 0.4 units on average. The reason 
may be that, although we try to add as many as 
100 classified instances to the labeled data 
according to the distribution of every major 
relation type in the training set, the top m 
instances with the highest confidence are usually 
focused on certain relation types (e.g. PHSY and 
PER-SOC), this leads to the stratified 
augmentation failing to function effectively. 
Hence, all the following experiments will only 
adopt Top n method for augmenting the labeled 
data. 
z With the increase of the number of the base 
examples, the performance fluctuates slightly, 
thus it is relatively difficult to recognize where 
the optima is. We think there are two 
contradictory factors that affect the performance. 
While the reliability of the instances extracted 
from the base examples decreases with the 
increase of the number of base examples, the 
probability of extracting instances of more 
relation types increases with the increase of the 
number of the base examples. These two factors 
inversely interact with each other, leading to the 
fluctuation in performance. 
 
Comparison of different threshold values for 
stoppage criterion 
We compare the performance and 
bootstrapping time (20 trials with the same initial 
seed set) when applying stoppage criterion in 
Formula (2) with different threshold p over 
various sampling strategies on the ACE RDC 
2004 corpus in Figure 4 and Figure 5 
respectively. These two figures jointly show that: 
64
65
66
67
68
69
70
71
0 0.2 0.22 0.24 0.26 0.28 0.3
p
F-
sc
or
e
Randomly
Stratified-M
Stratified-MR
Stratified-S
Stratified-SR
Figure 4. Performance for different p values 
0
10
20
30
40
50
60
70
80
90
0 0.2 0.22 0.24 0.26 0.28 0.3
p
Ti
m
e(
mi
n)
Randomly
Stratified-M
Stratified-MR
Stratified-S
Stratified-SR
Figure 5. Bootstrapping time for different p 
values 
z The performance decreases slowly while the 
bootstrapping time decreases dramatically with 
the increase of p from 0 to 0.3. Specifically, 
when the p equals to 0.3, the bootstrapping time 
tends to be neglected, while the performance is 
almost similar to the initial performance. It 
implies that we can find a reasonable point for 
each sampling strategy, at which the time falls 
greatly while the performance nearly does not 
degrade.  
1443
Bootproject LP-js Stratified Bootstrapping Relation types 
P R F P R F P R F 
ROLE 78.5 69.7 73.8 81.0 74.7 77.7 74.7 86.3 80.1
PART 65.6 34.1 44.9 70.1 41.6 52.2 66.4 47.0 55.0
AT 61.0 84.8 70.9 74.2 79.1 76.6 74.9 66.1 70.2
NEAR - - - 13.7 12.5 13.0 100.0 2.9 5.6
SOC 47.0 57.4 51.7 45.0 59.1 51.0 65.2 79.0 71.4
Average 67.9 67.4 67.6 73.6 69.4 70.9 73.8 73.3 73.5
Table 4. Comparison of semi-supervised relation classification systems on the ACE RDC 2003 corpus 
 
z Clearly, if the performance is the primary 
concern, then p=0.2 may be the best choice in 
that we can get ~30% saving on the time at the 
cost of only ~0.08 loss in F-score on average. If 
the time is a primary concern, then p=0.22 is a 
reasonable threshold in that we get ~50% saving 
on the time at the cost of ~0.25 units loss in F-
score on average. This suggests that our 
proposed stoppage criterion is effective to 
terminate the bootstrapping procedure with 
minor performance loss. 
 
Comparison of Stratified Bootstrapping with 
Bootproject and Label propagation  
Table 4 compares Bootproject (Zhang, 2004), 
Label propagation (Chen et al, 2006) with our 
Stratified Bootstrapping on the 5 major types of 
the ACE RDC 2003 corpus. 
Both Bootproject and Label propagation 
select 100 initial instances randomly, and at each 
iteration, the top 100 instances with the highest 
confidence are added to the labeled data. 
Differently, we choose 100 initial seeds using 
stratified sampling strategy; similarly, the top 
100 instances with the highest confidence are 
augmented to the labeled data at each iteration. 
Due to the lack of comparability followed from 
the different size of the labeled data used in 
(Zhou et al, 2008), we omit their results here. 
This table shows that our stratified 
bootstrapping procedure significantly 
outperforms both Bootproject and Label 
Propagation methods on the ACE RDC corpus, 
with the increase of 5.9/4.1 units in F-score on 
average respectively. Stratified bootstrapping 
consistently outperforms Bootproject in every 
major relation type, while it outperforms Label 
Propagation in three of the major relation types, 
especially SOC type, with the exception of AT 
and NEAR types. The reasons may be follows. 
Although there are many AT relation instances in 
the corpus, they are scattered divergently in 
multi-dimension space so that they tend to be 
relatively difficult to be recognized via SVM. 
For the NEAR relation instances, they occur least 
frequently in the whole corpus, so it is very hard 
for them to be identified via SVM. By contrast, 
even small size of labeled instances can be fully 
utilized to correctly induce the unlabeled 
instances via LP algorithm due to its ability to 
exploit manifold structures of both labeled and 
unlabeled instances (Chen et al, 2006). 
In general, these results again suggest that the 
sampling strategy in selecting the initial seed set 
plays a critical role for relation classification, and 
stratified sampling can significantly improve the 
performance due to proper selection of the initial 
seed set. 
6 Conclusion 
This paper explores several key issues in semi-
supervised learning based on bootstrapping for 
semantic relation classification. The application 
of stratified sampling originated from statistics 
theory to the selection of the initial seed set 
contributes most to the performance 
improvement in the bootstrapping procedure. In 
addition, the more strata the training data is 
divided into, the better performance will be 
achieved. However, the augmentation of the 
labeled data using the stratified strategy fails to 
function effectively largely due to the 
unbalanced distribution of the confidently 
classified instances, rather than the stratified 
sampling strategy itself. Furthermore, we also 
propose a mean entropy-based stoppage criterion 
in the bootstrapping procedure, which can 
significantly decrease the training time with little 
loss in performance. Finally, it also shows that 
our method outperforms other state-of-the-art 
semi-supervised ones. 
 
Acknowledgments 
This research is supported by Project 60673041 
and 60873150 under the National Natural 
Science Foundation of China, Project 
2006AA01Z147 under the ?863? National High-
Tech Research and Development of China, 
1444
Project BK2008160 under the Jiangsu Natural 
Science Foundation of China, and the National 
Research Foundation for the Doctoral Program 
of Higher Education of China under Grant No. 
20060285008. We would also like to thank the 
excellent and insightful comments from the three 
anonymous reviewers. 
References  
S. Abney. Bootstrapping. 2002. In Proceedings of the 
40th Annual Meeting of the Association for 
Computational  Linguistics (ACL 2002). 
ACE 2002-2007. The Automatic Content Extraction 
(ACE) Projects. 2007. http//www.ldc.upenn.edu/ 
Projects/ACE/. 
E. Agichtein and L. Gravano. 2000. Snowball: 
Extracting relations from large plain-text 
collections. In Proceedings of the 5th ACM 
international Conference on Digital Libraries 
(ACMDL 2000). 
A. Blum and T. Mitchell. 1996. Combining labeled 
and unlabeled data with co-training. In COLT: 
Proceedings of the workshop on Computational 
Learning Theory. Morgan Kaufmann Publishers. 
S. Brin. 1998. Extracting patterns and relations from 
the world wide web. In WebDB Workshop at 6th 
International Conference on Extending Database 
Technology (EDBT 98). 
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library 
for support vector machines. http:// 
www.csie.ntu.edu.tw/~cjlin/libsvm. 
M. Collins. 2003. Head-Driven Statistics Models for 
Natural Language Parsing. Computational 
linguistics, 29(4): 589-617. 
J.X. Chen, D.H. Ji, and L.T. Chew. 2006. Relation 
Extraction using Label Propagation Based Semi 
supervised Learning. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and the 44th Annual Meeting of the 
Association of Computational Linguistics 
(COLING/ACL 2006), pages 129-136. July 2006, 
Sydney, Australia.  
A. Culotta and J. Sorensen. 2004. Dependency tree 
kernels for relation extraction. In Proceedings of 
the 42nd Annual Meeting of the Association of 
Computational Linguistics (ACL 2004), pages 423-
439. 21-26 July 2004, Barcelona, Spain. 
T. Hasegawa, S. Sekine, and R. Grishman. 2004. 
Discovering Relations among Named Entities from 
Large Corpora. In Proceedings of the 42nd Annual 
Meeting of the Association of Computational 
Linguistics (ACL 2004). 21-26 July 2004, 
Barcelona, Spain. 
N. Kambhatla. Combining lexical, syntactic and 
semantic features with Maximum Entropy models 
for extracting relations. In Proceedings of the 42nd 
Annual Meeting of the Association of 
Computational Linguistics (ACL 2004)(posters), 
pages 178-181. 21-26 July 2004, Barcelona, Spain. 
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 
2000. A novel use of statistical parsing to extract 
information from text. In Proceedings of the 6th 
Applied Natural Language Processing Conference. 
29 April-4 May 2000, Seattle, USA. 
J. Neyman. 1934. On the Two Different Aspects of 
the Representative Method: The Method of 
Stratified Sampling and the Method of Purposive 
Selection. Journal of the Royal Statistical Society, 
97(4): 558-625. 
L.H. Qian, G.D. Zhou, Q.M. Zhu, and P.D Qian. 2008. 
Exploiting constituent dependencies for tree 
kernel-based semantic relation extraction. In 
Proceedings of The 22nd International Conference 
on Computational Linguistics (COLING 2008), 
pages 697-704. 18-22 August 2008, Manchester, 
UK. 
D. Yarowsky. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. In the 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics (ACL 
95), pages 189-196. 26-30 June 1995, MIT, 
Cambridge, Massachusetts, USA. 
D. Zelenko, C. Aone, and A. Richardella. 2003. 
Kernel Methods for Relation Extraction. Journal of 
Machine Learning Research, (2): 1083-1106. 
M. Zhang, J. Zhang, J. Su, and G.D. Zhou. 2006. A 
Composite Kernel to Extract Relations between 
Entities with both Flat and Structured Features. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and the 44th Annual 
Meeting of the Association of Computational 
Linguistics (COLING/ACL 2006), pages 825-832. 
Sydney, Australia. 
M. Zhang, J. Su, D. M. Wang, G. D. Zhou, and C. L. 
Tan. 2005. Discovering Relations between Named 
Entities from a Large Raw Corpus Using Tree 
Similarity-Based Clustering. In Proceedings of the 
2nd international Joint Conference on Natural 
Language Processing (IJCNLP-2005), pages 378-
389. Jeju Island, Korea.  
Z. Zhang. 2004. Weakly-supervised relation 
classification for Information Extraction. In 
Proceedings of ACM 13th conference on 
Information and Knowledge Management (CIKM 
2004). 8-13 Nov 2004, Washington D.C., USA. 
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. 
Exploring various knowledge in relation extraction. 
In Proceedings of the 43rd Annual Meeting of the 
Association of Computational Linguistics (ACL 
2005), pages 427-434. Ann Arbor, USA. 
G.D. Zhou, J.H. Li, L.H. Qian, and Q.M. Zhu. 2008. 
Semi-Supervised Learning for Relation Extraction. 
In Proceedings of the 3rd International Joint 
Conference on Natural Language Processing 
(IJCNLP-2008), page 32-38. 7-12 January 2008, 
Hyderabad, India. 
 
1445
Context-Sensitive Convolution Tree Kernel 
for Pronoun Resolution 
 
 
ZHOU GuoDong    KONG Fang    ZHU Qiaoming 
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow Univ.  Suzhou, China 215006 
Email: {gdzhou, kongfang, qmzhu}@suda.edu.cn 
 
 
Abstract 
This paper proposes a context-sensitive convo-
lution tree kernel for pronoun resolution. It re-
solves two critical problems in previous 
researches in two ways. First, given a parse 
tree and a pair of an anaphor and an antecedent 
candidate, it implements a dynamic-expansion 
scheme to automatically determine a proper 
tree span for pronoun resolution by taking 
predicate- and antecedent competitor-related 
information into consideration. Second, it ap-
plies a context-sensitive convolution tree ker-
nel, which enumerates both context-free and 
context-sensitive sub-trees by considering their 
ancestor node paths as their contexts. Evalua-
tion on the ACE 2003 corpus shows that our 
dynamic-expansion tree span scheme can well 
cover necessary structured information in the 
parse tree for pronoun resolution and the con-
text-sensitive tree kernel much outperforms 
previous tree kernels.  
1 Introduction 
It is well known that syntactic structured informa-
tion plays a critical role in many critical NLP ap-
plications, such as parsing, semantic role labeling, 
semantic relation extraction and co-reference reso-
lution. However, it is still an open question on 
what kinds of syntactic structured information are 
effective and how to well incorporate such struc-
tured information in these applications. 
Much research work has been done in this direc-
tion. Prior researches apply feature-based methods 
to select and define a set of flat features, which can 
be mined from the parse trees, to represent particu-
lar structured information in the parse tree, such as 
the grammatical role (e.g. subject or object), ac-
cording to the particular application. Indeed, such 
feature-based methods have been widely applied in 
parsing (Collins 1999; Charniak 2001), semantic 
role labeling (Pradhan et al2005), semantic rela-
tion extraction (Zhou et al2005) and co-reference 
resolution  (Lapin and Leass 1994; Aone and Ben-
nett 1995; Mitkov 1998; Yang et al2004; Luo and 
Zitouni 2005; Bergsma and Lin 2006). The major 
problem with feature-based methods on exploring 
structured information is that they may fail to well 
capture complex structured information, which is 
critical for further performance improvement.  
The current trend is to explore kernel-based 
methods (Haussler, 1999) which can implicitly 
explore features in a high dimensional space by 
employing a kernel to calculate the similarity be-
tween two objects directly. In particular, the ker-
nel-based methods could be very effective at 
reducing the burden of feature engineering for 
structured objects in NLP, e.g. the parse tree struc-
ture in coreference resolution. During recent years, 
various tree kernels, such as the convolution tree 
kernel (Collins and Duffy 2001), the shallow parse 
tree kernel (Zelenko et al2003) and the depend-
ency tree kernel (Culota and Sorensen 2004), have 
been proposed in the literature. Among previous 
tree kernels, the convolution tree kernel represents 
the state-of-the-art and have been successfully ap-
plied by Collins and Duffy (2002) on parsing, Mo-
schitti (2004) on semantic role labeling, Zhang et 
al (2006) on semantic relation extraction  and Yang 
et al(2006) on pronoun resolution.  
However, there exist two problems in Collins 
and Duffy?s kernel. The first is that the sub-trees 
enumerated in the tree kernel are context-free. That 
is, each sub-tree enumerated in the tree kernel does 
not consider the context information outside the 
sub-tree. The second is how to decide a proper tree 
span in the tree kernel computation according to 
the particular application. To resolve above two 
problems, this paper proposes a new tree span 
scheme and applies a new tree kernel and to better 
capture syntactic structured information in pronoun 
25
resolution, whose task is to find the corresponding 
antecedent for a given pronominal anaphor in text. 
The rest of this paper is organized as follows. In 
Section 2, we review related work on exploring 
syntactic structured information in pronoun resolu-
tion and their comparison with our method. Section 
3 first presents a dynamic-expansion tree span 
scheme by automatically expanding the shortest 
path to include necessary structured information, 
such as predicate- and antecedent competitor-
related information. Then it presents a context-
sensitive convolution tree kernel, which not only 
enumerates context-free sub-trees but also context-
sensitive sub-trees by considering their ancestor 
node paths as their contexts. Section 4 shows the 
experimental results. Finally, we conclude our 
work in Section 5.  
2 Related Work 
Related work on exploring syntactic structured 
information in pronoun resolution can be typically 
classified into three categories: parse tree-based 
search algorithms (Hobbs 1978), feature-based  
(Lappin and Leass 1994; Bergsma and Lin 2006) 
and tree kernel-based methods (Yang et al2006).  
As a representative for parse tree-based search 
algorithms, Hobbs (1978) found the antecedent for 
a given pronoun by searching the parse trees of 
current text. It processes one sentence at a time 
from current sentence to the first sentence in text 
until an antecedent is found. For each sentence, it 
searches the corresponding parse tree in a left-to-
right breadth-first way. The first antecedent candi-
date, which satisfies hard constraints (such as gen-
der and number agreement), would be returned as 
the antecedent. Since the search is completely done 
on the parse trees, one problem with the parse tree-
based search algorithms is that the performance 
would heavily rely on the accuracy of the parse 
trees. Another problem is that such algorithms are 
not good enough to capture necessary structured 
information for pronoun resolution. There is still a 
big performance gap even on correct parse trees. 
Similar to other NLP applications, feature-
based methods have been widely applied in pro-
noun resolution to explore syntactic structured in-
formation from the parse trees. Lappin and Leass 
(1994) derived a set of salience measures (e.g. sub-
ject, object or accusative emphasis) with manually 
assigned weights from the syntactic structure out-
put by McCord?s Slot Grammar parser. The candi-
date with the highest salience score would be 
selected as the antecedent. Bergsma and Lin (2006) 
presented an approach to pronoun resolution based 
on syntactic paths. Through a simple bootstrapping 
procedure, highly co-reference paths can be 
learned reliably to handle previously challenging 
instances and robustly address traditional syntactic 
co-reference constraints. Although feature-based 
methods dominate on exploring syntactic struc-
tured information in the literature of pronoun reso-
lution, there still exist two problems with them. 
One problem is that the structured features have to 
be selected and defined manually, usually by lin-
guistic intuition. Another problem is that they may 
fail to effectively capture complex structured parse 
tree information. 
As for tree kernel-based methods, Yang et al
(2006) captured syntactic structured information 
for pronoun resolution by using the convolution 
tree kernel (Collins and Duffy 2001) to measure 
the common sub-trees enumerated from the parse 
trees and achieved quite success on the ACE 2003 
corpus. They also explored different tree span 
schemes and found that the simple-expansion 
scheme performed best. One problem with their 
method is that the sub-trees enumerated in Collins 
and Duffy?s kernel computation are context-free, 
that is, they do not consider the information out-
side the sub-trees. As a result, their ability of ex-
ploring syntactic structured information is much 
limited. Another problem is that, among the three 
explored schemes, there exists no obvious over-
whelming one, which can well cover syntactic 
structured information.  
The above discussion suggests that structured 
information in the parse trees may not be well util-
ized in the previous researches, regardless of fea-
ture-based or tree kernel-based methods. This 
paper follows tree kernel-based methods. Com-
pared with Collins and Duffy?s kernel and its ap-
plication in pronoun resolution (Yang et al2006), 
the context-sensitive convolution tree kernel enu-
merates not only context-free sub-trees but also 
context-sensitive sub-trees by taking their ancestor 
node paths into consideration. Moreover, this paper 
also implements a dynamic-expansion tree span 
scheme by taking predicate- and antecedent com-
petitor-related information into consideration. 
26
3 Context Sensitive Convolution Tree 
Kernel for Pronoun Resolution 
In this section, we first propose an algorithm to 
dynamically determine a proper tree span for pro-
noun resolution and then present a context-
sensitive convolution tree kernel to compute simi-
larity between two tree spans. In this paper, all the 
texts are parsed using the Charniak parser 
(Charniak 2001) based on which the tree span is 
determined. 
3.1 Dynamic-Expansion Tree Span Scheme 
Normally, parsing is done on the sentence level. To 
deal with the cases that an anaphor and an antece-
dent candidate do not occur in the same sentence, 
we construct a pseudo parse tree for an entire text 
by attaching the parse trees of all its sentences to 
an upper ?S? node, similar to Yang et al(2006). 
Given the parse tree of a text, the problem is 
how to choose a proper tree span to well cover syn-
tactic structured information in the tree kernel 
computation. Generally, the more a tree span in-
cludes, the more syntactic structured information 
would be provided, at the expense of more noisy 
information. Figure 2 shows the three tree span 
schemes explored in Yang et al(2006): Min-
Expansion (only including the shortest path con-
necting the anaphor and the antecedent candidate), 
Simple-Expansion (containing not only all the 
nodes in Min-Expansion but also the first level 
children of these nodes) and Full-Expansion (cov-
ering the sub-tree between the anaphor and the 
candidate), such as the sub-trees inside the dash 
circles of Figures 2(a), 2(b) and 2(c) respectively. 
It is found (Yang et al2006) that the simple-
expansion tree span scheme performed best on the 
ACE 2003 corpus in pronoun resolution. This sug-
gests that inclusion of more structured information 
in the tree span may not help in pronoun resolution. 
To better capture structured information in the 
parse tree, this paper presents a dynamic-expansion 
scheme by trying to include necessary structured 
information in a parse tree. The intuition behind 
our scheme is that predicate- and antecedent com-
petitor- (all the other compatible1 antecedent can-
didates between the anaphor and the considered 
antecedent candidate) related information plays a 
critical role in pronoun resolution. Given an ana-
                                                           
1 With matched number, person and gender agreements. 
phor and an antecedent candidate, e.g. ?Mary? and 
?her? as shown in Figure 1, this is done by: 
1) Determining the min-expansion tree span via 
the shortest path, as shown in Figure 1(a). 
2) Attaching all the antecedent competitors along 
the corresponding paths to the shortest path. As 
shown in Figure 1(b), ?the woman? is attached 
while ?the room? is not attached since the for-
mer is compatible with the anaphor and the lat-
ter is not compatible with the anaphor. In this 
way, the competition between the considered 
candidate and other compatible candidates can 
be included in the tree span. In some sense, this 
is a natural extension of the twin-candidate 
learning approach proposed in Yang et al
(2003), which explicitly models the competition 
between two antecedent candidates. 
3) For each node in the tree span, attaching the 
path from the node to the predicate terminal 
node if it is a predicate-headed node. As shown 
in Figure 1(c), ?said? and ?bit? are attached. 
4) Pruning those nodes (except POS nodes) with 
the single in-arc and the single out-arc and with 
its syntactic phrase type same as its child node. 
As shown in Figure 1(d), the left child of the 
?SBAR? node, the ?NP? node, is removed and 
the sub-tree (NP the/DT woman/NN) is at-
tached to the ?SBAR? node directly.  
To show the difference among min-, simple-, 
full- and dynamic-expansion schemes, Figure 2 
compares them for three different sentences, given 
the anaphor ?her/herself? and the antecedent can-
didate ?Mary?. It shows that:  
? Min-, simple- and full-expansion schemes have 
the same tree spans (except the word nodes) for 
the three sentences regardless of the difference 
among the sentences while the dynamic-
expansion scheme can adapt to difference ones. 
? Normally, the min-expansion scheme is too 
simple to cover necessary information (e.g. ?the 
woman? in the 1st sentence is missing).  
? The full-expansion scheme can cover all the 
information at the expense of much noise (e.g. 
?the man in that room? in the 2nd sentence).  
? The simple-expansion scheme can cover some 
necessary predicate-related information (e.g. 
?said? and ?bit? in the sentences). However, it 
may introduce some noise (e.g. the left child of 
27
the ?SBAR? node, the ?NP? node, may not be 
necessary in the 2nd sentence) and ignore neces-
sary antecedent competitor-related information 
(e.g. ?the woman? in the 1st sentence). 
? The dynamic-expansion scheme normally 
works well. It can not only cover predicate-
related information but also structured informa-
tion related with the competitors of the consid-
ered antecedent candidate. In this way, the 
competition between the considered antecedent 
candidate and other compatible candidates can 
be included in the dynamic-expansion scheme. 
 
 
 
Figure 1: Dynamic-Expansion Tree Span Scheme 
 
  
 
Figure 2: Comparison of Min-, Simple-, Full-and Dynamic-Expansions: More Examples 
28
3.2 Context-Sensitive Convolution Tree Kernel 
Given any tree span scheme, e.g. the dynamic-
expansion scheme in the last subsection, we now 
study how to measure the similarity between two 
tree spans using a convolution tree kernel. 
A convolution kernel (Haussler D., 1999) aims 
to capture structured information in terms of sub-
structures. As a specialized convolution kernel, the 
convolution tree kernel, proposed in Collins and 
Duffy (2001), counts the number of common sub-
trees (sub-structures) as the syntactic structure 
similarity between two parse trees. This convolu-
tion tree kernel has been successfully applied by 
Yang et al(2006) in pronoun resolution. However, 
there is one problem with this tree kernel: the sub-
trees involved in the tree kernel computation are 
context-free (That is, they do not consider the in-
formation outside the sub-trees.). This is contrast 
to the tree kernel proposed in Culota and Sorensen 
(2004) which is context-sensitive, that is, it consid-
ers the path from the tree root node to the sub-tree 
root node. In order to integrate the advantages of 
both tree kernels and resolve the problem in 
Collins and Duffy?s kernel, this paper applies the 
same context-sensitive convolution tree kernel, 
proposed by Zhou et al(2007) on relation extrac-
tion. It works by taking ancestral information (i.e. 
the root node path) of sub-trees into consideration: 
? ?
=
?
?
D=
m
i
Nn
Nn
ii
C
ii
ii
nnTTK
1
]2[]2[
]1[]1[
11
11
11
])2[],1[(])2[],1[(  (1) 
where ][1 jN
i is the set of root node paths with 
length i in tree T[j] while the maximal length of a 
root node path is defined by m; and 
])2[],1[( 11
ii nnD  counts the common context-
sensitive sub-trees rooted at root node paths ]1[1
in  
and ]2[1
in . In the tree kernel, a sub-tree becomes 
context-sensitive via the ?root node path? moving 
along the sub-tree root. For more details, please 
refer to Zhou et al(2007). 
4 Experimentation 
This paper focuses on the third-person pronoun 
resolution and, in all our experiments, uses the 
ACE 2003 corpus for evaluation. This ACE corpus 
contains ~3.9k pronouns in the training data and 
~1.0k pronouns in the test data.  
Similar to Soon et al(2001), an input raw text is 
first preprocessed automatically by a pipeline of 
NLP components, including sentence boundary 
detection, POS tagging, named entity recognition 
and phrase chunking, and then a training or test 
instance is formed by a pronoun and one of its an-
tecedent candidates. During training, for each ana-
phor encountered, a positive instance is created by 
pairing the anaphor and its closest antecedent 
while a set of negative instances is formed by pair-
ing the anaphor with each of the non-coreferential 
candidates. Based on the training instances, a bi-
nary classifier is generated using a particular learn-
ing algorithm. In this paper, we use SVMLight 
deleveloped by Joachims (1998). During resolution, 
an anaphor is first paired in turn with each preced-
ing antecedent candidate to form a test instance, 
which is presented to a classifier. The classifier 
then returns a confidence value indicating the like-
lihood that the candidate is the antecedent. Finally, 
the candidate with the highest confidence value is 
selected as the antecedent. In this paper, the NPs 
occurring within the current and previous two sen-
tences are taken as the initial antecedent candidates, 
and those with mismatched number, person and 
gender agreements are filtered out. On average, an 
anaphor has ~7 antecedent candidates. The per-
formance is evaluated using F-measure instead of 
accuracy since evaluation is done on all the pro-
nouns occurring in the data.  
Scheme/m 1 2 3 4 
Min 78.5 79.8 80.8 80.8 
Simple 79.8 81.0 81.7 81.6 
Full 78.3 80.1 81.0 81.1 
Dynamic 80.8 82.3 83.0 82.9 
Table 1: Comparison of different context-sensitive  
convolution tree kernels and tree span schemes 
(with entity type info attached at both the anaphor 
and the antecedent candidate nodes by default) 
In this paper, the m parameter in our context-
sensitive convolution tree kernel as shown in 
Equation (1) indicates the maximal length of root 
node paths and is optimized to 3 using 5-fold cross 
validation on the training data. Table 1 systemati-
cally evaluates the impact of different m in our 
context-sensitive convolution tree kernel and com-
pares our dynamic-expansion tree span scheme 
with the existing three tree span schemes, min-, 
29
simple- and full-expansions as described in Yang 
et al(2006). It also shows that that our tree kernel 
achieves best performance with m = 3 on the test 
data, which outperforms the one with m = 1 by 
~2.2 in F-measure. This suggests that the parent 
and grandparent nodes of a sub-tree  contain much 
information for pronoun resolution while 
considering more ancestral nodes doesnot further 
improve the performance. This may be due to that, 
although our experimentation on the training data 
indicates that  more than 90% (on average) of 
subtrees has a root node path longer than 3 (since 
most of the subtrees are deep from the root node 
and more than 90% of the parsed trees are deeper 
than 6 levels in the ACE 2003 corpus), including a 
root node path longer than 3 may be vulnerable to 
the full parsing errors and have negative impact. It 
also shows that our dynamic-expansion tree span 
scheme outperforms min-expansion, simple-
expansion and full-expansion schemes by ~2.4, 
~1.2 and ~2.1 in F-measure respectively. This 
suggests the usefulness of dynamically expanding 
tree spans to cover necessary structured 
information in pronoun resolution. In all the 
following experiments, we will apply our tree 
kernel with m=3 and the dynamic-expansion tree 
span scheme by default, unless specified. 
We also evaluate the contributions of antecedent 
competitor-related information, predicate-related 
information and pruning in our dynamic-expansion 
tree span scheme by excluding one of them from 
the dynamic-expansion scheme. Table 2 shows that 
1) antecedent competitor-related information con-
tributes much to our scheme; 2) predicate-related 
information contributes moderately; 3) pruning 
only has slight contribution. This suggests the im-
portance of including the competition in the tree 
span and the effect of predicate-argument struc-
tures in pronoun resolution. This also suggests that 
our scheme can well make use of such predicate- 
and antecedent competitor-related information.  
Dynamic Expansion Effect 
- Competitors-related Info 81.1(-1.9) 
- Predicates-related Info 82.2 (-0.8) 
- Pruning 82.8(-0.2)  
All 83.0 
Table 2: Contributions of different factors in our 
dynamic-expansion tree span scheme 
Table 3 compares the performance of different 
tree span schemes for pronouns with antecedents in 
different sentences apart. It shows that our dy-
namic-expansion scheme is much more robust than 
other schemes with the increase of sentences apart. 
Scheme /  
#Sentences Apart 
0 1 2 
Min 86.3 76.7 39.6 
Simple 86.8 77.9 43.8 
Full 86.6 77.4 35.4 
Dynamic 87.6 78.8 54.2 
Table 3: Comparison of tree span schemes with 
antecedents in different sentences apart 
5 Conclusion 
Syntactic structured information holds great poten-
tial in many NLP applications. The purpose of this 
paper is to well capture syntactic structured infor-
mation in pronoun resolution. In this paper, we 
proposes a context-sensitive convolution tree ker-
nel to resolve two critical problems in previous 
researches in pronoun resolution by first automati-
cally determining a dynamic-expansion tree span, 
which effectively covers structured information in 
the parse trees by taking predicate- and antecedent 
competitor-related information into consideration, 
and then applying a context-sensitive convolution 
tree kernel, which enumerates both context-free 
sub-trees and context-sensitive sub-trees. Evalua-
tion on the ACE 2003 corpus shows that our dy-
namic-expansion tree span scheme can better 
capture necessary structured information than the 
existing tree span schemes and our tree kernel can 
better model structured information than the state-
of-the-art Collins and Duffy?s kernel.  
For the future work, we will focus on improving 
the context-sensitive convolution tree kernel by 
better modeling context-sensitive information and 
exploring new tree span schemes by better incor-
porating useful structured information. In the 
meanwhile, a more detailed quantitative evaluation 
and thorough qualitative error analysis will be per-
formed to gain more insights. 
Acknowledgement  
This research is supported by Project 60673041 
under the National Natural Science Foundation of 
China and Project 2006AA01Z147 under the ?863? 
National High-Tech Research and Development of 
China. 
30
References  
Aone C and Bennett W.W. (1995). Evaluating auto-
mated and manual acquisition of anaphora resolu-
tion strategies. ACL?1995:122-129. 
Bergsma S. and Lin D.K.(2006). Bootstrapping path-
based pronoun resolution. COLING-ACL?2006: 33-
40. 
Charniak E. (2001). Immediate-head Parsing for Lan-
guage Models. ACL?2001: 129-137. Toulouse, 
France 
Collins M. (1999) Head-driven statistical models for 
natural language parsing. Ph.D. Thesis. University 
of Pennsylvania. 
Collins M. and Duffy N. (2001). Convolution Ker-
nels for Natural Language. NIPS?2001: 625-632. 
Cambridge, MA 
Culotta A. and Sorensen J. (2004). Dependency tree 
kernels for relation extraction. ACL?2004. 423-429. 
21-26 July 2004. Barcelona, Spain. 
Haussler D. (1999). Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, Uni-
versity of California, Santa Cruz. 
Hobbs J. (1978). Resolving pronoun references. Lin-
gua. 44:339-352. 
Joachims T. (1998). Text Categorization with Sup-
port Vector Machine: learning with many relevant 
features. ECML-1998: 137-142.  Chemnitz, Ger-
many 
Lappin S. and Leass H. (1994). An algorithm for pro-
nominal anaphora resolution. Computational Lin-
guistics. 20(4):526-561. 
Mitkov R. (1998). Robust pronoun resolution with 
limited knowledge. COLING-ACL?1998:869-875. 
Montreal, Canada.  
Moschitti A. (2004). A study on convolution kernels 
for shallow semantic parsing. ACL?2004:335-342. 
Pradhan S., Hacioglu K., Krugler V., Ward W., Mar-
tin J.H. and Jurafsky D. (2005). Support Vector 
Learning for Semantic Argument Classification. 
Machine Learning. 60(1):11-39. 
Soon W. Ng H.T.and Lim D. (2001). A machine 
learning approach to creference resolution of noun 
phrases. Computational Linguistics. 27(4): 521-544. 
Yang X.F., Zhou G.D., Su J. and Tan C.L., Corefer-
ence Resolution Using Competition Learning Ap-
proach, ACL?2003):176-183. Sapporo, Japan, 7-12 
July 2003. 
Yang X.F., Su J. and Tan C.L. (2006). Kernel-based 
pronoun resolution with structured syntactic knowl-
edge. COLING-ACL?2006: 41-48. 
Zelenko D., Aone C. and Richardella. (2003). Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. 3(Feb):1083-1106. 
Zhang M., Zhang J., Su J. and Zhou G.D. (2006). A 
Composite Kernel to Extract Relations between En-
tities with both Flat and Structured Features. 
COLING-ACL-2006: 825-832. Sydney, Australia 
Zhou G.D., Su J. Zhang J. and Zhang M. (2005). Ex-
ploring various knowledge in relation extraction. 
ACL?2005. 427-434. 25-30 June, Ann Arbor, Mich-
gan, USA. 
Zhou G.D., Zhang M., Ji D.H. and Zhu Q.M. (2007). 
Tree Kernel-based Relation Extraction with Con-
text-Sensitive Structured Parse Tree Information. 
EMNLP-CoNLL?2007 
31
Semi-Supervised Learning for Relation Extraction  
 
ZHOU GuoDong    LI JunHui    QIAN LongHua    ZHU Qiaoming 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow Univ., Suzhou, China 215006 
Email : {gdzhou, lijunhui, qianlonghua, qmzhu}@suda.edu.cn 
 
Abstract 
This paper proposes a semi-supervised learn-
ing method for relation extraction. Given a 
small amount of labeled data and a large 
amount of unlabeled data, it first bootstraps a 
moderate number of weighted support vectors 
via SVM through a co-training procedure with 
random feature projection and then applies a 
label propagation (LP) algorithm via the boot-
strapped support vectors. Evaluation on the 
ACE RDC 2003 corpus shows that our method 
outperforms the normal LP algorithm via all 
the available labeled data without SVM boot-
strapping. Moreover, our method can largely 
reduce the computational burden. This sug-
gests that our proposed method can integrate 
the advantages of both SVM bootstrapping 
and label propagation.  
1 Introduction 
Relation extraction is to detect and classify various 
predefined semantic relations between two entities 
from text and can be very useful in many NLP ap-
plications such as question answering, e.g. to an-
swer the query ?Who is the president of the United 
States??, and information retrieval, e.g. to expand 
the query ?George W. Bush? with ?the president of 
the United States? via his relationship with ?the 
United States?. 
During the last decade, many methods have 
been proposed in relation extraction, such as su-
pervised learning (Miller et al2000; Zelenko et al
2003; Culota and Sorensen 2004; Zhao and Grish-
man 2005; Zhang et al2006; Zhou et al2005, 
2006), semi-supervised learning (Brin 1998; 
Agichtein and Gravano 2000; Zhang 2004; Chen et 
al 2006), and unsupervised learning (Hasegawa et 
al 2004; Zhang et al2005). Among these methods, 
supervised learning-based methods perform much 
better than the other two alternatives. However, 
their performance much depends on the availability 
of a large amount of manually labeled data and it is 
normally difficult to adapt an existing system to 
other applications and domains. On the other hand, 
unsupervised learning-based methods do not need 
the definition of relation types and the availability 
of manually labeled data. However, they fail to 
classify exact relation types between two entities 
and their performance is normally very low. To 
achieve better portability and balance between hu-
man efforts and performance, semi-supervised 
learning has drawn more and more attention re-
cently in relation extraction and other NLP appli-
cations. 
This paper proposes a semi-supervised learning 
method for relation extraction. Given a small 
amount of labeled data and a large amount of unla-
beled data, our proposed method first bootstraps a 
moderate number of weighted support vectors from 
all the available data via SVM using a co-training 
procedure with random feature projection and then 
applies a label propagation (LP) algorithm to cap-
ture the manifold structure in both the labeled and 
unlabeled data via the bootstrapped support vectors. 
Compared with previous methods, our method can 
integrate the advantages of both SVM bootstrap-
ping in learning critical instances for the labeling 
function and label propagation in capturing the 
manifold structure in both the labeled and unla-
beled data to smooth the labeling function. 
The rest of this paper is as follows. In Section 2, 
we review related semi-supervised learning work 
in relation extraction. Then, the LP algorithm via 
bootstrapped support vectors is proposed in Sec-
tion 3 while Section 4 shows the experimental re-
sults. Finally, we conclude our work in Section 5.  
2 Related Work 
Generally, supervised learning is preferable to un-
supervised learning due to prior knowledge in the 
32
annotated training data and better performance. 
However, the annotated data is usually expensive 
to obtain. Hence, there has been growing interest in 
semi-supervised learning, aiming at inducing clas-
sifiers by leveraging a small amount of labeled 
data and a large amount of unlabeled data. Related 
work in relation extraction using semi-supervised 
learning can be classified into two categories: 
bootstrapping-based (Brin 1998; Agichtein and 
Gravano 2000; Zhang 2004) and label propaga-
tion(LP)-based (Chen et al2006).  
Currently, bootstrapping-based methods domi-
nate semi-supervised learning in relation extraction. 
Bootstrapping works by iteratively classifying 
unlabeled instances and adding confidently classi-
fied ones into labeled data using a model learned 
from augmented labeled data in previous iteration. 
Brin (1998) proposed a bootstrapping-based 
method on the top of a self-developed pattern 
matching-based classifier to exploit the duality 
between patterns and relations. Agichtein and Gra-
vano (2000) shared much in common with Brin 
(1998). They employed an existing pattern match-
ing-based classifier (i.e. SNoW) instead. Zhang 
(2004) approached the much simpler relation clas-
sification sub-task by bootstrapping on the top of 
SVM. Although bootstrapping-based methods have 
achieved certain success, one problem is that they 
may not be able to well capture the manifold struc-
ture among unlabeled data. 
As an alternative to the bootstrapping-based 
methods, Chen et al(2006) employed a LP-based 
method in relation extraction. Compared with 
bootstrapping, the LP algorithm can effectively 
combine labeled data with unlabeled data in the 
learning process by exploiting the manifold struc-
ture (e.g. the natural clustering structure) in both 
the labeled and unlabeled data. The rationale be-
hind this algorithm is that the instances in high-
density areas tend to carry the same labels. The LP 
algorithm has also been successfully applied in 
other NLP applications, such as word sense disam-
biguation (Niu et al2005), text classification 
(Szummer and Jaakkola 2001; Blum and Chawla 
2001; Belkin and Niyogi 2002; Zhu and Ghahra-
mani 2002; Zhu et al2003; Blum et al2004), and 
information retrieval (Yang et al2006). However, 
one problem is its computational burden, espe-
cially when a large amount of labeled and unla-
beled data is taken into consideration. 
In order to take the advantages of both boot-
strapping and label propagation, our proposed 
method propagates labels via bootstrapped support 
vectors. On the one hand, our method can well 
capture the manifold structure in both the labeled 
and unlabeled data. On the other hand, our method 
can largely reduce the computational burden in the 
normal LP algorithm via all the available data. 
3 Label Propagation via Bootstrapped 
Support Vectors 
The idea behind our LP algorithm via bootstrapped 
support vectors is that, instead of propagating la-
bels through all the available labeled data, our 
method propagates labels through critical instances 
in both the labeled and unlabeled data. In this pa-
per, we use SVM as the underlying classifier to 
bootstrap a moderate number of weighted support 
vectors for this purpose. This is based on an as-
sumption that the manifold structure in both the 
labeled and unlabeled data can be well preserved 
through the critical instances (i.e. the weighted 
support vectors bootstrapped from all the available 
labeled and unlabeled data). The reason why we 
choose SVM is that it represents the state-of-the-
art in machine learning research and there are good 
implementations of the algorithm available. In par-
ticular, SVMLight (Joachims 1998) is selected as 
our classifier. For efficiency, we apply the one vs. 
others strategy, which builds K classifiers so as to 
separate one class from all others. Another reason 
is that we can adopt the weighted support vectors 
returned by the bootstrapped SVMs as the critical 
instances, via which label propagation is done.  
3.1 Bootstrapping Support Vectors 
This paper modifies the SVM bootstrapping algo-
rithm BootProject(Zhang 2004) to bootstrap sup-
port vectors. Given a small amount of labeled data 
and a large amount of unlabeled data, the modified 
BootProject algorithm bootstraps on the top of  
SVM by iteratively classifying  unlabeled  in-
stances  and moving   confidently  classified  ones  
into  labeled data using a model learned from the 
augmented labeled data in previous  iteration,  until 
not enough unlabeled instances can be classified 
confidently. Figure 1 shows the modified BootPro-
ject algorithm for bootstrapping support vectors.  
 
33
_________________________________________ 
Assume: 
L :  the labeled data; 
U :  the unlabeled data; 
S :  the batch size (100 in our experiments); 
P :  the number of views(feature projections); 
r :   the number of classes (including all the rela-
tion (sub)types and the non-relation)  
 
BEGIN 
REPEAT 
FOR i = 1 to P DO 
Generate projected feature space iF  from 
the original feature space F ; 
Project both L  and U  onto iF , thus gener-
ate iL  and iU ; 
Train SVM classifier ijSVM  on iL  for each 
class )1( rjr j K= ; 
Run ijSVM  on iU  for each class 
)1( rjr j K=  
END FOR 
Find (at most) S instances in U  with the 
highest agreement (with threshold 70% in 
our experiments) and the highest average 
SVM-returned confidence value (with 
threshold 1.0 in our experiments); 
Move them from U to L; 
UNTIL not enough unlabeled instances (less 
than 10 in our experiments) can be confidently 
classified; 
Return all the (positive and negative) support 
vectors  included in all the latest SVM classifi-
ers ijSVM  with their collective weight (abso-
lute alpha*y) information as the set of 
bootstrapped support vectors to act as the la-
beled data in the LP algorithm; 
Return U (those hard cases which can not be 
confidently classified) to act as the unlabeled 
data in the LP algorithm; 
END 
_________________________________________ 
Figure 1: The algorithm  
for bootstrapping support vectors 
 
In particular, this algorithm generates multiple 
overlapping ?views? by projecting from the origi-
nal feature space. In this paper, feature views with 
random feature projection, as proposed in Zhang 
(2004), are explored. Section 4 will discuss this 
issue in more details. During the iterative training 
process, classifiers trained on the augmented la-
beled data using the projected views are then asked 
to vote on the remaining unlabeled instances and 
those with the highest probability of being cor-
rectly labeled are chosen to augment the labeled 
data.  
During the bootstrapping process, the support 
vectors included in all the trained SVM classifiers 
(for all the relation (sub)types and the non-relation) 
are bootstrapped (i.e. updated) at each iteration. 
When the bootstrapping process stops, all the 
(positive and negative) support vectors included in 
the SVM classifiers are returned as bootstrapped 
support vectors with their collective weights (abso-
lute a*y) to act as the labeled data in the LP algo-
rithm and all the remaining unlabeled instances (i.e. 
those hard cases which can not be confidently clas-
sified in the bootstrapping process) in the unla-
beled data are returned to act as the unlabeled data 
in the LP algorithm. Through SVM bootstrapping, 
our LP algorithm will only depend on the critical 
instances (i.e. support vectors with their weight 
information bootstrapped from all the available 
labeled and unlabeled data) and those hard in-
stances, instead of all the available labeled and 
unlabeled data.  
3.2 Label Propagation 
In the LP algorithm (Zhu and Ghahramani 2002), 
the manifold structure in data is represented as a 
connected graph. Given the labeled data (the above 
bootstrapped support vectors with their weights) 
and unlabeled data (the remaining hard instances in 
the unlabeled data after bootstrapping, including 
all the test instances for evaluation), the LP algo-
rithm first represents labeled and unlabeled in-
stances as vertices in a connected graph, then 
propagates the label information from any vertex 
to nearby vertex through weighted edges and fi-
nally infers the labels of unlabeled instances until a 
global stable stage is achieved. Figure 2 presents 
the label propagation algorithm on bootstrapped 
support vectors in details. 
 
34
_________________________________________
Assume:  
Y : the rn * labeling matrix, where ijy  repre-
sents the probability of vertex )1( nixi K=  
with label )1( rjr j K=  (including the non-
relation label); 
LY : the top l  rows of 
0Y . LY corresponds to the 
l  labeled instances; 
UY : the bottom u  rows of 
0Y . UY corresponds 
to the u  unlabeled instances; 
T : a nn *  matrix, with ijt  is the probability 
jumping from vertex ix to vertex jx ; 
 
BEGIN (the algorithm) 
Initialization:  
1) Set the iteration index 0=t ;  
2) Let 0Y  be the initial soft labels attached to 
each vertex;  
3) Let 0LY  be consistent with the labeling in 
the labeled (including all the relation 
(sub)types and the non-relation) data, where 
0
ijy = the weight of the bootstrapped support 
vector if ix  has label jr  (Please note that 
jr  can be the non-relation label) and 0 oth-
erwise;  
4) Initialize 0UY ; 
REPEAT 
Propagate the labels of any vertex to nearby 
vertices by tt YTY =+1 ; 
Clamp the labeled data, that is, replace 1+tLY  
with 0LY ; 
UNTIL Y converges(e.g. 1+tLY  converges to 
0
LY ); 
Assign each unlabeled instance with a label: for 
)( nilxi ?p , find its label with 
j
ijymaxarg ; 
END (the algorithm) 
_________________________________________ 
Figure 2: The LP algorithm 
 
 
Here, each vertex corresponds to an instance, 
and the edge between any two instances ix  and jx  
is weighted by ijw  to measure their similarity. In 
principle, larger edge weights allow labels to travel 
through easier. Thus the closer the instances are, 
the more likely they have similar labels. The algo-
rithm first calculates the weight ijw  using a kernel, 
then transforms it to ?
=
=?=
n
k
kjijij wwijpt
1
/)( , 
which measures the probability of propagating a 
label from instance jx to instance ix , and finally 
normalizes ijt row by row using ?
=
=
n
k
ikijij ttt
1
/  to 
maintain the class probability interpretation of the 
labeling matrix Y .  
During the label propagation process, the label 
distribution of the labeled data is clamped in each 
loop using the weights of the bootstrapped support 
vectors and acts like forces to push out labels 
through the unlabeled data. With this push origi-
nates from the labeled data, the label boundaries 
will be pushed much faster along edges with larger 
weights and settle in gaps along those with lower 
weights. Ideally, we can expect that ijw  across 
different classes should be as small as possible and 
ijw  within the same class as big as possible. In this 
way, label propagation happens within the same 
class most likely. 
This algorithm has been shown to converge to 
a unique solution (Zhu and Ghahramani 2002), 
which can be obtained without iteration in theory, 
and the initialization of YU0 (the unlabeled data) is 
not important since YU0 does not affect its estima-
tion. However, proper initialization of YU0 actually 
helps the algorithm converge more rapidly in prac-
tice. In this paper, each row in YU0 is initialized to 
the average similarity with the labeled instances. 
4 Experimentation 
This paper uses the ACE RDC 2003 corpus pro-
vided by LDC for evaluation. This corpus is gath-
ered from various newspapers, newswires and 
broadcasts.  
 
35
Method 
LP via bootstrapped 
(weighted) SVs 
LP via bootstrapped  
(un-weighted) SVs 
LP w/o SVM  
bootstrapping 
SVM 
(BootProject) SVM  
Bootstrapping 
5% 46.5 (+1.4) 44.5 (+1.7) 43.1 (+1.0) 35.4 (-) 40.6 (+0.9) 
10% 48.6 (+1.7) 46.5 (+2.1) 45.2 (+1.5) 38.6 (-) 43.1 (+1.4) 
25% 51.7 (+1.9) 50.4 (+2.3) 49.6 (+1.8) 43.9 (-) 47.8 (+1.7) 
50% 53.6 (+1.8) 52.6 (+2.2) 52.1 (+1.7) 47.2 (-) 50.5 (+1.6) 
75% 55.2 (+1.3) 54.5 (+1.8) 54.2 (+1.2) 53.1 (-) 53.9 (+1.2) 
100% 56.2 (+1.0) 55.8 (+1.3) 55.6 (+0.8) 55.5 (-) 55.8 (+0.7) 
Table 1: Comparison of different methods using a state-of-the-art linear kernel on the ACE RDC 2003 
corpus (The numbers inside the parentheses indicate the increases in F-measure if we add the ACE RDC 
2004 corpus as the unlabeled data) 
4.1 Experimental Setting 
In the ACE RDC 2003 corpus, the training data 
consists of 674 annotated text documents (~300k 
words) and 9683 instances of relations. During 
development, 155 of 674 documents in the training 
set are set aside for fine-tuning. The test set is held 
out only for final evaluation. It consists of 97 
documents (~50k words) and 1386 instances of 
relations. The ACE RDC 2003 task defines 5 rela-
tion types and 24 subtypes between 5 entity types, 
i.e. person, organization, location, facility and GPE. 
All the evaluations are measured on the 24 sub-
types including relation identification and classifi-
cation. 
In all our experiments, we iterate over all pairs 
of entity mentions occurring in the same sentence 
to generate potential relation instances1. For better 
evaluation, we have adopted a state-of-the-art lin-
ear kernel as similarity measurements. In our linear 
kernel, we apply the same feature set as described 
in a state-of-the-art feature-based system (Zhou et 
al 2005): word, entity type, mention level, overlap, 
base phrase chunking, dependency tree, parse tree 
and semantic information. Given above various 
lexical, syntactic and semantic features, multiple 
overlapping feature views are generated in the 
bootstrapping process using random feature projec-
tion (Zhang 2004). For each feature projection in 
bootstrapping support vectors, a feature is ran-
domly selected with probability p and therefore the 
eventually projected feature space has p*F features 
                                                           
1  In this paper, we only measure the performance of 
relation extraction on ?true? mentions with ?true? 
chaining of co-reference (i.e. as annotated by the cor-
pus annotators) in the ACE corpora. We also explic-
itly model the argument order of the two mentions 
involved and only model explicit relations because of 
poor inter-annotator agreement in the annotation of 
implicit relations and their limited number. 
on average, where F is the size of the original fea-
ture space. In this paper, p and the number of dif-
ferent views are fine-tuned to 0.5 and 10 2 
respectively using 5-fold cross validation on the 
training data of the ACE RDC 2003 corpus. 
4.2 Experimental Results 
Table 1 presents the F-measures 3  (the numbers 
outside the parentheses) of our algorithm using the 
state-of-the-art linear kernel on different sizes of 
the ACE RDC training data with all the remaining 
training data and the test data4  as the unlabeled 
data on the ACE RDC 2003 corpus. In this paper, 
we only report the performance (averaged over 5 
trials) with the percentages of 5%, 10%, 25%, 50%, 
75% and 100%5. For example, our LP algorithm 
via bootstrapped (weighted) support vectors 
achieves the F-measure of 46.5 if using only 5% of 
the ACE RDC 2003 training data as the labeled 
data and the remaining training data and the test 
data in this corpus as the unlabeled data. Table 1 
                                                           
2 This suggests that the modified BootProject algorithm 
in the bootstrapping phase outperforms the SelfBoot 
algorithm (with p=1.0 and m=1) which uses all the 
features as the only view. In the related NLP literature, 
co-training has also shown to typically outperform 
self-bootstrapping. 
3 Our experimentation also shows that most of perform-
ance improvement with either bootstrapping or label 
propagation comes from gain in recall. Due to space 
limitation, this paper only reports the overall F-
measure. 
4  In our label propagation algorithm via bootstrapped 
support vectors, the test data is only included in the 
second phase (i.e. the label propagation phase) and not 
used in the first phase (i.e. bootstrapping support vec-
tors). This is to fairly compare different semi-
supervised learning methods. 
5 We have tried less percentage than 5%. However, our 
experiments show that using much less data will suffer 
from performance un-stability. Therefore, we only re-
port the performance with percentage not less than 5%. 
36
also compares our method with SVM and the 
original SVM bootstrapping algorithm BootPro-
ject(i.e. bootstrapping on the top of SVM with fea-
ture projection, as proposed in Zhang (2004)). 
Finally, Table 1 compares our LP algorithm via 
bootstrapped (weighted by default) support vectors 
with other possibilities, such as the scheme via 
bootstrapped (un-weighted, i.e. the importance of 
support vectors is not differentiated) support vec-
tors and the scheme via all the available labeled 
data (i.e. without SVM bootstrapping). Table 1 
shows that: 
1) Inclusion of unlabeled data using semi-
supervised learning, including the SVM boot-
strapping algorithm BootProject, the normal 
LP algorithm via all the available labeled and 
unlabeled data without SVM bootstrapping, 
and our LP algorithms via bootstrapped (either 
weighted or un-weighted) support vectors, 
consistently improves the performance, al-
though semi-supervised learning has shown to 
typically decrease the performance when a lot 
of (enough) labeled data is available (Nigam 
2001).  This may be due to the insufficiency of 
labeled data in the ACE RDC 2003 corpus. 
Actually, most of relation subtypes in the two 
corpora much suffer from the data sparseness 
problem (Zhou et al2006).  
2) All the three LP algorithms outperform the 
state-of-the-art SVM classifier and the SVM 
bootstrapping algorithm BootProject. Espe-
cially, when a small amount of labeled data is 
available, the performance improvements by 
the LP algorithms are significant. This indi-
cates the usefulness of the manifold structure 
in both labeled and unlabeled data and the 
powerfulness of the LP algorithm in modeling 
such information.  
3) Our LP algorithms via bootstrapped (either 
weighted or un-weighted) support vectors out-
performs the normal LP algorithm via all the 
available labeled data w/o SVM bootstrapping. 
For example, our LP algorithm via boot-
strapped (weighted) support vectors outper-
forms the normal LP algorithm from 0.6 to 3.4 
in F-measure on the ACE RDC 2003 corpus 
respectively when the labeled data ranges from 
100% to 5%. This suggests that the manifold 
structure in both the labeled and unlabeled data 
can be well preserved via bootstrapped support 
vectors, especially when only a small amount 
of labeled data is available. This implies that 
weighted support vectors may represent the 
manifold structure (e.g. the decision boundary 
from where label propagation is done) better 
than the full set of data ? an interesting result 
worthy more quantitative and qualitative justi-
fication in the future work.   
4) Our LP algorithms via bootstrapped (weighted) 
support vectors perform better than LP algo-
rithms via bootstrapped (un-weighted) support 
vectors by ~1.0 in F-measure on average. This 
suggests that bootstrapped support vectors with 
their weights can better represent the manifold 
structure in all the available labeled and unla-
beled data than bootstrapped support vectors 
without their weights. 
5) Comparison of SVM, SVM bootstrapping and 
label propagation with bootstrapped (weighted) 
support vectors shows that both bootstrapping 
and label propagation contribute much to the 
performance improvement. 
Table 1 also shows the increases in F-measure 
(the numbers inside the parentheses) if we add all 
the instances in the ACE RDC 20046 corpus into 
the ACE RDC 2003 corpus in consideration as 
unlabeled data in all the four semi-supervised 
learning methods. It shows that adding more unla-
beled data can consistently improve the perform-
ance. For example, compared with using only 5% 
of the ACE RDC 2003 training data as the labeled 
data and the remaining training data and the test 
data in this corpus as the unlabeled data, including 
the ACE RDC 2004 corpus as the unlabeled data 
increases the F-measures of 1.4 and 1.0 in our LP 
algorithm and the normal LP algorithm respec-
tively. Table 1 shows that the contribution grows 
first when the labeled data begins to increase and 
reaches a maximum of ~2.0 in F-measure at a cer-
tain point. 
Finally, it is found in our experiments that 
critical and hard instances normally occupy only 
15~20% (~18% on average) of all the available 
labeled and unlabeled data. This suggests that, 
through bootstrapped support vectors, our LP algo-
                                                           
6  Compared with the ACE RDC 2003 task, the ACE 
RDC 2004 task defines two more entity types, i.e. 
weapon and vehicle, much more entity subtypes, and 
different 7 relation types and 23 subtypes between 7 
entity types. The ACE RDC 2004 corpus from LDC 
contains 451 documents and 5702 relation instances. 
37
rithm can largely reduce the computational burden 
since it only depends on the critical instances (i.e. 
bootstrapped support vectors with their weights) 
and those hard instances.   
5 Conclusion 
This paper proposes a new effective and efficient 
semi-supervised learning method in relation ex-
traction. First, a moderate number of weighted 
support vectors are bootstrapped from all the avail-
able labeled and unlabeled data via SVM through a 
co-training procedure with feature projection. Here, 
a random feature projection technique is used to 
generate multiple overlapping feature views in 
bootstrapping using a state-of-the-art linear kernel. 
Then, a LP algorithm is applied to propagate labels 
via the bootstrapped support vectors, which, to-
gether with those hard unlabeled instances and the 
test instances, are represented as vertices in a con-
nected graph. During the classification process, the 
label information is propagated from any vertex to 
nearby vertex through weighted edges and finally 
the labels of unlabeled instances are inferred until a 
global stable stage is achieved.  In this way, the 
manifold structure in both the labeled and unla-
beled data can be well captured by label propaga-
tion via bootstrapped support vectors. Evaluation 
on the ACE RDC 2004 corpus suggests that our LP 
algorithm via bootstrapped support vectors can 
take the advantages of both SVM bootstrapping 
and label propagation.  
For the future work, we will systematically 
evaluate our proposed method on more corpora 
and explore better metrics of measuring the simi-
larity between two instances. 
Acknowledgement  
This research is supported by Project 60673041 
under the National Natural Science Foundation of 
China and Project 2006AA01Z147 under the ?863? 
National High-Tech Research and Development of 
China. 
References  
ACE. (2000-2005). Automatic Content Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/  
Agichtein E. and Gravano L. (2000). Snowball: 
Extracting relations from large plain-text collec-
tions. Proceedings of the 5th ACM International 
Conference on Digital Libraries 
(ACMDL?2000). 
Belkin, M. and Niyogi, P. (2002). Using Manifold 
Structure for Partially Labeled Classification. 
NIPS 15. 
Blum A. and Chawla S. (2001). Learning from la-
beled and unlabeled data using graph mincuts. 
ICML?2001. 
Blum A., Lafferty J., Rwebangira R and Reddy R. 
(2004). Semi-supervised learning using random-
ized mincuts. ICML?2004. 
Brin S. (1998). Extracting patterns and relations 
from world wide web. Proceedings of WebDB 
Workshop at 6th International Conference on 
Extending Database Technology:172-183. 
Charniak E. (2001). Immediate-head Parsing for 
Language Models. ACL?2001: 129-137. Tou-
louse, France 
Chen J.X., Ji D.H., Tan C.L. and Niu Z.Y. (2006). 
Relation extraction using label propagation 
based semi-supervised learning. COLING-
ACL?2006: 129-136. July 2006. Sydney, Austra-
lia. 
Culotta A. and Sorensen J. (2004). Dependency 
tree kernels for relation extraction. ACL?2004. 
423-429. 21-26 July 2004. Barcelona, Spain. 
Hasegawa T., Sekine S. and Grishman R. (2004). 
Discovering relations among named entities 
form large corpora. ACL?2004. Barcelona, Spain. 
Miller S., Fox H., Ramshaw L. and Weischedel R. 
(2000). A novel use of statistical parsing to ex-
tract information from text. ANLP?2000. 226-
233. 29 April  - 4 May 2000, Seattle, USA 
Moschitti A. (2004). A study on convolution ker-
nels for shallow semantic parsing. 
ACL?2004:335-342. 
Nigam K.P. (2001). Using unlabeled data to im-
prove text classification. Technical Report 
CMU-CS-01-126. 
Niu Z.Y., Ji D.H., and Tan C.L. (2005). Word 
Sense Disambiguation Using Label Propagation 
Based Semi-supervised Learning. 
ACL?2005:395-402., Ann Arbor, Michigan, 
USA. 
Szummer, M., & Jaakkola, T. (2001). Partially La-
beled Classification with Markov Random 
Walks. NIPS 14. 
38
Yang L.P., Ji D.H., Zhou G.D. and Nie Y. (2006). 
Document Re-ranking using cluster validation 
and label propagation. CIKM?2006. 5-11 Nov 
2006. Arlington, Virginia, USA. 
Zelenko D., Aone C. and Richardella. (2003). Ker-
nel methods for relation extraction. Journal of 
Machine Learning Research. 3(Feb):1083-1106. 
Zhang M., Su J., Wang D.M., Zhou G.D. and Tan 
C.L. (2005). Discovering Relations from a 
Large Raw Corpus Using Tree Similarity-based 
Clustering, IJCNLP?2005, Lecture Notes in Arti-
ficial Intelligence (LNAI 3651). 378-389. 
Zhang M., Zhang J., Su J. and Zhou G.D. (2006). 
A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured 
Features. COLING-ACL-2006: 825-832. Sydney, 
Australia 
Zhang Z. (2004). Weakly supervised relation clas-
sification for information extraction. 
CIKM?2004. 8-13 Nov 2004. Washington D.C. 
USA. 
Zhao S.B. and Grishman R. (2005). Extracting re-
lations with integrated information using kernel 
methods. ACL?2005: 419-426. Univ of Michi-
gan-Ann Arbor,  USA,  25-30 June 2005. 
Zhou G.D., Su J. Zhang J. and Zhang M. (2005). 
Exploring various knowledge in relation extrac-
tion. ACL?2005. 427-434. 25-30 June, Ann Ar-
bor, Michgan, USA.  
Zhou G.D., Su J. and Zhang M. (2006). Modeling 
commonality among related classes in relation 
extraction, COLING-ACL?2006: 121-128. Syd-
ney, Australia. 
Zhu, X. and Ghahramani, Z. (2002). Learning from 
Labeled and Unlabeled Data with Label 
Propagation. CMU CALD Technical Report. 
CMU-CALD-02-107. 
Zhu, X., Ghahramani, Z. and Lafferty, J. (2003). 
Semi-Supervised Learning Using Gaussian 
Fields and Harmonic Functions. ICML?2003. 
39
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 599?607,
Beijing, August 2010
Dependency-driven Anaphoricity Determination for Coreference 
Resolution
Fang Kong  Guodong Zhou  Longhua Qian  Qiaoming Zhu*
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology Soochow University 
{kongfang, gdzhou, qianlonghua, qmzhu}@suda.edu.cn 
                                                          
* Corresponding author 
Abstract
This paper proposes a dependency-driven 
scheme to dynamically determine the syn-
tactic parse tree structure for tree ker-
nel-based anaphoricity determination in 
coreference resolution. Given a full syntactic 
parse tree, it keeps the nodes and the paths 
related with current mention based on con-
stituent dependencies from both syntactic 
and semantic perspectives, while removing 
the noisy information, eventually leading to 
a dependency-driven dynamic syntactic 
parse tree (D-DSPT). Evaluation on the ACE 
2003 corpus shows that the D-DSPT out-
performs all previous parse tree structures on 
anaphoricity determination, and that apply-
ing our anaphoricity determination module 
in coreference resolution achieves the so far 
best performance. 
1 Introduction 
Coreference resolution aims to identify which 
noun phrases (NPs, or mentions) refer to the 
same real-world entity in a text. According to 
Webber (1979), coreference resolution can be 
decomposed into two complementary sub-tasks: 
(1) anaphoricity determination, determining 
whether a given NP is anaphoric or not; and (2) 
anaphor resolution, linking together multiple 
mentions of a given entity in the world. Al-
though machine learning approaches have per-
formed reasonably well in coreference resolu-
tion without explicit anaphoricity determina-
tion (e.g. Soon et al 2001; Ng and Cardie 
2002b; Yang et al 2003, 2008; Kong et al 
2009), knowledge of NP anaphoricity is ex-
pected to much improve the performance of a 
coreference resolution system, since a 
non-anaphoric NP does not have an antecedent 
and therefore does not need to be resolved. 
Recently, anaphoricity determination has 
been drawing more and more attention. One 
common approach involves the design of some 
heuristic rules to identify specific types of 
non-anaphoric NPs, such as pleonastic it (e.g. 
Paice and Husk 1987; Lappin and Leass 1994, 
Kennedy and Boguraev 1996; Denber 1998) 
and definite descriptions (e.g. Vieira and Poe-
sio 2000). Alternatively, some studies focus on 
using statistics to tackle this problem (e.g., 
Bean and Riloff 1999; Bergsma et al 2008) 
and others apply machine learning approaches 
(e.g. Evans 2001;Ng and Cardie 2002a, 
2004,2009; Yang et al 2005; Denis and Bal-
bridge 2007; Luo 2007; Finkel and Manning 
2008; Zhou and Kong 2009).  
As a representative, Zhou and Kong (2009) 
directly employ a tree kernel-based method to 
automatically mine the non-anaphoric informa-
tion embedded in the syntactic parse tree. One 
main advantage of the kernel-based methods is 
that they are very effective at reducing the 
burden of feature engineering for structured 
objects. Indeed, the kernel-based methods have 
been successfully applied to mine structured 
information in various NLP applications like 
syntactic parsing (Collins and Duffy, 2001; 
Moschitti, 2004), semantic relation extraction 
(Zelenko et al, 2003; Zhao and Grishman, 
2005; Zhou et al 2007; Qian et al, 2008), se-
mantic role labeling (Moschitti, 2004); corefer-
ence resolution (Yang et al, 2006; Zhou et al, 
2008). One of the key problems for the ker-
nel-based methods is how to effectively capture 
the structured information according to the na-
ture of the structured object in the specific task. 
This paper advances the state-of-the-art per-
formance in anaphoricity determination by ef-
599
fectively capturing the structured syntactic in-
formation via a tree kernel-based method. In 
particular, a dependency-driven scheme is 
proposed to dynamically determine the syntac-
tic parse tree structure for tree kernel-based 
anaphoricity determination by exploiting con-
stituent dependencies from both the syntactic 
and semantic perspectives to keep the neces-
sary information in the parse tree as well as 
remove the noisy information. Our motivation 
is to employ critical dependency information in 
constructing a concise and effective syntactic 
parse tree structure, specifically targeted for 
tree kernel-based anaphoricity determination.  
The rest of this paper is organized as follows. 
Section 2 briefly describes the related work on 
both anaphoricity determination and exploring 
syntactic parse tree structures in related tasks. 
Section 3 presents our dependency-driven 
scheme to determine the syntactic parse tree 
structure. Section 4 reports the experimental 
results. Finally, we conclude our work in Sec-
tion 5. 
2 Related Work 
This section briefly overviews the related work 
on both anaphoricity determination and ex-
ploring syntactic parse tree structures. 
2.1 Anaphoricity Determination 
Previous work on anaphoricity determination 
can be broadly divided into three categories: 
heuristic rule-based (e.g. Paice and Husk 
1987;Lappin and Leass 1994; Kennedy and 
Boguraev 1996; Denber 1998; Vieira and Poe-
sio 2000; Cherry and Bergsma 2005), statis-
tics-based (e.g. Bean and Riloff 1999; Cherry 
and Bergsma 2005; Bergsma et al 2008) and 
learning-based methods (e.g. Evans 2001; Ng 
and Cardie 2002a; Ng 2004; Yang et al 2005; 
Denis and Balbridge 2007; Luo 2007; Finkel 
and Manning 2008; Zhou and Kong 2009; Ng 
2009).  
The heuristic rule-based methods focus on 
designing some heuristic rules to identify spe-
cific types of non-anaphoric NPs. Representa-
tive work includes: Paice and Husk (1987), 
Lappin and Leass (1994) and Kennedy and 
Boguraev (1996). For example, Kennedy and 
Boguraev (1996) looked for modal adjectives 
(e.g. ?necessary?) or cognitive verbs (e.g. ?It is 
thought that?? in a set of patterned construc-
tions) in identifying pleonastic it.
Among the statistics-based methods, Bean 
and Riloff (1999) automatically identified ex-
istential definite NPs which are non-anaphoric.  
The intuition behind is that many definite NPs 
are not anaphoric since their meanings can be 
understood from general world knowledge, e.g. 
?the FBI?. They found that existential NPs ac-
count for 63% of all definite NPs and 76% of 
them could be identified by syntactic or lexical 
means. Cherry and Bergsma (2005) extended 
the work of Lappin and Leass (1994) for 
large-scale anaphoricity determination by addi-
tionally detecting pleonastic it. Bergsma et al 
(2008) proposed a distributional method in de-
tecting non-anaphoric pronouns. They first ex-
tracted the surrounding context of the pronoun 
and gathered the distribution of words that oc-
curred within the context from a large corpus, 
and then identified the pronoun either ana-
phoric or non-anaphoric based on the word dis-
tribution.
Among the learning-based methods, Evans 
(2001) automatically identified the 
non-anaphoricity of pronoun it using various 
kinds of lexical and syntactic features. Ng and 
Cardie (2002a) employed various do-
main-independent features in identifying ana-
phoric NPs. They trained an anaphoricity clas-
sifier to determine whether a NP was anaphoric 
or not, and employed an independently-trained 
coreference resolution system to only resolve 
those mentions which were classified as ana-
phoric. Experiments showed that their method 
improved the performance of coreference 
resolution by 2.0 and 2.6 to 65.8 and 64.2 in 
F1-measure on the MUC-6 and MUC-7 cor-
pora, respectively. Ng (2004) examined the 
representation and optimization issues in com-
puting and using anaphoricity information to 
improve learning-based coreference resolution. 
On the basis, he presented a corpus-based ap-
proach (Ng, 2009) for achieving global opti-
mization by representing anaphoricity as a fea-
ture in coreference resolution. Experiments on 
the ACE 2003 corpus showed that their method 
improved the overall performance by 2.8, 2.2 
and 4.5 to 54.5, 64.0 and 60.8 in F1-measure 
on the NWIRE, NPAPER and BNEWS do-
mains, respectively. However, he did not look 
into the contribution of anaphoricity determi-
600
nation on coreference resolution of different 
NP types. Yang et al (2005) made use of 
non-anaphors to create a special class of train-
ing instances in the twin-candidate model 
(Yang et al 2003) and improved the perform-
ance by 2.9 and 1.6 to 67.3 and 67.2 in 
F1-measure on the MUC-6 and MUC-7 cor-
pora, respectively. However, their experiments 
show that eliminating non-anaphors using an 
anaphoricity determination module in advance 
harms the performance. Denis and Balbridge 
(2007) employed an integer linear program-
ming (ILP) formulation for coreference resolu-
tion which modeled anaphoricity and corefer-
ence as a joint task, such that each local model 
informed the other for the final assignments. 
Experiments on the ACE 2003 corpus showed 
that this joint anaphoricity-coreference ILP 
formulation improved the F1-measure by 
3.7-5.3 on various domains. However, their 
experiments assume true ACE mentions (i.e. all 
the ACE mentions are already known from the 
annotated corpus). Therefore, the actual effect 
of this joint anaphoricity-coreference ILP for-
mulation on fully automatic coreference reso-
lution is still unclear. Luo (2007) proposed a 
twin-model for coreference resolution: a link 
component, which models the coreferential 
relationship between an anaphor and a candi-
date antecedent, and a creation component, 
which models the possibility that a NP was not 
coreferential with any candidate antecedent. 
This method combined the probabilities re-
turned by the creation component (an ana-
phoricity model) with the link component (a 
coreference model) to score a coreference par-
tition, such that a partition was penalized 
whenever an anaphoric mention was resolved. 
Finkel and Manning (2008) showed that transi-
tivity constraints could be incorporated into an 
ILP-based coreference resolution system and 
much improved the performance. Zhou and 
Kong (2009) employed a global learning 
method in determining the anaphoricity of NPs 
via a label propagation algorithm to improve 
learning-based coreference resolution. Experi-
ments on the ACE 2003 corpus demonstrated 
that this method was very effective. It could 
improve the F1-measure by 2.4, 3.1 and 4.1 on 
the NWIRE, NPAPER and BNEWS domains, 
respectively. Ng (2009) presented a novel ap-
proach to the task of anaphoricity determina-
tion based on graph minimum cuts and demon-
strated the effectiveness in improving a learn-
ing-based coreference resolution system. 
In summary, although anaphoricity determi-
nation plays an important role in coreference 
resolution and achieves certain success in im-
proving the overall performance of coreference 
resolution, its contribution is still far from ex-
pectation.
2.2 Syntactic Parse Tree Structures 
For a tree kernel-based method, one key prob-
lem is how to represent and capture the struc-
tured syntactic information. During recent 
years, various tree kernels, such as the convo-
lution tree kernel (Collins and Duffy, 2001), 
the shallow parse tree kernel (Zelenko et al
2003) and the dependency tree kernel (Culota 
and Sorensen, 2004), have been proposed in the 
literature. Among these tree kernels, the con-
volution tree kernel represents the state-of-the 
art and has been successfully applied by 
Collins and Duffy (2002) on syntactic parsing, 
Zhang et al (2006) on semantic relation extrac-
tion and Yang et al (2006) on pronoun resolu-
tion.
Given a tree kernel, the key issue is how to 
generate a syntactic parse tree structure for ef-
fectively capturing the structured syntactic in-
formation. In the literature, various parse tree 
structures have been proposed and successfully 
applied in some NLP applications. As a repre-
sentative, Zhang et al (2006) investigated five 
parse tree structures for semantic relation ex-
traction and found that the Shortest 
Path-enclosed Tree (SPT) achieves the best 
performance on the 7 relation types of the ACE 
RDC 2004 corpus. Yang et al (2006) con-
structed a document-level syntactic parse tree 
for an entire text by attaching the parse trees of 
all its sentences to a new-added upper node and 
examined three possible parse tree structures 
(Min-Expansion, Simple-Expansion and 
Full-Expansion) that contain different sub-
structures of the parse tree for pronoun resolu-
tion. Experiments showed that their method 
achieved certain success on the ACE 2003 
corpus and the simple-expansion scheme per-
forms best. However, among the three explored 
schemes, there exists no obvious overwhelming 
one, which can well cover structured syntactic 
information. One problem of Zhang et al (2006) 
601
and Yang et al (2006) is that their parse tree 
structures are context-free and do not consider 
the information outside the sub-trees. Hence, 
their ability of exploring structured syntactic 
information is much limited. Motivated by 
Zhang et al (2006) and Yang et al (2006), 
Zhou et al (2007) extended the SPT to become 
context-sensitive (CS-SPT) by dynamically 
including necessary predicate-linked path in-
formation. Zhou et al (2008) further proposed 
a dynamic-expansion scheme to automatically 
determine a proper parse tree structure for 
pronoun resolution by taking predicate- and 
antecedent competitor-related information in 
consideration. Evaluation on the ACE 2003 
corpus showed that the dynamic-expansion 
scheme can well cover necessary structured 
information in the parse tree for pronoun reso-
lution. One problem with the above parse tree 
structures is that they may still contain unnec-
essary information and also miss some useful 
context-sensitive information. Qian et al (2008) 
dynamically determined the parse tree structure 
for semantic relation extraction by exploiting 
constituent dependencies to keep the necessary 
information in the parse tree as well as remove 
the noisy information. Evaluation on the ACE 
RDC 2004 corpus showed that their dynamic 
syntactic parse tree structure outperforms all 
previous parse tree structures. However, their 
solution has the limitation in that the depend-
encies were found according to some manu-
ally-written ad-hoc rules and thus may not be 
easily applicable to new domains and applica-
tions.
This paper proposes a new scheme to dy-
namically determine the syntactic parse tree 
structure for anaphoricity determination and 
systematically studies the application of an ex-
plicit anaphoricity determination module in 
improving coreference resolution. 
3 Dependency-driven Dynamic Syn-
tactic Parse Tree 
Given a full syntactic parse tree and a NP in 
consideration, one key issue is how to choose a 
proper syntactic parse tree structure to well 
cover structured syntactic information in the 
tree kernel computation. Generally, the more a 
syntactic parse tree structure includes, the more 
structured syntactic information would be 
available, at the expense of more noisy (or un-
necessary) information.  
It is well known that dependency informa-
tion plays a key role in many NLP problems, 
such as syntactic parsing, semantic role label-
ing as well as semantic relation extraction. Mo-
tivated by Qian et al (2008) and Zhou et al 
(2008), we propose a new scheme to dynami-
cally determine the syntactic parse tree struc-
ture for anaphoricity determination by exploit-
ing constituent dependencies from both the 
syntactic and semantic perspectives to distin-
guish the necessary evidence from the unnec-
essary information in the syntactic parse tree. 
That is, constituent dependencies are explored 
from two aspects: syntactic dependencies and 
semantic dependencies.  
1) Syntactic Dependencies: The Stanford de-
pendency parser1 is employed as our syn-
tactic dependency parser to automatically 
extract various syntactic (i.e. grammatical) 
dependencies between individual words. In 
this paper, only immediate syntactic de-
pendencies with current mention are con-
sidered. The intuition behind is that the im-
mediate syntactic dependencies carry the 
major contextual information of current 
mention.
2) Semantic Dependencies: A state-of-the-art 
semantic role labeling (SRL) toolkit (Li et 
al. 2009) is employed for extracting various 
semantic dependencies related with current 
mention. In this paper, semantic dependen-
cies include all the predicates heading any 
node in the root path from current mention 
to the root node and their compatible argu-
ments (except those overlapping with cur-
rent mention). 
We name our parse tree structure as a depend-
ency-driven dynamic syntactic parse tree 
(D-DSPT). The intuition behind is that the de-
pendency information related with current 
mention in the same sentence plays a critical 
role in anaphoricity determination. Given the 
sentence enclosing the mention under consid-
eration, we can get the D-DSPT as follows: 
(Figure 1 illustrates an example of the D-DSPT 
generation given the sentence ?Mary said the 
woman in the room bit her? with ?woman? as 
current mention.) 
                                                          
1 http://nlp.stanford.edu/software/lex-parser.shtml
602
           
Figure 1:  An example of generating the dependency-driven dynamic syntactic parse tree  
1) Generating the full syntactic parse tree of 
the given sentence using a full syntactic parser. 
In this paper, the Charniak parser (Charniak 
2001) is employed and Figure 1 (a) shows the 
resulting full parse tree. 
2) Keeping only the root path from current 
mention to the root node of the full parse tree. 
Figure 1(b) shows the root path corresponding 
to the current mention ?woman?. In the fol-
lowing steps, we attach the above two types of 
dependency information to the root path.  
3) Extracting all the syntactic dependencies 
in the sentence using a syntactic dependency 
parser, and attaching all the nodes, which have 
immediate dependency relationship with cur-
rent mention, and their corresponding paths to 
the root path. Figure 1(c) illustrates the syntac-
tic dependences extracted from the sentence, 
where the ones in italic mean immediate de-
pendencies with current mention. Figure 1(d) 
shows the parse tree structure after considering 
syntactic dependencies. 
4) Attaching all the predicates heading any 
node in the root path from current mention to 
the root node and their corresponding paths to 
the root path. For the example sentence, there 
are two predicates ?said? and ?bit?, which head 
the ?VP? and ?S? nodes in the root path re-
spectively. Therefore, these two predicates and 
their corresponding paths should be attached to 
the root path as shown in Figure 1(e). Note that 
the predicate ?bit? and its corresponding path 
has already been attached in Stop (3). As a re-
sult, the predicate-related information can be 
attached. According to Zhou and Kong (2009), 
such information is important to definite NP 
resolution.
5) Extracting the semantic dependencies re-
lated with those attached predicates using a 
(shallow) semantic parser, and attaching all the 
compatible arguments (except those overlap-
ping with current mention) and their corre-
sponding paths to the root path. For example, 
as shown in Figure 1(e), since the arguments 
?Mary? and ?her? are compatible with current 
mention ?woman?, these two nodes and their 
corresponding paths are attached while the ar-
gument ?room? is not since its gender does not 
agree with current mention. 
In this paper, the similarity between two 
parse trees is measured using a convolution tree 
kernel, which counts the number of common 
sub-tree as the syntactic structure similarity 
between two parse trees. For details, please 
refer to Collins and Duffy (2001). 
603
4 Experimentation and Discussion 
This section evaluates the performance of de-
pendency-driven anaphoricity determination 
and its application in coreference resolution on 
the ACE 2003 corpus. 
4.1 Experimental Setting 
The ACE 2003 corpus contains three domains: 
newswire (NWIRE), newspaper (NPAPER), 
and broadcast news (BNEWS). For each do-
main, there exist two data sets, training and 
devtest, which are used for training and testing.  
For preparation, all the documents in the 
corpus are preprocessed automatically using a 
pipeline of NLP components, including to-
kenization and sentence segmentation, named 
entity recognition, part-of-speech tagging and 
noun phrase chunking. Among them, named 
entity recognition, part-of-speech tagging and 
noun phrase chunking apply the same 
state-of-the-art HMM-based engine with er-
ror-driven learning capability (Zhou and Su, 
2000 & 2002). Our statistics finds that 62.0%, 
58.5% and 61.4% of entity mentions are pre-
served after preprocessing on the NWIRE, 
NPAPER and BNEWS domains of the ACE 
2003 training data respectively while only 
89.5%, 89.2% and 94% of entity mentions are 
preserved after preprocessing on  the NWIRE, 
NPAPER and BNEWS domains of the ACE 
2003 devtest data. This indicates the difficulty 
of coreference resolution. In addition, the cor-
pus is parsed using the Charniak parser for 
syntactic parsing and the Stanford dependency 
parser for syntactic dependencies while corre-
sponding semantic dependencies are extracted 
using a state-of-the-art semantic role labeling 
toolkit (Li et al 2009). Finally, we use the 
SVM-light2 toolkit with the tree kernel func-
tion as the classifier. For comparison purpose, 
the training parameters C (SVM) and ?(tree
kernel) are set to 2.4 and 0.4 respectively, as 
done in Zhou and Kong (2009).  
For anaphoricity determination, we report 
the performance in Acc+ and Acc-, which 
measure the accuracies of identifying anaphoric 
NPs and non-anaphoric NPs, respectively. Ob-
viously, higher Acc+ means that more ana-
phoric NPs would be identified correctly, while 
                                                          
2 http://svmlight.joachims.org/ 
higher Acc- means that more non-anaphoric 
NPs would be filtered out. For coreference 
resolution, we report the performance in terms 
of recall, precision, and F1-measure using the 
commonly-used model theoretic MUC scoring 
program (Vilain et al 1995). To see whether an 
improvement is significant, we also conduct 
significance testing using paired t-test. In this 
paper, ?***?, ?**? and ?*? denote p-values of an 
improvement smaller than 0.01, in-between 
(0.01, 0,05] and bigger than 0.05, which mean 
significantly better, moderately better and 
slightly better, respectively. 
4.2 Experimental Results 
Performance of anaphoricity determination 
Table 1 presents the performance of anaphoric-
ity determination using the convolution tree 
kernel on D-DSPT. It shows that our method 
achieves the accuracies of 83.27/77.13, 
86.77/80.25 and 90.02/64.24 on identifying 
anaphoric/non-anaphoric NPs in the NWIRE, 
NPAPER and BNEWS domains, respectively.  
This suggests that our approach can effectively 
filter out about 75% of non-anaphoric NPs and 
keep about 85% of anaphoric NPs. In com-
parison, in the three domains Zhou and Kong 
(2009) achieve the accuracies of 76.5/82.3, 
78.9/81.6 and 74.3/83.2, respectively, using the 
tree kernel on a dynamically-extended tree 
(DET). This suggests that their method can fil-
ter out about 82% of non-anaphoric NPs and 
only keep about 76% of anaphoric NPs. In 
comparison, their method outperforms our 
method on filtering out more non-anaphoric 
NPs while our method outperforms their 
method on keeping more anaphoric NPs in 
coreference resolution. While a coreference 
resolution system can detect some 
non-anaphoric NPs (when failing to find the 
antecedent candidate), filtering out anaphoric 
NPs in anaphoricity determination would defi-
nitely cause errors and it is almost impossible 
to recover. Therefore, it is normally more im-
portant to keeping more anaphoric NPs than 
filtering out more non-anaphoric NPs. Table 1 
further presents the performance of anaphoric-
ity determination on different NP types. It 
shows that our method performs best at keep-
ing pronominal NPs and filtering out proper 
NPs.
604
NWIRE NPAPER BNEWS NP Type 
Acc+ Acc- Acc+ Acc- Acc+ Acc-
Pronoun 95.07 50.36 96.40 56.44 98.26 54.03 
Proper NP 84.61 83.17 83.78 79.62 87.61 71.77 
Definite NP 87.17 46.74 82.24 49.18 86.87 53.65 
Indefinite NP 86.01 47.52 80.63 48.45 89.71 47.32 
Over all 83.27 77.13 86.77 80.25 90.02 64.24 
Table 1: Performance of anaphoricity determination using the D-DSPT  
NWIRE NPAPER BNEWS Performance Change 
Acc+ Acc- Acc+ Acc- Acc+ Acc-
D-DSPT 83.27 77.13 86.77 80.25 90.02 64.24 
-Syntactic Dependencies 78.67 72.56 80.14 73.74 87.05 60.20 
-Semantic Dependencies 81.67 76.74 83.47 77.93 89.58 60.67 
Table 2: Contribution of including syntactic and semantic dependencies  
in D-DSPT on anaphoricity determination  
NWIRE NPAPER BNEWS System 
R% P% F R% P% F R% P% F 
Pronoun 70.8 57.9 63.7 76.5 63.5 69.4 70.0 60.3 64.8
Proper NP 80.3 80.1 80.2 81.8 83.6 82.7 76.3 76.8 76.6
Definite NP 35.9 43.4 39.2 43.1 48.5 45.6 47.9 51.9 49.8
Indefinite NP 40.3 26.3 31.8 39.7 22.9 29.0 23.6 10.7 14.7
Without ana-
phoricity de-
termination 
(Baseline)
Over all 55.0 63.8 59.1 62.1 65.0 63.5 53.2 60.5 56.6
Pronoun 65.9 70.2 68.0 72.6 78.7 75.5 67.7 75.8 71.5
Proper NP 80.3 81.0 80.6 81.2 85.1 83.1 76.3 84.4 80.1
Definite NP 32.3  63.1 42.7 38.4 61.7 47.3 42.5 66.4 51.8
Indefinite NP 36.4 55.3 43.9 34.7 50.7 41.2 20.3 45.4 28.1
With D-DSPT 
-based ana-
phoricity de-
termination 
Over all 52.4 79.6 63.2 58.1 80.3 67.4 50.1 79.8 61.6
Pronoun 68.6 71.5 70.1 75.2 80.4 77.7 69.1 77.8 73.5
Proper NP 81.7 89.3 85.3 82.6 90.1 86.2 78.6 88.7 83.3
Definite NP 41.8 85.9 56.2 44.9 85.2 58.8 45.2 87.9 59.7
Indefinite NP 40.3 67.6 50.5 41.2 65.1 50.5 40.9 50.1 45.1
With golden 
anaphoricity
determination 
Over all 54.6 81.7 65.5 60.4 82.1 69.6 51.9 82.1 63.6
Table 3: Performance of anaphoricity determination on coreference resolution 
NWIRE NPAPER BNEWS System 
R% P% F R% P% F R% P% F 
Without anaphoricity determina-
tion (Baseline) 53.1 67.4 59.4 57.7 67.0 62.1 48.0 65.9 55.5Zhou and 
Kong (2009) With Dynamically Extended 
Tree-based anaphoricity determi-
nation
51.6 77.2 61.8 55.2 78.6 65.2 47.5 80.3 59.6
Without anaphoricity determina-
tion (Baseline)
59.1 58. 58.6 60.8 62.6 61.7 57.7 52.6 55.0
Ng (2009) 
With Graph Minimum Cut-based 
anaphoricity determination
54.1 69.0 60.6 57.9 71.2 63.9 53.1 67.5 59.4
Table 4: Performance comparison with other systems 
Table 2 further presents the contribution of 
including syntactic and semantic dependencies 
in the D-DSPT on anaphoricity determination 
by excluding one or both of them. It shows that 
both syntactic dependencies and semantic de-
pendencies contribute significantly (***). 
Performance of coreference resolution 
We have evaluated the effect of our 
D-DSPT-based anaphoricity determination 
module on coreference resolution by including 
it as a preprocessing step to a baseline corefer-
ence resolution system without explicit ana-
phoricity determination, by filtering our those 
non-anaphoric NPs according to the anaphoric-
ity determination module. Here, the baseline 
system employs the same set of features, as 
adopted in the single-candidate model of Yang 
et al (2003) and uses a SVM-based classifier 
with the feature-based RBF kernel. Table 3 
presents the detailed performance of the 
coreference resolution system without ana-
605
phoricity determination, with D-DSPT-based 
anaphoricity determination and. with golden 
anaphoricity determination. Table 3 shows that: 
1) There is a performance gap of 6.4, 6.1 and 
7.0 in F1-measure on the NWIRE, NPAPER 
and BNEWS domain, respectively, between the 
coreference resolution system with golden 
anaphoricity determination and the baseline 
system without anaphoricity determination. 
This suggests the usefulness of proper ana-
phoricity determination in coreference resolu-
tion. This also agrees with Stoyanov et al 
(2009) which measured the impact of golden 
anaphoricity determination on coreference 
resolution using only the annotated anaphors in 
both training and testing.  
2) Compared to the baseline system without 
anaphoricity determination, the D-DSPT-based 
anaphoricity determination module improves 
the performance by 4.1(***), 3.9(***) and 
5.0(***) to 63.2, 67.4 and 61.6 in F1-measure 
on the NWIRE, NPAPER and BNEWS do-
mains, respectively, due to a large gain in pre-
cision and a much smaller drop in recall. In 
addition, D-DSPT-based anaphoricity determi-
nation can not only much improve the per-
formance of coreference resolution on pro-
nominal NPs (***) but also on definite 
NPs(***) and indefinite NPs(***) while the 
improvement on proper NPs can be ignored 
due to the fact that proper NPs can be well ad-
dressed by the simple abbreviation feature in 
the baseline system. 
3) D-DSPT-based anaphoricity determination 
still lags (2.3, 2.2 and 2.0 on the NWIRE, 
NPAPER and BNEWS domains, respectively) 
behind golden anaphoricity determination in 
improving the overall performance of corefer-
ence resolution. This suggests that there exists 
some room in the performance improvement 
for anaphoricity determination. 
Performance comparison with other systems 
Table 4 compares the performance of our sys-
tem with other systems. Here, Zhou and Kong 
(2009) use the same set of features with ours in 
the baseline system and a dynami-
cally-extended tree structure in anaphoricity 
determination. Ng (2009) uses 33 features as 
described in Ng (2007) and a graph minimum 
cut algorithm in anaphoricity determination. It 
shows that the overall performance of our 
baseline system is almost as good as that of 
Zhou and Kong (2009) and a bit better than 
Ng?s (2009).  
For overall performance, our coreference 
resolution system with D-DSPT-based ana-
phoricity determination much outperforms 
Zhou and Kong (2009) in F1-measure by 1.4, 
2.2 and 2.0 on the NWIRE, NPAPER and 
BNEWS domains, respectively, due to the bet-
ter inclusion of dependency information. De-
tailed evaluation shows that such improvement 
comes from coreference resolution on both 
pronominal and definite NPs (Please refer to 
Table 6 in Zhou and Kong, 2009). Compared 
with Zhou and Kong (2009) and Ng (2009), our 
approach achieves the best F1-measure so far 
for each dataset. 
5 Conclusion and Further Work 
This paper systematically studies a depend-
ency-driven dynamic syntactic parse tree 
(DDST) for anaphoricity determination and the 
application of an explicit anaphoricity deter-
mination module in improving learning-based 
coreference resolution. Evaluation on the ACE 
2003 corpus indicates that D-DSPT-based 
anaphoricity determination much improves the 
performance of coreference resolution. 
To our best knowledge, this paper is the first 
research which directly explores constituent 
dependencies in tree kernel-based anaphoricty 
determination from both syntactic and semantic 
perspectives. 
For further work, we will explore more 
structured syntactic information in coreference 
resolution. In addition, we will study the inter-
action between anaphoricity determination and 
coreference resolution and better integrate 
anaphoricity determination with coreference 
resolution.
Acknowledgments 
This research was supported by Projects 
60873150, 60970056, and 90920004 under the 
National Natural Science Foundation of China, 
Project 200802850006 and 20093201110006 
under the Specialized Research Fund for the 
Doctoral Program of Higher Education of 
China.
606
References 
D. Bean and E. Riloff 1999. Corpus-based Identifi-
cation of Non-Anaphoric Noun Phrases. ACL? 
1999 
S. Bergsma, D. Lin and R. Goebel 2008. Distribu-
tional Identification of Non-referential Pronouns. 
ACL?2008 
C. Cherry and S. Bergsma. 2005. An expectation 
maximization approach to pronoun resolution. 
CoNLL?2005 
M. Collins and N. Duffy. 2001. Covolution kernels 
for natural language. NIPS?2001  
M. Denber 1998. Automatic Resolution of Anapho-
ria in English. Technical Report, Eastman Ko-
dakCo. 
P. Denis and J. Baldridge. 2007. Global, joint de-
termination of anaphoricity and coreference 
resolution using integer programming. 
NAACL/HLT?2007
R. Evans 2001. Applying machine learning toward 
an automatic classification of it. Literary and 
Linguistic Computing, 16(1):45-57 
F. Kong, G.D. Zhou and Q.M. Zhu. 2009 Employ-
ing the Centering Theory in Pronoun Resolution 
from the Semantic Perspective. EMNLP?2009 
F. Kong, Y.C. Li, G.D. Zhou and Q.M. Zhu. 2009. 
Exploring Syntactic Features for Pronoun Reso-
lution Using Context-Sensitive Convolution Tree 
Kernel. IALP?2009 
S. Lappin and J. L. Herbert. 1994. An algorithm for 
pronominal anaphora resolution. Computational 
Linguistics, 20(4) 
J.H. Li. G.D. Zhou, H. Zhao, Q.M. Zhu and P.D. 
Qian. Improving nominal SRL in Chinese lan-
guage with verbal SRL information and auto-
matic predicate recognition. EMNLP '2009 
X. Luo. 2007. Coreference or not: A twin model for 
coreference resolution.  NAACL-HLT?2007 
V. Ng and C. Cardie 2002. Identify Anaphoric and 
Non-Anaphoric Noun Phrases to Improve 
Coreference Resolution. COLING?2002 
V. Ng and C. Cardie 2002. Improving machine 
learning approaches to coreference resolution. 
ACL?2002 
V. Ng 2004. Learning Noun Phrase Anaphoricity to 
Improve Coreference Resolution: Issues in Rep-
resentation and Optimization. ACL? 2004 
V. Ng 2009. Graph-cut based anaphoricity determi-
nation for coreference resolution. NAACL?2009 
L.H. Qian, G.D. Zhou, F. Kong, Q.M. Zhu and P.D. 
Qian. 2008. Exploiting constituent dependencies 
for  tree kernel-based semantic relation extrac-
tion. COLING?2008 
W. M. Soon, H. T. Ng and D. Lim  2001. A ma-
chine learning approach to coreference resolution 
of noun phrase. Computational Linguistics, 
27(4):521-544. 
V. Stoyanov, N. Gilbert, C. Cardie and E. Riloff. 
2009. Conundrums in Noun Phrase Coreference 
Resolution: Making Sense of the State-of-the Art. 
ACL?2009 
B. L. Webber. 1979. A Formal Approach to Dis-
course Anaphora. Garland Publishing, Inc. 
X.F. Yang, G.D. Zhou, J. Su and C.L. Chew. 2003. 
Coreference Resolution Using Competition 
Learning Approach. ACL?2003 
X.F. Yang, J. Su and C.L. Chew. 2005. A Twin 
Candidate Model of Coreference Resolution with 
Non-Anaphor Identification Capability. 
IJCNLP?2005 
X.F. Yang, J. Su and C.L. Chew. 2006. Ker-
nel-based pronoun resolution with structured 
syntactic knowledge. COLING-ACL?2006 
X.F. Yang, J. Su and C.L. Tan 2008. A 
Twin-Candidate Model for Learning-Based 
Anaphora Resolution. Computational Linguistics 
34(3):327-356 
M. Zhang, J. Zhang, J. Su and G.D. Zhou. 2006. A 
composite kernel to extract relations between en-
tities with both flat and structured features. 
COLING/ACL?2006 
S. Zhao and R. Grishman. 2005. Extracting relations 
with integered information using kernel methods. 
ACL?2005 
D. Zelenko, A. Chinatsu and R. Anthony. 2003. 
Kernel methods for relation extraction. Machine 
Learning Researching 3(2003):1083-1106 
G.D. Zhou, F. Kong and Q.M. Zhu. 2008. Con-
text-sensitive convolution tree kernel for pronoun 
resolution. IJCNLP?2008 
G.D. Zhou and F. Kong. 2009. Global Learning of 
Noun Phrase Anaphoricity in Coreference Reso-
lution via Label Propagetion. EMNLP?2009 
G.D. Zhou and J. Su. 2002. Named Entity recogni-
tion using a HMM-based chunk tagger. 
ACL?2002 
G.D. Zhou, M. Zhang, D.H. Ji and Q.M. Zhu. 2007. 
Tree kernel-based relation extraction with con-
text-sensitive structured parse tree information. 
EMNLP/CoNLL?2007
607
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 671?679,
Beijing, August 2010
Learning the Scope of Negation via Shallow Semantic Parsing 
Junhui Li  Guodong Zhou?  Hongling Wang  Qiaoming Zhu 
School of Computer Science and Technology 
        Soochow University at Suzhou 
{lijunhui, gdzhou, redleaf, qmzhu}@suda.edu.cn 
 
                                                          
? Corresponding author 
Abstract 
In this paper we present a simplified shallow 
semantic parsing approach to learning the 
scope of negation (SoN). This is done by 
formulating it as a shallow semantic parsing 
problem with the negation signal as the 
predicate and the negation scope as its ar-
guments. Our parsing approach to SoN 
learning differs from the state-of-the-art 
chunking ones in two aspects. First, we ex-
tend SoN learning from the chunking level 
to the parse tree level, where structured syn-
tactic information is available. Second, we 
focus on determining whether a constituent, 
rather than a word, is negated or not, via a 
simplified shallow semantic parsing frame-
work. Evaluation on the BioScope corpus 
shows that structured syntactic information 
is effective in capturing the domination rela-
tionship between a negation signal and its 
dominated arguments. It also shows that our 
parsing approach much outperforms the 
state-of-the-art chunking ones. 
1 Introduction 
Whereas negation in predicate logic is 
well-defined and syntactically simple, negation 
in natural language is much complex. Gener-
ally, learning the scope of negation involves 
two subtasks: negation signal finding and nega-
tion scope finding. The former decides whether 
the words in a sentence are negation signals 
(i.e., words indicating negation, e.g., no, not, 
fail, rather than), where the semantic informa-
tion of the words, rather than the syntactic in-
formation, plays a critical role. The latter de-
termines the sequences of words in the sen-
tence which are negated by the given negation 
signal. Compared with negation scope finding, 
negation signal finding is much simpler and has 
been well resolved in the literature, e.g. with 
the accuracy of 95.8%-98.7% on the three 
subcorpora of the Bioscope corpus (Morante 
and Daelemans, 2009). In this paper, we focus 
on negation scope finding instead. That is, we 
assume golden negation signal finding. 
Finding negative assertions is essential in 
information extraction (IE), where in general, 
the aim is to derive factual knowledge from 
free text. For example, Vincze et al (2008) 
pointed out that the extracted information 
within the scopes of negation signals should 
either be discarded or presented separately 
from factual information. This is especially 
important in the biomedical domain, where 
various linguistic forms are used extensively to 
express impressions, hypothesized explanations 
of experimental results or negative findings. 
Szarvas et al (2008) reported that 13.45% of 
the sentences in the abstracts subcorpus of the 
BioScope corpus and 12.70% of the sentences 
in the full papers subcorpus of the Bioscope 
corpus contain negative assertions. In addition 
to the IE tasks in the biomedical domain, SoN 
learning has attracted more and more attention 
in some natural language processing (NLP) 
tasks, such as sentiment classification (Turney, 
2002). For example, in the sentence ?The chair 
is not comfortable but cheap?, although both 
the polarities of the words ?comfortable? and 
?cheap? are positive, the polarity of ?the chair? 
regarding the attribute ?cheap? keeps positive 
while the polarity of ?the chair? regarding the 
attribute ?comfortable? is reversed due to the 
negation signal ?not?.  
Most of the initial research on SoN learning 
focused on negated terms finding, using either 
some heuristic rules (e.g., regular expression), 
or machine learning methods (Chapman et al, 
2001; Huang and Lowe, 2007; Goldin and 
Chapman, 2003). Negation scope finding has 
been largely ignored until the recent release of 
671
the BioScope corpus (Szarvas et al, 2008; 
Vincze et al, 2008). Morante et al (2008) and 
Morante and Daelemans (2009) pioneered the 
research on negation scope finding by formu-
lating it as a chunking problem, which classi-
fies the words of a sentence as being inside or 
outside the scope of a negation signal. How-
ever, this chunking approach suffers from low 
performance, in particular on long sentences, 
due to ignoring structured syntactic information. 
For example, given golden negation signals on 
the Bioscope corpus, Morante and Daelemans 
(2009) only got the performance of 50.26% in 
PCS (percentage of correct scope) measure on 
the full papers subcorpus (22.8 words per sen-
tence on average), compared to 87.27% in PCS 
measure on the clinical reports subcorpus (6.6 
words per sentence on average). 
This paper explores negation scope finding 
from a parse tree perspective and formulates it 
as a shallow semantic parsing problem, which 
has been extensively studied in the past few 
years (Carreras and M?rquez, 2005). In par-
ticular, the negation signal is recast as the pre-
dicate and the negation scope is recast as its 
arguments. The motivation behind is that 
structured syntactic information plays a critical 
role in negation scope finding and should be 
paid much more attention, as indicated by pre-
vious studies in shallow semantic parsing 
(Gildea and Palmer, 2002; Punyakanok et al, 
2005). Our parsing approach to negation scope 
finding differs from the state-of-the-art chunk-
ing ones in two aspects. First, we extend nega-
tion scope finding from the chunking level into 
the parse tree level, where structured syntactic 
information is available. Second, we focus on 
determining whether a constituent, rather than a 
word, is negated or not. Evaluation on the 
BioScope corpus shows that our parsing ap-
proach much outperforms the state-of-the-art 
chunking ones. 
The rest of this paper is organized as follows. 
Section 2 reviews related work. Section 3 in-
troduces the Bioscope corpus on which our 
approach is evaluated. Section 4 describes our 
parsing approach by formulating negation 
scope finding as a simplified shallow semantic 
parsing problem. Section 5 presents the ex-
perimental results. Finally, Section 6 concludes 
the work. 
2 Related Work 
While there is a certain amount of literature 
within the NLP community on negated terms 
finding (Chapman et al, 2001; Huang and 
Lowe, 2007; Goldin and Chapman, 2003), 
there are only a few studies on negation scope 
finding (Morante et al, 2008; Morante and 
Daelemans, 2009).  
Negated terms finding  
Rule-based methods dominated the initial re-
search on negated terms finding. As a repre-
sentative, Chapman et al (2001) developed a 
simple regular expression-based algorithm to 
detect negation signals and identify medical 
terms which fall within the negation scope. 
They found that their simple regular expres-
sion-based algorithm can effectively identify a 
large portion of the pertinent negative state-
ments from discharge summaries on determin-
ing whether a finding or disease is absent. Be-
sides, Huang and Lowe (2007) first proposed 
some heuristic rules from a parse tree perspec-
tive to identify negation signals, taking advan-
tage of syntactic parsing, and then located ne-
gated terms in the parse tree using a corre-
sponding negation grammar. 
As an alternative to the rule-based methods, 
various machine learning methods have been 
proposed for finding negated terms. As a rep-
resentative, Goldin and Chapman (2003) a-
dopted both Na?ve Bayes and decision trees to 
distinguish whether an observation is negated 
by the negation signal ?not? in hospital reports.  
Negation scope finding  
Morante et al (2008) pioneered the research on 
negation scope finding, largely due to the 
availability of a large-scale annotated corpus, 
the Bioscope corpus. They approached the ne-
gation scope finding task as a chunking prob-
lem which predicts whether a word in the sen-
tence is inside or outside of the negation scope, 
with proper post-processing to ensure consecu-
tiveness of the negation scope. Morante and 
Daelemans (2009) further improved the per-
formance by combing several classifiers.  
Similar to SoN learning, there are some ef-
forts in the NLP community on learning the 
scope of speculation. As a representative, 
?zg?r and Radev (2009) divided speculation 
672
learning into two subtasks: speculation signal 
finding and speculation scope finding. In par-
ticular, they formulated speculation signal 
finding as a classification problem while em-
ploying some heuristic rules from the parse tree 
perspective on speculation scope finding. 
3 Negation in the BioScope Corpus 
This paper employs the BioScope corpus 
(Szarvas et al, 2008; Vincze et al, 2008)1, a 
freely downloadable negation resource from 
the biomedical domain, as the benchmark cor-
pus. In this corpus, every sentence is annotated 
with negation signals and speculation signals 
(if it has), as well as their linguistic scopes. 
Figure 1 shows a self-explainable example. In 
this paper, we only consider negation signals, 
rather than speculation ones. Our statistics 
shows that 96.57%, 3.23% and 0.20% of nega-
tion signals are represented by one word, two 
words and three or more words, respectively. 
Additional, adverbs (e.g., not, never) and de-
terminers (e.g., no, neither) occupy 45.66% and 
30.99% of negation signals, respectively. 
 
The Bioscope corpus consists of three sub-
corpora: the full papers and the abstracts from 
the GENIA corpus (Collier et al, 1999), and 
clinical (radiology) reports. Among them, the 
full papers subcorpus and the abstracts subcor-
pus come from the same genre, and thus share 
some common characteristics in statistics, such 
as the number of words in the negation scope to 
the right (or left) of the negation signal and the 
average scope length. In comparison, the clini-
cal reports subcorpus consists of clinical radi-
ology reports with short sentences. For detailed 
statistics about the three subcorpora, please see 
Morante and Daelemans (2009). 
                                                          
                                                          
1 http://www.inf.u-szeged.hu/rgai/bioscope 
For preprocessing, all the sentences in the 
Bioscope corpus are tokenized and then parsed 
using the Berkeley parser2 (Petrov and Klein, 
2007) trained on the GENIA TreeBank (GTB) 
1.0 (Tateisi et al, 2005)3, which is a bracketed 
corpus in (almost) PTB style. 10-fold 
cross-validation on GTB1.0 shows that the 
parser achieves the performance of 86.57 in 
F1-measure. It is worth noting that the GTB1.0 
corpus includes all the sentences in the ab-
stracts subcorpus of the Bioscope corpus. 
4 Negation Scope Finding via Shallow 
Semantic Parsing 
In this section, we first formulate the negation 
scope finding task as a shallow semantic pars-
ing problem. Then, we deal with it using a sim-
plified shallow semantic parsing framework.  
4.1 Formulating Negation Scope Finding  
as a Shallow Semantic Parsing Prob-
lem 
Given a parse tree and a predicate in it, shallow 
semantic parsing recognizes and maps all the 
constituents in the sentence into their corre-
sponding semantic arguments (roles) of the 
predicate. As far as negation scope finding 
considered, the negation signal can be regarded 
as the predicate4, while the scope of the nega-
tion signal can be mapped into several con-
stituents which are negated and thus can be 
regarded as the arguments of the negation sig-
nal. In particular, given a negation signal and 
its negation scope which covers wordm, ?, 
wordn, we adopt the following two heuristic 
rules to map the negation scope of the negation 
signal into several constituents which can be 
deemed as its arguments in the given parse tree. 
<sentence id="S26.8">These findings <xcope 
id="X26.8.2"><cue type="speculation" 
ref="X26.8.2">indicate that</cue> <xcope 
id="X26.8.1">corticosteroid resistance in bron-
chial asthma <cue type="negation" 
ref="X26.8.1">can not</cue> be explained by 
abnormalities in corticosteroid receptor charac-
teristics</xcope></xcope>.</sentence> 
Figure 1: An annotated sentence in the BioScope 
corpus. 
1) The negation signal itself and all of its an-
cestral constituents are non-arguments. 
2) If constituent X is an argument of the given 
negation signal, then X should be the high-
est constituent dominated by the scope of 
wordm, ?, wordn. That is to say, X?s parent 
constituent must cross-bracket or include 
the scope of wordm, ?, wordn. 
2 http://code.google.com/p/berkeleyparser/ 
3 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA  
4 If a negation signal consists of multiply words 
(e.g., rather than), the last word (e.g., than) is cho-
sen to represent the negation signal. 
673
 Figure 2: An illustration of a negation signal and its arguments in a parse tree. 
These findings 
indicates 
that 
corticosteroid resistance
NP0,1
VBP2,2 SBAR3,11
can not
IN3,3
be
explained by abnormalities 
NP4,5
MD6,6 RB7,7
VB8,8 VP9,11
VP8,11
VP6,11
S4,11
VP2,11
S0,11
predicate
arguments
 
The first rule ensures that no argument cov-
ers the negation signal while the second rule 
ensures no overlap between any two arguments. 
For example, in the sentence ?These findings 
indicate that corticosteroid resistance can not 
be explained by abnormalities?, the negation 
signal ?can not? has the negation scope ?corti-
costeroid resistance can not be explained by 
abnormalities?. As shown in Figure 2, the node 
?RB7,7? (i.e., not) represents the negation signal 
?can not? while its arguments include three 
constituents {NP4,5, MD6,6, and VP8,11}. It is 
worth noting that according to the above rules, 
negation scope finding via shallow semantic 
parsing, i.e. determining the arguments of a 
given negation signal, is robust to some varia-
tions in parse trees. This is also empirically 
justified by our later experiments. For example, 
if the VP6,11 in Figure 2 is incorrectly expanded 
by the rule VP6,11?MD6,6+RB7,7+VB8,8+VP9,11, 
the negation scope of the negation signal ?can 
not? can still be correctly detected as long as 
{NP4,5, MD6,6, VB8,8, and VP9,11} are predicted 
as the arguments of the negation signal ?can 
not?. 
Compared with common shallow semantic 
parsing which needs to assign an argument 
with a semantic label, negation scope finding 
does not involve semantic label classification 
and thus could be divided into three consequent 
phases: argument pruning, argument identifica-
tion and post-processing. 
4.2 Argument Pruning 
Similar to the predicate-argument structures in 
common shallow semantic parsing, the nega-
tion signal-scope structures in negation scope 
finding can be also classified into several cer-
tain types and argument pruning can be done 
by employing several heuristic rules to filter 
out constituents, which are most likely 
non-arguments of a negation signal. Similar to 
the heuristic algorithm as proposed in Xue and 
Palmer (2004) for argument pruning in com-
mon shallow semantic parsing, the argument 
pruning algorithm adopted here starts from 
designating the negation signal as the current 
node and collects its siblings. It then iteratively 
moves one level up to the parent of the current 
node and collects its siblings. The algorithm 
ends when it reaches the root of the parse tree. 
To sum up, except the negation signal and its 
ancestral constituents, any constituent in the 
parse tree whose parent covers the given nega-
tion signal will be collected as argument can-
didates. Taking the negation signal node 
?RB7,7? in Figure 2 as an example, constituents 
{MD6,6, VP8,11, NP4,5, IN3,3, VBP2,2, and NP0,1} 
are collected as its argument candidates conse-
quently. 
4.3 Argument Identification 
Here, a binary classifier is applied to determine 
the argument candidates as either valid argu-
ments or non-arguments. Similar to argument 
674
identification in common shallow semantic 
parsing, the structured syntactic information 
plays a critical role in negation scope finding.  
Basic Features 
Table 1 lists the basic features for argument 
identification. These features are also widely 
used in common shallow semantic parsing for 
both verbal and nominal predicates (Xue, 2008; 
Li et al, 2009). 
Feature Remarks 
b1 Negation: the stem of the negation signal, 
e.g., not, rather_than. (can_not) 
b2 Phrase Type: the syntactic category of the
argument candidate. (NP) 
b3 Path: the syntactic path from the argument 
candidate to the negation signal. 
(NP<S>VP>RB) 
b4 Position: the positional relationship of the
argument candidate with the negation sig-
nal. ?left? or ?right?. (left) 
Table 1: Basic features and their instantiations for 
argument identification in negation scope finding, 
with NP4,5 as the focus constituent (i.e., the argu-
ment candidate) and ?can not? as the given negation 
signal, regarding Figure 2. 
Additional Features 
To capture more useful information in the ne-
gation signal-scope structures, we also explore 
various kinds of additional features. Table 2 
shows the features in better capturing the de-
tails regarding the argument candidate and the 
negation signal. In particular, we categorize the 
additional features into three groups according 
to their relationship with the argument candi-
date (AC, in short) and the given negation sig-
nal (NS, in short). 
Some features proposed above may not be 
effective in argument identification. Therefore, 
we adopt the greedy feature selection algorithm 
as described in Jiang and Ng (2006) to pick up 
positive features incrementally according to 
their contributions on the development data. 
The algorithm repeatedly selects one feature 
each time which contributes most, and stops 
when adding any of the remaining features fails 
to improve the performance. As far as the ne-
gation scope finding task concerned, the whole 
feature selection process could be done by first 
running the selection algorithm with the basic 
features (b1-b4) and then incrementally picking 
up effective features from (ac1-ac6, AC1-AC2, 
ns1-ns4, NS1-NS2, nsac1-nsac2, and NSAC1 
-NSAC7). 
Feature Remarks 
argument candidate (AC) related 
ac1 the headword (ac1H) and its POS (ac1P). 
(resistance, NN) 
ac2 the left word (ac2W) and its POS (ac2P). 
(that, IN) 
ac3 the right word (ac3W) and its POS (ac3P). 
(can, MD) 
ac4 the phrase type of its left sibling (ac4L) 
and its right sibling (ac4R). (NULL, VP) 
ac5 the phrase type of its parent node. (S) 
ac6 the subcategory. (S:NP+VP) 
combined features (AC1-AC2) 
b2&fc1H, b2&fc1P 
negation signal (NS) related 
ns1 its POS. (RB) 
ns2 its left word (ns2L) and right word (ns2R). 
(can, be) 
ns3 the subcategory. (VP:MD+RB+VP) 
ns4 the phrase type of its parent node. (VP) 
combined features (NS1-NS2) 
b1&ns2L, b1&ns2R 
NS-AC-related 
nsac1 the compressed path of b3: compressing 
sequences of identical labels into one.  
(NP<S>VP>RB) 
nsac2 whether AC and NS are adjacent in posi-
tion. ?yes? or ?no?. (no) 
combined features (NSAC1-NSAC7) 
b1&b2, b1&b3, b1&nsac1, b3&NS1, b3&NS2, 
b4&NS1, b4&NS2 
Table 2: Additional features and their instantiations 
for argument identification in negation scope find-
ing, with NP4,5 as the focus constituent (i.e., the 
argument candidate) and ?can not? as the given 
negation signal, regarding Figure 2. 
4.4 Post-Processing 
Although a negation signal in the BioScope 
corpus always has only one continuous block 
as its negation scope (including the negation 
signal itself), the negation scope finder may 
result in discontinuous negation scope due to 
independent prediction in the argument identi-
fication phase. Given the golden negation sig-
nals, we observed that 6.2% of the negation 
scopes predicted by our negation scope finder 
are discontinuous.  
Figure 3 demonstrates the projection of all 
the argument candidates into the word level. 
According to our argument pruning algorithm 
in Section 4.2, except the words presented by 
675
the negation signal, the projection covers the 
whole sentence and each constituent (LACi or 
RACj in Figure 3) receives a probability distri-
bution of being an argument of the given nega-
tion signal in the argument identification phase. 
 Since a negation signal is deemed inside of its 
negation scope in the BioScope corpus, our 
post-processing algorithm first includes the 
negation signal in its scope and then starts to 
identify the left and the right scope boundaries, 
respectively. 
As shown in Figure 3, the left boundary has 
m+1 possibilities, namely the negation signal 
itself, the leftmost word of constituent LACi 
(1<=i<=m). Supposing LACi receives prob-
ability of Pi being an argument, we use the fol-
lowing formula to determine LACk* whose 
leftmost word represents the boundary of the 
left scope. If k*=0, then the negation signal 
itself represents its left boundary. 
( )*
1 1
arg max 1
k m
i i
k i i k
k P
= = +
= ?? ? P?
                                                          
 
Similarly, the right boundary of the given 
negation signal can be decided. 
5 Experimentation 
We have evaluated our shallow semantic pars-
ing approach to negation scope finding on the 
BioScope corpus. 
5.1 Experimental Settings 
Following the experimental setting in Morante 
and Daelemans (2009), the abstracts subcorpus 
is randomly divided into 10 folds so as to per-
form 10-fold cross validation, while the per-
formance on both the papers and clinical re-
ports subcorpora is evaluated using the system 
trained on the whole abstracts subcorpus. In 
addition, SVMLight5 is selected as our classi-
fier. In particular, we adopt the linear kernel 
and the training parameter C is fine-tuned to 
0.2. 
1
5 http://svmlight.joachims.org/ 
The evaluation is made using the accuracy. 
We report the accuracy using three measures: 
PCLB and PCRB, which indicate the percent-
ages of correct left boundary and right bound-
ary respectively, PCS, which indicates the per-
centage of correct scope as a whole.  
LACm   ?.   LAC1 RAC1   ?.   RACn
m n 
Figure 3: Projecting the left and the right argument 
candidates into the word level. 
5.2 Experimental Results on Golden Parse 
Trees 
In order to select beneficial features from the 
additional features proposed in Section 4.3, we 
randomly split the abstracts subcorpus into 
training and development datasets with propor-
tion of 4:1. After performing the greedy feature 
selection algorithm on the development data, 
features {NSAC5, ns2R, NS1, ac1P, ns3, 
NSAC7, ac4R} are selected consecutively for 
argument identification. Table 3 presents the 
effect of selected features in an incremental 
way on the development data. It shows that the 
additional features significantly improve the 
performance by 11.66% in PCS measure from 
74.93% to 86.59% ( ). 2; 0.0p? <
 
Feature PCLB PCRB PCS 
Baseline 84.26 88.92 74.93 
+NSAC5 90.96 88.92 81.34 
+ns2R 91.55 88.92 81.92 
+NS1 92.42 89.50 83.09 
+ac1P 93.59 89.50 84.26 
+ns3 93.88 90.09 84.84 
+NSAC7 94.75 89.80 85.42 
+ac4R 95.04 90.67 86.59 
Table 3: Performance improvement (%) of includ-
ing the additional features in an incremental way on 
the development data (of the abstracts subcorpus). 
However, Table 3 shows that the additional 
features behave quite differently in terms of 
PCLB and PCRB measures. For example, 
PCLB measure benefits more from features 
NSAC5, ns2R, NS1, ac1P, and NSAC7 while 
PCRB measure benefits more from features 
NS1 and ac4R. It also shows that the features 
(e.g., NSAC5, ns2R, NS1, NSAC7) related to 
neighboring words of the negation signal play a 
critical role in recognizing both left and right 
boundaries. This may be due to the fact that 
neighboring words usually imply sentential 
information. For example, ?can not be? indi-
cates a passive clause while ?did not? indicates 
an active clause. Table 3 also shows that the 
recognition of left boundaries is much easier 
than that of right boundaries. This may be due 
676
to the fact that 83.6% of negation signals have 
themselves as the left boundaries in the ab-
stracts subcorpus.  
gument candidate is outside or cross-brackets 
with the golden negation scope, then it is a 
non-argument. The oracle performance is pre-
sented in the rows of oracle in Table 5 and Ta-
ble 6. 
Table 4 presents the performance on the ab-
stracts subcorpus by performing 10-fold 
cross-validation. It shows that the additional 
features significantly improve the performance 
over the three measures ( ). 2; 0.0p? <
Table 5 and Table 6 show that: 
1) Automatic syntactic parsing lowers the per-
formance of negation scope finding on the 
abstracts subcorpus in all three measures (e.g. 
from 83.10 to 81.84 in PCS). As expected, 
the parser trained on the whole GTB1.0 
corpus works better than that trained on 
6,691 sentences (e.g. 64.02 Vs. 62.70, and 
89.79 Vs. 85.21 in PCS measure on the full 
papers and the clinical reports subcorpora, 
respectively). However, the performance de-
crease shows that negation scope finding is 
not as sensitive to automatic syntactic pars-
ing as common shallow semantic parsing, 
whose performance might decrease by about 
~10 in F1-measure (Toutanova et al, 2005). 
This indicates that negation scope finding 
via shallow semantic parsing is robust to 
some variations in the parse trees. 
1
Feature PCLB PCRB PCS 
Baseline 84.29 87.82 74.05 
+selected features 93.06 88.96 83.10 
Table 4: Performance (%) of negation scope finding 
on the abstracts subcorpus using 10-fold 
cross-validation.  
5.3 Experimental Results on Automatic 
Parse Trees 
The GTB1.0 corpus contains 18,541 sentences 
in which 11,850 of them (63.91%) overlap with 
the sentences in the abstracts subcorpus6. In 
order to get automatic parse trees for the sen-
tences in the abstracts subcorpus, we train the 
Berkeley parser with the remaining 6,691 sen-
tences in GTB1.0. The Berkeley parser trained 
on 6,691 sentences achieves the performance of 
85.22 in F1-measure on the other sentences in 
GTB1.0. For both the full papers and clinical 
reports subcorpora, we get their automatic 
parse trees by using two Berkeley parsers: one 
trained on 6,691 sentences in GBT1.0, and the 
other trained on all the sentences in GTB1.0.  
2) autoparse(test) consistently outperforms 
autoparse(t&t) on both the abstracts and the 
full papers subcorpora. However, it is sur-
prising to find that autoparse(t&t) achieves 
better performance on the clinical reports 
subcorpus than autoparse(test). This may be 
due to the special characteristics of the 
clinical reports subcorpus, which mainly 
consists of much shorter sentences with 6.6 
words per sentence on average, and better 
adaptation of the argument identification 
classifier to the variations in the automatic 
parse trees. 
To test the performance on automatic parse 
trees, we employ two different configurations. 
First, we train the argument identification clas-
sifier on the abstracts subcorpus using auto-
matic parse trees produced by Berkeley parser 
trained on 6,691 sentences. The experimental 
results are presented in the rows of auto-
parse(t&t) in Table 5 and Table 6. Then, we 
train the argument identification classifier on 
the abstracts subcorpus using golden parse 
trees. The experimental results are presented in 
the rows of autoparse(test) in Table 5 and Ta-
ble 6.  
3) The performance on all three subcorpora 
indicates that the recognition of right 
boundary is much harder than that of left 
boundary. This may be due to the longer 
right boundary on an average. Our statistics 
shows that the average left/right boundaries 
are 1.1/6.9, 0.1/3.7, and 1.2/6.5 words on the 
abstracts, the full papers and the clinical re-
ports subcorpora, respectively. 
We also report an oracle performance to ex-
plore the best possible performance of our sys-
tem by assuming that our negation scope finder 
can always correctly determine whether a can-
didate is an argument or not. That is, if an ar-
4) The oracle performance is less sensitive to 
automatic syntactic parsing. In addition, 
given the performance gap between the per-
formance of our negation scope finder and 
the oracle performance, there is still much 
room for further performance improvement. 
                                                          
6 There are a few cases where two sentences in the 
abstracts subcorpus map into one sentence in GTB. 
677
 Abstracts Papers Clinical 
 PCLB PCRB PCS PCLB PCRB PCS PCLB PCRB PCS 
autoparse(t&t) 91.97 87.82 80.88 85.45 67.20 59.26 97.48 88.30 85.89
autoparse(test) 92.71 88.33 81.84 87.57 68.78 62.70 97.48 87.73 85.21
oracle 99.72 94.59 94.37 98.94 84.13 83.33 99.89 98.39 98.39
Table 5: Performance (%) of negation scope finding on the three subcorpora by using automatic parser trained 
with 6,691 sentences in GTB1.0.  
 Papers Clinical 
 PCLB PCRB PCS PCLB PCRB PCS 
autoparse(t&t) 85.98 67.99 60.32 97.48 92.66 90.48 
autoparse(test) 87.83 70.11 64.02 97.36 92.20 89.79 
oracle 98.94 83.86 83.07 99.77 97.94 97.82 
Table 6: Performance (%) of negation scope finding on the two subcorpora by using automatic parser trained 
with all the sentences in GTB1.0.  
 
Method Abstracts Papers Clinical 
M et al (2008) 57.33 n/a n/a 
M & D (2009) 73.36 50.26 87.27 
Our baseline 73.42 53.70 88.42 
Our final system 81.84 64.02 89.79 
Table 7: Performance comparison over the PCS 
measure (%) of our system with other 
state-of-the-art ones.  
Table 7 compares our performance in PCS 
measure with related work. It shows that even 
our baseline system with four basic features as 
presented in Table 1 performs better than 
Morante et al (2008) and Morante and Daele-
mans(2009). This indicates the appropriateness 
of our simplified shallow semantic parsing ap-
proach and the effectiveness of structured syn-
tactic information on negation scope finding. It 
also shows that our final system significantly 
outperforms the state-of-the-art ones using a 
chunking approach, especially on the abstracts 
and full papers subcorpora. However, the im-
provement on the clinical reports subcorpus is 
less apparent, partly due to the fact that the 
sentences in this subcorpus are much simpler 
(with average length of 6.6 words per sentence) 
and thus a chunking approach can achieve high 
performance. Following are two typical sen-
tences from the clinical reports subcorpus, 
where the negation scope covers the whole sen-
tence (except the period punctuation). Such 
sentences account for 57% of negation sen-
tences in the clinical reports subcorpus. 
 
6 Conclusion 
In this paper we have presented a simplified 
shallow semantic parsing approach to negation 
scope finding by formulating it as a shallow 
semantic parsing problem, which has been ex-
tensively studied in the past few years. In par-
ticular, we regard the negation signal as the 
predicate while mapping the negation scope 
into several constituents which are deemed as 
arguments of the negation signal. Evaluation on 
the Bioscope corpus shows the appropriateness 
of our shallow semantic parsing approach and 
that structured syntactic information plays a 
critical role in capturing the domination rela-
tionship between a negation signal and its ne-
gation scope. It also shows that our parsing 
approach much outperforms the state-of-the-art 
chunking ones. To our best knowledge, this is 
the first research on exploring negation scope 
finding via shallow semantic parsing. 
Future research will focus on joint learning 
of negation signal and its negation scope find-
ings. Although Morante and Daelemans (2009) 
reported the performance of 95.8%-98.7% on 
negation signal finding, it lowers the perform-
ance of negation scope finding by about 
7.29%-16.52% in PCS measure.  
Acknowledgments 
This research was supported by Projects 
60683150, 60970056, and 90920004 under the 
National Natural Science Foundation of China, 
Project 20093201110006 under the Specialized 
Research Fund for the Doctoral Program of 
Higher Education of China. 
(1) No evidence of focal pneumonia . 
 
(2) No findings to account for symptoms . 
678
References 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2005.  
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 
2001. A Simple Algorithm for Identifying Ne-
gated Findings and Diseases in Discharge Sum-
maries. Journal of Biomedical Informatics, 34: 
301-310. 
Nigel Collier, Hyun Seok Park, Norihiro Ogata, et 
al. 1999. The GENIA project: corpus-based 
knowledge acquisition and information extrac-
tion from genome research papers. In Proceed-
ings of EACL 1999.  
Daniel Gildea and Martha Palmer. 2002. The Ne-
cessity of Parsing for Predicate Argument Rec-
ognition. In Proceedings of ACL 2002. 
Ilya M. Goldin and Wendy W. Chapman. 2003. 
Learning to Detect Negation with ?Not? in Medi-
cal Texts. In Proceedings of SIGIR 2003. 
Yang Huang and Henry Lowe. 2007. A Novel Hy-
brid Approach to Automated Negation Detection 
in Clinical Radiology Reports. Journal of the 
American Medical Informatics Association, 14(3): 
304-311. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Seman-
tic Role Labeling of NomBank: A Maximum En-
tropy Approach. In Proceedings of EMNLP 
2006. 
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu, 
and Peide Qian. Improving Nominal SRL in 
Chinese Language with Verbal SRL Information 
and Automatic Predicate Recognition. In Pro-
ceedings of EMNLP 2009. 
Roser Morante, Anthony Liekens, and Walter 
Daelemans. 2008. Learning the Scope of Nega-
tion in Biomedical Texts. In Proceedings of 
EMNLP 2008. 
Roser Morante and Walter Daelemans. 2009. A 
Metalearning Approach to Processing the Scope 
of Negation. In Proceedings of CoNLL 2009. 
Arzucan ?zg?r; Dragomir R. Radev. 2009. Detect-
ing Speculations and their Scopes in Scientific 
Text. In Proceedings of EMNLP 2009. 
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceedings of 
NAACL 2007. 
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 
2005. The Necessity of Syntactic Parsing for 
Semantic Role Labeling. In Proceedings of IJCAI 
2005. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, 
and J?nos Csirik. 2008. The BioScope corpus: 
annotation for negation, uncertainty and their 
scope in biomedical texts. In Proceedings of 
BioNLP 2008. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun?ichi Tsujii. 2005. Syntax Annotation for the 
GENIA Corpus. In Proceedings of IJCNLP 2005, 
Companion volume. 
Kristina Toutanova, Aria Haghighi, and Christopher 
D. Manning. 2005. Joint Learning Improves Se-
mantic Role Labeling. In Proceedings of ACL 
2005. 
Peter D. Turney. 2002. Thumbs Up or Thumbs 
Down? Semantic Orientation Applied to Unsu-
pervised Classification of Reviews. In Proceed-
ings of ACL 2002. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for 
uncertainty, negation and their scopes. BMC 
Bioinformatics, 9(Suppl 11):S9. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
Features for Semantic Role Labeling. In Pro-
ceedings of EMNLP 2004. 
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
 
 
679
Coling 2010: Poster Volume, pages 294?302,
Beijing, August 2010
A Novel Method for Bilingual Web Page Acquisition from 
Search Engine Web Records 
Yanhui Feng, Yu Hong, Zhenxiang Yan, Jianmin Yao, Qiaoming Zhu 
School of Computer Science & Technology, Soochow University 
{20094227002, hongy, 20074227065071, jyao, qmzhu}@suda.edu.cn 
Abstract
A new approach has been developed 
for acquiring bilingual web pages from 
the result pages of search engines, 
which is composed of two challenging 
tasks. The first task is to detect web 
records embedded in the result pages 
automatically via a clustering method 
of a sample page. Identifying these 
useful records through the clustering 
method allows the generation of highly 
effective features for the next task 
which is high-quality bilingual web 
page acquisition. The task of 
high-quality bilingual web page 
acquisition is a classification problem. 
One advantage of our approach is that it 
is search engine and domain 
independent. The test is based on 2516 
records extracted from six search 
engines automatically and annotated 
manually, which gets a high precision 
of 81.3% and a recall of 94.93%. The 
experimental results indicate that our 
approach is very effective. 
1 Introduction 
There have been extensive studies on parallel 
resource extraction from parallel monolingual 
web pages of some bilingual web sites (Chen 
and Nie, 2000; Resnik and Smith, 2003; Zhang 
et al, 2006; Shi et al, 2006). Candidate parallel 
web pages are acquired by making use of URL 
strings or HTML tags, then the translation 
equivalence of the candidate pairs are verified 
via content-based features.  
  However, we observe that bilingual 
resources may exist not only in two parallel 
monolingual web pages, but also in single 
bilingual web pages. For example, many news 
web pages and English learning pages are 
bilingual. Based on this observation, 
researchers have proposed methods to improve 
parallel sentences extraction within a bilingual 
web page. Jiang (2009) uses an adaptive 
pattern-based method to mine interesting 
bilingual data based on the observation that 
bilingual data usually appears collectively 
following similar patterns. Because the World 
Wide Web is composed of billions of pages, it 
is a challenging task to locate valuable 
bilingual pages. 
  To acquire bilingual web pages 
automatically, a novel and effective method is 
proposed in this paper by making use of search 
engines, such as Baidu (http://www.baidu.com). 
By submitting parallel sentence pairs to the 
given search engine, lots of result pages with 
web records are returned, most of which are 
linked to bilingual web pages. We first identify 
and extract all result records automatically by 
selecting and analyzing a sample page with a 
clustering method, and then select high-quality 
bilingual web pages from candidates with 
classification algorithms. 
Our method has the following advantages: 
  1. Former researchers extract parallel corpus 
from specific bilingual web sites. Since search 
engines index amounts of web pages, and we 
aim to acquire bilingual pages based on them, 
our method expands the corpus source greatly. 
  2. For one search engine, only one sample 
result page is used to generate the record 
wrapper. Then the wrapper is used to identify 
web records from other result pages of the same 
search engine. Compared with existing data 
record extraction technologies, such as MDR 
(Liu et al, 2003; Zhai and Liu, 2006), our 
method is more effective and efficient. 
294
  3. We model the issue of verification 
bilingual pages as a binary-class classification 
problem. The records acquired automatically 
and annotated manually are utilized to train and 
test the classifier. This work is domain and 
search engine independent. That is to say, the 
records acquired from any search engine in any 
domain are used indiscriminately as training 
and testing dataset. 
  The rest of the paper is organized as follows. 
Related works are introduced in section 2. 
Section 3 provides an overview of our solution. 
The work about bilingual page acquisition and 
verification is introduced in section 4 and 5. 
Section 6 presents the experiments and results. 
Finally section 7 concludes the paper. 
2 Related Work 
As far as we know, there is no publication 
available on acquiring bilingual web pages. 
Most existing studies, such as Nie (1999), 
Resnik and Smith (2003) and Shi (2006), mine 
parallel web documents within bilingual web 
sites first and then extract bilingual sentences 
from mined parallel documents using sentence 
alignment method. 
  In this paper, the candidate bilingual web 
pages are acquired by analyzing web records 
embedded in the search engines? result pages. 
Therefore, record extraction from result pages 
is a critical technique in our method. Many 
researches, such as Laender (2002), have been 
developed various solutions in web information 
extraction from kinds of perspectives. 
  Earlier web information extraction systems 
(Baumgartner et al, 2001; Liu et al, 2000; Zhai 
and Liu, 2005) require users to provide labeled 
data so that the extraction rules could be learned. 
Yet such semi-automatic methods are not 
scalable enough to the whole Web which 
changes at any time. That?s why more and more 
researchers focus on fully or nearly fully 
automatic solutions.  
  Structured data objects are normally database 
records retrieved from underlying web 
databases and displayed on the web pages with 
some fixed templates, so automatic extraction 
methods try to find such patterns and use them 
to extract more data. Several approaches have 
succeeded to address the problem automatically 
without human assistance. IEPAD (Chang and 
Lui, 2001) identifies sub-strings that appear 
many times in a document. By traversing the 
DOM tree of the Web page, MDR extracts the 
data-rich sub-tree indirectly by detecting the 
existence of multiple similar generalized-nodes. 
The key limitation is its greedy manner of 
identifying a data region. DEPTA (Zhai and Liu, 
2005) uses visual information (locations on the 
screen at which the tags are rendered) to infer 
the structural relationship among tags and to 
construct a tag tree. NET (Liu and Zhai, 2005) 
extracts flat or nested data records by 
post-order or pre-order traversal of the tag tree. 
ViNTs (Zhao et al, 2005) considers the web 
page as a tag tree, and utilizes both visual 
content features as well as tag tree structures. It 
assumes that data records are located in a 
minimum data-rich sub-tree and separated by 
separators of tag forests. Zhao (2006) explicitly 
aims at extracting all dynamic sections from 
web pages, and extracting records in each 
section, whereas ViNTs focuses on record 
extraction from a single section. Miao (2009) 
figures out how tag paths format the whole page. 
Compared with the previous method, it 
compares pairs of tag path occurrence patterns 
to estimate how likely these tag paths represent 
the same list of objects instead of comparing 
one pair of individual sub-trees in the record. It 
brings some noise. We follow this method and 
make appropriate improvement for our task. 
3 Basic Concepts and Overview 
3.1 Basic Concepts 
Some basic concepts are introduced below. 
Figure 1. An example of search engine return
295
Tag Path: The path of a tag consists of all 
nodes from the tree root <html> to itself. We 
use tag path to specify the location of the tag. 
The tag paths are classified into two types: text 
tag paths and non-text tag paths. 
  Data Record: When a page is considered as 
strings of tokens, data records are enwrapped 
by one or more tag paths, which compose the 
visually repeating pattern in a page. This paper 
aims to extract such structured data records that 
are produced by computer programs following 
some fixed templates, while whose contents are 
usually retrieved from backend databases. For 
example, there are four records in Figure 1. 
3.2 Method Overview 
We can get much more bilingual web pages by 
submitting parallel sentence pairs to the search 
engine than submitting monolingual queries. 
Based on this observation, our work is as 
shown in Figure 2. The algorithm consists of 
two steps: 1) Record wrapper generation. By 
submitting parallel sentence pairs to search 
engines, result pages containing lots of web 
records are returned. In order to generate record 
wrappers, we select and analyze a sample page 
and then apply clustering method to tag paths 
with similar patterns. We apply these wrappers 
to extract more records, which are linked to 
candidate bilingual web pages. 2) High-quality 
bilingual page acquisition. In order to acquire 
high-quality bilingual pages from candidates, a 
binary classifier is constructed to decide 
whether the candidate pages are bilingual or not. 
In order to improve the classifier, some useful 
resources are used, such as a dictionary and 
translation equivalents. 
However, a result page often contains some 
information irrelevant to the query, such as 
information related to the hosting site of the 
search engine, which increases the difficulty of 
record extraction. Besides, there are also many 
irrelevant records irrelevant to the query. So 
our focus is to acquire plenty of features to 
filter out the irrelevant pages from the 
candidates.
In this paper, the first result page is chosen as 
the sample page and Affinity Propagation (AP) 
clustering is used. The reason lies in Frey and 
Dueck (2007), which proves that to produce the 
groups of tag paths; the AP algorithm does not 
require the three restrictions: 1) the samples 
must be of a specific kind, 2) the similarity 
values must be in a specific range, and 3) the 
similarity matrix must be symmetric. In order 
to decide the type of a page, the Support 
Vector Machines (SVM) (Cortes and Vapnik, 
1995) classifier on Fuzzy C-means is 
constructed combining with word-overlap, 
length and frequency measures. SVM is 
well-fitted to treat such classification problems 
that involve interrelated features likes ours, 
while most probabilistic classifiers, such as 
Na?ve Bayes classifier, strongly assume feature 
independence (DuVerle and Prendinger, 2009). 
Figure 2. Overview of the method 
4 Bilingual Page Acquisition 
4.1 Result Page Extraction 
The result pages of a search engine consist of a 
ranked list of document summaries linked to the 
actual documents or web pages. A web 
document summary typically contains the title 
and URL of the web page, links to live and 
cached versions of the page and, most 
importantly, a short text summary, or a snippet, 
to convey the contents of the page. Such 
snippets embedded in result pages of search 
engines are query-dependent summaries. White 
(2001) finds the result pages are sensitive to the 
content and language of the query. If the query 
is monolingual, the returned search results are 
mostly monolingual, while the result pages are 
bilingual if the query is bilingual. In order to 
acquire more bilingual web pages, we submit 
parallel translation pairs. Figure 1 gives an 
example result page from Baidu, in which the 
snapshot consists of four records related to the 
query, which consists of ?I see.? and its 
translation ???????. The results have 
296
more effective advantages than submitting the 
query ?I see.? or ??????? respectively. 
4.2 Clustering With Path Similarity 
Given a web page, we get the occurrence 
positions of each tag path the same as the 
sequence in the preorder traversal of the page?s 
DOM tree. Certainly, there are many tag paths 
which appear several times in the whole page. 
So an inverted mapping from HTML tag paths 
to their positions is built easily. For example, 
there are 599 tag paths formatting the sample 
page in Figure 1, and after the inverted mapping, 
we acquire 86 unique tag paths in all. Only tick 
off one part of the results as shown in Table 1, 
where Pi represents the ith unique tag path, and 
the vector Si is defined to store the occurrence 
positions of Pi in the third column.  
  As introduced above, detecting visually 
repeating tag paths is a clustering problem. 
Above all, a factor in determining the clustering 
performance is the choice of similarity 
functions, which captures the likelihood that 
two data samples belong to the same cluster. In 
our case, the similarity scores between two tag 
paths aim to capture how their positions are 
close to each other and how they interleave 
each other. 
  With the purpose of characterizing how close 
two tag paths appear, we only acquire the 
distance between paths? average positions, 
which is easy to obtain by the acquired 
occurrence vectors. For example, the average 
position of P11 and P15 in Table 1 is 227 and 
215, so the distance between them is 12. 
L UniqueTag  Path (Pi)
Occurrences (Si) of Pi
1 \html 1 
3 \html\head\#text 3,4,7,8,9 
9 \html\body\table 
84,93,115,146,180, 
217,258,292,335,372,
406,437 
11 \html\body\table\tr
15,85,94,116,147,181,
218,259,293,336,373,
407,438 
14
\html\body\table\tr
\td\#text 
18,21,24,27,55,79,87,
91,97,111,113 
15
\html\body\table\tr
\td\a 
19,88,118,149,183, 
220,261,295,338,375,
409,440 
Table 1. Unique tag paths of the sample page 
  However, the most difficult problem is how 
to capture the interleaving characteristic 
between two tag paths. Before doing that, 
another vector Oi is produced. Oi(k) indicates 
whether the tag path Pi occurs in the position k 
or not by its value. In addition, the value is 
binary that 0 or 1, and 0 shows Pi doesn?t occur 
in the position k, while 1 shows the opposite. Of 
particular note, the length of each Oi is equal to 
the total number of HTML tags that formatting 
the whole web page. Take the tag path P3
(?\html\head\#text?) in Table 1 as an example, 
whose position vector O3 is (0, 0, 1, 1, 0, 0, 1, 1, 
1, 0? 0), and the vector?s length is 599, 
because there are totally 599 tag paths 
formatting the sample page in Figure 1. 
  Based on the position vectors, we capture 
how tag path Pi and Pj interleave each other by 
a segment 
ji
OOD /  of Oi divided by Oj. We 
aim to find such tag paths that divide each other 
in average. In other words if the variance of 
counts in the segment 
ji
OOD /  is stable, they 
are likely to be grouped in the same cluster. So, 
we define the interleaving measureP in terms 
of the variances of 
ji
OOD /  and ij OOD /  as: 
)}( ),(   max{),( // ijji OOOOji DVarDVarOO  P (1)
where
ji
OOD /  is acquired by Oj as follows: if 
value of Oj(k) is 1, Oi(k) is a separator to
segment itself into several regions. The value of 
every element in the segment is the count of Pi
that occurs in every region, which is the 
number of 1 in the region. 
Figure 3. An Example of tag paths 
  In addition, there may be many consecutive 
separators in Oi, and we integrate them into one. 
Besides, the segment is a non-empty set. So if 
there is no occurrence of Pi in one region, we 
297
will ignore this special region. Figure 3 shows 
three tag paths. P1 and P2 are likely to belong 
to the same cluster because of their regular 
occurrences, whereas the occurrences of P3 are 
comparatively irregular. By our method, 
31 / OO
D  = {1, 1, 1} and 
13 /OO
D = {1, 2, 1}. We 
integrate separators once and ignore an empty 
region in the process of getting
31 / OO
D .
  Both the score of the closeness measure and 
the interleaving measure for any two tag paths 
are non-negative real numbers. And a smaller 
value of either measure indicates a high 
frequency that the two tag paths appear 
regularly. The measure ),( ji PPV  defined 
below is inversely proportional of these two 
measures. 
HP
HV
u
 
),(),(
),(
jiji
ji OOSSc
PP (2)
where H  is a non-negative term that avoids 
dividing by 0 and normalizes the similarity 
value so that it falls into the range (0, 1]. In our 
experiment, we choose H = 10. By Equation 2, 
we calculate the similarity value of any pair of 
tag paths. As expected, the pairwise similarity 
matrix is fed into the AP clustering algorithm 
directly, and each cluster acquired from AP 
clustering contains n tag paths, which indicates 
that those n paths appear repeatedly together 
with high frequency, and the tag paths that have 
no remarkable relation are spilt into different 
clusters. For the given sample page in Figure 1, 
the number of identified clusters is 16.  
  We observe that HTML code of most data 
records contain more than three HTML tags, so 
we only examine the clusters containing four or 
more visual signals. In the clustering result of 
sample page in Figure 1, there are three 
clusters? sizes less than four. Meanwhile, we 
also note that: 
 1. The feature page of a common search 
engine usually contains 10 or more web records 
with similar layout pattern. So we define a 
threshold T=3. If an ancestor tag path doesn?t 
occur more than T times, we believe these tag 
path dose not lead a record.  
 2. Usually the content of the result pages 
returned by search engines is completely related 
to the queries, which means the data records 
that we are interested in are distributed in the 
whole page as main component. So the 
occurrence position of valuable tag paths must 
be global optimization. In this paper, the scope 
between beginning and ending occurrence must 
be wider than three quarters of the length of the 
web page. 
  Thus, we get essential clusters fit with above 
observations, which is denoted by C= {C1,
C2?CM}. Once we have the essential clusters, 
we apply them in new web page of the same 
search engine to identify data records. 
4.3 Data Record Extraction  
Based on the essential clusters, we extract the 
exact data records from the real content of text 
tag path that follow the ancestor tag path.  
  In order to describe the extraction process in 
details, we firstly define DaI as the child tag 
paths of an ancestor tag path Pa, and suppose 
that (Pos1? Posi? Posm) is the occurrence 
vector of Pa, which means at each position Posi
the tag path Pi occurs. Da(i) is such a tag path set 
that the position Pos of every path in it is Posi
<Pos<Posi+1. In the meantime, such path strings 
must begin with the same prefix of Pa. Such as 
in Table 2, Da(i) contains tag paths from Posi to 
Posi+1-1, and we obtain the ith records 
embedded in the result pages by acquiring the 
real content of all text tag paths in Da(i).
Occurrence 
of Pa
DaI of 
Pa
Child tag path 
Pos1 Pa:\html\body\table\tr 
Pos1+1 Pt:\html\body\table\tr\?
?? ?? 
Pos2-1 
Da(1)
Pk: ?? 
? ? ?
Posi Pa:\html\body\table\tr 
Posi+1 Pt:\html\body\table\tr\?
  ?? ?? 
Posi+1-1 
Da(i)
Pn: ?? 
? ? ?
Posm Pa:\html\body\table\tr 
  ?? 
Da(m)
 ?? 
Table 2. Collection of child tag paths for 
ancestor tag path
298
5 Bilingual Web Page Verification 
Based on the previous work, we capture a list of 
records based on a holistic analysis of a result 
page, and each record contains snippets and 
URLs related to the query. In this section, we 
aim to decide whether the candidate pages that 
returned records are linked to are bilingual or 
not by putting some statistical features 
(collected from snippets) into an effective SVM 
classification. 
  To the acquired snippets, some necessary 
preprocessing is made before we acquire 
useful features. We remove most of the noise 
that affect the precision and robustness of the 
entire system by such methods as recovery of 
abbreviation words, deletion of noisy words, 
amendment for half or full punctuations and 
simplified or traditional characters, and so on. 
  The snippet is described with more regular 
contents after preprocessing. We cut the 
snippet into several segments by its language. 
Each segment of the snippet is just represented 
in one language, which is either English or 
Chinese in this paper and different from its 
adjacent segments. So the source snippets are 
transferred into such language strings that 
consist of C and E, where C stands for Chinese 
and E stands for English. It is unlikely that 
continuous C or E exists in the same language 
string. We store the real text Tc (Te) that each C 
(E) stands for. We take the snippet ?I see. ??
???I quit! ?????? as example, its 
language string is ?ECEC? and real text string 
is TeTcTeTc, where the two Te stand for ?I see? 
and ?I quit?, the two Tc stand for ??????
and ??????.
  Note that different feature functions for the 
classifier will lead to different results, it is 
important to choose feature functions that will 
help to discriminate different classes. In this 
paper, the SVM classifier involves 
word-overlap, length and frequency features. 
We define these three features based on the 
snippet itself as follows: 
(1) Word-Overlap measure  
  Word overlap judges the similarity of 
Chinese term and English term. In this paper, 
we acquire the word-overlap score between any 
two adjacent language segments. The similarity 
Score(c_res,e_res) of Chinese term and English 
term is based on word-overlap as following: 
1 1
( ( , ))
( _ , _ )
p
i j
i j q
Max Sim c e
Score c res e res
I
 d d 
?
 (3) 
where the denominator is normalization factor, 
and in our experiment we select p+q as its value, 
where p stands for the length of Chinese term 
and q stands for the length of English term. In 
addition, ci stands for the ith word of Chinese 
term and ej stands for the jth word of English 
term. Sim(ci,ej) in Liu (2003) and Deng (2004) 
stands for the similarity of Chinese word ci and
English word ej.
  In our experiment, the Chinese and English 
sub-snippets are equivalent to Chinese and 
English sentences of the bilingual pages. In the 
segmented snippet, with regard to each 
sub-snippet T, which is at even position in the 
language string, we separately evaluate the 
intermediate score for snippet T with its left 
and right neighbors by Equation 3. Especially 
when T doesn?t have right or left neighbor, the 
score for T with its null neighbor is 0. So for 
every sub-snippet that needs to be scored the 
word-overlap score, there are two candidate 
scores with its adjacent neighbors. Then we 
choose the higher value as one item of an 
intermediate result vector. Either the length of 
the language string is 2 u n or 2 u n+1, the 
length of intermediate vector is n, and the final 
score is computed as follows: 
mn
InV
sScore
n
k
k
u
 
?
 1)( (4)
where Score(s) stands for the final score of 
snippet s on the word-overlap measure, and 
vector InV is the intermediate result vector as 
mentioned before. The length of the vector InV
is n, and m is the number of its items that is not 
equal to zero. m/n is used as a useful measure 
of length, because it indicates how many 
parallel pairs are there in the same snippet. 
(2) Length-Based measure  
  We acquire three scores about length 
measure. Take the language string ?ECECEC? 
as example, we use ?E1C1E2C2E3C3? to replace 
it for simple description. We acquire one score 
of the length measure as follows: 
)(
))()((
)( 1
sLen
eLencLen
sScore
m
i
?
 

 (5)
299
where s and m stand for the same as in Equation 
4. In addition, c and e stands for such 
sub-snippet that Score(c,e) contributes to 
?
 
n
k
kInV
1
. The function Len(s) is to compute the 
number of words in the sentence. 
We acquire the length of language string. If 
the length is too long or too short, the 
associated web page is unlikely to be a 
bilingual page. At the same time, we are not 
interested in some language strings although 
the lengths of them are appropriate. So we also 
store the variances of lengths about each 
sub-snippet. 
(3) Frequency-Based measure 
  According to the result pages, queries often 
occur in the title, snippet, or advertisements. 
They are highlighted to make them easier to 
identify. Hence we aim to acquire the 
frequency of the query in one whole snippet as 
a feature. 
  Based on the three measures above, a 
number of records (containing snippets and 
URLs) for training and testing can be converted 
them into a 6-dimensional feature space. In our 
experiments, nonlinear SVM with Gaussian 
Radial Basis Function (RBF) kernel is used. 
The performance of the SVM classifier 
indicates that it is a reliable way to verify 
whether the page is bilingual or not by the 
content of snippet. 
6 Experiments and Results 
6.1 The Data Set 
To acquire enough experimental data, we 
collect from Google, Baidu, Yahoo, Youdao, 
Bing and Tecent Soso, and the effectiveness of 
our algorithm is evaluated based on the data set 
from these six search engines. 
  Result records of search engines are 
collected by program and by human beings 
with submitting different queries respectively. 
They are used for checking the performance of 
record extraction. When evaluating the method 
of verification bilingual web pages, 2300 
records (60% are positive instances) are 
chosen for training the SVM classifier, and 
other 230 are selected randomly as test records 
from the whole record set. 
  The training data is annotated by human in 
two methods. The first method is motivated by 
the content of each source snippet. The 
annotators assign the type of web pages by 
scanning the text of every snippet. If the snippet 
contains many parallel term pairs, we annotate 
the page as bilingual or monolingual if not 
parallel. We also use another annotation 
method, which is to reach the URL by the 
Internet Explorer. By checking the content of 
the real web page, annotators decide the type of 
the candidate pages. And the biggest difference 
between the two public hand-classified dataset 
appears when some snippets of candidate 
pages have no clues in their content to predict 
classifications. 
6.2 Evaluation On Bilingual Page 
Acquisition
The entire system is evaluated by measuring 
the performance of the binary SVM classifier. 
And how the classifier performance changes 
with three features is shown in Table 3, where 
W, L and F separately stand for the 
word-overlap, length and frequency measures. 
  In order to improve the performance of 
word-overlap measure, we use not only the 
bilingual dictionary but also translation 
equivalents, which are extracted from parallel 
corpora. Because the bilingual dictionary 
doesn?t contain all necessary entries, the 
classifier with only word-overlap measure 
accepts many wrong pairs.  
Feature W W +L W +L+F
Precision 70.2% 81.02% 85.10% 
Table 3. SVM Classifier Performance changes 
with more features added to the classifier 
Table 3 shows that the length feature and the 
frequency feature have a significant effect on 
bilingual web page verification because of the 
natural relationship among queries, snippets 
and true web pages.  
#1 #2 
N
P(%) R(%) P(%) R(%)
1 85.1 92.3 75% 84.8 
2 80.7 95.1 72.8 85.7 
3 78.1 97.4 71.0 93.0 
aver 81.3 94.93 72.93 87.83
Table 4. Performance versus training data types 
300
  Three experiments of verification bilingual 
web pages based on two different training 
datasets are conducted whose results are 
shown in Table 4. #1 stands for the data set 
annotated by snippets, and #2 stands for the 
training data annotated by URLs. Precision and 
recall are used to evaluate our method. The 
average precision based on training dataset #2 
is 73%, which is lower than the precision of 
81.3% resulting from the dataset #1, because in 
many cases, some snippets are weakly related 
with real text in the real pages introduced by 
search engine summarization algorithm. From 
the table, we also see that the recalls in dataset 
#1 and #2 are both relatively high, which 
means our classifier can select high-quality 
bilingual pages with high accuracy. 
6.3 Evaluation On Web Record 
Extraction
Record extraction has significant effect on 
bilingual web page collection. A useful 
intermediate evaluation of the whole scheme is 
conducted by measuring the performance of 
record extraction.  
  We built a prototype system to test the 
algorithm of record extraction based on the 
clustering of similar records. On a laptop with a 
Pentium M 1.7G processor, the process of 
constructing records wrapper for a given search 
engine is done in 10 to 30 seconds. Once the 
wrapper is built, the record extraction from a 
new result page is done in a small fraction of a 
second.
  In order to test the robustness of the 
generated wrapper, we compare the records 
extracted by our method with the test records 
acquired manually. The precision and recall 
measures are used to evaluate the result. 98% 
of all the records are extracted by program, 
with a precision of 99%. The precision 
indicates that the generated wrappers in our 
experiment are quite robust to acquire records. 
The recall is lower than the precision, which 
indicates that it sometimes misses a few records. 
The reason for this is that in the extraction step, 
the records different from more common ones 
are eliminated.  
  We compare our performance with the work 
in Zhao (2006), which addresses the issue of 
differentiating dynamic sections and records 
based on the sample result pages. It generates 
section wrappers by identifying section 
boundary markers in nine steps. It is more 
complicated in computation than ours because 
it renders each result page and extracts its 
content lines by a traversal of the DOM tree, 
while we use tag structure of a page. The 
accordance is making full use of the sample 
pages for given search engines. The method 
also gets a high precision of 98.8% and a recall 
of 98.7%.  
7 Conclusion
The paper presents a novel method to acquire 
bilingual web pages automatically via search 
engines. In order to improve the efficiency and 
effectiveness, the snippets of search engines 
rather than the contents of the massive pages 
are analyzed to locate bilingual pages. 
Bilingual web page verification is modeled as 
a classification problem with word-overlap, 
length and frequency measures. Based on the 
similarity of HTML structures, AP clustering 
is used to extract web records from result 
pages of search engines. Experiments show 
that our algorithm has good performance in 
precision and recall. 
  As a valuable resource for up-to-date 
bilingual terms and sentences, bilingual web 
pages are counterpart to parallel monolingual 
web pages. Our method brings an efficient and 
effective solution to bilingual language 
engineering. 
References 
Adelberg B., NoDoSE. 1998. A tool for semi- 
  Automatically extracting structured and sem- 
  istructured data from text documents. In:
  Proc.ACM SIGMOD Conference on  man- 
  agement of Data, Seattle, WA (1998).
Baumgartner R., S. Flesca and G. Gottlob.2001. 
  Visual Web Information Extraction with 
  Lixto. Proceedings of the 27th International  
  Conference on Very Large Data Bases,
   pp.119-128, September 11-14, 2001  
Chang C., S. Lui. 2001. Information Extraction  
  based on Pattern Discovery. In Proceedings
  of the 10th international conference on 
  World Wide Web. pp.681-688, May 01-05, 
  2001, Hong Kong. 
Chen Jiang and Jian-Yun Nie. 2000. Web 
301
  Parallel text mining for Chinese-English 
  cross-language information retrieval. Proce-
  edings of RIAO2000 Content-Based Multi- 
  media Information Access, CID, Paris
Cortes, C. and V. Vapnik. 1995. Support-vector 
  network. Machine Learning 20, pp.273-297. 
Deng Dan. 2004. Research on Chinese-English 
  word alignment. Institute of Computing 
  Technology Chinese Academy of Sciences,
  Master Thesis. (in Chinese). 
DuVerle David, Helmut Prendinger. 2009. A  
  Novel Discourse Parser Based on Support 
  Vector Machine Classification. The 47th  
  Annual Meeting of the Association for 
  Computational Linguistics. pp. 665-673 
Frey B. J. and D. Dueck. 2007. Clustering by 
  passing messages between data points. 
Science, 315(5814):972-976. 
Laender A, B. Ribeiro-Neto, A. da Silva, J.  
  Teixeira. 2002. A Brief Survey of Web Data 
  Extraction Tools. ACM SIGMOD Record.
Volume 31, Number 2.
Liu B. and Y. Zhai. 2005. System for extracting 
  Web data from flat and nested data records.  
  In Proceedings of the Conference on Web  
  Information Systems Engineering,
  pp.487-495. 
Liu B., R. Grossman and Y. Zhai. 2003. Mining 
  Data Records in Web Pages. In Proceedings
  of the ninth ACM SIGKDD international  
  conference on Knowledge Discovery and  
  Data mining, Washington, D.C, pp.601-606. 
Liu Feifan, Jun Zhao, Bo Xu. 2003. Building 
  Large-Scale Domain Independent Chinese-  
  English Bilingual Corpus and the Researches 
  on Sentence Alignment. Joint Symposium on 
  Computational Linguistics.
Liu L., C. Pu and W. Han. 2000. An XML- 
  Enabled Wrapper Construction System for 
  Web Information Sources. Proceedings of  
  the 16th International Conference on Data  
  Engineering, pp.611. 
Long Jiang, Shiquan Yang, Ming Zhou, Xiao- 
  hua Liu and Qingsheng Zhou. 2009. Mining 
  Bilingual Data from the Web with Adaptive- 
  ly Learnt Patterns. The 47th Annual Meeting 
  of the Association for Computational Lingui- 
  stics. pp. 870-878 (2009) 
Miao Gengxin, Junichi Tatemura, Wang-Pin 
  Hsiung, Arsany Sawires, Louise E. Moser.  
  2009. Extracting data records from the web  
  using tag path clustering. In Proceedings of  
  the 18th International Conference on World 
  Wide Web, Spain, Madrid. 
Nie Jian-Yun, Michel Simard, Pierre Isabelle, 
Richard Durand 1999. Cross-Language 
Information Retrieval based on Parallel 
Texts and Automatic Mining of Parallel 
Texts in the Web. SIGIR-1999; 74-81. 
Resnik Philip and Noah A. Smith. 2003. The  
  web as a Parallel Corpus. Computational  
  Linguistics.
Shi Lei, Cheng Niu, Ming Zhou, and Jianfeng 
  Gao. 2006. A DOM Tree Alignment Model  
  for Mining Parallel Data from the Web. In  
Joint Proceedings of the Association for  
  Computational Linguistics and the Internati- 
  onal Conference on Computational Linguist- 
  ics, Sydney, Australia. 
White, R., Jose, J. & Ruthven, R. 2001.Query- 
  biased web page summarisation: a task- 
  oriented evaluation. In Proceedings of the 
  24th ACM SIGIR Conference on Research  
  and Development of Information Retrieval.
  New Orleans, Louisiana, United States, pp.  
  412-413. 
Zhai Y., B. Liu. 2005. Extracting Web Data  
  Using Instance-Based Learning. Web Infor- 
  mation Systems Engineering.
Zhai Y., B. Liu. 2005. Web Data Extraction 
  Based on Partial Tree Alignment. In 
Proceedings of the 14th international  
  conference on World Wide Web. May 10-14, 
  2005, Chiba, Japan.
Zhang Ying, Ke Wu, Jianfeng Gao, Phil Vines. 
  2006. Automatic Acquisition of Chinese- 
  English Parallel Corpus from the web. In 
Proceedings of 28th European Conference  
  on Information Retrieval.
Zhao H., W. Meng, Z. Wu, V. Raghavan, C.  
  Yu. 2006. Automatic Extraction of Dynamic 
  Record Sections From Search Engine Result 
  Pages. In Proceedings of the 32nd Internatio- 
nal conference on Very large databases.
302
Coling 2010: Poster Volume, pages 436?444,
Beijing, August 2010
 
ABSTRACT 
Re-ranking for Information Retrieval 
aims to elevate relevant feedbacks and 
depress negative ones in initial retrieval 
result list. Compared to relevance feed-
back-based re-ranking method widely 
adopted in the literature, this paper pro-
poses a new method to well use three 
features in known negative feedbacks to 
identify and depress unknown negative 
feedbacks. The features include: 1) the 
minor (lower-weighted) terms in negative 
feedbacks; 2) hierarchical distance (HD) 
among feedbacks in a hierarchical clus-
tering tree; 3) obstinateness strength of 
negative feedbacks. We evaluate the 
method on the TDT4 corpus, which is 
made up of news topics and their relevant 
stories. And experimental results show 
that our new scheme substantially out-
performs its counterparts. 
1. INTRODUCTION 
When we start out an information retrieval jour-
ney on a search engine, the first step is to enter a 
query in the search box. The query seems to be 
the most direct reflection of our information 
needs. However, it is short and often out of stan-
dardized syntax and terminology, resulting in a 
large number of negative feedbacks. Some re-
searches focus on exploring long-term query logs 
to acquire query intent. This may be helpful for 
obtaining information relevant to specific inter-
ests but not to daily real-time query intents. Es-
pecially it is extremely difficult to determine 
whether the interests and which of them should 
be involved into certain queries. Therefore, given 
a query, it is important to ?locally? ascertain its 
intent by using the real-time feedbacks. 
Intuitively it is feasible to expand the query 
using the most relevant feedbacks (Chum et al, 
2007). Unfortunately search engines just offer 
?farraginous? feedbacks (viz. pseudo-feedback) 
which may involve a great number of negative 
feedbacks. And these negative feedbacks never 
honestly lag behind relevant ones in the retrieval 
results, sometimes far ahead because of their 
great literal similarity to query. These noisy 
feedbacks often mislead the process of learning 
query intent.  
For so long, there had no effective approaches 
to confirm the relevance of feedbacks until the 
usage of the web click-through data (Joachims et 
al., 2003). Although the data are sometimes in-
credible due to different backgrounds and habits 
of searchers, they are still the most effective way 
to specify relevant feedbacks. This arouses re-
cent researches about learning to rank based on 
supervised or semi-supervised machine learning 
methods, where the click-through data, as the 
direct reflection of query intent, offer reliable 
training data to learning the ranking functions. 
Although the learning methods achieve sub-
stantial improvements in ranking, it can be found 
that lots of ?obstinate? negative feedbacks still 
permeate retrieval results. Thus an interesting 
question is why the relevant feedbacks are able 
to describe what we really need, but weakly repel 
what we do not need. This may attribute to the 
inherent characteristics of pseudo-feedback, i.e. 
their high literal similarity to queries. Thus no 
matter whether query expansion or learning to 
rank, they may fall in the predicament that ?fa-
voring? relevant feedbacks may result in ?favor-
ing? negative ones, and that ?hurting? negative 
feedbacks may result in ?hurting? relevant ones. 
However, there are indeed some subtle differ-
ences between relevant and negative feedbacks, 
e.g. the minor terms (viz. low-weighted terms in 
texts). Although these terms are often ignored in 
Negative Feedback: The Forsaken Nature Available for Re-ranking
Yu Hong, Qing-qing Cai, Song Hua, Jian-min Yao, Qiao-ming Zhu 
School of Computer Science and Technology, Soochow University 
jyao@suda.edu.cn 
436
relevance measurement because their little effect 
on mining relevant feedbacks that have the same 
topic or kernel, they are useful in distinguishing 
relevant feedbacks from negative ones. As a re-
sult, these minor terms provides an opportunity 
to differentiate the true query intent from its 
counterpart intents (called ?opposite intents? 
thereafter in this paper). And the ?opposite in-
tents? are adopted to depress negative feedbacks 
without ?hurting? the ranks of relevant feedbacks. 
In addition, hierarchical clustering tree is helpful 
to establish the natural similarity correlation 
among information. So this paper adopts the hi-
erarchical distance among feedbacks in the tree 
to enhance the ?opposite intents? based division 
of relevant and negative feedbacks. Finally, an 
obstinateness factor is also computed to deal 
with some obstinate negative feedbacks in the 
top list of retrieval result list. In fact, Teevan 
(Teevan et al, 2008) observed that most search-
ers tend to browse only a few feedbacks in the 
first one or two result pages. So our method fo-
cuses on improving the precision of highly 
ranked retrieval results.  
The rest of the paper is organized as follows. 
Section 2 reviews the related work. Section 3 
describes our new irrelevance feedback-based 
re-ranking scheme and the HD measure. Section 
4 introduces the experimental settings while Sec-
tion 5 reports experimental results. Finally, Sec-
tion 6 draws the conclusion and indicates future 
work. 
2. RELATED WORK 
Our work is motivated by information search 
behaviors, such as eye-tracking and click through 
(Joachims, 2003). Thereinto, the click-through 
behavior is most widely used for acquiring query 
intent. Up to  present, several interesting fea-
tures, such as click frequency and hit time on 
click graph (Craswell et al, 2007), have been 
extracted from click-through data to improve 
search results. However, although effective on 
query learning, they fail to avoid the thorny 
problem that even when the typed query and the 
click-through data are the same, their intents may 
not be the same for different searchers.  
A considerable number of studies have ex-
plored pseudo-feedback to learn query intent, 
thus refining page ranking. However, most of 
them focus on the relevant feedbacks. It is until 
recently that negative ones begin to receive some 
attention. Zhang (Zhang et al, 2009) utilize the 
irrelevance distribution to estimate the true rele-
vance model. Their work gives the evidence that 
negative feedbacks are useful in the ranking 
process. However, their work focuses on gener-
ating a better description of query intent to attract 
relevant information, but ignoring that negative 
feedbacks have the independent effect on repel-
ling their own kind. That is, if we have a king, 
we will not refuse a queen. In contrast, Wang 
(Wang et al, 2008) benefit from the independent 
effect from the negative feedbacks. Their method 
represents the opposite of query intent by using 
negative feedbacks and adopts that to discount 
the relevance of each pseudo-feedback to a query. 
However, their work just gives a hybrid repre-
sentation of opposite intent which may overlap 
much with the relevance model. Although an-
other work (Wang et al, 2007) of them filters 
query terms from the opposite intent, such filter-
ing makes little effect because of the sparsity of 
the query terms in pseudo-feedback. 
Other related work includes query expansion, 
term extraction and text clustering. In fact, query 
expansion techniques are often the chief benefi-
ciary of click-through data (Chum et al, 2007). 
However, the query expansion techniques via 
clicked feedbacks fail to effectively repel nega-
tive ones. This impels us to focus on un-clicked 
feedbacks. Cao (Cao et al, 2008) report the ef-
fectiveness of selecting good expansion terms for 
pseudo-feedback. Their work gives us a hint 
about the shortcomings of the one-sided usage of 
high-weighted terms. Lee (Lee et al, 2008) adopt 
a cluster-based re-sampling method to emphasize 
the core topic of a query. Their repeatedly feed-
ing process reveals the hierarchical relevance of 
pseudo-feedback. 
3. RE-RANKING SCHEME 
3.1 Re-ranking Scheme 
The re-ranking scheme, as shown in Figure 1, 
consists of three components: acquiring negative 
feedbacks, measuring irrelevance feedbacks and 
re-ranking pseudo-feedback. 
Given a query and its search engine results, we 
start off the re-ranking process after a trigger 
point. The point may occur at the time when 
searchers click on ?next page? or any hyperlink. 
437
All feedbacks before the point are assumed to 
have been seen by searchers. Thus the un-clicked 
feedbacks before the point will be treated as the 
known negative feedbacks because they attract 
no attention of searchers. This may be questioned 
because searchers often skip some hyperlinks 
that have the same contents as before, even if the 
links are relevant to their interests. However, 
such skip normally reflects the true searching 
intent because novel relevant feedbacks always 
have more attractions after all. 
 
Figure 1. Re-ranking scheme 
Another crucial step after the trigger point is to 
generate the opposite intent by using the known 
negative feedbacks. But now we temporarily 
leave the issue to Section 3.2 and assume that we 
have obtained a good representation of the oppo-
site intent, and meanwhile that of query intent 
has been composed of the highly weighted terms 
in the known relevant feedbacks and query terms. 
Thus, given an unseen pseudo-feedback, we can 
calculate its overall ranking score predisposed to 
the opposite intent as follows: 
          scoreIscoreOscoreR ___ ??= ?        (1) 
where the O_score is the relevance score to the 
opposite intent, I_score is that to the query intent 
and ?  is a weighting factor. On the basis, we 
re-rank the unseen feedbacks in ascending order. 
That is, the feedback with the largest score ap-
pears at the bottom of the ranked list. 
It is worthwhile to emphasize that although the 
overall ranking score, i.e. R_score, looks similar 
to Wang (Wang et al, 2008) who adopts the in-
versely discounted value (i.e. the relevance score 
is calculated as -scoreI _ scoreO _?? ) to re-rank 
feedbacks in descending order, they are actually 
quite different because our overall ranking score 
as shown in Equation (1) is designed to depress 
negative feedbacks, thereby achieving the similar 
effect to filtering. 
3.2 Representing Opposite Intent 
It is necessary for the representation of opposite 
intent to obey two basic rules: 1) the opposite 
intent should be much different from the query 
intent; and 2) it should reflect the independent 
effect of negative feedbacks. 
Given a query, it seems easy to represent its 
opposite intent by using a vector of 
high-weighted terms of negative feedbacks. 
However, the vector is actually a ?close relative? 
of query intent because the terms often have 
much overlap with that of relevant feedbacks. 
And the overlapping terms are exactly the source 
of the highly ranked negative feedbacks. Thus 
we should throw off the overlapping terms and 
focus on the rest instead.  
In this paper, we propose two simple facilities 
in representing opposite intent. One is a vector of 
the weighted terms (except query terms) occur-
ring in the known negative feedbacks, named as 
)( qO ? , while another further filters out the 
high-weighted terms occurring in the known 
relevant feedbacks, named as . Although )( rqO ??
)( qO ?  filters out query terms, the terms are so 
sparse that they contribute little to opposite intent 
learning. Thus, we will not explore  fur-
ther in this paper (Our preliminary experiments 
confirm our reasoning). In contrast,  not 
only differs from the representation of query in-
tent due to its exclusion of query terms but also 
emphasize the low-weighted terms occurring in 
negative feedbacks due to exclusion of 
high-weighted terms occurring in the known 
relevant feedbacks. 
)( qO ?
)( rqO ??
3.3 Employing Opposite Intent 
Another key issue in our re-ranking scheme is 
how to measure the relevance of all the feed-
backs to the opposite intent, i.e. O_score, thereby 
the ranking score R_score. For simplicity, we 
only consider Boolean measures in employing 
opposite intent to calculate the ranking score 
R_score. 
Assume that given a query, there are  
known relevant feedbacks and 
N
N  known nega-
tive ones. First, we adopt query expansion to ac-
quire the representation of query intent. This is 
done by pouring all terms of the  relevant 
feedbacks and query terms into a bag of words, 
where all the occurring weights of each term are 
N
438
accumulated, and extracting n top-weighted 
terms to represent the query intent as )( rqI ++ . 
Then, we use the N  negative feedbacks to rep-
resent the n-dimensional opposite intents 
. For any unseen pseudo-feedback u, we 
also represent it using an n-dimensional vector 
 which contains its n top-weighted terms. In 
all the representation processes, the TFIDF 
weighting is adopted. 
)( rqO ??
)(uV
Thus, for an unseen pseudo-feedback u, the 
relevance scores to the query intent and the op-
posite intent can be measured as: 
                   (2) 
}  )(  ),(  {)(_
}  )(  ),(  {)(_
rqOuVBuscoreO
rqIuVBuscoreI
??=
++=
where  indicates Boolean calculation: },{ ??B
                       (3) 
??
?
?
?=
?=?
Yxif
Yxif
Yxb
XxYxbYXB
i
i
i
ii
        ,0
         ,1
},{
  },,{},{
In particular, we simply set the factor ? , as 
mentioned in Equation (1), to 1 so as to balance 
the effect of query intent and its opposite intent 
on the overall ranking score. The intuition is that 
if an unseen pseudo-feedback has more overlap-
ping terms with )( rqO ??  than , it will 
has higher probability of being depressed as an 
negative feedback. 
)( rqI ++
Two alternatives to the above Boolean meas-
ure are to employ the widely-adopted VSM co-
sine measure and Kullback-Liebler (KL) diver-
gence (Thollard et al, 2000). However, such 
term-weighting alternatives will seriously elimi-
nate the effect of low-weighted terms, which is 
core of our negative feedback-based re-ranking 
scheme.  
3.4 Hierarchical Distance (HD) Measure  
The proposed method in Section 3.3 ignores 
two key issues. First, given a query, although 
search engine has thrown away most opposite 
intents, it is unavoidable that the 
pseudo-feedback still involves more than one 
opposite intent. However, the representation 
 has the difficulty in highlighting all the 
opposite intents because the feature fusion of the 
representation smoothes the independent charac-
teristics of each opposite intent. Second, given 
several opposite intents, they have different lev-
els of effects on the negative score . 
And the effects cannot be measured by the uni-
lateral score.  
)( rqO ??
)(_ uscoreO
 
Figure 2. Weighted distance calculation 
To solve the issues, we propose a hierarchical 
distance based negative measure, abbr. HD, 
which measures the distances among feedbacks 
in a hierarchical clustering tree, and involves 
them into hierarchical division of relevance score. 
Given two random leaves u and v in the tree, 
their HD score is calculated as: 
             
),(
),(
),(_
vuW
vurel
vuscoreHD =           (4) 
where ),( ??rel  indicates textual similarity, ),( ??W  
indicates the weighted distance in the tree, which 
is calculated as: 
                ?
?
=
mi
i vuwvuW ),(),(              (5) 
where m is the total number of the edges between 
two leaves,  indicates the weight of the 
i-th edge. In this paper, we adopt CLUTO to 
generate the hierarchical binary tree, and simply 
let each  equal 1. Thus the 
),( ??iw
),( ??iw ),( ??W  be-
comes to be the number of edges m, for example, 
the  equals 5 in Figure 2. ),( kjW
On the basis, given an unseen feedback u, we 
can acquire its modified re-ranking score 
scoreR _ ?  by following steps. First, we regard 
each known negative feedback as an opposite 
intent, following the two generative rules (men-
tioned in section 3.2) to generate its 
n-dimensional representation . Addition-
ally we represent both the known relevant feed-
backs and the unseen feedback u as 
n-dimensional term vectors. Second, we cluster 
these feedbacks to generate a hierarchical binary 
tree and calculate the HD score for each pair of 
)( rqO ??
),( ?u , where ?  denotes a leaf in the tree except u. 
Thus the modified ranking score is calculated as: 
? ?
? ?
?=?
Ni Nj
ji vuscoreHDIvuscoreHDIscoreR ),(_),(__ (6) 
where iv  indicates the i-th known negative 
feedback in the leaves, N  is the total number of 
439
v , j  indicates the j-th known relevant feed-
back,  is the total number of 
v
N v . Besides, we 
still adopt Boolean value to measure the textual 
similarity  in both clustering process and 
ranking score calculation, thus the HD score in 
the formula (6) can be calculated as follows: 
),( ??rel
     
),(
)(_
),(_                   
),(
}  )(  ),(  {
),(_                
vuW
uscoreO
vuscoreHD
vuW
vVuVB
vuscoreHD
=
=
       (7) 
3.5 Obstinateness Factor 
Additionally we involve an interesting feature, 
i.e. the obstinate degree, into our re-ranking 
scheme. The degree is represented by the rank of 
negative feedbacks in the original retrieval re-
sults. That is, the more ?topping the list? an 
negative feedback is, the more obstinate it is.  
Therefore we propose a hypothesis that if a 
feedback is close to the obstinate feedback, it 
should be obstinate too. Thus given an unseen 
feedback u, its relevance to an opposite intent in 
HD can be modified as: 
          )(_)1()(_ uscoreO
rnk
uscoreO ?+=? ?        (8) 
where  indicates the rank of the opposite 
intent in original retrieval results (Note: in HD, 
every known negative feedback is an opposite 
intent), 
rnk
?  is a smoothing factor. Because as-
cending order is used in our re-ranking process, 
by the weighting coefficient, i.e. )/1( rnk?+ , the 
feedback close to the obstinate opposite intents 
will be further depressed. But the coefficient is 
not commonly used. In HD, we firstly ascertain 
the feedback closest to u, and if the feedback is 
known to be negative, set to maxv , we will use 
the Equation (8) to punish the pair of (u, maxv ) 
alone, otherwise without any punishment. 
4. EXPERIMENTAL SETTING 
4.1 Data Set 
We evaluate our methods with two TDT collec-
tions: TDT 2002 and TDT 2003. There are 3,085 
stories in the TDT 2002 collection are manually 
labeled as relevant to 40 news topics, 30,736 
ones irrelevant to any of the topics. And 3,083 
news stories in the TDT 2003 collection are la-
beled as relevant to another 40 news topics, 
15833 ones irrelevant to them. In our evaluation, 
we adopt TDT 2002 as training set, and TDT 
2003 as test set. Besides, only English stories are 
used, both Mandarin and Arabic ones are re-
placed by their machine-translated versions (i.e. 
mttkn2 released by LDC). 
Corpus good fair poor 
TDT 2002 26 7 7 
TDT 2003 22 10 8 
Table 1. Number of queries referring to different 
types of feedbacks (Search engine: Lucene 2.3.2) 
In our experiments, we realize a simple search 
engine based on Lucene 2.3.2 which applies 
document length to relevance measure on the 
basis of traditional literal term matching. To 
emulate the real retrieval process, we extract the 
title from the interpretation of news topic and 
regard it as a query, and then we run the search 
engine on the TDT sets and acquire the first 1000 
pseudo-feedback for each query. All feedbacks 
will be used as the input of our re-ranking proc-
ess, where the hand-crafted relevant stories de-
fault to the clicked feedbacks. By the search en-
gine, we mainly obtain three types of 
pseudo-feedback: ?good?, ?fair? and ?poor?, 
where ?good? denotes that more than 5 clicked 
(viz. relevant) feedbacks are in the top 10, ?fair? 
denotes more than 2 but less than 5, ?poor? de-
notes less than 2. Table 1 shows the number of 
queries referring to different types of feedbacks. 
4.2 Evaluation Measure 
We use three evaluation measures in experiments, 
P@n, NDCG@n and MAP. Thereinto, P@n de-
notes the precision of top n feedbacks. On the 
basis, NDCG takes into account the influence of 
position to precision. NDCG at position n is cal-
culated as: 
      
n
n
i
ur
n Z
iNDCG
Z
nNDCG
i?= +?=?= 1
)(
)1log(
12
@
1
@    (9) 
where i is the position in the result list, Zn is a 
normalizing factor and chosen so that for the 
perfect list DCG at each position equals one, and 
r(ui) equals 1 when ui is relevant feedback, else 0. 
While MAP additionally takes into account recall, 
calculated as:  
        ? ?= = ?= mi kj ijii jpurRmMAP 1 1 ))@()((11    (10) 
where m is the total number of queries, so MAP 
gives the average measure of precision and recall 
440
for multiple queries, Ri is the total number of 
feedbacks relevant to query i, and k is the num-
ber of pseudo-feedback to the query. Here k is 
indicated to be 1000, thus Map can give the av-
erage measure for all positions of result list. 
4.3 Systems 
We conduct experiments using four main sys-
tems, in which the search engine based on Lu-
cene 2.3.2, regarded as the basic retrieval system, 
provides the pseudo-feedback for the following 
three re-ranking systems. 
Exp-sys: Query is expanded by the first N known 
relevant feedbacks and represented by an 
n-dimensional vector which consists of n distinct 
terms. The standard TFIDF-weighted cosine 
metric is used to measure the relevance of the 
unseen pseudo-feedback to query. And the rele-
vance-based descending order is in use. 
Wng-sys: A system realizes the work of Wang 
(Wang et al, 2008), where the known relevant 
feedbacks are used to represent query intent, and 
the negative feedbacks are used to generate op-
posite intent. Thus, the relevance score of a feed-
back is calculated as I_scorewng- O_score?w? wng, 
and the relevance-based descending order is used 
in re-ranking. 
Our-sys: A system is approximately similar to 
Wng-sys except that the relevance is measured by 
O_scoreour- ?? I_scoreour and the pseudo-feedback 
is re-ranked in ascending order.  
Additionally both Wng-sys and Our-sys have 
three versions. We show them in Table 2, where 
?I? corresponds to the generation rule of query 
intent, ?O? to that of opposite intent, Rel. means 
relevance measure, u is an unseen feedback, v is 
a known relevant feedback, v  is a known nega-
tive feedback. 
5. RESULTS 
5.1 Main Training Result 
We evaluate the systems mainly in two circum-
stances: when both  and N N  equal 1 and 
when they equal 5. In the first case, we assume 
that retrieval capability is measured under given 
few known feedbacks; in the second, we emulate 
the first page turning after several feedbacks 
have been clicked by searchers. Besides, the ap-
proximately optimal value of n for the Exp-sys, 
which is trained to be 50, is adopted as the global 
value for all other systems. The training results 
are shown in Figure 3, where the Exp-sys never 
gains much performance improvement when n is 
greater than 50. In fairness to effects of ?I? and 
?O? on relevance measure, we also make n  
equal 50. In addition, all the discount factors 
(viz.? , ? w2 and ? w3) initially equal 1, and the 
smoothing factor ?  is trained to be 0.5. 
Table 2. All versions of both Wngs and Ours 
 
Figure 3. Parameter training of Exp-sys 
For each query we re-rank all the 
pseudo-feedback, including that defined as 
known, so P@20 and NDCG@20 are in use to 
avoid over-fitting (such as P@10 and 
NDCG@10 given both  and N N  equal 5 ). 
We show the main training results in Table 3, 
where our methods achieve much better per-
formances than the re-ranking methods based on 
relevant feedback learning when N= N =5. 
Thereinto, our basic system, i.e. Our-sys1, at 
least achieves approximate 5% improvement on 
P@20, 3% on NDCG@20 and 1% on MAP than 
the optimal wng-sys (viz. wng-sys1). And obvi-
?I? n-dimensional vector for each v, Number of v in use is N
?O? None 
Wng-sys1
Rel. NvuscoreR
N
i
w /)),cos((_
1
1 ?==  
?I?
Number of v in use is N, all v combine into a n-dimensional 
bag of words bw2
?O?
Number of v  in use is N , all v combine into a 
n-dimensional words bag 2wb  
Wng-sys2
Rel. ),cos(),cos(_ 2222 wwww bubuscoreR ??= ?  
?I?
?O?
Similar generation rules to Wng-sys2 except that query 
terms are removed from bag of words  and 3wb 3wb  Wng-sys3
Rel. ),cos(),cos(_ 3333 wwww bubuscoreR ??= ?  
?I? )( rqI ++  in section 3.3 
?O? )( rqO ??  in section 3.2 Our-sys1
Rel. scoreIscoreOscoreR ___ ??= ?  
?I?
?O?
The same generation rules to Our-sys1 
Our-sys2
Rel.
HD algorithm: ? ?
? ?
?=?
Ni Nj
ji vuscoreHDIvuscoreHDIscoreR ),(_),(__ 
?I?
?O?
The same generation rules to Our-sys1 
Our-sys3
Rel.
HD algorithm + obstinateness factor: 
)(_)1()(_ uscoreO
rnk
uscoreO ?+=? ?  
441
ously the most substantial improvements are 
contributed by the HD measure which even in-
creases the P@20 of Our-sys1 by 8.5%, 
NDCG@20 by 13% and MAP by 9%. But it is 
slightly disappointing that the obstinateness fac-
tor only has little effectiveness on performance 
improvement, although Our-sys3 nearly wins 
the best retrieval results. This may stem from 
?soft? punishment on obstinateness, that is, for 
an unseen feedback, only the obstinate com-
panion closest to the feedback is punished in 
relevance measure. 
Table 3. Main training results 
It is undeniable that all the re-ranking systems 
work worse than the basic search engine when 
the known feedbacks are rare, such as =N N =1. 
This motivates an additional test on the higher 
values of both  and N N ( =N N =9), as shown 
in Table 4. Thus it can be found that most of the 
re-ranking systems achieve much better per-
formance than the basic search engine. An im-
portant reason for this is that more key terms can 
be involved into representations of both query 
intent and its opposite intent. So it seems that 
more manual intervention is always reliable. 
However in practice, seldom searchers are will-
ing to use an unresponsive search engine that can 
only offer relatively satisfactory feedbacks after 
lots of click-through and page turning. And in 
fact at least two pages (if one page includes 10 
pseudo-feedback) need to be turned in the train-
ing corpus when both  and N N  equal 9. So 
we just regard the improvements benefiting from 
high click-through rate as an ideal status, and 
still adopt the practical numerical value of  
and 
N
N , i.e. =N N =5, to run following test. 
5.2 Constraint from Query 
A surprising result is that Exp-sys always 
achieves the worst MAP value, even worse than 
the basic search engine even if high value of N is 
in use, such as the performance when N equal 9 
in Table 4. It seems to be difficult to question the 
reasonability of the system because it always 
selects the most key terms to represent query in-
tent by query expansion. But an obvious differ-
ence between Exp-sys and other re-ranking sys-
tems could explain the result. That is the query 
terms consistently involved in query representa-
tion by Exp-sys. 
Table 4. Effects of  and N N  on re-ranking 
performance (when =N N =9, n= n =50) 
In fact, Wng-sys1 never overly favor the query 
terms because they are not always the main body 
of an independent feedback, and our systems 
even remove the query terms from the opposite 
intent directly. Conversely Exp-sys continuously 
enhances the weights of query terms which result 
in over-fitting and bias. The visible evidence for 
this is shown in Figure 4, where Exp-sys 
achieves better Precision and NDCG than the 
basic search engine at the top of result list but 
worse at the subsequent parts. The results illus-
trate that too much emphasis placed on query 
terms in query expansion is only of benefit to 
elevating the originally high-ranked relevant 
feedback but powerless to pull the straggler out 
of the bottom of result list.  
 
Figure 4. MAP comparison (basic vs Exp) 
5.3 Positive Discount Loss 
Obviously Wang (Wang et al, 2008) has noticed 
the negative effects of query terms on re-ranking. 
Therefore his work (reproduced by Wng-sys1, 2, 
3 in this paper) avoids arbitrarily enhancing the 
terms in query representation, even removes 
them as Wng-sys3. This indeed contributes to the 
- Our-sys1 Our-sys2 Exp-sys Wng-sys1 Basic 
P@20 0.6603 0.8141 0.63125 0.7051 0.6588
NDCG@20 0.7614 0.8587 0.8080 0.7797 0.6944
MAP 0.6583 0.7928 0.5955 0.7010 0.6440
systems N = N P@20 NDCG@20 MAP Factor 
Basic - 0.6588 0.6944 0.6440 - 
1 0.4388 0.4887 0.3683 - Exp-sys 
5 0.5613 0.6365 0.5259 - 
1 0.5653 0.6184 0.5253 - Wng-sys1
5 0.6564 0.7361 0.6506 - 
1 0.5436 0.6473 0.4970 2w? =1Wng-sys2
5 0.5910 0.7214 0.5642 2w? =1
1 0.5436 0.6162 0.4970 3w? =1Wng-sys3
5 0.5910 0.6720 0.5642 3w? =1
1 0.5628 0.6358 0.4812 ? =1 Our-sys1
5 0.7031 0.7640 0.6603 ? =1 
1 0.6474 0.6761 0.5967 ? =1 Our-sys2
5 0.7885 0.8381 0.7499 ? =1 
1 0.6026 0.6749 0.5272 ? =0.5Our-sys3
5 0.7897 0.8388 0.7464 ? =0.5
442
improvement of the re-ranking system, such as 
the better performances of Wng-sys1, 2, 3 shown 
in Table 3, although Wng-sys3 has no further 
improvement than Wng-sys2 because of the spar-
sity of query terms. On the basis, the work re-
gards the terms in negative feedbacks as noises 
and reduces their effects on relevance measure as 
much as possible. This should be a reasonable 
scheme, but interestingly it does not work well in 
our experiments. For example, although 
Wng-sys2 and Wng-sys3 eliminate the relevance 
score calculated by using the terms in negative 
feedbacks, they perform worse than Wng-sys1 
which never make any discount. 
systems ?? =0.5 ?? =1 ?? =2 
Our-sys1 0.4751 0.6603 0.6901 
Wng-sys2 0.6030 0.5642 0.4739 
Wng-sys3 0.6084 0.5642 0.4739 
Table 5. Effects on MAP  
 Additionally when we increase the discount 
factor 2w?  and 3w? , as shown in Table 5, the 
performances (MAP) of Wng-sys2 and Wng-sys3 
further decrease. This illustrates that the 
high-weighted terms of high-ranked negative 
feedbacks are actually not noises. Otherwise why 
do the feedbacks have high textual similarity to 
query and even to their neighbor relevant feed-
backs? Thus it actually hurts real relevance to 
discount the effect of the terms. 
Conversely Our-sys1 can achieve further im-
provement when the discount factor ?  in-
creases, as shown in Table 5. It is because the 
discount contributes to highlighting minor terms 
of negative feedbacks, and these terms always 
have little overlap with the kernel of relevant 
feedbacks. Additionally the minor terms are used 
to generate the main body of opposite intent in 
our systems, thus the discount can effectively 
separate opposite intent from positive query rep-
resentation. Thereby we can use relatively pure 
representation of opposite intent to detect and 
repel subsequent negative feedbacks. 
5.4 Availability of Minor Terms 
Intuitively we can involve more terms into query 
representation to alleviate the positive discount 
loss. But it does not work in practice. For exam-
ple, Wng-sys2 shown in Figure 5 has no obvious 
improvement no matter how many terms are in-
cluded in query representation. Conversely 
Our-sys1 can achieve much more improvement 
when it involves more terms into the opposite 
intent. For example, when the number of terms 
increases to 150, Our-sys1 has approximately 5% 
better MAP than Wng-sys2, shown in Figure 5. 
 
Figure 5. Effects on MAP in modifying the di-
mensionality n (when N= N =5, ? =1) 
 This result illustrates that minor terms are 
available for repelling negative feedbacks, but 
too weak to recall relevant feedbacks. In fact, the 
minor terms are just the low-weighted terms in 
text. Current text representation techniques often 
ignore them because of their marginality. How-
ever minor terms can reflect fine distinctions 
among feedbacks, even if they have the same 
topic. And the distinctions are of great impor-
tance when we determine why searchers say 
?Yes? to some feedbacks but ?No? to others. 
Table 6. Main test results 
5.5 Test Result 
We run all systems on test corpus, i.e. TDT2003, 
but only report four main systems: Wng-sys1, 
Our-sys1, Our-sys2 and Our-sys3. Other systems 
are omitted because of their poor performances. 
The test results are shown in Table 6 which in-
cludes not only global performances for all test 
queries but also local ones on three distinct types 
of queries, i.e. ?good?, ?fair? and ?poor?. There-
into, Our-sys2 achieves the best performance 
around all types of queries. So it is believable 
systems metric good fair poor global Factor
P@20 0.7682 0.5450 0.2643 0.6205
NDCG@20 0.8260 0.6437 0.4073 0.7041Wng-sys1
MAP 0.6634 0.4541 0.9549 0.6620
- 
P@20 0.8273 0.5700 0.2643 0.6603
NDCG@20 0.8679 0.6620 0.4017 0.7314Our-sys1
MAP 0.6740 0.4573 0.9184 0.6623
? =2,
? =0.5
P@20 0.8523 0.7600 0.2714 0.7244
NDCG@20 0.8937 0.8199 0.4180 0.7894Our-sys2
MAP 0.7148 0.6313 0.9897 0.7427
? =2,
? =0.5
P@20 0.8523 0.7600 0.2714 0.7244
NDCG@20 0.8937 0.8200 0.4180 0.7894Our-sys3
MAP 0.7145 0.6292 0.9897 0.7420
? =2,
? =0.5
443
that hierarchical distance of clustering tree al-
ways plays an active role in distinguishing nega-
tive feedbacks from relevant ones. But it is sur-
prising that Our-sys3 achieves little worse per-
formance than Our-sys2. This illustrates poor 
robustness of obstinateness factor. 
Interestingly, the four systems all achieve very 
high MAP scores but low P@20 and NDCG@20 
for ?poor? queries. This is because the queries 
have inherently sparse relevant feedbacks: less 
than 6? averagely. Thus the highest p@20 is 
only approximate 0.3, i.e. 6/20. And the low 
NDCG@20 is in the same way. Besides, all 
MAP scores for ?fair? queries are the worst. We 
find that this type of query involves more mac-
roscopic features which results in more kernels 
of negative feedbacks. Although we can solve 
the issue by increasing the dimensionality of op-
posite intent, it undoubtedly impairs the effi-
ciency of re-ranking.  
6. CONCLUSION 
This paper proposes a new re-ranking scheme 
to well explore the opposite intent. In particular, 
a hierarchical distance-based (HD) measure is 
proposed to differentiate the opposite intent from 
the true query intent so as to repel negative 
feedbacks. Experiments show substantial out-
performance of our methods. 
Although our scheme has been proven effec-
tive in most cases, it fails on macroscopic queries. 
In fact, the key difficulty of this issue lies in how 
to ascertain the focal query intent given various 
kernels in pseudo-feedback. Fortunately, 
click-through data provide some useful informa-
tion for learning real query intent. Although it 
seems feasible to generate focal intent represen-
tation by using overlapping terms in clicked 
feedbacks, such representation is just a reproduc-
tion of macroscopic query since the overlapping 
terms can only reflect common topic instead of 
focal intent. Therefore, it is important to segment 
clicked feedbacks into different blocks, and as-
certain the block of greatest interest to searchers.  
References 
Allan, J., Lavrenko, V., and Nallapati, R. 2002. 
UMass at TDT 2002, Topic Detection and 
Tracking: Workshop. 
Craswell, N., and Szummer, M. Random walks on 
the click graph. 2007. In Proceedings of the 
Conference on Research and Development in 
Information Retrieval. SIGIR '30. ACM Press, 
New York, NY, 239-246. 
Cao, G. H., Nie, J. Y., and Gao, J. F. 2008. Stephen 
Robertson. Selecting Good Expansion Terms for 
Pseudo-Relevance Feedback. In Proceedings of 
the Conference on Research and Development in 
Information Retrieval. SIGIR '31. ACM Press, 
New York, NY, 243-250. 
Chum, O., Philbin, J., Sivic, J., and Zisserman, A. 
2007. Automatic query expansion with a genera-
tive feature model for object retrieval. In Pro-
ceedings of the 11th International Conference on 
Computer Vision, Rio de Janeiro, Brazil, 1?8. 
Joachims, T., Granka, L., and Pan, B. 2003. Accu-
rately Interpreting Clickthrough Data as Implicit 
Feedback. In Proceedings of the Conference on 
Research and Development in Information Re-
trieval. SIGIR '28. New York, NY, 154-161. 
Lee, K. S., Croft, W. B., and Allan, J. 2008 A Clus-
ter-Based Resampling Method for 
Pseudo-Relevance Feedback. In Proceedings of 
the Conference on Research and Development in 
Information Retrieval. SIGIR '31. ACM Press, 
New York, NY, 235-242. 
Thollard, F., Dupont, P., and Higuera, L.2000. 
Probabilistic DFA Inference Using Kull-
back-Leibler Divergence and Minimality. In 
Proceedings of the 17th Int'l Conf on Machine 
Learning. San Francisco: Morgan Kaufmann, 
975-982. 
Teevan, J. T., Dumais, S. T., and Liebling, D. J. 
2008. To Personalize or Not to Personalize: 
Modeling Queries with Variation in User Intent. 
In Proceedings of the Conference on Research 
and Development in Information Retrieval. 
SIGIR '31. New York, NY, 163-170. 
Wang, X. H., Fang, H., and Zhai, C. X. 2008. A 
Study of Methods for Negative Relevance Feed-
back. In Proceedings of the Conference on Re-
search and Development in Information Re-
trieval. SIGIR '31. ACM Press, New York, NY, 
219-226. 
Wang, X. H., Fang, H., and Zhai, C. X. 2007. Im-
prove retrieval accuracy for difficult queries us-
ing negative feedback. In Proceedings of the 
sixteenth ACM conference on Conference on 
information and knowledge management. ACM 
press, New York, NY, USA, 991-994. 
Zhang, P., Hou, Y. X., and Song, D. 2009. Ap-
proximating True Relevance Distribution from a 
Mixture Model based on Irrelevance Data. In 
Proceedings of the Conference on Research and 
Development in Information Retrieval. SIGIR 
'31. ACM Press, New York, NY, 107-114. 
444
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1127?1136,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Cross-Entity Inference to Improve Event Extraction 
Yu Hong     Jianfeng Zhang     Bin Ma     Jianmin Yao     Guodong Zhou     Qiaoming Zhu 
School of Computer Science and Technology, Soochow University, Suzhou City, China 
{hongy, jfzhang, bma, jyao, gdzhou, qmzhu}@suda.edu.cn 
 
 
Abstract 
Event extraction is the task of detecting certain 
specified types of events that are mentioned in 
the source language data. The state-of-the-art 
research on the task is transductive inference 
(e.g. cross-event inference). In this paper, we 
propose a new method of event extraction by 
well using cross-entity inference. In contrast to 
previous inference methods, we regard entity-
type consistency as key feature to predict event 
mentions. We adopt this inference method to 
improve the traditional sentence-level event ex-
traction system. Experiments show that we can 
get 8.6% gain in trigger (event) identification, 
and more than 11.8% gain for argument (role) 
classification in ACE event extraction. 
1 Introduction 
The event extraction task in ACE (Automatic Con-
tent Extraction) evaluation involves three challeng-
ing issues: distinguishing events of different types, 
finding the participants of an event and determin-
ing the roles of the participants. 
The recent researches on the task show the 
availability of transductive inference, such as that 
of the following methods: cross-document, cross-
sentence and cross-event inferences. Transductive 
inference is a process to use the known instances to 
predict the attributes of unknown instances. As an 
example, given a target event, the cross-event in-
ference can predict its type by well using the re-
lated events co-occurred with it within the same 
document. From the sentence: 
(1)He left the company. 
it is hard to tell whether it is a Transport event in 
ACE, which means that he left the place; or an 
End-Position event, which means that he retired 
from the company. But cross-event inference can 
use a related event ?Then he went shopping? within 
the same document to identify it as a Transport 
event correctly. 
As the above example might suggest, the avail-
ability of transductive inference for event extrac-
tion relies heavily on the known evidences of an 
event occurrence in specific condition. However, 
the evidence supporting the inference is normally 
unclear or absent. For instance, the relation among 
events is the key clue for cross-event inference to 
predict a target event type, as shown in the infer-
ence process of the sentence (1). But event relation 
extraction itself is a hard task in Information Ex-
traction. So cross-event inference often suffers 
from some false evidence (viz., misleading by un-
related events) or lack of valid evidence (viz., un-
successfully extracting related events). 
In this paper, we propose a new method of 
transductive inference, named cross-entity infer-
ence, for event extraction by well using the rela-
tions among entities. This method is firstly 
motivated by the inherent ability of entity types in 
revealing event types. From the sentences: 
(2)He left the bathroom. 
(3)He left Microsoft. 
it is easy to identify the sentence (2) as a Transport 
event in ACE, which means that he left the place, 
because nobody would retire (End-Position type) 
from a bathroom. And compared to the entities in 
sentence (1) and (2), the entity ?Microsoft? in (3) 
would give us more confidence to tag the ?left? 
event as an End-Position type, because people are 
used to giving the full name of the place where 
they retired. 
The cross-entity inference is also motivated by 
the phenomenon that the entities of the same type 
often attend similar events. That gives us a way to 
predict event type based on entity-type consistency. 
From the sentence: 
(4)Obama beats McCain. 
it is hard to identify it as an Elect event in ACE, 
which means Obama wins the Presidential Election, 
1127
or an Attack event, which means Obama roughs 
somebody up. But if we have the priori knowledge 
that the sentence ?Bush beats McCain? is an Elect 
event, and ?Obama? was a presidential contender 
just like ?Bush? (strict type consistency), we have 
ample evidence to predict that the sentence (4) is 
also an Elect event. 
Indeed above cross-entity inference for event-
type identification is not the only use of entity-type 
consistency. As we shall describe below, we can 
make use of it at all issues of event extraction: 
y For event type: the entities of the same type 
are most likely to attend similar events. And the 
events often use consistent or synonymous trigger. 
y For event argument (participant): the enti-
ties of the same type normally co-occur with simi-
lar participants in the events of the same type. 
y For argument role: the arguments of the 
same type, for the most part, play the same roles in 
similar events. 
With the help of above characteristics of entity, 
we can perform a step-by-step inference in this 
order:  
y Step 1: predicting event type and labeling 
trigger given the entities of the same type. 
y Step 2: identifying arguments in certain event 
given priori entity type, event type and trigger that 
obtained by step 1. 
y Step 3: determining argument roles in certain 
event given entity type, event type, trigger and ar-
guments that obtained by step 1 and step 2. 
On the basis, we give a blind cross-entity infer-
ence method for event extraction in this paper. In 
the method, we first regard entities as queries to 
retrieve their related documents from large-scale 
language resources, and use the global evidences 
of the documents to generate entity-type descrip-
tions. Second we determine the type consistency of 
entities by measuring the similarity of the type de-
scriptions. Finally, given the priori attributes of 
events in the training data, with the help of the en-
tities of the same type, we perform the step-by-step 
cross-entity inference on the attributes of test 
events (candidate sentences). 
In contrast to other transductive inference meth-
ods on event extraction, the cross-entity inference 
makes every effort to strengthen effects of entities 
in predicting event occurrences. Thus the inferen-
tial process can benefit from following aspects: 1) 
less false evidence, viz. less false entity-type con-
sistency (the key clue of cross-entity inference), 
because the consistency can be more precisely de-
termined with the help of fully entity-type descrip-
tion that obtained based on the related information 
from Web; 2) more valid evidence, viz. more enti-
ties of the same type (the key references for the 
inference), because any entity never lack its con-
geners. 
2 Task Description 
The event extraction task we addressing is that of 
the Automatic Content Extraction (ACE) evalua-
tions, where an event is defined as a specific occur-
rence involving participants. And event extraction 
task requires that certain specified types of events 
that are mentioned in the source language data be 
detected. We first introduce some ACE terminol-
ogy to understand this task more easily: 
y Entity: an object or a set of objects in one of 
the semantic categories of interest, referred to in 
the document by one or more (co-referential) entity 
mentions. 
y Entity mention: a reference to an entity (typi-
cally, a noun phrase). 
y Event trigger: the main word that most clear-
ly expresses an event occurrence (An ACE event 
trigger is generally a verb or a noun). 
y Event arguments: the entity mentions that 
are involved in an event (viz., participants). 
y Argument roles: the relation of arguments to 
the event where they participate. 
y Event mention: a phrase or sentence within 
which an event is described, including trigger and 
arguments. 
The 2005 ACE evaluation had 8 types of events, 
with 33 subtypes; for the purpose of this paper, we 
will treat these simply as 33 separate event types 
and do not consider the hierarchical structure 
among them. Besides, the ACE evaluation plan 
defines the following standards to determine the 
correctness of an event extraction: 
y A trigger is correctly labeled if its event type 
and offset (viz., the position of the trigger word in 
text) match a reference trigger. 
y An argument is correctly identified if its event 
type and offsets match any of the reference argu-
ment mentions, in other word, correctly recogniz-
ing participants in an event. 
y An argument is correctly classified if its role 
matches any of the reference argument mentions. 
Consider the sentence: 
1128
(5) It has refused in the last five years to revoke 
the license of a single doctor for committing medi-
cal errors.1
The event extractor should detect an End-
Position event mention, along with the trigger 
word ?revoke?, the position ?doctor?, the person 
whose license should be revoked, and the time dur-
ing which the event happened: 
 Event type End-Position 
Trigger revoke 
a single doctor Role=Person 
doctor Role=Position Arguments 
the last five years Role=Time-within 
Table 1: Event extraction example 
It is noteworthy that event extraction depends on 
previous phases like name identification, entity 
mention co-reference and classification. Thereinto, 
the name identification is another hard task in ACE 
evaluation and not the focus in this paper. So we 
skip the phase and instead directly use the entity 
labels provided by ACE. 
3 Related Work 
Almost all the current ACE event extraction sys-
tems focus on processing one sentence at a time 
(Grishman et al, 2005; Ahn, 2006; Hardyet al 
2006). However, there have been several studies 
using high-level information from a wider scope:  
Maslennikov and Chua (2007) use discourse 
trees and local syntactic dependencies in a pattern-
based framework to incorporate wider context to 
refine the performance of relation extraction. They 
claimed that discourse information could filter noi-
sy dependency paths as well as increasing the reli-
ability of dependency path extraction. 
Finkel et al (2005) used Gibbs sampling, a sim-
ple Monte Carlo method used to perform approxi-
mate inference in factored probabilistic models. By 
using simulated annealing in place of Viterbi de-
coding in sequence models such as HMMs, CMMs, 
and CRFs, it is possible to incorporate non-local 
structure while preserving tractable inference. 
They used this technique to augment an informa-
tion extraction system with long-distance depend-
ency models, enforcing label consistency and 
extraction template consistency constraints. 
Ji and Grishman (2008) were inspired from the 
hypothesis of ?One Sense Per Discourse? (Ya-
                                                          
1 Selected from the file ?CNN_CF_20030304.1900.02? in 
ACE-2005 corpus. 
rowsky, 1995); they extended the scope from a 
single document to a cluster of topic-related docu-
ments and employed a rule-based approach to 
propagate consistent trigger classification and 
event arguments across sentences and documents. 
Combining global evidence from related docu-
ments with local decisions, they obtained an appre-
ciable improvement in both event and event 
argument identification. 
Patwardhan and Riloff (2009) proposed an event 
extraction model which consists of two compo-
nents: a model for sentential event recognition, 
which offers a probabilistic assessment of whether 
a sentence is discussing a domain-relevant event; 
and a model for recognizing plausible role fillers, 
which identifies phrases as role fillers based upon 
the assumption that the surrounding context is dis-
cussing a relevant event. This unified probabilistic 
model allows the two components to jointly make 
decisions based upon both the local evidence sur-
rounding each phrase and the ?peripheral vision?. 
Gupta and Ji (2009) used cross-event informa-
tion within ACE extraction, but only for recovering 
implicit time information for events. 
Liao and Grishman (2010) propose document 
level cross-event inference to improve event ex-
traction. In contrast to Gupta?s work, Liao do not 
limit themselves to time information for events, but 
rather use related events and event-type consis-
tency to make predictions or resolve ambiguities 
regarding a given event. 
4 Motivation 
In event extraction, current transductive inference 
methods focus on the issue that many events are 
missing or spuriously tagged because the local in-
formation is not sufficient to make a confident de-
cision. The solution is to mine credible evidences 
of event occurrences from global information and 
regard that as priori knowledge to predict unknown 
event attributes, such as that of cross-document 
and cross-event inference methods.  
However, by analyzing the sentence-level base-
line event extraction, we found that the entities 
within a sentence, as the most important local in-
formation, actually contain sufficient clues for 
event detection. It is only based on the premise that 
we know the backgrounds of the entities before-
hand. For instance, if we knew the entity ?vesu-
vius? is an active volcano, we could easily identify 
1129
the word ?erupt?, which co-occurred with the en-
tity, as the trigger of a ?volcanic eruption? event 
but not that of a ?spotty rash?. 
In spite of that, it is actually difficult to use an 
entity to directly infer an event occurrence because 
we normally don?t know the inevitable connection 
between the background of the entity and the event 
attributes. But we can well use the entities of the 
same background to perform the inference. In de-
tail, if we first know entity(a) has the same back-
ground with entity(b), and we also know that 
entity(a), as a certain role, participates in a specific 
event, then we can predict that entity(b) might par-
ticiptes in a similar event as the same role. 
Consider the two sentences2 from ACE corpus: 
(5) American case for war against Saddam. 
(6) Bush should torture the al Qaeda chief op-
erations officer. 
The sentences are two event mentions which 
have the same attributes: 
Event type Attack 
Trigger war 
American Role=Attacker 
(5) 
Arguments 
Saddam Role=Target 
Event type Attack 
Trigger torture 
Bush Role=Attacker 
(6) 
Arguments 
...Qaeda chief ... Role=Target 
Table 2: Cross-entity inference example 
From the sentences, we can find that the entities 
?Saddam? and ?Qaeda chief? have the same back-
ground (viz., terrorist leader), and they are both the 
arguments of Attack events as the role of Target. 
So if we previously know any of the event men-
tions, we can infer another one with the help of the 
entities of the same background. 
In a word, the cross-entity inference, we pro-
posed for event extraction, bases on the hypothesis: 
Entities of the consistent type normally partici-
pate in similar events as the same role. 
As we will introduce below, some statistical da-
ta from ACE training corpus can support the hy-
pothesis, which show the consistency of event type 
and role in event mentions where entities of the 
same type occur. 
4.1 Entity Consistency and Distribution 
Within the ACE corpus, there is a strong entity 
consistency: if one entity mention appears in a type 
                                                          
2 They are extracted from the files ?CNN_CF_20030305.1900. 
00-1? and ?CNN_CF_20030303.1900.06-1? respectively. 
of event, other entity mentions of the same type 
will appear in similar events, and even use the 
same word to trigger the events. To see this we 
calculated the conditional probability (in the ACE 
corpus) of a certain entity type appearing in the 33 
ACE event subtypes. 
0
50
100
150
200
250
Be?Born
M
arry
D
ivorce
Injure
D
ie
Transport
Transfer?
Transfer?
Start?O
rg
M
erge?
D
eclare?
End?O
rg
A
ttack
D
em
onstr
M
eet
Phone?
Start?
End?
N
om
inate
Elect
A
rrest?Jail
Release?
Trial?
Charge?
Sue
Convict
Sentence
Fine
Execute
Extradite
A
cquit
A
ppeal
Pardon
Event typeF
re
qu
en
cy
Population?Center
Exploding
Air
 
Figure 1. Conditional probability of a certain entity 
type appearing in the 33 ACE event subtypes (Here 
only the probabilities of Population-Center, Ex-
ploding and Air entities as examples) 
0
50
100
150
200
250
Person
Place
Buyer
Seller
Beneficiary
Price
A
rtifact
O
rigin
D
estination
G
iver
Recipient
M
oney
O
rg
A
gent
Victim
Instrum
ent
Entity
A
ttacker
Target
D
efendant
A
djudicator
Prosecutor
Plaintiff
Crim
e
Position
Sentence
Vehicle
Tim
e?A
fter
Tim
e?Before
Tim
e?A
t?
Tim
e?A
t?End
Tim
e?
Tim
e?
Tim
e?H
olds
Tim
e?
RoleF
re
qu
en
cy
Population?Center
Exploding
Air
 
Figure 2. Conditional probability of an entity type 
appearing as the 34 ACE role types (Here only the 
probabilities of Population-Center, Exploding and 
Air entities as examples) 
As there are 33 event subtypes and 43 entity 
types, there are potentially 33*43=1419 entity-
event combinations. However, only a few of these 
appear with substantial frequency. For example, 
the Population-Center entities only occur in 4 
types of event mentions with the conditional prob-
ability more than 0.05. From Table 3, we can find 
that only Attack and Transport events co-occur 
frequently with Population-Center entities (see 
Figure 1 and Table 3). 
Event Cond.Prob. Freq. 
Transport 0.368 197 
Attack 0.295 158 
Meet 0.073 39 
Die 0.069 37 
Table 3: Events co-occurring with Population-
Center with the conditional probability > 0.05 
Actually we find that most entity types appear in 
more restricted event mentions than Population-
Center entity. For example, Air entity only co-
occurs with 5 event types (Attack, Transport, Die, 
Transfer-Ownership and Injure), and Exploding 
1130
entity co-occurs with 4 event types (see Figure 1). 
Especially, they only co-occur with one or two 
event types with the conditional probability more 
than 0.05. 
 Evnt.<=5 5<Evnt.<=10 Evnt.>10 
Freq. > 0 24 7 12 
Freq. >10 37 4 2 
Freq. >50 41 1 1 
Table 4: Distribution of entity-event combination 
corresponding to different co-occurrence frequency 
Table 4 gives the distributions of whole ACE 
entity types co-occurring with event types. We can 
find that there are 37 types of entities (out of 43 in 
total) appearing in less than 5 types of event men-
tions when entity-event co-occurrence frequency is 
larger than 10, and only 2 (e.g. Individual) appear-
ing in more than 10 event types. And when the fre-
quency is larger than 50, there are 41 (95%) entity 
types co-occurring with less than 5 event types. 
These distributions show the fact that most in-
stances of a certain entity type normally participate 
in events of the same type. And the distributions 
might be good predictors for event type detection 
and trigger determination. 
Air (Entity type) 
Attack 
event 
Fighter plane (subtype 1): 
?MiGs? ?enemy planes? ?warplanes? ?allied 
aircraft? ?U.S. jets? ?a-10 tank killer? ?b-1 
bomber? ?a-10 warthog? ?f-14 aircraft? 
?apache helicopter? 
Spacecraft (subtype 2): 
?russian soyuz capsule? ?soyuz? 
Civil aviation (subtype 3): 
?airliners? ?the airport? ?Hooters Air execu-
tive? 
Transport 
event 
Private plane (subtype 4): 
?Marine One? ?commercial flight? ?private 
plane? 
Table 5: Event types co-occurred with Air entities 
Besides, an ACE entity type actually can be di-
vided into more cohesive subtypes according to 
similarity of background of entity, and such a sub-
type nearly always co-occur with unique event 
type. For example, the Air entities can be roughly 
divided into 4 subtypes: Fighter plane, Spacecraft, 
Civil aviation and Private plane, within which the 
Fighter plane entities all appear in Attack event 
mentions, and other three subtypes all co-occur 
with Transport events (see Table 5). This consis-
tency of entities in a subtype is helpful to improve 
the precision of the event type predictor. 
4.2 Role Consistency and Distribution 
The same thing happens for entity-role combina-
tions: entities of the same type normally play the 
same role, especially in the event mentions of the 
same type. For example, the Population-Center 
entities occur in ACE corpus as only 4 role types: 
Place, Destination, Origin and Entity respectively 
with conditional probability 0.615, 0.289, 0.093, 
0.002 (see Figure 2). And They mainly appear in 
Transport event mentions as Place, and in Attack 
as Destination. Particularly the Exploding entities 
only occur as Instrument and Artifact respectively 
with the probability 0.986 and 0.014. They almost 
entirely appear in Attack events as Instrument. 
 Evnt.<=5 5<Evnt.<=10 Evnt.>10 
Freq. > 0 32 5 6 
Freq. >10 38 3 2 
Freq. >50 42 1 0 
Table 6: Distribution of entity-role combination 
corresponding to different co-occurrence frequency 
Table 6 gives the distributions of whole entity-
role combinations in ACE corpus. We can find that 
there are 38 entity types (out of 43 in total) occur 
as less than 5 role types when the entity-role co-
occurrence frequency is larger than 10. There are 
42 (98%) when the frequency is larger than 50, and 
only 2 (e.g. Individual) when larger than 10. The 
distributions show that the instances of an entity 
type normally occur as consistent role, which is 
helpful for cross-entity inference to predict roles. 
5 Cross-entity Approach  
In this section we present our approach to using 
blind cross-entity inference to improve sentence-
level ACE event extraction. 
Our event extraction system extracts events in-
dependently for each sentence, because the defini-
tion of event mention constrains them to appear in 
the same sentence. Every sentence that at least in-
volves one entity mention will be regarded as a 
candidate event mention, and a randomly selected 
entity mention from the candidate will be the star-
ing of the whole extraction process. For the entity 
mention, information retrieval is used to mine its 
background knowledge from Web, and its type is 
determined by comparing the knowledge with 
those in training corpus. Based on the entity type, 
the extraction system performs our step-by-step 
cross-entity inference to predict the attributes of 
1131
the candidate event mention: trigger, event type, 
arguments, roles and whether or not being an event 
mention. The main frame of our event extraction 
system is shown in Figure 3, which includes both 
training and testing processes. 
 
Figure 3. The frame of cross-entity inference for event extraction (including training and testing processes) 
In the training process, for every entity type in 
the ACE training corpus, a clustering technique 
(CLUTO toolkit)3 is used to divide it into different 
cohesive subtypes, each of which only contains the 
entities of the same background. For instance, the 
Air entities will be divided into Fighter plane, 
Spacecraft, Civil aviation, Private plane, etc (see 
Table 5). And for each subtype, we mine event 
mentions where this type of entities appear from 
ACE training corpus, and extract all the words 
which trigger the events to establish corresponding 
trigger list. Besides, a set of support vector ma-
chine (SVM) based classifiers are also trained: 
y Argument Classifier: to distinguish arguments 
of a potential trigger from non-arguments4; 
y Role Classifier: to classify arguments by ar-
gument role; 
y Reportable-Event Classifier (Trigger Classi-
fier): Given entity types, a potential trigger, an 
event type, and a set of arguments, to determine 
whether there is a reportable event mention. 
                                                          
3http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=h
tml&identifier=ADA439508 
4 It is noteworthy that a sentence may include more than one 
event (more than one trigger). So it is necessary to distinguish 
arguments of a potential trigger from that of others. 
In the test process, for each candidate event 
mention, our event extraction system firstly pre-
dicts its triggers and event types: given an ran-
domly selected entity mention from the candidate, 
the system determines the entity subtype it belong-
ing to and the corresponding trigger list, and then 
all non-entity words in the candidate are scanned 
for a instance of triggers from the list. When an 
instance is found, the system tags the candidate as 
the event type that the most frequently co-occurs 
with the entity subtype in the events that triggered 
by the instance. Secondly the argument classifier is 
applied to the remaining mentions in the candidate; 
for any argument passing that classifier, the role 
classifier is used to assign a role to it. Finally, once 
all arguments have been assigned, the reportable-
event classifier is applied to the candidate; if the 
result is successful, this event mention is reported. 
5.1 Further Division of Entity Type  
One of the most important pretreatments before 
our blind cross-entity inference is to divide the 
ACE entity type into more cohesive subtype. The 
greater consistency among backgrounds of entities 
in such a subtype might be good to improve the 
precision of cross-entity inference.  
1132
For each ACE entity type, we collect all entity 
mentions of the type from training corpus, and re-
gard each such mention as a query to retrieve the 
50 most relevant documents from Web. Then we 
select 50 key words that the most weighted by 
TFIDF in the documents to roughly describe back-
ground of entity. After establishing the vector 
space model (VSM) for each entity mention of the 
type, we adopt a clustering toolkit (CLUTO) to 
further divide the mentions into different subtypes. 
Finally, for each subtype, we describe its centroid 
by using 100 key words which the most frequently 
occurred in relevant documents of entities of the 
subtype. 
In the test process, for an entity mention in a 
candidate event mention, we determine its type by 
comparing its background against all centroids of 
subtypes in training corpus, and the subtype whose 
centroid has the most Cosine similarity with the 
background will be assigned to the entity. It is 
noteworthy that global information from the Web 
is only used to measure the entity-background con-
sistency and not directly in the inference process. 
Thus our event extraction system actually still per-
forms a sentence-level inference based on local 
information. 
5.2 Cross-Entity Inference 
Our event extraction system adopts a step-by-
step cross-entity inference to predict event. As dis-
cussed above, the first step is to determine the trig-
ger in a candidate event mention and tag its event 
type based on consistency of entity type. Given the 
domain of event mention that restrained by the 
known trigger, event type and entity subtype, the 
second step is to distinguish the most probable ar-
guments that co-occurring in the domain from the 
non-arguments. Then for each of the arguments, 
the third step can use the co-occurring arguments 
in the domain as important contexts to predict its 
role. Finally, the inference process determines 
whether the candidate is a reportable event men-
tion according to a confidence coefficient. In the 
following sections, we focus on introducing the 
three classifiers: argument classifier, role classifier 
and reportable-event classifier. 
5.2.1   Cross-Entity Argument Classifier 
For a candidate event mention, the first step 
gives its event type, which roughly restrains the 
domain of event mentions where the arguments of 
the candidate might co-occur. On the basis, given 
an entity mention in the candidate and its type (see 
the pretreatment process in section 5.1), the argu-
ment classifier could predict whether other entity 
mentions co-occur with it in such a domain, if yes, 
all the mentions will be the arguments of the can-
didate. In other words, if we know an entity of a 
certain type participates in some event, we will 
think of what entities also should participate in the 
event. For instance, when we know a defendant 
goes on trial, we can conclude that the judge, law-
yer and witness should appear in court. 
Argument Classifier 
Feature 1: an event type (an event-mention domain) 
Feature 2: an entity subtype 
Feature 3: entity-subtype co-occurrence in domain 
Feature 4: distance to trigger 
Feature 5: distances to other arguments 
Feature 6: co-occurrence with trigger in clause 
Role Classifier 
Feature 1 and Feature 2 
Feature 7: entity-subtypes of arguments 
Reportable-Event Classifier 
Feature 1 
Feature 8: confidence coefficient of trigger in domain 
Feature 9: confidence coefficient of role in domain 
Table 7: Features selected for SVM-based cross-
entity classifiers 
A SVM-based argument classifier is used to de-
termine arguments of candidate event mention. 
Each feature of this classifier is the conjunction of: 
y The subtype of an entity 
y The event type we are trying to assign an ar-
gument to 
y A binary indicator of whether this entity sub-
type co-occurs with other subtypes in such an 
event type (There are 266 entity subtypes, and so 
266 features for each instance) 
Some minor features, such as another binary indi-
cator of whether arguments co-occur with trigger 
in the same clause (see Table 7). 
5.2.2 Cross-Entity Role Classifier 
For a candidate event mention, the arguments 
that given by the second step (argument classifier) 
provide important contextual information for pre-
dicting what role the local entity (also one of the 
arguments) takes on. For instance, when citizens 
(Arg1) co-occur with terrorist (Arg2), most likely 
the role of Arg1 is Victim. On the basis, with the 
help of event type, the prediction might be more 
1133
precise. For instance, if the Arg1 and Arg2 co-
occur in an Attack event mention, we will have 
more confidence in the Victim role of Arg1. 
Besides, as discussed in section 4, entities of the 
same type normally take on the same role in simi-
lar events, especially when they co-occur with sim-
ilar arguments in the events (see Table 2). 
Therefore, all instances of co-occurrence model 
{entity subtype, event type, arguments} in training 
corpus could provide effective evidences for pre-
dicting the role of argument in the candidate event 
mention. Based on this, we trained a SVM-based 
role classifier which uses following features: 
y Feature 1 and Feature 2 (see Table 7) 
y Given the event domain that restrained by the 
entity and event types, an indicator of what sub-
types of arguments appear in the domain. (266 en-
tity subtypes make 266 features for each instance) 
5.2.3 Reportable-Event Classifier 
At this point, there are still two issues need to be 
resolved. First, some triggers are common words 
which often mislead the extraction of candidate 
event mention, such as ?it?, ?this?, ?what?, etc. 
These words only appear in a few event mentions 
as trigger, but when they once appear in trigger list, 
a large quantity of noisy sentences will be regarded 
as candidates because of their commonness in sen-
tences. Second, some arguments might be tagged 
as more than one role in specific event mentions, 
but as ACE event guideline, one argument only 
takes on one role in a sentence. So we need to re-
move those with low confidence. 
A confidence coefficient is used to distinguish 
the correct triggers and roles from wrong ones. The 
coefficient calculate the frequency of a trigger (or a 
role) appearing in specific domain of event men-
tions and that in whole training corpus, then com-
bines them to represent its confidence degree, just 
like TFIDF algorithm. Thus, the more typical trig-
gers (or roles) will be given high confidence. 
Based on the coefficient, we use a SVM-based 
classifier to determine the reportable events. Each 
feature of this classifier is the conjunction of: 
y An event type (domain of event mentions) 
y Confidence coefficients of triggers in domain 
y Confidence coefficients of roles in the domain. 
6 Experiments 
We followed Liao (2010)?s evaluation and ran-
domly select 10 newswire texts from the ACE 
2005 training corpus as our development set, 
which is used for parameter tuning, and then con-
duct a blind test on a separate set of 40 ACE 2005 
newswire texts. We use the rest of the ACE train-
ing corpus (549 documents) as training data for our 
event extraction system.  
To compare with the reported work on cross-
event inference (Liao, 2010) and its sentence-level 
baseline system, we cross-validate our method on 
10 separate sets of 40 ACE texts, and report the 
optimum, worst and mean performances (see Table 
8) on the data by using Precision (P), Recall (R) 
and F-measure (F). In addition, we also report the 
performance of two human annotators on 40 ACE 
newswire texts (a random blind test set): one 
knows the rules of event extraction; the other 
knows nothing about it. 
6.1 Main Results  
From the results presented in Table 8, we can 
see that using the cross-entity inference, we can 
improve the F score of sentence-level event extrac-
tion for trigger classification by 8.59%, argument 
classification by 11.86%, and role classification by 
11.9% (mean performance). Compared to the 
cross-event inference, we gains 2.87% improve-
ment for argument classification, and 3.81% for 
role classification (mean performance). Especially, 
our worst results also have better performances 
than cross-event inference. 
Nonetheless, the cross-entity inference has 
worse F score for trigger determination. As we can 
see, the low Recall score weaken its F score (see 
Table 8). Actually, we select the sentence which at 
least includes one entity mention as candidate 
event mention, but lots of event mentions in ACE 
never include any entity mention. Thus we have 
missed some mentions at the starting of inference 
process. 
In addition, the annotator who knows the rules 
of event extraction has a similar performance trend 
with systems: high for trigger classification, mid-
dle for argument classification, and low for role 
classification (see Table 8). But the annotator who 
never works in this field obtains a different trend: 
higher performance for argument classification. 
This phenomenon might prove that the step-by-
step inference is not the only way to predicate 
event mention because human can determine ar-
guments without considering triggers and event 
types. 
1134
                            Performance 
System/Human Trigger (%) Argument (%) Role (%) 
 P R F P R F P R F 
Sentence-level baseline 67.56 53.54 59.74 46.45 37.15 41.29 41.02 32.81 36.46
Cross-event inference 68.71 68.87 68.79 50.85 49.72 50.28 45.06 44.05 44.55
Cross-entity inference (optimum) 73.4 66.2 69.61 56.96 55.1 56 49.3 46.59 47.9 
Cross-entity inference (worst) 71.3 64.17 66.1 51.28 50.3 50.78 46.3 44.3 45.28
Cross-entity inference (mean) 72.9 64.3 68.33 53.4 52.9 53.15 51.6 45.5 48.36
Human annotation 1 (blind) 58.9 59.1 59.0 62.6 65.9 64.2 50.3 57.69 53.74
Human annotation 2 (know rules) 74.3 76.2 75.24 68.5 75.8 71.97 61.3 68.8 64.86
Table 8: Overall performance on blind test data
6.2 Influence of Clustering on Inference  
A main part of our blind inference system is the 
entity-type consistency detection, which relies 
heavily on the correctness of entity clustering and 
similarity measurement. In training, we used 
CLUTO clustering toolkit to automatically gener-
ate different types of entities based on their back-
ground-similarities. In testing, we use K-nearest 
neighbor algorithm to determine entity type. 
Fighter plane (subtype 1 in Air entities): 
?warplanes? ?allied aircraft? ?U.S. jets? ?a-10 tank killer? 
?b-1 bomber? ?a-10 warthog? ?f-14 aircraft? ?apache heli-
copter? ?terrorist? ?Saddam? ?Saddam Hussein? ?Bagh-
dad??
Table 9: Noises in subtype 1 of ?Air? entities (The 
blod fonts are noises) 
We obtained 129 entity subtypes from training 
set. By randomly inspecting 10 subtypes, we found 
nearly every subtype involves no less than 19.2% 
noises. For example, the subtype 1 of ?Air? in Ta-
ble 5 lost the entities of ?MiGs? and ?enemy 
planes?, but involved ?terrorist?, ?Saddam?, etc 
(See Table 9). Therefore, we manually clustered 
the subtypes and retry the step-by-step cross-entity 
inference. The results (denoted as ?Visible 1?) are 
shown in Table 10, within which, we additionally 
show the performance of the inference on the 
rough entity types provided by ACE (denoted as 
?Visible 2?), such as the type of ?Air?, ?Popula-
tion-Center?, ?Exploding?, etc., which normally 
can be divided into different more cohesive sub-
types. And the ?Blind? in Table 10 denotes the 
performances on our subtypes obtained by CLUTO. 
It is surprised that the performances (see Table 
10, F-score) on ?Visible 1? entity subtypes are just 
a little better than ?Blind? inference. So it seems 
that the noises in our blind entity types (CLUTO 
clusters) don?t hurt the inference much. But by re-
inspecting the ?Visible 1? subtypes, we found that 
their granularities are not enough small: the 89 
manual entity clusters actually can be divided into 
more cohesive subtypes. So the improvements of 
inference on noise-free ?Visible 1? subtypes are 
partly offset by loss on weakly consistent entities 
in the subtypes. It can be proved by the poor per-
formances on ?Visible 2? subtypes which are much 
more general than ?Visible 1?. Therefore, a rea-
sonable clustering method is important in our in-
ference process. 
F-score Trigger  Argument Role 
Blind 68.33 53.15 48.36 
Visible 1 69.15 53.65 48.83 
Visible 2 51.34 43.40 39.95 
Table 10: Performances on visible VS blind  
7 Conclusions and Future Work  
We propose a blind cross-entity inference method 
for event extraction, which well uses the consis-
tency of entity mention to achieve sentence-level 
trigger and argument (role) classification. Experi-
ments show that the method has better perform-
ance than cross-document and cross-event 
inferences in ACE event extraction. 
The inference presented here only considers the 
helpfulness of entity types of arguments to role 
classification. But as a superior feature, contextual 
roles can provide more effective assistance to role 
determination of local argument. For instance, 
when an Attack argument appears in a sentence, a 
Target might be there. So if we firstly identify 
simple roles, such as the condition that an argu-
ment has only a single role, and then use the roles 
as priori knowledge to classify hard ones, may be 
able to further improve performance.
Acknowledgments 
We thank Ruifang He. And we acknowledge the 
support of the National Natural Science Founda-
tion of China under Grant Nos. 61003152, 
60970057, 90920004. 
1135
References  
David Ahn. 2006. The stages of event extraction. In 
Proc. COLING/ACL 2006 Workshop on Annotating 
and Reasoning about Time and Events.Sydney, Aus-
tralia. 
Jenny Rose Finkel, Trond Grenager and Christopher 
Manning. 2005. Incorporating Non-local Information 
into Information Extraction Systems by Gibbs Sam-
pling. In Proc. 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 363?370, 
Ann Arbor, MI, June. 
Prashant Gupta and Heng Ji. 2009. Predicting Unknown 
Time Arguments based on Cross-Event Propagation. 
In Proc. ACL-IJCNLP 2009. 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System Description. 
In Proc. ACE 2005 Evaluation Workshop, Gaithers-
burg, MD. 
Hilda Hardy, Vika Kanchakouskaya and Tomek Strzal-
kowski. 2006. Automatic Event Classification Using 
Surface Text Features. In Proc. AAAI06 Workshop on 
Event Extraction and Synthesis. Boston, MA. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction through Cross-Document Inference. In 
Proc. ACL-08: HLT, pages 254?262, Columbus, OH, 
June. 
Shasha Liao and Ralph Grishman. 2010. Using Docu-
ment Level Cross-Event Inference to Improve Event 
Extraction. In Proc. ACL-2010, pages 789-797, Upp-
sala, Sweden, July. 
Mstislav Maslennikov and Tat-Seng Chua. 2007. A 
Multi resolution Framework for Information Extrac-
tion from Free Text. In Proc. 45th Annual Meeting of 
the Association of Computational Linguistics, pages 
592?599, Prague, Czech Republic, June. 
Siddharth Patwardhan and Ellen Riloff. 2007. Effective 
Information Extraction with Semantic Affinity Pat-
terns and Relevant Regions. In Proc. Joint Confer-
ence on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, 2007, pages 717?727, Prague, Czech Re-
public, June. 
Siddharth Patwardhan and Ellen Riloff. 2009. A Unified 
Model of Phrasal and Sentential Evidence for Infor-
mation Extraction. In Proc. Conference on Empirical 
Methods in Natural Language Processing 2009, 
(EMNLP-09). 
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Proc. 
ACL 1995. Cambridge, MA. 
1136
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 253?257
Manchester, August 2008
Dependency Tree-based SRL with Proper Pruning and Extensive 
Feature Engineering 
Hongling Wang    Honglin Wang   Guodong Zhou   Qiaoming Zhu 
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology, 
Soochow University, Suzhou, China 215006 
{redleaf, 064227065055,gdzhou, qmzhu}@suda.edu.cn 
 
 
 
Abstract 
This paper proposes a dependency tree-
based SRL system with proper pruning and 
extensive feature engineering. Official 
evaluation on the CoNLL 2008 shared task 
shows that our system achieves 76.19 in la-
beled macro F1 for the overall task, 84.56 
in labeled attachment score for syntactic 
dependencies, and 67.12 in labeled F1 for 
semantic dependencies on combined test 
set, using the standalone MaltParser. Be-
sides, this paper also presents our unofficial 
system by 1) applying a new effective 
pruning algorithm; 2) including additional 
features; and 3) adopting a better depend-
ency parser, MSTParser. Unofficial evalua-
tion on the shared task shows that our sys-
tem achieves 82.53 in labeled macro F1, 
86.39 in labeled attachment score, and 
78.64 in labeled F1, using MSTParser on 
combined test set. This suggests that proper 
pruning and extensive feature engineering 
contributes much in dependency tree-based 
SRL.  
1 Introduction 
Although CoNLL 2008 shared task mainly 
evaluates joint learning of syntactic and semantic 
parsing, we focus on dependency tree-based se-
mantic role labeling (SRL). SRL refers to label 
the semantic roles of predicates (either verbs or 
nouns) in a sentence. Most of previous SRL sys-
tems (Gildea and Jurafsky, 2002; Gildea and 
Palmer, 2002; Punyakanok et al, 2005; Pradhan 
                                                 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
 
et al, 2004, 2005) work on constituent structure 
trees and has shown to achieve remarkable re-
sults. For example, Punyakanok et al (2005) 
achieved the best performance in the CoNLL 
2005 shared task with 79.44 in F-measure on the 
WSJ test set and 77.92 on the combined test set 
(WSJ +Brown). 
With rapid development of dependency pars-
ing in the last few years, more and more re-
searchers turn to dependency tree-based SRL 
with hope to advance SRL from viewpoint of 
dependency parsing. Hacioglu (2004) pioneered 
this work by formulating SRL as a classification 
problem of mapping various dependency rela-
tions into semantic roles. Compared with previ-
ous researches on constituent structure tree-based 
SRL which adopts constituents as labeling units, 
dependency tree-based SRL adopts dependency 
relations as labeling units. Due to the difference 
between constituent structure trees and depend-
ency trees, their feature spaces are expected to be 
somewhat different. 
In the CoNLL 2008 shared task, we extend the 
framework by Hacioglu (2004) with maximum 
entropy as our classifier. For evaluation, we will 
mainly report our official SRL performance us-
ing MaltParser (Nivre and Nilsson, 2005). Be-
sides, we will also present our unofficial system 
by 1) applying a new effective pruning algorithm; 
2) including additional features; and 3) adopting 
a better dependency parser, MSTParser (McDon-
ald, 2005). 
In the remainder of this paper, we will briefly 
describe our system architecture, present various 
features used by our models and report the per-
formance on CoNLL 2008 shared task (both offi-
cial and unofficial). 
253
2 System Description 
In CoNLL 2008 shared task, we adopt a standard 
three-stage process for SRL: pruning, argument 
identification and argument classification. To 
model the difference between verb and noun 
predicates, we carry out separate training and 
testing for verb and noun predicates respectively. 
In addition, we adopt OpenNLP maximum en-
tropy package 1  in argument identification and 
classification. 
2.1 Predicate identification 
Most of Previous SRL systems only consider 
given predicates. However, predicates are not 
given in CoNLL 2008 shared task and required 
to be determined automatically by the system. 
Therefore, the first step of the shared task is to 
identify the verb and noun predicates in a sen-
tence. Due to time limitation, a simple algorithm 
is developed to identify noun and verb predicates: 
1) For the WSJ corpus, we simply adopt the 
annotations provided by PropBank and 
NomBank. That is, we only consider the verb 
and noun predicates annotated in PropBank 
and NomBank respectively.  
2) For the Brown corpus, verb predicates are 
identified simply according to its POS tag 
and noun predicates are determined using a 
simple method that only those nouns which 
can also be used as verbs are identified. To 
achieve this goal, an English lexicon of about 
56K word is applied to identify noun predi-
cates.  
Evaluation on the test set of CoNLL 2008 
shared task shows that our simple predicate iden-
tification algorithm achieves the accuracies of 
98.6% and 92.7 in the WSJ corpus for verb and 
noun predicates respectively, with overall accu-
racy of 95.5%, while it achieves the accuracies of 
73.5% and 43.1% in the Brown corpus for verb 
and noun predicates respectively with overall 
accuracy of 61.8%. This means that the perform-
ance of predicate identification in the Brown 
corpus is much lower than the one in the WSJ 
corpus. This further suggests that much work is 
required to achieve reasonable predicate identifi-
cation performance in future work. 
2.2 Preprocessing 
Using the dependency relations returned by a 
dependency parser (either MaltParser or 
                                                 
1https://sourceforge.net/project/showfiles.php?group_id=59
61 
MSTParser in this paper), we can construct cor-
responding dependency tree for a given sentence. 
For example, Figure 1 shows the dependency 
tree of the sentence ?Meanwhile, overall evi-
dence on the economy remains fairly clouded.?. 
Here, W is composed of two parts: word and its 
POS tag with ?/? as a separator while R means a 
dependency relation and ARG represents a se-
mantic role. 
In Hacioglu (2004), a simple pruning algo-
rithm is applied to filter out unlikely dependency 
relation nodes in a dependency tree by only 
keeping the parent/children/grand-children of the 
predicate, the siblings of the predicates, and the 
children/grandchildren of the siblings. This paper 
extends the algorithm a little bit by including the 
nodes two more layers upward and downward 
with regard to the predicate?s parent, such as the 
predicate?s grandparent, the grandparent?s chil-
dren and the grandchildren?s children. For the 
example as shown in Figure 1, all the nodes in 
the entire tree are kept. Evaluation on the training 
set shows that our pruning algorithm signifi-
cantly reduces the training instances by 76.9%. 
This is at expanse of wrongly pruning 1.0% se-
mantic arguments for verb predicates. However, 
this figure increases to 43.5% for noun predicates 
due to our later observation that about half of 
semantic arguments of noun predicates distrib-
utes over ancestor nodes out of our consideration. 
This suggests that a specific pruning algorithm is 
necessary for noun predicates to include more 
ancestor nodes. 
2.3 Features 
Some of the features are borrowed from Ha-
cioglu (2004) with some additional features mo-
tivated by constituent structure tree-based SRL 
(Pradhan et al2005; Xue and Palmer, 2004). In 
the following, we explain these features and give 
examples with regard to the dependency tree as 
shown in Figure 1. We take the word evidence in 
Figure 1 as the predicate and the node ?on? as 
the node on focus.  
The following eight basic features are moti-
vated from constituent structure tree-based SRL:  
1)  Predicate: predicate lemma. (evidence) 
2) Predicate POS: POS of current predicate. 
(NN) 
3)  Predicate Voice: Whether the predicate (verb) 
is realized as an active or passive construc-
tion. If the predicate is a noun, the value is 
null and presented as ?_?. ( _ ) 
 
254
 
Figure 1. Example of a dependency tree augmented with semantic roles  
for the given predicate evidence. 
 
4)  Relation type: the dependency relation type 
of the current node. (NMOD) 
5) Path: the chain of relations from current rela-
tion node to the predicate. (NMOD->SBJ) 
6) Sub-categorization: The relation type of 
predicate and the left-to-right chain of the re-
lation label sequence of the predicate?s chil-
dren. (SBJ->NMOD-NMOD) 
7)  Head word: the head word in the relation, 
that is, the headword of the parent of the cur-
rent node. (evidence) 
8)  Position: the position of the headword of the 
current node with respect to the predicate po-
sition in the sentence, which can be before, 
after or equal. (equal) 
Besides, we also include following additional 
features borrowed from Hacioglu (2004): 
1) Family membership: the relationship be-
tween current node and the predicate node in 
the family tree, such as parent, child, sibling. 
(child) 
2)  Dependent word: the modifying word in the 
relation, that is, the word of current node. (on) 
3) POS of headword: the POS tag of the head-
word of current word. (NN) 
4)  POS of dependent word: the POS tag of cur-
rent word. (IN) 
5)  POS pattern of predicate's children: the 
left-to-right chain of the POS tag sequence of 
the predicate?s children. (JJ-IN) 
6)  Relation pattern of predicate?s children: 
the left-to-right chain of the relation label se-
quence of the predicate?s children. (NMOD-
NMOD) 
7)  POS pattern of predicate?s siblings: the 
left-to-right chain of the POS tag sequence of 
the predicate?s siblings. (RB-.-VBN-.) 
8)  Relation pattern of predicate?s siblings: the 
left-to-right chain of the relation label se-
quence of the predicate?s siblings. (TMP-P-
PRD-P) 
3 System Performance 
All  the training data are included in our system, 
which costs 70 minutes in training and 5 seconds 
on testing on a PC platform with a Pentium D 
3.0G CPU and 2G Memory. In particular, the 
argument identification stage filters out those 
nodes whose probabilities of not being semantic 
arguments are more than 0.98 for verb and noun 
predicates. 
   Labeled 
Macro F1 
Labeled 
F1 
LAS 
Test WSJ 78.39 70.41 85.50
Test Brown 59.89 42.67 77.06
Test WSJ+Brown 76.19 67.12 84.56
Table 1: Official performance using MaltParser 
(with the SRL model trained and tested on the 
automatic output of MaltParser) 
 
All the performance is returned on the test set 
using the CoNLL 2008 evaluation script 
eval08.pl provided by the organizers. Table 1 
shows the official performance using MaltParser 
(with the SRL model trained and tested on the 
automatic output of MaltParser provided by the 
task organizers) as the dependency parser. It 
shows that our system performs well on the WSJ 
corpus and badly on the Brown corpus largely 
due to bad performance on predicate identifica-
tion.  
4 Post-evaluation System 
To gain more insights into dependency tree-
based SRL, we improve the system with a new 
255
pruning algorithm and additional features, after 
submitting our official results. 
4.1 Effective pruning 
Our new pruning algorithm is motivated by the 
one proposed by Xue and Palmer (2004), which  
only keeps those siblings to a node on the path 
from current predicate to the root are included, 
for constituent structure tree-based SRL. Our 
pruning algorithm further cuts off the nodes 
which are not related with the predicate. Besides, 
it filters out those nodes which are punctuations 
or with ?symbol? dependency relations. Evalua-
tion on the Brown corpus shows that our pruning 
algorithm significantly reduces the training data 
by 75.5% at the expense of wrongly filtering out 
0.7% and 0.5% semantic arguments for verb and 
noun predicates respectively. This suggests that 
our new pruning algorithm significantly performs 
better than the old one in our official system, es-
pecially for the identification of noun predicates. 
Furthermore, the argument identification stage 
filters out those nodes whose probabilities of not 
being semantic arguments are more than 0.90 
and 0.85 for verb and noun predicates respec-
tively, since we that our original threshold of 
0.98 in the official system is too reserved. 
Finally, those rarely-occurred semantic roles 
which occur less than 200 in the training set are 
filtered out and thus not considered in our system, 
such as A5, AA, C-A0, C-AM-ADV, R-A2 and SU. 
4.2 Extensive Feature Engineering 
Motivated by constituent structure tree-based 
SRL, two more combined features are considered 
in our post-evaluation system:  
1) Predicate + Headword: (evidence + remain) 
2) Headword + Relation: (remain + Root) 
In order to better evaluate the contribution of 
various additional feature, we build a baseline 
system using hand-corrected dependency rela-
tions and the eight basic features, motivated by 
constituent structure tree-based SRL, as de-
scribed in Section 2.3. Table 2 shows the effect 
of various additional features by adding one in-
dividually to the baseline system. It shows that 
the feature of dependent word is most useful, 
which improves the labeled F1 score from 
81.38% to 84.84%. It also shows that the two 
features about predicate?s sibling deteriorate the 
performance. Therefore, we delete these two fea-
tures from remaining experiments. Although the 
combined feature of ?predicate+head word? is 
useful in constituent structure tree-based SRL, it 
slightly decrease the performance in dependency 
tree-based SRL. For convenience, we include it 
in our system. 
 P R F1 
Baseline 84.31 78.64 81.38
+ Family membership 84.70 78.87 81.68
+ Dependent word  86.74 83.01 84.84
+ POS of headword 84.44 78.55 81.38
+ POS of dependent 
word 
84.42 78.33 81.47
+ POS pattern of 
predicate's children 
84.35 78.73 81.47
+ Relation pattern of 
predicate?s children 
84.75 78.97 81.76
+ Relation pattern of 
predicate?s siblings 
84.29 78.52 81.30
+ POS pattern of 
predicate?s siblings 
83.75 78.32 80.95
+ Predicate  +  Head-
word 
83.30 78.94 81.30
+Headword + Relation 84.66 79.37 81.93
Table 2: Effects of various additional features 
4.3 Best performance 
Table 3 shows our system performance after ap-
plying above effective pruning strategy and addi-
tional features using the default MaltParser. Ta-
ble 3 also reports our performance using the 
state-of-the-art MSTParser. To show the impact 
of predicate identification in dependency tree-
based SRL, Table 4 report the performance on 
gold predicate identification, i.e. only using an-
notated predicates in the corpora. 
Comparison of Table 1 and Table 3 using the 
MaltParser shows that our new extension with 
effective pruning and extensive engineering sig-
nificantly improves the performance. It also 
shows that MSTParser-based SRL performs 
slightly better than MaltParser-based one, much 
less than the performance difference on depend-
ency parsing between them. This suggests that 
such difference between these two state-of-the-
art dependency parsers does not much affect cor-
responding SRL systems. This is also confirmed 
by the results in Table 4. 
Comparison of Table 3 and Table 4 in labeled 
F1 on the Brown test data shows that the system 
with gold predicate identification significantly 
outperforms the one with automatic predicate 
identification using our simple algorithm by 
about 22 in labeled F1. This suggests that the 
performance of predicate identification is critical 
to SRL.  
256
 
MSTParser MaltParser  
Labeled Macro 
F1 
Labeled F1 LAS Labeled Macro 
F1 
Labeled F1 LAS 
Test WSJ 84.50 81.95 87.01 83.69 81.82 85.50
Test Brown 67.61 53.69 81.46 65.09 53.03 77.06
Test 
WSJ+Brown 82.53 78.64 86.39 81.52 78.45 84.56
Table 3: Unofficial performance using MSTParser and MaltParser 
 with predicates automatically identified 
 
MSTParser MaltParser  
Labeled Macro 
F1 
Labeled F1 LAS Labeled Macro 
F1 
Labeled F1 LAS 
Test WSJ 84.75 82.45 87.01 84.04 82.52 85.50
Test Brown 78.31 75.07 81.46 75.72 74.28 77.06
Test 
WSJ+Brown 84.05 81.66 86.39 83.13 81.64 84.56
Table 4: Unofficial performance using MSTParser and MaltParser with gold predicate identification 
 
5 Conclusions 
This paper presents a dependency tree-based 
SRL system by proper pruning and extensive 
feature engineering. Evaluation on the CoNLL 
2008 shared task shows that proper pruning and 
extensive feature engineering contributes much. 
It also shows that SRL heavily depends on the 
performance of predicate identification. 
In future work, we will explore better ways in 
predicate identification. In addition, we will ex-
plore more on dependency parsing and further 
joint learning on syntactic and semantic parsing. 
Acknowledgment 
This research is supported by Project 60673041 
under the National Natural Science Foundation 
of China and Project 2006AA01Z147 under the 
?863? National High-Tech Research and Devel-
opment of China.  
References 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational Lin-
guistics, 28:3, pages 245-288. 
Gildea, Daniel and Martha Palmer. 2002. The Neces-
sity of Syntactic Parsing for Predicate Argument 
Recognition. In Proceedings of the 40th  Associa-
tion for Computational Linguistics,  2002.  
Hacioglu, Kadri. 2004. Semantic Role Labeling Using 
Dependency Trees. In Proceedings of the Interna-
tional Conference on Computational Linguistics 
(COLING). 2004. 
McDonald, Ryan, Fernando Pereira, Kiril Ribarov, 
Jan Haji?. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In the pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in 
Natural Language Processing, 2005 
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings 
of the 12th Conference on Computational Natural 
Language Learning (CoNLL-2008). 
Nivre, Joakim and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of 
the 43rd Annual Meeting of the Association for 
Computational Linguistics, pp. 99-106, 2005 
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, 
James H. Martin, Dan Jurafsky. 2004. Shallow 
Semantic Parsing Using Support Vector Machines. 
In Proceedings of (HLT-NAACL-2004), 2004. 
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, 
James H. Martin, Dan Jurafsky. 2005. Semantic 
role labeling using different syntactic views. In 
Proceedings of the 43rd  Association for Computa-
tional Linguistics (ACL-2005), 2005. 
Punyakanok, Vasin, Peter Koomen, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. In Pro-
ceedings of 9th Conference on Computational 
Natural Language Learning (CoNLL-2005).2005 
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Proceedings 
of Conference on Empirical Methods in Natural 
Language Processing (EMNLP), 2004. 
257
Jumping Distance based Chinese Person Name Disambiguation1
Yu Hong  Fei Pei  Yue-hui Yang  Jian-min Yao  Qiao-ming Zhu 
School of Computer Science and Technology, Soochow University 
No.1 Shizi street, Suzhou City, Jiansu Province, China 
{hongy, 20094527004, 0727401137, jyao, qmzhu}@suda.edu.cn
Abstract
In this paper, we describe a Chinese person 
name disambiguation system for news articles 
and report the results obtained on the data set of 
the CLP 2010 Bakeoff-31. The main task of the 
Bakeoff is to identify different persons from the 
news stories that contain the same person-name 
string. Compared to the traditional methods, 
two additional features are used in our system: 
1) n-grams co-occurred with target name string; 
2) Jumping distance among the n-grams. On the 
basis, we propose a two-stage clustering algo-
rithm to improve the low recall.
1   Our Novel Try
For this task, we propose a Jumping-Distance 
based n-gram model (abbr. DJ n-gram) to de-
scribe the semantics of the closest contexts of 
the target person-name strings. 
The generation of the DJ n-gram model 
mainly involves two steps. First, we mine the 
Jumping tree for the target string; second, we 
give the statistical description of the tree. 
z Jumping Tree 
Given a target string, we firstly extract the 
sentence where it locates as its closest context. 
Then we segment the sentence into n-
grams(Chen et al ,2009) (only Bi-gram and Tri-
gram are used in this paper). For each n-gram, 
we regard it as the beginning of a jumping jour-
ney. And the places where we jump are the sen-
tences which involve the n-gram. By the same 
way, we segment the sentences into n-grams 
which will be regarded as the new beginnings to 
open further jumping. The procedure will run 
iteratively until there are no sentences in the 
document (viz. the document which involves 
the target string) can be used to jump. Actually, 
we find there are only 3 jumps in average in our 
previous test and simultaneously 11 sentences 
in a document can be involved into the jumping 
journey. Thus, we can obtain a Jumping Tree 
where each jumping route from the initially n-
gram (viz. the gram in the closes context) refer 
to a branch. And for each intermediate node, its 
child-nodes are the n-grams co-occurred with it 
in the same sentences. 
The motivation to generate the Jumping Tree 
is to imitate the thinking model of human rec-
ognizing the word senses and semantics. In de-
tail, for each intermediate node of the tree, its 
child-nodes all come from its closest contexts, 
especially the nodes co-occur with it in the 
same sentences which involve the real grammar 
and semantic relations. Thus the child-nodes 
normally provide the natural inference for its 
word sense. For example, given the string 
?SARS?, we can deduce its sense from its child 
nodes ?Severe?, ?Acute?, ?Respiratory? and 
?Syndromes? even if we see the string for the 
first time. On the basis, the procedure of infer-
ence run iteratively, that is, the tree always use 
the child nodes deduce the meaning of their fa-
ther nodes then further ancestor nodes until the 
root. Thus the tree acts as a hierarchical under-
standing procedure. Additionally, the distances 
among nodes in the tree give the degree of se-
mantic relation.  
In the task of person-name disambiguation, 
we use the Jumping Tree to deduce the identi-
ties and backgrounds of a person. Each branch 
of the tree refers to a property of the person. 
z Jumping-Distance based n-gram model 
In this paper, we give a simple statistical 
model to describe the Jumping Tree. Given a 
node in the tree (viz. an n-gram), we record the Supported by the National Natural Science Foundation 
of China under Grant No. 60970057, No.60873105.
steps jumping from the root to it, viz. the depth 
of the node in the tree. Then based on the priori-
trained TFIDF value, we calculate the genera-
tion probability of the node as follows: 
depth
TFP D? 
where the D  denotes the smoothing factor.
In fact, we create more comprehensive mod-
els to describe the semantic correlations among 
the nodes in the Jumping Tree. The models well 
use the distances among the nodes in local 
Jumping Tree (viz. the tree generated based on 
the test document) and that normalized on the 
large-scale training data to calculate the prob-
ability of n-grams correboratively generate a 
semantics. They try to imitate the thinking 
model of human combine differents features to 
understand panoramic knowledge. In the task of 
name disambiguation, we can use the models to 
improve the distinguishment of different per-
sons who have the same name. And we have 
illustrate the well effectiveness on the topic de-
scription and relevance measurement in other 
tasks, such as Link Detection. But we actually 
didn?t use the models to perform the task of 
name diaambiguation this time with the aim to 
purely evaluate the usefulness of the Jumping 
Tree.
2    Systems
For the task of Chinese person name disam-
biguation, we submitted two systems as follows: 
z System1 
The system involves two main components: 
DJ-based name Identification error detection 
and DJ-based person name disambiguation. 
The first component, viz. DJ-based name 
segmentation error detection, aims to distin-
guish the target string referring to person name 
from that referring to something else. Such as, 
the string ???? can be a person name ?Hai 
Huang? but also a name of sea ?the Yellow 
Sea?. And the detection component focuses on 
obtaining the pure person name ?Hai Huang?. 
The detection component firstly establish two 
classes of features which respectively describe 
the nature of human and that of things. Such as, 
the features ?professor?, ?research?, ?honest? et 
al., can roughly be determined as the nature of 
human, and conversely the features ?solid?, 
?collapse?, ?deep? et al, can be that of things. 
For obtaining the features, we extract 10,000 
documents that discuss person, eg. ?Albert Ein-
stein? and 6000 documents that discuss tech-
nology, science, geography, et.al., from 
Wikipedia2. For each document, we generate its 
Jumping Tree, and regard the nodes in the tree 
as the features. After that, we combine the 
weights of the same features and normalized the 
value by dividing that by the average weight in 
the specific class of features. 
Based on the two classes of features, given a 
target string and the document where it occurs, 
the detection component firstly generate the 
Jumping Tree of the document, and then deter-
mines whether the string is person name or 
things by measuring the similarity of  the tree to 
the classes of features. Here, we simply use the 
VSM and Cosine metric ?Bagga and Baldwin, 
1998? to obtain the similarity. 
The second component, viz. DJ-based person 
name disambiguation, firstly generates the 
Jumping trees for all documents that involve 
specific person name. And a two-stage cluster-
ing algorithm is adopted to divide the docu-
ments and refer each cluster to a person. The 
first stage of the algorithm runs a strict division 
which focuses on obtaining high precision. The 
second stage performs a soft division which is 
used to improve recall. The two-stage clustering 
algorithm(Ikeda et al,2009) initially obtains the 
optimal parameters that respectively refer to the 
maximum precision and recall based on training 
data, and then regards a statistical tradeoff as 
the final value of the parameters. Here, the Af-
finity Propagation clustering tools (Frey BJ and 
Dueck D, 2007) is in use. 
z System2 
The system is similar to the system1 except 
that it additionally involve Named Entity Identi-
fication (Artiles et.al,2009B; Popescu,O. and 
Magnini, B.,2007)before the two-stage cluster-
ing in the component of person name disam-
biguation. In detail, given a person name and 
the documents that it occurs in, the disambigua-
tion component of System2 firstly adopt NER 
CRF++ toolkit3  provided by MSRA to identify 
Named Entities(Chen et al, 2006) that involve 
the given name string, such as the entity ???
?? (viz. Gao-ming Li in English) when given 
the target name string ????(viz. Ming Gao in 
English). Thus the documents can be roughly 
divided into different clusters of Named Entities 
without name segmentation errors. After that, 
we additionally adopt the two-stage clustering 
algorithm to further divide each cluster. Thus 
we can deal with the issue of disambiguation 
without the interruption of name segmentation 
errors.
3   Data sets 
z Training dataset: They contain about 30 
Chinese personal names, and a document set of 
about 100-300 news articles from collection of 
Xinhua news documents in a time span of four-
teen years are provided for each personal name. 
z External dataset: Chinese Wikipedia2 per-
sonal attribution (Cucerzan, 2007; Nguyen and 
Cao,2008).
z Test dataset: There are about 26 Chinese 
personal names, which are similar to train data 
sets.
4     Experiments 
The systems that run on test dataset are evalu-
ated by both B-Cubed (Bagga and  Baldwin, 
1998; Artiles et al,2009A) and P-IP (Artiles  et 
al., 2007 ;Artiles et al,2009A). And the systems 
that run on training dataset were only evaluated 
by B-Cubed. 
In experiments, we firstly evaluate the per-
formance of name segmentation error detection 
on the training dataset. For comparison, we ad-
ditionally perform another detection method 
which only using Name Entity Identifcation 
(NER CRF++ tools) to distinguish name-strings 
from the discarded ones. The results are shown 
in table 1. We can find that our error detection 
method can achieve more recall than NER, but 
lower precision. 
Besides, we evaluate the performance of the 
two-stage clustering in the component of name 
disambiguation step by step. Four steps are in 
use to evaluate the first-stage clustering method 
as follows: 
z DJ2
This step look like to run the system1 men-
tionedin in section 3 which don?t involve the 
prior-division of documents by using NER be-
fore the first-stage clustering in the component 
of name disambiguation. Especially it don?t 
perform the second-stage clustering to improve 
the recall probability. 
z DJ2+NER
This step is similar to the step of DJ2 men-
tioned above except that it perform the prior-
divison of documents by using NER. 
z NER+DJ 
This step is also similar to the step of DJ2 ex-
cept that its name segmentation error detection 
performs by using the NER. 
z NER2+DJ
This step is similar to the step of NER+DJ 
except that it involve the treatment of prior-
divison as that in DJ2+NER.
The performances of the four steps are shown 
in table 2. We can find that all steps achieve 
poor recall. And the step of DJ2 achieve the best 
F-score although it don?t involve the prior-
division. That is because NER is helpful to im-
prove precision but not recall, as shown in table 
1. Conversely, DJ2 can avoid the bias caused by 
the procedure of greatly maximizing the preci-
sion.
P recall F-score
DJ-based 0.62 0.81 0.70
NER-based 0.91 0.77 0.71
Table 1: Performance of name segmentation 
error detection 
P IP F-score
DJ2 80.49 53.85 60.12 
DJ2+NER 88.56 51.30 59.02 
NER+DJ 93.27 46.78 57.44 
NER2+DJ 97.79 42.13 55.47 
Table 2: Performances of the-stage clustering 
Additionally, another two steps are used to 
evluate the both two stages of clustering in 
name disambiguation. The steps are as follows: 
z DJ2+NER_2
This step is similar to the step of DJ2+NER 
except that it additionally run the second-stage 
clustering to improve recall. 
z NER2+DJ_2
This step also run the second-stage clustering 
on the basis of NER2+DJ. 
The performances of the two step are shown 
in table 3. We can find that the F-scores both 
have been improved substantially. And the two 
steps still maintain the original distribution be-
tween precision and recall. That is, the 
DJ2+NER_2, which has outperformance on re-
call in the name segmentation error detection, 
still maintain the higher recall at the second-
stage clustering. And NER2+DJ_2 also main-
tains higher precision. This illustrates that the 
clustering has no ability to remedy the short-
comings of NER in the prior-division. 
P IP F-score 
DJ2+NER_2 82.65 63.40    66.59 
NER2+DJ_2 87.71 60.45 66.23 
Table 3: Performances of two-stage clustering 
The test results of the two systems mentioned in 
section 3 are shown in the table 4. We also 
show the performances of each stage clustering 
as that on training dataset. We can find that the 
poor performance mainly come from the low 
recall, which illustrates that the DJ-based n-
gram disambiguation is not robust. 
B-Cubed
precision recall F-Score
System1(one 
t )
85.26 28.43 37.74
System1(both 
t )
84.51 44.17 51.42
P-IP
P IP F-Score
System2(one 
t )
88.4 39.47 50.52
System2(both 
t )
88.36 55.23 63.89
Table 4 :Test results 
5.Conclusions
In this paper, we report a hybrid Chinese per-
sonal disambiguation system and a novel algo-
rithm for extract useful global n-gram features 
from the context .Experiment showed that our 
algorithm performed high precision and poor 
recall. Furthermore, two-stage clustering can 
handl a change in the one-stage clustering algo-
rithm, especially for recall score. In the future, 
we will investigate global new types of features 
to improve the recall score and local new types 
of features to improve the precision score. For 
instance, the location and organization besides 
the person in the named-entities. And we try to 
use Hierarchical Agglomerative Clustering al-
gorithm to help raise the recall score.
References 
Artiles J, J Gonzalo and S Sekine. 2007. The 
SemEval-2007 WePS Evaluation: ?Establish-
ing a benchmark for the Web People Search 
Task.?, The SemEval-2007, 64-69, Associa-
tion for Computational Linguistics.
Artiles Javier, Julio Gonzalo and Satoshi Se-
kine.2009A. ?WePS 2 Evaluation Campaign: 
overview of the Web People Search Cluster-
ing Task,? In 2nd Web People Search 
Evaluation Workshop (WePS 2009), 18th 
WWW Conference. 
Artiles J, E Amig?o and J Gonzalo. 2009B.The 
Role of Named Entities in Web People 
Search. Proceedings of the 2009 Conference 
on Empirical Methods Natural Language 
Processing, 534?542,Singapore, August 2009.  
Bagga A and Baldwin B. 1998. Entity-based 
cross-document coreferenceing using the 
Vector Space Model.Proceedings of the 17th
international conference on computational 
linguistics. Volume 1, 79-85. 
Chen,Ying., Sophia Yat., Mei Lee and Chu-Ren 
Huang. 2009. PolyUHK:A Roubust Informa-
tion Extraction System for Web Personal 
Names In 2nd Web People Search Evaluation 
Workshop (WePS 2009), 18th WWW Con-
ference.
Chen Wen-liang, Zhang Yu-jie. 2006. Chinese 
Named Entity Recognition with Conditional 
Random Fields. Proceedings of the Fifth 
SIGHAN Workshop on Chinese Language 
Processing.
Cucerzan, Silviu. 2007. Large scale named en-
tity Disambiguation based on Wikipedia data. 
In The EMNLP-CoNLL-2007. 
Frey BJ and Dueck D. 2007. Clustering by 
Passing Messages Between Data 
Points .science, 2007 - sciencemag.org. 
Ikeda MS, Ono I, Sato MY and Nakagawa H. 
2009. Person Name disambiguation on the 
Web by Two-Stage Clustering. In 2nd Web 
People Search Evaluation Workshop(WePS 
2009),18th WWW Conference.
Popescu,O and Magnini, B. 2007. IRST-
BP:Web People Search Using Name Enti-
ties.Proceeding s of the 4th International 
Workshop on Semantic Evaluations (SemE-
val-2007), 195-198, Prague June 2007. Asso-
ciation for Computational Linguistics. 
