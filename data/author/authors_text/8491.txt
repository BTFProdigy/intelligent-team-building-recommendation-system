Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 448?457, Prague, June 2007. c?2007 Association for Computational Linguistics
Enhancing Single-document Summarization by Combining RankNet and
Third-party Sources
Krysta M. Svore
Microsoft Research
1 Microsoft Way
Redmond, WA 98052
ksvore@microsoft.com
Lucy Vanderwende
Microsoft Research
1 Microsoft Way
Redmond, WA 98052
Christopher J.C. Burges
Microsoft Research
1 Microsoft Way
Redmond, WA 98052
Abstract
We present a new approach to automatic
summarization based on neural nets, called
NetSum. We extract a set of features from
each sentence that helps identify its impor-
tance in the document. We apply novel
features based on news search query logs
and Wikipedia entities. Using the RankNet
learning algorithm, we train a pair-based
sentence ranker to score every sentence in
the document and identify the most impor-
tant sentences. We apply our system to
documents gathered from CNN.com, where
each document includes highlights and an
article. Our system significantly outper-
forms the standard baseline in the ROUGE-1
measure on over 70% of our document set.
1 Introduction
Automatic summarization was first studied almost
50 years ago by Luhn (Luhn, 1958) and has contin-
ued to be a steady subject of research. Automatic
summarization refers to the creation of a shortened
version of a document or cluster of documents by
a machine, see (Mani, 2001) for details. The sum-
mary can be an abstraction or extraction. In an ab-
stract summary, content from the original document
may be paraphrased or generated, whereas in an ex-
tract summary, the content is preserved in its original
form, i.e., sentences. Both summary types can in-
volve sentence compression, but abstracts tend to be
more condensed. In this paper, we focus on produc-
ing fully automated single-document extract sum-
maries of newswire articles.
To create an extract, most automatic systems use
linguistic and/or statistical methods to identify key
words, phrases, and concepts in a sentence or across
single or multiple documents. Each sentence is then
assigned a score indicating the strength of presence
of key words, phrases, and so on. Sentence scoring
methods utilize both purely statistical and purely se-
mantic features, for example as in (Vanderwende et
al., 2006; Nenkova et al, 2006; Yih et al, 2007).
Recently, machine learning techniques have been
successfully applied to summarization. The meth-
ods include binary classifiers (Kupiec et al, 1995),
Markov models (Conroy et al, 2004), Bayesian
methods (Daume? III and Marcu, 2005; Aone et al,
1998), and heuristic methods to determine feature
weights (Schiffman, 2002; Lin and Hovy, 2002).
Graph-based methods have also been employed
(Erkan and Radev, 2004a; Erkan and Radev, 2004b;
Mihalcea, 2005; Mihalcea and Tarau, 2005; Mihal-
cea and Radev, 2006).
In 2001?02, the Document Understanding Con-
ference (DUC, 2001), issued the task of creat-
ing a 100-word summary of a single news article.
The best performing systems (Hirao et al, 2002;
Lal and Ruger, 2002) used various learning and
semantic-based methods, although no system could
outperform the baseline with statistical significance
(Nenkova, 2005). After 2002, the single-document
summarization task was dropped.
In recent years, there has been a decline in stud-
ies on automatic single-document summarization,
in part because the DUC task was dropped, and in
part because the task of single-document extracts
may be counterintuitively more difficult than multi-
448
document summarization (Nenkova, 2005). How-
ever, with the ever-growing internet and increased
information access, we believe single-document
summarization is essential to improve quick ac-
cess to large quantities of information. Recently,
CNN.com (CNN.com, 2007a) added ?Story High-
lights? to many news articles on its site to allow
readers to quickly gather information on stories.
These highlights give a brief overview of the arti-
cle and appear as 3?4 related sentences in the form
of bullet points rather than a summary paragraph,
making them even easier to quickly scan.
Our work is motivated by both the addition of
highlights to an extremely visible and reputable on-
line news source, as well as the inability of past
single-document summarization systems to outper-
form the extremely strong baseline of choosing the
first n sentences of a newswire article as the sum-
mary (Nenkova, 2005). Although some recent sys-
tems indicate an improvement over the baseline (Mi-
halcea, 2005; Mihalcea and Tarau, 2005), statistical
significance has not been shown. We show that by
using a neural network ranking algorithm and third-
party datasets to enhance sentence features, our sys-
tem, NetSum, can outperform the baseline with sta-
tistical significance.
Our paper is organized as follows. Section 2 de-
scribes our two studies: summarization and high-
light extraction. We describe our dataset in detail in
Section 3. Our ranking system and feature vectors
are outlined in Section 4. We present our evaluation
measure in Section 5. Sections 6 and 7 report on our
results on summarization and highlight extraction,
respectively. We conclude in Section 8 and discuss
future work in Section 9.
2 Our Task
In this paper, we focus on single-document summa-
rization of newswire documents. Each document
consists of three highlight sentences and the article
text. Each highlight sentence is human-generated,
but is based on the article. In Section 4 we discuss
the process of matching a highlight to an article sen-
tence. The output of our system consists of purely
extracted sentences, where we do not perform any
sentence compression or sentence generation. We
leave such extensions for future work.
We develop two separate problems based on our
document set. First, can we extract three sentences
that best ?match? the highlights as a whole? In
this task, we concatenate the three sentences pro-
duced by our system into a single summary or block,
and similarly concatenate the three highlight sen-
tences into a single summary or block. We then
compare our system?s block against the highlight
block. Second, can we extract three sentences that
best ?match? the three highlights, such that order-
ing is preserved? In this task, we produce three sen-
tences, where the first sentence is compared against
the first highlight, the second sentence is compared
against the second highlight, and the third sentence
is compared against the third highlight. Credit is
not given for producing three sentences that match
the highlights, but are out of order. The second task
considers ordering and compares sentences on an in-
dividual level, whereas the first task considers the
three chosen sentences as a summary or block and
disregards sentence order. In both tasks, we assume
the title has been seen by the reader and will be listed
above the highlights.
3 Evaluation Corpus
Our data consists of 1365 news documents gathered
from CNN.com (CNN.com, 2007a). Each document
was extracted by hand, where a maximum of 50
documents per day were collected. The documents
were hand-collected on consecutive days during the
month of February.
Each document includes the title, timestamp,
story highlights, and article text. The timestamp
on articles ranges from December 2006 to Febru-
ary 2007, since articles remain posted on CNN.com
for up to several months. The story highlights are
human-generated from the article text. The number
of story highlights is between 3?4. Since all articles
include at least 3 story highlights, we consider only
the task of extracting three highlights from each ar-
ticle.
4 Description of Our System
Our goal is to extract three sentences from a single
news document that best match various characteris-
tics of the three document highlights. One way to
identify the best sentences is to rank the sentences
449
TIMESTAMP: 1:59 p.m. EST, January 31, 2007
TITLE: Nigeria reports first human death from bird flu
HIGHLIGHT 1: Government boosts surveillance after woman dies
HIGHLIGHT 2: Egypt, Djibouti also have reported bird flu in humans
HIGHLIGHT 3: H5N1 bird flu virus has killed 164 worldwide since 2003
ARTICLE: 1. Health officials reported Nigeria?s first cases of bird flu in humans on Wednesday,
saying one woman had died and a family member had been infected but was responding to
treatment. 2. The victim, a 22-year old woman in Lagos, died January 17, Information Minister
Frank Nweke said in a statement. 3. He added that the government was boosting surveillance
across Africa?s most-populous nation after the infections in Lagos, Nigeria?s biggest city. 4.
The World Health Organization had no immediate confirmation. 5. Nigerian health officials
earlier said 14 human samples were being tested. 6. Nweke made no mention of those cases on
Wednesday. 7. An outbreak of H5N1 bird flu hit Nigeria last year, but no human infections had
been reported until Wednesday. 8. Until the Nigerian report, Egypt and Djibouti were the only
African countries that had confirmed infections among people. 9. Eleven people have died in
Egypt. 10. The bird flu virus remains hard for humans to catch, but health experts fear H5N1
may mutate into a form that could spread easily among humans and possibly kill millions in
a flu pandemic. 11. Amid a new H5N1 outbreak reported in recent weeks in Nigeria?s north,
hundreds of miles from Lagos, health workers have begun a cull of poultry. 12. Bird flu is
generally not harmful to humans, but the H5N1 virus has claimed at least 164 lives worldwide
since it began ravaging Asian poultry in late 2003, according to the WHO. 13. The H5N1 strain
had been confirmed in 15 of Nigeria?s 36 states. 14. By September, when the last known case
of the virus was found in poultry in a farm near Nigeria?s biggest city of Lagos, 915,650 birds
had been slaughtered nationwide by government veterinary teams under a plan in which the
owners were promised compensation. 15. However, many Nigerian farmers have yet to receive
compensation in the north of the country, and health officials fear that chicken deaths may be
covered up by owners reluctant to slaughter their animals. 16. Since bird flu cases were first
discovered in Nigeria last year, Cameroon, Djibouti, Niger, Ivory Coast, Sudan and Burkina
Faso have also reported the H5N1 strain of bird flu in birds. 17. There are fears that it has
spread even further than is known in Africa because monitoring is difficult on a poor continent
with weak infrastructure. 18. With sub-Saharan Africa bearing the brunt of the AIDS epidemic,
there is concern that millions of people with suppressed immune systems will be particularly
vulnerable, especially in rural areas with little access to health facilities. 19. Many people keep
chickens for food, even in densely populated urban areas.
Figure 1: Example document containing highlights
and article text. Sentences are numbered by their
position. Article is from (CNN.com, 2007b).
using a machine learning approach, for example as
in (Hirao et al, 2002). A train set is labeled such
that the labels identify the best sentences. Then a
set of features is extracted from each sentence in the
train and test sets, and the train set is used to train
the system. The system is then evaluated on the test
set. The system learns from the train set the distri-
bution of features for the best sentences and outputs
a ranked list of sentences for each document. In this
paper, we rank sentences using a neural network al-
gorithm called RankNet (Burges et al, 2005).
4.1 RankNet
From the labels and features for each sentence, we
train a model that, when run on a test set of sen-
tences, can infer the proper ranking of sentences
in a document based on information gathered dur-
ing training about sentence characteristics. To ac-
complish the ranking, we use RankNet (Burges et
al., 2005), a ranking algorithm based on neural net-
works.
RankNet is a pair-based neural network algorithm
used to rank a set of inputs, in this case, the set
of sentences in a given document. The system is
trained on pairs of sentences (Si, Sj), such that Si
should be ranked higher or equal to Sj . Pairs are
generated between sentences in a single document,
not across documents. Each pair is determined from
the input labels. Since our sentences are labeled us-
ing ROUGE (see Section 4.3), if the ROUGE score
of Si is greater than the ROUGE score of Sj , then
(Si, Sj) is one input pair. The cost function for
RankNet is the probabilistic cross-entropy cost func-
tion. Training is performed using a modified version
of the back propagation algorithm for two layer nets
(Le Cun et al, 1998), which is based on optimiz-
ing the cost function by gradient descent. A simi-
lar method of training on sentence pairs in the con-
text of multi-document summarization was recently
shown in (Toutanova et al, 2007).
Our system, NetSum, is a two-layer neural net
trained using RankNet. To speed up the performance
of RankNet, we implement RankNet in the frame-
work of LambdaRank (Burges et al, 2006). For de-
tails, see (Burges et al, 2006; Burges et al, 2005).
We experiment with between 5 and 15 hidden nodes
and with an error rate between 10?2 and 10?7.
We implement 4 versions of NetSum. The first
450
version, NetSum(b), is trained for our first sum-
marization problem (b indicates block). The pairs
are generated using the maximum ROUGE scores
l1 (see Section 4.3). The other three rankers are
trained to identify the sentence in the document
that best matches highlight n. We train one ranker,
NetSum(n), for each highlight n, for n = 1, 2, 3,
resulting in three rankers. NetSum(n) is trained us-
ing pairs generated from the l1,n ROUGE scores be-
tween sentence Si and highlight Hn (see Section
4.3).
4.2 Matching Extracted to Generated
Sentences
In this section, we describe how to determine which
sentence in the document best matches a given high-
light. Choosing three sentences most similar to the
three highlights is very challenging since the high-
lights include content that has been gathered across
sentences and even paragraphs, and furthermore in-
clude vocabulary that may not be present in the
text. Jing showed, for 300 news articles, that 19%
of human-generated summary sentences contain no
matching article sentence (Jing, 2002). In addition,
only 42% of the summary sentences match the con-
tent of a single article sentence, where there are still
semantic and syntactic transformations between the
summary sentence and article sentence.. Since each
highlight is human generated and does not exactly
match any one sentence in the document, we must
develop a method to identify how closely related a
highlight is to a sentence. We use the ROUGE (Lin,
2004b) measure to score the similarity between an
article sentence and a highlight sentence. We antic-
ipate low ROUGE scores for both the baseline and
NetSum due to the difficulty of finding a single sen-
tence to match a highlight.
4.3 ROUGE
Recall-Oriented Understudy for Gisting Evaluation
(Lin, 2004b), known as ROUGE, measures the qual-
ity of a model-generated summary or sentence by
comparing it to a ?gold-standard?, typically human-
generated, summary or sentence. It has been shown
that ROUGE is very effective for measuring both
single-document summaries and single-document
headlines (Lin, 2004a).
ROUGE-N is a N -gram recall between a model-
generated summary and a reference summary. We
use ROUGE-N , for N = 1, for labeling and evalua-
tion of our model-generated highlights.1 ROUGE-
1 and ROUGE-2 have been shown to be statisti-
cally similar to human evaluations and can be used
with a single reference summary (Lin, 2004a). We
have only one reference summary, the set of human-
generated highlights, per document. In our work,
the reference summary can be a single highlight sen-
tence or the highlights as a block. We calculate
ROUGE-N as
?
gramj?R?Si Count(gramj)
?
gramj?R Count(gramj)
, (1)
where R is the reference summary, Si is the model-
generated summary, and N is the length of the N -
gram gramj .2 The numerator cannot excede the
number of N -grams (non-unique) in R.
We label each sentence Si by its ROUGE-1 score.
For the first problem of matching the highlights
as a block, we label each Si by l1, the maximum
ROUGE-1 score between Si and each highlight Hn,
for n = 1, 2, 3, given by l1 = maxn(R(Si,Hn)).
For the second problem of matching three sen-
tences to the three highlights individually, we label
each sentence Si by l1,n, the ROUGE-1 score be-
tween Si and Hn, given by l1,n = R(Si,Hn). The
ranker for highlight n, NetSum(n), is passed sam-
ples labeled using l1,n.
4.4 Features
RankNet takes as input a set of samples, where each
sample contains a label and feature vector. The la-
bels were previously described in Section 4.3. In this
section, we describe each feature in detail and moti-
vate in part why each feature is chosen. We generate
10 features for each sentence Si in each document,
listed in Table 1. Each feature is chosen to identify
characteristics of an article sentence that may match
those of a highlight sentence. Some of the features
such as position and N -gram frequencies are com-
monly used for scoring. Sentence scoring based on
1We use an implementation of ROUGE that does not per-
form stemming or stopword removal.
2ROUGE is typically used when the length of the reference
summary is equal to length of the model-generated summary.
Our reference summary and model-generated summary are dif-
ferent lengths, so there is a slight bias toward longer sentences.
451
Symbol Feature Name
F (Si) Is First Sentence
Pos(Si) Sentence Position
SB(Si) SumBasic Score
SBb(Si) SumBasic Bigram Score
Sim(Si) Title Similarity Score
NT (Si) Average News Query Term Score
NT+(Si) News Query Term Sum Score
NTr(Si) Relative News Query Term Score
WE(Si) Average Wikipedia Entity Score
WE+(Si) Wikipedia Entity Sum Score
Table 1: Features used in our model.
sentence position, terms common with the title, ap-
pearance of keyword terms, and other cue phrases
is known as the Edmundsonian Paradigm (Edmund-
son, 1969; Alfonesca and Rodriguez, 2003; Mani,
2001). We use variations on these features as well
as a novel set of features based on third-party data.
Typically, news articles are written such that the
first sentence summarizes the article. Thus, we in-
clude a binary feature F (Si) that equals 1 if Si is
the first sentence of the document: F (Si) = ?i,1,
where ? is the Kronecker delta function. This fea-
ture is used only for NetSum(b) and NetSum(1).
We include sentence position since we found in
empirical studies that the sentence to best match
highlight H1 is on average 10% down the article, the
sentence to best match H2 is on average 20% down
the article, and the sentence to best match H3 is 31%
down the article.3 We calculate the position of Si in
document D as
Pos(Si) =
i
? , (2)
where i = {1, . . . , ?} is the sentence number and ?
is the number of sentences in D.
We include the SumBasic score (Nenkova et al,
2006) of a sentence to estimate the importance of a
sentence based on word frequency. We calculate the
SumBasic score of Si in document D as
SB(Si) =
?
w?Si p(w)
|Si|
, (3)
3Though this is not always the case, as the sentence to match
H2 precedes that to match H1 in 22.03% of documents, and the
sentence to match H3 precedes that to match H2 in 29.32% of
and precedes that to match H1 in 28.81% of documents.
where p(w) is the probability of word w and |Si| is
the number of words in sentence Si. We calculate
p(w) as p(w) = Count(w)|D| , where Count(w) is the
number of times word w appears in document D and
|D| is the number of words in document D. Note
that the score of a sentence is the average probability
of a word in the sentence.
We also include the SumBasic score over bi-
grams, where w in Eq 3 is replaced by bigrams and
we normalize by the number of bigrams in Si.
We compute the similarity of a sentence Si in doc-
ument D with the title T of D as the relative proba-
bility of title terms t ? T in Si as
Sim(Si) =
?
t?Si p(t)
|Si|
, (4)
where p(t) = Count(t)|T | is the number of times term t
appears in T over the number of terms in T .
The remaining features we use are based on third-
party data sources. Previously, third-party sources
such as WordNet (Fellbaum, 1998), the web (Ja-
galamudi et al, 2006), or click-through data (Sun
et al, 2005) have been used as features. We pro-
pose using news query logs and Wikipedia entities
to enhance features. We base several features on
query terms frequently issued to Microsoft?s news
search engine http://search.live.com/news, and enti-
ties4 found in the online open-source encyclopedia
Wikipedia (Wikipedia.org, 2007). If a query term or
Wikipedia entity appears frequently in a CNN docu-
ment, then we assume highlights should include that
term or entity since it is important on both the doc-
ument and global level. Sentences containing query
terms or Wikipedia entities therefore contain impor-
tant content. We confirm the importance of these
third-party features in Section 7.
We collected several hundred of the most fre-
quently queried terms in February 2007 from the
news query logs. We took the daily top 200 terms
for 10 days. Our hypothesis is that a sentence with
a higher number of news query terms should be a
better candidate highlight. We calculate the average
probability of news query terms q in Si as
NT (Si) =
?
q?Si p(q)
|q ? Si|
, (5)
4We define an entity as a title of a Wikipedia page.
452
where p(q) is the probability of a news term q and
|q ? Si| is the number of news terms in Si. p(q) =
Count(q)
|q?D| , where Count(q) is the number of times
term q appears in D and |q ? D| is the number of
news query terms in D.
We also include the sum of news query terms in
Si, given by NT+(Si) =
?
q?Si p(q), and the rela-
tive probability of news query terms in Si, given by
NTr(Si) =
?
q?Si
p(q)
|Si| .
We perform term disambiguation on each doc-
ument using an entity extractor (Cucerzan, 2007).
Terms are disambiguated to a Wikipedia entity
only if they match a surface form in Wikipedia.
Wikipedia surface forms are terms that disambiguate
to a Wikipedia entity and link to a Wikipedia page
with the entity as its title. For example, ?WHO? and
?World Health Org.? both refer to the World Health
Organization, and should disambiguate to the entity
?World Health Organization?. Sentences in CNN
document D that contain Wikipedia entities that fre-
quently appear in CNN document D are considered
important. We calculate the average Wikipedia en-
tity score for Si as
WE(Si) =
?
e?Si p(e)
|e ? Si|
, (6)
where p(e) is the probability of entity e, given by
p(e) = Count(e)|e?D| , where Count(e) is the number of
times entity e appears in CNN document D and |e ?
D| is the total number of entities in CNN document
D.
We also include the sum of Wikipedia entities,
given by WE+(Si) =
?
e?Si p(e).
Note that all features except position features are
a variant of SumBasic over different term sets. All
features are computed over sentences where every
word has been lowercased and punctuation has been
removed after sentence breaking. We examined us-
ing stemming, but found stemming to be ineffective.
5 Evaluation
We evaluate the performance of NetSum using
ROUGE and by comparing against a baseline sys-
tem. For the first summarization task, we compare
against the baseline of choosing the first three sen-
tences as the block summary. For the second high-
lights task, we compare NetSum(n) against the base-
line of choosing sentence n (to match highlight n).
Both tasks are novel in attempting to match high-
lights rather than a human-generated summary.
We consider ROUGE-1 to be the measure of im-
portance and thus train our model on ROUGE-1 (to
optimize ROUGE-1 scores) and likewise evaluate
our system on ROUGE-1. We list ROUGE-2 scores
for completeness, but do not expect them to be sub-
stantially better than the baseline since we did not
directly optimize for ROUGE-2.5
For every document in our corpus, we compare
NetSum?s output with the baseline output by com-
puting ROUGE-1 and ROUGE-2 between the high-
light block and NetSum and between the highlight
block and the block of sentences. Similarly, for each
highlight, we compute ROUGE-1 and ROUGE-2
between highlight n and NetSum(n) and between
highlight n and sentence n, for n = 1, 2, 3. For
each task, we calculate the average ROUGE-1 and
ROUGE-2 scores of NetSum and of the baseline.
We also report the percent of documents where the
ROUGE-1 score of NetSum is equal to or better than
the ROUGE-1 score of the baseline.
We perform all experiments using five-fold cross-
validation on our dataset of 1365 documents. We
divide our corpus into five random sets and train on
three combined sets, validate on one set, and test on
the remaining set. We repeat this procedure for ev-
ery combination of train, validation, and test sets.
Our results are the micro-averaged results on the five
test sets. For all experiments, Table 3 lists the statis-
tical tests performed and the significance of perfor-
mance differences between NetSum and the baseline
at 95% confidence.
6 Results: Summarization
We first find three sentences that, as a block, best
match the three highlights as a block. NetSum(b)
produces a ranked list of sentences for each docu-
ment. We create a block from the top 3 ranked sen-
tences. The baseline is the block of the first 3 sen-
tences of the document. A similar baseline outper-
5NetSum can directly optimize for any measure by training
on it, such as training on ROUGE-2 or on a weighted sum of
ROUGE-1 and ROUGE-2 to optimize both. Thus, ROUGE-2
scores could be further improved. We leave such studies for
future work.
453
System Av. ROUGE-1 Av. ROUGE-2
Baseline 0.4642 ? 0.0084 0.1726 ? 0.0064
NetSum(b) 0.4956 ? 0.0075 0.1775 ? 0.0066
Table 2: Results on summarization task with stan-
dard error at 95% confidence. Bold indicates signif-
icance under paired tests.
ROUGE-1 ROUGE-2
System 1 2 3 1 2 3
NetSum(b) x x x x o o
NetSum(1) x x x o o o
NetSum(2) x x x x o x
NetSum(3) x x x x x x
Table 3: Paired tests for statistical significance
at 95% confidence between baseline and NetSum
performance; 1: McNemar, 2: Paired t-test, 3:
Wilcoxon signed-rank. ?x? indicates pass, ?o? in-
dicates fail. Since our studies are pair-wise, tests
listed here are more accurate than error bars reported
in Tables 2?5.
forms all previous systems for news article summa-
rization (Nenkova, 2005) and has been used in the
DUC workshops (DUC, 2001).
For each block produced by NetSum(b) and the
baseline, we compute the ROUGE-1 and ROUGE-2
scores of the block against the set of highlights as a
block. For 73.26% of documents, NetSum(b) pro-
duces a block with a ROUGE-1 score that is equal
to or better than the baseline score. The two systems
produce blocks of equal ROUGE-1 score for 24.69%
of documents. Under ROUGE-2, NetSum(b) per-
forms equal to or better than the baseline on 73.19%
of documents and equal to the baseline on 40.51%
of documents.
Table 2 shows the average ROUGE-1 and
ROUGE-2 scores obtained with NetSum(b) and the
baseline. NetSum(b) produces a higher quality
block on average for ROUGE-1.
Table 4 lists the sentences in the block produced
by NetSum(b) and the baseline block, for the arti-
cles shown in Figure 1. The NetSum(b) summary
achieves a ROUGE-1 score of 0.52, while the base-
line summary scores only 0.36.
System Sent. # ROUGE-1
Baseline S1, S2, S3 0.36
NetSum(b) S1, S7, S15 0.52
Table 4: Block results for the block produced by
NetSum(b) and the baseline block for the exam-
ple article. ROUGE-1 scores computed against the
highlights as a block are listed.
7 Results: Highlights
Our second task is to extract three sentences from
a document that best match the three highlights in
order. To accomplish this, we train NetSum(n) for
each highlight n = 1, 2, 3. We compare NetSum(n)
with the baseline of picking the nth sentence of the
document. We perform five-fold cross-validation
across our 1365 documents. Our results are reported
for the micro-average of the test results. For each
highlight n produced by both NetSum(n) and the
baseline, we compute the ROUGE-1 and ROUGE-
2 scores against the nth highlight.
We expect that beating the baseline for n = 1 is a
more difficult task than for n = 2 or 3 since the first
sentence of a news article typically acts as a sum-
mary of the article and since we expect the first high-
light to summarize the article. NetSum(1), however,
produces a sentence with a ROUGE-1 score that is
equal to or better than the baseline score for 93.26%
of documents. The two systems produce sentences
of equal ROUGE-1 scores for 82.84% of documents.
Under ROUGE-2, NetSum(1) performs equal to or
better than the baseline on 94.21% of documents.
Table 5 shows the average ROUGE-1 and
ROUGE-2 scores obtained with NetSum(1) and the
baseline. NetSum(1) produces a higher quality sen-
tence on average under ROUGE-1.
The content of highlights 2 and 3 is typically from
later in the document, so we expect the baseline to
not perform as well in these tasks. NetSum(2) out-
performs the baseline since it is able to identify sen-
tences from further down the document as impor-
tant. For 77.73% of documents, NetSum(2) pro-
duces a sentence with a ROUGE-1 score that is equal
to or better than the score for the baseline. The two
systems produce sentences of equal ROUGE-1 score
for 33.92% of documents. Under ROUGE-2, Net-
Sum(2) performs equal to or better than the baseline
454
System Av. ROUGE-1 Av. ROUGE-2
Baseline(1) 0.4343 ? 0.0138 0.1833 ? 0.0095
NetSum(1) 0.4478 ? 0.0133 0.1857 ? 0.0085
Baseline(2) 0.2451 ? 0.0128 0.0814 ? 0.0106
NetSum(2) 0.3036 ? 0.0117 0.0877 ? 0.0107
Baseline(3) 0.1707 ? 0.0103 0.0412 ? 0.0069
NetSum(3) 0.2603 ? 0.0133 0.0615 ? 0.0075
Table 5: Results on ordered highlights task with
standard error at 95% confidence. Bold indicates
significance under paired tests.
System Sent. # ROUGE-1
Baseline S1 0.167
NetSum(1) S1 0.167
Baseline S2 0.111
NetSum(2) S1 0.556
Baseline S3 0.000
NetSum(3) S15 0.400
Table 6: Highlight results for highlight n produced
by NetSum(n) and highlight n produced by the base-
line for the example article. ROUGE-1 scores com-
puted against highlight n are listed.
84.84% of the time. For 81.09% of documents, Net-
Sum(3) produces a sentence with a ROUGE-1 score
that is equal to or better than the score for the base-
line. The two systems produce sentences of equal
ROUGE-1 score for 28.45% of documents. Under
ROUGE-2, NetSum(3) performs equal to or better
than the baseline 89.91% of the time.
Table 5 shows the average ROUGE-1 and
ROUGE-2 scores obtained for NetSum(2), Net-
Sum(3), and the baseline. Both NetSum(2) and Net-
Sum(3) produce a higher quality sentence on aver-
age under both measures.
Table 6 gives highlights produced by NetSum(n)
and the highlights produced by the baseline, for the
article shown in Figure 1. The NetSum(n) highlights
produce ROUGE-1 scores equal to or higher than the
baseline ROUGE-1 scores.
In feature ablation studies, we confirmed that the
inclusion of news-based and Wikipedia-based fea-
tures improves NetSum?s peformance. For example,
we removed all news-based and Wikipedia-based
features in NetSum(3). The resulting performance
moderately declined. Under ROUGE-1, the base-
line produced a better highlight on 22.34% of docu-
ments, versus only 18.91% when using third-party
features. Similarly, NetSum(3) produced a sum-
mary of equal or better ROUGE-1 score on only
77.66% of documents, compared to 81.09% of doc-
uments when using third-party features. In addi-
tion, the average ROUGE-1 score dropped to 0.2182
and the average ROUGE-2 score dropped to 0.0448.
The performance of NetSum with third-party fea-
tures over NetSum without third-party features is
statistically significant at 95% confidence. However,
NetSum still outperforms the baseline without third-
party features, leading us to conclude that RankNet
and simple position and term frequency features
contribute the maximum performance gains, but in-
creased ROUGE-1 and ROUGE-2 scores are a clear
benefit of third-party features.
8 Conclusions
We have presented a novel approach to automatic
single-document summarization based on neural
networks, called NetSum. Our work is the first
to use both neural networks for summarization and
third-party datasets for features, using Wikipedia
and news query logs. We have evaluated our sys-
tem on two novel tasks: 1) producing a block of
highlights and 2) producing three ordered highlight
sentences. Our experiments were run on previously
unstudied data gathered from CNN.com. Our sys-
tem shows remarkable performance over the base-
line of choosing the first n sentences of the docu-
ment, where the performance difference is statisti-
cally significant under ROUGE-1.
9 Future Work
An immediate future direction is to further explore
feature selection. We found third-party features
beneficial to the performance of NetSum and such
sources can be mined further. In addition, feature se-
lection for each NetSum system could be performed
separately since, for example, highlight 1 has differ-
ent characteristics than highlight 2.
In our experiments, ROUGE scores are fairly low
because a highlight rarely matches the content of a
single sentence. To improve NetSum?s performance,
we must consider extracting content across sentence
455
boundaries. Such work requires a system to produce
abstract summaries. We hope to incorporate sen-
tence simplification and sentence splicing and merg-
ing in a future version of NetSum.
Another future direction is the identification of
?hard? and ?easy? inputs. Although we report av-
erage ROUGE scores, such measures can be mis-
leading since some highlights are simple to match
and some are much more difficult. A better system
evaluation measure would incorporate the difficulty
of the input and weight reported results accordingly.
References
E. Alfonesca and P. Rodriguez. 2003. Description of
the uam system for generating very short summaries
at DUC?2003. In DUC 2003: Document Under-
standing Conference, May 31?June 1, 2003, Edmon-
ton, Canada.
C. Aone, M. Okurowski, and J. Gorlinsky. 1998. Train-
able scalable summarization using robust nlp and ma-
chine learning. In Proceedings of the 17th COLING
and 36th ACL.
C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. 2005.
Learning to Rank using Gradient Descent. In Luc De
Raedt and Stefan Wrobel, editors, ICML, pages 89?96.
ACM.
C.J.C. Burges, R. Ragno, and Q. Le. 2006. Learning to
rank with nonsmooth cost functions. In NIPS 2006:
Neural Information Processing Systems, December 4-
7, 2006, Vancouver, CA.
CNN.com. 2007a. Cable news network.
http://www.cnn.com/.
CNN.com. 2007b. Nigeria reports
first human death from bird flu.
http://edition.cnn.com/2007/WORLD/africa/01/31/
nigeria.bird.flu.ap/index.html?eref=edition world.
J. Conroy, J. Schlesinger, J. Goldstein, and D. O?Leary.
2004. Left-brain/right-brain multi-document summa-
rization. In DUC 2004: Document Understanding
Workshop, May 6?7, 2004, Boston, MA, USA.
S. Cucerzan. 2007. Large scale named entity disam-
biguation based on wikipedia data. In EMNLP 2007:
Empirical Methods in Natural Language Processing,
June 28-30, 2007, Prague, Czech Republic.
H. Daume? III and D. Marcu. 2005. Bayesian multi-
document summarization at mse. In Proceedings of
MSE.
DUC. 2001. Document understanding conferences.
http://www-nlpir.nist.gov/projects/duc/index.html.
H.P. Edmundson. 1969. New methods in automatic ex-
tracting. Journal for the Association of Computing
Machinery, 16:159?165.
G. Erkan and D. R. Radev. 2004a. Lexpagerank:
Prestige in multi-document text summarization. In
EMNLP 2004: Empirical Methods in Natural Lan-
guage Processing, 2004, Barcelona, Spain.
G. Erkan and D. R. Radev. 2004b. Lexrank: Graph-
based centrality as salience in text summarization.
Journal of Artificial Intelligence Research (JAIR), 22.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
T. Hirao, Y. Sasaki, H. Isozaki, and E. Maeda. 2002.
Ntt?s text summarization system for DUC?2002. In
DUC 2002: Workshop on Text Summarization, July
11?12, 2002, Philadelphia, PA, USA.
J. Jagalamudi, P. Pingali, and V. Varma. 2006. Query
independent sentence scoring approach to DUC 2006.
In DUC 2006: Document Understanding Conference,
June 8?9, 2006, Brooklyn, NY, USA.
H. Jing. 2002. Using hidden markov modeling to de-
compose human-written summaries. Computational
Linguistics, 4(28):527?543.
J. Kupiec, J. Pererson, and F. Chen. 1995. A trainable
document summarizer. Research and Development in
Information Retrieval, pages 68?73.
P. Lal and S. Ruger. 2002. Extract-based summarization
with simplification. In DUC 2002: Workshop on Text
Summarization, July 11?12, 2002, Philadelphia, PA,
USA.
Y. Le Cun, L. Bottou, G.B. Orr, and K.R. Mu?ller.
1998. Efficient backprop. In Neural Networks, Tricks
of the Trade, Lecture Notes in Computer Science
LNCS 1524. Springer Verlag.
C.Y. Lin and E. Hovy. 2002. Automated multi-document
summarization in neats. In Proceedings of the Human
Language Technology Conference (HLT2002).
C.Y. Lin. 2004a. Looking for a few good metrics: Auto-
matic summarization evaluation ? how many samples
are enough? In Proceedings of the NTCIR Workshop
4, June 2?4, 2004, Tokyo, Japan.
C.Y. Lin. 2004b. Rouge: A package for automatic evalu-
ation of summaries. In WAS 2004: Proceedings of the
Workshop on Text Summarization Branches Out, July
25?26, 2004, Barcelona, Spain.
456
H. Luhn. 1958. The automatic creation of literature ab-
stracts. IBM Journal of Research and Development,
2(2):159?165.
I. Mani. 2001. Automatic Summarization. John Ben-
jamins Pub. Co.
R. Mihalcea and D. R. Radev, editors. 2006. Textgraphs:
Graph-based methods for NLP. New York City, NY.
R. Mihalcea and P. Tarau. 2005. An algorithm for lan-
guage independent single and multiple document sum-
marization. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP), October, 2005, Korea.
R. Mihalcea. 2005. Language independent extractive
summarization. In ACL 2005: Proceedings of the 43rd
Annual Meeting of the Association for Computational
Linguistics, June, 2005, Ann Arbor, MI, USA.
A. Nenkova, L. Vanderwende, and K. McKeown. 2006.
A compositional context sensitive multi-document
summarizer: exploring the factors that influence sum-
marization. In E. N. Efthimiadis, S. T. Dumais,
D. Hawking, and K. Ja?rvelin, editors, SIGIR, pages
573?580. ACM.
A. Nenkova. 2005. Automatic text summarization of
newswire: Lessons learned from the document un-
derstanding conference. In Proceedings of the 20th
National Conference on Artificial Intelligence (AAAI
2005), Pittsburgh, PA.
B. Schiffman. 2002. Building a resource for evaluat-
ing the importance of sentences. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation (LREC).
J.T. Sun, D. Shen, H.J. Zeng, Q. Yang, Y. Lu, and
Z. Chen. 2005. Web-page summarization using
click-through data. In R. A. Baeza-Yates, N. Ziviani,
G. Marchionini, A. Moffat, and J. Tait, editors, SIGIR.
ACM.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarla-
mudi, H. Suzuki, and L. Vanderwende. 2007. The
pythy summarization system: Microsoft research at
DUC2007. In DUC 2007: Document Understanding
Conference, April 26?27, 2007, Rochester, NY, USA.
L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Mi-
crosoft research at DUC2006: Task-focused summa-
rization with sentence simplification. In DUC 2006:
Document Understanding Workshop, June 8?9, 2006,
Brooklyn, NY, USA.
Wikipedia.org. 2007. Wikipedia org.
http://www.wikipedia.org.
W.T. Yih, J. Goodman, L. Vanderwende, and H. Suzuki.
2007. Multi-document summarization by maximizing
informative content words. In IJCAI 2007: 20th In-
ternational Joint Conference on Artificial Intelligence,
January, 2007.
457
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 8?9,
Vancouver, October 2005.
MindNet: an automatically-created lexical resource 
 
Lucy Vanderwende, Gary Kacmarcik, Hisami Suzuki, Arul Menezes 
Microsoft Research 
Redmond, WA 98052, USA 
{lucyv, garykac, hisamis, arulm}@microsoft.com 
 
Abstract 
We will demonstrate MindNet, a lexical resource 
built automatically by processing text.  We will 
present two forms of MindNet: as a static lexical 
resource, and, as a toolkit which allows MindNets 
to be built from arbitrary text.  We will also intro-
duce a web-based interface to MindNet lexicons 
(MNEX) that is intended to make the data con-
tained within MindNets more accessible for explo-
ration.  Both English and Japanese MindNets will 
be shown and will be made available, through 
MNEX, for research purposes. 
1 MindNet 
A MindNet is a collection of semantic relations 
that is automatically extracted from text data using 
a broad coverage parser. Previous publications on 
MindNet (Suzuki et al, 2005, Richardson et al, 
1998, Vanderwende 1995) have focused on the 
effort required to build a MindNet from the data 
contained in Japanese and English lexicons. 
Semantic Relations 
The semantic relations that are stored in MindNet 
are directed, labeled relationships between two 
words; see Table 1:  
Attributive Manner Source 
Cause Means Synonym 
Goal Part Time 
Hypernym Possessor TypicalObject 
Location Result TypicalSubject 
Table 1: A sampling of the semantic relations stored in 
MindNet 
 
These semantic relations are obtained from the 
Logical Form analysis of our broad coverage 
parser NLPwin (Heidorn, 2000).  The Logical 
Form is a labeled dependency analysis with func-
tion words removed.  We have not completed an 
evaluation of the quality of the extracted semantic 
relations.  Anecdotally, however, the quality varies 
according to the relation type, with Hypernym and 
grammatical relations TypicalSubject and Typi-
calObj being reliable, while relations such as Part 
and Purpose are less reliable. By making MindNet 
available, we solicit feedback on the utility of these 
labeled relationships, especially in contrast to sim-
ple co-occurrence statistics and to the heavily used 
hypernymy and synonymy links. Furthermore, we 
solicit feedback on the level of accuracy which is 
tolerable for specific applications. 
Semantic Relation Structures 
We refer to the hierarchical collection of semantic 
relations (semrels) that are automatically extracted 
from a source sentence as a semrel structure. Each 
semrel structure contains all of the semrels ex-
tracted from a single source sentence.  A semrel 
structure can be viewed from the perspective of 
each unique word that occurs in the structure; we 
call these inverted structures.  They contain the 
same information as the original, but with a differ-
ent word placed at the root of the structure. An ex-
ample semrel structure for the definition of 
swallow is given in Figure 1a, and its inversion, 
from the perspective of wing is given in Figure 1b: 
 
swallow           wing 
 Hyp bird           PartOf bird 
       Part wing             Attrib small 
       Attrib small          HypOf swallow 
 
Figure 1a and b: Figure 1a is the semrel structure for the 
definition of swallow1, Figure 1b the inversion on wing. 
2 MNEX 
MNEX (MindNet Explorer) is the web-based inter-
face to MindNet that is designed to facilitate 
browsing MindNet structure and relations. MNEX 
displays paths based on the word or words that the 
                                                          
1
 Swallow: a small bird with wings (LDOCE).  Definition 
abbreviated for purposes of exposition.   
8
user enters. A path is a set of links that connect one 
word to another within either a single semrel struc-
ture or by combining fragments from multiple 
semrel structures.  Paths are weighted for compari-
son (Richardson, 1997). Currently, either one or 
two words can be specified and we allow some 
restrictions to refine the path search.  A user can 
restrict the intended part of speech of the words 
entered, and/or the user can restrict the paths to 
include only the specified relation. When two 
words are provided, the UI returns a list of the 
highest ranked paths between those two words. 
When only one word is given, then all paths from 
that word are ranked and displayed.  Figure 2 
shows the MNEX interface, and a query requesting 
all paths from the word bird, restricted to Noun 
part of speech, through the Part relation:  
 
 
Figure 2: MNEX output for ?bird (Noun) Part? query 
3 Relation to other work 
For English, WordNet is the most widely used 
knowledgebase. Aside from being English-only, 
this database was hand-coded and significant effort 
is required to create similar databases for different 
domains and languages. Projects like EuroWord-
Net address the monolingual aspect of WordNet, 
but these databases are still labor intensive to cre-
ate.  On the other hand, the quality of the informa-
tion contained in a WordNet (Fellbaum et al, 
1998) is very reliable, exactly because it was 
manually created.  FrameNet (Baker et al, 1998) 
and OpenCyc are other valuable resources for Eng-
lish, also hand-created, that contain a rich set of 
relations between words and concepts. Their use is 
still being explored as they have been made avail-
able only recently. For Japanese, there are also 
concept dictionaries providing semantic relations, 
similarly hand-created, e.g., EDR and Nihongo 
Goi-taikei (NTT). 
The demonstration of MindNet will highlight 
that this resource is automatically created, allowing 
domain lexical resources to be built quickly, albeit 
with lesser accuracy.  We are confident that this is 
a trade-off worth making in many cases, and en-
courage experimentation in this area.  MNEX al-
lows the exploration of the rich set of relations 
through which paths connecting words are linked. 
4 References 
Baker, Collin F., Fillmore, Charles J., and Lowe, John 
B. (1998): The Berkeley FrameNet project. in Pro-
ceedings of the COLING-ACL, Montreal, Canada. 
Fellbaum, C. (ed). 1998. WordNet: An Electronic Lexi-
cal Database. MIT Press. 
Heidorn, G. 2000. Intelligent writing assistance. in 
R.Dale, H.Moisl and H.Somers (eds.), A Handbook 
of Natural Langauge Processing: Techniques and 
Applications for the Processing of Language as Text. 
New York: Marcel Dekker. 
National Institute of Information and Communications 
Technology. 2001. EDR Electronic Dictionary Ver-
sion 2.0 Technical Guide. 
NTT Communications Science Laboratories. 1999. Goi-
Taikei - A Japanese Lexicon. Iwanami Shoten. 
OpenCyc. Available at: http://www.cyc.com/opencyc. 
Richardson, S.D. 1997, Determining Similarity and In-
ferring Relations in a Lexical Knowledge Base. PhD. 
dissertation, City University of New York. 
Richardson, S.D., W. B. Dolan, and L. Vanderwende. 
1998. MindNet: Acquiring and Structuring Semantic 
Information from Text, In Proceedings of ACL-
COLING. Montreal, pp. 1098-1102. 
Suzuki, H., G. Kacmarcik, L. Vanderwende and A. 
Menezes. 2005. Mindnet and mnex. In Proceedings 
of the 11th Annual meeting of the Society of Natural 
Language Processing (in Japanese).  
Vanderwende, L. 1995. Ambiguity in the acquisition of 
lexical information. In Proceedings of the AAAI 
1995 Spring Symposium Series, symposium on rep-
resentation and acquisition of lexical knowledge, 
174-179. 
9
Using Contextual Speller Techniques and Language Modeling for 
ESL Error Correction 
Michael Gamon*, Jianfeng Gao*, Chris Brockett*, Alexandre Klementiev+, William 
B. Dolan*, Dmitriy Belenko*, Lucy Vanderwende* 
 
*Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
{mgamon,jfgao,chrisbkt,billdol, 
dmitryb,lucyv}@microsoft.com 
+Dept. of Computer Science 
University of Illinois 
Urbana, IL 61801 
klementi@uiuc.edu 
 
 
Abstract 
We present a modular system for detection 
and correction of errors made by non-
native (English as a Second Language = 
ESL) writers. We focus on two error types: 
the incorrect use of determiners and the 
choice of prepositions. We use a decision-
tree approach inspired by contextual 
spelling systems for detection and 
correction suggestions, and a large 
language model trained on the Gigaword 
corpus to provide additional information to 
filter out spurious suggestions. We show 
how this system performs on a corpus of 
non-native English text and discuss 
strategies for future enhancements. 
1 Introduction 
English is today the de facto lingua franca for 
commerce around the globe. It has been estimated 
that about 750M people use English as a second 
language, as opposed to 375M native English 
speakers (Crystal 1997), while as much as 74% of 
writing in English is done by non-native speakers. 
However, the errors typically targeted by 
commercial proofing tools represent only a subset 
of errors that a non-native speaker might make. For 
example, while many non-native speakers may 
encounter difficulty choosing among prepositions, 
this is typically not a significant problem for native 
speakers and hence remains unaddressed in 
proofing tools such as the grammar checker in 
Microsoft Word (Heidorn 2000). Plainly there is an 
opening here for automated proofing tools that are 
better geared to the non-native users.  
One challenge that automated proofing tools 
face is that writing errors often present a semantic 
dimension that renders it difficult if not impossible 
to provide a single correct suggestion. The choice 
of definite versus indefinite determiner?a 
common error type among writers with a Japanese, 
Chinese or Korean language background owing to 
the lack of overt markers for definiteness and 
indefiniteness?is highly dependent on larger 
textual context and world knowledge. It seems 
desirable, then, that proofing tools targeting such 
errors be able to offer a range of plausible 
suggestions, enhanced by presenting real-world 
examples that are intended to inform a user?s 
selection of the most appropriate wording in the 
context1. 
2 Targeted Error Types 
Our system currently targets eight different error 
types: 
1. Preposition presence and choice: 
In the other hand, ... (On the other hand ...) 
2. Definite and indefinite determiner presence 
and choice: 
 I am teacher... (am a teacher) 
3. Gerund/infinitive confusion: 
I am interesting in this book. (interested in) 
4. Auxiliary verb presence and choice: 
My teacher does is a good teacher (my teacher 
is...) 
                                                 
1 Liu et al 2000 take a similar approach, retrieving 
example sentences from a large corpus. 
449
5. Over-regularized verb inflection: 
 I writed a letter (wrote) 
6. Adjective/noun confusion: 
 This is a China book (Chinese book) 
7. Word order (adjective sequences and nominal 
compounds): 
I am a student of university (university student) 
8. Noun pluralization: 
 They have many knowledges (much knowledge) 
In this paper we will focus on the two most 
prominent and difficult errors: choice of 
determiner and prepositions. Empirical 
justification for targeting these errors comes from 
inspection of several corpora of non-native writing. 
In the NICT Japanese Learners of English (JLE) 
corpus (Izumi et al 2004), 26.6% of all errors are 
determiner related, and about 10% are preposition 
related, making these two error types the dominant 
ones in the corpus. Although the JLE corpus is 
based on transcripts of spoken language, we have 
no reason to believe that the situation in written 
English is substantially different. The Chinese 
Learners of English Corpus (CLEC, Gui and Yang 
2003) has a coarser and somewhat inconsistent 
error tagging scheme that makes it harder to isolate 
the two errors, but of the non-orthographic errors, 
more than 10% are determiner and number related. 
Roughly 2% of errors in the corpus are tagged as 
preposition-related, but other preposition errors are 
subsumed under the ?collocation error? category 
which makes up about 5% of errors. 
3 Related Work 
Models for determiner and preposition selection 
have mostly been investigated in the context of 
sentence realization and machine translation 
(Knight and Chander 1994, Gamon et al 2002,  
Bond 2005, Suzuki and Toutanova 2006, 
Toutanova and Suzuki 2007). Such approaches 
typically rely on the fact that preposition or 
determiner choice is made in otherwise native-like 
sentences. Turner and Charniak (2007), for 
example, utilize a language model based on a 
statistical parser for Penn Tree Bank data. 
Similarly, De Felice and Pulman (2007) utilize a 
set of sophisticated syntactic and semantic analysis 
features to predict 5 common English prepositions. 
Obviously, this is impractical in a setting where 
noisy non-native text is subjected to proofing. 
Meanwhile, work on automated error detection on 
non-native text focuses primarily on detection of 
errors, rather than on the more difficult task of 
supplying viable corrections (e.g., Chodorow and 
Leacock, 2000). More recently,  Han et al (2004, 
2006) use a maximum entropy classifier to propose 
article corrections in TESOL essays, while Izumi 
et al (2003) and Chodorow et al (2007) present 
techniques of automatic preposition choice 
modeling. These more recent efforts, nevertheless, 
do not attempt to integrate their methods into a 
more general proofing application designed to 
assist non-native speakers when writing English. 
Finally, Yi et al (2008) designed a system that 
uses web counts to determine correct article usage 
for a given sentence, targeting ESL users. 
4 System Description 
Our system consists of three major components: 
1. Suggestion Provider (SP) 
2. Language Model (LM) 
3. Example Provider (EP) 
The Suggestion Provider contains modules for 
each error type discussed in section 2. Sentences 
are tokenized and part-of-speech tagged before 
they are presented to these modules. Each module 
determines parts of the sentence that may contain 
an error of a specific type and one or more possible 
corrections. Four of the eight error-specific 
modules mentioned in section 2 employ machine 
learned (classification) techniques, the other four 
are based on heuristics. Gerund/infinitive 
confusion and auxiliary presence/choice each use a 
single classifier. Preposition and determiner 
modules each use two classifiers, one to determine 
whether a preposition/article should be present, 
and one for the choice of preposition/article. 
All suggestions from the Suggestion Provider 
are collected and passed through the Language 
Model. As a first step, a suggested correction has 
to have a higher language model score than the 
original sentence in order to be a candidate for 
being surfaced to the user. A second set of 
heuristic thresholds is based on a linear 
combination of class probability as assigned by the 
classifier and language model score. 
The Example Provider queries the web for 
exemplary sentences that contain the suggested 
correction. The user can choose to consult this 
information to make an informed decision about 
the correction. 
450
4.1 Suggestion Provider Modules for 
Determiners and Prepositions 
The SP modules for determiner and preposition 
choice are machine learned components. Ideally, 
one would train such modules on large data sets of 
annotated errors and corrected counterparts. Such a 
data set, however, is not currently available. As a 
substitute, we are using native English text for 
training, currently we train on the full text of the 
English Encarta encyclopedia (560k sentences) and 
a random set of 1M sentences from a Reuters news 
data set. The strategy behind these modules is 
similar to a contextual speller as described, for 
example, in (Golding and Roth 1999). For each 
potential insertion point of a determiner or 
preposition we extract context features within a 
window of six tokens to the right and to the left. 
For each token within the window we extract its 
relative position, the token string, and its part-of-
speech tag. Potential insertion sites are determined 
heuristically from the sequence of POS tags. Based 
on these features, we train a classifier for 
preposition choice and determiner choice. 
Currently we train decision tree classifiers with the 
WinMine toolkit (Chickering 2002). We also 
experimented with linear SVMs, but decision trees 
performed better overall and training and 
parameter optimization were considerably more 
efficient. Before training the classifiers, we 
perform feature ablation by imposing a count 
cutoff of 10, and by limiting the number of features 
to the top 75K features in terms of log likelihood 
ratio (Dunning 1993). 
We train two separate classifiers for both 
determiners and preposition: 
? decision whether or not a 
determiner/preposition should be present 
(presence/absence or pa classifier) 
? decision which determiner/preposition is 
the most likely choice, given that a 
determiner/preposition is present (choice 
or ch classifier) 
In the case of determiners, class values for the ch 
classifier are a/an and the. Preposition choice 
(equivalent to the ?confusion set? of a contextual 
speller) is limited to a set of 13 prepositions that 
figure prominently in the errors observed in the 
JLE corpus: about, as, at, by, for, from, in, like, of, 
on, since, to, with, than, "other" (for prepositions 
not in the list). 
The decision tree classifiers produce probability 
distributions over class values at their leaf nodes. 
For a given leaf node, the most likely 
preposition/determiner is chosen as a suggestion. If 
there are other class values with probabilities 
above heuristically determined thresholds2, those 
are also included in the list of possible suggestions. 
Consider the following example of an article-
related error: 
I am teacher from Korea. 
As explained above, the suggestion provider 
module for article errors consists of two classifiers, 
one for presence/absence of an article, the other for 
article choice. The string above is first tokenized 
and then part-of-speech tagged: 
0/I/PRP   1/am/VBP   2/teacher/NN   3/from/IN   
4/Korea/NNP   5/./.  
Based on the sequence of POS tags and 
capitalization of the nouns, a heuristic determines 
that there is one potential noun phrase that could 
contain an article: teacher. For this possible article 
position, the article presence/absence classifier 
determines the probability of the presence of an 
article, based on a feature vector of pos tags and 
surrounding lexical items: 
p(article + teacher) = 0.54 
Given that the probability of an article in this 
position is higher than the probability of not having 
an article, the second classifier is consulted to 
provide the most likely choice of article: 
p(the) = 0.04 
p(a/an) = 0.96 
Given  this probability distribution, a correction 
suggestion I am teacher from Korea -> I am a 
teacher from Korea is generated and passed on to 
evaluation by the language model component. 
4.2 The Language Model 
The language model is a 5-gram model trained 
on the English Gigaword corpus (LDC2005T12). 
In order to preserve (singleton) context information 
as much as possible, we used interpolated Kneser-
Ney smoothing (Kneser and Ney 1995) without 
count cutoff. With a 120K-word vocabulary, the 
trained language model contains 54 million 
bigrams, 338 million trigrams, 801 million 4-grams 
                                                 
2 Again, we are working on learning these thresholds 
empirically from data. 
451
and 12 billion 5-grams.  In the example from the 
previous section, the two alternative strings  of the 
original user input and the suggested correction are 
scored by the language model: 
I am teacher from Korea. score = 0.19 
I am a teacher from Korea. score = 0.60 
The score for the suggested correction is 
significantly higher than the score for the original, 
so the suggested correction is provided to the user. 
4.3 The Example Provider 
In many cases, the SP will produce several 
alternative suggestions, from which the user may 
be able to pick the appropriate correction reliably. 
In other cases, however, it may not be clear which 
suggestion is most appropriate. In this event, the 
user can choose to activate the Example Provider 
(EP) which will then perform a web search to 
retrieve relevant example sentences illustrating the 
suggested correction. For each suggestion, we 
create an exact string query including a small 
window of context to the left and to the right of the 
suggested correction. The query is issued to a 
search engine, and the retrieved results are 
separated into sentences. Those sentences that 
contain the string query are added to a list of 
example candidates.  The candidates are then 
ranked by two initially implemented criteria: 
Sentence length (shorter examples are preferred in 
order to reduce cognitive load) and context overlap 
(sentences that contain additional words from the 
user input are preferred). We have not yet 
performed a user study to evaluate the usefulness 
of the examples provided by the system. Some 
examples of usage that we retrieve are given below 
with the query string in boldface: 
Original: I am teacher from Korea. 
Suggestion: I am a teacher from Korea. 
All top 3 examples: I am a teacher.  
Original: So Smokers have to see doctor more often 
than non-smokers. 
Suggestion: So Smokers have to see a doctor more 
often than non-smokers. 
Top 3 examples: 
1. Do people going through withdrawal have 
to see a doctor? 
2. Usually, a couple should wait to see a 
doctor until after they've tried to get 
pregnant for a year. 
3. If you have had congestion for over a 
week, you should see a doctor. 
Original: I want to travel Disneyland in March. 
Suggestion: I want to travel to Disneyland in 
March. 
Top 3 examples: 
1. Timothy's wish was to travel to 
Disneyland in California. 
2. Should you travel to Disneyland in 
California or to Disney World in 
Florida? 
3. The tourists who travel to Disneyland in 
California can either choose to stay in 
Disney resorts or in the hotel for 
Disneyland vacations. 
5 Evaluation 
We perform two different types of evaluation on 
our system. Automatic evaluation is performed on 
native text, under the assumption that the native 
text does not contain any errors of the type targeted 
by our system. For example, the original choice of 
preposition made in the native text would serve as 
supervision for the evaluation of the preposition 
module. Human evaluation is performed on non-
native text, with a human rater assessing each 
suggestion provided by the system. 
5.1 Individual SP Modules 
For evaluation, we split the original training data 
discussed in section 4.1 into training and test sets 
(70%/30%). We then retrained the classifiers on 
this reduced training set and applied them to the 
held-out test set. Since there are two models, one 
for preposition/determiner presence and absence 
(pa), and one for preposition/determiner choice 
(ch), we report combined accuracy numbers of the 
two classifiers. Votes(a) stands for the counts of 
votes for class value = absence from pa, votes(p) 
stands for counts of votes for presence from pa. 
Acc(pa) is the accuracy of the pa classifier, acc(ch) 
the accuracy of the choice classifier. Combined 
accuracy is defined as in Equation 1. 
 
 
??? ?? ? ?????(?) + ??? ?? ? ??? ?? ? ?????(?)
????? ?????
 
Equation 1: Combined accuracy of the 
presence/absence and choice models 
452
The total number of cases in the test set is 
1,578,342 for article correction and 1,828,438 for 
preposition correction. 
5.1.1 Determiner choice 
Accuracy of the determiner pa and ch models 
and their combination is shown in Table 1. 
Model pa ch combined 
Accuracy 89.61% 85.97% 86.07% 
Table 1: Accuracy of the determiner pa, ch, and 
combined models. 
The baseline is 69.9% (choosing the most 
frequent class label none). The overall accuracy of 
this module is state-of-the-art compared with 
results reported in the literature (Knight and 
Chander 1994, Minnen et al 2000, Lee 2004, 
Turner and Charniak 2007). Turner and Charniak 
2007 obtained the best reported accuracy to date of 
86.74%, using a Charniak language model 
(Charniak 2001) based on a full statistical parser 
on the Penn Tree Bank. These numbers are, of 
course, not directly comparable, given the different 
corpora. On the other hand, the distribution of 
determiners is similar in the PTB (as reported in 
Minnen et al 2000) and in our data (Table 2). 
 PTB Reuters/Encarta 
mix 
no determiner 70.0% 69.9% 
the 20.6% 22.2% 
a/an 9.4% 7.8% 
Table 2: distribution of determiners in the Penn 
Tree Bank and in our Reuters/Encarta data. 
Precision and recall numbers for both models on 
our test set are shown in Table 3 and Table 4. 
Article 
pa classifier 
precision recall 
presence 84.99% 79.54% 
absence 91.43% 93.95% 
Table 3: precision and recall of the article pa 
classifier. 
Article  
ch classifier 
precision Recall 
the 88.73% 92.81% 
a/an 76.55% 66.58% 
Table 4: precision and recall of the article ch 
classifier. 
5.1.2 Preposition choice 
The preposition choice model and the combined 
model achieve lower accuracy than the 
corresponding determiner models, a result that can 
be expected given the larger choice of candidates 
and hardness of the task. Accuracy numbers are 
presented in Table 5. 
Model pa ch combined 
Accuracy 91.06%% 62.32% 86.07% 
Table 5:Accuracy of the preposition pa, ch, and 
combined models. 
The baseline in this task is 28.94% (using no 
preposition). Precision and recall numbers are 
shown in Table 6 and Table 7. From Table 7 it is 
evident that prepositions show a wide range of 
predictability. Prepositions such as than and about 
show high recall and precision, due to the lexical 
and morphosyntactic regularities that govern their 
distribution. At the low end, the semantically more 
independent prepositions since and at show much 
lower precision and recall numbers. 
 
Preposition  
pa classifier 
precision recall 
presence 90.82% 87.20% 
absence 91.22% 93.78% 
Table 6: Precision and recall of the preposition pa 
classifier. 
Preposition 
ch classifier 
precision recall 
other 53.75% 54.41% 
in 55.93% 62.93% 
for 56.18% 38.76% 
of 68.09% 85.85% 
on 46.94% 24.47% 
to 79.54% 51.72% 
with 64.86% 25.00% 
at 50.00% 29.67% 
by 42.86% 60.46% 
as 76.78% 64.18% 
from 81.13% 39.09% 
since 50.00% 10.00% 
about 93.88% 69.70% 
than 95.24% 90.91% 
Table 7: Precision and recall of the preposition ch 
classifier. 
 
453
Chodorow et al (2007) present numbers on an 
independently developed system for detection of 
preposition error in non-native English. Their 
approach is similar to ours in that they use a 
classifier with contextual feature vectors.  The 
major differences between the two systems are the 
additional use of a language model in our system 
and, from a usability perspective, in the example 
provider module we added to the correction 
process. Since both systems are evaluated on 
different data sets3, however, the numbers are not 
directly comparable. 
5.2 Language model Impact 
The language model gives us an additional piece 
of information to make a decision as to whether a 
correction is indeed valid. Initially, we used the 
language model as a simple filter: any correction 
that received a lower language model score than 
the original was filtered out. As a first approxi-
mation, this was an effective step: it reduced the 
number of preposition corrections by 66.8% and 
the determiner corrections by 50.7%, and increased 
precision dramatically. The language model alone, 
however, does not provide sufficient evidence: if 
we produce a full set of preposition suggestions for 
each potential preposition location and rank these 
suggestions by LM score alone, we only achieve 
58.36% accuracy on Reuters data. 
Given that we have multiple pieces of 
information for a correction candidate, namely the 
class probability assigned by the classifier and the 
language model score, it is more effective to 
combine these into a single score and impose a 
tunable threshold on the score to maximize 
precision. Currently, this threshold is manually set 
by analyzing the flags in a development set. 
5.3 Human Evaluation 
A complete human evaluation of our system would 
have to include a thorough user study and would 
need to assess a variety of criteria, from the 
accuracy of individual error detection and 
corrections to the general helpfulness of real web-
based example sentences. For a first human 
evaluation of our system prototype, we decided to 
                                                 
3 Chodorow et al (2007) evaluate their system on 
proprietary student essays from non-native students, 
where they achieve 77.8% precision at 30.4% recall for 
the preposition substitution task. 
simply address the question of accuracy on the 
determiner and preposition choice tasks on a 
sample of non-native text.  
For this purpose we ran the system over a 
random sample of sentences from the CLEC 
corpus (8k for the preposition evaluation and 6k 
for the determiner evaluation). An independent 
judge annotated each flag produced by the system 
as belonging to one of the following categories: 
? (1) the correction is valid and fixes the 
problem 
? (2) the error is correctly identified, but 
the suggested correction does not fix it 
? (3) the original and the rewrite are both 
equally good 
? (4) the error is at or near the suggested 
correction, but it is a different kind of 
error (not having to do with 
prepositions/determiners) 
? (5) There is a spelling error at or near 
the correction 
? (6) the correction is wrong, the original 
is correct 
Table 8 shows the results of this human 
assessment for articles and prepositions. 
 
Articles (6k 
sentences) 
Prepositions 
(8k 
sentences) 
count ratio count ratio 
(1) correction is 
valid 
240 55% 165 46% 
(2) error identified, 
suggestion does 
not fix it 
10 2% 17 5% 
(3) original and 
suggestion equally 
good 
17 4% 38 10% 
(4) misdiagnosis 65 15% 46 13% 
(5) spelling error 
near correction 
37 8% 20 6% 
(6) original correct 70 16% 76 21% 
Table 8: Article and preposition correction 
accuracy on CLEC data. 
The distribution of corrections across deletion, 
insertion and substitution operations is illustrated 
in Table 9. The most common article correction is 
insertion of a missing article. For prepositions, 
substitution is the most common correction, again 
an expected result given that the presence of a 
454
preposition is easier to determine for a non-native 
speaker than the actual choice of the correct 
preposition. 
 deletion insertion substitution 
Articles 8% 79% 13% 
Prepositions 15% 10% 76% 
Table 9: Ratio of deletion, insertion and 
substitution operations. 
6 Conclusion and Future Work 
Helping a non-native writer of English with the 
correct choice of prepositions and 
definite/indefinite determiners is a difficult 
challenge. By combining contextual speller based 
methods with language model scoring and 
providing web-based examples, we can leverage 
the combination of evidence from multiple 
sources. 
The human evaluation numbers presented in the 
previous section are encouraging. Article and 
preposition errors present the greatest difficulty for 
many learners as well as machines, but can 
nevertheless be corrected even in extremely noisy 
text with reasonable accuracy. Providing 
contextually appropriate real-life examples 
alongside with the suggested correction will, we 
believe, help the non-native user reach a more 
informed decision than just presenting a correction 
without additional evidence and information. 
The greatest challenge we are facing is the 
reduction of ?false flags?, i.e. flags where both 
error detection and suggested correction are 
incorrect. Such flags?especially for a non-native 
speaker?can be confusing, despite the fact that the 
impact is mitigated by the set of examples which 
may clarify the picture somewhat and help the 
users determine that they are dealing with an 
inappropriate correction. In the current system we 
use a set of carefully crafted heuristic thresholds 
that are geared towards minimizing false flags on a 
development set, based on detailed error analysis. 
As with all manually imposed thresholding, this is 
both a laborious and brittle process where each 
retraining of a model requires a re-tuning of the 
heuristics. We are currently investigating a learned 
ranker that combines information from language 
model and classifiers, using web counts as a 
supervision signal. 
7 Acknowledgements 
We thank Claudia Leacock (Butler Hill Group) for 
her meticulous analysis of errors and human 
evaluation of the system output, as well as for 
much invaluable feedback and discussion. 
References 
Bond, Francis. 2005.  Translating the Untranslatable: A 
Solution to the Problem of Generating English 
Determiners. CSLI Publications. 
Charniak, Eugene. 2001. Immediate-head parsing for 
language models. In Proceedingsof the 39th Annual 
Meeting of the Association for Computational 
Linguistics, pp 116-123. 
Chickering, David Maxwell. 2002. The WinMine 
Toolkit.  Microsoft Technical Report 2002-103. 
Chodorow, Martin, Joel R. Tetreault and Na-Rae Han. 
2007. Detection of Grammatical Errors Involving 
Prepositions. In Proceedings of the 4th ACL-SIGSEM 
Workshop on Prepositions, pp 25-30. 
Crystal, David. 1997.  Global English. Cambridge 
University Press. 
Rachele De Felice and Stephen G Pulman. 2007. 
Automatically acquiring models of preposition use. 
Proceedings of the ACL-07 Workshop on 
Prepositions. 
Dunning, Ted. 1993. Accurate Methods for the Statistics 
of Surprise and Coincidence. Computational 
Linguistics, 19:61-74. 
Gamon, Michael, Eric Ringger, and Simon Corston-
Oliver. 2002. Amalgam: A machine-learned 
generation module. Microsoft Technical Report, 
MSR-TR-2002-57. 
Golding, Andrew R. and Dan Roth. 1999. A Winnow 
Based Approach to Context-Sensitive Spelling 
Correction. Machine Learning, pp. 107-130. 
Gui, Shicun and Huizhong Yang (eds.). 2003. Zhongguo 
Xuexizhe Yingyu Yuliaohu. (Chinese Learner English 
Corpus). Shanghai Waiyu Jiaoyu Chubanshe.. 
Han, Na-Rae., Chodorow, Martin and Claudia Leacock. 
2004. Detecting errors in English article usage with a 
maximum entropy classifier trained on a large, 
diverse corpus. Proceedings of the 4th international 
conference on language resources and evaluation, 
Lisbon, Portugal. 
 
 
455
Han, Na-Rae. Chodorow, Martin., and Claudia Leacock. 
(2006). Detecting errors in English article usage by 
non-native speakers. Natural Language Engineering, 
12(2), 115-129. 
Heidorn, George. 2000. Intelligent Writing Assistance. 
In Robert Dale, Herman Moisl, and Harold Somers 
(eds.). Handbook of Natural Language Processing.  
Marcel Dekker.  pp 181 -207. 
Izumi, Emi, Kiyotaka Uchimoto and Hitoshi Isahara. 
2004. The NICT JLE Corpus: Exploiting the 
Language Learner?s Speech Database for Research 
and Education. International Journal of the 
Computer, the Internet and Management 12:2, pp 
119 -125. 
Kneser, Reinhard. and Hermann Ney. 1995. Improved 
backing-off for m-gram language modeling. 
Proceedings of the IEEE International Conference 
on Acoustics, Speech, and Signal Processing, volume 
1. 1995. pp. 181?184. 
Knight, Kevin and Ishwar Chander. 1994. Automatic 
Postediting of Documents. Proceedings of the 
American Association of Artificial Intelligence, pp 
779-784. 
Lee, John. 2004. Automatic Article Restoration. 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics, pp. 31-
36. 
Liu, Ting, Mingh Zhou, JianfengGao, Endong Xun, and 
Changning Huan. 2000. PENS: A Machine-Aided 
English Writing System for Chinese Users. 
Proceedings of ACL 2000, pp 529-536. 
Minnen, Guido, Francis Bond and Ann Copestake. 
2000. Memory-Based Learning for Article 
Generation. Proceedings of the Fourth Conference 
on Computational Natural Language Learning and 
of the Second Learning Language in Logic 
Workshop, pp 43-48. 
Suzuki, Hisami and Kristina Toutanova. 2006. Learning 
to Predict Case Markers in Japanese. Proceedings of 
COLING-ACL, pp. 1049-1056. 
Toutanova, Kristina and Hisami Suzuki. 2007 
Generating Case Markers in Machine Translation.  
Proceedings of NAACL-HLT. 
Turner, Jenine and Eugene Charniak. 2007. Language 
Modeling for Determiner Selection. In Human 
Language Technologies 2007: The Conference of the 
North American Chapter of the Association for 
Computational Linguistics; Companion Volume, 
Short Papers, pp 177-180. 
Yi, Xing, Jianfeng Gao and William B. Dolan. 2008. 
Web-Based English Proofing System for English as a 
Second Language Users. To be presented at IJCNLP 
2008. 
456
Using N-Grams to Understand the Nature of Summaries 
 
 
Michele Banko and Lucy Vanderwende 
One Microsoft Way 
Redmond, WA 98052 
{mbanko, lucyv}@microsoft.com 
 
 
Abstract 
Although single-document summarization is a 
well-studied task, the nature of multi-
document summarization is only beginning to 
be studied in detail.  While close attention has 
been paid to what technologies are necessary 
when moving from single to multi-document 
summarization, the properties of human-
written multi-document summaries have not 
been quantified.  In this paper, we empirically 
characterize human-written summaries 
provided in a widely used summarization 
corpus by attempting to answer the questions: 
Can multi-document summaries that are 
written by humans be characterized as 
extractive or generative?  Are multi-document 
summaries less extractive than single-
document summaries?  Our results suggest that 
extraction-based techniques which have been 
successful for single-document summarization 
may not be sufficient when summarizing 
multiple documents. 
1 Introduction 
The explosion of available online text has made it 
necessary to be able to present information in a succinct, 
navigable manner. The increased accessibility of 
worldwide online news sources and the continually 
expanding size of the worldwide web place demands on 
users attempting to wade through vast amounts of text.  
Document clustering and multi-document summarization 
technologies working in tandem promise to ease some of 
the burden on users when browsing related documents. 
Summarizing a set of documents brings about 
challenges that are not present when summarizing a 
single document. One might expect that a good multi-
document summary will present a synthesis of multiple 
views of the event being described over different 
documents, or present a high-level view of an event that 
is not explicitly reflected in any single document. A 
useful multi-document summary may also indicate the 
presence of new or distinct information contained within 
a set of documents describing the same topic (McKeown 
et. al., 1999, Mani and Bloedorn, 1999). To meet these 
expectations, a multi-document summary is required to 
generalize, condense and merge information coming 
from multiple sources. 
Although single-document summarization is a well-
studied task (see Mani and Maybury, 1999 for an 
overview), multi-document summarization is only 
recently being studied closely (Marcu & Gerber 2001). 
While close attention has been paid to multi-document 
summarization technologies (Barzilay et al 2002, 
Goldstein et al2000), the inherent properties of human-
written multi-document summaries have not yet been 
quantified. In this paper, we seek to empirically 
characterize ideal multi-document summaries in part by 
attempting to answer the questions: Can multi-document 
summaries that are written by humans be characterized 
as extractive or generative?  Are multi-document 
summaries less extractive than single-document 
summaries? Our aim in answering these questions is to 
discover how the nature of multi-document summaries 
will impact our system requirements. 
We have chosen to focus our experiments on the data 
provided for summarization evaluation during the 
Document Understanding Conference (DUC). While we 
recognize that other summarization corpora may exhibit 
different properties than what we report, the data 
prepared for DUC evaluations is widely used, and 
continues to be a powerful force in shaping directions in 
summarization research and evaluation.  
In the following section we describe previous work 
related to investigating the potential for extractive 
summaries. Section 3 describes a new approach for 
assessing the degree to which a summary can be 
described as extractive, and reports our findings for both 
single and multiple document summarization tasks. We 
conclude with a discussion of our findings in Section 4. 
2 Related Work 
Jing (2002) previously examined the degree to which 
single-document summaries can be characterized as 
extractive. Based on a manual inspection of 15 human-
written summaries, she proposes that for the task of 
single-document summarization, human summarizers 
use a ?cut-and-paste? approach in which six main 
operations are performed: sentence reduction, sentence 
combination, syntactic transformation, reordering, 
lexical paraphrasing, and generalization or specification.  
The first four operations are reflected in the construction 
of an HMM model that can be used to decompose human 
summaries.  According to this model, 81% of summary 
sentences contained in a corpus of 300 human-written 
summaries of news articles on telecommunications were 
found to fit the cut-and-paste method, with the rest 
believed to have been composed from scratch.1  
Another recent study (Lin and Hovy, 2003) 
investigated the extent to which extractive methods may 
be sufficient for summarization in the single-document 
case.  By computing a performance upper-bound for 
pure sentence extraction, they found that state-of-the-art 
extraction-based systems are still 15%-24%2  away from 
this limit, and 10% away from average human 
performance. While this sheds light on how much gain 
can be achieved by optimizing sentence extraction 
methods for single-document summarization, to our 
knowledge, no one has assessed the potential for 
extraction-based systems when attempting to summarize 
multiple documents. 
3 Using N-gram Sequences to 
Characterize Summaries 
Our approach to characterizing summaries is much 
simpler than what Jing has described and is based on the 
following idea: if human-written summaries are 
extractive, then we should expect to see long spans of 
text that have been lifted from the source documents to 
form a summary.  
Note that this holds under the assumptions made by 
Jing?s model of operations that are performed by human 
summarizers.  In the examples of operations given by 
Jing, we notice that long n-grams are preserved 
(designated by brackets), even in the operations mostly 
likely to disrupt the original text: 
 
                                                          
1
  Jing considers a sentence to have been generated from 
scratch if fewer than half of the words were composed of 
terms coming from the original document. 
2
 The range in potential gains is due to possible 
variations in summary length. 
Sentence Reduction:  
Document sentence: When it arrives sometime next 
year in new TV sets, the V-chip will give parents a new 
and potentially revolutionary device to block out 
programs they don?t want their children to see. 
Summary sentence: [The V-chip will give parents a] 
[device to block out programs they don?t want their 
children to see.] 
 
Syntactic Transformation: 
Document sentence: Since annoy.com enables 
visitors to send unvarnished opinions to political and 
other figures in the news, the company was concerned 
that its activities would be banned by the statute. 
Summary sentence: [Annoy.com enables visitors to 
send unvarnished opinions to political and other figures 
in the news] and feared the law could put them out of 
business. 
 
Sentence Combination: 
Document sentence 1: But it also raises serious 
questions about the privacy of such highly personal 
information wafting about the digital world. 
Document sentence 2: The issue thus fits squarely 
into the broader debate about privacy and security on the 
Internet, whether it involves protecting credit card 
numbers or keeping children from offensive information. 
Summary sentence: [But it also raises] the issue of 
[privacy of such] [personal information] and this issue 
hits the nail on the head [in the broader debate about 
privacy and security on the Internet.] 
3.1 Data and Experiments 
For our experiments we used data made available from 
the 2001 Document Understanding Conference (DUC), 
an annual large-scale evaluation of summarization 
systems sponsored by the National Institute of Standards 
and Technology (NIST).  In this corpus, NIST has 
gathered documents describing 60 events, taken from the 
Associated Press, Wall Street Journal, FBIS San Jose 
Mercury, and LA Times newswires. An event is 
described by between 3 and 20 separate (but not 
necessarily unique) documents; on average a cluster 
contains 10 documents. Of the 60 available clusters, we 
used the portion specifically designated for training, 
which contains a total of 295 documents distributed over 
30 clusters. 
As part of the DUC 2001 summarization corpus, 
NIST also provides four hand-written summaries of 
different lengths for every document cluster, as well as 
100-word summaries of each document. Since we 
wished to collectively compare single-document 
summaries against multi-document summaries, we used 
the 100-word multi-document summaries for our 
analysis.  It is important to note that for each cluster, all 
summaries (50, 100, 200 and 400-word multi-document 
and 100-word per-document) have been written by the 
same author. NIST used a total of ten authors, each 
providing summaries for 3 of the 30 topics. The 
instructions provided did not differ per task; in both 
single and multi-document scenarios, the authors were 
directed to use complete sentences and told to feel free to 
use their own words (Over, 2004). 
To compare the text of human-authored multi-
document summaries to the full-text documents 
describing the events, we automatically broke the 
documents into sentences, and constructed a minimal 
tiling of each summary sentence. Specifically, for each 
sentence in the summary, we searched for all n-grams 
that are present in both the summary and the documents, 
placing no restrictions on the potential size of an n-gram. 
We then covered each summary sentence with the n-
grams, optimizing to use as few n-grams as possible (i.e. 
favoring n-grams that are longer in length). For this 
experiment, we normalized the data by converting all 
terms to lowercase and removing punctuation. 
3.2  Results 
On average, we found the length of a tile to be 4.47 for 
single-document summaries, compared with 2.33 for 
multi-document summaries. We discovered that 61 out 
of all 1667 hand-written single-document summary 
sentences exactly matched a sentence in the source 
document, however we did not find any sentences for 
which this was the case when examining multi-document 
summaries.  
We also wanted to study how many sentences are  
fully tiled by phrases coming from exactly one sentence 
in the document corpus, and found that while no 
sentences from the multi-document summaries matched 
this criteria, 7.6% of sentences in the single-document 
summaries could be tiled in this manner.  When trying to 
tile sentences with tiles coming from only one document 
sentence, we found that we could tile, on average,  93% 
of a  single-document sentence in that manner, compared 
to an average of 36% of a multi-document sentence.  
This suggests that for multi-document summarization, 
we are not seeing any instances of what can be 
considered  single-sentence compression. Table 1 
summarizes the findings we have presented in this 
section. 
 SingleDoc MultiDoc 
Average Tile Size 
(words) 4.47 2.33 
Max Tile Size 
(words) 38 24 
Exact sentence 
matches 3.7% 0% 
Complete tiling 
from single 
sentence  
7.6% 0% 
Table 1. Comparison of Summary Tiling 
Figure 1 shows the relative frequency with which a 
summary sentence is optimally tiled using tile-sizes up to  
25 words in length in both the single and multi-
document scenarios. The data shows that the relative 
frequency with which a single-document summary 
sentence is optimally tiled using n-grams containing 3 or 
more words is consistently higher compared to the multi-
document case. Not shown on the histogram (due to 
insufficient readability) is that we found 379 tiles (of 
approximately 86,000) between 25 and 38 words long 
covering sentences from single-document summaries. 
No tiles longer than 24 words were found for multi-
document summaries. 
In order to test whether tile samples coming from 
tiling of single-document summaries and multi-
document summaries are likely to have come from the 
same underlying population, we performed two one-
tailed unpaired t-tests, in one instance assuming equal 
variances, and in the other case asssuming the variances 
were unequal.  For these statistical significance tests, we 
randomly sampled 100 summary sentences from each 
task, and extracted the lengths of the n-grams found via 
minmal tiling. This resulted in the creation of a sample 
of 551 tiles for single-document sentences and 735 tiles 
for multi-document sentences. 
For both tests (performed with ?=0.05), the P-values 
were low enough (0.00033 and 0.000858, respectively) 
to be able to reject the null hypothesis that the average 
tile length coming from single-document summaries is 
the same as the average tile length found in multi-
document summaries. We chose to use a one-tailed P-
value because based on our experiments we already 
suspected that the single-document tiles had a larger 
mean. 
 
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Tile Length
Singledoc Multidoc
 Figure 1. Comparison of Relative Frequencies of 
Optimal Tile Lengths 
4 Conclusions and Future Work 
Our experiments show that when writing multi-
document summaries, human summarizers do not appear 
to be cutting and pasting phrases in an extractive fashion.  
On average, they are borrowing text around the bigram 
level, instead of extracting long sequences of words or 
full sentences as they tend to do when summarizing a 
single document. The extent to which human 
summarizers form extractive summaries during single 
and multi-document summarization was found to be 
different at a level which is statistically significant.  
These findings are additionally supported by the fact that 
automatic n-gram-based evaluation measures now being 
used to assess predominately extractive multi-document 
summarization systems correlate strongly with human 
judgments when restricted to the usage of unigrams and 
bigrams, but correlate weakly when longer n-grams are 
factored into the equation (Lin & Hovy, 2003). In the 
future, we wish to apply our method to other corpora, 
and to explore the extent to which different 
summarization goals, such as describing an event or 
providing a biography, affect the degree to which 
humans employ rewriting as opposed to extraction. 
Despite the unique requirements for multi-document 
summarization, relatively few systems have crossed over 
into employing generation and reformulation (McKeown 
& Radev, 1995, Nenkova, et al 2003). For the most part, 
summarization systems continue to be based on sentence 
extraction methods. Considering that humans appear to 
be generating summary text that differs widely from 
sentences in the original documents, we suspect that 
approaches which make use of generation and 
reformulation techniques may yield the most promise for 
multi-document summarization.  We would like to 
empirically quantify to what extent current 
summarization systems reformulate text, by applying the 
techniques presented in this paper to system output. 
Finally, the potential impact of our findings with 
respect to recent evaluation metrics should not be 
overlooked. Caution must be given when employing 
automatic evaluation metrics based on the overlap of n-
grams between human references and system summaries.  
When reference summaries do not contain long n-grams 
drawn from the source documents, but are instead 
generated in the author?s own words, the use of a large 
number of reference summaries becomes more critical.  
Acknowledgements 
The authors wish to thank Eric Ringger for providing 
tools used to construct the tiles, and Bob Moore for 
suggestions pertaining to our experiments. 
References 
Regina Barzilay, Noemie Elhadad, Kathleen McKeown. 
2002. "Inferring Strategies for Sentence Ordering in 
Multidocument Summarization." JAIR, 17:35-55. 
Jade Goldstein, Vibhu Mittal, Mark Kantrowitz and 
Jaime Carbonell, 2000. Multi-Document 
Summarization by Sentence Extraction. In the 
Proceedings of the ANLP/NAACL Workshop on 
Automatic Summarization. Seattle, WA 
Hongyan Jing. 2002. Using Hidden Markov Modeling to 
Decompose Human-Written Summaries. 
Computational Linguistics 28(4): 527-543. 
Chin-Yew Lin, and E.H. Hovy 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of 2003 
Language Technology Conference (HLT-NAACL 
2003), Edmonton, Canada. 
Chin-Yew Lin and Eduard.H. Hovy. 2003. The Potential 
and Limitations of Sentence Extraction for 
Summarization. In Proceedings of the Workshop on 
Automatic Summarization post-conference workshop 
of HLT-NAACL-2003. Edmonton, Canada. 
Daniel Marcu and Laurie Gerber. 2001. An Inquiry into 
the Nature of Multidocument Abstracts, Extracts, and 
Their Evaluation. Proceedings of the NAACL-2001 
Workshop on Automatic Summarization 
Inderjeet Mani and Eric Bloedorn. 1999. Summarizing 
similarities and differences among related documents. 
Information Retrieval, 1, pp. 35-67. 
Inderjeet Mani and Mark Maybury (Eds.). 1999. 
Advances in Automatic Text Summarization. MIT 
Press, Cambridge MA. 
Kathy McKeown, Judith Klavans, Vasileios 
Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin. 
1999. Towards multidocument summarization by 
reformulation: Progress and prospects. In Proceedings 
of AAAI. 
Kathleen R. McKeown and Dragomir R. Radev. 
Generating summaries of multiple news articles. 
1995. In Proceedings, 18th Annual International 
ACM SIGIR Conference on Research and 
Development in Information Retrieval, pages 74-82. 
Ani Nenkova, Barry Schiffman, Andrew Schlaiker, 
Sasha Blair-Goldensohn, Regina Barzilay, Sergey 
Sigelman, Vasileios Hatzivassiloglou, and Kathleen 
McKeown. 2003. Columbia at the Document 
Understanding Conference. Document Understanding 
Conference 2003. 
Paul Over. 2004. Personal communication.
 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 33?40,
New York, June 2006. c?2006 Association for Computational Linguistics
Effectively Using Syntax for Recognizing False Entailment
Rion Snow
Computer Science Department
Stanford University
Stanford, CA 94305
rion@cs.stanford.edu
Lucy Vanderwende and Arul Menezes
Microsoft Research
One Microsoft Way
Redmond, WA 98027
{lucyv,arulm}@microsoft.com
Abstract
Recognizing textual entailment is a chal-
lenging problem and a fundamental com-
ponent of many applications in natural
language processing. We present a novel
framework for recognizing textual entail-
ment that focuses on the use of syntactic
heuristics to recognize false entailment.
We give a thorough analysis of our sys-
tem, which demonstrates state-of-the-art
performance on a widely-used test set.
1 Introduction
Recognizing the semantic equivalence of two frag-
ments of text is a fundamental component of many
applications in natural language processing. Recog-
nizing textual entailment, as formulated in the recent
PASCAL Challenge 1, is the problem of determining
whether some text sentence T entails some hypothe-
sis sentence H .
The motivation for this formulation was to iso-
late and evaluate the application-independent com-
ponent of semantic inference shared across many ap-
plication areas, reflected in the division of the PAS-
CAL RTE dataset into seven distinct tasks: Informa-
tion Extraction (IE), Comparable Documents (CD),
Reading Comprehension (RC), Machine Translation
(MT), Information Retrieval (IR), Question Answer-
ing (QA), and Paraphrase Acquisition (PP).
1http://www.pascal-network.org/Challenges/RTE. The ex-
amples given throughout this paper are from the first PASCAL
RTE dataset, described in Section 6.
The RTE problem as presented in the PASCAL
RTE dataset is particularly attractive in that it is a
reasonably simple task for human annotators with
high inter-annotator agreement (95.1% in one inde-
pendent labeling (Bos and Markert, 2005)), but an
extremely challenging task for automated systems.
The highest accuracy systems on the RTE test set
are still much closer in performance to a random
baseline accuracy of 50% than to the inter-annotator
agreement. For example, two high-accuracy systems
are those described in (Tatu and Moldovan, 2005),
achieving 60.4% accuracy with no task-specific in-
formation, and (Bos and Markert, 2005), which
achieves 61.2% task-dependent accuracy, i.e. when
able to use the specific task labels as input.
Previous systems for RTE have attempted a wide
variety of strategies. Many previous approaches
have used a logical form representation of the text
and hypothesis sentences, focusing on deriving a
proof by which one can infer the hypothesis logical
form from the text logical form (Bayer et al, 2005;
Bos and Markert, 2005; Raina et al, 2005; Tatu and
Moldovan, 2005). These papers often cite that a ma-
jor obstacle to accurate theorem proving for the task
of textual entailment is the lack of world knowledge,
which is frequently difficult and costly to obtain and
encode. Attempts have been made to remedy this
deficit through various techniques, including model-
building (Bos and Markert, 2005) and the addition
of semantic axioms (Tatu and Moldovan, 2005).
Our system diverges from previous approaches
most strongly by focusing upon false entailments;
rather than assuming that a given entailment is false
until proven true, we make the opposite assump-
33
tion, and instead focus on applying knowledge-free
heuristics that can act locally on a subgraph of syn-
tactic dependencies to determine with high confi-
dence that the entailment is false. Our approach is
inspired by an analysis of the RTE dataset that sug-
gested a syntax-based approach should be approxi-
mately twice as effective at predicting false entail-
ment as true entailment (Vanderwende and Dolan,
2006). The analysis implied that a great deal of syn-
tactic information remained unexploited by existing
systems, but gave few explicit suggestions on how
syntactic information should be applied; this paper
provides a starting point for creating the heuristics
capable of obtaining the bound they suggest2.
2 System Description
Similar to most other syntax-based approaches to
recognizing textual entailment, we begin by rep-
resenting each text and hypothesis sentence pair
in logical forms. These logical forms are gener-
ated using NLPWIN3, a robust system for natural
language parsing and generation (Heidorn, 2000).
Our logical form representation may be consid-
ered equivalently as a set of triples of the form
RELATION(nodei, nodej), or as a graph of syntac-
tic dependencies; we use both terminologies inter-
changeably. Our algorithm proceeds as follows:
1. Parse each sentence with the NLPWIN parser,
resulting in syntactic dependency graphs for the
text and hypothesis sentences.
2. Attempt an alignment of each content node in
the dependency graph of the hypothesis sen-
tence to some node in the graph of the text sen-
tence, using a set of heuristics for alignment
(described in Section 3).
3. Using the alignment, apply a set of syntactic
heuristics for recognizing false entailment (de-
scribed in Section 4); if any match, predict that
the entailment is false.
2(Vanderwende and Dolan, 2006) suggest that the truth or
falsehood of 48% of the entailment examples in the RTE test set
could be correctly identified via syntax and a thesaurus alone;
thus by random guessing on the rest of the examples one might
hope for an accuracy level of 0.48 + 0.522 = 74%.3To aid in the replicability of our experiments, we have
published the NLPWIN logical forms for all sentences from
the development and test sets in the PASCAL RTE dataset at
http://research.microsoft.com/nlp/Projects/RTE.aspx.
lemma: freepos: Verbfeatures: Past,Pass,T1,Proposition
lemma: _Xpos: PronTsub
lemma: hostagepos: Nounfeatures: Plur,Humn,Count,Anim,Conc,Humn_sr
Tobj
lemma: sixpos: Adjfeatures: Quant,Plur,Num,Value 6Lops
lemma: Iraqpos: Nounfeatures: Sing,PrprN,Pers3,Cntry
Locn_in
Figure 1: Logical form produced by NLPWIN for
the sentence ?Six hostages in Iraq were freed.?
4. If no syntactic heuristic matches, back off to
a lexical similarity model (described in section
5.1), with an attempt to align detected para-
phrases (described in section 5.2).
In addition to the typical syntactic information pro-
vided by a dependency parser, the NLPWIN parser
provides an extensive number of semantic features
obtained from various linguistic resources, creating
a rich environment for feature engineering. For ex-
ample, Figure 1 (from Dev Ex. #616) illustrates the
dependency graph representation we use, demon-
strating the stemming, part-of-speech tagging, syn-
tactic relationship identification, and semantic fea-
ture tagging capabilities of NLPWIN.
We define a content node to be any node whose
lemma is not on a small stoplist of common stop
words. In addition to content vs. non-content nodes,
among content nodes we distinguish between en-
tities and nonentities: an entity node is any node
classified by the NLPWIN parser as being a proper
noun, quantity, or time.
Each of the features of our system were developed
from inspection of sentence pairs from the RTE de-
velopment data set, and used in the final system only
if they improved the system?s accuracy on the de-
velopment set (or improved F-score if accuracy was
unchanged); sentence pairs in the RTE test set were
left uninspected and used for testing purposes only.
3 Linguistic cues for node alignment
Our syntactic heuristics for recognizing false entail-
ment rely heavily on the correct alignment of words
and multiword units between the text and hypothesis
logical forms. In the notation below, we will con-
sider h and t to be nodes in the hypothesis H and
34
Hypothesis: ??Hepburn, who won four Oscars...??
Text: ??Hepburn, a four-time Academy Award winner...??
HepburnNoun winVerbTsub
HepburnNoun
Stringmatch
OscarNounTobj
winnerNoun
Derivationalform match
fourAdjLops
Academy_AwardNoun
Synonymmatch
four-timeAdj
Valuematch
Appostn
Attrib
Mod
Figure 2: Example of synonym, value, and deriva-
tional form alignment heuristics, Dev Ex. #767
text T logical forms, respectively. To accomplish
the task of node alignment we rely on the following
heuristics:
3.1 WordNet synonym match
As in (Herrera et al, 2005) and others, we align
a node h ? H to any node t ? T that has both
the same part of speech and belongs to the same
synset in WordNet. Our alignment considers mul-
tiword units, including compound nouns (e.g., we
align ?Oscar? to ?Academy Award? as in Figure 2),
as well as verb-particle constructions such as ?set
off? (aligned to ?trigger? in Test Ex. #1983).
3.2 Numeric value match
The NLPWIN parser assigns a normalized numeric
value feature to each piece of text inferred to cor-
respond to a numeric value; this allows us to align
?6th? to ?sixth? in Test Ex. #1175. and to align ?a
dozen? to ?twelve? in Test Ex. #1231.
3.3 Acronym match
Many acronyms are recognized using the syn-
onym match described above; nonetheless, many
acronyms are not yet in WordNet. For these cases we
have a specialized acronym match heuristic which
aligns pairs of nodes with the following properties:
if the lemma for some node h consists only of cap-
italized letters (with possible interceding periods),
and the letters correspond to the first characters of
some multiword lemma for some t ? T , then we
consider h and t to be aligned. This heuristic allows
us to align ?UNDP? to ?United Nations Develop-
ment Programme? in Dev Ex. #357 and ?ANC? to
?African National Congress? in Test Ex. #1300.
3.4 Derivational form match
We would like to align words which have the same
root form (or have a synonym with the same root
form) and which possess similar semantic meaning,
but which may belong to different syntactic cate-
gories. We perform this by using a combination of
the synonym and derivationally-related form infor-
mation contained within WordNet. Explicitly our
procedure for constructing the set of derivationally-
related forms for a node h is to take the union of all
derivationally-related forms of all the synonyms of
h (including h itself), i.e.:
DERIV(h) = ?s?WN-SYN(h)WN-DERIV(s)
In addition to the noun/verb derivationally-related
forms, we detect adjective/adverb derivationally-
related forms that differ only by the suffix ?ly?.
Unlike the previous alignment heuristics, we do
not expect that two nodes aligned via derivationally-
related forms will play the same syntactic role in
their respective sentences. Thus we consider two
nodes aligned in this way to be soft-aligned, and we
do not attempt to apply our false entailment recog-
nition heuristics to nodes aligned in this way.
3.5 Country adjectival form / demonym match
As a special case of derivational form match, we
soft-align matches from an explicit list of place
names, adjectival forms, and demonyms4; e.g.,
?Sweden? and ?Swedish? in Test Ex. #1576.
3.6 Other heuristics for alignment
In addition to these heuristics, we implemented a hy-
ponym match heuristic similar to that discussed in
(Herrera et al, 2005), and a heuristic based on the
string-edit distance of two lemmas; however, these
heuristics yielded a decrease in our system?s accu-
racy on the development set and were thus left out
of our final system.
4 Recognizing false entailment
The bulk of our system focuses on heuristics for
recognizing false entailment. For purposes of no-
tation, we define binary functions for the existence
4List of adjectival forms and demonyms based on the list at:
http://en.wikipedia.org/wiki/List of demonyms
35
Unaligned Entity: ENTITY(h) ? ?t.?ALIGN(h, t) ? False.
Negation Mismatch: ALIGN(h, t) ? NEG(t) 6= NEG(h) ? False.
Modal Mismatch: ALIGN(h, t) ? MOD(t) ? ?MOD(h) ? False.
Antonym Match: ALIGN(h1, t1) ? REL(h0, h1) ? REL(t0, t1) ? LEMMA(t0) ? ANTONYMS(h0) ? False
Argument Movement: ALIGN(h1, t1) ? ALIGN(h2, t2) ? REL(h1, h2) ? ?REL(t1, t2) ? REL ? {SUBJ, OBJ, IND} ? False
Superlative Mismatch: ?(SUPR(h1) ? (ALIGN(h1, t1) ? ALIGN(h2, t2) ? REL1(h2, h1) ? REL1(t2, t1)
??t3.(REL2(t2, t3) ? REL2 ? {MOD,POSSR,LOCN} ? REL2(h2, h3) ? ALIGN(h3, t3))) ? False
Conditional Mismatch: ALIGN(h1, t1) ? ALIGN(h2, t2) ? COND ? PATH(t1, t2) ? COND /? PATH(h1, h2) ? False
Table 1: Summary of heuristics for recognizing false entailment
of each semantic node feature recognized by NLP-
WIN; e.g., if h is negated, we state that NEG(h) =
TRUE. Similarly we assign binary functions for
the existence of each syntactic relation defined over
pairs of nodes. Finally, we define the function
ALIGN(h, t) to be true if and only if the node h ? H
has been ?hard-aligned? to the node t ? T using one
of the heuristics in Section 3. Other notation is de-
fined in the text as it is used. Table 1 summarizes all
heuristics used in our final system to recognize false
entailment.
4.1 Unaligned entity
If some node h has been recognized as an entity (i.e.,
as a proper noun, quantity, or time) but has not been
aligned to any node t, we predict that the entailment
is false. For example, we predict that Test Ex. #1863
is false because the entities ?Suwariya?, ?20 miles?,
and ?35? in H are unaligned.
4.2 Negation mismatch
If any two nodes (h, t) are aligned, and one (and
only one) of them is negated, we predict that the en-
tailment is false. Negation is conveyed by the NEG
feature in NLPWIN. This heuristic allows us to pre-
dict false entailment in the example ?Pertussis is not
very contagious? and ?...pertussis, is a highly conta-
gious bacterial infection? in Test Ex. #1144.
4.3 Modal auxiliary verb mismatch
If any two nodes (h, t) are aligned, and t is modified
by a modal auxiliary verb (e.g, can, might, should,
etc.) but h is not similarly modified, we predict that
the entailment is false. Modification by a modal aux-
iliary verb is conveyed by the MOD feature in NLP-
WIN. This heuristic allows us to predict false en-
tailment between the text phrase ?would constitute
a threat to democracy?, and the hypothesis phrase
?constitutes a democratic threat? in Test Ex. #1203.
4.4 Antonym match
If two aligned noun nodes (h1, t1) are both subjects
or both objects of verb nodes (h0, t0) in their re-
spective sentences, i.e., REL(h0, h1)? REL(t0, t1)?
REL ? {SUBJ,OBJ}, then we check for a verb
antonym match between (h0, t0). We construct
the set of verb antonyms using WordNet; we con-
sider the antonyms of h0 to be the union of the
antonyms of the first three senses of LEMMA(h0),
or of the nearest antonym-possessing hypernyms if
those senses do not themselves have antonyms in
WordNet. Explicitly our procedure for constructing
the antonym set of a node h0 is as follows:
1. ANTONYMS(h0) = {}
2. For each of the first three listed senses s of
LEMMA(h0) in WordNet:
(a) While |WN-ANTONYMS(s)| = 0
i. s ? WN-HYPERNYM(s)
(b) ANTONYMS(h0) ? ANTONYMS(h0) ?
WN-ANTONYMS(s)
3. return ANTONYMS(h0)
In addition to the verb antonyms in WordNet, we
detect the prepositional antonym pairs (before/after,
to/from, and over/under). This heuristic allows us to
predict false entailment between ?Black holes can
lose mass...? and ?Black holes can regain some of
their mass...? in Test Ex. #1445.
4.5 Argument movement
For any two aligned verb nodes (h1, t1), we con-
sider each noun child h2 of h1 possessing any of
36
Hypothesis Text
killVerb
Prime MinisterRobert MalvalNoun
Tobj
AristideNoun
Tsub
killVerb
Prime MinisterRobert MalvalNoun
AristideNoun
Tsub
conferenceNoun
Tobj
 
 
   
 
callVerb
Attrib
conferenceNoun
TobjTsub
 
 
   
 
Port-au-PrinceNoun
Locn_in
Figure 3: Example of object movement signaling
false entailment
the subject, object, or indirect object relations to
h1, i.e., there exists REL(h1, h2) such that REL ?
{SUBJ, OBJ, IND}. If there is some node t2 such that
ALIGN(h2, t2), but REL(t1, t2) 6= REL(h1, h2), then
we predict that the entailment is false.
As an example, consider Figure 3, representing
subgraphs from Dev Ex. #1916:
T : ...U.N. officials are also dismayed that Aristide killed a con-
ference called by Prime Minister Robert Malval...
H: Aristide kills Prime Minister Robert Malval.
Here let (h1, t1) correspond to the aligned verbs
with lemma kill, where the object of h1 has lemma
Prime Minister Robert Malval, and the object of t1
has lemma conference. Since h2 is aligned to some
node t2 in the text graph, but ?OBJ(t1, t2), the sen-
tence pair is rejected as a false entailment.
4.6 Superlative mismatch
If some adjective node h1 in the hypothesis is iden-
tified as a superlative, check that all of the following
conditions are satisfied:
1. h1 is aligned to some superlative t1 in the text
sentence.
2. The noun phrase h2 modified by h1 is aligned
to the noun phrase t2 modified by t1.
3. Any additional modifier t3 of the noun phrase
t2 is aligned to some modifier h3 of h2 in the
hypothesis sentence (reverse subset match).
If any of these conditions are not satisfied, we pre-
dict that the entailment is false. This heuristic allows
us to predict false entailment in (Dev Ex. #908):
T : Time Warner is the world?s largest media and Internet com-
pany.
H: Time Warner is the world?s largest company.
Here ?largest media and Internet company? in T
fails the reverse subset match (condition 3) to
?largest company? in H .
4.7 Conditional mismatch
For any pair of aligned nodes (h1, t1), if there ex-
ists a second pair of aligned nodes (h2, t2) such
that the shortest path PATH(t1, t2) in the depen-
dency graph T contains the conditional relation,
then PATH(h1, h2) must also contain the conditional
relation, or else we predict that the entailment is
false. For example, consider the following false en-
tailment (Dev Ex. #60):
T : If a Mexican approaches the border, he?s assumed to be try-
ing to illegally cross.
H: Mexicans continue to illegally cross border.
Here, ?Mexican? and ?cross? are aligned, and the
path between them in the text contains the condi-
tional relation, but does not in the hypothesis; thus
the entailment is predicted to be false.
4.8 Other heuristics for false entailment
In addition to these heuristics, we additionally im-
plemented an IS-A mismatch heuristic, which at-
tempted to discover when an IS-A relation in the hy-
pothesis sentence was not implied by a correspond-
ing IS-A relation in the text; however, this heuristic
yielded a loss in accuracy on the development set
and was therefore not included in our final system.
5 Lexical similarity and paraphrase
detection
5.1 Lexical similarity using MindNet
In case none of the preceding heuristics for rejec-
tion are applicable, we back off to a lexical sim-
ilarity model similar to that described in (Glick-
man et al, 2005). For every content node h ? H
37
not already aligned by one of the heuristics in Sec-
tion 3, we obtain a similarity score MN(h, t) from a
similarity database that is constructed automatically
from the data contained in MindNet5 as described in
(Richardson, 1997). Our similarity function is thus:
sim(h, t) =
?
??
??
1 if ANY-ALIGN(h, t)
MN(h, t) if MN(h, t) > min
min otherwise
Where the minimum score min is a parameter
tuned for maximum accuracy on the development
set; min = 0.00002 in our final system. We then
compute the entailment score:
score(H,T ) = 1|H|
?
h?H
max
t?T
sim(h, t)
This approach is identical to that used in (Glick-
man et al, 2005), except that we use alignment
heuristics and MindNet similarity scores in place
of their web-based estimation of lexical entailment
probabilities, and we take as our score the geomet-
ric mean of the component entailment scores rather
than the unnormalized product of probabilities.
5.2 Measuring phrasal similarity using the web
The methods discussed so far for alignment are lim-
ited to aligning pairs of single words or multiple-
word units constituting single syntactic categories;
these are insufficient for the problem of detecting
more complicated paraphrases. For example, con-
sider the following true entailment (Dev Ex. #496):
T : ...Muslims believe there is only one God.
H: Muslims are monotheistic.
Here we would like to align the hypothesis phrase
?are monotheistic? to the text phrase ?believe there
is only one God?; unfortunately, single-node align-
ment aligns only the nodes with lemma ?Muslim?.
In this section we describe the approach used in our
system to approximate phrasal similarity via distrib-
utional information obtained using the MSN Search
search engine.
We propose a metric for measuring phrasal simi-
larity based on a phrasal version of the distributional
hypothesis: we propose that a phrase template Ph
5http://research.microsoft.com/mnex
(e.g. ?xh are monotheistic?) has high semantic simi-
larity to a template Pt (e.g. ?xt believe there is only
one God?), with possible ?slot-fillers? xh and xt, re-
spectively, if the overlap of the sets of observed slot-
fillers Xh ?Xt for those phrase templates is high in
some sufficiently large corpus (e.g., the Web).
To measure phrasal similarity we issue the sur-
face text form of each candidate phrase template as
a query to a web-based search engine, and parse the
returned sentences in which the candidate phrase oc-
curs to determine the appropriate slot-fillers. For ex-
ample, in the above example, we observe the set of
slot-fillers Xt = {Muslims, Christians, Jews, Saiv-
ities, Sikhs, Caodaists, People}, and Xh ? Xt =
{Muslims, Christians, Jews, Sikhs, People}.
Explicitly, given the text and hypothesis logical
forms, our algorithm proceeds as follows to compute
the phrasal similarity between all phrase templates
in H and T :
1. For each pair of aligned single node and un-
aligned leaf node (t1, tl) (or pair of aligned
nodes (t1, t2)) in the text T :
(a) Use NLPWIN to generate a surface text
string S from the underlying logical form
PATH(t1, t2).
(b) Create the surface string template phrase
Pt by removing from S the lemmas corre-
sponding to t1 (and t2, if path is between
aligned nodes).
(c) Perform a web search for the string Pt.
(d) Parse the resulting sentences containing
Pt and extract all non-pronoun slot fillers
xt ? Xt that satisfy the same syntactic
roles as t1 in the original sentence.
2. Similarly, extract the slot fillers Xh for each
discovered phrase template Ph in H .
3. Calculate paraphrase similarity as a function of
the overlap between the slot-filler sets Xt and
Xh, i.e: score(Ph, Pt) = |Xh?Xt||Xt| .
We then incorporate paraphrase similarity within the
lexical similarity model by allowing, for some un-
aligned node h ? Ph, where t ? Pt:
sim(h, t) = max(MN(h, t), score(Ph, Pt))
38
Our approach to paraphrase detection is most similar
to the TE/ASE algorithm (Szpektor et al, 2004), and
bears similarity to both DIRT (Lin and Pantel, 2001)
and KnowItAll (Etzioni et al, 2004). The chief
difference in our algorithm is that we generate the
surface text search strings from the parsed logical
forms using the generation capabilities of NLPWIN
(Aikawa et al, 2001), and we verify that the syn-
tactic relations in each discovered web snippet are
isomorphic to those in the original candidate para-
phrase template.
6 Results and Discussion
In this section we present the final results of our sys-
tem on the PASCAL RTE-1 test set, and examine our
features in an ablation study. The PASCAL RTE-1
development and test sets consist of 567 and 800 ex-
amples, respectively, with the test set split equally
between true and false examples.
6.1 Results and Performance Comparison on
the PASCAL RTE-1 Test Set
Table 2 displays the accuracy and confidence-
weighted score6 (CWS) of our final system on each
of the tasks for both the development and test sets.
Our overall test set accuracy of 62.50% rep-
resents a 2.1% absolute improvement over the
task-independent system described in (Tatu and
Moldovan, 2005), and a 20.2% relative improve-
ment in accuracy over their system with respect to
an uninformed baseline accuracy of 50%.
To compute confidence scores for our judgments,
any entailment determined to be false by any heuris-
tic was assigned maximum confidence; no attempts
were made to distinguish between entailments re-
jected by different heuristics. The confidence of
all other predictions was calculated as the ab-
solute value in the difference between the output
score(H,T ) of the lexical similarity model and the
threshold t = 0.1285 as tuned for highest accu-
racy on our development set. We would expect a
higher CWS to result from learning a more appro-
priate confidence function; nonetheless our overall
6As in (Dagan et al, 2005) we compute the confidence-
weighted score (or ?average precision?) over n examples
{c1, c2, ..., cn} ranked in order of decreasing confidence as
cws = 1n
?n
i=1
(#correct-up-to-rank-i)
i
Dev Set Test Set
Task acc cws acc cws
CD 0.8061 0.8357 0.7867 0.8261
RC 0.5534 0.5885 0.6429 0.6476
IR 0.6857 0.6954 0.6000 0.6571
MT 0.7037 0.7145 0.6000 0.6350
IE 0.5857 0.6008 0.5917 0.6275
QA 0.7111 0.7121 0.5308 0.5463
PP 0.7683 0.7470 0.5200 0.5333
All 0.6878 0.6888 0.6250 0.6534
Table 2: Summary of accuracies and confidence-
weighted scores, by task
Alignment Feature Dev Test
Synonym Match 0.0106 0.0038
Derivational Form 0.0053 0.0025
Paraphrase 0.0053 0.0000
Lexical Similarity 0.0053 0.0000
Value Match 0.0017 0.0013
Acronym Match 0.0017 0.0013
Adjectival Form7 0.0000 0.0063
False Entailment Feature Dev Test
Negation Mismatch 0.0106 0.0025
Argument Movement 0.0070 0.0250
Conditional Mismatch 0.0053 0.0037
Modal Mismatch 0.0035 0.0013
Superlative Mismatch 0.0035 -0.0025
Entity Mismatch 0.0018 0.0063
Table 3: Feature ablation study; quantity is the ac-
curacy loss obtained by removal of single feature
test set CWS of 0.6534 is higher than previously-
reported task-independent systems (however, the
task-dependent system reported in (Raina et al,
2005) achieves a CWS of 0.686).
6.2 Feature analysis
Table 3 displays the results of our feature ablation
study, analyzing the individual effect of each feature.
Of the seven heuristics used in our final system
for node alignment (including lexical similarity and
paraphrase detection), our ablation study showed
7As discussed in Section 2, features with no effect on devel-
opment set accuracy were included in the system if and only if
they improved the system?s unweighted F-score.
39
that five were helpful in varying degrees on our test
set, but that removal of either MindNet similarity
scores or paraphrase detection resulted in no accu-
racy loss on the test set.
Of the six false entailment heuristics used in the
final system, five resulted in an accuracy improve-
ment on the test set (the most effective by far was
the ?Argument Movement?, resulting in a net gain
of 20 correctly-classified false examples); inclusion
of the ?Superlative Mismatch? feature resulted in a
small net loss of two examples.
We note that our heuristics for false entailment,
where applicable, were indeed significantly more ac-
curate than our final system as a whole; on the set of
examples predicted false by our heuristics we had
71.3% accuracy on the training set (112 correct out
of 157 predicted), and 72.9% accuracy on the test set
(164 correct out of 225 predicted).
7 Conclusion
In this paper we have presented and analyzed a sys-
tem for recognizing textual entailment focused pri-
marily on the recognition of false entailment, and
demonstrated higher performance than achieved by
previous approaches on the widely-used PASCAL
RTE test set. Our system achieves state-of-the-
art performance despite not exploiting a wide ar-
ray of sources of knowledge used by other high-
performance systems; we submit that the perfor-
mance of our system demonstrates the unexploited
potential in features designed specifically for the
recognition of false entailment.
Acknowledgments
We thank Chris Brockett, Michael Gamon, Gary
Kacmarick, and Chris Quirk for helpful discussion.
Also, thanks to Robert Ragno for assistance with
the MSN Search API. Rion Snow is supported by
an NDSEG Fellowship sponsored by the DOD and
AFOSR.
References
Takako Aikawa, Maite Melero, Lee Schwartz, and Andi
Wu. 2001. Multilingual Sentence Generation. In
Proc. of 8th European Workshop on Natural Language
Generation.
Samuel Bayer, John Burger, Lisa Ferro, John Henderson,
and Alexander Yeh. 2005. MITRE?s Submissions to
the EU Pascal RTE Challenge. In Proc. of the PASCAL
Challenges Workshop on RTE 2005.
Johan Bos and Katja Markert. 2005. Recognizing Tex-
tual Entailment with Logical Inference. In Proc. HLT-
EMNLP 2005.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entailment
Challenge. In Proceedings of the PASCAL Challenges
Workshop on RTE 2005.
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel S. Weld, and Alexander Yates. 2004.
Web-scale information extraction in KnowItAll. In
Proc. WWW 2004.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005.
Web Based Probabilistic Textual Entailment. In Proc.
of the PASCAL Challenges Workshop on RTE 2005.
George E. Heidorn. 2000. Intelligent Writing Assis-
tance. In R. Dale, H. Moisl, and H. Somers (eds.),
A Handbook of Natural Language Processing: Tech-
niques and Applications for the Processing of Lan-
guage as Text. Marcel Dekker, New York. 181-207.
Jesu?s Herrera, Anselmo Pen?as, and Felisa Verdejo. 2005.
Textual Entailment Recognision Based on Depen-
dency Analysis and WordNet. In Proc. of the PASCAL
Challenges Workshop on RTE 2005.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text. In Proc. KDD 2001.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proc. AAAI 2005.
Stephen D. Richardson. 1997. Determining Similarity
and Inferring Relations in a Lexical Knowledge Base.
Ph.D. thesis, The City University of New York.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura
Coppola. 2004. Scaling Web-based Acquisition of
Entailment Relations. In Proc. EMNLP 2004.
Marta Tatu and Dan Moldovan. 2005. A Semantic Ap-
proach to Recognizing Textual Entailment. In Proc.
HLT-EMNLP 2005.
Lucy Vanderwende and William B. Dolan. 2006. What
Syntax Can Contribute in the Entailment Task. In
MLCW 2005, LNAI 3944, pp. 205?216. J. Quinonero-
Candela et al (eds.). Springer-Verlag.
40
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 362?370,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring Content Models for Multi-Document Summarization
Aria Haghighi
UC Berkeley, CS Division
aria42@cs.berkeley.edu
Lucy Vanderwende
Microsoft Research
Lucy.Vanderwende@microsoft.com
Abstract
We present an exploration of generative prob-
abilistic models for multi-document summa-
rization. Beginning with a simple word fre-
quency based model (Nenkova and Vander-
wende, 2005), we construct a sequence of
models each injecting more structure into the
representation of document set content and ex-
hibiting ROUGE gains along the way. Our
final model, HIERSUM, utilizes a hierarchi-
cal LDA-style model (Blei et al, 2004) to
represent content specificity as a hierarchy of
topic vocabulary distributions. At the task
of producing generic DUC-style summaries,
HIERSUM yields state-of-the-art ROUGE per-
formance and in pairwise user evaluation
strongly outperforms Toutanova et al (2007)?s
state-of-the-art discriminative system. We
also explore HIERSUM?s capacity to produce
multiple ?topical summaries? in order to facil-
itate content discovery and navigation.
1 Introduction
Over the past several years, there has been much in-
terest in the task of multi-document summarization.
In the common Document Understanding Confer-
ence (DUC) formulation of the task, a system takes
as input a document set as well as a short descrip-
tion of desired summary focus and outputs a word
length limited summary.1 To avoid the problem of
generating cogent sentences, many systems opt for
an extractive approach, selecting sentences from the
document set which best reflect its core content.2
1In this work, we ignore the summary focus. Here, the word
topic will refer to elements of our statistical model rather than
summary focus.
2Note that sentence extraction does not solve the problem of
selecting and ordering summary sentences to form a coherent
There are several approaches to modeling docu-
ment content: simple word frequency-based meth-
ods (Luhn, 1958; Nenkova and Vanderwende,
2005), graph-based approaches (Radev, 2004; Wan
and Yang, 2006), as well as more linguistically mo-
tivated techniques (Mckeown et al, 1999; Leskovec
et al, 2005; Harabagiu et al, 2007). Another strand
of work (Barzilay and Lee, 2004; Daume? III and
Marcu, 2006; Eisenstein and Barzilay, 2008), has
explored the use of structured probabilistic topic
models to represent document content. However, lit-
tle has been done to directly compare the benefit of
complex content models to simpler surface ones for
generic multi-document summarization.
In this work we examine a series of content
models for multi-document summarization and ar-
gue that LDA-style probabilistic topic models (Blei
et al, 2003) can offer state-of-the-art summariza-
tion quality as measured by automatic metrics (see
section 5.1) and manual user evaluation (see sec-
tion 5.2). We also contend that they provide con-
venient building blocks for adding more structure
to a summarization model. In particular, we uti-
lize a variation of the hierarchical LDA topic model
(Blei et al, 2004) to discover multiple specific ?sub-
topics? within a document set. The resulting model,
HIERSUM (see section 3.4), can produce general
summaries as well as summaries for any of the
learned sub-topics.
2 Experimental Setup
The task we will consider is extractive multi-
document summarization. In this task we assume
a document collection D consisting of documents
D1, . . . , Dn describing the same (or closely related)
narrative (Lapata, 2003).
362
set of events. Our task will be to propose a sum-
mary S consisting of sentences in D totaling at most
L words.3 Here as in much extractive summariza-
tion, we will view each sentence as a bag-of-words
or more generally a bag-of-ngrams (see section 5.1).
The most prevalent example of this data setting is
document clusters found on news aggregator sites.
2.1 Automated Evaluation
For model development we will utilize the DUC
2006 evaluation set4 consisting of 50 document sets
each with 25 documents; final evaluation will utilize
the DUC 2007 evaluation set (section 5).
Automated evaluation will utilize the standard
DUC evaluation metric ROUGE (Lin, 2004) which
represents recall over various n-grams statistics from
a system-generated summary against a set of human-
generated peer summaries.5 We compute ROUGE
scores with and without stop words removed from
peer and proposed summaries. In particular, we
utilize R-1 (recall against unigrams), R-2 (recall
against bigrams), and R-SU4 (recall against skip-4
bigrams)6. We present R-2 without stop words in the
running text, but full development results are pre-
sented in table 1. Official DUC scoring utilizes the
jackknife procedure and assesses significance using
bootstrapping resampling (Lin, 2004). In addition to
presenting automated results, we also present a user
evaluation in section 5.2.
3 Summarization Models
We present a progression of models for multi-
document summarization. Inference details are
given in section 4.
3.1 SumBasic
The SUMBASIC algorithm, introduced in Nenkova
and Vanderwende (2005), is a simple effective pro-
cedure for multi-document extractive summariza-
tion. Its design is motivated by the observation that
the relative frequency of a non-stop word in a doc-
ument set is a good predictor of a word appearing
in a human summary. In SUMBASIC, each sentence
3For DUC summarization tasks, L is typically 250.
4http://www-nlpir.nist.gov/projects/duc/data.html
5All words from peer and proposed summaries are lower-
cased and stemmed.
6Bigrams formed by skipping at most two words.
S is assigned a score reflecting how many high-
frequency words it contains,
Score(S) = ?
w?S
1
|S|PD(w) (1)
where PD(?) initially reflects the observed unigram
probabilities obtained from the document collection
D. A summary S is progressively built by adding
the highest scoring sentence according to (1).7
In order to discourage redundancy, the words
in the selected sentence are updated PnewD (w) ?
P oldD (w)2. Sentences are selected in this manner un-
til the summary word limit has been reached.
Despite its simplicity, SUMBASIC yields 5.3 R-2
without stop words on DUC 2006 (see table 1).8 By
comparison, the highest-performing ROUGE sys-
tem at the DUC 2006 evaluation, SUMFOCUS, was
built on top of SUMBASIC and yielded a 6.0, which
is not a statistically significant improvement (Van-
derwende et al, 2007).9
Intuitively, SUMBASIC is trying to select a sum-
mary which has sentences where most words have
high likelihood under the document set unigram dis-
tribution. One conceptual problem with this objec-
tive is that it inherently favors repetition of frequent
non-stop words despite the ?squaring? update. Ide-
ally, a summarization criterion should be more recall
oriented, penalizing summaries which omit moder-
ately frequent document set words and quickly di-
minishing the reward for repeated use of word.
Another more subtle shortcoming is the use of the
raw empirical unigram distribution to represent con-
tent significance. For instance, there is no distinc-
tion between a word which occurs many times in the
same document or the same number of times across
several documents. Intuitively, the latter word is
more indicative of significant document set content.
3.2 KLSum
The KLSUM algorithm introduces a criterion for se-
lecting a summary S given document collection D,
S? = min
S:words(S)?L
KL(PD?PS) (2)
7Note that sentence order is determined by the order in
which sentences are selected according to (1).
8This result is presented as 0.053 with the official ROUGE
scorer (Lin, 2004). Results here are scaled by 1,000.
9To be fair obtaining statistical significance in ROUGE
scores is quite difficult.
363
?B Z
W
?C
?D
?t
Document Set
Document
Sentence
Word
Figure 1: Graphical model depiction of TOPIC-
SUM model (see section 3.3). Note that many hyper-
parameter dependencies are omitted for compactness.
where PS is the empirical unigram distribution of
the candidate summary S and KL(P?Q) repre-
sents the Kullback-Lieber (KL) divergence given by?
w P (w) log P (w)Q(w) .10 This quantity represents thedivergence between the true distribution P (here the
document set unigram distribution) and the approx-
imating distribution Q (the summary distribution).
This criterion casts summarization as finding a set
of summary sentences which closely match the doc-
ument set unigram distribution. Lin et al (2006)
propose a related criterion for robust summarization
evaluation, but to our knowledge this criteria has
been unexplored in summarization systems. We ad-
dress optimizing equation (2) as well as summary
sentence ordering in section 4.
KLSUM yields 6.0 R-2 without stop words, beat-
ing SUMBASIC but not with statistical significance. It
is worth noting however that KLSUM?s performance
matches SUMFOCUS (Vanderwende et al, 2007), the
highest R-2 performing system at DUC 2006.
3.3 TopicSum
As mentioned in section 3.2, the raw unigram dis-
tribution PD(?) may not best reflect the content of
D for the purpose of summary extraction. We
propose TOPICSUM, which uses a simple LDA-like
topic model (Blei et al, 2003) similar to Daume?
III and Marcu (2006) to estimate a content distribu-
10In order to ensure finite values of KL-divergence we
smoothe PS(?) so that it has a small amount of mass on all doc-
ument set words.
System ROUGE -stop ROUGE all
R-1 R-2 R-SU4 R-1 R-2 R-SU4
SUMBASIC 29.6 5.3 8.6 36.1 7.1 12.3
KLSUM 30.6 6.0 8.9 38.9 8.3 13.7
TOPICSUM 31.7 6.3 9.1 39.2 8.4 13.6
HIERSUM 30.5 6.4 9.2 40.1 8.6 14.3
Table 1: ROUGE results on DUC2006 for models pre-
sented in section 3. Results in bold represent results sta-
tistically significantly different from SUMBASIC in the
appropriate metric.
tion for summary extraction.11 We extract summary
sentences as before using the KLSUM criterion (see
equation (2)), plugging in a learned content distribu-
tion in place of the raw unigram distribution.
First, we describe our topic model (see figure 1)
which generates a collection of document sets. We
assume a fixed vocabulary V :12
1. Draw a background vocabulary distribution ?B
from DIRICHLET(V ,?B) shared across docu-
ment collections13 representing the background
distribution over vocabulary words. This distri-
bution is meant to flexibly model stop words
which do not contribute content. We will refer
to this topic as BACKGROUND.
2. For each document set D, we draw a content
distribution ?C from DIRICHLET(V ,?C) repre-
senting the significant content of D that we
wish to summarize. We will refer to this topic
as CONTENT.
3. For each document D in D, we draw a
document-specific vocabulary distribution ?D
from DIRICHLET(V ,?D) representing words
which are local to a single document, but do
not appear across several documents. We will
refer to this topic as DOCSPECIFIC.
11A topic model is a probabilistic generative process that gen-
erates a collection of documents using a mixture of topic vo-
cabulary distributions (Steyvers and Griffiths, 2007). Note this
usage of topic is unrelated to the summary focus given for doc-
ument collections; this information is ignored by our models.
12In contrast to previous models, stop words are not removed
in pre-processing.
13DIRICHLET(V ,?) represents the symmetric Dirichlet
prior distribution over V each with a pseudo-count of ?. Con-
crete pseudo-count values will be given in section 4.
364
{ star: 0.21, wars: 0.15, phantom: 0.10, ... }  
General Content Topic
?C1
{ $: 0.39, million: 0.15, record: 0.8, ... }  
Specific Content Topic               "Financial"
?C1
{ toys: 0.22, spend: 0.18, sell: 0.10, ... }  
{ fans: 0.16, line: 0.12, film: 0.09, ... }  
Specific Content Topic               "Merchandise"
Specific Content Topic               "Fans"
?C2
?C3
Document Set
?C0
?CK?C1
ZS
?D
Document
Sentence
Word
Z
W
?T
?G
.........
?B
(a) Content Distributions (b) HIERSUM Graphical Model
Figure 2: (a): Examples of general versus specific content distributions utilized by HIERSUM (see section 3.4). The
general content distribution ?C0 will be used throughout a document collection and represents core concepts in a
story. The specific content distributions represent topical ?sub-stories? with vocabulary tightly clustered together but
consistently used across documents. Quoted names of specific topics are given manually to facilitate interpretation. (b)
Graphical model depiction of the HIERSUM model (see section 3.4). Similar to the TOPICSUM model (see section 3.3)
except for adding complexity in the content hierarchy as well as sentence-specific prior distributions between general
and specific content topics (early sentences should have more general content words). Several dependencies are
missing from this depiction; crucially, each sentence?s specific topic ZS depends on the last sentence?s ZS .
4. For each sentence S of each document
D, draw a distribution ?T over topics
(CONTENT,DOCSPECIFIC, BACKGROUND)
from a Dirichlet prior with pseudo-counts
(1.0, 5.0, 10.0).14 For each word position in
the sentence, we draw a topic Z from ?T ,
and a word W from the topic distribution Z
indicates.
Our intent is that ?C represents the core con-
tent of a document set. Intuitively, ?C does
not include words which are common amongst
several document collections (modeled with the
BACKGROUND topic), or words which don?t appear
across many documents (modeled with the DOCSPE-
CIFIC topic). Also, because topics are tied together
at the sentence level, words which frequently occur
with other content words are more likely to be con-
sidered content words.
We ran our topic model over the DUC 2006
document collections and estimated the distribution
?C(?) for each document set.15 Then we extracted
14The different pseudo-counts reflect the intuition that most
of the words in a document come from the BACKGROUND and
DOCSPECIFIC topics.
15While possible to obtain the predictive posterior CON-
a summary using the KLSUM criterion with our es-
timated ?C in place of the the raw unigram distribu-
tion. Doing so yielded 6.3 R-2 without stop words
(see TOPICSUM in table 1); while not a statistically
significant improvement over KLSUM, it is our first
model which outperforms SUMBASIC with statistical
significance.
Daume? III and Marcu (2006) explore a topic
model similar to ours for query-focused multi-
document summarization.16 Crucially however,
Daume? III andMarcu (2006) selected sentences with
the highest expected number of CONTENT words.17
We found that in our model using this extraction
criterion yielded 5.3 R-2 without stop words, sig-
nificantly underperforming our TOPICSUM model.
One reason for this may be that Daume? III and
Marcu (2006)?s criterion encourages selecting sen-
tences which have words that are confidently gener-
ated by the CONTENT distribution, but not necessar-
ily sentences which contain a plurality of it?s mass.
TENT distribution by analytically integrating over ?C (Blei et
al., 2003), doing so gave no benefit.
16Daume? III and Marcu (2006) note their model could be
used outside of query-focused summarization.
17This is phrased as selecting the sentence which has the
highest posterior probability of emitting CONTENT topic
words, but this is equivalent.
365
(a) HIERSUM output
The French government
Saturday announced sev-
eral emergency measures
to support the jobless
people, including sending
an additional 500 million
franc (84 million U.S. dol-
lars) unemployment aid
package. The unem-
ployment rate in France
dropped by 0.3 percent
to stand at 12.4 percent
in November, said the
Ministry of Employment
Tuesday.
(b) PYTHY output
Several hundred people
took part in the demon-
stration here today against
the policies of the world?s
most developed nations.
The 12.5 percent unem-
ployment rate is haunt-
ing the Christmas sea-
son in France as militants
and unionists staged sev-
eral protests over the past
week against unemploy-
ment.
(c) Ref output
High unemployment is
France?s main economic
problem, despite recent
improvements. A top
worry of French people,
it is a factor affecting
France?s high suicide rate.
Long-term unemployment
causes social exclusion
and threatens France?s
social cohesion.
(d) Reference Unigram Coverage
word Ref PYTHY HIERSUMunemployment 8 9 10france?s 6 1 4francs 4 0 1high 4 1 2economic 2 0 1french 2 1 3problem 2 0 1benefits 2 0 0social 2 0 2jobless 2 1 2
Table 2: Example summarization output for systems compared in section 5.2. (a), (b), and (c) represent the first two
sentences output from PYTHY, HIERSUM, and reference summary respectively. In (d), we present the most frequent
non-stop unigrams appearing in the reference summary and their counts in the PYTHY and HIERSUM summaries.
Note that many content words in the reference summary absent from PYTHY?s proposal are present in HIERSUM?s.
3.4 HIERSUM
Previous sections have treated the content of a doc-
ument set as a single (perhaps learned) unigram dis-
tribution. However, as Barzilay and Lee (2004) ob-
serve, the content of document collections is highly
structured, consisting of several topical themes, each
with its own vocabulary and ordering preferences.
For concreteness consider the DUC 2006 docu-
ment collection describing the opening of Star Wars:
Episode 1 (see figure 2(a)).
While there are words which indicate the general
content of this document collection (e.g. star, wars),
there are several sub-stories with their own specific
vocabulary. For instance, several documents in this
collection spend a paragraph or two talking about
the financial aspect of the film?s opening and use a
specific vocabulary there (e.g. $, million, record). A
user may be interested in general content of a docu-
ment collection or, depending on his or her interests,
one or more of the sub-stories. We choose to adapt
our topic modeling approach to allow modeling this
aspect of document set content.
Rather than drawing a single CONTENT distribu-
tion ?C for a document collection, we now draw
a general content distribution ?C0 from DIRICH-
LET(V ,?G) as well as specific content distribu-
tions ?Ci for i = 1, . . . ,K each from DIRICH-
LET(V ,?S).18 Our intent is that ?C0 represents the
18We choose K=3 in our experiments, but one could flexibly
general content of the document collection and each
?Ci represents specific sub-stories.
As with TOPICSUM, each sentence
has a distribution ?T over topics
(BACKGROUND,DOCSPECIFIC, CONTENT). When
BACKGROUND or DOCSPECIFIC topics are chosen,
the model works exactly as in TOPICSUM. However
when the CONTENT topic is drawn, we must decide
whether to emit a general content word (from ?C0)
or from one of the specific content distributions
(from one of ?Ci for i = 1, . . . ,K). The generative
story of TOPICSUM is altered as follows in this case:
? General or Specific? We must first decide
whether to use a general or specific content
word. Each sentence draws a binomial distribu-
tion ?G determining whether a CONTENT word
in the sentence will be drawn from the general
or a specific topic distribution. Reflecting the
intuition that the earlier sentences in a docu-
ment19 describe the general content of a story,
we bias ?G to be drawn from BETA(5,2), pre-
ferring general content words, and every later
sentence from BETA(1,2).20
? What Specific Topic? If ?G decides we are
choose K as Blei et al (2004) does.
19In our experiments, the first 5 sentences.
20BETA(a,b) represents the beta prior over binomial random
variables with a and b being pseudo-counts for the first and sec-
ond outcomes respectively.
366
emitting a topic specific content word, we must
decide which of ?C1 , . . . , ?CK to use. In or-
der to ensure tight lexical cohesion amongst the
specific topics, we assume that each sentence
draws a single specific topic ZS used for every
specific content word in that sentence. Reflect-
ing intuition that adjacent sentences are likely
to share specific content vocabulary, we uti-
lize a ?sticky? HMM as in Barzilay and Lee
(2004) over the each sentences? ZS . Con-
cretely, ZS for the first sentence in a docu-
ment is drawn uniformly from 1, . . . ,K, and
each subsequent sentence?s ZS will be identi-
cal to the previous sentence with probability ?,
and with probability 1 ? ? we select a succes-
sor topic from a learned transition distribution
amongst 1, . . . ,K.21
Our intent is that the general content distribution
?C0 now prefers words which not only appear in
many documents, but also words which appear con-
sistently throughout a document rather than being
concentrated in a small number of sentences. Each
specific content distribution ?Ci is meant to model
topics which are used in several documents but tend
to be used in concentrated locations.
HIERSUM can be used to extract several kinds
of summaries. It can extract a general summary
by plugging ?C0 into the KLSUM criterion. It can
also produce topical summaries for the learned spe-
cific topics by extracting a summary over each ?Ci
distribution; this might be appropriate for a user
who wants to know more about a particular sub-
story. While we found the general content distribu-
tion (from ?C0) to produce the best single summary,
we experimented with utilizing topical summaries
for other summarization tasks (see section 6.1). The
resulting system, HIERSUM yielded 6.4 R-2 without
stop words. While not a statistically significant im-
provement in ROUGE over TOPICSUM, we found the
summaries to be noticeably improved.
4 Inference and Model Details
Since globally optimizing the KLSUM criterion in
equation (equation (2)) is exponential in the total
number of sentences in a document collection, we
21We choose ? = 0.75 in our experiments.
opted instead for a simple approximation where sen-
tences are greedily added to a summary so long as
they decrease KL-divergence. We attempted more
complex inference procedures such as McDonald
(2007), but these attempts only yielded negligible
performance gains. All summary sentence order-
ing was determined as follows: each sentence in the
proposed summary was assigned a number in [0, 1]
reflecting its relative sentence position in its source
document, and sorted by this quantity.
All topic models utilize Gibbs sampling for in-
ference (Griffiths, 2002; Blei et al, 2004). In gen-
eral for concentration parameters, the more specific
a distribution is meant to be, the smaller its con-
centration parameter. Accordingly for TOPICSUM,
?G = ?D = 1 and ?C = 0.1. For HIERSUM we
used ?G = 0.1 and ?S = 0.01. These parameters
were minimally tuned (without reference to ROUGE
results) in order to ensure that all topic distribution
behaved as intended.
5 Formal Experiments
We present formal experiments on the DUC 2007
data main summarization task, proposing a general
summary of at most 250 words22 which will be eval-
uated automatically and manually in order to simu-
late as much as possible the DUC evaluation envi-
ronment.23 DUC 2007 consists of 45 document sets,
each consisting of 25 documents and 4 human refer-
ence summaries.
We primarily evaluate the HIERSUM model, ex-
tracting a single summary from the general con-
tent distribution using the KLSUM criterion (see sec-
tion 3.2). Although the differences in ROUGE be-
tween HIERSUM and TOPICSUM were minimal, we
found HIERSUM summary quality to be stronger.
In order to provide a reference for ROUGE
and manual evaluation results, we compare against
PYTHY, a state-of-the-art supervised sentence ex-
traction summarization system. PYTHY uses human-
generated summaries in order to train a sentence
ranking system which discriminatively maximizes
22Since the ROUGE evaluation metric is recall-oriented, it is
always advantageous - with respect to ROUGE - to use all 250
words.
23Although the DUC 2007 main summarization task provides
an indication of user intent through topic focus queries, we ig-
nore this aspect of the data.
367
System ROUGE w/o stop ROUGE w/ stop
R-1 R-2 R-SU4 R-1 R-2 R-SU4
HIERSUM unigram 34.6 7.3 10.4 43.1 9.7 15.3
HIERSUM bigram 33.8 9.3 11.6 42.4 11.8 16.7
PYTHY w/o simp 34.7 8.7 11.8 42.7 11.4 16.5
PYTHY w/ simp 35.7 8.9 12.1 42.6 11.9 16.8
Table 3: Formal ROUGE experiment results on DUC 2007 document set collection (see section 5.1). While HIER-
SUM unigram underperforms both PYTHY systems in statistical significance (for R-2 and RU-4 with and without stop
words), HIERSUM bigram?s performance is comparable and statistically no worse.
ROUGE scores. PYTHY uses several features to
rank sentences including several variations of the
SUMBASIC score (see section 3.1). At DUC 2007,
PYTHY was ranked first overall in automatic ROUGE
evaluation and fifth in manual content judgments.
As PYTHY utilizes a sentence simplification com-
ponent, which we do not, we also compare against
PYTHY without sentence simplification.
5.1 ROUGE Evaluation
ROUGE results comparing variants of HIERSUM and
PYTHY are given in table 3. The HIERSUM system
as described in section 3.4 yields 7.3 R-2 without
stop words, falling significantly short of the 8.7 that
PYTHY without simplification yields. Note that R-2
is a measure of bigram recall and HIERSUM does not
represent bigrams whereas PYTHY includes several
bigram and higher order n-gram statistics.
In order to put HIERSUM and PYTHY on equal-
footing with respect to R-2, we instead ran HIER-
SUM with each sentence consisting of a bag of bi-
grams instead of unigrams.24 All the details of the
model remain the same. Once a general content
distribution over bigrams has been determined by
hierarchical topic modeling, the KLSUM criterion
is used as before to extract a summary. This sys-
tem, labeled HIERSUM bigram in table 3, yields 9.3
R-2 without stop words, significantly outperform-
ing HIERSUM unigram. This model outperforms
PYTHY with and without sentence simplification, but
not with statistical significance. We conclude that
both PYTHY variants and HIERSUM bigram are com-
parable with respect to ROUGE performance.
24Note that by doing topic modeling in this way over bi-
grams, our model becomes degenerate as it can generate incon-
sistent bags of bigrams. Future work may look at topic models
over n-grams as suggested by Wang et al (2007).
Question PYTHY HIERSUM
Overall 20 49
Non-Redundancy 21 48
Coherence 15 54
Focus 28 41
Table 4: Results of manual user evaluation (see sec-
tion 5.2). 15 participants expressed 69 pairwise prefer-
ences between HIERSUM and PYTHY. For all attributes,
HIERSUM outperforms PYTHY; all results are statisti-
cally significant as determined by pairwise t-test.
5.2 Manual Evaluation
In order to obtain a more accurate measure of sum-
mary quality, we performed a simple user study. For
each document set in the DUC 2007 collection, a
user was given a reference summary, a PYTHY sum-
mary, and a HIERSUM summary;25 note that the orig-
inal documents in the set were not provided to the
user, only a reference summary. For this experiment
we use the bigram variant of HIERSUM and compare
it to PYTHY without simplification so both systems
have the same set of possible output summaries.
The reference summary for each document set
was selected according to highest R-2 without stop
words against the remaining peer summaries. Users
were presented with 4 questions drawn from the
DUC manual evaluation guidelines:26 (1) Overall
quality: Which summary was better overall? (2)
Non-Redundancy: Which summary was less redun-
dant? (3) Coherence: Which summary was more
coherent? (4) Focus: Which summary was more
25The system identifier was of course not visible to the user.
The order of automatic summaries was determined randomly.
26http://www-nlpir.nist.gov/projects/duc/duc2007/quality-
questions.txt
368
Figure 3: Using HIERSUM to organize content of document set into topics (see section 6.1). The sidebar gives key
phrases salient in each of the specific content topics in HIERSUM (see section 3.4). When a topic is clicked in the right
sidebar, the main frame displays an extractive ?topical summary? with links into document set articles. Ideally, a user
could use this interface to quickly find content in a document collection that matches their interest.
focused in its content, not conveying irrelevant de-
tails? The study had 16 users and each was asked
to compare five summary pairs, although some did
fewer. A total of 69 preferences were solicited. Doc-
ument collections presented to users were randomly
selected from those evaluated fewest.
As seen in table 5.2, HIERSUM outperforms
PYTHY under all questions. All results are statis-
tically significant as judged by a simple pairwise
t-test with 95% confidence. It is safe to conclude
that users in this study strongly preferred the HIER-
SUM summaries over the PYTHY summaries.
6 Discussion
While it is difficult to qualitatively compare one
summarization system over another, we can broadly
characterize HIERSUM summaries compared to some
of the other systems discussed. For example out-
put from HIERSUM and PYTHY see table 2. On the
whole, HIERSUM summaries appear to be signifi-
cantly less redundant than PYTHY and moderately
less redundant than SUMBASIC. The reason for this
might be that PYTHY is discriminatively trained to
maximize ROUGE which does not directly penalize
redundancy. Another tendency is for HIERSUM to se-
lect longer sentences typically chosen from an early
sentence in a document. As discussed in section 3.4,
HIERSUM is biased to consider early sentences in
documents have a higher proportion of general con-
tent words and so this tendency is to be expected.
6.1 Content Navigation
A common concern in multi-document summariza-
tion is that without any indication of user interest or
intent providing a single satisfactory summary to a
user may not be feasible. While many variants of
the general summarization task have been proposed
which utilize such information (Vanderwende et al,
2007; Nastase, 2008), this presupposes that a user
knows enough of the content of a document collec-
tion in order to propose a query.
As Leuski et al (2003) and Branavan et al (2007)
suggest, a document summarization system should
facilitate content discovery and yield summaries rel-
evant to a user?s interests. We may use HIERSUM in
order to facilitate content discovery via presenting
a user with salient words or phrases from the spe-
cific content topics parametrized by ?C1 , . . . , ?CK
(for an example see figure 3). While these topics are
not adaptive to user interest, they typically reflect
lexically coherent vocabularies.
Conclusion
In this paper we have presented an exploration of
content models for multi-document summarization
and demonstrated that the use of structured topic
models can benefit summarization quality as mea-
sured by automatic and manual metrics.
Acknowledgements The authors would like to
thank Bob Moore, Chris Brockett, Chris Quirk, and
Kristina Toutanova for their useful discussions as
well as the reviewers for their helpful comments.
369
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. JMLR.
David M. Blei, Thomas L. Griffiths, Michael I. Jordan,
and Joshua B. Tenenbaum. 2004. Hierarchical topic
models and the nested chinese restaurant process. In
NIPS.
S.R.K. Branavan, Pawan Deshpande, and Regina Barzi-
lay. 2007. Generating a table-of-contents. In ACL.
Hal Daume? III and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the Confer-
ence of the Association for Computational Linguistics
(ACL).
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In EMNLP-
SIGDAT.
Thomas Griffiths. 2002. Gibbs sampling in the genera-
tive model of latent dirichlet alocation.
Sanda Harabagiu, Andrew Hickl, and Finley Laca-
tusu. 2007. Satisfying information needs with multi-
document summaries. Inf. Process. Manage., 43(6).
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In ACL.
Jurij Leskovec, Natasa Milic-frayling, and Marko Gro-
belnik. 2005. Impact of linguistic analysis on the se-
mantic graph coverage and learning of document ex-
tracts. In In AAAI 05.
Anton Leuski, Chin-Yew Lin, and Eduard Hovy. 2003.
ineats: Interactive multi-document summarization. In
ACL.
Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-Yun
Nie. 2006. An information-theoretic approach to au-
tomatic evaluation of summaries. In HLT-NAACL.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out.
H.P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In ECIR.
Kathleen R. Mckeown, Judith L. Klavans, Vasileios
Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin.
1999. Towards multidocument summarization by re-
formulation: Progress and prospects. In In Proceed-
ings of AAAI-99.
Vivi Nastase. 2008. Topic-driven multi-document sum-
marization with encyclopedic knowledge and spread-
ing activation. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Technical report, Mi-
crosoft Research.
Dragomir R. Radev. 2004. Lexrank: graph-based cen-
trality as salience in text summarization. Journal of
Artificial Intelligence Research (JAIR.
M. Steyvers and T. Griffiths, 2007. Probabilistic Topic
Models.
Kristina Toutanova, Chris Brockett, Michael Gamon Ja-
gadeesh Jagarlamudi, Hisami Suzuki, and Lucy Van-
derwende. 2007. The pythy summarization system:
Microsoft research at duc 2007. In DUC.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett, and
Ani Nenkova. 2007. Beyond sumbasic: Task-focused
summarization with sentence simplification and lexi-
cal expansion. volume 43.
Xiaojun Wan and Jianwu Yang. 2006. Improved affinity
graph based multi-document summarization. In HLT-
NAACL.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In ICDM.
370
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 813?821,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Joint Inference for Knowledge Extraction from Biomedical Literature
Hoifung Poon?
Dept. of Computer Sci. & Eng.
University of Washington
Seattle, WA 98195
hoifung@cs.washington.edu
Lucy Vanderwende
Microsoft Research
Redmond, WA 98052
Lucy.Vanderwende@microsoft.com
Abstract
Knowledge extraction from online reposito-
ries such as PubMed holds the promise of
dramatically speeding up biomedical research
and drug design. After initially focusing on
recognizing proteins and binary interactions,
the community has recently shifted their at-
tention to the more ambitious task of recogniz-
ing complex, nested event structures. State-of-
the-art systems use a pipeline architecture in
which the candidate events are identified first,
and subsequently the arguments. This fails
to leverage joint inference among events and
arguments for mutual disambiguation. Some
joint approaches have been proposed, but they
still lag much behind in accuracy. In this pa-
per, we present the first joint approach for bio-
event extraction that obtains state-of-the-art
results. Our system is based on Markov logic
and adopts a novel formulation by jointly pre-
dicting events and arguments, as well as indi-
vidual dependency edges that compose the ar-
gument paths. On the BioNLP?09 Shared Task
dataset, it reduced F1 errors by more than 10%
compared to the previous best joint approach.
1 Introduction
Extracting knowledge from unstructured text has
been a long-standing goal of NLP and AI. The ad-
vent of the World Wide Web further increases its
importance and urgency by making available an as-
tronomical number of online documents containing
virtually unlimited amount of knowledge (Craven et
? This research was conducted during the author?s intern-
ship at Microsoft Research.
al., 1999). A salient example domain is biomedical
literature: the PubMed1 online repository contains
over 18 million abstracts on biomedical research,
with more than two thousand new abstracts added
each day; the abstracts are written in grammatical
English, which enables the use of advanced NLP
tools such as syntactic and semantic parsers.
Traditionally, research on knowledge extraction
from text is primarily pursued in the field of in-
formation extraction with a rather confined goal of
extracting instances for flat relational schemas with
no nested structures (e.g, recognizing protein names
and protein-protein interaction (PPI)). This restric-
tion mainly stems from limitations in available re-
sources and algorithms. The BioNLP?09 Shared
Task (Kim et al, 2009) is one of the first that
faced squarely information needs that are complex
and highly structured. It aims to extract nested
bio-molecular events from research abstracts, where
an event may have variable number of arguments
and may contain other events as arguments. Such
nested events are ubiquitous in biomedical literature
and can effectively represent complex biomedical
knowledge and subsequently support reasoning and
automated discovery. The task has generated much
interest, with twenty-four teams having submitted
their results. The top system by UTurku (Bjorne et
al., 2009) attained the state-of-the-art F1 of 52.0%.
The nested event structures make this task partic-
ularly attractive for applying joint inference. By al-
lowing information to propagate among events and
arguments, joint inference can facilitate mutual dis-
ambiguation and potentially lead to substantial gain
1http://www.ncbi.nlm.nih.gov/pubmed
813
in predictive accuracy. However, joint inference is
underexplored for this task. Most participants ei-
ther reduced the task to classification (e.g., by using
SVM), or used heuristics to combine manual rules
and statistics. The previous best joint approach was
Riedel et al (2009). While competitive, it still lags
UTurku by more than 7 points in F1.
In this paper, we present the first joint approach
that achieves state-of-the-art results for bio-event ex-
traction. Like Riedel et al (2009), our system
is based on Markov logic, but we adopted a novel
formulation that models dependency edges in ar-
gument paths and jointly predicts them along with
events and arguments. By expanding the scope of
joint inference to include individual argument edges,
our system can leverage fine-grained correlations to
make learning more effective. On the development
set, by merely adding a few joint inference formu-
las to a simple logistic regression model, our system
raised F1 from 28% to 54%, already tying UTurku.
We also presented a heuristic method to fix errors
in syntactic parsing by leveraging available semantic
information from task input, and showed that this in
turn led to substantial performance gain in the task.
Overall, our final system reduced F1 error by more
than 10% compared to Riedel et al (2009).
We begin by describing the shared task and re-
lated work. We then introduce Markov logic and our
Markov Logic Network (MLN) for joint bio-event
extraction. Finally, we present our experimental re-
sults and conclude.
2 Bio-Event Extraction
We follow the BioNLP?09 Shared Task (Kim et
al., 2009) on problem setup for bio-event extrac-
tion. A bio-molecular event (bio-event) refers to
the change of state for bio-molecules such as DNAs
and proteins. The goal is to extract these events
from unstructured text such as biomedical abstracts.
For each event, one needs to identify the trigger
words that signifies the event and the theme argu-
ments that undergo the change. In addition, for
regulation events, the cause argument also needs to
be identified if it is present. The task considers
nine event types: Expression, Transcription,
Localization, Phosphorylation, Catabolism,
Binding, Regulation, Positive regulation,
and Negative regulation. Only Binding can
take multiple themes. Regulation events may take
events as arguments. To facilitate evaluation, the
task fixes the type of non-event arguments to pro-
tein and provides ground truth of protein mentions
as input. 2
Like any NLP task, ambiguity is a central prob-
lem. The same event can be expressed in many
variations. For example, a Negative regulation
event may be signified by ?inhibition?, ?down-
regulation?, ?is abrogated by?, to name a few. On
the other hand, depending on the context, the same
expression may represent different events. For ex-
ample, ?level? may signify any one of five event
types in the training set, or signify none.
In addition, the nested event structures present
new challenges to knowledge extraction systems. To
recognize a complex event, besides from identifying
the event type and trigger words, one also needs to
identify its arguments and recursively identify their
event structures. A mistake in any part will render a
failure in this extraction.
The interdependencies among events and argu-
ments naturally argue for joint predictions. For
example, given the snippet ?the level of VCAM-
1 mRNA?, knowing that ?level? might signify an
event helps to recognize the prepositional phrase
(PP) as its theme. Conversely, the presence of the
PP suggests that ?level? is likely an event. More-
over, the word ?mRNA? in the PP indicates that the
event type is probably Transcription.
Most existing systems adopt a pipeline architec-
ture and reduce the task to independent classifica-
tions of events and arguments. For example, the best
system UTurku (Bjorne et al, 2009) first extracts a
list of candidate triggers with types, and then deter-
mines for each pair of candidate triggers or proteins
whether one is a theme or cause of the other. The
triggers missed in the first stage can never be recov-
ered in the second one. Moreover, since the second
stage is trained with gold triggers as input, any trig-
ger identified in the first stage tends to get at least
2The Shared Task also defines two other tasks (Tasks 2 and
3), which aim either to extract additional arguments (e.g., sites),
or to determine if an event is a negation or speculation. In this
paper, we focus on the core task (Task 1) as it is what most sys-
tems participate in, but our approach can be extended straight-
forwardly to handle the other tasks.
814
one argument, even though it may not be an event at
all. As a result, the authors had to use an ad hoc pro-
cedure to trade off precision and recall for the final
prediction task while training the first-stage extrac-
tor. In addition, each trigger or argument is classified
independently using a multi-class SVM.
While joint inference can potentially improve ac-
curacy, in practice, it is often very challenging to
make it work (Poon and Domingos, 2007). The pre-
vious best joint approach for this task was proposed
by Riedel et al (2009) (labeled UT+DBLS in Kim
et al (2009)). Their system is also based on Markov
logic (Domingos and Lowd, 2009). While compet-
itive (ranked fourth in the evaluation), their system
still lags UTurku by more than 7 points in F1.
Most systems, Riedel et al?s included, classify
each candidate argument path as a whole. A notable
exception is the UTokyo system (Saetre et al, 2009),
which incorporated sequential modeling by adapt-
ing a state-of-the-art PPI system based on MEMM.
But they considered adjacent words in the sentence,
which offered little help in this task, and their system
trailed UTurku by 15 points in F1.
All top systems for event extraction relied heav-
ily on syntactic features. We went one step further
by formulating joint predictions directly on depen-
dency edges. While this leverages sequential corre-
lation along argument paths, it also makes our sys-
tem more prone to the adverse effect of syntactic
errors. Joint syntactic and semantic processing has
received much attention lately (Hajic et al, 2009).
In this paper, we explore using a heuristic method
to correct syntactic errors based on semantic infor-
mation, and show that it leads to significant perfor-
mance gain for event extraction.
3 Markov Logic
In many NLP applications, there exist rich relation
structures among objects, and recent work in statisti-
cal relational learning (Getoor and Taskar, 2007) and
structured prediction (Bakir et al, 2007) has shown
that leveraging these can greatly improve accuracy.
One of the leading frameworks for joint inference
is Markov logic, a probabilistic extension of first-
order logic (Domingos and Lowd, 2009). A Markov
logic network (MLN) is a set of weighted first-order
clauses. Together with a set of constants, it defines a
Markov network with one node per ground atom and
one feature per ground clause. The weight of a fea-
ture is the weight of the first-order clause that gener-
ated it. The probability of a state x in such a network
is given by P (x) = (1/Z) exp (?i wifi(x)), where
Z is a normalization constant, wi is the weight of the
ith clause, fi = 1 if the ith clause is true, and fi = 0
otherwise.
Markov logic makes it possible to compactly
specify probability distributions over complex re-
lational domains. Efficient inference can be per-
formed using MC-SAT (Poon and Domingos, 2006).
MC-SAT is a ?slice sampling? Markov chain Monte
Carlo algorithm that uses an efficient satisfiability
solver to propose the next sample. It is orders of
magnitude faster than previous MCMC algorithms
like Gibbs sampling, making efficient sampling pos-
sible on a scale that was previously out of reach.
Supervised learning for Markov logic maximizes
the conditional log-likelihood of query predicates
given the evidence in the train data. This learning
objective is convex and can be optimized using gra-
dient descent, where the gradient is estimated using
MC-SAT.
In practice, it is often difficult to tune the learn-
ing rate, especially when the number of ground-
ings varies widely among clauses (known as ill-
conditioning in numerical optimization). This prob-
lem is particularly severe in relational domains. One
remedy is to apply preconditioning to the gradient.
For example, Poon & Domingos (2007) divided the
global learning rate by the number of true ground-
ings of the corresponding clause in the training data,
whereas Lowd & Domingos (2007) divided it by the
variance of the clause (also estimated using MC-
SAT). The latter can be viewed as approximating
the Hessian with its diagonal, and is guaranteed op-
timal when the weights are not correlated (e.g., in
logistic regression). Lowd & Domingos (2007) also
used a scaled conjugate gradient algorithm to incor-
porate second-order information and further adapt
the search direction.
The open-source Alchemy package (Kok et al,
2009) provides implementations of existing algo-
rithms for Markov logic.
815
4 An MLN for Joint Bio-Event Extraction
In this section, we present our MLN for joint bio-
event extraction. As standard for this task, we as-
sume that Stanford dependency parses are available
in the input. Our MLN jointly makes the following
predictions: for each token, whether it is a trigger
word (and if so, what is the event type), and for each
dependency edge, whether it is in an argument path
leading to a theme or cause.
To the best of our knowledge, the latter part makes
this formulation a novel one. By breaking the pre-
diction of an argument path into that on individual
dependency edges, it can leverage the correlation
among adjacent edges and make learning more ef-
fective. Indeed, compared to other top systems, our
MLN uses a much simpler set of features, but is still
capable of obtaining state-of-the-art results.3 Com-
putationally, this formulation is also attractive. The
number of predictions is bounded by the number of
tokens and edges, and is linear in sentence length,
rather than quadratic.
Our MLN also handles the regulation events
differently. We notice that events of the three
regulation types often occur in similar contexts, and
sometimes share trigger words (e.g., ?involve?).
Therefore, our MLN merges them into a single
event type Regulation, and additionally predicts
the regulation direction (Positive or Negative).
This allows it to pool information shared by the
three types.
Base MLN: The following are the main query pred-
icates we used, along with descriptions:
Event(i): token i signifies an event;
EvtType(i, e): i is of event type e;
RegType(i, r): i is of regulation type r;
InArgPath(i, j, a): the dependency edge from i
to j is in an argument path of type a, with a
being either Theme or Cause.
If event i has type Positive regulation,
both EvtType(i, Regulation) and
RegType(i, Positive) are true. Similarly
for Negative regulation. If the type is
3In future work, we plan to incorporate a much richer set of
features; Markov logic makes such extensions straightforward.
Table 1: Formulas in the base MLN.
Token(i,+t) ? EvtType(i,+e)
Token(i,+t) ? RegType(i,+r)
Token(j,+t) ? Dep(i, j, d) ? EvtType(i,+e)
Dep(i, j,+d) ? InArgPath(i, j,+a)
Dep(i, j,+d) ? Prot(i) ? InArgPath(i, j,+a)
Dep(i, j,+d) ? Prot(j) ? InArgPath(i, j,+a)
Token(i,+t) ? Dep(i, j,+d) ? InArgPath(i, j,+a)
Token(j,+t) ? Dep(i, j,+d) ? InArgPath(i, j,+a)
Regulation, only EvtType(i, Regulation) is
true.
The main evidence predicates are:
Token(i, w): token i has word w;
Dep(i, j, d): there is a dependency edge from i to
j with label d; 4
Prot(i): i is a protein.
Our base MLN is a logistic regression model, and
can be succintly captured by eight formulas in Ta-
ble 1. All free variables are implicitly universally
quantified. The ?+? notation signifies that the MLN
contains an instance of the formula, with a separate
weight, for each value combination of the variables
with a plus sign. The first three formulas predict
the event type and regulation direction based on the
token word or its neighbor in the dependency tree.
The next five formulas predict whether a depen-
dency edge is in an argument path, based on some
combinations of token word, dependency label, and
whether the nodes are proteins.
By default, we also added the unit formulas:
Theme(x, y), Cause(x, y), EventType(x,+e),
RegType(x,+r), which capture default regularities.
Joint Inference: Like any classification system, the
formulas in the base MLN make independent predic-
tions at inference time. This is suboptimal, because
query atoms are interdependent due to either hard
constraints (e.g., an event must have a type) or soft
correlation (e.g., ?increase? signifies an event and
the dobj edge from it leads to a theme). We thus
4For convenience, we include the reverse dependency edges
in the evidence. For example, if Dep(i, j, nn) is true, then so is
Dep(j, i,?nn).
816
augment the base MLN with two groups of joint-
inference formulas. First we incorporate the follow-
ing hard constraints.
Event(i) ? ?t. EvtType(i, t)
EvtType(i, t) ? Event(i)
RegType(i, r) ? EvtType(i, Regulation)
InArgPath(i, j, Theme) ? Event(i)
? ? k 6= j. InArgPath(k, i, Theme)
InArgPath(i, j, Cause)
? EvtType(i, Regulation)
? ? k 6= j. InArgPath(k, i, Cause)
InArgPath(i, j, Theme) ? Prot(j)
? ? k 6= i. InArgPath(j, k, Theme)
InArgPath(i, j, Cause) ? Event(j) ? Prot(j)
? ? k 6= i. InArgPath(j, k, Cause)
The first three formulas enforce that events must
have a type, that a token assigned an event (regula-
tion) type must be an (regulation) event. The next
four formulas enforce the consistency of argument
path assignments: an argument path must start with
an event, in particular, a cause path must start with a
regulation event; a theme path must eventually trace
to a protein, whereas a cause path may also stop at
an event (which does not have a cause itself). To
avoid looping, we forbid reverse edges in a path.5
Notice that with these constraints, adjacent edges
in the dependency tree correlate with each other
in their InArgPath assignments, much like in an
HMM for linear sequences. Moreover, these assign-
ments correlate with the event and event-type ones;
knowing that i probably signifies an event makes it
easier to detect an argument path, and vice versa.
In addition, events that share partial argument paths
can inform each other through the predictions on
edges. In the experiments section, we will see that
merely adding these hard constraints leads to 26-
point gain in F1.
We also notice that different trigger words may
use different dependencies to start an argument path
of a particular type. For example, for many verbs,
nsubj tends to start a cause path and dobj a theme
5This is violated in some cases, and can be relaxed. We
enforced it for simplicity in this paper.
path. However, for ?bind? that signifies a Binding
event, both lead to themes, as in ?A binds B?.
Such soft regularities can be captured by a single
joint formula: Token(i,+w) ? Dep(i, j,+d) ?
Event(i)? InArgPath(i, j,+a), which correlates
event and argument type with token and dependency.
Linguistically-Motivated Formulas: Natural lan-
guages often possess systematic syntactic alterna-
tions. For example, for the word ?increase?, if both
subject and object are present, as in ?A increases
the level of B?, the subject is the cause whereas
the object is the theme. However, if only sub-
ject is present, as in ?The level of B increases?,
the subject is the theme. We thus augment the
MLN with a number of context-specific formulas
such as: Token(i, increase)? Dep(i, j, nsubj)?
Dep(i, k, dobj) ? Event(i) ? Cause(i, j).6
5 Learning And Inference
When training data comprises of many independent
subsets (e.g., individual abstracts), stochastic gradi-
ent descent (SGD) is often a favorable method for
parameter learning. By adopting small and frequent
updates, it can dramatically speed up learning and
sometimes even improve accuracy. Moreover, it eas-
ily scales to large datasets since each time it only
needs to bring a few subsets into the memory.
In this paper, we used SGD to learn weights for
our MLN. During this process, we discovered some
general challenges for applying SGD to relational
domains. For example, the ill-conditioning problem
is particularly severe, and using a single learning
rate either makes learning extremely slow or leads
to divergence. Like Lowd & Domingos (2007),
we combat this by dividing the learning rate by the
variance. However, this still leads to divergence as
learning progresses. The reason is that some weights
are strongly correlated due to the joint formulas, es-
pecially the hard constraints. Therefore, the diag-
onal approximates the Hessian poorly. Inspired by
Poon & Domingos (2007), for each formula, we
count the numbers of true and false groundings in
the train data, and add the smaller of the two plus one
to the variance, before dividing the global rate by it.
6Available at http://research.microsoft.com/-
en-us/people/lucyv/naacl10.
817
We found that this is effective for making learning
stable in our experiments.
To compute the most probable state, we used MC-
SAT to estimate the marginal probability of each
query atom, and returned the ones with probability
above a threshold. This allows us to easily trade off
precision and recall by varying the threshold. To
speed up burn-in, we followed Poon et al (2009)
and first ran MC-SAT with deterministic annealing
for initialization.
6 Correcting Syntactic Errors With
Semantic Information
Two typical types of syntactic errors are PP-
attachment and coordination. For semantic tasks
such as bio-event extraction, these errors also have
the most adverse impact to performance. For ex-
ample, for the snippet ?involvement of p70 acti-
vation in IL-10 up-regulation by gp41?, the Stan-
ford parser makes two errors by attaching ?up-
regulation? to ?activation? instead of ?involvement?,
and attaching ?gp41? to ?involvement? instead of
?up-regulation?. This makes it very difficult to pre-
dict that ?gp41? is the cause of ?up-regulation?,
and that ?up-regulation? is the theme of ?involve-
ment?. For conjucts such as ?IL-2 and IL-4 ex-
pressions?, the parser will align ?IL-2? with ?ex-
pressions?, which makes it difficult to recognize the
expression event on ?IL-2?. For nested events like
?gp41 regulates IL-2 and IL-4 expressions?, this re-
sults in three extraction errors: IL-2 expression and
the regulation event on it are missing, whereas an
erroneous regulation event on IL-2 is predicted.
Syntactic errors are often incurred due to lack
of semantic information during parsing (e.g., the
knowledge that IL-2 and IL-4 are both proteins). In
this paper, we used a heuristic method to fix such
errors by incorporating two sources of semantic in-
formation: argument paths in training data and in-
put protein labels. For conjuncts (signified by prefix
conj in Stanford dependencies) between a protein
and a non-protein, we check whether the non-protein
has a protein child, if so, we remove the conjunct and
reattach the first protein to the non-protein. For PP-
attachments, we notice that often the errors can be
fixed by reattaching the child to the closest node that
fits a known attachment pattern (e.g., ?up-regulation
by PROTEIN?). We used the following heuristics to
gather attachment patterns. For each argument path
in the training data, if it consists of a single PP edge,
then we add the combination of governor, depen-
dency label, and dependent to the pattern. (Protein
names are replaced with a special string.) If a path
contains multiple edges, but a PP edge attaches to a
word to the left of the event trigger (e.g., ?gp41? at-
tached to ?involvement?), our system concludes that
the dependent should instead be attached to the trig-
ger and adds the corresponding pattern. In addition,
we added a few default patterns like ?involvement
in? and ?effect on?. For each PP edge, the candi-
dates for reattachment include the current governor,
and the governor?s parent and all rightmost descen-
dants (i.e., its rightmost child, the rightmost child of
that child, etc.) that are to the left of the dependent.
We reattach the dependent to the closest candidate
that fits an attachment pattern. If there is none, the
attachment remains unchanged. In total, the fraction
of reattachments is about 4%.
7 Experiments
We evaluated our system on the dataset for Task 1
in the BioNLP?09 Shared Task (Kim et al, 2009).
It consists of 800 abstracts for training, 150 for de-
velopment and 260 for test. We conducted feature
development and tuned hyperparameters using the
development set, and evaluated our final system on
test using the online tool provided by the organizers.
(The test annotations are not released to the public.)
All results reported were obtained using the main
evaluation criteria for the shared task.7
7.1 System
Our system first carries out lemmatization and
breaks up hyphenated words.8 It then uses the Stan-
ford parser (de Marneffe et al, 2006) to generate de-
pendencies. For simplicity, if an event contains mul-
tiple trigger words, only the head word is labeled.9
7Namely, ?Approximate Span/Approximate Recursive
Matching?. See Kim et al (2009) for details.
8E.g., ?gp41-induced? becomes ?gp41? and ?induced?, with
a new dependency edge labeled hyphen from ?induced? to
?gp41?. To avoid breaking up protein names with hyphens, we
only dehyphenate words with suffix in a small hand-picked list.
9Most events have only one trigger, and the chosen words
only need to lie within an approximate span in evaluation.
818
Table 2: Comparison of our full system with its variants
and with UTurku on the development set.
Rec. Prc. F1
BASE 17.4 67.2 27.7
BASE+HARD 49.4 58.5 53.6
FULL 51.5 60.0 55.5
?LING 50.5 59.6 54.7
?SYN-FIX 48.2 54.6 51.2
UTurku 51.5 55.6 53.5
We implemented our system as an extension to the
Alchemy system (Kok et al, 2009). In particular, we
developed an efficient parallelized implementation
of our stochastic gradient descent algorithm using
the message-passing interface (MPI). For learning,
we used a mini-batch of 20 abstracts and iterated
through the training files twice. For each mini-batch,
we estimated the gradient by running MC-SAT for
300 samples; the initialization was done by running
annealed MC-SAT for 200 samples, with tempera-
ture dropping from 10 to 0.1 at 0.05 decrements.
For inference, we initialized MC-SAT with 1000 an-
nealed samples, with temperature dropping from 10
to 0.1 at 0.01 decrements, we then ran MC-SAT for
5000 samples to compute the marginal probabilities.
This implementation is very efficient: learning took
about 20 minutes in a 32-core cluster with 800 train-
ing files; inference took a few minutes in average.
To obtain the final assignment, we set the query
atoms with probability no less than 0.4 to true and
the rest to false. The threshold is chosen to max-
imize F1 in the development set. To generate the
events, we first found arguments for each trigger i
by gathering all proteins and event triggers that were
accessible from i along an argument path without
first encountering another trigger. For triggers of
base event types, we dropped other triggers from
its argument list. For nested triggers, we generated
events recursively by first processing argument trig-
gers and generating their events, and then generating
events for the parent trigger by including all combi-
nations of argument events. For Binding triggers,
we group its arguments by the first dependency la-
bels in the argument paths, and generate events by a
cross-product of the group members.
Table 3: Per-type recall/precision/F1 for our full system
on the development set.
Rec. Prc. F1
Expression 75.6 79.1 77.3
Transcription 69.5 73.1 71.3
Phosphorylation 87.2 87.2 87.2
Catabolism 85.7 100 92.3
Localization 66.0 85.4 74.5
Binding 39.1 61.8 47.9
Positive regulation 41.8 51.0 46.0
Negative regulation 39.3 56.2 46.3
Regulation 41.4 33.2 36.8
7.2 Results
We first conducted experiments on the develop-
ment set to evaluate the contributions of individual
components. Table 2 compares their performances
along with that of UTurku. The base MLN (BASE)
alone performed rather poorly. Surprisingly, by just
adding the hard constraints to leverage joint infer-
ence (BASE+HARD), our system almost doubled
the F1, and tied UTurku. In addition, adding the
soft joint-inference formula results in further gain,
and our full system (FULL) attained an F1 of 55.5.
This is two points higher than UTurku and the best
reported result on this dataset. The linguistically-
motivated formulas are beneficial, as can seen by
comparing with the system without them (?LING),
although the difference is small. Fixing the syntactic
errors with semantic information, on the other hand,
leads to substantial performance gain. Without do-
ing it (?SYN-FIX), our system suffers an F1 loss of
more than four points. This verifies that the quality
of syntactic analysis is important for event extrac-
tion. The differences between FULL and other vari-
ants (except -LING) are all statistically significant at
1% level using McNemar?s test.
To understand the performance bottlenecks, we
show the per-type results in Table 3 and the re-
sults at the predicate level in Table 4.10 Both trig-
ger and argument-edge detections leave much room
for improvement. In particular, the system pro-
posed many incorrect regulation triggers, partly be-
cause regulation triggers have the most variations
10Numbers in Table 3 refer to events, whereas in Table 4 to
triggers. A trigger may signify multiple events, so numbers in
Table 4 can be smaller than that in Table 3.
819
Table 4: Predicate recall/precision/F1 for our full system
on the development set.
Rec. Prc. F1
Expression 80.1 82.0 81.0
Transcription 68.8 71.0 69.8
Phosphorylation 87.5 92.1 89.7
Catabolism 84.2 100 91.4
Localization 62.5 86.2 72.5
Binding 62.4 82.4 71.1
Positive regulation 65.8 70.7 68.2
Negative regulation 58.3 71.7 64.3
Regulation 61.7 43.4 50.9
All triggers 68.1 71.7 69.9
Argument edge 69.0 71.8 70.4
Table 5: Comparison of our full system with top systems
on the test set.
Rec. Prc. F1
UTurku 46.7 58.5 52.0
JULIELab 45.8 47.5 46.7
ConcordU 35.0 61.6 44.6
Riedel et al 36.9 55.6 44.4
FULL MLN 43.7 58.6 50.0
among all types. Our system did well in recognizing
Binding triggers, but performed much poorer at the
event level. This indicates that the bottleneck lies in
correctly identifying all arguments for multi-theme
events. Indeed, if we evaluate on individual event-
theme pairs for Binding, the F1 jumps 15 points to
62.8%, with precision 82.7% and recall 50.6%.
Finally, Table 5 compares our system with the top
systems on the test set. Our system trailed UTurku
due to a somewhat lower recall, but substantially
outperformed all other systems. In particular, it re-
duced F1 error by more than 10% compared to the
previous best joint approach by Riedel et al (2009).
7.3 Error Analysis
Through manual inspection, we found that many re-
maining errors were related to syntactic parses. The
problem is particularly severe when there are nested
or co-occuring PP-attachments and conjuncts (e.g.,
?increased levels of IL-2 and IL-4 mRNA and pro-
tein in the cell?). Our rule-based procedure in Sec-
tion 6 has high precision in fixing some of these er-
rors, but the coverage is limited. It also makes hard
decisions in a preprocessing step, which cannot be
reverted. A principled solution is to resolve syntactic
and semantic ambiguities in a joint model that inte-
grates reattachment decisions and extractions. This
can potentially resolve more syntactic errors, as ex-
traction makes more semantic information available,
and is more robust to reattachment uncertainty.
In some challenging cases, we found further op-
portunities for joint inference. For example, in the
sentence ?These cells are deficient in FasL expres-
sion, although their cytokine IL-2 production is nor-
mal?, ?normal? signifies a Positive regulation
event over ?IL-2 production? because of its contrast
with ?deficient?. Such events can be detected by in-
troducing additional joint inference rules that lever-
age syntactic structures such as subclauses.
We also found many cases where the annota-
tions differ for the same expressions. For ex-
ample, ?cotransfection with PROTEIN? is some-
times labeled as both an Expression event and a
Positive regulation event, and sometimes not
labeled at all. This occurs more often for regulation
events, which partly explains the low precision for
them.
8 Conclusion
This paper presents the first joint approach for bio-
event extraction that achieves state-of-the-art results.
This is made possible by adopting a novel formula-
tion that jointly predicts events, arguments, as well
as individual dependency edges in argument paths.
Our system is based on Markov logic and can be
easily extended to incorporate additional knowledge
and linguistic features to further improve accuracy.
Directions for future work include: leveraging ad-
ditional joint-inference opportunities, better integra-
tion of syntactic parsing and event extraction, and
applying this approach to other extraction tasks and
domains.
9 Acknowledgements
We give warm thanks to Sebastian Riedel and three
anonymous reviewers for helpful comments and
suggestions.
820
References
G. Bakir, T. Hofmann, B. B. Scho?lkopf, A. Smola,
B. Taskar, S. Vishwanathan, and (eds.). 2007. Pre-
dicting Structured Data. MIT Press, Cambridge, MA.
Jari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the BioNLP Workshop
2009.
Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew
McCallum, Tom Mitchell, Kamal Nigam, and Sean
Slattery. 1999. Learning to construct knowledge bases
from the world wide web. Artificial Intelligence.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference on
Language Resources and Evaluation, pages 449?454,
Genoa, Italy. ELRA.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Morgan & Claypool, San Rafael, CA.
Lise Getoor and Ben Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
Jan Hajic, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria Antonia Martii, Lluis Mar-
quez, Adam Meyers, Joakim Nivre, Sebastian Pado,
Jan Stepanek, Pavel Stranak, Mihai Surdeanu, Nian-
wen Xue, and Yi Zhang. 2009. The CoNLL-2009
Shared Task: syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2009. Overview of
BioNLP-09 Shared Task on event extraction. In Pro-
ceedings of the BioNLP Workshop 2009.
Stanley Kok, Parag Singla, Matt Richardson, Pedro
Domingos, Marc Sumner, Hoifung Poon, and Daniel
Lowd. 2009. The alchemy system for statistical re-
lational ai. Technical report, Dept. of CSE, Univ. of
Washington, http://alchemy.cs.washington.edu/.
Daniel Lowd and Pedro Domingos. 2007. Efficient
weight learning for markov logic networks. In Pro-
ceedings of the Eleventh European Conference on
Principles and Practice of Knowledge Discovery in
Databases, pages 200?211, Warsaw. Springer.
Hoifung Poon and Pedro Domingos. 2006. Sound and
efficient inference with probabilistic and determinis-
tic dependencies. In Proceedings of the Twenty First
National Conference on Artificial Intelligence, pages
458?463, Boston, MA. AAAI Press.
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In Proceedings of the
Twenty Second National Conference on Artificial In-
telligence, pages 913?918, Vancouver, Canada. AAAI
Press.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proceedings of NAACL-HLT,
Boulder, Colorado. ACL.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Junichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In Proceedings of
the BioNLP Workshop 2009.
Rune Saetre, Makoto Miwa, Kazuhiro Yoshida, and Ju-
nichi Tsujii. 2009. From protein-protein interaction
to molecular event extraction. In Proceedings of the
BioNLP Workshop 2009.
821
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 742?751,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Mind the Gap: Learning to Choose Gaps for Question Generation 
Lee Becker 
Department of Computer Science 
University of Colorado Boulder 
Boulder, CO 80309 
lee.becker@colorado.edu 
 
 
Sumit Basu and Lucy Vanderwende 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
{sumitb,lucyv}@microsoft.com 
 
 
Abstract 
Not all learning takes place in an educational 
setting: more and more self-motivated learners 
are turning to on-line text to learn about new 
topics. Our goal is to provide such learners 
with the well-known benefits of testing by au-
tomatically generating quiz questions for on-
line text. Prior work on question generation 
has focused on the grammaticality of generat-
ed questions and generating effective multi-
ple-choice distractors for individual question 
targets, both key parts of this problem. Our 
work focuses on the complementary aspect of 
determining what part of a sentence we should 
be asking about in the first place; we call this 
?gap selection.? We address this problem by 
asking human judges about the quality of 
questions generated from a Wikipedia-based 
corpus, and then training a model to effective-
ly replicate these judgments. Our data shows 
that good gaps are of variable length and span 
all semantic roles, i.e., nouns as well as verbs, 
and that a majority of good questions do not 
focus on named entities. Our resulting system 
can generate fill-in-the-blank (cloze) ques-
tions from generic source materials. 
1 Introduction 
Assessment is a fundamental part of teaching, both 
to measure a student?s mastery of the material and 
to identify areas where she may need reinforce-
ment or additional instruction. Assessment has also 
been shown an important part of learning, as test-
ing assists retention and can be used to guide learn-
ing (Anderson & Biddle, 1975). Thus, as learners 
move on from an educational setting to unstruc-
tured self-learning settings, they would still benefit 
from having the means for assessment available. 
Even in traditional educational settings, there is a 
need for automated test generation, as teachers 
want multiple tests for topics to give to different 
students, and students want different tests with 
which to study and practice the material. 
One possible solution to providing quizzes for 
new source material is the automatic generation of 
questions. This is a task the NLP community has 
already embraced, and significant progress has 
been made in recent years with the introduction of 
a shared task (Rus et al, 2010). However, thus far 
the research community has focused on the prob-
lem of generating grammatical questions (as in 
Heilman and Smith (2010a)) or generating effec-
tive distractors for multiple-choice questions 
(Agarwal and Mannem, 2011). 
While both of these research threads are of crit-
ical importance, there is another key issue that 
must be addressed ? which questions should we be 
asking in the first place? We have highlighted this 
aspect of the problem in the past (see 
Vanderwende (2008)) and begin to address it in 
this work, postulating that we can both collect hu-
man judgments on what makes a good question 
and train a machine learning model that can repli-
cate these judgments. The resulting learned model 
can then be applied to new material for automated 
question generation. We see this effort as comple-
mentary to the earlier progress.    
In our approach, we factor the problem of gen-
erating good questions into two parts: first, the se-
lection of sentences to ask about, and second, the 
identification of which part of the resulting sen-
tences the question should address. Because we 
want to focus on these aspects of the problem and 
not the surface form of the questions, we have cho-
sen to generate simple gap-fill (cloze) questions, 
742
though our results can also be used to trigger Wh-
questions or multiple-choice questions by leverag-
ing prior work. For sentence selection, we turn to 
methods in summarization and use the simple but 
effective SumBasic (Nenkova et al, 2006) algo-
rithm to prioritize and choose important sentences 
from the article. We cast the second part, gap se-
lection, as a learning problem. To do this, we first 
select a corpus of sentences from a very general 
body of instructional material (a range of popular 
topics from Wikipedia). We then generate a con-
strained subset of all possible gaps via NLP heuris-
tics, and pair each gap with a broad variety of 
features pertaining to how it was generated. We 
then solicit a large number of human judgments via 
crowdsourcing to help us rate the quality of various 
gaps. With that data in hand, we train a machine 
learning model to replicate these judgments. The 
results are promising, with one possible operating 
point producing a true positive rate of 83% with a 
corresponding false positive rate of 19% (83% of 
the possible Good gaps are kept, and 19% of the 
not-Good gaps are incorrectly marked); see Figure 
6 for the full ROC curve and Section 4 for an ex-
planation of the labels. As the final model has only 
minimal dependence on Wikipedia-specific fea-
tures, we expect that it can be applied to an even 
wider variety of material (blogs, news articles, 
health sites, etc.).   
2 Background and Related Work 
There already exists a large body of work in auto-
matic question generation (QG) for educational 
purposes dating back to the Autoquest system 
(Wolfe, 1976), which used an entirely syntactic 
approach to generate Wh-Questions from individu-
al sentences. In addition to Autoquest, several oth-
ers have created systems for Wh-question 
generation using approaches including transfor-
mation rules (Mitkov and Ha, 2003), template-
based generation (Chen et al, 2009; Curto et al, 
2011), and overgenerate-and-rank (Heilman and 
Smith, 2010a). The work in this area has largely 
focused on the surface form of the questions, with 
an emphasis on grammaticality.   
Alternatively, generation of gap-fill style ques-
tions (a.k.a. cloze questions) avoids these issues of 
grammaticality by blanking out words or spans in a 
known good sentence. There is a large body of ex-
isting work that has focused on generation of this 
type of question, most of which has focused on 
vocabulary and language learning. The recent work 
of Agarwal and Mannem (2011) is closer to our 
purposes; they generated fill-in-the-blank questions 
and distractor answers for reading comprehension 
tests using heuristic scoring measures and a small 
evaluation set. Our work has similar aims but em-
ploys a data-driven approach. 
The Question-Generation Shared Task and 
Evaluation Challenge (QG-STEC) (Rus et al, 
2010) marks a first attempt at creating a common 
task and corpus for empirical evaluation of ques-
tion generation components. However, evaluation 
in this task was manual and the number of instanc-
es in both the development and training set were 
small. As there exists no other dataset for question 
generation, we created a new corpus using Amazon 
Mechanical Turk by soliciting judgments from 
non-experts. Snow et al (2008) have validated 
AMT as a valid data source by comparing non-
expert with gold-standard expert judgments. Cor-
pus creation using AMT has numerous precedents 
now; see i.e. Callison-Burch and Dredze (2010) 
and Heilman and Smith (2010b). We have made 
our corpus (see Section 4) available online to ena-
ble others to continue research on the gap-selection 
problem we address here.  
3 Question Generation  
To achieve our goal of selecting better gap-fill 
questions, we have broken down the task into stag-
es similar to those proposed by Nielsen (2008): 1) 
sentence selection, 2) question construction, and 3) 
classification/scoring. Specifically, we utilize sum-
marization to identify key sentences from a pas-
sage. We then apply semantic and syntactic 
constraints to construct multiple candidate ques-
tion/answer pairs from a given source sentence.  
Lastly we extract features from these hypotheses 
for use with a question quality classification mod-
el. To train this final question-scoring component, 
we made use of crowdsourcing to collect ratings 
for a corpus of candidate questions. While this 
pipeline currently produces gap-fill questions, we 
envision these components can be used as input for 
more complex surface generation such as Wh- 
forms or distractor selection. 
 
 
743
3.1 Sentence Selection 
When learning about a new subject, a student will 
most likely want to learn about key concepts be-
fore moving onto more obscure details. As such, it 
is necessary to order target sentences in terms of 
their importance. This is fortunately very similar to 
the goals of automatic summarization, in which the 
selected sentences should be ordered by how cen-
tral they are to the article. 
As a result, we make use of our own implemen-
tation of SumBasic (Nenkova et al, 2006), a sim-
ple but competitive document summarization al-
gorithm motivated by the assumption that 
sentences containing the article?s most frequently 
occurring words are the most important. We thus 
use the SumBasic score for each sentence to order 
them as candidates for question construction.    
3.2 Question Construction 
We seek to empirically determine how to choose 
questions instead of relying on heuristics and rules 
for evaluating candidate surface forms. To do this, 
we cast question construction as a generate-and-
filter problem: we overgenerate potential ques-
tion/answer pairs from each sentence and train a 
discriminative classifier on human judgments of 
quality for those pairs. With the trained classifier, 
we can then apply this approach on unseen sen-
tences to return the highest-scoring ques-
tion/answer, all question/answer pairs scoring 
above a threshold, and so on. 
Generation 
Although it would be possible to select every word 
or phrase as a candidate gap, this tactic would pro-
duce a skewed dataset composed mostly of unusa-
ble questions, which would subsequently require 
much more annotation to discriminate good ques-
tions from bad ones. Instead we rely on syntactic 
and semantic constraints to reduce the number of 
questions that need annotation. 
To generate questions we first run the source 
sentence through a constituency parser and a se-
mantic role labeler (components of a state-of-the-
art natural language toolkit from (Quirk et al, 
2012)), with the rationale that important parts of 
the sentence will occur within a semantic role. 
Each verb predicate found within the roles then 
automatically becomes a candidate gap. From eve-
ry argument to the predicate, we extract all child 
noun phrases (NP) and adjectival phrases (ADJP) 
as candidate gaps as well. Figure 1 illustrates this 
generation process.  
Classification 
To train the classifier for question quality, we ag-
gregated per-question ratings into a single label 
(see Section 4 for details). Questions with an aver-
age rating of 0.67 or greater were considered as 
positive examples. This outcome was then paired 
with a vector of features (see Section 5) extracted 
from the source sentence and the generated ques-
tion. 
Because our goal is to score each candidate 
question in a meaningful way, we use a calibrated 
learner, namely L2-regularized logistic regression 
(Bishop 2006). This model?s output is 
(|	
	); in our case this is the posteri-
or probability of a candidate receiving a positive 
label based on its features. 
4 Corpus Construction 
We downloaded 105 articles from Wikipedia?s 
listing of vital articles/popular pages.1 These arti-
cles represent a cross section of historical, social, 
                                                        
1http://en.wikipedia.org/wiki/Wikipedia:Vital_articles/Popular
_pages 
In 1874 R?ntgen              a lecturer at the University of Strassburg. 
In          R?ntgen became a lecturer at the University of Strassburg. 
In 1874               became a lecturer at the University of Strassburg. 
In 1874 R?ntgen became a              at the University of Strassburg. 
In 1874 R?ntgen became a lecturer at                         of Strassburg. 
In 1874 R?ntgen became a lecturer at the University of                  . 
In 1874 R?ntgen became a lecturer at                                              . 
Figure 1 An example of the question generation process. 
744
and scientific topics. From each article we sampled 
10 sentences using the sentence selection algorithm 
described in Section 3.1 for 50% of the sentences; 
the other 50% were chosen at random to prevent 
any possible overfitting to the selection method. 
These sentences were then processed using the 
candidate generation method from Section 3.2. 
To collect training data outcomes for our ques-
tion classifier, we used Amazon?s Mechanical 
Turk (AMT) service to obtain human judgments of 
quality for each question. We initially considered 
asking about the quality of individual ques-
tion/answer pairs in isolation, but in pilot studies 
we found poor agreement in this case; we noticed 
that the inability to compare with other possible 
questions actually made the task seem difficult and 
arbitrary. We thus instead presented raters with a 
source sentence and a list of up to ten candidate 
questions along with their corresponding answers 
(see Figure 2). Raters were asked to rate questions? 
quality as Good, Okay, or Bad. The task instruc-
tions defined a Good question as ?one that tests 
key concepts from the sentence and would be rea-
sonable to answer.? An Okay question was defined 
as ?one that tests key concepts but might be diffi-
cult to answer (the answer is too lengthy, the an-
swer is ambiguous, etc.).? A Bad question was 
?one that asks about an unimportant aspect of the 
sentence or has an uninteresting answer that can be 
figured out from the context of the sentence.? The-
se ratings were binarized into a score of one for 
Good and zero for not-Good (Okay or Bad), as our 
goal was to find the probability of a question being 
truly Good (and not just Okay).2  
                                                        
2
 Heilman and Smith (2010a and b) asked raters to identify 
question deficiencies, including vague or obvious, but raters 
were not asked to differentiate between Good and Okay. Thus 
questions considered Good in their study would include Okay. 
Thus far we have run 300 HITs with 4 judges 
per HIT. Each HIT consisted of up to 10 candidate 
questions generated from a single sentence. In total 
this yielded 2252 candidate questions with 4 rat-
ings per question from 85 unique judges. We then 
wished to eliminate judges who were gaming the 
system or otherwise performing poorly on the task. 
It is common to do such filtering when using 
crowdsourced data by using the majority or median 
vote as the final judgment or to calibrate judges 
using expert judgments (Snow et al 2008). Other 
approaches to annotator quality control include 
using EM-based algorithms for estimating annota-
tor bias (Wiebe et al 1999, Ipeirotis et al 2010).  
In our case, we computed the distance for each 
judge from the median judgment (from all judges) 
on each question, then took the mean of this dis-
tance over all questions they rated. We removed 
judges with a mean distance two standard devia-
tions above the mean distance, which eliminated 
the five judges who disagreed most with others. 
In addition to filtering judges, we wanted to fur-
ther constrain the data to those questions on which 
the human annotators had reasonable agreement, as 
it would not make sense to attempt to train a model 
to replicate judgments on which the annotators 
themselves could not agree. To do this, we com-
puted the variance of the judgments for each ques-
tion. By limiting the variance to 0.3, we kept ques-
tions on which up to 1 judge (out of 4) disagreed; 
this eliminated 431 questions and retained the 1821 
with the highest agreement. Of these filtered ques-
tions, 700 were judged to be Good (38%). 
To formally assess inter-rater reliability we 
computed Krippendorff?s alpha (Krippendorff, 
2004), a statistical measure of agreement applica-
ble for situations with multiple raters and incom-
plete data (in our case not all raters provided 
ratings for all items). An alpha value of 1.0 indi-
cates perfect agreement, and an alpha value of 0.0 
Source Sentence:  
    The large scale production of chemicals was an important development during the Industrial Revolution. 
 
Question Answer Ratings 
The _ _ _ _ _ _ _ _ _ _ _ _  of chemicals was an important 
development during the Industrial Revolution. 
large scale production ? Good   ?  Okay   ? Bad 
The large scale production of _ _ _ _ _ _was an important 
development during the Industrial Revolution. 
chemicals ? Good   ?  Okay   ? Bad 
The large scale production of chemicals was an important  
development during the _ _ _ _ _ _ _ _ _ _ _ . 
Industrial Revolution ?  Good   ? Okay   ? Bad 
 Figure 2: Example question rating HIT 
745
indicates no agreement. Our original data yielded 
an alpha of 0.34, whereas after filtering judges and 
questions the alpha was 0.51. It should be noted 
that because Krippendorff?s Alpha accounts for 
variability due to multiple raters and sample size, 
its values tend to be more pessimistic than many 
Kappa values commonly used to measure inter-
rater reliability. 
For others interested in working with this data, 
we have made our corpus of questions and ratings 
available for download at the following location: 
http://research.microsoft.com/~sumitb/questiongen
eration.   
5 Model Features 
While intuition would suggest that selecting high-
quality gaps for cloze questions should be a 
straightforward task, analysis of our features im-
plies that identifying important knowledge depends 
on more complex interactions between syntax, se-
mantics, and other constraints. In designing fea-
tures, we focused on using commonly extracted 
NLP information to profile the answer (gap), the 
source sentence, and the relation between the two. 
To enable extraction of these features, we used 
the MSR Statistical Parsing and Linguistic Analy-
sis Toolkit (MSR SPLAT)3, a state-of-the-art, web-
based service for natural language processing 
(Quirk et al, 2012). Table 1 shows a breakdown of 
our feature categories and their relative proportion 
of the feature space. In the subsections below, we 
describe the intuitions behind our choice of fea-
tures and highlight example features from each of 
these categories. An exhaustive list of the features 
can be found at the corpus URL listed in Section 4. 
5.1 Token Count Features 
A good question gives the user sufficient context to 
answer correctly without making the answer obvi-
ous. At the same time, gaps with too many words 
may be impossible to answer. Figure 3 shows the 
distributions of number of tokens in the answer 
(i.e., the gap) for Good and not-Good questions. As 
intuition would predict, the not-Good class has 
higher likelihoods for the longer answer lengths. In 
addition to the number and percentage of tokens in 
the answer features, we also included other token 
                                                        
3
 http://research.microsoft.com/projects/msrsplat  
count features such as the number of tokens in the 
sentence, and the number of overlapping tokens 
between the answer and the remainder of the sen-
tence. 
 
Feature Category Number of Features 
Token Count 5 
Lexical 11 
Syntactic 112 
Semantic 40 
Named Entity 11 
Wikipedia link 3 
Total 182 
Table 1: Breakdown of features by category 
5.2 Lexical features 
Although lexical features play an important role in 
system performance for several NLP tasks like 
parsing, and semantic role labeling, they require a 
large number of examples to provide practical ben-
efit. Furthermore, because most sentences in Wik-
ipedia articles feature numerous domain-specific 
terms and names, we cannot rely on lexical fea-
tures to generalize across the variety of possible 
articles in our corpus. Instead we approximate lex-
icalization by computing densities of word catego-
ries found within the answer. The intuition behind 
these features is that an answer composed primari-
ly of pronouns and stopwords will make for a bad 
question while an answer consisting of specific 
entities may make for a better question. Examples 
of our semi-lexical features include answer pro-
noun density, answer abbreviation density, answer 
capitalized word density, and answer stopword 
density. 
5.3 Syntactic Features 
The answer?s structure relative to the sentence?s 
structure provides information as to where better 
spans for the gap may exist. Similarly, part-of-
speech (POS) tags give a topic-independent repre-
sentation of the composition and makeup of the 
questions and answers. The collection of syntactic 
features includes the answer?s depth with the sen-
tence?s constituent parse, the answer?s location 
relative to head verb (before/after), the POS tag 
before the answer, the POS tag after the answer, 
and the answer bag-of-POS tags. 
 
746
 Figure 3: Distribution of number of tokens in the answer 
for Good and not-Good questions. 
5.4 Semantic Role Label Features 
Beyond syntactic constraints, semantics can yield 
additional cues in identifying the important spans 
for questioning. Shallow-semantic parses like those 
found in Propbank (Palmer et al, 2005) provide a 
concise representation for linking predicates 
(verbs) to their arguments. Because these semantic 
role labels (SRLs) often correspond to the ?who, 
what, where, and when? of a sentence, they natu-
rally lend themselves for use as features for rating 
question quality. To compute SRL features, we 
used the MSR SPLAT?s semantic role labeler to 
find the SRLs whose spans cover the question?s 
answer, the SRLs whose spans are contained with-
in the answer, and the answer?s constituent parse 
depth within the closest covering SRL node. 
To investigate whether judges keyed in on spe-
cific roles or modifiers when rating questions, we 
plotted the distribution of the answer-covering 
SRLs (Figure 4). This graph indicates that good 
answers are not associated with only a single label 
but are actually spread across all SRL classes. 
While the bulk of questions came from the argu-
ments often corresponding to subjects and objects 
(ARG0-2, shown as A0-A2), we see that good and 
bad questions have mostly similar distributions 
over SRL classes. However, a notable exception 
are answers covered by verb predicates (shown as 
?predicate?), which were highly skewed with 190 
of the 216 (88.0%) question/answer pairs exhibit-
ing this feature labeled as Bad. Together these dis-
tributions may suggest that judges are more likely 
to rate gap-fill questions as Good if they corre-
spond   to  questions  of   ?who,  what,  where,  and  
 
Figure 4: Distribution of semantic role labels for 
Good and not-Good questions.  
 
when? over questions pertaining to ?why and 
how.? 
5.5 Named Entity Features 
For many topics, especially in the social sciences, 
knowing the relevant people and places marks the 
first step toward comprehending new material. To 
reflect these concerns we use the named-entity 
tagger in the toolkit to identify the spans of text 
that refer to persons, locations, or organizations, 
which are then used to derive additional features 
for distinguishing between candidate questions. 
Example named-entity features include: answer 
named entity density, answer named entity type 
frequency (LOC, ORG, PER), and sentence named 
entity frequency. 
Figure 5 shows the distribution of named entity 
types found within the answers for Good and not-
Good questions. From this graph, we see that Good 
questions have a higher class-conditional probabil-
ity of containing a named entity. Furthermore, we 
see that Good questions are not confined to a sin-
gle named entity type, but are spread across all 
types. Together, these distributions indicate that 
while named entities can help to identify important 
gaps, the majority of questions labeled Good do 
not contain any named entity (515/700, i.e. 74%). 
This provides substantial evidence for generating 
questions for more than only named entities. 
 
747
 Figure 5: Distribution of answer named entity type for 
Good and not-Good questions. 
5.6 Wikipedia Link Features 
Wikipedia?s markup language allows spans of text 
to link to other articles. This annotation inherently 
indicates a span of text as noteworthy, and can 
serve as evidence of an answer?s importance. We 
use the presence of this markup to compute fea-
tures such as answer link density, sentence link 
density, and the ratio of the number of linked 
words in the answer to the ratio of linked words in 
the sentence. 
6 Model and Training 
We chose logistic regression as our classifier be-
cause of its calibrated output of the class posterior; 
we combined it with an L2 regularizer to prevent 
overfitting. As the data likelihood is convex in the 
model parameters, we trained the model to maxim-
ize this quantity along with the regularization term 
using the L-BFGS algorithm for Quasi-Newton 
optimization (Nocedal, 1980). Evaluation was 
conducted with 10-fold cross validation, taking 
care to stratify folds so that all questions generated 
from the same source sentence are placed in the 
same fold. Results are shown in Section 7 below. 
To ensure that we were not overly narrow in 
this model choice, we tested two other more pow-
erful classifiers that do not have calibrated outputs, 
a linear SVM and a boosted mixture of decision 
trees (Caruana and Niculescu-Mizil, 2006); both 
produced accuracies within a percentage point of 
our model at the equal error rate. 
7 Results and Discussion 
Figure 6 shows ROC curves for our question quali-
ty classifier produced by sweeping the threshold on 
the output probability, using the raw collected data, 
our filtered version as described above, and a fur-
ther filtered version keeping only those questions 
where judges agreed perfectly; the benefits of fil-
tering can be seen in the improved performance. In 
this context, the true positive rate refers to the frac-
tion of Good questions that were correctly identi-
fied, and the false positive rate refers to the 
fraction of not-Good questions that were incorrect-
ly marked. At the equal error rate, the true positive 
rate was 0.83 and the false positive rate was 0.19.  
Figure 6: ROC for our model using unfiltered data 
(green dots), our filtered version (red dashes), and fil-
tered for perfect agreement (blue line). 
 
Choosing the appropriate operating point depends 
on the final application. By tuning the classifier?s 
true positive and false positive rates, we can cus-
tomize the system for several uses. For example, in 
a relatively structured scenario like compliance 
training, it may be better to reduce any possibility 
of confusion by eliminating false positives. On the 
other hand, a self-motivated learner attempting to 
explore a new topic may tolerate a higher false 
positive rate in exchange for a broader diversity of 
questions. The balance is subtle, though, as ill-
formed and irrelevant questions could leave the 
learner bored or frustrated, but alternatively, overly 
conservative question classification could poten-
tially eliminate all but the most trivial questions. 
 
T
ru
e 
P
os
iti
ve
 R
at
e
748
 Figure 7: ROC for our model with (red dash) and with-
out (blue line) Wikipedia-specific features. 
 
 
Figure 8: Classifier learning curve; each point repre-
sents mean accuracy over 40 folds. 
 
We next wanted to get a sense of how well the 
model would generalize to other text, and as such 
ran an analysis of training the classifier without the 
benefit of the Wikipedia-specific features (Figure 
7). The resulting model performs about the same as 
the original on average over the ROC, slightly bet-
ter in some places and slightly worse in others. We 
hypothesize the effect is small because these fea-
tures relate only to Wikipedia entities, and the oth-
er named entity features likely make them 
redundant. 
Finally, to understand the sensitivity of our 
model to the amount of training data, we plot a 
learning curve of the question classifier?s accuracy 
by training it against fractions of the available data 
(Figure 8). While the curve starts to level out 
around 1200 data points, the accuracy is still rising 
slightly, which suggests the system could achieve 
some small benefits in accuracy from more data. 
7.1 Error Analysis 
To explore the nature of our system?s misclassifi-
cations we examine the errors that occur at the 
equal error rate operating point. For our system, 
false positive errors occur when the system labels a 
question as Good when the raters considered it not-
Good. Table 2 lists three examples of this type of 
error. The incorrect high score in example 1 
(?Greeks declared ___?) suggests that system per-
formance can be improved via language modeling, 
as such features would penalize questions with an-
swers that could be predicted mostly by word tran-
sition probabilities. Similarly, when classifying 
questions like example 2 (?such as ____ for a 
mathematical function?), the system could benefit 
from some measure of word frequency or answer 
novelty. While our model included a feature for the 
number of overlapping words between the question 
and the answer, the high classifier score for exam-
ple 3 (?atop ______, the volcano?), suggests that 
this can be solved by explicitly filtering out such 
questions at generation time.  
With false negative errors the judges rated the 
question as Good, whereas the system classified it 
as Bad. The question and answer pairs listed in 
Table 3 demonstrate some of these errors. In ex-
ample 1 (?where Pompey was soon ___?), the sys-
tem was likely incorrect because a majority of 
questions with verb-predicate answers had Bad 
ratings (only 12% are Good). Conversely, classifi-
cation of example 2 (?Over the course of dec-
ades??) could be improved with a feature 
indicating the novelty of the words in the answers. 
Example 3 (?About 7.5% of the...?) appears to 
come from rater error or rater confusion as the 
question does little to test the understanding of the 
material. 
While the raters considered the answer to ex-
ample 4 as Good, the low classifier score argues 
for different handling of answers derived from 
long coordinated phrases. One alternative approach 
would be to generate questions that use multiple 
gaps. Conversely, one may argue that a learner 
may be better off answering any one of the noun 
phrases like palm oil or cocoa in isolation.  
 
 
ac
cu
ra
cy
749
 Question Answer Confidence 
1 In 1821 the Greeks 
declared ___ on the 
sultan. 
war 0.732 
2 He also introduced 
much of the modern 
mathematical terminol-
ogy and notation, par-
ticularly for 
mathematical analysis, 
such as _________ of a 
mathematical function. 
the notion 0.527 
3 Not only is there much 
ice atop ________, the 
volcano is also being 
weakened by hydro-
thermal activity. 
the volcano 0.790 
Table 2: Example false positives (human judges rated 
these as not-Good) 
 
 Question Answer Confidence 
1 Caesar then pursued 
Pompey to Egypt, 
where Pompey was 
soon  ____.  
murdered 0.471 
2 Over the course of dec-
ades, individual wells 
draw down local tem-
peratures and water 
levels until _______ is 
reached with natural 
flows. 
a new  
equilibrium 
0.306 
3 About 7.5% of world 
sea trade is carried via 
the canal ____.  
today 0.119 
4 Asante and Dahomey 
concentrated on the 
development of ?legiti-
mate commerce? in 
__________, forming 
the bedrock of West 
Africa?s modern export 
trade,  
the form of 
palm oil, 
cocoa, tim-
ber, and 
gold. 
0.029 
Table 3: Example false negatives (human judges rated 
these Good) 
 
7.2 Feature Analysis 
To ensure that all of the gain of the classifier was 
not coming from only a handful of isolated fea-
tures, we examined the mean values for each fea-
ture?s learned weight in the model over the course 
of 10 cross-validation folds, and then sorted the 
means for greater clarity (Figure 8). The weights 
indeed seem to be well distributed across many 
features. 
 
Figure 8: Feature weight means and standard deviations. 
8 Discussion and Future Work 
We have presented a method that determines 
which gaps in a sentence to ask questions about by 
training a classifier that largely agrees with human 
judgments on question quality. We feel this effort 
is complementary to the past work on question 
generation, and represents another step towards 
helping self-motivated learners with automatically 
generated tests. 
In our future work, we hope to expand the set of 
features as described in Section 7. We additionally 
intend to cast the sentence selection problem as a 
separate learning problem that can also be trained 
from human judgments. 
References 
Manish Agarwal and Prashanth Mannem. 2011. Auto-
matic Gap-fill Question Generation from Text 
Books. In Proceedings of the 6th Workshop on In-
novative Use of NLP for Building Educational Ap-
plications. Portland, OR, USA. pages 56-64. 
Richard C. Anderson and W. Barry Biddle. 1975. On 
asking people questions about what they are read-
ing. In G. Bower (Ed.) Psychology of Learning and 
Motivation, 9:90-132. 
Christopher M. Bishop. 2006. Pattern Recognition and 
Machine Learning. New York: Springer, 2006. 
Jonathan C. Brown, Gwen A. Frishkoff, and Maxine 
Eskenazi. 2005. Automatic Question Generation for 
Vocabulary Assessment. In Proceedings of 
HLT/EMNLP 2005. Vancouver, Canada: Associa-
tion for Computational Linguistics. pages 819-826. 
Rich Caruana and Alexandru Niculescu-Mizil. 2006. An 
Empirical Comparison of Supervised Learning Al-
gorithms. In Proceedings of ICML 2006. 
m
ea
n 
an
d 
st
d.
 d
ev
. o
f f
ea
tu
re
 w
ei
gh
t
750
Chris Callison-Burch and Mark Dredze. 2010. Creating 
Speech and Language Data with Amazon's Me-
chanical Turk. In Proceedings of NAACL 2010 
Workshop on Creating Speech and Language Data 
with Amazon's Mechanical Turk. Los Angeles, CA. 
pages 1-12. 
Wei Chen, Gregory Aist, and Jack Mostow. 2009. Gen-
erating Questions Automatically from Information-
al Text. In S. Craig & S. Dicheva (Ed.), 
Proceedings of the 2nd Workshop on Question 
Generation. 
S?rgio Curto, Ana Cristina Mendes, and Lu?sa Coheur. 
2011. Exploring linguistically-rich patterns for 
question generation. In Proceedings of the UCNLG 
+ eval: Language Generation and Evaluation 
Workshop. Edinburgh, Scotland: Association for 
Computational Linguistics. Pages 33-38. 
Michael Heilman and Noah A. Smith. 2010a. Good 
Question! Statistical Ranking for Question Genera-
tion. In Proceedings of NAACL/HLT 2010. pages 
609-617. 
Michael Heilman and Noah A. Smith. 2010b. Rating 
Computer-Generated Questions with Mechanical 
Turk. In Proceedings of NAACL 2010 Workshop on 
Creating Speech and Language Data with Ama-
zon's Mechanical Turk. Los Angeles, CA. pages 35-
40. 
Ayako Hoshino and Hiroshi Nakagawa. 2005. A real-
time multiple-choice question generation for lan-
guage testing - a preliminary study -. In Proceed-
ings of the 2nd Workshop on Building Educational 
Applications Using NLP. Ann Arbor, MI, USA: 
Association for Computational Linguistics. pages 
17-20. 
Panagiotis G. Ipeirotis, Foster Provost, Jing Wang . 
2010. In Proceedings of the ACM SIGKDD Work-
shop on Human Computation (HCOMP?10). 
Klaus Krippendorff. 2004. Content Analysis: An Intro-
duction to Its Methodology. Thousand Oaks, CA: 
Sage. 
Ruslan Mitkov and Le An Ha. 2003. Computer-Aided 
Generation of Multiple-Choice Tests. Proceedings 
of the HLT-NAACL 2003 Workshop on Building 
Educational Applications Using Natural Language 
Processing, pages 17-22. 
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis. 
2006. A computer-aided environment for generat-
ing multiple choice test items. Natural Language 
Engineering, 12(2): 177-194. 
Ani Nenkova, Lucy Vanderwende, and Kathleen 
McKeown. 2006. A Compositional Context Sensi-
tive Multidocument Summarizer. In Proceedings of 
SIGIR 2006. pages 573-580. 
Rodney D. Nielsen. 2008. Question Generation: Pro-
posed Challenge Tasks and Their Evaluation. In V. 
Rus, & A. Graesser (Ed.), In Proceedings of the 
Workshop on the Question Generation Shared Task 
and Evaluation Challenge. Arlington, VA. 
Jorge Nocedal. 1980. Updating Quasi-Newton Matrices 
with Limited Storage. Mathematics of Computa-
tion, 35:773-782. 
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. 
The Proposition Bank: An Annotated Corpus of 
Semantic Roles. Computational Linguistics, vol. 
31, no. 1, pp. 71--106. 
Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami 
Suzuki, Kristina Toutanova, Michael Gamon, Wen-
tau Yih, and Lucy Vanderwende. 2012. MSR 
SPLAT, a language analysis toolkit. In Proceedings 
of NAACL HLT 2012 Demonstration Session. 
http://research.microsoft.com/projects/msrsplat . 
Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean, 
Svetlana Stoyanchev and Cristian Moldovan. 2010. 
Overview of The First Question Generation Shared 
Task Evaluation Challenge. In Proceedings of the 
Third Workshop on Question Generation. Pitts-
burgh, PA, USA. pages 45-57. 
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and 
Andrew Y. Ng. 2008. Cheap and Fast?but is it 
Good? Evaluating non-Expert Annotations for Nat-
ural Language Tasks. In Proceedings of 
EMNLP?08. pages 254-263. 
Lucy Vanderwende. 2008. The Importance of Being 
Important: Question Generation. In Proceedings of 
the 1st Workshop on the Question Generation 
Shared Task Evaluation Challenge, Arlington, VA. 
Janyce M. Wiebe, Rebecca F. Bruce and Thomas P. 
O?Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In 
Proceedings of ACL 1999. 
John H. Wolfe. 1976. Automatic question generation 
from text - an aid to independent study. In Proceed-
ings of the ACM SIGCSE-SIGCUE technical sym-
posium on Computer science and education. New 
York, NY, USA: ACM. pages 104-112. 
 
 
751
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21?24,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
MSR SPLAT, a language analysis toolkit 
 
Chris Quirk, Pallavi Choudhury, Jianfeng 
Gao, Hisami Suzuki, Kristina Toutanova, 
Michael Gamon, Wen-tau Yih, Lucy 
Vanderwende 
Colin Cherry 
Microsoft Research National Research Council Canada 
Redmond, WA 98052 USA 1200 Montreal Road 
 Ottawa, Ontario K1A 0R6 
{chrisq, pallavic, jfgao, 
hisamis, kristout, 
mgamon,scottyih, 
lucyv@microsoft.com} 
colin.cherry@nrccnrc.gc.ca 
 
Abstract 
We describe MSR SPLAT, a toolkit for lan-
guage analysis that allows easy access to the 
linguistic analysis tools produced by the NLP 
group at Microsoft Research. The tools in-
clude both traditional linguistic analysis tools 
such as part-of-speech taggers, constituency 
and dependency parsers, and more recent de-
velopments such as sentiment detection and 
linguistically valid morphology. As we ex-
pand the tools we develop for our own re-
search, the set of tools available in MSR 
SPLAT will be extended. The toolkit is acces-
sible as a web service, which can be used 
from a broad set of programming languages. 
1 Introduction 
The availability of annotated data sets that have 
become community standards, such as the Penn 
TreeBank (Marcus et al, 1993) and PropBank 
(Palmer et al, 2005), has enabled many research 
institutions to build core natural language pro-
cessing components, including part-of-speech tag-
gers, chunkers, and parsers. There remain many 
differences in how these components are built, re-
sulting in slight but noticeable variation in the 
component output. In experimental settings, it has 
proved sometimes difficult to distinguish between 
improvements contributed by a specific component 
feature from improvements due to using a differ-
ently-trained linguistic component, such as tokeni-
zation. The community recognizes this difficulty, 
and shared task organizers are now providing ac-
companying parses and other analyses of the 
shared task data. For instance, the BioNLP shared 
task organizers have provided output from a num-
ber of parsers1, alleviating the need for participat-
ing systems to download and run unfamiliar tools. 
On the other hand, many community members 
provide downloads of NLP tools2 to increase ac-
cessibility and replicability of core components.  
Our toolkit is offered in this same spirit. We 
have created well-tested, efficient linguistic tools 
in the course of our research, using commonly 
available resources such as the PTB and PropBank. 
We also have created some tools that are less 
commonly available in the community, for exam-
ple linguistically valid base forms and semantic 
role analyzers. These components are on par with 
other state of the art systems. 
We hope that sharing these tools will enable 
some researchers to carry out their projects without 
having to re-create or download commonly used 
NLP components, or potentially allow researchers 
to compare our results with those of their own 
tools. The further advantage of designing MSR 
SPLAT as a web service is that we can share new 
components on an on-going basis. 
2 Parsing Functionality 
2.1 Constituency Parsing 
                                                          
1 See www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask  for 
the description of other resources made available in addition to 
the shared task data. 
2 See, for example, http://nlp.stanford.edu/software; 
http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; 
http://incubator.apache.org/opennlp 
21
The syntactic parser in MSR SPLAT attempts to 
reconstruct a parse tree according the Penn Tree-
Bank specification (Marcus et al, 1993). This rep-
resentation captures the notion of labeled syntactic 
constituents using a parenthesized representation. 
For instance, the sentence ?Colorless green ideas 
sleep furiously.? could be assigned the following 
parse tree, written in the form of an S expression: 
(TOP (S 
   (NP (JJ Colorless) (JJ green) (NNS ideas)) 
   (VP (VB sleep) (ADVP (RB furiously))) 
   (. .))) 
For instance, this parse tree indicates that ?Color-
less green ideas? is a noun phrase (NP), and ?sleep 
furiously? is a verb phrase (VP). 
Using the Wall Street Journal portion of the 
Penn TreeBank, we estimate a coarse grammar 
over the given grammar symbols. Next, we per-
form a series of refinements to automatically learn 
fine-grained categories that better capture the im-
plicit correlations in the tree using the split-merge 
method of Petrov et al (2006). Each input symbol 
is split into two new symbols, both with a new 
unique symbol label, and the grammar is updated 
to include a copy of each original rule for each 
such refinement, with a small amount of random 
noise added to the probability of each production 
to break ties. We estimate new grammar parame-
ters using an accelerated form of the EM algorithm 
(Salakhutdinov and Roweis, 2003). Then the low-
est 50% of the split symbols (according to their 
estimated contribution to the likelihood of the data) 
are merged back into their original form and the 
parameters are again re-estimated using AEM. We 
found six split-merge iterations produced optimal 
accuracy on the standard development set. 
The best tree for a given input is selected ac-
cording to the max-rule approach (cf. Petrov et al 
2006). Coarse-to-fine parsing with pruning at each 
level helps increase speed; pruning thresholds are 
picked for each level to have minimal impact on 
development set accuracy. However, the initial 
coarse pass still has runtime cubic in the length of 
the sentence. Thus, we limit the search space of the 
coarse parse by closing selected chart cells before 
the parse begins (Roark and Hollingshead, 2008). 
We train a classifier to determine if constituents 
may start or end at each position in the sentence. 
For instance, constituents seldom end at the word 
?the? or begin at a comma. Closing a number of 
chart cells can substantially improve runtime with 
minimal impact on accuracy. 
2.2 Dependency Parsing 
The dependency parses produced by MSR SPLAT 
are unlabeled, directed arcs indicating the syntactic 
governor of each word. 
These dependency trees are computed from the 
output of the constituency parser. First, the head of 
each non-terminal is computed according to a set 
of rules (Collins, 1999). Then, the tree is flattened 
into maximal projections of heads. Finally, we in-
troduce an arc from a parent word p to a child 
word c if the non-terminal headed by p is a parent 
of the non-terminal headed by c. 
2.3 Semantic Role Labeling 
The Semantic Role Labeling component of MSR 
SPLAT labels the semantic roles of verbs accord-
ing to the PropBank specification (Palmer et al, 
2005). The semantic roles represent a level of 
broad-coverage shallow semantic analysis which 
goes beyond syntax, but does not handle phenome-
na like co-reference and quantification.  
For example, in the two sentences ?John broke 
the window? and ?The window broke?, the phrase 
the window will be marked with a THEME label. 
Note that the syntactic role of the phrase in the two 
sentences is different but the semantic role is the 
same. The actual labeling scheme makes use of 
numbered argument labels, like ARG0, ARG1, ?, 
ARG5 for core arguments, and labels like ARGM-
TMP,ARGM-LOC, etc. for adjunct-like argu-
ments. The meaning of the numbered arguments is 
verb-specific, with ARG0 typically representing an 
agent-like role, and ARG1 a patient-like role. 
This implementation of an SRL system follows 
the approach described in (Xue and Palmer, 04), 
and includes two log-linear models for argument 
identification and classification. A single syntax 
tree generated by the MSR SPLAT split-merge 
parser is used as input. Non-overlapping arguments 
are derived using the dynamic programming algo-
rithm by Toutanova et al (2008).  
3 Other Language Analysis Functionality 
3.1 Sentence Boundary / Tokenization 
22
This analyzer identifies sentence boundaries and 
breaks the input into tokens. Both are represented 
as offsets of character ranges. Each token has both 
a raw form from the string and a normalized form 
in the PTB specification, e.g., open and close pa-
rentheses are replaced by -LRB- and -RRB-, re-
spectively, to remove ambiguity with parentheses 
indicating syntactic structure. A finite state ma-
chine using simple rules and abbreviations detects 
sentence boundaries with high accuracy, and a set 
of regular expressions tokenize the input. 
3.2 Stemming / Lemmatization 
We provide three types of stemming: Porter stem-
ming, inflectional morphology and derivational 
morphology. 
3.2.1 Stems  
The stemmer analyzer indicates a stem form for 
each input token, using the standard Porter stem-
ming algorithm (Porter, 1980). These forms are 
known to be useful in applications such as cluster-
ing, as the algorithm assigns the same form ?dai? 
to ?daily? and ?day?, but as these forms are not 
citation forms of these words, presentation to end 
users is known to be problematic. 
3.2.2 Lemmas 
The lemma analyzer uses inflectional morphology 
to indicate the dictionary lookup form of the word. 
For example, the lemma of ?daily? will be ?daily?, 
while the lemma of ?children? will be ?child?. We 
have mined the lemma form of input tokens using 
a broad-coverage grammar NLPwin (Heidorn, 
2000) over very large corpora. 
3.2.3 Bases  
The base analyzer uses derivational morphology to 
indicate the dictionary lookup form of the word; as 
there can be more than one derivation for a given 
word, the base type returns a list of forms. For ex-
ample, the base form of ?daily? will be ?day?, 
while the base form of ?additional? will be ?addi-
tion? and ?add?. We have generated a static list of 
base forms of tokens using a broad-coverage 
grammar NLPwin (Heidorn, 2000) over very large 
corpora. If the token form has not been observed in 
those corpora, we will not return a base form. 
3.3 POS tagging 
We train a maximum entropy Markov Model on 
part-of-speech tags from the Penn TreeBank. This 
optimized implementation has very high accuracy 
(over 96% on the test set) and yet can tag tens of 
thousands of words per second. 
3.4 Chunking 
The chunker (Gao et al, 2001) is based on a Cas-
caded Markov Model, and is trained on the Penn 
TreeBank. With state-of-the-art chunking accuracy 
as evaluated on the benchmark dataset, the chunker 
is also robust and efficient, and has been used to 
process very large corpora of web documents. 
4 The Flexibility of a Web Service 
By making the MSR SPLAT toolkit available as a 
web service, we can provide access to new tools, 
e.g. sentiment analysis. We are in the process of 
building out the tools to provide language analysis 
for languages other than English. One step in this 
direction is a tool for transliterating between Eng-
lish and Katakana words. Following Cherry and 
Suzuki (2009), the toolkit currently outputs the 10-
best transliteration candidates with probabilities for 
both directions.  
Another included service is the Triples analyz-
er, which returns the head of the subject, the verb, 
and the head of the object, whenever such a triple 
is encountered. We found this functionality to be 
useful as we were exploring features for our sys-
tem submitted to the BioNLP shared task. 
5 Programmatic Access 
5.1 Web service reference 
We have designed a web service that accepts a 
batch of text and applies a series of analysis tools 
to that text, returning a bag of analyses. This main 
web service call, named ?Analyze?, requires four 
parameters: the language of the text (such as ?en? 
for English), the raw text to be analyzed, the set of 
analyzers to apply, and an access key to monitor 
and, if necessary, constrain usage. It returns a list 
of analyses, one from each requested analyzer, in a 
23
simple JSON (JavaScript Object Notation) format 
easy to parse in many programming languages. 
In addition, there is a web service call ?Lan-
guages? that enumerates the list of available lan-
guages, and ?Analyzers? to discover the set of 
analyzers available in a given language.  
5.2 Data Formats 
We use a relatively standard set of data representa-
tions for each component. Parse trees are returned 
as S expressions, part-of-speech tags are returned 
as lists, dependency trees are returned as lists of 
parent indices, and so on. The website contains an 
authoritative description of each analysis format. 
5.3 Speed 
Speed of analysis is heavily dependent on the 
component involved. Analyzers for sentence sepa-
ration, tokenization, and part-of-speech tagging 
process thousands of sentences per second; our 
fastest constituency parser handles tens of sentenc-
es per second. Where possible, the user is encour-
aged to send moderate sized requests (perhaps a 
paragraph at a time) to minimize the impact of 
network latency. 
6 Conclusion 
We hope that others will find the tools that we 
have made available as useful as we have. We en-
courage people to send us their feedback so that we 
can improve our tools and increase collaboration in 
the community. 
7 Script Outline 
The interactive UI (Figure 1) allows an arbitrary 
sentence to be entered and the desired levels of 
analysis to be selected as output. As there exist 
other such toolkits, the demonstration is primarily 
aimed at allowing participants to assess the quality, 
utility and speed of the MSR SPLAT tools. 
http://research.microsoft.com/en-us/projects/msrsplat/ 
References  
Colin Cherry and Hisami Suzuki. 2009. Discriminative sub-
string decoding for transliteration. In Proceedings of 
EMNLP. 
Michael Collins. 1999. Head-driven statistical models for 
natural language parsing. PhD Dissertation, University of 
Pennsylvania. 
Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun, Ming 
Zhou and Chang-Ning Huang. 2001. Improving query 
translation for CLIR using statistical Models. In Proceed-
ings of SIGIR. 
George Heidorn. 2000. Intelligent writing assistance. In R. 
Dale, H. Moisl and H. Somers (eds.), A Handbook of Natu-
ral Language Processing: Techniques and Applications for 
the Processing of Text. New York: Marcel Dekker. 
Mitchell Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a Large Annotated Corpus 
of English: The Penn Treebank. Computational Linguistics 
19(2): 313-330. 
Martha Palmer, Dan Gildea, Paul Kingsbury. 2005. The Prop-
osition Bank: An Annotated Corpus of Semantic Roles. 
Computational Linguistics, 31(1): 71-105 
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3): 130-137. 
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 
2006. Learning Accurate, Compact, and Interpretable Tree 
Annotation. In Proceedings of ACL. 
Brian Roark and Kristy Hollingshead. 2008. Classifying chart 
cells for quadratic complexity context-free inference. In 
Proceedings of COLING. 
Ruslan Salakhutdinov and Sam Roweis. 2003. Adaptive Over-
relaxed Bound Optimization Methods. In Proceedings of 
ICML. 
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2008. A global joint model for semantic role labeling, 
Computational Linguistics, 34(2): 161-191. 
Nianwen Xue and Martha Palmer. 2004. Calibrating Features 
for Semantic Role Labeling. In Proceedings of EMNLP. 
Munmun de Choudhury, Scott Counts, Michael Gamon. Not 
All Moods are Created Equal! Exploring Human Emotional 
States in Social Media. Accepted for presentation in 
ICWSM 2012 
Munmun de Choudhury, Scott Counts, Michael Gamon. Hap-
py, Nervous, Surprised? Classification of Human Affective 
States in Social Media. Accepted for presentation (short 
paper) in ICWSM 2012 
 
 
Figure 1. Screenshot of the MSR SPLAT interactive UI 
showing selected functionalities which can be toggled 
on and off. This is the interface that we propose to 
demo at NAACL. 
 
 
24
Proceedings of NAACL-HLT 2013, pages 837?846,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Probabilistic Frame Induction
Jackie Chi Kit Cheung?
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
jcheung@cs.toronto.edu
Hoifung Poon
One Microsoft Way
Microsoft Research
Redmond, WA 98052, USA
hoifung@microsoft.com
Lucy Vanderwende
One Microsoft Way
Microsoft Research
Redmond, WA 98052, USA
lucyv@microsoft.com
Abstract
In natural-language discourse, related events
tend to appear near each other to describe a
larger scenario. Such structures can be formal-
ized by the notion of a frame (a.k.a. template),
which comprises a set of related events and
prototypical participants and event transitions.
Identifying frames is a prerequisite for infor-
mation extraction and natural language gen-
eration, and is usually done manually. Meth-
ods for inducing frames have been proposed
recently, but they typically use ad hoc proce-
dures and are difficult to diagnose or extend.
In this paper, we propose the first probabilistic
approach to frame induction, which incorpo-
rates frames, events, and participants as latent
topics and learns those frame and event transi-
tions that best explain the text. The number
of frame components is inferred by a novel
application of a split-merge method from syn-
tactic parsing. In end-to-end evaluations from
text to induced frames and extracted facts, our
method produces state-of-the-art results while
substantially reducing engineering effort.
1 Introduction
Events with causal or temporal relations tend to oc-
cur near each other in text. For example, a BOMB-
ING scenario in an article on terrorism might be-
gin with a DETONATION event, in which terrorists
set off a bomb. Then, a DAMAGE event might en-
sue to describe the resulting destruction and any
casualties, followed by an INVESTIGATION event
?This research was undertaken during the author?s internship
at Microsoft Research.
covering subsequent police investigations. After-
wards, the BOMBING scenario may transition into
a CRIMINAL-PROCESSING scenario, which begins
with police catching the terrorists, and proceeds to
a trial, sentencing, etc. A common set of partici-
pants serves as the event arguments; e.g., the agent
(or subject) of DETONATION is often the same as
the theme (or object) of INVESTIGATION and corre-
sponds to a PERPETRATOR.
Such structures can be formally captured by the
notion of a frame (a.k.a. template, scenario), which
consists of a set of events with prototypical transi-
tions, as well as a set of slots representing the com-
mon participants. Identifying frames is an explicit
or implicit prerequisite for many NLP tasks. Infor-
mation extraction, for example, stipulates the types
of events and slots that are extracted for a frame or
template. Online applications such as dialogue sys-
tems and personal-assistant applications also model
users? goals and subgoals using frame-like represen-
tations. In natural-language generation, frames are
often used to represent contents to be expressed as
well as to support surface realization.
Until recently, frames and related representations
have been manually constructed, which has limited
their applicability to a relatively small number of do-
mains and a few slots within a domain. Furthermore,
additional manual effort is needed after the frames
are defined in order to extract frame components
from text (e.g., in annotating examples and design-
ing features to train a supervised learning model).
This paradigm makes generalizing across tasks dif-
ficult, and might suffer from annotator bias.
Recently, there has been increasing interest in au-
837
tomatically inducing frames from text. A notable
example is Chambers and Jurafsky (2011), which
first clusters related verbs to form frames, and then
clusters the verbs? syntactic arguments to identify
slots. While Chambers and Jurafsky (2011) repre-
sents a major step forward in frame induction, it is
also limited in several aspects. The clustering used
ad hoc steps and customized similarity metrics, as
well as an additional retrieval step from a large ex-
ternal text corpus for slot generation. This makes it
hard to replicate their approach or adapt it to new
domains. Lacking a coherent model, it is also diffi-
cult to incorporate additional linguistic insights and
prior knowledge.
In this paper, we present PROFINDER (PROba-
bilistic Frame INDucER), the first probabilistic ap-
proach to frame induction. PROFINDER defines
a joint distribution over the words in a document
and their frame assignments by modeling frame
and event transitions, correlations among events and
slots, and their surface realizations. Given a set of
documents, PROFINDER outputs a set of induced
frames with learned parameters, as well as the most
probable frame assignments that can be used for
event and entity extraction. The numbers of events
and slots are dynamically determined by a novel
application of the split-merge approach from syn-
tactic parsing (Petrov et al, 2006). In end-to-end
evaluations from text to entity extraction using stan-
dard MUC and TAC datasets, PROFINDER achieved
state-of-the-art results while significantly reducing
engineering effort and requiring no external data.
2 Related Work
In information extraction and other semantic pro-
cessing tasks, the dominant paradigm requires two
stages of manual effort. First, the target representa-
tion is defined manually by domain experts. Then,
manual effort is required to construct an extractor
or to annotate examples to train a machine-learning
system. Recently, there has been a burgeoning body
of work in reducing such manual effort. For exam-
ple, a popular approach to reduce annotation effort is
bootstrapping from seed examples (Patwardhan and
Riloff, 2007; Huang and Riloff, 2012). However,
this still requires prespecified frames or templates,
and selecting seed words is often a challenging task
(Curran et al, 2007). Filatova et al (2006) construct
simple domain templates by mining verbs and the
named entity type of verbal arguments that are topi-
cal, whereas Shinyama and Sekine (2006) identify
query-focused slots by clustering common named
entities and their syntactic contexts. Open IE (Banko
and Etzioni, 2008) limits the manual effort to de-
signing a few domain-independent relation patterns,
which can then be applied to extract relational triples
from text. While extremely scalable, this approach
can only extract atomic factoids within a sentence,
and the resulting triples are noisy, non-canonicalized
text fragments.
More relevant to our approach is the recent work
in unsupervised semantic induction, such as un-
supervised semantic parsing (Poon and Domingos,
2009), unsupervised semantical role labeling (Swier
and Stevenson, 2004) and induction (Lang and Lap-
ata, 2011, e.g.), and slot induction from web search
logs (Cheung and Li, 2012). As in PROFINDER,
they model distributional contexts for slots and
roles. However, these approaches focus on the se-
mantics of independent sentences or queries, and do
not capture discourse-level dependencies.
The modeling of frame and event transitions in
PROFINDER is similar to a sequential topic model
(Gruber et al, 2007), and is inspired by the suc-
cessful applications of such topic models in sum-
marization (Barzilay and Lee, 2004; Daume? III and
Marcu, 2006; Haghighi and Vanderwende, 2009, in-
ter alia). There are, however, two main differences.
First, PROFINDER contains not a single sequential
topic model, but two (for frames and events, respec-
tively). In addition, it also models the interdepen-
dencies among events, slots, and surface text, which
is analogous to the USP model (Poon and Domin-
gos, 2009). PROFINDER can thus be viewed as a
novel combination of state-of-the-art models in un-
supervised semantics and discourse modeling.
In terms of aim and capability, PROFINDER is
most similar to Chambers and Jurafsky (2011),
which culminated from a series of work for iden-
tifying correlated events and arguments in narratives
(Chambers and Jurafsky, 2008; Chambers and Ju-
rafsky, 2009). By adopting a probabilistic approach,
PROFINDER has a sound theoretical underpinning,
and is easy to modify or extend. For example, in
Section 3, we show how PROFINDER can easily be
838
augmented with additional linguistically-motivated
features. Likewise, PROFINDER can easily be used
as a semi-supervised system if some slot designa-
tions and labeled examples are available.
The idea of representing and capturing stereotyp-
ical knowledge has a long history in artificial in-
telligence and psychology, and has assumed vari-
ous names such as frames (Minsky, 1974), schemata
(Rumelhart, 1975), and scripts (Schank and Abel-
son, 1977). In the linguistics and computational
linguistics communities, frame semantics (Fillmore,
1982) uses frames as the central representation of
word meaning, culminating in the development of
FrameNet (Baker et al, 1998), which contains over
1000 manually annotated frames. A similarly rich
lexical resource is the MindNet project (Richard-
son et al, 1998). Our notion of frame is related to
these representations, but there are also subtle differ-
ences. For example, Minsky?s frame emphasizes in-
heritance, which we do not model in this paper1. As
in semantic role labeling, FrameNet focuses on se-
mantic roles and does not model event or frame tran-
sitions, so the scope of its frames is often no more
than an event in our model. Perhaps the most sim-
ilar to our frame is Roger Schank?s scripts, which
capture prototypical events and participants in a sce-
nario such as restaurant dining. In their approach,
however, scripts are manually defined, making it
hard to generalize. In this regard, our work may be
viewed as an attempt to revive a long tradition in AI
and linguistics, by leveraging the recent advances in
computational power, NLP, and machine learning.
3 Probabilistic Frame Induction
In this section, we present PROFINDER, a proba-
bilistic model for frame induction. Let F be a set of
frames, where each frame F = (EF , SF ) comprises
a unique set of events EF and slots SF . Given a
document D and a word w in D, Zw = (f, e) repre-
sents an assignment of w to frame f ? F and frame
element e ? Ef ? Sf . At the heart of PROFINDER
is a generative model P?(D,Z) that defines a joint
distribution over document D and the frame assign-
ment to its words Z. Given a set of documents D,
1This should be a straightforward extension ? using the
split-and-merge approach, PROFINDER already produces a hi-
erarchy of events and slots in learning, although currently it
makes no use of the intermediate levels.
frame induction in PROFINDER amounts to deter-
mining the number of events and slots in each frame,
as well as learning the parameters ? by summing out
the latent assignments Z to maximize the likelihood
of the document set
?
D?D
P?(D).
The induced frames identify the key event structures
in the document set. Additionally, PROFINDER can
conduct event and entity extraction by computing
the most probable frame assignment Z. In the re-
mainder of the section, we first present the base
model for PROFINDER. We then introduce sev-
eral linguistically motivated refinements, as well as
efficient algorithms for learning and inference in
PROFINDER.
3.1 Base Model
The probabilistic formulation of PROFINDER makes
it extremely flexible for incorporating linguistic in-
tuition and prior knowledge. In this paper, we design
our PROFINDER model to capture three types of de-
pendencies.
Frame transitions between clauses A sentence
contains one or more clauses, each of which is a
minimal unit expressing a proposition. A clause is
unlikely to straddle different frames, so we stipu-
late that the words in a clause be assigned to the
same frame. On the other hand, frame transitions
can happen between clauses, and we adopt the com-
mon Markov assumption that the frame of a clause
only depends on the previous clause in the docu-
ment. Clauses are automatically extracted from the
dependency parse and further decomposed into an
event head and its syntactic arguments.
Event transitions within a frame Events tend to
transition into related events in the same frame, as
determined by their causal or temporal relations.
Each clause is assigned an event compatible with
its frame assignment (i.e., the event is in the given
frame). Like frame transitions, we assume that the
event assignment of a clause depends only on the
event of the previous clause.
Emission of event heads and slot words Simi-
lar to topics in topic models, each event determines
839
a multinomial from which the event head is gener-
ated; e.g., a DETONATION event might use verbs
such as detonate, set off or nouns such as denota-
tion, bombing as its event head. Additionally, as
in USP (Poon and Domingos, 2009), an event also
contains a multinomial of slots for each of its argu-
ment types2; e.g., the agent argument of a DETONA-
TION event is generally the PERPETRATOR slot of
the BOMBING frame. Finally, each slot has its own
multinomials for generating the argument head and
dependency label, regardless of the event.
Formally, let D be a document and C1, ? ? ? , Cl be
its clauses, the PROFINDER model is defined by
P?(D,Z) = PF?INIT(F1)?
?
i
PF?TRAN(Fi+1|Fi)
? PE?INIT(E1|F1)
?
?
i
PE?TRAN(Ei+1|Ei, Fi+1, Fi)
?
?
i
PE?HEAD(ei|Ei)
?
?
i,j
PSLOT(Si,j |Ei,j , Ai,j)
?
?
i,j
PA?HEAD(ai,j |Si,j)
?
?
i,j
PA?DEP(depi,j |Si,j)
Here, Fi, Ei denote the frame and event assign-
ment to clause Ci, respectively, and ei denotes the
event head. For the j-th argument of clause i,
Si,j denotes the slot assignment, Ai,j the argument
type, ai,j the head word, and depi,j the dependency
from the event head. PE?TRAN(Ei+1|Ei, Fi+1, Fi) =
PE?INIT(Ei+1|Fi+1) if Fi+1 6= Fi.
Essentially, PROFINDER combines a frame HMM
with an event HMM, where the first models frame
transition and emits events, and the second models
event transition within a frame and emits argument
slots.
3.2 Model refinements
The base model captures the main dependencies in
event narrative, but it can be easily extended to lever-
2USP generates the argument types along with events from
clustering. For simplicity, in PROFINDER we simply classify
a syntactic argument into subject, object, and prepositional ob-
ject, according to its Stanford dependency to the event head.
age additional linguistic intuition. PROFINDER in-
corporates three such refinements.
Background frame Event narratives often con-
tain interjections of general content common to all
frames. For example, in newswire articles, ATTRI-
BUTION is commonplace to describe who said or
reported a particular quote or fact. To avoid con-
taminating frames with generic content, we intro-
duce a background frame with its own events, slots,
and emission distributions, and a binary switch vari-
able Bi ? {BKG,CNT} that determines whether
clause i is generated from the actual content frame
Fi (CNT ) or background (BKG). We also stipu-
late that if BKG is chosen, the nominal frame stays
the same as the previous clause.
Stickiness in frame and event transitions Prior
work has demonstrated that promoting topic coher-
ence in natural-language discourse helps discourse
modeling (Barzilay and Lee, 2004). We extend
PROFINDER to leverage this intuition by incorporat-
ing a ?stickiness? prior (Haghighi and Vanderwende,
2009) to encourage neighboring clauses to stay in
the same frame. Specifically, along with introducing
the background frame, the frame transition compo-
nent now becomes
PF?TRAN(Fi+1|Fi, Bi+1) = (1)
?
??
??
1(Fi+1 = Fi), if Bi+1 = BKG
?1(Fi+1 = Fi)+
(1? ?)PF?TRAN(Fi+1|Fi),
if Bi+1 = CNT
where ? is the stickiness parameter, and the event
transition component correspondingly becomes
PE?TRAN(Ei+1|Ei, Fi+1, Fi, Bi+1) = (2)
?
??
??
1(Ei+1 = Ei), if Bi+1 = BKG
PE?TRAN(Ei+1|Ei), if Bi+1 = CNT,Fi = Fi+1
PE?INIT(Ei+1), if Bi+1 = CNT,Fi 6= Fi+1
Argument dependencies as caseframes As no-
ticed in previous work such as Chambers and Juraf-
sky (2011), the combination of an event head and a
dependency relation often gives a strong signal of
the slot that is indicated. For example, bomb >
nsubj (subject argument of bomb) often indicates
a PERPETRATOR. Thus, rather than simply emitting
840
Frame 
Event 
Background 
Event 
head 
?1 
?1 
?1 
?1 
?1 ?1 
???1 ?1 
? ?? 
?? 
?? 
?? 
?? ?? 
???? ?? 
. . .  
. . .  
|?| |?| |?| 
???????  
Arguments 
???????  ??????  
Figure 1: Graphical representation of our model. Hyper-
parameters, the stickiness factor, and the frame and event
initial and transition distributions are not shown for clar-
ity.
the dependency from the event head to an event ar-
gument depi,j , our model instead emits the pair of
event head and dependency relation, which we call
a caseframe following Bean and Riloff (2004).
3.3 Full generative story
To summarize, the distributions that are learned by
our model are the default distributions PBKG(B),
PF?INIT(F ), PE?INIT(E); the transition distri-
butions PF?TRAN(Fi+1|Fi), PE?TRAN(Ei+1|Ei);
and the emission distributions PSLOT(S|E,A,B),
PE?HEAD(e|E,B), PA?HEAD(a|S), PA?DEP(dep|S).
We used additive smoothing with uniform Dirich-
let priors for all the multinomials. The overall
generative story of our model is as follows:
1. Draw a Bernoulli distribution for PBKG(B)
2. Draw the frame, event, and slot distributions
3. Draw an event head emission distribution
PE?HEAD(e|E,B) for each frame including the
background frame
4. Draw event argument lemma and caseframe
emission distributions for each slot in each
frame including the background frame
5. For each clause in each document, generate the
clause-internal structure.
The clause-internal structure at clause i is gener-
ated by the following steps:
1. Generate whether this clause is background
(Bi ? {CNT,BKG} ? PBKG(B))
2. Generate the frame Fi and event Ei from
PF?INIT(F ), PE?INIT(E), or according to
equations 1 and 2
3. Generate the observed event head ei from
PE?HEAD(ei|Ei).
4. For each event argument:
(a) Generate the slot Si,j from
PSLOT(S|E,A,B).
(b) Generate the dependency/caseframe emis-
sion depi,j ? PA?DEP(dep|S) and the
lemma of the head word of the event ar-
gument ai,j ? PA?HEAD(a|S).
3.4 Learning and Inference
Our generative model admits efficient inference by
dynamic programming. In particular, after collaps-
ing the latent assignment of frame, event, and back-
ground into a single hidden variable for each clause,
the expectation and most probable assignment can
be computed using standard forward-backward and
Viterbi algorithms on fixed tree structures.
Parameter learning can be done using EM by al-
ternating the computation of expected counts and the
maximization of multinomial parameters. In par-
ticular, PROFINDER uses incremental EM, which
has been shown to have better and faster con-
vergence properties than standard EM (Liang and
Klein, 2009).
Determining the optimal number of events and
slots is challenging. One solution is to adopt a non-
parametric Bayesian method by incorporating a hi-
erarchical prior over the parameters (e.g., a Dirich-
let process). However, this approach can impose
unrealistic restrictions on the model choice and re-
sult in intractability which requires sampling or ap-
proximate inference to overcome. Additionally, EM
learning can suffer from local optima due to its non-
convex learning objective, especially when dealing
with a large number hidden states without a good
initialization.
To address these issues, we adopt a novel appli-
cation of the split-merge method previously used in
syntactic parsing for inferring refined latent syntac-
tic categories (Petrov et al, 2006). First, the model
is initialized with a number of frames, which is a
hyperparameter, and each frame is associated with
841
one event and two slots. Starting from this mini-
mal structure, EM training begins. After a number
of iterations, each event and slot state is ?split? in
two; that is, each original state now becomes two
new states. Each of the new states is generated with
half of the probability of the original, and contains
a duplicate of the associated emission distributions.
Some perturbation is then added to the probabilities
to break symmetry. After splitting, we merge back
a portion of the newly split events and slots that re-
sult in the least improvement in the likelihood of the
training data. For more details on split-merge, see
Petrov et al (2006)
By adjusting the number of split-merge cycles and
the merge parameters, our model learns the number
of events and slots in a dynamical fashion that is tai-
lored to the data. Moreover, our model starts with a
small number of frame elements, which reduces the
number of local optima and facilitates initial learn-
ing. After each split, the subsequent learning starts
with (a perturbed version of) the previously learned
parameters, which makes a good initialization that
is crucial for EM. Finally, it is also compatible with
the hierarchical nature of events and slots. For ex-
ample, slots can first be coarsely split into persons
versus locations, and later refined into subcategories
such as perpetrators and victims.
4 MUC-4 Entity Extraction Experiments
We first evaluate our model on a standard entity
extraction task, using the evaluation settings from
Chambers and Jurafsky (2011) (henceforth, C&J)
to enable a head-to-head comparison. Specifically,
we use the MUC-4 data set (1992) , which contains
1300 training and development documents on ter-
rorism in South America, with 200 additional doc-
uments for testing. MUC-4 contains four templates:
ATTACK, KIDNAPPING, BOMBING, and ARSON.3
All templates share the same set of predefined slots,
with the evaluation focusing on the following four:
PERPETRATOR, PHYSICAL TARGET, HUMAN TAR-
GET, and INSTRUMENT.
For each slot in a MUC template, the system
first identifies an induced slot that best maps to it
by F1 on the development set. As in C&J, tem-
3Two other templates have negligible counts and are ignored
as in C&J.
plate is ignored in final evaluation, so all the clusters
that belong to the same slot are then merged across
the templates; e.g., the PERPETRATOR clusters for
KIDNAPPING and BOMBING are merged. The fi-
nal precision, recall, and F1 are computed based on
these merged clusters. Correctness is determined by
matching head words, and slots marked as optional
in MUC are ignored when computing recall. All hy-
perparameters are tuned on the development set (see
Appendix A for their values).
Named entity type Named entity type is a useful
feature to filter out entities for particular slots; e.g. a
location cannot be an INSTRUMENT. We thus divide
each induced cluster into four clusters by named
entity type before performing the mapping, follow-
ing C&J?s heuristic and using a named entity recog-
nizer and word lists derived from WordNet: PER-
SON/ORGANIZATION, PHYSICAL OBJECT, LOCA-
TION, and OTHER.
Document classification The MUC-4 dataset
contains many documents that have words related
to MUC slots (e.g., plane and aviation), but are not
about terrorism. To reduce precision errors, C&J
first filtered irrelevant documents based on the speci-
ficity of event heads to learned frames. To estimate
the specificity, they used additional data retrieved
from a large external corpus. In PROFINDER, how-
ever, specificity can be easily estimated using the
probability distributions learned during training. In
particular, we define the probability of an event head
in a frame j as:
PF (w) =
?
EF?F
PE?HEAD(w|E)/|F |, (3)
and the probability of a frame given an event head
as:
P (F |w) = PF (w)/
?
F ??F
PF ?(w). (4)
We then follow the rest of C&J?s procedure to
score each learned frame with each MUC document.
Specifically, a document is mapped to a frame if the
average PF (w) in the document is above a threshold
and the document contains at least one trigger word
w? with P (F |w?) > 0.2. The threshold and the in-
duced frame were determined on the development
set, and were used to filter irrelevant documents in
the test set.
842
Unsupervised methods P R F1
PROFINDER (This work) 32 37 34
Chambers and Jurafsky (2011) 48 25 33
With additional information
PROFINDER +doc. classification 41 44 43
C&J 2011 +granularity 44 36 40
Table 1: Results on MUC-4 entity extraction. C&J 2011
+granularity refers to their experiment in which they
mapped one of their templates to five learned clusters
rather than one.
Results Compared to C&J, PROFINDER is con-
ceptually much simpler, using a single probabilis-
tic model and standard learning and inference algo-
rithms, and not requiring multiple processing steps
or customized similarity metrics. It only used the
data in MUC-4, whereas C&J required additional
text to be retrieved from a large external corpus (Gi-
gaword (Graff et al, 2005)) for each event cluster.
It currently does not make use of coreference infor-
mation, whereas C&J did. Remarkably, despite all
these, PROFINDER was still able to outperform C&J
on entity extraction, as shown in Table 1. We also
evaluated PROFINDER?s performance assuming per-
fect document classification (+doc. classification).
This led to a substantially higher precision, suggest-
ing that further improvement is possible from better
document classification.
Figure 2 shows part of a frame learned by
PROFINDER, which includes some slots and events
annotated in MUC. PROFINDER is also able to iden-
tify events and slots not annotated in MUC, a de-
sirable characteristic of unsupervised methods. For
example, it found a DISCUSSION event, an AR-
REST event (call, arrest, express, meet, charge), a
PEACE AGREEMENT slot (agreement, rights, law,
proposal), and an AUTHORITIES slot (police, gov-
ernment, force, command). The background frame
was able to capture many verbs related to attribu-
tion, such as say, continue, add, believe, although it
missed report.
5 Evaluating Frame Induction Using
Guided Summarization Templates
The MUC-4 dataset was originally designed for
information extraction and focuses on a limited
number of template and slot types. To evalu-
Event: Attack Event: Discussion
report, participate, kid-
nap, kill, release
hold, meeting, talk, dis-
cuss, investigate
Slot: Perpetrator Slot: Victim
PERSON/ORG PERSON/ORG
Words: guerrilla, po-
lice, source, person,
group
Words: people, priest,
leader, member, judge
Caseframes:
report>nsubj,
kidnap>nsubj,
kill>nsubj,
participate>nsubj,
release>nsubj
Caseframes:
kill>dobj,
murder>dobj,
release>dobj,
report>dobj,
kidnap>dobj
Figure 2: A partial frame learned by PROFINDER from
the MUC-4 data set, with the most probable emissions for
each event and slot. Labels are assigned by the authors
for readability.
ate PROFINDER?s capabilities in generalizing to
a greater variety of text, we designed and con-
ducted a novel evaluation based on the TAC guided-
summarization dataset. This evaluation was inspired
by the connection between summarization and infor-
mation extraction (White et al, 2001), and reflects a
conceptualization of summarization as inducing and
extracting structured information from source text.
Essentially, we adapted the TAC summarization an-
notation to create gold-standard slots, and used them
to evaluate entity extraction as in MUC-4.
Dataset We used the TAC 2010 guided-
summarization dataset in our experiments
(Owczarzak and Dang, 2010). This data set con-
sists of text from five domains (termed categories
in TAC), each with a template defined by TAC
organizers. In total, there are 46 document clusters
(termed topics in TAC), each of which contains 20
documents and has eight human-written summaries.
Each summary was manually segmented using
the Pyramid method (Nenkova and Passonneau,
2004) and each segment was annotated with a slot
(termed aspect in TAC) from the corresponding
template. Figure 3 shows an example and the full
set of templates is available at http://www.
nist.gov/tac/2010/Summarization/
Guided-Summ.2010.guidelines.html. In
843
(a) Accidents and Natural Disasters:
WHAT: what happened
WHEN: date, time, other temporal markers
WHERE: physical location
WHY: reasons for accident/disaster
WHO AFFECTED: casualties...
DAMAGES: ... caused by the disaster
COUNTERMEASURES: rescue efforts...
(b) (WHEN During the night of July 17,)
(WHAT a 23-foot <WHAT tsunami) hit the
north coast of Papua New Guinea (PNG)>,
(WHY triggered by a 7.0 undersea earth-
quake in the area).
(c) WHEN: night WHAT: tsunami, coast
WHY: earthquake
Figure 3: (a) A frame from the TAC Guided Summariza-
tion task with abbreviated slot descriptions. (b) A TAC
text span, segmented into several contributors with slot
labels. Note that the two WHAT contributors overlap, and
are demarcated by different bracket types. (c) The entities
that are extracted for evaluation.
TAC, each annotated segment (Figure 3b) is called
a contributor.
Evaluation Method We converted the contribu-
tors into a form that is more similar to the previ-
ous MUC evaluation, so that we can fairly compare
against previous work such as C&J that were de-
signed to extract information into that form. Specif-
ically, we extracted the head lemma from all the
maximal noun phrases found in the contributor (Fig-
ure 3c) and treated them as gold-standard entity slots
to extract. While this conversion may not be ideal in
some cases, it simplifies the TAC slots and enables
automatic evaluation. We leave the refinement of
this conversion to future work, and believe it could
be done by crowdsourcing.
For each TAC slot in a TAC category, we extract
entities from the summaries that belong to the given
TAC category. A system-induced entity is consid-
ered a match to a TAC-derived entity from the same
document if the head lemma in the former matches
one in the latter. Based on this matching criterion,
the system-induced slots are mapped to the TAC
slots in a way that achieves the best F1 for each
TAC slot. We allow a system slot to map to mul-
tiple TAC slots, due to potential overlaps in entities
1-best 5-best
Systems P R F1 P R F1
PROFINDER 24 25 24 21 38 27
C&J 58 6.1 11 50 12 20
Table 2: Results on TAC 2010 entity extraction with N -
best mapping for N = 1 and N = 5. Intermediate values
of N produce intermediate results, and are not shown for
brevity.
among TAC slots. For example, in a document about
a tsunami, earthquake may appear both in the WHAT
slot as a disaster itself, and in the CAUSE slot as a
cause for the tsunami.
One salient difference between TAC and MUC
slots is that TAC slots are often more general than
MUC slots. For example, TAC slots such as WHY
and COUNTERMEASURES likely correspond to mul-
tiple slots at the granularity of MUC. As a result, we
also consider mapping the N -best system-induced
slots to each TAC slot, for N up to 5.
Experiments We trained PROFINDER and a reim-
plementation of C&J on the 920 full source texts of
TAC 2010, and tested them on the 368 model sum-
maries. We did not provide C&J?s model with access
to external data, in order to enable fair comparison
with our model. Since all of the summary sentences
are expected to be relevant, we did not conduct doc-
ument or sentence relevance classification in C&J or
PROFINDER. We tuned all parameters by two-fold
cross validation on the summaries. We computed the
overall precision, recall, and F1 by taking a micro-
average over the results for each TAC slot.
Results The results are shown in Table 2.
PROFINDER substantially outperformed C&J in F1,
in both 1-best and N -best cases. As in MUC-4, the
precision of C&J is higher, partly because C&J often
did not do much in clustering and produced many
small clusters. For example, in the 1-best setting, the
average number of entities mapped to each TAC slot
by C&J is 21, whereas it is 208 for PROFINDER. For
both systems, the results are generally lower com-
pared to that in MUC-4, which is expected since this
task is harder given the greater diversity in frames
and slots to be induced.
844
6 Conclusion
We have presented PROFINDER, the first probabilis-
tic approach to frame induction and shown that it
achieves state-of-the-art results on end-to-end entity
extraction in standard MUC and TAC data sets. Our
model is inspired by recent advances in unsuper-
vised semantic induction and content modeling in
summarization. Our probabilistic approach makes
it easy to extend the model with additional linguistic
insights and prior knowledge. While we have made
a case for unsupervised methods and the importance
of robustness across domains, our method is also
amenable to semi-supervised or supervised learn-
ing if annotated data is available. In future work,
we would like to further investigate frame induction
evaluation, particularly in evaluating event cluster-
ing.
Acknowledgments
We would like to thank Nate Chambers for answer-
ing questions about his system. We would also like
to thank Chris Quirk for help with preprocessing the
MUC corpus, and the members of the NLP group at
Microsoft Research for useful discussions.
Appendix A. Hyperparameter Settings
We document below the hyperparameter settings for
PROFINDER that were used to generate the results
in the paper.
Hyperparameter MUC TAC
Number of frames, |F| 9 8
Frame stickiness, ? 0.125 0.5
Smoothing (frames, events, slots) 0.5 2
Smoothing (emissions) 0.05 0.2
Number of split-merge cycles 4 2
Iterations per cycle 10 10
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 17th International Conference on Compu-
tational linguistics.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. Pro-
ceedings of ACL-08: HLT, pages 28?36.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics: HLT-NAACL 2004.
David Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference reso-
lution. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics: HLT-
NAACL 2004.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT, pages 789?797, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP. Association for Computational Lin-
guistics.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 976?986, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Jackie C. K. Cheung and Xiao Li. 2012. Sequence clus-
tering and labeling for unsupervised query intent dis-
covery. In Proceedings of the 5th ACM International
Conference on Web Search and Data Mining, pages
383?392.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics.
Hal Daume? III and Daniel Marcu. 2006. Bayesian
Query-Focused summarization. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 305?312, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Elena Filatova, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic creation of do-
main templates. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 207?
214, Sydney, Australia, July. Association for Compu-
tational Linguistics.
845
Charles J. Fillmore. 1982. Frame semantics. Linguistics
in the Morning Calm, pages 111?137.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2005. English gigaword second edition. Linguistic
Data Consortium, Philadelphia.
Amit Gruber, Michael Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic markov models. Artificial Intelligence
and Statistics (AISTATS).
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362?370, Boulder, Colorado, June. Association
for Computational Linguistics.
Ruihong Huang and Ellen Riloff. 2012. Bootstrapped
training of event extraction classifiers. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 286?295, Avignon, France, April. Association
for Computational Linguistics.
Joel Lang and Mirella Lapata. 2011. Unsupervised se-
mantic role induction via split-merge clustering. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1117?1126, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Percy Liang and Dan Klein. 2009. Online EM for un-
supervised models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 611?619, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
Marvin Minsky. 1974. A framework for representing
knowledge. Technical report, Cambridge, MA, USA.
1992. Proceedings of the Fourth Message Understanding
Conference (MUC-4). Morgan Kaufmann.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics: HLT-
NAACL 2004, volume 2004, pages 145?152.
Karolina Owczarzak and Hoa T. Dang. 2010. TAC 2010
guided summarization task guidelines.
Siddharth Patwardhan and Ellen Riloff. 2007. Effec-
tive information extraction with semantic affinity pat-
terns and relevant regions. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 717?
727, Prague, Czech Republic, June. Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10.
Stephen D. Richardson, William B. Dolan, and Lucy Van-
derwende. 1998. MindNet: Acquiring and structuring
semantic information from text. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics, Volume 2, pages 1098?
1102, Montreal, Quebec, Canada, August. Association
for Computational Linguistics.
David Rumelhart, 1975. Notes on a schema for stories,
pages 211?236. Academic Press, Inc.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals, and Understanding: An Inquiry Into Hu-
man Knowledge Structures. Lawrence Erlbaum, July.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference,
New York City, USA, June. Association for Computa-
tional Linguistics.
Robert S. Swier and Suzanne Stevenson. 2004. Un-
supervised semantic role labelling. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 95?102, Barcelona, Spain, July. Association for
Computational Linguistics.
Michael White, Tanya Korelsky, Claire Cardie, Vincent
Ng, David Pierce, and Kiri Wagstaff. 2001. Multidoc-
ument summarization via information extraction. In
Proceedings of the First International Conference on
Human Language Technology Research. Association
for Computational Linguistics.
846
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 110?120,
Dublin, Ireland, August 23-24 2014.
See No Evil, Say No Evil:
Description Generation from Densely Labeled Images
Mark Yatskar
1
?
my89@cs.washington.edu
1
Computer Science & Engineering
University of Washington
Seattle, WA, 98195, USA
Michel Galley
2
mgalley@microsoft.com
Lucy Vanderwende
2
lucyv@microsoft.com
Luke Zettlemoyer
1
lsz@cs.washington.edu
2
Microsoft Research
One Microsoft Way
Redmond, WA, 98052, USA
Abstract
This paper studies generation of descrip-
tive sentences from densely annotated im-
ages. Previous work studied generation
from automatically detected visual infor-
mation but produced a limited class of sen-
tences, hindered by currently unreliable
recognition of activities and attributes. In-
stead, we collect human annotations of ob-
jects, parts, attributes and activities in im-
ages. These annotations allow us to build
a significantly more comprehensive model
of language generation and allow us to
study what visual information is required
to generate human-like descriptions. Ex-
periments demonstrate high quality output
and that activity annotations and relative
spatial location of objects contribute most
to producing high quality sentences.
1 Introduction
Image descriptions compactly summarize com-
plex visual scenes. For example, consider the de-
scriptions of the image in Figure 1, which vary in
content but focus on the women and what they are
doing. Automatically generating such descriptions
is challenging: a full system must understand the
image, select the relevant visual content to present,
and construct complete sentences. Existing sys-
tems aim to address all of these challenges but
use visual detectors for only a small vocabulary
of words, typically nouns, associated with objects
that can be reliably found.
1
Such systems are blind
?
This work was conducted at Microsoft Research.
1
While object recognition is improving (ImageNet accu-
racy is over 90% for 1000 classes) progress in activity recog-
nition has been slower; the state of the art is below 50% mean
average precision for 40 activity classes (Yao et al., 2011).
cars (Count:3) Isa: ride, vehicle,? Doing: parking,? Has: steering wheel,? Attrib: black, shiny,? 
children (Count:2) Isa: kids, children ? Doing: biking, riding ? Has: pants, bike ? Attrib: young, small ? 
bike (Count:1) Isa: bike, bicycle,? Doing: playing,? Has: chain, pedal,? Attrib: silver, white,? 
women(Count:3) Isa: girls, models,? Doing: smiling,...  Has: shorts, bags,? Attrib: young, tan,? 
purses(Count:3) Isa: accessory,? Doing: containing,? Has: body, straps,? Attrib: black, soft,? 
sidewalk(Count:1) Isa: sidewalk, street,? Doing: laying,? Has: stone, cracks,? Attrib: flat, wide,? 
woman(Count:1) Isa: person, female,? Doing: pointing,? Has: nose, legs,? Attrib: tall, skinny,? 
tree(Count:1) Isa: plant,? Doing: growing,? Has: branches,? Attrib: tall, green,? 
kids(Count:5) Isa: group, teens,? Doing: walking,? Has: shoes, bags,? Attrib: young,? 
Five young people on the street, two sharing a bicycle.
Several young people are walking near parked vehicles.
Three girls with large handbags walking down the sidewalk.
Three women walk down a city street, as seen from above.
Three young woman walking down a sidewalk looking up.
Figure 1: An annotated image with human generated sen-
tence descriptions. Each bounding polygon encompasses one
or more objects and is associated with a count and text la-
bels.This image has 9 high level objects annotated with over
250 textual labels.
to much of the visual content needed to generate
complete, human-like sentences.
In this paper, we instead study generation with
more complete visual support, as provided by hu-
man annotations, allowing us to develop more
comprehensive models than previously consid-
ered. Such models have the dual benefit of (1)
providing new insights into how to construct more
human-like sentences and (2) allowing us to per-
form experiments that systematically study the
contribution of different visual cues in generation,
suggesting which automatic detectors would be
most beneficial for generation.
In an effort to approximate relatively complete
visual recognition, we collected manually labeled
representations of objects, parts, attributes and ac-
tivities for a benchmark caption generation dataset
that includes images paired with human authored
110
descriptions (Rashtchian et al., 2010).
2
As seen
in Figure 1, the labels include object boundaries
and descriptive text, here including the facts that
the children are ?riding? and ?walking? and that
they are ?young.? Our goal is to be as exhaustive
as possible, giving equal treatment to all objects.
For example, the annotations in Figure 1 contain
enough information to generate the first three sen-
tences and most of the content in the remaining
two. Labels gathered in this way are a type of fea-
ture norms (McRae et al., 2005), which have been
used in the cognitive science literature to approxi-
mate human perception and were recently used as
a visual proxy in distributional semantics (Silberer
and Lapata, 2012). We present the first effort, that
we are aware of, for using feature norms to study
image description generation.
Such rich data allows us to develop significantly
more comprehensive generation models. We di-
vide generation into choices about which visual
content to select and how to realize a sentence that
describes that content. Our approach is grammar-
based, feature-rich, and jointly models both deci-
sions. The content selection model includes la-
tent variables that align phrases to visual objects
and features that, for example, measure how vi-
sual salience and spatial relationships influence
which objects are mentioned. The realization ap-
proach considers a number of cues, including lan-
guage model scores, word specificity, and relative
spatial information (e.g. to produce the best spa-
tial prepositions), when producing the final sen-
tence. When used with a reranking model, includ-
ing global cues such as sentence length, this ap-
proach provides a full generation system.
Our experiments demonstrate high quality vi-
sual content selection, within 90% of human per-
formance on unigram BLEU, and improved com-
plete sentence generation, nearly halving the dif-
ference from human performance to two base-
lines on 4-gram BLEU. In ablations, we measure
the importance of different annotations and visual
cues, showing that annotation of activities and rel-
ative bounding box information between objects
are crucial to generating human-like description.
2 Related Work
A number of approaches have been proposed
for constructing sentences from images, includ-
ing copying captions from other images (Farhadi
2
Available at : http://homes.cs.washington.edu/?my89/
et al., 2010; Ordonez et al., 2011), using text
surrounding an image in a news article (Feng
and Lapata, 2010), filling visual sentence tem-
plates (Kulkarni et al., 2011; Yang et al., 2011;
Elliott and Keller, 2013), and stitching together ex-
isting sentence descriptions (Gupta and Mannem,
2012; Kuznetsova et al., 2012). However, due to
the lack of reliable detectors, especially for activi-
ties, many previous systems have a small vocab-
ulary and must generate many words, including
verbs, with no direct visual support. These prob-
lems also extend to video caption systems (Yu and
Siskind, 2013; Krishnamoorthy et al., 2013).
The Midge algorithm (Mitchell et al., 2012)
is most closely related to our approach, and will
provide a baseline in our experiments. Midge is
syntax-driven but again uses a small vocabulary
without direct visual support for every word. It
outputs a large set of sentences to describe all
triplets of recognized objects in the scene, but does
not include a content selection model to select the
best sentence. We extend Midge with content and
sentence selection rules to use it as a baseline.
The visual facts we annotate are motivated by
research in machine vision. Attributes are a
good intermediate representation for categoriza-
tion (Farhadi et al., 2009). Activity recognition
is an emerging area in images (Li and Fei-Fei,
2007; Yao et al., 2011; Sharma et al., 2013) and
video (Weinland et al., 2011), although less stud-
ied than object recognition. Also, parts have been
widely used in object recognition (Felzenszwalb
et al., 2010). Yet, no work tests the contribution of
these labels for sentence generation.
There is also a significant amount of work
on other grounded language problems, where re-
lated models have been developed. Visual re-
ferring expression generation systems (Krahmer
and Van Deemter, 2012; Mitchell et al., 2013;
FitzGerald et al., 2013) aim to identify specific
objects, a sub-problem we deal with when de-
scribing images more generally. Other research
generates descriptions in simulated worlds and,
like this work, uses feature rich models (Angeli
et al., 2010), or syntactic structures like PCFGs
(Chen et al., 2010; Konstas and Lapata, 2012) but
does not combine the two. Finally, Zitnick and
Parikh (2013) study sentences describing clipart
scenes. They present a number of factors influenc-
ing overall descriptive quality, several of which we
use in sentence generation for the first time.
111
3 Dataset
We collected a dataset of richly annotated images
to approximate gold standard visual recognition.
In collecting the data, we sought a visual annota-
tion with sufficient coverage to support the gen-
eration of as many of the words in the original
image descriptions as possible. We also aimed to
make it as visually exhaustive as possible?giving
equal treatment to all visible objects. This ensures
less bias from annotators? perception about which
objects are important, since one of the problems
we would like to solve is content selection. This
dataset will be available for future experiments.
We built on the dataset from (Rashtchian et
al., 2010) which contained 8,000 Flickr images
and associated descriptions gathered using Ama-
zon Mechanical Turk (MTurk). Restricting our-
selves to Creative Commons images, we sampled
500 images for annotation.
We collected annotations of images in three
stages using MTurk, and assigned each annotation
task to 3-5 workers to improve quality through re-
dundancy (Callison-Burch, 2009). Below we de-
scribe the process for annotating a single image.
Stage 1: We prompted five turkers to list all ob-
jects in an image, ignoring objects that are parts of
larger objects (e.g., the arms of a person), which
we collected later in Stage 3. This list also in-
cluded groups, such as crowds of people.
Stage 2: For each unique object label from
Stage 1, we asked two turkers to draw a polygon
around the object identified.
3
In cases where the
object is a group, we also asked for the number of
objects present (1-6 or many). Finally, we created
a list of all references to the object from the first
stage, which we call the Object facet.
Stage 3: For each object or group, we prompted
three turkers to provide descriptive phrases of:
? Doing ? actions the object participates in, e.g.
?jumping.?
? Parts ? physical parts e.g. ?legs?, or other
items in the possession of the object e.g.
?shirt.?
? Attributes ? adjectives describing the object,
e.g. ?red.?
? Isa ? alternative names for a object e.g.
?boy?, ?rider.?
Figure 1 shows more examples for objects
3
We modified LabelMe (Torralba et al., 2010).
in a labeled image.
4
We refer to all of these
annotations, including the merged Object la-
bels, as facets. These labels provide feature
norms (McRae et al., 2005), which have recently
used as a visual proxy in distributional seman-
tics (Silberer and Lapata, 2012; Silberer et al.,
2013) but have not been previous studied for gen-
eration. This annotation of 500 images (2500
sentences) yielded over 4000 object instances and
100,000 textual labels.
4 Approach
Given such rich annotations, we can now de-
velop significantly more comprehensive genera-
tion models. In this section, we present an ap-
proach that first uses a generative model and then
a reranker. The generative model defines a dis-
tribution over content selection and content real-
ization choices, using diverse cues from the image
annotations. The reranker trades off our generative
model score, language model score (to encourage
fluency), and length to produce the final sentence.
Generative Model We want to generate a sen-
tence ~w = ?w
1
. . . w
n
? where each word w
i
? V
comes from a fixed vocabulary V . The vocabu-
lary V includes all 2700 words used in descriptive
sentences in the training set.
5
The model conditions on an annotated image I
that contains a set of objects O, where each ob-
ject o ? O has a bounding polygon and a number
of facets containing string labels. To model the
naming of specific objects, words w
i
can be asso-
ciated with alignment variables a
i
that range over
O. One such variable is introduced for each head
noun in the sentence. Figure 2 shows alignment
variable settings with colors that match objects in
the image. Finally, as a byproduct of the hierarchi-
cal generative process, we construct an undirected
dependency tree
~
d over the words in ~w.
The complete generative model defines the
probability p(~w,~a,
~
d | I) of a sentence ~w, word
alignments ~a, and undirected dependency tree
~
d,
given the annotated input image I . The overall
process unfolds recursively, as seen in Figure 3.
4
In the experiments, Parts and Isa facets do not improve
performance, so we do not use them in the final model. Isa
is redundant with the Object facet, as seen in Figure 1. Also
parts like clothing, were often annotated as separate objects.
5
We do not generate from image facets directly, because
only 20% of the sentences in our data can be produced like
this. Instead, we develop features which consider the similar-
ity between labels in the image and words in the vocabulary.
112
Three girls  with large  handbags  walking  down  the  sidewalk 
? = ?= ? = 
Figure 2: One path through the generative model and the
Bayesian network it induces. The first row of colored circles
are alignment variables to objects in the image. The second
row is words, generated conditioned on alignments.
The main clause is produced by first selecting the
subject alignment a
s
followed by the subject word
w
s
. It then chooses the verb and optionally the ob-
ject alignment a
o
and word w
o
. The process then
continues recursively, modifying the subject, verb,
and object of the sentence with noun and prepo-
sitional modifiers. The recursion begins at Step
2 in Figure 3. Given a parent word w and that
word?s relevant alignment variable a, the model
creates attachments where w is the grammatical
head of subsequently produced words. Choices
about whether to create noun modifiers or preposi-
tional modifiers are made in steps (a) and (b). The
process chooses values for the alignment variables
and then chooses content words, adding connec-
tive prepositions in the case of prepositional mod-
ifiers. It then chooses to end or submits new word-
alignment pairs to be recursively modified.
Each line defines a decision that must be made
according to a local probability distribution. For
example, Step 1.a defines the probability of align-
ing a subject word to various objects in the im-
age. The distributions are maximum entropy mod-
els, similar to previous work (Angeli et al., 2010),
using features described in the next section. The
induced undirected dependency tree
~
d has an edge
between each word and the previously generated
word (or the input word w in Steps 2.a.i and 2.a.ii,
when no previous word is available). Figure 2
shows a possible output from the process, along
with the Bayesian network that encodes what each
decision was conditioned on during generation.
Learning We learn the model from data
{(~w
i
,
~
d
i
, I
i
) | i = 1 . . .m} containing sentences
~w
i
, dependency trees
~
d
i
, computed with the Stan-
ford parser (de Marneffe et al., 2006), and images
1. for a main clause (d,e are optional), select:
(a) subject a
s
alignment from p
a
(a).
(b) subject word w
s
from p
n
(w | a
s
,
~
d
c
)
(c) verb word w
v
from p
v
(w | a
s
,
~
d
c
)
(d) object alignment a
o
from p
a
(a
?
| a
s
, w
v
,
~
d
c
)
(e) object word w
o
from p
n
(w | a
o
,
~
d
c
)
(f) end with p
stop
or go to (2) with (w
s
, a
s
)
(g) end with p
stop
or go to (2) with (w
v
, a
s
)
(h) end with p
stop
or go to (2) with (w
o
, a
o
)
2. for a (word, alignment) (w
?
, a) (a,b are optional):
(a) if w
?
not verb: modify w
?
with noun, select:
i. modifier word w
n
from p
n
(w | a,
~
d
c
).
ii. end with p
stop
or go to (2) with (a
m
, w
n
)
(b) modify w
?
with preposition, select:
i. preposition word w
p
if w
?
not a verb: from p
p
(w | a,
~
d
c
)
else: from p
p
(w | a,w
v
,
~
d
c
)
ii. object alignment a
p
from p
a
(a
?
| a,w
p
,
~
d
c
)
iii. object word w
n
from p
n
(w | a
p
,
~
d
c
).
iv. end with p
stop
or go to (2) with (a
p
, w
n
)
Figure 3: Generative process for producing words ~w, align-
ments ~a and dependencies
~
d. Each distribution is conditioned
on the partially complete path through generative process
~
d
c
to establish sentence context. The notation p
stop
is short hand
for p
stop
(STOP |~w,
~
d
c
) the stopping distribution.
I
i
. The dependency trees define the path that was
taken through the generative process in Figure 3
and are used to create a Bayesian network for ev-
ery sentence, like in Figure 2. However, object
alignments ~a
i
are latent during learning and we
must marginalize over them.
The model is trained to maximize the condi-
tional marginal log-likelihood of the data with reg-
ularization:
L(?) =
?
i
log
?
~a
p(~a, ~w
i
,
~
d
i
| I
i
; ?)? r|?|
2
where ? is the set of parameters and r is the regu-
larization coefficient. In essence, we maximize the
likelihood of every sentence?s observed Bayesian
network, while marginalizing over content selec-
tion variables we did not observe.
Because the model only includes pairwise de-
pendencies between the hidden alignment vari-
ables ~a, the inference problem is quadratic in the
number of objects and non-convex because ~a is
unobserved. We optimize this objective directly
with L-BFGS, using the junction-tree algorithm to
compute the sum and the gradient.
6
6
To compute the gradient, we differentiate the recurrence
in the junction-tree algorithm by applying the product rule.
113
Inference To describe an image, we need to
maximize over word, alignment, and the depen-
dency parse variables:
argmax
~w,~a,
~
d
p(~w,~a,
~
d | I)
This computation is intractable because we
need to consider all possible sentences, so we use
beam search for strings up to a fixed length.
Reranking Generating directly from the process
in Figure 3 results in sentences that may be short
and repetitive because the model score is a product
of locally normalized distributions. The reranker
takes as input a candidate list c, for an image I , as
decoded from the generative model. The candidate
list includes the top-k scoring hypotheses for each
sentence length up to a fixed maximum. A linear
scoring function is used for reranking optimized
with MERT (Och, 2003) to maximize BLEU-2.
5 Features
We construct indicator features to capture vari-
ation in usage in different parts of the sen-
tence, types of objects that are mentioned, visual
salience, and semantic and visual coordination be-
tween objects. The features are included in the
maximum entropy models used to parameterize
the distributions described in Figure 3. Whenever
possible, we use WordNet Synsets (Miller, 1995)
instead of lexical features to limit over-fitting.
Features in the generative model use tests for
local properties, such as the identity of a synset
of a word in WordNet, conjoined with an iden-
tifier that indicates context in the generative pro-
cess.
7
Generative model features indicate (1) vi-
sual and semantic information about objects in dis-
tributions over alignments (content selection) and
(2) preferences for referring to objects in distribu-
tions over words (content realization). Features in
the reranking model indicate global properties of
candidate sentences. Exact formulas for comput-
ing the features are in the appendix.
Visual features, such as an object?s position in
the image, are used for content selection. Pairwise
visual information between two objects, for exam-
ple the bounding box overlap between objects or
the relative position of the two objects, is included
in distributions where selection of an alignment
7
For example, in Figure 2 the context for the word ?side-
walk? would be ?word,syntactic-object,verb,preposition? in-
dicating it is a word, in the syntactic object of a preposition,
which was attached to a verb modifying prepositional phrase.
variable conditions on previously generated align-
ments. For verbs (Step 1.d in Figure 3) and prepo-
sitions (Step 2.b.ii), these features are conjoined
with the stem of the connective.
Semantic types of objects are also used in con-
tent selection. We define semantic types by finding
synsets of labels in objects that correspond to high
level types, a list motivated by the animacy hierar-
chy (Zaenen et al., 2004).
8
Type features indicate
the type of the object referred to by an alignment
variable as well as the cross product of types when
an alignment variable is on conditioning side of
a distribution (e.g. Step 1.d). Like above, in the
presence of a connective word, these features are
conjoined with the stem of the connective.
Content realization features help select words
when conditioning on chosen alignments (e.g.
Step 1.b). These features include the identity of
the WordNet synset corresponding to a word, the
word?s depth in the synset hierarchy, the language
model score for adding that word
9
and whether the
word matches labels in facets corresponding to the
object referenced by an alignment variable.
Reranking features are primarily used to over-
come issues of repetition and length in the genera-
tive distributions, more commonly used for align-
ment, than to create sentences. We use only four
features: length, the number of repetitions, gener-
ative model score, and language model score.
6 Experimental Setup
Data We used 70% of the data for training (1750
sentences, 350 images), 15% for development, and
15% for testing (375 sentences, 75 images).
Parameters The regularization parameter was
set on the held out data to r = 8. The reranker
candidate list included the top 500 sentences for
each sentence length up to 15 and weights were
optimized with Z-MERT (Zaidan, 2009).
Metrics Our evaluation is based on BLEU-n
(Papineni et al., 2001), which considers all n-
grams up to length n. To assess human perfor-
mance using BLEU, we score each of the five ref-
erences against the four other ones and finally av-
erage the five BLEU scores. In order to make these
results comparable to BLEU scores for our model
8
For example, human, animal, artifact (a human created
object), natural body (trees, water, ect.), or natural artifact
(stick, leaf, rock).
9
We use tri-grams with Kneser-Ney smoothing over the 1
million caption data set (Ordonez et al., 2011).
114
and baselines, we perform the same five-fold aver-
aging when computing BLEU for each system.
We also compute accuracy for different syn-
tactic positions in the sentence. We look at a
number of categories: the main clause?s compo-
nents (S,V,O), prepositional phrase components,
the preposition (Pp) and their objects (Po) and
noun modifying words (N), including determiners.
Phrases match if they have an exact string match
and share context identifiers as defined in the fea-
tures sections.
Human Evaluation Annotators rated sentences
output by our full model against either human or a
baseline system generated descriptions. Three cri-
teria were evaluated: grammaticality, which sen-
tence is more complete and well formed; truthful-
ness, which sentence is more accurately capturing
something true in the image; and salience, which
sentence is capturing important things in the image
while still being concise. Two annotators anno-
tated all test pairs for all criteria for a given pair of
systems. Six annotators were used (none authors)
and agreement was high (Cohen?s kappa = 0.963,
0.823 and 0.703 for grammar, truth and salience).
Machine Translation Baseline The first base-
line is designed to see if it is possible to generate
good sentences from the facet string labels alone,
with no visual information. We use an extension of
phrase-based machine translation techniques (Och
et al., 1999). We created a virtual bitext by pair-
ing each image description (the target sentence)
with a sequence
10
of visual identifiers (the source
?sentence?) listing strings from the facet labels.
Since phrases produced by turkers lack many of
the functions words needed to create fluent sen-
tences, we added one of 47 function words either
at the start or the end of each output phrase.
The translation model included standard fea-
tures such as language model score (using our cap-
tion language model described previously), word
count, phrase count, linear distortion, and the
count of deleted source words. We also define
three features that count the number of Object, Isa,
and Doing phrases, to learn a preference for types
of phrases. The feature weights are tuned with
MERT (Och, 2003) to maximize BLEU-4.
Midge Baseline As described in related work,
the Midge system creates a set of sentences to de-
scribe everything in an input image. These sen-
10
We defined a consistent ordering of visual identifiers and
set the distortion limit of the phrase-based decoder to infinity.
BL-1 BL-2 BL-3 BL-4
Human 61.0 42.0 27.8 18.3
Full Model 57.1 35.7 18.3 9.5
MT Baseline 39.8 23.6 13.2 6.1
Midge Baseline 43.5 20.2 9.4 0.0
Table 1: Results for the test set for the BLEU1-4 metrics.
Grammar Full Other Equal
Full vs Human 7.65 19.4 72.94
Full vs MT 6.47 5.29 88.23
Full vs Midge 40.59 15.88 43.53
Truth Full Other Equal
Full vs Human 0.59 67.65 31.76
Full vs MT 30.0 10.59 59.41
Full vs Midge 51.76 27.71 23.53
Salience Full Other Equal
Full vs Human 8.82 88.24 2.94
Full vs MT 51.76 16.47 31.77
Full vs Midge 71.18 14.71 14.12
Table 2: Human evaluation of our Full-Model in heads
up tests against Human authored sentences and baseline sys-
tems, the machine translation baseline (MT) and the Midge
inspired baseline. Bold indicates the better system. Other is
not the Full system. Equal indicates neither sentence is better.
tences must all be true, but do not have to select
the same content that a person would. It can be
adapted to our task by adding object selection and
sentence ranking rules. For object selection, we
choose the three most frequently named objects
in the scene according to a background corpus of
image descriptions. For sentence selection, we
take all sentences within one word of the average
length of a sentence in our corpus, 11, and select
the one with best Midge generation score.
7 Results
We report experiments for our generation pipeline
and ablations that remove data and features.
Overall Performance Table 1 shows the re-
sults on the test set. The full model consis-
tently achieves the highest BLEU scores. Overall,
these numbers suggest strong content selection by
getting high recall for individual words (BLEU-
1), but fall further behind human performance as
the length of the n-gram grows (BLEU-2 through
BLEU-4). These number match our perception
that the model is learning to produce high quality
sentences, but does not always describe all of the
important aspects of the scene or use exactly the
expected wording. Table 4 presents example out-
put, which we will discuss in more detail shortly.
115
Model BL-1 BL-2 BL-3 BL-4 S V O Pp Po N
Human 64.7 46.0 31.5 20.1 - - - - - -
Full-Model 59.0 36.9 19.3 10.5 64.9 40.4 36.8 50.0 20.7 69.1
? doing 51.1 32.6 16.9 9.2 63.2 15.8 10.5 45.5 21.6 69.7
? count 55.4 33.5 16.0 8.5 59.6 35.1 15.4 53.7 19.5 66.7
? properties 57.8 37.2 18.8 10.0 61.4 36.8 36.8 47.1 20.7 73.5
? visual 56.7 35.1 18.9 9.4 64.9 36.8 50.0 41.8 15.3 71.6
? pairwise 56.9 35.5 16.5 8.2 64.9 40.4 45.5 42.4 21.2 70.9
Table 3: Ablation results on development data using BLEU1-4 and reporting match accuracy for sentence structures.
S: A girl playing a
guitar in the grass
R: A woman with a nylon stringed
guitar is playing in a field
S: A man playing with two
dogs in the water
R: A man is throwing a log into
a waterway while two dogs watch
S: Two men playing with
a bench in the grass
R: Nine men are playing a game
in the park, shirts versus skins
S: Three kids sitting on a road
R: A boy runs in a race
while onlookers watch
Table 4: Two good examples of output (top), and two ex-
amples of poor performance (bottom). Each image has two
captions, the system output S and a human reference R.
Human Evaluation Table 2 presents the results
of a human evaluation. The full model outper-
forms all baselines on every measure, but is not
always competitive with human descriptions. It
performs the best on grammaticality, where it is
judged to be as grammatical as humans. How-
ever, surprisingly, in many cases it is also often
judged equal to the other baselines. Examination
of baseline output reveals that the MT baseline of-
ten generates short sentences, having little chance
of being judged ungrammatical. Furthermore, the
Midge baseline, like our system, is a syntax-based
system and therefore often produces grammatical
sentences. Although our system performs well
with respect to the baselines on truthfulness, of-
ten the system constructs sentences with incorrect
prepositions, an issue that could be improved with
better estimates of 3-d position in the image. On
truthfulness, the MT baseline is comparable to our
system, often being judged equal, because its out-
put is short. Our system?s strength is salience, a
factor the baselines do not model.
Data Ablation Table 3 shows annotation abla-
tion experiments on the development set, where
we remove different classes of data labels to mea-
sure the performance that can be achieved with
less visual information. In all cases, the overall
behavior of the system varies, as it tries to learn to
compensate for the missing information.
Ablating actions is by far the most detrimental.
Overall BLEU score suffers and prediction accu-
racy of the verb (V) degrades significantly causing
cascading errors that affect the object of the verb
(O). Removing count information affects noun at-
tachment (N) performance. Images where deter-
miner use is important or where groups of objects
are best identified by the number (for example,
three dogs) are difficult to describe naturally. Fi-
nally, we see a tradeoff when removing properties.
There is an increase in noun modifier accuracy (N)
but a decrease in content selection quality (BL-1),
showing recall has gone down. In essence, the ap-
proach learns to stop trying to generate adjectives
and other modifiers that would rely on the missing
properties. The difference in BLEU score with the
Full-Model is small, even without these modifiers,
because there often still exists a a short output with
high accuracy.
Feature Ablation The bottom two rows in Ta-
ble 3 show ablations of the visual and pairwise
features, measuring the contribution of the visual
information provided by the bounding box anno-
tations. The ablated visual information includes
bounding-box positions and relative pairwise vi-
sual information. The pairwise ablation removes
the ability to model any interactions between ob-
jects, for example, relative bounding box or pair-
wise object type information.
Overall, prepositional phrase accuracy is most
affected. Ablating visual features significantly im-
pacts accuracy of prepositional phrases (Pp and
Po), affecting the use of preposition words the
most, and lowering fluency (BL-4). Precision in
116
the object of the verb (O) rises; the model makes
? 50% fewer predictions in that position than the
Full-Model because it lacks features to coordinate
subject and object of the verb. Ablating pairwise
features has similar results. While the model cor-
rects errors in the object of the preposition (Po)
with the addition of visual features, fluency is still
worse than Full-Model, as reflected by BL-4.
Qualitative Results Table 4 has examples of
good and bad system output. The first two im-
ages are good examples, including both system
output (S) and a human reference (R). The sec-
ond two contain lower quality outputs. Overall,
the model captures common ways to refer to peo-
ple and scenes. However, it does better for images
with fewer sentient objects because content selec-
tion is less ambiguous.
Our system does well at finding important ob-
jects. For example, in the first good image, we
mention the guitar instead of the house, both of
which are prominent and have high overlap with
the woman. In the second case, we identify that
both dogs and humans tend to be important actors
in scenes but poorly identify their relationship.
The bad examples show difficult scenes. In the
first description the broad context is not identi-
fied, instead focusing on the bench (highlighted in
red). The second example identifies a weakness
in our annotation: it encodes contradictory group-
ings of the people. The groupings covers all of
the children, including the boy running, and many
subsets of the people near the grass. This causes
ambiguity and our methods cannot differentiate
them, incorrectly mentioning just the children and
picking an inappropriate verb (one participant in
the group is not sitting). Improved annotation of
groups would enable the study of generation for
more complex scenes, such as these.
8 Conclusion
In this work we used dense annotations of images
to study description generation. The annotations
allowed us to not only develop new models, better
capable of generating human-like sentences, but
also to explore what visual information is crucial
for description generation. Experiments showed
that activity and bounding-box information is im-
portant and demonstrated areas of future work. In
images that are more complex, for example multi-
ple sentient objects, object grouping and reference
will be important to generating good descriptions.
Issues of this type can be explored with annota-
tions of increasing complexity.
Appendix A
This appendix describes the feature templates for
the generative model in greater detail.
Features in the generative model conjoin indica-
tors for local tests, such as STEM(w) which in-
dicates the stem of a wordw, with a global contex-
tual identifier CONTEXT(v, d) that indicates
properties of the generation history, as described
in detail below. Table 5 provides a reference for
which feature templates are used in the generative
model distributions, as defined in Figure 3.
8.1 Feature Templates
CONTEXT(n, d) is an indicator for a contex-
tual identifier for a variable n in the model de-
pending on the dependency structure d. There is
an indicator for all combinations of the type of n
(alignment or word), the position of n (subject,
syntactic object, verb, noun-modifier, or preposi-
tion), the position of the earliest variable along
the path to generate n, and the type of attach-
ment to that variable (noun or prepositional mod-
ifier). For example, in Figure 2 the context for
the word ?sidewalk? would be ?word,syntactic-
object,verb,preposition? indicating it is a word, the
object of a preposition, whose path was along a
verb modifying prepositional phrase.
11
TYPE(a) indicates the high level type of an
object referred to by alignment variable a. We
use synsets to define high level types including
human, animal, artifact, natural artifact and var-
ious synsets that capture scene information,
12
a
list motivated by the animacy hierarchy (Zaenen
et al., 2004). Each object is assigned a type by
finding the synset for its name (object facet), and
tracing the hypernym structure in Wordnet to find
the appropriate class, if one exists. Additionally,
the type indicates whether the object is a group or
not. For example, in Figure 2, the blue polygon
has type ?person,group?, or the red bike polygon
has type ?artifact,single.?
11
Similarly ?large? is ?word,noun,subject,preposition?
while ?girls? is special cased to ?word,subject,root? be-
cause it has no initial attachment. The alignment vari-
able above the word handbags is ?alignment,syntactic-
object,subject,preposition? because it an alignment variable,
is in the syntactic object position of a preposition and can be
located by following a subject attached pp.
12
WordNet divides these into synsets expressing water,
weather, nature and a few more.
117
Feature Family Included In Steps
CONTEXT(a
?
,
~
d
c
)?
{TYPE(a
?
),MENTION(a
?
, do),MENTION(a
?
, obj ),VISUAL(a
?
)}
p
a
(a
?
|
~
d
c
)
p
a
(a
?
| a,w,
~
d
c
)
1.a, 1.d, 2.b.ii
CONTEXT(a
?
,
~
d
c
)? {TYPE(a)?TYPE(a
?
),VISUAL2(a, a
?
)} p
a
(a
?
| a,w,
~
d
c
) 1.d, 2.b.i
CONTEXT(a
?
,
~
d
c
)?
{TYPE(a)?TYPE(a
?
)? STEM(w),VISUAL2(a, a
?
)? STEM(w)}
p
a
(a
?
| a,w,
~
d
c
) 1.d, 2.b.i
CONTEXT(a,
~
d
c
)?
{WORDNET(w),MATCH(w, a),SPECIFICITY(w, a),
ADJECTIVE(w, a),DETERMINER(w, a)}
p
n
(w | a,
~
d
c
) 1.b, 1.e, 2.a.i
2.b.ii
CONTEXT(a,
~
d
c
)? {MATCH(w, a),TYPE(a)? STEM(w)} p
v
(w | a,
~
d
c
) 1.c
CONTEXT(a
?
,
~
d
c
)?TYPE(a)? STEM(w
p
) p
p
(w | a,
~
d
c
)
p
p
(w | a,w
v
,
~
d
c
)
2.b.i
CONTEXT(a
?
,
~
d
c
)? STEM(w
v
)? STEM(w) p
p
(w | a,w
v
,
~
d
c
) 2.b.i
Table 5: Feature families and distributions that include them. ? indicates the cross-product of the indi-
cator features. Distributions are listed more than once to indicate they use multiple feature families.
VISUAL(a) returns indicators for visual facts
about the object that a aligns to. There is an in-
dicator for two quantities: (1) overlap of object?s
polygon with every horizontal third of the image,
as a fraction of the object?s area, and (2) the ob-
ject?s distance to the center of the image as frac-
tion of the diagonal of the image. Each quantity,
v, is put into three overlapping buckets: if v > .1,
if v > .5, and if v > .9.
VISUAL2(a, a
?
) indicates pairwise visual
facts about two objects. There is an indicator for
the following quantities bucketed: the amount of
overlap between the polygons for a and a
?
as a
fraction of the size of a?s polygon, the distance
between the center of the polygon for a and a
?
as
a fraction of image?s diagonal, and the slope be-
tween the center of a and a
?
. Each quantity, v, is
put into three overlapping buckets: if v > .1, if
v > .5, and if v > .9. There is an indicator for the
relative position of extremities a and a
?
: whether
the rightmost point of a is further right than a
?
?s
rightmost or leftmost point, and the same for top,
left, and bottom.
WORDNET(w) returns indicators for all hy-
pernyms of a word w. The two most specific
synsets are not used when there at least 8 options.
MENTION(a, facet) returns the union of the
WORDNET(w) features for all words w in the
facet facet for the object referred to alignment a.
ADJECTIVE(w, a) indicates four types
of features specific to adjective usage. If
MENTION(w,Attributes) is not empty, indi-
cate : (1) the satellite adjective synset of w in
Wordnet, (2) the head adjective synset of w in
Wordnet, (3) the head adjective synset conjoined
withTYPE(a), and (4) the number of times there
exists a label in the Attributes facet of a that has
the same head adjective synset as w.
DETERMINER(w, a) indicates four deter-
miner specific features. If w is a determiner, then
indicate : (1) the identity of w conjoined with the
count (the label for numerosity) of a, (2) the iden-
tity of w conjoined with an indicator for if the
count of a is greater than one, (3) the identity of w
conjoined with TYPE(a) and (4) the frequency
with which w appears before its head word in the
Flikr corpus (Ordonez et al., 2011).
MATCH(w, a), indicates all facets of object
a that contain words with the same stem as w.
SPECIFICITY(w, a) is an indicator of the
specificity of the word w when referring to the ob-
ject aligned to a. Indicates the relative depth of
w in Wordnet, as compared to all words w
?
where
MATCH(w
?
, a) is not empty. The depth is buck-
eted into quintiles.
STEM(w) returns the Porter2 stem of w.
13
The distribution for stopping, p
stop
(STOP |
~
d
c
, ~w), contains two types of features. (1) Struc-
tural features indicating for the number of times
a contextual identifier has appeared so far in the
derivation and (2) mention features indicating the
types of objects mentioned.
14
To compute men-
tion features, we consider all possible types of ob-
jects, t, then there is an indicator for: (1) if ?o, ?w ?
~w : MATCH(w, o) 6= ? ?TYPE(o) = t, (2) whether
?o, 6 ?w ? ~w : MATCH(w, o) 6= ??TYPE(o) = t and
(3) if (1) does not hold but (2) does.
Acknowledgments This work is partially funded by DARPA
CSSG (D11AP00277) and ARO (W911NF-12-1-0197). We
thank L. Zitnick, B. Dolan, M. Mitchell, C. Quirk, A. Farhadi,
B. Russell for helpful conversations. Also, L. Zilles, Y. Atrzi,
N. FitzGerald, T. Kwiatkowski and reviewers for comments.
13
http://snowball.tartarus.org/algorithms/english/stemmer.html
14
Object mention features cannot contain ~a because that
creates large dependencies in inference for learning.
118
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP, pages 286?295, August.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. JAIR,
37:397?435.
Marie-Catherine de Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generating
typed dependency parses from phrase structure
parses. In LREC, volume 6, pages 449?454.
Desmond Elliott and Frank Keller. 2013. Image De-
scription using Visual Dependency Representations.
In EMNLP.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. In Computer Vision and Pattern Recogni-
tion, 2009. CVPR 2009. IEEE Conference on, pages
1778?1785. IEEE.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: Generating sentences from images.
In Proceedings of the 11th European conference on
Computer Vision, ECCV?10, pages 15?29.
Pedro F Felzenszwalb, Ross B Girshick, David
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part-based
models. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 32(9):1627?1645.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? Automatic caption gener-
ation for news images. In ACL, pages 1239?1249.
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logi-
cal forms for referring expression generation. In
EMNLP.
Ankush Gupta and Prashanth Mannem. 2012. From
image annotation to image description. In NIPS,
volume 7667, pages 196?204.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
ACL, pages 369?378.
Emiel Krahmer and Kees Van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173?218.
Niveda Krishnamoorthy, Girish Malkarnenkar, Ray-
mond Mooney, Kate Saenko, and Sergio Guadar-
rama. 2013. Generating natural-language video de-
scriptions using text-mined knowledge. Procedings
of AAAI, 2013(2):3.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C.
Berg, and T.L. Berg. 2011. Baby talk: Understand-
ing and generating simple image descriptions. In
Computer Vision and Pattern Recognition (CVPR),
pages 1601?1608.
Polina Kuznetsova, Vicente Ordonez, Alexander C
Berg, Tamara L Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
ACL, pages 359?368.
Li-Jia Li and Li Fei-Fei. 2007. What, where and who?
Classifying events by scene and object recognition.
In ICCV, pages 1?8. IEEE.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris Mcnorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior Research Methods, 37(4):547?
559.
George A Miller. 1995. WordNet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, and Hal Daum?e, III.
2012. Midge: Generating image descriptions from
computer vision detections. In EACL, pages 747?
756.
Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2013. Generating expressions that refer to vis-
ible objects. In Proceedings of NAACL-HLT, pages
1174?1184.
F. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint Conf. of Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, pages 20?28.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing images using 1 million
captioned photographs. In NIPS, pages 1143?1151.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL.
C. Rashtchian, P. Young, M. Hodosh, and J. Hock-
enmaier. 2010. Collecting image annotations us-
ing Amazon?s Mechanical Turk. In Proceedings
of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, pages 139?147.
119
Gaurav Sharma, Fr?ed?eric Jurie, Cordelia Schmid, et al.
2013. Expanded parts model for human attribute
and action recognition in still images. In CVPR.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In EMNLP, July.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with visual
attributes. In ACL, pages 572?582.
Antonio Torralba, Bryan C Russell, and Jenny Yuen.
2010. LabelMe: Online image annotation and appli-
cations. Proceedings of the IEEE, 98(8):1467?1484.
Daniel Weinland, Remi Ronfard, and Edmond Boyer.
2011. A survey of vision-based methods for action
representation, segmentation and recognition. Com-
puter Vision and Image Understanding, 115(2):224?
241.
Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing.
Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy Lai
Lin, Leonidas J. Guibas, and Li Fei-Fei. 2011.
Action recognition by learning bases of action at-
tributes and parts. In ICCV, Barcelona, Spain,
November.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, volume 1, pages 53?63.
Annie Zaenen, Jean Carletta, Gregory Garretson,
Joan Bresnan, Andrew Koontz-Garboden, Tatiana
Nikitina, M Catherine O?Connor, and Tom Wasow.
2004. Animacy encoding in English: why and how.
In ACL Workshop on Discourse Annotation, pages
118?125.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
C. Lawrence Zitnick and Devi Parikh. 2013. Bring-
ing semantics into focus using visual abstraction. In
CVPR.
120
Proceedings of BioNLP Shared Task 2011 Workshop, pages 155?163,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
MSR-NLP Entry in BioNLP Shared Task 2011 
 
 
Chris Quirk, Pallavi Choudhury, Michael Gamon, and Lucy Vanderwende 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 USA 
{chrisq,pallavic,mgamon,lucyv}@microsoft.com 
 
 
Abstract 
We describe the system from the Natural 
Language Processing group at Microsoft 
Research for the BioNLP 2011 Shared 
Task. The task focuses on event extraction, 
identifying structured and potentially 
nested events from unannotated text. Our 
approach follows a pipeline, first 
decorating text with syntactic information, 
then identifying the trigger words of 
complex events, and finally identifying the 
arguments of those events. The resulting 
system depends heavily on lexical and 
syntactic features. Therefore, we explored 
methods of maintaining ambiguities and 
improving the syntactic representations, 
making the lexical information less brittle 
through clustering, and of exploring novel 
feature combinations and feature reduction. 
The system ranked 4th in the GENIA task 
with an F-measure of 51.5%, and 3rd in the 
EPI task with an F-measure of 64.9%. 
1 Introduction 
We describe a system for extracting complex 
events and their arguments as applied to the 
BioNLP-2011 shared task.  Our goal is to explore 
general methods for fine-grained information 
extraction, to which the data in this shared task is 
very well suited.  We developed our system using 
only the data provided for the GENIA task, but 
then submitted output for two of the tasks, GENIA 
and EPI, training models on each dataset 
separately, with the goal of exploring how general 
the overall system design is with respect to text 
domain and event types. We used no external 
knowledge resources except a text corpus used to 
train cluster features. We further describe several 
system variations that we explored but which did 
not contribute to the final system submitted. We 
note that the MSR-NLP system consistently is 
among those with the highest recall, but needs 
additional work to improve precision. 
2 System Description 
Our event extraction system is a pipelined 
approach, closely following the structure used by 
the best performing system in 2009 (Bj?rne et al, 
2009). Given an input sentence along with 
tokenization information and a set of parses, we 
first attempt to identify the words that trigger 
complex events using a multiclass classifier. Next 
we identify edges between triggers and proteins, or 
between triggers and other triggers. Finally, given 
a graph of proteins and triggers, we use a rule-
based post-processing component to produce 
events in the format of the shared task. 
2.1 Preprocessing and Linguistic Analysis 
We began with the articles as provided, with an 
included tokenization of the input and 
identification of the proteins in the input. However, 
we did modify the token text and the part-of-
speech tags of the annotated proteins in the input to 
be PROT after tagging and parsing, as we found 
that it led to better trigger detection. 
The next major step in preprocessing was to 
produce labeled dependency parses for the input. 
Note that the dependencies may not form a tree: 
there may be cycles and some words may not be 
connected. During feature construction, this 
parsing graph was used to find paths between 
155
words in the sentence. Since proteins may consist 
of multiple words, for paths we picked a single 
representative word for each protein to act as its 
starting point and ending point. Generally this was 
the token inside the protein that is closest to the 
root of the dependency parse. In the case of ties, 
we picked the rightmost such node. 
2.1.1 McClosky-Charniak-Stanford parses 
The organizers provide parses from a version of 
the McClosky-Charniak parser, MCCC (McClosky 
and Charniak, 2008), which is a two-stage 
parser/reranker trained on the GENIA corpus. In 
addition, we used an improved set of parsing 
models that leverage unsupervised data, MCCC-I 
(McClosky, 2010). In both cases, the Stanford 
Parser was used to convert constituency trees in the 
Penn Treebank format into labeled dependency 
parses: we used the collapsed dependency format. 
2.1.2 Dependency posteriors 
Effectively maintaining and leveraging the 
ambiguity present in the underlying parser has 
improved task accuracy in some downstream tasks 
(e.g., Mi et al 2008). McClosky-Charniak parses 
in two passes: the first pass is a generative model 
that produces a set of n-best candidates, and the 
second pass is a discriminative reranker that uses a 
rich set of features including non-local 
information. We renormalized the outputs from 
this log-linear discriminative model to get a 
posterior distribution over the 50-best parses. This 
set of parses preserved some of the syntactic 
ambiguity present in the sentence. 
The Stanford parser deterministically converts 
phrase-structure trees into labeled dependency 
graphs (de Marneffe et al, 2006). We converted 
each constituency tree into a dependency graph 
separately and retained the probability computed 
above on each graph. 
One possibility was to run feature extraction on 
each of these 50 parses, and weight the resulting 
features in some manner. However, this caused a 
significant increase in feature count. Instead, we 
gathered a posterior distribution over dependency 
edges: the posterior probability of a labeled 
dependency edge was estimated by the sum of the 
probability of all parses containing that edge. 
Gathering all such edges produced a single labeled 
graph that retained much of the ambiguity of the 
input sentence. Figure 1 demonstrates this process 
on a simple example. We applied a threshold of 0.5 
and retained all edges above that threshold, 
although there are many alternative ways to exploit 
this structure.  
 
Figure 1: Example sentence from the GENIA corpus. (a) Two of the top 50 constituency parses from the MCCC-I 
parser; the first had a total probability mass of 0.43 and the second 0.25 after renormalization. Nodes that differ 
between parses are shaded and outlined. (b) The dependency posteriors (labels omitted due to space) after 
conversion of 50-best parses. Solid lines indicate edges with posterior > 0.95; edges with posterior < 0.05 were 
omitted. Most of the ambiguity is in the attachment of ?elicited?. 
156
As above, the resulting graph is likely no longer 
a connected tree, though it now may also be cyclic 
and rather strange in structure. Most of the 
dependency features were built on shortest paths 
between words. We used the algorithm in Cormen 
et al (2002, pp.595) to find shortest paths in a 
cyclic graph with non-negative edge weights. The 
shortest path algorithm used in feature finding was 
supplied uniform positive edge weights. We could 
also weight edges by the negative log probability 
to find the shortest, most likely path. 
2.1.3 ENJU 
We also experimented with the ENJU parses 
(Miyao and Tsujii, 2008) provided by the shared 
task organizers. The distribution contained the 
output of the ENJU parser in a format consistent 
with the Stanford Typed Dependency 
representation . 
2.1.4 Multiple parsers 
We know that even the best modern parsers are 
prone to errors. Including features from multiple 
parsers helps mitigate these errors. When different 
parsers agree, they can reinforce certain 
classification decisions. The features that were 
extracted from a dependency parse have names 
that include an identifier for the parser that 
produced them. In this way, the machine learning 
algorithm can assign different weights to features 
from different parsers. For finding heads of multi-
word entities, we preferred the ENJU parser if 
present in that experimental condition, then fell 
back to MCCC parses, and finally MCCC-I. 
2.1.5 Dependency conversion rules 
We computed our set of dependency features (see 
2.2.1) from the collapsed, propagated Stanford 
Typed Dependency representation (see 
http://nlp.stanford.edu/software/dependencies_man
ual.pdf and de Marneffe et al, 2006), made 
available by the organizers.  We chose this form of 
representation since we are primarily interested in 
computing features that hold between content 
words.  Consider, for example, the noun phrase 
?phosphorylation of TRAF2?. A dependency 
representation would specify head-modifier 
relations for the tuples (phosphorylation, of) and 
(of, TRAF2). Instead of head-modifier, a typed 
dependency representation specifies PREP and 
PPOBJ as the two grammatical relations: 
PREP(phosphorylation-1, of-2) and PPOBJ(of-2, 
TRAF2-3). A collapsed representation has a single 
triplet specifying the relation between the content 
words directly, PREP_OF(phosphorylation-1, 
TRAF2-3); we considered this representation to be 
the most informative.   
We experimented with a representation that 
further normalized over syntactic variation.  The 
system submitted for the GENIA subtask does not 
use these conversion rules, while the system 
submitted for the EPI subtask does use these rules.  
See Table 2 for further details. While for some 
applications it may be useful to distinguish 
whether a given relation was expressed in the 
active or passive voice, or in a main or a relative 
clause, we believe that for this application it is 
beneficial to normalize over these types of 
syntactic variation.  Accordingly, we had a set of 
simple renaming conversion rules, followed by a 
rule for expansion; this list was our first effort and 
could likely be improved.  We modeled this 
normalized level of representation on the logical 
form, described in Jensen (1993), though we were 
unable to explore NP-or VP-anaphora 
 
Renaming conversion rules: 
1. ABBREV -> APPOS 
2. NSUBJPASS -> DOBJ 
3. AGENT -> NSUBJ 
4. XSUBJ -> NSUBJ 
5. PARTMOD(head, modifier where last 3 
characters are "ing") -> NSUBJ(modifier, head) 
6. PARTMOD(head, modifier where last 3 
characters are "ed") -> DOBJ(modifier, head) 
Expansion: 
1. For APPOS, find all edges that point to the head 
(gene-20) and duplicate those edges, but 
replacing the modifier with the modifier of the 
APPOS relation (kinase-26).  
 
Thus, in the 2nd sentence in PMC-1310901-01-
introduction, ?... leading to expression of a bcr-abl 
fusion gene, an aberrant activated tyrosine kinase, 
....?, there are two existing grammatical relations: 
 
PREP_OF(expression-15, gene-20) 
APPOS(gene-20, kinase-26) 
 
to which this rule adds: 
 
PREP_OF(expression-15, kinase-26) 
157
2.2 Trigger Detection 
We treated trigger detection as a multi-class 
classification problem: each token should be 
annotated with its trigger type or with NONE if it 
was not a trigger. When using the feature set 
detailed below, we found that an SVM 
(Tsochantaridis et al, 2004) outperformed a 
maximum entropy model by a fair margin, though 
the SVM was sensitive to its free parameters. A 
large value of C, the penalty incurred during 
training for misclassifying a data point, was 
necessary to achieve good results. 
2.2.1 Features for Trigger Detection 
Our initial feature set for trigger detection was 
strongly influenced by features that were 
successful in Bj?rne et al, (2009).  
Token Features. We included stems of single 
tokens from the Porter stemmer (Porter, 1980), 
character bigrams and trigrams, a binary indicator 
feature if the token has upper case letters, another 
indicator for the presence of punctuation, and a 
final indicator for the presence of a number. We 
gathered these features for both the current token 
as well as the three immediate neighbors on both 
the left and right hand sides. 
We constructed a gazetteer of possible trigger 
lemmas in the following manner. First we used a 
rule-based morphological analyzer (Heidorn, 2000) 
to identify the lemma of all words in the training, 
development, and test corpora. Next, for each word 
in the training and development sets, we mapped it 
to its lemma. We then computed the number of 
times that each lemma occurred as a trigger for 
each type of event (and none). Lemmas that acted 
as a trigger more than 50% of the time were added 
to the gazetteer. 
During feature extraction for a given token, we 
found the lemma of the token, and then look up 
that lemma in the gazetteer. If found, we included 
a binary feature to indicate its trigger type. 
Frequency Features. We included as features 
the number of entities in the sentence, a bag of 
words from the current sentence, and a bag of 
entities in the current sentence. 
Dependency Features. We used primarily a set 
of dependency chain features that were helpful in 
the past (Bj?rne et al, 2009); these features walk 
the Stanford Typed Dependency edges up to a 
distance of 3. 
We also found it helpful to have features about 
the path to the nearest protein, regardless of 
distance. In cases of multiple shortest paths, we 
took only one, exploring the dependency tree 
generally in left to right order. For each potential 
trigger, we looked at the dependency edge labels 
leading to that nearest protein. In addition we had a 
feature including both the dependency edge labels 
and the token text (lowercased) along that path. 
Finally, we had a feature indicating whether some 
token along that path was also in the trigger 
gazetteer. The formulation of this set of features is 
still not optimal especially for the ?binding? events 
as the training data will include paths to more than 
one protein argument.  Nevertheless, in Table 3, 
 
Key Relation Value Key Relation Value 
quantities child(left, NNS?JJ) measurable measurable child-1(left, NNS?JJ) quantities 
found child(after, VBN?NNS) hours hours child-1(after, VBN?NNS) found 
found child(after, VBN?NN) ingestion ingestion child-1(after, VBN?NN) found 
 
Figure 2: A sample PubMed sentence along with its dependency parse, and some key/relation/value triples 
extracted from that parse for computation of distributional similarity. Keys with a similar distribution of values 
under the same relation are likely semantically related. Inverse relations are indicated with a superscript -1. 
Prepositions are handled specially: we add edges labeled with the preposition from its parent to each child 
(indicated by dotted edges). 
158
we can see that this set of features contributed to 
improved precision. 
Cluster Features. Lexical and stem features 
were crucial for accuracy, but were unfortunately 
sparse and did not generalize well. To mitigate 
this, we incorporated word cluster features. In 
addition to the lexical item and the stem, we added 
another feature indicating the cluster to which each 
word belongs. To train clusters, we downloaded all 
the PubMed abstracts (http://pubmed.gov), parsed 
them with a simple dependency parser (a 
reimplementation of McDonald, 2006 trained on 
the GENIA corpus), and extracted dependency 
relations to use in clustering: words that occur in 
similar contexts should fall into the same cluster. 
An example sentence and the relations that were 
extracted for distributional similarity computation 
are presented in Figure 2. We ran a distributional 
similarity clustering algorithm (Pantel et al, 2009) 
to group words into clusters. 
Tfidf features. This set of features was intended 
to capture the salience of a term in the medical and 
?general? domain, with the aim of being able to 
distinguish domain-specific terms from more 
ambiguous terms. We calculated the tf.idf score for 
each term in the set of all PubMed abstracts and 
did the same for each term in Wikipedia. For each 
token in the input data, we then produced three 
features: (i) the tf.idf value of the token in PubMed 
abstracts, (ii) the tf.idf value of the token in 
Wikipedia, and (iii) the delta between the two 
values. Feature values were rounded to the closest 
integer. We found, however, that adding these 
features did not improve results. 
2.2.2 Feature combination and reduction 
We experimented with feature reduction and 
feature combination within the set of features 
described here. For feature reduction we tried a 
number of simple approaches that typically work 
well in text classification. The latter is similar to 
the task at hand, in that there is a very large but 
sparse feature set. We tried two feature reduction 
methods: a simple count cutoff, and selection of 
the top n features in terms of log likelihood ratio 
(Dunning, 1993) with the target values. For a count 
cutoff, we used cutoffs from 3 to 10, but we failed 
to observe any consistent gains. Only low cutoffs 
(3 and occasionally 5) would ever produce any 
small improvements on the development set. Using 
log likelihood ratio (as determined on the training 
set), we reduced the total number of features to 
between 10,000 and 75,000. None of these 
experiments improved results, however. One 
potential reason for this negative result may be that 
there were a lot of features in our set that capture 
the same phenomenon in different ways, i.e. which 
correlate highly. By retaining a subset of the 
original feature set using a count cutoff or log 
likelihood ratio we did not reduce this feature 
overlap in any way. Alternative feature reduction 
methods such as Principal Component Analysis, on 
the other hand, would target the feature overlap 
directly. For reasons of time we did not experiment 
with other feature reduction techniques but we 
believe that there may well be a gain still to be had. 
For our feature combination experiments the 
idea was to find highly predictive Boolean 
combinations of features. For example, while the 
features a and b may be weak indicators for a 
particular trigger, the cases where both a and b are 
present may be a much stronger indicator. A linear 
classifier such as the one we used in our 
experiments by definition is not able to take such 
Boolean combinations into account. Some 
classifiers such as SVMs with non-linear kernels 
do consider Boolean feature combinations, but we 
found the training times on our data prohibitive 
when using these kernels. As an alternative, we 
decided to pre-identify feature combinations that 
are predictive and then add those combination 
features to our feature inventory. In order to pre-
identify feature combinations, we trained decision 
tree classifiers on the training set, and treated each 
path from the root to a leaf through the decision 
tree classifier as a feature combination. We also 
experimented with adding all partial paths through 
the tree (as long as they started from the root) in 
addition to adding all full paths. Finally, we tried 
to increase the diversity of our combination 
features by using a ?bagging? approach, where we 
trained a multitude of decision trees on random 
subsets of the data. Again, unfortunately, we did 
not find any consistent improvements. Two 
observations that held relatively consistently across 
our experiments with combination features and 
different feature sets were: (i) only adding full 
paths as combination features sometimes helped, 
while adding partial paths did not, and (ii) bagging 
hardly ever led to improvements. 
159
2.3 Edge Detection 
This phase of the pipeline was again modeled as 
multi-class classification. There could be an edge 
originating from any trigger word and ending in 
any trigger word or protein. Looking at the set of 
all such edges, we trained a classifier to predict the 
label of this edge, or NONE if the edge was not 
present. Here we found that a maximum entropy 
classifier performed somewhat better than an SVM, 
so we used an in-house implementation of a 
maximum entropy trainer to produce the models. 
2.3.1 Features for Edge Detection 
As with trigger detection, our initial feature set for 
edge detection was strongly influenced by features 
that were successful in Bj?rne et al (2009). 
Additionally, we included the same dependency 
path features to the nearest protein that we used for 
trigger detection, described in 2.2.1. Further, for a 
prospective edge between two entities, where the 
entities are either a trigger and a protein, or a 
trigger and a second trigger, we added a feature 
that indicates (i) if the second entity is in the path 
to the nearest protein, (ii) if the head of the second 
entity is in the path to the nearest protein, (iii) the 
type of the second entity.   
2.4 Post-processing 
Given the set of edges, we used a simple 
deterministic procedure to produce a set of events. 
This step is not substantially different from that 
used in prior systems (Bj?rne et al, 2009). 
2.4.1 Balancing Precision and Recall 
As in Bj?rne et al (2009), we found that the trigger 
detector had quite low recall. Presumably this is 
due to the severe class imbalance in the training 
data: less than 5% of the input tokens are triggers. 
Thus, our classifier had a tendency to overpredict 
NONE. We tuned a single free parameter ? ? ?? 
(the ?recall booster?) to scale back the score 
associated with the NONE class before selecting 
the optimal class. The value was tuned for whole-
system F-measure; optimal values tended to fall in 
the range 0.6 to 0.8, indicating that only a small 
shift toward recall led to the best results. 
  Development Set  Test Set 
Event Class Count Recall Precision F1 Count Recall Precision F1 
Gene_expression 749 76.37 81.46 78.83 1002 73.95 73.22 73.58 
Transcription 158 49.37 73.58 59.09 174 41.95 65.18 51.05 
Protein_catabolism 23 69.57 80.00 74.42 15 46.67 87.50 60.87 
Phosphorylation 111 73.87 84.54 78.85 185 87.57 81.41 84.37 
Localization 67 74.63 75.76 75.19 191 51.31 79.03 62.22 
=[SVT-TOTAL]= 1108 72.02 80.51 76.03 1567 68.99 74.03 71.54 
Binding 373 47.99 50.85 49.38 491 42.36 40.47 41.39 
=[EVT-TOTAL]= 1481 65.97 72.73 69.18 2058 62.63 65.46 64.02 
Regulation 292 32.53 47.05 38.62 385 24.42 42.92 31.13 
Positive_Regulation 999 38.74 51.67 44.28 1443 37.98 44.92 41.16 
Negative_Regulation 471 35.88 54.87 43.39 571 41.51 42.70 42.10 
=[REG-TOTAL]= 1762 36.95 51.79 43.13 2399 36.64 44.08 40.02 
ALL-Total 3243 50.20 62.60 55.72 4457 48.64 54.71 51.50 
Table 1: Approximate span matching/approximate recursive matching on development and test data 
sets for GENIA Shared Task -1 with our system. 
Trigger 
Detection 
Features 
Trigger 
Loss Recall Prec. F1 
B 2.14 48.44 64.08 55.18 
B + TI 2.14 48.17 62.49 54.40 
B + TI + C 2.14 50.32 60.90 55.11 
B + TI + C + PI 2.03 50.20 62.60 55.72 
B + TI + C + PI 
+D 
2.02 49.21 62.75 55.16 
Table 2: Recall/Precision/F1 on the GENIA 
development set using MCCC-I + Enju parse; 
adding different features for Trigger Detection. 
B = Base set Features, TI = Trigger inflect 
forms, 
160
3 Results 
Of the five evaluation tracks in the shared task, we 
participated in two: the GENIA core task, and the 
EPI (Epigenetics and Post-translational 
modifications) task. The systems used in each track 
were substantially similar; differences are called 
out below. Rather than building a system 
customized for a single trigger and event set, our 
goal was to build a more generalizable framework 
for event detection. 
3.1 GENIA Task 
Using F-measure performance on the development 
set as our objective function, we trained the final 
system for the GENIA task with all the features 
described in section 2, but without the conversion 
rules and without either feature combination or 
reduction. Furthermore, we trained the cluster 
features using the full set of PubMed documents 
(as of  January 2011). The results of our final 
submission are summarized in Table 1. Overall, we 
saw a substantial degradation in F-measure when 
moving from the development set to the test set, 
though this was in line with past experience from 
our and other systems.  
We compared the results for different parsers in 
Table 3. MCCC-I is not better in isolation but does 
produce higher F-measures in combination with 
other parsers. Although posteriors were not 
particularly helpful on the development set, we ran 
Parser 
SVT-Total Binding REG-Total All-Total 
Recall Prec. F1 Recall Prec. F1 Recall Prec. F1 Recall Prec. F1 
MCCC 70.94 82.72 76.38 45.04 55.26 49.63 34.39 51.88 41.37 48.10 64.39 55.07 
MCCC-I 68.59 82.59 74.94 42.63 58.67 49.38 32.58 52.76 40.28 46.06 65.50 54.07 
Enju 71.66 82.18 76.56 40.75 51.01 45.31 32.24 49.39 39.01 46.69 62.70 53.52 
MCCC-I + 
Posteriors 
70.49 78.87 74.44 47.72 51.59 49.58 35.64 50.40 41.76 48.94 61.47 54.49 
MCCC + 
Enju 
71.84 82.04 76.60 44.77 53.02 48.55 34.96 53.15 42.18 48.69 64.59 55.52 
MCCC-I + 
Enju 
72.02 80.51 76.03 47.99 50.85 49.38 36.95 51.79 43.13 50.20 62.60 55.72 
Table 3: Comparison of Recall/Precision/F1 on the GENIA Task-1 development set using various 
combinations of parsers: Enju, MCCC (Mc-Closky Charniak), and MCCC-I (Mc-Closky Charniak 
Improved self-trained biomedical parsing model) with Stanford collapsed dependencies were used for 
evaluation. Results on Simple, Binding and Regulation and all events are shown. 
 
  Development Set  Test Set 
Event Class Count Recall Precision F1 Count Recall Precision F1 
Hydroxylation 31 25.81 61.54 36.36 69 30.43 84.00 44.68 
Dehydroxylation 0 100.00 100.00 100.00 0 100.00 100.00 100.00 
Phosphorylation 32 71.88 85.19 77.97 65 72.31 85.45 78.33 
Dephosphorylation 1 0.00 0.00 0.00 4 0.00 0.00 0.00 
Ubiquitination 76 63.16 75.00 68.57 180 67.78 81.88 74.16 
Deubiquitination 8 0.00 0.00 0.00 10 0.00 0.00 0.00 
DNA_methylation 132 72.73 72.18 72.45 182 71.43 73.86 72.63 
DNA_demethylation 9 0.00 0.00 0.00 6 0.00 0.00 0.00 
Glycosylation 70 61.43 67.19 64.18 169 39.05 69.47 50.00 
Deglycosylation 7 0.00 0.00 0.00 12 0.00 0.00 0.00 
Acetylation 65 89.23 75.32 81.69 159 87.42 85.28 86.34 
Deacetylation 19 68.42 92.86 78.79 24 62.50 93.75 75.00 
Methylation 65 64.62 75.00 69.42 193 62.18 73.62 67.42 
Demethylation 7 0.00 0.00 0.00 10 0.00 0.00 0.00 
Catalysis 60 3.33 15.38 5.48 111 4.50 33.33 7.94 
====[TOTAL]==== 582 57.22 72.23 63.85 1194 55.70 77.60 64.85 
Table 4: Approximate span matching/approximate recursive matching on development and test data 
sets for EPI CORE Task with our system 
161
a system consisting of MCCC-I with posteriors 
(MCCC-I + Posteriors) on the test set after the 
final results were submitted, and found that it was 
competitive with our submitted system (MCCC-I + 
ENJU). We believe that ambiguity preservation 
has merit, and hope to explore more of this area in 
the future. Diversity is important: although the 
ENJU parser alone was not the best, combining it 
with other parsers led to consistently strong results.  
Table 2 explores feature ablation: TI appears to 
degrade performance, but clusters regain that loss. 
Protein depth information was helpful, but 
dependency rule conversion was not.  Therefore 
the B+TI+C+PI combination was our final 
submission on GENIA.  
3.2 EPI Task 
We trained the final system for the Epigenetics 
task with all the features described in section 2. 
Further, we produced the clusters for the 
Epigenetics task using only the set of GENIA 
documents provided in the shared task. 
In contrast to GENIA, we found that the 
dependency rule conversions had a positive impact 
on development set performance. Therefore, we 
included them in the final system. Otherwise the 
system was identical to the GENIA task system.  
4 Discussion 
After two rounds of the BioNLP shared task, in 
2009 and 2011, we wonder whether it might be 
possible to establish an upper-bound on recall and 
precision. There is considerable diversity among 
the participating systems, so it would be interesting 
to consider whether there are some annotations in 
the development set that cannot be predicted by 
any of the participating systems1. If this is the case, 
then those triggers and edges would present an 
interesting topic for discussion. This might result 
either in a modification of the annotation protocols, 
or an opportunity for all systems to learn more. 
After a certain amount of feature engineering, 
we found it difficult to achieve further 
improvements in F1. Perhaps we need a significant 
shift in architecture, such as a shift to joint 
inference (Poon and Vanderwende, 2010). Our 
system may be limited by the pipeline architecture. 
                                                          
1 Our system output for the 2011development set can be 
downloaded from http://research.microsoft.com/bionlp/ 
MWEs (multi-word entities) are a challenge. 
Better multi-word triggers accuracy may improve 
system performance. Multi-word proteins often led 
to incorrect part-of-speech tags and parse trees. 
Cursory inspection of the Epigenetics task 
shows that some domain-specific knowledge 
would have been beneficial. Our system had 
significant difficulties with the rare inverse event 
types, e.g. ?demethylation? (e.g., there are 319 
examples for ?methylation? in the combined 
training/development set, but only 12 examples for 
?demethylation?). Each trigger type was treated 
independently, thus we did not share information 
between an event and its related inverse event type. 
Furthermore, our system also failed to identify 
edges for these rare events. One approach would 
be to share parameters between types that differ 
only in a prefix, e.g., ?de?. In general, some 
knowledge about the hierarchy of events may let 
the learner generalize among related events. 
5 Conclusion and Future Work 
We have described a system designed for fine-
grained information extraction, which we show to 
be general enough to achieve good performance 
across different sets of event types and domains.  
The only domain-specific characteristic is the pre-
annotation of proteins as a special class of entities. 
We formulated some features based on this 
knowledge, for instance the path to the nearest 
protein.  This would likely have analogues in other 
domains, given that there is often a special class of 
target items for any Information Extraction task. 
As the various systems participating in the 
shared task mature, it will be viable to apply the 
automatic annotations in an end-user setting.  
Given a more specific application, we may have 
clearer criteria for balancing the trade-off between 
recall and precision.  We expect that fully-
automated systems coupled with reasoning 
components will need very high precision, while 
semi-automated systems, designed for information 
visualization or for assistance in curating 
knowledge bases, could benefit from high recall.  
We believe that the data provided for the shared 
tasks will support system development in either 
direction. As mentioned in our discussion, though, 
we find that improving recall continues to be a 
major challenge. We seek to better understand the 
data annotations provided. 
162
Our immediate plans to improve our system 
include semi-supervised learning and system 
combination.  We will also continue to explore 
new levels of linguistic representation to 
understand where they might provide further 
benefit.  Finally, we plan to explore models of joint 
inference to overcome the limitations of pipelining 
and deterministic post-processing. 
Acknowledgments 
We thank the shared task organizers for providing 
this interesting task and many resources, the Turku 
BioNLP group for generously providing their 
system and intermediate data output, and Patrick 
Pantel and the MSR NLP group for their help and 
support. 
References  
Jari Bj?rne, Juho Heimonen, Filip Ginter, Antti Airola, 
Tapio Pahikkala and Tapio Salakoski. 2009. 
Extracting Complex Biological Events with Rich 
Graph-Based Feature Sets. In Proceedings of  the 
Workshop on BioNLP: Shared Task. 
Thomas Cormen, Charles Leiserson, and Ronald Rivest. 
2002. Introduction to Algorithms. MIT Press. 
Ted Dunning. 1993. Accurate methods for the statistics 
of surprise and coincidence. Computational 
Linguistics, 19(1), pp. 61-74. 
George E. Heidorn, 2000. Intelligent Writing 
Assistance. In Handbook of Natural Language 
Processing, ed. Robert Dale, Hermann Moisl, and 
Harold Somers.  Marcel Dekker Publishers. 
Karen Jensen. 1993. PEGASUS: Deriving Argument 
Structures after Syntax. In Natural Language 
Processing: the PLNLP approach, ed. Jensen, K., 
Heidorn, G.E., and Richardson, S.D. Kluwer 
Academic Publishers. 
Marie-Catherine de Marneffe, Bill MacCartney and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. In 
LREC 2006. 
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest 
models for probabilistic HPSG parsing. 
Computational Linguistics 34(1): 35-80. 
David McClosky and Eugene Charniak. 2008.  Self-
Training for Biomedical Parsing. In Proceedings of 
the Association for Computational Linguistics 2008. 
David McClosky. 2010. Any Domain Parsing: 
Automatic Domain Adaptation for Natural Language 
Parsing. Ph.D. thesis, Department of Computer 
Science, Brown University.  
Ryan McDonald. 2006. Discriminative training and 
spanning tree algorithms for dependency parsing. Ph. 
D. Thesis. University of Pennsylvania. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based Translation.  In Proceedings of ACL 2008, 
Columbus, OH. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. In 
Proceedings of EMNLP 2009.  
Hoifung Poon and Lucy Vanderwende. 2010. Joint 
inference for knowledge extraction from biomedical 
literature. In Proceedings of NAACL-HLT 2010. 
Martin.F. Porter, 1980, An algorithm for suffix 
stripping, Program, 14(3):130?137. 
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten 
Joachims, and Yasemin Alton. 2004. Support vector 
machine learning for interdependent and structured 
output spaces. In ICML 2004. 
163
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 47?51,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Annotating Change of State for Clinical Events  
 
Lucy Vanderwende 
One Microsoft Way 
Redmond, WA 98052 
lucyv@microsoft.com 
 
Fei Xia 
University of Washington 
Seattle, WA 98195 
fxia@uw.edu 
 
Meliha Yetisgen-Yildiz 
University of Washington 
Seattle, WA 98195 
melihay@uw.edu 
 
 
 
 
 
 
 
Abstract 
Understanding the event structure of sentences 
and whole documents is an important step in 
being able to extract meaningful information 
from the text. Our task is the identification of 
phenotypes, specifically, pneumonia, from 
clinical narratives. In this paper, we consider 
the importance of identifying the change of 
state for events, in particular, events that 
measure and compare multiple states across 
time. Change of state is important to the clini-
cal diagnosis of pneumonia; in the example 
?there are bibasilar opacities that are un-
changed?, the presence of bibasilar opacities 
alone may suggest pneumonia, but not when 
they are unchanged, which suggests the need 
to modify events with change of state infor-
mation. Our corpus is comprised of chest X-
ray reports, where we find many descriptions 
of change of state comparing the volume and 
density of the lungs and surrounding areas. 
We propose an annotation schema to capture 
this information as a tuple of <location, attrib-
ute, value, change-of-state, time-reference>. 
1 Introduction 
The narrative accompanying chest X-rays contains 
a wealth of information that is used to assess the 
health of a patient. X-rays are obviously a single 
snapshot in time, but the X-ray report narrative 
often makes either explicit or, more often, implicit 
reference to a previous X-ray. In this way, the se-
quence of X-ray reports is used not only to assess a 
patient?s health at a moment in time but also to 
monitor change.  Phenotypes such as pneumonia 
are consensus-defined diseases, which means that 
the diagnosis is typically established by human 
inspection of the data rather than by means of a 
test.  Our recent efforts have focused on building a 
phenotype detection system. In order to train and 
evaluate the system, we asked medical experts to 
annotate the X-ray report with phenotype labels 
and to highlight the text snippets in the X-ray re-
port that supported their phenotype labeling. 
   Analysis of the text snippets that support the la-
beling of pneumonia and the Clinical Pulmonary 
Infection Score (CPIS) reveal that most of these 
snippets mention a change of state or the lack of a 
change of state (i.e. persistent state).  This is un-
derstandable given our task, which is to monitor 
patients for ventilator associated pneumonia 
(VAP), which can develop over time as a patient is 
kept on a ventilator for medical reasons. 
   Change of state (COS) is most often understood 
as an aspectual difference that is reflected in verb 
morphology (Comrie, 1976), where a state is de-
scribed as initiating, continuing or terminating (see 
also Quirk et al, 1973, Section 3.36). In our cor-
pus, however, COS is often reflected not in verbs, 
but more frequently in nouns. A careful analysis of 
our data indicates that the states expressed as 
nouns don?t have the traditional aspects but rather 
exhibit COS more closely associated with compar-
atives, as they are susceptible to subjective and to 
objective measurement (Quirk et al, 1973, Section 
5.38).  These events compare two states across 
time or comparing one state against an accepted 
norm. Monitoring the state of the patient, and 
47
therefore comparing current state with previous 
states, is of paramount importance in the clinical 
scenario. We therefore propose in this paper to ex-
pand the annotation of COS to include the compar-
ison of states over time. 
2 The Task  
Early detection and treatment of ventilator associ-
ated pneumonia (VAP) is important as it is the 
most common healthcare-associated infection in 
critically ill patients. Even short?term delays in 
appropriate antibiotic therapy for patients with 
VAP are associated with higher mortality rates, 
longer?term mechanical ventilation, and excessive 
hospital costs. Interpretation of meaningful infor-
mation from the electronica medical records at the 
bedside is complicated by high data volume, lack 
of integrated data displays and text-based clinical 
reports that can only be reviewed by manual 
search. This cumbersome data management strate-
gy obscures the subtle signs of early infection.  
   Our research goal is to build NLP systems that 
identify patients who are developing critical ill-
nesses in a manner timely enough for early treat-
ment. As a first step, we have built a system that 
determines whether a patient has pneumonia based 
on the patient?s chest X-ray reports; see Figure 1 
for an example. 
 
 
01 CHEST, PORTABLE 1 VIEW 
02 INDICATION: 
03 Shortness of breath 
04 COMPARISON: July 16 10 recent prior 
05 FINDINGS: 
06 Left central line, tip at mid-SVC. 
07 Cardiac and mediastinal contours as before 
08 No pneumothorax. 
09 Lungs: Interval increase in right lung base  
10 pulmonary opacity with air bronchograms,  
11 increasing  pneumonitis / atelectasis. 
 
Figure 1. Sample chest X-ray report 
 
2.1 Annotation 
To train and evaluate the system, we created a cor-
pus of 1344 chest X-ray reports from our institu-
tion (Xia and Yetisgen-Yildiz, 2012). Two 
annotators, one a general surgeon and the other  a 
data analyst in a surgery department, read each 
report and determined whether the patient has 
pneumonia (PNA) and also what the clinical pul-
monary infection score (CPIS) is for the patient. 
The CPIS is used to assist in the clinical diagnosis 
of VAP by predicting which patients will benefit 
from obtaining pulmonary cultures, an invasive 
procedure otherwise avoided. There are three pos-
sible labels for PNA: (2a) no suspicion (negative 
class), (2b) suspicion of PNA, and (2c) probable 
PNA (positive class). Likewise, there are three la-
bels for CPIS: (1a) no infiltrate, report can include 
mention of edema or pleural effusion, (1b) diffuse 
infiltrate or atelectasis (i.e. reduced lung volume), 
and (1c) localized infiltrate, where one opacity is 
specifically highlighted and either PNA or infec-
tion is also mentioned. 
   In addition to the labels, we also asked the anno-
tators to highlight the text snippet they used to as-
sign the CPIS and PNA categories to reports (see 
(Yu et al, 2011) for similar approach to capturing 
expert knowledge). Thus, the snippets represent the 
support found for the CPIS and PNA label deter-
mination. The snippet found in lines 9-11, in figure 
1, for example, was support for both the CPIS (1c) 
and the PNA label (2c). 
2.2 Preliminary Results 
We used this corpus to train two SVM classifiers, 
one for CPIS and the other for PNA, and evaluated 
them using 5-fold cross validation (for details, see 
Tepper et al, 2013). The micro F1-score of the 
CPIS classifier was 85.8% with unigram features 
and 85.2% with unigram+bigram features. The 
micro F1-score of the PNA classifier was 78.5% 
with unigrams and 78.0% with unigram+bigrams.  
   We analyzed errors made by the CPIS and PNA 
classifiers and observed that many of them were 
due to lack of in-depth semantic analysis of text. 
Consider the snippet ?The previously noted right 
upper lobe opacity consistent with right upper lobe 
collapse has resolved?, which is labeled in the gold 
standard 1A (no infiltrate). The system mislabeled 
it 1C, (localized infiltrate), because the snippet 
supports 1C entirely up until the crucial words ?has 
resolved?. This error analysis motivated the clini-
cal event annotation task described in this paper. 
3 Change of State for Clinical Events 
In our data, clinically relevant events are often ex-
pressed as nouns. A text that mentions ?a clear 
48
lung?, for instance, implicitly describes the event 
of checking the lung density for that patient and 
finding it to be clear1. The TimeML annotation 
guidelines (Saur? et al, 2012) specify that states 
are to be annotated when they ?identifiably change 
over the course of a document being marked up?.  
In our scenario, where the document is the collec-
tion of the patient?s medical notes during hospital 
stay, a noun phrase such as ?lung capacity? is then 
a state that can certainly change over the course of 
the document.  
   Our corpus contains radiology reports and high-
lighted snippets of text where annotators found 
support for their finding. It is noteworthy that these 
snippets frequently describe observations of 
change, either in lung volume or in density. In fact, 
these changes of state (henceforth COS) appear 
more often in these snippets than non-snippets. 
Taking a random sample of 100 snippets, we found 
that 83/100 included some signal for COS, while a 
random sample of 100 non-snippet sentences in-
cluded only 61/100 mentions of COS. 
   Let us consider some examples of snippets in 
which the clinical events, in italics, are referred to 
using nouns, a shorthand for examination / meas-
urement of the noun in question. We have marked 
the signal words expressing a comparison across 
time in bold.  
1. The lungs are clear. 
2. Lungs: No focal opacities. 
3. The chest is otherwise unchanged. 
4. Left base opacity has increased and right 
base opacity persists which could repre-
sent atelectasis, aspiration, or pneumonia. 
Snippets 1 and 2 describe states in the current X-
ray report and do not express a COS. A close look 
at 3 and 4, however, reveals language that indicates 
that the experts are comparing the state in the cur-
rent X-ray with at least one other X-ray for that 
patient and in doing so, are describing a COS. 
Consider the phrases ?otherwise unchanged? in 
snippet 3, and ?increased? and ?persists? in snippet 
                                                          
1 The guidelines for the 2012 i2b2 temporal relation challenge  
define events as ?clinically relevant events and situations, 
symptoms, tests, procedures, ?? (Sun et al, 2013)   
4. Such words signal that the radiologist is examin-
ing more than one report at a time and making 
comparisons across these X-rays, without explicit 
reference to the other X-rays. There are other ex-
amples which exhibit explicit reference, for exam-
ple, snippets 5 and 6, where the signal words and 
the explicit reference are in boldface, and the clini-
cal events in italics: 
5. Bilateral lower lobe opacities are similar 
to those seen on DATE 
6. Since the prior examination lung vol-
umes have diminished    
Previous COS analyses (e.g., (Sun et al, 2013;  
Saur?, 2005)) have largely been limited to an anal-
ysis where events are expressed as verbs, and so is 
usually restricted to aspectual distinctions such as 
start, stop, and continue. In our data, however, 
many of the events are expressed as nouns and so 
we propose to extend the COS analysis to include 
measurements comparing two or more successive 
states and so will include concepts such as more, 
less, and equal2.  
4 Annotating change of state 
While previous event annotation (Uzuner et al, 
2010; Uzuner et al, 2011; Albright et al, 2013) 
marks multiple types of events, temporal expres-
sions, and event relations, our annotation focuses 
on tracking changes in a patient?s medical condi-
tions.  An event in our corpus is represented as a 
(loc, attr, val, cos, ref) tuple, where loc is the ana-
tomical location (e.g., ?lung?), attr is an attribute 
of the location that the event is about (e.g., ?densi-
ty?), val is a possible value for the attribute (e.g., 
?clear?), cos indicates the change of state for the 
attribute value compared to some previous report 
(e.g., ?unchanged?), and ref is a link to the re-
port(s) that the change of state is compared to (e.g., 
?prior examination?). Not all the fields in the tuple 
will be present in an event. When a field is absent, 
either it can be inferred from the context or it is 
unspecified.  
                                                          
2 In English, the morphology provides evidence, though rarely, 
that the comparative is a property of the change of state of an 
adjective. Consider the verb ?redden?, a derived form of the 
adjective ?red?, which means ?to become more red?, combin-
ing the inchoative and comparative (Chris Brockett, pc.) 
49
   The annotations for Snippets 1-6 are as follows: 
a dash indicates that the field is unspecified, and 
<?> indicates the field is unspecified but can be 
inferred from the location and the attribute value. 
For instance, the attribute value clear when refer-
ring to the location lungs implies that the attribute 
being discussed is the density of the lung. 
 
Ex1: (lungs, <density>, clear, -, -) 
Ex2: (lungs, <density>, no focal opacities, -, -)  
Ex3: (chest, -, -, unchanged, -) 
Ex4: (left base, <density>, opacity, increased, -), 
and (right base, <density>, opacity, persists, -) 
Ex5: (Bilateral lower lobe, <density>, opacities, 
similar, DATE) 
Ex6: (lung, volumes, -, diminished, prior examina-
tion) 
 
  A few points are worth noting.  First, the mapping 
from the syntactic structure to fields in event tuples 
is many-to-many. For example, a noun phrase con-
sisting of an adjective and noun may correspond to 
one or more fields in an event tuple. For instance, 
in the NP left base opacity in example 4, left base 
is loc, and opacity is val.  In example 6, the NP 
lung volumes will be annotated with lung as loc 
and volumes as attr, but no val. Similarly, an adjec-
tive can be part of a loc (e.g., bilateral in example 
5), a val (e.g., clear in example 1), or a cos (e.g., 
unchanged in example 3). Finally, the cos field 
may also be filled by a verb (e.g., increase and 
persists, in example 4). Making such distinctions 
will not be easy, especially for annotators with no 
medical training.  
    Second, events often have other attributes such 
as polarity (positive or negative) and modality 
(e.g., factual, conditional, possible). Most events in 
X-ray reports are positive and factual. We will add 
those attributes to our representations if needed. 
5 Summary 
Annotating events in a general domain without 
targeting a particular application can be challeng-
ing because it is often not clear what should be 
marked as an event. Our annotation focuses on the 
marking of COS in medical reports because COS is 
an important indicator of the patient?s medical 
condition. We propose to extend COS analysis to 
include comparison of state over time.  
   We are currently annotating a corpus of X-ray 
reports with the COS events. Once the corpus is 
complete, we will use it to train a system to detect 
such events automatically. The events identified by 
the event detector will then be used as features for 
phenotype detection. We expect that the COS fea-
tures will improve phenotype detection accuracy, 
in the same way that using features that encode 
negation and assertion types improves classifica-
tion results as demonstrated by Bejan et al (2012).  
    Our ultimate goal is to use event detection, phe-
notype detection, and other NLP systems to moni-
tor patients? medical conditions over time and 
prompt physicians with early warning, and thus 
improve patient healthcare quality while reducing 
the overall cost of healthcare. 
Acknowledgments 
We wish to thank the anonymous reviewers for 
their comments and also our colleagues Heather 
Evans at UW Medicine, and Michael Tepper, 
Cosmin Bejan and Prescott Klassen at the Univer-
sity of Washington. This work is funded in part by 
Microsoft Research Connections and University of 
Washington Research Royalty Fund. 
References  
Daniel Albright, Arrick Lanfranchi, Anwen Fredriksen, 
William F. Styler IV, Colin Warner, Jena D. Hwang, 
Jinho D. Choi, Dmitry Dligach, Rodney D. Nielsen, 
James Martin, Wayne Ward, Martha Palmer, and 
Guergana K. Savova. 2013. Towards comprehensive 
syntactic and semantic annotations of the clinical 
narrative. Journal of American Medical Informatics 
Association (JAMIA). [Epub ahead of print]. 
Cosmin A. Bejan, Lucy Vanderwende, Fei Xia, and 
Meliha Yetisgen-Yildiz. 2013. Assertion modeling 
and its role in clinical phenotype identification. Jour-
nal of Biomedical Informatics, 46(1):68-74. 
Bernard Comrie. 1976. Aspect.  Cambridge Textbooks 
in Linguistics. 
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, 
and Jan Svartvik, 1973. A Grammar of Contempo-
rary English. . Longman Group Ltd, London 
Roser Saur?, Jessica Littman, Bob Knippen, Robert Gai-
zauskas, Andrea Setzer, and James Pustejovsky. 
2005. TimeML Annotation Guidelines Version 1.2.1. 
Manuscript, Available at 
http://www.timeml.org/site/publications/specs.html 
Weiyi Sun, Anna Rumshisky, Ozlem Uzuner. 2013. 
Evaluating temporal relations in clinical text: 2012 
i2b2 Challenge. In Journal of the American Medical 
50
Informatics Association (JAMIA). Published Online 
First: 5 April 2013 10.1136/amiajnl-2013-001628.  
Michael Tepper, Heather. Evans, Fei Xia, and Meliha 
Yetisgen-Yildiz. 2013. Modeling Annotator Ration-
ales with Application to Pneumonia Classification. In 
Proceedings of Expanding the Boundaries of Health 
Informatics Using AI Workshop in conjunction with  
AAAI'2013.  
?zlem Uzuner, Imre Solti, Fei Xia, and Eithon Cadag. 
2010. Community annotation experiment for ground 
truth generation for the i2b2 medication challenge. 
Journal of American Medical Informatics Associa-
tion (JAMIA), 17(5):519-23.  
?zlem Uzuner, Brent R. South, Shuying Shen, and Scott 
L. DuVall. 2011. 2010 i2b2/VA challenge on con-
cepts, assertions, and relations in clinical text. Jour-
nal of American Medical Informatics Association 
(JAMIA), 18(5):552-556. 
Fei Xia and Meliha Yetisgen-Yildiz. 2012. Clinical 
corpus annotation: challenges and strategies. In Pro-
ceedings of the Third Workshop on Building and 
Evaluating Resources for Biomedical Text Mining 
(BioTxtM'2012) in conjunction with the International 
Conference on Language Resources and Evaluation 
(LREC), Istanbul, Turkey. 
Shipeng Yu, Faisal Farooq, Balaji Krishnapuram, and 
Bharat Rao.  2011. Leveraging Rich Annotations to 
Improve Learning of Medical Concepts from Clinical 
Free Text. In Proceedings of the ICML workshop on 
Learning from Unstructured Clinical Text. Bellevue, 
WA. 
51
