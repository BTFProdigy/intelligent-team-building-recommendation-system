Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education ?LaTeCH ? SHELT&R 2009, pages 60?68,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Instance-driven Discovery of Ontological Relation Labels
Marieke van Erp, Antal van den Bosch, Sander Wubben, Steve Hunt
ILK Research Group
Tilburg centre for Creative Computing
Tilburg University
The Netherlands
{M.G.J.vanErp,Antal.vdnBosch,S.Wubben,S.J.Hunt}@uvt.nl
Abstract
An approach is presented to the auto-
matic discovery of labels of relations be-
tween pairs of ontological classes. Using
a hyperlinked encyclopaedic resource, we
gather evidence for likely predicative la-
bels by searching for sentences that de-
scribe relations between terms. The terms
are instances of the pair of ontological
classes under consideration, drawn from
a populated knowledge base. Verbs or
verb phrases are automatically extracted,
yielding a ranked list of candidate rela-
tions. Human judges rate the extracted
relations. The extracted relations provide
a basis for automatic ontology discovery
from a non-relational database. The ap-
proach is demonstrated on a database from
the natural history domain.
1 Introduction
The rapid growth in the digitisation of data has
caused many curators, researchers, and data man-
agers of cultural heritage institutions (libraries,
archives, museums) to turn to knowledge man-
agement systems. Using these systems typically
causes them to think about the ontological struc-
ture of their domain, involving the identification
of key classes in object data and metadata fea-
tures, and importantly, their relations. The start-
ing point of this process is often a more classi-
cal ?flat? database matrix model of size n ? m,
where n is the number of collection items, and m
is a fixed number of database columns, typically
denoting object metadata features, as cultural her-
itage institutions are generally well accustomed to
using databases of that type. An ontology can be
bootstrapped from such a database by first assum-
ing that the database columns can be mapped onto
the domain?s ontological classes. The next step
is then to determine which classes are related to
each other, and by which relation. In this paper
we present a method that partially automates this
process.
To gather evidence for a relation to exist be-
tween two ontological classes, it is not possible to
simply look up the classes in text. Rather, classes
are realised typically as a multitude of terms or
phrases. For example, the natural history class
?species? is realised as many different instances of
species names in text. The automatic discovery of
relations between ontological classes thus requires
at least a two-step approach: first, the identifica-
tion of instances of ontological classes in text and
their particular relations, and second, the aggrega-
tion of these analyses in order to find evidence for
a most likely relation.
It is common in ontology construction to use
predicative labels for relations. Although no regu-
lations for label names exist, often a verb or verb
phrase head is taken, optionally combined with a
prepositional head of the subsequent verb-attached
phrase (e. g., ?occurs in?, or ?donated by?). In
this study, we make the assumption that good can-
didate labels are frequent verbs or verb phrases
found between instances from a particular pair of
classes, and that this may sometimes involve a
verb-attached prepositional phrase containing one
of the two terms. In this paper we explore this
route, and present a case study on the discovery
of predicative labels on relations in an ontology
for animal specimen collections. The first step,
identifying instances of ontological classes, is per-
formed by selecting pairs of instances from a flat
n ?m specimen database, in which the instances
60
are organised by the database columns, and there
is a one-to-one relationship between the database
columns and the classes in our ontology.
Any approach that bases itself on text to dis-
cover relations, is dependent on the quality of
that text. In this study we opt for Wikipedia
as a resource from which to extract relations be-
tween terms. Although the status of Wikipedia
as a dependable resource is debated, in part be-
cause of its dynamic nature, there is some evi-
dence that Wikipedia can be as reliable a source
as one that is maintained solely by experts (Giles,
2005). Wikipedia is also an attractive resource due
to its size (currently nearly 12 million articles in
over 250 languages). Additionally, Wikipedia?s
strongly hyperlinked structure closely resembles a
semantic net, with its untyped (but directed) re-
lations between the concepts represented by the
article topics. Since the hyperlinks in Wikipedia
indicate a relations between two encyclopaedia ar-
ticles, we aim at discovering the type of relation
such a link denotes through the use of syntactic
parsing of the text in which the link occurs.
The idea of using Wikipedia for relation ex-
traction is not new (Auer and Lehmann, 2007;
Nakayama et al, 2008; Nguyen et al, 2007;
Suchanek et al, 2006; Syed et al, 2008). How-
ever, most studies so far focus on the structured
information already explicit in Wikipedia, such as
its infoboxes and categories. The main contribu-
tions of our work are that we focus on the in-
formation need emerging from a specific domain,
and that we test a method of pre-selection of sen-
tences to extract relations from. The selection is
based on the assumption that the strongest and
most reliable lexical relations are those expressed
by hyperlinks in Wikipedia pages that relate an ar-
ticle topic to another page (Kamps and Koolen,
2008). The selection procedure retains only sen-
tences in which the topic of the article, identified
by matching words in the article title, links to an-
other Wikipedia article. The benefit of the pre-
selection of sentences is that it reduces the work-
load for the syntactic parser.
Since the system is intentionally kept
lightweight, the extraction of relations from
Wikipedia is sufficiently fast, and we observe that
the results are sufficient to build a basic ontology
from the data. This paper is organised as follows.
In Section 2 we review related work. In Section 3
the data used in this work is described, followed
by the system in Section 4 and an explanation
of how we evaluated the possible relations our
system discovered is presented in Section 5. We
report on the results of our study in Section 6, and
formulate our conclusions and points for further
research in Section 7.
2 Related Work
A key property of Wikipedia is that it is for the
greater part unstructured. On the one hand, ed-
itors are encouraged to supply their articles with
categories. These categories can be subsumed by
broader categories, thus creating a taxonomy-like
structure. On the other hand, editors can link to
any other page in Wikipedia, no matter if it is part
of the same category, or any category at all. An
article can be assigned multiple categories, but the
number of hyperlinks provided in an average arti-
cle typically exceeds the number of categories as-
signed to it.
The free associative hyperlink structure of
Wikipedia is intrinsically different from the hier-
archical top down architecture as seen in Word-
Net, as a hyperlink has a direction, but not a
type. A Wikipedia article can contain any num-
ber of links, pointing to any other Wikipedia arti-
cle. Wikipedia guidelines state however that wik-
ilinks (hyperlinks referring to another Wikipedia
page) should only be added when relevant to the
topic of the article. Due to the fact that most users
tend to adhere to guidelines for editing Wikipedia
pages and the fact that articles are under constant
scrutiny of their viewers, most links in Wikipedia
are indeed relevant (Blohm and Cimiano, 2007;
Kamps and Koolen, 2008).
The structure and breadth of Wikipedia is a po-
tentially powerful resource for information extrac-
tion which has not gone unnoticed in the natu-
ral language processing (NLP) community. Pre-
processing of Wikipedia content in order to ex-
tract non-trivial relations has been addressed in a
number of studies. (Syed et al, 2008) for instance
utilise the category structure in Wikipedia as an
upper ontology to predict concepts common to a
set of documents. In (Suchanek et al, 2006) an
ontology is constructed by combining entities and
relations between these extracted from Wikipedia
through Wikipedia?s category structure and Word-
Net. This results in a large ?is-a? hierarchy, draw-
ing on the basis of WordNet, while further rela-
tion enrichments come from Wikipedia?s category
61
structure. (Chernov et al, 2006) also exploit the
Wikipedia category structure to which concepts in
the articles are linked to extract relations.
(Auer and Lehmann, 2007) take a different ap-
proach in that they focus on utilising the structure
present in infoboxes. Infoboxes are consistently
formatted tables in articles that provide summary
information, such as information about area, pop-
ulation and language for countries, and birth dates
and places for persons. Although infoboxes pro-
vide rich structured information, their templates
are not yet standardised, and their use has not per-
meated throughout the whole of Wikipedia.
Although the category and infobox structures in
Wikipedia already provide a larger coverage at the
concept or term level than for instance WordNet,
they do not express all possibly relevant seman-
tic relations. Especially in specific domains, re-
lations occur that would make the Wikipedia data
structure unnecessarily dense if added, thus an ap-
proach that exploits more of the linguistic content
of Wikipedia is desirable.
Such approaches can be found in (Nakayama et
al., 2008) and (Nguyen et al, 2007). In both works
full sections of Wikipedia articles are parsed, en-
tities are identified, and the verb between the enti-
ties is taken as the relation. They also extract re-
lations that are not backed by a link in Wikipedia,
resulting in common-sense factoids such as ?Bres-
cia is a city?. For a domain specific application
this approach lacks precision. In our approach, we
care more for high precision in finding relations
than for recall; hence, we carefully pre-select on-
tological classes among which relations need to be
found, and use these as filters on our search.
The usefulness of the link structure in
Wikipedia has been remarked upon by (Vo?lkel et
al., 2006). They acknowledge that the link struc-
ture in Wikipedia denotes a potentially meaning-
ful relation between two articles, though the re-
lation type is unknown. They propose an exten-
sion to the editing software of Wikipedia to enable
users to define the type of relation when they add
a link in Wikipedia. Potentially this can enrich
Wikipedia tremendously, but the work involved
would be tremendous as well. We believe some of
the type information is already available through
the linguistic content of Wikipedia.
3 Data Preparation
3.1 Data
The data used in this work comes from a manu-
ally created, non-relational research database of a
collection of reptiles and amphibians at a natural
history museum. The information contained in the
cells describes when a specimen entered the col-
lection, under what circumstances it was collected,
its current location, registration number, etc. We
argue that the act of retrieving information from
this flat database could be enhanced by providing
a meta-structure that describes relations between
the different database columns. If for instance a
relation of the type ?is part of? can be defined be-
tween the database columns province and country,
then queries for specimens found at a particular
location can be expanded accordingly.
Even though the main language of the database
is Dutch, we still chose to use the English
Wikipedia as the resource for retrieval of rela-
tion label candidates. Explicitly choosing the En-
glish Wikipedia has as a consequence that the
relation labels we are bound to discover will
be English phrases. Furthermore, articles in
the English Wikipedia on animal taxonomy have
a broader coverage and are far more elaborate
than those contained in the Dutch Wikipedia.
Since these database values use a Latin-based
nomenclature, using the wider-coverage English
Wikipedia yields a much higher recall than the
Dutch Wikipedia. The values of the other columns
mainly contain proper names, such as person
names and geographic locations and dates, which
are often the same; moreover, English and Dutch
are closely related languages. Different names ex-
ist for different countries in each language, but
here the inconsistency of the database aids us, as
it in fact contains many database entries partially
or fully in English, as well as some in German and
Portuguese.
The database contains 16,870 records in 39
columns. In this work we focus on 20 columns;
the rest are discarded as they are either extrinsic
features not directly pertaining to the object they
describe, e.g., a unique database key, or elaborate
textual information that would require a separate
processing approach. The columns we focus on
describe the position of the specimen in the zo-
ological taxonomy (6 columns), the geographical
location in which it was found (4 columns), some
of its physical properties (3 columns), its collector
62
Column Name Value
Taxonomic Class Reptilia
Taxonomic Order Crocodylia
Amphisbaenia
Taxonomic Genus Acanthophis
Xenobatrachus
Country Indonesia
Suriname
Location city walls
near Lake Mahalona
Collection Date 01.02.1888
02.01.1995
Type holotype
paralectotype
Determinator A. Dubois
M. S. Hoogmoed
Species defined by (Linnaeus, 1758)
(LeSueur, 1827)
Table 1: Example classes from test data
and/or determiner, donator and associated date (4
columns), and other information (3 columns). The
values in most columns are short, often consist-
ing of a single word. Table 1 lists some example
database values.
3.2 Preprocessing
As the database was created manually, it was nec-
essary to normalise spelling errors, as well as
variations on diacritics, names and date formats.
The database values were also stripped of all non-
alphanumeric characters.
In order to find meaningful relations between
two database columns, query pairs are generated
by combining two values occurring together in a
record. This approach already limits the number
of queries applied to Wikipedia, as no relations are
attempted to be found between values that would
not normally occur together. This approach yields
a query pair such as Reptilia Crocodylia from the
taxonomic class and order columns, but not Am-
phibia Crocodylia. Because not every database
field is filled, and some combinations occur more
often, this procedure results in 186,141 query
pairs.
For this study we use a database snapshot of the
English Wikipedia of July 27, 2008. This dump
contains about 2.5 million articles, including a vast
amount of domain-specific articles that one would
typically not find in general encyclopaedias. An
index was built of a subset of the link structure
present in Wikipedia. The subset of links included
in the index is constrained to those links occur-
ring in sentences from each article in which the
main topic of the Wikipedia article (as taken from
the title name) occurs. For example, from the
Wikipedia article on Anura the following sentence
would be included in the experiments1:
The frog is an [[amphibian]] in the order Anura
(meaning ?tail-less?, from Greek an-, without +
oura, tail), formerly referred to as Salientia (Latin
saltare, to jump)
whereas we would exclude the sentence:
An exception is the [[fire-bellied toad]] (Bombina
bombina): while its skin is slightly warty, it
prefers a watery habitat.
This approach limits the link paths to only
those between pages that are probably semanti-
cally strongly connected to each other. In the
following section the computation of the link
paths indicating semantic relatedness between two
Wikipedia pages is explained.
3.3 Computing Semantic Relatedness
Relation discovery between terms (instantiations
of different ontological classes) that have a page
in Wikipedia is best performed after establishing
if a sufficiently strong relation between the two
terms under consideration actually exists. To do
this, the semantic relatedness of those two terms
or concepts needs to be computed first. Seman-
tic relatedness can denote every possible relation
between two concepts, unlike semantic similarity,
which typically denotes only certain hierarchical
relations (like hypernymy and synonymy) and is
often computed using hierarchical networks like
WordNet (Budanitsky and Hirst, 2006).
A simple and effective way of computing se-
mantic relatedness between two concepts c1 and c2
is measuring their distance in a semantic network.
This results in a semantic distance metric, which
can be inversed to yield a semantic relatedness
metric. Computing the path-length between terms
c1 and c2 can be done using Formula 1 where P is
the set of paths connecting c1 to c2 and Np is the
number of nodes in path p.
1The double brackets indicate Wikilinks
63
relpath(c1, c2) = argmaxp?P
1
Np
(1)
We search for shortest paths in a semantic net-
work that is constructed by mapping the concepts
in Wikipedia to nodes, and the links between the
concepts to edges. This generates a very large
network (millions of nodes and tens of millions
of edges), but due to the fact that Wikipedia is
scale-free (Barabasi and Albert, 1999) (its con-
nectedness degree distribution follows a power-
law), paths stay relatively short. By indexing
both incoming and outgoing links, a bidirectional
breadth-first search can be used to find shortest
paths between concepts. This means that the
search is divided in two chains: a forward chain
from c1 and a backward chain to c2. As soon as the
two chains are connected, a shortest path is found.
4 Extracting Relations from Wikipedia
Each query pair containing two values from two
database columns are sent to the system. The sys-
tem processes each term pair in four steps. A
schematic overview of the system is given in Fig-
ure 1.
Indexed 
Wikipedia 
corpus
Term 1 Term 2
Art. 
1
Art. 
2
find path length
...<term 1>...<term 2>...
<term 1><relation><term 2>
extract
if path length == 1
if path 
length == 2
find intermediate 
value in database
Term 1
Term 2
Interm. 
Term 
Interm. 
Term 
if found
Step 4
Step 1
Step 3
Step 2
Figure 1: Schematic overview of the system
Step 1 We look for the most relevant Wikipedia
page for each term, by looking up the term in ti-
tles of Wikipedia articles. As Wikipedia format-
ting requires the article title to be an informative
and concise description of the article?s main topic,
we assume that querying only for article titles will
yield reliable results.
Step 2 The system finds the shortest link path be-
tween the two selected Wikipedia articles. If the
path distance is 1, this means that the two con-
cepts are linked directly to each other via their
Wikipedia articles. This is for instance the case
for Megophrys from the genus column, and Anura
from the order column. In the Wikipedia article on
Megophrys, a link is found to the Wikipedia arti-
cle on Anura. There is no reverse link from Anura
to Megophrys; hierarchical relationships in the zo-
ological taxonomy such as this one are often uni-
directional in Wikipedia as to not overcrowd the
parent article with links to its children.
Step 3 The sentence containing both target con-
cepts as links is selected from the articles.
From the Megophrys article this is for instance
?Megophrys is a genus of frogs, order [[Anura]],
in the [[Megophryidae]] family.?
Step 4 If the shortest path length between two
Wikipedia articles is 2, the two concepts are linked
via one intermediate article. In that case the sys-
tem checks whether the title of the intermediate ar-
ticle occurs as a value in a database column other
than the two database columns in focus for the
query. If this is indeed the case, the two addi-
tional relations between the first term and the in-
termediate article are also investigated, as well as
the second term and that of the intermediate ar-
ticle. Such a bridging relation pair is found for
instance for the query pair Hylidae from the tax-
onomic order column, and Brazil from the coun-
try column. Here, the initial path we find is Hyl-
idae? Sphaenorhynchys? Brazil. We find that
the article-in-the-middle value (Sphaenorhynchys)
indeed occurs in our database, in the taxonomic
genus column. We assume this link is evi-
dence for co-occurrence. Thus, the relevant sen-
tences from the Wikipedia articles on Hylidae
and Sphaenorhynchys, and between articles on
Sphaenorhynchys and Brazil are added to the pos-
sible relations between ?order? ? ?genus? and
?genus? ? ?country?.
Subsequently, the selected sentences are POS-
tagged and parsed using the Memory Based Shal-
low Parser (Daelemans et al, 1999). This parser
provides tokenisation, POS-tagging, chunking,
and grammatical relations such as subject and di-
rect object between verbs and phrases, and is
based on memory-based classification as imple-
mented in TiMBL (Daelemans et al, 2004). The
five most frequently recurring phrases that occur
64
between the column pairs, where the subject of the
sentence is a value from one of the two columns,
are presented to the human annotators. The cut-off
of five was chosen to prevent the annotators from
having to evaluate too many relations and to only
present those that occur more often, and are hence
less likely to be misses. Misses can for instance
be induced by ambiguous person names that also
accidentally match location names (e.g., Dakota).
In Section 7 we discuss methods to remedy this in
future work.
5 Evaluating Relations from Wikipedia
Four human judges evaluated the relations be-
tween the ontological class pairs that were ex-
tracted from Wikipedia. Evaluating semantic rela-
tions automatically is hard, if not impossible, since
the same relation can be expressed in many ways,
and would require a gold standard of some sort,
which for this domain (as well as for many cul-
tural heritage domains) is not available.
The judges were presented with the five highest-
ranked candidate labels per column pair, as well a
longer snippet of text containing the candidate la-
bel, to resolve possible ambiguity. The items in
each list were scored according to the total recip-
rocal rank (TRR) (Radev et al, 2002). For every
correct answer 1/n points are given, where n de-
notes the position of the answer in the ranked list.
If there is more than 1 correct answer the points
will be added up. For example, if in a list of five,
two correct answers occur on positions 2 and 4, the
TRR would be calculated as (1/2 + 1/4) = .75.
The TRR scores were normalised for the number
of relation candidates that were retrieved, as for
some column pairs less than five relation candi-
dates were retrieved.
As an example, for the column pair ?Province?
and ?Genus?, the judges were presented with the
relations shown in Table 2. The direction arrow
in the first column denotes that the ?Genus? value
occurred before the ?Province? value.
The human judges were sufficiently familiar
with the domain to evaluate the relations, and had
the possibility to gain extra knowledge about the
class pairs through access to the full Wikipedia
articles from which the relations were extracted.
Inter-annotator agreement was measured using
Fleiss?s Kappa coefficient (Fleiss, 1971).
6 Results and Evaluation
As expected, between certain columns there are
more relations than between others. In total 140
relation candidates were retrieved directly, and
303 relation label candidates were retrieved via an
intermediate Wikipedia article. We work with the
assumption that these columns have a stronger on-
tological relation than others. For some database
columns we could not retrieve any relations, such
as the ?collection date? field. This is not sur-
prising, as even though Wikipedia contains pages
about dates (?what happened on this day?), it is
unlikely that it would link to such a domain spe-
cific event such as an animal specimen collec-
tion. Relations between instances denoting per-
sons and other concepts in our domain are also not
discovered through this approach. This is due to
the fact that many of the biologists named in the
database do not have a Wikipedia page dedicated
to them, indicating the boundaries of Wikipedia?s
domain specific content. Although not ideal, a
named-entity recognition filter could be applied to
the database after which person names can be re-
trieved from other resources.
Occasionally we retrieve a Wikipedia article for
a value from a person name column, but in most
cases this mistakenly matches with a Wikipedia
article on a location, as last names in Dutch are
often derived from place names. Another problem
induced by incorrect data is the incorrect match
of Wikipedia pages on certain values from the
?Town? and ?Province? columns. Incorrect rela-
tion candidates are retrieved because for instance
the value ?China? occurs in both the ?Town? and
the ?Province? columns. A data cleaning step
would solve these two problems.
From each column pair the highest rated rela-
tion was selected with which we constructed the
ontology displayed in Figure 2. As the figure
shows, the relations that are discovered are not
only ?is a?-relations one would find in strictly hier-
archical resources such as a zoological taxonomy
or geographical resource.
The numbers in the relation labels in Figure 2
denote the average TRR scores given by the four
judges on all relation label candidates that the
judges were presented with for that column pair.
The scores for the relations between the taxo-
nomic classes in our domain were particularly
high, meaning that in many cases all relation can-
didates presented to the judges were assessed as
65
Direction Label Snippet
? is found in is a genus of venomous pitvipers found in Asia from Pakistan, through India,
? is endemic to Cross Frogs) is a genus of microhylid frogs endemic to Southern Philippine,
? are native to are native to only two countries: the United States and
? is known as is a genus of pond turtles also known as Cooter Turtles, especially in the state of
Table 2: Relation candidates for Province and Genus column pair
correct. The inter-annotator agreement was ? =
0.63, which is not perfect, but reasonable. Most
disagreement is due to vague relation labels such
as ?may refer to? as found between ?Province? and
?Country?. If a relation that occurred fewer than 5
times was judged incorrect by the majority of the
judges the relation was not included in Figure 2.
Manual fine-tuning and post-processing of the
results could filter out synonyms such as those
found for relations between ?Town? and other
classes in the domain. This would for instance de-
fine one particular relation label for the relations
?is a town in? and ?is a municipality in? that the sys-
tem discovered between ?Town? and ?Province?
and ?Town? and ?Country?, respectively.
7 Conclusion and Future Work
In this work we have shown that it is possible
to extract ontological relation labels for domain-
specific data from Wikipedia. The main contri-
bution that makes our work different from other
work on relation extraction from Wikipedia is that
the link structure is used as a strong indication of
the presence of a meaningful relation. The pres-
ence of a link is incorporated in our system by only
using sentences from Wikipedia articles that con-
tain links to other Wikipedia articles. Only those
sentences are parsed that contain the two terms we
aim to find a relation between, after which the verb
phrase and possibly the article or preposition fol-
lowing it are selected for evaluation by four human
judges.
The advantage of the pre-selection of content
that may contain a meaningful relation makes our
approach fast, as it is not necessary to parse the
whole corpus. By adding the constraint that at
least one of the query terms should be the sub-
ject of a sentence, and by ranking results by fre-
quency, our system succeeds in extracting correct
and informative relations labels. However, there is
clearly some room for improvement, for instance
in the coverage of more general types of infor-
mation such as dates and person names. For this
we intend to incorporate more domain specific re-
sources, such as research papers from the domain
that may mention persons from our database. We
are also looking into sending queries to the web,
whilst keeping the constraint of hyperlink pres-
ence.
Another factor that may help back up the rela-
tions already discovered is more evidence for ev-
ery relation. Currently we only include sentences
in our Wikipedia corpus that contain the literal
words from the title of the article, to ensure we
have content that is actually about the article and
not a related topic. This causes many sentences
in which the topic is referred to via anaphoric ex-
pressions to be missed. (Nguyen et al, 2007) take
the most frequently used pronoun in the article as
referring to the topic. This still leaves the prob-
lem of cases in which a person is first mentioned
by his/her full name and subsequently only by last
name. Coreference resolution may help to solve
this, although accuracies of current systems for en-
cyclopaedic text are often not much higher than
baselines such as those adopted by (Nguyen et al,
2007).
Errors in the database lead to some noise in
the selection of the correct Wikipedia article. The
queries we used are mostly single-word and two-
word terms, which makes disambiguation hard.
Fortunately, we have access to the class label (i.e.,
the database column name) which may be added
to the query to prevent retrieval of an article about
a country when a value from a person name col-
umn is queried. We would also like to inves-
tigate whether querying terms from a particular
database column to Wikipedia can identify incon-
sistencies in the database and hence perform a
database cleanup. Potentially, extraction of re-
lation labels from Wikipedia articles can also be
used to assign types to links in Wikipedia.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their comments. This research
66
Type
Location
on the island of 
(0.500)
Genus
Order
is a 
(1.000)
Family
Class
is a 
(0.750)
Country
is in 
(0.500)
Species
is a 
(1.000)
is a 
(0.833)
Type Name
Province
occur in 
(0.333)
occur in 
(0.750)
may refer to 
(0.560)
is a 
(0.854)
is found in 
(0.635)
Town
is found in 
(0.566)
is a town in 
(0.794)
may refer to 
(0.482)
is found in 
(0.573)
is a municipality in 
(0.891)
is a town in 
(0.759)
is a 
(1.000)
Figure 2: Graph of relations between columns, with TRR scores in parentheses
was funded as part of the Continuous Access to
Cultural Heritage (CATCH) programme of the
Netherlands Organisation for Scientific Research
(NWO).
References
So?ren Auer and Jens Lehmann. 2007. What have inns-
bruck and leipzig in common? extracting seman-
tics from wiki content. In Franconi et al, editor,
Proceedings of European Semantic Web Conference
(ESWC?07), volume 4519 of Lecture Notes in Com-
puter Science, pages 503?517, Innsbruck, Austria,
June 3 - 7. Springer.
A. L. Barabasi and R. Albert. 1999. Emer-
gence of scaling in random networks. Science,
286(5439):509?512, October.
Sebastian Blohm and Philipp Cimiano. 2007. Using
the web to reduce data sparseness in pattern-based
information extraction. In Proceedings of the 11th
European Conference on Principles and Practice of
Knowledge Discovery in Databases (PKDD), War-
saw, Poland, September. Springer.
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic relat-
edness. Computational Linguistics, 32(1):13?47.
Sergey Chernov, Tereza Iofciu, Wolfgang Nejdl, and
Xuan Zhou. 2006. Extracting semantic relation-
ships between wikipedia categories. In Proceedings
of the First Workshop on Semantic Wikis - From Wiki
to Semantics [SemWiki2006] - at ESWC 2006, pages
153 ? 163, Karlsruhe, Germany, May 15.
Walter Daelemans, Sabine Buchholz, and Jorn Veen-
stra. 1999. Memory-based shallow parsing. In Pro-
ceedings of CoNLL?99, pages 53?60, Bergen, Nor-
way, June 12.
Walter Daelemans, Jakub Zavrel, Ko Van der Sloot,
and Antal Van den Bosch. 2004. Timbl: Tilburg
memory based learner, version 5.1, reference guide.
Technical Report 04-02, ILK/Tilburg University.
J. L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Jim Giles. 2005. Internet encyclopaedias go head to
head. Nature, 438:900?901.
Jaap Kamps and Marijn Koolen. 2008. The impor-
tance of link evidence in wikipedia. In Craig Mac-
donald, Iadh Ounis, Vassilis Plachouras, Ian Rutven,
and Ryen W. White, editors, Advances in Infor-
mation Retrieval: 30th European Conference on
IR Research (ECIR 2008), volume 4956 of Lecture
Notes in Computer Science, pages 270?282, Glas-
gow, Scotland, March 30 - April 3. Springer Verlag.
Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio.
2008. Wikipedia link structure and text mining for
semantic relation extraction towards a huge scale
global web ontology. In Proceedings of Sem-
Search 2008 CEUR Workshop, pages 59?73, Tener-
ife, Spain, June 2.
Dat P. T. Nguyen, Yutaka Matsuo, and Mitsuru
Ishizuka. 2007. Exploiting syntactic and semantic
information for relation extraction from wikipedia.
In Proceedings of Workshop on Text-Mining & Link-
Analysis (TextLink 2007) at IJCAI 2007, pages
1414?1420, Hyderabad, India, January 7.
67
Dragomir R. Radev, Hong Q, Harris Wu, and Weiguo
Fan. 2002. Evaluating web-based question answer-
ing systems. In Demo section, LREC 2002, Las Pal-
mas, Spain, June.
F. M. Suchanek, G. Ifrim, and G. Wiekum. 2006.
Leila: Learning to extract information by linguistic
analysis. In Proceedings of the ACL-06 Workshop
on Ontology Learning and Population, pages 18?25,
Sydney, Australia, July.
Zareen Saba Syed, Tim Finin, and Anupam Joshi.
2008. Wikitology: Using wikipedia as an ontology.
Technical report, University of Maryland, Baltimore
County.
Max Vo?lkel, Markus Kro?tzsch, Denny Vrandecic,
Heiko Haller, and Rudi Studer. 2006. Semantic
wikipedia. In WWW 2006, pages 585?594, Edin-
burgh, Scotland.
68
Proceedings of the 12th European Workshop on Natural Language Generation, pages 122?125,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Clustering and Matching Headlines for Automatic Paraphrase Acquisition
Sander Wubben, Antal van den Bosch, Emiel Krahmer, Erwin Marsi
Tilburg centre for Creative Computing
Tilburg University
The Netherlands
{s.wubben,antal.vdnbosch,e.j.krahmer,e.c.marsi}@uvt.nl
Abstract
For developing a data-driven text rewriting
algorithm for paraphrasing, it is essential
to have a monolingual corpus of aligned
paraphrased sentences. News article head-
lines are a rich source of paraphrases; they
tend to describe the same event in vari-
ous different ways, and can easily be ob-
tained from the web. We compare two
methods of aligning headlines to construct
such an aligned corpus of paraphrases, one
based on clustering, and the other on pair-
wise similarity-based matching. We show
that the latter performs best on the task of
aligning paraphrastic headlines.
1 Introduction
In recent years, text-to-text generation has re-
ceived increasing attention in the field of Nat-
ural Language Generation (NLG). In contrast
to traditional concept-to-text systems, text-to-text
generation systems convert source text to target
text, where typically the source and target text
share the same meaning to some extent. Ap-
plications of text-to-text generation include sum-
marization (Knight and Marcu, 2002), question-
answering (Lin and Pantel, 2001), and machine
translation.
For text-to-text generation it is important to
know which words and phrases are semantically
close or exchangable in which contexts. While
there are various resources available that capture
such knowledge at the word level (e.g., synset
knowledge in WordNet), this kind of information
is much harder to get by at the phrase level. There-
fore, paraphrase acquisition can be considered an
important technology for producing resources for
text-to-text generation. Paraphrase generation has
already proven to be valuable for Question An-
swering (Lin and Pantel, 2001; Riezler et al,
2007), Machine Translation (Callison-Burch et al,
2006) and the evaluation thereof (Russo-Lassner
et al, 2006; Kauchak and Barzilay, 2006; Zhou et
al., 2006), but also for text simplification and ex-
planation.
In the study described in this paper, we make
an effort to collect Dutch paraphrases from news
article headlines in an unsupervised way to be
used in future paraphrase generation. News ar-
ticle headlines are abundant on the web, and
are already grouped by news aggregators such as
Google News. These services collect multiple arti-
cles covering the same event. Crawling such news
aggregators is an effective way of collecting re-
lated articles which can straightforwardly be used
for the acquisition of paraphrases (Dolan et al,
2004; Nelken and Shieber, 2006). We use this
method to collect a large amount of aligned para-
phrases in an automatic fashion.
2 Method
We aim to build a high-quality paraphrase corpus.
Considering the fact that this corpus will be the ba-
sic resource of a paraphrase generation system, we
need it to be as free of errors as possible, because
errors will propagate throughout the system. This
implies that we focus on obtaining a high precision
in the paraphrases collection process. Where pre-
vious work has focused on aligning news-items at
the paragraph and sentence level (Barzilay and El-
hadad, 2003), we choose to focus on aligning the
headlines of news articles. We think this approach
will enable us to harvest reliable training material
for paraphrase generation quickly and efficiently,
without having to worry too much about the prob-
lems that arise when trying to align complete news
articles.
For the development of our system we use
data which was obtained in the DAESO-project.
This project is an ongoing effort to build a Par-
allel Monolingual Treebank for Dutch (Marsi
122
Placenta sandwich? No, urban legend!
Tom wants to make movie with Katie
Kate?s dad not happy with Tom Cruise
Cruise and Holmes sign for eighteen million
Eighteen million for Tom and Katie
Newest mission Tom Cruise not very convincing
Latest mission Tom Cruise succeeds less well
Tom Cruise barely succeeds with MI:3
Tom Cruise: How weird is he?
How weird is Tom Cruise really?
Tom Cruise leaves family
Tom Cruise escapes changing diapers
Table 1: Part of a sample headline cluster, with
sub-clusters
and Krahmer, 2007) and will be made available
through the Dutch HLT Agency. Part of the data
in the DAESO-corpus consists of headline clusters
crawled from Google News Netherlands in the pe-
riod April?August 2006. For each news article,
the headline and the first 150 characters of the ar-
ticle were stored. Roughly 13,000 clusters were
retrieved. Table 1 shows part of a (translated) clus-
ter. It is clear that although clusters deal roughly
with one subject, the headlines can represent quite
a different perspective on the content of the arti-
cle. To obtain only paraphrase pairs, the clusters
need to be more coherent. To that end 865 clus-
ters were manually subdivided into sub-clusters of
headlines that show clear semantic overlap. Sub-
clustering is no trivial task, however. Some sen-
tences are very clearly paraphrases, but consider
for instance the last two sentences in the example.
They do paraphrase each other to some extent, but
their relation can only be understood properly with
world knowledge. Also, there are numerous head-
lines that can not be sub-clustered, such as the first
three headlines shown in the example.
We use these annotated clusters as development
and test data in developing a method to automat-
ically obtain paraphrase pairs from headline clus-
ters. We divide the annotated headline clusters in a
development set of 40 clusters, while the remain-
der is used as test data. The headlines are stemmed
using the porter stemmer for Dutch (Kraaij and
Pohlmann, 1994).
Instead of a word overlap measure as used by
Barzilay and Elhadad (2003), we use a modified
TF ?IDF word score as was suggested by Nelken
and Shieber (2006). Each sentence is viewed as a
document, and each original cluster as a collection
of documents. For each stemmed word i in sen-
tence j, TFi,j is a binary variable indicating if the
word occurs in the sentence or not. The TF ?IDF
score is then:
TF.IDFi = TFi,j ? log
|D|
|{dj : ti ? dj}|
|D| is the total number of sentences in the clus-
ter and |{dj : ti ? dj}| is the number of sen-
tences that contain the term ti. These scores are
used in a vector space representation. The similar-
ity between headlines can be calculated by using
a similarity function on the headline vectors, such
as cosine similarity.
2.1 Clustering
Our first approach is to use a clustering algorithm
to cluster similar headlines. The original Google
News headline clusters are reclustered into finer
grained sub-clusters. We use the k-means imple-
mentation in the CLUTO1 software package. The
k-means algorithm is an algorithm that assigns
k centers to represent the clustering of n points
(k < n) in a vector space. The total intra-cluster
variances is minimized by the function
V =
k?
i=1
?
xj?Si
(xj ? ?i)
2
where ?i is the centroid of all the points xj ? Si.
The PK1 cluster-stopping algorithm as pro-
posed by Pedersen and Kulkarni (2006) is used to
find the optimal k for each sub-cluster:
PK1(k) =
Cr(k)?mean(Cr[1...?K])
std(Cr[1...?K])
Here, Cr is a criterion function, which mea-
sures the ratio of withincluster similarity to be-
tweencluster similarity. As soon as PK1(k) ex-
ceeds a threshold, k?1 is selected as the optimum
number of clusters.
To find the optimal threshold value for cluster-
stopping, optimization is performed on the devel-
opment data. Our optimization function is an F -
score:
F? =
(1 + ?2) ? (precision ? recall)
(?2 ? precision + recall)
1http://glaros.dtc.umn.edu/gkhome/views/cluto/
123
We evaluate the number of aligments between pos-
sible paraphrases. For instance, in a cluster of four
sentences,
(4
2
)
= 6 alignments can be made. In
our case, precision is the number of alignments
retrieved from the clusters which are relevant, di-
vided by the total number of retrieved alignments.
Recall is the number of relevant retrieved alig-
ments divided by the total number of relevant
alignments.
We use an F?-score with a ? of 0.25 as we
favour precision over recall. We do not want to op-
timize on precision alone, because we still want to
retrieve a fair amount of paraphrases and not only
the ones that are very similar. Through optimiza-
tion on our development set, we find an optimal
threshold for the PK1 algorithm thpk1 = 1. For
each original cluster, k-means clustering is then
performed using the k found by the cluster stop-
ping function. In each newly obtained cluster all
headlines can be aligned to each other.
2.2 Pairwise similarity
Our second approach is to calculate the similarity
between pairs of headlines directly. If the similar-
ity exceeds a certain threshold, the pair is accepted
as a paraphrase pair. If it is below the thresh-
old, it is rejected. However, as Barzilay and El-
hadad (2003) have pointed out, sentence mapping
in this way is only effective to a certain extent.
Beyond that point, context is needed. With this
in mind, we adopt two thresholds and the Cosine
similarity function to calculate the similarity be-
tween two sentences:
cos(?) =
V 1 ? V 2
?V 1??V 2?
where V 1 and V 2 are the vectors of the two sen-
tences being compared. If the similarity is higher
than the upper threshold, it is accepted. If it is
lower than the lower theshold, it is rejected. In
the remaining case of a similarity between the two
thresholds, similarity is calculated over the con-
texts of the two headlines, namely the text snippet
that was retrieved with the headline. If this simi-
larity exceeds the upper threshold, it is accepted.
Threshold values as found by optimizing on the
development data using again an F0.25-score, are
Thlower = 0.2 and Thupper = 0.5. An optional
final step is to add alignments that are implied by
previous alignments. For instance, if headlineA is
paired with headline B, and headline B is aligned
to headline C, headline A can be aligned to C as
Type Precision Recall
k-means clustering 0.91 0.43
clusters only
k-means clustering 0.66 0.44
all headlines
pairwise similarity 0.93 0.39
clusters only
pairwise similarity 0.76 0.41
all headlines
Table 2: Precision and Recall for both methods
Playstation 3 more expensive than
competitor
Playstation 3 will become more
expensive than Xbox 360
Sony postpones Blu-Ray movies
Sony postpones coming of blu-ray dvds
Prices Playstation 3 known: from 499 euros
E3 2006: Playstation 3 from 499 euros
Sony PS3 with Blu-Ray for sale from
November 11th
PS3 available in Europe from
November 17th
Table 3: Examples of correct (above) and incorrect
(below) alignments
well. We do not add these alignments, because in
particular in large clusters when one wrong align-
ment is made, this process chains together a large
amount of incorrect alignments.
3 Results
The 825 clusters in the test set contain 1,751 sub-
clusters in total. In these sub-clusters, there are
6,685 clustered headlines. Another 3,123 head-
lines remain unclustered. Table 2 displays the
paraphrase detection precision and recall of our
two approaches. It is clear that k-means cluster-
ing performs well when all unclustered headlines
are artificially ignored. In the more realistic case
when there are also items that cannot be clustered,
the pairwise calculation of similarity with a back
off strategy of using context performs better when
we aim for higher precision. Some examples of
correct and incorrect alignments are given in Ta-
ble 3.
124
4 Discussion
Using headlines of news articles clustered by
Google News, and finding good paraphrases
within these clusters is an effective route for ob-
taining pairs of paraphrased sentences with rea-
sonable precision. We have shown that a cosine
similarity function comparing headlines and us-
ing a back off strategy to compare context can be
used to extract paraphrase pairs at a precision of
0.76. Although we could aim for a higher preci-
sion by assigning higher values to the thresholds,
we still want some recall and variation in our para-
phrases. Of course the coverage of our method is
still somewhat limited: only paraphrases that have
some words in common will be extracted. This
is not a bad thing: we are particularly interested
in extracting paraphrase patterns at the constituent
level. These alignments can be made with existing
alignment tools such as the GIZA++ toolkit.
We measure the performance of our approaches
by comparing to human annotation of sub-
clusterings. The human task in itself is hard. For
instance, is we look at the incorrect examples in
Table 3, the difficulty of distinguishing between
paraphrases and non-paraphrases is apparent. In
future research we would like to investigate the
task of judging paraphrases. The next step we
would like to take towards automatic paraphrase
generation, is to identify the differences between
paraphrases at the constituent level. This task has
in fact been performed by human annotators in the
DAESO-project. A logical next step would be to
learn to align the different constituents on our ex-
tracted paraphrases in an unsupervised way.
Acknowledgements
Thanks are due to the Netherlands Organization
for Scientific Research (NWO) and to the Dutch
HLT Stevin programme. Thanks also to Wauter
Bosma for originally mining the headlines from
Google News. For more information on DAESO,
please visit daeso.uvt.nl.
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25?
32.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, pages 17?24.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: exploiting massively parallel news sources. In
COLING ?04: Proceedings of the 20th international
conference on Computational Linguistics, page 350.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of the Human Language Technology Conference of
the NAACL, Main Conference, pages 455?462, June.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91?107.
Wessel Kraaij and Rene Pohlmann. 1994. Porters
stemming algorithm for dutch. In Informatieweten-
schap 1994: Wetenschappelijke bijdragen aan de
derde STINFON Conferentie, pages 167?180.
Dekang Lin and Patrick Pantel. 2001. Dirt: Discov-
ery of inference rules from text. In KDD ?01: Pro-
ceedings of the seventh ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 323?328.
Erwin Marsi and Emiel Krahmer. 2007. Annotating
a parallel monolingual treebank with semantic sim-
ilarity relations. In he Sixth International Workshop
on Treebanks and Linguistic Theories (TLT?07).
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for mono-
lingual corpora. In Proceedings of the 11th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-06), 3?7 April.
Ted Pedersen and Anagha Kulkarni. 2006. Automatic
cluster stopping with criterion functions and the gap
statistic. In Proceedings of the 2006 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 276?279.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu O. Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In ACL.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
2006. A paraphrase-based approach to machine
translation evaluation. Technical report, University
of Maryland, College Park.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating machine translation results with para-
phrase support. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 77?84, July.
125
Proceedings of the 8th International Conference on Computational Semantics, pages 355?358,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
A semantic relatedness metric based on free link
structure
Sander Wubben
TiCC
Tilburg University
s.wubben@uvt.nl
Antal van den Bosch
TiCC
Tilburg University
antal.vdnbosch@uvt.nl
Abstract
While shortest paths in WordNet are known to correlate well with
semantic similarity, an is-a hierarchy is less suited for estimating se-
mantic relatedness. We demonstrate this by comparing two free scale
networks ( ConceptNet and Wikipedia) to WordNet. Using the Finkelstein-
353 dataset we show that a shortest path metric run on Wikipedia
attains a better correlation than WordNet-based metrics. Concept-
Net attains a good correlation as well, but suffers from a low concept
coverage.
1 Introduction
In this paper we propose a new estimate metric for semantic relatedness, by
finding the shortest path between two concepts in a semantic network. The
semantic networks that we exploit are the extracted free link structures of
Wikipedia and the ConceptNet 3 database, a commonsense knowledgebase
[3]. Various metrics of calculating semantic similarity have been developed
for WordNet [1] and applied to Wikipedia?s category graph [7]. These mea-
sures tend to perform well on semantic similarity (how synonymous two
words are), but not very well on semantic relatedness (how related two
words are).
2 Free-link pathfinding
The graph extracted from Wikipedia contains over 2 million nodes and 55
million edges, from respectively the number of articles and the number of in-
ternal links. ConceptNet contains over 18 thousand usable concepts (nodes)
355
and over 254 thousand useful assertions (edges). To find paths between con-
cepts in these massive networks, they need to be indexed well and and the
algorithms need to be efficient.
For the current study, the English Wikipedia dump dated 12 March
2008 is downloaded
1
. To form the network, only the article names and
hyperlinks between the articles are extracted, constituting the link structure.
ConceptNet 3 can be freely downloaded
2
. Due to the database being in N3
format, the links can be extracted straightforwardly. The Porter stemmer
3
is used for normalization of the concepts.
Because of the richness and scale of the links in these networks, which
both can be argued to have scale free characteristics [6, 8], a breadth-first
search is the best option. As we index both incoming and outgoing links,
forward and backward chaining can be used simultaneously. Therefore, we
choose to utilize a bidirectional breadth-first search algorithm.
3 Experiments
To evaluate the task we compare our results to how humans would do given
the same task. Most available datasets focus on semantic similarity rather
than semantic relatedness. Examples of these are the Rubenstein and Good-
enough and Miller and Charles wordpairs. The Finkelstein-353
4
test collec-
tion is a dataset that does contain semantic relatedness scores. It contains
353 wordpairs, among which the 30 wordpairs from the Miller & Charles
dataset. The collection contains English word pairs along with human-
assigned relatedness judgements.
The breadth-first free-link pathfinding search algorithm is applied to
both the Wikipedia and the ConceptNet data, generating semantic relat-
edness estimates for the Finkelstein pairs. To compare our metric against
path-based metrics based on WordNet, the WordNet::Similarity package
5
is used [5] on WordNet 3.0 [4].
1
http://download.wikimedia.org/enwiki/20080312/enwiki-20080312-pages-articles.
xml.bz2
2
http://conceptnet.media.mit.edu/conceptnet\_en\_20080605.n3.bz2
3
http://www.ldc.usb.ve/
~
vdaniel/porter.pm
4
http://www.cs.technion.ac.il/
~
gabr/resources/data/wordsim353/wordsim353.
html
5
http://search.cpan.org/dist/WordNet-Similarity/
356
WordNet measures 0.18 - 0.45
WikiRelate! 0.19 - 0.48
ESA Wikipedia 0.75
Free-link Wikipedia, directed 0.45
Free-link Wikipedia, undirected 0.53
Free-link ConceptNet 0.35
Free-link ConceptNet, non-missing 0.47
Table 1: Spearman?s ? rank order coefficients of different measures with
human judgements.
4 Results
As displayed in Table 1 (displaying also the results from ESA and WikiRe-
late! taken from [2]), the basic directed free-link pahfinding measure applied
on Wikipedia shows a Spearman?s ? = 0.45, while the undirected measure
shows a ? = 0.53 with human judgements on the full Finkelstein dataset.
Only the word defeating was not found in Wikipedia.
In ConceptNet, 58 of the Finkelstein pairs were not found, significantly
impairing its score: ? = 0.35. When we look only at those word pairs
for which a path was found, the measure applied to ConceptNet performs
sightly better than the directed Wikipedia measure, but worse than the more
comparable undirected Wikipedia measure: ? = 0.47.
5 Discussion
With a correlation of 0.53 for undirected search versus 0.43 for directed
search with the human judging scores of the Finkelstein-353 dataset, the
former appears the better suited way of exploiting the Wikipedia network
for a semantic relatedness metric. It also outperforms undirected search in
ConceptNet, which only scores a correlation of 0.35. This lower score can
be mainly attributed to the lack of coverage of ConceptNet. But even when
only non-missing wordpairs are considered, ConceptNet performs worse than
Wikipedia with a correlation of 0.47 with human judgements.
The method introduced here outperforms any other existing pathfinding
method for calculating semantic relatedness. It also outperforms methods
that makes use of WordNet?s extended gloss vectors. This cannot be ex-
plained by coverage: both in WordNet 3.0 and in the Wikipedia dump that
was used only one wordpair was not found. Arguably, the results show that
357
free link structure in conceptual networks is better suited for finding se-
mantic relatedness than hierarchical structures organized along taxonomic
relations as WordNet.
While ESA?s performance (? = 0.75) is even higher, the comparison
is off. ESA is a fully integrated machine learning architecture that makes
extensive use of Wikipedia?s free text. Our free link measure is a simple
and portable method that uses only the link structure of any given free
associative conceptual network. We aim to improve the method and use it
in various applications, such as automated ontology building and memory
based paraphrasing.
References
[1] A. Budanitsky and G. Hirst. Evaluating wordnet-based measures of
lexical semantic relatedness. Computational Linguistics, 32(1):13?47,
2006.
[2] E. Gabrilovich and S. Markovitch. Computing Semantic Relatedness
using Wikipedia-based Explicit Semantic Analysis. Proceedings of the
20th International Joint Conference on Artificial Intelligence, pages 6?
12, 2007.
[3] H. Liu and P. Singh. Conceptnet ? a practical commonsense reasoning
tool-kit. BT Technology Journal, 22(4):211?226, 2004.
[4] G. A. Miller. Wordnet: a lexical database for english. Commun. ACM,
38(11):39?41, 1995.
[5] T. Pedersen, S. Patwardhan, and J. Michelizzi. Wordnet::similarity -
measuring the relatedness of concepts, 2004.
[6] S. Spek. Wikipedia: organisation from a bottom-up approach. In Wik-
iSym ?06: Proceedings of the 2006 international symposium on Wikis,
New York, NY, USA, Nov 2006. ACM.
[7] M. Strube and S. P. Ponzetto. Wikirelate! computing semantic related-
ness using wikipedia. In 21. AAAI / 18. IAAI 2006. AAAI Press, july
2006.
[8] S. Wubben. Using free link structure to calculate semantic relatedness.
Master?s thesis, Tilburg University, the Netherlands, 2008.
358
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 660?664,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Does Size Matter ? How Much Data is Required to Train a REG Algorithm?
Marie?t Theune
University of Twente
P.O. Box 217
7500 AE Enschede
The Netherlands
m.theune@utwente.nl
Ruud Koolen
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
r.m.f.koolen@uvt.nl
Emiel Krahmer
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
e.j.krahmer@uvt.nl
Sander Wubben
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
s.wubben@uvt.nl
Abstract
In this paper we investigate how much data
is required to train an algorithm for attribute
selection, a subtask of Referring Expressions
Generation (REG). To enable comparison be-
tween different-sized training sets, a system-
atic training method was developed. The re-
sults show that depending on the complexity
of the domain, training on 10 to 20 items may
already lead to a good performance.
1 Introduction
There are many ways in which we can refer to ob-
jects and people in the real world. A chair, for ex-
ample, can be referred to as red, large, or seen from
the front, while men may be singled out in terms
of their pogonotrophy (facial hairstyle), clothing and
many other attributes. This poses a problem for al-
gorithms that automatically generate referring ex-
pressions: how to determine which attributes to use?
One solution is to assume that some attributes
are preferred over others, and this is indeed what
many Referring Expressions Generation (REG) al-
gorithms do. A classic example is the Incremental
Algorithm (IA), which postulates the existence of
a complete ranking of relevant attributes (Dale and
Reiter, 1995). The IA essentially iterates through
this list of preferred attributes, selecting an attribute
for inclusion in a referring expression if it helps sin-
gling out the target from the other objects in the
scene (the distractors). Crucially, Dale and Reiter do
not specify how the ranking of attributes should be
determined. They refer to psycholinguistic research
suggesting that, in general, absolute attributes (such
as color) are preferred over relative ones (such as
size), but stress that constructing a preference order
is essentially an empirical question, which will dif-
fer from one domain to another.
Many other REG algorithms similarly rely on
preferences. The graph-based based REG algorithm
(Krahmer et al, 2003), for example, models prefer-
ences in terms of costs, with cheaper properties be-
ing more preferred. Various ways to compute costs
are possible; they can be defined, for instance, in
terms of log probabilities, which makes frequently
encountered properties cheap, and infrequent ones
more expensive. Krahmer et al (2008) argue that
a less fine-grained cost function might generalize
better, and propose to use frequency information
to, somewhat ad hoc, define three costs: 0 (free),
1 (cheap) and 2 (expensive). This approach was
shown to work well: the graph-based algorithm was
the best performing system in the most recent REG
Challenge (Gatt et al, 2009).
Many other attribute selection algorithms also
rely on training data to determine preferences in one
form or another (Fabbrizio et al, 2008; Gerva?s et
al., 2008; Kelleher, 2007; Spanger et al, 2008; Vi-
ethen and Dale, 2010). Unfortunately, suitable data
is hard to come by. It has been argued that determin-
ing which properties to include in a referring expres-
sion requires a ?semantically transparent? corpus
(van Deemter et al, 2006): a corpus that contains
the actual properties of all domain objects as well
as the properties that were selected for inclusion in
a given reference to the target. Obviously, text cor-
pora tend not to meet this requirement, which is why
660
semantically transparent corpora are often collected
using human participants who are asked to produce
referring expressions for targets in controlled visual
scenes for a given domain. Since this is a time con-
suming exercise, it will not be surprising that such
corpora are thin on the ground (and are often only
available for English). An important question there-
fore is how many human-produced references are
needed to achieve a certain level of performance. Do
we really need hundreds of instances, or can we al-
ready make informed decisions about preferences on
a few or even one training instance?
In this paper, we address this question by sys-
tematically training the graph-based REG algorithm
on a number of ?semantically transparent? data sets
of various sizes and evaluating on a held-out test
set. The graph-based algorithm seems a good can-
didate for this exercise, in view of its performance
in the REG challenges. For the sake of compari-
son, we also follow the evaluation methodology of
the REG challenges, training and testing on two do-
mains (a furniture and a people domain), and using
two automatic metrics (Dice and accuracy) to mea-
sure human-likeness. One hurdle needs to be taken
beforehand. Krahmer et al (2008) manually as-
signed one of three costs to properties, loosely based
on corpus frequencies. For our current evaluation
experiments, this would hamper comparison across
data sets, because it is difficult to do it in a manner
that is both consistent and meaningful. Therefore we
first experiment with a more systematic way of as-
signing a limited number of frequency-based costs
to properties using k-means clustering.
2 Experiment I: k-means clustering costs
In this section we describe our experiment with k-
means clustering to derive property costs from En-
glish and Dutch corpus data. For this experiment we
looked at both English and Dutch, to make sure the
chosen method does not only work well for English.
2.1 Materials
Our English training and test data were taken from
the TUNA corpus (Gatt et al, 2007). This semanti-
cally transparent corpus contains referring expres-
sions in two domains (furniture and people), col-
lected in one of two conditions: in the -LOC con-
dition, participants were discouraged from mention-
ing the location of the target in the visual scene,
whereas in the +LOC condition they could mention
any properties they wanted. The TUNA corpus was
used for comparative evaluation in the REG Chal-
lenges (2007-2009). For training in our current ex-
periment, we used the -LOC data from the training
set of the REG Challenge 2009 (Gatt et al, 2009):
165 furniture descriptions and 136 people descrip-
tions. For testing, we used the -LOC data from the
TUNA 2009 development set: 38 furniture descrip-
tions and 38 people descriptions.
Dutch data were taken from the D-TUNA corpus
(Koolen and Krahmer, 2010). This corpus uses the
same visual scenes and annotation scheme as the
TUNA corpus, but with Dutch instead of English
descriptions. D-TUNA does not include locations as
object properties at all, hence our restriction to -LOC
data for English (to make the Dutch and English data
more comparable). As Dutch test data, we used 40
furniture items and 40 people items, randomly se-
lected from the textual descriptions in the D-TUNA
corpus. The remaining furniture and people descrip-
tions (160 items each) were used for training.
2.2 Method
We first determined the frequency with which each
property was mentioned in our training data, relative
to the number of target objects with this property.
Then we created different cost functions (mapping
properties to costs) by means of k-means clustering,
using the Weka toolkit. The k-means clustering al-
gorithm assigns n points in a vector space to k clus-
ters (S1 to Sk) by assigning each point to the clus-
ter with the nearest centroid. The total intra-cluster
variance V is minimized by the function
V =
k
?
i=1
?
xj?Si
(xj ? ?i)2
where ?i is the centroid of all the points xj ? Si.
In our case, the points n are properties, the vector
space is one-dimensional (frequency being the only
dimension) and ?i is the average frequency of the
properties in Si. The cluster-based costs are defined
as follows:
?xj ? Si, cost(xj) = i? 1
661
where S1 is the cluster with the most frequent
properties, S2 is the cluster with the next most fre-
quent properties, and so on. Using this approach,
properties from cluster S1 get cost 0 and thus can be
added ?for free? to a description. Free properties are
always included, provided they help distinguish the
target. This may lead to overspecified descriptions,
mimicking the human tendency to mention redun-
dant properties (Dale and Reiter, 1995).
We ran the clustering algorithm on our English
and Dutch training data for up to six clusters (k = 2
to k = 6). Then we evaluated the performance of
the resulting cost functions on the test data from
the same language, using Dice (overlap between at-
tribute sets) and Accuracy (perfect match between
sets) as evaluation metrics. For comparison, we also
evaluated the best scoring cost functions from Theu-
ne et al (2010) on our test data. These ?Free-Na??ve?
(FN) functions were created using the manual ap-
proach sketched in the introduction.
The order in which the graph-based algorithm
tries to add attributes to a description is explicitly
controlled to ensure that ?free? distinguishing prop-
erties are included (Viethen et al, 2008). In our
tests, we used an order of decreasing frequency; i.e.,
always examining more frequent properties first.1
2.3 Results
For the cluster-based cost functions, the best perfor-
mance was achieved with k = 2, for both domains
and both languages. Interestingly, this is the coarsest
possible k-means function: with only two costs (0
and 1) it is even less fine-grained than the FN func-
tions advocated by Krahmer et al (2008). The re-
sults for the k-means costs with k = 2 and the FN
costs of Theune et al (2010) are shown in Table 1.
No significant differences were found, which sug-
gests that k-means clustering, with k = 2, can be
used as a more systematic alternative for the manual
assignment of frequency-based costs. We therefore
applied this method in the next experiment.
3 Experiment II: varying training set size
To find out how much training data is required
to achieve an acceptable attribute selection perfor-
1We used slightly different property orders than Theune et
al. (2010), leading to minor differences in our FN results.
Furniture People
Language Costs Dice Acc. Dice Acc.
English k-means 0.810 0.50 0.733 0.29
FN 0.829 0.55 0.733 0.29
Dutch k-means 0.929 0.68 0.812 0.33
FN 0.929 0.68 0.812 0.33
Table 1: Results for k-means costs with k = 2 and the
FN costs of Theune et al (2010) on Dutch and English.
mance, in the second experiment we derived cost
functions and property orders from different sized
training sets, and evaluated them on our test data.
For this experiment, we only used English data.
3.1 Materials
As training sets, we used randomly selected subsets
of the full English training set from Experiment I,
with set sizes of 1, 5, 10, 20 and 30 items. Be-
cause the accidental composition of a training set
may strongly influence the results, we created 5 dif-
ferent sets of each size. The training sets were built
up in a cumulative fashion: we started with five sets
of size 1, then added 4 items to each of them to cre-
ate five sets of size 5, etc. This resulted in five series
of increasingly sized training sets. As test data, we
used the same English test set as in Experiment I.
3.2 Method
We derived cost functions (using k-means clustering
with k = 2) and orders from each of the training
sets, following the method described in Section 2.2.
In doing so, we had to deal with missing data: not all
properties were present in all data sets.2 For the cost
functions, we simply assigned the highest cost (1)
to the missing properties. For the order, we listed
properties with the same frequency (0 for missing
properties) in alphabetical order. This was done for
the sake of comparability between training sets.
3.3 Results
To determine significance, we calculated the means
of the scores of the five training sets for each set
size, so that we could compare them with the scores
of the entire set. We applied repeated measures of
2This problem mostly affected the smaller training sets. By
set size 10 only a few properties were missing, while by set size
20, all properties were present in all sets.
662
variance (ANOVA) to the Dice and Accuracy scores,
using set size (1, 5, 10, 20, 30, entire set) as a within
variable. The mean results for each training set size
are shown in Table 2.3 The general pattern is that
the scores increase with the size of the training set,
but the increase gets smaller as the set sizes become
larger.
Furniture People
Set size Dice Acc. Dice Acc.
1 0.693 0.25 0.560 0.13
5 0.756 0.34 0.620 0.15
10 0.777 0.40 0.686 0.20
20 0.788 0.41 0.719 0.25
30 0.782 0.41 0.718 0.27
Entire set 0.810 0.50 0.733 0.29
Table 2: Mean results for the different set sizes.
In the furniture domain, we found a main effect
of set size (Dice: F(5,185) = 7.209, p < .001; Ac-
curacy: F(5,185) = 6.140, p < .001). To see which
set sizes performed significantly different as com-
pared to the entire set, we conducted Tukey?s HSD
post hoc comparisons. For Dice, the scores of set
size 10 (p = .141), set size 20 (p = .353), and set
size 30 (p = .197) did not significantly differ from
the scores of the entire set of 165 items. The Accu-
racy scores in the furniture domain show a slightly
different pattern: the scores of the entire training set
were still significantly higher than those of set size
30 (p < .05). This better performance when trained
on the entire set may be caused by the fact that not
all of the five training sets that were used for set sizes
1, 5, 10, 20 and 30 performed equally well.
In the people domain we also found a main effect
of set size (Dice: F(5,185) = 21.359, p < .001; Accu-
racy: F(5,185) = 8.074, p < .001). Post hoc pairwise
comparisons showed that the scores of set size 20
(Dice: p = .416; Accuracy: p = .146) and set size
30 (Dice: p = .238; Accuracy: p = .324) did not
significantly differ from those of the full set of 136
items.
3For comparison: in the REG Challenge 2008, (which in-
volved a different test set, but the same type of data), the best
systems obtained overall Dice and accuracy scores of around
0.80 and 0.55 respectively (Gatt et al, 2008). These scores may
well represent the performance ceiling for speaker and context
independent algorithms on this task.
4 Discussion
Experiment II has shown that when using small data
sets to train an attribute selection algorithm, results
can be achieved that are not significantly different
from those obtained using a much larger training
set. Domain complexity appears to be a factor in
how much training data is needed: using Dice as an
evaluation metric, training sets of 10 sufficed in the
simple furniture domain, while in the more complex
people domain it took a set size of 20 to achieve re-
sults that do not significantly differ from those ob-
tained using the full training set.
The accidental composition of the training sets
may strongly influence the attribute selection per-
formance. In the furniture domain, we found clear
differences between the results of specific training
sets, with ?bad sets? pulling the overall performance
down. This affected Accuracy but not Dice, possibly
because the latter is a less strict metric.
Whether the encouraging results found for the
graph-based algorithm generalize to other REG ap-
proaches is still an open question. We also need
to investigate how the use of small training sets af-
fects effectiveness and efficiency of target identifica-
tion by human subjects; as shown by Belz and Gatt
(2008), task-performance measures do not necessar-
ily correlate with similarity measures such as Dice.
Finally, it will be interesting to repeat Experiment II
with Dutch data. The D-TUNA data are cleaner than
the TUNA data (Theune et al, 2010), so the risk of
?bad? training data will be smaller, which may lead
to more consistent results across training sets.
5 Conclusion
Our experiment has shown that with 20 or less train-
ing instances, acceptable attribute selection results
can be achieved; that is, results that do not signif-
icantly differ from those obtained using the entire
training set. This is good news, because collecting
such small amounts of training data should not take
too much time and effort, making it relatively easy
to do REG for new domains and languages.
Acknowledgments
Krahmer and Koolen received financial support from
The Netherlands Organization for Scientific Re-
search (Vici grant 27770007).
663
References
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic
evaluation measures for referring expression genera-
tion. In Proceedings of ACL-08: HLT, Short Papers,
pages 197?200.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretation of the Gricean maxims in the generation of
referring expressions. Cognitive Science, 19(2):233?
263.
Giuseppe Di Fabbrizio, Amanda Stent, and Srinivas
Bangalore. 2008. Trainable speaker-based refer-
ring expression generation. In Twelfth Conference on
Computational Natural Language Learning (CoNLL-
2008), pages 151?158.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation (ENLG 2007), pages 49?56.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The
TUNA Challenge 2008: Overview and evaluation re-
sults. In Proceedings of the 5th International Natural
Language Generation Conference (INLG 2008), pages
198?206.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA-
REG Challenge 2009: Overview and evaluation re-
sults. In Proceedings of the 12th European Workshop
on Natural Language Generation (ENLG 2009), pages
174?182.
Pablo Gerva?s, Raquel Herva?s, and Carlos Le?on. 2008.
NIL-UCM: Most-frequent-value-first attribute selec-
tion and best-scoring-choice realization. In Proceed-
ings of the 5th International Natural Language Gener-
ation Conference (INLG 2008), pages 215?218.
John Kelleher. 2007. DIT - frequency based incremen-
tal attribute selection for GRE. In Proceedings of the
MT Summit XI Workshop Using Corpora for Natural
Language Generation: Language Generation and Ma-
chine Translation (UCNLG+MT), pages 90?92.
Ruud Koolen and Emiel Krahmer. 2010. The D-TUNA
corpus: A Dutch dataset for the evaluation of refer-
ring expression generation algorithms. In Proceedings
of the 7th international conference on Language Re-
sources and Evaluation (LREC 2010).
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Emiel Krahmer, Marie?t Theune, Jette Viethen, and Iris
Hendrickx. 2008. GRAPH: The costs of redundancy
in referring expressions. In Proceedings of the 5th In-
ternational Natural Language Generation Conference
(INLG 2008), pages 227?229.
Philipp Spanger, Takehiro Kurosawa, and Takenobu
Tokunaga. 2008. On ?redundancy? in selecting at-
tributes for generating referring expressions. In COL-
ING 2008: Companion volume: Posters, pages 115?
118.
Marie?t Theune, Ruud Koolen, and Emiel Krahmer. 2010.
Cross-linguistic attribute selection for REG: Compar-
ing Dutch and English. In Proceedings of the 6th In-
ternational Natural Language Generation Conference
(INLG 2010), pages 174?182.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus for
the generation of referring expressions. In Proceed-
ings of the 4th International Natural Language Gener-
ation Conference (INLG 2006), pages 130?132.
Jette Viethen and Robert Dale. 2010. Speaker-dependent
variation in content selection for referring expression
generation. In Proceedings of the 8th Australasian
Language Technology Workshop, pages 81?89.
Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t Theu-
ne, and Pascal Touset. 2008. Controlling redundancy
in referring expressions. In Proceedings of the Sixth
International Conference on Language Resources and
Evaluation (LREC 2008), pages 239?246.
664
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1015?1024,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Sentence Simplification by Monolingual Machine Translation
Sander Wubben
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
s.wubben@uvt.nl
Antal van den Bosch
Radboud University Nijmegen
P.O. Box 9103
6500 HD Nijmegen
The Netherlands
a.vandenbosch@let.ru.nl
Emiel Krahmer
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
e.j.krahmer@uvt.nl
Abstract
In this paper we describe a method for simpli-
fying sentences using Phrase Based Machine
Translation, augmented with a re-ranking
heuristic based on dissimilarity, and trained on
a monolingual parallel corpus. We compare
our system to a word-substitution baseline and
two state-of-the-art systems, all trained and
tested on paired sentences from the English
part of Wikipedia and Simple Wikipedia. Hu-
man test subjects judge the output of the dif-
ferent systems. Analysing the judgements
shows that by relatively careful phrase-based
paraphrasing our model achieves similar sim-
plification results to state-of-the-art systems,
while generating better formed output. We
also argue that text readability metrics such
as the Flesch-Kincaid grade level should be
used with caution when evaluating the output
of simplification systems.
1 Introduction
Sentence simplification can be defined as the process
of producing a simplified version of a sentence by
changing some of the lexical material and grammat-
ical structure of that sentence, while still preserving
the semantic content of the original sentence, in or-
der to ease its understanding. Particularly language
learners (Siddharthan, 2002), people with reading
disabilities (Inui et al, 2003) such as aphasia (Car-
roll et al, 1999), and low-literacy readers (Watanabe
et al, 2009) can benefit from this application. It can
serve to generate output in a specific limited format,
such as subtitles (Daelemans et al, 2004). Sentence
simplification can also serve to preprocess the input
of other tasks, such as summarization (Knight and
Marcu, 2000), parsing, machine translation (Chan-
drasekar et al, 1996), semantic role labeling (Vick-
rey and Koller, 2008) or sentence fusion (Filippova
and Strube, 2008).
The goal of simplification is to achieve an im-
provement in readability, defined as the ease with
which a text can be understood. Some of the factors
that are known to help increase the readability of text
are the vocabulary used, the length of the sentences,
the syntactic structures present in the text, and the
usage of discourse markers. One effort to create a
simple version of English at the vocabulary level has
been the creation of Basic English by Charles Kay
Ogden. Basic English is a controlled language with
a basic vocabulary consisting of 850 words. Accord-
ing to Ogden, 90 percent of all dictionary entries can
be paraphrased using these 850 words. An exam-
ple of a resource that is written using mainly Basic
English is the English Simple Wikipedia. Articles
on English Simple Wikipedia are similar to articles
found in the traditional English Wikipedia, but writ-
ten using a limited vocabulary (using Basic English
where possible). Generally the structure of the sen-
tences in English Simple Wikipedia is less compli-
cated and the sentences are somewhat shorter than
those found in English Wikipedia; we offer more de-
tailed statistics below.
1.1 Related work
Most earlier work on sentence simplification
adopted rule-based approaches. A frequently ap-
plied type of rule, aimed to reduce overall sentence
length, splits long sentences on the basis of syntactic
1015
information (Chandrasekar and Srinivas, 1997; Car-
roll et al, 1998; Canning et al, 2000; Vickrey and
Koller, 2008). There has also been work on lexi-
cal substitution for simplification, where the aim is
to substitute difficult words with simpler synonyms,
derived from WordNet or dictionaries (Inui et al,
2003).
Zhu et al (2010) examine the use of paired doc-
uments in English Wikipedia and Simple Wikipedia
for a data-driven approach to the sentence simplifi-
cation task. They propose a probabilistic, syntax-
based machine translation approach to the problem
and compare against a baseline of no simplification
and a phrase-based machine translation approach.
In a similar vein, Coster and Kauchak (2011) use
a parallel corpus of paired documents from Sim-
ple Wikipedia and Wikipedia to train a phrase-based
machine translation model coupled with a deletion
model. Another useful resource is the edit his-
tory of Simple Wikipedia, from which simplifica-
tions can be learned (Yatskar et al, 2010). Woods-
end and Lapata (2011) investigate the use of Simple
Wikipedia edit histories and an aligned Wikipedia?
Simple Wikipedia corpus to induce a model based
on quasi-synchronous grammar. They select the
most appropriate simplification by using integer lin-
ear programming.
We follow Zhu et al (2010) and Coster and
Kauchak (2011) in proposing that sentence simpli-
fication can be approached as a monolingual ma-
chine translation task, where the source and target
languages are the same and where the output should
be simpler in form from the input but similar in
meaning. We differ from the approach of Zhu et
al. (2010) in the sense that we do not take syntac-
tic information into account; we rely on PBMT to
do its work and implicitly learn simplifying para-
phrasings of phrases. Our approach differs from
Coster and Kauchak (2011) in the sense that instead
of focusing on deletion in the PBMT decoding stage,
we focus on dissimilarity, as simplification does not
necessarily imply shortening (Woodsend and Lap-
ata, 2011), or as the Simple Wikipedia guidelines
state, ?simpler does not mean short?1. Table 1.1
shows the average sentence length and the average
1http://simple.wikipedia.org/wiki/Main_
Page/Introduction
word length for Wikipedia and Simple Wikipedia
sentences in the PWKP dataset used in this study
(Zhu et al, 2010). These numbers suggest that, al-
though the selection criteria for sentences to be in-
cluded in this dataset are biased (see Section 2.2),
Simple Wikipedia sentences are about 17% shorter,
while the average word length is virtually equal.
Sent. length Token length
Simple Wikipedia 20.87 4.89
Wikipedia 25.01 5.06
Table 1: Sentence and token length statistics for the
PWKP dataset (Zhu et al, 2010).
Statistical machine translation (SMT) has already
been successfully applied to the related task of para-
phrasing (Quirk et al, 2004; Bannard and Callison-
Burch, 2005; Madnani et al, 2007; Callison-Burch,
2008; Zhao et al, 2009; Wubben et al, 2010). SMT
typically makes use of large parallel corpora to train
a model on. These corpora need to be aligned at
the sentence level. Large parallel corpora, such as
the multilingual proceedings of the European Parlia-
ment (Europarl), are readily available for many lan-
guages. Phrase-Based Machine Translation (PBMT)
is a form of SMT where the translation model aims
to translate longer sequences of words (?phrases?)
in one go, solving part of the word ordering problem
along the way that would be left to the target lan-
guage model in a word-based SMT system. PMBT
operates purely on statistics and no linguistic knowl-
edge is involved in the process: the phrases that are
aligned are motivated statistically, rather than lin-
guistically. This makes PBMT adaptable to any lan-
guage pair for which there is a parallel corpus avail-
able. The PBMT model makes use of a translation
model, derived from the parallel corpus, and a lan-
guage model, derived from a monolingual corpus in
the target language. The language model is typically
an n-gram model with smoothing. For any given in-
put sentence, a search is carried out producing an
n-best list of candidate translations, ranked by the
decoder score, a complex scoring function includ-
ing likelihood scores from the translation model,
and the target language model. In principle, all of
this should be transportable to a data-driven machine
translation account of sentence simplification, pro-
1016
vided that a parallel corpus is available that pairs text
to simplified versions of that text.
1.2 This study
In this work we aim to investigate the use of phrase-
based machine translation modified with a dissim-
ilarity component for the task of sentence simplifi-
cation. While Zhu et al (2010) have demonstrated
that their approach outperforms a PBMT approach
in terms of Flesch Reading Ease test scores, we are
not aware of any studies that evaluate PBMT for sen-
tence simplification with human judgements. In this
study we evaluate the output of Zhu et al (2010)
(henceforth referred to as ?Zhu?), Woodsend and La-
pata (2011) (henceforth referred to as ?RevILP?),
our PBMT based system with dissimilarity-based
re-ranking (henceforth referred to as ?PBMT-R?), a
word-substitution baseline, and, as a gold standard,
the original Simple Wikipedia sentences. We will
first discuss the baseline, followed by the Zhu sys-
tem, the RevILP system, and our PBMT-R system
in Section 2. We then describe the experiment with
human judges in Section 3, and its results in Sec-
tion 4. We close this paper by critically discussing
our results in Section 5.
2 Sentence Simplification Models
2.1 Word-Substitution Baseline
The word substitution baseline replaces words in
the source sentence with (near-)synonyms that are
more likely according to a language model. For
each noun, adjective and verb in the sentence this
model takes that word and its part-of-speech tag
and retrieves from WordNet al synonyms from all
synsets the word occurs in. The word is then re-
placed by all of its synset words, and each replace-
ment is scored by a SRILM language model (Stol-
cke, 2002) with probabilities that are obtained from
training on the Simple Wikipedia data. The alter-
native that has the highest probability according to
the language model is kept. If no relevant alterna-
tive is found, the word is left unchanged. We use
the Memory-Based Tagger (Daelemans et al, 1996)
trained on the Brown corpus to compute the part-of-
speech tags. The WordNet::QueryData2 Perl mod-
2http://search.cpan.org/dist/
WordNet-QueryData/QueryData.pm
ule is used to query WordNet (Fellbaum, 1998).
2.2 Zhu et al
Zhu et al (2010) learn a sentence simplification
model which is able to perform four rewrite op-
erations on the parse trees of the input sentences,
namely substitution, reordering, splitting, and dele-
tion. Their model is inspired by syntax-based
SMT (Yamada and Knight, 2001) and consists of
a language model, a translation model and a de-
coder. The four mentioned simplification opera-
tions together form the translation model. Their
model is trained on a corpus containing aligned sen-
tences from English Wikipedia and English Simple
Wikipedia called PWKP. The PWKP dataset con-
sists of 108,016 pairs of aligned lines from 65,133
Wikipedia and Simple Wikipedia articles. These ar-
ticles were paired by following the ?interlanguage
link?3. TF*IDF at the sentence level was used to
align the sentences in the different articles (Nelken
and Shieber, 2006).
Zhu et al (2010) evaluate their system using
BLEU and NIST scores, as well as various read-
ability scores that only take into account the output
sentence, such as the Flesch Reading Ease test and
n-gram language model perplexity. Although their
system outperforms several baselines at the level of
these readability metrics, they do not achieve better
when evaluated with BLEU or NIST.
2.3 RevILP
Woodsend and Lapata?s (2011) model is based
on quasi-synchronous grammar (Smith and Eisner,
2006). Quasi-synchronous grammar generates a
loose alignment between parse trees. It operates on
individual sentences annotated with syntactic infor-
mation in the form of phrase structure trees. Quasi-
synchronous grammar is used to generate all pos-
sible rewrite operations, after which integer linear
programming is employed to select the most ap-
propriate simplification. Their model is trained on
two different datasets: one containing alignments
between Wikipedia and English Simple Wikipedia
(AlignILP), and one containing alignments between
edits in the revision history of Simple Wikipedia
(RevILP). RevILP performs best according to the
3http://en.wikipedia.org/wiki/Help:
Interlanguage_links
1017
human judgements conducted in their study. They
show that it achieves better scores than Zhu et al
(2010)?s system and is not scored significantly dif-
ferently from English Simple Wikipedia. In this
study we compare against their best performing sys-
tem, the RevILP system.
0 2 4 6 8 10 12 14 16 180
1
2
3
4
n-best
Le
ve
ns
ht
ei
n
D
ist
an
ce
0 2 4 6 8 10 12 14 16 180
2
4
6
8
10
12
14
n-best
Fl
es
ch
-K
in
ca
id
Figure 1: Levenshtein distance and Flesch-Kincaid score
of output when varying the n of the n-best output of
Moses.
2.4 PBMT-R
We use the Moses software to train a PBMT
model (Koehn et al, 2007). The data we use is the
PWKP dataset created by Zhu et al (2010). In gen-
eral, a statistical machine translation model finds a
best translation e? of a text in language f to a text
in language e by combining a translation model that
finds the most likely translation p(f |e) with a lan-
guage model that outputs the most likely sentence
p(e):
e? = argmax
e?e?
p(f |e)p(e)
The GIZA++ statistical alignment package is
used to perform the word alignments, which are
later combined into phrase alignments in the Moses
pipeline (Och and Ney, 2003) to build the sentence
simplification model. GIZA++ utilizes IBM Models
1 to 5 and an HMM word alignment model to find
statistically motivated alignments between words.
We first tokenize and lowercase all data and use all
unique sentences from the Simple Wikipedia part
of the PWKP training set to train an n-gram lan-
guage model with the SRILM toolkit to learn the
probabilities of different n-grams. Then we invoke
the GIZA++ aligner using the training simplifica-
tion pairs. We run GIZA++ with standard settings
and we perform no optimization. This results in a
phrase table containing phrase pairs from Wikipedia
and Simple Wikipedia and their conditional proba-
bilities as assigned by Moses. Finally, we use the
Moses decoder to generate simplifications for the
sentences in the test set. For each sentence we let
the system generate the ten best distinct solutions
(or less, if fewer than ten solutions are generated) as
ranked by Moses.
Arguably, dissimilarity is a key factor in simpli-
fication (and in paraphrasing in general). As output
we would like to be able to select fluent sentences
that adequately convey the meaning of the original
input, yet that contain differences that operational-
ize the intended simplification. When training our
PBMT system on the PWKP data we may assume
that the system learns to simplify automatically, yet
there is no aspect of the decoder function in Moses
that is sensitive to the fact that it should try to be
different from the input ? Moses may well trans-
late input to unchanged output, as much of our train-
ing data consists of partially equal input and output
strings.
To expand the functionality of Moses in the in-
tended direction we perform post-hoc re-ranking on
the output based on dissimilarity to the input. We
do this to select output that is as different as possi-
ble from the source sentence, so that it ideally con-
1018
tains multiple simplifications; at the same time, we
base our re-ranking on a top-n of output candidates
according to Moses, with a small n, to ensure that
the quality of the output in terms of fluency and ade-
quacy is also controlled for. Setting n = 10, for each
source sentence we re-rank the ten best sentences
as scored by the decoder according to the Leven-
shtein Distance (or edit distance) measure (Leven-
shtein, 1966) at the word level between the input
and output sentence, counting the minimum num-
ber of edits needed to transform the source string
into the target string, where the allowable edit op-
erations are insertion, deletion, and substitution of a
single word. In case of a tie in Levenshtein Distance,
we select the sequence with the better decoder score.
When Moses is unable to generate ten different sen-
tences, we select from the lower number of outputs.
Figure 1 displays Levenshtein Distance and Flesch-
Kincaid grade level scores for different values of n.
We use the Lingua::EN::Fathom module4 to calcu-
late Flesch-Kincaid grade level scores. The read-
ability score stays more or less the same, indicating
no relation between n and readability. The average
edit distance starts out at just above 2 when selecting
the 1-best output string, and increases roughly until
n = 10.
2.5 Descriptive statistics
Table 2 displays the average edit distance and the
percentage of cases in which no edits were per-
formed for each of the systems and for Simple
Wikipedia. We see that the Levenshtein distance be-
tween Wikipedia and Simple Wikipedia is the most
substantial with an average of 12.3 edits. Given
that the average number of tokens is about 25 for
Wikipedia and 21 for Simple Wikipedia (cf. Ta-
ble 1.1), these numbers indicate that the changes in
Simple Wikipedia go substantially beyond the aver-
age four-word length difference. On average, eight
more words are interchanged for other words. About
half of the original tokens in the source sentence do
not return in the output. Of the three simplifica-
tion systems, the Zhu system (7.95) and the RevILP
(7.18) attain similar edit distances, less substantial
than the edits in Simple Wikipedia, but still consid-
4http://http://search.cpan.org/?kimryan/
Lingua-EN-Fathom-1.15/lib/Lingua/EN/
Fathom.pm
erable compared to the baseline word-substitution
system (4.26) and PBMT-R (3.08). Our system is
clearly conservative in its edits.
System LD Perc. no edits
Simple Wikipedia 12.30 3
Word Sub 4.26 0
Zhu 7.95 2
RevILP 7.18 22
PBMT-R 3.08 5
Table 2: Levenshtein Distance and percentage of unal-
tered output sentences.
On the other hand, we observe some differences
in the percentage of cases in which the systems de-
cide to produce a sentence identical to the input.
In 22 percent of the cases the RevILP system does
not alter the sentence. The other systems make this
decision about as often as the gold standard, Sim-
ple Wikipedia, where only 3% of sentences remain
unchanged. The word-substitution baseline always
manages to make at least one change.
3 Evaluation
3.1 Participants
Participants were 46 students of Tilburg University,
who participated for partial course credits. All were
native speakers of Dutch, and all were proficient in
English, having taken a course on Academic English
at University level.
3.2 Materials
We use the test set used by Zhu et al (2010) and
Woodsend and Lapata (2011). This test set consists
of 100 sentences from articles on English Wikipedia,
paired with sentences from corresponding articles in
English Simple Wikipedia. We selected only those
sentences where every system would perform min-
imally one edit, because we only want to compare
the different systems when they actually generate al-
tered, assumedly simplified output. From this sub-
set we randomly pick 20 source sentences, result-
ing in 20 clusters of one source sentence and 5 sim-
plified sentences, as generated by humans (Simple
Wikipedia) and the four systems.
1019
3.3 Procedure
The participants were told that they participated in
the evaluation of a system that could simplify sen-
tences, and that they would see one source sentence
and five automatically simplified versions of that
sentence. They were not informed of the fact that we
evaluated in fact four different systems and the orig-
inal Simple Wikipedia sentence. Following earlier
evaluation studies (Doddington, 2002; Woodsend
and Lapata, 2011), we asked participants to evalu-
ate Simplicity, Fluency and Adequacy of the target
headlines on a five point Likert scale. Fluency was
defined in the instructions as the extent to which a
sentence is proper, grammatical English. Adequacy
was defined as the extent to which the sentence has
the same meaning as the source sentence. Simplic-
ity was defined as the extent to which the sentence
was simpler than the original and thus easier to un-
derstand. The order in which the clusters had to be
judged was randomized and the order of the output
of the various systems was randomized as well.
4 Results
4.1 Automatic measures
The results of the automatic measures are displayed
in Table 3. In terms of the Flesch-Kincaid grade
level score, where lower scores are better, the Zhu
system scores best, with 7.86 even lower than Sim-
ple Wikipedia (8.57). Increasingly worse Flesch-
Kincaid scores are produced by RevILP (8.61) and
PBMT-R (13.38), while the word substitution base-
line scores worst (14.64). With regard to the BLEU
score, where Simple Wikipedia is the reference, the
PBMT-R system scores highest with 0.43, followed
by the RevILP system (0.42) and the Zhu system
(0.38). The word substitution baseline scores low-
est with a BLEU score of 0.34.
System Flesch-Kincaid BLEU
Simple Wikipedia 8.57 1
Word Sub 14.64 0.34
Zhu 7.86 0.38
RevILP 8.61 0.42
PBMT-R 13.38 0.43
Table 3: Flesch-Kincaid grade level and BLEU scores
4.2 Human judgements
To test for significance we ran repeated mea-
sures analyses of variance with system (Sim-
ple Wikipedia, PBMT-R, Zhu, RevILP, word-
substitution baseline) as the independent variable,
and the three individual metrics as well as their com-
bined mean as the dependent variables. Mauchlys
test for sphericity was used to test for homogeneity
of variance, and when this test was significant we
applied a Greenhouse-Geisser correction on the de-
grees of freedom (for the purpose of readability we
report the normal degrees of freedom in these cases).
Planned pairwise comparisons were made with the
Bonferroni method. Table 4 displays these results.
First, we consider the 3 metrics in isolation, be-
ginning with Fluency. We find that participants
rated the Fluency of the simplified sentences from
the four systems and Simple Wikipedia differently,
F (4, 180) = 178.436, p < .001, ?2p = .799. The
word-substitution baseline, Simple Wikipedia and
PBMT-R receive the highest scores (3.86, 3.84 and
3.83 respectively) and don?t achieve significantly
different scores on this dimension. All other pair-
wise comparisons are significant at p < .001. Rev-
ILP attains a score of 3.18, while the Zhu system
achieves the lowest mean judgement score of 2.59.
Participants also rated the systems significantly
differently on the Adequacy scale, F (4, 180) =
116.509, p < .001, ?2p = .721. PBMT-R scores
highest (3.71), followed by the word-substitution
baseline (3.58), RevILP (3.28), and then by Simple
Wikipedia (2.91) and the Zhu system (2.82). Sim-
ple Wikipedia and the Zhu system do not differ sig-
nificantly, and all other pairwise comparisons are
significant at p < .001. The low score of Simple
Wikipedia indicates indirectly that the human edi-
tors of Simple Wikipedia texts often choose to devi-
ate quite markedly from the meaning of the original
text.
Key to the task of simplification are the hu-
man judgements of Simplicity. Participants rated
the Simplicity of the output from the four sys-
tems and Simple Wikipedia differently, F (4, 180) =
74.959, p < .001, ?2p = .625. Simple Wikipedia
scores highest (3.68) and the word substitution base-
line scores lowest (2.42). Between them are the
RevILP (2.96), Zhu (2.93) and PBMT-R (2.88) sys-
1020
System Overall Fluency Adequacy Simplicity
Simple Wikipedia 3.46 (0.39) 3.84 (0.46) 2.91 (0.32) 3.68 (0.39)
Word Sub 3.39 (0.43) 3.86 (0.49) 3.58 (0.35) 2.42 (0.48)
Zhu 2.78 (0.45) 2.59 (0.48) 2.82 (0.37) 2.93 (0.50)
RevILP 3.13 (0.36) 3.18 (0.45) 3.28 (0.32) 2.96 (0.39)
PBMT-R 3.47 (0.46) 3.83 (0.49) 3.71 (0.44) 2.88 (0.46)
Table 4: Mean scores assigned by human subjects, with the standard deviation between brackets
Adequacy Simplicity Flesch-Kincaid BLEU
Fluency 0.45** 0.24* 0.42** 0.26**
Adequacy -0.19 0.40** -0.14
Simplicity -0.45** 0.42**
Flesch-Kincaid -0.11
Table 5: Pearson correlation between the different dimensions as assigned by humans and the automatic metrics.
Scores marked * are significant at p < .05 and scores marked ** are significant at p < .01
tems, which do not score significantly differently
from each other. All other pairwise comparisons are
significant at p < .001.
Finally we report on a combined score created by
averaging over the Fluency, Adequacy and Simplic-
ity scores. Inspection of this score, displayed in the
leftmost column of Table 4, reveals that the PBMT-
R system and Simple Wikipedia score best (3.47
and 3.46 respectively), followed by the word substi-
tution baseline (3.39), which in turn scores higher
than RevILP (3.13) and the Zhu system (2.78).
We find that participants rated the systems signifi-
cantly differently overall, F (4, 180) = 98.880, p <
.001, ?2p = .687. All pairwise comparisons were sta-
tistically significant (p < .01), except the one be-
tween the PBMT-R system and Simple Wikipedia.
4.3 Correlations
Table 5 displays the correlations between the scores
assigned by humans (Fluency, Adequacy and Sim-
plicity) and the automatic metrics (Flesch-Kincaid
and BLEU). We see a significant correlation be-
tween Fluency and Adequacy (0.45), as well as be-
tween Fluency and Simplicity (0.24). There is a neg-
ative significant correlation between Flesch-Kincaid
scores and Simplicity (-0.45) while there is a posi-
tive significant correlation between Flesch-Kincaid
and Adequacy and Fluency. The significant correla-
tions between BLEU and Simplicity (0.42) and Flu-
ency (0.26) are both in the positive direction. There
is no significant correlation between BLEU and Ad-
equacy, indicating BLEU?s relative weakness in as-
sessing the semantic overlap between input and out-
put. BLEU and Flesch-Kincaid do not show a sig-
nificant correlation.
5 Discussion
We conclude that a phrase-based machine trans-
lation system with added dissimilarity-based re-
ranking of the best ten output sentences can suc-
cessfully be used to perform sentence simplifica-
tion. Even though the system merely performs
phrase-based machine translation and is not specif-
ically geared towards simplification were it not for
the dissimilarity-based re-ranking of the output, it
performs not significantly differently from state-of-
the-art sentence simplification systems in terms of
human-judged Simplification. In terms of Fluency
and Adequacy our system is judged to perform sig-
nificantly better. From the relatively low average
numbers of edits made by our system we can con-
clude that our system performs relatively small num-
bers of changes to the input, that still constitute as
sensible simplifications. It does not split sentences
(which the Zhu and RevILP systems regularly do);
it only rephrases phrases. Yet, it does this better
than a word-substitution baseline, which can also be
considered a conservative approach; this is reflected
in the baseline?s high Fluency score (roughly equal
to PBMT-R and Simple Wikipedia) and Adequacy
score (only slightly worse than PBMT-R).
1021
Wikipedia the judge ordered that chapman should receive psychiatric treatment in prison and sentenced
him to twenty years to life , slightly less than the maximum possible of twenty-five years to
life .
Simple
Wikipedia
he was sentenced to twenty-five years to life in prison in 1981 .
Word-
substitution
baseline
the judge ordered that chapman should have psychiatric treatment in prison and sentenced
him to twenty years to life , slightly less than the maximum possible of twenty-five years to
life .
Zhu the judge ordered that chapman should get psychiatric treatment . in prison and sentenced
him to twenty years to life , less maximum possible of twenty-five years to life .
RevILP the judge ordered that chapman should will get psychiatric treatment in prison . he sentenced
him to twenty years to life to life .
PBMT-R the judge ordered that chapman should get psychiatric treatment in prison and sentenced him
to twenty years to life , a little bit less than the highest possible to twenty-five years to life .
Table 6: Example output
The output of all systems, the original and the
simplified version of an example sentence from the
PWKP dataset is displayed in Table 6. The Simple
Wikipedia sentences illustrate that significant por-
tions of the original sentences may be dropped, and
parts of the semantics of the original sentence dis-
carded. We also see the Zhu and RevILP systems
resorting to splitting the original sentence in two,
leading to better Flesch-Kincaid scores. The word-
substitution baseline changes ?receive? in ?have?,
while the PBMT-R system changes the same ?re-
ceive? in ?get?, ?slightly? to ?a little bit?, and ?maxi-
mum? to ?highest?.
In terms of automatic measures we see that the
Zhu system scores particularly well on the Flesch-
Kincaid metric, while the RevILP system and our
PBMT-R system achieve the highest BLEU scores.
We believe that for the evaluation of sentence sim-
plification, BLEU is a more appropriate metric than
Flesch-Kincaid or a similar readability metric, al-
though it should be noted that BLEU was found only
to correlate significantly with Fluency, not with Ad-
equacy. While BLEU and NIST may be used with
this in mind, readability metrics should be avoided
altogether in our view. Where machine translation
evaluation metrics such as BLEU take into account
gold references, readability metrics only take into
account characteristics of the sentence such as word
length and sentence length, and ignore grammatical-
ity or the semantic adequacy of the content of the
output sentence, which BLEU is aimed to implic-
itly approximate by measuring overlap in n-grams.
Arguably, readability metrics are best suited to be
applied to texts that can be considered grammati-
cal and meaningful, which is not necessarily true for
the output of simplification algorithms. A disrup-
tive example that would illustrate this point would
be a system that would randomly split original sen-
tences in two or more sequences, achieving consid-
erably lower Flesch-Kincaid scores, yet damaging
the grammaticality and semantic coherence of the
original text, as is evidenced by the negative cor-
relation for Simplicity and positive correlations for
Fluency and Adequacy in Table 5.
In the future we would like to investigate how we
can boost the number of edits the system performs,
while still producing grammatical and meaning-
preserving output. Although the comparison against
the Zhu system, which uses syntax-driven machine
translation, shows no clear benefit for syntax-based
machine translation, it may still be the case that ap-
proaches such as Hiero (Chiang et al, 2005) and
Joshua (Li et al, 2009), enhanced by dissimilarity-
based re-ranking, would improve over our current
system. Furthermore, typical simplification oper-
ations such as sentence splitting and more radical
syntax alterations or even document-level operations
such as manipulations of the co-reference structure
would be interesting to implement and test
Acknowledgements
We are grateful to Zhemin Zhu and Kristian Woods-
end for sharing their data. We would also like to
thank the anonymous reviewers for their comments.
1022
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 597?604,
Morristown, NJ, USA. Association for Computational
Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints
on paraphrases extracted from parallel corpora. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 196?205, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yvonne Canning, John Tait, Jackie Archibald, and Ros
Crawley. 2000. Cohesive regeneration of syntacti-
cally simplified newspaper text. In Proceedings of RO-
MAND 2000, Lausanne.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of English newspaper text to assist aphasic readers.
In AAAI-98 Workshop on Integrating Artificial Intelli-
gence and Assistive Technology, Madison, Wisconsin.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, and John Tait. 1999. Sim-
plifying text for language-impaired readers. In Pro-
ceedings of EACL?99, Bergen. ACL.
R. Chandrasekar and B. Srinivas. 1997. Automatic
rules for text simplification. Knowledge-Based Sys-
tems, 10:183?190.
Raman Chandrasekar, Christine Doran, and Bangalore
Srinivas. 1996. Motivations and methods for text
simplification. In Proceedings of the Sixteenth In-
ternational Conference on Computational Linguistics
(COLING?96), pages 1041?1044.
David Chiang, Adam Lopez, Nitin Madnani, Christof
Monz, Philip Resnik, and Michael Subotin. 2005. The
hiero machine translation system: extensions, evalua-
tion, and analysis. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages 779?
786, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Will Coster and David Kauchak. 2011. Learning to
simplify sentences using wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gener-
ation, pages 1?9, Portland, Oregon, June. Association
for Computational Linguistics.
Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven
Gillis. 1996. MBT: A Memory-Based Part of Speech
Tagger-Generator. In Proc. of Fourth Workshop on
Very Large Corpora, pages 14?27. ACL SIGDAT.
Walter Daelemans, Anja Hothker, and Erik Tjong
Kim Sang. 2004. Automatic sentence simplification
for subtitling in dutch and english. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation, pages 1045?1048.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, HLT ?02, pages 138?145, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press, May.
Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 177?185, Hon-
olulu, Hawaii, October. Association for Computational
Linguistics.
Kentaro Inui, Atsushi Fujita, Tetsuro Takahashi, Ryu
Iida, and Tomoya Iwakura. 2003. Text simplification
for reading assistance: A project note. In Proceedings
of the Second International Workshop on Paraphras-
ing, pages 9?16, Sapporo, Japan, July. Association for
Computational Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization ? step one: Sentence compression. In
Proceedings of the 17th National Conference on Ar-
tificial Intelligence (AAAI), pages 703 ? 710, Austin,
Texas, USA, July 30 ? August 3.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C.
Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens,
Chris Dyer, Ondrej Bojar, Alexandra Constantin, and
Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL. The Associa-
tion for Computer Linguistics.
V. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: an open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 135?139, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
Proceedings of the Second Workshop on Statistical
Machine Translation, StatMT ?07, pages 120?127,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
1023
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for monolin-
gual corpora. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL-06), Trento, Italy, 3?7
April.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51, March.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 142?149, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Advaith Siddharthan. 2002. An architecture for a text
simplification system. In Language Engineering Con-
ference, page 64. IEEE Computer Society.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine Translation,
pages 23?30, New York, June.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In In Proc. Int. Conf. on
Spoken Language Processing, pages 901?904, Denver,
Colorado.
D. Vickrey and D. Koller. 2008. Sentence simplification
for semantic role labeling. In Proceedings of the 46th
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies.
Willian Massami Watanabe, Arnaldo Candido Junior,
Vincius Rodriguez de Uz?da, Renata Pontin de Mat-
tos Fortes, Thiago Alexandre Salgueiro Pardo, and
Sandra M. Alusio. 2009. Facilita: reading assistance
for low-literacy readers. In Brad Mehlenbacher, Aris-
tidis Protopsaltis, Ashley Williams, and Shaun Slat-
tery, editors, SIGDOC, pages 29?36. ACM.
Kristian Woodsend and Mirella Lapata. 2011. Learning
to simplify sentences with quasi-synchronous gram-
mar and integer programming. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 409?420, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Sander Wubben, Antal van den Bosch, and Emiel Krah-
mer. 2010. Paraphrase generation as monolingual
translation: data and evaluation. In Proceedings of the
6th International Natural Language Generation Con-
ference, INLG ?10, pages 203?207, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings of
the 39th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?01, pages 523?530, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Unsupervised extraction of lexical simplifications
from Wikipedia. In Proceedings of the NAACL, pages
365?368.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2 - Volume 2, ACL ?09,
pages 834?842, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model for
sentence simplification. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 1353?1361, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
1024
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 260?263,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
UvT: Memory-based pairwise ranking of paraphrasing verbs
Sander Wubben
Tilburg centre for Cognition and Communication
Tilburg University
The Netherlands
s.wubben@uvt.nl
Abstract
In this paper we describe Mephisto, our
system for Task 9 of the SemEval-2 work-
shop. Our approach to this task is to de-
velop a machine learning classifier which
determines for each verb pair describing
a noun compound which verb should be
ranked higher. These classifications are
then combined into one ranking. Our clas-
sifier uses features from the Google N-
gram Corpus, WordNet and the provided
training data.
1 Introduction
We interpret the task of ranking a set of given
paraphrasing verbs as described by Butnariu et
al (2010) as a competition between these verbs.
Each verb competes with every other verb in the
set and receives a positive score if it is more likely
to describe the given noun compound (NC) than
the other verb and a negative score if it is less
likely to describe the NC. In line with this ap-
proach we regard the task as a classification prob-
lem where for each comparison our classification
algorithm picks the paraphrasing verb that is more
likely to describe the NC. This brings the clas-
sification problem down to three classes: higher,
equal or lower. Sometimes the paraphrasing verbs
are accompanied by a preposition. In this paper we
will simply refer to all verbs and verb-prepositions
as verbs.
The distribution of the verbs in the training data
provides us already with valuable information. We
incorporate basic features describing this distribu-
tion to train our classifier. We also need addi-
tional semantic features that provide us with in-
sight into the relation between the NC and the
verb, therefore we use features constructed from
WordNet and the Google N-gram Corpus to train
our Memory-based paraphrase interpretation scor-
ing tool (Mephisto).
2 System Description
The system consists of three components: the fea-
ture extraction component, the classification com-
ponent and the ranking component. We will de-
scribe all three components.
2.1 Feature Extraction
For each verb describing an NC we try to extract
those features that describe the probability that this
verb is a good interpretation of the NC. We assume
that given a NC N
1
N
2
and a verb V , the NC inter-
pretation should be N
2
V N
1
. The phrase ?Butter
made from peanuts? adequately describes peanut
butter.
The training data provides us with a total of
17,727 instances of NC verb pairs scored by hu-
man judges. This can be broken down into 4,360
unique verb phrases describing 250 NCs. This
distribution already gives us a good clue when we
are generating new rankings. The following are
the features we used:
Weighted mean in training data For each NC
that has to be ranked we find the most similar NC
in the training data by measuring the overlap in
verb phrases between the two NCs. We do this by
calculating the Jaccard coefficient over the sets of
verbs associated with the NCs. We adapt the high-
est ranking NC as most similar to our candidate
NC (the NC with most matching verbs). For each
verb V we then calculate the score as follows:
Score = J ? S
sim
+ (1 ? J) ?M
where J is the Jaccard score, S
sim
is the
assigned score of the verb in the most similar
set and M is the mean score for the verb in the
training data.
Rank in training data For this feature we
directly compare the two verbs V
1
and V
2
. We just
260
feature values info gain gain ratio
verb1 4,093 0.24 0.02
verb2 4,093 0.24 0.02
verb1-verb2 768,543 1.06 0.06
verb1-verb2-LCS 986,031 1.29 0.07
n-gram score1 7 0.07 0.02
n-gram score2 7 0.01 0.08
weighted mean 7 0.29 0.12
rank 3 0.68 0.43
Table 1: Features used in our system
count the number of times that V
1
is ranked higher
than V
2
and vice versa for every NC where both
verbs occur. We end up with a positive, equal or
negative class.
WordNet Least Common Subsumer In order
to distinguish between different kinds of NCs we
use WordNet (Fellbaum, 1998) to determine the
kind of relation between the nouns. This idea is
supported by work by Levi (1978), Warren (1978)
and Nastase & Szpakowicz (2003). Our intuition
is that the ranking of verb phrases is very depen-
dent on this relation between the nouns. To deter-
mine this we use the WordNet::QueryData (Ren-
nie, 2000) module. In the WordNet graph we look
for the Least Common Subsumer (LCS) of the
two nouns. The LCS is the lowest parent node of
both nouns. We combine the LCS with both verb
phrases into one feature.
Google N-gram features We use the Google N-
gram corpus to count co-occurence frequencies of
certain n-grams. An NC occurring often together
with a certain verb should indicate that that verb
is a good paraphrase for the NC. Using web text
for various NLP-tasks has been proven to be use-
ful (Lapata and Keller, 2005), also for NC inter-
pretation (Nakov and Hearst, 2005). Because of
data sparseness and the unlikelihood of finding a
perfect match for a certain n-gram, we adopt dif-
ferent strategies for constructing features. First of
all, we try to relax the matching conditions by ap-
plying certain regular expression. Given the NC
?abortion problem? and the paraphrasing verb ?be
related to? , it seems unlikely you will ever en-
counter the n-gram ?problem be related to abor-
tion?, yet in the training data ?be related to? is the
number three verb for ?abortion problem?. There-
fore, we first apply some simple inflection. Instead
of ?be? we match on ?is/are/being?. and we do a
comparable inflection for other verbs transforming
+up+ -dwn- =eq=
+up+ 23,494 7,099 8,912
-dwn- 7,168 23,425 8,912
=eq= 22,118 22,084 22,408
Table 2: Confusion matrix of the classes, with hor-
izontally the output classes and vertically the tar-
get classes
a verb such as ?involve? into ?involves/involving?.
Additionally we also match on singular and plural
nouns. We then use two different techniques to
find the n-gram frequencies:
N ? gram
1
=
f(N
2
V ) + f(V N
1
)
f(V )
N ? gram
2
=
f(N
2
V N
1
)
f(V )
where f stands for the occurrences of the given
sequences of nouns and verb. We do not divide by
noun occurrences because they are constant for
every pair of verbs we compare.
Pairwise comparison of features For each
verb pair in an NC set we compare all numeric
features and assign one of the following symbols
to characterize the relation of the two verbs:
+++: V
1
score is more than 10 times V
2
score
++: V
1
score is between 2 and 10 times V
2
score
+: V
1
score is between 1 and 2 times verb2 score
=: scores are equal
-: V
2
score is between 1 and 2 times V
1
score
- -: V
2
score is between 2 and 10 times V
1
score
- - -: V
2
score is more than 10 times V
1
score
An overview of the features is displayed in Ta-
ble 1.
2.2 Classification
Our system makes use of Memory-Based Learn-
ing (MBL) for classification. MBL stores feature
representations of training instances in memory
without abstraction and classifies unseen instances
by matching their feature representation to all in-
stances in memory, finding the most similar in-
stances. The class of these most similar instances
is then copied to the new instance The learning
algorithm our system uses is the IB1 classier as
implemented in TiMBL (version 6.1.5). IB1 is a
supervised decision-tree-based implementation of
261
Settings TiMBL F-score Spearman ? Pearson r KullbackLeibler div.
k=3 all features 0.48 0.50 0.44 1.91
k=3 no external features 0.53 0.48 0.41 2.05
k=11 all features 0.51 0.50 0.42 1.97
k=11 no external features 0.20 - - -
Table 3: Results for different settings on the development set
the k-nearest neighbor algorithm for learning clas-
sification tasks (Aha et al, 1991). The TiMBL pa-
rameters we used in the Mephisto system for the
IB1 classifier are the overlap metric, weighting us-
ing GainRatio, and k=3, taking into account the
instances on the 3 most similar positions to extrap-
olate the class of the instance. More information
about these settings can be found in the TiMBL
reference guide (Daelemans et al, 2009). We train
our classifier on the provided training data to clas-
sify instances into one of three classes; +up+ if
V
1
ranks higher than V
2
, =eq= if both verbs rank
equally and -dwn- if V
1
ranks lower than V
2
.
2.3 Ranking
The final step is to combine all the classification
into one score per verb. This is done in a very
straight forward way: a verb receives one point
every time it is classified as +up+. This results in
scores for each verb paraphrasing an NC. We then
perform a simple post processing step: we reas-
sign classes to each verb based on the final scores
they have received and recalculate their scores. We
repeat this process until the scores converge.
3 Results
For development the original training set was di-
vided in a development training set of 15,966 lines
and a development test set of 1,761 lines, which
contains 23 NCs. The distribution and ranking fea-
tures were calculated using only the development
training set. Because we compare for each NC ev-
ery verb to every other verb the TiMBL training
instance-base contains 1,253,872 lines, and the de-
velopment test set 145,620. The results for differ-
ent settings are in Table 3. Although the TiMBL
F-score (macro-averaged) of using all features is
actually lower than using only semantic features at
k=3, the final correlations are in favor of using all
features. There does not seem to be an improve-
ment when extrapolating from 11 neighbouring in-
stances in the instance-base over 3. In fact, when
using no external features and k=11, the classifier
overgeneralizes and classifies every instance as
=eq= and consequently does not provide a ranking
System Spearman ? Pearson r Cosine
UvT-MEPHISTO 0.450 0.411 0.635
UCD-PN 0.441 0.361 0.669
UCD-GOGGLE-III 0.432 0.395 0.652
UCD-GOGGLE-II 0.418 0.375 0.660
UCD-GOGGLE-I 0.380 0.252 0.629
UCAM 0.267 0.219 0.374
NC-INTERP 0.186 0.070 0.466
Baseline 0.425 0.344 0.524
Table 4: Final results for SemEval-2 Task 9
at all. Additionally, classifying with k=11 takes
considerably longer than with k=3. The settings
we use for our final system are k=3 and we use all
features. Table 2 displays a confusion matrix of
the classification on the development test set. Not
surprisingly the classifier is very bad at recogniz-
ing the =eq= class. These mistakes are not as bad
as miss-classifying a +up+ instance as -dwn- and
vice versa, and fortunately these mistakes happen
less often.
The official test set contains 32,830 instances,
almost twice as many as the training set. This
breaks down into 2,837,226 cases to classify. In
Table 4 are the final results of the task with all
participating systems and their macro-averaged
Spearman, Pearson and Cosine correlation. Also
shown is the baseline, which involves scoring a
given verb paraphrase by its frequency in the train-
ing set. The final results are quite a bit lower than
the results on the development set. This could
be coincidence (the final test set is about twenty
times larger than our development test set), but it
could also be due to overfitting on the development
set. The ten best and worst scoring compounds are
shown in Table 5 with their Least Common Sub-
sumer as taken from WordNet. The best-scoring
NC ?jute products? achieves a Spearman ? of 0.75
while the worst-scoring compound, ?electron mi-
croscope? only achieves 0.12.
4 Conclusion
We have shown that a Memory-based pairwise
approach to ranking with features taken from
WordNet and the Google N-gram corpus achieves
262
Best scoring NCs LCS Spearman ?
jute products physical entity 0.75
ceramics products artifact 0.75
steel frame physical entity 0.74
cattle population entity 0.74
metal body physical entity 0.74
winter blooming entity 0.73
warbler family entity 0.72
wool scarf artifact 0.71
fiber optics physical entity 0.70
petroleum products physical entity 0.70
Worst scoring NCs LCS Spearman ?
electron microscope whole 0.12
light bulb physical entity 0.15
yesterday evening measure 0.16
student loan entity 0.16
theater orchestra entity 0.17
sunday restrictions abstraction 0.20
yesterday afternoon measure 0.20
relations agency abstraction 0.21
crime novelist entity 0.21
office buildings structure 0.21
Table 5: Best and worst scoring noun compounds
with their Least Common Subsumer and Spear-
man ? correlation
good results on the task of ranking verbs para-
phrasing noun compounds. We outperform the
strong baseline and also systems using an unsuper-
vised approach. If we analyse our results we see
that our system scores particularly well on noun
compounds describing materials: in Table 5 we
see that all top ten compounds are either ?arti-
facts?, ?physical entities? or ?entities? according
to WordNet and the relation is quite direct: gen-
erally a made of relation seems appropriate. If we
look at the bottom ten on the other hand, we see re-
lations such as ?abstraction? and ?measure?: these
are harder to qualify. Also, an ?electron micro-
scope? will generally not be perceived as a micro-
scope made of electrons. We can conclude that for
NCs where the relation between the nouns is more
obscure the verbs are harder to rank.
If we look at the Information Gain Ratio, of
all features the rank difference of the verbs in the
training data seems to be the strongest feature, and
of the external features the frequency difference of
the entire phrase containing the NC and the verb.
A lot more investigations could be made into the
viability of using large n-gram collections such as
the Google N-gram corpus for paraphrase tasks.
It might also be interesting to explore a some-
what more challenging variant of this task by not
providing the verbs to be ranked a priori. This
would probably be more interesting for real world
applications because often the task is not only
ranking but finding the verbs in the first place. Our
system should be able to handle this task with mi-
nor modifications: we simply regards all verbs in
the training-data candidates to be ranked. Then,
a pre-filtering step should take place to weed out
irrelevant verbs based on an indicator such as the
LCS of the nouns. In addition a threshold could be
implemented to only accept a (further) limited set
of verbs in the final ranking.
References
David W. Aha, Dennis Kibler, and Marc K. Albert.
1991. Instance-based learning algorithms. Mach.
Learn.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-
armuid
?
O S?eaghdha, Stan Szpakowicz, and Tony
Veale. 2010. Semeval-2 task 9: The interpreta-
tion of noun compounds using paraphrasing verbs
and prepositions. In Proceedings of the 5th SIGLEX
Workshop on Semantic Evaluation.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2009. Timbl: Tilburg
memory-based learner - version 6.2 - reference
guide.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM
Trans. Speech Lang. Process.
Judith N. Levi. 1978. The Syntax and Semantics of
Complex Nominals.
Preslav Nakov and Marti Hearst. 2005. Search engine
statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of the 9th
Conference on Computational Natural Language
Learning.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings
of the 5th International Workshop on Computational
Semantics.
Jason Rennie. 2000. Wordnet::querydata: a perl mod-
ule for accessing the wordnet database.
Beatrice Warren. 1978. Semantic Patterns of Noun-
Noun Compounds.
263
Paraphrase Generation as Monolingual Translation: Data and Evaluation
Sander Wubben, Antal van den Bosch, Emiel Krahmer
Tilburg centre for Cognition and Communication
Tilburg University
Tilburg, The Netherlands
{s.wubben,antal.vdnbosch,e.j.krahmer}@uvt.nl
Abstract
In this paper we investigate the auto-
matic generation and evaluation of senten-
tial paraphrases. We describe a method
for generating sentential paraphrases by
using a large aligned monolingual cor-
pus of news headlines acquired automat-
ically from Google News and a stan-
dard Phrase-Based Machine Translation
(PBMT) framework. The output of this
system is compared to a word substitu-
tion baseline. Human judges prefer the
PBMT paraphrasing system over the word
substitution system. We demonstrate that
BLEU correlates well with human judge-
ments provided that the generated para-
phrased sentence is sufficiently different
from the source sentence.
1 Introduction
Text-to-text generation is an increasingly studied
subfield in natural language processing. In con-
trast with the typical natural language generation
paradigm of converting concepts to text, in text-
to-text generation a source text is converted into a
target text that approximates the meaning of the
source text. Text-to-text generation extends to
such varied tasks as summarization (Knight and
Marcu, 2002), question-answering (Lin and Pan-
tel, 2001), machine translation, and paraphrase
generation.
Sentential paraphrase generation (SPG) is the
process of transforming a source sentence into a
target sentence in the same language which dif-
fers in form from the source sentence, but approx-
imates its meaning. Paraphrasing is often used as
a subtask in more complex NLP applications to
allow for more variation in text strings presented
as input, for example to generate paraphrases of
questions that in their original form cannot be an-
swered (Lin and Pantel, 2001; Riezler et al, 2007),
or to generate paraphrases of sentences that failed
to translate (Callison-Burch et al, 2006). Para-
phrasing has also been used in the evaluation of
machine translation system output (Russo-Lassner
et al, 2006; Kauchak and Barzilay, 2006; Zhou
et al, 2006). Adding certain constraints to para-
phrasing allows for additional useful applications.
When a constraint is specified that a paraphrase
should be shorter than the input text, paraphras-
ing can be used for sentence compression (Knight
and Marcu, 2002; Barzilay and Lee, 2003) as well
as for text simplification for question answering or
subtitle generation (Daelemans et al, 2004).
We regard SPG as a monolingual machine trans-
lation task, where the source and target languages
are the same (Quirk et al, 2004). However, there
are two problems that have to be dealt with to
make this approach work, namely obtaining a suf-
ficient amount of examples, and a proper eval-
uation methodology. As Callison-Burch et al
(2008) argue, automatic evaluation of paraphras-
ing is problematic. The essence of SPG is to gen-
erate a sentence that is structurally different from
the source. Automatic evaluation metrics in re-
lated fields such as machine translation operate on
a notion of similarity, while paraphrasing centers
around achieving dissimilarity. Besides the eval-
uation issue, another problem is that for an data-
driven MT account of paraphrasing to work, a
large collection of data is required. In this case,
this would have to be pairs of sentences that are
paraphrases of each other. So far, paraphrasing
data sets of sufficient size have been mostly lack-
ing. We argue that the headlines aggregated by
Google News offer an attractive avenue.
2 Data Collection
Currently not many resources are available for
paraphrasing; one example is the Microsoft Para-
phrase Corpus (MSR) (Dolan et al, 2004; Nelken
and Shieber, 2006), which with its 139,000 aligned
Police investigate Doherty drug pics
Doherty under police investigation 
Police to probe Pete pics 
Pete Doherty arrested in drug-photo probe 
Rocker photographed injecting unconscious fan 
Doherty ?injected unconscious fan with drug? 
Photos may show Pete Doherty injecting passed-out fan
Doherty ?injected female fan? 
Figure 1: Part of a sample headline cluster, with
aligned paraphrases
paraphrases can be considered relatively small. In
this study we explore the use of a large, automat-
ically acquired aligned paraphrase corpus. Our
method consists of crawling the headlines aggre-
gated and clustered by Google News and then
aligning paraphrases within each of these clusters.
An example of such a cluster is given in Figure 1.
For each pair of headlines in a cluster, we calcu-
late the Cosine similarity over the word vectors of
the two headlines. If the similarity exceeds a de-
fined upper threshold it is accepted; if it is below
a defined lower threshold it is rejected. In the case
that it lies between the thresholds, the process is
repeated but then with word vectors taken from a
snippet from the corresponding news article. This
method, described in earlier work Wubben et al
(2009), was reported to yield a precision of 0.76
and a recall of 0.41 on clustering actual Dutch
paraphrases in a headline corpus. We adapted this
method to English. Our data consists of English
headlines that appeared in Google News over the
period of April to September 2006. Using this
method we end up with a corpus of 7,400,144 pair-
wise alignments of 1,025,605 unique headlines1.
3 Paraphrasing methods
In our approach we use the collection of au-
tomatically obtained aligned headlines to train
a paraphrase generation model using a Phrase-
Based MT framework. We compare this ap-
proach to a word substitution baseline. The gen-
erated paraphrases along with their source head-
1This list of aligned pairs is available at
http://ilk.uvt.nl/?swubben/resources.html
lines are presented to human judges, whose rat-
ings are compared to the BLEU (Papineni et al,
2002), METEOR (Banerjee and Lavie, 2005) and
ROUGE (Lin, 2004) automatic evaluation metrics.
3.1 Phrase-Based MT
We use the MOSES package to train a
Phrase-Based Machine Translation model
(PBMT) (Koehn et al, 2007). Such a model
normally finds a best translation e? of a text in
language f to a text in language e by combining
a translation model p(f |e) with a language model
p(e):
e? = argmax
e?e?
p(f |e)p(e)
GIZA++ is used to perform the word align-
ments (Och and Ney, 2003) which are then used in
the Moses pipeline to generate phrase alignments
in order to build the paraphrase model. We first to-
kenize our data before training a recaser. We then
lowercase all data and use all unique headlines in
the training data to train a language model with the
SRILM toolkit (Stolcke, 2002). Then we invoke
the GIZA++ aligner using the 7M training para-
phrase pairs. We run GIZA++ with standard set-
tings and we perform no optimization. Finally, we
use the MOSES decoder to generate paraphrases
for our test data.
Instead of assigning equal weights to language
and translation model, we assign a larger weight
of 0.7 to the language model to generate better
formed (but more conservative) paraphrases. Be-
cause dissimilarity is a factor that is very impor-
tant for paraphrasing but not implemented in a
PBMT model, we perform post-hoc reranking of
the different candidate outputs based on dissimi-
larity. For each headline in the testset we generate
the ten best paraphrases as scored by the decoder
and then rerank them according to dissimilarity to
the source using the Levenshtein distance measure
at the word level. The resulting headlines are re-
cased using the previously trained recaser.
3.2 Word Substitution
We compare the PBMT results with a simple word
substitution baseline. For each noun, adjective and
verb in the sentence this model takes that word and
its Part of Speech tag and retrieves from Word-
Net its most frequent synonym from the most fre-
quent synset containing the input word. We use the
Memory Based Tagger (Daelemans et al, 1996)
System Headline
Source Florida executes notorious serial killer
PBMT Serial killer executed in Florida
Word Sub. Florida executes ill-famed series slayer
Source Dublin evacuates airport due to bomb scare
PBMT Dublin airport evacuated after bomb threat
Word Sub. Dublin evacuates airdrome due to bomb panic
Source N. Korea blasts nuclear sanctions
PBMT N. Korea nuclear blast of sanctions
Word Sub. N. Korea blasts atomic sanctions
Table 1: Examples of generated paraphrased head-
lines
trained on the Brown corpus to generate the POS-
tags. The WordNet::QueryData2 Perl module is
used to query WordNet (Fellbaum, 1998). Gener-
ated headlines and their source for both systems
are given in Table 1.
4 Evaluation
For the evaluation of the generated paraphrases
we set up a human judgement study, and compare
the human judges? ratings to automatic evaluation
measures in order to gain more insight in the auto-
matic evaluation of paraphrasing.
4.1 Method
We randomly select 160 headlines that meet the
following criteria: the headline has to be compre-
hensible without reading the corresponding news
article, both systems have to be able to produce a
paraphrase for each headline, and there have to be
a minimum of eight paraphrases for each headline.
We need these paraphrases as multiple references
for our automatic evaluation measures to account
for the diversity in real-world paraphrases, as the
aligned paraphrased headlines in Figure 1 witness.
The judges are presented with the 160 head-
lines, along with the paraphrases generated by
both systems. The order of the headlines is ran-
domized, and the order of the two paraphrases for
each headline is also randomized to prevent a bias
towards one of the paraphrases. The judges are
asked to rate the paraphrases on a 1 to 7 scale,
where 1 means that the paraphrase is very bad and
7 means that the paraphrase is very good. The
judges were instructed to base their overall quality
judgement on whether the meaning was retained,
the paraphrase was grammatical and fluent, and
whether the paraphrase was in fact different from
2http://search.cpan.org/dist/WordNet-
QueryData/QueryData.pm
system mean stdev.
PBMT 4.60 0.44
Word Substitution 3.59 0.64
Table 2: Results of human judgements (N = 10)
the source sentence. Ten judges rated two para-
phrases per headline, resulting in a total of 3,200
scores. All judges were blind to the purpose of the
evaluation and had no background in paraphrasing
research.
4.2 Results
The average scores assigned by the human judges
to the output of the two systems are displayed in
Table 2. These results show that the judges rated
the quality of the PBMT paraphrases significantly
higher than those generated by the word substitu-
tion system (t(18) = 4.11, p < .001).
Results from the automatic measures as well
as the Levenshtein distance are listed in Table 3.
We use a Levenshtein distance over tokens. First,
we observe that both systems perform roughly the
same amount of edit operations on a sentence, re-
sulting in a Levenshtein distance over words of
2.76 for the PBMT system and 2.67 for the Word
Substitution system. BLEU, METEOR and three
typical ROUGE metrics3 all rate the PBMT sys-
tem higher than the Word Substitution system.
Notice also that the all metrics assign the high-
est scores to the original sentences, as is to be ex-
pected: because every operation we perform is in
the same language, the source sentence is also a
paraphrase of the reference sentences that we use
for scoring our generated headline. If we pick a
random sentence from the reference set and score
it against the rest of the set, we obtain similar
scores. This means that this score can be regarded
as an upper bound score for paraphrasing: we can
not expect our paraphrases to be better than those
produced by humans. However, this also shows
that these measures cannot be used directly as an
automatic evaluation method of paraphrasing, as
they assign the highest score to the ?paraphrase? in
which nothing has changed. The scores observed
in Table 3 do indicate that the paraphrases gener-
3ROUGE-1, ROUGE-2 and ROUGE-SU4 are also
adopted for the DUC 2007 evaluation campaign,
http://www-nlpir.nist.gov/projects/duc/
duc2007/tasks.html
System BLEU ROUGE-1 ROUGE-2 ROUGE-SU4 METEOR Lev.dist. Lev. stdev.
PBMT 50.88 0.76 0.36 0.42 0.71 2.76 1.35
Wordsub. 24.80 0.59 0.22 0.26 0.54 2.67 1.50
Source 60.58 0.80 0.45 0.47 0.77 0 0
Table 3: Automatic evaluation and sentence Levenshtein scores
0 1 2 3 4 5 6Levenshtein distance
0
0.2
0.4
0.6
0.8
corr
elat
ion
BLEU
ROUGE-1
ROUGE-2
ROUGE-SU4
METEOR
Figure 2: Correlations between human judge-
ments and automatic evaluation metrics for vari-
ous edit distances
ated by PBMT are less well formed than the orig-
inal source sentence.
There is an overall medium correlation between
the BLEU measure and human judgements (r =
0.41, p < 0.001). We see a lower correlation
between the various ROUGE scores and human
judgements, with ROUGE-1 showing the highest
correlation (r = 0.29, p < 0.001). Between the
two lies the METEOR correlation (r = 0.35, p <
0.001). However, if we split the data according to
Levenshtein distance, we observe that we gener-
ally get a higher correlation for all the tested met-
rics when the Levenshtein distance is higher, as
visualized in Figure 2. At Levenshtein distance 5,
the BLEU score achieves a correlation of 0.78 with
human judgements, while ROUGE-1 manages to
achieve a 0.74 correlation. Beyond edit distance
5, data sparsity occurs.
5 Discussion
In this paper we have shown that with an automat-
ically obtained parallel monolingual corpus with
several millions of paired examples, it is possi-
ble to develop an SPG system based on a PBMT
framework. Human judges preferred the output
of our PBMT system over the output of a word
substitution system. We have also addressed the
problem of automatic paraphrase evaluation. We
measured BLEU, METEOR and ROUGE scores,
and observed that these automatic scores corre-
late with human judgements to some degree, but
that the correlation is highly dependent on edit
distance. At low edit distances automatic metrics
fail to properly assess the quality of paraphrases,
whereas at edit distance 5 the correlation of BLEU
with human judgements is 0.78, indicating that at
higher edit distances these automatic measures can
be utilized to rate the quality of the generated para-
phrases. From edit distance 2, BLEU correlates
best with human judgements, indicating that MT
evaluation metrics might be best for SPG evalua-
tion.
The data we used for paraphrasing consists of
headlines. Paraphrase patterns we learn are those
used in headlines and therefore different from
standard language. The advantage of our approach
is that it paraphrases those parts of sentences that
it can paraphrase, and leaves the unknown parts
intact. It is straightforward to train a language
model on in-domain text and use the translation
model acquired from the headlines to generate
paraphrases for other domains. We are also inter-
ested in capturing paraphrase patterns from other
domains, but acquiring parallel corpora for these
domains is not trivial.
Instead of post-hoc dissimilarity reranking of
the candidate paraphrase sentences we intend to
develop a proper paraphrasing model that takes
dissimilarity into account in the decoding pro-
cess. In addition, we plan to investigate if our
paraphrase generation approach is applicable to
sentence compression and simplification. On the
topic of automatic evaluation, we aim to define
an automatic paraphrase generation assessment
score. A paraphrase evaluation measure should be
able to recognize that a good paraphrase is a well-
formed sentence in the source language, yet it is
clearly dissimilar to the source.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, June.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In NAACL ?03: Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
16?23.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, pages 17?24.
Chris Callison-Burch, Trevor Cohn, and Mirella Lap-
ata. 2008. Parametric: an automatic evaluation met-
ric for paraphrasing. In COLING ?08: Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 97?104.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. Mbt: A memory-based part of
speech tagger-generator. In Proc. of Fourth Work-
shop on Very Large Corpora, pages 14?27.
Walter Daelemans, Anja Hothker, and Erik Tjong
Kim Sang. 2004. Automatic sentence simplification
for subtitling in dutch and english. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation, pages 1045?1048.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: exploiting massively parallel news sources. In
COLING ?04: Proceedings of the 20th international
conference on Computational Linguistics, page 350.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database, May.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of the Human Language Technology Conference of
the NAACL, Main Conference, pages 455?462, June.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91?107.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C.
Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens,
Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL.
Dekang Lin and Patrick Pantel. 2001. Dirt: Discov-
ery of inference rules from text. In KDD ?01: Pro-
ceedings of the seventh ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 323?328.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out, page 10.
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for mono-
lingual corpora. In Proceedings of the 11th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-06), 3?7 April.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL ?02: Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318.
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 142?
149, July.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu O. Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In ACL.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
2006. A paraphrase-based approach to machine
translation evaluation. Technical report, University
of Maryland, College Park.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In In Proc. Int. Conf. on Spoken
Language Processing, pages 901?904.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
ENLG ?09: Proceedings of the 12th European Work-
shop on Natural Language Generation, pages 122?
125.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating machine translation results with para-
phrase support. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 77?84, July.
Workshop on Monolingual Text-To-Text Generation, pages 27?33,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 27?33,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Comparing Phrase-based and Syntax-based Paraphrase Generation
Sander Wubben
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
s.wubben@uvt.nl
Erwin Marsi
NTNU
Sem Saelandsvei 7-9
NO-7491 Trondheim
Norway
emarsi@idi.ntnu.no
Antal van den Bosch
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
antal.vdnbosch@uvt.nl
Emiel Krahmer
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
e.j.krahmer@uvt.nl
Abstract
Paraphrase generation can be regarded as ma-
chine translation where source and target lan-
guage are the same. We use the Moses statisti-
cal machine translation toolkit for paraphras-
ing, comparing phrase-based to syntax-based
approaches. Data is derived from a recently
released, large scale (2.1M tokens) paraphrase
corpus for Dutch. Preliminary results indicate
that the phrase-based approach performs bet-
ter in terms of NIST scores and produces para-
phrases at a greater distance from the source.
1 Introduction
One of the challenging properties of natural lan-
guage is that the same semantic content can typically
be expressed by many different surface forms. As
the ability to deal with paraphrases holds great po-
tential for improving the coverage of NLP systems,
a substantial body of research addressing recogni-
tion, extraction and generation of paraphrases has
emerged (Androutsopoulos and Malakasiotis, 2010;
Madnani and Dorr, 2010). Paraphrase Generation
can be regarded as a translation task in which source
and target language are the same. Both Paraphrase
Generation and Machine Translation (MT) are in-
stances of Text-To-Text Generation, which involves
transforming one text into another, obeying certain
restrictions. Here these restrictions are that the gen-
erated text must be grammatically well-formed and
semantically/translationally equivalent to the source
text. Addionally Paraphrase Generation requires
that the output should differ from the input to a cer-
tain degree.
The similarity between Paraphrase Generation
and MT suggests that methods and tools originally
developed for MT could be exploited for Paraphrase
Generation. One popular approach ? arguably the
most successful so far ? is Statistical Phrase-based
Machine Translation (PBMT), which learns phrase
translation rules from aligned bilingual text corpora
(Och et al, 1999; Vogel et al, 2000; Zens et al,
2002; Koehn et al, 2003). Prior work has explored
the use of PBMT for paraphrase generation (Quirk et
al., 2004; Bannard and Callison-Burch, 2005; Mad-
nani et al, 2007; Callison-Burch, 2008; Zhao et al,
2009; Wubben et al, 2010)
However, since many researchers believe that
PBMT has reached a performance ceiling, ongo-
ing research looks into more structural approaches
to statistical MT (Marcu and Wong, 2002; Och and
Ney, 2004; Khalilov and Fonollosa, 2009). Syntax-
based MT attempts to extract translation rules in
terms of syntactic constituents or subtrees rather
than arbitrary phrases, presupposing syntactic struc-
tures for source, target or both languages. Syntactic
information might lead to better results in the area
of grammatical well-formedness, and unlike phrase-
based MT that uses contiguous n-grams, syntax en-
ables the modeling of long-distance translation pat-
terns.
While the verdict on whether or not this approach
leads to any significant performance gain is still
out, a similar line of reasoning would suggest that
syntax-based paraphrasing may offer similar advan-
tages over phrase-based paraphrasing. Considering
the fact that the success of PBMT can partly be at-
tributed to the abundance of large parallel corpora,
27
and that sufficiently large parallel corpora are still
lacking for paraphrase generation, using more lin-
guistically motivated methods might prove benefi-
cial for paraphrase generation. At the same time,
automatic syntactic analysis introduces errors in the
parse trees, as no syntactic parser is perfect. Like-
wise, automatic alignment of syntactic phrases may
be prone to errors.
The main contribution of this paper is a systematic
comparison between phrase-based and syntax-based
paraphrase generation using an off-the-shelf statis-
tical machine translation (SMT) decoder, namely
Moses (Koehn et al, 2007) and the word-alignment
tool GIZA++ (Och and Ney, 2003). Training data
derives from a new, large scale (2.1M tokens) para-
phrase corpus for Dutch, which has been recently
released.
The paper is organized as follows. Section 2 re-
views the paraphrase corpus from which provides
training and test data. Next, Section 3 describes the
paraphrase generation methods and the experimen-
tal setup. Results are presented in Section 4. In
Section 5 we discuss our findings and formulate our
conclusions.
2 Corpus
The main bottleneck in building SMT systems is
the need for a substantial amount of parallel aligned
text. Likewise, exploiting SMT for paraphrasing re-
quires large amounts of monolingual parallel text.
However, paraphrase corpora are scarce; the situa-
tion is more dire than in MT, and this has caused
some studies to focus on the automatic harvesting
of paraphrase corpora. The use of monolingual par-
allel text corpora was first suggested by Barzilay
and McKeown (2001), who built their corpus us-
ing various alternative human-produced translations
of literary texts and then applied machine learn-
ing or multi-sequence alignment for extracting para-
phrases. In a similar vein, Pang et al (2003) used a
corpus of alternative English translations of Chinese
news stories in combination with a syntax-based al-
gorithm that automatically builds word lattices, in
which paraphrases can be identified.
So-called comparable monolingual corpora, for
instance independently written news reports describ-
ing the same event, in which some pairs of sentences
exhibit partial semantic overlap have also been in-
vestigated (Shinyama et al, 2002; Barzilay and Lee,
2003; Shen et al, 2006; Wubben et al, 2009)
The first manually collected paraphrase corpus is
the Microsoft Research Paraphrase (MSRP) Corpus
(Dolan et al, 2004), consisting of 5,801 sentence
pairs, sampled from a larger corpus of news arti-
cles. However, it is rather small and contains no sub-
sentential allignments. Cohn et al (2008) developed
a parallel monolingual corpus of 900 sentence pairs
annotated at the word and phrase level. However, all
of these corpora are small from an SMT perspective.
Recently a new large-scale paraphrase corpus for
Dutch, the DAESO corpus, was released. The cor-
pus contains both samples of parallel and compa-
rable text in which similar sentences, phrases and
words are aligned. One part of the corpus is manu-
ally aligned, whereas another part is automatically
aligned using a data-driven aligner trained on the
first part. The DAESO corpus is extensively de-
scribed in (Marsi and Krahmer, 2011); the summary
here is limited to aspects relevant to the work at
hand.
The corpus contains the following types of text:
(1) alternative translations in Dutch of three liter-
ary works of fiction; (2) autocue text from televion
broadcast news as read by the news reader, and the
corresponding subtitles; (3) headlines from similar
news articles obtained from Google News Dutch;
(4) press releases about the same news topic from
two different press agencies; (5) similar answers re-
trieved from a document collection in the medical
domain, originally created for evaluating question-
answering systems.
In a first step, similar sentences were automati-
cally aligned, after which alignments were manu-
ally corrected. In the case of the parallel book texts,
aligned sentences are (approximate) paraphrases. To
a lesser degree, this is also true for the news head-
lines. The autocue-subtitle pairs are mostly exam-
ples of sentence compression, as the subtitle tends
to be a compressed version of the read autocue text.
In contrast, the press releases and the QA answers,
are characterized by a great deal of one-to-many
sentence alignments, as well as sentences left un-
aligned, as is to be expected in comparable text.
Most sentences in these types of text tend to have
only partial overlap in meaning.
28
Table 1: Properties of the manually aligned corpus
Autosub Books Headlines News QA Overall
aligned trees 18 338 6 362 32 627 11 052 118 68 497
tokens 217 959 115 893 179 629 162 361 2 230 678 072
tokens/sent 11.89 18.22 5.51 14.69 18.90 9.90
nodes 365 157 191 636 318 399 271 192 3734 1 150 118
nodes/tree 19.91 30.12 9.76 24.54 31.64 16.79
uniquely aligned trees (%) 92.93 92.49 84.57 63.61 50.00 84.10
aligned nodes (%) 73.53 66.83 73.58 53.62 38.62 67.62
Next, aligned sentences were tokenized and
parsed with the Alpino parser for Dutch (Bouma et
al., 2001). The parser provides a relatively theory-
neutral syntactic analysis which is a blend of phrase
structure analysis and dependency analysis, with a
backbone of phrasal constituents and arcs labeled
with syntactic function/dependency labels.
The alignments not only concern paraphrases in
the strict sense, i.e., expressions that are semanti-
cally equivalent, but extend to expressions that are
semantically similar in less strict ways, for instance,
where one phrase is either more specific or more
general than the related phrase. For this reason,
alignments are also labeled according to a limited
set of semantic similarity relations. Since these rela-
tions were not used in the current study, we will not
discuss them further here.
The corpus comprises over 2.1 million tokens,
678 thousand of which are manually annotated and
1,511 thousand are automatically processed.
To give a more complete overview of the sizes
of different corpus segments, some properties of the
manually aligned corpus are listed in Table 1. Prop-
erties of the automatically aligned part are similar,
except for the fact that it only contains text of the
news and QA type.
3 Paraphrase generation
Phrase-based MT models consider translation as a
mapping of small text chunks, with possible re-
ordering (Och and Ney, 2004). Operations such as
insertion, deletion and many-to-one, one-to-many
or many-to-many translation are all covered in the
structure of the phrase table. Phrase-based models
have been used most prominently in the past decade,
as they have shown to outperform other approaches
(Callison-Burch et al, 2009).
One issue with the phrase-based approach is that
recursion is not handled explicitly. It is gener-
ally acknowledged that language contains recursive
structures up to certain depths. So-called hierarchi-
cal models have introduced the inclusion of non-
terminals in the mapping rules, to allow for recur-
sion (Chiang et al, 2005). However, using a generic
non-terminal X can introduce many substitutions
in translations that do not make sense. By mak-
ing the non-terminals explicit, using syntactic cat-
egories such as NP s and V P s, this phenomenon
is constrained, resulting in syntax-based translation.
Instead of phrase translations, translation rules in
terms of syntactic constituents or subtrees are ex-
tracted, presupposing the availability of syntactic
structures for source, target, or both languages.
Incorporating syntax can guide the translation
process and unlike phrase-based MT syntax it en-
ables the modeling of long-distance translation pat-
terns. Syntax-based systems may parse the data on
the target side (string-to-tree), source side (tree-to-
string), or both (tree-to-tree).
In our experiments we use tree-to-tree syntax-
based MT. We also experiment with relaxing the
parses by a method proposed under the label of
syntax-augmented machine translation (SAMT), de-
scribed in (Zollmann and Venugopal, 2006). This
method combines any neighboring nodes and labels
previously unlabeled nodes, removing the syntactic
constraint on the grammar1.
We train all systems on the DAESO data (218,102
lines of aligned sentences) and test on a held-out
set consisting of manually aligned headlines that ap-
1This method is implemented in the Moses package in the
program relax-parse as option SAMT 4
29
Table 2: Examples of output of the phrase-based and syntax-based systems
Source jongen ( 7 ) zwaargewond na aanrijding boy (7) severely-injured after crash
Phrase-based 7-jarige gewond na botsing 7-year-old injured after collision
Syntax-based jongen ( 7 ) zwaar gewond na aanrijding boy (7) severely injured after crash
Source jeugdwerkloosheid daalt vooral bij voldoende opleiding youth-unemployment drops especially with adequate training
Phrase-based werkloosheid jongeren daalt , vooral bij voldoende studie unemployment youths drops, especially with sufficient study
Syntax-based * jeugdwerkloosheid daalt vooral in voldoende opleiding youth-unemployment drops especially in adequate training
Source kritiek op boetebeleid ns criticism of fining-policy ns
Phrase-based * kritiek op de omstreden boetebeleid en criticism of the controversial and
Syntax-based kritiek op omstreden boetebeleid nederlandse spoorwegen criticism of controversial fining-policy dutch railways
Source weer bestuurders radboud weg again directors radboud [hospital] leaving
Phrase-based * weer de weg ziekenhuis again the leaving hospital
Syntax-based alweer bestuurders ziekenhuis weg yet-again directors hospital leaving
peared in May 2006.2 We test on 773 headlines that
have three or more aligned paraphrasing reference
headlines. We use an SRILM (Stolcke, 2002) lan-
guage model trained on the Twente news corpus3.
To investigate the effect of the amount of training
data on results, we also train a phrase-based model
on more data by adding more aligned headlines orig-
inating from data crawled in 2010 and aligned using
tf.idf scores over headline clusters and Cosine sim-
ilarity as described in (Wubben et al, 2009), result-
ing in an extra 612,158 aligned headlines.
Evaluation is based on the assumption that a good
paraphrase is well-formed and semantically similar
but structurally different from the source sentence.
We therefore score the generated paraphrases not
only by an MT metric (we use NIST scores), but
also factor in the edit distance between the input
sentence and the output sentence. We take the 10-
best generated paraphrases and select from these the
one most dissimilar from the source sentence in term
of Levenshtein distance on tokens. We then weigh
NIST scores according to their corresponding sen-
tence Levenshtein Distance, to calculate a weighted
2Syntactic trees were converted to the XML format used by
Moses for syntax-based MT. A minor complication is that the
word order in the tree is different from the word order in the
corresponding sentence in about half of the cases. The technical
reason is that Alpino internally produces dependency structures
that can be non-projective. Conversion to a phrase structure tree
therefore necessitates moving some words to a different posi-
tion in the tree. We performed a subsequent reordering of the
trees, moving terminals to make the word order match the sur-
face word order.
3http://www.vf.utwente.nl/?druid/TwNC/
TwNC-main.html
average score. This implies that we penalize sys-
tems that provide output at Levenshtein distance 0,
which are essentially copies of the input, and not
paraphrases. Formally, the score is computed as fol-
lows:
NISTweightedLD = ?
?
i=LD(1..8)
(i ?Ni ?NISTi)
?
i=LD(1..8)
(i ?Ni)
where ? is the percentage of output phrases that have
a sentence Levenshtein Distance higher than 0. In-
stead of NIST scores, other MT evaluation scores
can be plugged into this formula, such as METEOR
(Lavie and Agarwal, 2007) for languages for which
paraphrase data is available.
4 Results
Figure 1 shows NIST scores per Levenshtein Dis-
tance. It can be observed that overall the NIST score
decreases as the distance to the input increases, indi-
cating that more distant paraphrases are of less qual-
ity. The relaxed syntax-based approach (SAMT)
performs mildly better than the standard syntax-
based approach, but performs worse than the phrase-
based approach. The distribution of generated para-
phrases per Levenshtein Distance is shown in Fig-
ure 2. It reveals that the Syntax-based approaches
tend to stay closer to the source than the phrase-
based approaches.
In Table 2 a few examples of output from both
Phrase- and Syntax-based systems are given. The
30
2 4 6 8 10
2
4
6
8
10
LevenshteinDistance
N
IS
T
sc
or
e
Phrase
Phrase extra data
Syntax
Syntax relaxed
Figure 1: NIST scores per Levenshtein distance
top two examples show sentences where the phrase-
based approach scores better, and the bottom two
show examples where the syntax-based approach
scores better. In general, we observe that the
phrase-based approach is often more drastic with its
changes, as shown also in Figure 2. The syntax-
based approach is less risky, and reverts more to
single-word substitution.
The weighted NIST score for the phrase-based
approach is 7.14 versus 6.75 for the syntax-based
approach. Adding extra data does not improve the
phrase-based approach, as it yields a score of 6.47,
but the relaxed method does improve the syntax-
based approach (7.04).
5 Discussion and conclusion
We have compared a phrase-based MT approach
to paraphrasing with a syntax-based MT approach.
The Phrase-based approach performs better in terms
of NIST score weighted by edit distance of the out-
put. In general, the phrase-based MT system per-
forms more edits and these edits seem to be more
reliable than the edits done by the Syntax-based ap-
proach. A relaxed Syntax-based approach performs
better, while adding more data to the Phrase-based
approach does not yield better results. To gain a bet-
ter understanding of the quality of the output gener-
ated by the different approaches, it would be desir-
able to present the output of the different systems to
human judges. In future work, we intend to com-
pare the effects of using manual word alignments
from the DAESO corpus instead of the automatic
alignments produced by GIZA++. We also wish to
0 2 4 6 8 10
0
100
200
300
LevenshteinDistance
N
Phrase
Phrase extra data
Syntax
Syntax relaxed
Figure 2: Distribution of generated paraphrases per Lev-
enshtein distance
further explore the effect of the nature of the data
that we train on: the DAESO corpus consists of var-
ious data sources from different domains. Our aim
is also to incorporate the notion of dissimilarity into
the paraphrase model, by adding dissimilarity scores
to the model.
31
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entailment
methods. Journal of Artificial Intelligence Research,
38:135?187, May.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 597?604,
Morristown, NJ, USA. Association for Computational
Linguistics.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In NAACL ?03: Proceedings of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 16?23, Morristown,
NJ, USA. Association for Computational Linguistics.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of Meeting of the Association for Computational Lin-
guistics, pages 50?57, Toulouse, France.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage computational analy-
sis of Dutch. In Walter Daelemans, Khalil Sima?an,
Jorn Veenstra, and Jakub Zavre, editors, Computa-
tional Linguistics in the Netherlands 2000., pages 45?
59. Rodopi, Amsterdam, New York.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints
on paraphrases extracted from parallel corpora. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 196?205, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Chiang, Adam Lopez, Nitin Madnani, Christof
Monz, Philip Resnik, and Michael Subotin. 2005. The
hiero machine translation system: extensions, evalua-
tion, and analysis. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages 779?
786, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4):597?614.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 350?356, Morris-
town, NJ, USA.
Maxim Khalilov and Jose? A. R. Fonollosa. 2009. N-
gram-based statistical machine translation versus syn-
tax augmented machine translation: comparison and
system combination. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ?09, pages 424?
432, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 48?54.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C.
Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens,
Chris Dyer, Ondrej Bojar, Alexandra Constantin, and
Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL. The Associa-
tion for Computer Linguistics.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In Proceedings
of the Second Workshop on Statistical Machine Trans-
lation, StatMT ?07, pages 228?231, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nitin Madnani and Bonnie J. Dorr. 2010. Gener-
ating phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?387.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
Proceedings of the Second Workshop on Statistical
Machine Translation, StatMT ?07, pages 120?127,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Proceedings of the ACL-02 conference on
Empirical methods in natural language processing -
Volume 10, EMNLP ?02, pages 133?139, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Erwin Marsi and Emiel Krahmer. 2011. Construction of
an aligned monolingual treebank for studying seman-
tic similarity. (submitted for publication).
32
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Comput. Linguist., 30:417?449, December.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for Statistical Ma-
chine Translation. In Proceedings of the Joint Work-
shop on Empirical Methods in NLP and Very Large
Corpora, pages 20?28, Maryland, USA.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
HLT-NAACL.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 142?149, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Siwei Shen, Dragomir R. Radev, Agam Patel, and Gu?nes?
Erkan. 2006. Adding syntax to dynamic program-
ming for aligning comparable texts for the generation
of paraphrases. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 747?
754, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of Human
Language Technology Conference (HLT 2002), pages
313?318, San Diego, USA.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In In Proc. Int. Conf. on
Spoken Language Processing, pages 901?904, Denver,
Colorado.
S. Vogel, Franz Josef Och, and Hermann Ney. 2000. The
statistical translation module in the verbmobil system.
In KONVENS 2000 / Sprachkommunikation, Vortrge
der gemeinsamen Veranstaltung 5. Konferenz zur Ve-
rarbeitung natrlicher Sprache (KONVENS), 6. ITG-
Fachtagung ?Sprachkommunikation?, pages 291?293,
Berlin, Germany, Germany. VDE-Verlag GmbH.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
E. Krahmer and M. Theune, editors, The 12th Eu-
ropean Workshop on Natural Language Generation,
pages 122?125, Athens. Association for Computa-
tional Linguistics.
Sander Wubben, Antal van den Bosch, and Emiel Krah-
mer. 2010. Paraphrase generation as monolingual
translation: Data and evaluation. In B. Mac Namee
J. Kelleher and I. van der Sluis, editors, Proceedings of
the 10th International Workshop on Natural Language
Generation (INLG 2010), pages 203?207, Dublin.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In Pro-
ceedings of the 25th Annual German Conference on
AI: Advances in Artificial Intelligence, KI ?02, pages
18?32, London, UK. Springer-Verlag.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2 - Volume 2, ACL ?09,
pages 834?842, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, StatMT ?06, pages 138?141, Stroudsburg,
PA, USA. Association for Computational Linguistics.
33
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 11?19,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Using character overlap to improve language transformation
Sander Wubben
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
s.wubben@uvt.nl
Emiel Krahmer
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
e.j.krahmer@uvt.nl
Antal van den Bosch
Radboud University Nijmegen
P.O. Box 9103
6500 HD Nijmegen
The Netherlands
a.vandenbosch@let.ru.nl
Abstract
Language transformation can be defined
as translating between diachronically dis-
tinct language variants. We investigate the
transformation of Middle Dutch into Mod-
ern Dutch by means of machine transla-
tion. We demonstrate that by using char-
acter overlap the performance of the ma-
chine translation process can be improved
for this task.
1 Introduction
In this paper we aim to develop a system to para-
phrase between diachronically distinct language
variants. For research into history, historical lin-
guistics and diachronic language change, histori-
cal texts are of great value. Specifically from ear-
lier periods, texts are often the only forms of in-
formation that have been preserved. One prob-
lem that arises when studying these texts is the
difference between the language the text is writ-
ten in and the modern variant that the researchers
who want to study the texts know and speak them-
selves. It takes a great deal of deciphering and in-
terpretation to be able to grasp these texts. Our aim
is to facilitate scholars such as historians who do
not possess extensive knowledge of Middle Dutch
who are studying medieval texts. We do this by at-
tempting to generate literal translations of the sen-
tences in the text into modern language. In par-
ticular we focus on the task of translating Middle
Dutch to modern Dutch. The transformation be-
tween language variants, either synchronically or
diachronically, can be seen as a paraphrase and a
translation task, as it is often impossible to catego-
rize two languages as either variants or different
languages.
We define Middle Dutch as a collection of
closely related West Germanic dialects that were
spoken and written between 1150 and 1500 in the
area that is now defined as the Netherlands and
parts of Belgium. One of the factors that make
Middle Dutch difficult to read is the fact that at
the time no overarching standard language existed.
Modern Dutch is defined as Dutch as spoken from
1500. The variant we investigate is contempo-
rary Dutch. An important difference with regular
paraphrasing is the amount of parallel data avail-
able. The amount of parallel data for the vari-
ant pair Middle Dutch - Modern Dutch is sev-
eral orders of magnitude smaller than bilingual
parallel corpora typically used in machine trans-
lation (Koehn, 2005) or monolingual parallel cor-
pora used for paraphrase generation by machine
translation (Wubben et al, 2010).
We do expect many etymologically related
words to show a certain amount of character
overlap between the Middle and Modern variants.
An example of the data is given below, from the
work ?Van den vos Reynaerde? (?About Reynard
the Fox?), part of the Comburg manuscript that
was written between 1380-1425. Here, the first
text is the original text, the second text is a
modern translation in Dutch by Walter Verniers
and a translation in English is added below that
for clarity.
?Doe al dat hof versamet was,
Was daer niemen, sonder die das,
Hine hadde te claghene over Reynaerde,
Den fellen metten grijsen baerde.?
?Toen iedereen verzameld was,
was er niemand -behalve de das-
die niet kwam klagen over Reynaert,
die deugniet met zijn grijze baard.?
?When everyone was gathered,
there was noone -except the badger-
who did not complain about Reynaert,
that rascal with his grey beard.?
11
We can observe that although the two Dutch
texts are quite different, there is a large amount
of character overlap in the words that are used.
Our aim is to use this character overlap to compen-
sate for the lower amount of data that is available.
We compare three different approaches to trans-
late Middle Dutch into Modern Dutch: a standard
Phrase-Based machine translation (PBMT) ap-
proach, a PBMT approach with additional prepro-
cessing based on Needleman-Wunsch sequence
alignment, and a character bigram based PBMT
approach. The PBMT approach with preprocess-
ing identifies likely translations based on character
overlap and adds them as a dictionary to improve
the statistical alignment process. The PBMT ap-
proach based on character bigrams rather than
translating words, transliterates character bigrams
and in this way improves the transformation pro-
cess. We demonstrate that these two approaches
outperform standard PBMT in this task, and that
the PBMT transliteration approach based on char-
acter bigrams performs best.
2 Related work
Language transformation by machine translation
within a language is a task that has not been stud-
ied extensively before. Related work is the study
by Xu et al (2012). They evaluate paraphrase sys-
tems that attempt to paraphrase a specific style of
writing into another style. The plays of William
Shakespeare and the modern translations of these
works are used in this study. They show that their
models outperform baselines based on dictionar-
ies and out-of-domain parallel text. Their work
differs from our work in that they target writing
in a specific literary style and we are interested in
translating between diachronic variants of a lan-
guage.
Work that is slightly comparable is the work by
Zeldes (2007), who extrapolates correspondences
in a small parallel corpus taken from the Modern
and Middle Polish Bible. The correspondences are
extracted using machine translation with the aim
of deriving historical grammar and lexical items.
A larger amount of work has been published about
spelling normalization of historical texts. Baron
and Rayson (2008) developed tools for research in
Early Modern English. Their tool, VARD 2, finds
candidate modern form replacements for spelling
variants in historical texts. It makes use of a
dictionary and a list of spelling rules. By plug-
ging in other dictionaries and spelling rules, the
tool can be adapted for other tasks. Kestemont et
al. (2010) describe a machine learning approach
to normalize the spelling in Middle Dutch Text
from the 12th century. They do this by converting
the historical spelling variants to single canonical
(present-day) lemmas. Memory-based learning is
used to learn intra-lemma spelling variation. Al-
though these approaches normalize the text, they
do not provide a translation.
More work has been done in the area of trans-
lating between closely related languages and deal-
ing with data sparsity that occurs within these lan-
guage pairs (Hajic? et al, 2000; Van Huyssteen
and Pilon, 2009). Koehn et al (2003) have shown
that there is a direct negative correlation between
the size of the vocabulary of a language and the
accuracy of the translation. Alignment models
are directly affected by data sparsity. Uncommon
words are more likely to be aligned incorrectly to
other words or, even worse, to large segments of
words (Och and Ney, 2003). Out of vocabulary
(OOV) words also pose a problem in the trans-
lation process, as systems are unable to provide
translations for these words. A standard heuristic
is to project them into the translated sentence un-
translated.
Various solutions to data sparsity have been
studied, among them the use of part-of-speech
tags, suffixes and word stems to normalize
words (Popovic and Ney, 2004; De Gispert and
Marino, 2006), the treatment of compound words
in translation (Koehn and Knight, 2003), translit-
eration of names and named entities, and advanced
models that combine transliteration and transla-
tion (Kondrak et al, 2003; Finch et al, 2012)
or learn unknown words by analogical reason-
ing (Langlais and Patry, 2007).
Vilar et al (2007) investigate a way to han-
dle data sparsity in machine translation between
closely related languages by translating between
characters as opposed to words. The words in the
parallel sentences are segmented into characters.
Spaces between words are marked with a special
character. The sequences of characters are then
used to train a standard machine translation model
and a language model with n-grams up to n = 16.
They apply their system to the translation between
the related languages Spanish and Catalan, and
find that a word based system outperforms their
12
letter-based system. However, a combined sys-
tem performs marginally better in terms of BLEU
scores.
Tiedemann (2009) shows that combining
character-based translation with phrase-based
translation improves machine translation quality
in terms of BLEU and NIST scores when trans-
lating between Swedish and Norwegian if the
OOV-words are translated beforehand with the
character-based model.
Nakov and Tiedemann (2012) investigate the
use of character-level models in the translation be-
tween Macedonian and Bulgarian movie subtitles.
Their aim is to translate between the resource poor
language Macedonian to the related language Bul-
garian, in order to use Bulgarian as a pivot in or-
der to translate to other languages such as English.
Their research shows that using character bigrams
shows improvement over a word-based baseline.
It seems clear that character overlap can be used
to improve translation quality in related languages.
We therefore use character overlap in language
transformation between two diachronic variants of
a language.
3 This study
In this study we investigate the task of translating
from Middle Dutch to Modern Dutch. Similarly
to resource poor languages, one of the problems
that are apparent is the small amount of parallel
Middle Dutch - Modern Dutch data that is avail-
able. To combat the data sparseness we aim to use
the character overlap that exists between the Mid-
dle Dutch words and their Modern Dutch counter-
parts. Examples of overlap in some of the words
given in the example can be viewed in Table1. We
are interested in the question how we can use this
overlap to improve the performance of the transla-
tion model. We consider three approaches: (A)
Perform normal PBMT without any preprocess-
ing, (B) Apply a preprocessing step in which we
pinpoint words and phrases that can be aligned
based on character overlap and (C) perform ma-
chine translation not to words but to character bi-
grams in order to make use of the character over-
lap.
We will first discuss the PBMT baseline, fol-
lowed by the PBMT + overlap system and the
character bigram PBMT transliteration system in
Section 4. We then describe the experiment with
human judges in Section 6, and its results in Sec-
tion 7. We close with a discussion of our results in
Section 8.
Middle Dutch Modern Dutch
versamet verzameld
was was
niemen niemand
die de
das das
claghene klagen
over over
Reynaerde Reynaert
metten met zijn
grijsen grijze
baerde baard
Table 1: Examples of character overlap in words
from a fragment of ?Van den vos Reynaerde?
4 Language transformation Models
4.1 PBMT baseline
For our baseline we use the Moses software to
train a phrase based machine translation (PBMT)
model (Koehn et al, 2007). In general, a statistical
machine translation model normally finds a best
translation E? of a text in language F for a text
in language E by combining a translation model
P (F |E) with a language model P (E):
E? = arg max
E?E?
P (F |E)P (E)
In phrase-based machine translation the sen-
tence F is segmented into a sequence of I phrases
during decoding. Each source phrase is then trans-
lated into a target phrase to form sentence E.
Phrases may be reordered.
The GIZA++ statistical alignment package
(Och and Ney, 2003) is used to perform the word
alignments, which are later combined into phrase
alignments in the Moses pipeline to build the
language transformation model. GIZA++ imple-
ments IBM Models 1 to 5 and an HMM word
alignment model to find statistically motivated
alignments between words. We first tokenize our
data. We then lowercase all data and use all sen-
tences from the Modern Dutch part of the cor-
pus to train an n-gram language model with the
SRILM toolkit (Stolcke, 2002). Then we run the
GIZA++ aligner using the training pairs of sen-
tences in Middle Dutch and Modern Dutch. We
13
execute GIZA++ with standard settings and we
optimize using minimum error rate training with
BLEU scores. The Moses decoder is used to gen-
erate the translations.
4.2 PBMT with overlap-based alignment
Before using the Moses pipeline we perform a
preprocessing alignment step based on character
overlap. Word and phrase pairs that exhibit a large
amount of character overlap are added to the par-
allel corpus that GIZA++ is trained on. Every time
we find a phrase or word pair with large overlap it
is added to the corpus. This helps bias the align-
ment procedure towards aligning similar words
and reduces alignment errors. To perform the pre-
processing step we use the Needleman-Wunsch
algorithm (Needleman and Wunsch, 1970). The
Needleman-Wunsch algorithm is a dynamic pro-
gramming algorithm that performs a global align-
ment on two sequences. Sequence alignment is
a method to find commonalities in two (or more)
sequences of some items or characters. One of-
ten used example is the comparison of sequences
of DNA to find evolutionary differences and sim-
ilarities. Sequence alignment is also used in lin-
guistics, where it is applied to finding the longest
common substring or the differences or similari-
ties between strings.
The Needleman-Wunsch algorithm is a se-
quence alignment algorithm that optimizes a score
function to find an optimal alignment of a pair of
sequences. Each possible alignment is scored ac-
cording to the score function, where the alignment
giving the highest similarity score is the optimal
alignment of a pair of sequences. If more than
one alignment yields the highest score, there are
multiple optimal solutions. The algorithm uses
an iterative matrix to calculate the optimal solu-
tion. All possible pairs of characters containing
one character from each sequence are plotted in
a 2-dimensional matrix. Then, all possible align-
ments between those characters can be represented
by pathways through the matrix. Insertions and
deletions are allowed, but can be penalized by
means of a gap penalty in the alignment.
The first step is to initialize the matrix and fill in
the gap scores in the top row and leftmost column.
In our case we heuristically set the values of the
scores to +1 for matches, -2 for mismatches and
-1 for gaps after evaluating on our development
set. After initialization, we can label the cells
in the matrix C(i, j) where i = 1, 2, ..., N and
j = 1, 2, ...,M , the score of any cell C(i, j) is
then the maximum of:
qdiag = C(i? 1, j ? 1) + s(i, j)
qdown = C(i? 1, j) + g
qright = C(i, j ? 1) + g
Here, s(i, j) is the substitution score for let-
ters i and j, and g is the gap penalty score. If i
and j match, the substitution score is in fact the
matching score. The table is filled this way recur-
sively, filling each cell with the maximum score of
the three possible options (diagonally, down and
right). After this is done, an optimal path can
be found by performing a traceback, starting in
the lower right corner of the table and ending in
the upper left corner, by visiting cells horizontally,
vertically or diagonally, but only those cells with
the highest score. After this process we end up
with an alignment.
We use the Needleman-Wunsch algorithm to
find an optimal alignment of the Middle Dutch
- Modern Dutch sentence pairs. We regard each
line as a sentence. In case of rhyming text, a
frequent phenomenon in Middle Dutch text, lines
are usually parts of sentences. We then consider
each line a string, and we try to align as many
characters and whitespaces to their equivalents in
the parallel line. We split the aligned sentences
in each position where two whitespaces align and
we consider the resulting aligned words or phrases
as alignments. For each aligned word or phrase
pair we calculate the Jaccard coefficient and if that
is equal or higher than a threshold we add the
aligned words or phrases to the training material.
We heuristically set this threshold to 0.5. By using
this method we already can find many-to-one and
one-to-many alignments. In this way we help the
GIZA++ alignment process by biasing it towards
aligning words and phrases that show overlap. Ta-
ble 2 illustrates this process for two lines.
4.3 Character bigram transliteration
Another somewhat novel approach we propose
for Language Transformation is Character-based
transliteration. To circumvent the problem of
14
Middle Dutch: hine hadde+ ++te claghene over Reynaerde ,
Modern Dutch: di+e ++niet kwam klag+en+ over Reynaer+t ,
Jaccard 0.4 0.14 0 0.63 1 0.70 1
Middle Dutch: +den fe++llen met++ten grijsen baerde .
Modern Dutch: die+ deugniet met zijn grijze+ baard+ .
Jaccard 0.50 0.09 0.50 0.71 0.8 1
Table 2: Alignment of lines with Jaccard scores for the aligned phrases. A + indicates a gap introduced
by the Needleman Wunsch alignment.
OOV-words and use the benefits of character over-
lap more directly in the MT system, we build
a translation model based on character bigrams,
similar to (Nakov and Tiedemann, 2012). Where
they use this approach to translate between closely
related languages, we use it to translate between
diachronic variants of a language. The sentences
in the parallel corpus are broken into charac-
ter bigrams, with a special character representing
whitespaces. These bigrams are used to train the
translation model and the language model. An ex-
ample of the segmentation process is displayed in
Table 3. We train an SRILM language model on
the character bigrams and model sequences of up
to 10 bigrams. We then run the standard Moses
pipeline, using GIZA++ with standard settings to
generate the phrase-table and we use the Moses
decoder to decode the bigram sequences. A num-
ber of sample entries are shown in Table 4. As a
final step, we recombine the bigrams into words.
The different sizes of the Phrase-table for the dif-
ferent approaches can be observed in Table 5.
original segmented
Hine #H Hi in ne e#
hadde #h ha ad dd de e#
te #t te e#
claghene #c cl la ag gh he en ne e#
over #o ov ve er r#
Reynaerde #R Re ey yn na ae er rd de e#
, #, ,#
Table 3: Input and output of the character bigram
segmenter
5 Data Set
Our training data consists of various Middle Dutch
literary works with their modern Dutch transla-
tion. A breakdown of the different works is in
#d da at t# en n# #d da aa ar
#d da at t# et t# #s st
#d da at t# et t# #s
#d da at t# ie et t# #s st
#d da at t# ie et t# #s
#d da at t# la an n#
#d da at t# le et t#
#d da at t# n# #d da aa ar ro
#d da at t# n# #d da aa ar
#d da at t# n#
#d da at t# rd da at t#
#d da at ts s# #d da at t#
#d da at ts si i# #h he eb bb be en
#d da at ts #d da at t#
#d da at ts #w wa at t#
#d da at tt tu #w wa at t# #j
Table 4: Example entries from the character bi-
gram Phrase-table, without scores.
system lines
PBMT 20,092
PBMT + overlap 27,885
character bigram transliteration 93,594
Table 5: Phrase-table sizes of the different models
Table 6. All works are from the Digital Library of
Dutch Literature1. ?Middle Dutch? is a very broad
definition. It encompasses all Dutch language spo-
ken and written between 1150 and 1500 in the
Netherlands and parts of Belgium. Works stem-
ming from different centuries, regions and writers
can differ greatly in their orthography and spelling
conventions. No variant of Dutch was considered
standard or the norm; Middle Dutch can be con-
sidered a collection of related lects (regiolects, di-
alects). This only adds to the problem of data
1http://www.dbnl.org
15
sparsity. Our test set consists of a selection of
sentences from the Middle Dutch work Beatrijs,
a Maria legend written around 1374 by an anony-
mous author.
source text lines date of origin
Van den vos Reynaerde 3428 around 1260
Sint Brandaan 2312 12th century
Gruuthuuse gedichten 224 around 1400
?t Prieel van Trojen 104 13th century
Various poems 42 12th-14th cent.
Table 6: Middle Dutch works in the training set
Middle Si seide: ?Ic vergheeft u dan.
Dutch Ghi sijt mijn troest voer alle man
Modern Ze zei: ?ik vergeef het je dan.
Dutch Je bent voor mij de enige man
PBMT Ze zei : ? Ik vergheeft u dan .
Gij ze alles in mijn enige voor al man
PBMT + Ze zei : ? Ik vergheeft u dan .
Overlap dat ze mijn troest voor al man
Char. Bigram Ze zeide : ? Ik vergeeft u dan .
PBMT Gij zijt mijn troost voor alle man
Middle Dat si daer snachts mochte bliven.
Dutch ?Ic mocht u qualijc verdriven,?
Modern omdat ze nu niet verder kon reizen.
Dutch ?Ik kan u echt de deur niet wijzen,?
PBMT dat ze daar snachts kon bliven .
? Ik kon u qualijc verdriven , ?
PBMT + dat ze daar s nachts kon blijven .
Overlap ? Ik kon u qualijc verdriven , ?
Char. Bigram dat zij daar snachts mocht blijven .
PBMT ? Ik mocht u kwalijk verdrijven ,
Table 7: Example output
6 Experiment
In order to evaluate the systems, we ran an exper-
iment to collect human rankings of the output of
the systems. We also performed automatic evalu-
ation.
6.1 Materials
Because of the nature of our data, in which sen-
tences often span multiple lines, it is hard to eval-
uate the output on the level of separate lines. We
therefore choose to evaluate pairs of lines. We ran-
domly choose a line, and check if it is part of a
sensible sentence that can be understood without
more context. If that is the case, we select it to in-
clude in our test set. In this way we select 25 pairs
of lines. We evaluate the translations produced by
the three different systems for these sentences. Ex-
amples of the selected sentences and the generated
corresponding output are displayed in Table 7.
6.2 Participants
The participants in this evaluation study were 22
volunteers. All participants were native speakers
of Dutch, and participated through an online in-
terface. All participants were adults, and 12 were
male and 10 female. In addition to the 22 partici-
pants, one expert in the field of Middle Dutch also
performed the experiment, in order to be able to
compare the judgements of the laymen and the ex-
pert.
6.3 Procedure
The participants were asked to rank three different
automatic literal translations of Middle Dutch text.
For reference, they were also shown a modern (of-
ten not literal) translation of the text by Dutch au-
thor Willem Wilmink. The order of items to judge
was randomized for each participant, as well as the
order of the output of the systems for each sen-
tence. The criterium for ranking was the extent to
which the sentences could be deciphered and un-
derstood. The participants were asked to always
provide a ranking and were not allowed to assign
the same rank to multiple sentences. In this way,
each participant provided 25 rankings where each
pair of sentences received a distinct rank. The pair
with rank 1 is considered best and the pair with 3
is considered worst.
system mean rank 95 % c. i.
PBMT 2.44 (0.03) 2.38 - 2.51
PBMT + Overlap 2.00 (0.03) 1.94 - 2.06
char. bigram PBMT 1.56 (0.03) 1.50 - 1.62
Table 8: Mean scores assigned by human subjects,
with the standard error between brackets and the
lower and upper bound of the 95 % confidence in-
terval
7 Results
7.1 Human judgements
In this section we report on results of the exper-
iment with judges ranking the output of the sys-
tems. To test for significance of the difference
in the ranking of the different systems we ran re-
peated measures analyses of variance with system
(PBMT, PBMT + Overlap, character bigram MT)
as the independent variable, and the ranking of the
output as the dependent variable. Mauchly?s test
for sphericity was used to test for homogeneity of
16
PBMT PBMT +
overlap
char.
bigram
PBMT
X2
2.05 2.59 1.36 16.636**
2.77 1.82 1.41 21.545**
2.50 1.27 2.23 18.273**
1.95 1.45 2.59 14.273**
2.18 2.36 1.45 10.182**
2.45 2.00 1.55 9.091*
2.91 1.77 1.32 29.545**
2.18 2.27 1.55 6.903*
2.14 2.00 1.86 0.818
2.27 1.73 2.00 3.273
2.68 1.68 1.64 15.364**
2.82 1.95 1.23 27.909**
2.68 2.09 1.23 23.545**
1.95 2.55 1.50 12.091**
2.77 1.86 1.36 22.455**
2.32 2.23 1.45 9.909**
2.86 1.91 1.23 29.727**
2.18 1.09 2.73 30.545**
2.05 2.09 1.86 0.636
2.73 2.18 1.09 30.545**
2.41 2.27 1.32 15.545**
2.68 2.18 1.14 27.364**
1.82 2.95 1.23 33.909**
2.73 1.95 1.32 21.909**
2.91 1.77 1.32 29.545**
Table 9: Results of the Friedman test on each of
the 25 sentences. Results marked * are significant
at p < 0.05 and results marked ** are significant
at p < 0.01
variance, but was not significant, so no corrections
had to be applied. Planned pairwise comparisons
were made with the Bonferroni method. The mean
ranking can be found in Table 8 together with the
standard deviation and 95 % confidence interval.
We find that participants ranked the three systems
differently, F (2, 42) = 135, 604, p < .001, ?2p =
.866. All pairwise comparisons are significant at
p < .001. The character bigram model receives
the best mean rank (1.56), then the PBMT + Over-
lap system (2.00) and the standard PBMT system
is ranked lowest (2.44). We used a Friedman test
to detect differences across multiple rankings. We
ran the test on each of the 25 K-related samples,
and found that for 13 sentences the ranking pro-
vided by the test subjects was equal to the mean
ranking: the PBMT system ranked lowest, then the
PBMT + Overlap system and the character bigram
system scored highest for each of these cases at
p < .005. These results are detailed in Table 9.
When comparing the judgements of the partici-
pants with the judgements of an expert, we find
a significant medium Pearson correlation of .65
(p < .001) between the judgements of the expert
and the mean of the judgements of the participants.
This indicates that the judgements of the laymen
are indeed useful.
7.2 Automatic judgements
In order to attempt to measure the quality of the
transformations made by the different systems au-
tomatically, we measured NIST scores by compar-
ing the output of the systems to the reference trans-
lation. We do realize that the reference translation
is in fact a literary interpretation and not a literal
translation, making automatic assessment harder.
Having said that, we still hope to find some effect
by using these automatic measures. We only re-
port NIST scores here, because BLEU turned out
to be very uninformative. In many cases sentences
would receive a BLEU score of 0. Mauchly?s test
for sphericity was used to test for homogeneity of
variance for the NIST scores, and was not signifi-
cant. We ran a repeated measures test with planned
pairwise comparisons made with the Bonferroni
method. We found that the NIST scores for the dif-
ferent systems differed significantly (F (2, 48) =
6.404, p < .005, ?2p = .211). The average NIST
scores with standard error and the lower and up-
per bound of the 95 % confidence interval can be
seen in Table 10. The character bigram translit-
eration model scores highest with 2.43, followed
by the PBMT + Overlap model with a score of
2.30 and finally the MT model scores lowest with
a NIST score of 1.95. We find that the scores
for the PBMT model differ significantly from the
PBMT + Overlap model (p < .01) and the charac-
ter bigram PBMT model (p < .05), but the scores
for the PBMT + Overlap and the character bigram
PBMT model do not differ significantly. When
we compare the automatic scores to the human as-
signed ranks we find no significant Pearson corre-
lation.
8 Conclusion
In this paper we have described two modifications
of the standard PBMT framework to improve the
transformation of Middle Dutch to Modern Dutch
17
system mean NIST 95 % c. i.
PBMT 1.96 (0.18) 1.58 - 2.33
PBMT + overlap 2.30 (0.21) 1.87 - 2.72
char. bigram PBMT 2.43 (0.20) 2.01 - 2.84
Table 10: Mean NIST scores, with the standard
error between brackets and the lower and upper
bound of the 95 % confidence interval
by using character overlap in the two variants. We
described one approach that helps the alignment
process by adding words that exhibit a certain
amount of character overlap to the parallel data.
We also described another approach that translates
sequences of character bigrams instead of words.
Reviewing the results we conclude that the use of
character overlap between diachronic variants of a
language is beneficial in the translation process.
More specifically, the model that uses character
bigrams in translation instead of words is ranked
best. Also ranked significantly better than a stan-
dard Phrase Based machine translation approach
is the approach using the Needleman-Wunsch al-
gorithm to align sentences and identify words or
phrases that exhibit a significant amount of char-
acter overlap to help the GIZA++ statistical align-
ment process towards aligning the correct words
and phrases. We have seen that one issue that
is encountered when considering the task of lan-
guage transformation from Middle Dutch to Mod-
ern Dutch is data sparseness. The number of lines
used to train on amounts to a few thousand, and
not millions as is more common in SMT. It is
therefore crucial to use the inherent character over-
lap in this task to compensate for the lack of data
and to make more informed decisions. The char-
acter bigram approach is able to generate a trans-
lation for out of vocabulary words, which is also
a solution to the data sparseness problem. One
area where the character bigram model often fails,
is translating Middle Dutch words into Modern
Dutch words that are significantly different. One
example can be seen in Table 7, where ?mocht?
is translated by the PBMT and PBMT + Overlap
systems to ?kon? and left the same by the charac-
ter bigram transliteration model. This is probably
due to the fact that ?mocht? still exists in Dutch,
but is not as common as ?kon? (meaning ?could?).
another issue to consider is the fact that the char-
acter bigram model learns character mappings that
are occurring trough out the language. One exam-
ple is the disappearing silent ?h? after a ?g?. This
often leads to transliterated words of which the
spelling is only partially correct. Apparently the
human judges rate these ?half words? higher than
completely wrong words, but automatic measures
such as NIST are insensitive to this.
We have also reported the NIST scores for the
output of the standard PBMT approach and the
two proposed variants. We see that the NIST
scores show a similar patterns as the human judge-
ments: the PBMT + Overlap and character bigram
PBMT systems both achieve significantly higher
NIST scores than the normal PBMT system. How-
ever, the PBMT + Overlap and character bigram
PBMT models do not differ significantly in scores.
It is interesting that we find no significant corre-
lation between human and automatic judgements,
leading us to believe that automatic judgements
are not viable in this particular scenario. This is
perhaps due to the fact that the reference transla-
tions the automatic measures rely on are in this
case not literal translations but rather more loosely
translated literary interpretations of the source text
in modern Dutch. The fact that both versions are
written on rhyme only worsens this problem, as
the author of the Modern Dutch version is often
very creative.
We think techniques such as the ones described
here can be of great benefit to laymen wishing to
investigate works that are not written in contem-
porary language, resulting in improved access to
these older works. Our character bigram translit-
eration model may also play some role as a com-
putational model in the study of the evolution of
orthography in language variants, as it often will
generate words that are strictly speaking not cor-
rect, but do resemble Modern Dutch in some way.
Automatic evaluation is another topic for future
work. It would be interesting to see if an automatic
measure operating on the character level correlates
better with human judgements.
References
Alistair Baron and Paul Rayson. 2008. VARD 2: A
tool for dealing with spelling variation in historical
corpora. In Proceedings of the Postgraduate Confer-
ence in Corpus Linguistics, Birmingham, UK. Aston
University.
A. De Gispert and J. B. Marino. 2006. Linguistic
knowledge in statistical phrase-based word align-
ment. Natural Language Engineering, 12(1):91?
108.
18
Andrew Finch, Paul Dixon, and Eiichiro Sumita. 2012.
Rescoring a phrase-based machine transliteration
system with recurrent neural network language mod-
els. In Proceedings of the 4th Named Entity Work-
shop (NEWS) 2012, pages 47?51, Jeju, Korea, July.
Association for Computational Linguistics.
Jan Hajic?, Jan Hric, and Vladislav Kubon?. 2000. Ma-
chine translation of very close languages. In Pro-
ceedings of the sixth conference on Applied natural
language processing, pages 7?12. Association for
Computational Linguistics.
Mike Kestemont, Walter Daelemans, and Guy
De Pauw. 2010. Weigh your words?memory-
based lemmatization for Middle Dutch. Literary
and Linguistic Computing, 25(3):287?301, Septem-
ber.
Philipp Koehn and Kevin Knight. 2003. Feature-rich
statistical translation of noun phrases. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 311?
318, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Philip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C.
Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens,
Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL. The As-
sociation for Computer Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In HLT-NAACL.
Philippe Langlais and Alexandre Patry. 2007. Trans-
lating unknown words by analogical learning. In
EMNLP-CoNLL, pages 877?886. ACL.
Preslav Nakov and Jo?rg Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 301?305, Jeju Island, Korea,
July. Association for Computational Linguistics.
S. B. Needleman and C. D. Wunsch. 1970. A general
method applicable to the search for similarities in
the amino acid sequence of two proteins. Journal of
molecular biology, 48(3):443?453, March.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Maja Popovic and Hermann Ney. 2004. Towards the
use of word stems and suffixes for statistical ma-
chine translation. In LREC. European Language Re-
sources Association.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In In Proc. Int. Conf. on
Spoken Language Processing, pages 901?904, Den-
ver, Colorado.
Jo?rg Tiedemann. 2009. Character-based PSMT for
closely related languages. In Llu??s Marque?s and
Harold Somers, editors, Proceedings of 13th An-
nual Conference of the European Association for
Machine Translation (EAMT?09), pages 12 ? 19,
Barcelona, Spain, May.
Gerhard B Van Huyssteen and Sule?ne Pilon. 2009.
Rule-based conversion of closely-related languages:
a dutch-to-afrikaans convertor. 20th Annual Sympo-
sium of the Pattern Recognition Association of South
Africa, December.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can we translate letters? In Second Work-
shop on Statistical Machine Translation, pages 33?
39, Prague, Czech Republic, jun. Association for
Computational Linguistics.
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2010. Paraphrase generation as mono-
lingual translation: Data and evaluation. In
B. Mac Namee J. Kelleher and I. van der Sluis, ed-
itors, Proceedings of the 10th International Work-
shop on Natural Language Generation (INLG
2010), pages 203?207, Dublin.
Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and
Colin Cherry. 2012. Paraphrasing for style. In
COLING, pages 2899?2914.
Amir Zeldes. 2007. Machine translation between lan-
guage stages: Extracting historical grammar from a
parallel diachronic corpus of Polish. In Matthew
Davies, Paul Rayson, Susan Hunston, and Pernilla
Danielsson, editors, Proceedings of the Corpus Lin-
guistics Conference CL2007. University of Birming-
ham.
19
