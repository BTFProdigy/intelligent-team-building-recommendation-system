1A quantitative evaluation of naturalistic models of language acquisition; the
efficiency of the Triggering Learning Algorithm compared to a Categorial
Grammar Learner
Paula Buttery
Natural Language and Information Processing Group,
Computer Laboratory, Cambridge University,
15 JJ Thomson Avenue, Cambridge, CB3 0FD, UK
paula.buttery@cl.cam.ac.uk
Abstract
Naturalistic theories of language acquisition assume
learners to be endowed with some innate language
knowledge. The purpose of this innate knowledge
is to facilitate language acquisition by constrain-
ing a learner?s hypothesis space. This paper dis-
cusses a naturalistic learning system (a Categorial
Grammar Learner (CGL)) that differs from previous
learners (such as the Triggering Learning Algorithm
(TLA) (Gibson and Wexler, 1994)) by employing a
dynamic definition of the hypothesis-space which
is driven by the Bayesian Incremental Parameter
Setting algorithm (Briscoe, 1999). We compare
the efficiency of the TLA with the CGL when ac-
quiring an independently and identically distributed
English-like language in noiseless conditions. We
show that when convergence to the target gram-
mar occurs (which is not guaranteed), the expected
number of steps to convergence for the TLA is
shorter than that for the CGL initialized with uni-
form priors. However, the CGL converges more
reliably than the TLA. We discuss the trade-off of
efficiency against more reliable convergence to the
target grammar.
1 Introduction
A normal child acquires the language of her envi-
ronment without any specific training. Chomsky
(1965) claims that, given the ?relatively slight ex-
posure? to examples and ?remarkable complexity?
of language, it would be ?an extraordinary intellec-
tual achievement? for a child to acquire a language
if not specifically designed to do so. His Argument
from the Poverty of the Stimulus suggests that if we
know X, and X is undetermined by learning expe-
rience then X must be innate. For an example con-
sider structure dependency in language syntax:
A question in English can be formed by invert-
ing the auxiliary verb and subject noun-phrase: (1a)
?Dinah was drinking a saucer of milk?; (1b) ?was
Dinah drinking a saucer of milk??
Upon exposure to this example, a child could hy-
pothesize infinitely many question-formation rules,
such as: (i) swap the first and second words in the
sentence; (ii) front the first auxiliary verb; (iii) front
words beginning with w.
The first two of these rules are refuted if the child
encounters the following: (2a) ?the cat who was
grinning at Alice was disappearing?; (2b) ?was the
cat who was grinning at Alice disappearing??
If a child is to converge upon the correct hypoth-
esis unaided she must be exposed to sufficient ex-
amples so that all false hypotheses are refuted. Un-
fortunately such examples are not readily available
in child-directed speech; even the constructions in
examples (2a) and (2b) are rare (Legate, 1999). To
compensate for this lack of data Chomsky suggests
that some principles of language are already avail-
able in the child?s mind. For example, if the child
had innately ?known? that all grammar rules are
structurally-dependent upon syntax she would never
have hypothesized rules (i) and (iii). Thus, Chom-
sky theorizes that a human mind contains a Univer-
sal Grammar which defines a hypothesis-space of
?legal? grammars.1 This hypothesis-space must be
both large enough to contain grammar?s for all of
the world?s languages and small enough to ensure
successful acquisition given the sparsity of data.
Language acquisition is the process of searching the
hypothesis-space for the grammar that most closely
describes the language of the environment. With
estimates of the number of living languages being
around 6800 (Ethnologue, 2004) it is not sensible to
model the hypothesis-space of grammars explicitly,
rather it must be modeled parametrically. Language
acquisition is then the process of setting these pa-
rameters. Chomsky (1981) suggested that param-
eters should represent points of variation between
languages, however the only requirement for pa-
rameters is that they define the current hypothesis-
space.
1Discussion of structural dependence as evidence of the Ar-
gument from the Poverty of Stimulus is illustrative, the sig-
nificance being that innate knowledge in any form will place
constraints on the hypothesis-space
2The properties of the parameters used by this
learner (the CGL) are as follows: (1) Parameters are
lexical; (2) Parameters are inheritance based; (3) Pa-
rameter setting is statistical.
1 - Lexical Parameters
The CGL employs parameter setting as a means
to acquire a lexicon; differing from other paramet-
ric learners, (such as the Triggering Learning Al-
gorithm (TLA) (Gibson and Wexler, 1994) and the
Structural Triggers Learner (STL) (Fodor, 1998b),
(Sakas and Fodor, 2001)) which acquire general
syntactic information rather than the syntactic prop-
erties associated with individual words.2
In particular, a categorial grammar is acquired.
The syntactic properties of a word are contained in
its lexical entry in the form of a syntactic category.
A word that may be used in multiple syntactic situ-
ations (or sub-categorization frames) will have mul-
tiple entries in the lexicon.
Syntactic categories are constructed from a finite
set of primitive categories combined with two op-
erators (/ and \) and are defined by their members
ability to combine with other constituents; thus con-
stituents may be thought of as either functions or
arguments.
The arguments of a functional constituent are
shown to the right of the operators and the result
to the left. The forward slash operator (/) indicates
that the argument must appear to the right of the
function and a backward slash (\) indicates that it
must appear on the left. Consider the following
CFG structure which describes the properties of a
transitive verb:
s ? np vp
vp ? tv np
tv ? gets, finds, ...
Assume that there is a set of primitive categories
{s,np}. A vp must be in the category of func-
tional constituents that takes a np from the left and
returns an s. This can be written s\np. Likewise
a tv takes an np from the right and returns a vp
(whose type we already know). A tv may be writ-
ten (s\np)/np.
Rules may be used to combine categories. We
assume that our learner is innately endowed with the
rules of function application, function composition
and generalized weak permutation (Briscoe, 1999)
(see figures 1 and 2).
? Forward Application (>)
X/Y Y ? X
2The concept of lexical parameters and the lexical-linking
of parameters is to be attributed to Borer (1984).
? Backward Application (<)
Y X\Y ? X
? Forward Composition (> B)
X/Y Y/Z ? X/Z
? Backward Composition (< B)
Y \X Z\Y ? X\Z
? Generalized Weak Permutation (P )
((X | Y1)... | Yn) ? ((X | Yn)... | Y1)
where | is a variable over \ and /.
Alice
np
may
(s\np)/(s\np)
eat
(s\np)/np
> B
(s\np)/np
the cake???np
>
s\np
<
s
Figure 1: Illustration of forward/backward applica-
tion (>, <) and forward composition (> B)
the
np/n
rabbit
n
that
(n\n)/(s/np)
she
np
saw
(s\np)/np
P
(s/np)\np
<
(s/np)
>
n\n
<
n
>
np
Figure 2: Illustration of generalized weak permuta-
tion (P )
The lexicon for a language will contain a finite
subset of all possible syntactic categories, the size of
which depends on the language. Steedman (2000)
suggests that for English the lexical functional cate-
gories never need more than five arguments and that
these are needed only in a limited number of cases
such as for the verb bet in the sentence I bet you five
pounds for England to win.
The categorial grammar parameters of the CGL
are concerned with defining the set of syntactic
categories present in the language of the environ-
ment. Converging on the correct set aids acquisition
by constraining the learner?s hypothesized syntactic
categories for an unknown word. A parameter (with
3value of either ACTIVE or INACTIVE) is associ-
ated with every possible syntactic category to indi-
cate whether the learner considers the category to be
part of the target grammar.
Some previous parametric learners (TLA and
STL) have been primarily concerned with overall
syntactic phenomena rather than the syntactic prop-
erties of individual words. Movement parameters
(such as the V 2 parameter of the TLA) may be cap-
tured by the CGL using innate rules or multiple lex-
ical entries. For instance, Dutch and German word
order is captured by assuming that verbs in these
languages systematically have two categories, one
determining main clause order and the other subor-
dinate clause orders.
2 - Inheritance Based Parameters
The complex syntactic categories of a categorial
grammar are a sub-categorization of simpler cate-
gories; consequently categories may be arranged in
a hierarchy with more complex categories inheriting
from simpler ones. Figure 3 shows a fragment of a
possible hierarchy. This hierarchical organization of
parameters provides the learner with several bene-
fits: (1) The hierarchy can enforce an order on learn-
ing; constraints may be imposed such that a parent
parameter must be acquired before a child parame-
ter (for example, in Figure 3, the learner must ac-
quire intransitive verbs before transitive verbs may
be hypothesized). (2) Parameter values may be in-
herited as a method of acquisition. (3) The parame-
ters are stored efficiently.
s - ACTIVE
``````
      
s/s s\np - ACTIVE
XXXXX

[s\np]/np - ACTIVE [s\np]/[s\np]
Figure 3: Partial hierarchy of syntactic categories.
Each category is associated with a parameter indi-
cating either ACTIVE or INACTIVE status.
3 - Statistical Parameter Setting
The learner uses a statistical method to track rela-
tive frequencies of parameter-setting-utterances in
the input.3 We use the Bayesian Incremental Pa-
rameter Setting (BIPS) algorithm (Briscoe, 1999)
to set the categorial parameters. Such an approach
sets the parameters to the values that are most likely
given all the accumulated evidence. This represents
3Other statistical parameter setting models include Yang?s
Variational model (2002) and the Guessing STL (Fodor, 1998a)
a compromise between two extremes: implementa-
tions of the TLA are memoryless allowing a param-
eter values to oscillate; some implementations of the
STL set a parameter once, for all time.
Using the BIPS algorithm, evidence from an in-
put utterance will either strengthen the current pa-
rameter settings or weaken them. Either way, there
is re-estimation of the probabilities associated with
possible parameter values. Values are only assigned
when sufficient evidence has been accumulated, i.e.
once the associated probability reaches a threshold
value. By employing this method, it becomes un-
likely for parameters to switch between settings as
the consequence of an erroneous utterance.
Another advantage of using a Bayesian approach
is that we may set default parameter values by as-
signing Bayesian priors; if a parameter?s default
value is strongly biased against the accumulated ev-
idence then it will be difficult to switch. Also, we no
longer need to worry about ambiguity in parameter-
setting-utterances (Clark, 1992) (Fodor, 1998b): the
Bayesian approach allows us to solve this problem
?for free? since indeterminacy just becomes another
case of error due to misclassification of input data
(Buttery and Briscoe, 2004).
2 Overview of the Categorial Grammar
Learner
The learning system is composed of a three mod-
ules: a semantics learning module, syntax learning
module and memory module. For each utterance
heard the learner receives an input stream of word
tokens paired with possible semantic hypotheses.
For example, on hearing the utterance ?Dinah drinks
milk? the learner may receive the pairing: ({dinah,
drinks, milk}, drinks(dinah, milk)).
2.1 The Semantic Module
The semantic module attempts to learn the mapping
between word tokens and semantic symbols, build-
ing a lexicon containing the meaning associated
with each word sense. This is achieved by analyz-
ing each input utterance and its associated semantic
hypotheses using cross-situational techniques (fol-
lowing Siskind (1996)).
For a trivial example consider the utterances ?Al-
ice laughs? and ?Alice eats cookies?; they might
have word tokens paired with semantic expressions
as follows: ({alice, laughs}, laugh(alice)), ({alice,
eats, cookies}, eat(alice, cookies)).
From these two utterances it is possible to ascer-
tain that the meaning associated with the word token
alice must be alice since it is the only semantic ele-
ment that is common to both utterances.
42.2 The Syntactic Module
The learning system links the semantic module and
syntactic module by using a typing assumption: the
semantic arity of a word is usually the same as its
number of syntactic arguments. For example, if it is
known that likes maps to like(x,y), then the typ-
ing assumption suggests that its syntactic category
will be in one of the following forms: a\b\c, a/b\c,
a\b/c, a/b/c or more concisely a | b | c (where a, b
and c may be basic or complex syntactic categories
themselves).
By employing the typing assumption the number
of arguments in a word?s syntactic category can be
hypothesized. Thus, the objective of the syntactic
module is to discover the arguments? category types
and locations.
The module attempts to create valid parse trees
starting from the syntactic information already as-
sumed by the typing assumption (following But-
tery (2003)). A valid parse is one that adheres
to the rules of the categorial grammar as well as
the constraints imposed by the current settings of
the parameters. If a valid parse can not be found
the learner assumes the typing assumption to have
failed and backtracks to allow type raising.
2.3 Memory Module
The memory module records the current state of
the hypothesis-space. The syntactic module refers
to this information to place constraints upon which
syntactic categories may be hypothesized. The
module consists of two hierarchies of parameters
which may be set using the BIPS algorithm:
Categorial Parameters determine whether a cat-
egory is in use within the learner?s current model
of the input language. An inheritance hierarchy of
all possible syntactic categories (for up to five argu-
ments) is defined and a parameter associated with
each one (Villavicencio, 2002). Every parameter
(except those associated with primitive categories
such as S) is originally set to INACTIVE, i.e. no
categories (except primitives) are known upon the
commencement of learning. A categorial parameter
may only be set to ACTIVE if its parent category
is already active and there has been satisfactory ev-
idence that the associated category is present in the
language of the environment.
WordOrder Parameters determine the underly-
ing order in which constituents occur. They may be
set to either FORWARD or BACKWARD depend-
ing on whether the constituents involved are gen-
erally located to the right or left. An example is
the parameter that specifies the direction of the sub-
ject of a verb: if the language of the environment
is English this parameter would be set to BACK-
WARD since subjects generally appear to the left of
the verb. Evidence for the setting of word order pa-
rameters is collected from word order statistics of
the input language.
3 The acquisition of an English-type
language
The English-like language of the three-parameter
system studied by Gibson and Wexler has the
parameter settings and associated unembedded
surface-strings as shown in Figure 4. For this task
we assume that the surface-strings of the English-
like language are independent and identically dis-
tributed in the input to the learner.
Specifier Complement V2
0 (Left) 1 (Right) 0 (off )
1. Subj Verb
2. Subj Verb Obj
3. Subj Verb Obj Obj
4. Subj Aux Verb
5. Subj Aux Verb Obj
6. Subj Aux Verb Obj Obj
7. Adv Subj Verb
8. Adv Subj Verb Obj
9. Adv Subj Verb Obj Obj
10. Adv Subj Aux Verb
11. Adv Subj Aux Verb Obj
12. Adv Subj Aux Verb Obj Obj
Figure 4: Parameter settings and surface-strings of
Gibson and Wexler?s English-like Language.
3.1 Efficiency of Trigger Learning Algorithm
For the TLA to be successful it must converge to
the correct parameter settings of the English-like
language. Berwick and Niyogi (1996) modeled the
TLA as a Markov process (see Figure 5).
Using this model it is possible to calculate the
probability of converging to the target from each
starting grammar and the expected number of steps
before convergence.
Probability of Convergence:
Consider starting from Grammar 3, after the process
finishes looping it has a 3/5 probability of mov-
ing to Grammar 4 (from which it will never con-
verge) and a 2/5 probability of moving to Grammar
7 (from which it will definitely converge), therefore
there is a 40% probability of converging to the target
grammar when starting at Grammar 3.
5Expected number of Steps to Convergence:
Let Sn be the expected number of steps from state
n to the target state. For starting grammars 6, 7 and
8, which definitely converge, we know:
S6 = 1 +
5
6S6 (1)
S7 = 1 +
2
3S7 +
1
18S8 (2)
S8 = 1 +
1
12S6 +
1
36S7 +
8
9S8 (3)
and for the times when we do converge from gram-
mars 3 and 1 we can expect:
S1 = 1 +
3
5S1 (4)
S3 = 1 +
31
33S3 (5)
Figure 6 shows the probability of convergence and
expected number of steps to convergence for each
of the starting grammars. The expected number of
steps to convergence ranges from infinity (for start-
ing grammars 2 and 4) down to 2.5 for Grammar
1. If the distribution over the starting grammars is
uniform then the overall probability of converging
is the sum of the probabilities of converging from
each state divided by the total number of states:
1.00 + 1.00 + 1.00 + 1.00 + 0.40 + 0.66
8 = 0.63
(6)
and the expected number of steps given that you
converge is the weighted average of the number of
steps from each possibly converging state:
5.47 + 14.87 + 6 + 21.98 ? 0.4 + 2.5 ? 0.66
1.00 + 1.00 + 1.00 + 1.00 + 0.40 + 0.66 = 7.26
(7)
3.2 Efficiency of Categorial Grammar Learner
The input data to the CGL would usually be an ut-
terance annotated with a logical form; the only data
available here however, is surface-strings consist-
ing of word types. Hence, for the purpose of com-
parison with the TLA the semantic module of our
learner is by-passed; we assume that mappings to
semantic forms have previously been acquired and
that the subject and objects of surface-strings are
known. For example, given surface-string 1 (Subj
Verb) we assume the mapping Verb 7? verb(x),
which provides Verbwith a syntactic category of the
form a|b by the typing assumption (where a, b are
unknown syntactic categories and | is an operator
over \ and /); we also assume Subj to map to a prim-
itive syntactic category SB, since it is the subject of
Verb.
The criteria for success for the CGL when acquir-
ing Gibson and Wexler?s English-like language is a
lexicon containing the following:4
Adv S/S Aux [S\SB]/[S\SB]
Obj OB Verb S\SB
Subj SB [S\SB]/OB
[[S\SB]/OB]/OB
where S (sentence), SB (subject) and OB (ob-
ject) are primitive categories which are innate to the
learner with SB and OB assumed to be derivable
from the semantic module.
During the learning process the CGL will have
constructed a category hierarchy by setting appro-
priate categorial parameters to true (see Figure 7).
The learner will have also constructed a word-order
hierarchy (Figure 8), setting parameters to FOR-
WARDor BACKWARD. These hierarchies are used
during the learning process to constrain hypothe-
sized syntactic categories. For this task the set-
ting of the word-order parameters becomes trivial
and their role in constraining hypotheses negligible;
consequently, the rest of our argument will relate to
categorial parameters only. For the purpose of this
gendir = /
aaa
!!!
subjdir = \ vargdir = /
Figure 8: Word-order parameter settings required to
parse Gibson and Wexler?s English-like language.
analysis parameters are initialized with uniform pri-
ors and are originally set INACTIVE. Since the in-
put is noiseless, the switching threshold is set such
that parameters may be set ACTIVE upon the evi-
dence from one surface-string.
It is a requirement of the parameter setting de-
vice that the parent-types of hypothesized syntax
categories are ACTIVE before new parameters are
set. Thus, the learner is not allowed to hypoth-
esize the syntactic category for a transitive verb
[[S\SB]/OB] before it has learnt the category for
an intransitive verb [S\SB]; this behaviour con-
strains over-generation. Additionally, it is usually
not possible to derive a word?s full syntactic cate-
gory (i.e. without any remaining unknowns) unless
it is the only new word in the clause.
As a consequence of these issues, the order in
which the surface-strings appear to the learner af-
4Note that the lexicon would usually contain orthographic
entries for the words in the language rather than word type en-
tries.
6fects the speed of acquisition. For instance, the
learner prefers to see the surface-string Subj Verb
before Subj Verb Obj so that it can acquire the
maximum information without wasting any strings.
For the English-type language described by Gib-
son and Wexler the learner can optimally acquire
the whole lexicon after seeing only 5 surface-strings
(one string needed for each new complex syntactic
category to be learnt). However, the strings appear
to the learner in a random order so it is necessary to
calculate the expected number of strings (or steps)
before convergence.
The learner must necessarily see the string Subj
Verb before it can learn any other information. With
12 surface-strings the probability of seeing Subj
Verb is 1/12 and the expected number of strings be-
fore it is seen is 12. The learner can now learn from
3 surface-strings: Subj Verb Obj, Subj Aux Verb and
Adv Subj Verb. Figure 9 shows a Markov structure
of the process. From the model we can calculate the
expected number of steps to converge to be 24.53.
4 Conclusions
The TLA and CGL were compared for efficiency
(expected number of steps to convergence) when
acquiring the English-type grammar of the three-
parameter system studied by Gibson and Wexler.
The expected number of steps for the TLA was
found to be 7.26 but the algorithm only converged
63% of the time. The expected number of steps for
the CGL is 24.53 but the learner converges more re-
liably; a trade off between efficiency and success.
With noiseless input the CGL can only fail if there
is insufficient input strings or if Bayesian priors are
heavily biased against the target. Furthermore, the
CGL can be made robust to noise by increasing the
probability threshold at which a parameter may be
set ACTIVE; the TLA has no mechanism for coping
with noisy data.
The CGL learns incrementally; the hypothesis-
space from which it can select possible syntactic
categories expands dynamically and, as a conse-
quence of the hierarchical structure of parameters,
the speed of acquisition increases over time. For
instance, in the starting state there is only a 1/12
probability of learning from surface-strings whereas
in state k (when all but one category has been ac-
quired) there is a 1/2 probability. It is likely that
with a more complex learning task the benefits of
this incremental approach will outweigh the slow
starting costs. Related work on the effects of incre-
mental learning on STL performance (Sakas, 2000)
draws similar conclusions. Future work hopes to
compare the CGL with other parametric learners
(such as the STL) in larger domains.
References
R Berwick and P Niyogi. 1996. Learning from trig-
gers. Linguistic Inquiry, 27(4):605?622.
H Borer. 1984. Parametric Syntax: Case Studies
in Semitic and Romance Languages. Foris, Dor-
drecht.
E Briscoe. 1999. The acquisition of grammar in an
evolving population of language agents. Machine
Intelligence, 16.
P Buttery and T Briscoe. 2004. The significance of
errors to parametric models of language acquisi-
tion. Technical Report SS-04-05, American As-
sociation of Artificial Intelligence, March.
P Buttery. 2003. A computational model for first
language acquisition. In CLUK-6, Edinburgh.
N Chomsky. 1965. Aspects of the Theory of Syntax.
MIT Press.
N Chomsky. 1981. Lectures on Government and
Binding. Foris Publications.
R Clark. 1992. The selection of syntactic knowl-
edge. Language Acquisition, 2(2):83?149.
Ethnologue. 2004. Languages of the
world, 14th edition. SIL International.
http://www.ethnologue.com/.
J Fodor. 1998a. Parsing to learn. Journal of Psy-
cholinguistic Research, 27(3):339?374.
J Fodor. 1998b. Unambiguous triggers. Linguistic
Inquiry, 29(1):1?36.
E Gibson and K Wexler. 1994. Triggers. Linguistic
Inquiry, 25(3):407?454.
J Legate. 1999. Was the argument that was made
empirical? Ms, Massachusetts Institute of Tech-
nology.
W Sakas and J Fodor. 2001. The structural triggers
learner. In S Bertolo, editor, Language Acquisi-
tion and Learnability, chapter 5. Cambridge Uni-
versity Press, Cambridge, UK.
W Sakas. 2000. Ambiguity and the Computational
Feasibility of Syntax Acquisition. Ph.D. thesis,
City University of New York.
J Siskind. 1996. A computational study of
cross situational techniques for learning word-to-
meaning mappings. Cognition, 61(1-2):39?91,
Nov/Oct.
M Steedman. 2000. The Syntactic Process. MIT
Press/Bradford Books.
A Villavicencio. 2002. The acquisition of a
unification-based generalised categorial gram-
mar. Ph.D. thesis, University of Cambridge.
C Yang. 2002. Knowledge and Learning in Natural
Language. Oxford University Press.
7Figure 5: Gibson and Wexler?s TLA as a Markov structure. Circles represent possible grammars (a config-
uration of parameter settings). The target grammar lies at the centre of the structure. Arrows represent the
possible transitions between grammars. Note that the TLA is constrained to only allow movement between
grammars that differ by one parameter value. The probability of moving between Grammar Gi and Gram-
marGj is a measure of the number of target surface-strings that are inGj but not Gi normalized by the total
number of target surface-strings as well as the number of alternate grammars the learner can move to. For
example the probability of moving from Grammar 3 to Grammar 7 is 2/12 ? 1/3 = 1/18 since there are 2
target surface-strings allowed by Grammar 7 that are not allowed by Grammar 3 out of a possible of 12 and
three grammars that differ from Grammar 3 by one parameter value.
Initial Language Initial Grammar Prob. of Converging Expected no. of Steps
VOS -V2 110 0.66 2.50
VOS +V2 111 0.00 n/a
OVS -V2 100 0.40 21.98
OVS +V2 101 0.00 n/a
SVO -V2 010 1.00 0.00
SVO +V2 011 1.00 6.00
SOV -V2 000 1.00 5.47
SOV +V2 001 1.00 14.87
Figure 6: Probability and expected number of steps to convergence from each starting grammar to an
English-like grammar (SVO -V2) when using the TLA.
8top`
`````
!!!
      
SB OB S`
`````
      
S/S S\SB
XXXXX

[S\SB]/OB
[[S\SB]/OB]/OB
[S\SB]/[S\SB]
Figure 7: Category hierarchy required to parse Gibson and Wexler?s English-like language.
Figure 9: The CGL as a Markov structure. The states represent the set of known syntactic cate-
gories: state S - {}, state a - {S\SB}, state b - {S\SB, S/S}, state c - {S\SB, [S\SB]/OB},
state d - {S\SB, [S\SB]/[S\SB]}, state e - {S\SB, S/S, [S\SB]/OB}, state f - {S\SB,
[S\SB]/OB, [[S\SB]/OB]/OB}, state g - {S\SB, [S\SB]/[S\SB], S/S} state h - {S\SB,
[S\SB]/[S\SB], [S\SB]/OB}, state i - {S\SB, S/S, [S\SB]/OB, [S\SB]/[S\SB]}, state j -
{S\SB, S/S, [S\SB]/OB, [[S\SB]/OB]/OB}, state k - {S\SB, [S\SB]/OB, [[S\SB]/OB]/OB,
[S\SB]/[S\SB]}, state l - {S\SB, [S\SB]/OB, [[S\SB]/OB]/OB, [S\SB]/[S\SB], S/S}.
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 33?40,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
I will shoot your shopping down and you can shoot all my tins
Automatic Lexical Acquisition from the CHILDES Database
Paula Buttery and Anna Korhonen
RCEAL, University of Cambridge
9 West Road, Cambridge, CB3 9DB, UK
pjb48, alk23@cam.ac.uk
Abstract
Empirical data regarding the syntactic com-
plexity of children?s speech is important for
theories of language acquisition. Currently
much of this data is absent in the annotated
versions of the CHILDES database. In this
perliminary study, we show that a state-of-
the-art subcategorization acquisition system of
Preiss et al (2007) can be used to extract large-
scale subcategorization (frequency) informa-
tion from the (i) child and (ii) child-directed
speech within the CHILDES database without
any domain-specific tuning. We demonstrate
that the acquired information is sufficiently ac-
curate to confirm and extend previously re-
ported research findings. We also report quali-
tative results which can be used to further im-
prove parsing and lexical acquisition technol-
ogy for child language data in the future.
1 Introduction
Large empirical data containing children?s speech are
the key to developing and evaluating different theo-
ries of child language acquisition (CLA). Particularly
important are data related to syntactic complexity of
child language since considerable evidence suggests
that syntactic information plays a central role during
language acquisition, e.g. (Lenneberg, 1967; Naigles,
1990; Fisher et al, 1994).
The standard corpus in the study of CLA is the
CHILDES database (MacWhinney, 2000)1 which pro-
vides 300MB of transcript data of interactions be-
1See http://childes.psy.cmu.edu for details.
tween children and parents over 25 human languages.
CHILDES is currently available in raw, part-of-speech-
tagged and lemmatized formats. However, adequate
investigation of syntactic complexity requires deeper
annotations related to e.g. syntactic parses, subcatego-
rization frames (SCFs), lexical classes and predicate-
argument structures.
Although manual syntactic annotation is possible,
it is extremely costly. The alternative is to use natu-
ral language processing (NLP) techniques for annota-
tion. Automatic techniques are now viable, cost effec-
tive and, although not completely error-free, are suffi-
ciently accurate to yield annotations useful for linguis-
tic purposes. They also gather important qualitative
and quantitative information, which is difficult for hu-
mans to obtain, as a side-effect of the acquisition pro-
cess.
For instance, state-of-the-art statistical parsers,
e.g. (Charniak, 2000; Briscoe et al, 2006), have wide
coverage and yield grammatical representations capa-
ble of supporting various applications (e.g. summa-
rization, information extraction). In addition, lexi-
cal information (e.g. subcategorization, lexical classes)
can now be acquired automatically from parsed
data (McCarthy and Carroll, 2003; Schulte im Walde,
2006; Preiss et al, 2007). This information comple-
ments the basic grammatical analysis and provides ac-
cess to the underlying predicate-argument structure.
Containing considerable ellipsis and error, spoken
child language can be challenging for current NLP
techniques which are typically optimized for written
adult language. Yet Sagae et al (2005) have recently
demonstrated that existing statistical parsing tech-
niques can be usefully modified to analyse CHILDES
33
with promising accuracy. Although further improve-
ments are still required for optimal accuracy, this re-
search has opened up the exciting possibility of auto-
matic grammatical annotation of the entire CHILDES
database in the future.
However, no work has yet been conducted on au-
tomatic acquisition of lexical information from child
speech. The only automatic lexical acquisition study
involving CHILDES that we are aware of is that of
Buttery and Korhonen (2005). The study involved
extracting subcategorization information from (some
of) the adult (child-directed) speech in the database,
and showing that this information differs from that ex-
tracted from the spoken part of the British National
Corpus (BNC) (Burnard, 1995).
In this paper, we investigate whether state-of-the-
art subcategorization acquisition technology can be
used?without any domain-specific tuning?to obtain
large-scale verb subcategorization frequency informa-
tion from CHILDES which is accurate enough to show
differences and similarities between child and adult
speech, and thus be able to provide support for syn-
tactic complexity studies in CLA.
We use the new system of Preiss et al (2007) to
extract SCF frequency data from the (i) child and
(ii) child-directed speech within CHILDES. We show
that the acquired information is sufficiently accu-
rate to confirm and extend previously reported SCF
(dis)similarities between the two types of data. In par-
ticular, we demonstrate that children and adults have
different preferences for certain types of verbs, and
that these preferences seem to influence the way chil-
dren acquire subcategorization. In addition, we report
qualitative results which can be used to further im-
prove parsing and lexical acquisition technology for
spoken child language data in the future.
2 Subcategorization Acquisition System
We used for subcategorization acquisition the new sys-
tem of Preiss, Briscoe and Korhonen (2007) which
is essentially a much improved and extended version
of Briscoe and Carroll?s (1997) system. It incorpo-
rates 168 SCF distinctions, a superset of those found
in the COMLEX Syntax (Grishman et al, 1994) and
ANLT (Boguraev et al, 1987) dictionaries. Currently,
SCFs abstract over specific lexically governed parti-
cles and prepositions and specific predicate selectional
preferences but include some derived semi-predictable
bounded dependency constructions, such as particle
and dative movement?this will be revised in future
versions of the SCF system.
The system tokenizes, tags, lemmatizes and parses
input sentences using the recent (second) release of
the RASP (Robust Accurate Statistical Parsing) system
(Briscoe et al, 2006) which parses arbitrary English
text with state-of-the-art levels of accuracy. SCFs are
extracted from the grammatical relations (GRs) output
of the parser using a rule-based classifier. This clas-
sifier operates by exploiting the close correspondence
between the dependency relationships which the GRs
embody and the head-complement structure which
subcategorization acquisition attempts to recover. Lex-
ical entries of extracted SCFs are constructed for each
word in the corpus data. Finally, the entries may be
optionally filtered to obtain a more accurate lexicon.
This is done by setting empirically determined thresh-
olds on the relative frequencies of SCFs.
When evaluated on cross-domain corpora contain-
ing mainly adult language, this system achieves 68.9
F-measure2 in detecting SCF types?a result which
compares favourably to those reported with other com-
parable SCF acquisition systems.
3 Data
The English (British and American) sections of the
CHILDES database (MacWhinney, 2000) were used to
create two corpora: 1) CHILD and 2) CDS. Both cor-
pora contained c. 1 million utterances which were se-
lected from the data after some utterances contain-
ing un-transcribable sections were removed. Speak-
ers were identified using speaker-id codes within the
CHAT transcriptions of the data:3 CHILD contained
the utterances of speakers identified as target children;
CDS contained input from speakers identified as par-
ents/caretakers. The mean utterance length (measured
in words) in CHILD and CDS were 3.48 and 4.61, re-
spectively. The mean age of the child speaker in CHILD
is around 3 years 6 months.4
2See Section 4 for details of F-measure.
3CHAT is the transcription and coding format used by all the
transcriptions within CHILDES.
4The complete age range is from 1 year and 1 month up to 7
years.
34
3.1 Test Verbs and SCF Lexicons
We selected a set of 161 verbs for experimentation.
The words were selected at random, subject to the con-
straint that a sufficient number of SCFs would be ex-
tracted (> 100) from both corpora to facilitate max-
imally useful comparisons. All sentences containing
an occurrence of one of the test verbs were extracted
from the two corpora and fed into the SCF acquisition
system described earlier in section 2.
In some of our experiments the two lexicons were
compared against the VALEX lexicon (Korhonen et al,
2006)?a large subcategorization lexicon for English
which was acquired automatically from several cross-
domain corpora (containing both written and spoken
language). VALEX includes SCF and frequency infor-
mation for 6,397 English verbs. We employed the
most accurate version of the lexicon here (87.3 F-
measure)?this lexicon was obtained by selecting high
frequency SCFs and supplementing them with lower
frequency SCFs from manually built lexicons.
4 Analysis
4.1 Methods for Analysis
The similarity between verb and SCF distributions in
the lexicons was examined. To maintain a robust anal-
ysis in the presence of noise, multiple similarity mea-
sures were used to compare the verb and SCF distri-
butions (Korhonen and Krymolowski, 2002). In the
following p = (p
i
) and q = (q
i
) where p
i
and q
i
are
the probabilities associated with SCF
i
in distributions
(lexicons) P and Q:
? Intersection (IS) - the intersection of non-zero probability
SCFs in p and q;
? Spearman rank correlation (RC) - lies in the range [1; 1], with
values near 0 denoting a low degree of association and val-
ues near -1 and 1 denoting strong association;
? Kullback-Leibler (KL) distance - a measure of the additional
information needed to describe p using q, KL is always ? 0
and = 0 only when p ? q;
The SCFs distributions acquired from the corpora for
the chosen words were evaluated against: (i) a gold
standard SCF lexicon created by merging the SCFs in
the COMLEX and ANLT syntax dictionaries?this en-
abled us to determine the accuracy of the acquired
SCFs; (ii) another acquired SCF lexicon (as if it were
a gold standard)?this enabled us to determine simi-
larity of SCF types between two lexicons. In each case
Verb CHILD CDS
go 1 1
want 2 2
get 3 3
know 4 4
put 5 6
see 6 5
come 7 10
like 8 7
make 9 11
say 10 8
take 11 13
eat 12 14
play 13 15
need 14 16
look 15 12
fall 16 22
sit 17 21
think 18 9
break 19 27
give 20 17
Table 1: Ranks of the 20 most frequent verbs in CHILD
and in CDS
we recorded the number of true positives (TPs), correct
SCFs, false positives (FPs), incorrect SCFs, and false
negatives (FNs), correct SCFs not in the gold standard.
Using these counts, we calculated type precision
(the percentage of SCF types in the acquired lexicon
which are correct), type recall (the percentage of SCF
types in the gold standard that are in the lexicon) and
F-measure:
F =
2 ? precision ? recall
precision + recall
(1)
4.2 Verb Analysis
Before conducting the SCF comparisons we first com-
pared (i) our 161 test verbs and (ii) all the 1212
common verbs and their frequencies in CHILD and
CDS using the Spearman rank correlation (RC) and
the Kullback-Leibler distance (KL). The result was
a strong correlation between the 161 test verbs (RC =
0.920 ? 0.0791, KL = 0.05) as well as between all the
1212 verbs (RC = 0.851 ? 0.0287, KL = 0.07) in the
two corpora.
These figures suggest that the child-directed speech
(which is less diverse in general than speech between
adults, see e.g. the experiments of Buttery and Ko-
rhonen (2005)) contains a very similar distribution of
verbs to child speech. This is to be expected since the
35
corpora essentially contain separate halves of the same
interactions.
However, our large-scale frequency data makes it
possible to investigate the cause for the apparently
small differences in the distributions. We did this by
examining the strength of correlation throughout the
ranking. We compared the ranks of the individual
verbs and discovered that the most frequent verbs in
the two corpora have indeed very similar ranks. Ta-
ble 1 lists the 20 most frequent verbs in CHILD (starting
from the highest ranked verb) and shows their ranks
in CDS. As illustrated in the table, the top 4 verbs
are identical in the two corpora (go, want, get, know)
while the top 15 are very similar (including many ac-
tion verbs e.g. put, look, sit, eat, and play).
Yet some of the lower ranked verbs turned out to
have large rank differences between the two corpora.
Two such relatively highly ranked verbs are included
in the table?think which has a notably higher rank
in CDS than in CHILD, and break which has a higher
rank in CHILD than in CDS. Many other similar cases
were found in particular among the medium and low
frequency verbs in the two corpora.
To obtain a better picture of this, we calculated for
each verb its rank difference between CHILD vs. CDS.
Table 2 lists 40 verbs with substantial rank differences
between the two corpora. The first column shows
verbs which have higher ranks in CHILD than in CDS,
and the second column shows verbs with higher ranks
in CDS than in CHILD. We can see e.g. that children
tend to prefer verbs such as shoot, die and kill while
adults prefer verbs such as remember, send and learn.
To investigate whether these differences in pref-
erences are random or motivated in some manner,
we classified the verbs with the largest differences
in ranks (>10) into appropriate Levin-style lexical-
semantic classes (Levin, 1993) according to their pre-
dominant senses in the two corpora.5 We discovered
that the most frequent classes among the verbs that
children prefer are HIT (e.g. bump, hit, kick), BREAK
(e.g. crash, break, rip), HURT (e.g. hurt, burn, bite)
and MOTION (e.g. fly, jump, run) verbs. Overall, many
of the preferred verbs (regardless of the class) express
negative actions or feelings (e.g. shoot, die, scare,
hate).
5This classification was done manually to obtain a reliable re-
sult.
CHILD CDS
shoot tie remember hope
hate wish send suppose
die cut learn bet
write crash wipe kiss
use kick pay smell
bump scare feed guess
win step ask change
lock burn feel set
fight stand listen stand
jump care wait wonder
Table 2: 20 verbs ranked higher in (i) child speech and
(ii) child-directed speech.
In contrast, adults have a preference for verbs from
classes expressing cognitive processes (e.g. remember,
suppose, think, wonder, guess, believe, hope, learn) or
those that can be related to the education of children,
e.g. the WIPE verbs wash, wipe and brush and the PER-
FORMANCE verbs draw, dance and sing. In contrast to
children, adults prefer verbs which express positive ac-
tions and feelings (e.g. share, help, love, kiss).
It is commonly reported that child CLA is moti-
vated by a wish to communicate desires and emo-
tions, e.g. (Pinker, 1994), but a relative preference
in child speech over child-directed speech for certain
verb types or verbs expressing negative actions and
feelings has not been explicitly shown on such a scale
before. While this issue requires further investigation,
our findings already demonstrate the value of using
large scale corpora in producing novel data and hy-
potheses for research in CLA.
4.3 SCF Analysis
4.3.1 Quantitative SCF Comparison
The average number of SCFs taken by studied verbs
in the two corpora proved quite similar. In unfil-
tered SCF distributions, verbs in CDS took on average
a larger number of SCFs (29) than those in CHILD (24),
but in the lexicons filtered for accuracy the numbers
were identical (8?10, depending on the filtering thresh-
old applied). The intersection between the CHILD /
CDS SCFs and those in the VALEX lexicon was around
0.5, indicating that the two lexicons included only
50% of the SCFs in the lexicon extracted from general
(cross-domain) adult language corpora. Recall against
VALEXwas consequently low (between 48% and 68%
depending on the filtering threshold) but precision was
around 50-60% for both CHILDES and CDS lexicons
36
Measures Unfilt. Filt.
Precision (%) 82.9 88.7
Recall (%) 69.3 44.5
F-measure 75.5 59.2
IS 0.73 0.62
RC 0.69 0.72
KL 0.33 0.46
Table 3: Average results when SCF distributions in
CHILD and CDS are compared against each other.
(also depending on the filtering threshold), which is
a relatively good result for the challenging CHILDES
data. However, it should be remembered that with this
type of data it would not be expected for the SCF sys-
tem to achieve as high precision and recall as it would
on, for instance, adult written text and that the missing
SCFs and/or misclassified SCFs are likely to provide us
with the most interesting information.
As expected, there were differences between the
SCF distributions in the two lexicons. Table 3 shows
the results when the CHILD and CDS lexicons are com-
pared against each other (i.e. using the CDS as a gold
standard). The comparison was done using both the
unfiltered and filtered (using relative frequency thresh-
old of 0.004) versions of the lexicons. The similarity
in SCF types is 75.5 according to F-measure in the un-
filtered lexicons and 59.2 in filtered ones.6
4.3.2 Qualitative SCF Comparison
Our qualitative analysis of SCFs in the two corpora
revealed reasons for the differences. Table 4 lists the
10 most frequent SCFs in CHILD (starting from the
highest ranked SCF), along with their ranks in CDS
and VALEX. The top 3 SCFs (NP, INTRANSITIVE and
PP frames) are ranked quite similarly in all the cor-
pora. Looking at the top 10 SCFs, CHILD appears,
as expected, more similar to CDS than with VALEX,
but large differences can be detected in lower ranked
frames.
To identify those frames, we calculated for each SCF
its difference in rank between CHILD vs. CDS. Table 5
exemplifies some of the SCFs with the largest rank
differences. Many of these concern frames involving
sentential complementation. Children use more fre-
6The fact that the unfiltered lexicons appear so much more sim-
ilar suggests that some of the similarity is due to similarity in in-
correct SCFs (many of which are low in frequency, i.e. fall under
the threshold).
quently than adults SCFs involving THAT and HOW
complementation, while adults have a preference for
SCFs involving WHETHER, ING and IF complementa-
tion.
Although we have not yet looked at SCF differences
across ages, these discoveries are in line with previous
findings, e.g. (Brown, 1973), which indicate that chil-
dren master the sentential complementation SCFs pre-
ferred by adults (in our experiment) fairly late in the
acquisition process. With a mean utterance length for
CHILD at 3.48, we would expect to see relatively few of
these frames in the CHILD corpus?and consequently
a preference for the simpler THAT constructions.
4.4 The Impact of Verb Type Preferences on SCF
Differences
Given the new research findings reported in Sec-
tion 4.2 (i.e. the discovery that children and adults have
different preferences for many medium-low frequency
verbs) we investigated whether verb type preferences
play a role in SCF differences between the two corpora.
We chose for experimentation 10 verbs from 3 groups:
1. Group 1 ? verbs with similar ranks in CHILD and CDS: bring,
find, give, know, need, put, see, show, tell, want
2. Group 2 ? verbs with higher ranks in CDS: ask, feel, guess,
help, learn, like, pull, remember, start, think
3. Group 3 ? verbs with higher ranks in CHILD: break, die,
forget, hate, hit, jump, scare, shoot, burn, wish
The test verbs were selected randomly, subject to
the constraint that their absolute frequencies in the two
corpora were similar.7 We first correlated the unfil-
tered SCF distributions of each test verb in the two cor-
pora against each other and calculated the similarity in
the SCF types using the F-measure. We then evaluated
for each group, the accuracy of SCFs in unfiltered dis-
tributions against our gold standard (see Section 4.1).
Because the gold standard was too ambitious in terms
of recall, we only calculated the precision figures: the
average number of TP and FP SCFs taken by test verbs.
The results are included in Table 6. Verbs in Group
1 show the best SCF type correlation (84.7 F-measure)
between the two corpora although they are the rich-
est in terms of subcategorization (they take the highest
number of SCFs out of the three groups). The SCF cor-
relation is clearly lower in Groups 2 and 3, although
7This requirement was necessary because frequency may influ-
ence subcategorization acquisition performance.
37
SCF Example sentence CHILD CDS VALEX
NP I love rabbits 1 1 1
INTRANS I sleep with a pillow and blanket 2 2 2
PP He can jump over the fence 3 4 3
PART I can?t give up 4 7 9
TO-INF-SC I want to play with something else 5 3 6
PART-NP/NP-PART He looked it up 6 6 7
NP-NP Ask her all these questions 7 5 18
NP-INF-OC Why don?t you help her put the blocks in the can ? 8 9 60
INTR-RECIP So the kitten and the dog won?t fight 9 8 48
NP-PP He put his breakfast in the bin 10 10 4
Table 4: 10 most frequent SCFs in CHILD, along with their ranks in CDS and VALEX.
SCF Example sentence
CHILD MP I win twelve hundred dollars
INF-AC You can help me wash the dishes
PP-HOW-S He explained to her how she did it
HOW-TO-INF Daddy can you tell me how to spell Christmas carols?
NP-S He did not tell me that it was gonna cost me five dollars
CDS ING-PP Stop throwing a tantrum
NP-AS-NP I sent him as a messenger
NP-WH-S I?ll tell you whether you can take it off
IT WHS, SUBTYPE IF How would you like it if she pulled your hair?
NP-PP-PP He turned it from a disaster into a victory
Table 5: Typical SCFs with higher ranks in (i) CHILD and (ii) CDS.
Measures Group1 Group2 Group3
SCF similarity F-measure 84.7 72.17 75.60
SCF accuracy TPs CDS 12 11 7
TPs CHILD 10 9 8
FPs CDS 36 29 13
FPs CHILD 32 18 15
Table 6: Average results for 3 groups when (i) unfil-
tered SCF distributions in CHILD and CDS are com-
pared against each other (SCF similarity) and when (ii)
the SCFs in the distributions are evaluated against a
gold standard (SCF accuracy).
the verbs in these groups take fewer SCFs. Interest-
ingly, Group 3 is the only group where children pro-
duce more TPs and FPs on average than adults do, i.e.
both correct and incorrect SCFs which are not exem-
plified in the adult speech. The frequency effects con-
trolled, the reason for these differences is likely to lie
in the differing relative preferences children and adults
have for verbs in groups 2 and 3, which we think may
impact the richness of their language.
4.5 Further Analysis of TP and FP Differences
We looked further at the interesting TP and FP differ-
ences in Group 3 to investigate whether they tell us
something about (i) how children learn SCFs (via both
TPs and FPs), and (ii) how the parsing / SCF extraction
system could be improved for CHILDES data in the fu-
ture (via the FPs).
We first made a quantitative analysis of the rela-
tive difference in TPs and FPs for all the SCFs in both
corpora. The major finding of this high level anal-
ysis was a significantly high FP rate for some ING
frames (e.g. PART-ING-SC, ING-NP-OMIT, NP-ING-
OC) within CHILD (e.g. ?car going hit?, ?I hurt hand
moving?). This agrees with many previous studies,
e.g. (Brown, 1973), which have shown that children
overextend and incorrectly use the ?ing? morpheme
during early acquisition.
A qualitative analysis of the verbs from Group 3 was
then carried out, looking for the following scenarios:
? SCF is a FP in both CHILD and CDS - either i) the
gold standard is incomplete, or ii) there is error in
the parser/subcategorization system with respect to the
CHILDES domain.
? SCF is a TP in CDS and not present in CHILD - children have
not acquired the frame despite exposure to it (perhaps it is
complicated to acquire).
? SCF is a TP in CHILD but not present in CDS - adults are
not using the frame but the children have acquired it. This
indicates that either i) children are acquiring the frame from
elsewhere in their environment (perhaps from a television),
38
NP-INF NP-NP
INTRANS 
ADJP 
PART 
NP 
PP 
PART-NP 
PART-NP-PP PART-PP
PP-PP PP-BASE 
NP-S NP-PP NP-ADJP 
NP-NP-up 
Figure 1: SCFs obtained for the verb shoot
or ii) there is a misuse of the verb?s semantic class in child
speech.
? SCF is a FP in CHILD but not present in CDS - children should
not have been exposed to this frame but they have acquired
it. This indicates either i) a misuse of the verb?s semantic
class, or ii) error in the parsing/subcategorization technology
with respect to the child-speech domain.
These scenarios are illustrated in Figure 1 which
graphically depicts the differences in TPs and FPs for
the verb shoot. The SCFs have been arranged in a
complexity hierarchy where complexity is defined in
terms of increasing argument structure.8 SCFs found
within our ANLT-COMLEX gold standard lexicon for
shoot are indicated in bold-face. A right-angled rect-
angle drawn around a SCF indicates that the frame
is present in CHILD?a solid line indicating a strong
presence (relative frequency > 0.010) and a dotted
line indicating a weak presence (relative frequency >
0.005). Rounded-edge rectangles represent the pres-
ence of SCFs within CDS similarly. For example, the
frame NP represents a TP in both CHILD and CDS and
the frame NP-NP represents a FP within CHILD.
With reference to Figure 1, we notice that all of
the SCFs present in CHILD are directly connected
within the hierarchy and there is a tendency for weakly
present SCFs to inherit from those strongly present. A
possible explanation for this is that children are ex-
ploring SCFs?trying out frames that are slightly more
complex than those already acquired (for a learning
8For instance, the intransitive frame INTRANS is less complex
than the transitive frame NP, which in turn is less complex than the
di-transitive frame NP-NP. For a detailed description of all SCFs
see (Korhonen, 2002).
algorithm that exploits such a hypothesis in general
see (Buttery, 2006)).
The SCF NP-NP is strongly present in CHILD de-
spite being a FP. Inspection of the associated utter-
ances reveals that some instances NP-NP are legitimate
but so uncommon in adult language that they are omit-
ted from the gold-standard (e.g. ?can i shoot us all to
pieces?. However, other instances demonstrate a mis-
understanding of the semantic class of the verb; there
is possible confusion with the semantic class of send
or throw (e.g. ?i shoot him home?).
The frame NP-INF is a FP in both corpora and a fre-
quent FP in CHILD. Inspection of the associated utter-
ances flags up a parsing problem. Frame NP-INF can
be illustrated by the sentences ?he helped her bake the
cake? or ?he made her sing?, however, within CHILD
the NP-INF has been acquired from utterances such
as ?i want ta shoot him?. The RASP parser has mis-
tagged the word ?ta? leading to a misclassification
by the SCF extraction system. This problem could be
solved by augmenting RASP?s current grammar with a
lexical entry specifying ?ta? as an alternative to infini-
tival ?to?.
In summary, our analysis of TP and FP differ-
ences has confirmed previous studies regarding the
nature of child speech (the over-extension of the
?ing? morpheme). It has also demonstrated that
TP/FP analysis can be a useful diagnostic for pars-
ing/subcategorization extraction problems within a
new data domain. Further, we suggest that analysis
of FPs can provide empirical data regarding the man-
ner in which children learn the semantic classes of
39
verbs (a matter that has been much debated e.g. (Levin,
1993), (Brooks and Tomasello, 1999)).
5 Conclusion
We have reported the first experiment for automatically
acquiring verbal subcategorization from both child and
child-directed parts of the CHILDES database. Our re-
sults show that a state-of-the-art subcategorization ac-
quisition system yields useful results on challenging
child language data even without any domain-specific
tuning. It produces data which is accurate enough
to confirm and extend several previous research find-
ings in CLA. We explore the discovery that children
and adults have different relative preferences for cer-
tain verb types, and that these preferences influence
the way children acquire subcategorization. Our work
demonstrates the value of using NLP technology to an-
notate child language data, particularly where manual
annotations are not readily available for research use.
Our pilot study yielded useful information which will
help us further improve both parsing and lexical ac-
quisition performance on spoken/child language data.
In the future, we plan to optimize the technology so
that it can produce higher quality data for investiga-
tion of syntactic complexity in this domain. Using the
improved technology we plan to then conduct a more
thorough investigation of the interesting CLA topics
discovered in this study?first concentrating on SCF
differences in child speech across age ranges.
References
B. Boguraev, J. Carroll, E. J. Briscoe, D. Carter, and C. Grover.
1987. The derivation of a grammatically-indexed lexicon from
the Longman Dictionary of Contemporary English. In Proc. of
the 25th Annual Meeting of ACL, pages 193?200, Stanford, CA.
E Briscoe and J Carroll. 1997. Automatic extraction of subcatego-
rization from corpora. In 5th ACL Conference on Applied Nat-
ural Language Processing, pages 356?363, Washington, DC.
ACL.
E. J. Briscoe, J. Carroll, and R. Watson. 2006. The second re-
lease of the rasp system. In Proc. of the COLING/ACL 2006
Interactive Presentation Sessions, Sydney, Australia.
P Brooks and M Tomasello. 1999. Young children learn to pro-
duce passives with nonce verbs. Developmental Psychology,
35:29?44.
R Brown. 1973. A first Language: the early stages. Harvard
University Press, Cambridge, MA.
L. Burnard, 1995. The BNC Users Reference Guide. British Na-
tional Corpus Consortium, Oxford, May.
P. Buttery and A. Korhonen. 2005. Large-scale analysis of verb
subcategorization differences between child directed speech
and adult speech. In Proceedings of the Interdisciplinary Work-
shop on the Identification and Representation of Verb Features
and Verb Classes, Saarbrucken, Germany.
P Buttery. 2006. Computational Models for First Language Ac-
quisition. Ph.D. thesis, University of Cambridge.
E. Charniak. 2000. A maximum-entropy-inspired parser. In Pro-
ceedings of the 1st Meeting of the North American Chapter of
the Association for Computational Linguistics, Seattle, WA.
C. Fisher, G. Hall, S. Rakowitz, and L. Gleitman. 1994. When
it is better to receive than to give: syntactic and conceptual
constraints on vocabulary growth. Lingua, 92(1?4):333?375,
April.
R. Grishman, C. Macleod, and A. Meyers. 1994. COMLEX Syn-
tax: Building a Computational Lexicon. In Proc. of COLING,
Kyoto.
A. Korhonen and Y. Krymolowski. 2002. On the Robustness of
Entropy-Based Similarity Measures in Evaluation of Subcate-
gorization Acquisition Systems. In Proc. of the 6th CoNLL,
pages 91?97, Taipei, Taiwan.
A. Korhonen, Y. Krymolowski, and E. J. Briscoe. 2006. A large
subcategorization lexicon for natural language processing ap-
plications. In Proc. of the 5th LREC, Genova, Italy.
A Korhonen. 2002. Subcategorization Acquisition. Ph.D. thesis,
University of Cambridge. Thesis published as Technical Report
UCAM-CL-TR-530.
E Lenneberg. 1967. Biological Foundations of Language. Wiley
Press, New York, NY.
B Levin. 1993. English Verb Classes and Alternations. Chicago
University Press, Chicago, IL.
B. MacWhinney. 2000. The CHILDES Project: Tools for Analyz-
ing Talk. Lawrence Erlbaum, Mahwah, NJ, 3rd edition.
D. McCarthy and J. Carroll. 2003. Disambiguating nouns, verbs,
and adjectives using automatically acquired selectional prefer-
ences. Computational Linguistics, 29(4).
L Naigles. 1990. Children use syntax to learn verb meanings.
Journal of Child Language, 17:357?374.
S Pinker. 1994. The Language Instinct: How the Mind Creates
Language. Harper Collins, New York, NY.
J. Preiss, E. J. Briscoe, and A. Korhonen. 2007. A system for
large-scale acquisition of verbal, nominal and adjectival sub-
categorization frames from corpora. In Proceedings of the 45th
Annual Meeting of ACL, Prague, Czech Republic. To appear.
K. Sagae, A. Lavie, and B. MacWhinney. 2005. Automatic mea-
surement of syntactic development in child langugage. In Pro-
ceedings of the 42nd Meeting of the Association for Computa-
tional Linguistics, Ann Arbor, Michigan.
S. Schulte im Walde. 2006. Experiments on the automatic induc-
tion of german semantic verb classes. Computational Linguis-
tics, 32(2):159?194.
40
Proceedings of the Workshop on BioNLP: Shared Task, pages 37?40,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Biomedical Event Extraction without Training Data
Andreas Vlachos, Paula Buttery, Diarmuid O? Se?aghdha, Ted Briscoe
Computer Laboratory
University of Cambridge
Cambridge, UK
av308,pjb48,do242,ejb@cl.cam.ac.uk
Abstract
We describe our system for the BioNLP 2009
event detection task. It is designed to be as
domain-independent and unsupervised as pos-
sible. Nevertheless, the precisions achieved
for single theme event classes range from 75%
to 92%, while maintaining reasonable recall.
The overall F-scores achieved were 36.44%
and 30.80% on the development and the test
sets respectively.
1 Introduction
In this paper we describe the system built for the
BioNLP 2009 event detection and characterization
task (Task 1). The approach is based on the output
of a syntactic parser and standard linguistic process-
ing, augmented by rules acquired from the develop-
ment data. The key idea is that a trigger connected
with an appropriate argument along a path through
the syntactic dependency graph forms an event.
The goal we set for our approach was to avoid
using training data explicitly annotated for the task
and to preserve domain independence. While we
acknowledge the utility of supervision (in the form
of annotated data) and domain knowledge, we be-
lieve it is valuable to explore an unsupervised ap-
proach. Firstly, manually annotated data is ex-
pensive to create and the annotation process itself
is difficult and unavoidably results in inconsisten-
cies, even in well-explored tasks such as named en-
tity recognition (NER). Secondly, unsupervised ap-
proaches, even if they fail to reach the performance
of supervised ones, are likely to be informative in
identifying useful features for the latter. Thirdly, ex-
ploring the potential of such a system may highlight
what domain knowledge is useful and its potential
contribution to performance. Finally, preserving do-
main independence allows us to develop and evalu-
ate a system that could be used for similar tasks with
minimal adaptation.
The overall architecture of the system is as fol-
lows. Initiallly, event triggers are identified and la-
belled with event types using seed terms. Based on
the dependency output of the parser the triggers are
connected with candidate arguments using patterns
identified in the development data. Anaphoric can-
didate arguments are then resolved. Finally, the trig-
gers connected with appropriate arguments are post-
processed to generate the final set of events. Each
of these stages are described in detail in subsequent
sections, followed by experiments and discussion.
2 Trigger identification
We perform trigger identification using the assump-
tion that events are triggered in text either by verbal
or nominal prdicates (Cohen et al, 2008).
To build a dictionary of verbs and their associ-
ated event classes we use the triggers annotated in
the training data. We lemmatize and stem the trig-
gers with the morphology component of the RASP
toolkit (Briscoe et al, 2006)1 and the Porter stem-
mer2 respectively. We sort the trigger stem - event
class pairs found according to their frequency in
the training data and we keep only those pairs that
appear at least 10 times. The trigger stems are
then mapped to verbs. This excludes some rela-
tively common triggers, which will reduce recall,
but, given that we rely exclusively on the parser for
1http://www.cogs.susx.ac.uk/lab/nlp/rasp/
2http://www.tartarus.org/?martin/PorterStemmer
37
argument extraction, such triggers would be difficult
to handle. For verbs with more than one event class
we keep only the most frequent one.
We consider the assumption that each verb de-
notes a single event class to be a reasonable one
given the restricted task domain. It hinders us from
dealing with triggers denoting multiple event classes
but it simplifies the task so that we do not need anno-
tated data. While we use the training data triggers to
obtain the list of verbs and their corresponding event
types, we believe that such lists could be obtained by
clustering (Korhonen et al, 2008) with editing and
labelling by domain experts. This is the only use of
the training data we make in our system.
During testing, using the tokenized text provided,
we attempt to match each token with one of the
verbs associated with an event type. We perform
this by relaxing the matching successively, using the
token lemma, then stem, and finally allowing a par-
tial match in order to deal with particles (so that e.g.
co-transfect matches transfect). This process returns
single-token candidate triggers which, while they do
not reproduce the trigger annotation, are likely to be
adequate for event extraction. We overgenerate trig-
gers, since not all occurrences denote an event, ei-
ther because they are not connected with appropriate
arguments or because they are found in a non-event
denoting context, but we expect to filter these at the
argument extraction stage.
3 Argument extraction
Given a set of candidate triggers, we attempt to con-
nect them with appropriate arguments using the de-
pendency graph provided by a parser. In our ex-
periments we use the domain-independent unlexi-
calized RASP parser, which generates parses over
the part-of-speech (PoS) tags of the tokens generated
by an HMM-based tagger trained on balanced En-
glish text. While we expect that a parser adapted to
the biomedical domain may perform better, we want
to preserve the domain-independence of the system
and explore its potential.
The only adjustment we make is to change the
PoS tags of tokens that are part of a protein name
to proper names tags. We consider such an adjust-
ment domain-independent given that NER is avail-
able in many domains (Lewin, 2007). Following
Haghighi et al(2005), in order to ameliorate pars-
ing errors, we use the top-10 parses and return a
set of bilexical head-dependent grammatical rela-
tions (GRs) weighted according to the proportion
and probability of the top parses supporting that GR.
The GRs produced by the parser define directed
graphs between tokens in the sentence, and a partial
event is formed when a path that connects a trigger
with an appropriate argument is identified. GR paths
that are likely to generate events are selected using
the development data, which does not contradict the
goals of our approach because we do not require an-
notated training data. Development data is always
needed in order to build and test a system, and such
supervision could be provided by a human expert,
albeit not as easily as for the list of trigger verbs.
The set of GR paths identified follow:
VERB-TRIGGER ?subject? ARG
NOUN-TRIGGER ?iobj? PREP ?dobj? ARG
NOUN-TRIGGER ?modifier? ARG
TRIGGER ?modifier? PREP ?obj? ARG
TRIGGER ?passive subject? ARG
The final system uses three sets of GR paths:
one for Regulation events; one for Binding events;
and one for all other events. The difference be-
tween these sets is in the lexicalization of the link-
ing prepositions. For example, in Binding events
the linking preposition required lexicalization since
binds x to/with y denotes a correct event but not
binds x by y. Binding events also required additional
GR paths to capture constructions such as binding of
x to y. For Regulation events, the path set was fur-
ther augmented to differentiate between theme and
cause. When the lexicalized GR pattern sets yielded
no events we backed-off to the unlexicalized pattern
set, which is identical for all event types. In all GR
path sets, the trigger was unlexicalized and only re-
stricted by PoS tag.
4 Anaphora resolution
The events and arguments identified in the parsed
abstracts are post-processed in context to iden-
tify protein referents for event arguments that are
anaphoric (e.g., these proteins, its phosphorylation)
or too complex to be extracted directly from the
grammatical relations (phosphorylation of cellular
proteins , notably phospholipase C gamma 1). The
38
anaphoric linking is performed by a set of heuris-
tic rules manually designed to capture a number of
common cases observed in the development dataset.
A further phenomenon dealt with by rules is coref-
erence between events, for example in The expres-
sion of LAL-mRNA is induced. This induction is de-
pendent on. . . where the Induction event described
by the first sentence is the same as the theme of the
Regulation event in the second and should be given
the same event index. The development of the post-
processing rules favoured precision over recall, but
the low frequency of each case considered means
that some overfitting to the development data may
have been unavoidable.
5 Event post-processing
At the event post-processing stage, we form com-
plete events considering the trigger-argument pairs
produced at the argument extraction stage whose ar-
guments are resolved (possibly using anaphora res-
olution) either to a protein name or to a candidate
trigger. The latter are considered only for regula-
tion event triggers. Furthermore, regulation event
trigger-argument pairs are tagged either as theme or
cause at the argument extraction stage.
For each non-regulation trigger-argument pair, we
generate a single event with the argument marked as
theme. Given that we are dealing only with Task
1, this approach is expected to deal adequately with
all event types except Binding, which can have mul-
tiple themes. Regulation events are formed in the
following way. Given that the cause argument is
optional, we generate regulation events for trigger-
argument pairs whose argument is a protein name or
a trigger that has a formed event. Since regulation
events can have other regulation events as themes,
we repeat this process until no more events can be
formed. Occasionally, the use of multiple parses re-
sults in cycles between regulation triggers which are
resolved using the weighted GR scores. Then, we at-
tach any cause arguments that share the same trigger
with a formed regulation event.
In the analysis performed for trigger identification
in Section 2, we observed that certain verbs were
consistently annotated with two events (namely
overexpress and transfect), a non-regulation event
and a regulation event with the former event as its
theme. For candidate triggers that were recognized
due to such verbs, we treat them as non-regulation
events until the post-processing stage where we gen-
erate two events.
6 Experiments - Discussion
We expected that our approach would achieve high
precision but relatively low recall. The evaluation
of our final submissions on the development and test
data (Table 1) confirmed this to a large extent. For
the non-regulation event classes excluding Binding,
the precisions achieved range from 75% to 92% in
both development and test data, with the exception
of Transcription in the test data. Our approach ex-
tracts Binding events with a single theme, more suit-
ably evaluated by the Event Decomposition evalua-
tion mode in which a similar high precision/low re-
call trend is observed, albeit with lower scores.
Of particular interest are the event classes for
which a single trigger verb was identified, namely
Transcription, Protein catabolism and Phosphoryla-
tion, which makes it easier to identify the strengths
and weaknesses of our approach. For the Phos-
phorylation class, almost all the triggers that were
annotated in the training data can be captured us-
ing the verb phosporylate and as a result, the per-
formances achieved by our system are 70.59% and
60.63% F-score on the development and test data re-
spectively. The precision was approximately 78% in
both datasets, while recall was lower due to parser
errors and unresolved anaphoric references. For the
Protein catabolism class, degrade was identified as
the only trigger verb, resulting in similar high preci-
sion but relatively lower recall due to the higher lex-
ical variation of the triggers for this class. For the
Transcription class we considered only transcribe
as a trigger verb, but while the performance on the
development data is reasonable (55%), the perfor-
mance on the test data is substantially lower (20%).
Inspecting the event triggers in the training data re-
veals that some very common triggers for this class
either cannot be mapped to a verb (e.g., mrna) or are
commonly used as triggers for other event classes.
A notable case of the latter type is the verb express,
which, while mostly a Gene Expressions trigger, is
also annotated as Transcription more than 100 times
in the training data. Assuming that this is desirable,
39
Development Test
Event Class recall precision fscore recall precision fscore
Localization 45.28 92.31 60.76 25.86 90.00 40.18
Binding 12.50 24.41 16.53 12.68 31.88 18.14
Gene expression 52.25 80.79 63.46 45.57 75.81 56.92
Transcription 42.68 77.78 55.12 12.41 56.67 20.36
Protein catabolism 42.86 81.82 56.25 35.71 83.33 50.00
Phosphorylation 63.83 78.95 70.59 49.63 77.91 60.63
Event Total 39.03 65.97 49.05 33.16 68.15 44.61
Regulation 20.12 50.75 28.81 9.28 36.49 14.79
Positive regulation 16.86 48.83 25.06 11.39 38.49 17.58
Negative regulation 11.22 36.67 17.19 6.86 36.11 11.53
Regulation Total 16.29 47.06 24.21 9.98 37.76 15.79
Total 26.55 58.09 36.44 21.12 56.90 30.80
Binding (decomposed) 26.92 66.14 38.27 18.84 54.35 27.99
Table 1: Performance analysis on development and test data using Approximate Span/Partial Recursive Matching.
a more appropriate solution would need to take con-
text into account.
Our performance on the regulation events is sub-
stantially lower in both recall and precision. This
is expected, as they rely on the extraction of non-
regulation events. The variety of lexical triggers is
not causing the drop in performance though, since
our system performed reasonably well in the Gene
Expression and Localization classes which have
similar lexical variation. Rather it is due to the com-
bination of the lexical variation with the requirement
to make the distinction between the theme and op-
tional cause argument, which cannot be handled ap-
propriately by the small set of GR paths employed.
The contribution of anaphora resolution to our
system is limited as it relies on the argument ex-
traction stage which, apart from introducing noise,
is geared towards maintaining high precision. Over-
all, it contributes 22 additional events on the de-
velopment set, of which 14 out of 16 are correct
non-regulation events. Of the remaining 6 regula-
tion events only 2 were correct. Similar trends were
observed on the test data.
7 Conclusions - Future work
We described an almost unsupervised approach for
the BioNLP09 shared task on biomedical event ex-
traction which requires only a dictionary of verbs
and a set of argument extraction rules. Ignoring trig-
ger spans, the performance of the approach is parser-
dependent and while we used a domain-independent
parser in our experiments we also want to explore
the benefits of using an adapted one.
The main weakness of our approach is the han-
dling of events with multiple arguments and the dis-
tinctions between them, which are difficult to deal
with using simple unlexicalized rules. In our fu-
ture work we intend to explore semi-supervised ap-
proaches that allow us to acquire more complex
rules efficiently.
References
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL Interactive presentation ses-
sions, pages 77?80.
Kevin B. Cohen, Martha Palmer, and Lawrence Hunter.
2008. Nominalization and alternations in biomedical
language. PLoS ONE, 3(9).
Aria Haghighi, Kristina Toutanova, and Chris Manning.
2005. A Joint Model for Semantic Role Labeling. In
Proceedings of CoNLL-2005: Shared Task.
Anna Korhonen, Yuval Krymolowski, and Nigel Collier.
2008. The choice of features for classification of verbs
in biomedical texts. In Proceedings of Coling.
Ian Lewin. 2007. BaseNPs that contain gene names:
domain specificity and genericity. In Proceedings of
the ACL workshop BioNLP: Biological, translational,
and clinical language processing, pages 163?170.
40
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 43?51,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
You talking to me? A predictive model for zero auxiliary constructions
Andrew Caines
Computation, Cognition & Language Group
RCEAL, University of Cambridge, UK
apc38@cam.ac.uk
Paula Buttery
Computation, Cognition & Language Group
RCEAL, University of Cambridge, UK
pjb48@cam.ac.uk
Abstract
As a consequence of the established prac-
tice to prefer training data obtained from
written sources, NLP tools encounter
problems in handling data from the spo-
ken domain. However, accurate models
of spoken data are increasingly in demand
for naturalistic speech generation and ma-
chine translations in speech-like contexts
(such as chat windows and SMS). There
is a widely held assumption in the lin-
guistic field that spoken language is an
impoverished form of written language.
However, we show that spoken data is
not unpredictably irregular and that lan-
guage models can benefit from detailed
consideration of spoken language features.
This paper considers one specific con-
struction which is largely restricted to the
spoken domain - the ZERO AUXILIARY -
and makes a predictive model of that con-
struction for native speakers of British En-
glish. The model can predict zero auxil-
iary occurrence in the BNC with 96.9%
accuracy. We will demonstrate how this
model can be integrated into existing pars-
ing tools, increasing the number of suc-
cessful parses for this zero auxiliary con-
struction by around 30%, and thus improv-
ing the performance of NLP applications
which rely on parsing.
1 Introduction
Up to this point, statistical Natural Language Pro-
cessing (NLP) tools have generally been trained
on corpora that are representative of written rather
than spoken language. A major factor behind this
decision to use written data is that it is far easier to
collect than spoken data. Newswire, for instance,
may be harvested readily and in abundance. Once
collected, written language requires relatively lit-
tle processing before it can be used for training a
statistical model.
Processing of spoken data, on the other hand,
involves at the very least transcription - which usu-
ally requires a human transcriber. Since transcrip-
tion is a slow and laborious task, the collection of
spoken data is highly resource intensive. But this
relative difficulty in collection is not the only rea-
son that spoken language data has been sidelined.
Had spoken data been considered to be crucial to
the production of NLP applications greater efforts
might have been made to obtain it. However, on
account of some of its characteristic features such
as hesitations, interruptions and ellipsis, spoken
language is often dismissed as nothing more than
a noisy approximation to ?real? or ?intended? lan-
guage.
In some forums, written language is held up
as an idealised form of language toward which
speakers aspire and onto which spoken lan-
guage should be retrofitted. This is an arte-
fact of the theoretical notion of a ?competence?-
?performance? dichotomy (Chomsky 1965) with
the latter deemed irrelevant and ignored in main-
stream linguistic research.
The consequence of the established practice to
sideline spoken data is that NLP tools are inher-
ently error prone when handling data from the spo-
ken domain. With increasing calls for speech to be
considered the primary form of language and to be
treated as such (Sampson 2001: 7 1; Cerma?k 2009:
115 2; Haugh 2009: 74 3) and a growing trend for
NLP techniques to be integrated into cognitive and
neurolinguistic research as well as forensic appli-
1Speech is ?unquestionably the more natural, basic mode
of language behaviour?.
2?From a linguistic point of view, spoken corpora should
be primary for research but that has not been the case so far?.
3Haugh observes that ?spoken language and interaction
lie at the core of human experience? but bemoans the ?relative
neglect of spoken language in corpora to date?.
43
cations, there are now compelling reasons to ex-
amine spoken data more closely. Accurate mod-
els of spoken data are increasingly in demand for
naturalistic speech generation and machine trans-
lations in speech-like contexts (such as human-
machine dialogue, chat windows and SMS).
The main research aim of our work is to show
that spoken data should not be considered error
prone and therefore unpredictably irregular. We
show that language models can be improved in in-
crements as we deepen our understanding of spo-
ken language features. We investigate ZERO AUX-
ILIARY progressive aspect constructions - those
which do not feature the supposedly obligatory
auxiliary verb, as in (1a) below (cf. 1b):
(1a) What you doing? Who you looking
for? You been working?
(1b) What are you doing? Who are you
looking for? Have you been working?
The zero auxiliary is a non-standard feature
which for the most part is known to be restricted
to speech. A corpus study of spoken British En-
glish indicates that in progressive aspect interroga-
tives with second person subjects (as in (1) above)
the auxiliary occurs in zero form in 27% of con-
structions found. The equivalent figure from the
written section of the corpus is just 5.4%. Con-
sequently, existing NLP techniques - since they
are based on written training data - are unlikely
to deal appropriately with zero auxiliary construc-
tions. We report below on the corpus study in full
and use the results of logistic regression to design
a predictive model of zero auxiliary occurrence
in spoken English. The model is based on con-
textual grammatical features and can predict zero
auxiliary occurrence in the British National Cor-
pus (BNC; 2007) with 96.9% accuracy. Finally,
we discuss how this model can be used to improve
the performance of NLP techniques in the spoken
domain, demonstrating its implementation in the
RASP system (Robust Accurate Statistical Pars-
ing; (Briscoe, Carroll and Watson, 2006)).
This paper underlines why awareness of non-
standard linguistic features matters. Targeted data
extraction from large corpus resources allows the
construction of more informed language models
which have been trained on naturalistic spoken
usage rather than standard and restricted rules of
written language. Such work has only been made
possible with the advent of large spoken language
corpora such as the BNC. Even so, the resource-
heavy nature of spoken data collection means that
speech transcriptions constitute only one tenth of
this 100 million word corpus 4. Nevertheless, it
is an invaluable resource made up of a range of
speech genres including spontaneous face-to-face
conversation, a fact which makes it unique among
corpora. Since conversational dialogue is the pre-
dominant language medium, the BNC offers the
best chance of modelling speech as it occurs natu-
rally.
This work has important implications for both
computational and theoretical linguistics. On the
one hand, we can improve various NLP techniques
with more informed language models, and on the
other hand we are reminded that the space of
grammatical possibility is not restricted and that
continued empirical investigation is key in order
to arrive at the fullest possible description of lan-
guage use.
2 Spoken and written language
In the modern mainstream fields of linguistic
research, based on Chomsky?s ?ideal speaker-
listener? (1965), spoken language has been all
too easily dismissed from consideration on the
grounds that it is more error-prone and less impor-
tant than written language. In this idealisation, the
speaker-listener is ?unaffected by such grammati-
cally irrelevant conditions as memory limitations,
distractions, shifts of attention and interest, and er-
rors (random or characteristic)? (Chomsky, 1965).
The ?errors? Chomsky refers to are features of
speech production such as pauses, filled silence,
hesitation, repetition and elision, or of dialogue
such as backgrounding, overlap and truncation be-
tween speakers. Thus ?error? is essentially here de-
fined as that which is not normally found in well-
formed written data, that which is ?noisy? and ?un-
predictable?. It is on these grounds - the gram-
matical rigidity of the written medium relative to
speech - that the divide between spoken and writ-
ten language modelling has grown up.
The opposing, usage-based view is that spoken
language is systematic and that it should be mod-
elled as it is rather than as a crude approximation
of the written form. On this view, the speech pro-
duction and dialogue features listed above are not
4Cerma?k estimates our experience with each language
medium is in fact this ratio in reverse - 90:10 spoken to writ-
ten (2009: 115).
44
considered mistakes but ?regular products of the
system of spoken English? (Halliday, 1994). ?Er-
ror? is thus seen as a misnomer for these features
because they are in fact all regular in some way.
For example, the fact that people tend to put filler
pauses in specific places.
We propose a middle way: that which builds
on the NLP tools available, even though they are
trained on written data, and on top of these models
the features of spoken language as ?noise? in the
communicative channel. This is a pragmatic ap-
proach which recognises the considerable amount
of work underpinning existing NLP tools and sees
no value in discarding these and starting again
from scratch. We demonstrate that spoken lan-
guage is model-able and predictable, even with
a feature which would not be seen as ?correct?
in written form. For practical purposes we need
to recognise the regularities in the apparently ?in-
correct? features of speech and build these into
the functioning language models we already have
through statistical analysis of corpus distributions
and appropriate adjustment to parser tools.
3 The zero auxiliary construction
According to standard grammatical rules, the aux-
iliary verb is an obligatory feature of progressive
aspect constructions. However, this rule is based
on norms of written language and is in fact not al-
ways adhered to in the production of speech. As a
result, some progressive constructions do not fea-
ture an auxiliary verb. These are termed ?zero aux-
iliary? constructions and have been previously ex-
amined in studies of dialect (Labov, 1969; Ander-
sen, 1995) and first language acquisition (Brown,
1973; Rizzi, 1993/1994; Wexler, 1994; Lieven et
al, 2003; Wilson, 2003; Theakston et al 2005).
There are copious anecdotal examples of the
zero auxiliary:
(2) You talking to me? Travis Bickle in
Taxi Driver (1976).
(3) Where he going? Avon Barksdale
in The Wire, Season 1: ?Game Day?
(2002).
(4) What you doing? Holly Golightly in
Breakfast at Tiffany?s (1961).
Natural language data taken from the spoken
section of the British National Corpus (sBNC)
shows that the zero auxiliary features in 1330
(27%) of the 4923 second person progressive in-
terrogative constructions; as in (1), (2), (4) above.
In first person singular declaratives (cf. (5a) and
(5b)), in contrast, the proportion of zero auxiliary
occurrence is just 0.9% (158 of 17,838 construc-
tions). This already indicates the way that the zero
auxiliary occurs in predictable contexts and how
grammatical properties will feature in the predic-
tive model.
(5a) What I saying? I annoying you?
Why I doing this?
(5b) What am I saying? Am I annoying
you? Why am I doing this?
Subject person, subject number, subject type
(pronoun or other noun) and clause type (declar-
ative or interrogative) are four of the eight syntac-
tic properties incorporated in the predictive model.
The four other properties are clause tense (6), per-
fect or non-perfect aspect (7), clause polarity (8)
and presence or absence of subject (9).
(6) You are debugging. You were de-
bugging.
(7) We have been looking for a present.
We are looking for a present.
(8) She is watching the grand prix. He
is not watching the grand prix.
(9) I am going to town in a minute. Go-
ing to town in a minute.
We employ logistic regression to investigate the
precise nature of the relationships between zero
auxiliary use and these various linguistic vari-
ables. This allows us to build a predictive model
of zero auxiliary occurrence in spoken language
which will be useful for several reasons relating
to parsing of natural spoken language. Firstly,
for automatic parsing of spoken data being able
to predict when a zero auxiliary is likely to oc-
cur enables the parser to relax its normal rules
which are based on written standards. Secondly,
as technology improves and interaction with com-
puters becomes more humanistic the need to repli-
cate human-like communication increases in im-
portance: by knowing in which contexts the aux-
iliary verb might be absent, researchers can build
a language model which is more realistic and so
the user experience is improved and made more
naturalistic. Thirdly, a missing auxiliary might be
problematic for machine translation since it could
45
result in the loss of tense and aspect information,
but with the ability to predict where a zero auxil-
iary might occur, the auxiliary can be restored so
that translation can be performed with appropriate
tense and aspect.
For all these reasons, the zero auxiliary in spo-
ken English is an appropriate case study for find-
ing the common ground between NLP and Lin-
guistics. Awareness of this particular linguis-
tic phenomenon through corpus study allows the
construction of more informed language models
which in turn enhance relevant NLP techniques.
The cross-pollination of research from NLP and
linguistics benefits both fields and ties in with the
emergence of linguistic theories that ?conceive of
structure as gradient, malleable and probabilistic?
and incorporate ?knowledge of the frequency and
probability of use of these categories in speakers?
experience? (Tily et al 2009). These are collec-
tively known as ?usage-based? approaches to lan-
guage theory and are exerting a growing influ-
ence on the field (e.g. Barlow and Kemmer 2000;
Bybee and Hopper 2001; Bod, Hay and Jannedy
2003).
4 Corpus study
Training data was obtained through manual anno-
tation of progressive constructions in the British
National Corpus (2007). A preliminary study of
interrogatives with second person subjects con-
firmed that the zero auxiliary is more a feature
of the spoken rather than the written domain (Ta-
ble 1). Therefore a focus on the spoken section of
the corpus (sBNC) was justified and so we under-
took a comprehensive study of all progressive con-
structions in sBNC. The genres contained in sBNC
include a range of settings and levels of formality
- from academic lectures to radio programmes to
spontaneous, face-to-face conversation.
We extracted 93,253 sentences featuring a pro-
gressive construction from sBNC and each was
manually annotated for auxiliary realisation and
the eight syntactic properties described in Ta-
ble 2. In Table 3 the progressive constructions are
classified by auxiliary realisation. With approxi-
mately 4.2% occurrence in progressive construc-
tions, zero auxiliaries are a low frequency feature
of spoken language but ones which are significant
for the fact that existing NLP tools cannot suc-
cessfully parse them, thus one in twenty-five pro-
gressive constructions will not be fully parsed. We
Corpus Auxiliary
Full Contracted Zero
wBNC 3220 27 187
sBNC 3498 95 1330
Table 1: Auxiliary realisation in second person
progressive interrogatives in the BNC.
use the annotated corpus of these progressive con-
structions to design the predictive model described
below.
Properties Value encodings
Aux realisation
Zero auxiliary full(0), contracted(1), zero(2)
Variables
Subject person 1st(1), 2nd(2), 3rd(3)
Subject number singular(0), plural(1)
Subject type other noun(0), pronoun(1)
Subj supplied zero subj(0), subj supplied(1)
Clause type declarative(0), interrogative(1)
Clause tense present(0), past(1)
Perfect aspect non-perfect(0), perfect(1)
Polarity positive(0), negative(1)
Table 2: Syntactic features and their encodings in
the annotated sBNC Progressive Corpus
5 Model
To predict the zero auxiliary in spoken language
we use logistic regression. To train this model
we took a 90% sample from our corpus of 93,253
progressive constructions extracted from the spo-
ken section of the BNC, as described above and in
Caines 2010. The dataset was split into two cat-
egories: those sentences which exhibited the zero
auxiliary and those which did not5. A logistic re-
gression was then performed to ascertain the prob-
ability of category membership using the eight
previously described syntactic properties. Note
that subject person is arguably not a scalar vari-
5Contracted auxiliaries thus belong in the ?not zero auxil-
iary? category.
Corpus Full Contracted Zero
sBNC 38,015 51,295 3943
Table 3: Auxiliary realisation in progressive con-
structions in sBNC.
46
able and therefore is re-analysed as three boolean
variables with separate binary values for use of the
first, second and third person. However, the three
subject person variables are dependent (ie. If the
subject is not first or second person it will be in the
third). Thus the eight syntactic properties become
nine explanatory variables in the predictive model,
as reported in Table 4.
Corpus Predictor Coefficient
subject person: 1st 0.171
subject person: 2nd 1.280
plural subject -0.300
pronoun subject -0.470
zero subject 5.711
interrogative clause 2.139
past tense clause -4.852
perfect aspect -0.280
negated clause -1.163
constant -4.033
Table 4: Predictor coefficients for the presence of
a zero auxiliary construction.
5.1 Model Evaluation
The logistic function is defined by:
f(Z) =
1
1 + e?z
(1)
The variable z is representative of the set of pre-
dictors and is defined by:
z = ?0 + ?1x1 + ?2x2 + . . .+ ?kxk (2)
where ?0, ?1, ?2 ... ?k are the regression coeffi-
cients of predictors x1, x2 .. xk respectively. The
predictors explored in this paper are encodings of
the syntactic properties of the annotated sentences.
The predictors and their encodings are indicated in
Table 2.
The logistic function is constrained to values
between 0 and 1 and represents the probability
of membership of one of the two categories (zero
auxiliary or auxiliary supplied). In our case an
f(z) > 0.5 indicates that there is likely to be a
zero auxiliary.
The logistic function defined by the coefficients
in Table 4 is able to predict correct category mem-
bership for 96.9% of the sentences in the annotated
corpus. All coefficients are highly significant to
the logistic function (p<0.001) with the exception
of perfect aspect and first person subject - which
are both significant nevertheless (p<0.05).
For this model, positive coefficients indicate
that the associated syntactic properties raise the
probability of a zero auxiliary occurring. Large
coefficients more strongly influence the probabil-
ity of the zero auxiliary whereas near-zero coeffi-
cients have little influence. From the coefficients
in Table 4 we see that the strongest predictor of
a zero auxiliary is the occurrence of a zero sub-
ject (as in the utterance, ?leaving now.?). An inter-
rogative utterance is also a good candidate, as is
the second person subject (e.g. ?you eating those
olives??). However, a past tense utterance is an un-
likely candidate for a zero auxiliary construction,
as is a negated utterance.
6 Discussion ? using the predictive
model to aid parsing
As mentioned above, since parsers are trained on
written data they can often display poor perfor-
mance on text transcribed from the spoken do-
main. From the results of our corpus study we
know that the zero auxiliary occurs in approxi-
mately 4.2% of progressive constructions in spo-
ken language and we can extrapolate that it will
occur in less than 1% (approximately 0.8%) of
all progressive constructions in written language.
A statistical parser trained on written language
will therefore be prone to undergo parsing fail-
ure for every one in twenty-five progressive sen-
tences. This is no insignificant problem, espe-
cially when it is remarked that the progressive is
in high frequency usage (there are one thousand
ING-forms featuring in progressive constructions
for every one million words of sBNC) and that its
use is known to be spreading (Leech et al 2009).
Compounded with those parser breakdowns
caused by other speech phenomena (for instance,
repetition and elision), high numbers of parse fail-
ures on progressive constructions will render NLP
accuracy on spoken language intolerable for any
applications which rely on accurate parsing as a
foundation. However, we have shown above that
features of spoken language such as the zero aux-
iliary should not be thought of as errors or as un-
predictable deviations from the written form, but
rather can be considered to be consistent and pre-
dictable events. In this section we illustrate how
our predictive model for zero auxiliary occurrence
47
(|relation| |head| |dependant|) (3)
(|ncsubj| |play + ing : V V G| |you : PPY |)(4)
(|obj| |play + ing : V V G| |what : DDQ|) (5)
(|aux| |play + ing : V V G| |be+ : V BR|) (6)
(|arg| |play + ing : V V G| |you : PPY |) (7)
(|relation| |verb : V V G| |dependant|) (8)
Figure 1: Example grammatical relations from
RASP.
may be integrated into a parser pipeline in order
to aid the parsing of spoken language. In this way
we build on the increasingly robust engineering of
statistical NLP tools trained on written language
by allowing them to adapt to the spoken domain
on the basis of the linguistic study of speech phe-
nomena.
In general the notion of ?parsing? an utterance
involves a chain of several processes: utterance
boundary detection, tokenization, part-of-speech
tagging, and then parsing. We suggest that when it
is known that the language to be parsed is from the
spoken domain the pipeline of processes should be
run in a SPEECH AWARE MODE. Extra functional-
ity would be incorporated into each of the stages
according to the findings of linguistic research into
spoken language. In other work we have adapted
the tokenization and tagging stages of the pipeline
based on predictors that indicate when interjec-
tions (e.g. ?umm?, ?err? and ?ah?) have been ?used?
as punctuation or lexical items. We also incorpo-
rate intonation phrases as predictors for utterance
boundary detection (Buttery and Caines: in prepa-
ration). Here, we augment the parsing stage of the
pipeline by allowing an informed re-parse of ut-
terances in which a parse failure is likely to have
been caused by a zero auxiliary.
We present this section with reference to the
specific mechanics and output formats of the
RASP system but our algorithm is by no means
parser specific and could be adapted for other
parsers quite easily. Utterances parsed with
RASP may be expressed as ?grammatical rela-
tions?. RASP?s grammatical relations are theory-
general, binary relations between lexical terms and
are expressed in the form of head-dependancy re-
lations as shown in (3), Figure 1.
Consider the utterance ?what are you playing??.
When we parse this with RASP we get grammati-
cal relations (4), (5) and (6) in Figure 1. The capi-
tal letter codes following the ?:? symbols are part-
of-speech tags (from the CLAWS-2 tagset (Gar-
side, 1987)) which have been assigned to the lexi-
cal tokens by the tagger of the RASP system. Here
PPY indicates the pronoun ?you?; VVG indicates
the ING-form of lexical verb; VBR indicates ?be?
in 3rd person present tense; and DDQ indicates
a wh-determiner. The relation (4) tells us that
?you? is the subject of ?playing?; relation (5) tells
us that ?what? is taking the place of the object be-
ing played; and relation (6) tells us that there is an
auxiliary relationship between ?are? and ?playing?.
This is much as we would expect. However, if we
try to parse ?what you playing?? the parse fails.
The single relation (4) is returned where ideally
we would like both (4) and (5), as we did when
the auxiliary was present.
For the utterance, ?you playing?? RASP returns
the under-specified grammatical relation (7) which
is simply indicating that ?you? is an argument of
?playing? but not which type of argument (whether
a subject, direct object, etc). Ideally we would
like to retrieve at least (4) as we would have if we
parsed the utterance ?are you playing??. For these
examples, we shall consider the failure to identify
the correct subject and object of the progressive
verb to be a parsing failure.
We integrate the zero auxiliary predictive model
with parsing technology to improve the parsing of
zero auxiliaries in spoken language. Note that we
use the RASP system but our algorithm is by no
means parser specific. The only prerequisite is
that the parser must be able to identify relations
of some kind between the subject noun and ING-
form (possibly via a parsing rule) and also be able
extract values for the predictors (through either a
rich tagset or from the identification of key speech
tokens). The illustrative method we discuss here
is integrated into the parsing pipeline in the event
of a parse failure but there are several alternative
methods that might also be considered.
For instance, by using the predictive model
earlier in the parsing system pipeline a modified
tagset could be used which updates the ING-form
tag with a new tag to indicate that there is also a
missing auxiliary. Another method might involve
altering rule probabilities or adding extra parser
rules so that parsing only has to occur once. Our
other work in this area suggests that the final deci-
48
sion on where to add the spoken language modifi-
cations within the parsing pipeline will largely de-
pend on the interaction of the phenomena in ques-
tion with other speech phenomena.6
With the proviso that it is a preliminary integra-
tion of the predictive model into a parsing system,
we propose the following algorithm for zero aux-
iliaries in spoken language. When ?speech aware
mode? is activated, if we encounter a parse failure
then we first check the part-of-speech tags of the
utterance to ascertain if the sentence contains the
ING-form requisite for a progressive construction:
? IF no ING-form is found: STOP. Our
model predicts zero auxiliaries in progressive
constructions?there is nothing more we can
do with the input.
? ELSE: An ING-form is found. Extract all
grammatical relations that were obtained by
the parse which contained the ING-form in
the head position (these would be grammat-
ical relations that have the general format
of (8) in Figure 1). We will refer to this set of
grammatical relations as GRS.
? IF there is an auxiliary relation
present in GRS: STOP. If at least one
of the extracted grammatical relations is
an auxiliary relation, similar to (6) in
Figure 1, an auxiliary is present?we do
not have a zero auxiliary construction.7
? ELSE: The utterance is a candidate for
zero auxiliary.
Having determined a possible candidate for zero
auxiliary we carry out the following steps:
1. Ascertain values for the zero auxiliary pre-
dictors (explained in more detail below).
2. Calculate the value of the logistic function
f(z) using the obtained predictor values with
their coefficients (shown in Table 4).
3. If f(z) > 0.5, assume an auxiliary is miss-
ing.
6Although, another major consideration is the overall
computational efficiency of the parsing system.
7This step is actually subtly more complicated?auxiliary
relations involving ?been? are allowed to be present in GRS
(this allows us to capture zero auxiliaries in the perfect such
as ?been coming here long??) but if there is any other auxil-
iary relation present in GRS then we STOP here.
4. Add the auxiliary to the sentence (choos-
ing which auxiliary based on the predictor
values?see below).
5. Re-parse the sentence.
6. Remove (or flag) the auxiliary grammatical
relation from the newly obtained parser out-
put.8
For step 1 above properties of the current utter-
ance have to be obtained. The subject person, plu-
ral subject, zero subject and pronoun subject prop-
erties are ascertained by looking at the part-of-
speech of the dependant noun/pronoun within any
subject relations occurring in the set GRS (gram-
matical relations headed by the ING-form). Sub-
ject relations would look similar to (4) in Figure 1.
If there is no subject grammatical relation, any un-
derspecified ?arg? relation (such as (7) in Figure 1)
are considered. If neither of these relations are
present in GRS then a zero subject is inferred.
The person and plurality of the subject noun is en-
coded within its CLAWS2 part-of-speech tag. For
instance, a PPHS1 tag, which is used to indicate
?him? or ?her? would tell us we have a third per-
son, singular pronoun.9
The other properties are all ascertained by the
presence or absence of a token within the utter-
ance: interrogative property is inferred when the
utterance ends with a question mark; the nega-
tion property when either ?not? or ?n?t? (which are
tagged XX) is present; the perfect is inferred from
the presence of the word ?been?; and past tense is
ascertained from a set of temporal marker lexical
items (e.g. ?yesterday, ?before?). Once extracted
the properties are encoded as shown in Table 4 for
use as the predictor values in the logistic function.
In order to select the correct auxiliary and loca-
tion for insertion in step 4 the utterance values are
consulted. For instance, an interrogative utterance
in the present tense, not in perfect aspect, with a
second person singular subject will require inser-
tion of the auxiliary ?are? after the subject. A zero
subject zero auxiliary, on the other hand, requires
restoration of both subject and auxiliary. Where a
question mark indicates it has been used in an in-
terrogative clause the subject is assumed to be sec-
8We also remove (or flag) the subject relation in cases
where a subject also had to be added in step 4. This would
occur when the original utterance exhibited a zero subject.
9All common nouns are assumed to be 3rd person and all
instances of ?you? were considered to be singular (as was the
case during corpus annotation).
49
ond person - as is the case in most questions - and
so the auxiliary-subject combination ?are you? is
restored before the ING-form. Without a question
mark, the clause is assumed to be declarative and
so the first person singular subject-auxiliary com-
bination ?I am? is restored before the ING-form 10.
We withheld 10% of the zero auxiliary corpus
for test purposes. The integration of the predic-
tive model into the parser allowed us to success-
fully parse 31.4% of previously unparsable zero-
auxilaries. On cleaned spoken transcripts (i.e.
with speech phenomena other than the zero aux-
iliary, such as repetitions, removed) this algorithm
allows us to retrieve the correct subject-object re-
lations for an extra 1238 utterances within our
annotated corpus (which again accounts for ap-
proximately one third of the previously unparsable
zero-auxilaries). This is a significant step forward
for any applications building on top of a parsing
infrastructure.
7 Conclusion
We have shown how awareness of a specific
linguistic phenomenon enables improvements in
NLP techniques. The zero auxiliary is mainly a
feature of spoken language and so is not on the
whole handled successfully by existing parsers,
trained as they are on written data. As a so-
lution, rather than proposing the construction of
new models specifically designed for spoken lan-
guage, thereby doing away with all previous work
on NLP tools and starting again from scratch, we
demonstrated how new training data from a spo-
ken source could be applied to an existing parser
- RASP. We designed a predictive model of zero
auxiliary occurrence based on logistic regression
with nine syntactic variables. The data came from
an annotated corpus of 93,253 progressive con-
structions which showed zero auxiliary frequency
to be 4.2%. Without this new predictive informa-
tion in the parser, the status quo would continue
whereby one in twenty-five progressive construc-
tions would continue to be mis-parsed. We found
that instead the noise was regular and could be
modelled, and we illustrated how this specific lin-
guistic data could be integrated into existing NLP
technology. This is a case study of one specific
linguistic phenomenon. Our belief is that other
10A sample of one hundred zero subject declarative zero
auxiliaries indicates that the first person singular is the ap-
propriate subject type to restore on 60% of occasions.
such spoken language phenomena can be mod-
elled in the same way, given an appropriate corpus
resource, accurate annotation and implementation
into a parser.
By running in a ?speech aware mode? which
supplements existing parsing architecture we ben-
efit from the training that has already been under-
taken on a large scale based on written data and
complement it with specialized and predictable
linguistic properties of speech. Ideally, we would
like to train an entire parsing system on spoken
language but until spoken corpora become more
readily available this is not a practical option: the
resulting parser would suffer greatly from data
sparsity issues. Frustratingly, there is a circular
problem in generating corpora of an appropriate
size for training since until highly accurate mod-
els for spoken language are built we can not expect
speech-to-text systems to provide highly accurate
transcripts. But to build these highly accurate
models of spoken language in the first place a large
amount of data is required. Augmenting the ex-
isting statistical NLP tools trained on written lan-
guage with specialized linguistic knowledge from
the spoken domain is a pragmatic short-term fix
for this problem.
We should note that tailoring parsers to deal
with spoken language is by no means unheard of:
the RASP system itself, for example (which parses
using a probabilistic context-free grammar), al-
ready has several rules in its grammar which are
more appropriate for parsing spoken language.
However, use of these rules can contribute to much
over-generation and complexity in the parse for-
est (the parser internal structure which holds all
the possible parses for an utterance). In conse-
quence, the specialized rules have to be expertly
selected or deselected when configuring the parser.
This work - and our research program as a whole
- would instead allow parser configuration deci-
sions and algorithmic adaptions to be made non-
expertly and on-the-fly when running in ?speech
aware mode?. All rule activations and algorithm
adaptions would be made based on predictions
constructed from expert linguistic analysis of the
spoken domain.
Acknowledgements
This work was supported by the AHRC. We thank
three anonymous reviewers for their comments,
Andrew Rice and Barbara Jones.
50
References
Gisle Andersen. 1995. Omission of the primary verbs
BE and HAVE in London teenage speech - a so-
ciolinguistic study. Hovedfag thesis, University of
Bergen, Norway.
Michael Barlow and Suzanne Kemmer. 2000. Usage-
based models of language. Chicago: CSLI.
Rens Bod, Jennifer Hay and Stefanie Jannedy (eds.).
2003. Probabilistic Linguistics. Cambridge, MA:
MIT Press.
Ted Briscoe, John Carroll and Rebecca Watson. 2007.
The second release of the RASP system. Proceed-
ings of the COLING/ACL on Interactive presenta-
tion sessions, July 17-18, 2006, Sydney, Australia.
The British National Corpus, version 3. 2007.
Distributed by Oxford University Computing Ser-
vices on behalf of the BNC Consortium. URL:
http://www.natcorp.ox.ac.uk/
Roger Brown. 1973. A First Language: the early
stages. London: George Allen and Unwin.
Paula Buttery and Andrew Caines. In preparation. An
Empirical Approach to First Language Acquisition.
Cambridge: Cambridge University Press.
Joan Bybee and Paul Hopper (eds.). 2001. Frequency
and the emergence of linguistic structure. Amster-
dam: John Benjamins.
Andrew Caines. 2010. You talking to me? Zero aux-
iliary constructions in British English. Ph.D thesis,
University of Cambridge.
Frantisek Cerma?k. 2009. Spoken corpora design.
Their constitutive parameters. International Journal
of Corpus Linguistics 14: 113-123.
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. Cambridge, MA: MIT Press.
Roger Garside. 1987. The CLAWS Word-tagging Sys-
tem. In: Roger Garside, Geoffrey Leech and Ge-
offrey Sampson (eds.), The Computational Analy-
sis of English: A Corpus-based Approach. London:
Longman.
Michael A. K. Halliday. 1994. Spoken and Writ-
ten Modes of Meaning. In: David Graddol and
Oliver Boyd-Barrett (eds.), Media Texts: Authors
and Readers. Clevedon: Multilingual Matters.
Michael Haugh. 2009. Designing a Multimodal
Spoken Component of the Australian National Cor-
pus. In: Michael Haugh, Kate Burridge, Jean Mul-
der, and Pam Peters (eds.), Selected Proceedings of
the 2008 HCSNet Workshop on Designing the Aus-
tralian National Corpus. Somerville, MA: Cas-
cadilla Proceedings Project.
William Labov. 1969. Contraction, deletion, and in-
herent variability of the English copula. Language
45: 715-762.
Geoffrey Leech, Marianne Hundt, Christian Mair and
Nicholas Smith. 2009. Change in Contemporary
English: a grammatical study. Cambridge: Cam-
bridge University Press.
Elena Lieven, Heike Behrens, Jennifer Speares and
Michael Tomasello. 2003. Early syntactic creativ-
ity: a usage-based approach. Journal of Child Lan-
guage 30: 333-370.
Luigi Rizzi. 1993/1994. Some notes on linguistic the-
ory and language development: The case of root in-
finitives. Language Acquisition 3: 371-393.
Geoffrey Sampson. 2001. Empirical Linguistics. Lon-
don: Continuum.
Anna Theakston, Elena Lieven, Julian Pine and Caro-
line Rowland. 2005. The acquisition of auxiliary
syntax: BE and HAVE. Cognitive Linguistics 16:
247-277.
Harry Tily, Susanne Gahl, Inbal Arnon, Neal Snider,
Anubha Kothari and Joan Bresnan. 2009. Syntactic
probabilities affect pronunciation variation in spon-
taneous speech. Language and Cognition 1: 147-
165.
Kenneth Wexler. 1994. Optional Infinitives, head
movement and the economy of derivations. In:
David Lightfoot and Norbert Hornstein (eds.), Verb
Movement. Cambridge: Cambridge University
Press.
Stephen Wilson. 2003. Lexically specific construc-
tions in the acquisition of inflection in English.
Journal of Child Language 30: 75-115.
51
Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 38?42,
Gothenburg, Sweden, April 26 2014. c?2014 Association for Computational Linguistics
Towards a computational model of grammaticalization and
lexical diversity
Christian Bentz
University of Cambridge, DTAL
9 West Road, CB3 9DA
cb696@cam.ac.uk
Paula Buttery
University of Cambridge, DTAL
9 West Road, CB3 9DA
pjb48@cam.ac.uk
Abstract
Languages use different lexical inven-
tories to encode information, ranging
from small sets of simplex words to
large sets of morphologically complex
words. Grammaticalization theories
argue that this variation arises as
the outcome of diachronic processes
whereby co-occurring words merge
to one word and build up complex
morphology. To model these pro-
cesses we present a) a quantitative
measure of lexical diversity and b) a
preliminary computational model of
changes in lexical diversity over several
generations of merging higly frequent
collocates.
1 Introduction
All languages share the property of being car-
riers of information. However, they vastly dif-
fer in terms of the exact encoding strategies
they adopt. For example, German encodes in-
formation about number, gender, case, tense,
aspect, etc. in a multitude of different articles,
pronouns, nouns, adjectives and verbs. This
abundant set of word forms contrasts with a
smaller set of uninflected words in English.
Crucially, grammaticalization theories
(Heine and Kuteva, 2007, 2002; Bybee 2006,
2003; Hopper and Traugott, 2003; Lehmann,
1985) demonstrate that complex morpho-
logical marking can derive diachronically by
merging originally independent word forms
that frequently co-occur. Over several gen-
erations of language learning and usage such
grammaticalization and entrenchment pro-
cesses can gradually increase the complexity
of word forms and hence the lexical diversity
of languages.
To model these processes Section 2 will
present a quantitative measure of lexical diver-
sity based on Zipf-Mandelbrots law, which is
also used as a biodiversity index (Jost, 2006).
Based on this measure we present a prelimi-
nary computational model to reconstruct the
gradual change from lexically constrained to
lexically rich languages in Section 3. We
therefore use a simple grammaticalization al-
gorithm and show how historical developments
towards higher lexical diversity match the vari-
ation in lexical diversity of natural languages
today. This suggests that synchronic variation
in lexical diversity can be explained as the out-
come of diachronic language change.
The computational model we present will
therefore help to a) understand the diver-
sity of lexical encoding strategies across lan-
guages better, and b) to further uncover the
diachronic processes leading up to these syn-
chronic differences.
2 Zipf?s law as a measure of lexical
diversity
Zipf-Mandelbrot?s law (Mandelbrot, 1953;
Zipf, 1949) states that ordering of words ac-
cording to their frequencies in texts will render
frequency distributions of a specific shape: in
general, few words have high frequencies, fol-
lowed by a middle ground of medium frequen-
cies and a long tail of low frequency items.
However, a series of studies pointed out that
there are subtle differences in frequency dis-
tributions for different texts and languages
(Bentz et al., forthcoming; Ha et al., 2006;
Popescu and Altmann, 2008). Namely, lan-
guages with complex morphology tend to have
longer tails of low frequency words than lan-
guages with simplex morphology. The param-
eters of Zipf-Mandelbrot?s law reflect these dif-
ferences, and can be used as a quantitative
38
measure of lexical diversity.
2.1 Method
We use the definition of ZM?s law as captured
by equation (1):
f(r
i
) =
C
? + r
?
i
,
C > 0, ? > 0, ? > ?1, i = 1, 2, . . . , n (1)
where f(r
i
) is the frequency of the word
of the i
th
rank (r
i
), n is the number of ranks,
C is a normalizing factor and ? and ? are
parameters. To illustrate this, we use parallel
texts of the Universal Declaration of Human
Rights (UDHR) for Fijian, English, German
and Hungarian. For frequency distributions
of these texts (with tokens delimited by
white spaces) we can approximate the best
fitting parameters of the ZM law by means
of maximum likelihood estimation (Izsa?k,
2006; Murphy, 2013). In double logarithmic
space (see Figure 1) the normalizing factor
C would shift the line of best fit upwards or
downwards, ? is the slope of this line and ?
is Mandelbrot?s (1953) corrective for the fact
that the line of best fit will deviate from a
straight line for higher frequencies (upper left
corner in Figure 1).
As can be seen in Figure 1 Fijian has higher
frequencies towards the lowest ranks (upper
left corner) but the shortest tail of words with
frequency one (horizontal bars in the lower
right corner). For Hungarian the pattern runs
the other way round: it has the lowest frequen-
cies towards the low ranks and a long tail of
words with frequency one. German and En-
glish lie between these. These patterns are re-
flected in ZM parameter values. Namely, Fi-
jian has the highest parameters, followed by
English, German and Hungarian. By trend
there is a negative relationship between ZM
parameters and lexical diversity: low lexical
diversity is associated with high parameters,
high diversity is associated with low param-
eters. Cross-linguistically this effect can be
used to measure lexical diversity by means of
approximating the parameters of ZM?s law for
parallel texts.
In the following, we will present a compu-
tational model to elicit the diachronic path-
ways of grammaticalization through which a
Figure 1: Zipf frequency distributions for four
natural languages (Fijian, English, German,
Hungarian). Plots are in log-log space, val-
ues 0.15, 0.1 and 0.05 were added to Fijian,
English and German log-frequencies to avoid
overplotting. Values for the Zipf-Mandelbrot
parameters are given in the legend. The
straight black line is the line of best fit for
Fijian.
low lexical diversity language like Fijian might
develop towards a high diversity language like
Hungarian.
3 Modelling changes in lexical
diversity
Grammaticalization theorists have long
claimed that synchronic variation in word
complexity and lexical diversity might be the
outcome of diachronic processes. Namely, the
grammaticalization cline from content item
>grammatical word >clitic >inflectional affix
is seen as a ubiquitous process in language
change (Hopper and Traugott, 2003: 7).
In the final stage frequently co-occurring
words merge by means of phonological fusion
(Bybee, 2003: 617) and hence ?morphologize?
to built inflections and derivations.
Typical examples of a full cline of grammat-
icalization are the Old English noun l??c ?body?
becoming the derivational suffix -ly, the inflec-
tional future in Romance languages such as
Italian cantero` ?I will sing? derived from Latin
cantare habeo ?I have to sing?, or Hungarian
inflectional elative and inessive case markers
derived from a noun originally meaning ?in-
terior? (Heine and Kuteva, 2007: 66). These
processes can cause languages to distinguish
39
between a panoply of different word forms. For
example, Hungarian displays up to 20 different
noun forms where English would use a single
form (e.g. ship corresponding to Hungarian
hajo? ?ship?, hajo?ban ?in the ship?, hajo?ba ?into
the ship?, etc.).
As a consequence, once the full grammati-
calization cline is completed this will increase
the lexical diversity of a language. Note,
however, that borrowings (loanwords) and ne-
ologisms can also increase lexical diversity.
Hence, a model of changes in lexical diversity
will have to take both grammaticalization and
new vocabulary into account.
3.1 The model
Text: We use the Fijian UDHR as our start-
ing point for two reasons: a) Fijian is a lan-
guage that is well known to be largely lack-
ing complex morphology, b) the UDHR is a
parallel text and hence allows us to compare
different languages by controlling for constant
information content. Fijian has relatively low
lexical diversity and high ZM parameter val-
ues (see Figure 1). The question is whether
we can simulate a simple merging process over
several generations that will transform the fre-
quency distribution of the original Fijian text
to fit the frequency distribution of the mor-
phologically and lexically rich Hungarian text.
To answer this question, we simulate the out-
come of grammaticalization on the frequency
distributions in the following steps:
Simulation: Our program takes a given
text of generation i, calculates a frequency
distribution for this generation, changes the
text along various operations given below, and
gives the frequency distribution of the text for
a new generation i+ 1 as output.
We take the original UDHR in Fijian as our
starting point in generation 0 and run the pro-
gram for consecutive generations. We simulate
the change of this text over several generations
of language learning and usage by varying the
following variables:
? p
m
: Rank bigrams according to their fre-
quency and merge the highest p
m
per-
cent of them to one word. This simu-
lates a simple grammaticalization process
whereby two separate words that are fre-
quent collocates are merged to one word.
? p
v
: Percentage of words replaced by new
words. Choose p
v
of words randomly and
replace all instances of these words by in-
verting the letters. This simulates neolo-
gisms and loanwords replacing deprecated
words.
? r
R
: Range of ranks to be included in p
v
replacements. If set to 0, vocabulary from
anywhere in the distribution will be ran-
domly replaced.
? n
G
: Number of generations to simulate.
This simulation essentially allows us to vary
the degree of grammaticalization by means of
varying p
m
, and also to control for the fact
that frequency distributions might change due
to loanword borrowing and introduction of
new vocabulary (p
v
). Additionally, r
R
allows
us to vary the range of ranks where new words
might replace deprecated ones. For frequency
distributions calculated by generations we ap-
proximate ZM parameters by maximum likeli-
hood estimations and therefore document the
change of their shape.
Results: Figure 2 illustrates a simulation
of how the low lexical diversity language Fi-
jian approaches quantitative lexical properties
similar to the Hungarian text just by means of
merging high-frequent collocates. While the
frequency distribution of Fijian in generation
0 still reflects the original ZM values, the
ZM parameter values after 6 generations of
grammaticalization have become much closer
to the values of the Hungarian UDHR:
Fij (n
G
= 0): ? = 1.21,? = 2.1,C = 812
Fij (n
G
= 6): ? = 0.70,? = ?0.22,C = 73
Hun (n
G
= 0): ? = 0.76,? = ?0.31,C = 90
Note, that in this model there is actu-
ally no replacement of vocabulary necessary
to arrive at frequency distributions that
correspond to high lexical diversity variants.
After only six generations of merging 2.5% of
bigrams to a single grammaticalized word the
Fijian UDHR has ZM parameter properties
very close to the Hungarian UDHR. However,
in future research we want to scrutinize the
effect of parameter changes on frequency
distributions in more depth and in accordance
40
Figure 2: Simulation of grammaticalization processes and their reflections in Zipf distributions
for variable values p
m
= 2.5, p
v
= 0, r
R
= 0,n
G
= 10. Changes of ? are shown in the upper left
panel, changes in ? are shown in the upper right panel, changes in C are shown in the lower left
panel, and changes in log-transformed frequency distributions are illustrated in the lower right
panel.
with estimations derived from historical
linguistic studies.
4 Discussion
We have pointed out in Section 2 that lexical
diversity can be measured cross-linguistically
by means of calculating frequency distribu-
tions for parallel texts and approximating the
corresponding ZM parameters in a maximum
likelihood estimation.
It is assumed that cross-linguistic variation
is the outcome of diachronic processes of gram-
maticalization, whereby highly frequent bi-
grams are merged into a single word. The
preliminary computational model in Section 3
showed that indeed even by a strongly sim-
plified grammaticalization process a text with
low lexical diversity (Fijian UDHR) can gain
lexical richness over several generations, and
finally match the quantitative properties of a
lexically rich language (Hungarian UDHR).
However, there are several caveats that need
to be addressed in future research:
? More models with varying parameters
need to be run to scrutinize the interac-
tion between new vocabulary (loanwords,
neologisms) and grammaticalization.
? The grammaticalization algorithm used is
overly simplified. A more realistic pic-
ture is possible by using POS tagged and
parsed texts to ensure that only certain
parts of speech in certain syntactic con-
texts grammaticalize (e.g. pre- and post-
positions in combination with nouns).
? The model could be elaborated by consid-
ering not only bigram frequencies but also
frequencies of the individual words and
more complex frequency measures (see
Schmid, 2010).
41
5 Conclusion
Languages display an astonishing diversity
when it comes to lexical encoding of informa-
tion. This synchronic variation in encoding
strategies is most likely the outcome of di-
achronic processes of language change. We
have argued that lexical diversity can be mea-
sured quantitatively with reference to the pa-
rameters of Zipf-Mandelbrot?s law, and that
pathways of change in lexical diversity can be
modelled computationally. Elaboration and
refinement of these models will help to bet-
ter understand linguistic diversity as the out-
come of processes on historical and evolution-
ary time scales.
References
Marco Baroni. 2009. Distributions in text.
In Anke Lu?deling and Merja Kyto? (eds.),
Corpus Linguistics. An international
handbook. Berlin/ New York, Mouton de
Gruyter, pages 803-826.
Christian Bentz, Douwe Kiela, Felix Hill,
and Paula Buttery. forthcoming. Zipf?s law
and the grammar of languages. In Corpus
Linguistics and Linguistic Theory.
Joan Bybee. 2006. From usage to grammar:
The mind?s repsonse to repetition. In
Language, volume 82 (4), pages 711-733.
Joan Bybee. 2003. Mechanisms of change in
grammaticization: the role of frequency.
In B. D. Joseph and J. Janda(eds.), The
Handbook of Historical Linguistics. Oxford,
Blackwell, pages 711-733.
Le Q. Ha, Darryl Stewart, Philip Hanna,
and F. Smith. 2006. Zipf and type-token
rules for the English, Spanish, Irish and
Latin languages. In Web Journal of Formal,
Computational and Cognitive Linguistics,
volume 8.
Bernd Heine and Tania Kuteva. 2007.
The Genesis of Grammar: A Reconstruc-
tion. Oxford University Press.
Bernd Heine and Tania Kuteva. 2002.
World lexicon of grammaticalization. Cam-
bridge University Press.
Paul J. Hopper and Elizabeth C. Traugott.
2003. Grammaticalization. Cambridge
University Press.
Ja?nos Izsa?k. 2006. Some practical aspects
of fitting and testing the Zipf-Mandelbrot
model: A short essay. In Scientometrics,
volume 67(1), pages 107-120.
Lou Jost. 2006. Entropy and diversity.
In OIKOS, volume 113(2), pages 363-375.
Christian Lehmann. 1985. Grammatical-
ization: Synchronic variation and di-
achronic change. In Lingua e stile, volume
20, pages 303-318.
Benoit Mandelbrot. 1953. An informa-
tional theory of the statistical structure
of language. In William Jackson (ed.),
Communication Theory. Butterworths
Scientific Publications, London, pages
468-502.
Laura Murphy. 2013. R package likeli-
hood: Methods for maximum likeli-
hood estimation. Retrieved from cran.r-
project.org/web/packages/likelihood
Ioan-Iovitz Popescu, and Gabriel Altmann.
2008. Hapax legomena and language typol-
ogy. In Journal of Quantitative Linguistics,
volume 15(4), pages 370378.
Hans-J. Schmid. 2010. Does frequency
in text instantiate entrenchment in the
cognitive system? In Dylan Glynn and
Kerstin Fischer (eds.), Quantitative meth-
ods in cognitive semantics: Corpus-driven
approaches. Berlin, Walter de Gruyter,
pages 101-133.
George K. Zipf. 1949. Human behavior
and the principle of least effort. Addison,
Cambridge (Massachusetts).
42
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 74?81 Dublin, Ireland, August 23-29 2014.
The effect of disfluencies and learner errors on the parsing of spoken
learner language
Andrew Caines Paula Buttery
Institute for Automated Language Teaching and Assessment
Department of Theoretical and Applied Linguistics
University of Cambridge, Cambridge, U.K.
(apc38|pjb48)@cam.ac.uk
Abstract
NLP tools are typically trained on written data from native speakers. However, research into
language acquisition and tools for language teaching & proficiency assessment would benefit
from accurate processing of spoken data from second language learners. In this paper we dis-
cuss manual annotation schemes for various features of spoken language; we also evaluate the
automatic tagging of one particular feature (filled pauses) ? finding a success rate of 81%; and we
evaluate the effect of using our manual annotations to ?clean up? the transcriptions for sentence
parsing, resulting in a 25% improvement in parse success rate by completely cleaning the texts of
disfluencies and errors. We discuss the need to adapt existing NLP technology to non-canonical
domains such as spoken learner language, while emphasising the worth of continued integration
of manual and automatic annotation.
1 Introduction
Natural language processing (NLP) tools are typically trained on written data from native speakers.
However, research into language acquisition and tools for language proficiency assessment & language
teaching ? such as learner dialogue and feedback systems ? would benefit from accurate processing of
spoken data from second language learners. Being able to convert the text from unparseable to parseable
form will enable us to (a) posit a target hypothesis that the learner intended to produce, and (b) provide
feedback on this target based on the information removed or repaired in achieving that parseable form.
To proceed towards this goal, we need to adapt current NLP tools to the non-canonical domain of
spoken learner language in a persistent fashion rather than use ad hoc post-processing steps to ?correct?
the non-canonical data. Outcomes of this approach have been reported in the literature (e.g. Rimell &
Clark (2009) in the biomedical domain; Caines & Buttery (2010) for spoken language). These fully
adaptive approaches require large amounts of annotated data to be successful and, as we intend to work
along these lines in future, the discussion in this paper is pointed in that direction.
The work presented here will act as a foundation for more permanent adaptations to existing tools.
We annotate transcriptions of speech for linguistic features that are known to interfere with standard
NLP to assess whether large-scale annotation of these features will be useful for training purposes. Ob-
vious instances of this include disfluencies (e.g. filled pauses, false starts, repetition), formal errors of
morphology and syntax, as well as ?errors? of word and phrase selection1.
Since manual annotation is costly in terms of time and often money, one might question whether so
many feature types are strictly necessary or even helpful for the task in hand. Indeed, filled pauses
such as ?oh? and ?um? are already accounted for in the part-of-speech (POS) tagset we use (CLAWS2
(Garside, 1987)); and one might also argue that lexico-semantic errors might be dismissed a priori on
the assumption that both the original and proposed forms are of the same POS (and thus won?t affect
a parser that performs tagging before the parse). We investigate the contribution of these features to
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1The word ?error? appears here in quotes as it might be argued that questionable lexical selections are more a matter of
infelicity and improbability than any strict dichotomy; we put this concern aside for now as a matter for future research.
74
parsing success. From a theoretical perspective we are interested in these features with regard to second
language acquisition and therefore need to analyse them closely.
In this paper we describe our initial efforts to address the challenge of parsing learner speech with
tools trained on native speaker writing. We also present empirical results that demonstrate the utility of
annotated spoken transcription with respect to both tagging and parsing. We investigate: [i] the frequency
of disfluencies, formal errors of morpho-syntax, and idiomatic errors of lexico-semantics in a corpus of
spoken learner language; [ii] the accuracy of part-of-speech labels produced by the tagger associated
with the Robust Accurate Statistical Parsing System (RASP (Briscoe et al., 2006)) for a particular type
of disfluency (the filled pause)2; [iii] parse success rates and parse likelihoods using the RASP System
with the texts in various ?modes? ranging from unaltered transcription to fully edited and corrected3.
We find that in our spoken learner corpus of 2262 words, (i) around a quarter of words are annotated
as disfluencies or errors; (ii) 81% of filled pauses were correctly tagged, meaning 1 in 5 are incorrectly
tagged; (iii) mean parse likelihood for the text ?as is?, unaltered, is ?2.599 with a parse success rate of
47%, whereas completely ?cleaned up? text improves those scores to ?1.995 and 72%4. We discuss the
implications of these results below, along with the background context for our study and a more detailed
description of our investigations.
2 Background
Previous analyses of the NLP of learner language include various experiments on the tagging and parsing
of errorful text. Geertzen et al. (2013) employed the Stanford parser on written English learner data and
achieved labelled and unlabelled attachment scores (LAS, UAS)5 of 89.6% and 92.1%. They found that
errors at the morphological level lead to incorrect POS-tagging, which in turn can result in an erroneous
parse. Others have focused only on the POS-tagging of written learner corpora ? for example with
English (van Rooy and Sch?fer, 2003) and French learner data (Thou?sny, 2011) ? demonstrating that
post-hoc corrections for the most frequent tagging errors results in significant parse error reduction.
In other investigations of standard NLP tools on learner corpora, Ott & Ziai (2010) report general
robustness using MaltParser on written German learner language; however, they found that by manually
correcting POS tags, LAS improved from 79.15% to 85.71% and UAS from 84.81% to 90.22%. Wagner
& Foster (2009) ran a series of parsing experiments using parallel errorful/corrected corpora, including
a spoken learner corpus in which the likelihood of the highest ranked tree for corrected sentences was
higher than that of uncorrected sentences in 69.6% of 500 instances. Taken together, these studies suggest
that existing NLP tools remain robust to learner data, even more so if the original texts can be corrected
and if the tagging stage is in some way verified, or adapted (e.g. Zinsmeister et al. (2014)).
On the other hand, D?az-Negrillo et al. (2010) argue that treating learner data as a ?noisy variant? of
native language glosses over systematic differences between the two, and instead call for a dedicated tag-
ging format for ?interlanguage?, one that encodes distributional, morphological and lexical information.
For instance, ?walks? in ?John has walks? would morphologically be tagged as a present tense 3rd-person
verb, but distributionally tagged as a past participle6. This is the kind of adaptation of existing tools that
we advocate, though we would add that this system should be available for not just interlanguage but all
data, allowing for non-canonical language use by native speakers as much as learners.
As for spoken language, Caines & Buttery (2010) among others suggest that adaptation can also be
made to the parser, such that it enters a ?speech-aware mode? in which the parser refers to additional
and/or replacement rules adapted to the particular features of spoken language. They demonstrated this
with the omission of auxiliary verbs in progressive aspect sentences (?you talking to me??, ?how you
2The RASP POS-tagger was evaluated on the 560 randomly selected sentences from The Wall Street Journal that constitute
the PARC dependency bank (DepBank; (King et al., 2003)) and achieved 97% accuracy (Briscoe et al., 2006).
3The RASP parser achieves a 79.7% microaveraged F1 score on grammatical relations in DepBank (Briscoe and Carroll,
2006).
4N.B. the closer the parse likelihood to zero, the more probable the parse in the English language.
5LAS indicates the proportion of tokens that are assigned both the correct head and the correct dependency label; UAS
indicates the proportion of tokens assigned the correct head, irrespective of dependency label.
6We thank reviewer #1 for this example.
75
doing??) and achieved a 30% improvement in parsing success rate for this construction type.
3 The corpus
Our speech data consist of recordings from Business Language Testing Service (BULATS) speaking
tests7. In the test, learners are required to undertake five tasks; we exclude the tasks involving brief
question-answering (?can you tell me your full name??, ?where are you from??, etc) and elicited imitation,
leaving us with three free-form speech tasks. For this particular test the tasks were: [a] talk about some
advice from a colleague (monologue), [b] talk about a series of charts from Business Today magazine
(monologue), [c] give advice on starting a new retail business (dialogue with examiner).
In our full dataset the candidates come from India, Pakistan and Brazil, with various first languages
(L1) including Hindi, Gujarati, Malayalam, Urdu, Pashto and Portuguese, and an age range of 16 to 47
at the time of taking the test. However, in this analysis we have only sampled recordings from candidates
deemed to be at ?B2? upper intermediate level on the CEFR scale8, so that the proficiency level of
language used (and how that relates to NLP) is controlled for. In addition the L1s in our sample are
Gujarati, Punjabi and Urdu only. This gives us a sample corpus of 2262 tokens in ?as-is? format (i.e. the
true transcriptions before any corrections are made).
4 Manual annotation
The recordings were manually transcribed and annotated for various features falling into three categories
described and exemplified in the following non-exhaustive list.
? disfluencies ? interruptions to the flow of otherwise fluent speech;
? <fp> filled pauses (tokens such as uh, er, um that serve to fill time and hold the turn) and <rep
n="n"> repetition (the speaker repeats a word or phrase one or several times):
?or the other way is to <fp>um</fp> <rep n="1">is to</rep> raise finance?
? <false> false starts ? the speaker begins to express a word or phrase which he then corrects:
?in two thousand eight it was <false>thirty five p</false> thirty percent?
? formal errors of morpho-syntax, such as number agreement, verb inflection and word order errors;
? noun form: ?for becoming a chartered <NS type="FN"><i>accountants</i><c>accountant
</c></NS>?
? missing verb: ?as the charts <NS type="MV"><c>show</c></NS> its sales increased?
? word order: ?<NS type="W"><i>how it would be help for you mention</i><c>mention how
it helped you</c></NS>?
? idiomatic ?errors? ? infelicities in lexical selection, failure to express intended meaning, or less-
than-natural phrasing;
? idiomatic: ?all my class <NS type="ID"><i>fellows</i><c>mates</c></NS>?
? idiomatic: ?to <NS type="ID"><i>get in</i><c>make a</c></NS> profit?
? replace quantifier: ?for a bank to grant us <NS type="RQ"><i>some</i><c>a</c></NS> loan?
The annotation scheme for formal and idiomatic errors comes from the project to annotate the Cambridge
Learner Corpus (Briscoe et al., 2010). The ?error zone? is denoted by <NS> tags, with any original
token(s) enclosed by <i> and any proposed correction enclosed by <c>. The various error types are
defined in Nicholls (2003) and the categories are similar to the ones given: either self-defining (?ID? for
idiom error, ?W? for word order, etc) or a combination of operation plus part-of-speech (?FN? form of
noun, ?MV? missing verb, ?RQ? replace quantifier, etc).
In Table 1 we report the number of errors and disfluencies found in our corpus along with a relative
frequency per 100 words. Just under a quarter of the thousand tokens in our corpus are affected by
disfluencies and errors, with the former being far more prevalent.
7We thank Cambridge English Language Assessment for releasing these recordings for this pilot study; for further informa-
tion on BULATS go to http://www.bulats.org/
8The ?Common European Framework of Reference for Languages?: a schema for grading an individual learner?s language
level. For further information go to http://www.coe.int/lang-CEFR
76
All transcription and annotation has been carried out by a single annotator (the first author). It would
be interesting to obtain measures of inter-annotator agreement to assess the extent to which the nature of
error judgement (particularly in judgements as to idiomaticity) is subjective.
type instances in corpus relative frequency (per 100 words)
disfluency 316 14
formal error 143 6
idiomatic error 70 3
total 529 23
Table 1: Error counts in our corpus
5 Automated annotation: part-of-speech tagging
Since filled pauses such as ?er? and ?um? are included in the CLAWS2 tagset used by the RASP System
as UH, ?interjection?, one might question the worth of manually annotating filled pauses (FPs). Of the
disfluency set, it might be one small time-saving to leave these to the tagger. However, ?interjection? is
not a homogeneous set, as UH also covers exclamations of surprise (?oh?) and assent (?yes?). Moreover,
we find that the POS-tagging of tokens annotated as FPs is not entirely appropriate in this non-canonical
domain. Table 2 shows that the majority of FPs are correctly tagged UH, though others are tagged as
nouns (NN), verbs (VV), adjectives (JJ), adverbs (RR) and foreign words (&FW)9.
Token UH &FW JJ NN RR VV total
er 104 0 0 0 0 0 104
mm 0 0 0 8 0 0 8
uh 0 0 0 1 2 4 7
um 2 5 0 0 0 0 7
nuh 0 0 0 0 2 1 3
buh 0 0 0 1 0 1 2
other 2 0 1 0 0 0 3
total 108 5 1 10 6 9 134
Table 2: POS tagging of filled pauses
One possible solution is to append a dictionary of known FP tokens to the tagger, and specify that
they should be tagged UH, or even better, a new tag such as FP. But as the Table demonstrates, there
are standard, highly frequent FPs such as ?er?, ?uh? and ?um?, and then there are novel forms such as
?nuh?, ?buh? and ?nna? which we found to be rather idiosyncratic ? i.e. there might be novel FPs for every
individual. Moreover, the introduction of a closed class depends on consistent transcription practice, not
necessarily a given with even a lone annotator, let alone more than one.
Automatic identification and repair of disfluencies is a well-developed research topic, with continuing
refinements to joint parsing and disfluency detection models (e.g. Qian & Liu (2013), Rasooli & Tetreault
(2014), Honnibal & Johnson (2014)), plus applied work in the domains of automatic speech recognition
(Fitzgerald et al., 2009) and machine translation (Cho et al., 2014). We note the linguistic rules included
in the Lease, Johnson & Charniak (2006) tree adjoining grammar (TAG) noisy-channel model ? lexical,
POS and syntactic rules that reduce errors in the TAG model. This is another case of improvements to
NLP tools thanks to data-driven linguistic insight, and a design that we could incorporate into our work
on automated assessment and feedback.
9The ?other? filled pauses are singleton forms: eh, nna, ah.
77
6 Automated annotation: sentence parsing
In this section we report the results of our parsing experiment in which transcribed learner utterances
were processed by the RASP system in four different forms:
(A) as-is: without alteration;
(B) less-disfluency: with disfluencies removed;
(C) less-form-error: with morpho-syntactic errors corrected;
(D) less-lex-error: with semantic/idiomatic improvements.
We investigated the effect on the parsing output of each transcription format compared to the (A) format
as a baseline. We processed each format in turn singularly, as well as cumulative combinations of (B),
(C) and (D) in every possible order. The results are set out in Table 3, with mean likelihoods of the
highest ranked parse for each sentence (?)10, differences between this mean and the baseline where
applicable (?base), and success rates in terms of non-fragmentary tree outputs (i.e. parses labelled other
than ?T/frag? in the RASP System).
mode ? ?base ?T/frag mode ? ?base ?T/frag mode ? ?base ?T/frag
(A) ?2.599 0 .471 (A) ?2.599 0 .471 (A) ?2.599 0 .471
(B) ?2.094 +.505 .623 (BC) ?2.032 +.567 .689 (BCD) ?1.995 +.604 .715
(C) ?2.574 +.025 .484 (BD) ?2.049 +.550 .649 - - -
(D) ?2.563 +.036 .503 (CD) ?2.545 +.054 .523 - - -
Table 3: Mean parse likelihoods, deltas to baseline and parse success rates in all transcription modes
As can be seen in Table 3, the removal of disfluencies (B) is the single move of greatest benefit to
parse likelihood scores and parse tree success rates compared to the ?as-is? baseline (A). The correction
of morpho-syntactic (C) and idiomatic errors (D) have a lesser effect. All pairings have a positive effect
on parse likelihoods, especially those featuring disfluency removal (B); and the three ?corrective? steps
combined (BCD) have the greatest effect of all.
However, we show by analysis of two candidates in our corpus that these effects can differ on an
individual basis. In Figure 1, the candidate on the left has a less pronounced effect of disfluency removal
(B) compared to the baseline (A) than the candidate on the right. The effect of both formal (C) and
idiomatic (D) error correction are also seen to make improvements over (A), which is not the case for
the second candidate. Such observations serve as a reminder that when generalising about overall corpus
patterns we collapse over so many individual language models. It may well turn out that disfluencies are
an especially idiosyncratic type of language use, an avenue we will explore in future work.
7 Discussion
In this paper we have investigated NLP of transcribed learner speech, questioning how tools trained on
native speaker written data would handle such data. We found that the majority (81%) of filled pauses
were correctly tagged ?UH?, though this only covers three of eleven FP forms (er, um, eh). We propose a
dictionary of FPs and a specific FP POS-tag, while suggesting that the dictionary will not catch all novel
FPs (since they seem to be idiosyncratic) and that we can turn to state-of-the-art research on automated
disfluency detection to help us.
We also showed that sentence parsing could be improved from a 47% ?success? rate (i.e. non-
fragmentary (T/frag in RASP parlance) parse trees) in the ?as-is? transcriptions, to 72% in transcrip-
tions with disfluencies removed and errors corrected (see Table 3). We found that disfluency removal
is the main contributor to this improvement, though this was found to be somewhat idiosyncratic (as in
Figure 1).
10Note that parse likelihoods have been normalised for word length, as they increase in a near-linear manner according to the
number of terminal nodes in a tree.
78
l l l
l
l
l
l
l
l
l
l
l
l
S2BWWT9EVS S3R66XVRQ2
?6
?4
?2
0
A B C D BC BD CD ABC A B C D BC BD CD ABC
transcription mode
pa
rs
e 
lik
el
ih
oo
d 
(n
or
ma
lis
ed
 fo
r w
o
rd
 le
ng
th
)
Figure 1: Parse likelihoods for each transcription mode, for two individuals in our corpus; the whiskers
indicate the largest and smallest observation within 1.5?IQR (inter-quartile range; the distance between
first and third quartiles), while the upper hinge indicates the third quartile (75th percentile), the middle
is the median, the lower hinge is the first quartile (25th percentile), and the points are outliers.
The motivation for this work is to investigate what is required to convert texts from unparseable to
parseable form. The steps taken to achieve this can be used to inform automated learner dialogue or
feedback systems. We note that automated assessment may be improved by parse trees but may well
be performed without them: it can proceed on the basis of superficial detection of features known to
correlate with high grades (possibly including certain disfluency types, for instance). But to be able to
diagnose how the learner can improve, we need a deeper structural analysis of the text ? i.e. requiring
that the text is in parseable form. Our manual annotations are one step towards this goal.
Our annotations also indicate that spoken learner data features many disfluencies and errors, with over
a quarter of the 2262-word testset affected in some way. Automatic error detection (and correction) is
a burgeoning field (see for example the work on learner data by Briscoe et al. (2010), Andersen (2011)
and Kochmar & Briscoe (2014), as well as the most recent shared task on grammatical error correction
at CoNLL-2014 (Ng et al., 2014)). Such studies are based on written language. We envisage adding
speech-specific information and adaptations to such systems on the basis of our fuller annotation project.
Indeed, it so happens that the problem of NLP in the spoken domain is one we address here with
learner data. However, we do not assume that the problem of adapting or building NLP tools for spoken
data is substantially different for native speaker data. We intend to collect recordings of native speakers
undertaking the same tasks as the BULATS candidates, allowing for comparative studies of errors and
disfluencies in native and learner data, with the task and topic variables held constant as far as possible.
Finally, we emphasise that we intend to add to the corpus with more annotated data from a wider
range of L1s and a wider range of proficiency levels. We can then investigate the possible effects of more
varied syntactic complexity, lexical diversity and error types.
79
Acknowledgments
We thank Cambridge English Language Assessment for funding this work and providing the data. We
also thank Ted Briscoe, Mike McCarthy and ?istein Andersen for their support and advice, and we are
grateful to the three anonymous reviewers for their helpful comments and suggestions.
References
?istein E. Andersen. 2011. Semi-automatic ESOL error annotation. English Profile Journal, 2:e1.
Ted Briscoe and John Carroll. 2006. Evaluating the accuracy of an unlexicalized statistical parser on the PARC
DepBank. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions. Association for Com-
putational Linguistics.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP System. In Proceedings
of the COLING/ACL 2006 Interactive Presentations Session. Association for Computational Linguistics.
Ted Briscoe, Ben Medlock, and ?istein E. Andersen. 2010. Automated assessment of ESOL free text examina-
tions. University of Cambridge Computer Laboratory Technical Reports, 790.
Andrew Caines and Paula J. Buttery. 2010. ?You talking to me?? A predictive model for zero auxiliary construc-
tions. In Proceedings of the Workshop on Natural Language Processing and Linguistics, Finding the Common
Ground, Annual Meeting of the Association for Computational Linguistics (ACL) 2010. Association for Com-
putational Linguistics.
Eunah Cho, Jan Niehues, and Alex Waibel. 2014. Tight integration of speech disfluency removal into SMT. In
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics
(EACL 2014). Association for Computational Linguistics.
Ana D?az-Negrillo, Detmar Meurers, Salvador Valera, and Holger Wunsch. 2010. Towards interlanguage POS
annotation for effective learner corpora in SLA and FLT. Language Forum, 36:139?154.
Alison Edwards. 2014. The progressive aspect in the Netherlands and the ESL/EFL continuum. World Englishes,
33:173?194.
Erin Fitzgerald, Keith Hall, and Frederick Jelinek. 2009. Reconstructing false start errors in spontaneous speech
text. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational
Linguistics (EACL 2009). Association for Computational Linguistics.
Roger Garside. 1987. The CLAWS word-tagging system. In Roger Garside, Geoffrey Leech, and Geoffrey
Sampson, editors, The Computational Analysis of English: A Corpus-based Approach. London: Longman.
Jeroen Geertzen, Theodora Alexopoulou, and Anna Korhonen. 2013. Automatic linguistic annotation of large
scale L2 databases: the EF-Cambridge Open Language Database (EFCAMDAT). In Proceedings of the 31st
Second Language Research Forum. Somerville, MA: Cascadilla Proceedings Project.
Matthew Honnibal and Mark Johnson. 2014. Joint incremental disfluency detection and dependency parsing.
Transactions of the Association for Computational Linguistics, 2:131?142.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary Dalrymple, and Ronald M. Kaplan. 2003. The PARC700
Dependency Bank. In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora
(LINC 2003).
Ekaterina Kochmar and Ted Briscoe. 2014. Detecting learner errors in the choice of content words using com-
positional distributional semantics. In Proceedings of the 25th International Conference on Computational
Linguistics (COLING 2014). Association for Computational Linguistics.
Matthew Lease, Mark Johnson, and Eugene Charniak. 2006. Recognizing disfluencies in conversational speech.
IEEE Transactions on Audio, Speech, and Language Processing, 14:1566?1573.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher
Bryant. 2014. The CoNLL-2014 Shared Task on Grammatical Error Correction. In Eighteenth Conference
on Computational Natural Language Learning, Proceedings of the Shared Task. Association for Computational
Linguistics.
80
Diane Nicholls. 2003. The Cambridge Learner Corpus: error coding and analysis for lexicography and ELT. In
Dawn Archer, Paul Rayson, Andrew Wilson, and Tony McEnery, editors, Proceedings of the Corpus Linguistics
2003 conference; UCREL technical paper number 16. Lancaster University.
Niels Ott and Ramon Ziai. 2010. Evaluating dependency parsing performance on German learner language. In
Proceedings of the Ninth International Workshop on Treebanks and Linguistic Theories (NEALT 2010).
Xian Qian and Yang Liu. 2013. Disfluency detection using multi-step stacked learning. In Proceedings of the
2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT).
Mohammad Sadegh Rasooli and Joel Tetreault. 2014. Non-monotonic parsing of Fluent umm I Mean disfluent
sentences. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational
Linguistics (EACL 2014). Association for Computational Linguistics.
Laura Rimell and Stephen Clark. 2009. Porting a lexicalized-grammar parser to the biomedical domain. Journal
of Biomedical Informatics, 42:852?865.
Sylvie Thou?sny. 2011. Increasing the reliability of a part-of-speech tagging tool for use with learner language. In
Proceedings of the Pre-conference Workshop on Automatic Analysis of Learner Language, CALICO Conference
2009.
Bertus van Rooy and Lande Sch?fer. 2003. An evaluation of three POS taggers for the tagging of the Tswana
Learner English Corpus. In Proceedings of the Corpus Linguistics 2003 Conference. Lancaster University.
Joachim Wagner and Jennifer Foster. 2009. The effect of correcting grammatical errors on parse probabilities. In
Proceedings of the 11th International Conference on Parsing Technologies.
Heike Zinsmeister, Ulrich Heid, and Kathrin Beck. 2014. Adapting a part-of-speech tagset to non-standard text:
the case of STTS. In Proceedings of the Ninth International Conference on Language Resources and Evaluation
(LREC 2014). European Language Resources Association.
81
