Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 847?857, Prague, June 2007. c?2007 Association for Computational Linguistics
Multiple Alignment of Citation Sentences with
Conditional Random Fields and Posterior Decoding
Ariel S. Schwartz?
EECS, Computer Science Division
UC Berkeley
Berkeley, CA 94720-1776
sariel@cs.berkeley.edu
Anna Divoli, Marti A. Hearst
School of Information
UC Berkeley
Berkeley, CA 94720-4600
{hearst,divoli}@ischool.berkeley.edu
Abstract
In scientific literature, sentences that cite re-
lated work can be a valuable resource for
applications such as summarization, syn-
onym identification, and entity extraction.
In order to determine which equivalent en-
tities are discussed in the various citation
sentences, we propose aligning the words
within these sentences according to semantic
similarity. This problem is partly analogous
to the problem of multiple sequence align-
ment in the biosciences, and is also closely
related to the word alignment problem in sta-
tistical machine translation. In this paper
we address the problem of multiple citation
concept alignment by combining and mod-
ifying the CRF based pairwise word align-
ment system of Blunsom & Cohn (2006)
and a posterior decoding based multiple se-
quence alignment algorithm of Schwartz &
Pachter (2007). We evaluate the algorithm
on hand-labeled data, achieving results that
improve on a baseline.
1 Introduction
The scientific literature of biomedicine, genomics,
and other biosciences is a rich, complex, and con-
tinually growing resource. With appropriate infor-
mation extraction and retrieval tools, bioscience re-
searchers can use the contents of the literature to
further their research goals. With online full text
?Current address: Department of Bioengineering, Univer-
sity of California, San Diego, La Jolla, CA 92093-0412. Email:
sariel@ucsd.edu.
of journal articles finally becoming the norm, new
forms of citation analysis become possible.
Nearly every statement in biology articles is
backed up by at least one citation, and, conversely,
it is quite common for papers in the bioscience do-
main to be cited by 30?100 other papers. The cited
facts are typically stated in a more concise way in the
citing papers than in the original papers. Since the
same facts are repeatedly stated in different ways in
different papers, statistical models can be trained on
existing citation sentences to identify similar facts in
unseen text. Citation sentences also have the poten-
tial to be useful for text summarization and database
curation. Figure 1 shows an example of three differ-
ent citation sentences to the same target paper.
Most citation analysis work focuses on the cita-
tion network structure, to determine which papers
are most central, or uses co-citation analysis to de-
termine which papers are similar to one another in
content (White, 2004; Liu, 1993; Garfield, 1955;
Lipetz, 1965; Giles et al, 1998). In this paper we
focus instead on analyzing the sentences that sur-
round the citations to related work, which we termed
citances in Nakov et al (2004). In that paper we
note that one subproblem for using citances for au-
tomated analysis is to identify the different concepts
mentioned; a given paper may be cited for more than
one fact or relation.
Citances often state similar information using
varying words and phrases. In order to build con-
cise summaries, those entities and relations that are
expressed in different ways should be matched up,
or aligned, so that subsequent processing steps will
know what the core concepts are. In this paper we
847
Exam
ple o
f una
ligne
d cit
ance
s
?In r
espo
nse t
o ge
noto
xics
tress
, Ch
k1 a
nd C
hk2 
phos
phor
ylate
Cdc
25A
 on N
-term
inal 
sites
 and
 targ
et it 
rapid
ly fo
r ubi
quiti
n-de
pend
ent d
egra
datio
n (M
ailan
det 
al, 2
000,
 200
2; 
Mol
inari
et al
, 200
0 ; Fa
lcke
t al, 
2001
; Shi
muta
et al
, 200
2; B
usin
oet 
al, 2
003)
, wh
ich i
s 
thou
ght t
o be
 cen
tral t
o the
 S an
d G2
 cell
 cyc
le ch
eckp
oints
 (Ba
rtek
and 
Luk
as, 2
003;
 
Don
zelli
and 
Drae
tta, 2
003 )
.?
?Giv
en th
at C
hk1 
prom
otes
 Cdc
25A
 turn
over
 in r
espo
nse t
o DN
Ada
mag
e in 
vivo
 
(Fal
cke
t al. 
2001
; Sor
ense
n et 
al. 2
003)
 and
that 
Chk
1 is 
requ
ired 
for C
dc25
A 
ubiq
uitin
ation
by S
CF?-T
RCP
 in v
itro,
 we 
expl
ored
 the 
role 
of C
dc25
A 
phos
phor
ylati
onin
the u
biqu
itina
tion
proc
ess.?
?Sin
ce ac
tivat
ed p
hosp
hory
lated
Chk
2-T68
is in
volv
ed in
 pho
spho
rylat
iona
nd 
degr
adat
ion o
f Cd
c25A
 (Fal
cke
t al.,
 200
1 , Fa
lcke
t al.,
2002
; Ba
rtek
and 
Luk
as, 
2003
), we
 also
 exa
mine
d the
 leve
ls of
Cdc
25A
 in 2
fTG
H an
d U3
A ce
lls e
xpos
ed to
 
?-IR.?
Figure 1: Example of three unaligned citances.
Alig
nme
nt af
ter n
orm
aliza
tion
resp
onse
geno
toxi
cstr
essC
hk1
 Chk
2ph
osph
oryl
ateC
dc25
AN
 term
inal
 site
s tar
get 
rapi
dly u
biqu
itin
depe
nden
tdeg
rada
tion
thou
ght 
cent
ral S
 G2 
cell 
cycl
e ch
eckp
oint
s
Give
n Ch
k1p
rom
otes
 Cdc
25A
turn
over
resp
onse
DNA
 dam
age
vivo
 Chk
1re
quir
ed
Cdc
25A
ubiq
uitin
atio
nSC
F be
ta T
RCP
 vitr
o ex
plor
edr
ole C
dc25
Aph
osph
oryl
atio
n
ubiq
uitin
atio
npr
oces
s
activ
ated
 pho
spho
ryla
tedC
hk2
T68
invo
lved
 pho
spho
ryla
tion
degr
adat
ionC
dc25
A
exam
ined
leve
ls C
dc25
A2f
TGH
 U3A
 cell
s exp
osed
 gam
ma I
R
Figure 2: Example of three normalized aligned ci-
tances. Homologous entities are colored the same.
Unaligned entities are black.
build on the work of Nakov et al (2004) by tackling
the entity normalization step.
The citance alignment problem is partially anal-
ogous to the problem of multiple alignment of bi-
ological sequences (Durbin et al, 1998). In both
cases the goal is to align homologous entities that
are derived from the same ancestral entity. While in
biology homology is well-defined in the molecular
level, in the citances case it is defined in the seman-
tic level, which is much more subjective. Given a
group of citances that cite the same target paper, we
loosely define semantic homology as a symmetric,
transitive, and reflexive relation between two enti-
ties (words or phrases) in the same or different ci-
tance that have similar semantics in the context of
the cited paper.
Figure 1 shows an example of three citances that
cite the same target paper (Falck et al, 2001). A
multiple alignment of the entities in the same ci-
tances (after removal of stopwords) is shown in Fig-
ure 2. Homologous entities are colored the same.
This small example illustrates some of the main
challenges of multiple citance alignment (MCA).
While orthographic similarity can help to identify
semantic homology (e.g., phosphorylate and phos-
phorylation), it can also be misleading (e.g., cell cy-
cle and U3A cells). In addition, semantic homology
might not include any orthographic clues (e.g., geno-
toxic stress and DNA damage).
Unlike global multiple sequence alignment
(MSA) in genomics, where each character can be
aligned to at most one character in every other se-
quence, in multiple citance alignment, each word
can be aligned to any number of words in other sen-
tences. Another major difference between the two
problems is the fact that while the sequential order-
ing of characters must be maintained in multiple se-
quence alignment, this is not the case for multiple
citance alignment.
MCA is also related to the problem of word align-
ment in statistical machine translation (SMT) (Och
and Ney, 2003). However, unlike SMT alignment,
MCA aligns multiple citances in the same language
rather than a pair of sentences in different languages.
In this paper we present an MCA algorithm that
is based on an extension to the posterior decoding
algorithm for MSA called AMAP (Schwartz et al,
2006; Schwartz and Pachter, 2007), with an under-
lying pairwise alignment model based on the CRF
SMT alignment of Blunsom & Cohn (2006).
2 Multiple citance alignments
Let G , {C1, C2, . . . , CK} be a group of K ci-
tances that cite the same target paper, where the ith
citance is a sequence of words Ci , Ci1C
i
2 ? ? ?C
i
ni ,
and ci , {ci1, c
i
2, . . . , c
i
ni} is the set of word indices
of Ci. A pairwise citance alignment of Ci and Cj
is an equivalence (symmetric, reflexive, and transi-
tive) relation ?ij on the set ci ? cj . The expres-
sion cik ?ij c
j
l means that according to the pairwise
alignment?ij word k in citance Ci and word l in ci-
tance Cj are aligned. A multiple citance alignment
(MCA) is an equivalence relation ?,
(?
ij ?ij
)+
on the set
?
i c
i, which is the transitive closure of
the union of all pairwise alignments of citance pairs
in G. Taking the transitive closure and not only
the union of all pairwise alignments ensures that the
MCA is an equivalence relation.
An MCA ? defines a partition of the set of all
word indices c ,
?
ik {c
i
k}, which is of size n ,
848
|c| =
?
i n
i. Therefore, the number of distinct
MCAs of G is the number of partitions of a set of
size n. This number is called the nth Bell number
(Rota, 1964)
Bn ,
1
e
??
k=0
kn
k!
. (1)
Asymptotically,Bn grows faster than an exponential
but slower than a factorial. For example B100 ?
10116. Obviously, enumerating all possible MCAs
is impractical even for small problems.
3 Probabilistic model for MCA
Unlike biological sequences, for which pair-HMMs
are a natural choice for modeling evolutionary pro-
cesses between two sequences, there is no simple
generative model that can be used for modeling
pairwise citance alignment. Most of the work on
pairwise alignment of sentences at the word level
has been done in the statistical machine translation
(SMT) community.
Och & Ney (2003) present an overview and com-
parison of the most common models used for SMT
word alignments. Out of the models they describe,
the HMM models are the most expressive mod-
els that can compute posterior probabilities using
the forward-backward algorithm. However, unlike
sequence alignments, there are no ordering con-
straints in word alignments, and the alignments are
many-to-many as opposed to one-to-one. Therefore,
the SMT HMM models cannot be based on pair-
HMMs, which generate two sentences simultane-
ously. Rather, they are directional models that model
the probability of generating a target sentence given
a source sentence. In other words they only model
many-to-one alignments, recovering the many-to-
many alignments in a preprocessing step. Therefore,
SMT HMMs can only compute the posterior proba-
bilities P (cik ; c
j
l |C
i, Cj) and P (cjl ; c
i
k|C
i, Cj),
where the relation ; represents the (directional)
event that a source word is translated into a target
word. Nevertheless, recently such posterior proba-
bilities have been used in SMT word alignment sys-
tem as an alternative to Viterbi decoding, and helped
to improve the performance of such systems (Ma-
tusov et al, 2004; Liang et al, 2006).
Generative models like HMMs have several lim-
itations. First, they require relatively large train-
ing data, which is difficult to attain in case of SMT
word alignment, and even more so in the case of
MCA. Second, generative models explicitly model
the inter-dependence of different features, which re-
duces the ability to incorporate multiple arbitrary
features into the model. Since orthographic similar-
ity is not a strong enough indication for semantic ho-
mology in MCA, we would like to be able to incor-
porate multiple inter-dependent features into a single
model, including orthographic, contextual, ontolog-
ical, and lexical features.
Recently, several authors have described dis-
criminative SMT alignment models (Moore, 2005;
Lacoste-Julien et al, 2006; Blunsom and Cohn,
2006). However, to the best of our knowledge only
the model of Blunsom & Cohn (2006), which is
based on a Conditional Random Field (CRF) (Laf-
ferty et al, 2001), can compute word indices pairs?
directional posterior probabilities, like those com-
puted by the HMM models. Therefore, we decided
to adopt the CRF-based model to the MCA problem.
3.1 Conditional random fields for word
alignment
The model of Blunsom & Cohn (2006) is based on
a linear chain CRF, which can be viewed as the
undirected version of an HMM. The CRF models
a many-to-one pairwise alignment, in which every
source word can get algned to zero or one target
words, but every word in the target sentence can be
the target of multiple source words. CRFs define
a conditional distribution over a latent labeling se-
quence given observation sequence(s). In the case
of CRF for word alignment, the observed sequences
are the source and target sentences (citances), and
the latent labeling sequence is the mapping of source
words to target word-indices. Given a source citance
Ci of length ni, and a target citance Cj of length nj ,
the many-to-one alignment of Ci to Cj is the rela-
tion ;. Since this is a many-to-one alignment, ;
can be represented by a vector a of length ni. The
CRF models the probability of the alignment a con-
ditioned on Ci and Cj as follows:
P?(a|C
i, Cj) =
exp
(?
t
?
k ?kfk(t, at?1, at, C
i, Cj)
)
Z?(Ci, Cj)
, (2)
849
where f , {fk} are the model?s features, ? , {?k}
are the features? weights, and Z?(Ci, Cj) is the par-
tition (normalization) function which is defined as:
Z?(C
i, Cj) ,
?
a
exp
(
?
t
?
k
?kfk(t, at?1, at, C
i, Cj)
)
.
(3)
Parameters are estimated from fully observed data
(manually aligned citances) using a maximum a pos-
teriori estimate. The parameter estimation proce-
dure is described in more details in the original pa-
per. Blunsom & Cohn (2006) use Viterbi decoding
to find an alignment of two sentences given a trained
CRF model, a? , argmaxa P?(a|C
i, Cj). How-
ever, the posterior probabilities of the labels at each
position can be calculated as well using the forward-
backward algorithm:
P?(c
i
l ; c
j
k|C
i, Cj) = P?(al = c
j
k|C
i, Cj) =
?l(c
j
k|C
i, Cj)?l(c
j
k|C
i, Cj)
Z?(Ci, Cj)
(4)
where ?l and ?l are the forward and backward vec-
tors that are computed with the forward-backward
algorithm (Lafferty et al, 2001).
3.2 The posterior decoding algorithm for MCA
Ultimately, the success of an MCA algorithm should
be judged by its effect on the success of the citance
analysis systems that use MCAs as their input. How-
ever, measuring this effect directly is difficult, since
high-level tasks such as summarization are difficult
to evaluate objectively. More to the point, it is dif-
ficult to quantify the contribution of the MCA ac-
curacy to the accuracy of the high-level system that
uses it. A more practical alternative is to measure the
accuracy of MCAs directly using a meaningful ac-
curacy measure, under the simplifying assumption
that there is a strong correlation between the mea-
sured MCA accuracy and the performance of the
high-level application.
We argue that a useful utility function should be
correlated (or even identical) to the accuracy mea-
sure used to evaluate the performance of an algo-
rithm. In addition, the utility function should be
easily decomposable, to enable direct optimization
using posterior-decoding. Although any accuracy
measure that is acceptable as a single performance
measure can be used to guide the design of the util-
ity function, metric-based accuracy measures have
several noticeable advantages. First, a metric for-
malizes the intuitive notion of distance. Hence, an
accuracy measure which is based on a metric fol-
lows the intuition that reducing the distance to the
correct answer should increase the accuracy of the
predicted answer. Therefore, defining a metric space
for the objects of a given problem leads to a nat-
ural definition of accuracy. Another advantage of
using a metric-based accuracy measure is the abil-
ity to provide bounds in the search space using the
triangle inequality. For example, while searching for
the answer with the optimal (metric-based) expected
utility, a step of length x can only change the ex-
pected utility as well as the actual utility by at most
?x units. Examples of more complex bounds us-
ing metric loss functions are described in (Schlu?ter
et al, 2005) and (Domingos, 2000).
Schwartz et al (2006) define the alignment met-
ric accuracy (AMA), which is a utility function for
one-to-one MSA. Intuitively, AMA measures the
fraction of characters that are aligned correctly ac-
cording to the reference alignment, either to another
character or to a gap (null). We extend the definition
of AMA to the case of many-to-many MCA.
A good utility function for MCA should give par-
tial credit to word positions that align to some of the
correct word positions while penalizing for aligning
to wrong word positions. To help define such a util-
ity function we define the following. Let mij?(c
j
l ) ,
{cik ? c
i|cik ? c
j
l } be the set of all word positions
in citance Ci that align to word position l in citance
Cj according to MCA ?. We can then define the
following utility function for the MCA ?p of the ci-
tance group G given a reference MCA ?r:
UAMA(?
r,?p) ,
?
ijl|i6=j Uset agreement
(
mij?r(c
j
l ),m
ij
?p(c
j
l )
)
n(K ? 1)
, (5)
where n is the number of word indices in G, K ,
|G| is the number of citances in the group, and
Uset agreement is any utility function for agreement
between sets that assigns values in the range [0, 1].
850
Uset agreement can be viewed as a ?score? assigned
to each word position based on the agreement be-
tween the two alignments with regards to the other
word positions that align to it. Using a 0?1 loss as
the set agreement score is equivalent to the original
AMA. Other utility functions, such as Dice, Jaccard
and Hamming can be used as Uset agreement. How-
ever, only metric-based utility functions will result
in a metric-based UAMA utility function. It is easy
to see that 1 ? UAMA satisfies all the requirements
of a metric, i.e., it is non-negative, equals zero if
and only if ?r=?p, symmetric, and obeys the trian-
gle inequality, since if the triangle inequality holds
for 1 ? Uset agreement, it must hold for a sum of
1 ? Uset agreement values. (We refer the reader to
Schwartz (2007) for a longer discussion of the prop-
erties of the different utility functions.) We define
the AMA for MCA by setting the Uset agreement to
be the Braun-Blanquet coefficient (Braun-Blanquet,
1932), which is defined as:
UBraun?Blanquet
(
mij?r(c
j
l ),m
ij
?p(c
j
l )
)
,
?
???
???
1 if mij?r(c
j
l ) = ?
and mij?p(c
j
l ) = ?
|mij?r (c
j
l )?m
ij
?p (c
j
l )|
max{|mij?r (c
j
l )|,|m
ij
?p (c
j
l )|}
otherwise
.
(6)
Caillez & Kuntz (1996) show that the Braun-
Blanquet coefficient is based on a metric.
As with the MSA case, a family of utility func-
tions can be defined to enable control of the re-
call/precision trade-off. Unlike MSA, in the case of
MCA two free parameters are needed, in order to
have better control of the trade-off using posterior-
decoding. In addition to a gap-factor that controls
the threshold at which unaligned words start to get
aligned, a match-factor is added to enable control of
the number of word-positions each word aligns to.
The result is the following utility function:
U?,?(?
r,?p) ,
1
n(K ? 1)
?
ijl|i6=j
(
?|m
ij
?p (c
j
l )|
|mij?r(c
j
l ) ?m
ij
?p(c
j
l )|
max
{
|mij?r(c
j
l )|, |m
ij
?p(c
j
l )|, 1
}+
?1{mij?r(c
j
l ) = m
ij
?p(c
j
l ) = ?}
)
, (7)
where ? ? [0,?) is a gap-factor, and ? ? (0,?) is
a match factor. The neutral value for both parame-
ters is 1. Increasing ? results in increased utility to
sparser MCAs, while reducing ? increases the util-
ity of denser alignments. However, in the case of
MCA, the gap-factor only affects the first aligned
word position, but it cannot affect the number of
word positions each word is aligned to. The match-
factor adds this functionality by rewarding MCAs
that align words to multiple word positions when
? > 1, and penalizing such MCAs when ? < 1.
Given a group of K citances G and a trained
CRF model, the goal of the MCA algorithm is to
find the MCA ??, argmax?p E?tU?,?(?
t,?p)
that maximizes the expected utility. Since search-
ing the space of possible MCAs exhaustively is in-
feasible, we resort to a simple heuristic for predict-
ing an MCA. Instead of searching for a global opti-
mum, the predicted MCA is defined as the equiva-
lence (symmetric transitive) closure of the union of
multiple local optima. For each target word posi-
tion cjl and every source citance C
i the combina-
tion of source word positions ci? that maximize the
expected set-agreement score of cjl is added to the
predicted MCA. Formally, let P(ci) be the power-
set of ci, then we define the predicted MCA as
?p,
(
;p ?(;p)?1
)+
, where;p is defined as:
;p,
?
ijl|i6=j
{cjl } ? argmax
ci??P(ci)
Emij
?t
(cjl )
?
??|c
i
?|
|mij?t(c
j
l ) ? c
i
?|
max
{
|mij?t(c
j
l )|, |c
i
?|, 1
}+
?1{mij?t(c
j
l ) = c
i
? = ?}
)
. (8)
The value of ;p can be computed from the CRF
directional posterior probabilities as follows:
;p=
?
ijl|i6=j
{cjl } ? argmax
ci??P(ci)
?
ci??P(ci)
P
(
mij?t(c
j
l ) = c
i
?
)
(
?|c
i
?|
|ci? ? c
i
?|
max {|ci?|, |ci?|, 1}
+ ?1{ci? = c
i
? = ?}
)
,
(9)
851
and using an independence assumption we get:
;p?
?
ijl|i6=j
{cjl } ? argmax
ci??P(ci)
?
ci??P(ci)
?
?
?
cik
(
P?(c
i
k ; c
j
l |C
i, Cj)1{cik ? c
i
?}+
(1? P?(c
i
k ; c
j
l |C
i, Cj))1{cik /? c
i
?}
)
)
(
?|c
i
?|
|ci? ? c
i
?|
max {|ci?|, |ci?|, 1}
+ ?1{ci? = c
i
? = ?}
)
.
(10)
Note that although the directional posterior prob-
abilities are used to generate the predicted MCA,
the result is a many-to-many alignment, since the
union is done over all pairs of sequences in both
directions. The calculation in Equation (10) can
be computationally intensive in practice, as it re-
quires |P(ci)|2 = 22n
i
operations for each word
position cjl and citance C
i. This can be over-
come by restricting the combinations of source word
positions (ci? and c
i
?) to include only the the top
MAX SOURCES source words with a minimum
posterior probability of MIN PROB to align to cjl
(P?(cik ; c
j
l |C
i, Cj) ? MIN PROB). In our im-
plementation we set MAX SOURCES to 8 and
MIN PROB to 0.01. Additionally, the probabilities
of each combination ci? can be calculated only once,
since it is independent of ci?. This reduces the to-
tal computational complexity of calculating ?p to
O
(
216(K2 ?K)maxni
{
ni
})
.
4 Data sets
Since citance alignment is a new task, we had to
create our own evaluation and training sets. We re-
stricted the domain of the target papers to molec-
ular interactions, a domain which is actively re-
searched in the biosciences text mining community
(Hirschman et al, 2002). The biologist in our group
annotated citances to 6 target papers. The training
set consisted of 40 citances to 4 different target pa-
pers (10 citances each; we wanted to have variety in
the training set). The development set consisted of
51 citances to the fifth target paper, and the test set
contained 45 citances to the sixth target paper.
For each target paper we downloaded the full text
of those papers citing it that were available in HTML
format. The link structure of the cited references in
the HTML documents allowed us to automatically
extract citances to a given target paper. We defined a
citance to be the full sentence that contains a citation
to the target paper. Each citance was then tokenized,
and normalized by removing all stopwords from a
predefined list.
One goal of the annotation was to cover as much
of the content of the citances as possible. Another
goal was consistency; our biologist manually fol-
lowed a small number of rules to determine seman-
tic similarity. Distinct semantic units (words or
phrases) were identified and given an annotation ID.
Within each group of citances, words or phrases that
share semantic similarity were annotated with the
same ID.
Using the manually annotated citance groups,
pairwise word alignments were generated for every
source-target pair of citances from every group. That
resulted in a training, development, and test sets
of 180, 1275, and 990 pairwise alignments respec-
tively.
Alignments that were used for development and
testing were generated as many-to-many alignments.
However, many-to-many alignments are not suit-
able for the training the many-to-one CRF align-
ment model. When a given source word cik aligns
to multiple words in the target citance, the CRF
model chooses only one target word as a true pos-
itive, while incorrectly treating the other true posi-
tive target words as true negatives. To alleviate this
problem, in such cases we replaced all true-positive
target words other than the first with ?*?, thus forcing
them to act as real true negatives for the purpose of
training. This adjustment does not solve the inher-
ent limitation of the CRF?s many-to-one modeling of
a many-to-many alignment, but it prevents learning
incorrect weights for good features.
5 Feature engineering
The CRF alignment model can combine multiple
overlapping features. We evaluated the effectiveness
of different features by training models on the train-
ing set and evaluating their performance on the de-
velopment set. We considered variations of features
852
that were part of the original system of Blunsom &
Cohn (2006), and also designed new features that are
specific to the problem of MCA.
Orthographic features
We used the following orthographic features from
the original system of Blunsom & Cohn (2006) (be-
low all features are either Boolean indicator func-
tions (b) or real valued (r)):
(b) exact string similarity of source-target words;
(b) every possible source-target pair of length 3 prefixes;
(b) exact string match of length 3 prefixes;
(b) exact string match of length 3 suffixes;
(r) absolute difference in word lengths;
(b) both words are shorter than 4 characters.
In addition, the following orthographic features
were added: indicator that both words include capi-
tal letters, and normalized edit-similarity of the two
words (1?
edit distance(cik,c
j
l )
max{|cik|,|c
j
l |}
).
Markov features
We used the following Markov features from the
original system:
(r) absolute jump width (abs(at?at?1?1), which measures
the distance between the target words of adjacent source
words;
(r) positive jump width (max{at ? at?1 ? 1, 0});
(r) negative jump width (max{at?1 + 1? at, 0});
(b) transition from null aligned source-word to non-null
aligned source-word;
(b) transition from non-null aligned source-word to null
aligned source-word;
(b) transition from null aligned source-word to null aligned
source-word.
In addition we added the following Markov fea-
tures in order to model the tendency of certain words
to be part of longer phrases:
(b) source-word aligns to the same target-word as the previ-
ous source-word;
(b) source-word aligns to the same target-word as the next
source-word;
(b) transition from non-null aligned source-word to non-null
aligned source-word.
Sentence position: We included the relative sen-
tence position feature from the original system,
which is defined as abs( at|cj | ?
t
ci ). Although it was
not expected to be relevant for MCA, since the ci-
tances are not expected to align along the diagonal,
this feature slightly improved the performance of the
development set.
Null: An indicator function for leaving a source-
word unaligned was retained from the original sys-
tem. This is an essential feature since without it the
CRF tends to over-align words, and produces mean-
ingless posterior probabilities.
Ontological features: Orthographic and posi-
tional features alone do not cover all cases of se-
mantic homology. We therefore included features
that are based on domain specific ontologies.
Using an automated script we mapped specific
words and phrases in every citance to MeSH1 terms,
Gene identifiers from Entrez Gene,2 UniProt,3 and
OMIM.4 We then added features indicating when
the source and target words are annotated with the
same MeSH term or the same gene identifier. We
tried numerous features that compare MeSH terms
based on their distance in the ontology, and other
features that indicate whether a word is part of a
longer term. However, none of these feature were
selected for the final system.
In addition to biological ontologies we added a
feature for semantic word similarity between the
source and target words, based on the Lin (1998)
WordNet similarity measure.
6 Results
We modified the CRF alignment system of Blun-
som & Cohn (2006) to support MCA by incorpo-
rating the posterior decoding algorithm from Sec-
tion 3.2 into the existing system. The CRF model
was trained using the features that were selected us-
ing the development set, on a dataset that included
the training and development MCAs. All the perfor-
mance results in this section are reported on the test
set, which includes 990 pairs of citances (45?44/2),
with a total of 34188 words (8547 ? 44). On aver-
age, 20% of the source words are aligned to at least
one other target word in a given reference pairwise
alignment. Since the union of all the pairwise align-
ments results in only a single test MCA, it is hard
to make strong arguments about the performance
1http://www.nlm.nih.gov/mesh/
2http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=gene
3http://www.pir.uniprot.org/
4http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=OMIM
853
00.10.20.30.40.50.60.70.80.91 0.
3
0.4
0.5
0.6
0.7
0.8
Reca
ll
Precision
Viterb
i_Inte
rsect
ion
Viterb
i_Uni
on
Poste
rior d
ecod
ing
Figure 3: Recall/Precision curve of pairwise ci-
tance alignments comparing Viterbi to posterior
decoding.
of the system in general. Therefore, we concen-
trate our discussion on general trends, and do not
claim that the specific performance numbers we re-
port here are statistically significant. As a point of
comparison, the SMT community has been evalu-
ating performance of word-alignment systems on an
even smaller dataset of 447 pairs of non-overlapping
sentences (Mihalcea and Pedersen, 2003).
We first analyze the performance of the system
on pairwise citance alignments. Instead of tak-
ing the equivalence closure of ;p we take only
the symmetric closure. The result is 990 many-
to-many pairwise alignments. In order to evalu-
ate the effectiveness of the posterior-decoding al-
gorithm, we generate the Viterbi alignments using
the same CRF model. The Viterbi many-to-many
pairwise alignments are then generated by combin-
ing equivalent pairs of many-to-one alignments us-
ing three different standard symmetrization methods
for word-alignment?union, intersection, and the re-
fined method of Och & Ney (2003).
Figure 3 shows the recall/precision trade-off of
the pairwise posterior-decoding and Viterbi align-
ments. The curve for the posterior-decoding align-
ments was produced by varying the gap and match
factors. For the Viterbi alignments, only three re-
sults could be generated (one for each symmetriza-
tion method). However, since the refined method
produced a very similar result to the union, only
the union is displayed in the figure. The impor-
tant observation is that while posterior-decoding en-
ables refined control over the recall/precision trade-
off, the Viterbi decoding generates only three align-
ments, which cover only a small fraction of the curve
at its high precision range. The union of Viterbi
alignments achieves 0.531 recall at 0.913 preci-
sion, which is similar result to the 0.540 recall at
0.909 precision achieved using posterior-decoding
with gap-factor and match-factor set to 1. However,
unlike Viterbi, posterior-decoding produces align-
ments with much higher recall levels, by increas-
ing the match-factor and decreasing the gap-factor.
For example setting the gap-factor to 0.1 and match-
factor to 1.2 results in alignments with 0.636 recall
at 0.517 precision, and setting them to 0.05 and 1.5
results in 0.742 recall at 0.198 precision. Generally,
the gap and match factor affect the accuracy of the
alignments as expected. In particular, the alignments
with the best AMA (0.889) and the best F1-measure
(0.678) are generated when the gap match factor are
set to their natural values (1,1), which theoretically
should maximize the expected AMA.
The performance of the pairwise alignments
validates the underlying probabilistic model,
showing it behaves as theoretically expected.
However, the union of all pairwise alignments
is not a valid MCA. To evaluate the MCA
posterior decoding algorithm, we compared it
to baseline MCAs. The baseline MCAs are
constructed by using only the normalized-edit-
distance
edit distance(cik,c
j
l )
max{|cik|,|c
j
l |}
, and defining cik ;
?
cjl if and only if normalized edit distance(c
i
k, c
j
l ) ?
?, where ? is a distance threshold. The fi-
nal baseline MCA is constructed by taking the
equivalence closure of all pairwise alignments,
??,
(
;? ?(;?)?1)
)+
. The ? parameter can
be used to control the recall/precision trade-off,
since increasing it adds more position-pairs to the
alignment, thus increasing recall, while decreasing
it increases precision.
Figures 4 compares the performance of the CRF
posterior-decoding MCAs with the baseline MCAs.
The different MCAs were produced by varying the
gap and match factors in the case of the posterior-
decoding, and ? for the baseline MCAs. The CRF
curve clearly dominates the baseline curve. How-
ever, they do overlap in range between 0.52 and
0.55 recall (0.84 and 0.90 precision). This is prob-
854
00.10.20.30.40.50.60.70.80.91 0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Reca
ll
Precision
CRF
Base
line
Figure 4: Recall/Precision curve of MCAs
comparing CRF with posterior decoding to
normalized-edit-distance baseline.
ably a range in which for this particular MCA the
orthographic similarity is the most dominant fea-
ture. While the baseline curve drops sharply after
that range, the posterior-decoding curve keeps im-
proving recall up to 0.636 at 0.748 precision, be-
fore a major drop in precision. The additional recall
is due to the ability of the CRF model to incorpo-
rate multiple overlapping features. In particular, the
domain-specific features are important for aligning
words and phrases that have little or no orthographic
similarity. At the other end of the overlap range, the
posterior-decoding achieves better precision than the
baseline for the same recall levels. For example, the
posterior decoding gets 0.381 recall at 0.982 preci-
sion compared with 0.346 at 0.937 for the baseline.
Unlike the pairwise alignment case, the neutral
settings of the gap and match factors did not result in
the best AMA score. This is due to the equivalence
closure heuristic that results in MCAs that are too
dense, since a single link between two equivalence
classes causes them to merge. The best AMA score
(0.886) is obtained by reducing the gap-factor to 0.5
and match-factor to 0.45, in order to compensate for
the effect of the equivalence closure heuristic. For
comparison, the best F1-measure (0.690) is achieved
by setting the gap and match factors to 0.75.
An error analysis on the latter MCA shows that
out of 1400 unique errors, 1194 (85.3%) are false
negatives (FN) and 206 (14.7%) false positives (FP).
Most errors (more than 600) are due to misalign-
ment of subtypes (e.g., cdc, cdc6, cdc25A), oppo-
sites (e.g., phosphorylated and unphosphorylated)
and complex entities (e.g., cell cycle v.s. cell line).
In addition, a large portion of FN errors are due to
not aligning entities that belong to just four equiva-
lence classes (e.g., 97 FN errors caused by terms in
the class of motif, site and domain). Other types of
errors include not aligning plural and singular forms
of the same entities, aligning only part of multi-
word entities, and incorrectly aligning orthographi-
cally similar entities that belong to different classes.
7 Conclusions
We have shown how to derive a posterior-decoding
algorithm that aims at maximizing the expected util-
ity for the MCA problem, as a substitute for the
sequence-annealing algorithm for MSA. Adding a
gap and match factor to the utility function allows
control over the recall/precision trade-off when us-
ing posterior-decoding. Another advantage of opti-
mizing the expected utility with posterior-decoding
methods is the decoupling from the probabilistic
model that generated the posterior probabilities.
This allows the use of CRFs instead of HMMs with
a similar posterior decoding algorithm.
Our experiments were limited by the size of the
labeled data. However, the results support the the-
oretical predictions, and demonstrate the advantage
of posterior-decoding over Viterbi decoding.
Since citances are still a relatively unexplored re-
source, it is still unclear whether the formulation we
presented here for citance alignment is the most use-
ful for applications that use citances for compara-
tive analysis of bioscience text. Unlike biological
sequence alignment, citance alignments are much
more subjective, as they depend on a loose defini-
tion of semantic homology between entities. Even
the definition of the basic entities can vary, since in
many cases noun-compounds and other multi-word
entities seem to be a more natural choice for basic el-
ements of semantic homology and alignment. How-
ever, automatic segmentation and entity recognition
are still difficult tasks in the bioscience text domain
and so new methods are worth investigating.
Acknowledgements: We thank Phil Blunsom and
Trevor Cohn for sharing their CRF-based word
alignment code. This research was supported in part
by NSF DBI 0317510.
855
References
Phil Blunsom and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 65?72, Sydney, Australia, July. Association for
Computational Linguistics.
Josias Braun-Blanquet. 1932. Plant sociology: the study
of plant communities. McGraw-Hill, New York.
Francis Caillez and Pascale Kuntz. 1996. A contribution
to the study of the metric and euclidean structures of
dissimilarities. Psychometrika, 61(2):241?253.
Pedros Domingos. 2000. A unified bias-variance de-
composition and its applications. In Proceedings
of the Seventeenth International Conference on Ma-
chine Learning, pages 231?238, Stanford, CA. Mor-
gan Kaufmann.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological sequence analy-
sis. Probablistic models of proteins and nucleic acids.
Cambridge University Press.
Jacob Falck, Niels Mailand, Randi G. Syljuasen, Jiri
Bartek, and Jiri Lukas. 2001. The ATM-Chk2-
Cdc25A checkpoint pathway guards against radiore-
sistant DNA synthesis. Nature, 410(6830):842?847.
Eugene Garfield. 1955. Citation indexes for science: A
new dimension in documentation through association
of ideas. Science, 122(3159):108?111.
C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence.
1998. Citeseer: an automatic citation indexing sys-
tem. In Proceedings of the third ACM conference on
Digital libraries, pages 89?98. ACM Press.
Lynette Hirschman, Jong C. Park, Junichi Tsujii, Lim-
soon Wong, and Cathy H. Wu. 2002. Accomplish-
ments and challenges in literature data mining for bi-
ology. Bioinformatics, 18(12):1553?1561.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I. Jordan. 2006. Word alignment via
quadratic assignment. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 112?119, New York City, USA,
June. Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June. Association for Computational Linguistics.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proc. 15th International Conf. on Ma-
chine Learning, pages 296?304. Morgan Kaufmann,
San Francisco, CA.
Ben-Ami Lipetz. 1965. Improvements of the selectiv-
ity of citation indexes to science literature through in-
clusion of citation relationship indicators. American
Documentation, 16:81?90.
Mengxiong Liu. 1993. Progress in documentation. the
complexities of citation practice: A review of citation
studies. Journal of Documentation, 49(4):370?408.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical ma-
chine translation. In COLING ?04: Proceedings of the
20th international conference on Computational Lin-
guistics, page 219, Morristown, NJ, USA. Association
for Computational Linguistics.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Rada Mihalcea and
Ted Pedersen, editors, HLT-NAACL 2003 Workshop:
Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, pages 1?10, Edmonton,
Alberta, Canada, May 31. Association for Computa-
tional Linguistics.
Robert C. Moore. 2005. A discriminative framework for
bilingual word alignment. In HLT/EMNLP, pages 81?
88.
Preslav I. Nakov, Ariel S. Schwartz, and Marti A. Hearst.
2004. Citances: Citation sentences for semantic anal-
ysis of bioscience text. In SIGIR?04 Workshop on
Search and Discovery in Bioinformatics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Gian-Carlo Rota. 1964. The number of partitions of a
set. The American Mathematical Monthly, 71(5):498?
504, may.
Ralf Schlu?ter, Thomas Scharrenbach, Volker Steinbiss,
and Hermann Ney. 2005. Bayes risk minimization
using metric loss functions. In Proceedings of the
European Conference on Speech Communication and
Technology, Interspeech, pages 1449?1452, Portugal,
September.
Ariel S. Schwartz and Lior Pachter. 2007. Multiple
alignment by sequence annealing. Bioinformatics,
23(2):e24?29.
856
Ariel S. Schwartz, Eugene W. Myers, and Lior Pachter.
2006. Alignment metric accuracy. arXiv:q-
bio.QM/0510052.
Ariel S. Schwartz. 2007. Posterior Decoding Meth-
ods for Optimization and Accuracy Control of Multiple
Alignments. Ph.D. thesis, EECS Department, Univer-
sity of California, Berkeley.
Howard D. White. 2004. Citation analysis and discourse
analysis revisited. Applied Linguistics, 25(1):89?116.
857
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 732?739, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Multi-way Relation Classification:
Application to Protein-Protein Interactions
Barbara Rosario
SIMS
UC Berkeley
Berkeley, CA 94720
rosario@sims.berkeley.edu
Marti A. Hearst
SIMS
UC Berkeley
Berkeley, CA 94720
hearst@sims.berkeley.edu
Abstract
We address the problem of multi-way re-
lation classification, applied to identifica-
tion of the interactions between proteins
in bioscience text. A major impediment
to such work is the acquisition of appro-
priately labeled training data; for our ex-
periments we have identified a database
that serves as a proxy for training data.
We use two graphical models and a neu-
ral net for the classification of the inter-
actions, achieving an accuracy of 64%
for a 10-way distinction between relation
types. We also provide evidence that the
exploitation of the sentences surrounding
a citation to a paper can yield higher accu-
racy than other sentences.
1 Introduction
Identifying the interactions between proteins is one
of the most important challenges in modern ge-
nomics, with applications throughout cell biology,
including expression analysis, signaling, and ratio-
nal drug design. Most biomedical research and
new discoveries are available electronically but only
in free text format, so automatic mechanisms are
needed to convert text into more structured forms.
The goal of this paper is to address this difficult
and important task, the extraction of the interactions
between proteins from free text. We use graphical
models and a neural net that were found to achieve
high accuracy in the related task of extracting the re-
lation types might hold between the entities ?treat-
ment? and ?disease? (Rosario and Hearst, 2004).
Labeling training and test data is time-consuming
and subjective. Here we report on results using an
existing curated database, the HIV-1 Human Protein
Interaction Database1, to train and test the classifica-
tion system. The accuracies obtained by the classi-
fication models proposed are quite high, confirming
the validity of the approach. We also find support
for the hypothesis that the sentences surrounding ci-
tations are useful for extraction of key information
from technical articles (Nakov et al, 2004).
In the remainder of this paper we discuss related
work, describe the dataset, and show the results of
the algorithm on documents and sentences.
2 Related work
There has been little work in general NLP on trying
to identify different relations between entities. Many
papers that claim to be doing relationship recogni-
tion in actuality address the task of role extraction:
(usually two) entities are identified and the relation-
ship is implied by the co-occurrence of these enti-
ties or by some linguistic expression (Agichtein and
Gravano, 2000; Zelenko et al, 2002).
The ACE competition2 has a relation recognition
subtask, but assumes a particular type of relation
holds between particular entity types (e.g., if the two
entities in question are an EMP and an ORG, then
an employment relation holds between them; which
type of employment relation depends on the type of
entity, e.g., staff person vs partner).
1www.ncbi.nlm.nih.gov/RefSeq/HIVInteractions/index.html
2http://www.itl.nist.gov/iaui/894.01/tests/ace/
732
In the BioNLP literature there have recently
been a number of attempts to automatically extract
protein-protein interactions from PubMed abstracts.
Some approaches simply report that a relation exists
between two proteins but do not determine which
relation holds (Bunescu et al, 2005; Marcotte et al,
2001; Ramani et al, 2005), while most others start
with a list of interaction verbs and label only those
sentences that contain these trigger words (Blaschke
and Valencia, 2002; Blaschke et al, 1999; Rind-
flesch et al, 1999; Thomas et al, 2000; Sekimizu et
al., 1998; Ahmed et al, 2005; Phuong et al, 2003;
Pustejovsky et al, 2002). However, as Marcotte et
al. (2001) note, ?... searches for abstracts contain-
ing relevant keywords, such as interact*, poorly dis-
criminate true hits from abstracts using the words
in alternate senses and miss abstracts using different
language to describe the interactions.?
Most of the existing methods also suffer from low
recall because they use hand-built specialized tem-
plates or patterns (Ono et al, 2001; Corney et al,
2004). Some systems use link grammars in conjunc-
tion with trigger verbs instead of templates (Ahmed
et al, 2005; Phuong et al, 2003). Every paper eval-
uates on a different test set, and so it is quite difficult
to compare systems.
In this paper, we use state-of-the-art machine
learning methods to determine the interaction types
and to extract the proteins involved. We do not use
trigger words, templates, or dictionaries.
3 Data
We use the information from a domain-specific
database to gather labeled data for the task of classi-
fying the interactions between proteins in text. The
manually-curated HIV-1 Human Protein Interaction
Database provides a summary of documented inter-
actions between HIV-1 proteins and host cell pro-
teins, other HIV-1 proteins, or proteins from disease
organisms associated with HIV or AIDS. We use this
database also because it contains information about
the type of interactions, as opposed to other protein
interaction databases (BIND, MINT, DIP, for exam-
ple3) that list the protein pairs interacting, without
3DIP lists only the protein pairs, BIND has only some in-
formation about the method used to provide evidence for the
interaction, and MIND does have interaction type information
but the vast majority of the entries (99.9% of the 47,000 pairs)
Interaction #Triples Interaction #Triples
Interacts with 1115 Complexes with 45
Activates 778 Modulates 43
Stimulates 659 Enhances 41
Binds 647 Stabilizes 34
Upregulates 316 Myristoylated by 34
Imported by 276 Recruits 32
Inhibits 194 Ubiquitinated by 29
Downregulates 124 Synergizes with 28
Regulates 86 Co-localizes with 27
Phosphorylates 81 Suppresses 24
Degrades 73 Competes with 23
Induces 52 Requires 22
Inactivates 51
Table 1: Number of triples for the most common
interactions of the HIV-1 database, after removing
the distinction in directionality and the triples with
more than one interaction.
specifying the type of interactions.
In this database, the definitions of the interactions
depend on the proteins involved and the articles de-
scribing the interactions; thus there are several def-
initions for each interaction type. For the interac-
tion bind and the proteins ANT and Vpr, we find
(among others) the definition ?Interaction of HIV-
1 Vpr with human adenine nucleotide translocator
(ANT) is presumed based on a specific binding in-
teraction between Vpr and rat ANT.?
The database contains 65 types of interactions and
809 proteins for which there is interaction informa-
tion, for a total of 2224 pairs of interacting proteins.
For each documented protein-protein interaction the
database includes information about:
  A pair of proteins (PP),
  The interaction type(s) between them (I), and
  PubMed identification numbers of the journal
article(s) describing the interaction(s) (A).
A protein pair

can have multiple interactions
(for example, AIP1 binds to HIV-1 p6 and also is in-
corporated into it) for an average of 1.9 interactions
per

and a maximum of 23 interactions for the
pair CDK9 and tat p14.
We refer to the combination of a protein pair 
and an article  as a ?triple.? Our goal is to au-
tomatically associate to each triple an interaction
have been assigned the same type of interaction (aggregation).
These databases are all manually curated.
733
type. For the example above, the triple ?AIP1 HIV-
1-p6 14519844? is assigned the interaction binds
(14519844 being the PubMed number of the paper
providing evidence for this interaction)4.
Journal articles can contain evidence for multi-
ple interactions: there are 984 journal articles in the
database and on average each article is reported to
contain evidence for 5.9 triples (with a maximum
number of 90 triples).
In some cases the database reports multiple dif-
ferent interactions for a given triple. There are
5369 unique triples in the database and of these 414
(7.7%) have multiple interactions. We exclude these
triples from our analysis; however, we do include ar-
ticles and  s with multiple interactions. In other
words, we tackle cases such as the example above
of the pair AIP1, HIV-1-p6 (that can both bind and
incorporate) as long as the evidence for the different
interactions is given by two different articles.
Some of the interactions differ only in the direc-
tionality (e.g., regulates and regulated by, inhibits
and inhibited by, etc.); we collapsed these pairs of
related interactions into one5. Table 1 shows the
list of the 25 interactions of the HIV-1 database for
which there are more than 10 triples.
For these interactions and for a random subset of
the protein pairs
 (around 45% of the total pairs
in the database), we downloaded the corresponding
full-text papers. From these, we extracted all and
only those sentences that contain both proteins from
the indicated protein pair. We assigned each of these
sentences the corresponding interaction  from the
database (?papers?).
Nakov et al (2004) argue that the sentences sur-
rounding citations to related work, or citances, are a
useful resource for bioNLP. Building on that work,
we use citances as an additional form of evidence
to determine protein-protein interaction types. For a
given database entry containing PubMed article  ,
4To be precise, there are for this  (as there are often)
multiple articles (three in this case) describing the interaction
binds, thus we have the following three triples to which we
associate binds: ?AIP1 HIV-1-p6 14519844,? ?AIP1 HIV-1-p6
14505570? and ?AIP1 HIV-1-p6 14505569.?
5We collapsed these pairs because the directionality of the
interactions was not always reliable in the database. This im-
plies that for some interactions, we are not able to infer the dif-
ferent roles of the two proteins; we considered only the pair
?prot1 prot2? or ?prot2 prot1,? not both. However, our algo-
rithm can detect which proteins are involved in the interactions.
protein pair

, and interaction type  , we down-
loaded a subset of the papers that cite  . From these
citing papers, we extracted all and only those sen-
tences that mention  explicitly; we further filtered
these to include all and only the sentences that con-
tain

. We labeled each of these sentences with
interaction type  (?citances?).
There are often many different names for the same
protein. We use LocusLink6 protein identification
numbers and synonym names for each protein, and
extract the sentences that contain an exact match for
(some synonym of) each protein. By being conser-
vative with protein name matching, and by not doing
co-reference analysis, we miss many candidate sen-
tences; however this method is very precise.
On average, for ?papers,? we extracted 0.5 sen-
tences per triple (maximum of 79) and 50.6 sen-
tences per interaction (maximum of 119); for ?ci-
tances? we extracted 0.4 sentences per triple (with a
maximum of 105) and 49.2 sentences per interaction
(162 maximum). We required a minimum number
(40) of sentences for each interaction type for both
?papers? and ?citances?; the 10 interactions of Table
2 met this requirement. We used these sentences to
train and test the models described below7.
Since all the sentences extracted from one triple
are assigned the same interaction, we ensured that
sentences from the same triple did not appear in both
the testing and the training sets. Roughly 75% of the
data were used for training and the rest for testing.
As mentioned above the goal is to automatically
associate to each triple an interaction type. The task
tackled here is actually slightly more difficult: given
some sentences extracted from article  , assign to
 an interaction type  and extract the proteins 
involved. In other words, for the purpose of clas-
sification, we act as if we do not have information
about the proteins that interact. However, given the
way the sentence extraction was done, all the sen-
tences extracted from  contain the  .
6LocusLink was recently integrated into En-
trez Gene, a unified query environment for genes
(http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=gene).
7We also looked at larger chunks of text, in particular, we
extracted the sentence containing the  along with the pre-
vious and the following sentences, and the three consecutive
sentences that contained the  (the proteins could appear in
any of the sentences). However, the results obtained by using
these larger chunks were consistently worse.
734
Interaction Papers Citances
Degrades 60 63
Synergizes with 86 101
Stimulates 103 64
Binds 98 324
Inactivates 68 92
Interacts with 62 100
Requires 96 297
Upregulates 119 98
Inhibits 78 84
Suppresses 51 99
Total 821 1322
Table 2: Number of interaction sentences extracted.
f1
Role
f2 f
	
n
   . . . f1
Role
f2 f
	
n
   . . . f
	
1
Role
f2 fn
   . . .
 Inter.
Figure 1: Dynamic graphical model (DM) for pro-
tein interaction classification (and role extraction).
A hand-assessment of the individual sentences
shows that not every sentence that mentions the tar-
get proteins

actually describes the interaction 
(see Section 5.4). Thus the evaluation on the test set
is done at the document level (to determine if the
algorithm can predict the interaction that a curator
would assign to a document as a whole given the
protein pair).
Note that we assume here that the papers that pro-
vide the evidence for the interactions are given ? an
assumption not usually true in practice.
4 Models
For assigning interactions, we used two generative
graphical models and a discriminative model. Fig-
ure 1 shows the generative dynamic model, based
on previous work on role and relation extraction
(Rosario and Hearst, 2004) where the task was to ex-
tract the entities TREATMENT and DISEASE and
the relationships between them. The nodes labeled
?Role? represent the entities (in this case the choices
are PROTEIN and NULL); the children of the role
nodes are the words (which act as features), thus
there are as many role states as there are words in the
sentence; this model consists of a Markov sequence
of states where each state generates one or multiple
observations. This model makes the additional as-
sumption that there is an interaction present in the
sentence (represented by the node ?Inter.?) that gen-
erates the role sequence and the observations. (We
assume here that there is a single interaction for each
sentence.) The ?Role? nodes can be observed or
hidden. The results reported here were obtained us-
ing only the words as features (i.e., in the dynamic
model of Figure 1 there is only one feature node
per role) and with the ?Role? nodes hidden (i.e., we
had no information regarding which proteins were
involved). Inference is performed with the junction
tree algorithm8.
We used a second type of graphical model, a sim-
ple Naive Bayes, in which the node representing the
interaction generates the observable features (all the
words in the sentence). We did not include role in-
formation in this model.
We defined joint probability distributions over
these models, estimated using maximum likelihood
on the training set with a simple absolute discount-
ing smoothing method. We performed 10-fold cross
validation on the training set and we chose the
smoothing parameters for which we obtained the
best classification accuracies (averaged over the ten
runs) on the training data; the results reported here
were obtained using these parameters on the held-
out test sets9.
In addition to these two generative models, we
also used a discriminative model, a neural network.
We used the Matlab package to train a feed-forward
network with conjugate gradient descent. The net-
work has one hidden layer, with a hyperbolic tangent
function, and an output layer representing the rela-
tionships. A logistic sigmoid function is used in the
output layer. The network was trained for several
choices of numbers of hidden units; we chose the
best-performing networks based on training set er-
ror. We then tested these networks on held-out test-
ing data. The features were words, the same as those
used for the graphical models.
8Using Kevin Murphy?s BNT package:
http://www.cs.ubc.ca/?murphyk/Software/BNT/bnt.html.
9We did not have enough data to require that the sentences in
the training and test sets of the cross validation procedure orig-
inate from disjoint triples (they do originate from disjoint triple
in the final held out data). This may result in a less than optimal
choice of the parameters for the aggregate measures described
below.
735
All Papers Citances
Mj Cf Mj Cf Mj Cf
DM 60.5 59.7 57.8 55.6 53.4 54.5
NB 58.1 61.3 57.8 55.6 55.7 54.5
NN 63.7 ? 44.4 ? 55.8 ?
Key 20.1 ? 24.4 ? 20.4 ?
KeyB 25.8 ? 40.0 ? 26.1 ?
Base. 21.8 11.1 26.1
Table 3: Accuracies for classification of the 10
protein-protein interactions of Table 2. DM: dy-
namic model, NB: Naive Bayes, NN: neural net-
work. Baselines: Key: trigger word approach,
KeyB: trigger word with backoff, Base: the accu-
racy of choosing the most frequent interaction.
The task is the following: given a triple consist-
ing of a

and an article, extract the sentences
from the article that contain both proteins. Then,
predict for the entire document one of the interac-
tions of Table 2 given the sentences extracted for
that triple. This is a 10-way classification problem,
which is significantly more complex than much of
the related work in which the task is to make the bi-
nary prediction (see Section 2).
5 Results
The evaluation was done on a document-by-
document basis. During testing, we choose the inter-
action using the following aggregate measures that
use the constraint that all sentences coming from the
same triple are assigned the same interaction.
  Mj: For each triple, for each sentence of the
triple, find the interaction that maximizes the
posterior probability of the interaction given
the features; then assign to all sentences of
this triple the most frequent interaction among
those predicted for the individual sentences.
  Cf: Retain all the conditional probabilities (do
not choose an interaction per sentence), then,
for each triple, choose the interaction that max-
imizes the sum over all the triple?s sentences.
Table 3 reports the results in terms of classifi-
cation accuracies averaged across all interactions,
for the cases ?all? (sentences from ?papers? and
?citances? together), only ?papers? and only ?ci-
tances?. The accuracies are quite high; the dy-
namic model achieves around 60% for ?all,? 58%
for ?papers? and 54% for ?citances.? The neural
net achieves the best results for ?all? with around
64% accuracy. From these results we can make the
following observations: all models greatly outper-
form the baselines; the performances of the dynamic
model DM, the Naive Bayes NB and the NN are very
similar; for ?papers? the best results were obtained
with the graphical models; for ?all? and ?citances?
the neural net did best. The use of ?citances? al-
lowed the gathering of additional data (and therefore
a larger training set) that lead to higher accuracies
(see ?papers? versus ?all?).
In the confusion matrix in Table 5 we can see the
accuracies for the individual interactions for the dy-
namic model DM, using ?all? and ?Mj.? For three
interactions this model achieves perfect accuracy.
5.1 Hiding the protein names
In order to ensure that the algorithm was not over-
fitting on the protein names, we ran an experiment
in which we replaced the protein names in all sen-
tences with the token ?PROT NAME.? For example,
the sentence: ?Selective CXCR4 antagonism by Tat?
became: ?Selective PROT NAME2 antagonism by
PROT NAME1.?
Table 5.1 shows the results of running the mod-
els on this data. For ?papers? and ?citances? there
is always a decrease in the classification accuracy
when we remove the protein names, showing that
the protein names do help the classification. The
differences in accuracy in the two cases using ?ci-
tances? are much smaller than the differences using
?papers? at least for the graphical models. This sug-
gests that citation sentences may be more robust for
some language processing tasks and that the models
that use ?citances? learn better the linguistic context
of the interactions. Note how in this case the graph-
ical models always outperform the neural network.
5.2 Using a ?trigger word? approach
As mentioned above, much of the related work in
this field makes use of ?trigger words? or ?interac-
tion words? (see Section 2). In order to (roughly)
compare our work and to build a more realistic base-
line, we created a list of 70 keywords that are repre-
736
Prediction Acc.
Truth D SyW St B Ina IW R Up Inh Su (%)
Degrades (D) 5 0 0 0 0 0 0 0 0 0 100.0
Synergizes with (SyW) 0 1 0 0 0 1 0 3 3 0 12.5
Stimulates (St) 0 0 4 0 0 0 6 0 1 0 36.4
Binds (B) 0 0 0 18 0 4 1 1 3 0 66.7
Inactivates (Ina) 0 0 0 0 9 0 0 0 0 0 100.0
Interacts with (IW) 0 0 4 3 0 5 1 0 1 2 31.2
Requires (R) 0 0 0 0 0 3 3 0 1 1 37.5
Upregulates (Up) 0 0 0 2 1 0 0 12 2 0 70.6
Inhibits (Inh) 0 0 0 3 0 0 1 1 12 0 70.6
Suppresses (Su) 0 0 0 0 0 0 0 0 0 6 100.0
Table 4: Confusion matrix for the dynamic model DM for ?all,? ?Mj.? The overall accuracy is 60.5%. The
numbers indicate the number of articles  (each paper has several relevant sentences).
All Papers Citances
Mj Cf Diff Mj Cf Diff Mj Cf Diff
DM 60.5 60.5 0.7% 44.4 40.0 -25.6% 52.3 53.4 -2.0%
NB 59.7 59.7 0.1% 46.7 51.1 -11.7% 53.4 53.4 -3.1%
NN 51.6 -18.9% 44.4 0% 50.0 -10.4%
Table 5: Accuracies for the classification of the 10 protein-protein interactions of Table 2 with the protein
names removed. Columns marked Diff show the difference in accuracy (in percentages) with respect to the
original case of Table 3, averaged over all evaluation methods.
sentative of the 10 interactions. For example, for
the interaction degrade some of the keywords are
?degradation,? ?degrade,? for inhibit we have ?inhib-
ited,? ?inhibitor,? ?inhibitory? and others. We then
checked whether a sentence contained such key-
words. If it did, we assigned to the sentence the
corresponding interaction. If it contained more than
one keyword corresponding to multiple interactions
consisting of the generic interact with plus a more
specific one, we assigned the more specific interac-
tion; if the two predicted interactions did not include
interact with but two more specific interactions, we
did not assign an interaction, since we wouldn?t
know how to choose between them. Similarly, we
assigned no interaction if there were more than two
predicted interactions or no keywords present in the
sentence. The results are shown in the rows labeled
?Key? and ?KeyB? in Table 3. Case ?KeyB? is the
?Key? method with back-off: when no interaction
was predicted, we assigned to the sentence the most
frequent interaction in the training data. As before,
we calculated the accuracy when we force all the
sentences from one triple to be assign to the most
frequent interaction among those predicted for the
individual sentences.
KeyB is more accurate than Key and although
the KeyB accuracies are higher than the other base-
lines, they are significantly lower than those ob-
tained with the trained models. The low accuracies
of the trigger-word based methods show that the re-
lation classification task is nontrivial, in the sense
that not all the sentences contain the most obvious
word for the interactions, and suggests that the trig-
ger word approach is insufficient.
5.3 Protein extraction
The dynamic model of Figure 1 has the appealing
property of simultaneously performing interaction
recognition and protein name tagging (also known
as role extraction): the task consists of identifying
all the proteins present in the sentence, given a se-
quence of words. We assessed a slightly different
task: the identification of all (and only) the proteins
present in the sentence that are involved in the inter-
action.
The F-measure10 achieved by this model for this
task is 0.79 for ?all,? 0.67 for ?papers? and 0.79 for
?citances?; again, the model parameters were cho-
sen with cross validation on the training set, and ?ci-
10The F-measure is a weighted combination of precision and
recall. Here, precision and recall are given equal weight, that is,
F-measure = 

Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 835?842, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Using the Web as an Implicit Training Set:
Application to Structural Ambiguity Resolution
Preslav Nakov and Marti Hearst
EECS and SIMS
University of California at Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu, hearst@sims.berkeley.edu
Abstract
Recent work has shown that very large
corpora can act as training data for NLP
algorithms even without explicit labels. In
this paper we show how the use of sur-
face features and paraphrases in queries
against search engines can be used to infer
labels for structural ambiguity resolution
tasks. Using unsupervised algorithms, we
achieve 84% precision on PP-attachment
and 80% on noun compound coordination.
1 Introduction
Resolution of structural ambiguity problems such
as noun compound bracketing, prepositional phrase
(PP) attachment, and noun phrase coordination re-
quires using information about lexical items and
their cooccurrences. This in turn leads to the data
sparseness problem, since algorithms that rely on
making decisions based on individual lexical items
must have statistics about every word that may be
encountered. Past approaches have dealt with the
data sparseness problem by attempting to generalize
from semantic classes, either manually built or auto-
matically derived.
More recently, Banko and Brill (2001) have ad-
vocated for the creative use of very large text col-
lections as an alternative to sophisticated algorithms
and hand-built resources. They demonstrate the idea
on a lexical disambiguation problem for which la-
beled examples are available ?for free?. The prob-
lem is to choose which of 2-3 commonly confused
words (e.g., {principle, principal}) are appropriate
for a given context. The labeled data comes ?for
free? by assuming that in most edited written text,
the words are used correctly, so training can be done
directly from the text. Banko and Brill (2001) show
that even using a very simple algorithm, the results
continue to improve log-linearly with more training
data, even out to a billion words. A potential limita-
tion of this approach is the question of how applica-
ble it is for NLP problems more generally ? how can
we treat a large corpus as a labeled collection for a
wide range of NLP tasks?
In a related strand of work, Lapata and Keller
(2004) show that computing n-gram statistics over
very large corpora yields results that are competi-
tive with if not better than the best supervised and
knowledge-based approaches on a wide range of
NLP tasks. For example, they show that for the
problem of noun compound bracketing, the perfor-
mance of an n-gram based model computed using
search engine statistics was not significantly differ-
ent from the best supervised algorithm whose pa-
rameters were tuned and which used a taxonomy.
They find however that these approaches generally
fail to outperform supervised state-of-the-art models
that are trained on smaller corpora, and so conclude
that web-based n-gram statistics should be the base-
line to beat.
We feel the potential of these ideas is not yet fully
realized. We are interested in finding ways to further
exploit the availability of enormous web corpora as
implicit training data. This is especially important
for structural ambiguity problems in which the de-
cisions must be made on the basis of the behavior
835
of individual lexical items. The trick is to figure out
how to use information that is latent in the web as a
corpus, and web search engines as query interfaces
to that corpus.
In this paper we describe two techniques ? sur-
face features and paraphrases ? that push the ideas
of Banko and Brill (2001) and Lapata and Keller
(2004) farther, enabling the use of statistics gathered
from very large corpora in an unsupervised man-
ner. In recent work (Nakov and Hearst, 2005) we
showed that a variation of the techniques, when ap-
plied to the problem of noun compound bracketing,
produces higher accuracy than Lapata and Keller
(2004) and the best supervised results. In this pa-
per we adapt the techniques to the structural disam-
biguation problems of prepositional phrase attach-
ment and noun compound coordination.
2 Prepositional Phrase Attachment
A long-standing challenge for syntactic parsers is
the attachment decision for prepositional phrases. In
a configuration where a verb takes a noun comple-
ment that is followed by a PP, the problem arises of
whether the PP attaches to the noun or to the verb.
Consider the following contrastive pair of sentences:
(1) Peter spent millions of dollars. (noun)
(2) Peter spent time with his family. (verb)
In the first example, the PP millions of dollars at-
taches to the noun millions, while in the second the
PP with his family attaches to the verb spent.
Past work on PP-attachment has often cast these
associations as the quadruple (v, n1, p, n2), where v
is the verb, n1 is the head of the direct object, p is the
preposition (the head of the PP) and n2 is the head
of the NP inside the PP. For example, the quadruple
for (2) is (spent, time, with, family).
2.1 Related Work
Early work on PP-attachment ambiguity resolu-
tion relied on syntactic (e.g., ?minimal attachment?
and ?right association?) and pragmatic considera-
tions. Most recent work can be divided into su-
pervised and unsupervised approaches. Supervised
approaches tend to make use of semantic classes
or thesauri in order to deal with data sparseness
problems. Brill and Resnik (1994) used the su-
pervised transformation-based learning method and
lexical and conceptual classes derived from Word-
Net, achieving 82% precision on 500 randomly se-
lected examples. Ratnaparkhi et al (1994) cre-
ated a benchmark dataset of 27,937 quadruples
(v, n1, p, n2), extracted from the Wall Street Jour-
nal. They found the human performance on this
task to be 88%1. Using this dataset, they trained a
maximum entropy model and a binary hierarchy of
word classes derived by mutual information, achiev-
ing 81.6% precision. Collins and Brooks (1995)
used a supervised back-off model to achieve 84.5%
precision on the Ratnaparkhi test set. Stetina and
Makoto (1997) use a supervised method with a deci-
sion tree and WordNet classes to achieve 88.1% pre-
cision on the same test set. Toutanova et al (2004)
use a supervised method that makes use of morpho-
logical and syntactic analysis and WordNet synsets,
yielding 87.5% accuracy.
In the unsupervised approaches, the attachment
decision depends largely on co-occurrence statistics
drawn from text collections. The pioneering work
in this area was that of Hindle and Rooth (1993).
Using a partially parsed corpus, they calculate and
compare lexical associations over subsets of the tu-
ple (v, n1, p), ignoring n2, and achieve 80% preci-
sion at 80% recall.
More recently, Ratnaparkhi (1998) developed an
unsupervised method that collects statistics from
text annotated with part-of-speech tags and mor-
phological base forms. An extraction heuristic is
used to identify unambiguous attachment decisions,
for example, the algorithm can assume a noun at-
tachment if there is no verb within k words to the
left of the preposition in a given sentence, among
other conditions. This extraction heuristic uncov-
ered 910K unique tuples of the form (v, p, n2) and
(n, p, n2), although the results are very noisy, sug-
gesting the correct attachment only about 69% of the
time. The tuples are used as training data for clas-
sifiers, the best of which achieves 81.9% precision
on the Ratnaparkhi test set. Pantel and Lin (2000)
describe an unsupervised method that uses a collo-
cation database, a thesaurus, a dependency parser,
and a large corpus (125M words), achieving 84.3%
precision on the Ratnaparkhi test set. Using sim-
1When presented with a whole sentence, average humans
score 93%.
836
ple combinations of web-based n-grams, Lapata and
Keller (2005) achieve lower results, in the low 70?s.
Using a different collection consisting of German
PP-attachment decisions, Volk (2000) uses the web
to obtain n-gram counts. He compared Pr(p|n1) to
Pr(p|v), where Pr(p|x) = #(x, p)/#(x). Here x
can be n1 or v. The bigram frequencies #(x, p)
were obtained using the Altavista NEAR operator.
The method was able to make a decision on 58%
of the examples with a precision of 75% (baseline
63%). Volk (2001) then improved on these results
by comparing Pr(p, n2|n1) to Pr(p, n2|v). Using
inflected forms, he achieved P=75% and R=85%.
Calvo and Gelbukh (2003) experimented with a
variation of this, using exact phrases instead of the
NEAR operator. For example, to disambiguate Veo
al gato con un telescopio, they compared frequen-
cies for phrases such as ?ver con telescopio? and
?gato con telescopio?. They tested this idea on 181
randomly chosen Spanish disambiguation examples,
labelling 89.5% recall with a precision of 91.97%.
2.2 Models and Features
2.2.1 n-gram Models
We computed two co-occurrence models;
(i) Pr(p|n1) vs. Pr(p|v)
(ii) Pr(p, n2|n1) vs. Pr(p, n2|v).
Each of these was computed two different ways:
using Pr (probabilities) and # (frequencies). We es-
timate the n-gram counts using exact phrase queries
(with inflections, derived from WordNet 2.0) using
the MSN Search Engine. We also allow for deter-
miners, where appropriate, e.g., between the prepo-
sition and the noun when querying for #(p, n2). We
add up the frequencies for all possible variations.
Web frequencies were reliable enough and did not
need smoothing for (i), but for (ii), smoothing using
the technique described in Hindle and Rooth (1993)
led to better recall. We also tried back-off from (ii)
to (i), as well as back-off plus smoothing, but did not
find improvements over smoothing alone. We found
n-gram counts to be unreliable when pronouns ap-
pear in the test set rather than nouns, and disabled
them in these cases. Such examples can still be han-
dled by paraphrases or surface features (see below).
2.2.2 Web-Derived Surface Features
Authors sometimes (consciously or not) disam-
biguate the words they write by using surface-level
markers to suggest the correct meaning. We have
found that exploiting these markers, when they oc-
cur, can prove to be very helpful for making dis-
ambiguation decisions. The enormous size of web
search engine indexes facilitates finding such mark-
ers frequently enough to make them useful.
For example, John opened the door with a key is
a difficult verb attachment example because doors,
keys, and opening are all semantically related. To
determine if this should be a verb or a noun attach-
ment, we search for cues that indicate which of these
terms tend to associate most closely. If we see paren-
theses used as follows:
?open the door (with a key)?
this suggests a verb attachment, since the parenthe-
ses signal that ?with a key? acts as its own unit.
Similarly, hyphens, colons, capitalization, and other
punctuation can help signal disambiguation deci-
sions. For Jean ate spaghetti with sauce, if we see
?eat: spaghetti with sauce?
this suggests a noun attachment.
Table 1 illustrates a wide variety of surface fea-
tures, along with the attachment decisions they are
assumed to suggest (events of frequency 1 have been
ignored). The surface features for PP-attachment
have low recall: most of the examples have no sur-
face features extracted.
We gather the statistics needed by issuing queries
to web search engines. Unfortunately, search en-
gines usually ignore punctuation characters, thus
preventing querying directly for terms containing
hyphens, brackets, etc. We collect these numbers
indirectly by issuing queries with exact phrases and
then post-processing the top 1,000 resulting sum-
maries2, looking for the surface features of interest.
We use Google for both the surface feature and para-
phrase extractions (described below).
2.2.3 Paraphrases
The second way we extend the use of web counts
is by paraphrasing the relation of interest and see-
ing if it can be found in its alternative form, which
2We often obtain more than 1,000 summaries per example
because we usually issue multiple queries per surface pattern,
by varying inflections and inclusion of determiners.
837
suggests the correct attachment decision. We use
the following patterns along with their associated at-
tachment predictions:
(1) v n2 n1 (noun)
(2) v p n2 n1 (verb)
(3) p n2 * v n1 (verb)
(4) n1 p n2 v (noun)
(5) v pronoun p n2 (verb)
(6) be n1 p n2 (noun)
The idea behind Pattern (1) is to determine
if ?n1 p n2? can be expressed as a noun com-
pound; if this happens sufficiently often, we can
predict a noun attachment. For example, meet/v
demands/n1 from/p customers/n2 becomes meet/v
the customers/n2 demands/n1.
Note that the pattern could wrongly target ditran-
sitive verbs: e.g., it could turn gave/v an apple/n1
to/p him/n2 into gave/v him/n2 an apple/n1. To pre-
vent this, we do not allow a determiner before n1,
but we do require one before n2. In addition, we
disallow the pattern if the preposition is to and we
require both n1 and n2 to be nouns (as opposed to
numbers, percents, pronouns, determiners etc.).
Pattern (2) predicts a verb attachment. It presup-
poses that ?p n2? is an indirect object of the verb v
and tries to switch it with the direct object n1, e.g.,
had/v a program/n1 in/p place/n2 would be trans-
formed into had/v in/p place/n2 a program/n1. We
require n1 to be preceded by a determiner (to prevent
?n2 n1? forming a noun compound).
Pattern (3) looks for appositions, where the PP has
moved in front of the verb, e.g., to/p him/n2 I gave/v
an apple/n1. The symbol * indicates a wildcard po-
sition where we allow up to three intervening words.
Pattern (4) looks for appositions, where the PP has
moved in front of the verb together with n1. It would
transform shaken/v confidence/n1 in/p markets/n2
into confidence/n1 in/p markets/n2 shaken/v.
Pattern (5) is motivated by the observation that
if n1 is a pronoun, this suggests a verb attach-
ment (Hindle and Rooth, 1993). (A separate feature
checks if n1 is a pronoun.) The pattern substitutes
n1 with a dative pronoun (we allow him and her),
e.g., it will convert put/v a client/n1 at/p odds/n2
into put/v him at/p odds/n2.
Pattern (6) is motivated by the observation that the
verb to be is typically used with a noun attachment.
(A separate feature checks if v is a form of the verb
to be.) The pattern substitutes v with is and are, e.g.
it will turn eat/v spaghetti/n1 with/p sauce/n2 into is
spaghetti/n1 with/p sauce/n2.
These patterns all allow for determiners where ap-
propriate, unless explicitly stated otherwise. For a
given example, a prediction is made if at least one
instance of the pattern has been found.
2.3 Evaluation
For the evaluation, we used the test part (3,097 ex-
amples) of the benchmark dataset by Ratnaparkhi et
al. (1994). We used all 3,097 test examples in order
to make our results directly comparable.
Unfortunately, there are numerous errors in the
test set3. There are 149 examples in which a bare
determiner is labeled as n1 or n2 rather than the ac-
tual head noun. Supervised algorithms can compen-
sate for this problem by learning from the training
set that ?the? can act as a noun in this collection, but
unsupervised algorithms cannot.
In addition, there are also around 230 examples
in which the nouns contain special symbols like: %,
slash, &, ?, which are lost when querying against a
search engine. This poses a problem for our algo-
rithm but is not a problem with the test set itself.
The results are shown in Table 2. Following Rat-
naparkhi (1998), we predict a noun attachment if the
preposition is of (a very reliable heuristic). The table
shows the performance for each feature in isolation
(excluding examples whose preposition is of). The
surface features are represented by a single score in
Table 2: for a given example, we sum up separately
the number of noun- and verb-attachment pattern
matches, and assign the attachment with the larger
number of matches.
We combine the bold rows of Table 2 in a majority
vote (assigning noun attachment to all of instances),
obtaining P=85.01%, R=91.77%. To get 100% re-
call, we assign all undecided cases to verb (since
the majority of the remaining non-of instances at-
tach to the verb, yielding P=83.63%, R=100%. We
show 0.95-level confidence intervals for the preci-
sion, computed by a general method based on con-
stant chi-square boundaries (Fleiss, 1981).
A test for statistical significance reveals that our
results are as strong as those of the leading unsuper-
3Ratnaparkhi (1998) notes that the test set contains errors,
but does not correct them.
838
Example Predicts P(%) R(%)
open Door with a key noun 100.00 0.13
(open) door with a key noun 66.67 0.28
open (door with a key) noun 71.43 0.97
open - door with a key noun 69.70 1.52
open / door with a key noun 60.00 0.46
open, door with a key noun 65.77 5.11
open: door with a key noun 64.71 1.57
open; door with a key noun 60.00 0.23
open. door with a key noun 64.13 4.24
open? door with a key noun 83.33 0.55
open! door with a key noun 66.67 0.14
open door With a Key verb 0.00 0.00
(open door) with a key verb 50.00 0.09
open door (with a key) verb 73.58 2.44
open door - with a key verb 68.18 2.03
open door / with a key verb 100.00 0.14
open door, with a key verb 58.44 7.09
open door: with a key verb 70.59 0.78
open door; with a key verb 75.00 0.18
open door. with a key verb 60.77 5.99
open door! with a key verb 100.00 0.18
Table 1: PP-attachment surface features. Preci-
sion and recall shown are across all examples, not
just the door example shown.
vised approach on this collection (Pantel and Lin,
2000). Unlike that work, we do not require a collo-
cation database, a thesaurus, a dependency parser,
nor a large domain-dependent text corpus, which
makes our approach easier to implement and to ex-
tend to other languages.
3 Coordination
Coordinating conjunctions (and, or, but, etc.) pose
major challenges to parsers and their proper han-
dling is essential for the understanding of the sen-
tence. Consider the following ?cooked? example:
The Department of Chronic Diseases and Health
Promotion leads and strengthens global efforts to
prevent and control chronic diseases or disabilities
and to promote health and quality of life.
Conjunctions can link two words, two con-
stituents (e.g., NPs), two clauses or even two sen-
tences. Thus, the first challenge is to identify the
boundaries of the conjuncts of each coordination.
The next problem comes from the interaction of
the coordinations with other constituents that attach
to its conjuncts (most often prepositional phrases).
In the example above we need to decide between
[health and [quality of life]] and [[health and qual-
Model P(%) R(%)
Baseline (noun attach) 41.82 100.00
#(x, p) 58.91 83.97
Pr(p|x) 66.81 83.97
Pr(p|x) smoothed 66.81 83.97
#(x, p, n2) 65.78 81.02
Pr(p, n2|x) 68.34 81.62
Pr(p, n2|x) smoothed 68.46 83.97
(1) ?v n2 n1? 59.29 22.06
(2) ?p n2 v n1? 57.79 71.58
(3) ?n1 * p n2 v? 65.78 20.73
(4) ?v p n2 n1? 81.05 8.75
(5) ?v pronoun p n2? 75.30 30.40
(6) ?be n1 p n2? 63.65 30.54
n1 is pronoun 98.48 3.04
v is to be 79.23 9.53
Surface features (summed) 73.13 9.26
Maj. vote, of ? noun 85.01?1.21 91.77
Maj. vote, of ? noun, N/A ? verb 83.63?1.30 100.00
Table 2: PP-attachment results, in percentages.
ity] of life]. From a semantic point of view, we
need to determine whether the or in chronic dis-
eases or disabilities really means or or is used as an
and (Agarwal and Boggess, 1992). Finally, we need
to choose between a non-elided and an elided read-
ing: [[chronic diseases] or disabilities] vs. [chronic
[diseases or disabilities]].
Below we focus on a special case of the latter
problem: noun compound (NC) coordination. Con-
sider the NC car and truck production. Its real
meaning is car production and truck production.
However, due to the principle of economy of ex-
pression, the first instance of production has been
compressed out by means of ellipsis. By contrast,
in president and chief executive, president is simply
linked to chief executive. There is also an all-way co-
ordination, where the conjunct is part of the whole,
as in Securities and Exchange Commission.
More formally, we consider configurations of the
kind n1 c n2 h, where n1 and n2 are nouns, c is a
coordination (and or or) and h is the head noun4.
The task is to decide whether there is an ellipsis or
not, independently of the local context. Syntacti-
cally, this can be expressed by the following brack-
etings: [[n1 c n2] h] versus [n1 c [n2 h]]. (Collins?
parser (Collins, 1997) always predicts a flat NP for
such configurations.) In order to make the task more
4The configurations of the kind n h1 c h2 (e.g., company/n
cars/h1 and/c trucks/h2) can be handled in a similar way.
839
realistic (from a parser?s perspective), we ignore the
option of all-way coordination and try to predict the
bracketing in Penn Treebank (Marcus et al, 1994)
for configurations of this kind. The Penn Treebank
brackets NCs with ellipsis as, e.g.,
(NP car/NN and/CC truck/NN production/NN).
and without ellipsis as
(NP (NP president/NN) and/CC (NP chief/NN exec-
utive/NN))
The NPs with ellipsis are flat, while the others con-
tain internal NPs. The all-way coordinations can ap-
pear bracketed either way and make the task harder.
3.1 Related Work
Coordination ambiguity is under-explored, despite
being one of the three major sources of structural
ambiguity (together with prepositional phrase at-
tachment and noun compound bracketing), and be-
longing to the class of ambiguities for which the
number of analyses is the number of binary trees
over the corresponding nodes (Church and Patil,
1982), and despite the fact that conjunctions are
among the most frequent words.
Rus et al (2002) present a deterministic rule-
based approach for bracketing in context of coor-
dinated NCs of the kind n1 c n2 h, as a necessary
step towards logical form derivation. Their algo-
rithm uses POS tagging, syntactic parses, semantic
senses of the nouns (manually annotated), lookups
in a semantic network (WordNet) and the type of the
coordination conjunction to make a 3-way classifi-
cation: ellipsis, no ellipsis and all-way coordination.
Using a back-off sequence of 3 different heuristics,
they achieve 83.52% precision (baseline 61.52%) on
a set of 298 examples. When 3 additional context-
dependent heuristics and 224 additional examples
with local contexts are added, the precision jumps
to 87.42% (baseline 52.35%), with 71.05% recall.
Resnik (1999) disambiguates two kinds of pat-
terns: n1 and n2 n3 and n1 n2 and n3 n4
(e.g., [food/n1 [handling/n2 and/c storage/n3]
procedures/n4]). While there are two options for
the former (all-way coordinations are not allowed),
there are 5 valid bracketings for the latter. Follow-
ing Kurohashi and Nagao (1992), Resnik makes de-
cisions based on similarity of form (i.e., number
agreement: P=53%, R=90.6%), similarity of mean-
ing (P=66%, R=71.2%) and conceptual association
Example Predicts P(%) R(%)
(buy) and sell orders NO ellipsis 33.33 1.40
buy (and sell orders) NO ellipsis 70.00 4.67
buy: and sell orders NO ellipsis 0.00 0.00
buy; and sell orders NO ellipsis 66.67 2.80
buy. and sell orders NO ellipsis 68.57 8.18
buy[...] and sell orders NO ellipsis 49.00 46.73
buy- and sell orders ellipsis 77.27 5.14
buy and sell / orders ellipsis 50.54 21.73
(buy and sell) orders ellipsis 92.31 3.04
buy and sell (orders) ellipsis 90.91 2.57
buy and sell, orders ellipsis 92.86 13.08
buy and sell: orders ellipsis 93.75 3.74
buy and sell; orders ellipsis 100.00 1.87
buy and sell. orders ellipsis 93.33 7.01
buy and sell[...] orders ellipsis 85.19 18.93
Table 3: Coordination surface features. Precision
and recall shown are across all examples, not just the
buy and sell orders shown.
(P=75.0%, R=69.3%). Using a decision tree to com-
bine the three information sources, he achieves 80%
precision (baseline 66%) at 100% recall for the 3-
noun coordinations. For the 4-noun coordinations
the precision is 81.6% (baseline 44.9%), 85.4% re-
call.
Chantree et al (2005) cover a large set of ambi-
guities, not limited to nouns. They allow the head
word to be a noun, a verb or an adjective, and the
modifier to be an adjective, a preposition, an ad-
verb, etc. They extract distributional information
from the British National Corpus and distributional
similarities between words, similarly to (Resnik,
1999). In two different experiments they achieve
P=88.2%, R=38.5% and P=80.8%, R=53.8% (base-
line P=75%).
Goldberg (1999) resolves the attachment of am-
biguous coordinate phrases of the kind n1 p n2 c
n3, e.g., box/n1 of/p chocolates/n2 and/c roses/n3.
Using an adaptation of the algorithm proposed by
Ratnaparkhi (1998) for PP-attachment, she achieves
P=72% (baseline P=64%), R=100.00%.
Agarwal and Boggess (1992) focus on the identi-
fication of the conjuncts of coordinate conjunctions.
Using POS and case labels in a deterministic algo-
rithm, they achieve P=81.6%. Kurohashi and Na-
gao (1992) work on the same problem for Japanese.
Their algorithm looks for similar word sequences
among with sentence simplification, and achieves a
precision of 81.3%.
840
3.2 Models and Features
3.2.1 n-gram Models
We use the following n-gram models:
(i) #(n1, h) vs. #(n2, h)
(ii) #(n1, h) vs. #(n1, c, n2)
Model (i) compares how likely it is that n1 mod-
ifies h, as opposed to n2 modifying h. Model (ii)
checks which association is stronger: between n1
and h, or between n1 and n2. Regardless of whether
the coordination is or or and, we query for both and
we add up the corresponding counts.
3.2.2 Web-Derived Surface Features
The set of surface features is similar to the one we
used for PP-attachment. These are brackets, slash,
comma, colon, semicolon, dot, question mark, ex-
clamation mark, and any character. There are two
additional ellipsis-predicting features: a dash after
n1 and a slash after n2, see Table 3.
3.2.3 Paraphrases
We use the following paraphrase patterns:
(1) n2 c n1 h (ellipsis)
(2) n2 h c n1 (NO ellipsis)
(3) n1 h c n2 h (ellipsis)
(4) n2 h c n1 h (ellipsis)
If matched frequently enough, each of these pat-
terns predicts the coordination decision indicated in
parentheses. If found only infrequently or not found
at all, the opposite decision is made. Pattern (1)
switches the places of n1 and n2 in the coordinated
NC. For example, bar and pie graph can easily be-
come pie and bar graph, which favors ellipsis. Pat-
tern (2) moves n2 and h together to the left of the
coordination conjunction, and places n1 to the right.
If this happens frequently enough, there is no ellip-
sis. Pattern (3) inserts the elided head h after n1 with
the hope that if there is ellipsis, we will find the full
phrase elsewhere in the data. Pattern (4) combines
pattern (1) and pattern (3); it not only inserts h after
n1 but also switches the places of n1 and n2.
As shown in Table 4, we included four of the
heuristics by Rus et al (2002). Heuristic 1 predicts
no coordination when n1 and n2 are the same, e.g.,
milk and milk products. Heuristics 2 and 3 perform a
lookup in WordNet and we did not use them. Heuris-
tics 4, 5 and 6 exploit the local context, namely the
Model P(%) R(%)
Baseline: ellipsis 56.54 100.00
(n1, h) vs. (n2, h) 80.33 28.50
(n1, h) vs. (n1, c, n2) 61.14 45.09
(n2, c, n1, h) 88.33 14.02
(n2, h, c, n1) 76.60 21.96
(n1, h, c, n2, h) 75.00 6.54
(n2, h, c, n1, h) 78.67 17.52
Heuristic 1 75.00 0.93
Heuristic 4 64.29 6.54
Heuristic 5 61.54 12.15
Heuristic 6 87.09 7.24
Number agreement 72.22 46.26
Surface sum 82.80 21.73
Majority vote 83.82 80.84
Majority vote, N/A ? no ellipsis 80.61 100.00
Table 4: Coordination results, in percentages.
adjectives modifying n1 and/or n2. Heuristic 4 pre-
dicts no ellipsis if both n1 and n2 are modified by
adjectives. Heuristic 5 predicts ellipsis if the coor-
dination is or and n1 is modified by an adjective,
but n2 is not. Heuristic 6 predicts no ellipsis if n1
is not modified by an adjective, but n2 is. We used
versions of heuristics 4, 5 and 6 that check for deter-
miners rather than adjectives.
Finally, we included the number agreement fea-
ture (Resnik, 1993): (a) if n1 and n2 match in num-
ber, but n1 and h do not, predict ellipsis; (b) if n1
and n2 do not match in number, but n1 and h do,
predict no ellipsis; (c) otherwise leave undecided.
3.3 Evaluation
We evaluated the algorithms on a collection of 428
examples extracted from the Penn Treebank. On ex-
traction, determiners and non-noun modifiers were
allowed, but the program was only presented with
the quadruple (n1, c, n2, h). As Table 4 shows, our
overall performance of 80.61 is on par with other ap-
proaches, whose best scores fall into the low 80?s for
precision. (Direct comparison is not possible, as the
tasks and datasets all differ.)
As Table 4 shows, n-gram model (i) performs
well, but n-gram model (ii) performs poorly, proba-
bly because the (n1, c, n2) contains three words, as
opposed to two for the alternative (n1, h), and thus
a priori is less likely to be observed.
The surface features are less effective for resolv-
ing coordinations. As Table 3 shows, they are very
good predictors of ellipsis, but are less reliable when
841
predicting NO ellipsis. We combine the bold rows
of Table 4 in a majority vote, obtaining P=83.82%,
R=80.84%. We assign all undecided cases to no el-
lipsis, yielding P=80.61%, R=100%.
4 Conclusions and Future Work
We have shown that simple unsupervised algorithms
that make use of bigrams, surface features and para-
phrases extracted from a very large corpus are ef-
fective for several structural ambiguity resolutions
tasks, yielding results competitive with the best un-
supervised results, and close to supervised results.
The method does not require labeled training data,
nor lexicons nor ontologies. We think this is a
promising direction for a wide range of NLP tasks.
In future work we intend to explore better-motivated
evidence combination algorithms and to apply the
approach to other NLP problems.
Acknowledgements. This research was supported
by NSF DBI-0317510 and a gift from Genentech.
References
Rajeev Agarwal and Lois Boggess. 1992. A simple but useful
approach to conjunct identification. In Proceedings of ACL.
Michele Banko and Eric Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Proceed-
ings of ACL.
Eric Brill and Philip Resnik. 1994. A rule-based approach
to prepositional phrase attachment disambiguation. In Pro-
ceedings of COLING.
Hiram Calvo and Alexander Gelbukh. 2003. Improving prepo-
sitional phrase attachment disambiguation using the web as
corpus. In Progress in Pattern Recognition, Speech and
Image Analysis: 8th Iberoamerican Congress on Pattern
Recognition, CIARP 2003.
Francis Chantree, Adam Kilgarriff, Anne De Roeck, and Alis-
tair Willis. 2005. Using a distributional thesaurus to resolve
coordination ambiguities. In Technical Report 2005/02. The
Open University, UK.
Kenneth Church and Ramesh Patil. 1982. Coping with syntac-
tic ambiguity or how to put the block in the box on the table.
Amer. J. of Computational Linguistics, 8(3-4):139?149.
Michael Collins and James Brooks. 1995. Prepositional phrase
attachment through a backed-off model. In Proceedings of
EMNLP, pages 27?38.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In Proceedings of ACL, pages 16?23.
Joseph Fleiss. 1981. Statistical Methods for Rates and Propor-
tions (2nd Ed.). John Wiley & Sons, New York.
Miriam Goldberg. 1999. An unsupervised model for statis-
tically determining coordinate phrase attachment. In Pro-
ceedings of ACL.
Donald Hindle and Mats Rooth. 1993. Structural ambiguity
and lexical relations. Computational Linguistics, 19(1):103?
120.
Sadao Kurohashi and Makoto Nagao. 1992. Dynamic pro-
gramming method for analyzing conjunctive structures in
japanese. In Proceedings of COLING, volume 1.
Mirella Lapata and Frank Keller. 2004. The Web as a base-
line: Evaluating the performance of unsupervised Web-
based models for a range of NLP tasks. In Proceedings of
HLT-NAACL, pages 121?128, Boston.
Mirella Lapata and Frank Keller. 2005. Web-based models for
natural language processing. ACM Transactions on Speech
and Language Processing, 2:1?31.
Mitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz.
1994. Building a large annotated corpus of English: The
Penn Treebank. Computational Linguistics, 19(2):313?330.
Preslav Nakov and Marti Hearst. 2005. Search engine statistics
beyond the n-gram: Application to noun compound bracket-
ing. In Proceedings of CoNLL 2005.
Patrick Pantel and Dekang Lin. 2000. An unsupervised ap-
proach to prepositional phrase attachment using contextually
similar words. In Proceedings of ACL.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994.
A maximum entropy model for prepositional phrase attach-
ment. In Proceedings of the ARPA Workshop on Human Lan-
guage Technology., pages 250?255.
Adwait Ratnaparkhi. 1998. Statistical models for unsuper-
vised prepositional phrase attachment. In Proceedings of
COLING-ACL, volume 2, pages 1079?1085.
Philip Resnik. 1993. Selection and information: a class-based
approach to lexical relationships. Ph.D. thesis, University
of Pennsylvania, UMI Order No. GAX94-13894.
Philip Resnik. 1999. Semantic similarity in a taxonomy: An
information-based measure and its application to problems
of ambiguity in natural language. JAIR, 11:95?130.
Vasile Rus, Dan Moldovan, and Orest Bolohan. 2002. Brack-
eting compound nouns for logic form derivation. In Su-
san M. Haller and Gene Simmons, editors, FLAIRS Confer-
ence. AAAI Press.
Jiri Stetina and Makoto. 1997. Corpus based PP attachment
ambiguity resolution with a semantic dictionary. In Proceed-
ings of WVLC, pages 66?80.
Kristina Toutanova, Christopher D. Manning, and Andrew Y.
Ng. 2004. Learning random walk models for inducing word
dependency distributions. In Proceedings of ICML.
Martin Volk. 2000. Scaling up. using the WWW to resolve PP
attachment ambiguities. In Proceedings of Konvens-2000.
Sprachkommunikation.
Martin Volk. 2001. Exploiting the WWW as a corpus to resolve
PP attachment ambiguities. In Proc. of Corpus Linguistics.
842
A Critique and Improvement of an
Evaluation Metric for Text Segmentation
Lev Pevzner Marti A. Hearsty
Harvard University University of California, Berkeley
The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is be-
coming the standard measure for assessing text segmentation algorithms. However, a theoretical
analysis of the metric finds several problems: the metric penalizes false negatives more heavily
than false positives, overpenalizes near misses, and is affected by variation in segment size dis-
tribution. We propose a simple modification to the Pk metric that remedies these problems. This
new metric?called WindowDiff?moves a fixed-sized window across the text and penalizes the
algorithm whenever the number of boundaries within the window does not match the true number
of boundaries for that window of text.
1. Introduction
Text segmentation is the task of determining the positions at which topics change in
a stream of text. Interest in automatic text segmentation has blossomed over the last
few years, with applications ranging from information retrieval to text summariza-
tion to story segmentation of video feeds. Early work in multiparagraph discourse
segmentation examined the problem of subdividing texts into multiparagraph units
that represent passages or subtopics. An example, drawn from Hearst (1997), is a 21-
paragraph science news article, called ?Stargazers,? whose main topic is the existence
of life on earth and other planets. Its contents can be described as consisting of the
following subtopic discussions (numbers indicate paragraphs):
1?3 Introduction: The search for life in space
4?5 The moon?s chemical composition
6?8 How early earth-moon proximity shaped the moon
9?12 How the moon helped life evolve on earth
13 Improbability of the earth-moon system
14?16 Binary/trinary star systems make life unlikely
17?18 The low probability of nonbinary/trinary systems
19?20 Properties of earth?s sun that facilitate life
21 Summary
The TextTiling algorithm (Hearst 1993, 1994, 1997) attempts to recognize these
subtopic changes by making use of patterns of lexical co-occurrence and distribution;
subtopic boundaries are assumed to occur at the point in the documents at which large
shifts in vocabulary occur. Many others have used this technique, or slight variations
 Harvard University, 380 Leverett Mail Center, Cambridge, MA 02138. E-mail:
pevzner@post.harvard.edu
y University of California, Berkeley, 102 South Hall #4600, Berkeley, CA 94720. E-mail:
hearst@sims.berkeley.edu
c? 2002 Association for Computational Linguistics
Computational Linguistics Volume 28, Number 1
of it, for subtopic segmentation (Nomoto and Nitta 1994; Hasnah 1996; Richmond,
Smith, and Amitay 1997; Heinonen 1998; Boguraev and Neff 2000). Other techniques
use clustering and/or similarity matrices based on word co-occurrences (Reynar 1994;
Yaari 1997; Choi 2000), and still others use machine learning techniques to detect cue
words, or hand-selected cue words to detect segment boundaries (Passonneau and
Litman 1993; Beeferman, Berger, and Lafferty 1997; Manning 1998).
Researchers have explored the use of this kind of document segmentation to im-
prove automated summarization (Salton et al 1994; Barzilay and Elhadad 1997; Kan,
Klavans, and McKeown 1998; Mittal et al 1999; Boguraev and Neff 2000) and auto-
mated genre detection (Karlgren 1996). Text segmentation issues are also important
for passage retrieval, a subproblem of information retrieval (Hearst and Plaunt 1993;
Salton, Allan, and Buckley 1993; Callan 1994; Kaszkiel and Zobel 1997). More recently,
a great deal of interest has arisen in using automatic segmentation for the detection
of topic and story boundaries in news feeds (Mani et al 1997; Merlino, Morey, and
Maybury 1997; Ponte and Croft 1997; Hauptmann and Witbrock 1998; Allan et al
1998; Beeferman, Berger, and Lafferty 1997, 1999). Sometimes segmentation is done
at the clause level, for the purposes of detecting nuances of dialogue structure or for
more sophisticated discourse-processing purposes (Morris and Hirst 1991; Passonneau
and Litman 1993; Litman and Passonneau 1995; Hirschberg and Nakatani 1996; Marcu
2000). Some of these algorithms produce hierarchical dialogue segmentations whose
evaluation is outside the scope of this discussion.
1.1 Evaluating Segmentation Algorithms
There are two major difficulties associated with evaluating algorithms for text seg-
mentation. The first is that since human judges do not always agree where boundaries
should be placed and how fine grained an analysis should be, it is difficult to choose
a reference segmentation for comparison. Some evaluations circumvent this difficulty
by detecting boundaries in sets of concatenated documents, where there can be no dis-
agreements about the fact of the matter (Reynar 1994; Choi 2000); others have several
human judges make ratings to produce a ?gold standard.?
The second difficulty with evaluating these algorithms is that for different applica-
tions of text segmentation, different kinds of errors become important. For instance, for
information retrieval, it can be acceptable for boundaries to be off by a few sentences?
a condition called a near miss?but for news boundary detection, accurate placement
is crucial. For this reason, some researchers prefer not to measure the segmentation
algorithm directly, but consider its impact on the end application (Manning 1998; Kan,
Klavans, and McKeown 1998). Our approach to these two difficulties is to evaluate al-
gorithms on real segmentations using a ?gold standard? and to develop an evaluation
algorithm that suits all applications reasonably well.
Precision and recall are standard evaluation measures for information retrieval
tasks and are often applied to evaluation of text segmentation algorithms as well.
Precision is the percentage of boundaries identified by an algorithm that are indeed
true boundaries; recall is the percentage of true boundaries that are identified by
the algorithm. However, precision and recall are problematic for two reasons. The
first is that there is an inherent trade-off between precision and recall; improving
one tends to cause the score for the other to decline. In the segmentation example,
positing more boundaries will tend to improve the recall but at the same time reduce
the precision. Some evaluators use a weighted combination of the two known as
the F-measure (Baeza-Yates and Ribeiro-Neto 1999), but this is difficult to interpret
(Beeferman, Berger, and Lafferty 1999). Another approach is to plot a precision-recall
curve, showing the scores for precision at different levels of recall.
20
Pevzner and Hearst An Evaluation Metric for Text Segmentation
Figure 1
Two hypothetical segmentations of the same reference (ground truth) document segmentation.
The boxes indicate sentences or other units of subdivision, and spaces between boxes indicate
potential boundary locations. Algorithm A-0 makes two near misses, while Algorithm A-1
misses both boundaries by a wide margin and introduces three false positives. Both
algorithms would receive scores of 0 for both precision and recall.
Another problem with precision and recall is that they are not sensitive to near
misses. Consider, for example, a reference segmentation and the results obtained by
two different text segmentation algorithms, as depicted in Figure 1. In both cases, the
algorithms fail to match any boundary precisely; both receive scores of 0 for precision
and recall. However, Algorithm A-0 is close to correct in almost all cases, whereas
Algorithm A-1 is entirely off, adding extraneous boundaries and missing important
boundaries entirely. In some circumstances, it would be useful to have an evaluation
metric that penalizes A-0 less harshly than A-1.
1.2 The Pk Evaluation Metric
Beeferman, Berger, and Lafferty (1997) introduce a new evaluation metric that attempts
to resolve the problems with precision and recall, including assigning partial credit to
near misses. They justify their metric as follows (page 43):
Segmentation : : : is about identifying boundaries between successive
units of information in a text corpus. Two such units are either related
or unrelated by the intent of the document author. A natural way
to reason about developing a segmentation algorithm is therefore to
optimize the likelihood that two such units are correctly labeled as
being related or being unrelated. Our error metric P is simply the
probability that two sentences drawn randomly from the corpus are correctly
identified as belonging to the same document or not belonging to the same
document.
The derivation of P is rather involved, and a much simpler version is adopted
in the later work (Beeferman, Berger, and Lafferty 1999) and by others. This version,
referred to as Pk, is calculated by setting k to half of the average true segment size
and then computing penalties via a moving window of length k. At each location, the
algorithm determines whether the two ends of the probe are in the same or differ-
ent segments in the reference segmentation and increases a counter if the algorithm?s
segmentation disagrees. The resulting count is scaled between 0 and 1 by dividing
by the number of measurements taken. An algorithm that assigns all boundaries cor-
rectly receives a score of 0. Beeferman, Berger, and Lafferty (1999) state as part of
21
Computational Linguistics Volume 28, Number 1
Figure 2
An illustration of how the Pk metric handles false negatives. The arrowed lines indicate the
two poles of the probe as it moves from left to right; the boxes indicate sentences or other
units of subdivision; and the width of the window (k) is four, meaning four potential
boundaries fall between the two ends of the probe. Solid lines indicate no penalty is assigned;
dashed lines indicate a penalty is assigned. Total penalty is always k for false negatives.
the justification for this metric that, to discourage ?cheating? of the metric, degener-
ate algorithms?those that place boundaries at every position, or place no boundaries
at all?are assigned (approximately) the same score. Additionally, the authors define
a false negative (also referred to as a miss) as a case when a boundary is present in
the reference segmentation but missing in the algorithm?s hypothesized segmentation,
and a false positive as an assignment of a boundary that does not exist in the reference
segmentation.
2. Analysis of the Pk Error Metric
The Pk metric is fast becoming the standard among researchers working in text seg-
mentation (Allan et al 1998; Dharanipragada et al 1999; Eichmann et al 1999; van
Mulbregt et al 1999; Choi 2000). However, we have reservations about this metric.
We claim that the fundamental premise behind it is flawed; additionally, it has sev-
eral significant drawbacks, which we identify in this section. In the remainder of the
paper, we suggest modifications to resolve these problems, and we report the results
of simulations that validate the analysis and suggest that the modified metric is an
improvement over the original.
2.1 Problem 1: False Negatives Penalized More Than False Positives
Assume a text with segments of average size 2k, where k is the distance between
the two ends of the Pk probe. If the algorithm misses a boundary?produces a false
negative?it receives k penalties. To see why, suppose S1 and S2 are two segments
of length 2k, and the algorithm misses the transition from S1 to S2. When Pk sweeps
across S1, if both ends of the probe point to sentences that are inside S1, the two
sentences are in the same segment in both the reference and the hypothesis, and no
penalty is incurred. When the right end of the probe crosses the reference boundary
between S1 and S2, it will start recording nonmatches, since the algorithm assigns the
two sentences to the same segment, while the reference does not. This circumstance
happens k times, until both ends of the probe point to sentences that are inside S2.
(See Figure 2.) This analysis assumes average size segments; variation in segment size
is discussed below, but does not have a large effect on this result.
22
Pevzner and Hearst An Evaluation Metric for Text Segmentation
Figure 3
An illustration of how the Pk metric handles false positives. Notation is as in Figure 2. Total
penalty depends on the distance between the false positive and the relevant correct
boundaries; on average, it is k2 , assuming a uniform distribution of boundaries across the
document. This example shows the consequences of two different locations of false positives:
on the left, the penalty is k2 ; on the right, it is k.
Now, consider false positives. A false positive occurs when the algorithm places
a boundary at some position where there is no boundary in the reference segmenta-
tion. The number of times that this false positive is noted by Pk depends on where
exactly inside S2 the false positive occurs. (See Figure 3.) If it occurs in the middle
of the segment, the false positive is noted k times (as seen on the right-hand side of
Figure 3). If it occurs j < k sentences from the beginning or the end of the segment, the
segmentation is penalized j times. Assuming uniformly distributed false positives, on
average a false positive is noted k2 times by the metric?half the rate for false negatives.
This average increases with segment size, as we will discuss later, and changes if one
assumes different distributions of false positives throughout the document. However,
this does not change the fact that in most cases, false positives are penalized some
amount less than false negatives.
This is not an entirely undesirable side effect. This metric was devised to take into
account how close an assigned boundary is to the true one, rather than just marking
it as correct or incorrect. This method of penalizing false positives achieves this goal:
the closer the algorithm?s boundary is to the actual boundary, the less it is penalized.
However, overpenalizing false negatives to do this is not desirable.
One way to fix the problem of penalizing false negatives more than false positives
is to double the false positive penalty (or halve the false negative penalty). However,
this would undermine the probabilistic nature of the metric. In addition, doubling the
penalty may not always be the correct solution, since segment size will vary from the
average, and false positives are not necessarily uniformly distributed throughout the
document.
2.2 Problem 2: Number of Boundaries Ignored
Another important problem with the Pk metric is that it allows some errors to go unpe-
nalized. In particular, it does not take into account the number of segment boundaries
between the two ends of the probe. (See Figure 4.) Let ri indicate the number of bound-
aries between the ends of the probe according to the reference segmentation, and let ai
indicate the number of boundaries proposed by some text segmentation algorithm for
the same stretch of text. If ri = 1 (the reference segmentation indicates one boundary)
and ai = 2 (the algorithm marks two boundaries within this range), then the algorithm
makes at least one false positive (spurious boundary) error. However, the evaluation
metric Pk does not assign a penalty in this situation. Similarly, if ri = 2 and ai = 1, the
23
Computational Linguistics Volume 28, Number 1
Figure 4
An illustration of the fact that the Pk metric fails to penalize false positives that fall within k
sentences of a true boundary. Notation is as in Figure 2.
algorithm has made at least one false negative (missing boundary) error, but it is not
penalized for this error under Pk.
2.3 Problem 3: Sensitivity to Variations in Segment Size
The size of the segment plays a role in the amount that a false positive within the
segment or a false negative at its boundary is penalized. Let us consider false negatives
(missing boundaries) first. As seen above, with average size segments, the penalty for
a false negative is k. For larger segments, it remains at k?it cannot be any larger
than that, since for a given position i there can be at most k intervals of length k
that include that position. As segment size gets smaller, however, the false negative
penalty changes. Suppose we have two segments, A and B, and the algorithm misses
the boundary between them. Then the algorithm will be penalized k times if Size(A)+
Size(B) > 2k, that is, as long as each segment is about half the average size or larger.
The penalty will then decrease linearly with Size(A)+Size(B) so long as k < Size(A)+
Size(B) < 2k. To be more exact, the penalty actually decreases linearly as the size of
either segment decreases below k. This is intuitively clear from the simple observation
that in order to incur a penalty at any range ri for a false negative, it has to be
the case that ri > ai. In order for this to be true, both the segment to the left and
the segment to the right of the missed boundary have to be of size greater than
k; otherwise, the penalty can only be equal to the size of the smaller segment. When
Size(A)+Size(B) < k, the penalty disappears completely, since then the probe?s interval
is larger than the combined size of both segments, making it not sensitive enough to
detect the false negative. It should be noted that fixing Problem 2 would at least
partially fix this bias as well.
Now, consider false positives (extraneous boundaries). For average segment size
and a uniform distribution of false positives, the average penalty is k2 , as described
earlier. In general, in large enough segments, the penalty when the false positive is
a distance d < k from a boundary is d, and the penalty when the false positive is a
distance d > k from a boundary is k. Thus, for larger segments, the average penalty
assuming a uniform distribution becomes larger, because there are more places in the
segment that are at least k positions away from a boundary. The behavior at the edges
of the segments remains the same, though, so the average penalty never reaches k.
Now, consider what happens with smaller segments. Suppose we have a false positive
in Segment A. As Size(A) decreases from 2k to k, the average false positive penalty
decreases linearly with it, because when Size(A) decreases below 2k, the maximum
distance any sentence can be from a boundary becomes less than k. Therefore, the
24
Pevzner and Hearst An Evaluation Metric for Text Segmentation
Figure 5
A reference segmentation and five different hypothesized segmentations with different
properties.
maximum possible penalty for a false positive in A is less than k, and this number
continues to decrease as Size(A) decreases. When Size(A) < k, the false positive penalty
disappears, for the same reason that the false negative penalty disappears for smaller
segments. Again, fixing Problem 2 would go a long way toward eliminating this bias.
Thus, errors in larger-than-average segments increase the penalty slightly (for false
positives) or not at all (for false negatives) as compared to average size segments, while
errors in smaller-than-average segments decrease the penalty significantly for both
types of error. This means that as the variation of segment size increases, the metric
becomes more lenient, since it severely underpenalizes errors in smaller segments,
while not making up for this by overpenalizing errors in larger segments.
2.4 Problem 4: Near-Miss Error Penalized Too Much
Reconsider the segmentation made by Algorithm A-0 in Figure 1. In both cases of
boundary assignment, Algorithm A-0 makes both a false positive and a false negative
error, but places the boundary very close to the actual one. We will call this kind of
error a near-miss error, distinct from a false positive or false negative error. Distinguish-
ing this type of error from ?pure? false positives better reflects the goal of creating a
metric different from precision and recall, since it can be penalized less than a false
negative or a false positive.
Now, consider the algorithm segmentations shown in Figure 5. Each of the five
algorithms makes a mistake either on the boundary between the first and second seg-
ment of the reference segmentation, or within the second segment. How should these
various segmentations be penalized? In the analysis below, we assume an application
for which it is important not to introduce spurious boundaries. These comparisons
will most likely vary depending on the goals of the target application.
Algorithm A-4 is arguably the worst of the examples, since it has a false positive
and a false negative simultaneously. Algorithms A-0 and A-2 follow: they contain a
pure false negative and false positive, respectively. Comparing Algorithms A-1 and
A-3, we see that Algorithm A-3 is arguably better, because it recognizes that only one
boundary is present rather than two. Algorithm A-1 does not recognize this, and inserts
an extra segment. Even though Algorithm A-1 actually places a correct boundary, it
also places an erroneous boundary, which, although close to the actual one, is still
a false positive?in fact, a pure false positive. For this reason, Algorithm A-3 can be
considered better than Algorithm A-1.
25
Computational Linguistics Volume 28, Number 1
Now, consider how Pk treats the five types of mistakes above. Again, assume the
first and second segments in the reference segmentation are average size segments.
Algorithm A-4 is penalized the most, as it should be. The penalty is as much as 2k if the
false positive falls in the middle of Segment C, and it is > k as long as the false positive
is a distance > k2 from the actual boundary between the first and second reference
segments. The penalty is large because the metric catches both the false negative
and the false positive errors. The segmentations assigned by Algorithms A-0 and A-2
are treated as discussed earlier in conjunction with Problem 1: the one assigned by
Algorithm A-0 has a false negative and thus incurs a penalty of k, and the one assigned
by Algorithm A-2 has a false positive, and thus incurs a penalty of k. Finally, consider
the segmentations assigned by Algorithms A-1 and A-3, and suppose that both contain
an incorrect boundary some small distance e from the actual one. Then the penalty for
Algorithm A-1 is e, while the penalty for Algorithm A-3 is 2e. This should not be the
case; Algorithm A-1 should be penalized more than Algorithm A-3, since a near-miss
error is better than a pure false positive, even if it is close to the boundary.
2.5 Problem 5: What Do the Numbers Mean?
Pk is nonintuitive because it measures the probability that two sentences k units apart
are incorrectly labeled as being in different segments, rather than directly reflecting
the competence of the algorithm. Although perfect algorithms score 0, and various de-
generate ones score 0:5, numerical interpretation and comparison are difficult because
it is not clear how the scores are scaled.
3. A Solution
It turns out that a simple change to the error metric algorithm remedies most of the
problems described above, while retaining the desirable characteristic of penalizing
near misses less than pure false positives and pure false negatives. The amended
metric, which we call WindowDiff, works as follows: for each position of the probe,
simply compare the number of reference segmentation boundaries that fall in this
interval (ri) with the number of boundaries that are assigned by the algorithm (ai).
The algorithm is penalized if ri 6= ai (which is computed as jri ? aij > 0).
More formally,
WindowDi (ref , hyp) =
1
N ? k
N?k
X
i=1
(jb(ref i, ref i+k)? b(hyp i, hyp i+k)j > 0),
where b(i, j) represents the number of boundaries between positions i and j in the text
and N represents the number of sentences in the text.
This approach clearly eliminates the asymmetry between the false positive and
false negative penalties seen in the Pk metric. It also catches false positives and false
negatives within segments of length less than k.
To understand the behavior of WindowDiff with respect to the other problems,
consider again the examples in Figure 5. This metric penalizes Algorithm A-4 (which
contains both a false positive and a false negative) the most, assigning it a penalty
of about 2k. Algorithms A-0, A-1, and A-2 receive the same penalty (about k), and
Algorithm A-3 receives the smallest penalty (2e, where e is the offset from the actual
boundary, presumed to be much smaller than k). Thus, although it makes the mistake
of penalizing Algorithm A-1 as much as Algorithms A-0 and A-2, it correctly recog-
nizes that the error made by Algorithm A-3 is a near miss and assigns it a smaller
penalty than Algorithm A-1 or any of the others. We argue that this kind of error is
less detrimental than the errors made by Pk. WindowDiff successfully distinguishes
26
Pevzner and Hearst An Evaluation Metric for Text Segmentation
the near-miss error as a separate kind of error and penalizes it a different amount,
something that Pk is unable to do.
We explored a weighted version of WindowDiff, in which the penalty is weighted
by the difference jri? aij. However, the results of the simulations were nearly identical
with those of the nonweighted version of this metric, so we do not consider the
weighted version further.
4. Validation via Simulations
This section describes a set of simulations that verify the theoretical analysis of the
Pk metric presented above. It also reports the results of simulating two alternatives,
including the proposed solution just described.
For the simulation runs described below, three metrics were implemented:
 the Pk metric;
 the Pk metric modified to double the false positive penalty (henceforth
P0k); and
 our proposed alternative, WindowDiff (henceforth WD), which counts
the number of segment boundaries between the two ends of the probe
and assigns a penalty if this number is different for the experimental and
reference segmentations.
In these studies, a single trial consists of generating a reference segmentation of
1,000 segments with some distribution, generating different experimental segmenta-
tions of a specific type 100 times, computing the metric based on the comparison of
the reference and experimental segmentations, and averaging the 100 results. For ex-
ample, we might generate a reference segmentation R, then generate 100 experimental
segmentations that have false negatives with probability 0.5, and then compute the
average of their Pk penalties. We carried out 10 such trials for each experiment and
averaged the average penalties over these trials.
4.1 Variation in the Segment Sizes
The first set of tests was designed to test the metric?s performance on texts with
different segment size distributions (Problem 3). We generated four sets of reference
segmentations with segment size uniformly distributed between two numbers. Note
that the units of segmentation are deliberately left unspecified. So a segment of size 25
can refer to 25 words, clauses, or sentences?whichever is applicable to the task under
consideration. Also note that the same tests were run using larger segment sizes than
those reported here, with the results remaining nearly identical.
For these tests, the mean segment size was held constant at 25 for each set of
reference segments, in order to produce distributions of segment size with the same
means but different variances. The four ranges of segment sizes were (20, 30), (15, 35),
(10, 40), and (5, 45). The results of these tests are shown in Table 1. The tests used the
following types of experimental segmentations:
 FN: segmentation with false negative probability 0.5 at each boundary;
 FP: segmentation with false positive probability 0.5 in each segment,
with the probability uniformly distributed within each segment; and
 FNP: segmentation with false positive probability 0.5 (uniformly
distributed), and false negative probability 0.5.
27
Computational Linguistics Volume 28, Number 1
Table 1
Average error score for Pk, P0k, and WD over 10 trials of 100
measurements each, shown by segment size distribution range.
(a) False negatives were placed with probability 0.5 at each
boundary; (b) false positives were placed with probability 0.5,
uniformly distributed within each segment; and (c) both false
negatives and false positives were placed with probability 0.5.
(a) False negatives, p = 0:5
(20, 30) (15, 35) (10, 40) (5, 45)
Pk 0.245 0.245 0.240 0.223
P0k 0.245 0.245 0.240 0.223
WD 0.245 0.245 0.242 0.237
(b) False positives, p = 0:5
(20, 30) (15, 35) (10, 40) (5, 45)
Pk 0.128 0.122 0.112 0.107
P0k 0.256 0.245 0.225 0.213
WD 0.240 0.241 0.238 0.236
(c) False positives and false negatives, p = 0:5
(20, 30) (15, 35) (10, 40) (5, 45)
Pk 0.317 0.309 0.290 0.268
P0k 0.446 0.432 0.403 0.375
WD 0.376 0.370 0.357 0.343
The results indicate that variation in segment size does make a difference, but not
a very big one. (As we will show, the differences are similar when we use a smaller
probability of false negative/positive occurrence.) The Pk value for the (20, 30) range
with FN segmentation is on average 0.245, and it decreases to 0.223 for the (5, 45)
range. Similarly, the FP segmentation decreases from 0.128 for the (20, 30) range to
0.107 for the (5, 45) range, and the FNP segmentation decreases from 0.317 for the (20,
30) range to 0.268 for the (5, 45) range. Thus, variation in segment size has an effect
on Pk, as predicted.
Note that for false negatives, the Pk value for the (20, 30) range is not much
different than for the (15, 35) range. This is expected since there are no segments of
size less than k (12.5) in these conditions. For the (10, 40) range, the Pk value is slightly
smaller; and for the (5, 45) range, it is smaller still. These results are to be expected,
since more segments in these ranges will be of length less than k.
For the FP segmentations, on the other hand, the decrease in Pk value is more
pronounced, falling from 0.128 to 0.107 as the segment size range changes from (20,
30) to (5, 45). This is also consistent with our earlier analysis of the behavior of the
metric on false positives as segment size decreases. Notice that the difference in Pk
values between (15, 35) and (10, 40) is slightly larger than the other two differences.
This happens because for segment sizes < k, the false positive penalty disappears
completely. The results for the FNP segmentation are consistent with what one would
expect of a mix of the FN and FP segmentations.
Several other observations can be made from Table 1. We can begin to make some
judgments about how the metric performs on algorithms prone to different kinds of
errors. First, Pk penalizes false negatives about twice as much as false positives, as
28
Pevzner and Hearst An Evaluation Metric for Text Segmentation
predicted by our analysis. The experimental segmentations in Table 1a contain on
average 500 false negatives, while the ones in Table 1b contain on average 500 false
positives, but the penalty for the Table 1b segmentations is consistently about half
that for those in Table 1a. Thus, algorithms prone to false positives are penalized less
harshly than those prone to false negatives.
The table also shows the performance of the two other metrics. P0k simply doubles
the false positive penalty, while WD counts and compares the number of boundaries
between the two ends of the probe, as described earlier. Both P0k and WD appear to
solve the problem of underpenalizing false positives, but WD has the added benefit of
being more stable across variations in segment size distribution. Thus, WD essentially
solves Problems 1, 2, and 3.
Table 1c shows that for the FNP segmentation (in which both false positives and
false negatives occur), there is a disparity between the performances of P0k and WD.
It appears that P0k is harsher in this situation. From the above discussion, we know
that WD is more lenient in situations where a false negative and a false positive occur
near each other (where ?near? means within a distance of k2 ) than P
0
k is. However,
P0k is more lenient for pure false positives that occur close to boundaries. Thus, it is
not immediately clear why P0k is harsher in this situation, but a more detailed look
provides the answer.
Let us begin the analysis by trying to explain why Pk scores for the FNP seg-
mentation make sense. The FNP segmentation places both false negatives and false
positives with probability 0.5. Since we are working with reference segmentations of
1,000 segments, this means 500 missed boundaries and 500 incorrect boundaries. Since
the probabilities are uniformly distributed across all segments and all boundaries, on
average one would expect the following distribution of errors:
 250 false positives with no false negative within k sentences of them
(Type A);
 250 false negatives with no false positive within k sentences of them
(Type B); and
 250 ?joint? errors, where a false positive and a false negative occur
within k sentences of each other (Type C).
A Type A error is a standard false positive, so the average penalty is k2 . A Type B
error is a standard false negative, so the average penalty is k. It remains to fig-
ure out what the average penalty is for a Type C error. Modeling the behavior of
the metric, a Type C error occurrence in which a false positive and a false nega-
tive are some distance e < k from each other incurs a penalty of 2e, where e is as-
signed for the false positive and another e is assigned for the false negative. This
may range from 0 to 2k, and since error distribution is uniform, the penalty is k
on average?the same as for a regular false negative. To translate this into actual
values, we assume the metric is linear with respect to the number of errors (a rea-
sonable assumption, supported by our experiments). Thus, if Pk outputs a penalty
of p for 500 false negatives, it would have a penalty of p2 for 250 false negatives.
Let a be the penalty for 500 Type A errors, b the penalty for 500 Type B errors,
and c the penalty for 500 Type C errors; then the penalty for the FNP segmenta-
tion is p = a2 +
b
2 +
c
2 . Assuming the metric is linear, we know that c = b = 2a
(because Pk penalized false negatives twice as much as false positives on average).
We can thus substitute either b or 2a for c. We choose to substitute 2a, because Pk is
strongly affected by segment size variation for Type A and Type C errors, but not for
29
Computational Linguistics Volume 28, Number 1
Type B errors. Thus, replacing c with 2a is more accurate. Performing the substitu-
tion, we have p = 3  a2 + b2 . We have a and b from the FP and FN data, respectively,
so we can compute p. The results, arranged by segment size variation, are as fol-
lows:
(20, 30) (15, 35) (10, 40) (5, 45)
Estimate 0.315 0.306 0.288 0.272
Actual 0.317 0.309 0.290 0.268
As can easily be seen, the estimate produced using this method is very similar to the
actual Pk value.
The same sort of analysis applies for P0k and WD. In P
0
k, Type A errors are penalized
k on average, since the false positive penalty is doubled. Type B errors have an average
penalty of k, as for Pk. Type C errors have an average penalty of 3e, where 2e is
assigned for the false positive and e is assigned for the false negative. This means that
the average penalty for a Type C error is 3  k2 . Since we know that c = 1:5a by the
linear metric assumption, we have p = a2 +
b
2 + 1:5  a2 = 5  a4 + b2 (the choice of 1:5a
over 1:5b was made for the same reason as the choice of 2a over b in the calculations
for Pk). The results, arranged by segment size variation, are as follows:
(20, 30) (15, 35) (10, 40) (5, 45)
Estimate 0.443 0.429 0.401 0.378
Actual 0.446 0.432 0.403 0.375
Finally, WD incurs an average penalty of k for both Type A and Type B errors. For
Type C errors, the penalty is 2e, so it is also k on average. Thus, we get p = a2 +
b
2 +
a
2 =
a + b2 . The results, arranged by segment size variation, are as follows:
(20, 30) (15, 35) (10, 40) (5, 45)
Estimate 0.363 0.364 0.359 0.355
Actual 0.376 0.370 0.357 0.343
These estimates do not correspond to the actual results quite as closely as the estimates
for Pk and P0k did, but they are still very close. One reason why these estimates are
a little less accurate is that for WD, Type C errors are more affected by variation in
segment size than either Type A or Type B errors. This is clear from the fact that the
decrease is greater in the actual data than in the estimate.
Table 2 shows data similar to those of Table 1, but using two different probability
values for error occurrence: 0:05 and 0:25. These results have the same tendencies as
those shown above for p = 0:5.
4.2 Variation in the Error Distributions
The second set of tests was designed to assess the performance of the metrics on algo-
rithms prone to different kinds of errors. This would determine whether the metrics
are consistent in applying penalties, or whether they favor certain kinds of errors over
others. For these trials, we generated the reference segmentation using a uniform dis-
tribution of segment sizes in the (15, 35) range. We picked this range because it has
reasonably high segment size variation, but segment size does not dip below k. For the
30
Pevzner and Hearst An Evaluation Metric for Text Segmentation
Table 2
Average error score for Pk, P0k, and WD over 10 trials
of 100 measurements each, shown by segment size
distribution range. (a) False negatives were placed
with probability 0.05 at each boundary; (b) false
positives were placed with probability 0.05,
uniformly distributed within each segment; and (c)
both false negatives and false positives were placed
with probability 0.05. (d) False negatives were placed
with probability 0.25 at each boundary; (e) false
positives were placed with probability 0.25,
uniformly distributed within each segment; and (f)
both false negatives and false positives were placed
with probability 0.25.
(a) False negatives, p = 0:05
(20, 30) (15, 35) (10, 40) (5, 45)
Pk 0.025 0.025 0.024 0.022
P0k 0.025 0.025 0.024 0.022
WD 0.025 0.025 0.024 0.024
(b) False positives, p = 0:05
(20, 30) (15, 35) (10, 40) (5, 45)
Pk 0.013 0.012 0.011 0.011
P0k 0.026 0.025 0.023 0.021
WD 0.024 0.024 0.024 0.024
(c) False positives and false negatives, p = 0:05
(20, 30) (15, 35) (10, 40) (5, 45)
Pk 0.037 0.036 0.035 0.032
P0k 0.050 0.048 0.046 0.042
WD 0.048 0.048 0.048 0.047
(d) False negatives, p = 0:25
(20, 30) (15, 35) (10, 40) (5, 45)
Pk 0.122 0.122 0.121 0.110
P0k 0.122 0.122 0.121 0.110
WD 0.122 0.122 0.122 0.121
(e) False positives, p = 0:25
(20, 30) (15, 35) (10, 40) (5, 45)
Pk 0.064 0.061 0.056 0.053
P0k 0.129 0.123 0.112 0.106
WD 0.121 0.121 0.121 0.120
(f) False positives and false negatives, p = 0:25
(20, 30) (15, 35) (10, 40) (5, 45)
Pk 0.172 0.168 0.161 0.147
P0k 0.236 0.229 0.217 0.200
WD 0.215 0.213 0.211 0.205
31
Computational Linguistics Volume 28, Number 1
Table 3
Average error score for Pk, P0k, and WD over 10 trials of
100 measurements each over the segment distribution
range (15, 35) and with error probabilities of 0.5. The
average penalties computed by the three metrics are
shown for seven different error distributions.
FN FP1 FP2 FP3 FNP1 FNP2 FNP3
Pk .245 .122 .091 .112 .308 .267 .304
P0k .245 .244 .182 .224 .431 .354 .416
WD .245 .240 .236 .211 .370 .341 .363
reasons described above, this means the results will not be skewed by the sensitivity
of Pk and P0k to segment size variations.
The tests analyzed below were performed using the high error occurrence proba-
bilities of 0.5, but similar results were obtained using probabilities of 0.25 and 0.05 as
well. The following error distributions were used:1
 FN: false negatives, probability p = 0:5;
 FP1: false positives uniformly distributed in each segment, probability
p = 0:5;
 FP2: false positives normally distributed around each boundary with
standard deviation equal to 14 the segment size, probability p = 0:5;
 FP3: false positives uniformly distributed throughout the document,
occurring at each point with probability p = number of segmentslength2 (this
corresponds to a 0.5 probability value for each individual segment);
 FNP1: FN and FP1 combined;
 FNP2: FN and FP2 combined;
 FNP3: FN and FP3 combined.
The results are shown in Table 3. Pk penalizes FP2 less than FP1 and FP3, and
FNP2 less than FNP1 and FNP3. This result is as expected. FP2 and FNP2 have false
positives normally distributed around each boundary, which means that more of the
false positives are close to the boundaries and thus are penalized less. If we made the
standard deviation smaller, we would expect this difference to be even more apparent.
P0k penalized FP2 and FNP2 the least in their respective categories, and FP1 and
FNP1 the most, with FP3 and FNP3 falling in between. These results are as expected,
for the same reasons as for Pk. The difference in the penalty for FP1 and FP3 (and
FNP1 vs. FNP3)?for both Pk and P0k, but especially apparent for P
0
k?is interesting. In
FP/FNP1, false positive probability is uniformly distributed throughout each segment,
whereas in FP/FNP3, false positive probability is uniformly distributed throughout the
entire document. Thus, the FP/FNP3 segmentations are more likely to have boundaries
that are very close to each other, since they are not segment dependent, while FP/FNP1
1 Normal distributions were calculated using the gaussrand() function from Box and Muller (1958),
found online at http://www.eskimo.com/scs/C-faq/q13.20.html.
32
Pevzner and Hearst An Evaluation Metric for Text Segmentation
are limited to at most one false positive per segment. This results in P0k assigning
smaller penalties for FP/FNP3, since groups of false positives close together (to be
more exact, within k sentences of each other) would be underpenalized. This difference
is also present in the Pk results, but is about half for obvious reasons.
WD penalized FP1 the most and FP3 the least among the FP segmentations. Among
the FNP segmentations, FNP1 was penalized the most and FNP2 the least. To see why,
we examine the results for the FP segmentations. WD penalizes pure false positives
the same amount regardless of how close they are to a boundary; the only way false
positives are underpenalized is if they occur in bunches. As mentioned earlier, this is
most likely to happen in FP3. It is least likely to happen in FP1, since in FP1 there is a
maximum of one false positive per segment, and this false positive is not necessarily
close to a boundary. In FP2, false positives are also limited to one per segment, but
they are also more likely to be close to boundaries. This increases the likelihood that
2 false positives will be within k sentences of each other and thus makes WD give a
slightly lower score to the FP2 segmentation than to the FP1 segmentation.
Now let us look at the FNP segmentations. FNP3 is penalized less than FNP1
for the same reason described above, and FNP2 is penalized even less than FNP3.
The closer a Type C error is to the boundary, the lower the penalty. FNP2 has more
errors distributed near the boundaries than the others: thus, the FNP2 segmentation
is penalized less than either FNP1 or FNP3.
The same tests were run for different error occurrence probabilities (p = 0:05 and
p = 0:25), achieving results similar to those for p = 0:5 just described. There is a slight
difference for the case of p = 0:05 because the error probability is too small for some of
the trends to manifest themselves. In particular, the differences in the way WD treats
the different segmentations disappear when the error probability is this small.
4.3 Variation in the Error Types
We also performed a small set of tests to verify the theoretical finding that Pk and P0k
overpenalize near-miss errors as compared with pure false positives, and that WD does
the opposite, overpenalizing the pure false positives. Space limitations prevent detailed
reporting of these results, but the simulations did indeed verify these expectations.
5. Conclusions
We have found that the Pk error metric for text segmentation algorithms is affected
by the variation of segment size distribution, becoming slightly more lenient as the
variance increases. It penalizes false positives significantly less than false negatives,
particularly if the false positives are uniformly distributed throughout the document. It
penalizes near-miss errors more than pure false positives of equal magnitude. Finally,
it fails to take into account situations in which multiple boundaries occur between
the two sides of the probe, and it often misses or underpenalizes mistakes in small
segments.
We proposed two modifications to tackle these problems. The first, which we call
P0k, simply doubles the false positive penalty. This solves the problem of overpenalizing
false negatives, but it is not effective at dealing with the other problems. The second,
which we call WindowDiff (WD), counts the number of boundaries between the two
ends of a fixed-length probe, and compares this number with the number of boundaries
found in the same window of text for the reference segmentation. This modification
addresses all of the problems listed above. WD is only slightly affected by variation of
segment size distribution, gives equal weight to the false positive penalty and the false
negative penalty, is able to catch mistakes in small segments just as well as mistakes in
33
Computational Linguistics Volume 28, Number 1
large segments, and penalizes near-miss errors less than pure false positives of equal
magnitude. However, it has some problems of its own. WD penalizes all pure false
positives the same amount regardless of how close they are to an actual boundary.
It is not clear whether this is a good thing or not, but it seems to be preferable to
overpenalizing near misses.
The discussion above addresses Problems 1 through 4 but does not address Prob-
lem 5: how does one interpret the values produced by the metric? From the tests we
have run, it appears that the WD metric grows in a roughly linear fashion with the
difference between the reference and the experimental segmentations. In addition, we
feel that WD is a more meaningful metric than Pk. Comparing two stretches of text to
see how many discrepancies occur between the reference and the algorithm?s result
seems more intuitive than determining how often two text units are incorrectly labeled
as being in different segments.
Acknowledgments
This work was completed while the second
author was a visiting professor at Harvard
University. Both authors thank Barbara
Grosz and Stuart Shieber, without whom
this work would not have happened, and
Freddy Choi for some helpful explanations.
They would also like to thank the
anonymous reviewers for their valuable
comments.
Partial support for the research reported
in this paper was provided by National
Science Foundation Grants IRI-9618848 and
CDA-94-01024.
References
Allan, James, Jaime Carbonell, George
Doddington, Jonathan Yamron, and
Yiming Yang. 1998. Topic detection and
tracking pilot study: Final report. In
Proceedings of the DARPA Broadcast News
Transcription and Understanding Workshop,
pages 194?218, Lansdowne, VA.
Baeza-Yates, Ricardo and Berthier
Ribeiro-Neto. 1999. Modern Information
Retrieval. Addison-Wesley Longman.
Barzilay, Regina and Michael Elhadad. 1997.
Using lexical chains for text
summarization. In Proceedings of the ACL
Intelligent Scalable Text Summarization
Workshop (ISTS?97), Madrid, Spain.
Beeferman, Douglas, Adam Berger, and
John Lafferty. 1997. Text segmentation
using exponential models. In Proceedings
of the 2nd Conference on Empirical Methods in
Natural Language Processing, pages 35?46,
Providence, RI.
Beeferman, Douglas, Adam Berger, and
John Lafferty. 1999. Statistical models of
text segmentation. Machine Learning,
34(1?3):177?210.
Boguraev, Branimir and Mary Neff. 2000.
Discourse segmentation in aid of
document summarization. In Proceedings
of the 33rd Hawaii International Conference on
System Sciences, Maui, HI.
Box, G. E. P. and M. E. Muller. 1958. A note
on the generation of random normal
deviates. Annals of Mathematical Statistics,
29:610?611.
Callan, James P. 1994. Passage-level
evidence in document retrieval. In
Proceedings of the 17th Annual International
ACM/SIGIR Conference, pages 302?310,
Dublin, Ireland.
Choi, Freddy. 2000. Advances in domain
independent linear text segmentation. In
Proceedings of the 1st Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 26?33,
Seattle, WA.
Dharanipragada, S., M. Franz, Jeffrey S.
McCarley, S. Roukos, and Todd Ward.
1999. Story segmentation and topic
detection in the broadcast news domain.
In Proceedings of the DARPA Broadcast News
Workshop, Herndon, VA.
Eichmann, David, Miguel Ruiz, Padmini
Srinivasan, Nick Street, Chris Culy, and
Filippo Menczer. 1999. A cluster-based
approach to tracking, detection and
segmentation of broadcast news. In
Proceedings of the DARPA Broadcast News
Workshop, Herndon, VA.
Hasnah, Ahmad. 1996. Full Text Processing
and Retrieval: Weight Ranking, Text
Structuring, and Passage Retrieval for Arabic
Documents. Ph.D. thesis, Illinois Institute
of Technology.
Hauptmann, Alexander G. and Michael J.
Witbrock. 1998. Story segmentation and
detection of commercials in broadcast
news video. In Proceedings of the Advances
in Digital Libraries Conference,
pages 168?179, Santa Barbara, CA.
Hearst, Marti A. 1993. TextTiling: A
quantitative approach to discourse
34
Pevzner and Hearst An Evaluation Metric for Text Segmentation
segmentation. Technical Report Sequoia
93/24, Computer Science Division,
University of California, Berkeley.
Hearst, Marti A. 1994. Multi-paragraph
segmentation of expository text. In
Proceedings of the 32nd Annual Meeting of the
Association for Computational Linguistics,
pages 9?16, Las Cruces, NM.
Hearst, Marti A. 1997. TextTiling:
Segmenting text into multi-paragraph
subtopic passages. Computational
Linguistics, 23(1):33?64.
Hearst, Marti A. and Christian Plaunt. 1993.
Subtopic structuring for full-length
document access. In Proceedings of the 16th
Annual International ACM/SIGIR
Conference, pages 59?68, Pittsburgh, PA.
Heinonen, Oskari. 1998. Optimal
multi-paragraph text segmentation by
dynamic programming. In Proceedings of
the 17th International Conference on
Computational Linguistics and the 36th
Annual Meeting of the Association for
Computational Linguistics (ACL-COLING
?98), pages 1484?1486, Montreal.
Hirschberg, Julia and Christine H. Nakatani.
1996. A prosodic analysis of discourse
segments in direction-giving monologues.
In Proceedings of the 34th Annual Meeting of
the Association for Computational Linguistics,
pages 286?293, Santa Cruz, CA.
Kan, Min-Yen, Judith L. Klavans, and
Kathleen R. McKeown. 1998. Linear
segmentation and segment relevance. In
Proceedings of the Sixth Workshop on Very
Large Corpora (WVLC 6), pages 197?205,
Montreal.
Karlgren, Jussi. 1996. Stylistic variation in
an information retrieval experiment. In
Proceedings of the 2nd International
Conference on New Methods in Language
Processing (NeMLaP 2), Ankara, Turkey.
Kaszkiel, Marcin and Justin Zobel. 1997.
Passage retrieval revisited. In Proceedings
of the 20th International Conference on
Research and Development in Information
Access (ACM SIGIR), pages 178?185,
Philadelphia, PA.
Litman, Diane J. and Rebecca J. Passonneau.
1995. Combining multiple knowledge
sources for discourse segmentation. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics,
pages 108?115, Cambridge, MA.
Mani, Inderjeet, David House, Mark
Maybury, and Morgan Green. 1997.
Towards content-based browsing of
broadcast news video. In Mark Maybury,
editor, Intelligent Multimedia Information
Retrieval. AAAI/MIT Press,
pages 241?258.
Manning, Christopher D. 1998. Rethinking
text segmentation models: An information
extraction case study. Technical Report
SULTRY-98-07-01, University of Sydney.
Marcu, Daniel. 2000. The Theory and Practice
of Discourse Parsing and Summarization.
MIT Press.
Merlino, Andy, Daryl Morey, and Mark
Maybury. 1997. Broadcast news
navigation using story segmentation. In
Proceedings of the ACM International
Multimedia Conference, pages 157?164,
Seattle, WA.
Mittal, Vibhu, Mark Kantrowitz, Jade
Goldstein, and Jaime Carbonell. 1999.
Selecting text spans for document
summaries: Heuristics and metrics. In
Proceedings of the 16th Annual Conference on
Artificial Intelligence (AAAI ?99),
pages 467?473, Orlando, FL.
Morris, Jane and Graeme Hirst. 1991.
Lexical cohesion computed by thesaural
relations as an indicator of the structure of
text. Computational Linguistics, 17(1):21?48.
Nomoto, Tadashi and Yoshihiko Nitta. 1994.
A grammatico-statistical approach to
discourse partitioning. In Proceedings of the
15th International Conference on
Computational Linguistics (COLING?94),
pages 1145?1150, Kyoto, Japan.
Passonneau, Rebecca J. and Diane J. Litman.
1993. Intention-based segmentation:
Human reliability and correlation with
linguistic cues. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics, pages 148?155,
Columbus, OH.
Ponte, Jay and Bruce Croft. 1997. Text
segmentation by topic. In Proceedings of the
1st European Conference on Research and
Advanced Technology for Digital Libraries,
pages 113?125.
Reynar, Jeffrey C. 1994. An automatic
method of finding topic boundaries. In
Proceedings of the Student Session of the 32nd
Annual Meeting of the Association for
Computational Linguistics, pages 331?333,
Las Cruces, NM.
Richmond, Korin, Andrew Smith, and Einat
Amitay. 1997. Detecting subject
boundaries within text: A language
independent statistical approach. In
Proceedings of the Second Conference on
Empirical Methods in Natural Language
Processing, pages 47?54. Association for
Computational Linguistics.
Salton, Gerard, James Allan, and Chris
Buckley. 1993. Approaches to passage
retrieval in full text information systems.
In Proceedings of the 16th Annual
International ACM/SIGIR Conference, pages
35
Computational Linguistics Volume 28, Number 1
49?58, Pittsburgh, PA.
Salton, Gerard, James Allan, Chris Buckley,
and Amit Singhal. 1994. Automatic
analysis, theme generation, and
summarization of machine-readable texts.
Science, 264(5164):1421?1426.
van Mulbregt, P., Ira Carp, Larry Gillick,
Stephen Lowe, and Jonathan Yamron.
1999. Segmentation of automatically
transcribed broadcast news text. In
Proceedings of the DARPA Broadcast News
Workshop, Herndon, VA.
Yaari, Yaakov. 1997. Segmentation of
expository text by hierarchical
agglomerative clustering. In Recent
Advances in NLP (RANLP?97), Bulgaria.
36
Category-Based Pseudowords
Preslav I. Nakov
EECS, UC Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Marti A. Hearst
SIMS, UC Berkeley
Berkeley, CA 94720
hearst@sims.berkeley.edu
Abstract
A pseudoword is a composite comprised of two
or more words chosen at random; the individual
occurrences of the original words within a text
are replaced by their conflation. Pseudowords
are a useful mechanism for evaluating the im-
pact of word sense ambiguity in many NLP
applications. However, the standard method
for constructing pseudowords has some draw-
backs. Because the constituent words are cho-
sen at random, the word contexts that surround
pseudowords do not necessarily reflect the con-
texts that real ambiguous words occur in. This
in turn leads to an optimistic upper bound on
algorithm performance. To address these draw-
backs, we propose the use of lexical categories
to create more realistic pseudowords, and eval-
uate the results of different variations of this
idea against the standard approach.
1 Introduction
In order to evaluate a word sense disambiguation (WSD)
algorithm in a new language or domain, a sense-tagged
evaluation corpus is needed, but this is expensive to pro-
duce manually. As an alternative, researchers often use
pseudowords. To create a pseudoword, two or more
randomly-chosen words (e.g., banana and door) are se-
lected and their individual occurrences are replaced by
their conflation (e.g., banana-door). Since their introduc-
tion (Gale et al, 1992; Schuetze, 1992), pseudowords
have been accepted as an upper bound of the true accu-
racy of algorithms that assign word sense distinctions.
In most cases, constituent words are chosen entirely
randomly. When used to evaluate a real WSD system on
the SENSEVAL1 corpus, pseudowords were found to be
optimistic in their estimations compared to real ambigu-
ous words with the same distribution (Gaustad, 2001).
Real ambiguous words often have senses that are similar
in meaning, and thus difficult to distinguish (as measured
by low inter-annotator agreement), while pseudowords,
because they are randomly chosen, are highly likely to
combine semantically distinct words. Another drawback
is that the results produced using pseudowords are dif-
ficult to characterize in terms of the types of ambiguity
they model.
To create more plausibly-motivated pseudoword pair-
ings, we introduce the use of lexical category member-
ship for pseudoword generation. The main idea is to take
note of the relative frequencies at which pairs of lexi-
cal categories tend to represent real ambiguous words,
and then use unambiguous words drawn from those cate-
gories to generate pseudowords. In the remainder of this
paper we describe the category-based pseudoword gener-
ation process and evaluate the results against the standard
methods and against a real word sense disambiguation
task.
2 MeSH and Medline
In this paper we use the MeSH (Medical Subject Head-
ings) lexical hierarchy1, but the approach should be
equally applicable to other domains using other thesauri
and ontologies. In MeSH, each concept is assigned one
or more alphanumeric descriptor codes corresponding
to particular positions in the hierarchy. For example,
A (Anatomy), A01 (Body Regions), A01.456 (Head),
A01.456.505 (Face), A01.456.505.420 (Eye). Eye is
ambiguous according to MeSH and has a second code:
A09.371 (A09 represents Sense Organs).
In the studies reported here, we truncate the MeSH
code at the first period. This allows for generalization
over different words; e.g., for eye, we discriminate be-
tween senses represented by A01 and A09. This trun-
cation reduces the average number of senses per token
from 2.12 to 1.39, and the maximum number of ambigu-
ity classes for a given word to 7; 71.18% of the tokens
have a single class and 22.14% have two classes. From
a collection of 180,226 abstracts from Medline 20032,
1http://www.nlm.nih.gov/mesh
2235 MB of plain text, after XML removal, from files med-
Ambig. pair Pair freq. Class 1 freq. Class 2 freq
{A11,A15} 16127 49350 3417
{A12,A15} 13662 7403 3417
{D12,D24} 12608 28805 17064
{E05,H01} 11753 17506 40744
{I01,N03} 6988 7721 11046
{A02,A10} 6834 4936 14083
Table 1: Most frequent ambiguous 2-category pairs.
training was done on 2/3 of the abstracts (120,150) and
testing on the remaining 1/3 (60,076).
3 Pseudoword Generation
For the creation of pseudowrods with two-sense ambigui-
ties, we first determined which ambiguous words fall into
exactly two MeSH categories and built a list L of pairs
(see Table 1). We then generated pseudowords with the
following characteristics:
? The two possible pseudoword categories represent a
pair that is really seen in the testing corpus and thus
needs to be disambiguated;
? The number of pseudowords drawn from a particular
pair is proportional to its frequency;
? Multi-word concepts can be used as pseudoword
elements: e.g., ion-exchange chromatography
and long-term effects can be conflated as ion-
exchange chromatography long-term effects
? Only unambiguous words are used as pseudoword
constituents.
An important aspect of pseudoword creation is the rel-
ative frequencies of the underlying words. Since the stan-
dard baseline for a WSD algorithm is to always choose
the most frequent sense, a baseline that is evaluated on
words whose senses are evenly balanced will be expected
to do more poorly than one tested against words that are
heavily skewed towards one sense (Sanderson & van Ri-
jsbergen, 1999).
In naturally occurring text, the more frequent sense for
the two-sense distinction is reported to occur 92% of the
time on average; this result has been found both on the
CACM collection and on the WordNet SEMCOR sense-
tagged corpus (Sanderson & van Rijsbergen, 1999).
However, the challenge for WSD programs is to work on
the harder cases, and the artificially constructed SENSE-
VAL1 corpus has more evenly distributed senses (Gaus-
tad, 2001).
In these experiments, we explicitly compare pseu-
dowords whose underlying word frequencies are even
line03n0201.xml through med-line03n0209.xml.
w1 w2 pair #w1 #w2
artifact triton {E05,H01} 55 40
humerus mucus memb. {A02,A10} 51 38
lovastatin palmitic acid {D04,D10} 35 54
child abuse Minnesota {I01,Z01} 39 45
thumb pupils {A01,A09} 56 38
haptoglobin hla antigens {D12,D24} 46 53
Table 2: Sample pseudowords.
against those that are skewed. To generate pseudowords
with more uniform underlying distributions, we first cal-
culate the expected testing corpus frequency of those
words wi that have been unambiguously mapped to
MeSH and whose class is used in at least one pair in L. In
this collection the expected frequency was E = 45.21 with
a standard deviation of 451.19. We then built a list W of
all MeSH concepts mapped in the text that have a class
used in a pair in L and whose frequency is in the interval
[E/2;3E/2], i.e. [34;56]. This yields a list of concepts that
could potentially be combined in 64,596 pseudowords for
evaluation of the WSD algorithm performance over the
classes in L.
We then generated a random subset of 1,000 pseu-
dowords (88,758 instances) out of the possible 64,596 by
applying the following importance sampling procedure:
1) Select a category pair c1,c2 from L by sampling
from a multinomial distribution whose parameters are
proportional to the frequencies of the elements of L.
2) Sample uniformly to draw two random distinct
words w1 and w2 from W whose classes correspond to
the classes selected in step 1).
3) If the word pair w1,w2 has been sampled already, go
to step 1) and try again.
Table 2 shows a random selection of pseudowords gen-
erated by the algorithm. Note that the more unusual pair-
ings come from the less frequent category pairs, whereas
those in which word senses are closer in meaning are
drawn from more common category pairs.
4 Results
For the experiments reported below, we trained an un-
supervised Naive Bayes classifier using the categories as
both targets and as context features. For example, an oc-
currence of the word haptoglobin in the context surround-
ing the word to be disambiguated will be replaced by its
category label D12. Only unambiguous context words
were used. The result of the disambiguation step is a cat-
egory name, standing as a proxy for the word sense.
Table 3 reports accuracies for several experiments in
terms of macroaverages (average over the individual ac-
curacies for each pseudoword). Baseline refers to choos-
CW Base. Pess. Real. Abbrev. Opt.
10 53.24 62.93 64.60 70.37 71.35
20 53.24 66.80 68.90 73.83 76.36
40 53.24 69.92 73.28 76.46 80.03
300 53.24 72.79 75.34 77.99 81.88
Table 3: Accuracies (in %?s) of Baseline, Pessimistic, Re-
alistic, Abbreviation, and Optimistic datasets for different
context window (CW) sizes.
AAP: acetaminophen D02
auricular acupuncture E02
GST: general systems theory H01
glutathione s-transferase D08
ED: eating disorders F03
endogenous depression F03
elemental diet J02
Table 4: Sample category mappings for abbreviations.
ing the most frequent sense3. Pessimistic refers to the
evenly distributed category-based pseudowords, gener-
ated by requiring the word frequency to fall in the interval
[E/2;3E/2]. In the column labeled Realistic, the require-
ment for evenly distributed senses is dropped, although
the component words must have a frequency of at least
5. The column labeled Optimistic refers to the results
when the pseudowords are generated the standard way:
the words are selected at random rather than according to
the category sets.
We expected the Realistic pseudowords to produce
a better lower-bound estimate of the performance of a
WSD algorithm on real word senses than Optimistic. To
test this hypothesis we followed a method suggested by
Liu et al (2002) and evaluated the classifier on a set of
217 two-sense abbreviations (see Table 4).
Abbreviations are real ambiguous words, but they are
also artificial in a sense. Many homonyms are similar in
meaning as well as spelling because they derive etymo-
logically from the same root. By contrast, similar spelling
in abbreviations is often simply an accident of shared ini-
tial characters in compound nouns. Thus abbreviations
occupy an intermediate position between entirely random
pseudowords and standard real ambiguous words.
We extracted 98,841 unique abbreviation-expansion
pairs4 using code developed by Schwartz & Hearst
(2003), and retained only those abbreviations whose ex-
pansions could be fully and unambiguously mapped to
a single truncated MeSH category. The different expan-
sions of each abbreviation were required to correspond
3The baseline is dependent on the (pseudo)words used. The
one shown is the baseline for the abbreviations collection.
4From med-line03n0210.xml to med-line03n0229.xml.
to exactly two distinct categories (with overlap allowed
when there were more than two expansions for a given
abbreviation).
The question we wanted to explore is how well does
the classifier do on category-based pseudowords versus
abbreviations. As can be seen from Table 3, the ac-
curacies for the abbreviations (evaluated on 332,020 in-
stances) fall between the Realistic and Optimistic pseu-
dowords, as expected.
5 Conclusions
We have shown that creating pseudowords based on dis-
tributions from lexical category co-occurrence can pro-
duce a more accurate lower-bound for WSD systems
that use pseudowords than the standard approach. This
method allows for the detailed study of a particular sense
ambiguity set since many different pseudowords can be
generated from one category pair. Additionally, this
method provides a better-motivated basis for the grouping
of words into pseudowords, since they more realistically
model the meaning similarity patterns of real ambiguous
words than do randomly paired words.
Acknowledgements Special thanks to Barbara
Rosario for the discussions and valuable suggestions
and to Ariel Schwartz for providing the abbreviation
extraction code. This work was supported by a gift from
Genentech and an ARDA Aquaint contact.
References
William A. Gale, Kenneth W. Church and David
Yarowsky. 1992. Work on statistical methods for word
sense disambiguation., In R. Goldman et al (Eds.),
Working Notes of the AAAI Fall Symposium on Prob-
abilistic Approaches to Natural Language, 54-60.
Tanja Gaustad. 2001. Statistical Corpus-Based Word
Sense Disambiguation: Pseudowords vs. Real Am-
biguous Words., Proc. 39th Annual Meeting of ACL
(ACL/EACL 2001) - Student Research Workshop.
Hongfang Liu, Stephen B. Johnson and Carol Friedman.
2002. Automatic Resolution of Ambiguous Terms
Based on Machine Learning and Conceptual Relations
in the UMLS, JAMIA 2002.
Mark Sanderson and Keith van Rijsbergen. 1999. The
impact on retrieval effectiveness of skewed frequency
distributions., TOIS 17(4): 440-465.
Hinrich Schuetze. 1992. Context space., In R. Goldman
et al (Eds.), Working Notes of the AAAI Fall Sym-
posium on Probabilistic Approaches to Natural Lan-
guage, 54-60.
Ariel Schwartz and Marti Hearst. 2003. A Simple
Algorithm for Identifying Abbreviation Definitions in
Biomedical Text., In Proceedings of the Pacific Sympo-
sium on Biocomputing (PSB 2003) Kauai, Jan 2003.
Nearly-Automated Metadata Hierarchy Creation
Emilia Stoica and Marti A. Hearst
School of Information Management & Systems
University of California, Berkeley
102 South Hall, Berkeley CA 94720
 
estoica,hearst  @sims.berkeley.edu
Abstract
Currently, information architects create meta-
data category hierarchies manually. We present
a nearly-automated approach for deriving such
hierarchies, by converting the lexical hierarchy
WordNet into a format that reflects the contents
of a target information collection. We use the
term ?nearly-automated? because an informa-
tion architect should have to make only small
adjustments to produce an acceptable metadata
structure. We contrast the results with an algo-
rithm that uses lexical co-occurrence statistics.
1 Introduction
Human-readable hierarchies of category metadata are
needed for a wide range of information-centric applica-
tions, including information architectures for web sites
(Rosenfeld and Morville, 2002) and metadata for brows-
ing image and document collections (Yee et al, 2003).
In the information architecture community, methods
for creation of content-oriented metadata tend to be al-
most entirely manual (Rosenfeld and Morville, 2002).
The standard procedure is to gather lists of terms from ex-
isting resources, and organize them by selecting, merging
and augmenting the term lists to produce a set of hierar-
chical category labels. Usually the metadata categories
are used as labels which are assigned manually to the
items in the collection.
We advocate instead a nearly-automated approach to
building hierarchical subject category metadata, where
suggestions for metadata terms are automatically gen-
erated and grouped into hierarchies and then presented
to information architects for limited pruning and editing.
To be truly useful, these suggested groupings should be
close to the final product; if the results are too scattered, a
simple list of the most well-distributed terms is probably
more useful (a similar phenomenon is seen in machine-
aided translation systems (Church and Hovy, 1993)).
More specifically, we aim to develop algorithms for
generating category sets that (a) are intuitive to the tar-
get audience who will be browsing a web site or collec-
tion, (b) reflect the contents of the collection, and (c) al-
low for (nearly) automated assignment of the categories
to the items in the collection.
For a category system to be intuitive, modern informa-
tion science practice finds that it should consist of a set
of IS-A (hypernym) hierarchies1, from which multiple
labels can be selected and assigned to an item, follow-
ing the tenants of faceted classification (Rosenfeld and
Morville, 2002; Yee et al, 2003). For example, a medical
journal article will often simultaneously have terms as-
signed to it from anatomy, disease, and drug category hi-
erarchies. Furthermore, usability studies suggest that the
hierarchies should not be overly deep nor overly wide,
and preferably should have concave structure (meaning
broader at the root and leaves, narrower in the middle)
(Bernard, 2002).
Previous work on automated methods has primarily fo-
cused on using clustering techniques, which have the ad-
vantage of being automated and data-driven. However, a
major problem with clustering is that the groupings show
terms that are associated with one another, rather than
hierarchical parent-child relations. Studies indicate that
users prefer organized categories over associational clus-
ters (Chen et al, 1998; Pratt et al, 1999).
We have tested several approaches, including K-means
clustering, subsumption (Sanderson and Croft, 1999),
computing lexical co-occurrences (Schutze, 1993) and
building on the WordNet lexical hierarchy (Fellbaum,
1998). We have found that the latter produces by far the
most intuitive groupings that would be useful for creation
of a re-usable, human-readable category structure. Al-
though the idea of using a resource like WordNet for this
type of application seems rather obvious, to our knowl-
edge it has not been used to create subject-oriented meta-
data for browsing. This may be in part because it is very
1Part-of (meronymy) relations are also intuitive, but are not
considered here.
large and the word senses are assumed to be too fine-
grained (Mihalcea and Moldovan, 2001), or its structure
is assumed to be inappropriate.
However, we have found that, for some collections,
starting with the assumption that there will be a small
amount of hand-editing done after the automated pro-
cessing, combined with a bottom-up approach that ex-
tracts out those parts of the hypernym hierarchy that are
relevant to the collection, and a compression algorithm
that simplifies the hierarchical structure, we can produce
a structure that is close to the target goals.
Below we describe related work, the method for con-
verting WordNet into a more usable form, and the results
of using the algorithm on a test collection.
2 Related Work
There has been surprisingly little work on precisely the
problem that we tackle in this paper. The literature on au-
tomated text categorization is enormous, but assumes that
a set of categories has already been created, whereas the
problem here is to determine the categories of interest.
There has also been extensive work on finding synony-
mous terms and word associations, as well as automatic
acquisition of IS-A (or genus-head) relations from dic-
tionary definitions and glosses (Klavans and Whitman,
2001) and from free text (Hearst, 1992; Caraballo, 1999).
Sanderson and Croft (1999) propose a method called
subsumption for building a hierarchy for a set of docu-
ments retrieved for a query. For two terms x and y, x
is said to subsume y if the following conditions hold:
 	
 	 
ffProceedings of NAACL HLT 2007, pages 244?251,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Automating Creation of Hierarchical Faceted Metadata Structures
Emilia Stoica and Marti A. Hearst
School of Information
UC Berkeley, Berkeley, CA
estoica,hearst@ischool.berkeley.edu
Megan Richardson
Department of Mathematical Sciences
NMSU, Las Cruces, NM
merichar@nmsu.edu
Abstract
We describe Castanet, an algorithm for auto-
matically generating hierarchical faceted meta-
data from textual descriptions of items, to be in-
corporated into browsing and navigation inter-
faces for large information collections. From
an existing lexical database (such as WordNet),
Castanet carves out a structure that reflects
the contents of the target information collec-
tion; moderate manual modifications improve
the outcome. The algorithm is simple yet ef-
fective: a study conducted with 34 information
architects finds that Castanet achieves higher
quality results than other automated category
creation algorithms, and 85% of the study par-
ticipants said they would like to use the system
for their work.
1 Introduction
It is becoming widely accepted that the standard search
interface, consisting of a query box and a list of retrieved
items, is inadequate for navigation and exploration in
large information collections such as online catalogs, dig-
ital libraries, and museum image collections. Instead,
user interfaces which organize and group retrieval results
have been shown to be helpful for and preferred by users
over the straight results-list model when engaged in ex-
ploratory tasks (Yee et al, 2003; Pratt et al, 1999; Kaki,
2005). In particular, a representation known as hierarchi-
cal faceted metadata is gaining great traction within the
information architecture and enterprise search communi-
ties (Yee et al, 2003; Weinberger, 2005).
A considerable impediment to the wider adoption of
collection navigation via metadata in general, and hierar-
chical faceted metadata in particular, is the need to cre-
ate the metadata hierarchies and assign the appropriate
category labels to the information items. Usually, meta-
data category structures are manually created by infor-
mation architects (Rosenfeld and Morville, 2002). While
manually created metadata is considered of high qual-
ity, it is costly in terms of time and effort to produce,
which makes it difficult to scale and keep up with the vast
amounts of new content being produced.
In this paper, we describe Castanet, an algorithm that
makes considerable progress in automating faceted meta-
data creation. Castanet creates domain-specific overlays
on top of a large general-purpose lexical database, pro-
ducing surprisingly good results in a matter of minutes
for a wide range of subject matter.
In the next section we elaborate on the notion of hier-
archical faceted metadata and show how it can be used in
interfaces for navigation of information collections. Sec-
tion 3 describes other algorithms for inducing category
structure from textual descriptions. Section 4 describes
the Castanet alorithm, Section 5 describes the results of
an evaluation with information architects, and Section 6
draws conclusions and discusses future work.
2 Hierarchical Faceted Metadata
A hierarchical faceted metadata system (HFC) creates a
set of category hierarchies, each of which corresponds to
a different facet (dimension or type). The main applica-
tion of hierarchical faceted metadata is in user interfaces
for browsing and navigating collections of like items.
In the case of a recipe collection, for example, facets
may consist of dish type (salad, appetizer), ingredients
such as fruits (apricot, apple), vegetables (broccoli, cab-
bage), meat (beef, fish), preparation method (fry, bake,
etc.), calorie count, and so on. Decomposing the descrip-
tion into independent categories allows users to move
through large information spaces in a flexible manner.
The category metadata guides the user toward possible
choices, and organizes the results of keyword searches,
allowing users to both refine and expand the current
query, while maintaining a consistent representation of
the collection?s structure. This use of metadata should be
integrated with free-text search, allowing the user to fol-
low links, then add search terms, then follow more links,
without interrupting the interaction flow.
244
Usability studies have shown that, when incorpo-
rated into a properly-designed user interface, hierarchical
faceted metadata provides a flexible, intuitive way to ex-
plore a large collection of items that enhances feelings of
discovery without inducing a feeling of being lost (Yee et
al., 2003).
Note that the HFC representation is intermediate in
complexity between that of a monolithic hierarchy and
a full-blown ontology. HFC does not capture relations
and inferences that are essential for some applications.
For example, faceted metadata can express that an image
contains a hat and a man and a tree, and perhaps a wear-
ing activity, but does not indicate who is wearing what.
This relative simplicity of representation suggests that au-
tomatically inferring facet hierarchies may be easier than
the full ontology inference problem.
3 Related Work
There is a large literature on document classification and
automated text categorization (Sebastiani, 2002). How-
ever, that work assumes that the categories of interest
are already known, and tries to assign documents to cate-
gories. In contrast, in this paper we focus on the problem
of determining the categories of interest.
Another thread of work is on finding synonymous
terms and word associations, as well as automatic acqui-
sition of IS-A (or genus-head) relations from dictionary
definitions and free text (Hearst, 1992; Caraballo, 1999).
That work focuses on finding the right position for a word
within a lexicon, rather than building up comprehensible
and coherent faceted hierarchies.
A major class of solutions for creating subject hier-
archies uses data clustering. The Scatter/Gather sys-
tem (Cutting et al, 1992) uses a greedy global agglomer-
ative clustering algorithm where an initial set of
 
clusters
is recursively re-clustered until only documents remain.
Hofmann (1999) proposes the probabilistic latent seman-
tic analysis algorithm (pLSA), a probabilistic version of
clustering that uses latent semantic analysis for grouping
words and annealed EM for model fitting.
The greatest advantage of clustering is that it is fully
automatable and can be easily applied to any text col-
lection. Clustering can also reveal interesting and po-
tentially unexpected or new trends in a group of docu-
ments. The disadvantages of clustering include their lack
of predictability, their conflation of many dimensions si-
multaneously, the difficulty of labeling the groups, and
the counter-intuitiveness of cluster sub-hierarchies (Pratt
et al, 1999).
Blei et al (2003) developed the LDA (Latent Dirichlet
Allocation) method, a generative probabilistic model of
discrete data, which creates a hierarchical probabilistic
model of documents. It attempts to analyze a text cor-
pus and extract the topics that combined to form its doc-
uments. The output of the algorithm was evaluated in
terms of perplexity reduction but not in terms of under-
standability of the topics produced.
Sanderson and Croft (1999) propose a method called
subsumption for building a hierarchy for a set of doc-
uments retrieved for a query. For two terms x and y,
x is said to subsume y if the following conditions hold:
 
	 	
. In other words, x subsumes
y and is a parent of y, if the documents which contain y,
are a subset of the documents which contain x. To evalu-
ate the algorithm the authors asked 8 participants to look
at parent-child pairs and state whether or not they were
?interesting?. Participants found 67% to be interesting as
compared to 51% for randomly chosen pairs of words.
Of those interesting pairs, 72% were found to display a
?type-of? relationship.
Nevill-Manning et.al (1999), Anick et.al (1999) and
Vossen (2001) build hierarchies based on substring inclu-
sion. For example, the category full text indexing and
retrieval is the child of indexing and retrieval which in
turn is the child of index. While these string inclusion ap-
proaches expose some structure of the dataset, they can
only create subcategories which are substrings of the par-
ent category, which is very restrictive.
Another class of solutions make use of existing lex-
ical hierarchies to build category hierarchies, as we do
in this paper. For example, Navigli and Velardi (2003)
use WordNet (Fellbaum, 1998) to build a complex ontol-
ogy consisting of a wide range of relation types (demon-
strated on a travel agent domain), as opposed to a set of
human-readable hierarchical facets. They develop a com-
plex algorithm for choosing among WordNet senses; it
requires building a rich semantic network using Word-
Net glosses, meronyms, holonyms, and other lexical rela-
tions, and using the semantically annotated SemCor col-
lection. The semantic nets are intersected and the correct
sense is chosen based on a score assigned to each inter-
section. Mihalcea and Moldovan (2001) describe a so-
phisticated method for simplifying WordNet in general,
rather than tailoring it to a specific collection.
4 Method
The main idea behind the Castanet alorithm1 is to carve
out a structure from the hypernym (IS-A) relations within
the WordNet (Fellbaum, 1998) lexical database. The pri-
mary unit of representation in WordNet is the synset,
which is a set of words that are considered synonyms for a
particular concept. Each synset is linked to other synsets
via several types of lexical and semantic relations; we
only use hypernymy (IS-A relations) in this algorithm.
1A simpler, un-evaluated version of this algorithm was pre-
sented previously in a short paper (Stoica and Hearst, 2004).
245
4.1 Algorithm Overview
The Castanet alorithm assumes that there is text associ-
ated with each item in the collection, or at least with a
representative subset of the items. The textual descrip-
tions are used both to build the facet hierarchies and to
assign items (documents, images, citations, etc.) to the
facets. The text does not need to be particularly coher-
ent for the algorithm to work; we have applied it to frag-
mented image annotations and short journal titles, but if
the text is impoverished, the information items will not be
labeled as thoroughly as desirable and additional manual
annotation may be needed.
The algorithm has five major steps:
1. Select target terms from textual descriptions of in-
formation items.
2. Build the Core Tree:
  For each term, if the term is unambiguous (see
below), add its synset?s IS-A path to the Core
Tree.
  Increment the counts for each node in the
synset?s path with the number of documents in
which the target term appears.
3. Augment the Core Tree with the remaining terms?
paths:
  For each candidate IS-A path for the ambigu-
ous term, choose the path for which there is the
most document representation in the Core Tree.
4. Compress the augmented tree.
5. Remove top-level categories, yielding a set of facet
hierarchies.
We describe each step in more detail below.
4.2 Select Target Terms
Castanet selects only a subset of terms, called target
terms, that are intended to best reflect the topics in the
documents. Similarly to Sanderson and Croft (1999), we
use the term distribution ? defined as the number of item
descriptions containing the term ? as the selection crite-
rion. The algorithm retains those terms that have a distri-
bution larger than a threshold and eliminates terms on a
stop list. One and two-word consecutive noun phrases are
eligible to be considered as terms. Terms that can be ad-
jectives or verbs as well as nouns are optionally deleted.
4.3 Build the Core Tree
The Core Tree acts as the ?backbone? for the final cate-
gory structure. It is built by using paths derived from un-
ambiguous terms, with the goal of biasing the final struc-
ture towards the appropriate senses of words.
 
     
     
(a) (b)
 
     
entity
  substance, matter
 food, nutrient
 nutriment
        course  
dessert, sweet, afters
     frozen dessert
     sundae
     ice cream sundae 
  substance, matter
food, nutrient
 nutriment
       course  
dessert, sweet, afters
       ambrosia  ambrosiafrozen dessert
     sundae
dessert, sweet, afters
entity
  substance, matter
 food, nutrient
 nutriment
        course  
entity
       parfait    ice cream sundae     sherbet, sorbet
     sherbet
(c)
Figure 1: Merging hypernym paths.
4.3.1 Disambiguate using Wordnet Domains
A term is considered unambiguous if it meets at least
one of two conditions:
(1) The term has only one sense within WordNet, or
(2) (Optional) The term matches one of the pre-selected
WordNet domains (see below).
From our experiments, about half of the eligible terms
have only one sense within WordNet. For the rest of
terms, we disambiguate between multiple senses as fol-
lows.
WordNet provides a cross-categorization mechanism
known as domains, whereby some synsets are assigned
general category labels. However, only a small subset of
the nouns in WordNet have domains assigned to them.
For example, for a medicine collection, we found that
only 4% of the terms have domains medicine or biology
associated with them. For this reason, we use an addi-
tional resource called Wordnet Domains (Magnini, 2000),
which assigns domains to WordNet synsets. In this re-
source, every noun synset in WordNet has been semi-
automatically annotated with one of about 200 Dewey
Decimal Classification labels. Examples include history,
literature, plastic arts, zoology, etc.
In Castanet, Wordnet Domains are used as follows.
First, the system counts how many times each domain
is represented by target terms, building a list of the most
well-represented domains for the collection. Then, in a
manual intervention step, the information architect se-
lects the subset of the well-represented domains which
are meaningful for the collection in question.
For example, for a collection of biomedical journal ti-
tles, Surgery should be selected as a domain, whereas
for an art history image collection, Architecture might be
chosen. When processing the word lancet, the choice of
domain distinguishes between the hyponym path entity

object  artifact  instrumentality  device  in-
strument  medical instrument  surgical instrument
246
 parfait   sundae   sherbet ambrosia
dessert, sweet, aftersdessert, sweet, afters
frozen dessert ambrosia
      parfait        sundae      sherbet
(a) (b)
Figure 2: Compressing the tree.
 lancet and entity  object  artifact  structure,
construction  arch  pointed arch  Gothic arch 
lancet arch, lancet  lancet.
In some cases, more than one domain may be rele-
vant for a given term and for a given collection. For
example, the term brain is annotated with two domains,
Anatomy and Psychology, which are both relevant do-
mains for a biomedical journal collection. Currently
for these cases the algorithm breaks the tie by choosing
the sense with the lowest WordNet sense number (corre-
sponding to the most common sense), which in this case
selects the Anatomy sense. However, we see this forced
choice as a limitation, and in future work we plan to ex-
plore how to allow a term to have more than one occur-
rence in the metadata hierarchies.
4.3.2 Add Paths to Core Tree
To build the Core Tree, the algorithm marches down
the list of unambiguous terms and for each term looks
up its synset and its hypernym path in WordNet. (If a
term does not have representation in WordNet, then it is
not included in the category structure.) To add a path to
the Core Tree, its path is merged with those paths that
have already been placed in the tree. Figure 1(a-b) shows
the hypernym paths for the synsets corresponding to the
terms sundae and ambrosia. Note that they have several
hypernym path nodes in common: (entity), (substance,
matter), (food, nutrient), (nutriment), (course), (dessert,
sweet, afters). Those shared paths are merged by the al-
gorithm; the results, along with the paths for parfait and
sherbert are shown in Figure 1(c).
In addition to augmenting the nodes in the tree, adding
in a new term increases a count associated with each node
on its path; this count corresponds to how many docu-
ments the term occurs in. Thus the more common a term,
the more weight it places on the path it falls within.
4.4 Augment the Core Tree / Disambiguate Terms
The Core Tree contains only a subset of terms in the col-
lection (those that have only one path or whose sense can
be selected with WordNet Domains). The next step is to
add in the paths for the remaining target terms which are
ambiguous according to WordNet.
The Core Tree is built with a bias towards paths that are
most likely to be appropriate for the collection as a whole.
When confronted with a term that has multiple possible
Figure 3: Two path choices for an ambiguous term.
IS-A paths corresponding to multiple senses, the system
favors the more common path over other alternatives.
Assume that we want to add the term date to the Core
Tree for a collection of recipes, and that currently there
are two paths corresponding to two of its senses in the
Core Tree (see Figure 3). To decide which of the two
paths to merge date into, the algorithm looks at the num-
ber of items assigned to the deepest node that is held in
common between the existing Core Tree and each candi-
date path for the ambiguous term. The path for the calen-
dar day sense has fewer than 20 documents assigned to
it (corresponding to terms like Valentine?s Day), whereas
the path for the edible fruit sense has more than 700 doc-
uments assigned. Thus date is added to the fruit sense
path. (The counts for the ambiguous terms? document
hits are not incorporated into the new tree.)
Also, to eliminate unlikely senses, each candidate
sense?s hypernym path is required to share at least  
of its nodes with nodes already in the Core Tree, where
the user sets   (usually between 40 and 60%). Thus the
romantic appointment sense of date would not be consid-
ered as most of its hypernym path is not in the Core Tree.
If no path passes the threshold, then the first sense?s hy-
pernym path (according to WordNet?s sense ordering) is
placed in the tree.
4.5 Compress the Tree
The tree that is obtained in the previous step usually is
very deep, which is undesirable from a user interface per-
spective. Castanet uses two rules for compressing the
tree:
1. Starting from the leaves, recursively eliminate a par-
ent that has fewer than k children, unless the par-
ent is the root or has an item count larger than
0.1  (maximum term distribution).
2. Eliminate a child whose name appears within the
parent?s name, unless the child contains a WordNet
domain name.
247
     
     kitchen utensil
skillet potsaucepan
    
 
     
 
     
 
     
  substance, matter
 food, nutrient
 food stuff, food product
        ingredient, fixings            implement
(a)
  flavorer
         herb sweetening, sweetener      
     kitchen utensil
skillet pot
artifact, artefact
(b)
          object, physical object
entity
  flavorer
         herb sweetening, sweetener 
     parsley brown sugar      syrup   powdered sugar    thyme  oregano
saucepan
     parsley   thyme  oregano brown sugar      syrup   powdered sugar  
                 instrumentality
double boiler
double boiler
Figure 4: Eliminating top levels.
For example, consider the tree in Figure 1(c) and as-
sume that
  
, which means eliminate parents that have
fewer than two children.
Starting from the leaves, by applying Rule 2, nodes (ice
cream sundae), (sherbet, sorbet), (course), (nutriment),
(food, nutrient), (substance, matter) and (entity) are elim-
inated since they have only one child. Figure 2(a) shows
the resulting tree. Next, by applying Rule 3, the node
frozen dessert is eliminated, since it contains the word
dessert which also appears in the name of its parent. The
final tree is presented in Figure 2(b). Note that this is a
rather aggressive compression strategy, and the algorithm
can be adjusted to allow more hierarchy to be retained.
4.6 Prune Top Level Categories / Create Facets
The final step is to create a set of facet sub-hierarchies.
The goal is to create a moderate set of facets, each of
which has moderate depth and breadth at each level, in
order to enhance the navigability of the categories. Prun-
ing the top levels can be automated, but a manual editing
pass over the outcome will produce the best results.
To eliminate the top levels in an automated fashion, for
each of the nine tree roots in the WordNet noun database,
manually cut the top  levels (where    for the recipes
collection). Then, for each of the resulting trees, recur-
sively test if its root has more than 
 	
children. If it
does, then the tree is considered a facet; otherwise, the
current root is deleted and the algorithm tests to see if
each new root has  children. Those subtrees that do not
meet the criterion are omitted from the final set of facets.
Consider the tree in Figure 4(a). In this case, the cate-
gories of interest are (flavorer) and (kitchen utensil) along
with their children. However, to reach any of these cate-
gories, the user has to descend six levels, each of which
has very little information. Figure 4(b) shows the re-
sulting facets, which (subjectively) are at an informative
level of description for an information architecture. (In
this illustration, 
 
.)
Often the internal nodes of WordNet paths do not have
the most felicitous names, e.g., edible fruit instead of
fruit. Although we did not edit these names for the us-
ability study, it is advisable to do so.
5 Evaluation
The intended users of the Castanet alorithm are infor-
mation architects and others who need to build structures
for information collections. A successful algorithm must
be perceived by information architects as making their
job easier. If the proposed category system appears to re-
quire a lot of work to modify, then IAs are likely to reject
it. Thus, to evaluate Castanet?s output, we recruited in-
formation architects and asked them to compare it to one
other state-of-the-art approach as well as a baseline. The
participants were asked to assess the qualities of each cat-
egory system and to express how likely they would be to
use each in their work.
5.1 Study Design
The study compared the output of four algorithms: (a)
Baseline (frequent words and two-word phrases), (b)
Castanet, (c) LDA (Blei et al, 2003)2 and (d) Subsump-
tion (Sanderson and Croft, 1999). The algorithms were
applied to a dataset of 
     recipes from Southwest-
cooking.com. Participants were recruited via email and
were required to have experience building information ar-
chitectures and to be at least familiar with recipe websites
(to show their interest in the domain).
Currently there are no standard tools used by informa-
tion architects for building category systems from free
text. Based on our own experience, we assumed a strong
baseline would be a list of the most frequent words and
two-word phrases (stopwords removed); the study results
confirmed this assumption. The challenge for an auto-
mated system is to be preferred to the baseline.
The study design was within-participants, where each
participant evaluated Castanet, a Baseline approach, and
either Subsumption (N=16) or LDA (N=18).3 Order of
showing Castanet and the alternative algorithm was coun-
terbalanced across participants in each condition.
Because the algorithms produce a large number of
hierarchical categories, the output was shown to the
2Using code by Blei from www.cs.princeton.edu/?blei/lda-c/
3Pilot studies found that participants became very frustrated
when asked to compare LDA against Subsumption, since nei-
ther tested well, so we dropped this condition. We did not
consider asking any participant to evaluate all three systems,
to avoid fatigue. To avoid biasing participants towards any ap-
proach, the target alorithms were given the neutral names of
Pine, Birch, and Oak. Castanet was run without Domains for a
fairer comparison. Top level pruning was done automatically as
described, but with a few manual adjustments.
248
Cas. Bas. LDA Cas. Bas. Sub.
Def. Yes 4 2 0 2 2 0
Yes 10 10 0 13 11 6
No 2 2 2 1 3 2
Def. No 2 4 16 0 0 8
Table 1: Responses to the question ?Would you be likely
to use this algorithm in your work?? comparing Castanet
to the Baseline and LDA (N=18), and comparing Cas-
tanet to the Baseline and Subsumption (N=16).
Cas. (34) LDA (18) Sub. (16)
Meaningful 2.9 1.2 1.8
Systematic 2.8 1.4 1.8
Import. Concepts 2.8 1.3 1.9
Table 2: Average responses to questions about the quality
of the category systems. N shown in parentheses. As-
sessed on a four point scale where higher is better.
participants using the open source Flamenco collection
browser4 (see Figure 5). Clicking on a link shows sub-
categories as well as items that have been assigned that
category. For example, clicking on the Penne subcategory
beneath Pasta in the Castanet condition shows 5 recipes
that contain the word penne as well as the other categories
that have been assigned to these recipes. Since LDA does
not create names for its output groups, they were assigned
the generic names Category 1, 2, etc. Assignment of cat-
egories to items was done on a strict word-match basis;
participants were not asked to assess the item assignment
aspect of the interface.
At the start of the study, participants answered ques-
tions about their experience designing information archi-
tectures. They were then asked to look at a partial list of
recipes and think briefly about what their goals would be
in building a website for navigating the collection.
Next they viewed an ordered list of frequent terms
drawn automatically from the collection (Baseline condi-
tion). After this, they viewed the output of one of the two
target category systems. For each algorithm, participants
were asked questions about the top-level categories, such
as Would you add any categories? (possible responses:
(a) No, None, (b) Yes, one or two, (c) Yes, a few, and
(d) Yes, many). They were then asked to examine two
specific top level categories in depth (e.g., For category
Bread, would you remove any subcategories?). At the
end of each assessment, they were asked to comment on
general aspects of the category system as a whole (dis-
cussed below). After having seen both category systems,
participants were asked to state how likely they would be
to use the algorithm (e.g., Would you use Oak? Would you
4Available at flamenco.berkeley.edu
use Birch? Would you use the frequent words list?) An-
swer types were (a) No, definitely not, (b) Probably not,
(c) Yes, I might want to use this system in some cases,
and (d) Yes, I would definitely use this system.
5.2 Results
Table 1 shows the responses to the final question about
how likely the participants are to use the results of each
algorithm for their work. Both Castanet and the Baseline
fare well, with Castanet doing somewhat better. 85% of
the Castanet evaluators said yes or definitely yes to us-
ing it, compared to 74% for the Baseline. Only one par-
ticipant said ?no? to Castanet but ?yes? to the Baseline,
suggesting that both kinds of information are useful for
information architects.
The comparison algorithms did poorly. Subsumption
received 38% answering ?yes? or ?definitely yes? to the
question about likelihood of use. LDA was rejected by
all participants. A t-test (after converting responses to a
1-4 scale) shows that Castanet obtains significantly better
scores than LDA (  = 7.88   2.75) and Subsumption ( 
= 4.50   2.75), for  = 0.005. The differences between
Castanet and the Baseline are not significant.
Table 2 shows the average responses to the questions
(i) Overall, these are categories meaningful; (ii) Overall,
these categories describe the collection in a systematic
way; (iii) These categories capture the important con-
cepts.) They were scored as 1= Strongly disagree, 2
= Disagree Somewhat, 3 = Agree Somewhat, and 4 =
Strongly agree. Castanet?s score was about 35% higher
than Subsumption?s, and about 50% higher than LDA?s.
Participants were asked to scrutinize the top-level cate-
gories and assess whether they would add categories, re-
move some, merge or rename some. The ratings were
again converted to a four point scale (no changes = 4,
change one or two = 3, change a few = 2, change many =
1). Table 3 shows the results. Castanet scores as well as
or better than the others on all measures except Rename;
Subsumption scores slightly higher on this measure, and
does well on Split as well, but very poorly on Remove,
reflecting the fact that it produces well-named categories
at the top level, but too many at too fine a granularity.
Participants were also asked to examine two subcate-
gories in detail. Table 4 shows results averaged across
the two subcategories for number of categories to add,
remove, promote, move, and how well the subcategories
matched their expectations. Castanet performs especially
well on this last measure (2.5 versus 1.5 and 1.7). Partic-
ipants generally did not suggest moves or promotions.
Thus on all measures, we see Castanet outperforming
the other state-of-the-art algorithms. Note that we did not
explicitly evaluate the ?facetedness? of the category sys-
tems, as we thought this would be too difficult for the
participants to do. We feel the questions about the coher-
249
Cas. (34). LDA (18) Sub. (16)
Add 2.8 2.6 2.0
Remove 2.3 2.4 1.9
Rename 2.7 2.7 3.3
Merge 2.7 2.5 2.4
Split 3.8 3.3 3.8
Table 3: Assessing top-level categories.
Cas. (34). LDA (18) Sub. (16)
Add 2.8 2.8 2.4
Remove 3.4 2.2 2.5
Promote 3.7 3.4 3.8
Move 3.8 3.3 3.6
Matched Exp. 2.5 1.5 1.7
Table 4: Assessing second-level categories.
ence, systematicity, and coverage of the category systems
captured this to some degree.
6 Conclusions and Future Work
We have presented an algorithm called Castanet that cre-
ates hierarchical faceted metadata using WordNet and
Wordnet Domains. A questionnaire revealed that 85%
information architects thought it was likely to be use-
ful, compared to 0% for LDA and 38% for Subsumption.
Although not discussed here, we have successfully ap-
plied the algorithm to other domains including biomedi-
cal journal titles and art history image descriptions, and
to another lexical hierarchy, MeSH.5
Although quite useful ?out of the box,? the algorithm
could benefit by several improvements and additions.
The processing of the terms should recognize spelling
variations (such as aging vs. ageing) and morphological
variations. Verbs and adjectives are often quite impor-
tant for a collection (e.g., stir-fry for cooking) and should
be included, but with caution. Some terms should be al-
lowed to occur with more than one sense if this is re-
quired by the dataset (and some in more than one facet
even with the same sense, as seen in the brain example).
Currently if a term is in a document it is assumed to use
the sense assigned in the facet hierarchies; this is often in-
correct, and so terms should be disambiguated within the
text before automatic category assignment is done. And
finally, WordNet is not exhaustive and some mechanism
is needed to improve coverage for unknown terms.
Acknowledgements Thanks to Lou Rosenfeld and Rashmi
Sinha for their help finding participants, and to all the partic-
ipants themselves. This work was funded in part by NSF DBI-
0317510 and in part by the Summer Undergraduate Program in
Engineering Research at Berkeley (SUPERB).
5MEdical Subject Headings, http://www.nlm.nih.gov/mesh/
References
Peter Anick and Susesh Tipirneni. 1999. The paraphrase
search assistant:terminological feedback for iterative infor-
mation seeking. In Procs. of SIGIR?99.
David Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning Research,
3:993?1022.
Sharon A. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In ACL ?99.
Douglas Cutting, David Karger D., Jan Pedersen, and John W.
Tukey. 1992. Scatter/gather: A cluster-based approach to
browsing large document collections. In Proc. of SIGIR?92.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of COLING ?92.
Thomas Hofmann. 1999. The cluster-abstraction model: Un-
supervised learning of topic hierarchies from text data. In
Procs. of IJCAI?99, Stolckholm, July.
Mika Kaki. 2005. Findex: Search result categories help users
when document ranking fails. In Proc. of CHI ?05.
Bernardo Magnini. 2000. Integrating subject field codes into
WordNet. In Procs. of LREC 2000, Athens, Greece.
Rada Mihalcea and Dan I. Moldovan. 2001. Ez.wordnet: Prin-
ciples for automatic generation of a coarse grained wordnet.
In Procs. of FLAIRS Conference 2001, May.
Roberto Navigli, Paola Velardi, and Aldo Gangemi. 2003. On-
tology learning and its application to automated terminology
translation. Intelligent Systems, 18(1):22?31.
Craig Nevill-Manning, I. Witten, and G. Paynter. 1999. Lexi-
cally generated subject hierarchies for browsing large collec-
tions. Inter. J. on Digital Libraries, 2(2+3):111?123.
Wanda Pratt, Marti Hearst, and Larry Fagan. 1999. A
knowledge-based approach to organizing retrieved docu-
ments. In Procs. of AAAI 99, Orlando, FL.
Louis Rosenfeld and Peter Morville. 2002. Information Archi-
tecture for the World Wide Web: Designing Large-scale Web
Sites. O?Reilly & Associates, Inc.
Mark Sanderson and Bruce Croft. 1999. Deriving concept hi-
erarchies from text. In Procs. of SIGIR ?99.
Fabrizio Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1?47.
Emilia Stoica and Marti Hearst. 2004. Nearly-automated meta-
data hierarchy creation. In Proc. of HLT-NAACL 2004.
Piek Vossen. 2001. Extending, trimming and fussing word-
net for technical documents. In NAACL 2001 Workshop and
Other Lexical Resources, East Stroudsburg, PA.
Dave Weinberger. 2005. Taxonomies and tags: From trees to
piles of leaves. In Release 1.0, Feb.
Ka-Ping Yee, Kirsten Swearingen, Kevin Li, and Marti Hearst.
2003. Faceted metadata for image search and browsing. In
Procs. of CHI ?03, Fort Lauderdale, FL, April.
250
(a)
(b)
(c)
Figure 5: Partial view of categories obtained by (a) Castanet, (b) LDA and (c) Subsumption on the Recipes collection,
displayed in the Flamenco interface.
251
The Descent of Hierarchy, and Selection in Relational Semantics  
Barbara Rosario
SIMS
UC Berkeley
Berkeley, CA 94720
rosario@sims.berkeley.edu
Marti A. Hearst
SIMS
UC Berkeley
Berkeley, CA 94720
hearst@sims.berkeley.edu
Charles Fillmore
ICSI
UC Berkeley
Berkeley, CA 94720
fillmore@icsi.berkeley.edu
Abstract
In many types of technical texts, meaning is
embedded in noun compounds. A language un-
derstanding program needs to be able to inter-
pret these in order to ascertain sentence mean-
ing. We explore the possibility of using an ex-
isting lexical hierarchy for the purpose of plac-
ing words from a noun compound into cate-
gories, and then using this category member-
ship to determine the relation that holds be-
tween the nouns. In this paper we present the
results of an analysis of this method on two-
word noun compounds from the biomedical do-
main, obtaining classification accuracy of ap-
proximately 90%. Since lexical hierarchies are
not necessarily ideally suited for this task, we
also pose the question: how far down the hi-
erarchy must the algorithm descend before all
the terms within the subhierarchy behave uni-
formly with respect to the semantic relation in
question? We find that the topmost levels of the
hierarchy yield an accurate classification, thus
providing an economic way of assigning rela-
tions to noun compounds.
1 Introduction
A major difficulty for the interpretation of sentences from
technical texts is the complex structure of noun phrases
and noun compounds. Consider, for example, this title,
taken from a biomedical journal abstract:
Open-labeled long-term study of the subcutaneous
sumatriptan efficacy and tolerability in acute mi-
graine treatment.
An important step towards being able to interpret such
technical sentences is to analyze the meaning of noun
compounds, and noun phrases more generally.

With apologies to Charles Darwin.
Interpretation of noun compounds (NCs) is highly de-
pendent on lexical information. Thus we explore the use
of a large corpus (Medline) and a large lexical hierarchy
(MeSH, Medical Subject Headings) to determine the re-
lations that hold between the words in noun compounds.
Surprisingly, we find that we can simply use the juxta-
position of category membership within the lexical hier-
archy to determine the relation that holds between pairs
of nouns. For example, for the NCs leg paresis, skin
numbness, and hip pain, the first word of the NC falls into
the MeSH A01 (Body Regions) category, and the second
word falls into the C10 (Nervous System Diseases) cat-
egory. From these we can declare that the relation that
holds between the words is ?located in?. Similarly, for
influenza patients and aids survivors, the first word falls
under C02 (Virus Diseases) and the second is found in
M01.643 (Patients), yielding the ?afflicted by? relation.
Using this technique on a subpart of the category space,
we obtain 90% accuracy overall.
In some sense, this is a very old idea, dating back to
the early days of semantic nets and semantic grammars.
The critical difference now is that large lexical resources
and corpora have become available, thus allowing some
of those old techniques to become feasible in terms of
coverage. However, the success of such an approach de-
pends on the structure and coverage of the underlying lex-
ical ontology.
In the following sections we discuss the linguistic mo-
tivations behind this approach, the characteristics of the
lexical ontology MeSH, the use of a corpus to examine
the problem space, the method of determining the rela-
tions, the accuracy of the results, and the problem of am-
biguity. The paper concludes with related work and a
discussion of future work.
2 Linguistic Motivation
One way to understand the relations between the words
in a two-word noun compound is to cast the words into
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 247-254.
                         Proceedings of the 40th Annual Meeting of the Association for
a head-modifier relationship, and assume that the head
noun has an argument structure, much the way verbs do,
as well as a qualia structure in the sense of Pustejovsky
(1995). Then the meaning of the head noun determines
what kinds of things can be done to it, what it is made of,
what it is a part of, and so on.
For example, consider the noun knife. Knives are cre-
ated for particular activities or settings, can be made of
various materials, and can be used for cutting or manip-
ulating various kinds of things. A set of relations for
knives, and example NCs exhibiting these relations is
shown below:
(Used-in): kitchen knife, hunting knife
(Made-of): steel knife, plastic knife
(Instrument-for): carving knife
(Used-on): meat knife, putty knife
(Used-by): chef?s knife, butcher?s knife
Some relationships apply to only certain classes of nouns;
the semantic structure of the head noun determines the
range of possibilities. Thus if we can capture regularities
about the behaviors of the constituent nouns, we should
also be able to predict which relations will hold between
them.
We propose using the categorization provided by a lex-
ical hierarchy for this purpose. Using a large collection
of noun compounds, we assign semantic descriptors from
the lexical hierarchy to the constituent nouns and deter-
mine the relations between them. This approach avoids
the need to enumerate in advance all of the relations that
may hold. Rather, the corpus determines which relations
occur.
3 The Lexical Hierarchy: MeSH
MeSH (Medical Subject Headings)1 is the National Li-
brary of Medicine?s controlled vocabulary thesaurus; it
consists of set of terms arranged in a hierarchical struc-
ture. There are 15 main sub-hierarchies (trees) in MeSH,
each corresponding to a major branch of medical termi-
nology. For example, tree A corresponds to Anatomy,
tree B to Organisms, tree C to Diseases and so on. Every
branch has several sub-branches; Anatomy, for example,
consists of Body Regions (A01), Musculoskeletal System
(A02), Digestive System (A03) etc. We refer to these as
?level 0? categories.
These nodes have children, for example, Abdomen
(A01.047) and Back (A01.176) are level 1 children
of Body Regions. The longer the ID of the MeSH
term, the longer the path from the root and the more
precise the description. For example migraine is
C10.228.140.546.800.525, that is, C (a disease), C10
(Nervous System Diseases), C10.228 (Central Nervous
1http://www.nlm.nih.gov/mesh/meshhome.html; the work
reported in this paper uses MeSH 2001.
System Diseases) and so on. There are over 35,000
unique IDs in MeSH 2001. Many words are assigned
more than one MeSH ID and so occur in more than one
location within the hierarchy; thus the structure of MeSH
can be interpreted as a network.
Some of the categories are more homogeneous than
others. The tree A (Anatomy) for example, seems to be
quite homogeneous; at level 0, the nodes are all part of
(meronymic to) Anatomy: the Digestive (A03), Respi-
ratory (A04) and the Urogenital (A05) Systems are all
part of anatomy; at level 1, the Biliary Tract (A03.159)
and the Esophagus (A03.365) are part of the Digestive
System (level 0) and so on. Thus we assume that every
node is a (body) part of the parent node (and all the nodes
above it).
Tree C for Diseases is also homogeneous; the child
nodes are a kind of (hyponym of) the disease at the par-
ent node: Neoplasms (C04) is a kind of Disease C and
Hamartoma (C04.445) is a kind of Neoplasms.
Other trees are more heterogeneous, in the sense that
the meanings among the nodes are more diverse. Infor-
mation Science (L01), for example, contains, among oth-
ers, Communications Media (L01.178), Computer Secu-
rity (L01.209) and Pattern Recognition (L01.725). An-
other heterogeneous sub-hierarchy is Natural Science
(H01). Among the children of H01 we find Chemistry
(parent of Biochemistry), Electronics (parent of Ampli-
fiers and Robotics), Mathematics (Fractals, Game The-
ory and Fourier Analysis). In other words, we find a wide
range of concepts that are not described by a simple rela-
tionship.
These observations suggest that once an algorithm de-
scends to a homogeneous level, words falling into the
subhierarchy at that level (and below it) behave similarly
with respect to relation assignment.
4 Counting Noun Compounds
In this and the next section, we describe how we investi-
gated the hypothesis:
For all two-word noun compounds (NCs) that
can be characterized by a category pair (CP), a
particular semantic relationship holds between
the nouns comprising those NCs.
The kinds of relations we found are similar to those
described in Section 2. Note that, in this analysis we fo-
cused on determining which sets of NCs fall into the same
relation, without explicitly assigning names to the rela-
tions themselves. Furthermore, the same relation may be
described by many different category pairs (see Section
5.5).
First, we extracted two-word noun compounds from
approximately 1M titles and abstracts from the Med-
line collection of biomedical journal articles, resulting
Figure 1: Distribution of Level 0 Category Pairs. Mark size
indicates the number of unique NCs that fall under the CP. Only
those for which  NCs occur are shown.
in about 1M NCs. The NCs were extracted by finding
adjacent word pairs in which both words are tagged as
nouns by a tagger and appear in the MeSH hierarchy, and
the words preceding and following the pair do not appear
in MeSH2 Of these two-word noun compounds, 79,677
were unique.
Next we used MeSH to characterize the NCs according
to semantic category(ies). For example, the NC fibroblast
growth was categorized into A11.329.228 (Fibroblasts)
and G07.553.481 (Growth).
Note that the same words can be represented at differ-
ent levels of description. For example, fibroblast growth
can be described by the MeSH descriptors A11.329.228
G07.553.481 (original level), but also by A11 G07 (Cell
and Physiological Processes) or A11.329 G07.553 (Con-
nective Tissue Cells and Growth and Embryonic Devel-
opment). If a noun fell under more than one MeSH ID,
we made multiple versions of this categorization. We re-
fer to the result of this renaming as a category pair (CP).
We placed these CPs into a two-dimensional table,
with the MeSH category for the first noun on the X axis,
and the MeSH category for the second noun on the Y
axis. Each intersection indicates the number of NCs that
are classified under the corresponding two MeSH cate-
gories.
A visualization tool (Ahlberg and Shneiderman, 1994)
allowed us to explore the dataset to see which areas of
the category space are most heavily populated, and to get
a feeling for whether the distribution is uniform or not
(see Figure 1). If our hypothesis holds (that NCs that fall
2Clearly, this simple approach results in some erroneous ex-
tractions.
within the same category pairs are assigned the same re-
lation), then if most of the NCs fall within only a few
category pairs then we only need to determine which re-
lations hold between a subset of the possible pairs. Thus,
the more clumped the distribution, the easier (potentially)
our task is. Figure 1 shows that some areas in the CP
space have a higher concentration of unique NCs (the
Anatomy, and the E through N sub-hierarchies, for ex-
ample), especially when we focus on those for which at
least 50 unique NCs are found.
5 Labeling NC Relations
Given the promising nature of the NC distributions, the
question remains as to whether or not the hypothesis
holds. To answer this, we examined a subset of the CPs to
see if we could find positions within the sub-hierarchies
for which the relation assignments for the member NCs
are always the same.
5.1 Method
We first selected a subset of the CPs to examine in detail.
For each of these we examined, by hand, 20% of the NCs
they cover, paraphrasing the relation between the nouns,
and seeing if that paraphrase was the same for all the NCs
in the group. If it was the same, then the current levels of
the CP were considered to be the correct levels of descrip-
tion. If, on the other hand, several different paraphrases
were found, then the analysis descended one level of the
hierarchy. This repeated until the resulting partition of
the NCs resulted in uniform relation assignments.
For example, all the following NCs were mapped to the
same CP, A01 (Body Regions) and A07 (Cardiovascular
System): scalp arteries, heel capillary, shoulder artery,
ankle artery, leg veins, limb vein, forearm arteries, fin-
ger capillary, eyelid capillary, forearm microcirculation,
hand vein, forearm veins, limb arteries, thigh vein, foot
vein. All these NCs are ?similar? in the sense that the
relationships between the two words are the same; there-
fore, we do not need to descend either hierarchy. We call
the pair (A01, A07) a ?rule?, where a rule is a CP for
which all the NCs under it have the same relationship. In
the future, when we see an NC mapped to this rule, we
will assign this semantic relationship to it.
On the other hand, the following NCs, having the CP
A01 (Body Regions) and M01 (Persons), do not have
the same relationship between the component words: ab-
domen patients, arm amputees, chest physicians, eye pa-
tients, skin donor. The relationships are different depend-
ing on whether the person is a patient, a physician or a
donor. We therefore descend the M01 sub-hierarchy, ob-
taining the following clusters of NCs:
A01 M01.643 (Patients): abdomen patients, ankle
inpatient, eye outpatient
A01 H01 (Natural Sciences):
A01 H01 abdomen x-ray, ankle motion
A01 H01.770 (Science): skin observation
A01 H01.548 (Mathematics): breast risk
A01 H01.939 (Weights and Measures): head calibration
A01 H01.181 (Chemistry): skin iontophoresis
A01 H01.671 (Physics)
A01 H01.671.538 (Motion): shoulder rotations
A01 H01.671.100 (Biophysics): shoulder biomechanics
A01 H01.671.691 (Pressure): eye pressures
A01 H01.671.868 (Temp.): forehead temperature
A01 H01.671.768 (Radiation): thorax x-ray
A01 H01.671.252 (Electricity): chest electrode
A01 H01.671.606 (Optics): skin color
Figure 2: Levels of descent needed for NCs classified un-
der A01 H01.
A01 M01.526 (Occupational Groups): chest physician,
eye nurse, eye physician
A01, M01.898 (Donors): eye donor, skin donor
A01, M01.150 (Disabled Persons): arm amputees, knee
amputees.
In other words, to correctly assign a relationship to
these NCs, we needed to descend one level for the second
word. The resulting rules in this case are (A01 M01.643),
(A01, M01.150) etc. Figure 2 shows one CP for which we
needed to descend 3 levels.
In our collection, a total of 2627 CPs at level 0 have at
least 10 unique NCs. Of these, 798 (30%) are classified
with A (Anatomy) for either the first or the second noun.
We randomly selected 250 of such CPs for analysis.
We also analyzed 21 of the 90 CPs for which the sec-
ond noun was H01 (Natural Sciences); we decided to ana-
lyze this portion of the MeSH hierarchy because the NCs
with H01 as second noun are frequent in our collection,
and because we wanted to test the hypothesis that we do
indeed need to descend farther for heterogeneous parts of
MeSH.
Finally, we analyzed three CPs in category C (Dis-
eases); the most frequent CP in terms of the total number
of non-unique NCs is C04 (Neoplasms) A11 (Cells), with
30606 NCs; the second CP was A10 C04 (27520 total
NCs) and the fifth most frequent, A01 C04, with 20617
total NCs; we analyzed these CPs.
We started with the CPs at level 0 for both words, de-
scending when the corresponding clusters of NCs were
not homogeneous and stopping when they were. We did
this for 20% of the NCs in each CP. The results were as
follows.
For 187 of 250 (74%) CPs with a noun in the Anatomy
category, the classification remained at level 0 for both
words (for example, A01 A07). For 55 (22%) of the CPs
we had to descend 1 level (e.g., A01 M01: A01 M01.898,
A01 M01.643) and for 7 CPs (2%) we descended two
levels. We descended one level most of the time for the
sub-hierarchies E (Analytical, Diagnostic and Therapeu-
tic Techniques), G (Biological Sciences) and N (Health
Care) (around 50% of the time for these categories com-
bined). We never descended for B (Organisms) and did
so only for A13 (Animal Structures) in A. This was to be
able to distinguish a few non-homogeneous subcategories
(e.g., milk appearing among body parts, thus forcing a
distinction between buffalo milk and cat forelimb).
For CPs with H01 as the second noun, of the 21
CPs analyzed, we observed the following (level number,
count) pairs: (0, 1) (1, 8) (2, 12).
In all but three cases, the descending was done for the
second noun only. This may be because the second noun
usually plays the role of the head noun in two-word noun
compounds in English, thus requiring more specificity.
Alternatively, it may reflect the fact that for the exam-
ples we have examined so far, the more heterogeneous
terms dominate the second noun. Further examination is
needed to answer this decisively.
5.2 Accuracy
We tested the resulting classifications by developing a
randomly chosen test set (20% of the NCs for each
CP), entirely distinct from the labeled set, and used the
classifications (rules) found above to automatically pre-
dict which relations should be assigned to the member
NCs. An independent evaluator with biomedical training
checked these results manually, and found high accura-
cies: For the CPs which contained a noun in the Anatomy
domain, the assignments of new NCs were 94.2% accu-
rate computed via intra-category averaging, and 91.3%
accurate with extra-category averaging. For the CPs in
the Natural Sciences (H01) we found 81.6% accuracy via
intra-category averaging, and 78.6% accuracy with extra-
category averaging. For the three CPs in the C04 category
we obtained 100% accuracy.
The total accuracy across the portions of the A, H01
and C04 hierarchies that we analyzed were 89.6% via
intra-category averaging, and 90.8% via extra-category
averaging.
The lower accuracy for the Natural Sciences category
illustrates the dependence of the results on the proper-
ties of the lexical hierarchy. We can generalize well if
the sub-hierarchies are in a well-defined semantic rela-
tion with their ancestors. If they are a list of ?unrelated?
topics, we cannot use the generalization of the higher lev-
els; most of the mistakes for the Natural Sciences CPs oc-
curred in fact when we failed to descend for broad terms
such as Physics. Performing this evaluation allowed us
to find such problems and update the rules; the resulting
categorization should now be more accurate.
5.3 Generalization
An important issue is whether this method is an economic
way of classifying the NCs. The advantage of the high
level description is, of course, that we need to assign by
hand many fewer relationships than if we used all CPs at
their most specific levels. Our approach provides gener-
alization over the ?training? examples in two ways. First,
we find that we can use the juxtaposition of categories
in a lexical hierarchy to identify semantic relationships.
Second, we find we can use the higher levels of these cat-
egories for the assignments of these relationships.
To assess the degree of this generalization we calcu-
lated how many CPs are accounted for by the classifica-
tion rules created above for the Anatomy categories. In
other words, if we know that A01 A07 unequivocally de-
termines a relationship, how many possible (i.e., present
in our collection) CPs are there that are ?covered by? A01
A07 and that we do not need to consider explicitly? It
turns out that our 415 classification rules cover 46001
possible CP pairs3.
This, and the fact that we achieve high accuracies with
these classification rules, show that we successfully use
MeSH to generalize over unique NCs.
5.4 Ambiguity
A common problem for NLP tasks is ambiguity. In this
work we observe two kinds: lexical and ?relationship?
ambiguity. As an example of the former, mortality can
refer to the state of being mortal or to death rate. As an
example of the latter, bacteria mortality can either mean
?death of bacteria? or ?death caused by bacteria?.
In some cases, the relationship assignment method de-
scribed here can help disambiguate the meaning of an
ambiguous lexical item. Milk for example, can be both
Animal Structures (A13) and Food and Beverages (J02).
Consider the NCs chocolate milk, coconut milk that fall
under the CPs (B06 -Plants-, J02) and (B06, A13). The
CP (B06, J02) contains 180 NCs (other examples are
berry wines, cocoa beverages) while (B06, A13) has
only 6 NCs (4 of which with milk). Assuming then that
(B06, A13) is ?wrong?, we will assign only (B06, J02)
to chocolate milk, coconut milk, therefore disambiguat-
ing the sense for milk in this context (Beverage). Anal-
ogously, for buffalo milk, caprine milk we also have two
CPs (B02, J02) (B02, A13). In this case, however, it is
easy to show that only (B02 -Vertebrates-, A13) is the
correct one (i.e. yielding the correct relationship) and we
then assign the MeSH sense A13 to milk.
Nevertheless, ambiguity may be a problem for this
method. We see five different cases:
3Although we began with 250 CPs in the A category, when a
descend operation is performed, the CP is split into two or more
CPs at the level below. Thus the total number of CPs after all
assignments are made was 415.
1) Single MeSH senses for the nouns in the NC (no lex-
ical ambiguity) and only one possible relationship which
can predicted by the CP; that is, no ambiguity. For in-
stance, in abdomen radiography, abdomen is classified
exclusively under Body Regions and radiography ex-
clusively under Diagnosis, and the relationship between
them is unambiguous. Other examples include aciclovir
treatment (Heterocyclic Compounds, Therapeutics) and
adenocarcinoma treatment (Neoplasms, Therapeutics).
2) Single MeSH senses (no lexical ambiguity) but mul-
tiple readings for the relationships that therefore cannot
be predicted by the CP. It was quite difficult to find exam-
ples of this case; disambiguating this kind of NC requires
looking at the context of use. The examples we did find
include hospital databases which can be databases re-
garding (topic) hospitals, databases found in (location)
or owned by hospitals. Education efforts can be efforts
done through (education) or done to achieve education.
Kidney metabolism can be metabolism happening in (lo-
cation) or done by the kidney. Immunoglobulin stain-
ing, (D12 -Amino Acids, Peptides-, and Proteins, E05 -
Investigative Techniques-) can mean either staining with
immunoglobulin or staining of immunoglobulin.
3) Multiple MeSH mappings but only one possible re-
lation. One example of this case is alcoholism treatment
where treatment is Therapeutics (E02) and alcoholism is
both Disorders of Environmental Origin (C21) and Men-
tal Disorders (F03). For this NC we have therefore 2 CPs:
(C21, E02) as in wound treatments, injury rehabilitation
and (F03, E02) as in delirium treatment, schizophrenia
therapeutics. The multiple mappings reflect the conflict-
ing views on how to classify the condition of alcoholism,
but the relationship does not change.
4) Multiple MeSH mappings and multiple relations
that can be predicted by the different CPs. For exam-
ple, Bread diet can mean either that a person usually eats
bread or that a physician prescribed bread to treat a con-
dition. This difference is reflected by the different map-
pings: diet is both Investigative Techniques (E05) and
Metabolism and Nutrition (G06), bread is Food and Bev-
erages (J02). In these cases, the category can help disam-
biguate the relation (as opposed to in case 5 below); word
sense disambiguation algorithms that use context may be
helpful.
5) Multiple MeSH mappings and multiple relations
that cannot be predicted by the different CPs. As an ex-
ample of this case, bacteria mortality can be both ?death
of bacteria? or ?death caused by bacteria?. The multiple
mapping for mortality (Public Health, Information Sci-
ence, Population Characteristics and Investigative Tech-
niques) does not account for this ambiguity. Similarly,
for inhibin immunization, the first noun falls under Hor-
mones and Amino Acids, while immunization falls under
Environment and Public Health and Investigative Tech-
niques. The meanings are immunization against inhibin
or immunization using inhibin, and they cannot be dis-
ambiguated using only the MeSH descriptors.
We currently do not have a way to determine how many
instances of each case occur. Cases 2 and 5 are the most
problematic; however, as it was quite difficult to find ex-
amples for these cases, we suspect they are relatively rare.
A question arises as to if representing nouns using the
topmost levels of the hierarchy causes a loss in informa-
tion about lexical ambiguity. In effect, when we represent
the terms at higher levels, we assume that words that have
multiple descriptors under the same level are very similar,
and that retaining the distinction would not be useful for
most computational tasks. For example, osteosarcoma
occurs twice in MeSH, as C04.557.450.565.575.650 and
C04.557.450.795.620. When described at level 0, both
descriptors reduce to C04, at level 1 to C04.557, remov-
ing the ambiguity. By contrast, microscopy also occurs
twice, but under E05.595 and H01.671.606.624. Reduc-
ing these descriptors to level 0 retains the two distinct
senses.
To determine how often different senses are grouped
together, we calculated the number of MeSH senses for
words at different levels of the hierarchy. Table 1 shows
a histogram of the number of senses for the first noun of
all the unique NCs in our collection, the average degree
of ambiguity and the average description lengths.4 The
average number of MeSH senses is always less than two,
and increases with length of description, as is to be ex-
pected.
We observe that 3.6% of the lexical ambiguity is at lev-
els higher that 2, 16% at L2, 21.4% at L1 and 59% at L0.
Level 1 and 2 combined account for more than 80% of the
lexical ambiguity. This means that when a noun has mul-
tiple senses, those senses are more likely to come from
different main subtrees of MeSH (A and B, for exam-
ple), than from different deeper nodes in the same subtree
(H01.671.538 vs. H01.671.252). This fits nicely with our
method of describing the NCs with the higher levels of
the hierarchy: if most of the ambiguity is at the highest
levels (as these results show), information about lexical
ambiguity is not lost when we describe the NCs using the
higher levels of MeSH. Ideally, however, we would like
to reduce the lexical ambiguity for similar senses and to
retain it when the senses are semantically distinct (like,
for example, for diet in case 4). In other words, ideally,
the ambiguity left at the levels of our rules accounts for
only (and for all) the semantically different senses. Fur-
ther analysis is needed, but the high accuracy we obtained
in the classification seems to indicate that this indeed is
what is happening.
4We obtained very similar results for the second noun.
# Senses Original L2 L1 L0
1 (Unambiguous) 51539 51766 54087 58763
2 18637 18611 18677 17373
3 5719 5816 4572 2177
4 2222 2048 1724 1075
5 831 827 418 289
6 223 262 167 0
7 384 254 32 0
8 2 2 0 0
9 61 91 0 0
10 59 0 0 0
Total(Ambiguous) 28138 27911 25590 20914
Avg # Senses 1.56 1.54 1.45 1.33
Avg Desc Len 3.71 2.79 1.97 1
Table 1: The number of MeSH senses for N1 when truncated
to different levels of MeSH. Original refers to the actual (non-
truncated) MeSH descriptor. Avg # Senses is the average num-
ber of senses computed for all first nouns in the collection. Avg
Desc Len is the average description length; the value for level 1
is less than 2 and for level 2 is less that 3, because some nouns
are always mapped to higher levels (for example, cell is always
mapped to A11).
5.5 Multiple Occurrences of Semantic Relations
Because we determine the possible relations in a data-
driven manner, the question arises of how often does the
same semantic relation occur for different category pairs.
To determine the answer, we could (i) look at all the CPs,
give a name to the relations and ?merge? the CPs that
have the same relationships; or (ii) draw a sample of NC
examples for a given relation, look at the CPs for those
examples and verify that all the NCs for those CPs are
indeed in the same relationship.
We may not be able to determine the total number of
relations, or how often they repeat across different CPs,
until we examine the full spectrum of CPs. However, we
did a preliminary analysis to attempt to find relation repe-
tition across category pairs. As one example, we hypoth-
esized a relation afflicted by and verified that it applies
to all the CPs of the form (Disease C, Patients M01.643),
e.g.: anorexia (C23) patients, cancer (C04) survivor, in-
fluenza (C02) patients. This relation also applies to some
of the F category (Psychiatry), as in delirium (F03) pa-
tients, anxiety (F01) patient.
It becomes a judgement call whether to also include
NCs such as eye (A01) patient, gallbladder (A03) pa-
tients, and more generally, all the (Anatomy, Patients)
pairs. The question is, is ?afflicted-by (unspecified) Dis-
ease in Anatomy Part? equivalent to ?afflicted by Dis-
ease?? The answer depends on one?s theory of rela-
tional semantics. Another quandary is illustrated by the
NCs adolescent cancer, child tumors, adult dementia (in
which adolescent, child and adult are Age Groups) and
the heads are Diseases. Should these fall under the af-
flicted by relation, given the references to entire groups?
6 Related Work
6.1 Noun Compound Relation Assignment
Several approaches have been proposed for empirical
noun compound interpretation. Lauer & Dras (1994)
point out that there are three components to the prob-
lem: identification of the compound from within the text,
syntactic analysis of the compound (left versus right as-
sociation), and the interpretation of the underlying se-
mantics. Several researchers have tackled the syntactic
analysis (Lauer, 1995), (Pustejovsky et al, 1993), (Liber-
man and Church, 1992), usually using a variation of the
idea of finding the subconstituents elsewhere in the cor-
pus and using those to predict how the larger compounds
are structured.
We are interested in the third task, interpretation of the
underlying semantics. Most related work relies on hand-
written rules of one kind or another. Finin (1980) exam-
ines the problem of noun compound interpretation in de-
tail, and constructs a complex set of rules. Vanderwende
(1994) uses a sophisticated system to extract semantic in-
formation automatically from an on-line dictionary, and
then manipulates a set of hand-written rules with hand-
assigned weights to create an interpretation. Rindflesch
et al (2000) use hand-coded rule-based systems to ex-
tract the factual assertions from biomedical text. Lapata
(2000) classifies nominalizations according to whether
the modifier is the subject or the object of the underly-
ing verb expressed by the head noun.
Barker & Szpakowicz (1998) describe noun com-
pounds as triplets of information: the first constituent, the
second constituent, and a marker that can indicate a num-
ber of syntactic clues. Relations are initially assigned by
hand, and then new ones are classified based on their sim-
ilarity to previously classified NCs. However, similarity
at the lexical level means only that the same word occurs;
no generalization over lexical items is made. The algo-
rithm is assessed in terms of how much it speeds up the
hand-labeling of relations. Barrett et al (2001) have a
somewhat similar approach, using WordNet and creating
heuristics about how to classify a new NC given its simi-
larity to one that has already been seen.
In previous work (Rosario and Hearst, 2001), we
demonstrated the utility of using a lexical hierarchy for
assigning relations to two-word noun compounds. We
use machine learning algorithms and MeSH to success-
fully generalize from training instances, achieving about
60% accuracy on an 18-way classification problem us-
ing a very small training set. That approach is bottom
up and requires good coverage in the training set; the ap-
proach described in this paper is top-down, characteriz-
ing the lexical hierarchies explicitly rather than implicitly
through machine learning algorithms.
6.2 Using Lexical Hierarchies
Many approaches attempt to automatically assign seman-
tic roles (such as case roles) by computing semantic
similarity measures across a large lexical hierarchy; pri-
marily using WordNet (Fellbaum, 1998). Budanitsky &
Hirst (2001) provide a comparative analysis of such algo-
rithms.
However, it is uncommon to simply use the hier-
archy directly for generalization purposes. Many re-
searchers have noted that WordNet?s words are classi-
fied into senses that are too fine-grained for standard NLP
tasks. For example, Buitelaar (1997) notes that the noun
book is assigned to seven different senses, including fact
and section, subdivision. Thus most users of WordNet
must contend with the sense disambiguation issue in or-
der to use the lexicon.
The most closely related use of a lexical hierarchy
that we know of is that of Li & Abe (1998), which uses
an information-theoretic measure to make a cut through
the top levels of the noun portion of WordNet. This is
then used to determine acceptable classes for verb argu-
ment structure, and for the prepositional phrase attach-
ment problem and is found to perform as well as or better
than existing algorithms.
Additionally, Boggess et al (1991) ?tag? veterinary
text using a small set of semantic labels, assigned in much
the same way a parser works, and describe this in the
context of prepositional phrase attachment.
7 Conclusions and Future Work
We have provided evidence that the upper levels of a lex-
ical hierarchy can be used to accurately classify the re-
lations that hold between two-word technical noun com-
pounds. In this paper we focus on biomedical terms us-
ing the biomedical lexical ontology MeSH. It may be that
such technical, domain-specific terminology is better be-
haved than NCs drawn from more general text; we will
have to assess the technique in other domains to fully as-
sess its applicability.
Several issues need to be explored further. First, we
need to ensure that this technique works across the full
spectrum of the lexical hierarchy. We have demonstrated
the likely usefulness of such an exercise, but all of our
analysis was done by hand. It may be useful enough to
simply complete the job manually; however, it would be
preferable to automate some or all of the analysis. There
are several ways to go about this. One approach would be
to use existing statistical similarity measures (Budanitsky
and Hirst, 2001) to attempt to identify which subhierar-
chies are homogeneous. Another approach would be to
see if, after analyzing more CPs, those categories found
to be heterogeneous should be assumed to be heteroge-
neous across classifications, and similarly for those that
seem to be homogeneous.
The second major issue to address is how to extend the
technique to multi-word noun compounds. We will need
to distinguish between NCs such as acute migraine treat-
ment and oral migraine treatment, and handle the case
when the relation must first be found between the left-
most words. Thus additional steps will be needed; one
approach is to compute statistics to indicate likelihood of
the various CPs.
Finding noun compound relations is part of our larger
effort to investigate what we call statistical semantic pars-
ing (as in (Burton and Brown, 1979); see Grishman
(1986) for a nice overview). For example, we would like
to be able to interpret titles in terms of semantic relations,
for example, transforming Congenital anomalies of tra-
cheobronchial branching patterns into a form that allows
questions to be answered such as ?What kinds of irreg-
ularities can occur in lung structure?? We hope that by
compositional application of relations to entities, such in-
ferences will be possible.
Acknowledgements We thank Kaichi Sung for her
work on the relation labeling, Steve Maiorano for his
support of this research, and the anonymous reviewers
for their comments on the paper. This research was sup-
ported by a grant from ARDA.
References
Christopher Ahlberg and Ben Shneiderman. 1994. Vi-
sual information seeking: Tight coupling of dynamic
query filters with starfield displays. In Proceedings of
ACM CHI?94, pages 313?317.
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Pro-
ceedings of COLING-ACL ?98, Montreal, Canada.
Leslie Barrett, Anthony R. Davis, and Bonnie J. Dorr.
2001. Interpreting noun-noun compounds using word-
net. In Proceedings of 2001 CICLing Conference,
Mexico City.
Lois Boggess, Rajeev Agarwal, and Ron Davis. 1991.
Disambiguation of prepositional phrases in automati-
cally labelled technical text. In AAAI 91, pages 155?
159.
Alexander Budanitsky and Graeme Hirst. 2001. Seman-
tic distance in wordnet: an experimental, application-
oriented evaluation of five measures. In Proceedings
of the NAACL 2001 Workshop on WordNet and Other
Lexical Resources, Pittsburgh, PA, June.
P. Buitelaar. 1997. A lexicon for underspecified semantic
tagging. In Proceedings of ANLP 97, SIGLEX Work-
shop, Washington DC.
R. R. Burton and J. S. Brown. 1979. Toward a natural-
language capability for computer-assisted instruction.
In H. O?Neil, editor, Procedures for Instructional Sys-
tems Development, pages 273?313. Academic Press,
New York.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Timothy W. Finin. 1980. The Semantic Interpretation of
Compound Nominals. Ph.d. dissertation, University of
Illinois, Urbana, Illinois.
Ralph Grishman. 1986. Computational Linguistics.
Cambridge University Press, Cambridge.
Maria Lapata. 2000. The automatic interpretation of
nominalizations. In Proceedings of AAAI.
Mark Lauer and Mark Dras. 1994. A probabilistic model
of compound nouns. In Proceedings of the 7th Aus-
tralian Joint Conference on AI.
Mark Lauer. 1995. Corpus statistics meet the compound
noun. In Proceedings of the 33rd Meeting of the Asso-
ciation for Computational Linguistics, June.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDI principle. Computa-
tional Linguistics, 24(2):217?244.
Mark Y. Liberman and Kenneth W. Church. 1992. Text
analysis and word pronunciation in text-to-speech syn-
thesis. In Sadaoki Furui and Man Mohan Sondhi, ed-
itors, Advances in Speech Signal Processing, pages
791?831. Marcel Dekker, Inc.
James Pustejovsky, Sabine Bergler, and Peter Anick.
1993. Lexical semantic techniques for corpus analy-
sis. Computational Linguistics, 19(2).
James Pustejovsky, editor. 1995. The Generative Lexi-
con. MIT Press.
Thomas Rindflesch, Lorraine Tanabe, John N. Weinstein,
and Lawrence Hunter. 2000. Extraction of drugs,
genes and relations from the biomedical literature. Pa-
cific Symposium on Biocomputing, 5(5).
Barbara Rosario and Marti A. Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing. ACL.
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings of
COLING-94, pages 782?788.
Classifying Semantic Relations
in Bioscience Texts
Barbara Rosario
SIMS
UC Berkeley
Berkeley, CA 94720
rosario@sims.berkeley.edu
Marti A. Hearst
SIMS
UC Berkeley
Berkeley, CA 94720
hearst@sims.berkeley.edu
Abstract
A crucial step toward the goal of au-
tomatic extraction of propositional in-
formation from natural language text is
the identification of semantic relations
between constituents in sentences. We
examine the problem of distinguishing
among seven relation types that can oc-
cur between the entities ?treatment? and
?disease? in bioscience text, and the
problem of identifying such entities. We
compare five generative graphical mod-
els and a neural network, using lexical,
syntactic, and semantic features, finding
that the latter help achieve high classifi-
cation accuracy.
1 Introduction
The biosciences literature is rich, complex and
continually growing. The National Library of
Medicine?s MEDLINE database1 contains bibli-
ographic citations and abstracts from more than
4,600 biomedical journals, and an estimated half a
million new articles are added every year. Much
of the important, late-breaking bioscience infor-
mation is found only in textual form, and so meth-
ods are needed to automatically extract semantic
entities and the relations between them from this
text. For example, in the following sentences, hep-
atitis and its variants, which are DISEASES, are
found in different semantic relationships with var-
ious TREATMENTs:
1http://www.nlm.nih.gov/pubs/factsheets/medline.html
(1) Effect of interferon on hepatitis B
(2) A two-dose combined hepatitis A and B vac-
cine would facilitate immunization programs
(3) These results suggest that con A-induced hep-
atitis was ameliorated by pretreatment with
TJ-135.
In (1) there is an unspecified effect of the treat-
ment interferon on hepatitis B. In (2) the vaccine
prevents hepatitis A and B while in (3) hepatitis
is cured by the treatment TJ-135.
We refer to this problem as Relation Classifi-
cation. A related task is Role Extraction (also
called, in the literature, ?information extraction?
or ?named entity recognition?), defined as: given
a sentence such as ?The fluoroquinolones for uri-
nary tract infections: a review?, extract all and
only the strings of text that correspond to the roles
TREATMENT (fluoroquinolones) and DISEASE
(urinary tract infections). To make inferences
about the facts in the text we need a system that
accomplishes both these tasks: the extraction of
the semantic roles and the recognition of the rela-
tionship that holds between them.
In this paper we compare five generative graph-
ical models and a discriminative model (a multi-
layer neural network) on these tasks. Recogniz-
ing subtle differences among relations is a diffi-
cult task; nevertheless the results achieved by our
models are quite promising: when the roles are not
given, the neural network achieves 79.6% accu-
racy and the best graphical model achieves 74.9%.
When the roles are given, the neural net reaches
96.9% accuracy while the best graphical model
gets 91.6% accuracy. Part of the reason for the
Relationship Definition and Example
Cure TREAT cures DIS
810 (648, 162) Intravenous immune globulin for
recurrent spontaneous abortion
Only DIS TREAT not mentioned
616 (492, 124) Social ties and susceptibility to the
common cold
Only TREAT DIS not mentioned
166 (132, 34) Flucticasone propionate is safe in
recommended doses
Prevent TREAT prevents the DIS
63 (50, 13) Statins for prevention of stroke
Vague Very unclear relationship
36 (28, 8) Phenylbutazone and leukemia
Side Effect DIS is a result of a TREAT
29 (24, 5) Malignant mesodermal mixed tu-
mor of the uterus following irradi-
ation
NO Cure TREAT does not cure DIS
4 (3, 1) Evidence for double resistance to
permethrin and malathion in head
lice
Total relevant: 1724 (1377, 347)
Irrelevant TREAT and DIS not present
1771 (1416, 355) Patients were followed up for 6
months
Total: 3495 (2793, 702)
Table 1: Candidate semantic relationships be-
tween treatments and diseases. In parentheses are
shown the numbers of sentences used for training
and testing, respectively.
success of the algorithms is the use of a large
domain-specific lexical hierarchy for generaliza-
tion across classes of nouns.
In the remainder of this paper we discuss related
work, describe the annotated dataset, describe the
models, present and discuss the results of running
the models on the relation classification and en-
tity extraction tasks and analyze the relative im-
portance of the features used.
2 Related work
While there is much work on role extraction, very
little work has been done for relationship recogni-
tion. Moreover, many papers that claim to be do-
ing relationship recognition in reality address the
task of role extraction: (usually two) entities are
extracted and the relationship is implied by the co-
occurrence of these entities or by the presence of
some linguistic expression. These linguistic pat-
terns could in principle distinguish between differ-
ent relations, but instead are usually used to iden-
tify examples of one relation. In the related work
for statistical models there has been, to the best of
our knowledge, no attempt to distinguish between
different relations that can occur between the same
semantic entities.
In Agichtein and Gravano (2000) the goal is to
extract pairs such as (Microsoft, Redmond), where
Redmond is the location of the organization Mi-
crosoft. Their technique generates and evaluates
lexical patterns that are indicative of the relation.
Only the relation location of is tackled and the en-
tities are assumed given.
In Zelenko et al (2002), the task is to ex-
tract the relationships person-affiliation and
organization-location. The classification (done
with Support Vector Machine and Voted Percep-
tron algorithms) is between positive and negative
sentences, where the positive sentences contain
the two entities.
In the bioscience NLP literature there are
also efforts to extract entities and relations. In
Ray and Craven (2001), Hidden Markov Models
are applied to MEDLINE text to extract the enti-
ties PROTEINS and LOCATIONS in the relation-
ship subcellular-location and the entities GENE
and DISORDER in the relationship disorder-
association. The authors acknowledge that the
task of extracting relations is different from the
task of extracting entities. Nevertheless, they con-
sider positive examples to be all the sentences
that simply contain the entities, rather than an-
alyzing which relations hold between these enti-
ties. In Craven (1999), the problem tackled is re-
lationship extraction from MEDLINE for the re-
lation subcellular-location. The authors treat it
as a text classification problem and propose and
compare two classifiers: a Naive Bayes classi-
fier and a relational learning algorithm. This
is a two-way classification, and again there is
no mention of whether the co-occurrence of the
entities actually represents the target relation.
Pustejovsky et al (2002) use a rule-based system
to extract entities in the inhibit-relation. Their ex-
periments use sentences that contain verbal and
nominal forms of the stem inhibit. Thus the ac-
tual task performed is the extraction of entities
that are connected by some form of the stem in-
hibit, which by requiring occurrence of this word
explicitly, is not the same as finding all sen-
tences that talk about inhibiting actions. Similarly,
Rindflesch et al (1999) identify noun phrases sur-
rounding forms of the stem bind which signify
entities that can enter into molecular binding re-
lationships. In Srinivasan and Rindflesch (2002)
MeSH term co-occurrences within MEDLINE ar-
ticles are used to attempt to infer relationships be-
tween different concepts, including diseases and
drugs.
In the bioscience domain the work on relation
classification is primary done through hand-built
rules. Feldman et al (2002) use hand-built rules
that make use of syntactic and lexical features
and semantic constraints to find relations between
genes, proteins, drugs and diseases. The GENIES
system (Friedman et al, 2001) uses a hand-built
semantic grammar along with hand-derived syn-
tactic and semantic constraints, and recognizes
a wide range of relationships between biological
molecules.
3 Data and Features
For our experiments, the text was obtained from
MEDLINE 20012. An annotator with biology ex-
pertise considered the titles and abstracts sepa-
rately and labeled the sentences (both roles and
relations) based solely on the content of the indi-
vidual sentences. Seven possible types of relation-
ships between TREATMENT and DISEASE were
identified. Table 1 shows, for each relation, its def-
inition, one example sentence and the number of
sentences found containing it.
We used a large domain-specific lexical hi-
erarchy (MeSH, Medical Subject Headings3) to
map words into semantic categories. There are
about 19,000 unique terms in MeSH and 15 main
sub-hierarchies, each corresponding to a major
branch of medical ontology; e.g., tree A corre-
sponds to Anatomy, tree C to Disease, and so on.
As an example, the word migraine maps to the
term C10.228, that is, C (a disease), C10 (Ner-
vous System Diseases), C10.228 (Central Ner-
2We used the first 100 titles and the first 40 abstracts from
each of the 59 files medline01n*.xml in Medline 2001; the
labeled data is available at biotext.berkeley.edu
3http://www.nlm.nih.gov/mesh/meshhome.html
vous System Diseases). When there are multi-
ple MeSH terms for one word, we simply choose
the first one. These semantic features are shown
to be very useful for our tasks (see Section 4.3).
Rosario et al (2002) demonstrate the usefulness
of MeSH for the classification of the semantic re-
lationships between nouns in noun compounds.
The results reported in this paper were obtained
with the following features: the word itself, its part
of speech from the Brill tagger (Brill, 1995), the
phrase constituent the word belongs to, obtained
by flattening the output of a parser (Collins, 1996),
and the word?s MeSH ID (if available). In addi-
tion, we identified the sub-hierarchies of MeSH
that tend to correspond to treatments and diseases,
and convert these into a tri-valued attribute indi-
cating one of: disease, treatment or neither. Fi-
nally, we included orthographic features such as
?is the word a number?, ?only part of the word is a
number?, ?first letter is capitalized?, ?all letters are
capitalized?. In Section 4.3 we analyze the impact
of these features.
4 Models and Results
This section describes the models and their perfor-
mance on both entity extraction and relation clas-
sification. Generative models learn the prior prob-
ability of the class and the probability of the fea-
tures given the class; they are the natural choice
in cases with hidden variables (partially observed
or missing data). Since labeled data is expensive
to collect, these models may be useful when no
labels are available. However, in this paper we
test the generative models on fully observed data
and show that, although not as accurate as the dis-
criminative model, their performance is promising
enough to encourage their use for the case of par-
tially observed data.
Discriminative models learn the probability of
the class given the features. When we have fully
observed data and we just need to learn the map-
ping from features to classes (classification), a dis-
criminative approach may be more appropriate,
as shown in Ng and Jordan (2002), but has other
shortcomings as discussed below.
For the evaluation of the role extraction task, we
calculate the usual metrics of precision, recall and
F-measure. Precision is a measure of how many of
the roles extracted by the system are correct and
recall is the measure of how many of the true roles
were extracted by the system. The F-measure is
a weighted combination of precision and recall4.
Our role evaluation is very strict: every token is as-
sessed and we do not assign partial credit for con-
stituents for which only some of the words are cor-
rectly labeled. We report results for two cases: (i)
considering only the relevant sentences and (ii) in-
cluding also irrelevant sentences. For the relation
classification task, we report results in terms of
classification accuracy, choosing one out of seven
choices for (i) and one out of eight choices for (ii).
(Most papers report the results for only the rele-
vant sentences, while some papers assign credit to
their algorithms if their system extracts only one
instance of a given relation from the collection. By
contrast, in our experiments we expect the system
to extract all instances of every relation type.) For
both tasks, 75% of the data were used for training
and the rest for testing.
4.1 Generative Models
In Figure 1 we show two static and three dynamic
models. The nodes labeled ?Role? represent the
entities (in this case the choices are DISEASE,
TREATMENT and NULL) and the node labeled
?Relation? represents the relationship present in
the sentence. We assume here that there is a single
relation for each sentence between the entities5.
The children of the role nodes are the words and
their features, thus there are as many role states as
there are words in the sentence; for the static mod-
els, this is depicted by the box (or ?plate?) which
is the standard graphical model notation for repli-
cation. For each state, the features
 
are those
mentioned in Section 3.
The simpler static models S1 and S2 do not
assume an ordering in the role sequence. The
dynamic models were inspired by prior work on
HMM-like graphical models for role extraction
(Bikel et al, 1999; Freitag and McCallum, 2000;
Ray and Craven, 2001). These models consist of a
4In this paper, precision and recall are given equal weight,
that is, F-measure = 	



 .
5We found 75 sentences which contain more than one re-
lationship, often with multiple entities or the same entities
taking part in several interconnected relationships; we did not
include these in the study.
f1
Role
f2 fn
   . . .
Relati
on
T
Supporting Annotation Layers for Natural Language Processing
Preslav Nakov, Ariel Schwartz, Brian Wolf
Computer Science Division
University of California, Berkeley
Berkeley, CA 94720
{nakov,sariel}@cs.berkeley.edu
Marti Hearst
SIMS
University of California, Berkeley
Berkeley, CA 94720
hearst@sims.berkeley.edu
Abstract
We demonstrate a system for flexible
querying against text that has been anno-
tated with the results of NLP processing.
The system supports self-overlapping and
parallel layers, integration of syntactic and
ontological hierarchies, flexibility in the
format of returned results, and tight inte-
gration with SQL. We present a query lan-
guage and its use on examples taken from
the NLP literature.
1 Introduction
Today most natural language processing (NLP)
algorithms make use of the results of previous
processing steps. For example, a word sense disam-
biguation algorithm may combine the output of a to-
kenizer, a part-of-speech tagger, a phrase boundary
recognizer, and a module that classifies noun phrases
into semantic categories. Currently there is no stan-
dard way to represent and store the results of such
processing for efficient retrieval.
We propose a framework for annotating text with
the results of NLP processing and then querying
against those annotations in flexible ways. The
framework includes a query language and an in-
dexing architecture for efficient retrieval, built on
top of a relational database management system
(RDBMS). The model allows for both hierarchical
and overlapping layers of annotation as well as for
querying at multiple levels of description.
In the remainder of the paper we describe related
work, illustrate the annotation model and the query
language and describe the indexing architecture and
the experimental results, thus showing the feasibility
of the approach for a variety of NLP tasks.
2 Related Work
There are several specialized tools for indexing and
querying treebanks. (See Bird et al (2005) for an
overview and critical comparisons.) TGrep21 is a
a grep-like utility for the Penn Treebank corpus of
parsed Wall Street Journal texts. It allows Boolean
expressions over nodes and regular expressions in-
side nodes. Matching uses a binary index and is
performed recursively starting at the top node in the
query. TIGERSearch2 is associated with the German
syntactic corpus TIGER. The tool is more typed than
TGrep2 and allows search over discontinuous con-
stituents that are common in German. TIGERSearch
stores the corpus in a Prolog-like logical form and
searches using unification matching. LPath is an
extension of XPath with three features: immedi-
ate precedence, subtree scoping and edge alignment.
The queries are executed in an SQL database (Lai
and Bird, 2004). Other tree query languages include
CorpusSearch, Gsearch, Linguist?s Search Engine,
Netgraph, TIQL, VIQTORYA etc.
Some tools go beyond the tree model and al-
low multiple intersecting hierarchies. Emu (Cas-
sidy and Harrington, 2001) supports sequential lev-
els of annotations over speech datasets. Hierarchi-
cal relations may exist between tokens in different
levels, but precedence is defined only between el-
ements within the same level. The queries cannot
1http://tedlab.mit.edu/?dr/Tgrep2/
2http://www.ims.uni-stuttgart.de/projekte/TIGER/TIGERSearch/
express immediate precedence and are executed us-
ing a linear search. NiteQL is the query language
for the MATE annotation workbench (McKelvie et
al., 2001). It is highly expressive and, similarly to
TIGERSearch, allows quering of intersecting hier-
archies. However, the system uses XML for stor-
age and retrieval, with an in-memory representation,
which may limit its scalability.
Bird and Liberman (2001) introduce an abstract
general annotation approach, based on annotation
graphs.3 The model is best suited for speech data,
where time constraints are limited within an inter-
val, but it is unnecessarily complex for supporting
annotations on written text.
3 The Layered Query Language
Our framework differs from others by simultane-
ously supporting several key features:
? Multiple overlapping layers (which cannot be
expressed in a single XML file), including self-
overlapping (e.g., a word shared by two phrases
from the same layer), and parallel layers, as
when multiple syntactic parses span the same
text.
? Integration of multiple intersecting hierarchies
(e.g., MeSH, UMLS, WordNet).
? Flexible results format.
? Tight integration with SQL, including applica-
tion of SQL operators over the returned results.
? Scalability to large collections such as MED-
LINE (containing millions of documents).4
While existing systems possess some of these fea-
tures, none offers all of them.
We assume that the underlying text is fairly static.
While we support addition, removal and editing of
annotations via a Java API, we do not optimize for
efficient editing, but instead focus on compact rep-
resentation, easy query formulation, easy addition
and removal of layers, and straightforward trans-
lation into SQL. Below we illustrate our Layered
Query Language (LQL) using examples from bio-
science NLP.5
3http://agtk.sourceforge.net/
4http://www.nlm.nih.gov/pubs/factsheets/medline.html
5See http://biotext.berkeley.edu/lql/ for a formal description
of the language and additional examples.
Figure 1 illustrates the layered annotation of a
sentence from biomedical text. Each annotation rep-
resents an interval spanning a sequence of charac-
ters, using absolute beginning and ending positions.
Each layer corresponds to a conceptually different
kind of annotation (e.g., word, gene/protein6, shal-
low parse). Layers can be sequential, overlapping
(e.g., two concepts sharing the same word), and hi-
erarchical (either spanning, when the intervals are
nested as in a parse tree, or ontologically, when the
token itself is derived from a hierarchical ontology).
Word, POS and shallow parse layers are sequen-
tial (the latter can skip or span multiple words). The
gene/protein layer assigns IDs from the LocusLink
database of gene names.7 For a given gene there are
as many LocusLink IDs as the number of organisms
it is found in (e.g., 4 in the case of the gene Bcl-2).
The MeSH layer contains entities from the hier-
archical medical ontology MeSH (Medical Subject
Headings).8 The MeSH annotations on Figure 1 are
overlapping (share the word cell) and hierarchical
both ways: spanning, since blood cell (with MeSH
id D001773) orthographically spans the word cell
(id A11), and ontologically, since blood cell is a kind
of cell and cell death (id D016923) is a kind of Bio-
logical Phenomena.
Given this annotation, we can extract potential
protein-protein interactions from MEDLINE text.
One simple approach is to follow (Blaschke et al,
1999), who developed a list of verbs (and their de-
rived forms) and scanned for sentences containing
the pattern PROTEIN ... INTERACTION-VERB ...
PROTEIN. This can be expressed in LQL as follows:
FROM
[layer=?sentence? { ALLOW GAPS }
[layer=?protein?] AS prot1
[layer=?pos? && tag_type="verb" &&
content=?activates?]
[layer=?protein?] AS prot2
] SELECT prot1.content, prot2.content
This example extracts sentences containing a pro-
tein name in the gene/protein layer, followed by any
sequence of words (because of ALLOW GAPS), fol-
lowed by the interaction verb activates, followed by
any sequence of words, and finally followed by an-
6Genes and their corresponding proteins often share the
same name and the difference between them is often elided.
7http://www.ncbi.nlm.nih.gov/LocusLink
8http://www.nlm.nih.gov/mesh/meshhome.html
Figure 1: Illustration of the annotation layers. The full parse, sentence and section layers are not shown.
other protein name. All possible protein matches
within the same sentence will be returned. The re-
sults are presented as pairs of protein names.
Each query level specifies a layer (e.g., sentence,
part-of-speech, gene/protein) and optional restric-
tions on the attribute values. A binding statement
is allowed after the layer?s closing bracket. We
can search for more than one verb simultaneously,
e.g., by changing the POS layer of the query above
to [layer=?pos? && (content=?activates?
|| content=?inhibit? || content=?binds?)].
Further, a wildcard like content ? ?activate%?
can match the verb forms activate, activates and
activated. We can also use double quotes " to make
the comparison case insensitive. Finally, since LQL
is automatically translated into SQL, SQL code
can be written to surround the LQL query and to
reference its results, thus allowing the use of SQL
operators such as GROUP BY, COUNT, DISTINCT,
ORDER BY, etc., as well as set operations like UNION.
Now consider the task of extracting interactions
between chemicals and diseases. Given the sen-
tence ?Adherence to statin prevents one coronary
heart disease event for every 429 patients.?, we
want to extract the relation that statin (potentially)
prevents coronary heart disease. The latter is in
the MeSH hierarchy (id D003327) with tree codes
C14.280.647.250 and C14.907.553.470.250, while
the former is listed in the MeSH supplementary con-
cepts (ID C047068). In fact, the whole C subtree
in MeSH contains diseases and all supplementary
MeSH concepts represent chemicals. So we can find
potentially useful sentences (to be further processed
by another algorithm) using the following query:
FROM
[layer=?sentence? {NO ORDER, ALLOW GAPS}
[layer=?shallow_parse? && tag_type=?NP?
[layer=?chemicals?] AS chem $
]
[layer=?shallow_parse? && tag_type=?NP?
[layer=?MeSH? && label BELOW "C"] AS dis $
]
] AS sent
SELECT chem.content,dis.content,sent.content
This looks for sentences containing two NPs in any
order without overlaps (NO ORDER) and separated by
any number of intervening elements. We further re-
quire one of the NPs to end (ensured by the $ sym-
bol) with a chemical, and the other (the disease) to
end with a MeSH term from the C subtree.
4 System Architecture
Our basic model is similar to that of TIPSTER (Gr-
ishman, 1996): each annotation is stored as a record,
which specifies the character-level beginning and
ending positions, the layer and the type. The ba-
sic table9 contains the following columns: (1) an-
notation id; (2) doc id; (3) section: title, abstract
or body; (4) layer id: layer identifier (word, POS,
shallow parse, sentence, etc.); (5) start char pos:
beginning character position, relative to section and
doc id; (6) end char pos: ending character posi-
tion; (7) tag type: a layer-specific token identifier.
After evaluating various different extensions
of the structure above, we have arrived at one
with some additional columns, which improves
cross-layer query performance: (8) sentence id;
(9) word id; (10) first word pos; and (11)
last word pos. Columns (9)-(11) treat the word
layer as atomic and require all annotations to coin-
cide with word boundaries.
Finally, we use two types of composite indexes:
forward, which looks for positions in a given docu-
ment, and inverted, which supports searching based
on annotation values.10 An index lookup can be per-
formed on any column combination that corresponds
to an index prefix. An RDBMS? query optimizer
estimates the optimal access paths (index and table
scans), and join orders based on statistics collected
over the stored records. In complex queries a com-
bination of forward (F) and inverted (I) indexes is
typically used. The particular ones we used are:11
(F) +doc id+section+layer id+sentence
+first word pos+last word pos+tag type
(I) +layer id+tag type+doc id+section+sentence
+first word pos+last word pos
(I) +word id+layer id+tag type+doc id+section
+sentence+first word pos
We have experimented with the system on a col-
lection of 1.4 million MEDLINE abstracts, which
include 10 million sentences annotated with 320
million multi-layered annotations. The current data-
base size is around 70 GB. Annotations are indexed
as they are inserted into the database.
9There are some additional tables mapping token IDs to en-
tities (the string in case of a word, the MeSH label(s) in case of
a MeSH term etc.)
10These inverted indexes can be seen as a direct extension of
the widely used inverted file indexes in traditional IR systems.
11There is also an index on annotation id, which allows for
annotating relations between annotations.
Our initial evaluation shows variation in the exe-
cution time, depending on the kind and complexity
of the query. Response time for simple queries is
usually less than a minute, while for more complex
ones it can be much longer. We are in the process of
further investigating and tuning the system.
5 Conclusions and Future Work
We have provided a mechanism to effectively store
and query layers of textual annotations, focusing
on compact representation, easy query formulation,
easy addition and removal of layers, and straight-
forward translation into SQL. Using a collection of
1.4 MEDLINE abstracts, we have evaluated vari-
ous structures for data storage and have arrived at
a promising one.
We have also designed a concise language (LQL)
to express queries that span multiple levels of anno-
tation structure, allowing users to express queries in
a syntax that closely resembles the underlying anno-
tation structure. We plan to release the software to
the research community for use in their own annota-
tion and querying needs.
Acknowledgements This research was supported
by NSF DBI-0317510 and a gift from Genentech.
References
Steven Bird and Mark Liberman. 2001. A formal framework
for linguistic annotation. Speech Communication, 33(1-
2):23?60.
Steven Bird, Yi Chen, Susan Davidson, Haejoong Lee, and
Yifeng Zheng. 2005. Extending XPath to support linguis-
tic queries. In Proceedings of PLANX, pages 35?46.
Christian Blaschke, Miguel Andrade, Christos Ouzounis, and
Alfonso Valencia. 1999. Automatic extraction of biological
information from scientific text: Protein-protein interactions.
In Proceedings of ISMB, pages 60?67.
Steve Cassidy and Jonathan Harrington. 2001. Multi-level an-
notation in the Emu speech database management system.
Speech Communication, 33(1-2):61?77.
Ralph Grishman. 1996. Building an architecture: a CAWG
saga. Advances in Text Processing: Tipster Program Ph. II.
Catherine Lai and Steven Bird. 2004. Querying and updating
treebanks: A critical survey and requirements analysis. In
Proceedings Australasian Language Technology Workshop,
pages 139?146.
David McKelvie, Amy Isard, Andreas Mengel, Morten Moeller,
Michael Grosse, and Marion Klein. 2001. The MATE work-
bench - an annotation tool for XML coded speech corpora.
Speech Communication, 33(1-2):97?112.
Proceedings of ACL-08: HLT, pages 452?460,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Solving Relational Similarity Problems Using the Web as a Corpus
Preslav Nakov?
EECS, CS division
University of California at Berkeley
Berkeley, CA 94720, USA
nakov@cs.berkeley.edu
Marti A. Hearst
School of Information
University of California at Berkeley
Berkeley, CA 94720, USA
hearst@ischool.berkeley.edu
Abstract
We present a simple linguistically-motivated
method for characterizing the semantic rela-
tions that hold between two nouns. The ap-
proach leverages the vast size of the Web
in order to build lexically-specific features.
The main idea is to look for verbs, preposi-
tions, and coordinating conjunctions that can
help make explicit the hidden relations be-
tween the target nouns. Using these fea-
tures in instance-based classifiers, we demon-
strate state-of-the-art results on various rela-
tional similarity problems, including mapping
noun-modifier pairs to abstract relations like
TIME, LOCATION and CONTAINER, charac-
terizing noun-noun compounds in terms of ab-
stract linguistic predicates like CAUSE, USE,
and FROM, classifying the relations between
nominals in context, and solving SAT verbal
analogy problems. In essence, the approach
puts together some existing ideas, showing
that they apply generally to various semantic
tasks, finding that verbs are especially useful
features.
1 Introduction
Despite the tremendous amount of work on word
similarity (see (Budanitsky and Hirst, 2006) for an
overview), there is surprisingly little research on the
important related problem of relational similarity ?
semantic similarity between pairs of words. Stu-
dents who took the SAT test before 2005 or who
?After January 2008 at the Linguistic Modeling Depart-
ment, Institute for Parallel Processing, Bulgarian Academy of
Sciences, nakov@lml.bas.bg
are taking the GRE test nowadays are familiar with
an instance of this problem ? verbal analogy ques-
tions, which ask whether, e.g., the relationship be-
tween ostrich and bird is more similar to that be-
tween lion and cat, or rather between primate and
monkey. These analogies are difficult, and the aver-
age test taker gives a correct answer 57% of the time
(Turney and Littman, 2005).
Many NLP applications could benefit from solv-
ing relational similarity problems, including but
not limited to question answering, information re-
trieval, machine translation, word sense disambigua-
tion, and information extraction. For example, a
relational search engine like TextRunner, which
serves queries like ?find all X such that X causes
wrinkles?, asking for all entities that are in a par-
ticular relation with a given entity (Cafarella et al,
2006), needs to recognize that laugh wrinkles is
an instance of CAUSE-EFFECT. While there are
not many success stories so far, measuring seman-
tic similarity has proven its advantages for textual
entailment (Tatu and Moldovan, 2005).
In this paper, we introduce a novel linguistically-
motivated Web-based approach to relational simi-
larity, which, despite its simplicity, achieves state-
of-the-art performance on a number of problems.
Following Turney (2006b), we test our approach
on SAT verbal analogy questions and on mapping
noun-modifier pairs to abstract relations like TIME,
LOCATION and CONTAINER. We further apply it
to (1) characterizing noun-noun compounds using
abstract linguistic predicates like CAUSE, USE, and
FROM, and (2) classifying the relation between pairs
of nominals in context.
452
2 Related Work
2.1 Characterizing Semantic Relations
Turney and Littman (2005) characterize the relation-
ship between two words as a vector with coordinates
corresponding to the Web frequencies of 128 fixed
phrases like ?X for Y ? and ?Y for X? instantiated
from a fixed set of 64 joining terms like for, such
as, not the, is *, etc. These vectors are used in a
nearest-neighbor classifier to solve SAT verbal anal-
ogy problems, yielding 47% accuracy. The same ap-
proach is applied to classifying noun-modifier pairs:
using the Diverse dataset of Nastase and Szpakow-
icz (2003), Turney&Littman achieve F-measures of
26.5% with 30 fine-grained relations, and 43.2%
with 5 course-grained relations.
Turney (2005) extends the above approach by in-
troducing the latent relational analysis (LRA), which
uses automatically generated synonyms, learns suit-
able patterns, and performs singular value decom-
position in order to smooth the frequencies. The full
algorithm consists of 12 steps described in detail in
(Turney, 2006b). When applied to SAT questions,
it achieves the state-of-the-art accuracy of 56%. On
the Diverse dataset, it yields an F-measure of 39.8%
with 30 classes, and 58% with 5 classes.
Turney (2006a) presents an unsupervised algo-
rithm for mining the Web for patterns expressing
implicit semantic relations. For example, CAUSE
(e.g., cold virus) is best characterized by ?Y * causes
X?, and ?Y in * early X? is the best pattern for
TEMPORAL (e.g., morning frost). With 5 classes,
he achieves F-measure=50.2%.
2.2 Noun-Noun Compound Semantics
Lauer (1995) reduces the problem of noun com-
pound interpretation to choosing the best paraphras-
ing preposition from the following set: of, for, in,
at, on, from, with or about. He achieved 40% accu-
racy using corpus frequencies. This result was im-
proved to 55.7% by Lapata and Keller (2005) who
used Web-derived n-gram frequencies.
Barker and Szpakowicz (1998) use syntactic clues
and the identity of the nouns in a nearest-neighbor
classifier, achieving 60-70% accuracy.
Rosario and Hearst (2001) used a discriminative
classifier to assign 18 relations for noun compounds
from biomedical text, achieving 60% accuracy.
Rosario et al (2002) reported 90% accuracy with
a ?descent of hierarchy? approach which character-
izes the relationship between the nouns in a bio-
science noun-noun compound based on the MeSH
categories the nouns belong to.
Girju et al (2005) apply both classic (SVM and
decision trees) and novel supervised models (seman-
tic scattering and iterative semantic specialization),
using WordNet, word sense disambiguation, and a
set of linguistic features. They test their system
against both Lauer?s 8 prepositional paraphrases and
another set of 21 semantic relations, achieving up to
54% accuracy on the latter.
In a previous work (Nakov and Hearst, 2006), we
have shown that the relationship between the nouns
in a noun-noun compound can be characterized us-
ing verbs extracted from the Web, but we provided
no formal evaluation.
Kim and Baldwin (2006) characterized the se-
mantic relationship in a noun-noun compound us-
ing the verbs connecting the two nouns by compar-
ing them to predefined seed verbs. Their approach
is highly resource intensive (uses WordNet, CoreLex
and Moby?s thesaurus), and is quite sensitive to the
seed set of verbs: on a collection of 453 examples
and 19 relations, they achieved 52.6% accuracy with
84 seed verbs, but only 46.7% with 57 seed verbs.
2.3 Paraphrase Acquisition
Our method of extraction of paraphrasing verbs and
prepositions is similar to previous paraphrase ac-
quisition approaches. Lin and Pantel (2001) ex-
tract paraphrases from dependency tree paths whose
ends contain semantically similar sets of words by
generalizing over these ends. For example, given
?X solves Y?, they extract paraphrases like ?X finds
a solution to Y?, ?X tries to solve Y?, ?X resolves
Y?, ?Y is resolved by X?, etc. The approach is ex-
tended by Shinyama et al (2002), who use named
entity recognizers and look for anchors belong-
ing to matching semantic classes, e.g., LOCATION,
ORGANIZATION. The idea is further extended by
Nakov et al (2004), who apply it in the biomedical
domain, imposing the additional restriction that the
sentences from which the paraphrases are extracted
cite the same target paper.
453
2.4 Word Similarity
Another important group of related work is on us-
ing syntactic dependency features in a vector-space
model for measuring word similarity, e.g., (Alshawi
and Carter, 1994), (Grishman and Sterling, 1994),
(Ruge, 1992), and (Lin, 1998). For example, given a
noun, Lin (1998) extracts verbs that have that noun
as a subject or object, and adjectives that modify it.
3 Method
Given a pair of nouns, we try to characterize the
semantic relation between them by leveraging the
vast size of the Web to build linguistically-motivated
lexically-specific features. We mine the Web for
sentences containing the target nouns, and we ex-
tract the connecting verbs, prepositions, and coordi-
nating conjunctions, which we use in a vector-space
model to measure relational similarity.
The process of extraction starts with exact phrase
queries issued against a Web search engine (Google)
using the following patterns:
?infl1 THAT * infl2?
?infl2 THAT * infl1?
?infl1 * infl2?
?infl2 * infl1?
where: infl1 and infl2 are inflected variants of
noun1 and noun2 generated using the Java Word-
Net Library1; THAT is a complementizer and can be
that, which, or who; and * stands for 0 or more (up
to 8) instances of Google?s star operator.
The first two patterns are subsumed by the last
two and are used to obtain more sentences from the
search engine since including e.g. that in the query
changes the set of returned results and their ranking.
For each query, we collect the text snippets from
the result set (up to 1,000 per query). We split them
into sentences, and we filter out all incomplete ones
and those that do not contain the target nouns. We
further make sure that the word sequence follow-
ing the second mentioned target noun is nonempty
and contains at least one nonnoun, thus ensuring
the snippet includes the entire noun phrase: snippets
representing incomplete sentences often end with a
period anyway. We then perform POS tagging us-
ing the Stanford POS tagger (Toutanova et al, 2003)
1JWNL: http://jwordnet.sourceforge.net
Freq. Feature POS Direction
2205 of P 2 ? 1
1923 be V 1 ? 2
771 include V 1 ? 2
382 serve on V 2 ? 1
189 chair V 2 ? 1
189 have V 1 ? 2
169 consist of V 1 ? 2
148 comprise V 1 ? 2
106 sit on V 2 ? 1
81 be chaired by V 1 ? 2
78 appoint V 1 ? 2
77 on P 2 ? 1
66 and C 1 ? 2
66 be elected V 1 ? 2
58 replace V 1 ? 2
48 lead V 2 ? 1
47 be intended for V 1 ? 2
45 join V 2 ? 1
. . . . . . . . . . . .
4 be signed up for V 2 ? 1
Table 1: The most frequent Web-derived features for
committee member. Here V stands for verb (possibly
+preposition and/or +particle), P for preposition and C
for coordinating conjunction; 1 ? 2 means committee
precedes the feature and member follows it; 2 ? 1means
member precedes the feature and committee follows it.
and shallow parsing with the OpenNLP tools2, and
we extract the following types of features:
Verb: We extract a verb if the subject NP of that
verb is headed by one of the target nouns (or an in-
flected form), and its direct object NP is headed by
the other target noun (or an inflected form). For ex-
ample, the verb include will be extracted from ?The
committee includes many members.? We also ex-
tract verbs from relative clauses, e.g., ?This is a com-
mittee which includes many members.? Verb parti-
cles are also recognized, e.g., ?The committee must
rotate off 1/3 of its members.? We ignore modals
and auxiliaries, but retain the passive be. Finally, we
lemmatize the main verb using WordNet?s morpho-
logical analyzer Morphy (Fellbaum, 1998).
Verb+Preposition: If the subject NP of a verb is
headed by one of the target nouns (or an inflected
form), and its indirect object is a PP containing an
NP which is headed by the other target noun (or an
inflected form), we extract the verb and the preposi-
2OpenNLP: http://opennlp.sourceforge.net
454
tion heading that PP, e.g., ?The thesis advisory com-
mittee consists of three qualified members.? As in
the verb case, we extract verb+preposition from rel-
ative clauses, we include particles, we ignore modals
and auxiliaries, and we lemmatize the verbs.
Preposition: If one of the target nouns is the head
of an NP containing a PP with an internal NP headed
by the other target noun (or an inflected form), we
extract the preposition heading that PP, e.g., ?The
members of the committee held a meeting.?
Coordinating conjunction: If the two target
nouns are the heads of coordinated NPs, we extract
the coordinating conjunction.
In addition to the lexical part, for each extracted
feature, we keep a direction. Therefore the preposi-
tion of represents two different features in the fol-
lowing examples ?member of the committee? and
?committee of members?. See Table 1 for examples.
We use the above-described features to calculate
relational similarity, i.e., similarity between pairs of
nouns. In order to downweight very common fea-
tures like of, we use TF.IDF-weighting:
w(x) = TF (x)? log
(
N
DF (x)
)
(1)
In the above formula, TF (x) is the number of
times the feature x has been extracted for the tar-
get noun pair, DF (x) is the total number of training
noun pairs that have that feature, and N is the total
number of training noun pairs.
Given two nouns and their TF.IDF-weighted fre-
quency vectors A and B, we calculate the similarity
between them using the following generalized vari-
ant of the Dice coefficient:
Dice(A,B) =
2?
?n
i=1 min(ai, bi)?n
i=1 ai +
?n
i=1 bi
(2)
Other variants are also possible, e.g., Lin (1998).
4 Relational Similarity Experiments
4.1 SAT Verbal Analogy
Following Turney (2006b), we use SAT verbal anal-
ogy as a benchmark problem for relational similar-
ity. We experiment with the 374 SAT questions
collected by Turney and Littman (2005). Table 2
shows two sample questions: the top word pairs
ostrich:bird palatable:toothsome
(a) lion:cat (a) rancid:fragrant
(b) goose:flock (b) chewy:textured
(c) ewe:sheep (c) coarse:rough
(d) cub:bear (d) solitude:company
(e) primate:monkey (e) no choice
Table 2: SAT verbal analogy: sample questions. The
stem is in bold, the correct answer is in italic, and the
distractors are in plain text.
are called stems, the ones in italic are the solu-
tions, and the remaining ones are distractors. Tur-
ney (2006b) achieves 56% accuracy on this dataset,
which matches the average human performance of
57%, and represents a significant improvement over
the 20% random-guessing baseline.
Note that the righthand side example in Table
2 is missing one distractor; so do 21 questions.
The dataset alo mixes different parts of speech:
while solitude and company are nouns, all remaining
words are adjectives. Other examples contain verbs
and adverbs, and even relate pairs of different POS.
This is problematic for our approach, which requires
that both words be nouns3. After having filtered all
examples containing nonnouns, we ended up with
184 questions, which we used in the evaluation.
Given a verbal analogy example, we build six fea-
ture vectors ? one for each of the six word pairs. We
then calculate the relational similarity between the
stem of the analogy and each of the five candidates,
and we choose the pair with the highest score; we
make no prediction in case of a tie.
The evaluation results for a leave-one-out cross-
validation are shown in Table 3. We also show 95%-
confidence intervals for the accuracy. The last line
in the table shows the performance of Turney?s LRA
when limited to the 184 noun-only examples. Our
best model v + p + c performs a bit better, 71.3%
vs. 67.4%, but the difference is not statistically sig-
nificant. However, this ?inferred? accuracy could be
misleading, and the LRA could have performed bet-
ter if it was restricted to solve noun-only analogies,
which seem easier than the general ones, as demon-
strated by the significant increase in accuracy for
LRA when limited to nouns: 67.4% vs. 56%.
3It can be extended to handle adjective-noun pairs as well,
as demonstrated in section 4.2 below.
455
Model X ? ? Accuracy Cover.
v + p + c 129 52 3 71.3?7.0 98.4
v 122 56 6 68.5?7.2 96.7
v + p 119 61 4 66.1?7.2 97.8
v + c 117 62 5 65.4?7.2 97.3
p + c 90 90 4 50.0?7.2 97.8
p 84 94 6 47.2?7.2 96.7
baseline 37 147 0 20.0?5.2 100.0
LRA 122 59 3 67.4?7.1 98.4
Table 3: SAT verbal analogy: 184 noun-only examples.
v stands for verb, p for preposition, and c for coordinating
conjunction. For each model, the number of correct (X),
wrong (?), and nonclassified examples (?) is shown, fol-
lowed by accuracy and coverage (in %s).
Model X ? ? Accuracy Cover.
v + p 240 352 8 40.5?3.9 98.7
v + p + c 238 354 8 40.2?3.9 98.7
v 234 350 16 40.1?3.9 97.3
v + c 230 362 8 38.9?3.8 98.7
p + c 114 471 15 19.5?3.0 97.5
p 110 475 15 19.1?3.0 97.5
baseline 49 551 0 8.2?1.9 100.0
LRA 239 361 0 39.8?3.8 100.0
Table 4: Head-modifier relations, 30 classes: evaluation
on the Diverse dataset, micro-averaged (in %s).
4.2 Head-Modifier Relations
Next, we experiment with the Diverse dataset of
Barker and Szpakowicz (1998), which consists of
600 head-modifier pairs: noun-noun, adjective-noun
and adverb-noun. Each example is annotated with
one of 30 fine-grained relations, which are fur-
ther grouped into the following 5 coarse-grained
classes (the fine-grained relations are shown in
parentheses): CAUSALITY (cause, effect, purpose,
detraction), TEMPORALITY (frequency, time at,
time through), SPATIAL (direction, location, lo-
cation at, location from), PARTICIPANT (agent,
beneficiary, instrument, object, object property,
part, possessor, property, product, source, stative,
whole) and QUALITY (container, content, equa-
tive, material, measure, topic, type). For example,
exam anxiety is classified as effect and therefore as
CAUSALITY, and blue book is property and there-
fore also PARTICIPANT.
Some examples in the dataset are problematic for
our method. First, in three cases, there are two mod-
ifiers, e.g., infectious disease agent, and we had to
ignore the first one. Second, seven examples have
an adverb modifier, e.g., daily exercise, and 262 ex-
amples have an adjective modifier, e.g., tiny cloud.
We treat them as if the modifier was a noun, which
works in many cases, since many adjectives and ad-
verbs can be used predicatively, e.g., ?This exercise
is performed daily.? or ?This cloud looks very tiny.?
For the evaluation, we created a feature vector for
each head-modifier pair, and we performed a leave-
one-out cross-validation: we left one example for
testing and we trained on the remaining 599 ones,
repeating this procedure 600 times so that each ex-
ample be used for testing. Following Turney and
Littman (2005) we used a 1-nearest-neighbor classi-
fier. We calculated the similarity between the feature
vector of the testing example and each of the train-
ing examples? vectors. If there was a unique most
similar training example, we predicted its class, and
if there were ties, we chose the class predicted by the
majority of tied examples, if there was a majority.
The results for the 30-class Diverse dataset are
shown in Table 4. Our best model achieves 40.5%
accuracy, which is slightly better than LRA?s 39.8%,
but the difference is not statistically significant.
Table 4 shows that the verbs are the most impor-
tant features, yielding about 40% accuracy regard-
less of whether used alone or in combination with
prepositions and/or coordinating conjunctions; not
using them results in 50% drop in accuracy.
The reason coordinating conjunctions do not help
is that head-modifier relations are typically ex-
pressed with verbal or prepositional paraphrases.
Therefore, coordinating conjunctions only help with
some infrequent relations like equative, e.g., finding
player and coach on the Web suggests an equative
relation for player coach (and for coach player).
As Table 3 shows, this is different for SAT ver-
bal analogy, where verbs are still the most important
feature type and the only whose presence/absence
makes a statistical difference. However, this time
coordinating conjunctions (with prepositions) do
help a bit (the difference is not statistically signifi-
cant) since SAT verbal analogy questions ask for a
broader range of relations, e.g., antonymy, for which
coordinating conjunctions like but are helpful.
456
Model Accuracy
v + p + c + sent + query (type C) 68.1?4.0
v 67.9?4.0
v + p + c 67.8?4.0
v + p + c + sent (type A) 67.3?4.0
v + p 66.9?4.0
sent (sentence words only) 59.3?4.2
p 58.4?4.2
Baseline (majority class) 57.0?4.2
v + p + c + sent + query (C), 8 stars 67.0?4.0
v + p + c + sent (A), 8 stars 65.4?4.1
Best type C on SemEval 67.0?4.0
Best type A on SemEval 66.0?4.1
Table 5: Relations between nominals: evaluation on the
SemEval dataset. Accuracy is macro-averaged (in %s),
up to 10 Google stars are used unless otherwise stated.
4.3 Relations Between Nominals
We further experimented with the SemEval?07 task
4 dataset (Girju et al, 2007), where each example
consists of a sentence, a target semantic relation, two
nominals to be judged on whether they are in that re-
lation, manually annotated WordNet senses, and the
Web query used to obtain the sentence:
"Among the contents of the
<e1>vessel</e1> were a set of
carpenter?s <e2>tools</e2>, several
large storage jars, ceramic utensils,
ropes and remnants of food, as well
as a heavy load of ballast stones."
WordNet(e1) = "vessel%1:06:00::",
WordNet(e2) = "tool%1:06:00::",
Content-Container(e2, e1) = "true",
Query = "contents of the * were a"
The following nonexhaustive and possibly over-
lapping relations are possible: Cause-Effect
(e.g., hormone-growth), Instrument-Agency
(e.g., laser-printer), Theme-Tool (e.g., work-
force), Origin-Entity (e.g., grain-alcohol),
Content-Container (e.g., bananas-basket),
Product-Producer (e.g., honey-bee), and
Part-Whole (e.g., leg-table). Each relation is
considered in isolation; there are 140 training and at
least 70 test examples per relation.
Given an example, we reduced the target entities
e1 and e2 to single nouns by retaining their heads
only. We then mined the Web for sentences con-
taining these nouns, and we extracted the above-
described feature types: verbs, prepositions and co-
ordinating conjunctions. We further used the follow-
ing problem-specific contextual feature types:
Sentence words: after stop words removal and
stemming with the Porter (1980) stemmer;
Entity words: lemmata of the words in e1 and e2;
Query words: words part of the query string.
Each feature type has a specific prefix which pre-
vents it from mixing with other feature types; the
last feature type is used for type C only (see below).
The SemEval competition defines four types of
systems, depending on whether the manually anno-
tatedWordNet senses and theGoogle query are used:
A (WordNet=no, Query=no), B (WordNet=yes,
Query=no), C (WordNet=no, Query=yes), and D
(WordNet=yes, Query=yes). We experimented with
types A and C only since we believe that having the
manually annotated WordNet sense keys is an unre-
alistic assumption for a real-world application.
As before, we used a 1-nearest-neighbor classifier
with TF.IDF-weighting, breaking ties by predicting
the majority class on the training data. The evalu-
ation results are shown in Table 5. We studied the
effect of different subsets of features and of more
Google star operators. As the table shows, using
up to ten Google stars instead of up to eight (see
section 3) yields a slight improvement in accuracy
for systems of both type A (65.4% vs. 67.3%) and
type C (67.0% vs. 68.1%). Both results represent
a statistically significant improvement over the ma-
jority class baseline and over using sentence words
only, and a slight improvement over the best type A
and type C systems on SemEval?07, which achieved
66% and 67% accuracy, respectively.4
4.4 Noun-Noun Compound Relations
The last dataset we experimented with is a subset
of the 387 examples listed in the appendix of (Levi,
1978). Levi?s theory is one of the most impor-
tant linguistic theories of the syntax and semantics
of complex nominals ? a general concept grouping
4The best type B system on SemEval achieved 76.3% ac-
curacy using the manually-annotated WordNet senses in context
for each example, which constitutes an additional data source,
as opposed to an additional resource. The systems that used
WordNet as a resource only, i.e., ignoring the manually anno-
tated senses, were classified as type A or C. (Girju et al, 2007)
457
USING THAT NOT USING THAT
Model Accuracy Cover. ANF ASF Accuracy Cover. ANF ASF
Human: all v 78.4?6.0 99.5 34.3 70.9 ? ? ?
Human: first v from each worker 72.3?6.4 99.5 11.6 25.5 ? ? ? ?
v + p + c 50.0?6.7 99.1 216.6 1716.0 49.1?6.7 99.1 206.6 1647.6
v + p 50.0?6.7 99.1 208.9 1427.9 47.6?6.6 99.1 198.9 1359.5
v + c 46.7?6.6 99.1 187.8 1107.2 43.9?6.5 99.1 177.8 1038.8
v 45.8?6.6 99.1 180.0 819.1 42.9?6.5 99.1 170.0 750.7
p 33.0?6.0 99.1 28.9 608.8 33.0?6.0 99.1 28.9 608.8
p + c 32.1?5.9 99.1 36.6 896.9 32.1?5.9 99.1 36.6 896.9
Baseline 19.6?4.8 100.0 ? ? ? ? ? ?
Table 6: Noun-noun compound relations, 12 classes: evaluation on Levi-214 dataset. Shown are micro-averaged
accuracy and coverage in %s, followed by average number of features (ANF) and average sum of feature frequencies
(ASF) per example. The righthand side reports the results when the query patterns involving THAT were not used. For
comparison purposes, the top rows show the performance with the human-proposed verbs used as features.
together the partially overlapping classes of nom-
inal compounds (e.g., peanut butter), nominaliza-
tions (e.g., dream analysis), and nonpredicate noun
phrases (e.g., electric shock).
In Levi?s theory, complex nominals can be derived
from relative clauses by removing one of the fol-
lowing 12 abstract predicates: CAUSE1 (e.g., tear
gas), CAUSE2 (e.g., drug deaths), HAVE1 (e.g., ap-
ple cake), HAVE2 (e.g., lemon peel), MAKE1 (e.g.,
silkworm), MAKE2 (e.g., snowball), USE (e.g., steam
iron), BE (e.g., soldier ant), IN (e.g., field mouse),
FOR (e.g., horse doctor), FROM (e.g., olive oil), and
ABOUT (e.g., price war). In the resulting nominals,
the modifier is typically the object of the predicate;
when it is the subject, the predicate is marked with
the index 2. The second derivational mechanism in
the theory is nominalization; it produces nominals
whose head is a nominalized verb.
Since we are interested in noun compounds only,
we manually cleansed the set of 387 examples. We
first excluded all concatenations (e.g., silkworm) and
examples with adjectival modifiers (e.g., electric
shock), thus obtaining 250 noun-noun compounds
(Levi-250 dataset). We further filtered out all nom-
inalizations for which the dataset provides no ab-
stract predicate (e.g., city planner), thus ending up
with 214 examples (Levi-214 dataset).
As in the previous experiments, for each of the
214 noun-noun compounds, we mined the Web
for sentences containing both target nouns, from
which we extracted paraphrasing verbs, prepositions
and coordinating conjunctions. We then performed
leave-one-out cross-validation experiments with a
1-nearest-neighbor classifier, trying to predict the
correct predicate for the testing example. The re-
sults are shown in Table 6. As we can see, us-
ing prepositions alone yields about 33% accuracy,
which is a statistically significant improvement over
the majority-class baseline. Overall, the most impor-
tant features are the verbs: they yield 45.8% accu-
racy when used alone, and 50% together with prepo-
sitions. Adding coordinating conjunctions helps a
bit with verbs, but not with prepositions. Note how-
ever that none of the differences between the differ-
ent feature combinations involving verbs are statis-
tically significant.
The righthand side of the table reports the results
when the query patterns involving THAT (see section
3) were not used. We can observe a small 1-3% drop
in accuracy for all models involving verbs, but it is
not statistically significant.
We also show the average number of distinct fea-
tures and sum of feature counts per example: as we
can see, there is a strong positive correlation be-
tween number of features and accuracy.
5 Comparison to Human Judgments
Since in all above tasks the most important fea-
tures were the verbs, we decided to compare our
Web-derived verbs to human-proposed ones for all
noun-noun compounds in the Levi-250 dataset. We
asked human subjects to produce verbs, possibly
458
followed by prepositions, that could be used in a
paraphrase involving that. For example, olive oil
can be paraphrased as ?oil that comes from olives?,
?oil that is obtained from olives? or ?oil that is from
olives?. Note that this implicitly allows for prepo-
sitional paraphrases ? when the verb is to be and is
followed by a preposition, as in the last paraphrase.
We used the Amazon Mechanical Turk Web ser-
vice5 to recruit human subjects, and we instructed
them to propose at least three paraphrasing verbs
per noun-noun compound, if possible. We randomly
distributed the noun-noun compounds into groups of
5 and we requested 25 different human subjects per
group. Each human subject was allowed to work
on any number of groups, but not on the same one
twice. A total of 174 different human subjects pro-
duced 19,018 verbs. After filtering the bad submis-
sions and normalizing the verbs, we ended up with
17,821 verbs. See (Nakov, 2007) for further de-
tails on the process of extraction and cleansing. The
dataset itself is freely available (Nakov, 2008).
We compared the human-proposed and the Web-
derived verbs for Levi-214, aggregated by relation.
Given a relation, we collected all verbs belong-
ing to noun-noun compounds from that relation to-
gether with their frequencies. From a vector-space
model point of view, we summed their correspond-
ing frequency vectors. We did this separately for
the human- and the program-generated verbs, and
we compared the resulting vectors using Dice co-
efficient with TF.IDF, calculated as before. Figure
1 shows the cosine correlations using all human-
proposed verbs and the first verb from each judge.
We can see a very-high correlation (mid-70% to
mid-90%) for relations like CAUSE1, MAKE1, BE,
but low correlations of 11-30% for reverse relations
like HAVE2 and MAKE2. Interestingly, using the first
verb only improves the results for highly-correlated
relations, but negatively affects low-correlated ones.
Finally, we repeated the cross-validation exper-
iment with the Levi-214 dataset, this time using
the human-proposed verbs6 as features. As Table
6 shows, we achieved 78.4% accuracy using all
verbs (and and 72.3% with the first verb from each
worker), which is a statistically significant improve-
5http://www.mturk.com
6Note that the human subjects proposed their verbs without
any context and independently of our Web-derived sentences.
Figure 1: Cosine correlation (in %s) between the
human- and the program- generated verbs by rela-
tion: using all human-proposed verbs vs. the first verb.
ment over the 50% of our best Web-based model.
This result is strong for a 12-way classification prob-
lem, and confirms our observation that verbs and
prepositions are among the most important features
for relational similarity problems. It further suggests
that the human-proposed verbs might be an upper
bound on the accuracy that could be achieved with
automatically extracted features.
6 Conclusions and Future Work
We have presented a simple approach for character-
izing the relation between a pair of nouns in terms
of linguistically-motivated features which could be
useful for many NLP tasks. We found that verbs
were especially useful features for this task. An im-
portant advantage of the approach is that it does not
require knowledge about the semantics of the indi-
vidual nouns. A potential drawback is that it might
not work well for low-frequency words.
The evaluation on several relational similarity
problems, including SAT verbal analogy, head-
modifier relations, and relations between complex
nominals has shown state-of-the-art performance.
The presented approach can be further extended to
other combinations of parts of speech: not just noun-
noun and adjective-noun. Using a parser with a
richer set of syntactic dependency features, e.g., as
proposed by Pado? and Lapata (2007), is another
promising direction for future work.
Acknowledgments
This research was supported in part by NSF DBI-
0317510.
459
References
Hiyan Alshawi and David Carter. 1994. Training
and scaling preference functions for disambiguation.
Computational Linguistics, 20(4):635?648.
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Proc. of
Computational linguistics, pages 96?102.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13?47.
Michael Cafarella, Michele Banko, and Oren Etzioni.
2006. Relational Web search. Technical Report 2006-
04-02, University of Washington, Department of Com-
puter Science and Engineering.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Journal of Computer Speech and Language - Special
Issue on Multiword Expressions, 4(19):479?496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic rela-
tions between nominals. In Proceedings of SemEval,
pages 13?18, Prague, Czech Republic.
Ralph Grishman and John Sterling. 1994. Generalizing
automatically generated selectional patterns. In Pro-
ceedings of the 15th conference on Computational lin-
guistics, pages 742?747.
Su Nam Kim and Timothy Baldwin. 2006. Interpret-
ing semantic relations in noun compounds via verb se-
mantics. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 491?498.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM Trans.
Speech Lang. Process., 2(1):3.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Dept. of Computing, Macquarie University,
Australia.
Judith Levi. 1978. The Syntax and Semantics of Complex
Nominals. Academic Press, New York.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question-answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML, pages 296?304.
Preslav Nakov and Marti Hearst. 2006. Using verbs to
characterize noun-noun relations. In AIMSA, volume
4183 of LNCS, pages 233?244. Springer.
Preslav Nakov, Ariel Schwartz, and Marti Hearst. 2004.
Citances: Citation sentences for semantic analysis of
bioscience text. In Proceedings of SIGIR?04Workshop
on Search and Discovery in Bioinformatics, pages 81?
88, Sheffield, UK.
Preslav Nakov. 2007. Using the Web as an Implicit
Training Set: Application to Noun Compound Syntax
and Semantics. Ph.D. thesis, EECS Department, Uni-
versity of California, Berkeley, UCB/EECS-2007-173.
Preslav Nakov. 2008. Paraphrasing verbs for noun com-
pound interpretation. In Proceedings of the LREC?08
Workshop: Towards a Shared Task for Multiword Ex-
pressions (MWE?08), Marrakech, Morocco.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301, Tilburg, The Netherlands.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Barbara Rosario and Marti Hearst. 2001. Classifying the
semantic relations in noun compounds via a domain-
specific lexical hierarchy. In Proceedings of EMNLP,
pages 82?90.
Barbara Rosario, Marti Hearst, and Charles Fillmore.
2002. The descent of hierarchy, and selection in rela-
tional semantics. In Proceedings of ACL, pages 247?
254.
Gerda Ruge. 1992. Experiment on linguistically-based
term associations. Inf. Process. Manage., 28(3):317?
332.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of HLT, pages 313?318.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Proceed-
ings of HLT, pages 371?378.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL, pages 252?259.
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning Journal, 60(1-3):251?278.
Peter Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proceedings of IJCAI,
pages 1136?1141.
Peter Turney. 2006a. Expressing implicit semantic re-
lations without supervision. In Proceedings of ACL,
pages 313?320.
Peter Turney. 2006b. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
460
Proceedings of ACL-08: HLT, pages 701?709,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improving Search Results Quality by Customizing Summary Lengths
Michael Kaisser
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
m.kaisser@sms.ed.ac.uk
Marti A. Hearst
UC Berkeley
102 South Hall
Berkeley, CA 94705
hearst@ischool.berkeley.edu
John B. Lowe
Powerset, Inc.
475 Brannan St.
San Francisco, CA 94107
johnblowe@gmail.com
Abstract
Web search engines today typically show re-
sults as a list of titles and short snippets that
summarize how the retrieved documents are
related to the query. However, recent research
suggests that longer summaries can be prefer-
able for certain types of queries. This pa-
per presents empirical evidence that judges
can predict appropriate search result summary
lengths, and that perceptions of search result
quality can be affected by varying these result
lengths. These findings have important impli-
cations for search results presentation, espe-
cially for natural language queries.
1 Introduction
Search results listings on the web have become stan-
dardized as a list of information summarizing the
retrieved documents. This summary information is
often referred to as the document?s surrogate (Mar-
chionini et al, 2008).
In older search systems, such as those used in
news and legal search, the document surrogate typ-
ically consisted of the title and important metadata,
such as date, author, source, and length of the article,
as well as the document?s manually written abstract.
In most cases, the full text content of the document
was not available to the search engine and so no ex-
tracts could be made.
In web search, document surrogates typically
show the web page?s title, a URL, and information
extracted from the full text contents of the docu-
ment. This latter part is referred to by several dif-
ferent names, including summary, abstract, extract,
and snippet. Today it is standard for web search en-
gines to show these summaries as one or two lines
of text, often with ellipses separating sentence frag-
ments. However, there is evidence that the ideal re-
sult length is often longer than the standard snippet
length, and that furthermore, result length depends
on the type of answer being sought.
In this paper, we systematically examine the ques-
tion of search result length preference, comparing
different result lengths for different query types. We
find evidence that desired answer length is sensitive
to query type, and that for some queries longer an-
swers are judged to be of higher quality.
In the following sections we summarize the re-
lated work on result length variation and on query
topic classification. We then describe two studies. In
the first, judges examined queries and made predic-
tions about the expected answer types and the ideal
answer lengths. In the second study, judges rated
answers of different lengths for these queries. The
studies find evidence supporting the idea that differ-
ent query types are best answered with summaries
of different lengths.
2 Related Work
2.1 Query-biased Summaries
In the early days of the web, the result summary
consisted of the first few lines of text, due both to
concerns about intellectual property, and because of-
ten that was the only part of the full text that the
search engines retained from their crawls. Eventu-
ally, search engines started showing what are known
variously as query-biased summaries, keyword-in-
701
context (KWIC) extractions, and user-directed sum-
maries (Tombros and Sanderson, 1998). In these
summaries, sentence fragments, full sentences, or
groups of sentences that contain query terms are ex-
tracted from the full text. Early versions of this idea
were developed in the Snippet Search tool (Peder-
sen et al, 1991) and the Superbook tool?s Table-of-
Contents view (Egan et al, 1989).
A query-biased summary shows sentences that
summarize the ways the query terms are used within
the document. In addition to showing which subsets
of query terms occur in a retrieved document, this
display also exposes the context in which the query
terms appear with respect to one another.
Research suggests that query-biased summaries
are superior to showing the first few sentences from
documents. Tombros & Sanderson (1998), in a
study with 20 participants using TREC ad hoc data,
found higher precision and recall and higher sub-
jective preferences for query-biased summaries over
summaries showing the first few sentences. Simi-
lar results for timing and subjective measurements
were found by White et al (2003) in a study with
24 participants. White et al (2003) also describe
experiments with different sentence selection mech-
anisms, including giving more weight to sentences
that contained query words along with text format-
ting.
There are significant design questions surround-
ing how best to formulate and display query-biased
summaries. As with standard document summariza-
tion and extraction, there is an inherent trade-off
between showing long, informative summaries and
minimizing the screen space required by each search
hit. There is also a tension between showing short
snippets that contain all or most of the query terms
and showing coherent stretches of text. If the query
terms do not co-occur near one another, then the ex-
tract has to become very long if full sentences and
all query terms are to be shown. Many web search
engine snippets compromise by showing fragments
instead of sentences.
2.2 Studies Comparing Results Lengths
Recently, a few studies have analyzed the results of
varying search summary length.
In the question-answering context (as opposed to
general web search), Lin et al (2003) conducted
a usability study with 32 computer science students
comparing four types of answer context: exact an-
swer, answer-in-sentence, answer-in-paragraph, and
answer-in-document. To remove effects of incorrect
answers, they used a system that produced only cor-
rect answers, drawn from an online encyclopedia.
Participants viewed answers for 8 question scenar-
ios. Lin et al (2003) found no significant differ-
ences in task completion times, but they did find dif-
ferences in subjective responses. Most participants
(53%) preferred paragraph-sized chunks, noting that
a sentence wasn?t much more information beyond
the exact answer, and a full document was often-
times too long. That said, 23% preferred full docu-
ments, 20% preferred sentences, and one participant
preferred exact answer, thus suggesting that there is
considerable individual variation.
Paek et al (2004) experimented with showing dif-
fering amounts of summary information in results
listings, controlling the study design so that only
one result in each list of 10 was relevant. For half
the test questions, the target information was visi-
ble in the original snippet, and for the other half, the
participant needed to use their mouse to view more
information from the relevant search result. They
compared three interface conditions:
(i) a standard search results listing, in which a
mouse click on the title brings up the full text
of the web page,
(ii) ?instant? view, for which a mouseclick ex-
panded the document summary to show addi-
tional sentences from the document, and those
sentences contained query terms and the an-
swer to the search task, and
(iii) a ?dynamic? view that responded to a mouse
hover, and dynamically expanded the summary
with a few words at a time.
Eleven out of 18 participants preferred instant
view over the other two views, and on average all
participants produced faster and more accurate re-
sults with this view. Seven participants preferred dy-
namic view over the others, but many others found
this view disruptive. The dynamic view suffered
from the problem that, as the text expanded, the
mouse no longer covered the selected results, and
702
so an unintended, different search result sometimes
started to expand. Notably, none of the participants
preferred the standard results listing view.
Cutrell & Guan (2007), compared search sum-
maries of varying length: short (1 line of text),
medium (2-3 lines) and long (6-7 lines) using search
engine-produced snippets (it is unclear if the sum-
mary text was contiguous or included ellipses).
They also compared 6 navigational queries (where
the goal is to find a website?s homepage), with 6 in-
formational queries (e.g., ?find when the Titanic set
sail for its only voyage and what port it left from,?
?find out how long the Las Vegas monorail is?). In
a study with 22 participants, they found that partic-
ipants were 24 seconds faster on average with the
long view than with the short and medium view. The
also found that participants were 10 seconds slower
on average with the long view for the navigational
tasks. They present eye tracking evidence which
suggests that on the navigational task, the extra text
distracts the eye from the URL. They did not re-
port on subjective responses to the different answer
lengths.
Rose et al (2007) varied search results summaries
along several dimensions, finding that text choppi-
ness and sentence truncation had negative effects,
and genre cues had positive effects. They did not
find effects for varying summary length, but they
only compared relatively similar summary lengths
(2 vs. 3 vs. 4 lines long).
2.3 Categorizing Questions by Expected
Answer Types
In the field of automated question-answering, much
effort has been expended on automatically deter-
mining the kind of answer that is expected for a
given question. The candidate answer types are
often drawn from the types of questions that have
appeared in the TREC Question Answering track
(Voorhees, 2003). For example, the Webclopedia
project created a taxonomy of 180 types of ques-
tion targets (Hovy et al, 2002), and the FALCON
project (Harabagiu et al, 2003) developed an an-
swer taxonomy with 33 top level categories (such
as PERSON, TIME, REASON, PRODUCT, LOCA-
TION, NUMERICAL VALUE, QUOTATION), and
these were further refined into an unspecified num-
ber of additional categories. Ramakrishnan et al
(2004) show an automated method for determining
expected answer types using syntactic information
and mapping query terms to WordNet.
2.4 Categorizing Web Queries
A different line of research is the query log cate-
gorization problem. In query logs, the queries are
often much more terse and ill-defined than in the
TREC QA track, and, accordingly, the taxonomies
used to classify what is called the query intent have
been much more general.
In an attempt to demonstrate how information
needs for web search differ from the assumptions
of pre-web information retrieval systems, Broder
(2002) created a taxonomy of web search goals, and
then estimated frequency of such goals by a com-
bination of an online survey (3,200 responses, 10%
response rate) and a manual analysis of 1,000 query
from the AltaVista query logs. This taxonomy has
been heavily influential in discussions of query types
on the Web.
Rose & Levinson (2004) followed up on Broder?s
work, again using web query logs, but developing
a taxonomy that differed somewhat from Broder?s.
They manually classified a set of 1,500 AltaVista
search engine log queries. For two sets of 500
queries, the labeler saw just the query and the re-
trieved documents; for the third set the labeler also
saw information about which item(s) the searcher
clicked on. They found that the classifications that
used the extra information about clickthrough did
not change the proportions of assignments to each
category. Because they did not directly compare
judgments with and without click information on
the same queries, this is only weak evidence that
query plus retrieved documents is sufficient to clas-
sify query intent.
Alternatively, queries from web query logs can be
classified according to the topic of the query, inde-
pendent of the type of information need. For ex-
ample, a search involving the topic of weather can
consist of the simple information need of looking
at today?s forecast, or the rich and complex infor-
mation need of studying meteorology. Over many
years, Spink & Jansen et al (2006; 2007) have man-
ually analyzed samples of query logs to track a num-
ber of different trends. One of the most notable is
the change in topic mix. As an alternative to man-
703
ual classification of query topics, Shen et al (2005)
described an algorithm for automatically classifying
web queries into a set of pre-defined topics. More re-
cently, Broder et al (2007) presented a highly accu-
rate method (around .7 F-score) for classifying short,
rare queries into a taxonomy of 6,000 categories.
3 Study Goals
Related work suggests that longer results are prefer-
able, but not for all query types. The goal of our
efforts was to determine preferred result length for
search results, depending on type of query. To do
this, we performed two studies:
1. We asked a set of judges to categorize a large
set of web queries according to their expected
preferred response type and expected preferred
response length.
2. We then developed high-quality answer pas-
sages of different lengths for a subset of these
queries by selecting appropriate passages from
the online encyclopedia Wikipedia, and asked
judges to rate the quality of these answers.
The results of this study should inform search in-
terface designers about what the best presentation
format is.
3.1 Using Mechanical Turk
For these studies, we make use of a web service of-
fered by Amazon.com called Mechanical Turk, in
which participants (called ?turkers?) are paid small
sums of money in exchange for work on ?Human
Intelligence tasks? (HITs).1 These HITs are gener-
ated from an XML description of the task created
by the investigator (called a ?requester?). The par-
ticipants can come from any walk of life, and their
identity is not known to the requesters. We have in
past work found the results produced by these judges
to be of high quality, and have put into place vari-
ous checks to detect fraudulent behavior. Other re-
searchers have investigated the efficacy of language
1Website: http://www.mturk.com. For experiment 1, ap-
proximately 38,000 HITs were completed at a cost of about
$1,500. For experiment 2, approximately 7,300 HITs were
completed for about $170. Turkers were paid between $.01 and
$.05 per HIT depending on task complexity; Amazon imposes
additional charges.
1. Person(s)
2. Organization(s)
3. Time(s) (date, year, time span etc.)
4. Number or Quantity
5. Geographic Location(s) (e.g., city, lake, address)
6. Place(s) (e.g.,?the White House?, ?at a supermar-
ket?)
7. Obtain resource online (e.g., movies, lyrics, books,
magazines, knitting patterns)
8. Website or URL
9. Purchase and product information
10. Gossip and celebrity information
11. Language-related (e.g., translations, definitions,
crossword puzzle answers)
12. General information about a topic
13. Advice
14. Reason or Cause, Explanation
15. Yes/No, with or without explanation or evidence
16. Other
17. Unjudgable
Table 1: Allowable responses to the question: ?What sort
of result or results does the query ask for?? in the first
experiment.
1. A word or short phrase
2. A sentence
3. One or more paragraphs (i.e. at least several sen-
tences)
4. An article or full document
5. A list
6. Other, or some combination of the above
Table 2: Allowable responses to the question: ?How long
is the best result for this query?? in the first experiment.
annotation using this service and have found that the
results are of high quality (Su et al, 2007).
3.2 Estimating Ideal Answer Length and Type
We developed a set of 12,790 queries, drawn from
Powerset?s in house query database which con-
tains representative subsets of queries from different
search engines? query logs, as well as hand-edited
query sets used for regression testing. There are a
disproportionally large number of natural language
queries in this set compared with query sets from
typical keyword engines. Such queries are often
complete questions and are sometimes grammatical
fragments (e.g., ?date of next US election?) and so
are likely to be amenable to interesting natural lan-
guage processing algorithms, which is an area of in-
704
Figure 1: Results of the first experiment. The y-axis shows the semantic type of the predicted answer, in the same
order as listed in Table 1; the x-axis shows the preferred length as listed in Table 2. Three bars with length greater
than 1,500 are trimmed to the maximum size to improve readability (GeneralInfo/Paragraphs, GeneralInfo/Article,
and Number/Phrase).
terest of our research. The average number of words
per query (as determined by white space separation)
was 5.8 (sd. 2.9) and the average number of char-
acters (including punctuation and white space) was
32.3 (14.9). This is substantially longer than the cur-
rent average for web search query, which was ap-
proximately 2.8 in 2005 (Jansen et al, 2007); this is
due to the existence of natural language queries.
Judges were asked to classify each query accord-
ing to its expected response type into one of 17 cat-
egories (see Table 1). These categories include an-
swer types used in question answering research as
well as (to better capture the diverse nature of web
queries) several more general response types such
as Advice and General Information. Additionally,
we asked judges to anticipate what the best result
length would be for the query, as shown in Table 2.
Each of the 12,790 queries received three assess-
ments by MTurk judges. For answer types, the
number of times all three judges agreed was 4537
(35.4%); two agreed 6030 times (47.1%), and none
agreed 2223 times (17.4%). Not surprisingly, there
was significant overlap between the label General-
Info and the other categories. For answer length
estimations, all three judges agreed in 2361 cases
(18.5%), two agreed in 7210 cases (56.4%) and none
3219 times (25.2%).
Figure 1 summarizes expected length judgments
by estimated answer category. Distribution of the
length categories differs a great deal across the in-
dividual expected response categories. In general,
the results are intuitive: judges preferred short re-
sponses for ?precise queries? (e.g., those asking for
numbers) and they preferred longer responses for
queries in broad categories like Advice or Gener-
alInfo. But some results are less intuitive: for ex-
ample, judges preferred different response lengths
for queries categorized as Person and Organization
? in fact for the latter the largest single selection
made was List. Reviewing the queries for these
two categories, we note that most queries about or-
ganizations in our collection asked for companies
705
length type average std dev
Word or Phrase 38.1 25.8
Sentence 148.1 71.4
Paragraph 490.5 303.1
Section 1574.2 1251.1
Table 3: Average number of characters for each answer
length type for the stimuli used in the second experiment.
(e.g. ?around the world travel agency?) and for
these there usually is more than one correct answer,
whereas the queries about persons (?CEO of mi-
crosoft? ) typically only had one relevant answer.
The results of this table show that there are some
trends but not definitive relationships between query
type (as classified in this study) and expected answer
length. More detailed classifications might help re-
solve some of the conflicts.
3.3 Result Length Study
The purpose of the second study was twofold: first,
to see if doing a larger study confirms what is hinted
at in the literature: that search result lengths longer
than the standard snippet may be desirable for at
least a subset of queries. Second, we wanted to
see if judges? predictions of desirable results lengths
would be confirmed by other judges? responses to
search results of different lengths.
3.3.1 Method
It has been found that obtaining judges? agree-
ment on intent of a query from a log can be difficult
(Rose and Levinson, 2004; Kellar et al, 2007). In
order to make the task of judging query relevance
easier, for the next phase of the study we focused
on only those queries for which all three assessors
in the first experiment agreed both on the category
label and on the estimated ideal length. There were
1099 such high-confidence queries, whose average
number of words was 6.3 (2.9) and average number
of characters was 34.5 (14.3).
We randomly selected a subset of the high-
agreement queries from the first experiment and
manually excluded queries for which it seemed ob-
vious that no responses could be found in Wikipedia.
These included queries about song lyrics, since in-
tellectual property restrictions prevent these being
posted, and crossword puzzle questions such as ?a
four letter word for water.?
The remaining set contained 170 queries. MTurk
annotators were asked to find one good text passage
(in English) for each query from the Wikipedia on-
line encyclopedia. They were also asked to subdi-
vide the text of this answer into each of the following
lengths: a word or phrase, a sentence, a paragraph,
a section or an entire article.2 Thus, the shorter an-
swer passages are subsumed by the longer ones.
Table 3 shows the average lengths and standard
deviations of each result length type. Table 4 con-
tains sample answers for the shorter length formats
for one query. For 24 of the 170 queries the annota-
tors could not find a suitable response in Wikipedia,
e.g., ?How many miles between NY and Milwau-
kee?? We collected two to five results for each of the
remaining 146 queries and manually chose the best
of these answer passages. Note that, by design, all
responses were factually correct; they only differed
in their length.
Ten MTurk judges saw each query/answer length
pair, and for each of these, were told: ?Below you
see a search engine query and a possible response.
We would like you to give us your opinion about the
response. We are especially interested in the length
of the response. Is it suitable for the query? Is there
too much or not enough information? Please rate the
response on a scale from 0 (very bad response) to 10
(very good response).? There were 124 judges in to-
tal; of these, 16 did more than 146 HITs, meaning
they saw the same query more than one time (but
with different lengths). Upon examination of the re-
sults, we determined that two of these high-volume
judges were not trying to do the task properly, and so
we dropped their judgments from the final analysis.
3.3.2 Results
Our results show that judges prefer results of dif-
ferent lengths, depending on the query. The re-
sults also suggest that judges? estimates of a pre-
ferred result length in the first experiment are ac-
curate predictors when there is strong agreement
among them. Figure 2 shows in four diagrams
2Note the slight difference between the length categories in
the first and second experiment: The List and Other options
were dropped for the second experiment because we wanted to
concentrate on textual length. Additionally, to provide more
than one option between Sentence and Article, the category
One or more paragraphs was split up into two: (One) Para-
graph and (One) Section.
706
query Who was the first person to scale K2?
Paragraph An Italian expedition finally succeeded in ascending to the summit of K2 on July 31, 1954.
The expedition was led by Ardito Desio, although the two climbers who actually reached
the top were Lino Lacedelli and Achille Compagnoni. The team included a Pakistani mem-
ber, Colonel Muhammad Ata-ullah. He had been a part of an earlier 1953 American expe-
dition which failed to make the summit because of a storm which killed a key climber, Art
Gilkey. On the expedition also was the famous Italian climber Walter Bonatti. He proved
vital to the expeditions success in that he carried vital oxygen to 26,600ft for Lacedelli
and Compagnoni. His dramatic bivouac, at that altitude with the equipment, wrote another
chapter in the saga of Himalayan climbing.
Sentence The expedition was led by Ardito Desio, although the two climbers who actually reached
the top were Lino Lacedelli and Achille Compagnoni.
Phrase Lino Lacedelli and Achille Compagnoni
Table 4: Sample answers of differing lengths used as input for the second study. Note that the shorter answers are
contained in the longer ones. For the full article case, judges were asked to follow a hyperlink to an article.
Figure 2: Results of the second experiment, where each query/answer-length pair was assessed by 8?10 judges using
a scale of 0 (?very bad?) to 10 (?very good?). Marks indicate means and standard errors. The top left graph shows
responses of different lengths for queries that were classified as best answered with a phrase in the first experiment.
The upper right shows responses for queries predicted to be best answered with a sentence, lower left for best answered
with one or more paragraphs and lower right for best answered with an article.
707
Slope Std. Error p-value
Phrase -0.850 0.044 < 0.0001
Sentence -0.550 0.050 < 0.0001
Paragraph 0.328 0.049 < 0.0001
Article 0.856 0.053 < 0.0001
Table 5: Results of unweighted linear regression on the
data for the second experiment, which was separated into
four groups based on the predicted preferred length.
how queries assigned by judges to one of the four
length categories from the first experiment were
judged when presented with responses of the five
answer lengths from the second experiment. The
graphs show the means and standard error of the
judges? scores across all queries for each predicted-
length/presented-length combination.
In order to test whether these results are signifi-
cant we performed four separate linear regressions;
one for each of the predicted preferred length cat-
egories. The snippet length, the independent vari-
able, was coded as 1-5, shortest to longest. The
score for each query-snippet pair is the dependent
variable. Table 5 shows that for each group there is
evidence to reject the null hypothesis that the slope
is equal to 0 at the 99% confidence level. High
scores are associated with shorter snippet lengths
for queries with predicted preferred length phrase
or sentence and also with longer snippet lengths for
queries with predicted preferred length paragraphs
or article. These associations are strongest for the
queries with the most extreme predicted preferred
lengths (phrase and article).
Our results also suggest the intuition that the best
answer lengths do not form strictly distinct classes,
but rather lie on a continuum. If the ideal response is
from a certain category (e.g., a sentence), returning a
result from an adjacent category (a phrase or a para-
graph) is not strongly penalized by judges, whereas
retuning a result from a category further up or down
the scale (an article) is.
One potential drawback of this study format is
that we do not show judges a list of results for
queries, as is standard in search engines, and so they
do not experience the tradeoff effect of longer results
requiring more scrolling if the desired answer is not
shown first. However, the earlier results of Cutrell &
Guan (2007) and Paek et al (2004) suggest that the
preference for longer results occurs even in contexts
that require looking through multiple results. An-
other potential drawback of the study is that judges
only view one relevant result; the effects of showing
a list of long non-relevant results may be more neg-
ative than that of showing short non-relevant results;
this study would not capture that effect.
4 Conclusions and Future Work
Our studies suggest that different queries are best
served with different response lengths (Experi-
ment 1), and that for a subset of especially clear
queries, human judges can predict the preferred re-
sult lengths (Experiment 2). The results furthermore
support the contention that standard results listings
are too short in many cases, at least assuming that
the summary shows information that is relevant for
the query. These findings have important implica-
tions for the design of search results presentations,
suggesting that as user queries become more expres-
sive, search engine results should become more re-
sponsive to the type of answer desired. This may
mean showing more context in the results listing, or
perhaps using more dynamic tools such as expand-
on-mouseover to help answer the query in place.
The obvious next step is to determine how to au-
tomatically classify queries according to their pre-
dicted result length and type. For classifying ac-
cording to expected length, we have run some initial
experiments based on unigram word counts which
correctly classified 78% of 286 test queries (on 805
training queries) into one of three length bins. We
plan to pursue this further in future work. For classi-
fying according to type, as discussed above, most
automated query classification for web logs have
been based on the topic of the query rather than on
the intended result type, but the question answering
literature has intensively investigated how to pre-
dict appropriate answer types. It is likely that the
techniques from these two fields can be productively
combined to address this challenge.
Acknowledgments. This work was supported in
part by Powerset, Inc., and in part by Microsoft Re-
search through the MSR European PhD Scholarship
Programme. We would like to thank Susan Gruber
and Bonnie Webber for their helpful comments and
suggestions.
708
References
A. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, V. Josi-
fovski, and T. Zhang. 2007. Robust classification of
rare queries using web knowledge. Proceedings of SI-
GIR 2007.
A. Broder. 2002. A taxonomy of web search. ACM
SIGIR Forum, 36(2):3?10.
E. Cutrell and Z. Guan. 2007. What Are You Looking
For? An Eye-tracking Study of Information Usage in
Web Search. Proceedings of ACM SIGCHI 2007.
D.E. Egan, J.R. Remde, L.M. Gomez, T.K. Landauer,
J. Eberhardt, and C.C. Lochbaum. 1989. Formative
design evaluation of Superbook. ACM Transactions
on Information Systems (TOIS), 7(1):30?57.
S.M. Harabagiu, S.J. Maiorano, and M.A. Pasca. 2003.
Open-domain textual question answering techniques.
Natural Language Engineering, 9(03):231?267.
E. Hovy, U. Hermjakob, and D. Ravichandran. 2002.
A question/answer typology with surface text patterns.
Proceedings of the second international conference on
Human Language Technology Research, pages 247?
251.
B.J. Jansen and Spink. 2006. How are we searching
the World Wide Web? A comparison of nine search
engine transaction logs. Information Processing and
Management, 42(1):248?263.
B.J. Jansen, A. Spink, and S. Koshman. 2007. Web
searcher interaction with the Dogpile.com metasearch
engine. Journal of the American Society for Informa-
tion Science and Technology, 58(5):744?755.
M. Kellar, C. Watters, and M. Shepherd. 2007. A Goal-
based Classification of Web Information Tasks. JA-
SIST, 43(1).
J. Lin, D. Quan, V. Sinha, K. Bakshi, D. Huynh, B. Katz,
and D.R. Karger. 2003. What Makes a Good Answer?
The Role of Context in Question Answering. Human-
Computer Interaction (INTERACT 2003).
G. Marchionini, R.W. White, and Marchionini. 2008.
Find What You Need, Understand What You Find.
Journal of Human-Computer Interaction (to appear).
T. Paek, S.T. Dumais, and R. Logan. 2004. WaveLens:
A new view onto internet search results. Proceedings
on the ACM SIGCHI Conference on Human Factors in
Computing Systems, pages 727?734.
J. Pedersen, D. Cutting, and J. Tukey. 1991. Snippet
search: A single phrase approach to text access. Pro-
ceedings of the 1991 Joint Statistical Meetings.
G. Ramakrishnan and D. Paranjpe. 2004. Is question an-
swering an acquired skill? Proceedings of the 13th
international conference on World Wide Web, pages
111?120.
D.E. Rose and D. Levinson. 2004. Understanding user
goals in web search. Proceedings of the 13th interna-
tional conference on World Wide Web, pages 13?19.
D.E. Rose, D. Orr, and R.G.P. Kantamneni. 2007. Sum-
mary attributes and perceived search quality. Pro-
ceedings of the 16th international conference on World
Wide Web, pages 1201?1202.
D. Shen, R. Pan, J.T. Sun, J.J. Pan, K. Wu, J. Yin,
and Q. Yang. 2005. Q2C@UST: our winning solu-
tion to query classification in KDDCUP 2005. ACM
SIGKDD Explorations Newsletter, 7(2):100?110.
Q. Su, D. Pavlov, J. Chow, and W. Baker. 2007. Internet-
Scale Collection of Human-Reviewed Data. Proceed-
ings of WWW 2007.
A. Tombros and M. Sanderson. 1998. Advantages of
query biased summaries in information retrieval. Pro-
ceedings of the 21st annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 2?10.
E.M. Voorhees. 2003. Overview of the TREC 2003
Question Answering Track. Proceedings of the
Twelfth Text REtrieval Conference (TREC 2003).
R.W. White, J. Jose, and I. Ruthven. 2003. A task-
oriented study on the influencing effects of query-
biased summarisation in web searching. Information
Processing and Management, 39(5):707?733.
709
Classifying the Semantic Relations in Noun Compounds via a
Domain-Specific Lexical Hierarchy
Barbara Rosario and Marti Hearst
School of Information Management & Systems
University of California, Berkeley
Berkeley, CA 94720-4600
{rosario,hearst}@sims.berkeley.edu
Abstract
We are developing corpus-based techniques for iden-
tifying semantic relations at an intermediate level of
description (more specific than those used in case
frames, but more general than those used in tra-
ditional knowledge representation systems). In this
paper we describe a classification algorithm for iden-
tifying relationships between two-word noun com-
pounds. We find that a very simple approach using
a machine learning algorithm and a domain-specific
lexical hierarchy successfully generalizes from train-
ing instances, performing better on previously un-
seen words than a baseline consisting of training on
the words themselves.
1 Introduction
We are exploring empirical methods of determin-
ing semantic relationships between constituents in
natural language. Our current project focuses on
biomedical text, both because it poses interesting
challenges, and because it should be possible to make
inferences about propositions that hold between sci-
entific concepts within biomedical texts (Swanson
and Smalheiser, 1994).
One of the important challenges of biomedical
text, along with most other technical text, is the
proliferation of noun compounds. A typical article
title is shown below; it consists a cascade of four
noun phrases linked by prepositions:
Open-labeled long-term study of the effi-
cacy, safety, and tolerability of subcuta-
neous sumatriptan in acute migraine treat-
ment.
The real concern in analyzing such a title is in de-
termining the relationships that hold between differ-
ent concepts, rather than on finding the appropriate
attachments (which is especially difficult given the
lack of a verb). And before we tackle the prepo-
sitional phrase attachment problem, we must find
a way to analyze the meanings of the noun com-
pounds.
Our goal is to extract propositional information
from text, and as a step towards this goal, we clas-
sify constituents according to which semantic rela-
tionships hold between them. For example, we want
to characterize the treatment-for-disease relation-
ship between the words of migraine treatment ver-
sus the method-of-treatment relationship between
the words of aerosol treatment. These relations are
intended to be combined to produce larger proposi-
tions that can then be used in a variety of interpreta-
tion paradigms, such as abductive reasoning (Hobbs
et al, 1993) or inductive logic programming (Ng and
Zelle, 1997).
Note that because we are concerned with the se-
mantic relations that hold between the concepts, as
opposed to the more standard, syntax-driven com-
putational goal of determining left versus right as-
sociation, this has the fortuitous effect of changing
the problem into one of classification, amenable to
standard machine learning classification techniques.
We have found that we can use such algorithms to
classify relationships between two-word noun com-
pounds with a surprising degree of accuracy. A
one-out-of-eighteen classification using a neural net
achieves accuracies as high as 62%. By taking ad-
vantage of lexical ontologies, we achieve strong re-
sults on noun compounds for which neither word is
present in the training set. Thus, we think this is a
promising approach for a variety of semantic label-
ing tasks.
The reminder of this paper is organized as follows:
Section 2 describes related work, Section 3 describes
the semantic relations and how they were chosen,
and Section 4 describes the data collection and on-
tologies. In Section 5 we describe the method for
automatically assigning semantic relations to noun
compounds, and report the results of experiments
using this method. Section 6 concludes the paper
and discusses future work.
2 Related Work
Several approaches have been proposed for empiri-
cal noun compound interpretation. Lauer and Dras
(1994) point out that there are three components to
the problem: identification of the compound from
within the text, syntactic analysis of the compound
(left versus right association), and the interpreta-
tion of the underlying semantics. Several researchers
have tackled the syntactic analysis (Lauer, 1995;
Pustejovsky et al, 1993; Liberman and Sproat,
1992), usually using a variation of the idea of find-
ing the subconstituents elsewhere in the corpus and
using those to predict how the larger compounds are
structured.
We are interested in the third task, interpretation
of the underlying semantics. Most related work re-
lies on hand-written rules of one kind or another.
Finin (1980) examines the problem of noun com-
pound interpretation in detail, and constructs a
complex set of rules. Vanderwende (1994) uses a so-
phisticated system to extract semantic information
automatically from an on-line dictionary, and then
manipulates a set of hand-written rules with hand-
assigned weights to create an interpretation. Rind-
flesch et al (2000) use hand-coded rule based sys-
tems to extract the factual assertions from biomed-
ical text. Lapata (2000) classifies nominalizations
according to whether the modifier is the subject or
the object of the underlying verb expressed by the
head noun.1
In the related sub-area of information extraction
(Cardie, 1997; Riloff, 1996), the main goal is to find
every instance of particular entities or events of in-
terest. These systems use empirical techniques to
learn which terms signal entities of interest, in order
to fill in pre-defined templates. Our goals are more
general than those of information extraction, and
so this work should be helpful for that task. How-
ever, our approach will not solve issues surrounding
previously unseen proper nouns, which are often im-
portant for information extraction tasks.
There have been several efforts to incorporate lex-
ical hierarchies into statistical processing, primar-
ily for the problem of prepositional phrase (PP)
attachment. The current standard formulation is:
given a verb followed by a noun and a prepositional
phrase, represented by the tuple v, n1, p, n2, deter-
mine which of v or n1 the PP consisting of p and
n2 attaches to, or is most closely associated with.
Because the data is sparse, empirical methods that
train on word occurrences alone (Hindle and Rooth,
1993) have been supplanted by algorithms that gen-
eralize one or both of the nouns according to class-
membership measures (Resnik, 1993; Resnik and
Hearst, 1993; Brill and Resnik, 1994; Li and Abe,
1998), but the statistics are computed for the par-
ticular preposition and verb.
It is not clear how to use the results of such anal-
ysis after they are found; the semantics of the rela-
1Nominalizations are compounds whose head noun is a
nominalized verb and whose modifier is either the subject or
the object of the verb. We do not distinguish the NCs on the
basis of their formation.
tionship between the terms must still be determined.
In our framework we would cast this problem as
finding the relationship R(p, n2) that best character-
izes the preposition and the NP that follows it, and
then seeing if the categorization algorithm deter-
mines their exists any relationship R?(n1, R(p, n2))
or R?(v,R(p, n2)).
The algorithms used in the related work reflect the
fact that they condition probabilities on a particular
verb and noun. Resnik (1993; 1995) use classes in
Wordnet (Fellbaum, 1998) and a measure of concep-
tual association to generalize over the nouns. Brill
and Resnik (1994) use Brill?s transformation-based
algorithm along with simple counts within a lexi-
cal hierarchy in order to generalize over individual
words. Li and Abe (1998) use a minimum descrip-
tion length-based algorithm to find an optimal tree
cut over WordNet for each classification problem,
finding improvements over both lexical association
(Hindle and Rooth, 1993) and conceptual associa-
tion, and equaling the transformation-based results.
Our approach differs from these in that we are us-
ing machine learning techniques to determine which
level of the lexical hierarchy is appropriate for gen-
eralizing across nouns.
3 Noun Compound Relations
In this work we aim for a representation that is in-
termediate in generality between standard case roles
(such as Agent, Patient, Topic, Instrument), and the
specificity required for information extraction. We
have created a set of relations that are sufficiently
general to cover a significant number of noun com-
pounds, but that can be domain specific enough to
be useful in analysis. We want to support relation-
ships between entities that are shown to be impor-
tant in cognitive linguistics, in particular we intend
to support the kinds of inferences that arise from
Talmy?s force dynamics (Talmy, 1985). It has been
shown that relations of this kind can be combined in
order to determine the ?directionality? of a sentence
(e.g., whether or not a politician is in favor of, or op-
posed to, a proposal) (Hearst, 1990). In the medical
domain this translates to, for example, mapping a
sentence into a representation showing that a chem-
ical removes an entity that is blocking the passage
of a fluid through a channel.
The problem remains of determining what the ap-
propriate kinds of relations are. In theoretical lin-
guistics, there are contradictory views regarding the
semantic properties of noun compounds (NCs). Levi
(1978) argues that there exists a small set of se-
mantic relationships that NCs may imply. Downing
(1977) argues that the semantics of NCs cannot be
exhausted by any finite listing of relationships. Be-
tween these two extremes lies Warren?s (1978) tax-
onomy of six major semantic relations organized into
a hierarchical structure.
We have identified the 38 relations shown in Ta-
ble 1. We tried to produce relations that correspond
to the linguistic theories such as those of Levi and
Warren, but in many cases these are inappropriate.
Levi?s classes are too general for our purposes; for
example, she collapses the ?location? and ?time?
relationships into one single class ?In? and there-
fore field mouse and autumnal rain belong to the
same class. Warren?s classification schema is much
more detailed, and there is some overlap between
the top levels of Warren?s hierarchy and our set
of relations. For example, our ?Cause (2-1)? for
flu virus corresponds to her ?Causer-Result? of hay
fever, and our ?Person Afflicted? (migraine patient)
can be thought as Warren?s ?Belonging-Possessor?
of gunman. Warren differentiates some classes also
on the basis of the semantics of the constituents,
so that, for example, the ?Time? relationship is di-
vided up into ?Time-Animate Entity? of weekend
guests and ?Time-Inanimate Entity? of Sunday pa-
per. Our classification is based on the kind of re-
lationships that hold between the constituent nouns
rather than on the semantics of the head nouns.
For the automatic classification task, we used only
the 18 relations (indicated in bold in Table 1) for
which an adequate number of examples were found
in the current collection. Many NCs were ambigu-
ous, in that they could be described by more than
one semantic relationship. In these cases, we sim-
ply multi-labeled them: for example, cell growth is
both ?Activity? and ?Change?, tumor regression is
?Ending/reduction? and ?Change? and bladder dys-
function is ?Location? and ?Defect?. Our approach
handles this kind of multi-labeled classification.
Two relation types are especially problematic.
Some compounds are non-compositional or lexical-
ized, such as vitamin k and e2 protein; others defy
classification because the nouns are subtypes of one
another. This group includes migraine headache,
guinea pig, and hbv carrier. We placed all these NCs
in a catch-all category. We also included a ?wrong?
category containing word pairs that were incorrectly
labeled as NCs.2
The relations were found by iterative refinement
based on looking at 2245 extracted compounds (de-
scribed in the next section) and finding commonal-
ities among them. Labeling was done by the au-
thors of this paper and a biology student; the NCs
were classified out of context. We expect to con-
tinue development and refinement of these relation-
ship types, based on what ends up clearly being use-
2The percentage of the word pairs extracted that were not
true NCs was about 6%; some examples are: treat migraine,
ten patient, headache more. We do not know, however, how
many NCs we missed. The errors occurred when the wrong
label was assigned by the tagger (see Section 4).
ful ?downstream? in the analysis.
The end goal is to combine these relationships in
NCs with more that two constituent nouns, like in
the example intranasal migraine treatment of Sec-
tion 1.
4 Collection and Lexical Resources
To create a collection of noun compounds, we per-
formed searches from MedLine, which contains ref-
erences and abstracts from 4300 biomedical journals.
We used several query terms, intended to span across
different subfields. We retained only the titles and
the abstracts of the retrieved documents. On these
titles and abstracts we ran a part-of-speech tagger
(Cutting et al, 1991) and a program that extracts
only sequences of units tagged as nouns. We ex-
tracted NCs with up to 6 constituents, but for this
paper we consider only NCs with 2 constituents.
The Unified Medical Language System (UMLS)
is a biomedical lexical resource produced and
maintained by the National Library of Medicine
(Humphreys et al, 1998). We use the MetaThe-
saurus component to map lexical items into unique
concept IDs (CUIs).3 The UMLS also has a map-
ping from these CUIs into the MeSH lexical hier-
archy (Lowe and Barnett, 1994); we mapped the
CUIs into MeSH terms. There are about 19,000
unique main terms in MeSH, as well as additional
modifiers. There are 15 main subhierarchies (trees)
in MeSH, each corresponding to a major branch
of medical ontology. For example, tree A cor-
responds to Anatomy, tree B to Organisms, and
so on. The longer the name of the MeSH term,
the longer the path from the root and the more
precise the description. For example migraine is
C10.228.140.546.800.525, that is, C (a disease), C10
(Nervous System Diseases), C10.228 (Central Ner-
vous System Diseases) and so on.
We use the MeSH hierarchy for generalization
across classes of nouns; we use it instead of the other
resources in the UMLS primarily because of MeSH?s
hierarchical structure. For these experiments, we
considered only those noun compounds for which
both nouns can be mapped into MeSH terms, re-
sulting in a total of 2245 NCs.
5 Method and Results
Because we have defined noun compound relation
determination as a classification problem, we can
make use of standard classification algorithms. In
particular, we used neural networks to classify across
all relations simultaneously.
3In some cases a word maps to more than one CUI; for the
work reported here we arbitrarily chose the first mapping in
all cases. In future work we will explore how to make use of
all of the mapped terms.
Name N Examples
Wrong parse (1) 109 exhibit asthma, ten drugs, measure headache
Subtype (4) 393 headaches migraine, fungus candida, hbv carrier,
giant cell, mexico city, t1 tumour, ht1 receptor
Activity/Physical process (5) 59 bile delivery, virus reproduction, bile drainage,
headache activity, bowel function, tb transmission
Ending/reduction 8 migraine relief, headache resolution
Beginning of activity 2 headache induction, headache onset
Change 26 papilloma growth, headache transformation,
disease development, tissue reinforcement
Produces (on a genetic level) (7) 47 polyomavirus genome, actin mrna, cmv dna, protein gene
Cause (1-2) (20) 116 asthma hospitalizations, aids death, automobile accident
heat shock, university fatigue, food infection
Cause (2-1) 18 flu virus, diarrhoea virus, influenza infection
Characteristic (8) 33 receptor hypersensitivity, cell immunity,
drug toxicity, gene polymorphism, drug susceptibility
Physical property 9 blood pressure, artery diameter, water solubility
Defect (27) 52 hormone deficiency, csf fistulas, gene mutation
Physical Make Up 6 blood plasma, bile vomit
Person afflicted (15) 55 aids patient, bmt children, headache group, polio survivors
Demographic attributes 19 childhood migraine, infant colic, women migraineur
Person/center who treats 20 headache specialist, headache center, diseases physicians,
asthma nurse, children hospital
Research on 11 asthma researchers, headache study, language research
Attribute of clinical study (18) 77 headache parameter, attack study, headache interview,
biology analyses, biology laboratory, influenza epidemiology
Procedure (36) 60 tumor marker, genotype diagnosis, blood culture,
brain biopsy, tissue pathology
Frequency/time of (2-1) (22) 25 headache interval, attack frequency,
football season, headache phase, influenza season
Time of (1-2) 4 morning headache, hour headache, weekend migraine
Measure of (23) 54 relief rate, asthma mortality, asthma morbidity,
cell population, hospital survival
Standard 5 headache criteria, society standard
Instrument (1-2) (33) 121 aciclovir therapy, chloroquine treatment,
laser irradiation, aerosol treatment
Instrument (2-1) 8 vaccine antigen, biopsy needle, medicine ginseng
Instrument (1) 16 heroin use, internet use, drug utilization
Object (35) 30 bowel transplantation, kidney transplant, drug delivery
Misuse 11 drug abuse, acetaminophen overdose, ergotamine abuser
Subject 18 headache presentation, glucose metabolism, heat transfer
Purpose (14) 61 headache drugs, hiv medications, voice therapy,
influenza treatment, polio vaccine
Topic (40) 38 time visualization, headache questionnaire, tobacco history,
vaccination registries, health education, pharmacy database
Location (21) 145 brain artery, tract calculi, liver cell, hospital beds
Modal 14 emergency surgery, trauma method
Material (39) 28 formaldehyde vapor, aloe gel, gelatin powder, latex glove,
Bind 4 receptor ligand, carbohydrate ligand
Activator (1-2) 6 acetylcholine receptor, pain signals
Activator (2-1) 4 headache trigger, headache precipitant
Inhibitor 11 adrenoreceptor blockers, influenza prevention
Defect in Location (21 27) 157 lung abscess, artery aneurysm, brain disorder
Table 1: The semantic relations defined via iterative refinement over a set of noun compounds. The relations
shown in boldface are those used in the experiments reported on here. Relation ID numbers are shown in
parentheses by the relation names. The second column shows the number of labeled examples for each class;
the last row shows a class consisting of compounds that exhibit more than one relation. The notation (1-2)
and (2-1) indicates the directionality of the relations. For example, Cause (1-2) indicates that the first noun
causes the second, and Cause (2-1) indicates the converse.
flu vaccination
Model 2 D 4 G 3
Model 3 D 4 808 G 3 770
Model 4 D 4 808 54 G 3 770
Model 5 D 4 808 54 79 G 3 770 670
Model 6 D 4 808 54 79 429 G 3 770 670 310
Table 2: Different lengths of the MeSH descriptors
for the different models
Model Feature Vector
2 42
3 315
4 687
5 950
6 1111
Lexical 1184
Table 3: Length of the feature vectors for different
models.
We ran the experiments creating models that used
different levels of the MeSH hierarchy. For example,
for the NC flu vaccination, flu maps to the MeSH
term D4.808.54.79.429.154.349 and vaccination to
G3.770.670.310.890. Flu vaccination for Model 4
would be represented by a vector consisting of the
concatenation of the two descriptors showing only
the first four levels: D4.808.54.79 G3.770.670.310
(see Table 2). When a word maps to a general MeSH
term (like treatment, Y11) zeros are appended to the
end of the descriptor to stand in place of the missing
values (so, for example, treatment in Model 3 is Y
11 0, and in Model 4 is Y 11 0 0, etc.).
The numbers in the MeSH descriptors are cate-
gorical values; we represented them with indicator
variables. That is, for each variable we calculated
the number of possible categories c and then repre-
sented an observation of the variable as a sequence of
c binary variables in which one binary variable was
one and the remaining c ? 1 binary variables were
zero.
We also used a representation in which the words
themselves were used as categorical input variables
(we call this representation ?lexical?). For this col-
lection of NCs there were 1184 unique nouns and
therefore the feature vector for each noun had 1184
components. In Table 3 we report the length of the
feature vectors for one noun for each model. The en-
tire NC was described by concatenating the feature
vectors for the two nouns in sequence.
The NCs represented in this fashion were used as
input to a neural network. We used a feed-forward
network trained with conjugate gradient descent.
Model Acc1 Acc2 Acc3
Lexical: Log Reg 0.31 0.58 0.62
Lexical: NN 0.62 0.73 0.78
2 0.52 0.65 0.72
3 0.58 0.70 0.76
4 0.60 0.70 0.76
5 0.60 0.72 0.78
6 0.61 0.71 0.76
Table 4: Test accuracy for each model, where the model
number corresponds to the level of the MeSH hierarchy
used for classification. Lexical NN is Neural Network on
Lexical and Lexical: Log Reg is Logistic Regression on
NN. Acc1 refers to how often the correct relation is the
top-scoring relation, Acc2 refers to how often the correct
relation is one of the top two according to the neural net,
and so on. Guessing would yield a result of 0.077.
The network had one hidden layer, in which a hy-
perbolic tangent function was used, and an output
layer representing the 18 relations. A logistic sig-
moid function was used in the output layer to map
the outputs into the interval (0, 1).
The number of units of the output layer was the
number of relations (18) and therefore fixed. The
network was trained for several choices of numbers of
hidden units; we chose the best-performing networks
based on training set error for each of the models.
We subsequently tested these networks on held-out
testing data.
We compared the results with a baseline in which
logistic regression was used on the lexical features.
Given the indicator variable representation of these
features, this logistic regression essentially forms a
table of log-odds for each lexical item. We also com-
pared to a method in which the lexical indicator vari-
ables were used as input to a neural network. This
approach is of interest to see to what extent, if any,
the MeSH-based features affect performance. Note
also that this lexical neural-network approach is fea-
sible in this setting because the number of unique
words is limited (1184) ? such an approach would
not scale to larger problems.
In Table 4 and in Figure 1 we report the results
from these experiments. Neural network using lex-
ical features only yields 62% accuracy on average
across all 18 relations. A neural net trained on
Model 6 using the MeSH terms to represent the
nouns yields an accuracy of 61% on average across
all 18 relations. Note that reasonable performance is
also obtained for Model 2, which is a much more gen-
eral representation. Table 4 shows that both meth-
ods achieve up to 78% accuracy at including the cor-
rect relation among the top three hypothesized.
Multi-class classification is a difficult problem
(Vapnik, 1998). In this problem, a baseline in which
2 3 4 5 6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Testing set performance on the best models for each  MeSH level
Levels of the MeSH Hierarchy
Ac
cu
ra
cy
 o
n 
te
st
 s
et
Accuracy for the largest NN output
within 2 largest NN output
within 3 largest NN output
Figure 1: Accuracies on the test sets for all the models.
The dotted line at the bottom is the accuracy of guess-
ing (the inverse of the number of classes). The dash-dot
line above this is the accuracy of logistic regression on
the lexical data. The solid line with asterisks is the ac-
curacy of our representation, when only the maximum
output value from the network is considered. The solid
line with circles if the accuracy of getting the right an-
swer within the two largest output values from the neural
network and the last solid line with diamonds is the ac-
curacy of getting the right answer within the first three
outputs from the network. The three flat dashed lines
are the corresponding performances of the neural net-
work on lexical inputs.
the algorithm guesses yields about 5% accuracy. We
see that our method is a significant improvement
over the tabular logistic-regression-based approach,
which yields an accuracy of only 31 percent. Addi-
tionally, despite the significant reduction in raw in-
formation content as compared to the lexical repre-
sentation, the MeSH-based neural network performs
as well as the lexical-based neural network. (And we
again stress that the lexical-based neural network is
not a viable option for larger domains.)
Figure 2 shows the results for each relation.
MeSH-based generalization does better on some re-
lations (for example 14 and 15) and Lexical on others
(7, 22). It turns out that the test set for relation-
ship 7 (?Produces on a genetic level?) is dominated
by NCs containing the words alleles and mrna and
that all the NCs in the training set containing these
words are assigned relation label 7. A similar situa-
tion is seen for relation 22, ?Time(2-1)?. In the test
set examples the second noun is either recurrence,
season or time. In the training set, these nouns ap-
pear only in NCs that have been labeled as belonging
to relation 22.
On the other hand, if we look at relations 14 and
15, we find a wider range of words, and in some cases
1 4 5 7 8 14 15 18 20 21 22 23 27 33 35 36 39 40 2027
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Performance of each class  for the LEXICAL model
Classes
Ac
cu
ra
cie
s 
on
 th
e 
te
st
 s
et
 fo
r t
he
 b
es
t m
od
el
 
MeSH
Lexical
Figure 2: Accuracies for each class. The numbers at the
bottom refer to the class numbers in Table 1. Note the
very high accuracy for the ?mixed? relationship 20-27
(last bar on the right).
the words in the test set are not present in the train-
ing set. In relationship 14 (?Purpose?), for example,
vaccine appears 6 times in the test set (e.g., varicella
vaccine). In the training set, NCs with vaccine in
it have also been classified as ?Instrument? (anti-
gen vaccine, polysaccharide vaccine), as ?Object?
(vaccine development), as ?Subtype of? (opv vac-
cine) and as ?Wrong? (vaccines using). Other words
in the test set for 14 are varicella which is present
in the trainig set only in varicella serology labeled
as ?Attribute of clinical study?, drainage which is
in the training set only as ?Location? (gallbladder
drainage and tract drainage) and ?Activity? (bile
drainage). Other test set words such as immunisa-
tion and carcinogen do not appear in the training
set at all.
In other words, it seems that the MeSHk-based
categorization does better when generalization is re-
quired. Additionally, this data set is ?dense? in the
sense that very few testing words are not present in
the training data. This is of course an unrealistic
situation and we wanted to test the robustness of
the method in a more realistic setting. The results
reported in Table 4 and in Figure 1 were obtained
splitting the data into 50% training and 50% testing
for each relation and we had a total of 855 training
points and 805 test points. Of these, only 75 ex-
amples in the testing set consisted of NCs in which
both words were not present in the training set.
We decided to test the robustness of the MeSH-
based model versus the lexical model in the case of
unseen words; we are also interested in seeing the
relative importance of the first versus the second
noun. Therefore, we split the data into 5% training
(73 data points) and 95% testing (1587 data points)
Model All test 1 2 3 4
Lexical: NN 0.23 0.54 0.14 0.33 0.08
2 0.44 0.62 0.25 0.53 0.38
3 0.41 0.62 0.18 0.47 0.35
4 0.42 0.58 0.26 0.39 0.38
5 0.46 0.64 0.28 0.54 0.40
6 0.44 0.64 0.25 0.50 0.39
Table 5: Test accuracy for the four sub-partitions of
the test set.
and partitioned the testing set into 4 subsets as fol-
lows (the numbers in parentheses are the numbers
of points for each case):
? Case 1: NCs for which the first noun was not
present in the training set (424)
? Case 2: NCs for which the second noun was not
present in the training set (252)
? Case 3: NCs for which both nouns were present
in the training set (101)
? Case 4: NCs for which both nouns were not
present in the training set (810).
Table 5 and Figures 3 and 4 present the accuracies
for these test set partitions. Figure 3 shows that
the MeSH-based models are more robust than the
lexical when the number of unseen words is high and
when the size of training set is (very) small. In this
more realistic situation, the MeSHmodels are able to
generalize over previously unseen words. For unseen
words, lexical reduces to guessing.4
Figure 4 shows the accuracy for the MeSH based-
model for the the four cases of Table 5. It is interest-
ing to note that the accuracy for Case 1 (first noun
not present in the training set) is much higher than
the accuracy for Case 2 (second noun not present in
the training set). This seems to indicate that the
second noun is more important for the classification
that the first one.
6 Conclusions
We have presented a simple approach to corpus-
based assignment of semantic relations for noun
compounds. The main idea is to define a set of rela-
tions that can hold between the terms and use stan-
dard machine learning techniques and a lexical hi-
erarchy to generalize from training instances to new
examples. The initial results are quite promising.
In this task of multi-class classification (with 18
classes) we achieved an accuracy of about 60%.
These results can be compared with Vanderwende
4Note that for unseen words, the baseline lexical-based
logistic regression approach, which essentially builds a tabular
representation of the log-odds for each class, also reduces to
random guessing.
2 3 4 5 6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 Testing set  performances for different partitions on the test set 
Levels of the MeSH Hierarchy
Ac
cu
ra
cy
 o
n 
te
st
 s
et
Accuracy for MeSH for the entire test
Accuracy for MeSH for Case 4
Accuracy for Lexical  for the entire test
Accuracy for Lexical  for Case 4
Guessing
Figure 3: The unbroken lines represent the MeSH mod-
els accuracies (for the entire test set and for case 4) and
the dashed lines represent the corresponding lexical ac-
curacies. The accuracies are smaller than the previous
case of Table 4 because the training set is much smaller,
but the point of interest is the difference in the perfor-
mance of MeSH vs. lexical in this more difficult setting.
Note that lexical for case 4 reduces to random guessing.
2 3 4 5 6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Testing set  performances for different partitions on the test set for the MeSH?based model
Levels of the MeSH Hierarchy
Ac
cu
ra
cy
 o
n 
te
st
 s
et
Accuracy for the entire test
Case 3
Case 1
Case 2
Case 4
Figure 4: Accuracy for the MeSH based-model for the
the four cases. All these curves refer to the case of get-
ting exactly the right answer. Note the difference in
performance between case 1 (first noun not present in
the training set) and case 2 (second noun not present in
training set).
(1994) who reports an accuracy of 52% with 13
classes and Lapata (2000) whose algorithm achieves
about 80% accuracy for a much simpler binary clas-
sification.
We have shown that a class-based representation
performes as well as a lexical-based model despite
the reduction of raw information content and de-
spite a somewhat errorful mapping from terms to
concepts. We have also shown that representing the
nouns of the compound by a very general represen-
tation (Model 2) achieves a reasonable performance
of aout 52% accuracy on average. This is particu-
larly important in the case of larger collections with
a much bigger number of unique words for which
the lexical-based model is not a viable option. Our
results seem to indicate that we do not lose much
in terms of accuracy using the more compact MeSH
representation.
We have also shown how MeSH-besed models out
perform a lexical-based approach when the num-
ber of training points is small and when the test
set consists of words unseen in the training data.
This indicates that the MeSH models can generalize
successfully over unseen words. Our approach han-
dles ?mixed-class? relations naturally. For the mixed
class Defect in Location, the algorithm achieved an
accuracy around 95% for both ?Defect? and ?Loca-
tion? simultaneously. Our results also indicate that
the second noun (the head) is more important in
determining the relationships than the first one.
In future we plan to train the algorithm to allow
different levels for each noun in the compound. We
also plan to compare the results to the tree cut algo-
rithm reported in (Li and Abe, 1998), which allows
different levels to be identified for different subtrees.
We also plan to tackle the problem of noun com-
pounds containing more than two terms.
Acknowledgments
We would like to thank Nu Lai for help with the
classification of the noun compound relations. This
work was supported in part by NSF award number
IIS-9817353.
References
Eric Brill and Philip Resnik. 1994. A rule-based
approach to prepositional phrase attachment dis-
amibuation. In Proceedings of COLING-94.
Claire Cardie. 1997. Empirical methods in informa-
tion extraction. AI Magazine, 18(4).
Douglass R. Cutting, Julian Kupiec, Jan O. Peder-
sen, and Penelope Sibun. 1991. A practical part-
of-speech tagger. In The 3rd Conference on Ap-
plied Natural Language Processing, Trento, Italy.
P. Downing. 1977. On the creation and use of en-
glish compound nouns. Language, (53):810?842.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
Timothy W. Finin. 1980. The Semantic Interpreta-
tion of Compound Nominals. Ph.d. dissertation,
University of Illinois, Urbana, Illinois.
Marti A. Hearst. 1990. A hybrid approach to re-
stricted text interpretation. In Paul S. Jacobs, ed-
itor, Text-Based Intelligent Systems: Current Re-
search in Text Analysis, Information Extraction,
and Retrieval, pages 38?43. GE Research & De-
velopment Center, TR 90CRD198.
Donald Hindle and Mats Rooth. 1993. Structual
ambiguity and lexical relations. Computational
Linguistics, 19(1).
Jerry R. Hobbs, Mark Stickel, Douglas Appelt, and
Paul Martin. 1993. Interpretation as abduction.
Artificial Intelligence, 63(1-2).
L. Humphreys, D.A.B. Lindberg, H.M. Schoolman,
and G. O. Barnett. 1998. The unified medical
language system: An informatics research collab-
oration. Journal of the American Medical Infor-
matics Assocation, 5(1):1?13.
Maria Lapata. 2000. The automatic interpretation
of nominalizations. In AAAI Proceedings.
Mark Lauer and Mark Dras. 1994. A probabilistic
model of compound nouns. In Proceedings of the
7th Australian Joint Conference on AI.
Mark Lauer. 1995. Corpus statistics meet the com-
pound noun. In Proceedings of the 33rd Meeting
of the Association for Computational Linguistics,
June.
Judith Levi. 1978. The Syntax and Semantics of
Complex Nominals. Academic Press, New York.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDI principle.
Computational Linguistics, 24(2):217?244.
Mark Liberman and Richard Sproat. 1992. The
stress and structure of modified noun phrases in
english. In I.l Sag and A. Szabolsci, editors, Lex-
ical Matters. CSLI Lecture Notes No. 24, Univer-
sity of Chicago Press.
Henry J. Lowe and G. Octo Barnett. 1994. Un-
derstanding and using the medical subject head-
ings (MeSH) vocabulary to perform literature
searches. Journal of the American Medical Asso-
cation (JAMA), 271(4):1103?1108.
Hwee Tou Ng and John Zelle. 1997. Corpus-based
approaches to semantic interpretation in natural
language processing. AI Magazine, 18(4).
James Pustejovsky, Sabine Bergler, and Peter An-
ick. 1993. Lexical semantic techniques for corpus
analysis. Computational Linguistics, 19(2).
Philip Resnik and Marti A. Hearst. 1993. Structural
ambiguity and conceptual relations. In Proceed-
ings of the ACL Workshop on Very Large Corpora,
Columbus, OH.
Philip Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania, Decem-
ber. (Institute for Research in Cognitive Science
report IRCS-93-42).
Philip Resnik. 1995. Disambiguating noun group-
ings with respect to WordNet senses. In Third
Workshop on Very Large Corpora. Association for
Computational Linguistics.
Ellen Riloff. 1996. Automatically generating ex-
traction patterns from untagged text. In Pro-
ceedings of the Thirteenth National Conference on
Artificial Intelligence and the Eighth Innovative
Applications of Artificial Intelligence Conference,
Menlo Park. AAAI Press / MIT Press.
Thomas Rindflesch, Lorraine Tanabe, John N. We-
instein, and Lawrence Hunter. 2000. Extraction
of drugs, genes and relations from the biomedical
literature. Pacific Symposium on Biocomputing,
5(5).
Don R. Swanson and N. R. Smalheiser. 1994. As-
sessing a gap in the biomedical literature: Mag-
nesium deficiency and neurologic disease. Neuro-
science Research Communications, 15:1?9.
Len Talmy. 1985. Force dynamics in language and
thought. In Parasession on Causatives and Agen-
tivity, University of Chicago. Chicago Linguistic
Society (21st Regional Meeting).
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings
of COLING-94, pages 782?788.
V. Vapnik. 1998. Statistical Learning Theory. Ox-
ford University Press.
Beatrice Warren. 1978. Semantic Patterns of Noun-
Noun Compounds. Acta Universitatis Gothobur-
gensis.
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 1?8,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Teaching Applied Natural Language Processing: Triumphs and Tribulations
Marti Hearst
School of Information Management & Systems
University of California, Berkeley
Berkeley, CA 94720
hearst@sims.berkeley.edu
Abstract
In Fall 2004 I introduced a new course
called Applied Natural Language Process-
ing, in which students acquire an under-
standing of which text analysis techniques
are currently feasible for practical appli-
cations. The class was intended for in-
terdisciplinary students with a somewhat
technical background. This paper de-
scribes the topics covered and the pro-
gramming exercises, emphasizing which
aspects were successful and which prob-
lematic, and makes recommendations for
future versions of the course.
1 Introduction
In Fall 2005 I introduced a new graduate level course
called Applied Natural Language Processing.1 The
goal of this course was to acquaint students with the
state-of-the-art of the field of NLP with an empha-
sis on applications. The intention was for students
to leave the class with an understanding of what is
currently feasible (and just on the horizon) to ex-
pect from content analysis, and how to use and ex-
tend existing NLP tools and technology. The course
did not emphasize the theoretical underpinnings of
NLP, although we did cover the most important al-
gorithms. A companion graduate course on Statis-
tical NLP was taught by Dan Klein in the Com-
puter Science department. Dan?s course focused on
1Lecture notes, assignments, and other resources can be
found at http://www.sims.berkeley.edu/courses/is290-2/f04/ .
foundations and core NLP algorithms. Several com-
puter science students took both courses, and thus
learned both the theoretical and the applied sides of
NLP. Dan and I discussed the goals and content of
our respective courses in advance, but developed the
courses independently.
2 Course Role within the SIMS Program
The primary target audience of the Applied NLP
course were masters students, and to a lesser ex-
tent, PhD students, in the School of Information
Management and Systems. (Nevertheless, PhD stu-
dents in computer science and other fields also took
the course.) MIMS students (as the SIMS mas-
ters students are known) pursue a professional de-
gree studying information at the intersection of tech-
nology and social sciences. The students? techni-
cal backgrounds vary widely; each year a signifi-
cant fraction have Computer Science undergraduate
degrees, and another significant fraction have so-
cial science or humanities backgrounds. All stu-
dents have an interest in technology and are re-
quired to take some challenging technical courses,
but most non-CS background students are uncom-
fortable with advanced mathematics and are not as
comfortable with coding as CS students are.
A key aspect of the program is the capstone fi-
nal project, completed in the last semester, that (ide-
ally) combines knowledge and skills obtained from
throughout the program. Most students form a team
of 3-4 students and build a system, usually to meet
the requirements of an outside client or customer
(although some students write policy papers and
others get involved in research with faculty mem-
1
bers). Often the execution of these projects makes
use of user-centered design, including a needs as-
sessment, and iterative design and testing of the arti-
fact. These projects often also have a backend de-
sign component using database design principles,
document engineering modeling, or information ar-
chitecture and organization principles, with sensitiv-
ity to legal considerations for privacy and intellec-
tual property. Students are required to present their
work to an audience of students, faculty, and pro-
fessionals, produce a written report, and produce a
website that describes and demonstrates their work.
In many cases these projects would benefit greatly
from content analysis. Past projects have included
a system to query on and monitor news topics as
they occur across time and sources, a system to ana-
lyze when and where company names are mentioned
in text and graph interconnections among them, a
system to allow customization of news channels by
topic, and systems to search and analyze blogs. Our
past course offerings in this space focused on infor-
mation retrieval with very little emphasis on content
analysis, so students were using only IR-type tech-
niques for these projects.
The state of the art in NLP had advanced suffi-
ciently that the available tools can be employed for a
number of projects like these. Furthermore, it is im-
portant for students attempting such projects to have
an understanding of what is currently feasible and
what is too ambitious. In fact, I find that this is a key
aspect of teaching an applied class: learning what
is possible with existing tools, what is feasible but
requires more expertise than can be engineered in a
semester with existing tools, and what is beyond the
scope of current techniques.
3 Choosing Tools and Readings
The main challenges for a hands-on course as I?d
envisioned surrounded finding usable interoperable
tools, and defining feasible assignments that make
use of programming without letting it interfere with
learning.
There is of course the inevitable decision of which
programming language(s) to work with. Scripting
tools such as python are fast and easy to prototype
with, but require the students to learn a new pro-
gramming language. Java is attractive because many
tools are written in it and the MIMS students were
familiar with java ? they are required to use it for
two of their required courses but still tend to strug-
gle with it. I did not consider perl since python is a
more principled language and is growing in accep-
tance and in tool availability.
In the end I decided to require the students to learn
python because I wanted to use NLTK, the Natural
Language Toolkit (Loper and Bird, 2002). One goal
of NLTK is to remove the emphasis on programming
to enable students to achieve results quickly; and
this aligned with my primary goal. NLTK seemed
promising because it contained some well-written
tutorials on n-grams, POS tagging and chunking,
and contained text categorization modules. (I also
wanted support for entity extraction, which NLTK
does not supply.) NLTK is written in python, and
so I decided to try it and have the students learn a
new programming language. As will be described in
detail below, our use of NLTK was somewhat suc-
cessful, but we experienced numerous problems as
well.
I made a rather large mistake early on by not
spending time introducing python, since I wanted
the assignments to correspond to the lectures and did
not want to spend lecture time on the programming
language itself. I instructed students who had regis-
tered for the course to learn python during the sum-
mer, but (not surprisingly) many of did not and had
to struggle in the first few weeks. In retrospect, I re-
alize I should have allowed time for people to learn
python, perhaps via a lab session that met only dur-
ing the first few weeks of class.
Another sticking point was student exposure to
regular expressions. Regex?s were very important
and useful practical tools both for tokenization as-
signments and for shallow parsing. I assumed that
the MIMS students had gotten practice with regu-
lar expressions because they are required to take a
computer concepts foundations course which I de-
signed several years ago. Unfortunately, the lecturer
who took over the class from me had decided to
omit regex?s and related topics. I realized that I had
to do some remedial coverage of the topic, which
of course bored the CS students and which was not
complete enough for the MIMS students. Again this
suggests that perhaps some kind of lab is needed for
getting people caught up in topics, or that perhaps
2
the first few weeks of the class should be optional
for more advanced students.
I was also unable to find an appropriate textbook.
Neither Schu?tze & Manning nor Jurafsky & Mar-
tin focus on the right topics. The closest in terms
of topic is Natural Language Processing for Online
Applications by Peter Jackson & Isabelle Moulinier,
but much of this book focuses on Information Re-
trieval (which we teach in two other courses) and did
not go into depth on the topics I most cared about.
Instead of a text, students read a small selection of
research papers and the NLTK tutorials.
4 Topics
The course met twice weekly for 80 minute periods.
The topic coverage is shown below; topics followed
by (2) indicate two lecture periods were needed.
Course Introduction
Using Large Collections (intro to NLTK)
Tokenization, Morphological Analysis
Part-of-Speech Tagging
Conditional Probabilities
Shallow Parsing (2)
Text Classification: Introduction
Text Classification: Feature Selection
Text Classification: Algorithms
Text Classification: Using Weka
Information Extraction (2)
Email and Anti-Spam Analysis
Text Data Mining
Lexicons and Ontologies
FrameNet (guest lecture by Chuck Fillmore)
Enron email dataset (in-class work) (2)
Spelling Correction / Clustering
Summarization (guest lecture by Drago Radev)
Question Answering (2)
Machine Translation (slides by Kevin Knight)
Topic Segmentation / Discourse Processing
Class Presentations
Note the lack of coverage of full syntactic parsing,
which is covered extensively in Dan Klein?s course.
I touched on it briefly in the second shallow pars-
ing lecture and felt this level of coverage was ac-
ceptable because shallow parsing is often as useful
if not more so than full parsing for most applica-
tions. Note also the lack of coverage of word sense
disambiguation. This topic is rich in algorithms, but
was omitted primarily due to time constraints, but in
part because of the lack of well-known applications.
Based on the kinds of capstone projects the MIMS
students have done in the past, I knew that the most
important techniques for their needs surrounded
text categorization and information extraction/entity
recognition. There are terrific software resources for
text categorization and the field is fairly mature, so
I had my PhD students Preslav Nakov and Barbara
Rosario gave the lectures on this topic, in order to
provide them with teaching experience.
The functionality provided by named entity
recognition is very important for a wide range of
real-world applications. Unfortunately, none of the
free tools that we tried were particularly successful.
Those that are available are difficult to configure and
get running in a short amount of time, and have vir-
tually no documentation. Furthermore, the state-of-
the-art in algorithms is not present in the available
tools in the way that more mature technologies such
as POS tagging, parsing, and categorization are.
5 Using NLTK
5.1 Benefits
We used the latest version of NLTK, which at the
time was version 1.4.2 NLTK supplies some pre-
processed text collections, which are quite useful.
(Unfortunately, the different corpora have different
types of preprocessing applied to them, which of-
ten lead to confusion and extra work for the class.)
The NLTK tokenizer, POS taggers and the shallow
parser (chunker) have terrific functionality once they
are understood; some students were able to get quite
accurate results using these and the supplied train-
ing sets. The ability to combine different n-gram
taggers within the structure of a backoff tagger also
supported an excellent exercise. However, a some-
what minor problem with the taggers is that there is
no compact way to store the model resulting from
tagging for later use. A serialized object could be
created and stored, but the size of such object was
so large that it takes about as long to load it into
memory as it does to retrain the tagger.
2http://nltk.sourceforge.org
3
5.2 Drawbacks
There were four major problems with NLTK from
the perspective of this course. The first major prob-
lem was the inconsistency in the different releases
of code, both in terms of incompatibilities between
the data structures in the different versions, and
incompatibility of the documentation and tutorials
within the different versions. It was tricky to de-
termine which documentation was associated with
which code version. And much of the contributed
code did not work with the current version.
The second major problem was related to the first,
but threw a major wrench into our plans: some of the
advertised functionality simply was not available in
the current version of the software. Notably, NLTK
advertised a text categorization module; without this
I would not have adopted NLTK as the coding plat-
form for the class. Unfortunately, the most current
version did not in fact support categorization, and
we discovered this just days before we were to be-
gin covering this topic.
The third major problem was the incompleteness
of the documentation for much of the code. This
to some degree undermined the goal of reducing the
amount of work for students, since they (and I) had
to struggle to figure out what was going on in the
code and data structures.
One of these documentation problems centered
around the data structure for conditional probabil-
ities. NLTK creates a FreqDist class which is ex-
plained well in the documentation (it records a count
for each occurrence of some phenomenon, much
like a hash table) and provides methods for retriev-
ing the max, the count and frequency of each oc-
currence, and so on. It also provides a class called
a CondFreqDist, but does not document its meth-
ods nor explain its implementation. Users have to
scrutinize the examples given and try to reverse en-
gineer the data structure. Eventually I realized that
it is simply a list of objects of type FreqDist, but
this was difficult to determine at first, and caused
much wasting of time and confusion among the stu-
dents. There is also confusion surrounding the use
of the method names count and frequency for Fre-
qDist. Count refers to number of occurrences and
frequency to a probability distribution across items,
but this distinction is never stated explicitly although
it can be inferred from a table of methods in the tu-
torial.
A less dramatic but still hampering problem was
with the design of the core data structures, which
make use of attribute tags rather than classes. This
leads to rather awkward code structures. For exam-
ple, after a sentence is tokenized, the results of tok-
enization are appended to the sentence data structure
and are accessed via use of a subtoken keyword such
as ?TOKENS?. To then run a POS tagger over the
tokenized results, the ?TOKENS? keyword has to be
specified as the value for a SUBTOKENS attribute,
and another keyword must be supplied to act as the
name of the tagged results. In my opinion it would
be better to use the class system and define objects
of different types and operations on those objects.
6 Assignments
One of the major goals of the class was for the stu-
dents to obtain hands-on experience using and ex-
tending existing NLP tools. This was accomplished
through a series of homework assignments and a fi-
nal project. My pedagogical philosophy surround-
ing assignments is to supply as much as the function-
ality as necessary so that the coding that students do
leads directly to learning. Thus, I try to avoid mak-
ing students deal with details of formatting files and
so on. I also try to give students a starting point to
build up on.
The first assignment made use of some exercises
from the NLTK tutorials. Students completed to-
kenizing exercises which required the use of the
NLTK corpus tool accessors and the FreqDist and
CondFreqDist classes. They also did POS tagging
exercises which exposed them to the idea of n-
grams, backoff algorithms, and to the process of
training and testing. This assignment was challeng-
ing (especially because of some misleading text in
the tagging tutorial, which has since been fixed) but
the students learned a great deal. As mentioned
above, I should have begun with a preliminary as-
signment which got students familiar with python
basics before attempting this assignment.
For assignment 2, I provided a simple set of regu-
lar expression grammar rules for the shallow parser
class, and asked the students to improve on these.
After building the chunker, students were asked to
4
choose a verb and then analyze verb-argument struc-
ture (they were provided with two relevant papers
(Church and Hanks, 1990; Chklovski and Pantel,
2004)). As mentioned above, most of the MIMS stu-
dents were not familiar with regular expressions, so
I should have done a longer unit on this topic, at the
expense of boring the CS students.
The students learned a great deal from working to
improve the grammar rules, but the verb-argument
analysis portion was not particularly successful, in
part because the corpus analyzed was too small to
yield many sentences for a given verb and because
we did not have code to automatically find regu-
larities about the semantics of the arguments of the
verbs. Other causes of difficulty were the students?
lack of linguistic background, and the fact that the
chunking part took longer than I expected, leaving
students little time for the analysis portion of the as-
signment.
Assignments 3 and 4 are described in the follow-
ing subsections.
6.1 Text Categorization Assignment
As mentioned above, text categorization is useful for
a wide range SIMS applications, and we made it a
centerpiece of the course. Unfortunately, we had to
make a mid-course correction when I suddenly real-
ized that text categorization was no longer available
in NLTK.
After looking at a number of tools, we decided
to use the Weka toolkit for categorization (Witten
and Frank, 2000). We did not want the students to
feel they had wasted their time learning python and
NLTK, so we decided to make it easy for the stu-
dents to reuse their python code by providing an in-
terface between it and Weka.
My PhD student Preslav Nakov provided great
help by writing code to translate the output of our
python code into the input format expected by Weka.
(Weka is written in java but has command line and
GUI interfaces, and can read in input files and store
models as output files.) As time went on we added
increasingly more functionality to this code, tying it
in with the NLTK modules so that the students could
use the NLTK corpora for training and testing.3
3Available at http://www.sims.berkeley.edu/courses/is290-
2/f04/assignments/assignment3.html
Both Preslav and I had used Weka in the past but
mainly with the command-line interface, and not
taking advantage of its rich functionality. As with
NLTK, the documentation for Weka was incomplete
and out of date, and it was difficult to determine how
to use the more advanced features. We performed
extended experimentation with the system and de-
veloped a detailed tutorial on how to use the system;
this tutorial should be of general use.4
For the categorization task, we used the ?twenty
newsgroups? collection that was supplied with
NLTK. Unfortunately, it was not preprocessed into
sentences, so I also had to write some sentence split-
ting code (based on Palmer and Hearst (1997)) so
students could make use of their tokenizer and tag-
ger code.
We selected one pair of newsgroups which con-
tained very different content (rec.motorcycles
vs. sci.space). We called this the diverse
set. We then created two groups of news-
groups with more homogeneous content (a)
rec.autos, rec.motorcycles, rec.sport.baseball,
rec.sport.hockey, and (b) sci.crypt, sci.electronics,
sci.med.original, sci.space. The intention was to
show the students that it is easier to automatically
distinguish the heterogeneous groups than the
homogeneous ones.
We set up the code to allow students to adjust the
size of their training and development sets, and to
separate out a reserved test set that would be used
for comparing students? solutions.
We challenged the students to get the best scores
possible on the held out test set, telling them not to
use this test set until they were completely finished
training and testing on the development set. (We re-
lied on the honor system for this.) We made it known
that we would announce which were the top-scoring
assignments. As a general rule I avoid competition
in my classes, but this was kept very low-key; only
the top-scoring results would be named. Further-
more, innovative approaches that perhaps did not do
as well as some others were also highlighted. Stu-
dents were required to try at least 2 different types
of features and 3 different classifiers.
This assignment was quite successful, as the stu-
4Available at http://www.sims.berkeley.edu/courses/is290-
2/f04/lectures/lecture11.ppt
5
dents were creative about building their features,
and it was possible to achieve very strong results
(much stronger than I expected) on both sets of
newsgroups. The best scoring approaches got 99%
accuracy on the 2-way diverse distinction and 97%
accuracy on the 4-way homogeneous distinction.
6.2 Enron Email Assignment
Many of the SIMS students are interested in social
networking and related topics. I decided as part of
the class that we would analyze a relatively new text
collection that had become available and that con-
tained the potential for interesting text mining and
analysis. I was also interested in having the class
help produce a resource that would be of use to other
classes and researchers. Thus we decided to take on
the Enron email corpus,5 on which limited analysis
had been done.
My PhD student Andrew Fiore wrote code to pre-
process this text, removing redundancies, normal-
izing email addresses, labeling quoted text, and so
on. He and I designed a database schema for repre-
senting much of the structure of the collection and
loaded in the parsed text. I created a Lucene6 in-
dex for doing free text queries while Andrew built a
highly functional web interface for searching fielded
components. Andrew?s system eventually allowed
for individual students to login and register annota-
tions on the email messages.
This collection consists of approximately 200,000
messages after the duplicates have been removed.
We wanted to identify a subset of emails that might
be interesting for analysis while at the same time
avoiding highly personal messages, messages con-
sisting mainly of jokes, and so on. After doing nu-
merous searches, we decided to try to focus primar-
ily on documents relating to the California energy
crisis, trading discrepancies, and messages occur-
ring near the end of the time range (just before the
company?s stock crashed).
After selecting about 1500 messages, I devised an
initial set of categories. In class we refined these.
One student had the interesting idea of trying to
identify change in emotional tone as the scandals
surrounding the company came to light, so we added
emotional tone as a category type. Each message
5http://www-2.cs.cmu.edu/ enron/
6http://lucene.apache.org
was then read and annotated by two students using
the pre-defined categories. Students were asked to
reconcile their differences when they had them.
Despite these safeguards, my impression is that
the resulting assignments are far from consistent and
the categories themselves are still rather ad hoc and
oftentimes overlapping. There were many difficult
curation issues, such as how to categorize a message
with forwarded content when that content differed
in kind from the new material. If we?d spent more
time on this we could have done a better job, but as
this was not an information organization course, I
felt we could not spend more time on perfecting the
labels. Thus, I do not recommend the category la-
bels be used for serious analysis. Nevertheless, a
number of researchers have asked for the cleaned
up database and categories, and we have made them
publicly available, along with the search interface.7
The students were then given two weeks to pro-
cess the collection in some manner. I made sev-
eral suggestions, including trying to automatically
assign the hand-assigned categories, extending some
automatic acronym recognition work that we?d done
in our research (Schwartz and Hearst, 2003), using
named entity recognition code to identify various ac-
tors, clustering the collection, or doing some kind of
social network analysis. Students were told that they
could extend this assignment into their final projects
if they chose.
For most students it was difficult to obtain a strong
result using this collection. The significant excep-
tion was for those students who worked on ex-
tending our acronym recognition algorithm; these
projects were quite successful. (In fact, one student
managed to improve on our results with a rather sim-
ple modification to our code.) Students often had
creative ideas that were stymied by the poor quality
of the available tools. Two groups used the MAL-
LET named entity recognizer toolkit8 in order to do
various kinds of social network analysis, but the re-
sults were poor. (Students managed to make up for
this deficiency in creative ways.)
I was a bit worried about students trying to use
clustering to analyze the results, given the general
difficulty of making sense of the results of cluster-
7http://bailando.sims.berkeley.edu/enron email.html
8http://mallet.cs.umass.edu
6
ing, and this concern was justified. Clustering based
on Weka and other tools is of course memory- and
compute-intensive, but more problematically, the re-
sults are difficult to interpret. I would recommend
against allowing students to do a text clustering exer-
cise unless within a more constrained environment.
In summary, students were excited about build-
ing a resource based on relatively untapped and very
interesting data. The resulting analysis on this un-
tamed text was somewhat disappointing, but given
that only two weeks were spent on this part of the
assignment, I believe it was a good learning experi-
ence. Furthermore, the resulting resource seems to
be of interest to a number of researchers, as was our
intention.
6.3 Final Projects
I deliberately kept the time for the final projects
short (about 3 weeks) so students would not go over-
board or feel pressure to do something hugely time-
consuming. The goal was to allow students to tie
together some of the different ideas and skills they?d
acquired in the class (and elsewhere), and to learn
them in more depth by applying them to a topic of
personal interest.
Students were encouraged to work in pairs, and
I suggested a list of project ideas. Students who
adopted suggested projects tended to be more suc-
cessful than those who developed their own. Those
who tried other topics were often too ambitious and
had trouble getting meaningful results. However,
several of those students were trying ideas that they
planned to apply to their capstone projects, and so
it was highly valuable for them to get a preview of
what worked and what did not.
One suggestion I made was to create a back-of-
the-book indexer, specifically for a recipe book, and
one team did a good job with this project. Another
was to improve on or apply an automatic hierarchy
generation tool that we have developed in our re-
search (Stoica and Hearst, 2004). Students working
on a project to collect metadata for camera phone
images successfully applied this tool to this prob-
lem. Again, social networking analysis topics were
popular but not particularly successful; NLP tools
are not advanced enough yet to meet the needs of
this intriguing topic area. Not surprisingly, when
students started with a new (interesting) text collec-
tion, they were bogged down in the preprocessing
stage before they could get much interesting work
done.
6.4 Reflecting on Assignments
Although students were excited about the Enron col-
lection and we created a resource that is actively be-
ing used by other researchers, I think in future ver-
sions of the class I will omit this kind of assignment
and have the students start their final projects sooner.
This will allow them time to do any preprocessing
necessary to get the text into shape for doing the
interesting work. I will also exercise more control
over what they are allowed to attempt (which is not
my usual style) in order to ensure more successful
outcomes.
I am not sure if I will use NLTK again or not. If
the designers make significant improvements on the
code and documentation, then I probably will. The
style and intent of the tutorials are quite appropriate
for the goals of the class. Students with stronger
coding background tended to use java for their final
projects, whereas the others tended to build on the
python code we developed in the class assignments,
which suggests that this kind of toolkit approach is
useful for them.
7 Conclusions
Overall, I feel the main goals of the course were met.
Although I am emphasizing how the course could be
improved, most students were quite positive about
the class, giving it an overall score of 5.8 out of 7
with a mode of 6 in their anonymous course reviews.
(This is on the low side for my courses; most who
gave it low scores found the programming too diffi-
cult.)
Most students found the material highly stimulat-
ing and the work challenging but not overwhelming.
Several students mentioned that a lab session with
a dedicated TA would have been desirable. Sev-
eral suggested covering less material in more depth
and several commented that the Enron exercise was
a neat idea although not entirely successful in execu-
tion. Students remarked on liking reading research
papers rather than a textbook (they also liked the rel-
atively light reading load, which I feel was appropri-
ate given the heavy assignment load). Some students
7
wanted more emphasis on real-world applications; I
think it would be useful to have guest speakers from
industry talk about this if possible.
I would like to see more research tools devel-
oped to a point to which they can be applied more
successfully, especially in the area of information
extraction. I would also recommend to colleagues
that careful control be retained over assignments and
projects to ensure feasibility in the outcome. It is
more difficult to get good results on class projects in
NLP than in other areas I?ve taught. As we so often
see in text analysis work, it can often be difficult to
do better than simple word counts for many projects.
I am interested in hearing ideas about how to ac-
commodate both the somewhat technical and the
highly technical students, especially in the early
parts of the course. Perhaps the best solution is to
offer an optional laboratory section, at least for the
first few weeks, but perhaps for the entire term, but
this solution obviously requires more resources.
When designing this course I did a fairly extensive
web search looking for courses that offered what I
was interested in, but didn?t find much. I used the
proceedings of the ACL-02 workshop on teaching
NLP (where I learned about NLTK) as well as the
NLP Universe. I think it would be a good idea to
start an archive of teaching resources; ACM SIGCHI
is in the midst of creating such an educational digital
library and this example is worth studying.9
Acknowledgements
Thanks to Preslav Nakov, Andrew Fiore, and Bar-
bara Rosario for their help with the class, and for
all the students who took the class. Thanks also to
Steven Bird and Edward Loper for developing and
sharing NLTK, and for their generous time and help
with the system during the course of the class. This
work was supported in part by NSF DBI-0317510.
References
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP, Barcelona.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
9http://hcc.cc.gatech.edu/
phy. American Journal of Computational Linguistics,
16(1):22?29.
Edward Loper and Steven Bird. 2002. Nltk: The natural
language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics, Philadelphia.
David Palmer and Marti A. Hearst. 1997. Adaptive mul-
tilingual sentence boundary disambiguation. Compu-
tational Lingiustics, 23(2).
Ariel Schwartz and Marti Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In Proceedings of the Pacific Sym-
posium on Biocomputing (PSB 2003), Kauai, Hawaii.
Emilia Stoica and Marti Hearst. 2004. Nearly-automated
metadata hierarchy creation. In Proceedings of HLT-
NAACL Companion Volume, Boston.
Ian H. Witten and Eibe Frank. 2000. Data Mining:
Practical machine learning tools with Java implemen-
tations. Morgan Kaufmann, San Francisco.
8
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 17?24, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Search Engine Statistics Beyond the n-gram:
Application to Noun Compound Bracketing
Preslav Nakov
EECS, Computer Science Division
University of California, Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Marti Hearst
SIMS
University of California, Berkeley
Berkeley, CA 94720
hearst@sims.berkeley.edu
Abstract
In order to achieve the long-range goal
of semantic interpretation of noun com-
pounds, it is often necessary to first de-
termine their syntactic structure. This pa-
per describes an unsupervised method for
noun compound bracketing which extracts
statistics from Web search engines using a
?2 measure, a new set of surface features,
and paraphrases. On a gold standard, the
system achieves results of 89.34% (base-
line 66.80%), which is a sizable improve-
ment over the state of the art (80.70%).
1 Introduction
An important but understudied language analy-
sis problem is that of noun compound bracketing,
which is generally viewed as a necessary step to-
wards noun compound interpretation. Consider the
following contrastive pair of noun compounds:
(1) liver cell antibody
(2) liver cell line
In example (1) an antibody targets a liver cell, while
(2) refers to a cell line which is derived from the
liver. In order to make these semantic distinctions
accurately, it can be useful to begin with the cor-
rect grouping of terms, since choosing a particular
syntactic structure limits the options left for seman-
tics. Although equivalent at the part of speech (POS)
level, these two noun compounds have different syn-
tactic trees. The distinction can be represented as a
binary tree or, equivalently, as a binary bracketing:
(1b) [ [ liver cell ] antibody ] (left bracketing)
(2b) [ liver [cell line] ] (right bracketing)
In this paper, we describe a highly accurate un-
supervised method for making bracketing decisions
for noun compounds (NCs). We improve on the cur-
rent standard approach of using bigram estimates to
compute adjacency and dependency scores by intro-
ducing the use of the ?2 measure for this problem.
We also introduce a new set of surface features for
querying Web search engines which prove highly ef-
fective. Finally, we experiment with paraphrases for
improving prediction statistics. We have evaluated
the application of combinations of these features to
predict NC bracketing on two distinct collections,
one consisting of terms drawn from encyclopedia
text, and another drawn from bioscience text.
The remainder of this paper describes related
work, the word association models, the surface fea-
tures, the paraphrase features and the results.
2 Related Work
The syntax and semantics of NCs is an active area of
research; the Journal of Computer Speech and Lan-
guage has an upcoming special issue on Multiword
Expressions.
The best known early work on automated un-
supervised NC bracketing is that of Lauer (1995)
who introduces the probabilistic dependency model
for the syntactic disambiguation of NCs and argues
against the adjacency model, proposed by Marcus
(1980), Pustejovsky et al (1993) and Resnik (1993).
Lauer collects n-gram statistics from Grolier?s en-
cyclopedia, containing about 8 million words. To
17
overcome data sparsity problems, he estimates prob-
abilities over conceptual categories in a taxonomy
(Roget?s thesaurus) rather than for individual words.
Lauer evaluated his models on a set of 244 unam-
biguous NCs derived from the same encyclopedia
(inter-annotator agreement 81.50%) and achieved
77.50% for the dependency model above (baseline
66.80%). Adding POS and further tuning allowed
him to achieve the state-of-the-art result of 80.70%.
More recently, Keller and Lapata (2003) evalu-
ate the utility of using Web search engines for ob-
taining frequencies for unseen bigrams. They then
later propose using Web counts as a baseline unsu-
pervised method for many NLP tasks (Lapata and
Keller, 2004). They apply this idea to six NLP tasks,
including the syntactic and semantic disambigua-
tion of NCs following Lauer (1995), and show that
variations on bigram counts perform nearly as well
as more elaborate methods. They do not use tax-
onomies and work with the word n-grams directly,
achieving 78.68% with a much simpler version of
the dependency model.
Girju et al (2005) propose a supervised model
(decision tree) for NC bracketing in context, based
on five semantic features (requiring the correct
WordNet sense to be given): the top three Word-
Net semantic classes for each noun, derivationally
related forms and whether the noun is a nominaliza-
tion. The algorithm achieves accuracy of 83.10%.
3 Models and Features
3.1 Adjacency and Dependency Models
In related work, a distinction is often made between
what is called the dependency model and the adja-
cency model. The main idea is as follows. For a
given 3-word NC w1w2w3, there are two reasons it
may take on right bracketing, [w1[w2w3]]. Either (a)
w2w3 is a compound (modified byw1), or (b)w1 and
w2 independently modify w3. This distinction can
be seen in the examples home health care (health
care is a compound modified by home) versus adult
male rat (adult and male independently modify rat).
The adjacency model checks (a), whether w2w3
is a compound (i.e., how strongly w2 modifies w3
as opposed to w1w2 being a compound) to decide
whether or not to predict a right bracketing. The
dependency model checks (b), does w1 modify w3
(as opposed to w1 modifying w2).
Left bracketing is a bit different since there is only
modificational choice for a 3-word NC. If w1 modi-
fies w2, this implies that w1w2 is a compound which
in turn modifies w3, as in law enforcement agent.
Thus the usefulness of the adjacency model vs.
the dependency model can depend in part on the mix
of left and right bracketing. Below we show that the
dependency model works better than the adjaceny
model, confirming other results in the literature. The
next subsections describe several different ways to
compute these measures.
3.2 Using Frequencies
The most straightforward way to compute adjacency
and dependency scores is to simply count the cor-
responding frequencies. Lapata and Keller (2004)
achieved their best accuracy (78.68%) with the de-
pendency model and the simple symmetric score
#(wi, wj).1
3.3 Computing Probabilities
Lauer (1995) assumes that adjacency and depen-
dency should be computed via probabilities. Since
they are relatively simple to compute, we investigate
them in our experiments.
Consider the dependency model, as introduced
above, and the NC w1w2w3. Let Pr(wi ? wj |wj)
be the probability that the word wi precedes a
given fixed word wj . Assuming that the distinct
head-modifier relations are independent, we obtain
Pr(right) = Pr(w1 ? w3|w3)Pr(w2 ? w3|w3)
and Pr(left) = Pr(w1 ? w2|w2)Pr(w2 ? w3|w3).
To choose the more likely structure, we can drop
the shared factor and compare Pr(w1 ? w3|w3) to
Pr(w1 ? w2|w2).
The alternative adjacency model compares
Pr(w2 ? w3|w3) to Pr(w1 ? w2|w2), i.e. the
association strength between the last two words vs.
that between the first two. If the first probability is
larger than the second, the model predicts right.
The probability Pr(w1 ? w2|w2) can be esti-
mated as #(w1, w2)/#(w2), where #(w1, w2) and
#(w2) are the corresponding bigram and unigram
1This score worked best on training, when Keller&Lapata
were doing model selection. On testing, Pr (with the depen-
dency model) worked better and achieved accuracy of 80.32%,
but this result was ignored, as Pr did worse on training.
18
frequencies. They can be approximated as the num-
ber of pages returned by a search engine in response
to queries for the exact phrase ?w1 w2? and for the
word w2. In our experiments below we smoothed2
each of these frequencies by adding 0.5 to avoid
problems caused by nonexistent n-grams.
Unless some particular probabilistic interpreta-
tion is needed,3 there is no reason why for a given
ordered pair of words (wi, wj), we should use
Pr(wi ? wj |wj) rather than Pr(wj ? wi|wi),
i < j. This is confirmed by the adjacency model
experiments in (Lapata and Keller, 2004) on Lauer?s
NC set. Their results show that both ways of
computing the probabilities make sense: using Al-
tavista queries, the former achieves a higher accu-
racy (70.49% vs. 68.85%), but the latter is better on
the British National Corpus (65.57% vs. 63.11%).
3.4 Other Measures of Association
In both models, the probability Pr(wi ? wj |wj)
can be replaced by some (possibly symmetric) mea-
sure of association between wi and wj , such as Chi
squared (?2). To calculate ?2(wi, wj), we need:
(A) #(wi, wj);
(B) #(wi, wj), the number of bigrams in which the
first word is wi, followed by a word other than
wj ;
(C) #(wi, wj), the number of bigrams, ending in
wj , whose first word is other than wi;
(D) #(wi, wj), the number of bigrams in which the
first word is not wi and the second is not wj .
They are combined in the following formula:
?2 =
N(AD ?BC)2
(A+ C)(B +D)(A+B)(C +D)
(1)
Here N = A + B + C + D is the total num-
ber of bigrams, B = #(wi) ? #(wi, wj) and C =
#(wj) ?#(wi, wj). While it is hard to estimate D
2Zero counts sometimes happen for #(w1, w3), but are rare
for unigrams and bigrams on the Web, and there is no need for
a more sophisticated smoothing.
3For example, as used by Lauer to introduce a prior for left-
right bracketing preference. The best Lauer model does not
work with words directly, but uses a taxonomy and further needs
a probabilistic interpretation so that the hidden taxonomy vari-
ables can be summed out. Because of that summation, the term
Pr(w2 ? w3|w3) does not cancel in his dependency model.
directly, we can calculate it asD = N?A?B?C.
Finally, we estimate N as the total number of in-
dexed bigrams on the Web. They are estimated as 8
trillion, since Google indexes about 8 billion pages
and each contains about 1,000 words on average.
Other measures of word association are possible,
such as mutual information (MI), which we can use
with the dependency and the adjacency models, sim-
ilarly to #, ?2 or Pr. However, in our experiments,
?2 worked better than other methods; this is not sur-
prising, as ?2 is known to outperform MI as a mea-
sure of association (Yang and Pedersen, 1997).
3.5 Web-Derived Surface Features
Authors sometimes (consciously or not) disam-
biguate the words they write by using surface-level
markers to suggest the correct meaning. We have
found that exploiting these markers, when they oc-
cur, can prove to be very helpful for making brack-
eting predictions. The enormous size of Web search
engine indexes facilitates finding such markers fre-
quently enough to make them useful.
One very productive feature is the dash (hyphen).
Starting with the term cell cycle analysis, if we can
find a version of it in which a dash occurs between
the first two words: cell-cycle, this suggests a left
bracketing for the full NC. Similarly, the dash in
donor T-cell favors a right bracketing. The right-
hand dashes are less reliable though, as their scope
is ambiguous. In fiber optics-system, the hyphen in-
dicates that the noun compound fiber optics modifies
system. There are also cases with multiple hyphens,
as in t-cell-depletion, which preclude their use.
The genitive ending, or possessive marker is an-
other useful indicator. The phrase brain?s stem
cells suggests a right bracketing for brain stem cells,
while brain stem?s cells favors a left bracketing.4
Another highly reliable source is related to inter-
nal capitalization. For example Plasmodium vivax
Malaria suggests left bracketing, while brain Stem
cells would favor a right one. (We disable this fea-
ture on Roman digits and single-letter words to pre-
vent problems with terms like vitamin D deficiency,
where the capitalization is just a convention as op-
posed to a special mark to make the reader think that
the last two terms should go together.)
4Features can also occur combined, e.g. brain?s stem-cells.
19
We can also make use of embedded slashes. For
example in leukemia/lymphoma cell, the slash pre-
dicts a right bracketing since the first word is an al-
ternative and cannot be a modifier of the second one.
In some cases we can find instances of the NC
in which one or more words are enclosed in paren-
theses, e.g., growth factor (beta) or (growth fac-
tor) beta, both of which indicate a left structure, or
(brain) stem cells, which suggests a right bracketing.
Even a comma, a dot or a colon (or any spe-
cial character) can act as indicators. For example,
?health care, provider? or ?lung cancer: patients?
are weak predictors of a left bracketing, showing
that the author chose to keep two of the words to-
gether, separating out the third one.
We can also exploit dashes to words external to
the target NC, as in mouse-brain stem cells, which
is a weak indicator of right bracketing.
Unfortunately, Web search engines ignore punc-
tuation characters, thus preventing querying directly
for terms containing hyphens, brackets, apostrophes,
etc. We collect them indirectly by issuing queries
with the NC as an exact phrase and then post-
processing the resulting summaries, looking for the
surface features of interest. Search engines typically
allow the user to explore up to 1000 results. We col-
lect all results and summary texts that are available
for the target NC and then search for the surface pat-
terns using regular expressions over the text. Each
match increases the score for left or right bracket-
ing, depending on which the pattern favors.
While some of the above features are clearly
more reliable than others, we do not try to weight
them. For a given NC, we post-process the returned
Web summaries, then we find the number of left-
predicting surface feature instances (regardless of
their type) and compare it to the number of right-
predicting ones to make a bracketing decision.5
3.6 Other Web-Derived Features
Some features can be obtained by using the over-
all counts returned by the search engine. As these
counts are derived from the entire Web, as opposed
to a set of up to 1,000 summaries, they are of differ-
ent magnitude, and we did not want to simply add
them to the surface features above. They appear as
5This appears as Surface features (sum) in Tables 1 and 2.
independent models in Tables 1 and 2.
First, in some cases, we can query for possessive
markers directly: although search engines drop the
apostrophe, they keep the s, so we can query for
?brain?s? (but not for ?brains? ?). We then com-
pare the number of times the possessive marker ap-
peared on the second vs. the first word, to make a
bracketing decision.
Abbreviations are another important feature. For
example, ?tumor necrosis factor (NF)? suggests a
right bracketing, while ?tumor necrosis (TN) fac-
tor? would favor left. We would like to issue exact
phrase queries for the two patterns and see which
one is more frequent. Unfortunately, the search en-
gines drop the brackets and ignore the capitalization,
so we issue queries with the parentheses removed, as
in ?tumor necrosis factor nf?. This produces highly
accurate results, although errors occur when the ab-
breviation is an existing word (e.g., me), a Roman
digit (e.g., IV), a state (e.g., CA), etc.
Another reliable feature is concatenation. Con-
sider the NC health care reform, which is left-
bracketed. Now, consider the bigram ?health care?.
At the time of writing, Google estimates 80,900,000
pages for it as an exact term. Now, if we try the
word healthcare we get 80,500,000 hits. At the
same time, carereform returns just 109. This sug-
gests that authors sometimes concatenate words that
act as compounds. We find below that comparing
the frequency of the concatenation of the left bigram
to that of the right (adjacency model for concatena-
tions) often yields accurate results. We also tried the
dependency model for concatenations, as well as the
concatenations of two words in the context of the
third one (i.e., compare frequencies of ?healthcare
reform? and ?health carereform?).
We also used Google?s support for ?*?, which al-
lows a single word wildcard, to see how often two of
the words are present but separated from the third by
some other word(s). This implicitly tries to capture
paraphrases involving the two sub-concepts making
up the whole. For example, we compared the fre-
quency of ?health care * reform? to that of ?health
* care reform?. We also used 2 and 3 stars and
switched the word group order (indicated with rev.
in Tables 1 and 2), e.g., ?care reform * * health?.
We also tried a simple reorder without inserting
stars, i.e., compare the frequency of ?reform health
20
care? to that of ?care reform health?. For exam-
ple, when analyzing myosin heavy chain we see that
heavy chain myosin is very frequent, which provides
evidence against grouping heavy and chain together
as they can commute.
Further, we tried to look inside the internal inflec-
tion variability. The idea is that if ?tyrosine kinase
activation? is left-bracketed, then the first two words
probably make a whole and thus the second word
can be found inflected elsewhere but the first word
cannot, e.g., ?tyrosine kinases activation?. Alterna-
tively, if we find different internal inflections of the
first word, this would favor a right bracketing.
Finally, we tried switching the word order of the
first two words. If they independently modify the
third one (which implies a right bracketing), then we
could expect to see also a form with the first two
words switched, e.g., if we are given ?adult male
rat?, we would also expect ?male adult rat?.
3.7 Paraphrases
Warren (1978) proposes that the semantics of the re-
lations between words in a noun compound are of-
ten made overt by paraphrase. As an example of
prepositional paraphrase, an author describing the
concept of brain stem cells may choose to write it
in a more expanded manner, such as stem cells in
the brain. This contrast can be helpful for syntactic
bracketing, suggesting that the full NC takes on right
bracketing, since stem and cells are kept together in
the expanded version. However, this NC is ambigu-
ous, and can also be paraphrased as cells from the
brain stem, implying a left bracketing.
Some NCs? meaning cannot be readily expressed
with a prepositional paraphrase (Warren, 1978). An
alternative is the copula paraphrase, as in office
building that/which is a skyscraper (right bracket-
ing), or a verbal paraphrase such as pain associated
with arthritis migraine (left).
Other researchers have used prepositional para-
phrases as a proxy for determining the semantic rela-
tions that hold between nouns in a compound (Lauer,
1995; Keller and Lapata, 2003; Girju et al, 2005).
Since most NCs have a prepositional paraphrase,
Lauer builds a model trying to choose between the
most likely candidate prepositions: of, for, in, at,
on, from, with and about (excluding like which is
mentioned by Warren). This could be problematic
though, since as a study by Downing (1977) shows,
when no context is provided, people often come up
with incompatible interpretations.
In contrast, we use paraphrases in order to make
syntactic bracketing assignments. Instead of trying
to manually decide the correct paraphrases, we can
issue queries using paraphrase patterns and find out
how often each occurs in the corpus. We then add
up the number of hits predicting a left versus a right
bracketing and compare the counts.
Unfortunately, search engines lack linguistic an-
notations, making general verbal paraphrases too ex-
pensive. Instead we used a small set of hand-chosen
paraphrases: associated with, caused by, contained
in, derived from, focusing on, found in, involved in,
located at/in, made of, performed by, preventing,
related to and used by/in/for. It is however feasi-
ble to generate queries predicting left/right brack-
eting with/without a determiner for every preposi-
tion.6 For the copula paraphrases we combine two
verb forms is and was, and three complementizers
that, which and who. These are optionally combined
with a preposition or a verb form, e.g. themes that
are used in science fiction.
4 Evaluation
4.1 Lauer?s Dataset
We experimented with the dataset from (Lauer,
1995), in order to produce results comparable to
those of Lauer and Keller & Lapata. The set consists
of 244 unambiguous 3-noun NCs extracted from
Grolier?s encyclopedia; however, only 216 of these
NCs are unique.
Lauer (1995) derived n-gram frequencies from
the Grolier?s corpus and tested the dependency and
the adjacency models using this text. To help combat
data sparseness issues he also incorporated a taxon-
omy and some additional information (see Related
Work section above). Lapata and Keller (2004) de-
rived their statistics from the Web and achieved re-
sults close to Lauer?s using simple lexical models.
4.2 Biomedical Dataset
We constructed a new set of noun compounds from
the biomedical literature. Using the Open NLP
6In addition to the articles (a, an, the), we also used quanti-
fiers (e.g. some, every) and pronouns (e.g. this, his).
21
tools,7 we sentence splitted, tokenized, POS tagged
and shallow parsed a set of 1.4 million MEDLINE
abstracts (citations between 1994 and 2003). Then
we extracted all 3-noun sequences falling in the last
three positions of noun phrases (NPs) found in the
shallow parse. If the NP contained other nouns, the
sequence was discarded. This allows for NCs which
are modified by adjectives, determiners, and so on,
but prevents extracting 3-noun NCs that are part of
longer NCs. For details, see (Nakov et al, 2005).
This procedure resulted in 418,678 different NC
types. We manually investigated the most frequent
ones, removing those that had errors in tokeniza-
tion (e.g., containing words like transplan or tation),
POS tagging (e.g., acute lung injury, where acute
was wrongly tagged as a noun) or shallow parsing
(e.g., situ hybridization, that misses in). We had to
consider the first 843 examples in order to obtain
500 good ones, which suggests an extraction accu-
racy of 59%. This number is low mainly because the
tokenizer handles dash-connected words as a single
token (e.g. factor-alpha) and many tokens contained
other special characters (e.g. cd4+), which cannot
be used in a query against a search engine and had
to be discarded.
The 500 NCs were annotated independently by
two judges, one of which has a biomedical back-
ground; the other one was one of the authors. The
problematic cases were reconsidered by the two
judges and after agreement was reached, the set con-
tained: 361 left bracketed, 69 right bracketed and
70 ambiguous NCs. The latter group was excluded
from the experiments.8
We calculated the inter-annotator agreement on
the 430 cases that were marked as unambiguous
after agreement. Using the original annotator?s
choices, we obtained an agreement of 88% or 82%,
depending on whether we consider the annotations,
that were initially marked as ambiguous by one of
the judges to be correct. The corresponding values
for the kappa statistics were .606 (substantial agree-
ment) and .442 (moderate agreement).
7http://opennlp.sourceforge.net/
8Two NCs can appear more than once but with a different
inflection or with a different word variant, e.g,. colon cancer
cells and colon carcinoma cells.
4.3 Experiments
The n-grams, surface features, and paraphrase
counts were collected by issuing exact phrase
queries, limiting the pages to English and request-
ing filtering of similar results.9 For each NC, we
generated all possible word inflections (e.g., tumor
and tumors) and alternative word variants (e.g., tu-
mor and tumour). For the biomedical dataset they
were automatically obtained from the UMLS Spe-
cialist lexicon.10 For Lauer?s set we used Carroll?s
morphological tools.11 For bigrams, we inflect only
the second word. Similarly, for a prepositional para-
phrase we generate all possible inflected forms for
the two parts, before and after the preposition.
4.4 Results and Discussion
The results are shown in Tables 1 and 2. As NCs
are left-bracketed at least 2/3rds of the time (Lauer,
1995), a straightforward baseline is to always as-
sign a left bracketing. Tables 1 and 2 suggest that
the surface features perform best. The paraphrases
are equally good on the biomedical dataset, but on
Lauer?s set their performance is lower and is compa-
rable to that of the dependency model.
The dependency model clearly outperforms the
adjacency one (as other researchers have found) on
Lauer?s set, but not on the biomedical set, where it
is equally good. ?2 barely outperforms #, but on the
biomedical set ?2 is a clear winner (by about 1.5%)
on both dependency and adjacency models.
The frequencies (#) outperform or at least rival the
probabilities on both sets and for both models. This
is not surprising, given the previous results by Lap-
ata and Keller (2004). Frequencies also outperform
Pr on the biomedical set. This may be due to the
abundance of single-letter words in that set (because
of terms like T cell, B cell, vitamin D etc.; similar
problems are caused by Roman digits like ii, iii etc.),
whose Web frequencies are rather unreliable, as they
are used by Pr but not by frequencies. Single-letter
words cause potential problems for the paraphrases
9In our experiments we used MSN Search statistics for the
n-grams and the paraphrases (unless the pattern contained a
?*?), and Google for the surface features. MSN always re-
turned exact numbers, while Google and Yahoo rounded their
page hits, which generally leads to lower accuracy (Yahoo was
better than Google for these estimates).
10http://www.nlm.nih.gov/pubs/factsheets/umlslex.html
11http://www.cogs.susx.ac.uk/lab/nlp/carroll/morph.html
22
Model
?
? ? P(%) C(%)
# adjacency 183 61 0 75.00 100.00
Pr adjacency 180 64 0 73.77 100.00
MI adjacency 182 62 0 74.59 100.00
?2 adjacency 184 60 0 75.41 100.00
# dependency 193 50 1 79.42 99.59
Pr dependency 194 50 0 79.51 100.00
MI dependency 194 50 0 79.51 100.00
?2 dependency 195 50 0 79.92 100.00
# adjacency (*) 152 41 51 78.76 79.10
# adjacency (**) 162 43 39 79.02 84.02
# adjacency (***) 150 51 43 74.63 82.38
# adjacency (*, rev.) 163 48 33 77.25 86.47
# adjacency (**, rev.) 165 51 28 76.39 88.52
# adjacency (***, rev.) 156 57 31 73.24 87.30
Concatenation adj. 175 48 21 78.48 91.39
Concatenation dep. 167 41 36 80.29 85.25
Concatenation triples 76 3 165 96.20 32.38
Inflection Variability 69 36 139 65.71 43.03
Swap first two words 66 38 140 63.46 42.62
Reorder 112 40 92 73.68 62.30
Abbreviations 21 3 220 87.50 9.84
Possessives 32 4 208 88.89 14.75
Paraphrases 174 38 32 82.08 86.89
Surface features (sum) 183 31 30 85.51 87.70
Majority vote 210 22 12 90.52 95.08
Majority vote? left 218 26 0 89.34 100.00
Baseline (choose left) 163 81 0 66.80 100.00
Table 1: Lauer Set. Shown are the numbers for cor-
rect (?), incorrect (?), and no prediction (?), fol-
lowed by precision (P, calculated over? and? only)
and coverage (C, % examples with prediction). We
use ??? for back-off to another model in case of ?.
as well, by returning too many false positives, but
they work very well with concatenations and dashes:
e.g., T cell is often written as Tcell.
As Table 4 shows, most of the surface features
that we predicted to be right-bracketing actually in-
dicated left. Overall, the surface features were very
good at predicting left bracketing, but unreliable for
right-bracketed examples. This is probably in part
due to the fact that they look for adjacent words, i.e.,
they act as a kind of adjacency model.
We obtained our best overall results by combining
the most reliable models, marked in bold in Tables
1, 2 and 4. As they have independent errors, we used
a majority vote combination.
Table 3 compares our results to those of Lauer
(1995) and of Lapata and Keller (2004). It is impor-
tant to note though, that our results are directly com-
parable to those of Lauer, while the Keller&Lapata?s
are not, since they used half of the Lauer set for de-
Model
?
? ? P(%) C(%)
# adjacency 374 56 0 86.98 100.00
Pr adjacency 353 77 0 82.09 100.00
MI adjacency 372 58 0 86.51 100.00
?2 adjacency 379 51 0 88.14 100.00
# dependency 374 56 0 86.98 100.00
Pr dependency 369 61 0 85.81 100.00
MI dependency 369 61 0 85.81 100.00
?2 dependency 380 50 0 88.37 100.00
# adjacency (*) 373 57 0 86.74 100.00
# adjacency (**) 358 72 0 83.26 100.00
# adjacency (***) 334 88 8 79.15 98.14
# adjacency (*, rev.) 370 59 1 86.25 99.77
# adjacency (**, rev.) 367 62 1 85.55 99.77
# adjacency (***, rev.) 351 79 0 81.63 100.00
Concatenation adj. 370 47 13 88.73 96.98
Concatenation dep. 366 43 21 89.49 95.12
Concatenation triple 238 37 155 86.55 63.95
Inflection Variability 198 49 183 80.16 57.44
Swap first two words 90 18 322 83.33 25.12
Reorder 320 78 32 80.40 92.56
Abbreviations 133 23 274 85.25 36.27
Possessives 48 7 375 87.27 12.79
Paraphrases 383 44 3 89.70 99.30
Surface features (sum) 382 48 0 88.84 100.00
Majority vote 403 17 10 95.95 97.67
Majority vote? right 410 20 0 95.35 100.00
Baseline (choose left) 361 69 0 83.95 100.00
Table 2: Biomedical Set.
velopment and the other half for testing.12 We, fol-
lowing Lauer, used everything for testing. Lapata &
Keller also used the AltaVista search engine, which
no longer exists in its earlier form. The table does
not contain the results of Girju et al (2005), who
achieved 83.10% accuracy, but used a supervised al-
gorithm and targeted bracketing in context. They
further ?shuffled? the Lauer?s set, mixing it with ad-
ditional data, thus making their results even harder
to compare to these in the table.
Note that using page hits as a proxy for n-gram
frequencies can produce some counter-intuitive re-
sults. Consider the bigrams w1w4, w2w4 and w3w4
and a page that contains each bigram exactly once.
A search engine will contribute a page count of 1 for
w4 instead of a frequency of 3; thus the page hits
for w4 can be smaller than the page hits for the sum
of the individual bigrams. See Keller and Lapata
(2003) for more issues.
12In fact, the differences are negligible; their system achieves
pretty much the same result on the half split as well as on the
whole set (personal communication).
23
Model Acc. %
LEFT (baseline) 66.80
Lauer adjacency 68.90
Lauer dependency 77.50
Our ?2 dependency 79.92
Lauer tuned 80.70
?Upper bound? (humans - Lauer) 81.50
Our majority vote? left 89.34
Keller&Lapata: LEFT (baseline) 63.93
Keller&Lapata: best BNC 68.03
Keller&Lapata: best AltaVista 78.68
Table 3: Comparison to previous unsupervised
results on Lauer?s set. The results of Keller & La-
pata are on half of Lauer?s set and thus are only in-
directly comparable (note the different baseline).
5 Conclusions and Future Work
We have extended and improved upon the state-of-
the-art approaches to NC bracketing using an un-
supervised method that is more robust than Lauer
(1995) and more accurate than Lapata and Keller
(2004). Future work will include testing on NCs
consisting of more than 3 nouns, recognizing the
ambiguous cases, and bracketing NPs that include
determiners and modifiers. We plan to test this ap-
proach on other important NLP problems.
As mentioned above, NC bracketing should be
helpful for semantic interpretation. Another possi-
ble application is the refinement of parser output.
Currently, NPs in the Penn TreeBank are flat, with-
out internal structure. Absent any other information,
probabilistic parsers typically assume right bracket-
ing, which is incorrect about 2/3rds of the time for
3-noun NCs. It may be useful to augment the Penn
TreeBank with dependencies inside the currently flat
NPs, which may improve their performance overall.
Acknowledgements We thank Dan Klein, Frank
Keller and Mirella Lapata for valuable comments,
Janice Hamer for the annotations, and Mark Lauer
for his dataset. This research was supported by NSF
DBI-0317510, and a gift from Genentech.
References
Pamela Downing. 1977. On the creation and use of english
compound nouns. Language, (53):810?842.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On the
semantics of noun compounds. Journal of Computer Speech
and Language - Special Issue on Multiword Expressions.
Example Predicts Accuracy Coverage
brain-stem cells left 88.22 92.79
brain stem?s cells left 91.43 16.28
(brain stem) cells left 96.55 6.74
brain stem (cells) left 100.00 1.63
brain stem, cells left 96.13 42.09
brain stem: cells left 97.53 18.84
brain stem cells-death left 80.69 60.23
brain stem cells/tissues left 83.59 45.35
brain stem Cells left 90.32 36.04
brain stem/cells left 100.00 7.21
brain. stem cells left 97.58 38.37
brain stem-cells right 25.35 50.47
brain?s stem cells right 55.88 7.90
(brain) stem cells right 46.67 3.49
brain (stem cells) right 0.00 0.23
brain, stem cells right 54.84 14.42
brain: stem cells right 44.44 6.28
rat-brain stem cells right 17.97 68.60
neural/brain stem cells right 16.36 51.16
brain Stem cells right 24.69 18.84
brain/stem cells right 53.33 3.49
brain stem. cells right 39.34 14.19
Table 4: Surface features analysis (%s), run over
the biomedical set.
Frank Keller and Mirella Lapata. 2003. Using the Web to
obtain frequencies for unseen bigrams. Computational Lin-
guistics, 29:459?484.
Mirella Lapata and Frank Keller. 2004. The Web as a base-
line: Evaluating the performance of unsupervised Web-
based models for a range of NLP tasks. In Proceedings of
HLT-NAACL, pages 121?128, Boston.
Mark Lauer. 1995. Designing Statistical Language Learners:
Experiments on Noun Compounds. Ph.D. thesis, Department
of Computing Macquarie University NSW 2109 Australia.
Mitchell Marcus. 1980. A Theory of Syntactic Recognition for
Natural Language. MIT Press.
Preslav Nakov, Ariel Schwartz, Brian Wolf, and Marti Hearst.
2005. Scaling up BioNLP: Application of a text annotation
architecture to noun compound bracketing. In Proceedings
of SIG BioLINK.
James Pustejovsky, Peter Anick, and Sabine Bergler. 1993.
Lexical semantic techniques for corpus analysis. Compu-
tational Linguistics, 19(2):331?358.
Philip Resnik. 1993. Selection and information: a class-based
approach to lexical relationships. Ph.D. thesis, University
of Pennsylvania, UMI Order No. GAX94-13894.
Beatrice Warren. 1978. Semantic patterns of noun-noun com-
pounds. In Gothenburg Studies in English 41, Goteburg,
Acta Universtatis Gothoburgensis.
Y. Yang and J. Pedersen. 1997. A comparative study on feature
selection in text categorization. In Proceedings of ICML?97),
pages 412?420.
24
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 134?135,
New York City, June 2006. c?2006 Association for Computational Linguistics
Summarizing Key Concepts using Citation Sentences
Ariel S. Schwartz and Marti Hearst
EECS and SIMS
University of California at Berkeley
Berkeley, CA 94720
sariel@cs.berkeley.edu, hearst@sims.berkeley.edu
Citations have great potential to be a valuable re-
source in mining the bioscience literature (Nakov et
al., 2004). The text around citations (or citances)
tends to state biological facts with reference to the
original papers that discovered them. The cited facts
are typically stated in a more concise way in the
citing papers than in the original. We hypothesize
that in many cases, as time goes by, the citation
sentences can more accurately indicate the most im-
portant contributions of a paper than its original ab-
stract.
One can use various NLP tools to identify and
normalize the important entities in (a) the abstract
of the original article, (b) the body of the original
article, and (c) the citances to the article. We hy-
pothesize that grouping entities by their occurrence
in the citances represents a better summary of the
original paper than using only the first two sources
of information.
To help determine the utility of the approach, we
are applying it to the problem of identifying arti-
cles that discuss critical residue functionality, for use
in PhyloFacts a phylogenomic database (Sjolander,
2004).
Consider the article shown in Figure 1. This paper
is a prominent one, published in 1992, with nearly
500 papers citing it. For about 200 of these papers,
we downloaded the sentences that surround the cita-
tion within the full text. Some examples are shown
in Figure 2.
We are developing a statistical model that will
group these entities into potentially overlapping
groups, where each group represents a central idea
in the original paper. In the example shown, some of
the citances emphasize what the paper reports about
the structural elements of the SH2 domain, whereas
other emphasize its findings on interactions and oth-
ers focus on the critical residues.
Often several articles are cited in the same citance,
so it is important to untangle which entities belong
to which citation; by pursuing overlapping sets, our
model should be able to eliminate most spurious ref-
erences.
The same entity is often described in many differ-
ent ways. Prior work has shown how to use redun-
dant information across citations to help normalize
entities (Wellner et al, 2004; Pasula et al, 2003);
similar techniques may work with entities men-
tioned in citances. This can be combined with prior
work on normalizing entity names in bioscience text,
e.g, (Morgan et al, 2004). For a detailed review of
related work see (Nakov et al, 2004).
By emphasizing entities the model potentially
misses important relationships between the entities.
It remains to be determined whether or not relation-
ships must be modeled explicitly in order to create a
useful summary.
134
References
A. A. Morgan, L. Hirschman, M. Colosimo, A. S. Yeh, and J. B. Colombe. 2004. Gene name identification and normalization
using a model organism database. Journal of Biomedical Informatics, 37(6):396?410.
P. I. Nakov, A. S. Schwartz, and M. Hearst. 2004. Citances: Citation sentences for semantic analysis of bioscience text. In
Proceedings of the SIGIR?04 workshop on Search and Discovery in Bioinformatics.
H. Pasula, B. Marthi, B. Milch, S. Russell, and I. Shiptser. 2003. Identity uncertainty and citation matching. Advances In Neural
Information Processing Systems, 15.
K. Sjolander. 2004. Phylogenomic inference of protein molecular function: advances and challenges. Bioinf., 20(2):170?179.
B. Wellner, A. McCallum, F. Peng, and M. Hay. 2004. An integrated, conditional model of information extraction and coreference
with application to citation graph construction. In 20th Conference on Uncertainty in Artificial Intelligence (UAI).
Waksman G, Kominos D, Robertson SC, Pant N, Baltimore D, Birge RB, Cowburn D, Hanafusa H,
Mayer BJ, Overduin M, et al, Abstract Crystal structure of the phosphotyrosine recognition domain
SH2 of v-src complexed with tyrosine-phosphorylated peptides.
Nature. 1992 Aug 20;358(6388):646-53. [PMID: 1379696]
Three-dimensional structures of complexes of the SH2 domain of the v-src oncogene product with two
phosphotyrosyl peptides have been determined by X-ray crystallography at resolutions of 1.5 and 2.0
A, respectively. A central antiparallel beta-sheet in the structure is flanked by two alpha-helices, with
peptide binding mediated by the sheet, intervening loops and one of the helices. The specific recognition
of phosphotyrosine involves amino-aromatic interactions between lysine and arginine side chains and the
ring system in addition to hydrogen-bonding interactions with the phosphate.
Figure 1: Target article for summarization.
Binding of IFNgamma R and gp130 phosphotyrosine peptides to the STAT SH2 domains was mod-
eled by using the coordinates of peptides pYIIPL (pY, phosphotyrosine) and pYVPML bound to the
phospholipase C-gamma 1 and v-src kinase SH2 domains, respectively (#OTHER CITATION, #TAR-
GET CITATION).
The ligand-binding surface of the SH2 domain of the Lck nonreceptor protein tyrosine kinase con-
tains two pockets, one for the Tyr(P) residue and another for the amino acid residue three positions
C-terminal to it, the +3 amino acid (#OTHER CITATION, #TARGET CITATION).
Given the inherent specificity of SH2 phosphopeptide interactions (#TARGET CITATION), a high
degree of selectivity is possible for STAT dimerizations and for STAT activation by different ligand-
receptor combinations.
In fact, the v-src SH2 domain was previously shown to bind a peptide pYVPML of the platelet-derived
growth factor receptor in a rather unconventional manner (#TARGET CITATION).
Figure 2: Sample citances pointing to target article, with some key terms highlighted.
135
Proceedings of the Second Workshop on Statistical Machine Translation, pages 212?215,
Prague, June 2007. c?2007 Association for Computational Linguistics
UCB System Description for the WMT 2007 Shared Task
Preslav Nakov
EECS, CS division
University of California at Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Marti Hearst
School of Information
University of California at Berkeley
Berkeley, CA 94720
hearst@ischool.berkeley.edu
Abstract
For the WMT 2007 shared task, the UC
Berkeley team employed three techniques of
interest. First, we used monolingual syntac-
tic paraphrases to provide syntactic variety
to the source training set sentences. Sec-
ond, we trained two language models: a
small in-domain model and a large out-of-
domain model. Finally, we made use of re-
sults from prior research that shows that cog-
nate pairs can improve word alignments. We
contributed runs translating English to Span-
ish, French, and German using various com-
binations of these techniques.
1 Introduction
Modern Statistical Machine Translation (SMT) sys-
tems are trained on aligned sentences of bilingual
corpora, typically from one domain. When tested on
text from that same domain, such systems demon-
strate state-of-the art performance; however, on
out-of-domain text the results can get significantly
worse. For example, on the WMT 2006 Shared
Task evaluation, the French to English translation
BLEU scores dropped from about 30 to about 20 for
nearly all systems, when tested on News Commen-
tary rather than Europarl (Koehn and Monz, 2006).
Therefore, this year the shared task organizers
have provided 1M words of bilingual News Com-
mentary training data in addition to the Europarl
data (about 30M words), thus challenging the par-
ticipants to experiment with domain adaptation.
Below we describe our domain adaptation exper-
iments, trying to achieve better results on the News
Commentary data. In addition to training on both
data sets, we make use of monolingual syntactic
paraphrases of the English side of the data.
2 Monolingual Syntactic Paraphrasing
In many cases, the testing text contains ?phrases?
that are equivalent, but syntactically different from
the phrases learned on training, and the potential for
a high-quality translation is missed. We address this
problem by using nearly equivalent syntactic para-
phrases of the original sentences. Each paraphrased
sentence is paired with the foreign translation that is
associated with the original sentence in the training
data. This augmented training corpus can then be
used to train an SMT system. Alternatively, we can
paraphrase the test sentences making them closer to
the target language syntax.
Given an English sentence, we parse it with the
Stanford parser (Klein and Manning, 2003) and then
generate paraphrases using the following syntactic
transformations:
1. [NP NP1 P NP2]? [NP NP2 NP1].
inequality in income? income inequality.
2. [NP NP1 of NP2]? [NP NP2 poss NP1].
inequality of income? income?s inequality.
3. NPposs ? NP.
income?s inequality? income inequality.
4. NPposs ? NPPPof .
income?s inequality? inequality of income.
5. NPNC ? NPposs.
income inequality? income?s inequality.
6. NPNC ? NPPP .
income inequality? inequality in incomes.
212
Sharply rising income inequality has raised the stakes of the economic game .
Sharply rising income inequality has raised the economic game ?s stakes .
Sharply rising income inequality has raised the economic game stakes .
Sharply rising inequality of income has raised the stakes of the economic game .
Sharply rising inequality of income has raised the economic game ?s stakes .
Sharply rising inequality of income has raised the economic game stakes .
Sharply rising inequality of incomes has raised the stakes of the economic game .
Sharply rising inequality of incomes has raised the economic game ?s stakes .
Sharply rising inequality of incomes has raised the economic game stakes .
Sharply rising inequality in income has raised the stakes of the economic game .
Sharply rising inequality in income has raised the economic game ?s stakes .
Sharply rising inequality in income has raised the economic game stakes .
Sharply rising inequality in incomes has raised the stakes of the economic game .
Sharply rising inequality in incomes has raised the economic game ?s stakes .
Sharply rising inequality in incomes has raised the economic game stakes .
Table 1: Sample sentence and automatically generated paraphrases. Paraphrased NCs are in italics.
7. remove that where optional
I think that he is right? I think he is right.
8. add that where optional
I think he is right? I think that he is right.
where:
poss possessive marker: ? or ?s;
P preposition;
NPPP NP with internal PP-attachment;
NPPPof NP with internal PP headed by of;
NPposs NP with internal possessive marker;
NPNC NP that is a Noun Compound.
While the first four and the last two transfor-
mations are purely syntactic, (5) and (6) are not.
The algorithm must determine whether a possessive
marker is feasible for (5) and must choose the cor-
rect preposition for (6). In either case, for noun com-
pounds (NCs) of length 3 or more, it also needs to
choose the position to modify, e.g., inquiry?s com-
mittee chairman vs. inquiry committee?s chairman.
In order to ensure accuracy of the paraphrases,
we use statistics gathered from the Web, using a
variation of the approaches presented in Lapata and
Keller (2004) and Nakov and Hearst (2005). We use
patterns to generate possible prepositional or copula
paraphrases in the context of the preceding and the
following word in the sentence, First we split the
NC into two parts N1 and N2 in all possible ways,
e.g., beef import ban lifting would be split as: (a)
N1=?beef?, N2=?import ban lifting?, (b) N1=?beef
import?, N2=?ban lifting?, and (c) N1=?beef import
ban?, N2=?lifting?. For every split, we issue exact
phrase queries to the Google search engine using
the following patterns:
"lt N1 poss N2 rt"
"lt N2 prep det N ?1 rt"
"lt N2 that be det N ?1 rt"
"lt N2 that be prep det N ?1 rt"
where: lt is the word preceding N1 in the original
sentence or empty if none, rt is the word following
N2 in the original sentence or empty if none, poss
is a possessive marker (?s or ?), that is that, which
or who, be is is or are, det is a determiner (the, a,
an, or none), prep is one of the 8 prepositions used
by Lauer (1995) for semantic interpretation of NCs:
about, at, for, from, in, of, on, and with, and N ?1 can
be either N1, or N1 with the number of its last word
changed from singular/plural to plural/singular.
For all splits, we collect the number of page hits
for each instantiation of each pattern, filtering out
the paraphrases whose page hit count is less than 10.
We then calculate the total number of page hitsH for
all paraphrases (for all splits and all patterns), and
retain those ones whose page hits count is at least
10% of H . Note that this allows for multiple para-
phrases of an NC. If no paraphrases are retained, we
213
repeat the above procedure with lt set to the empty
string. If there are still no good paraphrases, we set
the rt to the empty string. If this does not help ei-
ther, we make a final attempt, by setting both lt and
rt to the empty string.
Table 1 shows the paraphrases for a sample sen-
tence. We can see that income inequality is para-
phrased as inequality of income, inequality of in-
comes, inequality in income and inequality in in-
comes; also economic game?s stakes becomes eco-
nomic game stakes and stakes of the economic game.
3 Experiments
Table 2 shows a summary of our submissions: the
official runs are marked with a ?. For our experi-
ments, we used the baseline system, provided by the
organizers, which we modified in different ways, as
described below.
3.1 Domain Adaptation
All our systems were trained on both corpora.
? Language models. We used two language
models (LM) ? a small in-domain one (trained
onNews Commentary) and a big out-of-domain
one (trained on Europarl). For example, for EN
? ES (from English to Spanish), on the low-
ercased tuning data set, using in-domain LM
only achieved a BLEU of 0.332910, while us-
ing both LMs yielded 0.354927, a significant
effect.
? Cognates. Previous research has found that
using cognates can help get better word align-
ments (and ultimately better MT results), espe-
cially in case of a small training set. We used
the method described in (Kondrak et al, 2003)
in order to extract cognates from the two data
sets. We then added them as sentence pairs to
the News Commentary corpus before training
the word alignment models1 for ucb3, ucb4 and
ucb5.
1Following (Kondrak et al, 2003), we considered words of
length 4 or more, we required the length ratio to be between
7
10 and
10
7 , and we accepted as potential cognates all pairs for
which the longest common subsequence ratio (LCSR) was 0.58
or more. We repeated 3 times the cognate pairs extracted from
the Europarl, and 4 times the ones from News Commentary.
? Phrases. The ucb5 system uses the Europarl
data in order to learn an additional phrase ta-
ble and an additional lexicalized re-ordering
model.
3.2 Paraphrasing the Training Set
In two of our experiments (ucb3, ucb4 and ucb5),
we used a paraphrased version of the training News
Commentary data, using all rules (1)-(8). We trained
two separate MT systems: one on the original cor-
pus, and another one on the paraphrased version.
We then used both resulting lexicalized re-ordering
models and a merged phrase table with extra para-
meters: if a phrase appeared in both phrase tables,
it now had 9 instead of 5 parameters (4 from each
table, plus a phrase penalty), and if it was in one
of the phrase tables only, the 4 missing parameters
were filled with 1e-40.
The ucb5 system is also trained on Europarl,
yielding a third lexicalized re-ordering model and
adding 4 new parameters to the phrase table entries.
Unfortunately, longer sentences (up to 100 to-
kens, rather than 40), longer phrases (up to 10 to-
kens, rather than 7), two LMs (rather than just
one), higher-order LMs (order 7, rather than 3),
multiple higher-order lexicalized re-ordering mod-
els (up to 3), etc. all contributed to increased sys-
tem?s complexity, and, as a result, time limitations
prevented us from performing minimum-error-rate
training (MERT) (Och, 2003) for ucb3, ucb4 and
ucb5. Therefore, we used the MERT parameter val-
ues from ucb1 instead, e.g. the first 4 phrase weights
of ucb1 were divided by two, copied twice and used
in ucb3 as the first 8 phrase-table parameters. The
extra 4 parameters of ucb5 came from training a sep-
arate MT system on the Europarl data (scaled ac-
cordingly).
3.3 Paraphrasing the Test Set
In some of our experiments (ucb2 and ucb4), given
a test sentence, we generated the single most-likely
paraphrase, which makes it syntactically closer to
Spanish and French. Unlike English, which makes
extensive use of noun compounds, these languages
strongly prefer connecting the nouns with a preposi-
tion (and less often turning a noun into an adjective).
Therefore, we paraphrased all NCs using preposi-
tions, by applying rules (4) and (6). In addition, we
214
Languages System LM size Paraphrasing Cognates? Extra phrases MERT
News Europarl train? test? Europarl finished?
EN? ES ucb1? 3 5 +
ucb2 3 5 + +
ucb3 5 7 + +
ucb4 5 7 + + +
ucb5 5 7 + + +
EN? FR ucb3 5 7 + +
ucb4? 5 7 + + +
EN? DE ucb1? 5 7 + +
ucb2 5 7 + + +
Table 2: Summary of our submissions. All runs are for the News Commentary test data. The official
submissions are marked with a star.
applied rule (8), since its Spanish/French equivalent
que (as well as the German da?) is always obliga-
tory. These transformations affected 927 out of the
2007 test sentences. We also used this transformed
data set when translating to German (however, Ger-
man uses NCs as much as English does).
3.4 Other Non-standard Settings
Below we discuss some non-standard settings that
differ from the ones suggested by the organizers in
their baseline system. First, following Birch et al
(2006), who found that higher-order LMs give bet-
ter results2, we used a 5-gram LM for News Com-
mentary, and 7-gram LM for Europarl (as opposed
to 3-gram, as done normally). Second, for all runs
we trained our systems on all sentences of length up
to 100 (rather than 40, as suggested in the baseline
system). Third, we used a maximum phrase length
limit of 10 (rather than 7, as typically done). Fourth,
we used both a lexicalized and distance-based re-
ordering models (as opposed to lexicalized only, as
in the baseline system). Finally, while we did not
use any resources other than the ones provided by
the shared task organizers, we made use of Web fre-
quencies when paraphrasing the training corpus, as
explained above.
4 Conclusions and Future Work
We have presented various approaches to domain
adaptation and their combinations. Unfortunately,
2They used a 5-gram LM trained on Europarl, but we
pushed the idea further, using a 7-gram LM with a Kneser-Ney
smoothing.
computational complexity and time limitations pre-
vented us from doing proper MERT for the interest-
ing more complex systems. We plan to do a proper
MERT training and to study the impact of the indi-
vidual components in isolation.
Acknowledgements: This work supported in part
by NSF DBI-0317510.
References
Alexandra Birch, Chris Callison-Burch, Miles Osborne, and
Philipp Koehn. 2006. Constraining the phrase-based, joint
probability statistical translation model. In Proc. of Work-
shop on Statistical Machine Translation, pages 154?157.
Dan Klein and Christopher Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL ?03.
Philipp Koehn and Christof Monz. 2006. Manual and auto-
matic evaluation of machine translation between european
languages. In Proceedings on the Workshop on Statistical
Machine Translation, pages 102?121.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight. 2003.
Cognates can improve statistical translation models. In Pro-
ceedings of NAACL, pages 46?48.
Mirella Lapata and Frank Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised web-based
models for a range of nlp tasks. In Proceedings of HLT-
NAACL ?04.
Mark Lauer. 1995. Corpus statistics meet the noun compound:
some empirical results. In Proceedings of ACL ?95.
Preslav Nakov and Marti Hearst. 2005. Search engine statistics
beyond the n-gram: Application to noun compound bracket-
ing. In Proceedings of CoNLL ?05.
Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL, pages
160?167.
215
BioNLP 2007: Biological, translational, and clinical language processing, pages 73?80,
Prague, June 2007. c?2007 Association for Computational Linguistics
Exploring the Efficacy of
Caption Search for Bioscience Journal Search Interfaces
Marti A. Hearst, Anna Divoli, Jerry Ye
School of Information, UC Berkeley
Berkeley, CA 94720
{hearst,divoli,jerryye}@ischool.berkeley.edu
Michael A. Wooldridge
California Digital Library
Oakland, CA 94612
mikew@ucop.edu
Abstract
This paper presents the results of a pilot us-
ability study of a novel approach to search
user interfaces for bioscience journal arti-
cles. The main idea is to support search over
figure captions explicitly, and show the cor-
responding figures directly within the search
results. Participants in a pilot study ex-
pressed surprise at the idea, noting that they
had never thought of search in this way.
They also reported strong positive reactions
to the idea: 7 out of 8 said they would use a
search system with this kind of feature, sug-
gesting that this is a promising idea for jour-
nal article search.
1 Introduction
For at least two decades, the standard way to search
for bioscience journal articles has been to use the
National Library of Medicine?s PubMed system to
search the MEDLINE collection of journal articles.
PubMed has innovated search in many ways, but to
date search in PubMed is restricted to the title, ab-
stract, and several kinds of metadata about the doc-
ument, including authors, Medical Subject Heading
(MeSH) labels, publication year, and so on.
On the Web, searching within the full text of doc-
uments has been standard for more than a decade,
and much progress has been made on how to do
this well. However, until recently, full text search
of bioscience journal articles was not possible due
to two major constraints: (1) the full text was not
widely available online, and (2) publishers restrict
researchers from downloading these articles in bulk.
Recently, online full text of bioscience journal ar-
ticles has become ubiquitous, eliminating one bar-
rier. The intellectual property restriction is under
attack, and we are optimistic that it will be nearly
entirely diffused in a few years. In the meantime,
the PubMedCentral Open Access collection of jour-
nals provides an unrestricted resource for scientists
to experiment with for providing full text search.1
Full text availability requires a re-thinking of how
search should be done on bioscience journal arti-
cles. One opportunity is to do information extrac-
tion (text mining) to extract facts and relations from
the body of the text, as well as from the title and
abstract as done by much of the early text mining
work. (The Biocreative competition includes tasks
that allow for extraction within full text (Yeh et al,
2003; Hirschman et al, 2005).) The results of text
extraction can then be exposed in search interfaces,
as done in systems like iHOP (Hoffmann and Va-
lencia, 2004) and ChiliBot (Chen and Sharp, 2004)
(although both of these search only over abstracts).
Another issue is how to adjust search ranking al-
gorithms when using full text journal articles. For
example, there is evidence that ranking algorithms
should consider which section of an article the query
terms are found in, and assign different weights to
different sections for different query types (Shah et
al., 2003), as seen in the TREC 2006 Genomics
Track (Hersh et al, 2006).
Recently Google Scholar has provided search
1The license terms for use for BioMed Central can be
found at: http://www.biomedcentral.com/info/authors/license
and the license for PubMedCentral can be found at:
http://www.pubmedcentral.gov/about/openftlist.html
73
over the full text of journal articles from a wide
range of fields, but with no special consideration
for the needs of bioscience researchers2. Google
Scholar?s distinguishing characteristic is its ability
to show the number of papers that cite a given arti-
cle, and rank papers by this citation count. We be-
lieve this is an excellent starting point for full text
search, and any future journal article search system
should use citation count as a metric. Unfortunately,
citation count requires access to the entire collection
of articles; something that is currently only avail-
able to a search system that has entered into con-
tracts with all of the journal publishers.
In this article, we focus on another new opportu-
nity: the ability to search over figure captions and
display the associated figures. This idea is based
on the observation, noted by our own group as well
as many others, that when reading bioscience arti-
cles, researchers tend to start by looking at the title,
abstract, figures, and captions. Figure captions can
be especially useful for locating information about
experimental results. A prominent example of this
was seen in the 2002 KDD competition, the goal
of which was to find articles that contained exper-
imental evidence for gene products, where the top-
performing team focused its analysis on the figure
captions (Yeh et al, 2003).
In the Biotext project, we are exploring how to
incorporate figures and captions into journal article
search explicitly, as part of a larger effort to provide
high-quality article search interfaces. This paper re-
ports on the results of a pilot study of the caption
search idea. Participants found the idea novel, stim-
ulating, and most expressed a desire to use a search
interface that supports caption search and figure dis-
play.3
2 Related Work
2.1 Automated Caption Analysis
Several research projects have examined the auto-
mated analysis of text from captions. Srihari (1991;
1995) did early work on linking information be-
tween photographs and their captions, to determine,
for example, which person?s face in a newspaper
2http://scholar.google.com
3The current version of the interface can be seen at
http://biosearch.berkeley.edu
photograph corresponded to which name in the cap-
tion. Shatkay et al (2006) combined information
from images as well as captions to enhance a text
categorization algorithm.
Cohen, Murphy, et al have explored several dif-
ferent aspects of biological text caption analysis. In
one piece of work (Cohen et al, 2003) they devised
and tested algorithms for parsing the structure of im-
age captions, which are often quite complex, espe-
cially when referring to a figure that has multiple
images within it. In another effort, they developed
tools to extract information relating to subcellular
localization by automatically analyzing fluorescence
microscope images of cells (Murphy et al, 2003).
They later developed methods to extract facts from
the captions referring to these images (Cohen et al,
2003).
Liu et al (2004) collected a set of figures and
classified them according to whether or not they de-
picted schematic representations of protein interac-
tions. They then allowed users to search for a gene
name within the figure caption, returning only those
figures that fit within the one class (protein interac-
tion schematics) and contained the gene name.
Yu et al (2006) created a bioscience image tax-
onomy (consisting of Gel-Image, Graph, Image-of-
Thing, Mix, Model, and Table) and used Support
Vector Machines to classify the figures, using prop-
erties of both the textual captions and the images.
2.2 Figures in Bioscience Article Search
Some bioscience journal publishers provide a ser-
vice called ?SummaryPlus? that allows for display
of figures and captions in the description of a partic-
ular article, but the interface does not apply to search
results listings.4
A medical image retrieval and image annotation
task have been part of the ImageCLEF competition
since 2005 (Muller et al, 2006).5 The datasets for
this competition are clinical images, and the task is
to retrieve images relevant to a query such as ?Show
blood smears that include polymorphonuclear neu-
4Recently a commercial offering by a company called CSA
Illustrata was brought to our attention; it claims to use figures
and tables in search in some manner, but detailed information is
not freely available.
5CLEF stands for Cross-language Evaluation Forum; it orig-
inally evaluated multi-lingual information retrieval, but has
since broadened its mission.
74
trophils.? Thus, the emphasis is on identifying the
content of the images themselves.
Yu and Lee (2006) hypothesized that the infor-
mation found in the figures of a bioscience article
are summarized by sentences from that article?s ab-
stract. They succeeded in having 119 scientists mark
up the abstract of one of their own articles, indicat-
ing which sentence corresponded to each figure in
the article. They then developed algorithms to link
sentences from the abstract to the figure caption con-
tent. They also developed and assessed a user inter-
face called BioEx that makes use of this linking in-
formation. The interface shows a set of very small
image thumbnails beneath each abstract. When the
searcher?s mouse hovers over the thumbnail, the cor-
responding sentence from the abstract is highlighted
dynamically.
To evaluate BioEx, Yu and Lee (2006) sent a ques-
tionnaire to the 119 biologists who had done the
hand-labeling linking abstract sentences to images,
asking them to assess three different article display
designs. The first design looked like the PubMed
abstract view. The second augmented the first view
with very small thumbnails of figures extracted from
the article. The third was the second view aug-
mented with color highlighting of the abstract?s sen-
tences. It is unclear if the biologists were asked to
do searches over a collection or were just shown a
sample of each view and asked to rate it. 35% of the
biologists responded to the survey, and of these, 36
out of 41 (88%) preferred the linked abstract view
over the other views. (It should be noted that the
effort invested in annotating the abstracts may have
affected the scientists? view of the design.)
It is not clear, however, whether biologists would
prefer to see the caption text itself rather than the
associated information from the abstract. The sys-
tem described did not allow for searching over text
corresponding to the figure caption. The system also
did not focus on how to design a full text and caption
search system in general.
3 Interface Design and Implementation
The Biotext search engine indexes all Open Access
articles available at PubMedCentral. This collection
consists of more than 150 journals, 20,000 articles
and 80,000 figures. The figures are stored locally,
and at different scales, in order to be able to present
thumbnails quickly. The Lucene search engine6 is
used to index, retrieve, and rank the text (default sta-
tistical ranking). The interface is web-based and is
implemented in Python and PHP. Logs and other in-
formation are stored and queried using MySQL.
Figure 1a shows the results of searching over the
caption text in the Caption Figure view. Figure
1b shows the same search in the Caption Figure
with additional Thumbnails (CFT) view. Figure 2a-
b shows two examples of the Grid view, in which
the query terms are searched for in the captions, and
the resulting figures are shown in a grid, along with
metadata information.7 The Grid view may be espe-
cially useful for seeing commonalities among topics,
such as all the phylogenetic trees that include a given
gene, or seeing all images of embryo development of
some species.
The next section describes the study participants?
reaction to these designs.
4 Pilot Usability Study
The design of search user interfaces is difficult; the
evidence suggests that most searchers are reluctant
to switch away from something that is familiar. A
search interface needs to offer something qualita-
tively better than what is currently available in order
to be acceptable to a large user base (Hearst, 2006).
Because text search requires the display of text,
results listings can quickly obtain an undesirably
cluttered look, and so careful attention to detail is
required in the elements of layout and graphic de-
sign. Small details that users find objectionable can
render an interface objectionable, or too difficult to
use. Thus, when introducing a new search interface
idea, great care must be taken to get the details right.
The practice of user-centered design teaches how to
achieve this goal: first prototype, then test the results
with potential users, then refine the design based on
their responses, and repeat (Hix and Hartson, 1993;
Shneiderman and Plaisant, 2004).
Before embarking on a major usability study to
determine if a new search interface idea is a good
one, it is advantageous to run a series of pilot stud-
ies to determine which aspects of the design work,
6http://lucene.apache.org
7These screenshots represent the system as it was evaluated.
The design has subsequently evolved and changed.
75
(a)
(b)
Figure 1: Search results on a query of zebrafish over the captions within the articles with (a) CF view, and
(b) CFT view. The thumbnail is shown to the left of a blue box containing the bibliographic information
above a yellow box containing the caption text. The full-size view of the figure can be overlaid over the
current page or in a new browser window. In (b) the first few figures are shown as mini-thumbnails in a row
below the caption text with a link to view all the figures and captions.
76
(a)
(b)
Figure 2: Grid views of the first sets of figures returned as the result of queries for (a) mutagenesis and for
(b) pathways over captions in the Open Access collection.
77
ID status sex lit search area(s) of specialization
1 undergrad F monthly organic chemistry
2 graduate F weekly genetics / molecular bio.
3 other F rarely medical diagnostics
4 postdoc M weekly neurobiology, evolution
5 graduate F daily evolutionary bio., entomology
6 undergrad F weekly molecular bio., biochemistry
7 undergrad F monthly cell developmental bio.
8 postdoc M daily molecular / developmental bio.
Table 1: Participant Demographics. Participant 3 is
an unemployed former lab worker.
which do not, make adjustments, and test some
more. Once the design has stabilized and is re-
ceiving nearly uniform positive feedback from pilot
study participants, then a formal study can be run
that compares the novel idea to the state-of-the-art,
and evaluates hypotheses about which features work
well for which kinds of tasks.
The primary goal of this pilot study was to deter-
mine if biological researchers would find the idea of
caption search and figure display to be useful or not.
The secondary goal was to determine, should cap-
tion search and figure display be useful, how best
to support these features in the interface. We want
to retain those aspects of search interfaces that are
both familiar and useful, and to introduce new ele-
ments in such a way as to further enhance the search
experience without degrading it.
4.1 Method
We recruited participants who work in our campus?
main biology buildings to participate in the study.
None of the participants were known to us in ad-
vance. To help avoid positive bias, we told partici-
pants that we were evaluating a search system, but
did not mention that our group was the one who
was designing the system. The participants all had
strong interests in biosciences; their demographics
are shown in Table 1.
Each participant?s session lasted approximately
one hour. First, they were told the purpose of the
study, and then filled out an informed consent form
and a background questionnaire. Next, they used the
search interfaces (the order of presentation was var-
ied). Before the use of each search interface, we
explained the idea behind the design. The partici-
pant then used the interface to search on their own
Figure 3: Likert scores on the CF view. X-axis:
participant ID, y-axis: Likert scores: 1 = strongly
disagree, 7 = strongly agree. (Scale reversed for
questionnaire-posed cluttered and overwhelming.)
queries for about 10 minutes, and then filled out a
questionnaire describing their reaction to that de-
sign. After viewing all of the designs, they filled
out a post-study questionnaire where they indicated
whether or not they would like to use any of the
designs in their work, and compared the design to
PubMed-type search.
Along with these standardized questions, we had
open discussions with participants about their reac-
tions to each view in terms of design and content.
Throughout the study, we asked participants to as-
sume that the new designs would eventually search
over the entire contents of PubMed and not just the
Open Access collection.
We showed all 8 participants the Caption with
Figure (CF) view (see Figure 1a), and Caption with
Figure and additional Thumbnails (CFT) (see Figure
1b), as we didn?t know if participants would want to
see additional figures from the caption?s paper.8 We
did not show the first few participants the Grid view,
as we did not know how the figure/caption search
would be received, and were worried about over-
whelming participants with new ideas. (Usability
study participants can become frustrated if exposed
to too many options that they find distasteful or con-
fusing.) Because the figure search did receive pos-
8We also experimented with showing full text search to the
first five participants, but as that view was problematic, we dis-
continued it and substituted a title/abstract search for the re-
maining three participants. These are not the focus of this study
and are not discussed further here.
78
itive reactions from 3 of the first 4 participants, we
decided to show the Grid view to the next 4.
4.2 Results
The idea of caption search and figure display was
very positively perceived by all but one participant.
7 out of 8 said they would want to use either CF
or CFT in their bioscience journal article searches.
Figure 3 shows Likert scores for CF view.
The one participant (number 2) who did not like
CF nor CFT thought that the captions/figures would
not be useful for their tasks, and preferred seeing
the articles? abstracts. Many participants noted that
caption search would be better for some tasks than
others, where a more standard title & abstract or full-
text search would be preferable. Some participants
said that different views serve different roles, and
they would use more than one view depending on
the goal of their search. Several suggested combin-
ing abstract and figure captions in the search and/or
the display. (Because this could lead to search re-
sults that require a lot of scrolling, it would probably
be best to use modern Web interface technologies
to dynamically expand long abstracts and captions.)
When asked for their preference versus PubMed, 5
out of 8 rated at least one of the figure searches
above PubMed?s interface. (In some cases this may
be due to a preference for the layout in our design as
opposed to entirely a preference for caption search.)
Two of the participants preferred CFT to CF; the
rest thought CFT was too busy. It became clear
through the course of this study that it would be
best to show all the thumbnails that correspond to a
given article as the result of a full-text or abstract-
text search interface, and to show only the figure
that corresponds to the caption in the caption search
view, with a link to view all figures from this article
in a new page.
All four participants who saw the Grid view liked
it, but noted that the metadata shown was insuffi-
cient; if it were changed to include title and other
bibliographic data, 2 of the 4 who saw Grid said they
would prefer that view over the CF view. Several
participants commented that they have used Google
Images to search for images but they rarely find what
they are looking for. They reacted very positively
to the idea of a Google Image-type system special-
ized to biomedical images. One participant went so
far as to open up Google Image search and compare
the results directly, finding the caption search to be
preferable.
All participants favored the ability to browse all
figures from a paper once they find the abstract or
one of the figures relevant to their query. Two partic-
ipants commented that if they were looking for gen-
eral concepts, abstract search would be more suit-
able but for a specific method, caption view would
be better.
4.3 Suggestions for Redesign
All participants found the design of the new views
to be simple and clear. They told us that they gen-
erally want information displayed in a simple man-
ner, with as few clicks needed as possible, and with
as few distracting links as possible. Only a few ad-
ditional types of information were suggested from
some participants: display, or show links to, related
papers and provide a link to the full text PDF directly
in the search results, as opposed to having to access
the paper via PubMed.
Participants also made clear that they would of-
ten want to start from search results based on title
and abstract, and then move to figures and captions,
and from there to the full article, unless they are do-
ing figure search explicitly. In that case, they want
to start with CF or Grid view, depending on how
much information they want about the figure at first
glance.
They also wished to have the ability to sort the re-
sults along different criteria, including year of pub-
lication, alphabetically by either journal or author
name, and by relevance ranking. This result has
been seen in studies of other kinds of search inter-
faces as well (Reiterer et al, 2005; Dumais et al,
2003). We have also received several requests for ta-
ble caption search along with figure caption search.
5 Conclusions and Future Work
The results of this pilot study suggest that caption
search and figure display is a very promising direc-
tion for bioscience journal article search, especially
paired with title/abstract search and potentially with
other forms of full-text search. A much larger-scale
study must be performed to firmly establish this re-
sult, but this pilot study provides insight about how
79
to design a search interface that will be positively re-
ceived in such a study. Our results also suggest that
web search systems like Google Scholar and Google
Images could be improved by showing images from
the articles along lines of specialization.
The Grid view should be able to show images
grouped by category type that is of interest to biolo-
gists, such as heat maps and phylogenetic trees. One
participant searched on pancreas and was surprised
when the top-ranked figure was an image of a ma-
chine. This idea underscores the need for BioNLP
research in the study of automated caption classifi-
cation. NLP is needed both to classify images and
perhaps also to automatically determine which im-
ages are most ?interesting? for a given article.
To this end, we are in the process of building a
classifier for the figure captions, in order to allow
for grouping by type. We have developed an im-
age annotation interface and are soliciting help with
hand-labeling from the research community, to build
a training set for an automated caption classifier.
In future, we plan to integrate table caption
search, to index the text that refers to the cap-
tion, along with the caption, and to provide inter-
face features that allow searchers to organize and
filter search results according to metadata such as
year published, and topical information such as
genes/proteins mentioned. We also plan to conduct
formal interface evaluation studies, including com-
paring to PubMed-style presentations.
Acknowledgements: This work was supported in
part by NSF DBI-0317510. We thank the study par-
ticipants for their invaluable help.
References
H. Chen and B.M. Sharp. 2004. Content-rich biological net-
work constructed by mining PubMed abstracts. BMC Bioin-
formatics, 5(147).
W.W. Cohen, R. Wang, and R.F. Murphy. 2003. Understand-
ing captions in biomedical publications. Proceedings of the
ninth ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 499?504.
S. Dumais, E. Cutrell, J.J. Cadiz, G. Jancke, R. Sarin, and D.C.
Robbins. 2003. Stuff I?ve seen: a system for personal in-
formation retrieval and re-use. Proceedings of SIGIR 2003,
pages 72?79.
M. Hearst. 2006. Design recommendations for hierarchi-
cal faceted search interfaces. In ACM SIGIR Workshop on
Faceted Search, Seattle, WA.
W. Hersh, A. Cohen, P. Roberts, and Rekapalli H. K. 2006.
TREC 2006 genomics track overview. The Fifteenth Text
Retrieval Conference.
L. Hirschman, A. Yeh, C. Blaschke, and A. Valencia. 2005.
Overview of BioCreAtIvE: critical assessment of informa-
tion extraction for biology. BMC Bioinformatics, 6:1.
D. Hix and H.R. Hartson. 1993. Developing user interfaces:
ensuring usability through product & process. John Wiley
& Sons, Inc. New York, NY, USA.
R. Hoffmann and A. Valencia. 2004. A gene network for navi-
gating the literature. Nature Genetics, 36(664).
F. Liu, T-K. Jenssen, V. Nygaard, J. Sack, and E. Hovig. 2004.
FigSearch: a figure legend indexing and classification sys-
tem. Bioinformatics, 20(16):2880?2882.
H. Muller, T. Deselaers, T. Lehmann, P. Clough, E. Kim, and
W. Hersh. 2006. Overview of the ImageCLEF 2006 Medical
Image Retrieval Tasks. In Working Notes for the CLEF 2006
Workshop.
R.F. Murphy, M. Velliste, and G. Porreca. 2003. Robust Nu-
merical Features for Description and Classification of Sub-
cellular Location Patterns in Fluorescence Microscope Im-
ages. The Journal of VLSI Signal Processing, 35(3):311?
321.
B. Rafkind, M. Lee, S.F. Chang, and H. Yu. 2006. Exploring
text and image features to classify images in bioscience lit-
erature. Proceedings of the BioNLP Workshop on Linking
Natural Language Processing and Biology at HLT-NAACL,
6:73?80.
H. Reiterer, G. Tullius, and T. M. Mann. 2005. Insyder:
a content-based visual-information-seeking system for the
web. International Journal on Digital Libraries, 5(1):25?
41, Mar.
P.K. Shah, C. Perez-Iratxeta, P. Bork, and M.A. Andrade.
2003. Information extraction from full text scientific arti-
cles: where are the keywords? BMC Bioinformatics, 4(20).
H. Shatkay, N. Chen, and D. Blostein. 2006. Integrating im-
age data into biomedical text categorization. Bioinformatics,
22(14):e446.
B. Shneiderman and C. Plaisant. 2004. Designing the user in-
terface: strategies for effective human-computer interaction,
4/E. Addison Wesley.
R.K. Srihari. 1991. PICTION: A System that Uses Captions
to Label Human Faces in Newspaper Photographs. Proceed-
ings AAAI-91, pages 80?85.
RK Srihari. 1995. Automatic indexing and content-based re-
trieval of captioned images. Computer, 28(9):49?56.
A.S. Yeh, L. Hirschman, and A.A. Morgan. 2003. Evaluation
of text data mining for database curation: lessons learned
from the KDD Challenge Cup. Bioinformatics, 19(1):i331?
i339.
H. Yu and M. Lee. 2006. Accessing bioscience images from
abstract sentences. Bioinformatics, 22(14):e547.
80
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366?369,
Prague, June 2007. c?2007 Association for Computational Linguistics
UCB: System Description for SemEval Task #4
Preslav I. Nakov
EECS, CS division
University of California at Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Marti A. Hearst
School of Information
University of California at Berkeley
Berkeley, CA 94720
hearst@ischool.berkeley.edu
Abstract
The UC Berkeley team participated in the
SemEval 2007 Task #4, with an approach
that leverages the vast size of the Web in or-
der to build lexically-specific features. The
idea is to determine which verbs, preposi-
tions, and conjunctions are used in sentences
containing a target word pair, and to com-
pare those to features extracted for other
word pairs in order to determine which are
most similar. By combining these Web fea-
tures with words from the sentence context,
our team was able to achieve the best results
for systems of category C and third best for
systems of category A.
1 Introduction
Semantic relation classification is an important but
understudied language problem arising in many
NLP applications, including question answering, in-
formation retrieval, machine translation, word sense
disambiguation, information extraction, etc. This
year?s SemEval (previously SensEval) competition
has included a task targeting the important special
case of Classification of Semantic Relations between
Nominals. In the present paper we describe the UCB
system which took part in that competition.
The SemEval dataset contains a total of 7 se-
mantic relations (not exhaustive and possibly over-
lapping), with 140 training and about 70 testing
sentences per relation. Sentence classes are ap-
proximately 50% negative and 50% positive (?near
misses?). Table 1 lists the 7 relations together with
some examples.
# Relation Name Examples
1 Cause-Effect hormone-growth, laugh-wrinkle
2 Instrument-Agency laser-printer, ax-murderer
3 Product-Producer honey-bee, philosopher-theory
4 Origin-Entity grain-alcohol, desert-storm
5 Theme-Tool work-force, copyright-law
6 Part-Whole leg-table, door-car
7 Content-Container apple-basket, plane-cargo
Table 1: SemEval dataset: Relations with examples
(context sentences are not shown).
Each example consists of a sentence, two nomi-
nals to be judged on whether they are in the target
semantic relation, manually annotated WordNet 3.0
sense keys for these nominals, and the Web query
used to obtain that example:
"Among the contents of the <e1>vessel</e1>
were a set of carpenters <e2>tools</e2>,
several large storage jars, ceramic
utensils, ropes and remnants of food, as
well as a heavy load of ballast stones."
WordNet(e1) = "vessel%1:06:00::",
WordNet(e2) = "tool%1:06:00::",
Content-Container(e2, e1) = "true",
Query = "contents of the * were a"
2 Related Work
Lauer (1995) proposes that eight prepositions are
enough to characterize the relation between nouns
in a noun-noun compound: of, for, in, at, on, from,
with or about. Lapata and Keller (2005) improve
on his results by using Web statistics. Rosario et al
(2002) use a ?descent of hierarchy?, which charac-
terizes the relation based on the semantic category of
the two nouns. Girju et al (2005) apply SVM, deci-
sion trees, semantic scattering and iterative seman-
366
tic specialization, using WordNet, word sense dis-
ambiguation, and linguistic features. Barker and Sz-
pakowicz (1998) propose a two-level hierarchy with
5 classes at the upper level and 30 at the lower level.
Turney (2005) introduces latent relational analysis,
which uses the Web, synonyms, patterns like ?X for
Y ?, ?X such as Y ?, etc., and singular value decom-
position to smooth the frequencies. Turney (2006)
induces patterns from the Web, e.g. CAUSE is best
characterized by ?Y * causes X?, and ?Y in * early
X? is the best pattern for TEMPORAL. Kim and Bald-
win (2006) propose to use a predefined set of seed
verbs and multiple resources: WordNet, CoreLex,
and Moby?s thesaurus. Finally, in a previous publi-
cation (Nakov and Hearst, 2006), we make the claim
that the relation between the nouns in a noun-noun
compound can be characterized by the set of inter-
vening verbs extracted from the Web.
3 Method
Given an entity-annotated example sentence, we re-
duce the target entities e1 and e2 to single nouns
noun1 and noun2, by keeping their last nouns
only, which we assume to be the heads. We then
mine the Web for sentences containing both noun1
and noun2, from which we extract features, con-
sisting of word(s), part of speech (verb, preposi-
tion, verb+preposition, coordinating conjunction),
and whether noun1 precedes noun2. Table 2 shows
some example features and their frequencies.
We start with a set of exact phrase queries
against Google: ?infl1 THAT * infl2?, ?infl2
THAT * infl1?, ?infl1 * infl2?, and ?infl2 *
infl1?, where infl1 and infl2 are inflectional vari-
ants of noun1 and noun2, generated using WordNet
(Fellbaum, 1998); THAT can be that, which, or who;
and * stands for 0 or more (up to 8) stars separated
by spaces, representing the Google * single-word
wildcard match operator. For each query, we collect
the text snippets from the result set (up to 1000 per
query), split them into sentences, assign POS tags
using the OpenNLP tagger1, and extract features:
Verb: If one of the nouns is the subject, and the
other one is a direct or indirect object of that verb,
we extract it and we lemmatize it using WordNet
(Fellbaum, 1998). We ignore modals and auxil-
1OpenNLP: http://opennlp.sourceforge.net
Freq. Feature POS Direction
2205 of P 2? 1
1923 be V 1? 2
771 include V 1? 2
382 serve on V 2? 1
189 chair V 2? 1
189 have V 1? 2
169 consist of V 1? 2
148 comprise V 1? 2
106 sit on V 2? 1
81 be chaired by V 1? 2
78 appoint V 1? 2
77 on P 2? 1
66 and C 1? 2
. . . . . . . . . . . .
Table 2: Most frequent features for committee
member. V stands for verb, P for preposition, and
C for coordinating conjunction.
iaries, but retain the passive be, verb particles and
prepositions (in case of indirect object).
Preposition: If one of the nouns is the head of
an NP which contains a PP, inside which there is an
NP headed by the other noun (or an inflectional form
thereof), we extract the preposition heading that PP.
Coordination: If the two nouns are the heads of
two coordinated NPs, we extract the coordinating
conjunction.
In addition, we include some non-Web features2:
Sentence word: We use as features the words
from the context sentence, after stop words removal
and stemming with the Porter stemmer.
Entity word: We also use the lemmas of the
words that are part of ei (i = 1, 2).
Query word: Finally, we use the individual
words that are part of the query string. This feature
is used for category C runs only (see below).
Once extracted, the features are used to calculate
the similarity between two noun pairs. Each feature
triplet is assigned a weight. We wish to downweight
very common features, such as ?of? used as a prepo-
sition in the 2 ? 1 direction, so we apply tf.idf
weighting to each feature. We then use the following
variant of the Dice coefficient to compare the weight
vectors A = (a1, . . . , an) and B = (b1, . . . , bn):
Dice(A,B) =
2?
?n
i=1 min(ai, bi)
?n
i=1 ai +
?n
i=1 bi
(1)
This vector representation is similar to that of
2Features have type prefix to prevent them from mixing.
367
System Relation P R F Acc
UCB-A1 Cause-Effect 58.2 78.0 66.7 60.0
Instrument-Agency 62.5 78.9 69.8 66.7
Product-Producer 77.3 54.8 64.2 59.1
Origin-Entity 67.9 52.8 59.4 67.9
Theme-Tool 50.0 31.0 38.3 59.2
Part-Whole 51.9 53.8 52.8 65.3
Content-Container 62.2 60.5 61.3 60.8
average 61.4 58.6 58.9 62.7
UCB-A2 Cause-Effect 58.0 70.7 63.7 58.8
Instrument-Agency 65.9 71.1 68.4 67.9
Product-Producer 80.0 77.4 78.7 72.0
Origin-Entity 60.6 55.6 58.0 64.2
Theme-Tool 45.0 31.0 36.7 56.3
Part-Whole 41.7 38.5 40.0 58.3
Content-Container 56.4 57.9 57.1 55.4
average 58.2 57.5 57.5 61.9
UCB-A3 Cause-Effect 62.5 73.2 67.4 63.8
Instrument-Agency 65.9 76.3 70.7 69.2
Product-Producer 75.0 67.7 71.2 63.4
Origin-Entity 48.4 41.7 44.8 54.3
Theme-Tool 62.5 51.7 56.6 67.6
Part-Whole 50.0 46.2 48.0 63.9
Content-Container 64.9 63.2 64.0 63.5
average 61.3 60.0 60.4 63.7
UCB-A4 Cause-Effect 63.5 80.5 71.0 66.2
Instrument-Agency 70.0 73.7 71.8 71.8
Product-Producer 76.3 72.6 74.4 66.7
Origin-Entity 50.0 47.2 48.6 55.6
Theme-Tool 61.5 55.2 58.2 67.6
Part-Whole 52.2 46.2 49.0 65.3
Content-Container 65.8 65.8 65.8 64.9
average 62.7 63.0 62.7 65.4
Baseline (majority) 81.3 42.9 30.8 57.0
Table 3: Task 4 results. UCB systems A1-A4.
Lin (1998), who measures word similarity by using
triples extracted from a dependency parser. In par-
ticular, given a noun, he finds all verbs that have it
as a subject or object, and all adjectives that modify
it, together with the corresponding frequencies.
4 Experiments and Results
Participants were asked to classify their systems
into categories depending on whether they used the
WordNet sense (WN) and/or the Google query (GC).
Our team submitted runs for categories A (WN=no,
QC=no) and C (WN=no, QC=yes) only, since we
believe that having the target entities annotated with
the correct WordNet senses is an unrealistic assump-
tion for a real-world application.
Following Turney and Littman (2005) and Barker
and Szpakowicz (1998), we used a 1-nearest-
neighbor classifier. Given a test example, we calcu-
lated the Dice coefficient between its feature vector
System Relation P R F Acc
UCB-C1 Cause-Effect 58.5 75.6 66.0 60.0
Instrument-Agency 65.2 78.9 71.4 69.2
Product-Producer 81.4 56.5 66.7 62.4
Origin-Entity 67.9 52.8 59.4 67.9
Theme-Tool 50.0 31.0 38.3 59.2
Part-Whole 51.9 53.8 52.8 65.3
Content-Container 62.2 60.5 61.3 60.8
Average 62.4 58.5 59.4 63.5
UCB-C2 Cause-Effect 58.0 70.7 63.7 58.8
Instrument-Agency 67.5 71.1 69.2 69.2
Product-Producer 80.3 79.0 79.7 73.1
Origin-Entity 60.6 55.6 58.0 64.2
Theme-Tool 50.0 37.9 43.1 59.2
Part-Whole 43.5 38.5 40.8 59.7
Content-Container 56.4 57.9 57.1 55.4
Average 59.5 58.7 58.8 62.8
UCB-C3 Cause-Effect 62.5 73.2 67.4 63.8
Instrument-Agency 68.2 78.9 73.2 71.8
Product-Producer 74.1 69.4 71.7 63.4
Origin-Entity 56.8 58.3 57.5 61.7
Theme-Tool 62.5 51.7 56.6 67.6
Part-Whole 50.0 42.3 45.8 63.9
Content-Container 64.9 63.2 64.0 63.5
Average 62.7 62.4 62.3 65.1
UCB-C4 Cause-Effect 63.5 80.5 71.0 66.2
Instrument-Agency 70.7 76.3 73.4 73.1
Product-Producer 76.7 74.2 75.4 67.7
Origin-Entity 59.0 63.9 61.3 64.2
Theme-Tool 63.0 58.6 60.7 69.0
Part-Whole 52.2 46.2 49.0 65.3
Content-Container 64.1 65.8 64.9 63.5
Average 64.2 66.5 65.1 67.0
Baseline (majority) 81.3 42.9 30.8 57.0
Table 4: Task 4 results. UCB systems C1-C4.
and the vector of each of the training examples. If
there was a single highest-scoring training example,
we predicted its class for that test example. Oth-
erwise, if there were ties for first, we assumed the
class predicted by the majority of the tied examples.
If there was no majority, we predicted the class that
was most likely on the training data. Regardless of
the classifier?s prediction, if the head words of the
two entities e1 and e2 had the same lemma, we clas-
sified that example as negative.
Table 3 and 4 show the results for our A and C
runs for different amounts of training data: 45 (A1,
C1), 90 (A2, C2), 105 (A3, C3) and 140 (A4, C4).
All results are above the baseline: always propose
the majority label (?true?/?false?) in the test set. In
fact, our category C system is the best-performing
(in terms of F and Acc) among the participating
systems, and we achieved the third best results for
category A. Our category C results are slightly but
368
consistently better than forA for all measures (P ,R,
F , Acc), which suggests that knowing the query is
helpful. Interestingly, systems UCB-A2 and UCB-
C2 performed worse than UCB-A1 and UCB-C1,
which means that having more training data does not
necessarily help with a 1NN classifier.
Table 5 shows additional analysis for A4 and C4.
We study the effect of adding extra Google contexts
(using up to 10 stars, rather than 8), and using differ-
ent subsets of features. We show the results for: (a)
leave-one-out cross-validation on the training data,
(b) on the test data, and (c) our official UCB runs.
Acknowledgements: This work is supported in
part by NSF DBI-0317510.
References
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Proceedings
of COLING-ACL?98, pages 96?102.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe.
2005. On the semantics of noun compounds. Computer
Speech and Language, 19(4):479?496.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting seman-
tic relations in noun compounds via verb semantics. In Pro-
ceedings of COLING/ACL 2006. (poster), pages 491?498.
Mirella Lapata and Frank Keller. 2005. Web-based models for
natural language processing. ACM Transactions on Speech
and Language Processing, 2:1?31.
Mark Lauer. 1995. Designing Statistical Language Learners:
Experiments on Noun Compounds. Ph.D. thesis, Department
of Computing Macquarie University NSW 2109 Australia.
Dekang Lin. 1998. An information-theoretic definition of sim-
ilarity. In Proceedings of International Conference on Ma-
chine Learning, pages 296?304.
Preslav Nakov and Marti Hearst. 2006. Using verbs to charac-
terize noun-noun relations. In Proceedings of AIMSA, pages
233?244.
Barbara Rosario, Marti Hearst, and Charles Fillmore. 2002.
The descent of hierarchy, and selection in relational seman-
tics. In ACL, pages 247?254.
Peter Turney and Michael Littman. 2005. Corpus-based learn-
ing of analogies and semantic relations. Machine Learning
Journal, 60(1-3):251?278.
Peter Turney. 2005. Measuring semantic similarity by latent
relational analysis. In Proceedings IJCAI, pages 1136?1141.
Peter Turney. 2006. Expressing implicit semantic relations
without supervision. In Proceedings of COLING-ACL,
pages 313?320.
Features Used Leave-1-out Test UCB
Cause-Effect
sent 45.7 50.0
p 55.0 53.8
v 59.3 68.8
v + p 57.1 63.7
v + p + c 70.5 67.5
v + p + c + sent 58.5 66.2 66.2
v + p + c + sent + query 59.3 66.2 66.2
Instrument-Agency
sent 63.6 59.0
p 62.1 70.5
v 71.4 69.2
v + p 70.7 70.5
v + p + c 70.0 70.5
v + p + c + sent 68.6 71.8 71.8
v + p + c + sent + query 70.0 73.1 73.1
Product-Producer
sent 47.9 59.1
p 55.7 58.1
v 70.0 61.3
v + p 66.4 65.6
v + p + c 67.1 65.6
v + p + c + sent 66.4 69.9 66.7
v + p + c + sent + query 67.9 69.9 67.7
Origin-Entity
sent 64.3 72.8
p 63.6 56.8
v 69.3 71.6
v + p 67.9 69.1
v + p + c 66.4 70.4
v + p + c + sent 68.6 72.8 55.6
v + p + c + sent + query 67.9 72.8 64.2
Theme-Tool
sent 66.4 69.0
p 56.4 56.3
v 61.4 70.4
v + p 56.4 67.6
v + p + c 57.1 69.0
v + p + c + sent 52.1 62.0 67.6
v + p + c + sent + query 52.9 62.0 69.0
Part-Whole
sent 47.1 51.4
p 57.1 54.1
v 60.0 66.7
v + p 62.1 63.9
v + p + c 61.4 63.9
v + p + c + sent 60.0 61.1 65.3
v + p + c + sent + query 60.0 61.1 65.3
Content-Container
sent 56.4 54.1
p 57.9 59.5
v 71.4 67.6
v + p 72.1 67.6
v + p + c 72.9 67.6
v + p + c + sent 69.3 67.6 64.9
v + p + c + sent + query 71.4 71.6 63.5
Average A4 67.3 65.4
Average C4 68.1 67.0
Table 5: Accuracy for different features and extra
Web contexts: on leave-one-out cross-validation,
on testing data, and in the official UCB runs.
369
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 62?70,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
NLP Support for Faceted Navigation in Scholarly Collections
Marti A. Hearst
School of Information, UC Berkeley
102 South Hall, Berkeley, CA 94720
hearst@ischool.berkeley.edu
Emilia Stoica
Ask.com
555 12th Street, Oakland, CA 94607
emilia.stoica@ask.com
Abstract
Hierarchical faceted metadata is a proven
and popular approach to organizing infor-
mation for navigation of information col-
lections. More recently, digital libraries
have begun to adopt faceted navigation for
collections of scholarly holdings. A key
impediment to further adoption is the need
for the creation of subject-oriented faceted
metadata. The Castanet alorithm was de-
veloped for the purpose of (semi) auto-
mated creation of such structures. This pa-
per describes the application of Castanet to
journal title content, and presents an eval-
uation suggesting its efficacy. This is fol-
lowed by a discussion of areas for future
work.
1 Introduction
Faceted navigation for searching and browsing
?vertical? content collections has become the stan-
dard interface paradigm for e-commerce shopping
web sites. Faceted navigation, when properly de-
signed, has been shown to be understood by users
and preferred over other organizations (Hearst et
al., 2002; Yee et al, 2003; English et al, 2001).
Although text clustering is an easily automated
technique, numerous studies have found that the
results of clustering are difficult for lay people to
understand (Kleiboemer et al, 1996; Russell et al,
2006; Hornb?k and Fr?kj?r, 1999) and that the
coherent and predictable structure of categorical
metadata is superior from a usability prespective
(Rodden et al, 2001; Pratt et al, 1999; Hearst,
2006a).
An interface using hierarchical faceted naviga-
tion simultaneously shows previews of where to
go next and how to return to previous states in
the exploration, while seamlessly integrating free
text search within the category structure. Faceted
metadata provides organizing context for results
and for subsequent queries, which can act as im-
portant scaffolding for exploration and discovery.
The mental work of searching an information col-
lection is reduced by promoting recognition over
recall and suggesting logical but perhaps unex-
pected alternatives, while at the same time avoid-
ing empty results sets.
Recently, faceted navigation has emerged as the
dominant method for new interfaces for navigat-
ing digital library collections. The NCSU library
catalog was an early adopter among university li-
braries, using the Endeca product as its backend
(Antelman et al, 2006). A usability study with
10 undergraduates comparing this system to the
old library catalog interface found a 48% improve-
ment in task completion time, although the study
did not account for the effects of facets vs. the
effects of fuller coverage in the keyword search.
Additionally, a consortium of university li-
braries (the OCLC) is now using the WorldCat
shared catalog and interface, which features a
faceted navigation component (see Figures 1 and
2). And another popular interface solution is pro-
vided by AquaBrowser, in this case, shown on the
University of Chicago website (see Figure 3). A
recent study on this site found significant benefits
attributable to the faceted navigation facility (Ol-
son, 2007). And finally, the online citation system
DBLP has not one but two different faceted inter-
faces, as does the ACM Digital Library.
These interfaces do a good job of allowing users
to filter by bibliographic attributes such as media,
date, and library. However, in most cases the sub-
ject metadata still is not as rich as it should be to
fully facilitate information browsing and discov-
ery in these systems. In fact, there are a number of
open problems with the use of faceted navigation
for scholarly work. Some of these have to do with
how best to present faceted navigation in the inter-
face (Hearst, 2006b), but others are more relevant
62
Figure 1: Worldcat consortium digital library interface using faceted navigation. The instance shown is
the University of California version, from http://berkeley.worldcat.org .
Figure 2: Digital library interface with faceted navigation, continued, from http://berkeley.worldcat.org .
63
Figure 3: University of Chicago digital library interface using faceted navigation, using an interface from
AquaBrowser.
to NLP, including:
? How to automatically or semi-automatically
create rich subject-oriented faceted metadata
for scholarly text?
? How to automatically assign information
items to faceted category labels?
This paper describes the results of applying
Castanet, a semi-automated approach to creating
faceted metadata, to a scholarly collection. (In
past work it has been shown to work well on a dif-
ferent kind of text (Stoica et al, 2007; Stoica and
Hearst, 2004).) It then discusses some open prob-
lems in building navigation structures for schol-
arly digital libraries.
2 Creating Faceted Metadata
This section first defines faceted metadata, and
then describes the CastaNet alorithm. More de-
tails about the algorithm can be found in a prior
publication (Stoica et al, 2007).
Rather than one large category hierarchy,
faceted metadata consists of a set of categories
(flat or hierarchical), each of which corresponds
to a different facet (dimension or feature type) rel-
evant to the collection to be navigated. After the
facets are designed, each item in the collection is
assigned any number of labels from the facets.
Faceted metadata is intermediate in complexity
between flat categories and full knowledge repre-
sentation. The idea is to develop a set of ?orthog-
onal? categories that characterize the information
space in a meaningful way, using terminology that
is useful for browsing the contents of a domain.
Each facet is a different topic, subject, attribute, or
feature, and some facets have hierarchical ?is-a?
structure. For instance, the facets of a biomedical
collection should cover disease, anatomy, drugs,
symptoms, side-effects, properties of experimen-
tal subjects, and so on. Each biomedical article
can then be assigned any number of category la-
bels from any number of facets. An article on the
effects of tamoxifen on ovarian cancer when tested
on mice could then be navigated to by first starting
with cancer, then selecting drug tamoxifen, and
then body part ovary, or first with tamoxifen, then
navigating to ovary, and further refining by dis-
ease type. This ability to ?mix and match? both
for describing the articles and for navigating the
category structure is key.
The term ?faceted classification? was deliber-
ately chosen in the Flamenco project to echo the
old library science term of that name (Hearst,
2000), but with a rejection of the strict terms re-
quired for construction of controlled vocabulary,
which mandates exhaustive, mutually exclusive
category composition. Rather, the faceted naviga-
64
tion approach for design of search interfaces calls
for category systems that are expressed at a mean-
ingful level of description, use approachable lan-
guage (unless designed for specialists), are consis-
tent in terms of specificity at each level, avoiding
becoming too broad or too deep.
The most difficult part of the design is determin-
ing whether or not compound concepts should be
created. For instance, when evaluating tags for a
digital library like librarything, should terms like
?african history? and ?british literature? be sepa-
rated into two facets, one containing major writing
types (history, literature), and another nationalities
(african, british), or should the modifying struc-
ture be retained, as there are many kinds of history
and many kinds of literature? Most likely, the an-
swer should depend on the makeup of the collec-
tion and the usage that the users are expected to
want to make of it.
The next subsections briefly describe related
work in automated creation of structure from text,
the Castanet alorithm and its output on journal
article title text, and the results of a usability study
on this output.
2.1 Related Work
One way to create faceted metadata is to start with
existing vocabularies, and in fact work has been
done on this area. The Library of Congress Sub-
ject headings are shown in the U Chicago cata-
log, despite a statement by Antelman et al (2006)
about the ?unsuitability of Library of Congress
Subject Headings (LCSH) as an entry vocabulary.?
There has also been work on converting LCSH
into faceted metadata (Anderson and Hofmann,
2006). Work on the Flamenco project converted
the Art and Architecture thesaurus to a faceted cat-
egory system manually (Hearst et al, 2002). How-
ever, automated techniques are desirable.
Other methods that are influential but claimed
to make a meaningful category structure, but not
necessarily a faceted one, include the LDA (Latent
Dirichlet Allocation) method (Blei et al, 2003),
which uses a generative probabilistic model of dis-
crete data to create a model of documents? topics.
It attempts to analyze a text corpus and extract the
topics that combine to form the documents. The
output of the algorithm was originally evaluated
in terms of perplexity reduction but not in terms of
understandability of the topics produced.
Sanderson and Croft (1999) propose a method
called Subsumption for building a hierarchy for a
set of documents retrieved for a query. For two
terms x and y, x is said to subsume y if the follow-
ing conditions hold: P (x|y) ? 0.8, P (y|x) < 1.
To evaluate the algorithm the authors asked 8 par-
ticipants to look at parent-child pairs and state
whether or not they were ?interesting.? Partici-
pants found 67% to be interesting as compared
to 51% for randomly chosen pairs of words. Of
those interesting pairs, 72% were found to display
a ?type-of? relationship.
Another class of solutions make use of exist-
ing lexical hierarchies to build category hierar-
chies, as we do in this paper. For example, Nav-
igli and Velardi (2003) use WordNet (Fellbaum,
1998) to build a complex ontology consisting of
a wide range of relation types (demonstrated on
a travel agent domain), as opposed to a set of
human-readable hierarchical facets. Mihalcea and
Moldovan (2001) describe a sophisticated method
for simplifying WordNet in general, rather than
tailoring it to a specific collection.
Zelevinsky et al (2008) used an approach
of looking at keywords assigned by authors
of ACM publications to documents, computing
which terms had high importance within those
documents, and then using the highest scoring
among those documents to assign new keywords
(referred to in the paper as tags) to the documents.
The tags were shown as query term refinements in
a digital library interface.
Only limited related work has attempted to
make faceted category hierarchies explicitly.
Dakka et al (Dakka and Ipeirotis, 2008; Dakka
et al, 2005) is one of these. Their approach is a
combination of Subsumption and Castanet; they
use lexical resources like WordNet and Wikipedia
to find structure among words, but also use them
to determine which words in a collection are most
useful to include in a faceted system. The facet hi-
erarchy is made via Subsumption. The evaluation
of their most recent work on news text finds strong
results for assessments made by judges of preci-
sion and recall. Furthermore, when facets were
shown in a search interface to five users, the key-
word usage dropped in favor of clicking on cate-
gories, as task completion time was reduced while
satisfaction remained unchanged. No examples
of facet categories produced by the algorithm are
shown, and the role of hierarchy is not clear, but
the approach appears especially promising for de-
65
termining which words of long documents to in-
clude in building facet systems.
2.2 Castanet Applied to Journal Titles
The main idea behind the Castanet alorithm is
to carve out a structure from the hypernym (?is-
a?) relations within the WordNet (Fellbaum, 1998)
lexical database (Stoica et al, 2007; Stoica and
Hearst, 2004). The Castanet alorithm assumes
that there is text associated with each item in the
collection, or at least with a representative subset
of the items. The textual descriptions are used
both to build the facet hierarchies and to assign
items (documents, images, citations, etc.) to the
facets, and the text can be fragmented.
The algorithm has five major steps which are
briefly outlined here. For details, see (2007).
1. Select target terms from textual descriptions
of information items.
2. Build the Core Tree:
? For each term, if the term is unambigu-
ous, add its synset?s IS-A path to the
Core Tree.
? Increment the counts for each node in
the synset?s path with the number of
documents in which the target term ap-
pears.
3. Augment the Core Tree with the remaining
terms? paths:
? For each candidate IS-A path for the am-
biguous term, choose the path for which
there is the most document representa-
tion in the Core Tree.
4. Compress the augmented tree.
5. Remove top-level categories, yielding a set of
facet hierarchies.
In addition to augmenting the nodes in the tree,
adding in a new term increases a count associ-
ated with each node on its path; this count corre-
sponds to how many documents the term occurs in.
Thus the more common a term, the more weight it
places on the path it falls within. The Core Tree
acts as the ?backbone? for the final category struc-
ture. It is built by using paths derived from unam-
biguous terms, with the goal of biasing the final
structure towards the appropriate senses of words.
Currently a word can appear in only one sense in
the final structure; allowing multiple senses is an
area of research.
Figures 4 and 5 show the output of the Cas-
tanet alorithm when applied to the titles of jour-
nals from the bioscience literature. Note that even
the highly ambiguous common anatomy words are
successfully grouped using this algorithm, pre-
sumably because of the requirement that each
word occur in only one location in the ontology
and because the anatomy part of the ontology is
strongly favored during the part of the process
in which the core tree is built with unambiguous
terms. (Although some versions of Castanet use an
advanced version of WordNet Domains (Magnini,
2000), they were not used in the construction of
this category set.)
As reported earlier (Stoica et al, 2007), an eval-
uation of this algorithm was conducted by asking
information architects with expertise in the do-
main over which the algorithm was run to state
whether or not they would like to use the output
of the algorithm to build a website. The output of
Castanet was compared to Subsumption (Sander-
son and Croft, 1999) and to LDA (Blei et al,
2003).
As reported earlier, on a recipes collection, all
34 information architects overwhelming preferred
Castanet. They were asked to respond to how
likely they would be to use the output, on a scale
of: definitely no, no, yes, definitely yes. For Cas-
tanet, 85% of the evaluators said yes or definitely
yes for intent to use. Subsumption received 38%
answering yes or definitely yes, and LDA was re-
jected by all participants.
The study was also conducted using a biologi-
cal journal titles collection. 3275 titles were used
(although a significant number are not in English
and so many are missed by the algorithm). The
15 participants who evaluated the Biomedical ti-
tles collection were required to be frequent users
of PubMed (the online library for biomedicine),
but were not required to be information architects,
as it was difficult to finding information architects
with biological expertise. These participants were
biologists, doctors, medical students and medical
librarians.
7 participants saw both LDA and Castanet, and
8 participants saw both Subsumption and Cas-
tanet (a pilot test found that participants who saw
both Subsumption and LDA became very frus-
trated with the tasks, so the two options were com-
pared pairwise to Castanet for subsequent trials).
For Castanet, 11 out of 15 participants (73%) an-
66
Figure 4: Castanet output on journal title text.
Figure 5: Castanet output on journal title text, continued.
67
Figure 6: LDA output on journal title text.
Figure 7: Subsumption output on journal title text.
68
swered yes or definitely yes to a desire to use its
output. 1 out of 7 participants answered yes to a
desire to use LDA, and 1 out of 8 answered yes to
Subsumption. LDA received 4 ?definitely no? re-
sponses, whereas Subsumption received only one
of these, and no one said definitely no to Castanet.
2.3 Open Problems
Although quite useful ?out of the box,? the Cas-
tanet alorithm could benefit by several improve-
ments and additions:
1. The processing of the terms should recognize
spelling variations (such as aging vs. ageing)
and morphological variations. Verbs and ad-
jectives are often quite important for a collec-
tion and should be included, but with caution.
2. In a related point, the system should have a
way of suggesting synonyms to annotate a
given node, as opposed to listing closely re-
lated words as children or siblings of one an-
other.
3. Some terms should be allowed to occur with
more than one sense if this is required by the
dataset. For example, the term brain is an-
notated with two domains, Anatomy and Psy-
chology, which are both relevant domains for
a biomedical journal collection.
4. Words that appear in noun compounds and
phrases that are not in WordNet should re-
ceive special processing.
5. Currently if a term is in a document it is as-
sumed to use the sense assigned in the facet
hierarchies; this is often incorrect, and so
terms should be disambiguated within the
text before automatic category assignment is
done.
6. WordNet is not exhaustive and some mecha-
nism is needed to improve coverage for un-
known terms.
7. Castanet seems to work better when applied
to short pieces of text (e.g., journal titles vs.
full text); to remedy this, better methods are
needed to select the target terms.
8. A method for dynamically adding facets and
adding terms to facets should be developed,
especially a method for allowing user tags to
be incorporated into the exising facet hierar-
chies.
Recent work by Dakka et al (Dakka and Ipeiro-
tis, 2008) can help with point 7, and some recent
work by Koren et al (Koren et al, 2008) seems
promising for 8.
Robust evaluation methods are also needed;
making use of log information about which facets
are heavily used can help inform decisions about
which facets work well and which need modifica-
tion or additions.
Acknowledgements: Megan Richardson pro-
vided valuable contributions in her work on the
study reported on here. Emilia Stoica did this
work while a postdoctoral researcher at UC Berke-
ley.
References
J.D. Anderson and M.A. Hofmann. 2006. A fully
faceted syntax for Library of Congress subject
headings. Cataloging & Classification Quarterly,
43(1):7?38.
K. Antelman, E. Lynema, and A.K. Pace. 2006. To-
ward a twenty-first century library catalog. Infor-
mation technology and libraries, 25(3):128?138.
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
W. Dakka and P.G. Ipeirotis. 2008. Automatic extrac-
tion of useful facet hierarchies from text databases.
In IEEE 24th International Conference on Data En-
gineering, 2008. ICDE 2008, pages 466?475.
W. Dakka, P.G. Ipeirotis, and K.R. Wood. 2005. Au-
tomatic construction of multifaceted browsing inter-
faces. In Proceedings of the 14th ACM international
conference on Information and knowledge manage-
ment, pages 768?775. ACM New York, NY, USA.
J. English, M.A. Hearst, R. Sinha, K. Swearingen,
and K.-P. Yee. 2001. Examining the usability
of web site search. Unpublished Manuscript,
http://flamenco.berkley.edu/papers/epicurious-
study.pdf.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
M.A. Hearst, J. English, R. Sinha, K. Swearingen, and
K.-P. Yee. 2002. Finding the flow in web site search.
Communications of the ACM, 45(9), September.
M.A. Hearst. 2000. Next Generation Web Search: Set-
ting Our Sites. IEEE Data Engineering Bulletin,
23(3):38?48.
M.A. Hearst. 2006a. Clustering Versus Faceted Cat-
egories For Information Exploration. Communca-
tions Of The Acm, 49(4):59?61.
69
M.A. Hearst. 2006b. Design recommendations for
hierarchical faceted search interfaces. In SIGIR?06
Workshop On Faceted Search, Seattle, Wa, August.
K. Hornb?k and E. Fr?kj?r. 1999. Do Thematic Maps
Improve Information Retrieval. Human-Computer
Interaction (INTERACT?99), pages 179?186.
A.J. Kleiboemer, M.B. Lazear, and J.O. Pedersen.
1996. Tailoring a retrieval system for naive users. In
Proceedings of the Fifth Annual Symposium on Doc-
ument Analysis and Information Retrieval (SDAIR
?96), Las Vegas, NV.
J. Koren, Y. Zhang, and X. Liu. 2008. Personalized
interactive faceted search. WWW ?08: Proceeding
of the 17th international conference on World Wide
Web.
Bernardo Magnini. 2000. Integrating subject field
codes into WordNet. In Proc. of LREC 2000,
Athens, Greece.
Rada Mihalcea and Dan I. Moldovan. 2001.
Ez.wordnet: Principles for automatic generation of
a coarse grained wordnet. In Proc. of FLAIRS Con-
ference 2001, May.
Roberto Navigli, Paola Velardi, and Aldo Gangemi.
2003. Ontology learning and its application to auto-
mated terminology translation. Intelligent Systems,
18(1):22?31.
T.A. Olson. 2007. Utility of a faceted catalog for
scholarly research. Library Hi Tech, 25(4):550?561.
W. Pratt, M.A. Hearst, and L. Fagan. 1999. A
knowledge-based approach to organizing retrieved
documents. In Proceedings of 16th Annual Con-
ference on Artificial Intelligence(AAAI 99), Orlando,
FL.
K. Rodden, W. Basalaj, D. Sinclair, and K. R. Wood.
2001. Does organisation by similarity assist im-
age browsing? In Proceeedings of ACM CHI 2001,
pages 190?197.
D.M. Russell, M. Slaney, Y. Qu, and M. Hous-
ton. 2006. Being literate with large document
collections: Observational studies and cost struc-
ture tradeoffs. In Proceedings of the 39th Annual
Hawaii International Conference on System Sci-
ences (HICSS?06).
Mark Sanderson and Bruce Croft. 1999. Deriving con-
cept hierarchies from text. In Proceedings of SIGIR
1999.
E. Stoica and M. Hearst. 2004. Nearly-automated
metadata hierarchy creation. In Companion Pro-
ceedings of HLT-NAACL?04, pages 117?120.
E. Stoica, M.A. Hearst, and M. Richardson. 2007. Au-
tomating Creation of Hierarchical Faceted Metadata
Structures. In Human Language Technologies: The
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-HLT 2007), pages 244?251.
K.-P. Yee, K. Swearingen, K. Li, and M.A. Hearst.
2003. Faceted metadata for image search and
browsing. In Proceedings of ACM CHI 2003, pages
401?408. ACM New York, NY, USA.
V. Zelevinsky, J. Wang, and D. Tunkelang. 2008. Sup-
porting Exploratory Search for the ACM Digital Li-
brary. In Workshop on Human-Computer Interac-
tion and Information Retrieval (HCIR?08).
70
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 272?277,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Improving the Recognizability of Syntactic Relations Using
Contextualized Examples
Aditi Muralidharan
Computer Science Division
University of California, Berkeley
Berkeley, CA
asm@berkeley.edu
Marti A. Hearst
School of Information
University of California, Berkeley
Berkeley, CA
hearst@berkeley.edu
Abstract
A common task in qualitative data analy-
sis is to characterize the usage of a linguis-
tic entity by issuing queries over syntac-
tic relations between words. Previous in-
terfaces for searching over syntactic struc-
tures require programming-style queries.
User interface research suggests that it is
easier to recognize a pattern than to com-
pose it from scratch; therefore, interfaces
for non-experts should show previews of
syntactic relations. What these previews
should look like is an open question that
we explored with a 400-participant Me-
chanical Turk experiment. We found
that syntactic relations are recognized with
34% higher accuracy when contextual ex-
amples are shown than a baseline of nam-
ing the relations alone. This suggests
that user interfaces should display contex-
tual examples of syntactic relations to help
users choose between different relations.
1 Introduction
The ability to search over grammatical relation-
ships between words is useful in many non-
scientific fields. For example, a social scientist
trying to characterize different perspectives on im-
migration might ask how adjectives applying to
?immigrant? have changed in the last 30 years. A
scholar interested in gender might search a col-
lection to find out whether different nouns enter
into possessive relationships with ?his? and ?her?
(Muralidharan and Hearst, 2013). In other fields,
grammatical queries can be used to develop pat-
terns for recognizing entities in text, such as med-
ical terms (Hirschman et al, 2005; MacLean and
Heer, 2013), and products and organizations (Cu-
lotta and McCallum, 2005), and for coding quali-
tative data such as survey results.
Most existing interfaces for syntactic search
(querying over grammatical and syntactic struc-
tures) require structured query syntax. For exam-
ple, the popular Stanford Parser includes Tregex,
which allows for sophisticated regular expression
search over syntactic tree structures (Levy and An-
drew, 2006). The Finite Structure Query tool for
querying syntactically annotated corpora requires
its queries to be stated in first order logic (Kepser,
2003). In the Corpus Query Language (Jakubicek
et al, 2010), a query is a pattern of attribute-
value pairs, where values can include regular ex-
pressions containing parse tree nodes and words.
Several approaches have adopted XML represen-
tations and the associated query language families
of XPATH and SPARQL. For example, LPath aug-
ments XPath with additional tree operators to give
it further expressiveness (Lai and Bird, 2010).
However, most potential users do not have pro-
gramming expertise, and are not likely to be at
ease composing rigidly-structured queries. One
survey found that even though linguists wished
to make very technical linguistic queries, 55% of
them did not know how to program (Soehn et
al., 2008). In another (Gibbs and Owens, 2012),
humanities scholars and social scientists are fre-
quently skeptical of digital tools, because they are
often difficult to use. This reduces the likelihood
that existing structured-query tools for syntactic
search will be usable by non-programmers (Ogden
and Brooks, 1983).
A related approach is the query-by-example
work seen in the past in interfaces to database sys-
tems (Androutsopoulos et al, 1995). For instance,
the Linguist?s Search Engine (Resnik et al, 2005)
uses a query-by-example strategy in which a user
types in an initial sentence in English, and the sys-
tem produces a graphical view of a parse tree as
output, which the user can alter. The user can ei-
ther click on the tree or modify the LISP expres-
sion to generalize the query. SPLICR also contains
272
a graphical tree editor tool (Rehm et al, 2009).
According to Shneiderman and Plaisant (2010),
query-by-example has largely fallen out of favor
as a user interface design approach. A downside
of QBE is that the user must manipulate an exam-
ple to arrive at the desired generalization.
More recently auto-suggest, a faster technique
that does not require the manipulation of query by
example, has become a widely-used approach in
search user interfaces with strong support in terms
of its usability (Anick and Kantamneni, 2008;
Ward et al, 2012; Jagadish et al, 2007). A list
of selectable options is shown under the search
bar, filtered to be relevant as the searcher types.
Searchers can recognize and select the option that
matches their information need, without having to
generate the query themselves.
The success of auto-suggest depends upon
showing users options they can recognize. How-
ever, we know of no prior work on how to dis-
play grammatical relations so that they can be
easily recognized. One current presentation (not
used with auto-suggest) is to name the relation
and show blanks where the words that satisfy it
would appear as in X is the subject of Y (Muralid-
haran and Hearst, 2013); we used this as the base-
line presentation in our experiments because it em-
ploys the relation definitions found in the Stan-
ford Dependency Parser?s manual (De Marneffe et
al., 2006). Following the principle of recognition
over recall, we hypothesized that showing contex-
tualized usage examples would make the relations
more recognizable.
Our results confirm that showing examples in
the form of words or phrases significantly im-
proves the accuracy with which grammatical re-
lationships are recognized over the standard base-
line of showing the relation name with blanks. Our
findings also showed that clausal relationships,
which span longer distances in sentences, bene-
fited significantly more from example phrases than
either of the other treatments.
These findings suggest that a query interface in
which a user enters a word of interest and the sys-
tem shows candidate grammatical relations aug-
mented with examples from the text will be more
successful than the baseline of simply naming the
relation and showing gaps where the participating
words appear.
2 Experiment
We gave participants a series of identification
tasks. In each task, they were shown a list of sen-
tences containing a particular syntactic relation-
ship between highlighted words. They were asked
to identify the relationship type from a list of four
options. We presented the options in three differ-
ent ways, and compared the accuracy.
We chose Amazon?s Mechanical Turk (MTurk)
crowdsourcing platform as a source of study par-
ticipants. The wide range of backgrounds pro-
vided by MTurk is desirable because our goal is to
find a representation that is understandable to most
people, not just linguistic experts or programmers.
This platform has become widely used for both
obtaining language judgements and for usability
studies (Kittur et al, 2008; Snow et al, 2008).
Our hypothesis was:
Grammatical relations are identified
more accurately when shown with ex-
amples of contextualizing words or
phrases than without.
To test it, participants were given a series of
identification tasks. In each task, they were shown
a list of 8 sentences, each containing a particu-
lar relationship between highlighted words. They
were asked to identify the relationship from a list
of 4 choices. Additionally, one word was chosen
as a focus word that was present in all the sen-
tences, to make the relationship more recognizable
(?life? in Figure 1).
The choices were displayed in 3 different ways
(Figure 1). The baseline presentation (Figure 1a)
named the linguistic relation and showed a blank
space with a pink background for the varying word
in the relationship, the focus word highlighted in
yellow and underlined, and any necessary addi-
tional words necessary to convey the relationship
(such as ?of? for the prepositional relationship
?of?, the third option).
The words presentation showed the baseline de-
sign, and in addition beneath was the word ?Exam-
ples:? followed by a list of 4 example words that
could fill in the pink blank slot (Figure 1b). The
phrases presentation again showed the baseline
design, beneath which was the phrase ?Patterns
like:? and a list of 4 example phrases in which
fragments of text including both the pink and the
yellow highlighted portions of the relationship ap-
peared (Figure 1c).
273
(a) The options as they appear in the baseline condition. (b) The same options as they appear in the words condition.
(c) The same options in the phrases condition, shown as they appeared in an identification task for the relationship
amod(life, ) (where different adjectives modify the noun ?life?). The correct answer is ?adjective modifier? (4th option),
and the remaining 3 options are distractors.
Figure 1: The appearance of the choices shown in the three experiment conditions.
Method: We used a between-subjects design.
The task order and the choice order were not var-
ied: the only variation between participants was
the presentation of the choices. To avoid the pos-
sibility of guessing the right answer by pattern-
matching, we ensured that there was no overlap
between the list of sentences shown, and the ex-
amples shown in the choices as words or phrases.
Tasks: The tasks were generated using the
Stanford Dependency Parser (De Marneffe et al,
2006) on the text of Moby Dick by Herman
Melville. We tested the 12 most common gram-
matical relationships in the novel in order to cover
the most content and to be able to provide as many
real examples as possible. These relationships fell
into two categories, listed below with examples.
Clausal or long-distance relations:
? Adverbial clause: I walk while talking
? Open clausal complement: I love to sing
? Clausal complement: he saw us leave
? Relative clause modifier: the letter I wrote
reached
Non-clausal relations:
? Subject of verb: he threw the ball
? Object of verb: he threw the ball
? Adjective modifier red ball
? Preposition (in): a hole in a bucket
? Preposition (of): the piece of cheese
? Conjunction (and) mind and body
274
? Adverb modifier: we walk slowly
? Noun compound: Mr. Brown
We tested each of these 12 relations with 4 dif-
ferent focus words, 2 in each role. For example,
the Subject of Verb relation was tested in the fol-
lowing forms:
? (Ahab, ): the sentences each contained
?Ahab?, highlighted in yellow, as the subject of
different verbs highlighted in pink.
? (captain, )
? ( , said): the sentences each contained
the verb ?said?, highlighted in yellow, but with
different subjects, highlighted in pink.
? ( , stood)
To maximize coverage, yet keep the total task
time reasonable (average 6.8 minutes), we divided
the relations above into 4 task sets, each testing
recognition of 3 different relations. Each of rela-
tions was tested with 4 different words, making a
total of 12 tasks per participant.
Participants: 400 participants completed the
study distributed randomly over the 4 task sets and
the 3 presentations. Participants were paid 50c
(U.S.) for completing the study, with an additional
50c bonus if they correctly identified 10 or more
of the 12 relationships. They were informed of the
possibility of the bonus before starting.
To gauge their syntactic familiarity, we also
asked them to rate how familiar they were with
the terms ?adjective? (88% claimed they could de-
fine it), ?infinitive? (43%), and ?clausal comple-
ment? (18%). To help ensure the quality of effort,
we included a multiple-choice screening question,
?What is the third word of this sentence?? The 27
participants (out of 410) who answered incorrectly
were eliminated.
Results: The results (Figure 2) confirm our hy-
pothesis. Participants in conditions that showed
examples (phrases and words) were significantly
more accurate at identifying the relations than
participants in the baseline condition. We used
the Wilcoxson signed-rank test, an alternative to
the standard T-test that does not assume sam-
ples are normally distributed. The average suc-
cess rate in the baseline condition was 41%,
which is significantly less accurate than words:
52%, (p=0.00019, W=6136), and phrases: 55%,
(p=0.00014, W=5546.5).
Clausal relations operate over longer distances
in sentences, and so it is to be expected that show-
ing longer stretches of context would perform bet-
0?0.1?
0.2?0.3?
0.4?0.5?
0.6?0.7?
0.8?
Overall? Clausal Relations? Non-Clausal Relations? Adverb Modifier?
Average Recognition Success Rate per Relation?Baseline? Phrases? Words?
Figure 2: Recognition rates for different types of
relations under the 3 experiment conditions, with
95% confidence intervals.
ter in these cases; that is indeed what the re-
sults showed. Phrases significantly outperformed
words and baseline for clausal relations. The av-
erage success rate was 48% for phrases, which
is significantly more than words: 38%, (p=0.017
W=6976.5) and baseline: 24%, (p=1.9?10
?9
W=4399.0), which was indistinguishable from
random guessing (25%). This is a strong improve-
ment, given that only 18% of participants reported
being able to define ?clausal complement?.
For the non-clausal relations, there was no sig-
nificant difference between phrases and words,
although they were both overall significantly bet-
ter than the baseline (words: p=0.0063 W=6740,
phrases: p=0.023 W=6418.5). Among these rela-
tions, adverb modifiers stood out (Figure 2), be-
cause evidence suggested that words (63% suc-
cess) made the relation more recognizable than
phrases (47% success, p=0.056, W=574.0) ? but
the difference was only almost significant, due to
the smaller sample size (only 96 participants en-
countered this relation). This may be because the
words are the most salient piece of information in
an adverbial relation ? adverbs usually end in ?ly?
? and in the phrases condition the additional infor-
mation distracts from recognition of this pattern.
3 Conclusions
The results imply that user interfaces for syntactic
search should show candidate relationships aug-
mented with a list of phrases in which they occur.
A list of phrases is the most recognizable presenta-
tion for clausal relationships (34% better than the
baseline), and is as good as a list of words for the
other types of relations, except adverb modifiers.
For adverb modifiers, the list of words is the most
275
recognizable presentation. This is likely because
Enlglish adverbs usually end in ?-ly? are therefore
a distinctive set of words.
The list of candidates can be ordered by fre-
quency of occurrence in the collection, or by an
interestingness measure given the search word. As
the user becomes more familiar with a given re-
lation, it may be expedient to shorten the cues
shown, and then re-introduce them if a relation
has not been selected after some period of time
has elapsed. If phrases are used, there is a tradeoff
between recognizability and the space required to
display the examples of usage. However, it is im-
portant to keep in mind that because the sugges-
tions are populated with items from the collection
itself, they are informative.
The best strategy, phrases, had an overall suc-
cess rate of only 55%, although the intended user
base may have more familiarity with grammatical
relations than the participants did, and therefore
may perform better in practice. Nonetheless, there
is room for improvement in scores, and it may be
that additional visual cues, such as some kind of
bracketing, will improve results. Furthermore, the
current study did not test three-word relationships
or more complex combinations of structures, and
those may require improvements to the design.
4 Acknowledgements
We thank Bj?orn Hartmann for his helpful com-
ments. This work is supported by National En-
dowment for the Humanities grant HK-50011.
References
I Androutsopoulos, GD Ritchie, and P Thanisch. 1995.
Natural language interfaces to databases?an intro-
duction. Natural Language Engineering, 1(01):29?
81.
Peter Anick and Raj Gopal Kantamneni. 2008. A lon-
gitudinal study of real-time search assistance adop-
tion. In Proceedings of the 31st annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 701?702. ACM.
Aron Culotta and Andrew McCallum. 2005. Reduc-
ing labeling effort for structured prediction tasks. In
AAAI, pages 746?751.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al 2006. Generat-
ing typed dependency parses from phrase structure
parses. In LREC, volume 6, pages 449?454.
Fred Gibbs and Trevor Owens. 2012. Building better
digital humanities tools. DH Quarterly, 6(2).
Lynette Hirschman, Alexander Yeh, Christian
Blaschke, and Alfonso Valencia. 2005. Overview
of biocreative: critical assessment of information
extraction for biology. BMC bioinformatics,
6(Suppl 1):S1.
HV Jagadish, Adriane Chapman, Aaron Elkiss,
Magesh Jayapandian, Yunyao Li, Arnab Nandi, and
Cong Yu. 2007. Making database systems usable.
In Proceedings of the 2007 ACM SIGMOD interna-
tional conference onManagement of data, pages 13?
24. ACM.
Milos Jakubicek, Adam Kilgarriff, Diana McCarthy,
and Pavel Rychl`y. 2010. Fast syntactic searching in
very large corpora for many languages. In PACLIC,
volume 24, pages 741?747.
Stephan Kepser. 2003. Finite structure query: A
tool for querying syntactically annotated corpora. In
EACL, pages 179?186.
Aniket Kittur, Ed H Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with mechanical turk.
In Proceedings of the SIGCHI conference on human
factors in computing systems, pages 453?456. ACM.
Catherine Lai and Steven Bird. 2010. Querying lin-
guistic trees. Journal of Logic, Language and Infor-
mation, 19(1):53?73.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In LREC, pages 2231?2234.
Diana Lynn MacLean and Jeffrey Heer. 2013. Iden-
tifying medical terms in patient-authored text: a
crowdsourcing-based approach. Journal of the
American Medical Informatics Association.
Aditi Muralidharan and Marti A Hearst. 2013. Sup-
porting exploratory text analysis in literature study.
Literary and Linguistic Computing, 28(2):283?295.
William C Ogden and Susan R Brooks. 1983. Query
languages for the casual user: Exploring the mid-
dle ground between formal and natural languages.
In Proceedings of the SIGCHI conference on Hu-
man Factors in Computing Systems, pages 161?165.
ACM.
Georg Rehm, Oliver Schonefeld, Andreas Witt, Erhard
Hinrichs, and Marga Reis. 2009. Sustainability of
annotated resources in linguistics: A web-platform
for exploring, querying, and distributing linguistic
corpora and other resources. Literary and Linguistic
Computing, 24(2):193?210.
Philip Resnik, Aaron Elkiss, Ellen Lau, and Heather
Taylor. 2005. The web in theoretical linguistics re-
search: Two case studies using the linguists search
engine. In Proc. 31st Mtg. Berkeley Linguistics So-
ciety, pages 265?276.
276
Ben Shneiderman and Catherine Plaisant. 2010. De-
signing The User Interface: Strategies for Effective
Human-Computer Interaction, 5/e (Fifth Edition).
Addison Wesley.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254?263. Association for Computational
Linguistics.
Jan-Philipp Soehn, Heike Zinsmeister, and Georg
Rehm. 2008. Requirements of a user-friendly,
general-purpose corpus query interface. Sustain-
ability of Language Resources and Tools for Natural
Language Processing, 6:27.
David Ward, Jim Hahn, and Kirsten Feist. 2012. Au-
tocomplete as research tool: A study on providing
search suggestions. Information Technology and Li-
braries, 31(4):6?19.
277
