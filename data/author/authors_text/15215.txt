2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 667?676,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Grammatical structures for word-level sentiment detection
Asad B. Sayeed
MMCI Cluster of Excellence
Saarland University
66123 Saarbru?cken, Germany
asayeed@coli.uni-sb.de
Jordan Boyd-Graber,
Bryan Rusk, Amy Weinberg
{iSchool / UMIACS, Dept. of CS, CASL}
University of Maryland
College Park, MD 20742 USA
{jbg@umiacs,brusk@,
aweinberg@casl}.umd.edu
Abstract
Existing work in fine-grained sentiment anal-
ysis focuses on sentences and phrases but ig-
nores the contribution of individual words and
their grammatical connections. This is because
of a lack of both (1) annotated data at the word
level and (2) algorithms that can leverage syn-
tactic information in a principled way. We ad-
dress the first need by annotating articles from
the information technology business press via
crowdsourcing to provide training and testing
data. To address the second need, we propose
a suffix-tree data structure to represent syntac-
tic relationships between opinion targets and
words in a sentence that are opinion-bearing.
We show that a factor graph derived from this
data structure acquires these relationships with
a small number of word-level features. We
demonstrate that our supervised model per-
forms better than baselines that ignore syntac-
tic features and constraints.
1 Introduction
The terms ?sentiment analysis? and ?opinion mining?
cover a wide body of research on and development of
systems that can automatically infer emotional states
from text (after Pang and Lee (2008) we use the two
names interchangeably). Sentiment analysis plays a
large role in business, politics, and is itself a vibrant
research area (Bollen et al, 2010).
Effective sentiment analysis for texts such as
newswire depends on the ability to extract who
(source) is saying what (target). Fine-grained sen-
timent analysis requires identifying the sources and
targets directly relevant to sentiment bearing expres-
sions (Ruppenhofer et al, 2008). For example, con-
sider the following sentence from a major informa-
tion technology (IT) business journal:
Lloyd Hession, chief security officer at BT
Radianz in New York, said that virtualiza-
tion also opens up a slew of potential net-
work access control issues.
There are three entities in the sentence that have the
capacity to express an opinion: Lloyd Hession, BT
Radianz, and New York. These are potential opinion
sources. There are also a number of mentioned con-
cepts that could serve as the topic of an opinion in
the sentence, or target. These include all the sources,
but also ?virtualization?, ?network access control?,
?network?, and so on.
The challenging task is to discriminate between
these mentions and choose the ones that are rele-
vant to the user. Furthermore, such a system must
also indicate the content of the opinion itself. This
means that we are actually searching for all triples
{source, target, opinion} in this sentence (Kim and
Hovy, 2006) and throughout each document in the
corpus. In this case, we want to identify that Lloyd
Hession is the source of an opinion, ?slew of network
issues,? about a target, virtualization. Providing such
fine-grained annotations would enrich information
extraction, question answering, and corpus explo-
ration applications by letting users see who is saying
what with what opinion (Wilson et al, 2005; Stoy-
anov and Cardie, 2006).
We motivate the need for a grammatically-focused
approach to fine-grained opinion mining and situate it
667
within the context of existing work in Section 2. We
propose a supervised technique for learning opinion-
target relations from dependency graphs in a way that
preserves syntactic coherence and semantic compo-
sitionality. In addition to being theoretically sound
? a lacuna identified in many sentiment systems1
? such approaches improve downstream sentiment
tasks (Moilanen and Pulman, 2007).
There are multiple types of downstream tasks that
potentially require the retrieval of {source, target,
opinion} relations on a sentence-by-sentence basis.
An increasingly significant application area is in the
use of large corpora in social science. This area of
research requires the exploration and aggregation of
data about the relationships between discourses, orga-
nizations, and people. For example, the IT business
press data that we use in this work belongs to a larger
research program (Tsui et al, 2009; Sayeed et al,
2010) of exploring industry opinion leadership. IT
business press text is one type of text in which many
entities and opinions can appear intermingled with
one another in a small amount of text.
Another application for fine-grained sentiment re-
lation retrieval of this type is paraphrasing, where
attribution of which opinion belongs to which entities
may be important for producing useful and accurate
output, since source and target identification errors
can change the entire meaning of an output text.
Unlike previous approaches that ignore syntax, we
use a sentence?s syntactic structure to build a proba-
bilistic model that encodes whether a word is opinion
bearing as a latent variable. We build a data structure
we call a ?syntactic relatedness trie? (Section 3) that
serves as the skeleton for a graphical model over the
sentiment relevance of words (Section 4). This ap-
proach allows us to learn features that predict opinion
bearing constructions from grammatical structures.
Because of a dearth of resources for this fine-grained
task, we also develop new crowdsourcing techniques
for labeling word-level, syntactically informed sen-
1Alm (2011) recently argued that work on sentiment anal-
ysis needs to de-emphasize the goal of building systems that
are ?high-performing? by traditional measures, because the field
risks sacrificing ?opportunities that may lead to a more thorough
understanding of language uses and users? in relation to subjec-
tive phenomena. The work we present in this paper therefore
focuses on extracting meaningful features as an investment in
future work that directly improves retrieval performance.
timent (Section 5). We use inference techniques to
uncover grammatical patterns that connect opinion-
expressing words and target entities (Section 6) per-
forming better than using syntactically uninformed
methods.
2 Background and existing work
We call opinion mining ?fine-grained? when it re-
trieves many different {source, target, opinion}
triples per document. This is particularly challenging
when there are multiple triples even within a sen-
tence. There is considerable work on identifying the
source of an opinion. However, it is much harder
to find obvious features that tell us whether ?virtual-
ization? is the target of an opinion. The most recent
target identification techniques use machine learning
to determine the presence of a target from known
opinionated language (Jakob and Gurevych, 2010).
Even when targets are identified we must decide if
an opinion is expressed, since not all target mentions
will necessarily be accompanied by opinion expres-
sions. Returning to the first example sentence, we
could say that the negative opinion about virtualiza-
tion is expressed by the words ?slew? and ?issues?.
A system that could automatically make this dis-
covery must draw on grammatical relationships be-
tween targets and the opinion bearing words. Parsers
reveal these relationships, but the relationships are
often indirect. The variability of language prevents
a complete enumeration of all intervening items that
make the relationships indirect, but examples include
negation and intensifiers, which change opinion, and
sentiment-neutral words, which fill syntactic or stylis-
tic needs. In this paper, we cope with the variability
of expression by using supervised machine learning
to generalize across observations and learn which fea-
tures best enable us to identify opinionated language.
Existing work in this area often uses semantic
frames and role labeling (Kim and Hovy, 2006; Choi
et al, 2006), but resources typically used in these
tasks (e.g. FrameNet) are not exhaustive. More gen-
eral approaches (Ruppenhofer et al, 2008) describe
semantic and discourse contexts of opinion sources
and targets cannot recognize them.
When techniques do identify targets via syntax,
they often only use grammar as a feature in an oth-
erwise syntax-agnostic model. Some work of this
668
nature merely identifies targets without providing the
syntactic evidence necessary to find domain-relevant
opinionated language (Jakob and Gurevych, 2010),
relying on lists of opinion keywords. There is also
work (Qiu et al, 2011) that uses predefined heuristics
over dependency parses to identify both targets and
opinion keywords but does not acquire new syntactic
heuristics. Other work (Nakagawa et al, 2010) is sim-
ilar to ours in that it uses factor graph modeling over
a dependency parse formalism, but it assumes that
opinionated language is known a priori and focuses
on polarity classification, while our work tackles the
more fundamental problem of identifying the opin-
ionated language itself.
Little work has been done to perform target and
opinion-expression extraction jointly, especially in a
way that extracts features for downstream processing.
This dearth persists despite evidence that such infor-
mation improves sentiment analysis (Moilanen and
Pulman, 2007).
An advantage of our proposed approach is that we
can use dependency paths in order to capture situa-
tions where the relations are non-compositional or
semantically motivated. In Section 5, we describe a
data set that has the additional property that opinion
is expressed in ways that require external pragmatic
knowledge of the domain. An advantage of arbi-
trary, non-local dependencies is that we can treat this
knowledge as part of the model we learn via long-
distance chains, which can capture pragmatics.
3 Syntactic relatedness tries
We now describe how we build the syntactic related-
ness trie (SRT) that forms the scaffolding for the prob-
abilistic models needed to identify sentiment-bearing
words via syntactic constraints extracted from a de-
pendency parse (Ku?bler et al, 2009).
We use the Stanford Parser (de Marneffe and Man-
ning, 2008) to produce a dependency graph and con-
sider the resulting undirected graph structure over
words. We construct a trie for each possible target
word in a sentence (it is possible for a sentence to
induce multiple tries if the sentence contains multi-
ple potential targets). Each trie encodes paths from
the possible target word to other words, and each
path represents a sequence of words connected by
undirected edges in the parse.
3.1 Encoding Dependencies in an SRT
SRTs enable us to encode the connections between
a single linguistic object of interest?in this appli-
cation, a possible target word?and a set of related
objects. SRTs are data structures consisting of nodes
and edges.
This description is very similar to the definition
of a dependency parse. The key difference is that
while a token only appears once as a node in a de-
pendency parse, an SRT can contain multiple nodes
that originate from the same token. This encodes the
possible connections between an opinion target and
opinion-conveying words.
The object of interest is the opinion target, defined
as the SRT root node (e.g. in Figure 1 ?policy? is a
known target, so it becomes the root of an SRT). Each
SRT edge corresponds to a grammatical relationship
between words and is labeled with that relationship.
We use the notation a
R
?? b to signify that node a has
the relationship (?role?) R with b. We say in this case
that node b is a descendent of node a with the role
R. The directed edges constitute a trie or suffix tree
that represents the fact that multiple paths may share
elements that all provide evidence for the relevance
of multiple leaves. 2
In the remainder of this section we describe the
necessary steps to create a training corpus for fine-
grained sentiment analysis. We provide an example
of how to create an SRT from a dependency parse and
then to attach latent variable assignments to an SRT
based on human annotations in a way that respects
syntactic constraints.
3.2 Using sentiment flow to label an SRT
Our goal is to discriminate between parts of the struc-
ture that are relevant to target-opinion word relations
and those that are not. We use the term sentiment
flow (shortened to ?flow? when space is an issue)
for relevant sentiment-bearing words in the SRT and
inert for the remainder of the sentence. We use the
term ?flow? because our invariant (section 3.3) con-
strains a sentiment flow in a SRT to be a contiguous
subgraph; this corresponds to linguistic intuitions
that, for example, in the sentence ?Linux with Wine
2The SRT will be used to create an undirected graphical
model; the notion of directedness refers to the traversal of paths
used to construct the SRT.
669
the dominant
role
the european climate protection
policy
has
benefits
our
economy
policy
policy
policy
protection
role
role
has
dominant
benefits
Dependency Parse
Paths for "policy" SRT
Figure 1: Dependency parse example. A dependency
parse (top) is used to generate a syntactic relatedness
trie for all possible targets of a sentiment-bearing
expression. For the target word ?policy?, there are a
number of paths (colors are consistent in paths to be
added to the SRT and in the dependency parse) that
connect it to other words; once extracted, these paths
will be inserted into a target-specific SRT.
is very usable?, {?Linux?, ?is?, ?very?} could not
be part of a sentiment flow without also including
{?usable?}.
Now that we have the structure of the model, we
need training data: sentences where sentiment bear-
ing words have been labeled. We describe how to go
from sentiment-labeled words to valid flows using
this sentence from the MPQA:
The dominant role of the European climate
protection policy has benefits for our econ-
omy.
In this sentence, the target word ?policy? is con-
nected to multiple sentiment-bearing words via paths
in the dependency parse (Figure 1). We can represent
these relationships using paths through the graph as
in Figure 2(a). (For clarity, we do not show some
paths.)
Suppose that an annotator decides that ?protec-
tion? and ?benefits? are directly expressing an opin-
ion about the policy, but ?dominant? is ambiguous (it
has some negative connotations). The nodes ?protec-
tion? and ?benefits? are a flow, and the ?dominant?
policy
protection
role
has benefits
dominant
policy
protection
role
has benefits
dominant
policy
protection
role
has benefits
dominant
policy
protection
role
has benefits
dominant
(a)
(b)
(c)
(d)
Figure 2: Labeled SRTs rooted on the target word
?policy?; green-filled nodes represent words that are
part of a sentiment flow and nodes with a red outline
represent inert nodes. (a) Initial labels for SRT (e.g.
as provided by annotators) (b) propagating labels to
yield a valid sentiment flow (c) a change of ?role? to
inert also renders its children inert (d) a change of
?dominant? to be part of a sentiment flow also causes
its parents to be part of a flow.
node is inert. However, there is considerable overlap
between the ?dominant? path and the ?benefits? path.
That is the motivation for combining them into a trie
structure and labeling them in such a way that the
path remains a flow until there is no path element that
leads to a flow leaf (Figure 2).
In other words, we want the path elements com-
mon to a flow path and an inert path to reinforce
sentiment flow. The transition from flow to inert is
learned by the classifier.
We enforce this requirement through the procedure
shown in Figure 2, which is equivalent to finding the
depth first search tree of the dependency graph and
applying the node-labeling scheme as above.
3.3 Invariant
Anything that follows a node with an inert label is
by definition not reachable from the root of the tree.
670
Consequently, any node that is part of a sentiment
flow that follows an inert node is not reachable along
a path and is actually inert itself. We specify this
directly as an invariant on the data structure:
Invariant: no node descending from a
node labeled inert can be labeled as a part
of a sentiment flow.
This specifies that flow labels spread out from the
root of the SRT. Our inference algorithm requires
that we be able to change the labels of nodes for
test data, thus we need to define invariant-respecting
operations for switching labels from flow to flow and
vice-versa. A flow label switched to inert will require
all the descendents of that particular node to switch
to inert as well as in figure 2(c). Similarly, an inert
label switched to flow will require all of the ancestors
of that node to switch to flow as in 2(d).
4 Encoding SRTs as a factor graph
In this section, we develop supervised machine learn-
ing tools to produce a labeled SRT from unlabeled,
held-out data in a single, unified model, without per-
mitting the sorts of inconsistencies that may be ad-
mitted by using a local classifier at each node.
4.1 Sampling labels
A factor graph (Kschischang et al, 1998) is a rep-
resentation of a joint probability distribution in the
form of a graph with two types of vertices: vari-
able vertices and factor vertices. Given a set of vari-
ables Z = {z1 . . . zn}, we connect them via factors
F = {f1 . . . fm}. Factors are functions that repre-
sent relationships, i.e. probabilistic dependencies,
among the variables; the product of all factors gives
the complete joint distribution p. Each factor fi can
take as input some corresponding subset of variables
Yi from Z. We can then write the relationship as
follows:
p(Z) ?
?m
k=1 fk(Yk)
Our goal is to discover the values for the variables
that best explain a dataset. While there are many
approaches for inference in statistical models, we
turn to MCMC methods (Neal, 1993) to discover the
underlying structure of the model. More specifically,
we seek a posterior distribution over latent variables
parent
node
child
1
child
2
child
3
h
g
f
Figure 3: Graphical model of SRT factors
that partition words in a sentence into flow and in-
ert groups; we estimate this posterior using Gibbs
sampling (Finkel et al, 2005).
The sampler requires an initial state that respects
the invariant. Our initial setting is produced by iterat-
ing through all labels in the SRT forest and randomly
setting them as either flow or inert with uniform
probability.
A Gibbs sampler samples new variable assign-
ments from the conditional distribution, treating the
variable assignments for all other variables fixed.
However, the assignment of a single node is highly
coupled with its neighbors, so a block sampler is used
to propose changes to groups nodes that respect the
flow labeling of the overall assignments. This was
implemented by changing the proposal distribution
used by the FACTORIE framework (McCallum et al,
2009).
We can thus represent a node and its contribution
to the overall score using the graph in Figure 3. This
graph contains the given node, its parent, and a vari-
able number of children. The factors that go into the
labeling decision for each node are thus constrained
to a small, computationally tractable space around
the given node. This graph contains three factors:
? g represents a function over features of the given
node itself, or ?node features.?
? f represents a function over a bigram of features
taken from the parent node and the given node,
or ?parent-node? features.
? h represents a function over a combination fea-
tures on the node and features of all its children,
or ?node-child? features.
We provide further details about these factors in the
next section.
671
In addition to the latent value associated with each
word, we associate each node with features derived
from the dependency parse: the word from the sen-
tence itself, the part-of-speech (POS) tag assigned
by the Stanford parser, and the label of the incoming
dependency edge. We treat the edge labels from the
original dependency parse as a feature of the node.
We can represent the set of possible observed lin-
guistic feature classes as the set of features ?. Fig-
ure 3 induces a scoring function with contributions
of each node to the score(label|node) =
?
???
(
f(parent?, node?|label)g(node?|label)
h(node?, child1?, . . . , childn?|label)
)
.
After assignments for the latent variables are sampled,
the weights for the factors (which when combined
create individual factors f that define the joint) must
be learned. This is accomplished via the sample-rank
algorithm (Wick et al, 2009).
5 Data source
Our goal is to identify opinion-bearing words and tar-
gets using supervised machine learning techniques.
Sentiment corpora with sub-sentential annotations,
such as the Multi-Perspective Question-Answering
(MPQA) corpus (Wilson and Wiebe, 2005) and the
J. D. Power and Associates (JDPA) blog post cor-
pus (Kessler et al, 2010), exist, but most of these
annotations are at a phrase level. Within a phrase,
however, some words may contribute more than oth-
ers to the statement of an opinion. We developed our
own annotations to discover such distinctions3. We
describe these briefly here; more information about
the development of the data source can be found in
Sayeed et al (2011).
5.1 Information technology business press
Our work is part of a larger collaboration with so-
cial scientists to study the diffusion of information
technology (IT) innovations through society by iden-
tifying opinion leaders and IT-relevant opinionated
language Rogers (2003). Thus, we focus on a col-
lection of articles from the IT professional maga-
zine, Information Week, from the years 1991 to 2008.
3To download the corpus, visit http://www.umiacs.
umd.edu/?asayeed/naacl12data/.
This consists of 33K articles including news bulletins
and opinion columns. Our IT concept target list (59
terms) comes from our application. Thus, we con-
struct a trie for each appearance of any of these possi-
ble target terms. We consider this list of target terms
to be complete, which allows us to focus on discover-
ing opinion-bearing text associated with these targets.
5.2 Crowdsourced annotation process
Our process for obtaining gold standard data involves
multiple levels of human annotation including on
crowdsourcing platforms Hsueh et al (2009).
There are 75K sentences with IT concept mentions,
only a minority of which express relevant opinions.
Hired undergraduate students searched a random se-
lection of these sentences and found 219 that contain
these opinions. We used cosine-similarity to rank the
remaining sentences against the 219.
We then needed to identify which of the words
contained an opinion. We excluded all words that
were common function words (e.g.,?the?, ?in?) but
left negations. We engineered tasks so that only
a randomly-selected five or six words appear high-
lighted for classification in order to limit annotator
boredom. We called this group a ?highlight group?.
The virtualization example would look like this:
Lloyd Hession, chief security officer at BT
Radianz in New York, said that virtual-
ization also opens up a slew of potential
network access control issues.
In the virtualization example, the worker would see
that virtualization is highlighted as the IT concept
target. Other words are highlighted as candidates that
the worker must classify as being opinion-relevant to
?virtualization?. Each highlight group corresponds to
a syntactic relatedness trie (Section 3).
A task was presented to a worker in the form of
a highlight group and some list boxes that represent
classes for the highlighted words: ?positive?, ?nega-
tive?, ?not opinion-relevant?, and ?ambiguous?. The
worker was required to drag each highlighted can-
didate word to exactly one of the boxes. As we are
not doing opinion polarity classification, the ?posi-
tive? and ?negative? boxes were intended as a form
of misdirection intended to avoid having the worker
consider what an opinion is; we treated this input as
a single ?opinion-relevant? category.
672
Three or more users annotated each highlight
group, and an aggregation scheme was applied af-
terwards: ?ambiguous? answers were rolled into ?not
opinion-relevant? and ties were dropped. Our qual-
ity control process involved filtering out workers
who performed poorly on a small subset of gold-
standard answers We annotated 30 evaluation units to
determine that our process retrieved opinion-relevant
words at 85% precision and 74% recall.
Annotators labeled 700 highlight groups for the
results in this paper. The total cost of this exercise
was approximately 250 USD, which includes the fees
charged by Amazon and CrowdFlower. These last
highlight groups were converted to SRTs and divided
into training and testing groups, 465 and 196 SRTs
respectively, with a small number lost to fatal errors
in the Stanford parser.
6 Experiments and discussion
During the training phase, we evaluate the quality
of a candidate labeling based on label accuracy. We
need to identify both flow nodes and inert nodes in
order to distinguish between relevant and irrelevant
subcomponents. We thus also employ precision and
recall as performance metrics.
An example of how this works can be seen by com-
paring figure 2(b) to figure 2(d), viewing the former
as the gold standard and the latter as a hypothetical
system output. If we run the evaluation over that
single SRT and treat flow as the positive class, we
find that 3 true positives, 1 false positive, 2 false neg-
atives, and no true negatives. There are 6 labels in
total. That yields 0.50 accuracy, 0.75 precision, 0.60
recall, and 0.67 F-measure.
We run every experiment (training a model and
testing on held-out data) 10 times and take the mean
average and range of all measures. F-measure is
calculated for each run and averaged post hoc.
6.1 Experiments
Our baseline system is the initial setting of the labels
for the sampler: uniform random assignment of flow
labels, respecting the invariant. This leads to a large
class imbalance in favor of inert as any switch to
inert converts all nodes downstream from the root to
convert to inert, while a switch to flow causes only
one ancestor branch to convert to flow.
Our next systems involve combinations of our SRT
factors with the observed linguistic features. All our
experiments include the factor g that pertains only to
the features of the node. Then we add factor f?the
parent-node ?bigram? features?and finally factor h,
the variable-length node-child features. We also ex-
periment with including and excluding combinations
of POS, role, and word features. We also explored
models that only made local decisions, ignoring the
consistency constraints over sentiment flows. Al-
though such models cannot be used in techniques
such as Nakagawa et al?s polarity classifier, they
function as a baseline and inform whether syntactic
constraints help performance.
We ran the inferencer for 200 iterations to train a
model with a particular factor-feature combination.
We use the learned model to predict the labels on
the held-out testing data by running the inference
algorithm (sampling labels only) for 50 iterations.
6.2 Discussion
We present a sampling of possible feature-factor com-
binations in table 1 in order to show trends in the
performance of the system.
Unsurprisingly, the invariant-respecting baseline
had very high precision but low recall. Simply includ-
ing the node-only g factor with all features increases
the recall while hurting precision. On removing word
features, recall increases without changing precision.
This suggests that some words in some SRTs are as-
sociated with flow labels in the training data, but not
as much in the testing data.
Including parent-node f features with the g fea-
tures yields higher precision and lower recall, sug-
gesting that parent-node word features support preci-
sion. Including all features on all factors (f , g, and h)
preserves most of the precision but improves recall.
Excluding h features increases recall slightly more
than it hurts precision. Excluding both word features
for all factors and role h features hurts all measures.
The accuracy measure, however, does show over-
all improvement with the inclusion of more feature-
factor combinations. In particular, the node-child h
factor does appear to have an effect on the perfor-
mance. The presence of some combinations of child
word, POS tags, and roles appear to provide some
indication of the flow labeling of some of the nodes.
The best models in terms of accuracy include all or
673
Experiment Features Invariant? Precision Recall F Accuracy
Baseline N/A
Yes 0.78 ? 0.05 0.06 ? 0.01 0.11 ? 0.02 0.51 ? 0.01
No 0.50 ? 0.00 0.49 ? 0.00 0.50 ? 0.00 0.50 ? 0.00
Node only
All
Yes 0.63 ? 0.10 0.34 ? 0.10 0.42 ? 0.07 0.54 ? 0.03
No 0.51 ? 0.00 0.88 ? 0.03 0.65 ? 0.01 0.51 ? 0.01
All but word
Yes 0.63 ? 0.16 0.40 ? 0.22 0.42 ? 0.19 0.53 ? 0.03
No 0.57 ? 0.04 0.56 ? 0.17 0.55 ? 0.07 0.55 ? 0.03
Parent, node
Parent: all but word
Yes 0.71 ? 0.06 0.21 ? 0.04 0.31 ? 0.05 0.55 ? 0.01
Node: all
All Yes 0.84 ? 0.07 0.11 ? 0.04 0.19 ? 0.06 0.53 ? 0.01
Full graph
Parent: all but word
Yes 0.59 ? 0.06 0.39 ? 0.11 0.46 ? 0.07 0.54 ? 0.03Node: all but word
Children: POS only
Parent: all
Yes 0.67 ? 0.05 0.39 ? 0.08 0.47 ? 0.06 0.59 ? 0.02Node: all
Children: all but word
All
Yes 0.70 ? 0.05 0.35 ? 0.08 0.46 ? 0.07 0.59 ? 0.02
No 0.70 ? 0.03 0.20 ? 0.05 0.36 ? 0.06 0.56 ? 0.01
Table 1: Performance using different feature combinations, including some without enforcing the invariant.
Mean averages and standard deviation for 10 runs.
almost all of the features.
Our non-invariant-respecting baseline unsurpris-
ingly was nearly 50% on all measures. Including the
node-only features dramatically increases recall, less
if we exclude word features. The word features ap-
pear to have an effect on recall just as in the invariant-
respecting case with node-only features. With all
features, precision is dramatically improved, but with
a large cost to recall. However, it underperforms
the equivalent invariant-respecting model in recall,
F-measure, and accuracy.
Though these invariant-violating models are un-
constrained in the way they label the graph, our
invariant-respecting models still outperform them.
A coherent path contains more information than an
incoherent one; it is important to find negating and
intensifying elements in context. Our SRT invariant
allows us to achieve better performance and will be
more useful to downstream tasks.
Finally, it appears that using more factors and lin-
guistic features promotes stability in performance
and decreases sensitivity to the initial setting.
6.3 Manual inspection
One pattern that prominently stood out in the testing
data with the full-graph model was the misclassifica-
tion of flow labels as inert in the vicinity of Stanford
dependency labels such as conj and. These kinds
of labels have high ?fertility?; the labels immediately
following them in the SRT could be a variety of types,
creating potential data sparsity issues.
This problem could be resolved by making some
features transparent to the learner. For example, if
node q has an incoming conj and dependency edge
label, then q?s parent could also be directly connected
to q?s children, as a conjunction should be linguisti-
cally transparent to the status of the children in the
sentiment flow.
There are many fewer incidents of inert labels be-
ing classified as flow. There are paths through an
SRT where a flow candidate word is the ancestor of
an inert candidate word from the set of crowdsourced
candidates. The model sometimes appears to ?over-
shoot? the flow candidate. Considering that recall is
already fairly low, attempts to address this problem
risks making the model too conservative. One poten-
tial solution is to prune or separate paths that contain
multiple flow candidates.
6.3.1 Paths found
We examined the labeling on the held-out testing
data of the best-performing model of the full graph
system with all linguistic features. For example, con-
sider the following highlight group:
But Microsoft?s informal approach may not be
enough as the number of blogs at the company
grows, especially since the line between ?personal?
Weblogs and those done as part of the job can be
hard to distinguish.
In this case, the Turkers decided that ?distinguish?
expressed a negative opinion about blogs, in the sense
674
that something that was difficult to distinguish was
a problem: the modifier ?hard? is what makes it
negative. The system found an entirely flow path that
connected these attributes into a single unit:
Blog:flow prepof????? number:flow nsubj????
grows:flow ccomp????? hard:flow xcomp?????
distinguish:flow
In this path, ?blog? and ?distinguish? are both con-
nected to one another by ?hard?, giving ?distinguish?
its negative spin. There are two non-local dependen-
cies in this example: xcomp, ccomp. Very often,
more than one unique path connects the concept to
the opinion candidate word.
7 Conclusions and future work
In this work, we have applied machine learning to
produce a robust modeling of syntactic structure for
an information extraction application. A solution to
the problem of modeling these structures requires the
development of new techniques that model complex
linguistic relationships in an application-dependent
way. We have shown that we can mine these relation-
ships without being overcome by the data-sparsity
issues that typically stymie learning over complex
linguistic structure.
The limitations on these techniques ultimately find
their root in the difficulty in modeling complex syn-
tactic structures that simultaneously exclude irrel-
evant portions of the structure while maintaining
connected relations. Our technique uses a structure-
labelling scheme that enforces connectedness. En-
forcing connected structure is not only necessary to
produce useful results but also to improve accuracy.
Further performance gains might be possible by en-
riching the feature set. For example, the POS tagset
used by the Stanford parser contains multiple verb
tags that represent different English tenses and num-
bers. For the purpose of sentiment relations, it is
possible that the differences between verb tags are
too small to matter and are causing data sparsity is-
sues. Thus, we could additional features that ?back
off? to general verb tags.
Acknowledgements
This paper is based upon work supported by the
US National Science Foundation under Grant IIS-
0729459. Additional support came from the Cluster
of Excellence ?Multimodal Computing and Innova-
tion?, Germany. Jordan Boyd-Graber is also sup-
ported by US National Science Foundation Grant
NSF grant #1018625 and the Army Research Labora-
tory through ARL Cooperative Agreement W911NF-
09-2-0072. Any opinions, findings, conclusions, or
recommendations expressed are the authors? and do
not necessarily reflect those of the sponsors.
References
Alm, C. O. (2011). Subjective natural language prob-
lems: Motivations, applications, characterizations,
and implications. In ACL (Short Papers).
Bollen, J., Mao, H., and Zeng, X.-J. (2010). Twit-
ter mood predicts the stock market. CoRR,
abs/1010.3003.
Choi, Y., Breck, E., and Cardie, C. (2006). Joint ex-
traction of entities and relations for opinion recog-
nition. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
de Marneffe, M.-C. and Manning, C. D. (2008). The
stanford typed dependencies representation. In
CrossParser ?08: Coling 2008: Proceedings of
the workshop on Cross-Framework and Cross-
Domain Parser Evaluation, Morristown, NJ, USA.
Association for Computational Linguistics.
Finkel, J. R., Grenager, T., and Manning, C. (2005).
Incorporating non-local information into informa-
tion extraction systems by gibbs sampling. In
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Hsueh, P.-Y., Melville, P., and Sindhwani, V. (2009).
Data quality from crowdsourcing: a study of anno-
tation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learn-
ing for Natural Language Processing, HLT ?09,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jakob, N. and Gurevych, I. (2010). Extracting opin-
ion targets in a single and cross-domain setting
with conditional random fields. In EMNLP.
Kessler, J. S., Eckert, M., Clark, L., and Nicolov,
N. (2010). The 2010 ICWSM JDPA sentment
675
corpus for the automotive domain. In 4th Int?l
AAAI Conference on Weblogs and Social Media
Data Workshop Challenge (ICWSM-DWC 2010).
Kim, S.-M. and Hovy, E. (2006). Extracting opinions,
opinion holders, and topics expressed in online
news media text. In SST ?06: Proceedings of the
Workshop on Sentiment and Subjectivity in Text,
pages 1?8, Morristown, NJ, USA. Association for
Computational Linguistics.
Kschischang, F. R., Frey, B. J., and andrea Loeliger,
H. (1998). Factor graphs and the sum-product algo-
rithm. IEEE Transactions on Information Theory,
47:498?519.
Ku?bler, S., McDonald, R., and Nivre, J. (2009). De-
pendency parsing. Synthesis Lectures on Human
Language Technologies, 2(1).
McCallum, A., Schultz, K., and Singh, S. (2009).
Factorie: Probabilistic programming via impera-
tively defined factor graphs. In Neural Information
Processing Systems (NIPS).
Moilanen, K. and Pulman, S. (2007). Sentiment com-
position. In Proceedings of the Recent Advances in
Natural Language Processing International Con-
ference (RANLP-2007), Borovets, Bulgaria.
Nakagawa, T., Inui, K., and Kurohashi, S. (2010). De-
pendency tree-based sentiment classification using
crfs with hidden variables. In HLT-NAACL.
Neal, R. M. (1993). Probabilistic inference using
Markov chain Monte Carlo methods. Technical
Report CRG-TR-93-1, University of Toronto.
Pang, B. and Lee, L. (2008). Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2).
Qiu, G., Liu, B., Bu, J., and Chen, C. (2011). Opin-
ion word expansion and target extraction through
double propagation. Computational linguistics,
37(1):9?27.
Rogers, E. M. (2003). Diffusion of Innovations, 5th
Edition. Free Press.
Ruppenhofer, J., Somasundaran, S., and Wiebe, J.
(2008). Finding the sources and targets of sub-
jective expressions. In Calzolari, N., Choukri, K.,
Maegaard, B., Mariani, J., Odjik, J., Piperidis, S.,
and Tapias, D., editors, Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco. European Lan-
guage Resources Association (ELRA).
Sayeed, A. B., Nguyen, H. C., Meyer, T. J., and
Weinberg, A. (2010). Expresses-an-opinion-about:
using corpus statistics in an information extraction
approach to opinion mining. In Proceedings of the
23rd International Conference on Computational
Linguistics, COLING ?10.
Sayeed, A. B., Rusk, B., Petrov, M., Nguyen, H. C.,
Meyer, T. J., and Weinberg, A. (2011). Crowd-
sourcing syntactic relatedness judgements for opin-
ion mining in the study of information technology
adoption. In Proceedings of the Association for
Computational Linguistics 2011 workshop on Lan-
guage Technology for Cultural Heritage, Social
Sciences, and the Humanities (LaTeCH). Associa-
tion for Computational Linguistics.
Stoyanov, V. and Cardie, C. (2006). Partially su-
pervised coreference resolution for opinion sum-
marization through structured rule learning. In
EMNLP ?06: Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 336?344, Morristown, NJ, USA.
Association for Computational Linguistics.
Tsui, C.-J., Wang, P., Fleischmann, K., Oard, D.,
and Sayeed, A. (2009). Understanding IT innova-
tions by computational analysis of discourse. In
International conference on information systems.
Wick, M., Rohanimanesh, K., Culotta, A., and Mccal-
lum, A. (2009). SampleRank: Learning preference
from atomic gradients. In NIPS WS on Advances
in Ranking.
Wilson, T. and Wiebe, J. (2005). Annotating attribu-
tions and private states. In CorpusAnno ?05: Pro-
ceedings of the Workshop on Frontiers in Corpus
Annotations II, Morristown, NJ, USA. Association
for Computational Linguistics.
Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Rec-
ognizing contextual polarity in phrase-level senti-
ment analysis. In HLT/EMNLP.
676
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 69?77,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Crowdsourcing syntactic relatedness judgements for opinion mining in the
study of information technology adoption
Asad B. Sayeed, Bryan Rusk, Martin Petrov,
Hieu C. Nguyen, Timothy J. Meyer
Department of Computer Science
University of Maryland
College Park, MD 20742 USA
asayeed@cs.umd.edu,brusk@umd.edu,
martin@martinpetrov.com,
{hcnguyen88,tmeyer88}@gmail.com
Amy Weinberg
Center for the Advanced
Study of Language
and Department of Linguistics
University of Maryland
College Park, MD 20742 USA
aweinberg@casl.umd.edu
Abstract
We present an end-to-end pipeline including
a user interface for the production of word-
level annotations for an opinion-mining task
in the information technology (IT) domain.
Our pre-annotation pipeline selects candidate
sentences for annotation using results from a
small amount of trained annotation to bias the
random selection over a large corpus. Our
user interface reduces the need for the user to
understand the ?meaning? of opinion in our
domain context, which is related to commu-
nity reaction. It acts as a preliminary buffer
against low-quality annotators. Finally, our
post-annotation pipeline aggregates responses
and applies a more aggressive quality filter.
We present positive results using two differ-
ent evaluation philosophies and discuss how
our design decisions enabled the collection of
high-quality annotations under subjective and
fine-grained conditions.
1 Introduction
Crowdsourcing permits us to use a bank of anony-
mous workers with unknown skill levels to perform
complex tasks given a simple breakdown of these
tasks with user interface design that hides the full
task complexity. Use of these techniques is growing
in the areas of computational linguistics and infor-
mation retrieval, particularly since these fields now
rely on the collection of large datasets for use in ma-
chine learning. Considering the variety of applica-
tions, a variety of datasets is needed, but trained,
known workers are an expense in principle that must
be furnished for each one. Consequently, crowd-
sourcing offers a way to collect this data cheaply and
quickly (Snow et al, 2008; Sayeed et al, 2010a).
We applied crowdsourcing to perform the fine-
grained annotation of a domain-specific corpus. Our
user interface design and our annotator quality con-
trol process allows these anonymous workers to per-
form a highly subjective task in a manner that cor-
relates their collective understanding of the task to
our own expert judgements about it. The path to
success provides some illustration of the pitfalls in-
herent in opinion annotation. Our task is: domain
and application-specific sentiment classification at
the sub-sentence level?at the word level.
1.1 Opinions
For our purposes, we define opinion mining (some-
times known as sentiment analysis) to be the re-
trieval of a triple {source, target, opinion} (Sayeed
et al, 2010b; Pang and Lee, 2008; Kim and Hovy,
2006) in which the source is the entity that origi-
nated the opinionated language, the target is a men-
tion of the entity or concept that is the opinion?s
topic, and the opinion is a value (possibly a struc-
ture) that reflects some kind of emotional orientation
expressed by the source towards the target.
In much of the recent literature on automatic
opinion mining, opinion is at best a gradient be-
tween positive and negative or a binary classifica-
tion thereof; further complexity affects the reliability
of machine-learning techniques (Koppel and Schler,
2006).
We call opinion mining ?fine-grained? when we
are attempting to retrieve potentially many different
69
{source, target, opinion} triples per document. This
is particularly challenging when there are multiple
triples even at a sentence level.
1.2 Corpus-based social science
Our work is part of a larger collaboration with social
scientists to study the diffusion of information tech-
nology (IT) innovations through society by identify-
ing opinion leaders and IT-relevant opinionated lan-
guage (Rogers, 2003). A key hypothesis is that the
language used by opinion leaders causes groups of
others to encourage the spread of the given IT con-
cept in the market.
Since the goal of our exercise is to ascertain the
correlation between the source?s behaviour and that
of others, then it may be more appropriate to look
at opinion analysis with the view that what we are
attempting to discover are the views of an aggregate
reader who may otherwise have an interest in the IT
concept in question. We thus define an expression of
opinion in the following manner:
A expresses opinion about B if an in-
terested third party C?s actions towards B
may be affected by A?s textually recorded
actions, in a context where actions have
positive or negative weight.
This perspective runs counter to a widespread view
(Ruppenhofer et al, 2008) which has assumed a
treatment of opinionated language as an observation
of a latent ?private state? held by the source. This
definition reflects the relationship of sentiment and
opinion with the study of social impact and market
prediction. We return to the question of how to de-
fine opinion in section 6.2.
1.3 Crowdsourcing in sentiment analysis
Paid crowdsourcing is a relatively new trend in com-
putational linguistics. Work exists at the paragraph
and document level, and it exists for the Twitter and
blog genres (Hsueh et al, 2009).
A key problem in crowdsourcing sentiment analy-
sis is the matter of quality control. A crowdsourced
opinion mining task is an attempt to use untrained
annotators over a task that is inherently very subjec-
tive. It is doubly difficult for specialized domains,
since crowdsourcing platforms have no way of di-
rectly recruiting domain experts.
Hsueh et al (2009) present results in quality con-
trol over snippets of political blog posts in a task
classifying them by sentiment and political align-
ment. They find that they can use a measurement of
annotator noise to eliminate low-quality annotations
at this coarse level by reweighting snippet ambigu-
ity scores with noise scores. We demonstrate that we
can use a similar annotator quality measure alone to
eliminate low-quality annotations on a much finer-
grained task.
1.4 Syntactic relatedness
We have a downstream application for this annota-
tion task which involves acquiring patterns in the
distribution of opinion-bearing words and targets us-
ing machine learning (ML) techniques. In partic-
ular, we want to acquire the syntactic relationships
between opinion-bearing words and within-sentence
targets. Supervised ML techniques require gold
standard data annotated in advance.
The Multi-Perspective Question-Answering
(MPQA) newswire corpus (Wilson and Wiebe,
2005) and the J. D. Power & Associates (JDPA)
automotive review blog post (Kessler et al, 2010)
corpus are appropriate because both contain sub-
sentence annotations of sentiment-bearing language
as text spans. In some cases, they also include links
to within-sentence targets. This is an example of an
MPQA annotation:
That was the moment at which the fabric
of compassion tore, and worlds cracked
apart; when the contrast and conflict of
civilisational values became so great as
to remove any sense of common ground -
even on which to do battle.
The italicized portion is intended to reflect a negative
sentiment about the bolded portion. However, while
it is the case that the whole italicized phrase repre-
sents a negative sentiment, ?remove? appears to rep-
resent far more of the negativity than ?common? and
?ground?. While there are techniques that depend
on access to entire phrases, our project is to identify
sentiment spans at the length of a single word.
2 Data source
Our corpus for this task is a collection of arti-
cles from the IT professional magazine, Information
70
Week, from the years 1991 to 2008. This consists
of 33K articles of varying lengths including news
bulletins, full-length magazine features, and opin-
ion columns. We obtained the articles via an institu-
tional subscription, and reformatted them in XML1.
Certain IT concepts are particularly significant in
the context of the social science application. Our tar-
get list consists of 59 IT innovations and concepts.
The list includes plurals, common variations, and
abbreviations. Examples of IT concepts include ?en-
terprise resource planning? and ?customer relation-
ship management?. To avoid introducing confound-
ing factors into our results, we only include explicit
mentions and omit pronominal coreference.
3 User interface
Our user interface (figure 1) uses a drag-and-drop
process through which workers make decisions
about whether particular highlighted words within
a given sentence reflect an opinion about a particu-
lar mentioned IT concept or innovation. The user
is presented with a sentence from the corpus sur-
rounded by some before and after context. Under-
neath the text are four boxes: ?No effect on opin-
ion? (none), ?Affects opinion positively? (postive),
?Affects opinion negatively? (negative), and ?Can?t
tell? (ambiguous).
The worker must drag each highlighted word in
the sentence into one of the boxes, as appropriate. If
the worker cannot determine the appropriate box for
a particular word, she is expected to drag this to the
ambiguous box. The worker is presented with de-
tailed instructions which also remind her that most
of words in the sentence are not actually likely to be
involved in the expression of an opinion about the
relevant IT concept2. The worker is not permitted
to submit the task without dragging all of the high-
lighted words to one of the boxes. When a word
is dragged to a box, the word in context changes
colour; the worker can change her mind by clicking
an X next to the word in the box.
1We will likely be able to provide a sample of sentence data
annotated by our process as a resource once we work out docu-
mentation and distribution issues.
2We discovered when testing the interface that workers can
feel obliged to find a opinion about the selected IT concept. We
reduced it by explicitly reminding them that most words do not
express a relevant opinion and by placing the none box first.
We used CrowdFlower to manage the task with
Amazon Mechanical Turk as its distribution chan-
nel. We set CrowdFlower to present three sentences
at a time to users. Only users with USA-based IP
addresses were permitted to perform the final task.
4 Procedure
In this section, we discuss the data processing
pipeline (figure 3) through which we select candi-
dates for annotations and the crowdsourcing inter-
face we present to the end user for classifying indi-
vidual words into categories that reflect the effect of
the word on the worker.
4.1 Data preparation
4.1.1 Initial annotation
Two social science undergraduate students were
hired to do annotations on Information Week with
the original intention of doing all the annotations
this way. There was a training period where they an-
notated about 60 documents in sets of 20 in iterative
consultation with one of the authors. Then they were
given 142 documents to annotate simultaneously in
order to assess their agreement after training.
Annotation was performed in Atlas.ti, an anno-
tation tool popular with social science researchers.
It was chosen for its familiarity to the social sci-
entists involved in our project and because of their
stated preference for using tools that would allow
them to share annotations with colleagues. Atlas.ti
has limitations, including the inability to create hier-
archical annotations. We overcame these limitations
using a special notation to connect related annota-
tions. An annotator highlights a sentence that she
believes contains an opinion about a mentioned tar-
get on one of the lists. She then highlights the men-
tion of the target and, furthermore, highlights the in-
dividual words that express the opinion about the tar-
get, using the notation to connect related highlights.
4.1.2 Candidate selection
While the use of trained annotators did not pro-
duce reliable results (section 6.2) in acceptable time
frames, we decided to use the annotations in a pro-
cess for selecting candidate sentences for crowd-
sourcing. All 219 sentences that the annotators se-
lected as having opinions about within-sentence IT
71
Figure 1: A work unit presented in grayscale. ?E-business? is the IT concept and would be highlighted in blue. The
words in question are highlighted in gray background and turn red after they are dragged to the boxes.
concepts were concatenated into a single string and
converted into a TFIDF unit vector.
We then selected all the sentences that contain
IT concept mentions from the entire Information
Week corpus using an OpenNLP 1.4.3 model as
our sentence-splitter. This produced approximately
77K sentences. Every sentence was converted into a
TFIDF unit vector, and we took the cosine similar-
ity of each sentence with the TFIDF vector. We then
ranked the sentences by cosine similarity.
4.1.3 Selecting highlighted words
We ran every sentence through the Stanford
part-of-speech tagger. Words that belonged to
open classes such as adjectives and verbs were se-
lected along with certain closed-class words such as
modals and negation words. These candidate words
were highlighted in the worker interface.
We did not want to force workers to classify every
single word in a sentence, because this would be too
tedious. So we instead randomly grouped the high-
lighted words into non-overlapping sets of six. (Re-
mainders less than five were dropped from the task.)
We call these combinations of sentence, six words,
and target IT concept a ?highlight group? (figure 2).
Each highlight group represents a task unit which
we present to the worker in our crowdsourcing ap-
plication. We generated 1000 highlight groups from
The amount of industry attention paid to this
new class of integration software speaks volumes
about the need to extend the reach of ERP systems.
The amount of industry attention paid to this
new class of integration software speaks volumes
about the need to extend the reach of ERP systems.
Figure 2: Two highlight groups consisting of the
same sentence and concept (ERP) but different non-
overlapping sets of candidate words.
the top-ranked sentences.
4.2 Crowdsourced annotation
4.2.1 Training gold
We used CrowdFlower partly because of its au-
tomated quality control process. The bedrock of
this process is the annotation of a small amount of
gold standard data by the task designers. Crowd-
Flower randomly selects gold-annotated tasks and
presents them to workers amidst other unannotated
tasks. Workers are evaluated by the percentage of
gold-annotated tasks they perform correctly. The re-
sult of a worker performing a task unit is called a
?judgement.?
Workers are initially presented their gold-
annotated tasks without knowing that they are an-
swering a test question. If they get the question
wrong, CrowdFlower presents the correct answer to
72
them along with a reason why their answer was an
error. They are permitted to write back to the task
designer if they disagree with the gold judgement.
This process functions in a manner analogous to
the training of a machine-learning system. Further-
more, it permits CrowdFlower to exclude or reject
low-quality results. Judgements from a worker who
slips below 65% correctness are rated as untrustwor-
thy and not included in the CrowdFlower?s results.
We created training gold in the manner recom-
mended by CrowdFlower. We randomly selected
50 highlight groups from the 1000 mentioned in the
previous section. We ran these examples through
CrowdFlower using the interface we discuss in the
next section. Then we used the CrowdFlower gold
editor to select 30 highlight groups that contained
clear classification decisions where it appeared that
the workers were in relative consensus and where we
agreed with their decision. Of these, we designated
only the clearest-cut classifications as gold, leav-
ing more ambiguous-seeming ones up to the users.
For example, in the second highlight group in 2, we
would designate software and systems as none and
extend as positive in the training gold and the re-
mainder as up to the workers. That would be a ?min-
imum effort? to indicate that the worker understands
the task the way we do.
Unfortunately, CrowdFlower has some limita-
tions in the way it processes the responses to gold?
it is not possible to define a minimum effort pre-
cisely. CrowdFlower?s setting either allow us to pass
workers based on getting at least one item in each
class correct or by placing all items in their correct
classes. The latter is too strict a criterion for an in-
herently subjective task. So we accepted the former.
We instead applied our minimum effort criterion in
some of our experiments as described in section 4.3.
4.2.2 Full run
We randomly selected another 200 highlight
groups and posted them at 12 US cents for each set
of three highlight groups, with at least three Me-
chanical Turk workers seeing each highlight group.
The 30 training gold highlight groups were posted
along with them. Including CrowdFlower and Ama-
zon fees, the total cost was approximately 60 USD.
We permitted only USA-based workers to access the
task. Once initiated, the entire task took approxi-
Figure 3: Schematic view of pipeline.
mately 24 hours to complete.
4.3 Post-processing
4.3.1 Aggregation
Each individual worker?s ambiguous annotations
are converted to none annotations, as the ambigu-
ous box is intended as an outlet for a worker?s un-
certainty, but we choose to interpret anything that
a worker considers too uncertain to be classified
as positive or negative as something that is not
strongly opinionated under our definitions.
Aggregation is performed by majority vote of the
annotators on each word in each highlight group. If
no classification obtains more than 50% for a given
word, the word is dropped as too ambiguous to be
accepted either way as a result. This aggregation
has the effect of smoothing out individual annotator
differences.
4.3.2 Extended quality control
While CrowdFlower provides a first-pass quality
control system for selecting annotators who are do-
ing the task in good faith and with some understand-
ing of the instructions, we wanted particularly to
select annotators who would be more likely to be
consistent on the most obvious cases without overly
constraining them. Even with the same general idea
of our intentions, some amount of variation among
the annotators is unavoidable; how do we then reject
annotations from those workers who pass Crowd-
Flower?s liberal criteria but still do not have an idea
of annotation close enough to ours?
73
Our solution was to score the annotators post hoc
by their accuracy on our minimum-effort training
gold data. Then we progressively dropped the worst
n annotators starting from n = 0 and measured the
quality of the aggregated annotations as per the fol-
lowing section.
5 Results
This task can be interpreted in two different ways:
as an annotation task and as a retrieval system. An-
notator reliability is an issue insofar as it is impor-
tant that the annotations themselves conform to a
predetermined standard. However, for the machine
learning task that is downstream in our processing
pipeline, obtaining a consistent pattern is more im-
portant than conformance to an explicit definition.
We can thus interpret the results as being the out-
put of a system whose computational hardware hap-
pens to be a crowd of humans rather than silicon,
considering that the time of the ?run? is compara-
ble to many automated systems; Amazon Mechani-
cal Turk?s slogan is ?artificial artificial intelligence?
for a reason.
Nevertheless, we evaluated our procedure under
both interpretations by comparing against our own
annotations in order to assess the quality of our col-
lection, aggregation, and filtering process:
1. As an annotation task: we use Cohen?s ?
between the aggregated and filtered data vs.
our annotations in the belief that higher above-
chance agreement would imply that the aggre-
gate annotation reflected collective understand-
ing of our definition of sentiment. Consider-
ing the inherently subjective nature of this task
and the interdependencies inherent in within-
sentence judgements, Cohen?s ? is not a defini-
tive proof of success or failure.
2. As a retrieval task: Relative to our own an-
notations, we use the standard information re-
trieval measures of precision, recall, and F-
measure (harmonic mean) as well as accuracy.
We merge positive and negative annotations
into a single opinion-bearing class and measure
whether we can retrieve opinion-bearing words
while minimizing words that are, in context,
not opinion-bearing relative to the given target.
(We do not merge the classes for agreement-
based evaluation as there was not much over-
lap between positive and negative classifica-
tions.) The particular relative difference be-
tween precision and recall will suggest whether
the workers had a consistent collective under-
standing of the task.
It should be noted that the MPQA and the JDPA do
not report Cohen?s ? for subjective text spans partly
for the reason we suggest above: the difficulty of as-
sessing objective agreement on a task in which sub-
jectivity is inherent and desirable. There is also a
large class imbalance problem. Both these efforts
substitute retrieval-based measures into their assess-
ment of agreement.
We annotated a randomly-selected 30 of the 200
highlight groups on our own. Those 30 had 169
annotated words of which 117 were annotated as
none, 35 as positive, and 17 as negative. The re-
sults of our process are summarized in table 1.
In the 30 highlight groups, there were 155 total
words for which a majority consensus (>50%) was
reached. 48 words were determined by us in our
own annotation to have opinion weight (positive or
negative). There are only 22 annotators who passed
CrowdFlower?s quality control.
The stringent filter on workers based on their ac-
curacy on our minimum-effort gold annotations has
a remarkable effect on the results. As we exclude
workers, the F-measure and the Cohen?s ? appear
to rise, up to a point. By definition, each exclu-
sion raises the threshold score for acceptance. As
we cross the 80% threshold, the performance of the
system drops noticeably, as the smoothing effect of
voting is lost. Opinion-bearing words also reduce
in number as the threshold rises as some highlight
groups simply have no one voting for them. We
achieve our best result in terms of Cohen?s ? on
dropping the 7 lowest workers. We achieve our high-
est precision and accuracy after dropping the 10 low-
est workers.
Between the 7th and 10th underperforming an-
notator, we find that precision starts to exceed re-
call, possibly due to the loss of retrievable words as
some highlight groups lose all their annotators. Lost
words can be recovered in another round of annota-
tion.
74
Workers excluded No. of words lost (of 48) Prec/Rec/F Acc Cohen?s ? Score threshold
(prior polarity) N/A 0.87 / 0.38 / 0.53 0.79 -0.26 N/A
0 0 0.64 / 0.71 / 0.67 0.79 0.48 0.333
1 0 0.64 / 0.71 / 0.67 0.79 0.48 0.476
3 0 0.66 / 0.73 / 0.69 0.80 0.51 0.560
5 0 0.69 / 0.73 / 0.71 0.81 0.53 0.674
7 2 0.81 / 0.76 / 0.79 0.86 0.65 0.714
10 9 0.85 / 0.74 / 0.79 0.88 0.54 0.776
12 11 0.68 / 0.68 / 0.68 0.82 0.20 0.820
Table 1: Results by number of workers excluded from the task. The prior polarity baseline comes from a lexicon by
Wilson et al (2005) that is not specific to the IT domain.
6 Discussion
We have been able to show that crowdsourcing a
very fine-grained, domain-specific sentiment analy-
sis task with a nonstandard, application-specific def-
inition of sentiment is possible with careful user in-
terface design and mutliple layers of quality control.
Our techniques succeed on two different interpreta-
tions of the evaluation measure, and we can reclaim
any lost words by re-running the task. We used an
elaborate processing pipeline before and after anno-
tation in order to accomplish this. In this section, we
discuss some aspects of the pipeline that led to the
success of this technique.
6.1 Quality
There are three major aspects of our procedure that
directly affect the quality of our results: the first-
pass quality control in CrowdFlower, the majority-
vote aggregation, and the stringent post hoc filtering
of workers. These interact in particular ways.
The first-pass quality control interacts with the
stringent filter in that even if it were possible to
have run the stringent filter on CrowdFlower itself,
it would probably not have been a good idea. Al-
though we intended the stringent filter to be a min-
imum effort, it would have rejected workers too
quickly. It is technically possible to implement the
stringent filtering directly without the CrowdFlower
built-in control, but that would have entailed spend-
ing an unpredictable amount more money paying for
additional unwanted annotations from workers.
Furthermore, the majority-vote aggregation re-
quires that there not be too few annotators; our re-
sults show that filtering the workers too aggressively
harms the aggregation?s smoothing effect. The les-
son we take from this is that it can be beneficial to
accept some amount of ?bad? with the ?good? in im-
plementing a very subjective crowdsourcing task.
6.2 Design decisions
Our successful technique for identifying opinionated
words was developed after multiple iterations using
other approaches which did not succeed in them-
selves but produced outputs that were amenable to
refinement, and so these techniques became part of
a larger pipeline. However, the reasons why they did
not succeed on their own are illustrative of some of
the challenges in both fine-grained domain-specific
opinion annotation and in annotation via crowd-
sourcing under highly subjective conditions.
6.2.1 Direct annotation
We originally intended to stop with the trained an-
notation we described in 4.1.1, but collecting opin-
ionated sentences in this corpus turned out to be very
slow. Despite repeated training rounds, the annota-
tors had a tendency to miss a large number of sen-
tences that the authors found to be relevant. On dis-
cussion with the annotators, it turned out that the
variable length of the articles made it easy to miss
relevant sentences, particularly in the long feature
articles likely to contain opinionated language?a
kind of ?needle-in-a-haystack? problem.
Even worse, however, the annotators were vari-
ably conservative about what constituted an opinion.
One annotator produced far fewer annotations than
the other one?but the majority of her annotations
were also annotated by the other one. Discussion
with the annotators revealed that one of them simply
had a tighter definition of what constituted an opin-
ion. Attempts to define opinion explicitly for them
still led to a situations in which one was far more
conservative than the other.
75
6.2.2 Cascaded crowdsourcing technique
Insofar as we were looking for training data for
use in downstream machine learning techniques,
getting uniform sentence-by-sentence coverage of
the corpus was not necessary. There are 77K sen-
tences in this corpus which mention the relevant IT
concepts; even if only a fraction of them mention the
IT concepts with opinionated language, we would
still have a potentially rich source of training data.
Nevertheless the direct annotation with trained
annotators provided data for selecting candidate sen-
tences for a more rapid annotation. We used the
process in section 4.1.2 and chose the top-ranked
sentences. Then we constructed a task design that
divided the annotation into two phases. In the first
phase, for each candidate sentence, we ask the anno-
tator whether or not the sentence contains opinion-
ated language about the mentioned IT concept. (We
permit ?unsure? answers.)
In the second phase, for each candidate sentence
for which a majority vote of annotators decided that
the sentence contained a relevant opinion, we run
a second task asking whether particular words (se-
lected as per section 4.1.3) were words directly in-
volved in the expression of the opinion.
We tested this process with the 90 top-ranked
sentences. Four individuals in our laboratory an-
swered the ?yes/no/unsure? question of the first
phase. However, when we took their pairwise Co-
hen?s ? score, no two got more than approximately
0.4. We also took majority votes of each subset of
three annotators and found the Cohen?s ? between
them and the fourth. The highest score was 0.7, but
the score was not stable, and we could not trust the
results enough to move onto the second phase.
We also ran this first phase through Amazon Me-
chanical Turk. It turned out that it was far too easy
to cheat on this yes/no question, and some workers
simply answered ?yes? or ?no? all the time. Agree-
ment scores of a Turker majority vote vs. one of the
authors turned out to yield a Cohen?s ? of 0.05?
completely unacceptable.
Discussion with the in-laboratory annotators sug-
gested the roots of the problem: it was the same
problem as with the direct Atlas.ti annotation we re-
ported in the previous section. It was very difficult
for them to agree on what it meant for a sentence to
contain an opinion expressed about a particular con-
cept. Opinions about the nature of opinion ranged
from very ?conservative? to very ?liberal.? Even
explicit definition with examples led annotators to
reach very different conclusions. Furthermore, the
longer the annotators thought about it, the more con-
fused and uncertain they were about the criterion.
What is an opinion can itself be a matter of opin-
ion. It became clear that without very tight review
of annotation and careful task design, asking users
an explicit yes/no question about whether a particu-
lar concept has a particular opinion mentioned in a
particular sentence has the potential to induce over-
thinking by annotators, despite our variations on the
task. The difficulty may also lead to a tendency to
cheat. Crowdsourcing allows us to make use of non-
expert labour on difficult tasks if we can break the
tasks down into simple questions and aggregate non-
expert responses, but we needed a somewhat more
complex task design in order to eliminate the diffi-
culty of the task and the tendency to cheat.
7 Future work
Foremost among the avenues for future work is ex-
perimentation with other vote aggregration and post
hoc filtering schemes. For example, one type of ex-
periment could be the reweighting of votes by an-
notator quality rather than the wholesale dropping
of annotators. Another could involve the use of
general-purpose sentiment analysis lexica to bias the
vote aggregation in the manner of work in sentiment
domain transfer (Tan et al, 2007).
This work also points to the potential for crowd-
sourcing in computational linguistics applications
beyond opinion mining. Our task is a sentiment-
specific instance of a large class of syntactic relat-
edness problems that may suitable for crowdsourc-
ing. One practical application would be in obtaining
training data for coreference detection. Another one
may be in the establishment of empirical support for
theories about syntactic structure.
Acknowledgements
This paper is based on work supported by the Na-
tional Science Foundation under grant IIS-0729459.
76
References
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: a study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, HLT ?09, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Jason S. Kessler, Miriam Eckert, Lyndsay Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA sent-
ment corpus for the automotive domain. In 4th Int?l
AAAI Conference on Weblogs and Social Media Data
Workshop Challenge (ICWSM-DWC 2010).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In SST ?06: Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, pages 1?8,
Morristown, NJ, USA. Association for Computational
Linguistics.
Moshe Koppel and Jonathan Schler. 2006. The im-
portance of neutral examples for learning sentiment.
Computational Intelligence, 22(2).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2).
Everett M. Rogers. 2003. Diffusion of Innovations, 5th
Edition. Free Press.
Josef Ruppenhofer, Swapna Somasundaran, and Janyce
Wiebe. 2008. Finding the sources and targets of
subjective expressions. In Nicoletta Calzolari, Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik,
Stelios Piperidis, and Daniel Tapias, editors, Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08), Marrakech, Morocco. Eu-
ropean Language Resources Association (ELRA).
Asad B. Sayeed, Timothy J. Meyer, Hieu C. Nguyen,
Olivia Buzek, and Amy Weinberg. 2010a. Crowd-
sourcing the evaluation of a domain-adapted named
entity recognition system. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Asad B. Sayeed, Hieu C. Nguyen, Timothy J. Meyer, and
Amy Weinberg. 2010b. Expresses-an-opinion-about:
using corpus statistics in an information extraction ap-
proach to opinion mining. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, COLING ?10.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP 2008.
Songbo Tan, Gaowei Wu, Huifeng Tang, and Xueqi
Cheng. 2007. A novel scheme for domain-transfer
problem in the context of sentiment analysis. In Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
CIKM ?07, New York, NY, USA.
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In CorpusAnno ?05:
Proceedings of the Workshop on Frontiers in Corpus
Annotations II, Morristown, NJ, USA. Association for
Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP.
77
