Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 93?96,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Structural Information for Identifying Similar Chinese Characters 
 
 
Chao-Lin Liu Jen-Hsiang Lin 
Department of Computer Science, National Chengchi University, Taipei 11605, Taiwan 
{chaolin, g9429}@cs.nccu.edu.tw 
 
 
Abstract 
Chinese characters that are similar in their 
pronunciations or in their internal structures 
are useful for computer-assisted language 
learning and for psycholinguistic studies. Al-
though it is possible for us to employ image-
based methods to identify visually similar 
characters, the resulting computational costs 
can be very high. We propose methods for 
identifying visually similar Chinese characters 
by adopting and extending the basic concepts 
of a proven Chinese input method--Cangjie. 
We present the methods, illustrate how they 
work, and discuss their weakness in this paper. 
1 Introduction 
A Chinese sentence consists of a sequence of char-
acters that are not separated by spaces. The func-
tion of a Chinese character is not exactly the same 
as the function of an English word. Normally, two 
or more Chinese characters form a Chinese word to 
carry a meaning, although there are Chinese words 
that contain only one Chinese character. For in-
stance, a translation for ?conference? is ????? 
and a translation for ?go? is ???. Here ????? 
is a word formed by three characters, and ??? is a 
word with only one character. 
Just like that there are English words that are 
spelled similarly, there are Chinese characters that 
are pronounced or written alike. For instance, in 
English, the sentence ?John plays an important roll 
in this event.? contains an incorrect word. We 
should replace ?roll? with ?role?. In Chinese, the 
sentence ????????????? contains an 
incorrect word. We should replace ???? (a place 
for taking examinations) with ???? (a market). 
These two words have the same pronunciation, 
shi(4) chang(3) ?, and both represent locations. The 
sentence ????????????? also con-
                                                          
? We use Arabic digits to denote the four tones in Mandarin. 
tains an error, and we need to replace ???? with 
????. ???? is considered an incorrect word, 
but can be confused with ???? because the first 
characters in these words look similar. 
Characters that are similar in their appear-
ances or in their pronunciations are useful for 
computer-assisted language learning (cf. Burstein 
& Leacock, 2005). When preparing test items for 
testing students? knowledge about correct words in 
a computer-assisted environment, a teacher pro-
vides a sentence which contains the character that 
will be replaced by an incorrect character. The 
teacher needs to specify the answer character, and 
the software will provide two types of incorrect 
characters which the teachers will use as distracters 
in the test items. The first type includes characters 
that look similar to the answer character, and the 
second includes characters that have the same or 
similar pronunciations with the answer character. 
Similar characters are also useful for studies 
in Psycholinguistics. Yeh and Li (2002) studied 
how similar characters influenced the judgments 
made by skilled readers of Chinese. Taft, Zhu, and 
Peng (1999) investigated the effects of positions of 
radicals on subjects? lexical decisions and naming 
responses. Computer programs that can automati-
cally provide similar characters are thus potentially 
helpful for designing related experiments. 
2 Identifying Similar Characters with In-
formation about the Internal Structures 
We present some similar Chinese characters in the 
first subsection, illustrate how we encode Chinese 
characters in the second subsection, elaborate how 
we improve the current encoding method to facili-
tate the identification of similar characters in the 
third subsection, and discuss the weakness of our 
current approach in the last subsection. 
2.1 Examples of Similar Chinese Characters 
We show three categories of confusing Chinese 
characters in Figures 1, 2, and 3. Groups of similar 
93
characters are separated by spaces in these figures. 
In Figure 1, characters in each group differ at the 
stroke level. Similar characters in every group in 
the first row in Figure 2 share a common part, but 
the shared part is not the radical of these characters. 
Similar characters in every group in the second 
row in Figure 2 share a common part, which is the 
radical of these characters. Similar characters in 
every group in Figure 2 have different pronuncia-
tions. We show six groups of homophones that 
also share a component in Figure 3. Characters that 
are similar in both pronunciations and internal 
structures are most confusing to new learners. 
It is not difficult to list all of those characters 
that have the same or similar pronunciations, e.g., 
???? and ????, if we have a machine readable 
lexicon that provides information about pronuncia-
tions of characters and when we ignore special pat-
terns for tone sandhi in Chinese (Chen, 2000).  
In contrast, it is relatively difficult to find 
characters that are written in similar ways, e.g., 
??? with ???, in an efficient way. It is intriguing 
to resort to image processing methods to find such 
structurally similar words, but the computational 
costs can be very high, considering that there can 
be tens of thousands of Chinese characters. There 
are more than 22000 different characters in large 
corpus of Chinese documents (Juang et al, 2005), 
so directly computing the similarity between im-
ages of these characters demands a lot of computa-
tion. There can be more than 4.9 billion 
combinations of character pairs. The Ministry of 
Education in Taiwan suggests that about 5000 
characters are needed for ordinary usage. In this 
case, there are about 25 million pairs. 
The quantity of combinations is just one of 
the bottlenecks. We may have to shift the positions 
of the characters ?appropriately? to find the com-
mon part of a character pair. The appropriateness 
for shifting characters is not easy to define, making 
the image-based method less directly useful; for 
instance, the common part of the characters in the 
right group in the second row in Figure 3 appears 
in different places in the characters. 
Lexicographers employ radicals of Chinese 
characters to organize Chinese characters into sec-
tions in dictionaries. Hence, the information should 
be useful. The groups in the second row in Figure 
2 show some examples. The shared components in 
these groups are radicals of the characters, so we 
can find the characters of the same group in the 
same section in a Chinese dictionary. However, 
information about radicals as they are defined by 
the lexicographers is not sufficient. The groups of 
characters shown in the first row in Figure 2 have 
shared components. Nevertheless, the shared com-
ponents are not considered as radicals, so the char-
acters, e.g., ???and ???, are listed in different 
sections in the dictionary.   
2.2 Encoding the Chinese Characters 
The Cangjie? method is one of the most popular 
methods for people to enter Chinese into com-
puters. The designer of the Cangjie method, Mr. 
Bong-Foo Chu, selected a set of 24 basic elements 
in Chinese characters, and proposed a set of rules 
to decompose Chinese characters into elements 
that belong to this set of building blocks (Chu, 
2008). Hence, it is possible to define the similarity 
between two Chinese characters based on the simi-
larity between their Cangjie codes.  
Table 1, not counting the first row, has three 
                                                          
? http://en.wikipedia.org/wiki/Cangjie_method 
????? ??? ????
?? ?? ?? ?? ?? ??
 
Figure 1. Some similar Chinese characters 
?? ?? ?? ?? ?? ??
?? ???? ??? ????  
Figure 2. Some similar Chinese characters that have 
different pronunciations 
??? ??? ??? ???
??? ?????  
Figure 3. Homophones with a shared component
 Cangjie Codes  Cangjie Codes
? ?? ? ? 
? ??? ? ?? 
? ??? ?  ??? 
? ?? ? ?? 
? ????? ? ???? 
? ????? ? ????? 
? ???? ? ???? 
? ???? ? ???? 
? ?? ? ?? 
? ??? ? ??? 
? ????? ? ????? 
? ???? ? ???? 
? ????? ? ???? 
? ???? ? ??? 
? ???? ? ???? 
? ???? ? ???? 
? ????? ? ???? 
Table 1. Cangjie codes for some characters
94
sections, each showing the Cangjie codes for some 
characters in Figures 1, 2, and 3. Every Chinese 
character is decomposed into an ordered sequence 
of elements. (We will find that a subsequence of 
these elements comes from a major component of a 
character, shortly.) Evidently, computing the num-
ber of shared elements provides a viable way to 
determine ?visually similar? characters for charac-
ters that appeared in Figure 2 and Figure 3. For 
instance, we can tell that ??? and ??? are similar 
because their Cangjie codes share ?????, which 
in fact represent ???.  
Unfortunately, the Cangjie codes do not ap-
pear to be as helpful for identifying the similarities 
between characters that differ subtly at the stroke 
level, e.g., ?????? and other characters listed 
in Figure 1. There are special rules for decompos-
ing these relatively basic characters in the Cangjie 
method, and these special encodings make the re-
sulting codes less useful for our tasks. 
The Cangjie codes for characters that contain 
multiple components were intentionally simplified 
to allow users to input Chinese characters more 
efficiently. The longest Cangjie code for any Chi-
nese character contains no more than five elements. 
In the Cangjie codes for ??? and ???, we see ??
??? for the component ???, but this component 
is represented only by ???? in the Cangjie codes 
for ??? and ???. The simplification makes it 
relatively harder to identify visually similar charac-
ters by comparing the actual Cangjie codes.  
2.3 Engineering the Original Cangjie Codes 
Although useful for the sake of designing input 
method, the simplification of Cangjie codes causes 
difficulties when we use the codes to find similar 
characters. Hence, we choose to use the complete 
codes for the components in our database. For in-
stance, in our database, the codes for ???, ???, 
???, ???, and ??? are, respectively, ??????, 
???????, ????????, ???????
???, and ????????.  
The knowledge about the graphical structures 
of the Chinese characters (cf. Juang et al, 2005; 
Lee, 2008) can be instrumental as well. Consider 
the examples in Figure 2. Some characters can be 
decomposed vertically; e.g., ??? can be split into 
two smaller components, i.e., ??? and ???. Some 
characters can be decomposed horizontally; e.g., 
??? is consisted of ??? and ???. Some have 
enclosing components; e.g., ??? is enclosed in 
??? in ???. Hence, we can consider the locations 
of the components as well as the number of shared 
components in determining the similarity between 
characters. 
Figure 4 illustrates possible layouts of the 
components in Chinese characters that were 
adopted by the Cangjie method (cf. Lee, 2008). A 
sample character is placed below each of these 
layouts. A box in a layout indicates a component in 
a character, and there can be at most three compo-
nents in a character.  We use digits to indicate the 
ordering the components. Notice that, in the sec-
ond row, there are two boxes in the second to the 
rightmost layout. A larger box contains a smaller 
one. There are three boxes in the rightmost layout, 
and two smaller boxes are inside the outer box. 
Due to space limits, we do not show ?1? for this 
outer box. 
After recovering the simplified Cangjie code 
for a character, we can associate the character with 
a tag that indicates the overall layout of its compo-
nents, and separate the code sequence of the char-
acter according to the layout of its components. 
Hence, the information about a character includes 
the tag for its layout and between one to three se-
quences of code elements. Table 2 shows the anno-
? ? ??
? ? ? ??
1 1 2 1 2 3
1
2 3 3
2
1
1
2
3
2
2
1
1
2
3
Figure 4. Arrangements of components in Chinese 
 Layout Part 1 Part 2 Part 3
? 1 ????   
? 2 ?? ??  
? 3 ? ?? ? 
? 4 ???? ??? ?? 
? 5 ?? ?  
? 6 ? ? ? 
? 7 ? ?? ? 
? 8 ? ?  
? 9 ? ? ?? 
? 2 ???? ????  
? 2 ?? ????  
? 5 ? ???  
? 9 ? ? ???
? 2 ? ??  
? 5 ??? ?  
? 6 ? ? ?? 
Table 2. Annotated and expanded code
95
tated and expanded codes of the sample characters 
in Figure 4 and the codes for some characters that 
we will discuss. The layouts are numbered from 
left to right and from top to bottom in Figure 4. 
Elements that do not belong to the original Canjie 
codes of the characters are shown in smaller font.  
Recovering the elements that were dropped 
out by the Cangjie method and organizing the sub-
sequences of elements into parts facilitate the iden-
tification of similar characters. It is now easier to 
find that the character (?) that is represented by 
?????? and ?????? looks similar to the 
character (?) that is represented by ???? and 
?????? in our database than using their origi-
nal Cangjie codes in Table 1. Checking the codes 
for ??? and ??? in Table 1 and Table 2 will offer 
an additional support for our design decisions. 
In the worst case, we have to compare nine 
pairs of code sequences for two characters that 
both have three components. Since we do not sim-
plify codes for components and all components 
have no more than five elements, conducting the 
comparisons operations are simple. 
2.4 Drawbacks of Using the Cangjie Codes 
Using the Cangjie codes as the basis for comparing 
the similarity between characters introduces some 
potential problems.  
It appears that the Cangjie codes for some 
characters, particular those simple ones, were not 
assigned without ambiguous principles. Relying on 
Cangjie codes to compute the similarity between 
such characters can be difficult. For instance, ??? 
uses the fifth layout, but ??? uses the first layout 
in Figure 4. The first section in Table 1 shows the 
Cangjie codes for some character pairs that are dif-
ficult to compare.  
Due to the design of the Cangjie codes, there 
can be at most one component at the left hand side 
and at most one component at the top in the layouts. 
The last three entries in Table 2 provide an exam-
ple for these constraints. As a standalone character, 
??? uses the second layout. Like the standalone 
???, the ??? in ??? was divided into two parts. 
However, in ???,  ??? is treated as an individual 
component because it is on top of ???. Similar 
problems may occur elsewhere, e.g., ???? and 
????. There are also some exceptional cases; e.g., 
??? uses the sixth layout, but ??? uses the fifth 
layout. 
3 Concluding Remarks 
We adopt the Cangjie alphabet to encode Chinese 
characters, but choose not to simplify the code se-
quences, and annotate the characters with the lay-
out information of their components. The resulting 
method is not perfect, but allows us to find visually 
similar characters more efficient than employing 
the image-based methods.  
Trying to find conceptually similar but con-
textually inappropriate characters should be a natu-
ral step after being able to find characters that have 
similar pronunciations and that are visually similar. 
Acknowledgments 
Work reported in this paper was supported in part 
by the plan NSC-95-2221-E-004-013-MY2 from 
the National Science Council and in part by the 
plan ATU-NCCU-96H061 from the Ministry of 
Education of Taiwan. 
References  
Jill Burstein and Claudia Leacock. editors. 2005. Pro-
ceedings of the Second Workshop on Building Educa-
tional Applications Using NLP, ACL. 
Matthew Y. Chen. 2000. Tone Sandhi: Patterns across 
Chinese Dialects. (Cambridge. Studies in Linguistics 
92.) Cambridge: Cambridge University Press. 
Bong-Foo Chu. 2008. Handbook of the Fifth Generation 
of the Cangjie Input Method, web version, available 
at http://www.cbflabs.com/book/ocj5/ocj5/index.html. 
Last visited on 14 Mar. 2008. 
Hsiang Lee. 2008. Cangjie Input Methods in 30 Days, 
http://input.foruto.com/cjdict/Search_1.php, Foruto 
Company, Hong Kong. Last visited on 14 Mar. 2008. 
Derming Juang, Jenq-Haur Wang, Chen-Yu Lai, Ching-
Chun Hsieh, Lee-Feng Chien, and Jan-Ming Ho. 
2005. Resolving the unencoded character problem for 
Chinese digital libraries. Proceedings of the Fifth 
ACM/IEEE Joint Conference on Digital Libraries, 
311?319. 
Marcus Taft, Xiaoping Zhu, and Danling Peng. 1999. 
Positional specificity of radicals in Chinese character 
recognition, Journal of Memory and Language, 40, 
498?519. 
Su-Ling Yeh and Jing-Ling Li. 2002. Role of structure 
and component in judgments of visual similarity of 
Chinese characters, Journal of Experimental Psy-
chology: Human Perception and Performance, 28(4), 
933?947. 
 
96
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 25?28,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Capturing Errors in Written Chinese Words 
Chao-Lin Liu1 Kan-Wen Tien2 Min-Hua Lai3 Yi-Hsuan Chuang4 Shih-Hung Wu5
1-4National Chengchi University, 5Chaoyang University of Technology, Taiwan 
{1chaolin, 296753027, 395753023, 494703036}@nccu.edu.tw, 5shwu@cyut.edu.tw 
 
Abstract 
A collection of 3208 reported errors of Chinese 
words were analyzed. Among which, 7.2% in-
volved rarely used character, and 98.4% were 
assigned common classifications of their causes 
by human subjects. In particular, 80% of the er-
rors observed in writings of middle school stu-
dents were related to the pronunciations and 
30% were related to the compositions of words. 
Experimental results show that using intuitive 
Web-based statistics helped us capture only 
about 75% of these errors. In a related task, the 
Web-based statistics are useful for recommend-
ing incorrect characters for composing test items 
for "incorrect character identification" tests 
about 93% of the time. 
1 Introduction 
Incorrect writings in Chinese are related to our under-
standing of the cognitive process of reading Chinese 
(e.g., Leck et al, 1995), to our understanding of why 
people produce incorrect characters and our offering 
corresponding remedies (e.g., Law et al, 2005), and 
to building an environment for assisting the prepara-
tion of test items for assessing students? knowledge of 
Chinese characters (e.g., Liu and Lin, 2008). 
Chinese characters are composed of smaller parts 
that can carry phonological and/or semantic informa-
tion. A Chinese word is formed by Chinese characters. 
For example, ??? (Singapore) is a word that con-
tains three Chinese characters. The left (?) and the 
right (?) part of ?, respectively, carry semantic and 
phonological information. Evidences show that pro-
duction of incorrect characters are related to either 
phonological or the semantic aspect of the characters. 
In this study, we investigate several issues that are 
related to incorrect characters in Chinese words. In 
Section 2, we present the sources of the reported er-
rors. In Section 3, we analyze the causes of the ob-
served errors. In Section 4, we explore the effective-
ness of relying on Web-based statistics to correct the 
errors. The current results are encouraging but de-
mand further improvements. In Section 5, we employ 
Web-based statistics in the process of assisting teach-
ers to prepare test items for assessing students? 
knowledge of Chinese characters. Experimental re-
sults showed that our method outperformed the one 
reported in (Liu and Lin, 2008), and captured the best 
candidates for incorrect characters 93% of the time. 
2 Data Sources 
We obtained data from three major sources. A list that 
contains 5401 characters that have been believed to be 
sufficient for everyday lives was obtained from the 
Ministry of Education (MOE) of Taiwan, and we call 
the first list the Clist, henceforth. We have two lists of 
words, and each word is accompanied by an incorrect 
way to write certain words. The first list is from a 
book published by MOE (MOE, 1996). The MOE 
provided the correct words and specified the incorrect 
characters which were mistakenly used to replace the 
correct characters in the correct words. The second 
list was collected, in 2008, from the written essays of 
students of the seventh and the eighth grades in a 
middle school in Taipei. The incorrect words were 
entered into computers based on students? writings, 
ignoring those characters that did not actually exist 
and could not be entered.  
We will call the first list of incorrect words the 
Elist, and the second the Jlist from now on. Elist and 
Jlist contain, respectively, 1490 and 1718 entries. 
Each of these entries contains a correct word and the 
incorrect character. Hence, we can reconstruct the 
incorrect words easily. Two or more different ways to 
incorrectly write the same words were listed in differ-
ent entries and considered as two entries for simplic-
ity of presentation. 
3 Error Analysis of Written Words 
Two subjects, who are native speakers of Chinese and 
are graduate students in Computer Science, examined 
Elist and Jlist and categorized the causes of errors. 
They compared the incorrect characters with the cor-
rect characters to determine whether the errors were 
pronunciation-related or semantic-related. Referring 
to an error as being ?semantic-related? is ambiguous. 
Two characters might not contain the same semantic 
part, but are still semantically related. In this study, 
we have not considered this factor. For this reason we 
refer to the errors that are related to the sharing of 
semantic parts in characters as composition-related. 
It is interesting to learn that native speakers had a 
high consensus about the causes for the observed er-
rors, but they did not always agree. Hence, we studied 
the errors that the two subjects had agreed categoriza-
tions. Among the 1490 and 1718 words in Elist and 
Jlist, respectively, the two human subjects had con-
sensus over causes of 1441 and 1583 errors.  
The statistics changed when we disregarded errors 
that involved characters not included in Clist. An er-
ror would be ignored if either the correct or the incor-
rect character did not belong to the Clist. It is possible 
for students to write such rarely used characters in an 
incorrect word just by coincidence. 
After ignoring the rare characters, there were 1333 
and 1645 words in Elist and Jlist, respectively. The 
subjects had consensus over the categories for 1285 
25
and 1515 errors in Elist and Jlist, respectively.  
Table 1 shows the percentages of five categories of 
errors: C for the composition-related errors, P for the 
pronunciation-related errors, C&P for the intersection 
of C and P, NE for those errors that belonged to nei-
ther C nor P, and D for those errors that the subjects 
disagreed on the error categories. There were, respec-
tively, 505 composition-related and 1314 pronuncia-
tion-related errors in Jlist, so we see 30.70% 
(=505/1645) and 79.88% (=1314/1645) in the table. 
Notice that C&P represents the intersection of C and 
P, so we have to deduct C&P from the sum of C, P, 
NE, and D to find the total probability, namely 1. 
It is worthwhile to discuss the implication of the 
statistics in Table 1. For the Jlist, similarity between 
pronunciations accounted for nearly 80% of the errors, 
and the ratio for the errors that are related to composi-
tions and pronunciations is 1:2.6. In contrast, for the 
Elist, the corresponding ratio is almost 1:1. The Jlist 
and Elist differed significantly in the ratios of the er-
ror types. It was assumed that the dominance of pro-
nunciation-related errors in electronic documents was 
a result of the popularity of entering Chinese with 
pronunciation-based methods. The ratio for the Jlist 
challenges this popular belief, and indicates that even 
though the errors occurred during a writing process, 
rather than typing on computers, students still pro-
duced more pronunciation-related errors than compo-
sition-related errors. Distribution over error types is 
not as related to input method as one may have be-
lieved. Nevertheless, the observation might still be a 
result of students being so used to entering Chinese 
text with pronunciation-based method that the organi-
zation of their mental lexicons is also pronunciation 
related. The ratio for the Elist suggests that editors of 
the MOE book may have chosen the examples with a 
special viewpoint in their minds ? balancing the errors 
due to pronunciation and composition. 
4 Reliability of Web-based Statistics  
In this section, we examine the effectiveness of using 
Web-based statistics to differentiate correct and incor-
rect characters. The abundant text material on the 
Internet gives people to treat the Web as a corpus (e.g., 
webascorpus.org). When we send a query to Google, 
we will be informed of the number of pages (NOPs) 
that possibly contain relevant information. If we put 
the query terms in quotation marks, we should find 
the web pages that literally contain the query terms. 
Hence, it is possible for us to compare the NOPs for 
two competing phrases for guessing the correct way 
of writing. At the time of this writing, Google found 
107000 and 3220 pages, respectively, for ?strong tea? 
and ?powerful tea?. (When conducting such advanced 
searches with Google, the quotation marks are needed 
to ensure the adjacency of individual words.) Hence, 
?strong? appears to be a better choice to go with ?tea?. 
How does this strategy serve for learners of Chinese? 
We verified this strategy by sending the words in 
both the Elist and the Jlist to Google to find the NOPs. 
We can retrieve the NOPs from the documents re-
turned by Google, and compare the NOPs for the cor-
rect and the incorrect words to evaluate the strategy. 
Again, we focused on those in the 5401 words that the 
human subjects had consensus about their error types. 
Recall that we have 1285 and 1515 such words in 
Elist and Jlist, respectively. As the information avail-
able on the Web changes all the time, we also have to 
note that our experiments were conducted during the 
first half of March 2009. The queries were submitted 
at reasonable time intervals to avoid Google?s treating 
our programs as malicious attackers. 
Table 2 shows the results of our investigation. We 
considered that we had a correct result when we found 
that the NOP for the correct word larger than the NOP 
for the incorrect word. If the NOPs were equal, we 
recorded an ambiguous result; and when the NOP for 
the incorrect word is larger, we recorded an incorrect 
event. We use ?C?, ?A?, and ?I? to denote ?correct?, 
?ambiguous?, and ?incorrect? events in Table 2.  
The column headings of Table 2 show the setting 
of the searches with Google and the set of words that 
were used in the experiments. We asked Google to 
look for information from web pages that were en-
coded in traditional Chinese (denoted Trad). We 
could add another restriction on the source of infor-
mation by asking Google to inspect web pages from 
machines in Taiwan (denoted Twn+Trad). We were 
not sure how Google determined the languages and 
locations of the information sources, but chose to trust 
Google. The headings ?Comp? and ?Pron? indicate 
whether the words whose error types were composi-
tion and pronunciation-related, respectively.  
Table 2 shows eight distributions, providing ex-
perimental results that we observed under different 
settings. The distribution printed in bold face showed 
that, when we gathered information from sources that 
were encoded in traditional Chinese, we found the 
correct words 73.12% of the time for words whose 
error types were related to composition in Elist. Under 
the same experimental setting, we could not judge the 
correct word 4.58% of the time, and would have cho-
sen an incorrect word 22.30% of the time. 
Statistics in Table 2 indicate that web statistics is 
not a very reliable factor to judge the correct words. 
The average of the eight numbers in the ?C? rows is 
only 71.54% and the best sample is 76.59%, suggest-
Table 2. Reliability of Web-based statistics 
Trad Twn+Trad  
Comp Pron Comp Pron 
C 73.12% 73.80% 69.92% 68.72%
A 4.58% 3.76% 3.83% 3.76%
E
list 
I 22.30% 22.44% 26.25% 27.52%
C 76.59% 74.98% 69.34% 65.87%
A 2.26% 3.97% 2.47% 5.01%
Jlist 
I 21.15% 21.05% 28.19% 29.12%
Table 1. Error analysis for Elist and Jlist 
 C P C&P NE D 
Elist 66.09% 67.21% 37.13% 0.23% 3.60%
Jlist 30.70% 79.88% 20.91% 2.43% 7.90%
26
ing that we did not find the correct words frequently. 
We would made incorrect judgments 24.75% of the 
time. The statistics also show that it is almost equally 
difficult to find correct words for errors that are com-
position and pronunciation related. In addition, the 
statistics reveal that choosing more features in the 
advanced search affected the final results. Using 
?Trad? offered better results in our experiments than 
using ?Twn+Trad?. This observation may arouse a 
perhaps controversial argument. Although Taiwan has 
proclaimed to be the major region to use traditional 
Chinese, their web pages might not have used as ac-
curate Chinese as web pages located in other regions. 
We have analyzed the reasons for why using Web-
based statistics did not find the correct words. Fre-
quencies might not have been a good factor to deter-
mine the correctness of Chinese. However, the myriad 
amount of data on the Web should have provided a 
better performance. Google?s rephrasing our submit-
ted queries is an important factor, and, in other cases, 
incorrect words were more commonly used. 
5 Facilitating Test Item Authoring 
Incorrect character correction is a very popular type of 
test in Taiwan. There are simple test items for young 
children, and there are very challenging test items for 
the competitions among adults. Finding an attractive 
incorrect character to replace a correct character to 
form a test item is a key step in authoring test items.  
We have been trying to build a software environ-
ment for assisting the authoring of test items for in-
correct character correction (Liu and Lin, 2008, Liu et 
al., 2009). It should be easy to find a lexicon that con-
tains pronunciation information about Chinese charac-
ters. In contrast, it might not be easy to find visually 
similar Chinese characters with computational meth-
ods. We expanded the original Cangjie codes (OCC), 
and employed the expanded Cangjie codes (ECC) to 
find visually similar characters (Liu and Lin, 2008).  
With a lexicon, we can find characters that can be 
pronounced in a particular way. However, this is not 
enough for our goal. We observed that there were 
different symptoms when people used incorrect char-
acters that are related to their pronunciations. They 
may use characters that could be pronounced exactly 
the same as the correct characters. They may also use 
characters that have the same pronunciation and dif-
ferent tones with the correct character. Although rela-
tively infrequently, people may use characters whose 
pronunciations are similar to but different from the 
pronunciation of the correct character.  
As Liu and Lin (2008) reported, replacing OCC 
with ECC to find visually similar characters could 
increase the chances to find similar characters. Yet, it 
was not clear as to which components of a character 
should use ECC. 
5.1 Formalizing the Extended Cangjie Codes 
We analyzed the OCCs for all the words in Clist to 
determine the list of basic components. We treated a 
Cangjie basic symbol as if it was a word, and com-
puted the number of occurrences of n-grams based on 
the OCCs of the words in Clist. Since the OCC for a 
character contains at most five symbols, the longest n-
grams are 5-grams. Because the reason to use ECC 
was to find common components in characters, we 
disregarded n-grams that repeated no more than three 
times. In addition, the n-grams that appeared more 
than three times might not represent an actual compo-
nent in Chinese characters. Hence, we also removed 
such n-grams from the list of our basic components. 
This process naturally made our list include radicals 
that are used to categorize Chinese characters in typi-
cal printed dictionaries. The current list contains 794 
components, and it is possible to revise the list of ba-
sic components in our work whenever necessary. 
After selecting the list of basic components with 
the above procedure, we encoded the words in Elist 
with our list of basic components. We adopted the 12 
ways that Liu and Lin (2008) employed to decompose 
Chinese characters. There are other methods for de-
composing Chinese characters into components. 
Juang et al (2005) and the research team at the Sinica 
Academia propose 13 different ways for decomposing 
characters. 
5.2 Recommending Incorrect Alternatives 
With a dictionary that provides the pronunciation of 
Chinese characters and the improved ECC encodings 
for words in the Elist, we can create lists of candidate 
characters for replacing a specific correct character in 
a given word to create a test item for incorrect charac-
ter correction.  
There are multiple strategies to create the candidate 
lists. We may propose the candidate characters be-
cause their pronunciations have the same sound and 
the same tone with those of the correct character (de-
noted SSST). Characters that have same sounds and 
different tones (SSDT), characters that have similar 
sounds and same tones (MSST), and characters that 
have similar sounds and different tones (MSDT) can 
be considered as candidates as well. It is easy to judge 
whether two Chinese characters have the same tone. 
In contrast, it is not trivial to define ?similar? sound. 
We adopted the list of similar sounds that was pro-
vided by a psycholinguistic researcher (Dr. Chia-Ying 
Lee) at the Sinica Academia. 
In addition, we may propose characters that look 
similar to the correct character. Two characters may 
look similar for two reasons. They may contain the 
same components, or they contain the same radical 
and have the same total number of strokes (RS). 
When two characters contain the same component, the 
shared component might or might not locate at the 
same position within the bounding boxes of characters.  
In an authoring tool, we could recommend a lim-
ited number of candidate characters for replacing the 
correct character. We tried two strategies to compare 
and choose the visually similar characters. The first 
strategy (denoted SC1) gave a higher score to the 
shared component that located at the same location in 
the two characters being compared. The second strat-
27
egy (SC2) gave the same score to any shared compo-
nent even if the component did not reside at the same 
location in the characters. When there were more than 
20 characters that receive nonzero scores, we chose to 
select at most 20 characters that had leading scores as 
the list of recommended characters. 
5.3 Evaluating the Recommendations 
We examined the usefulness of these seven categories 
of candidates with errors in Elist and Jlist. The first 
set of evaluation (the inclusion tests) checked only 
whether the lists of recommended characters con-
tained the incorrect character in our records. The sec-
ond set of evaluation (the ranking tests) was designed 
for practical application in computer assisted item 
generation. Only for those words whose actual incor-
rect characters were included in the recommended list, 
we replaced the correct characters in the words with 
the candidate incorrect characters, submitted the in-
correct words to Google, and ordered the candidate 
characters based on their NOPs. We then recorded the 
ranks of the incorrect characters among all recom-
mended characters.  
Since the same character may appear simultane-
ously in SC1, SC2, and RS, we computed the union of 
these three sets, and checked whether the incorrect 
characters were in the union. The inclusion rate is 
listed under Comp. Similarly, we computed the union 
for SSST, SSDT, MSST, and MSDT, checked whether 
the incorrect characters were in the union, and re-
corded the inclusion rate under Pron. Finally, we 
computed the union of the lists created by the seven 
strategies, and recorded the inclusion rate under Both. 
The second and the third rows of Table 3 show the 
results of the inclusion tests. The data show the per-
centage of the incorrect characters being included in 
the lists that were recommended by the seven strate-
gies. Notice that the percentages were calculated with 
different denominators. The number of composition-
related errors was used for SC1, SC2, RS, and Comp 
(e.g. 505 that we mentioned in Section 3 for the Jlist); 
the number of pronunciation-related errors for SSST, 
SSDT, MSST, MSDT, and Pron (e.g., 1314 mentioned 
in Section 3 for the Jlist); the number of either of 
these two errors for Both (e.g., 1475 for Jlist).  
The results recorded in Table 3 show that we were 
able to find the incorrect character quite effectively, 
achieving better than 93% for both Elist and Jlist. The 
statistics also show that it is easier to find incorrect 
characters that were used for pronunciation-related 
problems. Most of the pronunciation-related problems 
were misuses of characters that had exactly the same 
pronunciations with the correct characters. Unex-
pected confusions, e.g., those related to pronuncia-
tions in Chinese dialects, were the main for the failure 
to capture the pronunciation-related errors. SSDT is a 
crucial complement to SSST. There is still room to 
improve our methods to find confusing characters 
based on their compositions. We inspected the list 
generated by SC1 and SC2, and found that, although 
SC2 outperformed SC1 on the inclusion rate, SC1 and 
SC2 actually generated complementary lists and 
should be used together. The inclusion rate achieved 
by the RS strategy was surprisingly high.  
The fourth and the fifth rows of Table 3 show the 
effectiveness of relying on Google to rank the candi-
date characters for recommending an incorrect charac-
ter. The rows show the average ranks of the included 
cases. The statistics show that, with the help of 
Google, we were able to put the incorrect character on 
top of the recommended list when the incorrect char-
acter was included.  This allows us to build an envi-
ronment for assisting human teachers to efficiently 
prepare test items for incorrect character identification. 
6 Summary  
The analysis of the 1718 errors produced by real stu-
dents show that similarity between pronunciations of 
competing characters contributed most to the ob-
served errors. Evidences show that the Web statistics 
are not very reliable for differentiating correct and 
incorrect characters. In contrast, the Web statistics are 
good for comparing the attractiveness of incorrect 
characters for computer assisted item authoring.  
Acknowledgements 
This research has been funded in part by the National 
Science Council of Taiwan under the grant NSC-97-
2221-E-004-007-MY2. We thank the anonymous re-
viewers for invaluable comments, and more responses 
to the comments are available in (Liu et al 2009). 
References  
D. Juang, J.-H. Wang, C.-Y. Lai, C.-C. Hsieh, L.-F. Chien, 
J.-M. Ho. 2005. Resolving the unencoded character 
problem for Chinese digital libraries, Proc. of the 5th 
ACM/IEEE Joint Conf. on Digital Libraries, 311?319. 
S.-P. Law, W. Wong, K. M. Y. Chiu. 2005. Whole-word 
phonological representations of disyllabic words in the 
Chinese lexicon: Data from acquired dyslexia, Behav-
ioural Neurology, 16, 169?177. 
K. J. Leck, B. S. Weekes, M. J. Chen. 1995. Visual and 
phonological pathways to the lexicon: Evidence from 
Chinese readers, Memory & Cognition, 23(4), 468?476. 
C.-L. Liu et al 2009. Phonological and logographic influ-
ences on errors in written Chinese words, Proc. of the 7th 
Workshop on Asian Language Resources, 47th ACL. 
C.-L. Liu, J.-H. Lin. 2008. Using structural information for 
identifying similar Chinese characters, Proc. of the 46th 
ACL, short papers, 93?96. 
MOE. 1996. Common Errors in Chinese Writings (???
???), Ministry of Education, Taiwan. 
Table 3. Incorrect characters were contained and ranked high in the recommended lists 
 SC1 SC2 RS SSST SSDT MSST MSDT Comp Pron Both 
Elist 73.92% 76.08% 4.08% 91.64% 18.39% 3.01% 1.67% 81.97% 99.00% 93.37% 
Jlist 67.52% 74.65% 6.14% 92.16% 20.24% 4.19% 3.58% 77.62% 99.32% 97.29% 
Elist 3.25 2.91 1.89 2.30 1.85 2.00 1.58 
Jlist 2.82 2.64 2.19 3.72 2.24 2.77 1.16 
28
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 1?8, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Applications of Lexical Information for Algorithmically Composing
Multiple-Choice Cloze Items
Chao-Lin Liu? Chun-Hung Wang? Zhao-Ming Gao? Shang-Ming Huang?
?Department of Computer Science, National Chengchi University, Taipei 11605, Taiwan
?Dept. of Foreign Lang. and Lit., National Taiwan University, Taipei 10617, Taiwan
?chaolin@nccu.edu.tw, ?zmgao@ntu.edu.tw
ABSTRACT1
We report experience in applying techniques for nat-
ural language processing to algorithmically generat-
ing test items for both reading and listening cloze
items. We propose a word sense disambiguation-
based method for locating sentences in which des-
ignated words carry specific senses, and apply a
collocation-based method for selecting distractors
that are necessary for multiple-choice cloze items.
Experimental results indicate that our system was
able to produce a usable item for every 1.6 items it
returned. We also attempt to measure distance be-
tween sounds of words by considering phonetic fea-
tures of the words. With the help of voice synthe-
sizers, we were able to assist the task of compos-
ing listening cloze items. By providing both reading
and listening cloze items, we would like to offer a
somewhat adaptive system for assisting Taiwanese
children in learning English vocabulary.
1 Introduction
Computer-assisted item generation (CAIG) allows
the creation of large-scale item banks, and has at-
tracted active study in the past decade (Deane and
Sheehan, 2003; Irvine and Kyllonen, 2002). Ap-
plying techniques for natural language processing
(NLP), CAIG offers the possibility of creating a
large number of items of different challenging lev-
els, thereby paving a way to make computers more
adaptive to students of different competence. More-
over, with the proliferation of Web contents, one
may search and sift online text files for candidate
sentences, and come up with a list of candidate cloze
1A portion of results reported in this paper will be expanded
in (Liu et al, 2005; Huang et al, 2005).
items economically. This unleashes the topics of the
test items from being confined by item creators? per-
sonal interests.
NLP techniques serve to generate multiple-choice
cloze items in different ways. (For brevity, we use
cloze items or items for multiple-choice cloze items
henceforth.) One may create sentences from scratch
by applying template-based methods (Dennis et al,
2002) or more complex methods based on some pre-
determined principles (Deane and Sheehan, 2003).
Others may take existing sentences from a corpus,
and select those that meet the criteria for becoming
test items. The former approach provides specific
and potentially well-controlled test items at the costs
of more complex systems than the latter, e.g., (Shee-
han et al, 2003). Nevertheless, as the Web provides
ample text files at our disposal, we may filter the
text sources stringently for obtaining candidate test
items of higher quality. Administrators can then se-
lect really usable items from these candidates at a
relatively lower cost.
Some researchers have already applied NLP tech-
niques to the generation of sentences for multiple-
choice cloze items. Stevens (1991) employs the con-
cepts of concordance and collocation for generating
items with general corpora. Coniam (1997) relies on
factors such as word frequencies in a tagged corpus
for creating test items of particular types.
There are other advanced NLP techniques that
may help to create test items of higher quality. For
instance, many words in English may carry multiple
senses, and test administrators usually want to test a
particular usage of the word in an item. In this case,
blindly applying a keyword matching method, such
as a concordancer, may lead us to a list of irrelevant
sentences that would demand a lot of postprocess-
1
Figure 1: A multiple-choice cloze item for English
ing workload. In addition, composing a cloze item
requires not just a useful sentence.
Figure 1 shows a multiple-choice item, where we
call the sentence with a gap the stem, the answer to
the gap the key, and the other choices the distrac-
tors. Given a sentence, we still need distractors for
a multiple-choice item. The selection of distractors
affects the item facility and item discrimination of
the cloze items (Poel and Weatherly, 1997). There-
fore, the selection of distractors calls for deliberate
strategies, and simple considerations alone, such as
word frequencies, may not satisfy the demands.
To remedy these shortcomings, we employ the
techniques for word sense disambiguation (WSD)
for choosing sentences in which the keys carries spe-
cific senses, and utilize the techniques for comput-
ing collocations (Manning and Schu?tze, 1999) for
selecting distractors. Results of empirical evaluation
show that our methods could create items of satisfac-
tory quality, and we have actually used the generated
cloze items in freshmen-level English classes.
For broadening the formats of cloze items, we
also design software that assists teachers to create
listening cloze items. After we defining a metric
for measuring similarity between pronunciations of
words, our system could choose distractors for lis-
tening cloze items. This addition opens a door to
offering different challenging levels of cloze items.
We sketch the flow of the item generation pro-
cess in Section 2, and explain the preparation of the
source corpus in Section 3. In Section 4, we elab-
orate on the application of WSD to selecting sen-
tences for cloze items, and, in Section 5, we delve
into the application of collocations to distractor gen-
eration. Results of evaluating the created reading
cloze items are presented in Section 6. We then
outline methods for creating listening cloze items in
Section 7 before making some concluding remarks.
2 System Architecture
Figure 2 shows major steps for creating cloze items.
Constrained by test administrator?s specifications
and domain dependent requirements, the Sentence
Retriever chooses a candidate sentence from the
7DJJHG
&RUSXV
7DUJHW'HSHQGHQW
,WHP5HTXLUHPHQWV
,WHP
6SHFLILFDWLRQ
7DUJHW
6HQWHQFH
6HQWHQFH
5HWULHYHUZLWK:6'
'LVWUDFWRU
*HQHUDWRU
&OR]H
,WHP
Figure 2: Main components of our item generator
Tagged Corpus. Target-Dependent Item Require-
ments specify general principles that should be fol-
lowed by all items for a particular test. For example,
the number of words in cloze items for College En-
trance Examinations in Taiwan (CEET) ranges be-
tween 6 and 28 (Liu et al, 2005), and one may want
to follow this tradition in creating drill tests.
Figure 3 shows the interface to the Item Specifi-
cation. Through this interface, test administrators
select the key for the desired cloze item, and specify
part-of-speech and sense of the key that will be used
in the item. Our system will attempt to create the re-
quested number of items. After retrieving the target
sentence, the Distractor Generator considers such
constraining factors as word frequencies and collo-
cations in selecting the distractors at the second step.
Figure 3: Interface for specifying cloze items
Figure 4 shows a sample output for the specifica-
tion shown in Figure 3. Given the generated items,
the administrator may choose and edit the items, and
save the edited items into the item bank. It is possi-
ble to retrieve previously saved items from the item
bank, and compile the items for different tests.
3 Source Corpus and Lexicons
Employing a web crawler, we retrieve the con-
tents of Taiwan Review <publish.gio.gov.tw>, Tai-
wan Journal <taiwanjournal.nat.gov.tw>, and China
Post <www.chinapost.com.tw>. Currently, we have
127,471 sentences that consist of 2,771,503 words
in 36,005 types in the corpus. We look for use-
ful sentences from web pages that are encoded in
the HTML format. We need to extract texts from
2
Figure 4: An output after Figure 3
the mixture of titles, main body of the reports,
and multimedia contents, and then segment the ex-
tracted paragraphs into individual sentences. We
segment sentences with the help of MXTERMINA-
TOR (Reynar and Ratnaparkhi, 1997). We then tok-
enize words in the sentences before assigning useful
tags to the tokens.
We augment the text with an array of tags that
facilitate cloze item generation. We assign tags of
part-of-speech (POS) to the words with MXPOST
that adopts the Penn Treebank tag set (Ratnaparkhi,
1996). Based on the assigned POS tags, we annotate
words with their lemmas. For instance, we annotate
classified with classify and classified, respectively,
when the original word has VBN and JJ as its POS
tag. We also employ MINIPAR (Lin, 1998) to ob-
tain the partial parses of sentences that we use exten-
sively in our system. Words with direct relationships
can be identified easily in the partially parsed trees,
and we rely heavily on these relationships between
words for WSD. For easy reference, we will call
words that have direct syntactic relationship with a
word W as W ?s signal words or simply signals.
Since we focus on creating items for verbs, nouns,
adjectives, and adverbs (Liu et al, 2005), we care
about signals of words with these POS tags in sen-
tences for disambiguating word senses. Specifically,
the signals of a verb include its subject, object, and
the adverbs that modify the verb. The signals of a
noun include the adjectives that modify the noun and
the verb that uses the noun as its object or predicate.
For instance, in ?Jimmy builds a grand building.?,
both ?build? and ?grand? are signals of ?building?.
The signals of adjectives and adverbs include the
words that they modify and the words that modify
the adjectives and adverbs.
When we need lexical information about English
words, we resort to electronic lexicons. We use
WordNet <www.cogsci.princeton.edu/?wn/> when
we need definitions and sample sentences of words
for disambiguating word senses, and we employ
HowNet <www.keenage.com> when we need infor-
mation about classes of verbs, nouns, adjectives, and
adverbs.
HowNet is a bilingual lexicon. An entry in
HowNet includes slots for Chinese words, English
words, POS information, etc. We rely heavily on the
slot that records the semantic ingredients related to
the word being defined. HowNet uses a limited set
of words in the slot for semantic ingredient, and the
leading ingredient in the slot is considered to be the
most important one generally.
4 Target Sentence Retriever
The sentence retriever in Figure 2 extracts qualified
sentences from the corpus. A sentence must contain
the desired key of the requested POS to be consid-
ered as a candidate target sentence. Having identi-
fied such a candidate sentence, the item generator
needs to determine whether the sense of the key also
meets the requirement. We conduct this WSD task
based on an extended notion of selectional prefer-
ences.
4.1 Extended Selectional Preferences
Selectional preferences generally refer to the phe-
nomenon that, under normal circumstances, some
verbs constrain the meanings of other words in
a sentence (Manning and Schu?tze, 1999; Resnik,
1997). We can extend this notion to the relation-
ships between a word of interest and its signals, with
the help of HowNet. Let w be the word of interest,
and pi be the first listed class, in HowNet, of a signal
word that has the syntactic relationship ? with w.
We define the strength of the association of w and pi
as follows:
A?(w, pi) = Pr?(w, pi)Pr?(w) , (1)
where Pr?(w) is the probability of w participating in
the ? relationship, and Pr?(w, pi) is the probability
that both w and pi participate in the ? relationship.
4.2 Word Sense Disambiguation
We employ the generalized selectional preferences
to determine the sense of a polysemous word in a
sentence. Consider the task of determining the sense
3
of ?spend? in the candidate target sentence ?They
say film makers don?t spend enough time developing
a good story.? The word ?spend? has two possible
meanings in WordNet.
1. (99) spend, pass ? (pass (time) in a specific
way; ?How are you spending your summer va-
cation??)
2. (36) spend, expend, drop ? (pay out; ?I spend
all my money in two days.?)
Each definition of the possible senses include (1)
the head words that summarize the intended mean-
ing and (2) a sample sentence for sense. When we
work on the disambiguation of a word, we do not
consider the word itself as a head word in the follow-
ing discussion. Hence, ?spend? has one head word,
i.e., ?pass?, in the first sense and two head words,
i.e., ?extend? and ?drop?, in the second sense.
An intuitive method for determining the mean-
ing of ?spend? in the target sentence is to replace
?spend? with its head words in the target sentence.
The head words of the correct sense should go with
the target sentence better than head words of other
senses. This intuition leads to the a part of the scores
for senses, i.e., St that we present shortly.
In addition, we can compare the similarity of the
contexts of ?spend? in the target sentence and sam-
ple sentences, where context refers to the classes of
the signals of the word being disambiguated. For the
current example, we can check whether the subject
and object of ?spend? in the target sentence have the
same classes as the subjects and objects of ?spend?
in the sample sentences. The sense whose sample
sentence offers a more similar context for ?spend? in
the target sentence receives a higher score. This in-
tuition leads to the other part of the scores for senses,
i.e., Ss that we present below.
Assume that the key w has n senses. Let ? =
{?1, ?2, ? ? ? , ?n} be the set of senses of w. Assume
that sense ?j of word w has mj head words in Word-
Net. (Note that we do not consider w as its own head
word.) We use the set ?j = {?j,1, ?j,2, ? ? ? , ?j,mj}
to denote the set of head words that WordNet pro-
vides for sense ?j of word w.
When we use the partial parser to parse the tar-
get sentence T for a key, we obtain information
about the signal words of the key. Moreover, for
each of these signals, we look up their classes in
HowNet, and adopt the first listed class for each of
the signals when the signal covers multiple classes.
Assume that there are ?(T ) signals for the key
w in a sentence T . We use the set ?(T,w) =
{?1,T , ?2,T , ? ? ? , ??(T ),T } to denote the set of sig-
nals for w in T . Correspondingly, we use ?j,T to de-
note the syntactic relationship between w and ?j,T
in T , use ?(T,w) = {?1,T , ?2,T , ? ? ? , ??(T ),T } for
the set of relationships between signals in ?(T,w)
and w, use pij,T for the class of ?j,T , and use
?(T,w) = {pi1,T , pi2,T , ? ? ? , pi?(T ),T } for the set of
classes of the signals in ?(T,w).
Equation (2) measures the average strength of as-
sociation of the head words of a sense with signals
of the key in T , so we use (2) as a part of the score
for w to take the sense ?j in the target sentence T .
Note that both the strength of association and St fall
in the range of [0,1].
St(?j |w, T )
= 1mj
mj?
k=1
1
?(T )
?(T )?
l=1
A?l,T (?j,k, pil,T ) (2)
In (2), we have assumed that the signal words
are not polysemous. If they are polysemous, we as-
sume that each of the candidate sense of the signal
words are equally possible, and employ a slightly
more complicated formula for (2). This assumption
may introduce errors into our decisions, but relieves
us from the needs to disambiguate the signal words
in the first place (Liu et al, 2005).
Since WordNet provides sample sentences for im-
portant words, we also use the degrees of similarity
between the sample sentences and the target sen-
tence to disambiguate the word senses of the key
word in the target sentence. Let T and S be the tar-
get sentence of w and a sample sentence of sense ?j
of w, respectively. We compute this part of score,
Ss, for ?j using the following three-step procedure.
If there are multiple sample sentences for a given
sense, say ?j of w, we will compute the score in (3)
for each sample sentence of ?j , and use the average
score as the final score for ?j .
Procedure for computing Ss(?j |w, T )
1. Compute signals of the key and their relation-
ships with the key in the target and sample sen-
tences.
4
?(T,w) = {?1,T , ?2,T , ? ? ? , ??(T ),T },
?(T,w) = {?1,T , ?2,T , ? ? ? , ??(T ),T },
?(S,w) = {?1,S , ?2,S , ? ? ? , ??(S),S}, and
?(S,w) = {?1,S , ?2,S , ? ? ? , ??(S),S}
2. We look for ?j,T and ?k,S such that ?j,T =
?k,S , and then check whether pij,T = pik,S .
Namely, for each signal of the key in T , we
check the signals of the key in S for matching
syntactic relationships and word classes, and
record the counts of matched relationship in
M(?j , T ) (Liu et al, 2005).
3. The following score measures the proportion of
matched relationships among all relationships
between the key and its signals in the target sen-
tence.
Ss(?j |w, T ) = M(?j , T )?(T ) (3)
The score for w to take sense ?j in a target sen-
tence T is the sum of St(?j |w, T ) defined in (2)
and Ss(?j |w, T ) defined in (3), so the sense of w
in T will be set to the sense defined in (4) when
the score exceeds a selected threshold. When the
sum of St(?j |w, T ) and Ss(?j |w, T ) is smaller than
the threshold, we avoid making arbitrary decisions
about the word senses. We discuss and illustrate ef-
fects of choosing different thresholds in Section 6.
argmax
?j??
St(?j |w, T ) + Ss(?j |w, T ) (4)
5 Distractor Generation
Distractors in multiple-choice items influence the
possibility of making lucky guesses to the answers.
Should we use extremely impossible distractors in
the items, examinees may be able to identify the
correct answers without really knowing the keys.
Hence, we need to choose distractors that appear to
fit the gap, and must avoid having multiple answers
to items in a typical cloze test at the same time.
There are some conceivable principles and al-
ternatives that are easy to implement and follow.
Antonyms of the key are choices that average exam-
inees will identify and ignore. The part-of-speech
tags of the distractors should be the same as the
key in the target sentence. We may also take cul-
tural background into consideration. Students in
Taiwan tend to associate English vocabularies with
their Chinese translations. Although this learning
strategy works most of the time, students may find
it difficult to differentiate English words that have
very similar Chinese translations. Hence, a culture-
dependent strategy is to use English words that have
similar Chinese translations with the key as the dis-
tractors.
To generate distractors systematically, we employ
ranks of word frequencies for selecting distractors
(Poel and Weatherly, 1997). Assume that we are
generating an item for a key whose part-of-speech
is ?, that there are n word types whose part-of-
speech may be ? in the dictionary, and that the rank
of frequency of the key among these n types is m.
We randomly select words that rank in the range
[m?n/10,m+n/10] among these n types as candi-
date distractors. These distractors are then screened
by their fitness into the target sentence, where fitness
is defined based on the concept of collocations of
word classes, defined in HowNet, of the distractors
and other words in the stem of the target sentence.
Recall that we have marked words in the corpus
with their signals in Section 3. The words that have
more signals in a sentence usually contribute more to
the meaning of the sentence, so should play a more
important role in the selection of distractors. Since
we do not really look into the semantics of the tar-
get sentences, a relatively safer method for selecting
distractors is to choose those words that seldom col-
locate with important words in the target sentence.
Let T = {t1, t2, ? ? ? , tn} denote the set of words
in the target sentence. We select a set T ? ? T such
that each t?i ? T ? has two or more signals in T and is
a verb, noun, adjective, or adverb. Let ? be the first
listed class, in HowNet, of the candidate distractor,
and ? = {?i|?i is the first listed class of a t?i ? T ?}.
The fitness of a candidate distractor is defined in (5).
?1
|?|
?
?i??
log Pr(?, ?i)Pr(?) Pr(?i) (5)
The candidate whose score is better than 0.3 will
be admitted as a distractor. Pr(?) and Pr(?i) are
the probabilities that each word class appears indi-
vidually in the corpus, and Pr(?, ?i) is the proba-
bility that the two classes appear in the same sen-
tence. Operational definitions of these probabilities
5
Table 1: Accuracy of WSD
POS baseline threshold=0.4 threshold=0.7
verb 38.0%(19/50) 57.1%(16/28) 68.4%(13/19)
noun 34.0%(17/50) 63.3%(19/30) 71.4%(15/21)
adj. 26.7%(8/30) 55.6%(10/18) 60.0%(6/10)
adv. 36.7%(11/30) 52.4%(11/21) 58.3%(7/12)
are provided in (Liu et al, 2005). The term in the
summation is a pointwise mutual information, and
measures how often the classes ? and ?i collocate
in the corpus. We negate the averaged sum so that
classes that seldom collocate receive higher scores.
We set the threshold to 0.3, based on statistics of (5)
that are observed from the cloze items used in the
1992-2003 CEET.
6 Evaluations and Applications
6.1 Word Sense Disambiguation
Different approaches to WSD were evaluated in dif-
ferent setups, and a very wide range of accuracies in
[40%, 90%] were reported (Resnik, 1997; Wilks and
Stevenson, 1997). Objective comparisons need to be
carried out on a common test environment like SEN-
SEVAL, so we choose to present only our results.
We arbitrarily chose, respectively, 50, 50, 30,
and 30 sentences that contained polysemous verbs,
nouns, adjectives, and adverbs for disambiguation.
Table 1 shows the percentage of correctly disam-
biguated words in these 160 samples.
The baseline column shows the resulting accu-
racy when we directly use the most frequent sense,
recorded in WordNet, for the polysemous words.
The rightmost two columns show the resulting accu-
racy when we used different thresholds for applying
(4). As we noted in Section 4.2, our system selected
fewer sentences when we increased the threshold, so
the selected threshold affected the performance. A
larger threshold led to higher accuracy, but increased
the rejection rate at the same time. Since the cor-
pus can be extended to include more and more sen-
tences, we afford to care about the accuracy more
than the rejection rate of the sentence retriever.
We note that not every sense of all words have
sample sentences in the WordNet. When a sense
does not have any sample sentence, this sense will
receive no credit, i.e., 0, for Ss. Consequently,
our current reliance on sample sentences in Word-
Table 2: Correctness of the generated sentences
POS of the key # of items % of correct sentences
verb 77 66.2%
noun 62 69.4%
adjective 35 60.0%
adverb 26 61.5%
overall 65.5%
Table 3: Uniqueness of answers
item category key?s POS number of items results
verb 64 90.6%
noun 57 94.7%
cloze adjective 46 93.5%
adverb 33 84.8%
overall 91.5%
Net makes us discriminate against senses that do not
have sample sentences. This is an obvious draw-
back in our current design, but the problem is not
really detrimental and unsolvable. There are usually
sample sentences for important and commonly-used
senses of polysemous words, so the discrimination
problem does not happen frequently. When we do
want to avoid this problem once and for all, we can
customize WordNet by adding sample sentences to
all senses of important words.
6.2 Cloze Item Generation
We asked the item generator to create 200 items in
the evaluation. To mimic the distribution over keys
of the cloze items that were used in CEET, we used
77, 62, 35, and 26 items for verbs, nouns, adjectives,
and adverbs, respectively, in the evaluation.
In the evaluation, we requested one item at a time,
and examined whether the sense and part-of-speech
of the key in the generated item really met the re-
quests. The threshold for using (4) to disambiguate
word sense was set to 0.7. Results of this experi-
ment, shown in Table 2, do not differ significantly
from those reported in Table 1. For all four major
classes of cloze items, our system was able to re-
turn a correct sentence for less than every 2 items
it generated. In addition, we checked the quality of
the distractors, and marked those items that permit-
ted unique answers as good items. Table 3 shows
that our system was able to create items with unique
answers for another 200 items most of the time.
6
 
Figure 5: A phonetic concordancer
6.3 More Applications
We have used the generated items in real tests in a
freshman-level English class at National Chengchi
University, and have integrated the reported item
generator in a Web-based system for learning En-
glish. In this system, we have two major subsys-
tems: the authoring and the assessment subsystems.
Using the authoring subsystem, test administrators
may select items from the interface shown in Fig-
ure 4, save the selected items to an item bank, edit
the items, including their stems if necessary, and fi-
nalize the selection of the items for a particular ex-
amination. Using the assessment subsystem, stu-
dents answer the test items via the Internet, and
can receive grades immediately if the administra-
tors choose to do so. The answers of students are
recorded for student modelling and analysis of the
item facility and the item discrimination.
7 Generating Listening Cloze Items
We apply the same infrastructure for generating
reading cloze items, shown in Figure 2, for the gen-
eration of listening cloze items (Huang et al, 2005).
Due to the educational styles in Taiwan, students
generally find it more difficult to comprehend mes-
sages by listening than by reading. Hence, we can
regard listening cloze tests as an advanced format of
reading cloze tests. Having constructed a database
of sentences, we can extract sentences that contain
the key for which the test administrator would like
to have a listening cloze, and employ voice synthe-
sizers to create the necessary recordings.
Figure 5 shows an interface through which ad-
ministrators choose and edit sentences for listening
cloze items. Notice that we employ the concept that
is related to ordinary concordance in arranging the
extracted sentences. By defining a metric for mea-
suring similarity between sounds, we can put sen-
tences that have similar phonetic contexts around the
key near each other. We hope this would better help
teachers in selecting sentences by this rudimentary
 
Figure 6: The most simple form of listening cloze
clustering of sentences.
Figure 6 shows the most simple format of listen-
ing cloze items. In this format, students click on the
options, listen to the recorded sounds, and choose
the option that fit the gap. The item shown in this
figure is very similar to that shown in Figure 1, ex-
cept that students read and hear the options. From
this most primitive format, we can image and imple-
ment other more challenging formats. For instance,
we can replace the stem, currently in printed form in
Figure 6, into clickable links, demanding students
to hear the stem rather than reading the stem. A
middle ground between this more challenging for-
mat and the original format in the figure is to allow
the gap to cover more words in the original sentence.
This would require the students to listen to a longer
stream of sound, so can be a task more challenging
than the original test. In addition to controlling the
lengths of the answer voices, we can try to modulate
the speed that the voices are replayed. Moreover,
for multiple-word listening cloze, we may try to find
word sequences that sound similar to the answer se-
quence to control the difficulty of the test item.
Defining a metric for measuring similarity be-
tween two recordings is the key to support the afore-
mentioned functions. In (Huang et al, 2005), we
consider such features of phonemes as place and
manner of pronunciation in calculating the similarity
between sounds. Using this metric we choose as dis-
tractors those sounds of words that have similar pro-
nunciation with the key of the listening cloze. We
have to define the distance between each phoneme
so that we could employ the minimal-edit-distance
algorithm for computing the distance between the
sounds of different words.
7
8 Concluding Remarks
We believe that NLP techniques can play an impor-
tant role in computer assisted language learning, and
this belief is supported by papers in this workshop
and the literature. What we have just explored is
limited to the composition of cloze items for English
vocabulary. With the assistance of WSD techniques,
our system was able to identify sentences that were
qualified as candidate cloze items 65% of the time.
Considering both word frequencies and collocation,
our system recommended distractors for cloze items,
resulting in items that had unique answers 90% of
the time. In addition to assisting the composition
of cloze items in the printed format, our system is
also capable of helping the composition of listening
cloze items. The current system considers features
of phonemes in computing distances between pro-
nunciations of different word strings.
We imagine that NLP and other software tech-
niques could empower us to create cloze items for a
wide range of applications. We could control the for-
mats, contents, and timing of the presented material
to manipulate the challenging levels of the test items.
As we have indicated in Section 7, cloze items in the
listening format are harder than comparable items in
the printed format. We can also control when and
what the students can hear to fine tune the difficul-
ties of the listening cloze items.
We must admit, however, that we do not have suf-
ficient domain knowledge in how human learn lan-
guages. Consequently, tools offered by computing
technologies that appear attractive to computer sci-
entists or computational linguists might not provide
effective assistance for language learning or diagno-
sis. Though we have begun to study item compari-
son from a mathematical viewpoint (Liu, 2005), the
current results are far from being practical. Exper-
tise in psycholinguistics may offer a better guidance
on our system design, we suppose.
Acknowledgements
We thank anonymous reviewers for their invaluable
comments on a previous version of this report. We
will respond to some suggestions that we do not have
space to do so in this report in the workshop. This
research was supported in part by Grants 93-2213-E-
004-004 and 93-2411-H-002-013 from the National
Science Council of Taiwan.
References
D. Coniam. 1997. A preliminary inquiry into using corpus
word frequency data in the automatic generation of English
language cloze tests. Computer Assisted Language Instruc-
tion Consortium, 16(2?4):15?33.
P. Deane and K. Sheehan. 2003. Automatic item gen-
eration via frame semantics. Education Testing Service:
http://www.ets.org/research/dload/ncme03-deane.pdf.
I. Dennis, S. Handley, P. Bradon, J. Evans, and S. Nestead.
2002. Approaches to modeling item-generative tests. In
Item generation for test development (Irvine and Kyllonen,
2002), pages 53?72.
S.-M. Huang, C.-L. Liu, and Z.-M. Gao. 2005. Computer-
assisted item generation for listening cloze tests and dictation
practice in English. In Proc. of the 4th Int. Conf. on Web-
based Learning. to appear.
S. H. Irvine and P. C. Kyllonen, editors. 2002. Item Genera-
tion for Test Development. Lawrence Erlbaum Associates,
Mahwah, NJ.
D. Lin. 1998. Dependency-based evaluation of MINIPAR. In
Proc. of the Workshop on the Evaluation of Parsing Systems
in the 1st Int. Conf. on Language Resources and Evaluation.
C.-L. Liu, C.-H. Wang, and Z.-M. Gao. 2005. Using lexi-
cal constraints for enhancing computer-generated multiple-
choice cloze items. Int. J. of Computational Linguistics and
Chinese Language Processing, 10:to appear.
C.-L. Liu. 2005. Using mutual information for adaptive item
comparison and student assessment. J. of Educational Tech-
nology & Society, 8(4):to appear.
C. D. Manning and H. Schu?tze. 1999. Foundations of Statisti-
cal Natural Language Processing. MIT Press, Cambridge.
C. J. Poel and S. D. Weatherly. 1997. A cloze look at place-
ment testing. Shiken: JALT (Japanese Assoc. for Language
Teaching) Testing & Evaluation SIG Newsletter, 1(1):4?10.
A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tag-
ger. In Proc. of the Conf. on Empirical Methods in Natural
Language Processing, pages 133?142.
P. Resnik. 1997. Selectional preference and sense disambigua-
tion. In Proc. of the Applied NLP Workshop on Tagging Text
with Lexical Semantics: Why, What and How, pages 52?57.
J. C. Reynar and A. Ratnaparkhi. 1997. A maximum entropy
approach to identifying sentence boundaries. In Proc. of the
Conf. on Applied Natural Language Processing, pages 16?
19.
K. M. Sheehan, P. Deane, and I. Kostin. 2003. A partially auto-
mated system for generating passage-based multiple-choice
verbal reasoning items. Paper presented at the Nat?l Council
on Measurement in Education Annual Meeting.
V. Stevens. 1991. Classroom concordancing: vocabulary ma-
terials derived from relevant authentic text. English for Spe-
cific Purposes, 10(1):35?46.
Y. Wilks and M. Stevenson. 1997. Combining independent
knowledge sources for word sense disambiguation. In Proc.
of the Conf. on Recent Advances in Natural Language Pro-
cessing, pages 1?7.
8
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 84?91,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Phonological and Logographic Influences on Errors in Written 
Chinese Words 
Chao-Lin Liu1 Kan-Wen Tien2 Min-Hua Lai3 Yi-Hsuan Chuang4 Shih-Hung Wu5
1-4National Chengchi University, 5Chaoyang University of Technology, Taiwan 
{1chaolin, 296753027, 395753023, 494703036}@nccu.edu.tw, 5shwu@cyut.edu.tw 
 
Abstract 
We analyze a collection of 3208 reported errors 
of Chinese words. Among these errors, 7.2% in-
volved rarely used character, and 98.4% were 
assigned common classifications of their causes 
by human subjects. In particular, 80% of the er-
rors observed in the writings of middle school 
students were related to the pronunciations and 
30% were related to the logographs of the words. 
We conducted experiments that shed light on us-
ing the Web-based statistics to correct the errors, 
and we designed a software environment for pre-
paring test items whose authors intentionally re-
place correct characters with wrong ones. Ex-
perimental results show that using Web-based 
statistics can help us correct only about 75% of 
these errors. In contrast, Web-based statistics are 
useful for recommending incorrect characters for 
composing test items for ?incorrect character 
identification? tests about 93% of the time. 
1 Introduction 
Incorrect writings in Chinese are related to our under-
standing of the cognitive process of reading Chinese 
(e.g., Leck et al, 1995), to our understanding of why 
people produce incorrect characters and our offering 
corresponding remedies (e.g., Law et al, 2005), and 
to building an environment for assisting the prepara-
tion of test items for assessing students? knowledge of 
Chinese characters (e.g., Liu and Lin, 2008). 
Chinese characters are composed of smaller parts 
that can carry phonological and/or semantic informa-
tion. A Chinese word is formed by Chinese characters. 
For example, ??? (Singapore) is a word that con-
tains three Chinese characters. The left (?) and the 
right (?) part of ?, respectively, carry semantic and 
phonological information. The semantic information, 
in turn, is often related to the logographs that form the 
Chinese characters. Evidences show that production 
of incorrect characters are related to phonological, 
logographic, or the semantic aspect of the characters. 
Although the logographs of Chinese characters can be 
related to the lexical semantics, not all errors that are 
related to semantics were caused by the similarity in 
logographs. Some were due to the context of the 
words and/or permissible interpretations of different 
words.  
In this study, we investigate issues that are related 
to the phonological and logographical influences on 
the occurrences of incorrect characters in Chinese 
words. In Section 2, we present the details about the 
sources of the reported errors. We have collected er-
rors from a published book and from a group of mid-
dle school students. In Section 3, we analyze the 
causes of the observed errors. Native speakers of Chi-
nese were asked to label whether the observed errors 
were related to the phonological or the logographic 
reasons. In Section 4, we explore the effectiveness of 
relying on Web-based statistics to correct the errors. 
We submitted an incorrect word and a correct word 
separately to Google to find the number of web pages 
that contained these words. The correct and incorrect 
words differed in just the incorrect character. We ex-
amine whether the number of web pages that con-
tained the words can help us find the correct way of 
writing. In Section 5, we employ Web-based statistics 
in the process of assisting teachers to prepare test 
items for assessing students? knowledge of Chinese 
characters. Experimental results showed that our 
method outperformed the one reported in (Liu and Lin, 
2008), and captured the incorrect characters better 
than 93% of the time. 
2 Data Sources 
We obtained data from three major sources. A list that 
contains 5401 characters that have been believed to be 
sufficient for everyday lives was obtained from the 
Ministry of Education (MOE) of Taiwan, and we call 
the first list the Clist, henceforth. The 5401 characters 
form the core basis for the BIG-5 code, and an official 
introduction of these 5401 characters is available at 
http://www.cns11643.gov.tw/AIDB/encodings.do#encode4.  
We have two lists of words, and each word is ac-
companied by an incorrect way to write the word. The 
first list is from a book published by MOE (1996). 
The MOE provided the correct words and specified 
the incorrect characters which were mistakenly used 
to replace the correct characters in the correct words. 
The second list was collected, in 2008, from the writ-
ten essays of students of the seventh and the eighth 
grades in a middle school in Taipei. The incorrect 
characters were entered into computers based on stu-
dents? writings, ignoring those characters that did not 
actually exist and could not be entered.  
We will call the first list of word the Elist, and the 
second the Jlist from now on. Elist and Jlist contain, 
respectively, 1490 and 1718 entries. Each of these 
entries contains a correct word and the incorrect char-
acter. Hence, we can reconstruct the incorrect words 
84
easily. Two or more different ways to incorrectly 
write the same words were listed in different entries 
and considered as two entries for simplicity of presen-
tation. 
3 Error Analysis of Written Words 
Two human subjects, who are native speakers of Chi-
nese and are graduate students in Computer Science, 
examined Elist and Jlist and categorized the causes of 
errors. They compared the incorrect characters with 
the correct characters to determine whether the errors 
were pronunciation-related or logographs-related. 
Referring to an error as being ?semantics-related? is 
ambiguous. Two characters might not contain the 
same semantic part, but are still semantically related, 
e.g., misusing ???(tou1) for ???(tou2) in ????
??. In this study, we have not considered this factor. 
For this reason we refer to the errors that are related to 
the sharing of logographic parts in characters as com-
position-related. 
Among the 1490 and 1718 words in Elist and Jlist, 
respectively, the two human subjects had consensus 
over causes of 1441 and 1583 errors. It is interesting 
to learn that native speakers had a high consensus 
about the causes for the observed errors, but they did 
not always agree. To have a common standard in 
comparison, we studied the errors that the two sub-
jects had agreed categorizations.  
The statistics changed when we disregarded errors 
that involved characters not included in Clist. An er-
ror would be ignored if either the correct or the incor-
rect character did not belong to the Clist. It is possible 
for students to write such rarely used characters in an 
incorrect word just by coincidence.  
After ignoring the rare characters, there were 1333 
and 1645 words in Elist and Jlist, respectively. The 
subjects had consensus over the causes of errors for 
1285 and 1515 errors in Elist and Jlist, respectively.  
Table 1 shows the percentages of five categories of 
errors: C for the composition-related errors, P for the 
pronunciation-related errors, C&P for the intersection 
of C and P, NE for those errors that belonged to nei-
ther C nor P, and D for those errors that the subjects 
disagreed on the error categories. There were, respec-
tively, 505 composition-related and 1314 pronuncia-
tion-related errors in Jlist, so we see 
505/1645=30.70% and 1314/1645=79.88% in the 
table. Notice that C&P represents the intersection of 
C and P, so we have to deduct C&P from the sum of 
C, P, NE, and D to find the total probability, namely 1. 
It is worthwhile to discuss the implication of the 
statistics in Table 1. For the Jlist, similarity between 
pronunciations accounted for nearly 80% of the errors, 
and the ratio for the errors that are related to composi-
tions and pronunciations is 1:2.6. In contrast, for the 
Elist, the corresponding ratio is almost 1:1. The Jlist 
and Elist differed significantly in the ratios of the er-
ror types. It was assumed that the dominance of pro-
nunciation-related errors in electronic documents was 
a result of the popularity of entering Chinese with 
pronunciation-based methods. The ratio for the Jlist 
challenges this popular belief, and indicates that even 
though the errors occurred during a writing process, 
rather than typing on computers, students still pro-
duced more pronunciation-related errors than compo-
sition-related errors. Distribution over error types is 
not as related to input method as one may have be-
lieved. Nevertheless, the observation might still be a 
result of students being so used to entering Chinese 
text with pronunciation-based method that the organi-
zation of their mental lexicons is also pronunciation 
related. The ratio for the Elist suggests that editors of 
the MOE book may have chosen the examples with a 
special viewpoint in their minds ? balancing pronun-
ciation and composition related errors. 
4 Reliability of Web-based Statistics  
In this section, we examine the effectiveness of using 
Web-based statistics to differentiate correct and incor-
rect characters. The abundant text material on the 
Internet gives people to treat the Web as a corpus (e.g., 
webascorpus.org). When we send a query to Google, 
we will be informed of the number of pages (NOPs) 
that possibly contain relevant information. If we put 
the query terms in quotation marks, we should find 
the web pages that literally contain the query terms. 
Hence, it is possible for us to compare the NOPs for 
two competing phrases for guessing the correct way 
of writing. At the time of this writing, Google found 
107000 and 3220 pages, respectively, for ?strong tea? 
and ?powerful tea?. (When conducting such advanced 
searches with Google, the quotation marks are needed 
to ensure the adjacency of individual words.) Hence, 
?strong? appears to be a better choice to go with ?tea?. 
This is an idea similar to the approach that compute 
collocations based on word frequencies (cf. Manning 
and Sch?tze, 1999). Although the idea may not work 
very well for small database, the size of the current 
Web should be considered large enough. 
Using the quotation marks for the query terms en-
forced the influences of the surrounding characters in 
Chinese words, and provides a better clue for judging 
correct usage of Chinese characters. For instance,  
without the context, ??? and ??? might be used 
incorrectly to replace each other because they have 
the same pronunciation, i.e., Mei3. It is relatively 
unlikely for one to replace ??? with ??? when we 
write ???? (every one), but these two characters can 
become admissible candidates when we write ???? 
(USA) and ???? (every country).  
Table 1. Error analysis for Elist and Jlist 
 C P C&P NE D 
Elist 66.09% 67.21% 37.13% 0.23% 3.60%
Jlist 30.70% 79.88% 20.91% 2.43% 7.90%
85
4.1 Field Tests 
We test this strategy by sending the words in Elist and 
Jlist to Google to find the NOPs. We can retrieve the 
NOPs from the documents returned by Google, and 
compare the NOPs for the correct and the incorrect 
words to evaluate the strategy. Again, we focused on 
those in the 5401 words that the human subjects had 
consensus about their error types. Recall that we have 
1285 and 1515 such words in Elist and Jlist, respec-
tively. As the information available on the Web 
changes all the time, we also have to note that our 
experiments were conducted during the first half of 
March 2009. The queries were submitted at reason-
able time intervals to avoid Google?s treating our pro-
grams as malicious attackers. 
Table 2 shows the results of our investigation. We 
considered that we had a correct result when we found 
that the NOP for the correct word was larger than the 
NOP for the incorrect word. If the NOPs were equal, 
we recorded an ambiguous result; and when the NOP 
for the incorrect word was larger, we recorded an in-
correct event. We use ?C?, ?A?, and ?I? to denote ?cor-
rect?, ?ambiguous?, and ?incorrect? events in Table 2.  
The column headings of Table 2 show the setting 
of the searches with Google and the set of words that 
were used in the experiments. We asked Google to 
look for information from web pages that were en-
coded in traditional Chinese (denoted Trad). We 
could add another restriction on the source of infor-
mation by asking Google to inspect web pages from 
machines in Taiwan (denoted Twn+Trad). We were 
not sure how Google determined the languages and 
locations of the information sources, but chose to trust 
Google. The headings ?Comp? and ?Pron? indicate 
whether the words whose error types were composi-
tion and pronunciation-related, respectively.  
Table 2 shows eight distributions, providing ex-
perimental results that we observed under different 
settings. The distribution printed in bold face showed 
that, when we gathered information from sources that 
were encoded in traditional Chinese, we found the 
correct words 73.12% of the time for words whose 
error types were related to composition in Elist. Under 
the same experimental setting, we could not judge the 
correct word 4.58% of the time, and would have cho-
sen an incorrect word 22.30% of the time. 
Statistics in Table 2 indicate that web statistics is 
not a very reliable factor to judge the correct words. 
The average of the eight numbers in the ?C? rows is 
only 71.54% and the best sample is 76.59%, suggest-
ing that we did not find the correct words frequently. 
We would made incorrect judgments 24.75% of the 
time. The statistics also show that it is almost equally 
difficult to find correct words for errors that are com-
position and pronunciation related. In addition, the 
statistics reveal that choosing more features in the 
advanced search affected the final results. Using 
?Trad? offered better results in our experiments than 
using ?Twn+Trad?. This observation may arouse a 
perhaps controversial argument. Although Taiwan is 
the main area to use traditional Chinese, their web 
pages might not have used as accurate Chinese as web 
pages located in other regions. 
4.2 An Error Analysis for the Field Tests 
We have analyzed the reasons for why using Web-
based statistics did not always find the correct words. 
Frequencies might not have been a good factor to de-
termine the correctness of Chinese. However, the 
myriad amount of data on the Web should have pro-
vided a better performance.  
The most common reason for errors is that some of 
the words are really confusing such that the majority 
of the Web pages actually used the incorrect words. 
Some of errors were so popular that even one of the 
Chinese input methods on Windows XP offered 
wrong words as possible choices, e.g., ????? (the 
correct one) vs. ?????. It is interesting to note that 
people may intentionally use incorrect words in some 
occasions; for instance, people may choose to write 
homophones in advertisements.  
Another popular reason is that whether a word is 
correct depends on a larger context. For instance, ??
?? is more popular than ???? because the former 
is a popular nickname. Unless we had provided more 
contextual information about the queried words, 
checking only the NOPs of ???? and ???? led us 
to choose ????, which happened to be an incorrect 
word when we meant to find the right way to write 
????. Another difficult pair of words to distinguish 
is ???? and ????. 
Yet another reason for having a large NOP of the 
incorrect words was due to errors in segmenting Chi-
nese character strings. Consider a correct character 
string ?WXYZ?, where ?WX? and ?YZ? are two cor-
rect words. It is possible that ?XY? happens to be an 
incorrect way to write a correct word. This is the case 
for having the counts for ?????? to contribute to 
the count for ???? which is an incorrect form of 
????. 
5 Facilitating Test Item Authoring 
Incorrect character correction is a very popular type of 
test in Taiwan. There are simple test items for young 
children, and there are very challenging test items for 
the competitions among adults. Finding an attractive 
incorrect character to replace a correct character to 
form a test item is a key step in authoring test items.  
Table 2. Reliability of Web-based statistics 
Trad Twn+Trad  
Comp Pron Comp Pron 
C 73.12% 73.80% 69.92% 68.72%
A 4.58% 3.76% 3.83% 3.76%
E
list 
I 22.30% 22.44% 26.25% 27.52%
C 76.59% 74.98% 69.34% 65.87%
A 2.26% 3.97% 2.47% 5.01%
Jlist 
I 21.15% 21.05% 28.19% 29.12%
86
We have been trying to build a software environ-
ment for assisting the authoring of test items for in-
correct character correction (Liu and Lin, 2008, Liu et 
al., 2009). It should be easy to find a lexicon that con-
tains pronunciation information about Chinese charac-
ters. In contrast, it might not be easy to find visually 
similar Chinese characters with computational meth-
ods. We expanded the original Cangjie codes (OCC), 
and employed the expanded Cangjie codes (ECC) to 
find visually similar characters (Liu and Lin, 2008). 
Cangjie encoding (Chu, 2009) is a special system 
for representing the formation of Chinese characters 
with a sequence of at most five basic symbols. For 
instance, ??? and ??? are represented by ????
?? and ??????, respectively. It is evident that 
the Cangjie codes are useful for finding visually simi-
lar characters. 
With a lexicon, we can find characters that can be 
pronounced in a particular way. However, this is not 
enough for our goal. We observed that there were 
different symptoms when people used incorrect char-
acters that are related to their pronunciations. They 
may use characters that could be pronounced exactly 
the same as the correct characters. They may also use 
characters that have the same pronunciation and dif-
ferent tones with the correct character. Although rela-
tively infrequently, people may use characters whose 
pronunciations are similar to but different from the 
pronunciation of the correct character.  
We reported that replacing OCCs with ECCs to 
find visually similar characters could increase the 
chances to find similar characters. Instead of saving 
?????? for ??? directly, we divide a Chinese 
character into subareas systematically, and save the 
Cangjie codes for each of the subareas. A Chinese 
character is stored with the information about how it 
is divided into subareas and the Cangjie sequences for 
each of its subareas.  The internal code for how we 
divide ??? is 2, and the ECC for ??? has two parts: 
??? and ?????. Yet, it was not clear as to which 
components of a character should use ECCs (Liu and 
Lin, 2008; Liu et al, 2009). 
5.1 Formalizing the Extended Cangjie Codes 
We analyzed the OCCs for all the characters in Clist 
to determine the list of basic components, with com-
puter programs. We treated a basic Cangjie symbol as 
if it was a word, and computed the number of occur-
rences of n-grams based on the OCCs of the charac-
ters in Clist. Since the OCC for a character contains at 
most five symbols, the longest n-grams are 5-grams. 
Because the reason to use ECCs was to find common 
components in characters, we saved n-grams that re-
peated no less than three times in a list. After obtain-
ing this initial list of n-grams, we removed those n-
grams that were substrings of longer n-grams in the 
list.  
In addition, the n-grams that appeared no less than 
three times might not represent an actual part in any 
Chinese characters. This may happen by chance be-
cause we considered only frequencies of n-grams 
when we generated the initial list at the previous step. 
For instance, the OCC codes for ??? (shai4), ??? 
(wu4), and ??? (chen2) are ??????, ??????, 
and ??????, respectively. Although the substring 
????? appears three times, it does represent an 
actual part of Chinese characters. Hence, we manually 
examined all of the n-grams in the initial list, and re-
moved such n-grams from the list.  
In addition to considering the frequencies of n-
grams formed by the basic Cangjie codes to determine 
the list of components, we also took advantage of 
radicals that are used to categorize Chinese characters 
in typical printed dictionaries. Radicals that are stand-
alone Chinese words were included in the list of com-
ponents.  
After selecting the list of basic components with 
the above procedure, we encoded the words in Elist 
with these basic components. We inherited the 12 
ways reported in a previous work (Liu and Lin, 2008) 
to decompose Chinese characters. There are other 
methods for decomposing Chinese characters into 
components. Juang et al (2005) and their team at the 
Sinica Academia propose 13 different ways for de-
composing characters. 
At the same time when we annotated individual 
characters with their ECCs, we may revise the list of 
basic components. If a character that actually con-
tained an intuitively ?common? part and that part had 
not been included in the list of basic component, we 
would add this part into the list to make it a basic 
component and revised the ECC for all characters 
accordingly.  The judgment of being ?common? is 
subjective, but we still maintained the rule that such 
common parts must appear in more than three charac-
ters. When defining the basic components, not all 
judgments are completely objectively yet, and this is 
also the case of defining the original Cangjie codes. 
We tried to be as systematic as possible, but intuition 
sometimes stepped in. 
We repeated the procedure described in the preced-
ing paragraph five times to make sure that we were 
satisfied with the ECCs for all of the 5401 characters. 
The current list contains 794 components, and we can 
revise the list of basic components in our work when-
ever necessary. 
5.2 Recommending Incorrect Alternatives 
With the pronunciation of Chinese characters in a 
dictionary and with our ECC encodings for words in 
the Elist, we can create lists of candidate characters 
for replacing a specific correct character in a given 
word to create a test item for incorrect character cor-
rection.  
There are multiple strategies to create the candidate 
lists. We may propose the candidate characters be-
cause their pronunciations have the same sound and 
the same tone with those of the correct character (de-
noted SSST). Characters that have same sounds and 
87
different tones (SSDT), characters that have similar 
sounds and same tones (MSST), and characters that 
have similar sounds and different tones (MSDT) can 
be considered as candidates as well. It is easy to judge 
whether two Chinese characters have the same tone. 
In contrast, it is not trivial to define ?similar? sound. 
We adopted the list of similar sounds that was pro-
vided by a psycholinguistic researcher (Dr. Chia-Ying 
Lee) at the Sinica Academia. ??? (po) and ??? (bo) 
and ???(fan4) and ???(huan4) are pairs that have 
similar sounds. It was observed that these are four 
possible reasons that people used incorrect characters 
in writing. 
Because a Chinese character might be pronounced 
in multiple ways, character lists generated based on 
these strategies may include the same characters. 
More specifically, the lists SSST and SSDT may over-
lap when a character that can be pronounced in multi-
ple ways, and these pronunciations share the same 
sound and have different tones. The characters ??? 
and ??? are such examples. ??? can be pronounced 
as ?dai1? or ?dai4?, and ??? can be pronounced as 
?hao3? or ?hao4?. Hence, characters that can be pro-
nounced as ?hao3? will be listed in both SSST and 
SSDT for ???.  
In addition, we may propose characters that look 
similar to the correct character. Two characters may 
look similar for many reasons (Liu et al, 2009). The 
most common reason is that they contain the same 
components, and the other is that they belong to the 
same radical category and have the same total number 
of strokes (RS), e.g., the pairs ??? and ???, ??? 
and ???, and ??? and ???. When two characters 
contain the same component, the shared component 
might or might not locate at the same position, e.g., 
??? and ???.  
In an authoring tool, we could recommend a se-
lected number of candidate characters for replacing 
the correct character. We tried two different strategies 
to compare and choose the visually similar characters. 
The similarity is computed based on the number and 
the locations of shared Cangjie symbols in the ECCs 
of the characters. The first strategy (denoted SC1) 
gave a higher score to the shared component that lo-
cated at the same location in the two characters being 
compared. The second strategy (SC2) gave the same 
score to any shared component even if the component 
did not reside at the same location in the characters. 
The characters ???, ???, and ??? share the same 
component ???. When computing the similarity be-
tween these characters with SC1, the contribution of 
??? will be the same for any pair. When computing 
with SC2, the contribution of ??? will be larger for 
the pair ??? and ??? than for the pair ??? and ???. 
In the former case, ??? appears at the same location 
in the characters. 
 When there were more than 20 characters that re-
ceive nonzero scores in the SC1 and SC2 categories, 
we chose to select at most 20 characters that had lead-
ing scores as the list of recommended characters. 
We had to set a bound on the number of candidate 
characters, i.e., 20, for strategies SC1 and SC2.  The 
number of candidates generated from these two 
strategies can be large and artificial, depending on our 
scoring functions for determining similarities between 
characters. We did not limit the sizes of candidate 
lists that were generated by other strategies because 
those lists were created based on more objective 
methods. The rules for determining ?similar? sounds 
were given by the domain experts, so we considered 
the rules objective in this research. 
For the experiments that we reported in the follow-
ing subsection, we submitted more than 300 thousand 
of queries to Google. As we mentioned in Section 4.1, 
a frequent continual submission of queries to Google 
will make Google treat our programs as malicious 
processes. (We are studying the Google API for a 
more civilized solution.) Without the bound, it is pos-
sible to offer a very long list of candidates. On the 
other hand, it is also possible that our program does 
not find any visually similar characters for some spe-
cial characters, and this is considered a possible phe-
nomenon.  
5.3 Evaluating the Recommendations 
We examined the usefulness of these seven categories 
of candidates with errors in Elist and Jlist. The first 
set of evaluation (the inclusion tests) checked whether 
the lists of recommended characters contained the 
incorrect character in our records. The second set of 
evaluation (the ranking tests) was designed for practi-
cal application in computer assisted item generation. 
Only for those words whose actual incorrect charac-
ters were included in the recommended list, we re-
placed the correct characters in the words with the 
candidate incorrect characters, submitted the incorrect 
words to Google, and ordered the candidate characters 
based on their NOPs. We then recorded the ranks of 
the incorrect characters among all recommended 
characters.  
Since the same character may appear simultane-
ously in SC1, SC2, and RS, we computed the union of 
these three sets, and checked whether the incorrect 
characters were in the union. The inclusion rate is 
listed under Comp, representing the inclusion rate 
when we consider only logographic influences. Simi-
larly, we computed the union for SSST, SSDT, MSST, 
and MSDT, checked whether the incorrect characters 
were in the union, and recorded the inclusion rate 
under Pron, representing the inclusion rate when we 
consider only phonological influences. Finally, we 
computed the union of the lists created by the seven 
strategies, and recorded the inclusion rate under Both. 
The second and the third rows of Table 3 show the 
results of the inclusion tests when we recommended 
candidate characters with the methods indicated in the 
column headings. The data show the percentage of the 
incorrect characters being included in the lists that 
88
were recommended by the seven strategies. Notice 
that the percentages were calculated with different 
denominators. The number of composition-related 
errors was used for SC1, SC2, RS, and Comp (e.g., 
505 that we mentioned in Section 3 for Jlist); the 
number of pronunciation-related errors for SSST, 
SSDT, MSST, MSDT, and Pron (e.g., 1314 mentioned 
in Section 3 for the Jlist); the number of either of 
these two  types of errors for Both (e.g., 1475 for Jlist).  
The results recorded in Table 3 show that we were 
able to find the incorrect character quite effectively, 
achieving better than 93% for both Elist and Jlist. The 
statistics also show that it is easier to find incorrect 
characters that were used for pronunciation-related 
problems. Most of the pronunciation-related problems 
were misuses of homophones. Unexpected confusions, 
e.g., those related to pronunciations in Chinese dia-
lects, were the main reason for the failure to capture 
the pronunciation-related errors. (Namely, few pro-
nunciation-related errors were not considered in the 
information that the psycholinguist provided.) SSDT 
is a crucial complement to SSST.  
There is still room to improve our methods to find 
confusing characters based on their compositions. We 
inspected the list generated by SC1 and SC2, and 
found that, although SC2 outperformed SC1 on the 
inclusion rate, SC1 and SC2 actually generated com-
plementary lists in many cases, and should be used 
together. The inclusion rate achieved by the RS strat-
egy was surprisingly high. We found that many of the 
errors that were captured by the RS strategy were also 
captured by the SSST strategy. 
The fourth and the fifth rows of Table 3 show the 
effectiveness of relying on Google to rank the candi-
date characters for recommending an incorrect charac-
ter. The rows show the average ranks of the included 
cases. The statistics show that, with the help of 
Google, we were able to put the incorrect character on 
top of the recommended list when the incorrect char-
acter was included.  This allows us to build an envi-
ronment for assisting human teachers to efficiently 
prepare test items for incorrect character identification. 
Note that we did not provide data for all columns 
in the fourth and the firth rows. Unlike that we show 
the inclusion rates in the second and the third rows, 
the fourth and the fifth rows show how the actual in-
correct characters were ranked in the recommended 
lists. Hence, we need to have a policy to order the 
characters of different lists to find the ranks of the 
incorrect characters in the integrated list.  
However, integrating the lists is not necessary and 
can be considered confusing to the teachers. The se-
lection of incorrect characters from different lists is 
related to the goals of the assessment, and it is better 
to leave the lists separated for the teachers to choose. 
The same phenomenon and explanation apply to the 
sixth and the seventh rows as well. 
The sixth and the seventh rows show the average 
numbers of candidate characters proposed by different 
methods. Statistics shown between the second and the 
fifth rows are related to the recall rates (cf. Manning 
and Sch?tz, 1999) achieved by our system. For these 
four rows, we calculated how well the recommended 
lists contained the reported errors and how the actual 
incorrect characters ranked in the recommended lists. 
The sixth and the seventh rows showed the costs for 
these achievements, measured by the number of rec-
ommended characters. The sum of the sixth and the 
seventh rows, i.e., 103.59 and 108.75, are, respec-
tively, the average numbers of candidate characters 
that our system recommended as possible errors re-
corded in Elist and Jlist. (Note that some of these 
characters were repeated.) 
There are two ways to interpret the statistics shown 
in the sixth and the seventh rows. Comparing the cor-
responding numbers on the fourth and the sixth rows, 
e.g., 3.25 and 19.27, show the effectiveness of using 
the NOPs to rank the candidate characters. The ranks 
of the actual errors were placed at very high places, 
considering the number of the originally recom-
mended lists. The other way to use the statistics in the 
sixth and the seventh rows is to compute the average 
precision. For instance, we recommended an average 
19.13 characters in SSST to achieve the 91.64 inclu-
sion rate. The recall rate is very high, but the averaged 
precision is very low. This, however, is not a very 
convincing interpretation of the results. Having as-
sumed that there was only one best candidate as in our 
experiments, it was hard to achieve high precision 
rates. The recall rates are more important than the 
precision rates, particularly when we have proved that 
the actual errors were ranked among the top five al-
ternatives. 
When designing a system for assisting the author-
ing of test items, it is not really necessary to propose 
all of the characters in the categories. In the reported 
experiments, choosing the top 5 or top 10 candidates 
will contain the most of the actual incorrect characters 
based on the statistics shown in the fourth and the 
fifth rows. Hence the precision rates can be signifi-
cantly increased practically. We do not have to merge 
the candidate characters among different categories 
Table 3. Incorrect characters were contained and ranked high in the recommended lists 
 SC1 SC2 RS SSST SSDT MSST MSDT Comp Pron Both 
Elist 73.92% 76.08% 4.08% 91.64% 18.39% 3.01% 1.67% 81.97% 99.00% 93.37% 
Jlist 67.52% 74.65% 6.14% 92.16% 20.24% 4.19% 3.58% 77.62% 99.32% 97.29% 
Elist 3.25 2.91 1.89 2.30 1.85 2.00 1.58 
Jlist 2.82 2.64 2.19 3.72 2.24 2.77 1.16 
Elist 19.27 17.39 11.34 19.13 8.29 19.02 9.15 
Jlist 17.58 16.24 12.52 22.85 9.75 22.11 7.68 
89
because choosing the categories of incorrect charac-
ters depends on the purpose of the assessment. Reduc-
ing the length of the candidate list increases the 
chances of reducing the recall rates. Achieving the 
best trade off between precision and recall rates relies 
on a more complete set of experiments that involve 
human subjects. 
Furthermore, in a more realistic situation, there can 
be more than one ?good? incorrect character, not just 
one and only gold standard as in the reported experi-
ments. It is therefore more reasonable the compute the 
precision rates based the percentage of ?acceptable? 
incorrect characters. Hence, the precision rates are 
likely to increase and become less disconcerting.  
We reported experimental results in which we 
asked 20 human subjects to choose an incorrect char-
acter for 20 test items (Liu et al, 2009). The best so-
lutions were provided by a book. The recommenda-
tions provided by our previous system and chosen by 
the human subjects achieved comparable qualities.  
Notice that the numbers do not directly show the 
actual number of queries that we had to submit to 
Google to receive the NOPs for ranking the characters. 
Because the lists might contain the same characters, 
the sum of the rows showed just the maximum num-
ber of queries that we submitted. Nevertheless, they 
still served as good estimations, and we actually sub-
mitted 103.59?1441(=149273) and 108.75?1583 
(=172151) queries to Google for Elist and Jlist in ex-
periments from which we obtained the data shown in 
the fourth and the fifth rows. These quantities ex-
plained why we had to be cautious about how we 
submitted queries to Google. When we run our pro-
gram for just a limited number of characters, the prob-
lems caused by intensive queries should not be very 
serious. 
5.4 Discussions 
Dividing characters into subareas proved to be crucial 
in our experiments (Liu and Lin, 2008; Liu et al, 
2009), but this strategy is not perfect, and could not 
solve all of the problems. The way we divided Chi-
nese characters into subareas like (Juang et al, 2005; 
Liu and Lin, 2008) sometimes contributed to the fail-
ure of our current implementation to capture all of the 
errors that were related to the composition of the 
words. The most eminent reason is that how we di-
vide characters into areas. Liu and Lin (2008) fol-
lowed the division of Cangjie (Chu, 2009), and Juang 
et al (2005) proposed an addition way to split the 
characters.   
The best divisions of characters appear to depend 
on the purpose of the applications. Recall that each 
part of the character is represented by a string of 
Cangjie codes in ECCs. The separation of Cangjie 
codes in ECCs was instrumental to find the similarity 
of ??? and ??? because ??? is a standalone subpart 
in both ??? and ???. The Cangjie system has a set 
of special rules to divide Chinese characters (Chu, 
2009; Lee, 2008). Take ??? and ??? for example. 
The component ??? is recorded as an standalone part 
in ???, but is divided into two parts in ???. Hence, 
??? is stored as one string, ?????, in ??? and as 
two strings, ???? and ???, in ???. The different 
ways of saving ??? in two different words made it 
harder to find the similarity between ??? and ???. 
An operation of concatenation is in need, but the 
problems are that it is not obvious to tell when the 
concatenation operations are useful and which of the 
parts should be rejoined. Hence, using the current 
methods to divide Chinese characters, it is easy to 
find the similar between ??? and ??? but difficult to 
find the similar between ??? and ???. In contrast, if 
we enforce a rule to save ??? as one string of Cang-
jie code, it will turn the situations around. Determin-
ing the similarity between ??? and ??? will be more 
difficult than finding the similarity between ??? and 
???. 
Due to this observation, we have come to believe 
that it is better to save the Chinese characters with 
more detailed ECCs. By saving all detailed informa-
tion about a character, our system can offer candidate 
characters based on users? preferences which can be 
provided via a good user interface. This flexibility can 
be very helpful when we are preparing text materials 
for experiments for psycholinguistics or cognitive 
sciences (e.g., Leck et al 1995; Yeh and Li, 2002).  
6 Summary  
The analysis of the 1718 errors produced by real stu-
dents show that similarity between pronunciations of 
competing characters contributed most to the ob-
served errors. Evidences show that the Web statistics 
are not very reliable for differentiating correct and 
incorrect characters. In contrast, the Web statistics are 
good for comparing the attractiveness of incorrect 
characters for computer assisted item authoring.  
Acknowledgments 
This research was supported in part by the National 
Science Council of Taiwan under grant NSC-97-
2221-E-004-007-MY2. We thank anonymous review-
ers for their invaluable comments. 
References  
B.-F. Chu. 2009. Handbook of the Fifth Generation of 
the Cangjie Input Method, available at 
http://www.cbflabs.com/book/ocj5/ocj5/index.html. 
Last visited on 30 April 2009. 
D. Juang, J.-H. Wang, C.-Y. Lai, C.-C. Hsieh, L.-F. 
Chien, J.-M. Ho. 2005. Resolving the unencoded 
character problem for Chinese digital libraries, 
Proc. of the 5th ACM/IEEE Joint Conf. on Digital 
Libraries, 311?319. 
S.-P. Law, W. Wong, K. M. Y. Chiu. 2005. Whole-
word phonological representations of disyllabic 
90
words in the Chinese lexicon: Data from acquired 
dyslexia, Behavioural Neurology, 16, 169?177. 
K. J. Leck, B. S. Weekes, M. J. Chen. 1995. Visual 
and phonological pathways to the lexicon: Evi-
dence from Chinese readers, Memory & Cognition, 
23(4), 468?476. 
H. Lee. 2008. Cangjie Input Methods in 30 Days, 
http://input.foruto.com/cjdict/Search_1.php, Foruto 
Company, Hong Kong. Last visited on 30 April 
2009. 
C.-L. Liu, K.-W. Tien, Y.-H. Chuang, C.-B. Huang, 
J.-Y. Weng. 2009. Two applications of lexical in-
formation to computer-assisted item authoring for 
elementary Chinese, Proc. of the 22nd Int?l Conf. 
on Industrial Engineering & Other Applications of 
Applied Intelligent Systems, 470?480. 
C.-L. Liu, J.-H. Lin. 2008. Using structural informa-
tion for identifying similar Chinese characters, 
Proc. of the 46th ACL, short papers, 93?96. 
C. D. Manning, H. Sch?tze. Foundations of Statistical 
Natural Language Processing. The MIT Press. 
1999. 
MOE. 1996. Common Errors in Chinese Writings (?
?????), Ministry of Education, Taiwan. 
S.-L. Yeh, J.-L. Li. 2002. Role of structure and com-
ponent in judgments of visual similarity of Chinese 
characters, Journal of Experimental Psychology: 
Human Perception and Performance, 28(4), 933?
947.
 
91
Coling 2010: Poster Volume, pages 739?747,
Beijing, August 2010
Visually and Phonologically Similar Characters in Incorrect 
Simplified Chinese Words 
Chao-Lin Liu? Min-Hua Lai? Yi-Hsuan Chuang? Chia-Ying Lee?
???Department of Computer Science; ??Center for Mind, Brain, and Learning 
National Chengchi University 
?Institute of Linguistics, Academia Sinica 
{?chaolin, ?g9523, ?g9804}@cs.nccu.edu.tw, ?chiaying@gate.sinica.edu.tw 
Abstract
Visually and phonologically similar cha-
racters are major contributing factors for 
errors in Chinese text. By defining ap-
propriate similarity measures that consid-
er extended Cangjie codes, we can identi-
fy visually similar characters within a 
fraction of a second. Relying on the pro-
nunciation information noted for individ-
ual characters in Chinese lexicons, we 
can compute a list of characters that are 
phonologically similar to a given charac-
ter. We collected 621 incorrect Chinese 
words reported on the Internet, and ana-
lyzed the causes of these errors. 83% of 
these errors were related to phonological 
similarity, and 48% of them were related 
to visual similarity between the involved 
characters. Generating the lists of phono-
logically and visually similar characters, 
our programs were able to contain more 
than 90% of the incorrect characters in 
the reported errors. 
1 Introduction 
In this paper, we report the experience of our 
studying the errors in simplified Chinese words. 
Chinese words consist of individual characters. 
Some words contain just one character, but most 
words comprise two or more characters. For in-
stance, ??? (mai4)1 has just one character, and 
???? (yu3 yan2) is formed by two characters. 
Two most common causes for writing or typing 
incorrect Chinese words are due to visual and 
phonological similarity between the correct and 
1 We show simplified Chinese characters followed by 
their Hanyu pinyin. The digit that follows the symbols 
for the sound is the tone for the character. 
the incorrect characters. For instance, one might 
use ??? (hwa2) in the place of ???(hwa4)  in 
?????? (ke1 hwa4 xing2 xiang4) partially 
because of phonological similarity; one might 
replace ??? (zhuo2) in ?????? (xin1 lao2 
li4 zhuo2) with ??? (chu4) partially due to visu-
al similarity. (We do not claim that the visual or 
phonological similarity alone can explain the 
observed errors.) 
Similar characters are important for under-
standing the errors in both traditional and simpli-
fied Chinese. Liu et al (2009a-c) applied tech-
niques for manipulating correctness of Chinese 
words to computer assisted test-item generation. 
Research in psycholinguistics has shown that the 
number of neighbor characters influences the 
timing of activating the mental lexicon during the 
process of understanding Chinese text (Kuo et al 
2004; Lee et al 2006).  Having a way to compute 
and find similar characters will facilitate the 
process of finding neighbor words, so can be in-
strumental for related studies in psycholinguistics. 
Algorithms for optical character recognition for 
Chinese and for recognizing written Chinese try 
to guess the input characters based on sets of 
confusing sets (Fan et al 1995; Liu et al, 2004). 
The confusing sets happen to be hand-crafted 
clusters of visually similar characters. 
It is relatively easy to judge whether two cha-
racters have similar pronunciations based on 
their records in a given Chinese lexicon. We will 
discuss more related issues shortly.  
To determine whether two characters are vi-
sually similar is not as easy. Image processing 
techniques may be useful but is not perfectly 
feasible, given that there are more than fifty 
thousand Chinese characters (HanDict, 2010) 
and that many of them are similar to each other 
in special ways.  Liu et al (2008) extend the 
Cangjie codes (Cangjie, 2010; Chu, 2010) to en-
code the layouts and details about traditional 
739
Chinese characters for computing visually simi-
lar characters. Evidence observed in psycholin-
guistic studies offers a cognition-based support 
for the design of Liu et al?s approach (Yeh and 
Li, 2002). In addition, the proposed method 
proves to be effective in capturing incorrect tra-
ditional Chinese words (Liu et al, 2009a-c). 
In this paper, we work on the errors in simpli-
fied Chinese words by extending the Cangjie 
codes for simplified Chinese. We obtain two lists 
of incorrect words that were reported on the In-
ternet, analyze the major reasons that contribute 
to the observed errors, and evaluate how the new 
Cangjie codes help us spot the incorrect charac-
ters. Results of our analysis show that phonolog-
ical and visual similarities contribute similar por-
tions of errors in simplified and traditional Chi-
nese. Experimental results also show that, we can 
catch more than 90% of the reported errors. 
We go over some issues about phonological 
similarity in Section 2, elaborate how we extend 
and apply Cangjie codes for simplified Chinese 
in Section 3, present details about our experi-
ments and observations in Section 4, and discuss 
some technical issues in Section 5.  
2 Phonologically Similar Characters 
The pronunciation of a Chinese character in-
volves a sound, which consists of the nucleus and 
an optional onset, and a tone. In Mandarin Chi-
nese, there are four tones. (Some researchers in-
clude the fifth tone.) 
In our work, we consider four categories of 
phonological similarity between two characters: 
same sound and same tone (SS), same sound and 
different tone (SD), similar sound and same tone 
(MS), and similar sound and different tone (MD).  
We rely on the information provided in a lex-
icon (Dict, 2010) to determine whether two cha-
racters have the same sound or the same tone. 
The judgment of whether two characters have 
similar sound should consider the language expe-
rience of an individual. One who live in the 
southern and one who live in the northern China 
may have quite different perceptions of ?similar? 
sound. In this work, we resort to the confusion 
sets observed in a psycholinguistic study con-
ducted at the Academic Sinica. 
Some Chinese characters are heteronyms. Let 
C1 and C2 be two characters that have multiple 
pronunciations. If C1 and C2 share one of their 
pronunciations, we consider that C1 and C2 be-
long to the SS category. This principle applies 
when we consider phonological similarity in oth-
er categories. 
One challenge in defining similarity between 
characters is that the pronunciations of a charac-
ter can depend on its context. The most common 
example of tone sandhi in Chinese (Chen, 2000) 
is that the first third-tone character in words 
formed by two adjacent third-tone characters will 
be pronounced in the second tone. At present, we 
ignore the influences of context when determin-
ing whether two characters are phonologically 
similar.  
Although we have confined our definition of 
phonological similarity to the context of the 
Mandarin Chinese, it is important to note the in-
fluence of sublanguages within the Chinese lan-
guage family will affect the perception of phono-
logical similarity. Sublanguages used in different 
areas in China, e.g., Shanghai, Min, and Canton 
share the same written forms with the Mandarin 
Chinese, but have quite different though related 
pronunciation systems. Hence, people living in 
different areas in China may perceive phonologi-
cal similarity in very different ways. The study in 
this direction is beyond the scope of the current 
study.  
3 Visually Similar Characters 
Figure 1 shows four groups of visually similar 
characters. Characters in group 1 and group 2 
differ subtly at the stroke level. Characters in 
group 3 share the components on their right sides. 
The shared component of the characters in group 
4 appears at different places within the characters. 
Radicals are used in Chinese dictionaries to 
organize characters, so are useful for finding vi-
sually similar characters. The characters in group 
1 and group 2 belong to the radicals ??? and ? ?,
respectively. Notice that, although the radical for 
group 2 is clear, the radical for group 1 is not 
obvious because ??? is not a standalone compo-
nent.
However, the shared components might not be 
the radicals of characters. The shared compo-
nents in groups 3 and 4 are not the radicals. In 
Figure 1. Examples of visually similar characters
740
many cases, radicals are semantic components of 
Chinese characters. In groups 3 and 4, the shared 
components carry information about the pronun-
ciations of the characters. Hence, those charac-
ters are listed under different radicals, though 
they do look similar in some ways.  
Hence, a mechanism other than just relying on 
information about characters in typical lexicons 
is necessary, and we will use the extended Cang-
jie codes for finding visually similar characters. 
3.1 Cangjie Codes for Simplified Chinese 
Table 1 shows the Cangjie codes for the 13 
characters listed in Figure 1 and five other 
characters. The ?ID? column shows the 
identification number for the characters, and we 
will refer to the ith character by ci, where i is the 
ID. The ?CC? column shows the Chinese 
characters, and the ?Cangjie? column shows the 
Cangjie codes. Each symbol in the Cangjie codes 
corresponds to a key on the keyboard, e.g. ???
and ? ? ? collocate with ?W? and ?L?, 
respectively. Information about the complete 
correspondence is available on the Wikipedia2.
Using the Cangjie codes saves us from using 
image processing methods to determine the de-
grees of similarity between characters. Take the 
Cangjie codes for the characters in group 2 (c5, c6,
and c7) for example. It is possible to find that the 
characters share a common component, based on 
the shared substrings of the Cangjie codes, i.e., 
????.  Using the common substring  (shown in 
black bold) of the Cangjie codes, we may also 
find the shared component ??? for characters in 
group 3 (c10, c11, and c12), the shared component 
??? in c13 and c14, the shared component ??? in 
c15 and c16, and the shared component ? ? in c16
and c17.
 Despite the perceivable advantages, these 
original Cangjie codes are not good enough. In 
order to maintain efficiency in inputting Chinese 
characters, the Cangjie codes have been limited 
to no more than five keys. Thus, users of the 
Cangjie input method must familiarize them-
selves with the principles for simplifying the 
Cangjie codes. While the simplified codes help 
the input efficiency, they also introduce difficul-
ties and ambiguities when we compare the Cang-
2en.wikipedia.org/wiki/Cangjie_input_method#Keyboard_la
yout ; last visited on 22 April 2010. 
jie codes for computing similar characters. The 
prefix ???? in c16 and c17 can represent ? ?, 
??? (e.g., c8), and ??? (e.g., c9). Characters 
whose Cangjie codes include ???? may con-
tain any of these three components, but they do 
not really look alike. 
Therefore, we augment the original Cangjie 
codes by using the complete Cangjie codes and 
annotate each Chinese character with a layout 
identification that encodes the overall contours of 
the characters. This is how Liu and his col-
leagues (2008) did for the Cangjie codes for tra-
ditional Chinese characters, and we employ a 
similar exploration for the simplified Chinese. 
3.2 Augmenting the Cangjie Codes 
Figure 2 shows the twelve possible layouts that 
are considered for the Cangjie codes for 
simplified Chinese characters. Some of the 
layouts contain smaller areas, and the rectangles 
show a subarea within a character. The smaller 
areas are assigned IDs between one and three. 
Notice that, to maintain read-ability of the 
figures, not all IDs for subareas are shown in 
Figure 2. An example character is provided 
below each layout. From left to right and from 
top to bottom, each layout is assigned an 
identification number from 1 to 12. For example, 
the layout ID of ??? is 8. ??? has two parts, i.e., 
??? and ???.
Researchers have come up with other ways to 
ID CC Cangjie ID CC Cangjie 
1 ? ?! 10 ?! ????!
2 ? ??! 11 ?! ???!
3 ? ??! 12 ?! ???!
4 ? ???! 13 ?! ???!
5 ? ????! 14 ?! ????!
6 ? ????! 15 ?! ????!
7 ? ???! 16 ?! ????!
8 ? ???? 17 ?! ?????!
9 ? ???? 18 ?! ?????!
Table 1. Examples of Cangjie codes 
Figure 2. Layouts of Chinese characters 
741
decompose individual Chinese characters. The 
Chinese Document Lab at the Academia Sinica 
proposed a system with 13 operators for describ-
ing the relationships among components in Chi-
nese characters (CDL, 2010). Lee (2010b) pro-
pose more than 30 possible layouts.  
The layout of a character affects how people 
perceive visual similarity between characters. 
For instance, c16 in Table 1 is more similar to c17
than to c18, although they share ? ?. We rely on 
the expertise in Cangjie codes reported in (Lee, 
2010a) to split the codes into parts. 
Table 2 shows the extended codes for some 
characters listed in Table 1. The ?ID? column 
provides links between the characters listed in 
both Table 1 and Table 2. The ?CC? column 
shows the Chinese characters. The ?LID? column 
shows the identifications for the layouts of the 
characters. The columns with headings ?P1?, 
?P2?, and ?P3? show the extended Cangjie codes, 
where ?Pi? shows the ith part of the Cangjie 
codes, as indicated in Figure 2. 
We decide the extended codes for the parts 
with the help of computer programs and subjec-
tive judgments. Starting from the original Cang-
jie codes, we can compute the most frequent sub-
strings just like we can compute the frequencies 
of n-grams in corpora (cf. Jurafsky and Martin, 
2009). Computing the most common substrings 
in the original codes is not a complex task be-
cause the longest original Cangjie codes contain 
just five symbols.   
Often, the frequent substrings are simplified 
codes for popular components in Chinese charac-
ters, e.g., ? ? and ? ?. The original codes for ? ?
and ? ? are ????? and ?????, but they are 
often simplified to ???? and ????, respec-
tively.  When simplified, ? ? have the same 
Cangjie code with ???, and ? ? have the same 
Cangjie code with ??? and ???.
After finding the frequent substrings, we veri-
fy whether these frequent substrings are simpli-
fied codes for meaningful components. For mea-
ningful components, we replace the simplified 
codes with complete codes. For instance the 
Cangjie codes for ??? and ??? are extended to 
include ??? in Table 2, where we indicate the 
extended keys that did not belong to the original 
Cangjie codes in boldface and with a surrounding 
box. Most of the non-meaningful frequent sub-
strings have two keys: one is the last key of a 
part, and the other is the first key of another part. 
They were by observed by coincidence. 
Although most of the examples provided in 
Table 2 indicate that we expand only the first 
part of the Cangjie codes, it is absolutely possible 
that the other parts, i.e., P2 and P3, may need to 
be extended too. c19 shows such an example. 
Replacing simplified codes with complete 
codes not only help us avoid incorrect matches 
but also help us find matches that would be 
missed due to simplification of Cangjie codes. 
Using just the original Cangjie codes in Table 1, 
it is not easy to determine that c18 (???) in Table 
1 shares a component (? ?) with c16 and c17 (???
and ???). In contrast, there is a chance to find 
the similarity with the extended Cangjie codes in 
Table 2, given that all of the three Cangjie codes 
include ?????.
We can see an application of the LIDs, using 
???, ??? and ??? as an example. Consider the 
case that we want to determine which of ???
and ??? is more similar to ???. Their extended 
Cangjie codes will indicate that ??? is the an-
swer to this question for two reasons. First, ???
and ??? belong to the same type of layout; and, 
second, the shared components reside at the same 
area in ??? and ???.
3.3 Similarity Measures 
The main differences between the original and 
the extended Cangjie codes are the degrees of 
details about the structures of the Chinese cha-
racters. By recovering the details that were ig-
nored in the original codes, our programs will be 
ID CC LID P1 P2 P3
5 ? 2 ???! ??! !
6 ? 2 ???! ??! !
7 ? 2 ???! ?! !
10 ? 10 ??! ?! ?!
11 ? 10 ?! ?! ?!
12 ? 10 ?! ?! ?!
13 ? 5 ?! ??! !
14 ? 9 ?! ?! ??!
15 ? 2 ???! ??! !
16 ? 2 ???! ??! !
17 ? 2 ???! ???! !
18 ? 3 ???! ??! ?!
19 ? 4 ?! ???! ??!
Table 2. Examples of extended Cangjie codes 
742
better equipped to find the similarity between 
characters.  
In the current study, we experiment with three 
different scoring methods to measure the visual 
similarity between two characters based on their 
extended Cangjie codes. Two of these methods 
had been tried by Liu and his colleagues? study 
for traditional Chinese characters (Liu et al, 
2009b-c). The first method, denoted SC1, con-
siders the total number of matched keys in the 
matched parts (without considering their part 
IDs). Let ci denote the i
th character listed in Table 
2. We have SC1(c15, c16) = 2 because of the 
matched ????. Analogously, we have SC1(c19,
c16) = 2.  
The second method, denoted SC2, includes 
the score of SC1 and considers the following 
conditions: (1) add one point if the matched parts 
locate at the same place in the characters and (2) 
if the first condition is met, an extra point will be 
added if the characters belong to the same layout.  
Hence, we have SC2(c15, c16) =SC1(c15,
c16)+1+1=4 because (1) the matched ???? lo-
cate at P2 in both characters and (2) c15 and c16
belong to the same layout. Assuming that c16 be-
longs to layout 5, than SC2(c15, c16) would be-
come 3. In contrast, we have SC2(c19, c16)=2. No 
extra weights for the matching ???? because it 
locates at different parts in the characters. The 
extra weight considers the spatial influences of 
the matched parts on the perception of similarity. 
While splitting the extended Cangjie codes in-
to parts allows us to tell that c15 is more similar 
to c16 than to c19, it also creates a new barrier in 
computing similarity scores. An example of this 
problem is that SC2(c17, c18)=0. This is because 
that ????? at P1 in c17 can match neither ??
?? at P2 nor ??? at P3 in c18.
To alleviate this problem, we consider SC3 
which computes the similarity in three steps. 
First, we concatenate the parts of a Cangjie code 
for a character. Then, we compute the longest 
common subsequence (LCS) (cf. Cormen et al, 
2009) of the concatenated codes of the two cha-
racters being compared, and compute a Dice?s 
coefficient (cf. Croft et al, 2010) as the similari-
ty. Let X and Y denote the concatenated, ex-
tended Cangjie codes for two characters, and let 
Z be the LCS of X and Y. The similarity is de-
fined by the following equation.  
S
YX
Z
DiceLCS stringoflength theisS where,
2

u
   (1) 
We compute another Dice?s coefficient be-
tween X and Y. The formula is the similar to (1), 
except that we set Z to the longest common con-
secutive subsequence. We call this score 
LCCSDice . Notice that LCSLCCS DiceDice d ,
1dLCCSDice , and 1dLCSDice  . Finally, SC3 of two 
characters is the sum of their SC2, LCCSDiceu10 ,
and LCSDiceu5 . We multiply the Dice?s coeffi-
cients with constants to make them as influential 
as the SC2 component in SC3. The constants 
were not scientifically chosen, but were selected 
heuristically. 
4 Error Analysis and Evaluation 
We evaluate the effectiveness of using the pho-
nologically and visually similar characters to 
captures errors in simplified Chinese words with 
two lists of reported errors that were collected 
from the Internet.  
4.1 Data Sources 
We need two types of data for the experiments. 
The information about the pronunciation and 
structures of the Chinese characters help us gen-
erate lists of similar characters. We also need 
reported errors so that we can evaluate whether 
the similar characters catch the reported errors. 
A lexicon that provides the pronunciation in-
formation about Chinese characters and a data-
base that contains the extended Cangjie codes are 
necessary for our programs to generate lists of 
characters that are phonologically and visually 
similar to a given character. 
It is not difficult to acquire lexicons that show 
standard pronunciations for Chinese characters. 
As we stated in Section 2, the main problem is 
that it is not easy to predict how people in differ-
ent areas in China actually pronounce the charac-
ters. Hence, we can only rely on the standards 
that are recorded in lexicons.  
With the procedure reported in Section 3.2, we 
built a database of extended Cangjie codes for 
the simplified Chinese. The database was de-
signed to contain 5401 common characters in the 
BIG5 encoding, which was originally designed 
for the traditional Chinese. After converting the 
traditional Chinese characters to the simplified 
counterparts, the database contained only 5170
743
different characters. 
We searched the Internet for reported errors 
that were collected in real-world scenarios, and 
obtained two lists of errors. The first list3 came 
from the entrance examinations for senior high 
schools in China, and the second list4 contained 
errors observed at senior high schools in China. 
We used 160 and 524 errors from the first and 
the second lists, respectively, and we refer to the 
combined list as the Ilist. An item of reported 
error contained two parts: the correct word and 
the mistaken character, both of which will be 
used in our experiments. 
4.2 Preliminary Data Analysis 
Since our programs can compare the similarity 
only between characters that are included in our 
lexicon, we have to exclude some reported errors 
from the Ilist. As a result, we used only 621 er-
rors in this section.  
Two native speakers subjectively classified the 
causes of these errors into three categories based 
on whether the errors were related to phonologi-
cal similarity, visual similarity, or neither. Since 
the annotators did not always agree on their clas-
sifications, the final results have five interesting 
categories: ?P?, ?V?, ?N?, ?D?, and ?B? in Table 
3. P and V indicate that the annotators agreed on 
the types of errors to be related to phonological 
and visual similarity, respectively. N indicates 
that the annotators believed that the errors were 
not due to phonological or visual similarity. D 
indicates that the annotators believed that the 
errors were due to phonological or visual similar-
ity, but they did not have a consensus. B indi-
cates the intersection of P and V.  
Table 3 shows the percentages of errors in 
these categories. To get 100% from the table, we 
can add up P, V, N, and D, and subtract B from 
the total. In reality there are errors of type N, and 
Liu and his colleagues (2009b) reported this type 
of errors. Errors in this category happened to be 
missing in the Ilist. Based on our and Liu?s ob-
3www.0668edu.com/soft/4/12/95/2008/2008091357140.htm
 ; last visited on 22 April 2010. 
4 gaozhong.kt5u.com/soft/2/38018.html; last visited on 22 
April 2010. 
servations, the percentages of phonological and 
visual similarities contribute to the errors in sim-
plified and traditional Chinese words with simi-
lar percentages.  
4.3 Experimental Procedure 
We design and employ the ICCEval procedure 
for the evaluation task. 
At step 1, given the correct word and the cor-
rect character to be intentionally replaced with 
incorrect characters, we created a list of charac-
ters based on the selection criterion. We may 
choose to evaluate phonologically or visually 
similar characters. For a given character, ICCEv-
al can generate characters that are in the SS, SD, 
MS, and MD categories for phonologically simi-
lar characters (cf. Section 2). For visually similar 
characters, ICCEval can select characters based 
on SC1, SC2, and SC3 (cf. Section 3.3). In addi-
tion, ICCEval can generate a list of characters 
that belong to the same radical and have the same 
number of strokes with the correct character. In 
the experimental results, we refer to this type of 
similar characters as RS.
At step 2, for a correct word that people origi-
nally wanted to write, we replaced the correct 
character with an incorrect character with the 
characters that were generated at step 1, submit-
ted the incorrect word to Google AJAX Search 
 P V N D B 
Ilist 83.1 48.3 0 3.7 35.1
Table 3. Percentages of types of errors
Procedure ICCEval
Input:
ccr: the correct character; cwd:
the correct word; crit: the selec-
tion criterion; num: number of re-
quested characters; rnk: the cri-
terion to rank the incorrect 
words;
Output: a list of ranked candidates 
for ccr 
Steps:
1. Generate a list, L, of charac-
ters for ccr with the specified 
criterion, crit. When using SC1, 
SC2, or SC3 to select visually 
similar characters, at most num
characters will be selected. 
2. For each c in L, replace ccr in 
cwd with c, submit the resulting 
incorrect word to Google, and 
record the ENOP. 
3. Rank the list of incorrect words 
generated at step 2, using the 
criterion specified by rnk.
4. Return the ranked list. 
744
API, and extracted the estimated numbers of 
pages (ENOP) 5  that contained the incorrect 
words. In an ordinary interaction with Google, an 
ENOP can be retrieved from the search results, 
and it typically follows the string ?Results 1-
10 of about? on the upper part of the browser 
window. Using the AJAX API, we just have to 
parse the returned results with a simple method.  
Larger ENOPs for incorrect words suggest 
that these words are incorrect words that people 
frequently used on their web pages. Hence, we 
ranked the similar characters based on their 
ENOPs at step 3, and return the list. 
Since the reported errors contained informa-
tion about the incorrect ways to write the correct 
words, we could check whether the real incorrect 
characters were among the similar characters that 
our programs generated at step 1 (inclusion tests). 
We could also check whether the actual incorrect 
characters were ranked higher in the ranked lists 
(ranking tests). 
Take the word ?????? as an example. In 
the collected data, it is reported that people wrote 
this word as ??????, i.e., the second charac-
ter was incorrect. Hoping to capture the error, 
ICCEval generated a list of possible substitutions 
for ??? at step 1. Depending on the categories 
of sources of errors, ICCEval generated a list of 
characters. When aiming to test the effectiveness 
of visually similar characters, we could ask IC-
CEval to apply SC3 to generate a list of alterna-
tives for ???, possibly including ???, ???, 
???, and other candidates. At step 2, we created 
and submitted query strings ??????, ???
???, and ?????? to obtain the ENOPs for 
the candidates. If the ENOPs were, respectively, 
410000, 26100, and 7940, these candidates 
would be returned in the order of ???, ???, and 
???. As a result, the returned list contained the 
actual incorrect character ???, and placed ???
on top of the ranked list. 
Notice that we considered the contexts in 
which the incorrect characters appeared to rank. 
We did not rank the incorrect characters with just 
the unigrams. In addition, although this running 
example shows that we ranked the characters 
directly with the ENOPs, we also ranked the list 
5According to (Croft et al, 2010), the ENOPs may not re-
flect the actual number of pages on the Internet. 
of alternatives with pointwise mutual information: 
 
)Pr()Pr(
)Pr(
,
XC
XC
XCPMI
u
?
 ,                 (2) 
where X is the candidate character to replace the 
correct character and C is the correct word ex-
cluding the correct character to be replaced. To 
compute the score of replacing ??? with ??? in 
??????, X = ???, C=??????, and (C?X)
is ??????. (?!denotes a character to be re-
placed.) PMI is a common tool for judging collo-
cations in natural language processing. (cf. Ju-
rafsky and Martin, 2009). 
It would demand very much computation ef-
fort to find Pr(C). Fortunately, we do not have to 
consider Pr(C) because it is a common denomi-
nator for all incorrect characters. Let X1 and X2
be two competing candidates for the correct cha-
racter. We can ignore Pr(C) because of the fol-
lowing relationship. 
   
)Pr(
)Pr(
)Pr(
)Pr(
,,
2
2
1
1
21 X
XC
X
XC
XCPMIXCPMI
?
t
?
?t
Hence, X1 prevails if  1, XCscore  is larger. 
 
)Pr(
)Pr(
,
X
XC
XCscore
?                    (3) 
In our work, we approximate the probabilities 
used in (3) by the corresponding frequencies that 
we can collect through Google, similar to the 
methods that we used to collect the ENOPs. 
4.4 Experimental Results: Inclusion Tests 
We ran ICCEval with 621 errors in the Ilist. The 
experiments were conducted for all categories of 
phonological and visual similarity. When using 
SS, SD, MS, MD, and RS as the selection crite-
rion, we did not limit the number of candidate 
characters. When using SC1, SC2, and SC3 as 
the criterion, we limited the number candidates 
to be no more than 30. We consider only words 
that the native speakers have consensus over the 
causes of errors. Hence, we dropped those 3.7% 
of words in Table 3, and had just 598 errors. The 
ENOPs were obtained during March and April 
2010. 
Table 4 shows the chances that the lists, gen-
SS SD MS MD Phone
Ilist 82.6 29.3 1.7 1.6 97.3 
SC1 SC2 SC3 RS Visual
Ilist 78.3 71.0 87.7 1.3 90.0 
Table 4. Chances of the recommended list con-
tains the incorrect character
745
erated with different crit at step 1, contained the 
incorrect character in the reported errors. In the 
Ilist, there were 516 and 3006  errors that were 
related to phonological and visual similarity, re-
spectively. Using the characters generated with 
the SS criterion, we captured 426 out of 516 
phone-related errors, so we showed 426/516 = 
82.6% in the table. 
Results in Table 4 show that we captured 
phone-related errors more effectively than visual-
ly-similar errors. With a simple method, we can 
compute the union of the characters that were 
generated with the SS, SD, MS, and MD criteria. 
This integrated list suggested how well we cap-
tured the errors that were related to phones, and 
we show its effectiveness under ?Phone?. Simi-
larly, we integrated the lists generated by SC1, 
SC2, SC3, and RS to explore the effectiveness of 
finding errors that are related to visual similarity, 
and the result is shown under ?Visual?. 
4.5 Experimental Results: Ranking Tests 
To put the generated characters into work, we 
wish to put the actual incorrect character high in 
the ranked list. This will help the efficiency in 
supporting computer assisted test-item writing. 
Having short lists that contain relatively more 
confusing characters may facilitate the data prep-
aration for psycholinguistic studies. 
At step 3, we ranked the candidate characters 
by forming incorrect words with other characters 
in the correct words as the context and submitted 
the words to Google for ENOPs. The results of 
ranking, shown in Table 5, indicate that we may 
just offer the leading five candidates to cover the 
actual incorrect characters in almost all cases.  
The ?Total? column shows the total number of 
errors that were captured by the selection crite-
rion. The column ?Ri? shows the percentage of 
all errors, due to phonological or visual similarity, 
that were re-created and ranked ith at step 3 in 
ICCEVAL. The row headings show the selection 
criteria that were used in the experiments. For 
instance, using SS as the criterion, 70.3% of ac-
tual phone-related errors were rank first, 7.4% of 
the phone-related errors were ranked second, etc. 
If we recommended only 5 leading incorrect cha-
6The sum of 516 and 300 is larger than 598 because 
some of the characters are similar both phonologically 
and visually.
racters only with SS, we would have captured the 
actual incorrect characters that were phone re-
lated 81.6% (the sum of R1 to R5) of the time. 
For errors that were related to visual similarity, 
recommending the top five candidates with SC3 
would capture the actual incorrect characters 
87.1% of the time. Since we do not show the 
complete distributions, the sums over the rows 
are not 100%. In the current experiments, the 
worst rank was 21. 
We also used PMI to rank the incorrect words. 
Due to page limits, we cannot show complete 
details about the results. The observed distribu-
tions in ranks were not very different from those 
shown in Table 5. 
5 Discussion 
Compared with Liu et al?s analysis (2009b-c) 
for the traditional Chinese, the proportions of 
errors related to phonological factors are almost 
the same, both at about 80%. The proportion of 
errors related to visual factors varied, but the av-
erages in both studies were about 48%. A larger 
scale of study is needed for how traditional and 
simplified characters affect the distributions of 
errors. Results shown in Table 4 suggest that it is 
relatively easy to capture errors related to visual 
factors in simplified Chinese. Although we can-
not elaborate, we note that Cangjie codes are not 
good for comparing characters that have few 
strokes, e.g., c1 to c4 in Table 1. In these cases, 
the coding method for Wubihua input method 
(Wubihua, 2010) should be applied. 
Acknowledgement 
This research was supported in part by the research 
contract NSC-97-2221-E-004-007-MY2 from the Na-
tional Science Council of Taiwan. We thank the ano-
nymous reviewers for constructive comments. Al-
though we are not able to respond to all the comments 
Total R1 R2 R3 R4 R5
SS 426 70.3 7.4 2.9 0.4 0.6
SD 151 25.6 2.7 0.6 0.0 0.4
MS 9 1.4 0.4 0.0 0.0 0.0
MD 8 1.6 0.0 0.0 0.0 0.0
SC1 235 61.3 10.3 4.3 2.0 0.3
SC2 213 53.7 11.0 3.7 2.3 0.3
SC3 263 66.7 12.7 5.7 1.7 0.3
RS 4 1.3 0.0 0.0 0.0 0.0
Table 5. Ranking the candidates 
746
in this paper, we have done so in an extended version 
of this paper. 
References 
Cangjie. 2010. Last visited on 22 April 2010: 
en.wikipedia.org/wiki/Cangjie_input_method. 
CDL. 2010. Chinese document laboratory, Academia 
Sinica. Last visited on 22 April, 2010; 
cdp.sinica.edu.tw/cdphanzi/. (in Chinese) 
Chen, Matthew. Y. 2000. Tone Sandhi: Patterns 
across Chinese Dialects, (Cambridge Studies in 
Linguistics 92). Cambridge University Press. 
Chu, Bong-Foo. 2010. Handbook of the Fifth Genera-
tion of the Cangjie Input Method. last visited on 22 
April 2010: www.cbflabs.com/book/5cjbook/. (in Chi-
nese) 
Cormen, Thomas H., Charles E. Leiserson, Ronald L. 
Rivest, and Clifford Stein. 2009. Introduction to 
Algorithms, third edition. MIT Press. 
Croft, W. Bruce, Donald Metzler, and Trevor Stroh-
man, 2010. Search Engines: Information Retrieval 
in Practice, Pearson. 
Dict. 2010. Last visited on 22 April 2010,
www.cns11643.gov.tw/AIDB/welcome.do 
Fan, Kuo-Chin, Chang-Keng Lin, and Kuo-Sen Chou. 
1995. Confusion set recognition of on-line Chinese 
characters by artificial intelligence technique. Pat-
tern Recognition, 28(3):303?313. 
HanDict. 2010. Last visit on 22 April 2010, 
www.zdic.net/appendix/f19.htm. 
Jurafsky, Daniel and James H. Martin. 2009. Speech 
and Language Processing, second edition, Pearson. 
Kuo, Wen-Jui, Tzu-Chen Yeh, Jun-Ren Lee, Li-Fen 
Chen, Po-Lei Lee, Shyan-Shiou Chen, Low-Tone 
Ho, Daisy L. Hung, Ovid J.-L. Tzeng, and Jen-
Chuen Hsieh. 2004. Orthographic and phonological 
processing of Chinese characters: An fMRI study. 
NeuroImage, 21(4):1721?1731. 
Lee, Chia-Ying, Jie-Li Tsai, Hsu-Wen Huang, Daisy 
L. Hung, Ovid J.-L. Tzeng. 2006. The temporal 
signatures of semantic and phonological activations 
for Chinese sublexical processing: An even-related 
potential study. Brain Research, 1121(1):150-159. 
Lee, Hsiang. 2010a. Cangjie Input Methods in 30 
Days 2. Foruto. Last visited on 22 April 2010:  in-
put.foruto.com/cccls/cjzd.html. 
Lee, Mu. 2010b. A quantitative study of the formation 
of Chinese characters. Last visited on 22 April 
2010: chinese.exponode.com/0_1.htm. (in Chinese) 
Liu, Chao-Lin, and Jen-Hsiang Lin. 2008. Using 
structural information for identifying similar Chi-
nese characters. Proc. of the 46th Annual Meeting 
of the Association for Computational Linguistics,
short papers, 93?96.
Liu, Chao-Lin, Kan-Wen Tien, Yi-Hsuan Chuang, 
Chih-Bin Huang, and Juei-Yu Weng. 2009a. Two 
applications of lexical information to computer-
assisted item authoring for elementary Chinese. 
Proc. of the 22nd Int?l Conf. on Industrial En-
gineering & Other Applications of Applied Intel-
ligent Systems, 470?480. 
Liu, Chao-Lin, Kan-Wen Tien, Min-Hua Lai, Yi-
Hsuan Chuang, and Shih-Hung Wu. 2009b. Cap-
turing errors in written Chinese words. Proc. of the 
47th Annual Meeting of the Association for Compu-
tational Linguistics, short papers, 25?28. 
Liu, Chao-Lin, Kan-Wen Tien, Min-Hua Lai, Yi-
Hsuan Chuang, and Shih-Hung Wu. 2009c. Phono-
logical and logographic influences on errors in 
written Chinese words. Proc. of the 7th Workshop 
on Asian Language Resources, the 47th Annual 
Meeting of the ACL, 84?91. 
Liu, Cheng-Lin, Stefan Jaeger, and Masaki Nakagawa. 
2004. Online recognition of Chinese characters: 
The state-of-the-art. IEEE Transaction on Pattern 
Analysis and Machine Intelligence, 26(2):198?213. 
Wubihua. 2010. Last visited on 22 April 2010: 
en.wikipedia.org/wiki/Wubihua_method. 
Yeh, Su-Ling, and Jing-Ling Li. 2002. Role of struc-
ture and component in judgments of visual simi-
larity of Chinese Characters. Journal of Expe-
rimental Psychology: Human Perception and Per-
formance, 28(4):933?947. 
747
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?6,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Applications of GPC Rules and Character Structures in Games for 
Learning Chinese Characters 
?Wei-Jie Huang ?Chia-Ru Chou ?Yu-Lin Tzeng ?Chia-Ying Lee ?Chao-Lin Liu 
??National Chengchi University, Taiwan ???Academia Sinica, Taiwan 
?chaolin@nccu.edu.tw, ?chiaying@gate.sinica.edu.tw 
Abstract 
We demonstrate applications of psycholin-
guistic and sublexical information for learn-
ing Chinese characters. The knowledge 
about the grapheme-phoneme conversion 
(GPC) rules of languages has been shown to 
be highly correlated to the ability of reading 
alphabetic languages and Chinese. We build 
and will demo a game platform for 
strengthening the association of phonologi-
cal components in Chinese characters with 
the pronunciations of the characters. Results 
of a preliminary evaluation of our games 
indicated significant improvement in learn-
ers? response times in Chinese naming 
tasks. In addition, we construct a Web-
based open system for teachers to prepare 
their own games to best meet their teaching 
goals. Techniques for decomposing Chinese 
characters and for comparing the similarity 
between Chinese characters were employed 
to recommend lists of Chinese characters 
for authoring the games. Evaluation of the 
authoring environment with 20 subjects 
showed that our system made the authoring 
of games more effective and efficient. 
1 Introduction 
Learning to read and write Chinese characters is a 
challenging task for learners of Chinese. To read 
everyday news articles, one needs to learn thou-
sands of Chinese characters. The official agents in 
Taiwan and China, respectively, chose 5401 and 
3755 characters as important basic characters in 
national standards. Consequently, the general pub-
lic has gained the impression that it is not easy to 
read Chinese articles, because each of these thou-
sands of characters is written in different ways. 
Teachers adopt various strategies to help learn-
ers to memorize Chinese characters. An instructor 
at the University of Michigan made up stories 
based on decomposed characters to help students 
remember their formations  (Tao, 2007). Some take 
linguistics-based approaches. Pictogram is a major 
formation of Chinese characters, and radicals carry 
partial semantic information about Chinese charac-
ters. Hence, one may use radicals as hints to link 
the meanings and writings of Chinese characters. 
For instance, ???(he2, river) [Note: Chinese char-
acters will be followed by their pronunciations, 
denoted in Hanyu pinyin, and, when necessary, an 
English translation.], ???(hai3, sea), and 
???(yang2, ocean) are related to huge water sys-
tems, so they share the semantic radical, ?, which 
is a pictogram for ?water? in Chinese. Applying 
the concepts of pictograms, researchers designed 
games, e.g.,  (Lan et al, 2009) and animations, e.g., 
(Lu, 2011) for learning Chinese characters. 
The aforementioned approaches and designs 
mainly employ visual stimuli in activities. We re-
port exploration of using the combination of audio 
and visual stimuli. In addition to pictograms, more 
than 80% of Chinese characters are phono-
semantic characters (PSCs, henceforth)  (Ho and 
Bryant, 1997). A PSC consists of a phonological 
component (PC, henceforth) and a semantic com-
ponent. Typically, the semantic components are the 
radicals of PSCs. For instance, ???(du2), 
???(du2), ??? (du2), ???(du2) contain different 
radicals, but they share the same phonological 
components, ???(mai4), on their right sides. Due 
to the shared PC, these four characters are pro-
nounced in exactly the same way. If a learner can 
learn and apply this rule, one may guess and read 
???(du2) correctly easily. 
In the above example, ??? is a normal Chinese 
character, but not all Chinese PCs are standalone 
characters. The characters ???(jian3), ??? 
(jian3), and ???(jian3) share their PCs on their 
right sides, but that PC is not a standalone Chinese 
character. In addition, when a PC is a standalone 
character, it might not indicate its own or similar 
pronunciation when it serves as a PC in the hosting 
character, e.g., ??? and ??? are pronounced as 
/mai4/ and /du2/, respectively. In contrast, the pro-
nunciations of ???, ???, ???, and ??? are 
/tao2/. 
Pronunciations of specific substrings in words of 
alphabetic languages are governed by grapheme-
phoneme conversion (GPC) rules, though not all 
languages have very strict GPC rules. The GPC 
rules in English are not as strict as those in Finish 
1
 (Ziegler and Goswami, 2005), for instance. The 
substring ?ean? are pronounced consistently in 
?bean?, ?clean?, and ?dean,? but the substring ?ch? 
does not have a consistent pronunciation in 
?school?, ?chase?, and ?machine.? PCs in Chinese 
do not follow strict GPC rules either, but they re-
main to be good agents for learning to read. 
Despite the differences among phoneme systems 
and among the degrees of strictness of the GPC 
rules in different languages, ample psycholinguis-
tic evidences have shown that phonological aware-
ness is a crucial factor in predicting students? read-
ing ability, e.g.,  (Siok and Fletcher, 2001). Moreo-
ver, the ability to detect and apply phonological 
consistency in GPCs, including the roles of PCs in 
PSCs in Chinese, plays an instrumental role in 
learners? competence in reading Chinese. Phono-
logical consistency is an important concept for 
learners of various alphabetic languages  (Jared et 
al., 1990; Ziegler and Goswami, 2005) and of Chi-
nese, e.g., (Lee et al, 2005), and is important for 
both young readers  (Ho and Bryant, 1997; Lee, 
2009) and adult readers  (Lin and Collins, 2012). 
This demonstration is unique on two aspects: (1) 
students play games that are designed to strengthen 
the association between Chinese PCs and the pro-
nunciations of hosting characters and (2) teachers 
compile the games with tools that are supported by 
sublexical information in Chinese. The games aim 
at implicitly informing players of the Chinese GPC 
rules, mimicking the process of how infants would 
apply statistical learning  (Saffran et al, 1996). We 
evaluated the effectiveness of the game platform 
with 116 students between grade 1 and grade 6 in 
Taiwan, and found that the students made progress 
in the Chinese naming tasks. 
As we will show, it is not trivial to author games 
for learning a GPC rule to meet individualized 
teaching goals. For this reason, techniques reported 
in a previous ACL conference for decomposing 
and comparing Chinese characters were employed 
to assist the preparation of games (Liu et al, 2011). 
Results of our evaluation showed that the author-
ing tool facilitates the authoring process, improv-
ing both efficiency and effectiveness. 
We describe the learning games in Section 2, 
and report the evaluation results of the games in 
Section 3. The authoring tool is presented in Sec-
tion 4, and its evaluation is discussed in Section 5. 
We provide some concluding remarks in Section 6. 
2 The Learning Games 
A game platform should include several functional 
components such 
as the manage-
ment of players? 
accounts and the 
maintenance of 
players? learning 
profiles. Yet, due 
to the page limits, 
we focus on the 
parts that are 
most relevant to the demonstration. 
Figure 1 shows a screenshot when a player is 
playing the game. This is a game of ?whac-a-
mole? style. The target PC appears in the upper 
middle of the window (???(li3) in this example), 
and a character and an accompanying monster (one 
at a time) will pop up randomly from any of the six 
holes on the ground. The player will hear the pro-
nunciation of the character (i.e., ???(li3)), such 
that the player receives both audio and visual stim-
uli during a game. Players? task is to hit the mon-
sters for the characters that contain the shown PC. 
The box at the upper left corner shows the current 
credit (i.e., 3120) of the player. The player?s credit 
will be increased or decreased if s/he hits a correct 
or an incorrect character, respectively. If the player 
does not hit, the credit will remain the same. Play-
ers are ranked, in the Hall of Fame, according to 
their total credits to provide an incentive for them 
to play the game after school. 
In Figure 1, the player has to hit the monster be-
fore the monster disappears to get the credit. If the 
player does not act in time, the credit will not 
change. 
On ordinary computers, the player manipulates 
the mouse to hit the monster. On multi-touch tablet 
computers, the play can just touch the monsters 
with fingers. Both systems will be demoed. 
2.1 Challenging Levels 
At the time of logging into the game, players can 
choose two parameters: (1) class level: lower class 
(i.e., grades 1 and 2), middle class (i.e., grades 3 
and 4), or upper class (i.e., grades 5 and 6) and (2) 
speed level: the duration between the monsters? 
popping up and going down. The characters for 
lower, middle, and upper classes vary in terms of 
frequency and complexity of the characters. A stu-
dent can choose the upper class only if s/he is in 
the upper class or if s/he has gathered sufficient 
credits. There are three different speeds for the 
monsters to appear and hide: 2, 3, and 5 seconds. 
Choosing different combinations of these two pa-
Figure 1. The learning game 
2
rameters affect how the credits are added or de-
ducted when the players hit the monsters correctly 
or incorrectly, respectively. Table 1 shows the in-
crements of credits for different settings. The num-
bers on the leftmost column are speed levels. 
2.2  Feedback Information 
After finishing a 
game, the player 
receives feed-
back about the 
correct and in-
correct actions 
that were taken 
during the game. 
Figure 2 shows 
such an example. 
The feedback informs the players what characters 
were correctly hit (???(mai2), ???(li3), 
???(li3), and ???(li3)), incorrectly hit 
(???(ting2) and ???(show4)), and should have 
been hit (???(li2)). When the player moves mouse 
over these characters, a sample Chinese word that 
shows how the character is used in daily lives will 
show up in a vertical box near the middle (i.e., 
????(li3 mian4)). 
The main purpose of providing the feedback in-
formation is to allow players a chance to reflect on 
what s/he had done during the game, thereby 
strengthening the learning effects. 
On the upper right hand side of Figure 2 are four 
tabs for more functions. Clicking on the top tab 
(???) will take the player to the next game. In 
the next game, the focus will switch to a different 
PC. The selection of the next PC is random in the 
current system, but we plan to make the switching 
from a game to another adaptive to the students? 
performance in future systems. Clicking on the 
second tab (???) will see the player list in the 
Hall of Fame, clicking on the third tab 
(?????) will return to the main menu, and 
clicking on the fourth (???) will lead to games 
for extra credits. We have extended our games to 
lead students to learning Chinese words from char-
acters, and details will be illustrated during the 
demo. 
2.3 Behind the Scene 
The data structure of a game is simple. When com-
piling a game, a teacher selects the PC for the 
game, and prepares six characters that contain the 
PC (to be referred as an In-list henceforth) and 
four characters as distracter characters that do not 
contain the PC (to be referred as an Out-list hence-
forth). The simplest internal form of a game looks 
like {target PC= ???, In-list= ????????, 
Out-list= ?????? }. We can convert this struc-
ture into a game easily. Through this simple struc-
ture, teachers choose the PCs to teach with charac-
ter combinations of different challenging levels. 
During the process of playing, our system ran-
domly selects one character from the list of 10 
characters. In a game, 10 characters will be pre-
sented to the player. 
3 Preliminary Evaluation and Analysis 
The game platform was evaluated with 116 stu-
dents, and was found to shorten students? response 
times in Chinese naming tasks. 
3.1 Procedure and Participants 
The evaluation was conducted at an elementary 
school in Taipei, Taiwan, during the winter break 
between late January and the end of February 
2011. The lunar new year of 2011 happened to be 
within this period. 
Students were divided into an experimental 
group and a control group. We taught students of 
the experimental group and showed them how to 
play the games in class hours before the break be-
gan. The experimental group had one month of 
time to play the games, but there were no rules 
asking the participants how much time they must 
spend on the games. Instead, they were told that 
they would be rewarded if they were ranked high 
in the Hall of Fame. Table 2 shows the numbers of 
participants and their actual class levels. 
As we explained in Section 2.1, a player could 
choose the class level before the game begins. 
Hence, for example, it is possible for a lower class 
player to play the games designed for middle or 
even upper class levels to increase their credits 
faster. However, if the player is not competent, the 
credits may be deducted faster as well. In the eval-
uation, 20 PCs were used in the games for each 
class level in Table 1. 
Pretests and posttests were administered with the 
standardized (1) Chinese Character Recognition 
Figure 2. Feedback information
 Lower Middle Upper 
Experimental 11 23 24 
Control 11 23 24 
Table 2. Number of participants 
 Lower Middle Upper 
5 10 20 30 
3 15 25 35 
2 20 30 40 
Table 1.Credits for challenging levels
3
Test (CCRT) and (2) Rapid Automatized Naming 
Task (RAN). In CCRT, participants needed to 
write the pronunciations in Jhuyin, which is a pho-
netic system used in Taiwan, for 200 Chinese 
characters. The number of correctly written 
Jhuyins for the characters was recorded. In RAN, 
participants read 20 Chinese characters as fast as 
they could, and their speeds and accuracies were 
recorded. 
3.2 Results and Analysis 
Table 3 shows the statistics for the control group. 
After the one month evaluation period, the perfor-
mance of the control group did not change signifi-
cantly, except participants in the upper class. This 
subgroup improved their speeds in RAN. (Statisti-
cally significant numbers are highlighted.) 
Table 4 shows the statistics for the experimental 
group. After the evaluation period, the speeds in 
RAN of all class levels improved significantly. 
The correct rates in RAN of the control group 
did not improve or fall, though not statistically sig-
nificant. In contrast, the correct rates in RAN of 
the experimental group improved, but the im-
provement was not statistically significant either. 
The statistics for the CCRT tests were not statis-
tically significant. The only exception is that the 
middle class in the experimental group achieved 
better CCRT results. We were disappointed in the 
falling of the performance in CCRT of the lower 
class, though the change was not significant. The 
lower class students were very young, so we con-
jectured that it was harder for them to remember 
the writing of Jhuyin symbols after the winter 
break. Hence, after the evaluation, we strengthened 
the feedback by adding Jhuyin information. In Fig-
ure 2, the Jhuyin information is now added beside 
the sample Chinese words, i.e., ???? (li3 mian4). 
4 An Open Authoring Tool for the Games 
Our game platform has attracted the attention of 
teachers of several elementary schools. To meet 
the teaching goals of teacher in different areas, we 
have to allow the teachers to compile their own 
games for their needs. 
The data structure for a game, as we explained 
in Section  2.3, is not complex. A teacher needs to 
determine the PC to be taught first, then s/he must 
choose an In-list and an Out-list. In the current im-
plementation, we choose to have six characters in 
the In-list and four characters in the Out-list. We 
allow repeated characters when the qualified char-
acters are not enough. 
This authoring process is far less trivial as it 
might seem to be. In a previous evaluation, even 
native speakers of Chinese found it challenging to 
list many qualified characters out of the sky. Be-
cause PCs are not radicals, ordinary dictionaries 
would not help very much. For instance, ??? 
(mai2), ???(li2), ???(li3), and ???(li3) belong 
to different radicals and have different pronuncia-
tions, so there is no simple way to find them at just 
one place. 
Identifying characters for the In-list of a PC is 
not easy, and finding the characters for the Out-list 
is even more challenging. In Figure 1, ??? (li3) is 
the PC to teach in the game. Without considering 
the characters in In-list for the game, we might 
believe that ??? (jia3) and ??? (cheng2) look 
equally similar to ???, so both are good distract-
ers. If, assuming that ???(li3) is in the In-list, 
??? (jia3) will be a better distracter than ??? 
(cheng2) for the Out-list, because ??? and ??? 
are more similar in appearance. By contrast, if we 
have ??? in the In-list, we may prefer to having 
??? (cheng2) than having ??? in the Out-list. 
Namely, given a PC to teach and a selected In-
list, the ?quality? of the Out-list is dependent on 
the characters in In-list. Out-lists of high quality 
influence the challenging levels of the games, and 
will become a crucial ingredient when we make the 
games adaptive to players? competence. 
4.1 PC Selection 
Control Group 
 Class Pretests Posttests p-value 
CCRT 
(charac-
ters) 
Lower 59 61 .292 
Middle 80 83 .186 
Upper 117 120 .268 
RAN 
Correct 
Rate 
Lower 83% 79% .341 
Middle 59% 64% .107 
Upper 89% 89% 1.00 
RAN 
Speed 
(second) 
Lower 23.1 20.6 .149 
Middle 24.3 20.2 .131 
Upper 15.7 14.1 .026 
Table 3. Results for control group
Experimental Group 
 Class Pretests Posttests p-value 
CCRT 
(charac-
ters) 
Lower 64 61 .226 
Middle 91 104 .001 
Upper 122 124 .52 
RAN 
Correct 
Rate 
Lower 73% 76% .574
Middle 70% 75% .171 
Upper 89% 91% .279 
RAN 
Speed 
(second) 
Lower 21.5 16.9 .012 
Middle 24.6 19.0 .001 
Upper 16.9 14.7 <0.001 
Table 4. Results for experimental group 
4
In a realistic teaching situation, a teacher will be 
teaching new characters and would like to provide 
students games that are related to the structures of 
the new characters. Hence, it is most convenient 
for the teachers that our tool decomposes a given 
character and recommends the PC in the character. 
For instance, given ???, we show the teacher that 
we could compile a game for ???. This is achiev-
able using the techniques that we illustrate in the 
next subsection. 
4.2 Character Recommendation 
Given a selected PC, a teacher has to prepare the 
In-list and Out-list for the game. Extending the 
techniques we reported in  (Liu et al, 2011), we 
decompose every Chinese character into a se-
quence of detailed Cangjie codes, which allows us 
to infer the PC contained in a character and to infer 
the similarity between two Chinese characters. 
For instance, the internal codes for ???, ???, 
???, and ??? are, respectively, ?WG?, 
?MGWG?, ?LWG?, and ?MGWL?. The English 
letters denote the basic elements of Chinese char-
acters. For instance, ?WG? stands for ????, 
which are the upper and the lower parts of ???, 
?WL? stands for ????, which could be used to 
rebuild ??? in a sense. By comparing the internal 
codes of Chinese characters, it is possible to find 
that (1) ??? and ??? include ??? and that (2) 
??? and ??? are visually similar based on the 
overlapping codes. 
For the example problem that we showed in 
Figures 1 and 2, we may apply an extended proce-
dure of  (Liu et al, 2011) to find an In-list for ???: 
???????????. This list includes more 
characters than most native speakers can produce 
for ??? within a short period. Similar to what we 
reported previously, it is not easy to find a perfect 
list of characters. More specifically, it was relative-
ly easy to achieve high recall rates, but the preci-
sion rates varied among different PCs. However, 
with a good scoring function to rank the characters, 
it is not hard to achieve quality recommendations 
by placing the characters that actually contain the 
target PCs on top of the recommendation. 
Given that ??? is the target PC and the above 
In-list, we can recommend characters that look like 
the correct characters, e.g., ????? for ???, 
????? for ???, ????? for ???, 
?????? for ??? , and ???? for  ???. 
We employed similar techniques to recommend 
characters for In-lists and Out-lists. The database 
that contains information about the decomposed 
Chinese charac-
ters was the 
same, but we 
utilized different 
object functions 
in selecting and 
ranking the 
characters.  We 
considered all 
elements in a 
character to rec-
ommend charac-
ters for In-lists, but focused on the inclusion of 
target PCs in the decomposed characters to rec-
ommend characters for Out-lists. Again our rec-
ommendations for the Out-lists were not perfect, 
and different ranking functions affect the perceived 
usefulness of the authoring tools.  
Figure 3 shows the step to choose characters in 
the Out-list for characters in the In-list. In this ex-
ample, six characters for the In-list for the PC ? ? 
had been chosen, and were listed near the top: 
????????.  Teachers can find characters 
that are similar to these six correct characters in 
separate pull-down lists. The screenshot shows the 
operation to choose a character that is similar to 
??? (yao2) from the pull-down list. The selected 
character would be added into the Out-list. 
4.3 Game Management 
We allow teachers to apply for accounts and pre-
pare the games based on their own teaching goals. 
However, we cannot describe this management 
subsystem for page limits. 
5 Evaluation of the Authoring Tool 
We evaluated how well our tools can help teachers 
with 20 native speakers. 
5.1 Participants and Procedure 
We recruited 20 native speakers of Chinese: nine 
of them are undergraduates, and the rest are gradu-
ate students. Eight are studying some engineering 
fields, and the rest are in liberal arts or business. 
The subjects were equally split into two groups. 
The control group used only paper and pens to au-
thor the games, and the experimental group would 
use our authoring tools. We informed and showed 
the experimental group how to use our tool, and 
members of the experimental group must follow an 
illustration to create a sample game before the 
evaluation began. 
Every subject must author 5 games, each for a 
Figure 3. Selecting a character for an Out-list 
5
different PC. A game needed 6 characters in the In-
list and 4 characters in the Out-list. Every evalua-
tor had up to 15 minutes to finish all tasks. 
The games authored by the evaluators were 
judged by psycholinguists who have experience in 
teaching. The highest possible scores for the In-list 
and the Out-list were both 30 for a game. 
5.2 Gains in Efficiency and Effectiveness 
Table 5 shows the results of the evaluation. The 
experimental group outperformed the control 
group in both the quality of the games and in the 
time spent on the authoring task. The differences 
are clearly statistically significant. 
Table 6 shows the scores for the In-list and Out-
list achieved by the control and the experimental 
groups. Using the authoring tools helped the evalu-
ators to achieved significantly higher scores for the 
Out-list. Indeed, it is not easy to find characters 
that (1) are similar to the characters in the In-list 
and (2) cannot contain the target PC. 
Due to the page limits, we could not present the 
complete authoring system, but hope to have the 
chance to show it during the demonstration. 
6 Concluding Remarks 
We reported a game for strengthening the associa-
tion of the phonetic components and the pronun-
ciations of Chinese characters. Experimental re-
sults indicated that playing the games helped stu-
dents shorten the response times in naming tasks. 
To make our platform more useable, we built an 
authoring tool so that teachers could prepare games 
that meet specific teaching goals. Evaluation of the 
tool with college and graduate students showed 
that our system offered an efficient and effective 
environment for this authoring task. 
Currently, players of our games still have to 
choose challenge levels. In the near future, we 
wish to make the game adaptive to players? compe-
tence by adopting more advanced techniques, in-
cluding the introduction of ?consistency values? 
 (Jared et al, 1990). Evidence shows that foreign 
students did not take advantage of the GPC rules in 
Chinese to learn Chinese characters  (Shen, 2005). 
Hence, it should be interesting to evaluate our sys-
tem with foreign students to see whether our ap-
proach remains effective. 
Acknowledgement 
We thank the partial support of NSC-100-2221-E-004-014 
and NSC-98-2517-S-004-001-MY3 projects of the Nation-
al Science Council, Taiwan. We appreciate reviewers? 
invaluable comments, which we will respond in an ex-
tended version of this paper. 
References  
C. S.-H. Ho and P. Bryant. 1997. Phonological skills are im-
portant in learning to read Chinese, Developmental Psy-
chology, 33(6), 946?951. 
D. Jared, K. McRae, and M. S. Seidenberg. 1990. The basis of 
consistency effects in word naming, J. of Memory & Lan-
guage, 29(6), 687?715. 
Y.-J. Lan, Y.-T. Sung, C.-Y. Wu, R.-L. Wang, and K.-E. 
Chang. 2009. A cognitive interactive approach to Chinese 
characters learning: System design and development, Proc. 
of the Int?l Conf. on Edutainment, 559?564. 
C.-Y. Lee. 2009. The cognitive and neural basis for learning to 
reading Chinese, J. of Basic Education, 18(2), 63?85. 
C.-Y. Lee, J.-L. Tsai, E. C.-I Su, O. J.-L. Tzeng, and D.-L. 
Hung. 2005. Consistency, regularity, and frequency effects 
in naming Chinese characters, Language and Linguistics, 
6(1), 75?107. 
C.-H. Lin and P. Collins. 2012. The effects of L1 and ortho-
graphic regularity and consistency in naming Chinese char-
acters. Reading and Writing. 
C.-L. Liu, M.-H. Lai, K.-W. Tien, Y.-H. Chuang, S.-H. Wu, 
and C.-Y. Lee. 2011. Visually and phonologically similar 
characters in incorrect Chinese words: Analyses, identifica-
tion, and applications, ACM Trans. on Asian Language In-
formation Processing, 10(2), 10:1?39. 
M.-T. P. Lu. 2011. The Effect of Instructional Embodiment 
Designs on Chinese Language Learning: The Use of Em-
bodied Animation for Beginning Learners of Chinese 
Characters, Ph.D. Diss., Columbia University, USA. 
J. R. Saffran, R. N. Aslin, and E. L. Newport. 1996. Statistical 
learning by 8-month-old infants, Science, 274(5294), 
1926?1928. 
H. H. Shen. 2005. An investigation of Chinese-character 
learning strategies among non-native speakers of Chinese, 
System, 33, 49?68. 
W.T. Siok and P. Fletcher. 2001. The role of phonological 
awareness and visual-orthographic skills in Chinese read-
ing acquisition, Developmental Psychology, 37(6), 886?
899. 
H. Tao. 2007. Stories for 130 Chinese characters, textbook 
used at the University of Michigan, USA. 
J. C. Ziegler and U. Goswami. 2005. Reading acquisition, 
developmental dyslexia, and skilled reading across lan-
guages: A psycholinguistic grain size theory, Psychological 
Bulletin, 131(1), 3?29. 
 Avg. scores 
(In-list and Out-list) 
Avg. time 
Control 16.8 15 min 
Experimental 52.8 7.1 min 
p-value < 0.0001 < 0.0001 
Table 5. Improved effectiveness and efficiency 
 Avg. scores 
In-list Out-list 
Control 15.9 1 
Experimental 29.9 22.9 
Table 6. Detailed scores for the average scores
6
Reducing the False Alarm Rate of Chinese Character Error Detection 
and Correction 
 
Shih-Hung Wu, Yong-Zhi Chen  
Chaoyang University of Technol-
ogy, Taichung Country 
shwu@cyut.edu.tw 
 
Ping-che Yang, Tsun Ku 
Institute for information industry,
Taipei City 
maciaclark@iii.org.tw, 
cujing@iii.org.tw 
Chao-Lin Liu  
National Chengchi Universi-
ty, Taipei City 
chaolin@nccu.edu.tw 
 
 
Abstract 
The main drawback of previous Chinese cha-
racter error detection systems is the high false 
alarm rate. To solve this problem, we propose 
a system that combines a statistic method and 
template matching to detect Chinese character 
errors. Error types include pronunciation-
related errors and form-related errors. Possible 
errors of a character can be collected to form a 
confusion set. Our system automatically gene-
rates templates with the help of a dictionary 
and confusion sets. The templates can be used 
to detect and correct errors in essays. In this 
paper, we compare three methods proposed in 
previous works. The experiment results show 
that our system can reduce the false alarm sig-
nificantly and give the best performance on f-
score. 
1 Introduction 
Since many Chinese characters have similar forms 
and similar or identical pronunciation, improperly 
used characters in Chinese essays are hard to be de-
tectted. Previous works collected these hard-to-
distinguish characters and used them to form confu-
sion sets. Confusion sets are critical for detecting and 
correcting improperly used Chinese characters. A 
confusion set of a Chinese character consists of cha-
racters with similar pronunciation, similar forms, and 
similar meaning. Most Chinese character detection 
systems were built based on confusion sets and a lan-
guage model. Ren et.al proposed a rule-based method 
that was also integrated with a language model to 
detect character errors in Chinese (Ren, Shi, & Zhou, 
1994). Chang used confusion sets to represent all 
possible errors to reduce the amount of computation. 
A language model was also used to make decisions. 
The confusion sets were edited manually. Zhang et al 
proposed a way to automatically generate confusion 
sets based on the Wubi input method (Zhang, Zhou, 
Huang, & Sun, 2000). The basic assumption was that 
characters with similar input sequences must have 
similar forms. Therefore, by replacing one code in the 
input sequence of a certain character, the system 
could generate characters with similar forms. In the 
following work, Zhang et al designed a Chinese cha-
racter detection system based on the confusion sets 
(Zhang, Zhou, Huang, & Lu, 2000). Another input 
method was also used to generate confusion sets. Lin 
et al used the Cangjie input method to generate con-
fusion sets (Lin, Huang, & Yu, 2002). The basic as-
sumption was the same. By replacing one code in the 
input sequence of a certain character, the system 
could generate characters with similar forms. Since 
the two input methods have totally different represen-
tations of the same character, the confusion set of any 
given character will be completely different. 
In recent years, new systems have been incorporat-
ing more NLP technology for Chinese character error 
detection. Huang et al proposed that a word segmen-
tation tool can be used to detect character error in 
Chinese (Huang, Wu, & Chang, 2007). They used a 
new word detection function in the CKIP word seg-
mentation toolkit to detect error candidates (CKIP, 
1999). With the help of a dictionary and confusion set, 
the system can decide whether a new word is a cha-
racter error or not. Hung et al proposed a system that 
can detect character errors in student essays and then 
suggest corrections (Hung & Wu, 2008). The system 
was based on common error templates which were 
manually edited. The precision of this system is the 
highest, but the recall remains average. The main 
drawback of this approach is the cost of editing com-
mon error templates. Chen et al proposed an automat-
ic method for common error template generation 
(Chen, Wu, Lu, & Ku, 2009). The common errors 
were collected from a large corpus automatically. The 
template is a short phrase with one error in it. The 
assumption is the frequency of a correct phrase must 
be higher than the frequency of the corresponding 
template, with one error character. Therefore, a statis-
tical test can be used to decide weather there is a 
common error or not. 
The main drawback of previous systems is the high 
false alarm rate. The drawback is found by comparing 
the systems with sentences without errors. As we will 
show in our experiments, the systems in previous 
works tent to report more errors in an essay than the 
real ones, thus, cause false alarms.  
In this paper, we will further improve upon the 
Chinese character checker using a new error model 
and a simplified common error template generation 
method. The idea of error model is adopted from the 
noise channel model, which is used in many natural 
language processing applications, but never on Chi-
nese character error detection. With the help of error 
model, we can treat the error detection problem as a 
kind of translation, where a sentence with errors can 
be translated into a sentence without errors. The sim-
plified template generation is based on given confu-
sion sets and a lexicon.  
The paper is organized as follows. We introduce 
briefly the methods in previous works in section 2. 
Section 3 reports the necessary language resources 
used to build such systems. Our approach is described 
in section 4. In section 5, we report the experiment 
settings and results of our system, as well as give the 
comparison of our system to the three previous sys-
tems. Finally, we give the conclusions in the final 
section. 
2 Previous works 
In this paper, we compare our method to previous 
works. Since they are all not open source systems, we 
will reconstruct the systems proposed by Chang 
(1995), Lin, Huang, & Yu (2002), and Huang, Wu, & 
Chang (2007). We cannot compare our system to the 
system proposed by Zhang, Zhou, Huang, & Sun 
(2000), since the rule-based system is not available. 
We describe the systems below. 
Chang?s system (1995) consists of five steps. First, 
the system segments the input article into sentences. 
Second, each character in the sentence is replaced by 
the characters in the corresponding confusion set. 
Third, the probability of a sentence is calculated ac-
cording to a bi-gram language model. Fourth, the 
probability of the sentences before and after replace-
ment is compared. If the replacement causes a higher 
probability, then the replacement is treated as a cor-
rection of a character error. Finally, the results are 
outputted. There are 2480 confusion sets used in this 
system. Each confusion set consists of one to eight 
characters with similar forms or similar pronunciation. 
The system uses OCR results to collect characters 
with similar forms. The average size of the confusion 
sets was less than two. The language model was built 
from a 4.7 million character news corpus. 
The system proposed by Lin, Huang, & Yu (2002) 
has two limitations. First, there is only one spelling 
error in one sentence. Second, the error was caused by 
the Cangjie input method. The system also has five 
steps. First, sentences are inputted. Second, a search is 
made of the characters in a sentence that have similar 
input sequences. Third, a language model is used to 
determine whether the replacement improves the 
probability of the sentence or not. Fourth, the three 
steps for all input sentences are repeated. Finally, the 
results are outputted. The confusion sets of this sys-
tem were constructed from the Cangjie input method. 
Similarity of characters in a confusion set is ranked 
according to the similarity of input sequences. The 
language model was built from a 59 million byte news 
corpus. 
The system by Huang, Wu, & Chang (2007) con-
sists of six steps. First, the input sentences are seg-
mented into words according to the CKIP word seg-
mentation toolkit. Second, each of the characters in 
the new words is replaced by the characters in the 
confusion sets. Third, a word after replacement 
checked in the dictionary. Fourth, a language model is 
used to assess the replacement. Fifth, the probability 
of the sentence before and after replacement is com-
pared. Finally, the result with the highest probability 
is outputted. The confusion set in this system, which 
also consists of characters with similar forms or simi-
lar pronunciation, was edited manually.  
Since the test data in the papers were all different 
test sets, it is improper to compare their results direct-
ly, therefore; there was no comparison available in the 
literature on this problem. To compare these systems 
with our method, we used a fixed dictionary, inde-
pendently constructed confusion sets, and a fixed lan-
guage model to reconstruct the systems. We per-
formed tests on the same test set. 
3 Data in Experiments  
3.1 Confusion sets 
Confusion sets are a collection of sets for each indi-
vidual Chinese character. A confusion set of a certain 
character consists of phonologically or logographical-
ly similar characters. For example, the confusion set 
of ??? might consist of the following characters with 
the same pronunciation????????? or with 
similar forms????????????????
??????. In this study, we use the confusion sets 
used by Liu, Tien, Lai, Chuang, & Wu (2009). The 
similar Cangjie (SC1 and SC2) sets of similar forms, 
and both the same-sound-same-tone (SSST) and 
same-sound-different-tone (SSDT) sets for similar 
pronunciation were used in the experiments. There 
were 5401 confusion sets for each of the 5401 high 
frequency characters. The size of each confusion set 
was one to twenty characters. The characters in each 
confusion set were ranked according to Google search 
results. 
3.2 Language model 
Since there is no large corpus of student essays, we 
used a news corpus to train the language model. The 
size of the news corpus is 1.5 GB, which consists of 
1,278,787 news articles published between 1998 and 
2001. The n-gram language model was adopted to 
calculate the probability of a sentence p(S). The gen-
eral n-gram formula is: 
)|()( 1 1
?
+?= n Nnn wwpSp    (1) 
Where N was set to two for bigram and N was set to 
one for unigram. The Maximum Likelihood Estima-
tion (MLE) was used to train the n-gram model. We 
adopted the interpolated Kneser-Ney smoothing me-
thod as suggested by Chen & Goodman (1996). As 
following: 
 
)()1()|(
)|(
1
1int
wpwwp
wwp
unigramibigram
ierpolate
?? ?+= ?
?
  (2)
 
To determine whether a replacement is good or not, 
our system use the modified perplexity:  
 
NSpPerplexity /))(log(2?=   (3) 
Where N is the length of a sentence and p(S) is the bi-
gram probability of a sentence after smoothing. 
3.3 Dictionary and test set 
We used a free online dictionary provided by Tai-
wan?s Ministry of Education, MOE (2007). We fil-
tered out one character words and used the remaining 
139,976 words which were more than one character as 
our lexicon in the following experiments. 
The corpus is 5,889 student essays collected from a 
primary high school. The students were 13 to 15 years 
old. The essays were checked by teachers manually, 
and all the errors were identified and corrected. Since 
our algorithm needed a training set, we divided the 
essays into two sets to test our method. The statistics 
is given in Table 1. There are less than two errors in 
an essay on average. We find that most (about 97%) 
of characters in the essays were among the 5,401 most 
common characters, and most errors were characters 
of similar forms or pronunciation. Therefore, the 
5,401 confusion sets constructed according to form 
and pronunciation were suitable for error detection. 
Table 2 shows the error types of errors in students? 
essays.  More than 70% errors are characters with 
similar pronunciation, 40% errors are characters with 
similar form, and there are 20% errors are characters 
with both similar pronunciation and similar form. 
Only 10% errors are in other types. Therefore, in this 
study, our system aimed to identify and correct the 
errors of the two common types. 
 
Table 1. Training set and test set statistics 
 # of Essays 
Average 
length 
of essay 
Average 
# of 
errors 
% of 
common 
characters
Training 
set 5085 403.18 1.76 96.69% 
Test set 804 387.08 1.2 97.11% 
 
Table 2. Error type analysis 
 Similar form Similar pronunciation Both Other
Training set 41.54% 72.60% 24.24% 10.10%
Test set 40.36% 76.98% 27.66% 10.30%
4 System Architecture 
4.1 System flowchart 
Figure 1 shows the flowchart of our system. First, the 
input essays are segmented into words. Second, the 
words are sent to two different error detection mod-
ules. The first one is the template module, which can 
detect character errors based on the stored templates 
as in the system proposed by Chen, Wu, Lu, & Ku, 
(2009). The second module is the new language mod-
el module, which treats error detection as a kind of 
translation. Third, the results of the two modules can 
be merged to get a better system result. The details 
will be described in the following subsections. 
 
Figure 1. System flowchart 
 
4.2 Word segmentation 
The first step in our system uses word segmentation to 
find possible errors. In this study, we do not use the 
CKIP word segmentation tool (CKIP, 1999) as Huang, 
Wu, & Chang (2007) did, since it has a merge 
algorithm that might merge error charactersto form 
new words (Ma & Chen, 2003).  We use a backward 
longest first approach to build our system. The lex-
icon is taken from an online dictionary (MOE, 2007).  
We consider an input sentence with an error, ????
????????????, as an example. The 
sentence will be segmented into ??|??|??|?
??|?|?|?|?|???. The sequence of single 
characters will be our focus. In this case, it is ????
??. These kinds of sequences will be the output of 
the first step and will be sent to the following two 
modules. The error character can be identified and 
corrected by a ????-???? template. 
4.3 Template Module 
The template module in this study is a simplified ver-
sion of a module from a previous work (Chen, Wu, 
Lu, & Ku, 2009), which collects templates from a 
corpus. The simplified approach replaces one 
character of each word in a dictionary with one 
character in the corresponding confusion set. For 
example, a correct word ???? might be written with 
an error character ???? since ??(bian4)? is in the 
confusion set of ??(ban4)??. This method generates 
all possible error words with the help of confusion 
sets. Once the error template ???? is matched in an 
essay, our system can conclude that the character is an 
error and make a suggestion on correction ???? 
based on the ????-???? template. 
4.4 Translate module 
To improve the n-gram language model method, we 
use a statistical machine translation formula (Brown, 
1993) as a new way to detect character error. We treat 
the sentences with/without errors as a kind of transla-
tion. Given a sentence S that might have character 
errors in the sentence in the source language, the out-
put sentence C
~
 is the sentence in the target language 
with the highest probability of different replacements 
C. The replacement of each character is treated as a 
translation without alignment. 
)|(maxarg
~
SCpC
c
=   (4) 
From the Bayesian rule and when the fixed value of 
p(w) is ignored, this equation can be rewritten as (5): 
)()|(maxarg
)(
)()|(
maxarg
~
CpCSp
Sp
CpCSp
C
c
c
?
=
  (5) 
The formula is known as noisy channel model. We 
call p(S|C) an ?error model?, that is,  the probability 
which a character can be incorrect. It can be defined 
as the product of the error probability of each charac-
ter in the sentence. 
?
=
=
n
i
jiij cspCWp
1
 )|()|(    (6) 
where n is the length of the sentence S, and si ith cha-
racter of input sentence S. Cj is the jth replacement 
and cij.is the ith character at the jth replacement. The 
error model was built from the training set of student 
essays. Where p(C) is the n-gram language model as 
was described in section 3.2. Note that the number of 
replacements is not fixed, since the number of re-
placements depends on the size of all possible errors 
in the training set. 
For example, consider a segmented sentence with 
an error: ??|??|?|?|?|???, we will use the 
error model to evaluate the replacement of each cha-
racter in the subsequence: ?????. Here p(?|?) 
and p(?|?) are 0.0456902 and 0.025729 respective-
ly, which are estimated according to the training cor-
pus. And in training corpus, no one write the character 
?, therefore, there is no any replacement. Therefore, 
the probability of our error model and the n-gram lan-
guage model can be shown in the following table. Our 
system then multiplies the two probabilities and gets 
the perplexity of each replacement. The replacement 
????? gets the lowest perplexity, therefore, it is 
the output of our system and is both a correct error 
detection and correction. 
 
Table 3. An example of calculating perplexity 
according the new error model 
  Error Model LM multiply Perplexity
??? 0.025728988 1.88E-05 4.83E-07 127.442812
??? 0.001175563 1.05E-04 1.24E-07 200.716961
??? 1 2.09E-09 2.09E-09 782.669809
??? 0.045690212 1.17E-08 5.34E-10 1232.6714
 
4.5 Merge corrections 
Since the two modules detect errors using an inde-
pendent information source, we can combine the deci-
sions of the two modules to get a higher precision or a 
higher recall on the detection and correction of errors. 
We designed two working modes, the Precision Mode 
(PM) and the Detection Mode (DM). The output of 
PM is the intersection of the output of the template 
module and translation module, while the output of 
DM is the union of the two modules. 
5 Experiment Settings and Results  
Since there is no open source system in previous 
works and the data in use is not available, we repro-
duced the systems with the same dictionary, the same 
confusion set, and the same language model. Then we 
performed a test on the same test set. Since the confu-
sion sets are quite large, to reduce the number of 
combinations during the experiment, the size must be 
limited. Since Liu?s experiments show that it takes 
about 3 candidates to find the correct character, we 
use the top 1 to top 10 similar characters as the candi-
dates only in our experiments. That is, we take 1 to 10 
characters from each of the SC1, SC2, SSST, and 
SSDT sets. Thus, the size of each confusion set is 
limited to 4 for the top 1 mode and 40 for the top 10 
mode. 
The evaluation metrics is the same as Chang?s 
(1995). We also define the precision rate, detection 
rate, and correction rate as follows: 
Precision = C / B * 100%  (7) 
Detection = C / A * 100%  (8) 
Correction = D / A * 100%  (9) 
where A is the number of all spelling errors, B is 
the number of errors detected by be system, C is the 
number of errors detected correctly by the system, and 
D is the number of spelling errors that is detected and 
corrected. Note that some errors can be detected but 
cannot be corrected. Since the correction is more im-
portant in an error detection and correction system, 
we define the corresponding f-score as: 
CorrectionPrecision
Correction*Precision*2
scoreF +=?  (10) 
 
 
 
 
 
Figure 2. The comparison of different methods on 
full test set 
5.1 Results of our initial system 
Table 4 shows the initial results of the template mod-
ule (TM), the translation module (LMM) and the 
combined results of the precision mode (PM) and 
detection mode (DM). We find that the precision 
mode gets the highest precision and f-score, while the 
detection mode gets the highest correction rate, as 
expected. The precision and detection rate improved 
dramatically. The precision improved from 14.28% to 
61.68% for the best setting and to 58.82% for the best 
f-score setting. The detection rate improved from 
58.06% to around 72%. The f-score improved from 
22.28% to 43.80%. The result shows that combining 
two independent methods yield better performance 
than each single method does. 
5.2 Results of our system when more know-
ledge and enlarged training sets are added 
The templates used in the initial system were the sim-
plified automatic generated templates, as described in 
section 4.3. Since there were many manually edited 
templates in previous works, we added the 6,701 ma-
nually edited templates and the automatically generat-
ed templates into our system. The results are shown in 
Table 5. All the performance increased for both the 
template module and the translation module. The best 
f-score increased from 43.80% to 45.03%. We believe 
that more knowledge will increase the performance of 
our system. 
5.3 Results of methods in previous works 
We compared the performance of our method to the 
methods in previous works. The result is shown in 
Table 6. Chang?s method has the highest detection 
rate, at 91.79%. Note that the price of this high detec-
tion rate is the high false alarm. The corresponding 
precision is only 0.94%. The precision mode in our 
method has the highest precision, correction, and f-
score. The comparison is shown in Figure 2. The ho-
rizontal axis is the size of confusion sets in our expe-
riment. We can find that the performances converge. 
That is, the size of confusion sets is large enough to 
detect and correct errors in students? essays. 
5.4 Comparison to methods in previous works 
related to sentences with errors 
The numbers in Table 6 are much lower than that in 
the original paper. The reason is the false alarms in 
sentences without any errors, since most previous 
works tested their systems on sentences with errors 
only. In addition, our test set was built on real essays, 
and there were only one or two errors in an essay. 
Most of the sentences contained no errors. The pre-
vious methods tend to raise false alarms.  
To clarify this point, we designed the last experiment 
to test the methods on sentences with at least one er-
ror. We extracted 949 sentences from our test set. 
Among them, 883 sentences have one error, 61 sen-
tences have two errors, 2 sentences have three errors, 
0%
10%
20%
30%
40%
50%
60%
70%
1 2 3 4 5 6 7 8 9 10
Top N of Confusion sets
Precision
Chang Lin Huang PM DM
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 2 3 4 5 6 7 8 9 10
Top N of Confusion sets
Detection
0%
10%
20%
30%
40%
50%
60%
70%
1 2 3 4 5 6 7 8 9 10
Top N of Confusion sets
Correction
0%
5%
10%
15%
20%
25%
30%
35%
40%
45%
50%
1 2 3 4 5 6 7 8 9 10
Top N of Confusion sets
F-Score
and 3 sentences that have four errors. The result is 
shown in Table 7. All the methods have better per-
formance. The precision of Chang?s method rose from 
3% to 43%. The precision of Lin?s method rose from 
3.5% to 61%. The precision of Huang?s method rose 
from 27% to 84%, while PM?s precision rose from 
60% to 97% and DM?s precision rose from 7% to 
62%. The detection mode of our system still has the 
highest f-score.  
The differences of performances in Table 7 and Table 
6 show that, systems in previous works tent to have 
false alarms in sentences without errors.  
5.5 Processing time comparison 
Processing complexity was not discussed in previous 
works. Since all the systems require different re-
sources, it is hard to compare the time or space com-
plexity. We list the average time it takes to process an 
essay for each method on our server as a reference. 
The processing time is less than 0.5 second for both 
our method and Huang?s method. Lin?s method re-
quired 3.85 sec and Chang?s method required more 
than 237 seconds. 
6 Conclusions 
In this paper, we proposed a new Chinese character 
checker that combines two kinds of technology and 
compared it to three previous methods. Our system 
achieved the best F-score performance by reducing 
the false alarm significantly. An error model adopted 
from the noisy channel model was proposed to make 
use of the frequency of common errors that we col-
lected from a training set. A simplified version of 
automatic template generation was also proposed to 
provide high precision character error detection. Fine 
tuning of the system can be done by adding more 
templates manually.  
The experiment results show that the main draw-
back of previous works is false alarms. Our systems 
have fewer false alarms. The combination of two in-
dependent methods gives the best results on real 
world data. In the future, we will find a way to com-
bine the independent methods with theoretical foun-
dation. 
Acknowledgement 
This study is conducted under the ?Intelligent Web - 
enabled Service Research and Development Project? 
of the Institute for Information Industry which is sub-
sidized by the Ministry of Economy Affairs of the 
Republic of China. 
References  
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. (1993). The ma-
thematics of statistical machine translation: Parame-
ter estimation. Computational Linguistics 19 (pp. 
263-311). 
Chang, C.-H. (1995). A New Approach for Automatic 
Chinese Spelling Correction. In Proceedings of 
Natural Language Processing Pacific Rim Sympo-
sium, (pp. 278-283). Korea. 
Chen, S. F., & Goodman, J. (1996). An Empirical 
Study of Smoothing Techniques for Language 
Modeling. Proc. of the 34th annual meeting on As-
sociation for Computational Linguistics, (pp. 310-
318). Santa Cruz, California. 
Chen, Y.-Z., Wu, S.-H., Lu, C.-C., & Ku, T. (2009). 
Chinese Confusion Word Set for Automatic Gen-
eration of Spelling Error Detecting Template. The 
21th Conference on Computational Linguistics and 
Speech Processing (Rocling 2009), (pp. 359-372). 
Taichung. 
CKIP. (1999). AutoTag. Academia Sinica. 
Huang, C.-M., Wu, M.-C., & Chang, C.-C. (2007). 
Error Detection and Correction Based on Chinese 
Phonemic Alphabet in Chinese Text. Proceedings of 
the Fourth Conference on Modeling Decisions for 
Artificial Intelligence (MDAI IV), (pp. 463-476). 
Hung, T.-H., & Wu, S.-H. (2008). Chinese Essay Er-
ror Detection and Suggestion System. Taiwan E-
Learning Forum.  
Lin, Y.-J., Huang, F.-L., & Yu, M.-S. (2002). A 
CHINESE SPELLING ERROR CORRECTION 
SYSTEM. Proceedings of the Seventh Conference 
on Artificial Intelligence and Applications (TAAI).  
Liu, C.-L., Tien, K.-W., Lai, M.-H., Chuang, Y.-H., & 
Wu, S.-H. (2009). Capturing errors in written Chi-
nese words. Proceedings of the Forty Seventh An-
nual Meeting of the Association for Computational 
Linguistics (ACL'09), (pp. 25-28). Singapore. 
Liu, C.-L., Tien, K.-W., Lai, M.-H., Chuang, Y.-H., & 
Wu, S.-H. (2009). Phonological and logographic in-
fluences on errors in written Chinese words. Pro-
ceedings of the Seventh Workshop on Asian Lan-
guage Resources (ALR7), the Forty Seventh An-
nual Meeting of the Association for Computational 
Linguistics (ACL'09), (pp. 84-91). Singapore. 
Ma, W.-Y., & Chen, K.-J. (2003). A Bottom-up 
Merging Algorithm for Chinese. Proceedings of 
ACL workshop on Chinese Language Processing, 
(pp. 31-38). 
MOE. (2007). MOE Dictionary new edition. Taiwan: 
Ministry of Education. 
Ren, F., Shi, H., & Zhou, Q. (1994). A hybrid ap-
proach to automatic Chinese text checking and er-
ror correction. In Proceedings of the ARPA Work 
shop on Human Language Technology, (pp. 76-81). 
Zhang, L., Zhou, M., Huang, C., & Lu, M. (2000). 
Approach in automatic detection and correction of 
errors in Chinese text based on feature and learning. 
Proceedings of the 3rd world congress on Intelli-
gent Control and Automation, (pp. 2744-2748). He-
fei. 
Zhang, L., Zhou, M., Huang, C., & Sun, M. (2000). 
Automatic Chinese Text Error Correction Approach 
Based-on Fast Approximate Chinese Word-
Matching Algorithm. Proceedings of the 3rd world 
congress on Intelligent Control and Automation, (pp. 
2739-2743). Hefei. 
 
Table 4. Results of our initial system  
  Top 1 2 3 4 5 6 7 8 9 10 
TM 
P 5.74% 5.63% 5.21% 5.02% 4.90% 4.65% 4.36% 4.12% 4.06% 3.95% 
D 29.23% 41.25% 45.36% 49.17% 52.00% 53.47% 54.94% 55.13% 56.79% 57.09% 
C 26.00% 36.75% 40.08% 43.40% 45.65% 46.33% 46.92% 46.82% 48.00% 48.58% 
F 9.40% 9.76% 9.23% 8.99% 8.85% 8.46% 7.99% 7.58% 7.49% 7.31% 
LMM 
P 14.28% 
D 58.06% 
C 50.63% 
F 22.28% 
PM 
P 55.52% 60.03% 60.60% 61.58% 60.65% 61.68% 60.51% 61.19% 58.82% 59.03% 
D 21.60% 29.52% 31.28% 32.74% 34.21% 34.31% 34.31% 33.91% 35.19% 34.79% 
C 21.60% 29.42% 31.18% 32.64% 34.01% 34.11% 34.01% 33.62% 34.89% 34.50% 
F 31.10% 39.49% 41.17% 42.67% 43.58% 43.93% 43.55% 43.40% 43.80% 43.55% 
DM 
P 7.32% 6.15% 5.64% 5.33% 5.11% 4.87% 4.62% 4.42% 4.30% 4.19% 
D 62.75% 65.59% 67.44% 69.40% 70.38% 71.06% 71.94% 72.23% 72.62% 72.72%
C 54.05% 56.69% 58.16% 59.62% 60.60% 60.99% 61.68% 61.77% 61.58% 61.68%
F 12.89% 11.10% 10.28% 9.79% 9.43% 9.02% 8.60% 8.25% 8.04% 7.85% 
 
Table 5. Results of our system after adding more knowledge and enlarged the train set 
  Top 1 2 3 4 5 6 7 8 9 10 
TM 
P 7.31% 6.45% 5.73% 5.41% 5.12% 4.83% 4.51% 4.26% 4.20% 4.08% 
D 37.93% 47.70% 50.15% 53.18% 54.45% 55.62% 56.89% 57.09% 58.75% 59.04% 
C 34.70% 43.21% 44.87% 47.41% 47.70% 48.48% 48.88% 48.78% 49.95% 50.54% 
F 12.08% 11.23% 10.17% 9.70% 9.25% 8.79% 8.26% 7.84% 7.74% 7.55% 
LMM 
P 14.03% 
D 63.14% 
C 55.52% 
F 22.40% 
PM 
P 59.95% 62.72% 62.50% 62.88% 60.66% 61.72% 59.51% 60.29% 58.08% 58.54% 
D 27.66% 34.21% 35.19% 36.26% 35.58% 35.77% 35.77% 35.48% 36.85% 36.85% 
C 27.66% 34.11% 35.09% 36.16% 35.48% 35.67% 35.58% 35.28% 36.65% 36.65% 
F 37.85% 44.19% 44.95% 45.92% 44.77% 45.21% 44.53% 44.51% 44.94% 45.08%
DM 
P 7.76% 6.46% 5.85% 5.51% 5.28% 5.04% 4.78% 4.57% 4.45% 4.33% 
D 69.50% 71.26% 72.04% 73.50% 74.48% 75.26% 75.95% 76.05% 76.34% 76.34%
C 60.50% 62.17% 62.65% 63.73% 64.71% 65.29% 65.78% 65.68% 65.39% 65.39%
F 13.76% 11.70% 10.70% 10.14% 9.76% 9.36% 8.91% 8.55% 8.33% 8.12% 
 
Table 6. Results of methods in previous works 
  Top 1 2 3 4 5 6 7 8 9 10 
Chang 
P 2.82% 1.95% 1.63% 1.43% 1.25% 1.13% 1.07% 0.98% 0.94% 0.91% 
D 72.04% 81.72% 84.55% 88.27% 89.54% 90.32% 91.50% 91.50% 91.79% 91.59%
C 27.66% 39.10% 43.30% 45.45% 44.77% 45.16% 46.33% 45.26% 43.30% 44.28%
F 5.11% 3.71% 3.14% 2.77% 2.43% 2.21% 2.08% 1.92% 1.83% 1.77% 
Lin 
P 3.59% 3.19% 2.93% 2.82% 2.60% 2.51% 2.39% 2.35% 2.32% 2.31% 
D 25.12% 28.93% 29.91% 31.18% 30.98% 31.37% 31.18% 31.57% 32.16% 32.74%
C 19.45% 25.51% 26.78% 27.95% 28.05% 28.15% 28.25% 28.25% 28.73% 29.42%
F 6.06% 5.67% 5.28% 5.12% 4.76% 4.61% 4.41% 4.34% 4.29% 4.28% 
Huang 
P 27.02% 25.81% 25.02% 24.05% 23.30% 22.54% 22.04% 21.16% 20.98% 20.62%
D 10.75% 17.79% 23.06% 26.00% 28.54% 30.49% 31.37% 31.86% 33.33% 33.43%
C 8.30% 12.02% 15.54% 17.00% 17.39% 18.57% 19.64% 18.76% 17.69% 18.27%
F 12.70% 16.40% 19.17% 19.92% 19.92% 20.36% 20.77% 19.89% 19.20% 19.37%
 
Table 7. Results of methods in previous works on sentences with errors 
  Top 1 2 3 4 5 6 7 8 9 10 
Chang 
P 42.94% 37.21% 33.30% 31.18% 29.31% 27.19% 25.98% 24.48% 23.61% 23.14%
D 72.33% 81.62% 84.55% 88.26% 89.63% 90.51% 91.78% 91.79% 92.08% 91.89%
C 27.95% 39.58% 43.98% 46.23% 45.65% 46.04% 47.31% 46.14% 44.28% 45.26%
F 33.86% 38.36% 37.90% 37.24% 35.70% 34.19% 33.54% 31.99% 30.80% 30.62%
Lin 
P 60.59% 59.33% 57.32% 57.19% 55.10% 55.35% 54.27% 53.88% 53.80% 53.97%
D 25.70% 29.52% 30.59% 31.86% 31.67% 32.35% 32.25% 32.55% 33.13% 33.82%
C 19.55% 25.80% 27.37% 28.64% 29.03% 29.52% 29.61% 29.52% 30.00% 30.69%
F 29.56% 35.96% 37.05% 38.17% 38.03% 38.50% 38.32% 38.14% 38.52% 39.13%
Huang 
P 84.16% 76.99% 78.51% 76.11% 73.66% 74.07% 73.21% 70.19% 66.23% 66.66%
D 9.87% 16.03% 20.72% 23.36% 25.70% 27.37% 28.05% 28.54% 29.91% 29.91%
C 7.62% 10.85% 14.17% 15.64% 15.83% 16.71% 17.79% 17.20% 16.12% 16.61%
F 13.97% 19.02% 24.01% 25.95% 26.06% 27.27% 28.62% 27.63% 25.93% 26.59%
PM 
P 96.72% 96.66% 96.76% 96.57% 96.51% 96.54% 96.54% 96.23% 96.11% 96.10%
D 25.90% 31.09% 32.16% 33.04% 32.45% 32.75% 32.75% 32.45% 33.82% 33.72%
C 25.90% 30.98% 32.06% 32.94% 32.36% 32.65% 32.55% 32.26% 33.63% 33.53%
F 40.86% 46.92% 48.16% 49.13% 48.46% 48.80% 48.69% 48.32% 49.82% 49.71%
DM 
P 61.83% 58.45% 56.46% 54.75% 54.21% 53.48% 52.80% 51.53% 51.15% 50.45%
D 69.20% 70.97% 71.74% 73.22% 74.19% 74.98% 75.66% 75.76% 76.05% 76.05%
C 55.62% 57.28% 57.77% 58.84% 59.82% 60.41% 60.90% 60.80% 60.51% 60.51%
F 58.56% 57.86% 57.11% 56.72% 56.88% 56.73% 56.56% 55.78% 55.44% 55.03%
 
